file_path,api_count,code
capsule_layer.py,12,"b'""""""Capsule layer\n\nPyTorch implementation of CapsNet in Sabour, Hinton et al.\'s paper\nDynamic Routing Between Capsules. NIPS 2017.\nhttps://arxiv.org/abs/1710.09829\n\nAuthor: Cedric Chee\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport utils\n\n\nclass CapsuleLayer(nn.Module):\n    """"""\n    The core implementation of the idea of capsules\n    """"""\n\n    def __init__(self, in_unit, in_channel, num_unit, unit_size, use_routing,\n                 num_routing, cuda_enabled):\n        super(CapsuleLayer, self).__init__()\n\n        self.in_unit = in_unit\n        self.in_channel = in_channel\n        self.num_unit = num_unit\n        self.use_routing = use_routing\n        self.num_routing = num_routing\n        self.cuda_enabled = cuda_enabled\n\n        if self.use_routing:\n            """"""\n            Based on the paper, DigitCaps which is capsule layer(s) with\n            capsule inputs use a routing algorithm that uses this weight matrix, Wij\n            """"""\n            # weight shape:\n            # [1 x primary_unit_size x num_classes x output_unit_size x num_primary_unit]\n            # == [1 x 1152 x 10 x 16 x 8]\n            self.weight = nn.Parameter(torch.randn(1, in_channel, num_unit, unit_size, in_unit))\n        else:\n            """"""\n            According to the CapsNet architecture section in the paper,\n            we have routing only between two consecutive capsule layers (e.g. PrimaryCapsules and DigitCaps).\n            No routing is used between Conv1 and PrimaryCapsules.\n\n            This means PrimaryCapsules is composed of several convolutional units.\n            """"""\n            # Define 8 convolutional units.\n            self.conv_units = nn.ModuleList([\n                nn.Conv2d(self.in_channel, 32, 9, 2) for u in range(self.num_unit)\n            ])\n\n    def forward(self, x):\n        if self.use_routing:\n            # Currently used by DigitCaps layer.\n            return self.routing(x)\n        else:\n            # Currently used by PrimaryCaps layer.\n            return self.no_routing(x)\n\n    def routing(self, x):\n        """"""\n        Routing algorithm for capsule.\n\n        :input: tensor x of shape [128, 8, 1152]\n\n        :return: vector output of capsule j\n        """"""\n        batch_size = x.size(0)\n\n        x = x.transpose(1, 2) # dim 1 and dim 2 are swapped. out tensor shape: [128, 1152, 8]\n\n        # Stacking and adding a dimension to a tensor.\n        # stack ops output shape: [128, 1152, 10, 8]\n        # unsqueeze ops output shape: [128, 1152, 10, 8, 1]\n        x = torch.stack([x] * self.num_unit, dim=2).unsqueeze(4)\n\n        # Convert single weight to batch weight.\n        # [1 x 1152 x 10 x 16 x 8] to: [128, 1152, 10, 16, 8]\n        batch_weight = torch.cat([self.weight] * batch_size, dim=0)\n\n        # u_hat is ""prediction vectors"" from the capsules in the layer below.\n        # Transform inputs by weight matrix.\n        # Matrix product of 2 tensors with shape: [128, 1152, 10, 16, 8] x [128, 1152, 10, 8, 1]\n        # u_hat shape: [128, 1152, 10, 16, 1]\n        u_hat = torch.matmul(batch_weight, x)\n\n        # All the routing logits (b_ij in the paper) are initialized to zero.\n        # self.in_channel = primary_unit_size = 32 * 6 * 6 = 1152\n        # self.num_unit = num_classes = 10\n        # b_ij shape: [1, 1152, 10, 1]\n        b_ij = Variable(torch.zeros(1, self.in_channel, self.num_unit, 1))\n        if self.cuda_enabled:\n            b_ij = b_ij.cuda()\n\n        # From the paper in the ""Capsules on MNIST"" section,\n        # the sample MNIST test reconstructions of a CapsNet with 3 routing iterations.\n        num_iterations = self.num_routing\n\n        for iteration in range(num_iterations):\n            # Routing algorithm\n\n            # Calculate routing or also known as coupling coefficients (c_ij).\n            # c_ij shape: [1, 1152, 10, 1]\n            c_ij = F.softmax(b_ij, dim=2)  # Convert routing logits (b_ij) to softmax.\n            # c_ij shape from: [128, 1152, 10, 1] to: [128, 1152, 10, 1, 1]\n            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n\n            # Implement equation 2 in the paper.\n            # s_j is total input to a capsule, is a weigthed sum over all ""prediction vectors"".\n            # u_hat is weighted inputs, prediction \xcb\x86uj|i made by capsule i.\n            # c_ij * u_hat shape: [128, 1152, 10, 16, 1]\n            # s_j output shape: [batch_size=128, 1, 10, 16, 1]\n            # Sum of Primary Capsules outputs, 1152D becomes 1D.\n            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n\n            # Squash the vector output of capsule j.\n            # v_j shape: [batch_size, weighted sum of PrimaryCaps output,\n            #             num_classes, output_unit_size from u_hat, 1]\n            # == [128, 1, 10, 16, 1]\n            # So, the length of the output vector of a capsule is 16, which is in dim 3.\n            v_j = utils.squash(s_j, dim=3)\n\n            # in_channel is 1152.\n            # v_j1 shape: [128, 1152, 10, 16, 1]\n            v_j1 = torch.cat([v_j] * self.in_channel, dim=1)\n\n            # The agreement.\n            # Transpose u_hat with shape [128, 1152, 10, 16, 1] to [128, 1152, 10, 1, 16],\n            # so we can do matrix product u_hat and v_j1.\n            # u_vj1 shape: [1, 1152, 10, 1]\n            u_vj1 = torch.matmul(u_hat.transpose(3, 4), v_j1).squeeze(4).mean(dim=0, keepdim=True)\n\n            # Update routing (b_ij) by adding the agreement to the initial logit.\n            b_ij = b_ij + u_vj1\n\n        return v_j.squeeze(1) # shape: [128, 10, 16, 1]\n\n    def no_routing(self, x):\n        """"""\n        Get output for each unit.\n        A unit has batch, channels, height, width.\n        An example of a unit output shape is [128, 32, 6, 6]\n\n        :return: vector output of capsule j\n        """"""\n        # Create 8 convolutional unit.\n        # A convolutional unit uses normal convolutional layer with a non-linearity (squash).\n        unit = [self.conv_units[i](x) for i, l in enumerate(self.conv_units)]\n\n        # Stack all unit outputs.\n        # Stacked of 8 unit output shape: [128, 8, 32, 6, 6]\n        unit = torch.stack(unit, dim=1)\n\n        batch_size = x.size(0)\n\n        # Flatten the 32 of 6x6 grid into 1152.\n        # Shape: [128, 8, 1152]\n        unit = unit.view(batch_size, self.num_unit, -1)\n\n        # Add non-linearity\n        # Return squashed outputs of shape: [128, 8, 1152]\n        return utils.squash(unit, dim=2) # dim 2 is the third dim (1152D array) in our tensor\n'"
conv_layer.py,1,"b'""""""Convolutional layer\n\nPyTorch implementation of CapsNet in Sabour, Hinton et al.\'s paper\nDynamic Routing Between Capsules. NIPS 2017.\nhttps://arxiv.org/abs/1710.09829\n\nAuthor: Cedric Chee\n""""""\n\nimport torch\nimport torch.nn as nn\n\n\nclass ConvLayer(nn.Module):\n    """"""\n    Conventional Conv2d layer\n    """"""\n\n    def __init__(self, in_channel, out_channel, kernel_size):\n        super(ConvLayer, self).__init__()\n\n        self.conv0 = nn.Conv2d(in_channels=in_channel,\n                               out_channels=out_channel,\n                               kernel_size=kernel_size,\n                               stride=1)\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        """"""Forward pass""""""\n        # x shape: [128, 1, 28, 28]\n        # out_conv0 shape: [128, 256, 20, 20]\n        out_conv0 = self.conv0(x)\n        # out_relu shape: [128, 256, 20, 20]\n        out_relu = self.relu(out_conv0)\n        return out_relu\n'"
decoder.py,3,"b'""""""Decoder Network\n\nPyTorch implementation of CapsNet in Sabour, Hinton et al.\'s paper\nDynamic Routing Between Capsules. NIPS 2017.\nhttps://arxiv.org/abs/1710.09829\n\nAuthor: Cedric Chee\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport utils\n\n\nclass Decoder(nn.Module):\n    """"""\n    Implement Decoder structure in section 4.1, Figure 2 to reconstruct a digit\n    from the `DigitCaps` layer representation.\n\n    The decoder network consists of 3 fully connected layers. For each\n    [10, 16] output, we mask out the incorrect predictions, and send\n    the [16,] vector to the decoder network to reconstruct a [784,] size\n    image.\n\n    This Decoder network is used in training and prediction (testing).\n    """"""\n\n    def __init__(self, num_classes, output_unit_size, input_width,\n                 input_height, num_conv_in_channel, cuda_enabled):\n        """"""\n        The decoder network consists of 3 fully connected layers, with\n        512, 1024, 784 (or 3072 for CIFAR10) neurons each.\n        """"""\n        super(Decoder, self).__init__()\n\n        self.cuda_enabled = cuda_enabled\n\n        fc1_output_size = 512\n        fc2_output_size = 1024\n        self.fc3_output_size = input_width * input_height * num_conv_in_channel\n        self.fc1 = nn.Linear(num_classes * output_unit_size, fc1_output_size) # input dim 10 * 16.\n        self.fc2 = nn.Linear(fc1_output_size, fc2_output_size)\n        self.fc3 = nn.Linear(fc2_output_size, self.fc3_output_size)\n        # Activation functions\n        self.relu = nn.ReLU(inplace=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x, target):\n        """"""\n        We send the outputs of the `DigitCaps` layer, which is a\n        [batch_size, 10, 16] size tensor into the Decoder network, and\n        reconstruct a [batch_size, fc3_output_size] size tensor representing the image.\n\n        Args:\n            x: [batch_size, 10, 16] The output of the digit capsule.\n            target: [batch_size, 10] One-hot MNIST dataset labels.\n\n        Returns:\n            reconstruction: [batch_size, fc3_output_size] Tensor of reconstructed images.\n        """"""\n        batch_size = target.size(0)\n\n        """"""\n        First, do masking.\n        """"""\n        # Method 1: mask with y.\n        # Note: we have not implement method 2 which is masking with true label.\n        # masked_caps shape: [batch_size, 10, 16, 1]\n        masked_caps = utils.mask(x, self.cuda_enabled)\n\n        """"""\n        Second, reconstruct the images with 3 Fully Connected layers.\n        """"""\n        # vector_j shape: [batch_size, 160=10*16]\n        vector_j = masked_caps.view(x.size(0), -1) # reshape the masked_caps tensor\n\n        # Forward pass of the network\n        fc1_out = self.relu(self.fc1(vector_j))\n        fc2_out = self.relu(self.fc2(fc1_out)) # shape: [batch_size, 1024]\n        reconstruction = self.sigmoid(self.fc3(fc2_out)) # shape: [batch_size, fc3_output_size]\n\n        assert reconstruction.size() == torch.Size([batch_size, self.fc3_output_size])\n\n        return reconstruction\n'"
main.py,12,"b'""""""\nPyTorch implementation of CapsNet in Sabour, Hinton et al.\'s paper\nDynamic Routing Between Capsules. NIPS 2017.\nhttps://arxiv.org/abs/1710.09829\n\nUsage:\n    python main.py\n    python main.py --epochs 30\n    python main.py --epochs 30 --num-routing 1\n\nAuthor: Cedric Chee\n""""""\n\nfrom __future__ import print_function\nimport argparse\nfrom timeit import default_timer as timer\nimport os\n\nimport torch\nimport torch.optim as optim\nfrom torch.backends import cudnn\nfrom torch.autograd import Variable\nimport torchvision.utils as vutils\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\n\nimport utils\nfrom model import Net\n\n\ndef train(model, data_loader, optimizer, epoch, writer):\n    """"""\n    Train CapsuleNet model on training set\n\n    Args:\n        model: The CapsuleNet model.\n        data_loader: An interator over the dataset. It combines a dataset and a sampler.\n        optimizer: Optimization algorithm.\n        epoch: Current epoch.\n    """"""\n    print(\'===> Training mode\')\n\n    num_batches = len(data_loader) # iteration per epoch. e.g: 469\n    total_step = args.epochs * num_batches\n    epoch_tot_acc = 0\n\n    # Switch to train mode\n    model.train()\n\n    if args.cuda:\n        # When we wrap a Module in DataParallel for multi-GPUs\n        model = model.module\n\n    start_time = timer()\n\n    for batch_idx, (data, target) in enumerate(tqdm(data_loader, unit=\'batch\')):\n        batch_size = data.size(0)\n        global_step = batch_idx + (epoch * num_batches) - num_batches\n\n        labels = target\n        target_one_hot = utils.one_hot_encode(target, length=args.num_classes)\n        assert target_one_hot.size() == torch.Size([batch_size, 10])\n\n        data, target = Variable(data), Variable(target_one_hot)\n\n        if args.cuda:\n            data = data.cuda()\n            target = target.cuda()\n\n        # Train step - forward, backward and optimize\n        optimizer.zero_grad()\n        output = model(data) # output from DigitCaps (out_digit_caps)\n        loss, margin_loss, recon_loss = model.loss(data, output, target)\n        loss.backward()\n        optimizer.step()\n\n        # Calculate accuracy for each step and average accuracy for each epoch\n        acc = utils.accuracy(output, labels, args.cuda)\n        epoch_tot_acc += acc\n        epoch_avg_acc = epoch_tot_acc / (batch_idx + 1)\n\n        # TensorBoard logging\n        # 1) Log the scalar values\n        writer.add_scalar(\'train/total_loss\', loss.data[0], global_step)\n        writer.add_scalar(\'train/margin_loss\', margin_loss.data[0], global_step)\n        if args.use_reconstruction_loss:\n            writer.add_scalar(\'train/reconstruction_loss\', recon_loss.data[0], global_step)\n        writer.add_scalar(\'train/batch_accuracy\', acc, global_step)\n        writer.add_scalar(\'train/accuracy\', epoch_avg_acc, global_step)\n\n        # 2) Log values and gradients of the parameters (histogram)\n        for tag, value in model.named_parameters():\n            tag = tag.replace(\'.\', \'/\')\n            writer.add_histogram(tag, utils.to_np(value), global_step)\n            writer.add_histogram(tag + \'/grad\', utils.to_np(value.grad), global_step)\n\n        # Print losses\n        if batch_idx % args.log_interval == 0:\n            template = \'Epoch {}/{}, \' \\\n                    \'Step {}/{}: \' \\\n                    \'[Total loss: {:.6f},\' \\\n                    \'\\tMargin loss: {:.6f},\' \\\n                    \'\\tReconstruction loss: {:.6f},\' \\\n                    \'\\tBatch accuracy: {:.6f},\' \\\n                    \'\\tAccuracy: {:.6f}]\'\n            tqdm.write(template.format(\n                epoch,\n                args.epochs,\n                global_step,\n                total_step,\n                loss.data[0],\n                margin_loss.data[0],\n                recon_loss.data[0] if args.use_reconstruction_loss else 0,\n                acc,\n                epoch_avg_acc))\n\n    # Print time elapsed for an epoch\n    end_time = timer()\n    print(\'Time elapsed for epoch {}: {:.0f}s.\'.format(epoch, end_time - start_time))\n\n\ndef test(model, data_loader, num_train_batches, epoch, writer):\n    """"""\n    Evaluate model on validation set\n\n    Args:\n        model: The CapsuleNet model.\n        data_loader: An interator over the dataset. It combines a dataset and a sampler.\n    """"""\n    print(\'===> Evaluate mode\')\n\n    # Switch to evaluate mode\n    model.eval()\n\n    if args.cuda:\n        # When we wrap a Module in DataParallel for multi-GPUs\n        model = model.module\n\n    loss = 0\n    margin_loss = 0\n    recon_loss = 0\n\n    correct = 0\n\n    num_batches = len(data_loader)\n\n    global_step = epoch * num_train_batches + num_train_batches\n\n    for data, target in data_loader:\n        batch_size = data.size(0)\n        target_indices = target\n        target_one_hot = utils.one_hot_encode(target_indices, length=args.num_classes)\n        assert target_one_hot.size() == torch.Size([batch_size, 10])\n\n        data, target = Variable(data, volatile=True), Variable(target_one_hot)\n\n        if args.cuda:\n            data = data.cuda()\n            target = target.cuda()\n\n        # Output predictions\n        output = model(data) # output from DigitCaps (out_digit_caps)\n\n        # Sum up batch loss\n        t_loss, m_loss, r_loss = model.loss(data, output, target, size_average=False)\n        loss += t_loss.data[0]\n        margin_loss += m_loss.data[0]\n        recon_loss += r_loss.data[0]\n\n        # Count number of correct predictions\n        # v_magnitude shape: [128, 10, 1, 1]\n        v_magnitude = torch.sqrt((output**2).sum(dim=2, keepdim=True))\n        # pred shape: [128, 1, 1, 1]\n        pred = v_magnitude.data.max(1, keepdim=True)[1].cpu()\n        correct += pred.eq(target_indices.view_as(pred)).sum()\n\n    # Get the reconstructed images of the last batch\n    if args.use_reconstruction_loss:\n        reconstruction = model.decoder(output, target)\n        # Input image size and number of channel.\n        # By default, for MNIST, the image width and height is 28x28 and 1 channel for black/white.\n        image_width = args.input_width\n        image_height = args.input_height\n        image_channel = args.num_conv_in_channel\n        recon_img = reconstruction.view(-1, image_channel, image_width, image_height)\n        assert recon_img.size() == torch.Size([batch_size, image_channel, image_width, image_height])\n\n        # Save the image into file system\n        utils.save_image(recon_img, \'results/recons_image_test_{}_{}.png\'.format(epoch, global_step))\n        utils.save_image(data, \'results/original_image_test_{}_{}.png\'.format(epoch, global_step))\n\n        # Add and visualize the image in TensorBoard\n        recon_img = vutils.make_grid(recon_img.data, normalize=True, scale_each=True)\n        original_img = vutils.make_grid(data.data, normalize=True, scale_each=True)\n        writer.add_image(\'test/recons-image-{}-{}\'.format(epoch, global_step), recon_img, global_step)\n        writer.add_image(\'test/original-image-{}-{}\'.format(epoch, global_step), original_img, global_step)\n\n    # Log test losses\n    loss /= num_batches\n    margin_loss /= num_batches\n    recon_loss /= num_batches\n\n    # Log test accuracies\n    num_test_data = len(data_loader.dataset)\n    accuracy = correct / num_test_data\n    accuracy_percentage = 100. * accuracy\n\n    # TensorBoard logging\n    # 1) Log the scalar values\n    writer.add_scalar(\'test/total_loss\', loss, global_step)\n    writer.add_scalar(\'test/margin_loss\', margin_loss, global_step)\n    if args.use_reconstruction_loss:\n        writer.add_scalar(\'test/reconstruction_loss\', recon_loss, global_step)\n    writer.add_scalar(\'test/accuracy\', accuracy, global_step)\n\n    # Print test losses and accuracy\n    print(\'Test: [Loss: {:.6f},\' \\\n        \'\\tMargin loss: {:.6f},\' \\\n        \'\\tReconstruction loss: {:.6f}]\'.format(\n            loss,\n            margin_loss,\n            recon_loss if args.use_reconstruction_loss else 0))\n    print(\'Test Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n        correct, num_test_data, accuracy_percentage))\n\n\ndef main():\n    """"""The main function\n    Entry point.\n    """"""\n    global args\n\n    # Setting the hyper parameters\n    parser = argparse.ArgumentParser(description=\'Example of Capsule Network\')\n    parser.add_argument(\'--epochs\', type=int, default=10,\n                        help=\'number of training epochs. default=10\')\n    parser.add_argument(\'--lr\', type=float, default=0.01,\n                        help=\'learning rate. default=0.01\')\n    parser.add_argument(\'--batch-size\', type=int, default=128,\n                        help=\'training batch size. default=128\')\n    parser.add_argument(\'--test-batch-size\', type=int,\n                        default=128, help=\'testing batch size. default=128\')\n    parser.add_argument(\'--log-interval\', type=int, default=10,\n                        help=\'how many batches to wait before logging training status. default=10\')\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA training. default=false\')\n    parser.add_argument(\'--threads\', type=int, default=4,\n                        help=\'number of threads for data loader to use. default=4\')\n    parser.add_argument(\'--seed\', type=int, default=42,\n                        help=\'random seed for training. default=42\')\n    parser.add_argument(\'--num-conv-out-channel\', type=int, default=256,\n                        help=\'number of channels produced by the convolution. default=256\')\n    parser.add_argument(\'--num-conv-in-channel\', type=int, default=1,\n                        help=\'number of input channels to the convolution. default=1\')\n    parser.add_argument(\'--num-primary-unit\', type=int, default=8,\n                        help=\'number of primary unit. default=8\')\n    parser.add_argument(\'--primary-unit-size\', type=int,\n                        default=1152, help=\'primary unit size is 32 * 6 * 6. default=1152\')\n    parser.add_argument(\'--num-classes\', type=int, default=10,\n                        help=\'number of digit classes. 1 unit for one MNIST digit. default=10\')\n    parser.add_argument(\'--output-unit-size\', type=int,\n                        default=16, help=\'output unit size. default=16\')\n    parser.add_argument(\'--num-routing\', type=int,\n                        default=3, help=\'number of routing iteration. default=3\')\n    parser.add_argument(\'--use-reconstruction-loss\', type=utils.str2bool, nargs=\'?\', default=True,\n                        help=\'use an additional reconstruction loss. default=True\')\n    parser.add_argument(\'--regularization-scale\', type=float, default=0.0005,\n                        help=\'regularization coefficient for reconstruction loss. default=0.0005\')\n    parser.add_argument(\'--dataset\', help=\'the name of dataset (mnist, cifar10)\', default=\'mnist\')\n    parser.add_argument(\'--input-width\', type=int,\n                        default=28, help=\'input image width to the convolution. default=28 for MNIST\')\n    parser.add_argument(\'--input-height\', type=int,\n                        default=28, help=\'input image height to the convolution. default=28 for MNIST\')\n\n    args = parser.parse_args()\n\n    print(args)\n\n    # Check GPU or CUDA is available\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n\n    # Get reproducible results by manually seed the random number generator\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    # Load data\n    train_loader, test_loader = utils.load_data(args)\n\n    # Build Capsule Network\n    print(\'===> Building model\')\n    model = Net(num_conv_in_channel=args.num_conv_in_channel,\n                num_conv_out_channel=args.num_conv_out_channel,\n                num_primary_unit=args.num_primary_unit,\n                primary_unit_size=args.primary_unit_size,\n                num_classes=args.num_classes,\n                output_unit_size=args.output_unit_size,\n                num_routing=args.num_routing,\n                use_reconstruction_loss=args.use_reconstruction_loss,\n                regularization_scale=args.regularization_scale,\n                input_width=args.input_width,\n                input_height=args.input_height,\n                cuda_enabled=args.cuda)\n\n    if args.cuda:\n        print(\'Utilize GPUs for computation\')\n        print(\'Number of GPU available\', torch.cuda.device_count())\n        model.cuda()\n        cudnn.benchmark = True\n        model = torch.nn.DataParallel(model)\n\n    # Print the model architecture and parameters\n    print(\'Model architectures:\\n{}\\n\'.format(model))\n\n    print(\'Parameters and size:\')\n    for name, param in model.named_parameters():\n        print(\'{}: {}\'.format(name, list(param.size())))\n\n    # CapsNet has:\n    # - 8.2M parameters and 6.8M parameters without the reconstruction subnet on MNIST.\n    # - 11.8M parameters and 8.0M parameters without the reconstruction subnet on CIFAR10.\n    num_params = sum([param.nelement() for param in model.parameters()])\n\n    # The coupling coefficients c_ij are not included in the parameter list,\n    # we need to add them manually, which is 1152 * 10 = 11520 (on MNIST) or 2048 * 10 (on CIFAR10)\n    print(\'\\nTotal number of parameters: {}\\n\'.format(num_params + (11520 if args.dataset == \'mnist\' else 20480)))\n\n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n    # Make model checkpoint directory\n    if not os.path.exists(\'results/trained_model\'):\n        os.makedirs(\'results/trained_model\')\n\n    # Set the logger\n    writer = SummaryWriter()\n\n    # Train and test\n    for epoch in range(1, args.epochs + 1):\n        train(model, train_loader, optimizer, epoch, writer)\n        test(model, test_loader, len(train_loader), epoch, writer)\n\n        # Save model checkpoint\n        utils.checkpoint({\n            \'epoch\': epoch + 1,\n            \'state_dict\': model.state_dict(),\n            \'optimizer\': optimizer.state_dict()\n        }, epoch)\n\n    writer.close()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
model.py,7,"b'""""""CapsNet Architecture\n\nPyTorch implementation of CapsNet in Sabour, Hinton et al.\'s paper\nDynamic Routing Between Capsules. NIPS 2017.\nhttps://arxiv.org/abs/1710.09829\n\nAuthor: Cedric Chee\n""""""\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom conv_layer import ConvLayer\nfrom capsule_layer import CapsuleLayer\nfrom decoder import Decoder\n\n\nclass Net(nn.Module):\n    """"""\n    A simple CapsNet with 3 layers\n    """"""\n\n    def __init__(self, num_conv_in_channel, num_conv_out_channel, num_primary_unit,\n                 primary_unit_size, num_classes, output_unit_size, num_routing,\n                 use_reconstruction_loss, regularization_scale, input_width, input_height,\n                 cuda_enabled):\n        """"""\n        In the constructor we instantiate one ConvLayer module and two CapsuleLayer modules\n        and assign them as member variables.\n        """"""\n        super(Net, self).__init__()\n\n        self.cuda_enabled = cuda_enabled\n\n        # Configurations used for image reconstruction.\n        self.use_reconstruction_loss = use_reconstruction_loss\n        # Input image size and number of channel.\n        # By default, for MNIST, the image width and height is 28x28\n        # and 1 channel for black/white.\n        self.image_width = input_width\n        self.image_height = input_height\n        self.image_channel = num_conv_in_channel\n\n        # Also known as lambda reconstruction. Default value is 0.0005.\n        # We use sum of squared errors (SSE) similar to paper.\n        self.regularization_scale = regularization_scale\n\n        # Layer 1: Conventional Conv2d layer.\n        self.conv1 = ConvLayer(in_channel=num_conv_in_channel,\n                               out_channel=num_conv_out_channel,\n                               kernel_size=9)\n\n        # PrimaryCaps\n        # Layer 2: Conv2D layer with `squash` activation.\n        self.primary = CapsuleLayer(in_unit=0,\n                                    in_channel=num_conv_out_channel,\n                                    num_unit=num_primary_unit,\n                                    unit_size=primary_unit_size, # capsule outputs\n                                    use_routing=False,\n                                    num_routing=num_routing,\n                                    cuda_enabled=cuda_enabled)\n\n        # DigitCaps\n        # Final layer: Capsule layer where the routing algorithm is.\n        self.digits = CapsuleLayer(in_unit=num_primary_unit,\n                                   in_channel=primary_unit_size,\n                                   num_unit=num_classes,\n                                   unit_size=output_unit_size, # 16D capsule per digit class\n                                   use_routing=True,\n                                   num_routing=num_routing,\n                                   cuda_enabled=cuda_enabled)\n\n        # Reconstruction network\n        if use_reconstruction_loss:\n            self.decoder = Decoder(num_classes, output_unit_size, input_width,\n                                   input_height, num_conv_in_channel, cuda_enabled)\n\n    def forward(self, x):\n        """"""\n        Defines the computation performed at every forward pass.\n        """"""\n        # x shape: [128, 1, 28, 28]. 128 is for the batch size.\n        # out_conv1 shape: [128, 256, 20, 20]\n        out_conv1 = self.conv1(x)\n        # out_primary_caps shape: [128, 8, 1152].\n        # Total PrimaryCapsules has [32 \xc3\x97 6 \xc3\x97 6 = 1152] capsule outputs.\n        out_primary_caps = self.primary(out_conv1)\n        # out_digit_caps shape: [128, 10, 16, 1]\n        # batch size: 128, 10 digit class, 16D capsule per digit class.\n        out_digit_caps = self.digits(out_primary_caps)\n        return out_digit_caps\n\n    def loss(self, image, out_digit_caps, target, size_average=True):\n        """"""Custom loss function\n\n        Args:\n            image: [batch_size, 1, 28, 28] MNIST samples.\n            out_digit_caps: [batch_size, 10, 16, 1] The output from `DigitCaps` layer.\n            target: [batch_size, 10] One-hot MNIST dataset labels.\n            size_average: A boolean to enable mean loss (average loss over batch size).\n\n        Returns:\n            total_loss: A scalar Variable of total loss.\n            m_loss: A scalar of margin loss.\n            recon_loss: A scalar of reconstruction loss.\n        """"""\n        recon_loss = 0\n        m_loss = self.margin_loss(out_digit_caps, target)\n        if size_average:\n            m_loss = m_loss.mean()\n\n        total_loss = m_loss\n\n        if self.use_reconstruction_loss:\n            # Reconstruct the image from the Decoder network\n            reconstruction = self.decoder(out_digit_caps, target)\n            recon_loss = self.reconstruction_loss(reconstruction, image)\n\n            # Mean squared error\n            if size_average:\n                recon_loss = recon_loss.mean()\n\n            # In order to keep in line with the paper,\n            # they scale down the reconstruction loss by 0.0005\n            # so that it does not dominate the margin loss.\n            total_loss = m_loss + recon_loss * self.regularization_scale\n\n        return total_loss, m_loss, (recon_loss * self.regularization_scale)\n\n    def margin_loss(self, input, target):\n        """"""\n        Class loss\n\n        Implement equation 4 in section 3 \'Margin loss for digit existence\' in the paper.\n\n        Args:\n            input: [batch_size, 10, 16, 1] The output from `DigitCaps` layer.\n            target: target: [batch_size, 10] One-hot MNIST labels.\n\n        Returns:\n            l_c: A scalar of class loss or also know as margin loss.\n        """"""\n        batch_size = input.size(0)\n\n        # ||vc|| also known as norm.\n        v_c = torch.sqrt((input**2).sum(dim=2, keepdim=True))\n\n        # Calculate left and right max() terms.\n        zero = Variable(torch.zeros(1))\n        if self.cuda_enabled:\n            zero = zero.cuda()\n        m_plus = 0.9\n        m_minus = 0.1\n        loss_lambda = 0.5\n        max_left = torch.max(m_plus - v_c, zero).view(batch_size, -1)**2\n        max_right = torch.max(v_c - m_minus, zero).view(batch_size, -1)**2\n        t_c = target\n        # Lc is margin loss for each digit of class c\n        l_c = t_c * max_left + loss_lambda * (1.0 - t_c) * max_right\n        l_c = l_c.sum(dim=1)\n\n        return l_c\n\n    def reconstruction_loss(self, reconstruction, image):\n        """"""\n        The reconstruction loss is the sum of squared differences between\n        the reconstructed image (outputs of the logistic units) and\n        the original image (input image).\n\n        Implement section 4.1 \'Reconstruction as a regularization method\' in the paper.\n\n        Based on naturomics\'s implementation.\n\n        Args:\n            reconstruction: [batch_size, 784] Decoder outputs of reconstructed image tensor.\n            image: [batch_size, 1, 28, 28] MNIST samples.\n\n        Returns:\n            recon_error: A scalar Variable of reconstruction loss.\n        """"""\n\n        # Calculate reconstruction loss.\n        batch_size = image.size(0) # or another way recon_img.size(0)\n        # error = (recon_img - image).view(batch_size, -1)\n        image = image.view(batch_size, -1) # flatten 28x28 by reshaping to [batch_size, 784]\n        error = reconstruction - image\n        squared_error = error**2\n\n        # Scalar Variable\n        recon_error = torch.sum(squared_error, dim=1)\n\n        return recon_error\n'"
utils.py,17,"b'""""""Utilities\n\nPyTorch implementation of CapsNet in Sabour, Hinton et al.\'s paper\nDynamic Routing Between Capsules. NIPS 2017.\nhttps://arxiv.org/abs/1710.09829\n\nAuthor: Cedric Chee\n""""""\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nfrom torchvision import transforms, datasets\nimport torchvision.utils as vutils\nimport argparse\n\n\ndef one_hot_encode(target, length):\n    """"""Converts batches of class indices to classes of one-hot vectors.""""""\n    batch_s = target.size(0)\n    one_hot_vec = torch.zeros(batch_s, length)\n\n    for i in range(batch_s):\n        one_hot_vec[i, target[i]] = 1.0\n\n    return one_hot_vec\n\n\ndef checkpoint(state, epoch):\n    """"""Save checkpoint""""""\n    model_out_path = \'results/trained_model/model_epoch_{}.pth\'.format(epoch)\n    torch.save(state, model_out_path)\n    print(\'Checkpoint saved to {}\'.format(model_out_path))\n\n\ndef load_mnist(args):\n    """"""Load MNIST dataset.\n    The data is split and normalized between train and test sets.\n    """"""\n    # Normalize MNIST dataset.\n    data_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    kwargs = {\'num_workers\': args.threads,\n              \'pin_memory\': True} if args.cuda else {}\n\n    print(\'===> Loading MNIST training datasets\')\n    # MNIST dataset\n    training_set = datasets.MNIST(\n        \'./data\', train=True, download=True, transform=data_transform)\n    # Input pipeline\n    training_data_loader = DataLoader(\n        training_set, batch_size=args.batch_size, shuffle=True, **kwargs)\n\n    print(\'===> Loading MNIST testing datasets\')\n    testing_set = datasets.MNIST(\n        \'./data\', train=False, download=True, transform=data_transform)\n    testing_data_loader = DataLoader(\n        testing_set, batch_size=args.test_batch_size, shuffle=True, **kwargs)\n\n    return training_data_loader, testing_data_loader\n\n\ndef load_cifar10(args):\n    """"""Load CIFAR10 dataset.\n    The data is split and normalized between train and test sets.\n    """"""\n    # Normalize CIFAR10 dataset.\n    data_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    kwargs = {\'num_workers\': args.threads,\n              \'pin_memory\': True} if args.cuda else {}\n\n    print(\'===> Loading CIFAR10 training datasets\')\n    # CIFAR10 dataset\n    training_set = datasets.CIFAR10(\n        \'./data\', train=True, download=True, transform=data_transform)\n    # Input pipeline\n    training_data_loader = DataLoader(\n        training_set, batch_size=args.batch_size, shuffle=True, **kwargs)\n\n    print(\'===> Loading CIFAR10 testing datasets\')\n    testing_set = datasets.CIFAR10(\n        \'./data\', train=False, download=True, transform=data_transform)\n    testing_data_loader = DataLoader(\n        testing_set, batch_size=args.test_batch_size, shuffle=True, **kwargs)\n\n    return training_data_loader, testing_data_loader\n\n\ndef load_data(args):\n    """"""\n    Load dataset.\n    """"""\n    dst = args.dataset\n\n    if dst == \'mnist\':\n        return load_mnist(args)\n    elif dst == \'cifar10\':\n        return load_cifar10(args)\n    else:\n        raise Exception(\'Invalid dataset, please check the name of dataset:\', dst)\n\n\ndef squash(sj, dim=2):\n    """"""\n    The non-linear activation used in Capsule.\n    It drives the length of a large vector to near 1 and small vector to 0\n\n    This implement equation 1 from the paper.\n    """"""\n    sj_mag_sq = torch.sum(sj**2, dim, keepdim=True)\n    # ||sj||\n    sj_mag = torch.sqrt(sj_mag_sq)\n    v_j = (sj_mag_sq / (1.0 + sj_mag_sq)) * (sj / sj_mag)\n    return v_j\n\n\ndef mask(out_digit_caps, cuda_enabled=True):\n    """"""\n    In the paper, they mask out all but the activity vector of the correct digit capsule.\n\n    This means:\n    a) during training, mask all but the capsule (1x16 vector) which match the ground-truth.\n    b) during testing, mask all but the longest capsule (1x16 vector).\n\n    Args:\n        out_digit_caps: [batch_size, 10, 16] Tensor output of `DigitCaps` layer.\n\n    Returns:\n        masked: [batch_size, 10, 16, 1] The masked capsules tensors.\n    """"""\n    # a) Get capsule outputs lengths, ||v_c||\n    v_length = torch.sqrt((out_digit_caps**2).sum(dim=2))\n\n    # b) Pick out the index of longest capsule output, v_length by\n    # masking the tensor by the max value in dim=1.\n    _, max_index = v_length.max(dim=1)\n    max_index = max_index.data\n\n    # Method 1: masking with y.\n    # c) In all batches, get the most active capsule\n    # It\'s not easy to understand the indexing process with max_index\n    # as we are 3D animal.\n    batch_size = out_digit_caps.size(0)\n    masked_v = [None] * batch_size # Python list\n    for batch_ix in range(batch_size):\n        # Batch sample\n        sample = out_digit_caps[batch_ix]\n\n        # Masks out the other capsules in this sample.\n        v = Variable(torch.zeros(sample.size()))\n        if cuda_enabled:\n            v = v.cuda()\n\n        # Get the maximum capsule index from this batch sample.\n        max_caps_index = max_index[batch_ix]\n        v[max_caps_index] = sample[max_caps_index]\n        masked_v[batch_ix] = v # append v to masked_v\n\n    # Concatenates sequence of masked capsules tensors along the batch dimension.\n    masked = torch.stack(masked_v, dim=0)\n\n    return masked\n\n\ndef save_image(image, file_name):\n    """"""\n    Save a given image into an image file\n    """"""\n    # Check number of channels in an image.\n    if image.size(1) == 2:\n        # 2-channel image\n        zeros = torch.zeros(image.size(0), 1, image.size(2), image.size(3))\n        image_tensor = torch.cat([zeros, image.data.cpu()], dim=1)\n    else:\n        # Grayscale or RGB image\n        image_tensor = image.data.cpu() # get Tensor from Variable\n\n    vutils.save_image(image_tensor, file_name)\n\n\ndef accuracy(output, target, cuda_enabled=True):\n    """"""\n    Compute accuracy.\n\n    Args:\n        output: [batch_size, 10, 16, 1] The output from DigitCaps layer.\n        target: [batch_size] Labels for dataset.\n\n    Returns:\n        accuracy (float): The accuracy for a batch.\n    """"""\n    batch_size = target.size(0)\n\n    v_length = torch.sqrt((output**2).sum(dim=2, keepdim=True))\n    softmax_v = F.softmax(v_length, dim=1)\n    assert softmax_v.size() == torch.Size([batch_size, 10, 1, 1])\n\n    _, max_index = softmax_v.max(dim=1)\n    assert max_index.size() == torch.Size([batch_size, 1, 1])\n\n    pred = max_index.squeeze() #max_index.view(batch_size)\n    assert pred.size() == torch.Size([batch_size])\n\n    if cuda_enabled:\n        target = target.cuda()\n        pred = pred.cuda()\n\n    correct_pred = torch.eq(target, pred.data) # tensor\n    # correct_pred_sum = correct_pred.sum() # scalar. e.g: 6 correct out of 128 images.\n    acc = correct_pred.float().mean() # e.g: 6 / 128 = 0.046875\n\n    return acc\n\n\ndef to_np(param):\n    """"""\n    Convert values of the model parameters to numpy.array.\n    """"""\n    return param.clone().cpu().data.numpy()\n\n\ndef str2bool(v):\n    """"""\n    Parsing boolean values with argparse.\n    """"""\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n'"
