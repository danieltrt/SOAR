file_path,api_count,code
imagenet_benchmarking.py,0,"b'#!/usr/bin/env python\n# coding: utf-8\n\n# In[ ]:\n\n\n# These imports enhance Python2/3 compatibility.\nfrom __future__ import print_function, absolute_import, division, unicode_literals, with_statement\n\n\n# In[ ]:\n\n\nimport numpy as np\nimport os\nimport argparse\nfrom torchvision import datasets\nVAL_SIZE = 50000\n\n\n# In[ ]:\n\n\nmodels = {\n    ""keras"" : [\n        ""densenet121"",\n        ""densenet169"",\n        ""densenet201"",\n        ""mobilenet"",\n        ""mobilenetV2"",\n        ""nasnetmobile"",\n        ""resnet50"",\n        ""vgg16"",\n        ""vgg19"",\n        ""xception"",\n        ""inceptionresnetv2"",\n        ""inceptionv3"",\n        ""nasnetlarge"",\n    ],\n    ""pytorch"" : [\n        ""inception_v3"",\n        ""vgg11"",\n        ""vgg11_bn"",\n        ""vgg13"",\n        ""vgg13_bn"",\n        ""vgg16"",\n        ""vgg16_bn"",\n        ""vgg19"",\n        ""vgg19_bn"",\n        ""densenet121"",\n        ""densenet161"",\n        ""densenet169"",\n        ""densenet201"",\n        ""alexnet"",\n        ""squeezenet1_0"",\n        ""squeezenet1_1"",\n        ""resnet18"",\n        ""resnet34"",\n        ""resnet50"",\n        ""resnet101"",\n        ""resnet152"",\n        ],\n}\n\n\n# In[ ]:\n\n\n# Set up argument parser\nparser = argparse.ArgumentParser(description=\'PyTorch and Keras ImageNet Benchmarking\')\nparser.add_argument(\'val_dir\', metavar=\'DIR\',\n                    help=\'path to imagenet val dataset folder\')\nparser.add_argument(\'-k\', \'--keras-dir\', metavar=\'MODEL\', default= ""keras_imagenet/"",\n                    help=\'directory where Keras model outputs are stored.\')\nparser.add_argument(\'-p\', \'--pytorch-dir\', metavar=\'MODEL\', default= ""pytorch_imagenet/"",\n                    help=\'directory where PyTorch model outputs are stored.\')\nparser.add_argument(\'-i\', \'--indices_to_omit\', metavar=\'INDICES_TO_OMIT\', default= None,\n                    help=\'directory of .npy file of storing val indices to omit.\')\n\n\n# In[ ]:\n\n\ndef compute_val_acc(top5preds, top5probs, labels, indices_to_ignore = None):\n    # Create a mask of having True for each example we want to include in scoring.\n    bool_mask = np.ones(VAL_SIZE).astype(bool)\n    if indices_to_ignore is not None:\n        bool_mask[indices_to_ignore] = False\n    pred = top5preds[range(len(top5preds)), np.argmax(top5probs, axis = 1)]\n    pred = pred[bool_mask]\n    true = labels[bool_mask]\n    acc1 = sum(pred == true) / float(len(true))\n    acc5 = sum([true[i] in row for i, row in enumerate(top5preds[bool_mask])]) / float(len(true))\n    return acc1, acc5\n\n\n# In[ ]:\n\n\ndef main(args = parser.parse_args()):\n    \n    # Grab imagenet data\n    val_dataset = datasets.ImageFolder(args.val_dir)\n    img_paths, labels = (list(t) for t in zip(*val_dataset.imgs))\n    labels = np.asarray(labels)\n    \n    if args.indices_to_omit:\n        indices_to_omit = np.load(args.indices_to_omit)\n        \n    dirs = {\n        ""keras"" : args.keras_dir,\n        ""pytorch"" : args.pytorch_dir,\n    }\n            \n    data = []\n    for platform_name in [""Keras 2.2.4"", ""PyTorch 1.0""]:\n        platform = platform_name.split()[0].lower()\n    \n        # Read in data and compute accuracies\n        pred_suffix = ""_{}_imagenet_top5preds.npy"".format(platform)\n        prob_suffix = ""_{}_imagenet_top5probs.npy"".format(platform)\n        for model in models[platform]:\n            top5preds = np.load(os.path.join(dirs[platform], model + pred_suffix))\n            top5probs = np.load(os.path.join(dirs[platform], model + prob_suffix))\n            acc1, acc5 = compute_val_acc(top5preds, top5probs, labels)\n            if args.indices_to_omit:\n                acc1c, acc5c = compute_val_acc(top5preds, top5probs, labels, indices_to_omit)\n                data.append((platform_name, model, acc1, acc1c, acc5, acc5c))\n            else:            \n                data.append((platform_name, model, acc1, acc5))\n\n    # Order final accuracies\n    data.sort(key=lambda x: x[2], reverse = True)\n\n    # Compute ranking for each column in data\n    ranks = [list(np.argsort(z)[::-1] + 1) for z in [list(z) for z in zip(*data)][2:]]\n    data = list(zip(*[list(z) for z in zip(*data)] + ranks))\n\n    #### Print results\n    if args.indices_to_omit is None:        \n        header_row = (""{:<13}{:<19}{:<8}{:<8}{:<8}{:<8}"")\n        print(header_row.format(""Platform"", ""Model"", ""Acc@1"", ""Acc@5"", ""Rank@1"", ""Rank@5""))\n        print(header_row.format(""-----"", ""-----"", ""-----"", ""------"", ""------"", ""-------""))\n        data_row = (""{:<13}{:<19}{:<8.2%}{:<8.2%}{:<8}{:<8}"")\n    else:\n        header_row = (""{:<13}{:<19}{:<8}{:<8}{:<8}{:<8}{:<8}{:<8}{:<8}{:<8}"")\n        print(header_row.format(""Platform"", ""Model"", ""Acc@1"", ""cAcc@1"", ""Acc@5"", ""cAcc@5"", ""Rank@1"", ""cRank@5"", ""Rank@5"", ""cRank@5""))\n        print(header_row.format(""-----"", ""-----"", ""-----"", ""------"", ""-----"", ""------"", ""------"", ""-------"", ""------"", ""-------""))\n        data_row = (""{:<13}{:<19}{:<8.2%}{:<8.2%}{:<8.2%}{:<8.2%}{:<8}{:<8}{:<8}{:<8}"")\n        \n    for d in data:\n        print(data_row.format(*d))\n\n\n# In[ ]:\n\n\nif __name__ == \'__main__\':\n    main()\n\n\n# In[ ]:\n\n\n# Platform     Model              Acc@1   Acc@5   Rank@1  Rank@5\n# -----        -----              -----   ------  ------  -------\n# Keras 2.2.4  nasnetlarge        80.83%  95.27%  1       1\n# Keras 2.2.4  inceptionresnetv2  78.93%  94.45%  2       2\n# PyTorch 1.0  resnet152          77.62%  93.81%  3       3\n# Keras 2.2.4  xception           77.18%  93.49%  4       4\n# PyTorch 1.0  densenet161        76.92%  93.49%  5       5\n# PyTorch 1.0  resnet101          76.64%  93.30%  6       6\n# PyTorch 1.0  densenet201        76.41%  93.18%  7       7\n# Keras 2.2.4  inceptionv3        76.02%  92.90%  8       8\n# PyTorch 1.0  densenet169        75.59%  92.69%  9       9\n# PyTorch 1.0  resnet50           75.06%  92.48%  10      10\n# Keras 2.2.4  densenet201        74.77%  92.32%  11      11\n# PyTorch 1.0  densenet121        74.07%  91.94%  12      12\n# Keras 2.2.4  densenet169        73.92%  91.76%  13      13\n# PyTorch 1.0  vgg19_bn           72.90%  91.35%  14      14\n# PyTorch 1.0  resnet34           72.34%  90.84%  15      16\n# PyTorch 1.0  vgg16_bn           72.29%  91.01%  16      15\n# Keras 2.2.4  densenet121        72.09%  90.70%  17      17\n# Keras 2.2.4  nasnetmobile       71.59%  90.19%  18      19\n# PyTorch 1.0  vgg19              71.19%  90.40%  19      18\n# PyTorch 1.0  vgg16              70.66%  89.93%  20      20\n# Keras 2.2.4  resnet50           70.35%  89.55%  21      22\n# PyTorch 1.0  vgg13_bn           70.12%  89.56%  22      21\n# Keras 2.2.4  mobilenetV2        69.98%  89.49%  23      23\n# PyTorch 1.0  vgg11_bn           69.36%  89.06%  24      24\n# PyTorch 1.0  inception_v3       69.25%  88.69%  25      25\n# Keras 2.2.4  mobilenet          69.02%  88.48%  26      27\n# PyTorch 1.0  vgg13              68.69%  88.65%  27      28\n# PyTorch 1.0  resnet18           68.37%  88.56%  28      26\n# PyTorch 1.0  vgg11              67.85%  88.11%  29      29\n# Keras 2.2.4  vgg19              65.58%  86.54%  30      30\n# Keras 2.2.4  vgg16              65.24%  86.20%  31      31\n# PyTorch 1.0  squeezenet1_0      56.49%  79.06%  32      33\n# PyTorch 1.0  squeezenet1_1      56.42%  79.21%  33      32\n# PyTorch 1.0  alexnet            54.50%  77.64%  34      34\n\n'"
imagenet_keras_get_predictions.py,0,"b'#!/usr/bin/env python\n# coding: utf-8\n\n# In[2]:\n\n\n# These imports enhance Python2/3 compatibility.\nfrom __future__ import print_function, absolute_import, division, unicode_literals, with_statement\n\n\n# In[3]:\n\n\n# General imports\nimport argparse\nimport numpy as np\nimport os\nimport sys\n\n# Use PyTorch/torchvision for dataloading (more reliable/faster)\nfrom torchvision import datasets\n\n\n# In[1]:\n\n\n# Keras modules\nfrom keras.preprocessing import image\n\n# Keras models\nfrom keras.applications import (\n    DenseNet121,\n    DenseNet169,\n    DenseNet201,\n    InceptionResNetV2,\n    InceptionV3,\n    MobileNet,\n    MobileNetV2,\n    NASNetLarge,\n    NASNetMobile,\n    ResNet50,\n    VGG16,\n    VGG19,\n    Xception,\n)\n\n# Import preprocess_inputs parent modules\nfrom keras.applications import (\n    densenet,\n    inception_resnet_v2,\n    inception_v3,\n    mobilenet,\n    mobilenet_v2,\n    nasnet,\n    resnet50,\n    vgg16,\n    vgg19,\n    xception,\n)\n\n# # Helps with compatibilty with CUDA version 10 and the RTX 2080 GPU line. Uncomment if relevant.\n# import tensorflow as tf\n# from keras.backend.tensorflow_backend import set_session\n# gpu_options = tf.GPUOptions2(allow_growth=True)\n# config = tf.ConfigProto(gpu_options=gpu_options)\n# set_session(tf.Session(config=config))\n\n\n# In[ ]:\n\n\nkeras_models = {\n    ""densenet121"" : DenseNet121,\n    ""densenet169"" : DenseNet169,\n    ""densenet201"" : DenseNet201,\n    ""mobilenet"" : MobileNet,\n    ""mobilenetV2"" : MobileNetV2,\n    ""nasnetmobile"" : NASNetMobile,\n    ""resnet50"" : ResNet50,\n    ""vgg16"" : VGG16,\n    ""vgg19"" : VGG19,\n    ""xception"" : Xception,\n    ""inceptionresnetv2"" : InceptionResNetV2,\n    ""inceptionv3"" : InceptionV3,\n    ""nasnetlarge"" : NASNetLarge,\n}\n\nmodels_preprocessing = {\n    ""densenet121"" : densenet.preprocess_input,\n    ""densenet169"" : densenet.preprocess_input,\n    ""densenet201"" : densenet.preprocess_input,\n    ""mobilenet"" : mobilenet.preprocess_input,\n    ""mobilenetV2"" : mobilenet_v2.preprocess_input,\n    ""nasnetmobile"" : nasnet.preprocess_input,\n    ""resnet50"" : resnet50.preprocess_input,\n    ""vgg16"" : vgg16.preprocess_input,\n    ""vgg19"" : vgg19.preprocess_input,\n    ""xception"" : xception.preprocess_input,\n    ""inceptionresnetv2"" : inception_resnet_v2.preprocess_input,\n    ""inceptionv3"" : inception_v3.preprocess_input,\n    ""nasnetlarge"" : nasnet.preprocess_input,\n}\n\nmodels_img_size = {\n    ""densenet121"" : (224, 224),\n    ""densenet169"" : (224, 224),\n    ""densenet201"" : (224, 224),\n    ""mobilenet"" : (224, 224),\n    ""mobilenetV2"" : (224, 224),\n    ""nasnetmobile"" : (224, 224),\n    ""resnet50"" : (224, 224),\n    ""vgg16"" : (224, 224),\n    ""vgg19"" : (224, 224),\n    ""xception"" : (299, 299),\n    ""inceptionresnetv2"" : (299, 299),\n    ""inceptionv3"" : (299, 299),\n    ""nasnetlarge"" : (331, 331),\n}\n\n\n# In[ ]:\n\n\n# Set up argument parser\nparser = argparse.ArgumentParser(description=\'Keras ImageNet Inference\')\nparser.add_argument(\'val_dir\', metavar=\'DIR\',\n                    help=\'path to imagenet val dataset folder\')\nparser.add_argument(\'-m\', \'--model\', metavar=\'MODEL\', default=None,\n                    choices=keras_models.keys(),\n                    help=\'model architecture: \' +\n                        \' | \'.join(keras_models.keys()) +\n                        \' (example: resnet50)\' +\n                        \' (default: Runs across all Keras models)\')\nparser.add_argument(\'-g\', \'--gpu\', metavar=\'MODEL\', default=0,\n                    help=\'int of GPU to use. Only uses single GPU.\')\nparser.add_argument(\'-o\', \'--output-dir\', metavar=\'OUTPUT_DIR\', default=""keras_imagenet/"",\n                    help=\'directory folder to store output results in.\')\nparser.add_argument(\'--save-all-probs\',  action=\'store_true\', default = False, \n                    help=\'Store entire softmax output for all examples (100 MB)\')\n\n\n# In[ ]:\n\n\ndef main(args = parser.parse_args()):\n    \'\'\'Select GPU and set up data loader for ImageNet val set.\'\'\'\n    os.environ[""CUDA_VISIBLE_DEVICES""]=str(args.gpu)\n    \n    # Create output directory if it does not exist\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n        \n    # Grab imagenet data\n    val_dataset = datasets.ImageFolder(args.val_dir)\n    img_paths, labels = (list(t) for t in zip(*val_dataset.imgs))\n        \n    # Run forward pass inference on all models for all examples in val set.\n    models = keras_models if args.model is None else [args.model]\n    for model in models:\n        process_model(model, img_paths, labels, args.output_dir, args.save_all_probs)\n\n\n# In[6]:\n\n\n# def images2data(img_paths, img_size = (224, 224)):\n#     result = []\n#     for i, img_path in enumerate(img_paths):\n#         if i % 50 == 0:\n#             print(""\\rComplete: {:.1%}"".format(i / len(img_paths)), end = """")\n#         img = image.load_img(img_path, target_size=img_size)\n#         result.append(np.expand_dims(image.img_to_array(img), axis=0))\n#     print()\n#     return result\n\n\n# In[9]:\n\n\ndef process_model(\n    model_name, \n    img_paths, \n    labels, \n    out_dir=""keras_imagenet/"",\n    save_all_probs = False,\n):\n    \'\'\'Actual work is done here. This runs inference on a Keras model,\n    by computing the output of a forward pass, individually for each example.\n    Running examples in batches or using vectorized operations results in\n    random outputs and lack of reproducibility in Keras (this is a bug).\n    This method will avoid those issues by running the forward pass on each\n    example, one at a time. This is slower, but accurate.\n    \n    Top5 predictions and probabilities for each example for the model\n    are stored in the keras_imagenet/ output directory.\'\'\'\n    \n    preprocess_model = models_preprocessing[model_name]  \n    img_size = models_img_size[model_name]\n    Model = keras_models[model_name] \n    wfn_base = os.path.join(out_dir, model_name + ""_keras_imagenet_"")\n\n    # Create Keras model\n    model = Model(weights=\'imagenet\')\n    \n    # Preprocessing and Forward pass through validation set.\n    probs = []\n    for i, img_path in enumerate(img_paths):\n        if i % 32 == 0:\n            print(""\\r{} completed: {:.2%}"".format(model_name, i / len(img_paths)), end="""")\n            sys.stdout.flush()\n        img = image.load_img(img_path, target_size=img_size)\n        img = np.expand_dims(image.img_to_array(img), axis=0)            \n        probs.append(model.predict(preprocess_model(img)))\n    probs = np.vstack(probs)\n    if save_all_probs:\n        np.save(wfn_base + ""probs.npy"", probs.astype(np.float16))\n    \n    # Extract top 5 predictions for each example\n    n = 5\n    top = np.argpartition(-probs, n, axis = 1)[:,:n]\n    top_probs = probs[np.arange(probs.shape[0])[:, None], top]\n    acc1 = sum(top[range(len(top)), np.argmax(top_probs, axis = 1)] == labels) / float(len(labels))\n    acc5 = sum([labels[i] in row for i, row in enumerate(top)]) / float(len(labels))\n    print(\'\\n{}: acc1: {:.2%}, acc5: {:.2%}\'.format(model_name, acc1, acc5))\n    \n    # Save top 5 predictions and associated probabilities\n    np.save(wfn_base + ""top5preds.npy"", top)\n    np.save(wfn_base + ""top5probs.npy"", top_probs.astype(np.float16))\n\n\n# In[ ]:\n\n\nif __name__ == \'__main__\':\n    main()\n\n'"
imagenet_pytorch_get_predictions.py,5,"b'#!/usr/bin/env python\n# coding: utf-8\n\n# In[1]:\n\n\n# These imports enhance Python2/3 compatibility.\nfrom __future__ import (\n    print_function, absolute_import, division, unicode_literals, with_statement\n)\n\n\n# In[13]:\n\n\nfrom torchvision import models, datasets, transforms\nimport torch\nimport multiprocessing\nimport numpy as np\nimport os\nimport sys\nimport argparse\n\n# This helps with dataloading for inference\ntorch.multiprocessing.set_sharing_strategy(\'file_system\')\nVAL_SIZE = 50000\n\n\n# In[3]:\n\n\npytorch_models = [\n    ""inception_v3"",\n    ""vgg11"",\n    ""vgg11_bn"",\n    ""vgg13"",\n    ""vgg13_bn"",\n    ""vgg16"",\n    ""vgg16_bn"",\n    ""vgg19"",\n    ""vgg19_bn"",\n    ""densenet121"",\n    ""densenet161"",\n    ""densenet169"",\n    ""densenet201"",\n    ""alexnet"",\n    ""squeezenet1_0"",\n    ""squeezenet1_1"",\n    ""resnet18"",\n    ""resnet34"",\n    ""resnet50"",\n    ""resnet101"",\n    ""resnet152"",\n]\n\n\n# In[22]:\n\n\n# Set up argument parser\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Inference\')\nparser.add_argument(\'val_dir\', metavar=\'DIR\',\n                    help=\'path to imagenet val dataset folder\')\nparser.add_argument(\'-m\', \'--model\', metavar=\'MODEL\', default=None,\n                    choices=pytorch_models,\n                    help=\'model architecture: \' +\n                        \' | \'.join(pytorch_models) +\n                        \' (example: resnet50)\' +\n                        \' (default: Runs across all PyTorch models)\')\nparser.add_argument(\'-g\', \'--gpu\', metavar=\'MODEL\', default=0,\n                    help=\'int of GPU to use. Only uses single GPU.\')\nparser.add_argument(\'-b\', \'--batch-size\', metavar=\'BATCHSIZE\', default=32,\n                    help=\'Number of examples to run forward pass on at once.\')\nparser.add_argument(\'-o\', \'--output-dir\', metavar=\'OUTPUTDIR\',\n                    default=""pytorch_imagenet/"",\n                    help=\'directory folder to store output results in.\')\nparser.add_argument(\'--save-all-probs\',  action=\'store_true\', default = False, \n                    help=\'Store entire softmax output of all examples (100 MB)\')\nparser.add_argument(\'--save-labels\', action=\'store_true\', default = False, \n                    help=\'Store labels\')\n\n\n# In[23]:\n\n\ndef main(args = parser.parse_args()):\n    \'\'\'Select GPU and set up data loader for ImageNet val set.\'\'\'\n    global device\n    os.environ[""CUDA_VISIBLE_DEVICES""]=str(args.gpu)\n    # Detect if we have a GPU available\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n    \n    # Create output directory if it does not exist\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n        \n    dataloaders = {}\n    for img_size in [224, 299]:\n        val_transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(img_size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n        val_dataset = datasets.ImageFolder(args.val_dir, val_transform)\n        dataloaders[img_size] = torch.utils.data.DataLoader(\n            val_dataset, \n            batch_size=args.batch_size,                                             \n            shuffle=False, \n            num_workers=max(1, multiprocessing.cpu_count() - 2),\n        )\n        \n    # Run forward pass inference on all models for all examples in val set.\n    models = pytorch_models if args.model is None else [args.model]\n    for model in models:\n        process_model(model, dataloaders, args.output_dir,\n                      args.save_all_probs, args.save_labels)\n\n\n# In[ ]:\n\n\ndef process_model(\n    model_name,\n    dataloaders,\n    out_dir = ""pytorch_imagenet/"",\n    save_all_probs = False,\n    save_labels = False,\n):\n    \'\'\'Actual work is done here. This runs inference on a pyTorch model,\n    using the pyTorch batch loader.\n    \n    Top5 predictions and probabilities for each example for the model\n    are stored in the pytorch_imagenet/ output directory.\'\'\'\n    \n    # Load PyTorch model pre-trained on ImageNet\n    model = eval(""models.{}(pretrained=True)"".format(model_name))\n    # Send the model to GPU/CPU\n    model = model.to(device)\n    wfn_base = os.path.join(out_dir, model_name + ""_pytorch_imagenet_"")\n    probs, labels = [], []\n    if model_name is ""inception_v3"":\n        loader = dataloaders[299]\n    else:\n        loader = dataloaders[224]\n    \n    # Inference, with no gradient changing\n    model.eval() # set model to inference mode (not train mode)\n    with torch.set_grad_enabled(False):\n        for i, (x_val, y_val) in enumerate(loader):\n            print(""\\r{} completed: {:.2%}"".format(\n                model_name, i / len(loader)), end="""")\n            sys.stdout.flush()\n            out = torch.nn.functional.softmax(model(x_val.to(device)), dim=1)\n            probs.append(out.numpy() if device == \'cpu\' else out.cpu().numpy())\n            labels.append(y_val)\n            \n    # Convert batches to single numpy arrays    \n    probs = np.stack([p for l in probs for p in l])\n    labels = np.array([t for l in labels for t in l])\n    if save_labels:\n        np.save(wfn_base + ""labels.npy"", labels.astype(int))\n    if save_all_probs:\n        np.save(wfn_base + ""probs.npy"", probs.astype(np.float16))\n    \n    # Extract top 5 predictions for each example\n    n = 5\n    top = np.argpartition(-probs, n, axis = 1)[:,:n]\n    top_probs = probs[np.arange(probs.shape[0])[:, None], top]\n    right1 = sum(top[range(len(top)), np.argmax(top_probs, axis = 1)] == labels)\n    acc1 = right1 / float(len(labels))\n    count5 = sum([labels[i] in row for i, row in enumerate(top)])\n    acc5 = count5 / float(len(labels))\n    print(\'\\n{}: acc1: {:.2%}, acc5: {:.2%}\'.format(model_name, acc1, acc5))\n    \n    # Save top 5 predictions and associated probabilities\n    np.save(wfn_base + ""top5preds.npy"", top)\n    np.save(wfn_base + ""top5probs.npy"", top_probs.astype(np.float16))\n\n\n# In[ ]:\n\n\nif __name__ == \'__main__\':\n    main()\n\n'"
