file_path,api_count,code
infer_celeba.py,1,"b'""""""Train script.\n\nUsage:\n    infer_celeba.py <hparams> <dataset_root> <z_dir>\n""""""\nimport os\nimport cv2\nimport random\nimport torch\nimport vision\nimport numpy as np\nfrom docopt import docopt\nfrom torchvision import transforms\nfrom glow.builder import build\nfrom glow.config import JsonConfig\n\n\ndef select_index(name, l, r, description=None):\n    index = None\n    while index is None:\n        print(""Select {} with index [{}, {}),""\n              ""or {} for random selection"".format(name, l, r, l - 1))\n        if description is not None:\n            for i, d in enumerate(description):\n                print(""{}: {}"".format(i, d))\n        try:\n            line = int(input().strip())\n            if l - 1 <= line < r:\n                index = line\n                if index == l - 1:\n                    index = random.randint(l, r - 1)\n        except Exception:\n            pass\n    return index\n\n\ndef run_z(graph, z):\n    graph.eval()\n    x = graph(z=torch.tensor([z]).cuda(), eps_std=0.3, reverse=True)\n    img = x[0].permute(1, 2, 0).detach().cpu().numpy()\n    img = img[:, :, ::-1]\n    img = cv2.resize(img, (256, 256))\n    return img\n\n\ndef save_images(images, names):\n    if not os.path.exists(""pictures/infer/""):\n        os.makedirs(""pictures/infer/"")\n    for img, name in zip(images, names):\n        img = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n        cv2.imwrite(""pictures/infer/{}.png"".format(name), img)\n        cv2.imshow(""img"", img)\n        cv2.waitKey()\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    hparams = args[""<hparams>""]\n    dataset_root = args[""<dataset_root>""]\n    z_dir = args[""<z_dir>""]\n    assert os.path.exists(dataset_root), (\n        ""Failed to find root dir `{}` of dataset."".format(dataset_root))\n    assert os.path.exists(hparams), (\n        ""Failed to find hparams josn `{}`"".format(hparams))\n    if not os.path.exists(z_dir):\n        print(""Generate Z to {}"".format(z_dir))\n        os.makedirs(z_dir)\n        generate_z = True\n    else:\n        print(""Load Z from {}"".format(z_dir))\n        generate_z = False\n\n    hparams = JsonConfig(""hparams/celeba.json"")\n    dataset = vision.Datasets[""celeba""]\n    # set transform of dataset\n    transform = transforms.Compose([\n        transforms.CenterCrop(hparams.Data.center_crop),\n        transforms.Resize(hparams.Data.resize),\n        transforms.ToTensor()])\n    # build\n    graph = build(hparams, False)[""graph""]\n    dataset = dataset(dataset_root, transform=transform)\n\n    # get Z\n    if not generate_z:\n        # try to load\n        try:\n            delta_Z = []\n            for i in range(hparams.Glow.y_classes):\n                z = np.load(os.path.join(z_dir, ""detla_z_{}.npy"".format(i)))\n                delta_Z.append(z)\n        except FileNotFoundError:\n            # need to generate\n            generate_z = True\n            print(""Failed to load {} Z"".format(hparams.Glow.y_classes))\n            quit()\n    if generate_z:\n        delta_Z = graph.generate_attr_deltaz(dataset)\n        for i, z in enumerate(delta_Z):\n            np.save(os.path.join(z_dir, ""detla_z_{}.npy"".format(i)), z)\n        print(""Finish generating"")\n\n    # interact with user\n    base_index = select_index(""base image"", 0, len(dataset))\n    attr_index = select_index(""attritube"", 0, len(delta_Z), dataset.attrs)\n    attr_name = dataset.attrs[attr_index]\n    z_delta = delta_Z[attr_index]\n    graph.eval()\n    z_base = graph.generate_z(dataset[base_index][""x""])\n    # begin to generate new image\n    images = []\n    names = []\n    images.append(run_z(graph, z_base))\n    names.append(""reconstruct_origin"")\n    interplate_n = 5\n    for i in range(0, interplate_n+1):\n        d = z_delta * float(i) / float(interplate_n)\n        images.append(run_z(graph, z_base + d))\n        names.append(""attr_{}_{}"".format(attr_name, interplate_n + i))\n        if i > 0:\n            images.append(run_z(graph, z_base - d))\n            names.append(""attr_{}_{}"".format(attr_name, interplate_n - i))\n    save_images(images, names)\n'"
test_modules.py,15,"b'""""""\nTest the modules\n""""""\nimport cv2\nimport torch\nimport tensorflow as tf\nimport numpy as np\nfrom glow import thops\nfrom glow import modules\nfrom glow import models\n\n\ndef is_equal(a, b, eps=1e-5):\n    if a.shape != b.shape:\n        return False\n    max_delta = np.max(np.abs(a - b))\n    return max_delta < eps\n\n\ndef test_multidim_sum():\n    x = np.random.rand(2, 3, 4, 4)\n    th_x = torch.Tensor(x)\n    tf_x = tf.constant(x)\n    test_axis_list = [[1], [1, 2], [0, 2, 3], [0, 1, 2, 3]]\n    with tf.Session():\n        print(""[Test] multidim sum, compared with tensorflow"")\n        for axis in test_axis_list:\n            for keep in [False, True]:\n                # tf\n                tf_y = tf.reduce_sum(tf_x, axis=axis, keepdims=keep)\n                tf_y = tf_y.eval()\n                # th\n                th_y = thops.sum(th_x, dim=axis, keepdim=keep).numpy()\n                if is_equal(th_y, tf_y):\n                    print(""  Pass: dim={}, keepdim={}"", axis, keep)\n                else:\n                    raise ValueError(""sum with dim={} error"".format(axis))\n\n\ndef test_multidim_mean():\n    x = np.random.rand(2, 3, 4, 4)\n    th_x = torch.Tensor(x)\n    tf_x = tf.constant(x)\n    test_axis_list = [[1], [1, 2], [0, 2, 3], [0, 1, 2, 3]]\n    with tf.Session():\n        print(""[Test] multidim mean, compared with tensorflow"")\n        for axis in test_axis_list:\n            for keep in [False, True]:\n                # tf\n                tf_y = tf.reduce_mean(tf_x, axis=axis, keepdims=keep)\n                tf_y = tf_y.eval()\n                # th\n                th_y = thops.mean(th_x, dim=axis, keepdim=keep).numpy()\n                if is_equal(th_y, tf_y):\n                    print(""  Pass: dim={}, keepdim={}"", axis, keep)\n                else:\n                    raise ValueError(""mean with dim={} error"".format(axis))\n\n\ndef test_actnorm():\n    print(""[Test]: actnorm"")\n    actnorm = modules.ActNorm2d(12)\n    x = torch.Tensor(np.random.rand(2, 12, 64, 64))\n    actnorm.initialize_parameters(x)\n    y, det = actnorm(x, 0)\n    x_, _ = actnorm(y, None, True)\n    print(""actnorm (forward,reverse) delta"", float(torch.max(torch.abs(x_-x))))\n    print(""  det"", float(det))\n\n\ndef test_conv1x1():\n    print(""[Test]: invconv1x1"")\n    conv = modules.InvertibleConv1x1(96)\n    x = torch.Tensor(np.random.rand(2, 96, 16, 16))\n    y, det = conv(x, 0)\n    x_, _ = conv(y, None, True)\n    print(""conv1x1 (forward,reverse) delta"", float(torch.max(torch.abs(x_-x))))\n    print(""  det"", float(det))\n\n\ndef test_gaussian():\n    # mean = torch.zeros((4, 32, 16, 16))\n    # logs = torch.ones((4, 32, 16, 16))\n    # x = torch.Tensor(np.random.rand(4, 32, 16, 16))\n    # lh = modules.GaussianDiag.likelihood(mean, logs, x)\n    # logp = modules.GaussianDiag.logp(mean, logs, x)\n    pass\n\n\ndef test_flow_step():\n    print(""[Test]: flow step"")\n    step = models.FlowStep(32, 256, flow_coupling=""affine"")\n    x = torch.Tensor(np.random.rand(2, 32, 16, 16))\n    y, det = step(x, 0, False)\n    x_, det0 = step(y, det, True)\n    print(""flowstep (forward,reverse)delta"", float(torch.max(torch.abs(x_-x))))\n    print(""  det"", det, det0)\n\n\ndef test_squeeze():\n    print(""[Test]: SqueezeLayer"")\n    layer = modules.SqueezeLayer(2)\n    img = cv2.imread(""pictures/tsuki.jpeg"")\n    img = cv2.resize(img, (256, 256))\n    img = img.transpose((2, 0, 1))\n    x = torch.Tensor([img])\n    y, _ = layer(x, 0, False)\n    x_, _ = layer(y, 0, True)\n    z = y[0].numpy().transpose((1, 2, 0))\n    cv2.imshow(""0_3"", z[:, :, 0: 3].astype(np.uint8))\n    cv2.imshow(""3_6"", z[:, :, 3: 6].astype(np.uint8))\n    cv2.imshow(""6_9"", z[:, :, 6: 9].astype(np.uint8))\n    cv2.imshow(""9_12"", z[:, :, 9: 12].astype(np.uint8))\n    cv2.imshow(""x_"", x_[0].numpy().transpose((1, 2, 0)).astype(np.uint8))\n    cv2.imshow(""x"", x[0].numpy().transpose((1, 2, 0)).astype(np.uint8))\n    cv2.waitKey()\n\n\ndef test_flow_net():\n    print(""[Test]: flow net"")\n    net = models.FlowNet((64, 64, 3), 256, 16, 3)\n    x = torch.Tensor(np.random.rand(4, 3, 64, 64))\n    y, det = net(x)\n    x_ = net(y, reverse=True)\n    print(""z"", y.size())\n    print(""x_"", x_.size())\n    print(det)\n\n\ndef test_glow():\n    print(""[Test]: Glow"")\n    from glow.config import JsonConfig\n    glow = models.Glow(JsonConfig(""hparams_celeba.json""))\n    img = cv2.imread(""tsuki.jpeg"")\n    img = cv2.resize(img, (64, 64))\n    img = (img / 255.0).astype(np.float32)\n    img = img[:, :, ::-1].transpose(2, 0, 1)\n    x = torch.Tensor([img]*8)\n    y_onehot = torch.zeros((8, 40))\n    z, det, y_logits = glow(x=x, y_onehot=y_onehot)\n    print(z.size())\n    print(det)\n    print(models.Glow.loss_generative(det))\n\n\nif __name__ == ""__main__"":\n    test_multidim_sum()\n    test_multidim_mean()\n    test_actnorm()\n    test_conv1x1()\n    test_gaussian()\n    test_flow_step()\n    test_squeeze()\n    test_flow_net()\n    test_glow()\n'"
train.py,0,"b'""""""Train script.\n\nUsage:\n    train.py <hparams> <dataset> <dataset_root>\n""""""\nimport os\nimport vision\nfrom docopt import docopt\nfrom torchvision import transforms\nfrom glow.builder import build\nfrom glow.trainer import Trainer\nfrom glow.config import JsonConfig\n\n\nif __name__ == ""__main__"":\n    args = docopt(__doc__)\n    hparams = args[""<hparams>""]\n    dataset = args[""<dataset>""]\n    dataset_root = args[""<dataset_root>""]\n    assert dataset in vision.Datasets, (\n        ""`{}` is not supported, use `{}`"".format(dataset, vision.Datasets.keys()))\n    assert os.path.exists(dataset_root), (\n        ""Failed to find root dir `{}` of dataset."".format(dataset_root))\n    assert os.path.exists(hparams), (\n        ""Failed to find hparams josn `{}`"".format(hparams))\n    hparams = JsonConfig(hparams)\n    dataset = vision.Datasets[dataset]\n    # set transform of dataset\n    transform = transforms.Compose([\n        transforms.CenterCrop(hparams.Data.center_crop),\n        transforms.Resize(hparams.Data.resize),\n        transforms.ToTensor()])\n    # build graph and dataset\n    built = build(hparams, True)\n    dataset = dataset(dataset_root, transform=transform)\n    # begin to train\n    trainer = Trainer(**built, dataset=dataset, hparams=hparams)\n    trainer.train()\n'"
glow/builder.py,2,"b'import re, os\nimport copy\nimport torch\nfrom collections import defaultdict\nfrom . import learning_rate_schedule\nfrom .config import JsonConfig\nfrom .models import Glow\nfrom .utils import load, save, get_proper_device\n\n\ndef build_adam(params, args):\n    return torch.optim.Adam(params, **args)\n\n\n__build_optim_dict = {\n    ""adam"": build_adam\n}\n\n\ndef build(hparams, is_training):\n    if isinstance(hparams, str):\n        hparams = JsonConfig(hparams)\n    # get graph and criterions from build function\n    graph, optim, lrschedule, criterion_dict = None, None, None, None  # init with None\n    cpu, devices = ""cpu"", None\n    get_loss = None\n    # 1. build graph and criterion_dict, (on cpu)\n    # build and append `device attr` to graph\n    graph = Glow(hparams)\n    graph.device = hparams.Device.glow\n    if graph is not None:\n        # get device\n        devices = get_proper_device(graph.device)\n        graph.device = devices\n        graph.to(cpu)\n    # 2. get optim (on cpu)\n    try:\n        if graph is not None and is_training:\n            optim_name = hparams.Optim.name\n            optim = __build_optim_dict[optim_name](graph.parameters(), hparams.Optim.args.to_dict())\n            print(""[Builder]: Using optimizer `{}`, with args:{}"".format(optim_name, hparams.Optim.args))\n            # get lrschedule\n            schedule_name = ""default""\n            schedule_args = {}\n            if ""Schedule"" in hparams.Optim:\n                schedule_name = hparams.Optim.Schedule.name\n                schedule_args = hparams.Optim.Schedule.args.to_dict()\n            if not (""init_lr"" in schedule_args):\n                schedule_args[""init_lr""] = hparams.Optim.args.lr\n            assert schedule_args[""init_lr""] == hparams.Optim.args.lr,\\\n                ""Optim lr {} != Schedule init_lr {}"".format(hparams.Optim.args.lr, schedule_args[""init_lr""])\n            lrschedule = {\n                ""func"": getattr(learning_rate_schedule, schedule_name),\n                ""args"": schedule_args\n            }\n    except KeyError:\n        raise ValueError(""[Builder]: Optimizer `{}` is not supported."".format(optim_name))\n    # 3. warm start and move to devices\n    if graph is not None:\n        # 1. warm start from pre-trained model (on cpu)\n        pre_trained = None\n        loaded_step = 0\n        if is_training:\n            if ""warm_start"" in hparams.Train and len(hparams.Train.warm_start) > 0:\n                pre_trained = hparams.Train.warm_start\n        else:\n            pre_trained = hparams.Infer.pre_trained\n        if pre_trained is not None:\n            loaded_step = load(os.path.basename(pre_trained),\n                        graph=graph, optim=optim, criterion_dict=None,\n                        pkg_dir=os.path.dirname(pre_trained),\n                        device=cpu)\n        # 2. move graph to device (to cpu or cuda)\n        use_cpu = any([isinstance(d, str) and d.find(""cpu"") >= 0 for d in devices])\n        if use_cpu:\n            graph = graph.cpu()\n            print(""[Builder]: Use cpu to train."")\n        else:\n            if ""data"" in hparams.Device:\n                data_gpu = hparams.Device.data\n                if isinstance(data_gpu, str):\n                    data_gpu = int(data_gpu[5:])\n            else:\n                data_gpu = devices[0]\n            # move to first\n            graph = graph.cuda(device=devices[0])\n            if is_training and pre_trained is not None:\n                # note that it is possible necessary to move optim\n                if hasattr(optim, ""state""):\n                    def move_to(D, device):\n                        for k in D:\n                            if isinstance(D[k], dict) or isinstance(D[k], defaultdict):\n                                move_to(D[k], device)\n                            elif torch.is_tensor(D[k]):\n                                D[k] = D[k].cuda(device)\n                    move_to(optim.state, devices[0])\n            print(""[Builder]: Use cuda {} to train, use {} to load data and get loss."".format(devices, data_gpu))\n\n    return {\n        ""graph"": graph,\n        ""optim"": optim,\n        ""lrschedule"": lrschedule,\n        ""devices"": devices,\n        ""data_device"": data_gpu if not use_cpu else ""cpu"",\n        ""loaded_step"": loaded_step\n    }\n'"
glow/config.py,0,"b'import os\nimport json\nimport datetime\n\n\nclass JsonConfig(dict):\n    """"""\n    The configures will be loaded and dumped as json file.\n    The Structure will be maintained as json.\n    [TODO]: Some `asserts` can be make by key `__assert__`\n    """"""\n    Indent = 2\n\n    def __init__(self, *argv, **kwargs):\n        super().__init__()\n        super().__setitem__(""__name"", ""default"")\n        # check input\n        assert len(argv) == 0 or len(kwargs) == 0, (\n            ""[JsonConfig]: Cannot initialize with""\n            "" position parameters (json file or a dict)""\n            "" and named parameters (key and values) at the same time."")\n        if len(argv) > 0:\n            # init from a json or dict\n            assert len(argv) == 1, (\n                ""[JsonConfig]: Need one positional parameters, found two."")\n            arg = argv[0]\n        else:\n            arg = kwargs\n        # begin initialization\n        if isinstance(arg, str):\n            super().__setitem__(""__name"",\n                                os.path.splitext(os.path.basename(arg))[0])\n            with open(arg, ""r"") as load_f:\n                arg = json.load(load_f)\n        if isinstance(arg, dict):\n            # case 1: init from dict\n            for key in arg:\n                value = arg[key]\n                if isinstance(value, dict):\n                    value = JsonConfig(value)\n                super().__setitem__(key, value)\n        else:\n            raise TypeError((""[JsonConfig]: Do not support given input""\n                             "" with type {}"").format(type(arg)))\n\n    def __setattr__(self, attr, value):\n        raise Exception(""[JsonConfig]: Can\'t set constant key {}"".format(attr))\n\n    def __setitem__(self, item, value):\n        raise Exception(""[JsonConfig]: Can\'t set constant key {}"".format(item))\n\n    def __getattr__(self, attr):\n        return super().__getitem__(attr)\n\n    def __str__(self):\n        return self.__to_string("""", 0)\n\n    def __to_string(self, name, intent):\n        ret = "" "" * intent + name + "" {\\n""\n        for key in self:\n            if key.find(""__"") == 0:\n                continue\n            value = self[key]\n            line = "" "" * intent\n            if isinstance(value, JsonConfig):\n                line += value.__to_string(key, intent + JsonConfig.Indent)\n            else:\n                line += "" "" * JsonConfig.Indent + key + "": "" + str(value)\n            ret += line + ""\\n""\n        ret += "" "" * intent + ""}""\n        return ret\n\n    def __add__(self, b):\n        assert isinstance(b, JsonConfig)\n        for k in b:\n            v = b[k]\n            if k in self:\n                if isinstance(v, JsonConfig):\n                    super().__setitem__(k, self[k] + v)\n                else:\n                    if k == ""__name"":\n                        super().__setitem__(k, self[k] + ""&"" + v)\n                    else:\n                        assert v == self[k], (\n                            ""[JsonConfig]: Two config conflicts at""\n                            ""`{}`, {} != {}"".format(k, self[k], v))\n            else:\n                # new key, directly add\n                super().__setitem__(k, v)\n        return self\n\n    def date_name(self):\n        date = str(datetime.datetime.now())\n        date = date[:date.rfind("":"")].replace(""-"", """")\\\n                                     .replace("":"", """")\\\n                                     .replace("" "", ""_"")\n        return date + ""_"" + super().__getitem__(""__name"") + "".json""\n\n    def dump(self, dir_path, json_name=None):\n        if json_name is None:\n            json_name = self.date_name()\n        json_path = os.path.join(dir_path, json_name)\n        with open(json_path, ""w"") as fout:\n            print(str(self))\n            json.dump(self.to_dict(), fout, indent=JsonConfig.Indent)\n\n    def to_dict(self):\n        ret = {}\n        for k in self:\n            if k.find(""__"") == 0:\n                continue\n            v = self[k]\n            if isinstance(v, JsonConfig):\n                ret[k] = v.to_dict()\n            else:\n                ret[k] = v\n        return ret\n'"
glow/learning_rate_schedule.py,0,"b'import numpy as np\n\n\ndef default(init_lr, global_step):\n    return init_lr\n\n\n# https://github.com/tensorflow/tensor2tensor/issues/280#issuecomment-339110329\ndef noam_learning_rate_decay(init_lr, global_step, warmup_steps=4000, minimum=None):\n     # Noam scheme from tensor2tensor:\n    warmup_steps = float(warmup_steps)\n    step = global_step + 1.\n    lr = init_lr * warmup_steps**0.5 * np.minimum(\n        step * warmup_steps**-1.5, step**-0.5)\n    if minimum is not None and global_step > warmup_steps:\n        if lr < minimum:\n            lr = minimum\n    return lr\n\n\ndef step_learning_rate_decay(init_lr, global_step,\n                             anneal_rate=0.98,\n                             anneal_interval=30000):\n    return init_lr * anneal_rate ** (global_step // anneal_interval)\n\n\ndef cyclic_cosine_annealing(init_lr, global_step, T, M):\n    """"""Cyclic cosine annealing\n\n    https://arxiv.org/pdf/1704.00109.pdf\n\n    Args:\n        init_lr (float): Initial learning rate\n        global_step (int): Current iteration number\n        T (int): Total iteration number (i,e. nepoch)\n        M (int): Number of ensembles we want\n\n    Returns:\n        float: Annealed learning rate\n    """"""\n    TdivM = T // M\n    return init_lr / 2.0 * (np.cos(np.pi * ((global_step - 1) % TdivM) / TdivM) + 1.0)\n'"
glow/models.py,15,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom tqdm import tqdm\nfrom . import thops\nfrom . import modules\nfrom . import utils\n\n\ndef f(in_channels, out_channels, hidden_channels):\n    return nn.Sequential(\n        modules.Conv2d(in_channels, hidden_channels), nn.ReLU(inplace=False),\n        modules.Conv2d(hidden_channels, hidden_channels, kernel_size=[1, 1]), nn.ReLU(inplace=False),\n        modules.Conv2dZeros(hidden_channels, out_channels))\n\n\nclass FlowStep(nn.Module):\n    FlowCoupling = [""additive"", ""affine""]\n    FlowPermutation = {\n        ""reverse"": lambda obj, z, logdet, rev: (obj.reverse(z, rev), logdet),\n        ""shuffle"": lambda obj, z, logdet, rev: (obj.shuffle(z, rev), logdet),\n        ""invconv"": lambda obj, z, logdet, rev: obj.invconv(z, logdet, rev)\n    }\n\n    def __init__(self, in_channels, hidden_channels,\n                 actnorm_scale=1.0,\n                 flow_permutation=""invconv"",\n                 flow_coupling=""additive"",\n                 LU_decomposed=False):\n        # check configures\n        assert flow_coupling in FlowStep.FlowCoupling,\\\n            ""flow_coupling should be in `{}`"".format(FlowStep.FlowCoupling)\n        assert flow_permutation in FlowStep.FlowPermutation,\\\n            ""float_permutation should be in `{}`"".format(\n                FlowStep.FlowPermutation.keys())\n        super().__init__()\n        self.flow_permutation = flow_permutation\n        self.flow_coupling = flow_coupling\n        # 1. actnorm\n        self.actnorm = modules.ActNorm2d(in_channels, actnorm_scale)\n        # 2. permute\n        if flow_permutation == ""invconv"":\n            self.invconv = modules.InvertibleConv1x1(\n                in_channels, LU_decomposed=LU_decomposed)\n        elif flow_permutation == ""shuffle"":\n            self.shuffle = modules.Permute2d(in_channels, shuffle=True)\n        else:\n            self.reverse = modules.Permute2d(in_channels, shuffle=False)\n        # 3. coupling\n        if flow_coupling == ""additive"":\n            self.f = f(in_channels // 2, in_channels // 2, hidden_channels)\n        elif flow_coupling == ""affine"":\n            self.f = f(in_channels // 2, in_channels, hidden_channels)\n\n    def forward(self, input, logdet=None, reverse=False):\n        if not reverse:\n            return self.normal_flow(input, logdet)\n        else:\n            return self.reverse_flow(input, logdet)\n\n    def normal_flow(self, input, logdet):\n        assert input.size(1) % 2 == 0\n        # 1. actnorm\n        z, logdet = self.actnorm(input, logdet=logdet, reverse=False)\n        # 2. permute\n        z, logdet = FlowStep.FlowPermutation[self.flow_permutation](\n            self, z, logdet, False)\n        # 3. coupling\n        z1, z2 = thops.split_feature(z, ""split"")\n        if self.flow_coupling == ""additive"":\n            z2 = z2 + self.f(z1)\n        elif self.flow_coupling == ""affine"":\n            h = self.f(z1)\n            shift, scale = thops.split_feature(h, ""cross"")\n            scale = torch.sigmoid(scale + 2.)\n            z2 = z2 + shift\n            z2 = z2 * scale\n            logdet = thops.sum(torch.log(scale), dim=[1, 2, 3]) + logdet\n        z = thops.cat_feature(z1, z2)\n        return z, logdet\n\n    def reverse_flow(self, input, logdet):\n        assert input.size(1) % 2 == 0\n        # 1.coupling\n        z1, z2 = thops.split_feature(input, ""split"")\n        if self.flow_coupling == ""additive"":\n            z2 = z2 - self.f(z1)\n        elif self.flow_coupling == ""affine"":\n            h = self.f(z1)\n            shift, scale = thops.split_feature(h, ""cross"")\n            scale = torch.sigmoid(scale + 2.)\n            z2 = z2 / scale\n            z2 = z2 - shift\n            logdet = -thops.sum(torch.log(scale), dim=[1, 2, 3]) + logdet\n        z = thops.cat_feature(z1, z2)\n        # 2. permute\n        z, logdet = FlowStep.FlowPermutation[self.flow_permutation](\n            self, z, logdet, True)\n        # 3. actnorm\n        z, logdet = self.actnorm(z, logdet=logdet, reverse=True)\n        return z, logdet\n\n\nclass FlowNet(nn.Module):\n    def __init__(self, image_shape, hidden_channels, K, L,\n                 actnorm_scale=1.0,\n                 flow_permutation=""invconv"",\n                 flow_coupling=""additive"",\n                 LU_decomposed=False):\n        """"""\n                             K                                      K\n        --> [Squeeze] -> [FlowStep] -> [Split] -> [Squeeze] -> [FlowStep]\n               ^                           v\n               |          (L - 1)          |\n               + --------------------------+\n        """"""\n        super().__init__()\n        self.layers = nn.ModuleList()\n        self.output_shapes = []\n        self.K = K\n        self.L = L\n        H, W, C = image_shape\n        assert C == 1 or C == 3, (""image_shape should be HWC, like (64, 64, 3)""\n                                  ""C == 1 or C == 3"")\n        for i in range(L):\n            # 1. Squeeze\n            C, H, W = C * 4, H // 2, W // 2\n            self.layers.append(modules.SqueezeLayer(factor=2))\n            self.output_shapes.append([-1, C, H, W])\n            # 2. K FlowStep\n            for _ in range(K):\n                self.layers.append(\n                    FlowStep(in_channels=C,\n                             hidden_channels=hidden_channels,\n                             actnorm_scale=actnorm_scale,\n                             flow_permutation=flow_permutation,\n                             flow_coupling=flow_coupling,\n                             LU_decomposed=LU_decomposed))\n                self.output_shapes.append(\n                    [-1, C, H, W])\n            # 3. Split2d\n            if i < L - 1:\n                self.layers.append(modules.Split2d(num_channels=C))\n                self.output_shapes.append([-1, C // 2, H, W])\n                C = C // 2\n\n    def forward(self, input, logdet=0., reverse=False, eps_std=None):\n        if not reverse:\n            return self.encode(input, logdet)\n        else:\n            return self.decode(input, eps_std)\n\n    def encode(self, z, logdet=0.0):\n        for layer, shape in zip(self.layers, self.output_shapes):\n            z, logdet = layer(z, logdet, reverse=False)\n        return z, logdet\n\n    def decode(self, z, eps_std=None):\n        for layer in reversed(self.layers):\n            if isinstance(layer, modules.Split2d):\n                z, logdet = layer(z, logdet=0, reverse=True, eps_std=eps_std)\n            else:\n                z, logdet = layer(z, logdet=0, reverse=True)\n        return z\n\n\nclass Glow(nn.Module):\n    BCE = nn.BCEWithLogitsLoss()\n    CE = nn.CrossEntropyLoss()\n\n    def __init__(self, hparams):\n        super().__init__()\n        self.flow = FlowNet(image_shape=hparams.Glow.image_shape,\n                            hidden_channels=hparams.Glow.hidden_channels,\n                            K=hparams.Glow.K,\n                            L=hparams.Glow.L,\n                            actnorm_scale=hparams.Glow.actnorm_scale,\n                            flow_permutation=hparams.Glow.flow_permutation,\n                            flow_coupling=hparams.Glow.flow_coupling,\n                            LU_decomposed=hparams.Glow.LU_decomposed)\n        self.hparams = hparams\n        self.y_classes = hparams.Glow.y_classes\n        # for prior\n        if hparams.Glow.learn_top:\n            C = self.flow.output_shapes[-1][1]\n            self.learn_top = modules.Conv2dZeros(C * 2, C * 2)\n        if hparams.Glow.y_condition:\n            C = self.flow.output_shapes[-1][1]\n            self.project_ycond = modules.LinearZeros(\n                hparams.Glow.y_classes, 2 * C)\n            self.project_class = modules.LinearZeros(\n                C, hparams.Glow.y_classes)\n        # register prior hidden\n        num_device = len(utils.get_proper_device(hparams.Device.glow, False))\n        assert hparams.Train.batch_size % num_device == 0\n        self.register_parameter(\n            ""prior_h"",\n            nn.Parameter(torch.zeros([hparams.Train.batch_size // num_device,\n                                      self.flow.output_shapes[-1][1] * 2,\n                                      self.flow.output_shapes[-1][2],\n                                      self.flow.output_shapes[-1][3]])))\n\n    def prior(self, y_onehot=None):\n        B, C = self.prior_h.size(0), self.prior_h.size(1)\n        h = self.prior_h.detach().clone()\n        assert torch.sum(h) == 0.0\n        if self.hparams.Glow.learn_top:\n            h = self.learn_top(h)\n        if self.hparams.Glow.y_condition:\n            assert y_onehot is not None\n            yp = self.project_ycond(y_onehot).view(B, C, 1, 1)\n            h += yp\n        return thops.split_feature(h, ""split"")\n\n    def forward(self, x=None, y_onehot=None, z=None,\n                eps_std=None, reverse=False):\n        if not reverse:\n            return self.normal_flow(x, y_onehot)\n        else:\n            return self.reverse_flow(z, y_onehot, eps_std)\n\n    def normal_flow(self, x, y_onehot):\n        pixels = thops.pixels(x)\n        z = x + torch.normal(mean=torch.zeros_like(x),\n                             std=torch.ones_like(x) * (1. / 256.))\n        logdet = torch.zeros_like(x[:, 0, 0, 0])\n        logdet += float(-np.log(256.) * pixels)\n        # encode\n        z, objective = self.flow(z, logdet=logdet, reverse=False)\n        # prior\n        mean, logs = self.prior(y_onehot)\n        objective += modules.GaussianDiag.logp(mean, logs, z)\n\n        if self.hparams.Glow.y_condition:\n            y_logits = self.project_class(z.mean(2).mean(2))\n        else:\n            y_logits = None\n\n        # return\n        nll = (-objective) / float(np.log(2.) * pixels)\n        return z, nll, y_logits\n\n    def reverse_flow(self, z, y_onehot, eps_std):\n        with torch.no_grad():\n            mean, logs = self.prior(y_onehot)\n            if z is None:\n                z = modules.GaussianDiag.sample(mean, logs, eps_std)\n            x = self.flow(z, eps_std=eps_std, reverse=True)\n        return x\n\n    def set_actnorm_init(self, inited=True):\n        for name, m in self.named_modules():\n            if (m.__class__.__name__.find(""ActNorm"") >= 0):\n                m.inited = inited\n\n    def generate_z(self, img):\n        self.eval()\n        B = self.hparams.Train.batch_size\n        x = img.unsqueeze(0).repeat(B, 1, 1, 1).cuda()\n        z,_, _ = self(x)\n        self.train()\n        return z[0].detach().cpu().numpy()\n\n    def generate_attr_deltaz(self, dataset):\n        assert ""y_onehot"" in dataset[0]\n        self.eval()\n        with torch.no_grad():\n            B = self.hparams.Train.batch_size\n            N = len(dataset)\n            attrs_pos_z = [[0, 0] for _ in range(self.y_classes)]\n            attrs_neg_z = [[0, 0] for _ in range(self.y_classes)]\n            for i in tqdm(range(0, N, B)):\n                j = min([i + B, N])\n                # generate z for data from i to j\n                xs = [dataset[k][""x""] for k in range(i, j)]\n                while len(xs) < B:\n                    xs.append(dataset[0][""x""])\n                xs = torch.stack(xs).cuda()\n                zs, _, _ = self(xs)\n                for k in range(i, j):\n                    z = zs[k - i].detach().cpu().numpy()\n                    # append to different attrs\n                    y = dataset[k][""y_onehot""]\n                    for ai in range(self.y_classes):\n                        if y[ai] > 0:\n                            attrs_pos_z[ai][0] += z\n                            attrs_pos_z[ai][1] += 1\n                        else:\n                            attrs_neg_z[ai][0] += z\n                            attrs_neg_z[ai][1] += 1\n                # break\n            deltaz = []\n            for ai in range(self.y_classes):\n                if attrs_pos_z[ai][1] == 0:\n                    attrs_pos_z[ai][1] = 1\n                if attrs_neg_z[ai][1] == 0:\n                    attrs_neg_z[ai][1] = 1\n                z_pos = attrs_pos_z[ai][0] / float(attrs_pos_z[ai][1])\n                z_neg = attrs_neg_z[ai][0] / float(attrs_neg_z[ai][1])\n                deltaz.append(z_pos - z_neg)\n        self.train()\n        return deltaz\n\n    @staticmethod\n    def loss_generative(nll):\n        # Generative loss\n        return torch.mean(nll)\n\n    @staticmethod\n    def loss_multi_classes(y_logits, y_onehot):\n        if y_logits is None:\n            return 0\n        else:\n            return Glow.BCE(y_logits, y_onehot.float())\n\n    @staticmethod\n    def loss_class(y_logits, y):\n        if y_logits is None:\n            return 0\n        else:\n            return Glow.CE(y_logits, y.long())\n'"
glow/modules.py,31,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport scipy.linalg\nfrom . import thops\n\n\nclass _ActNorm(nn.Module):\n    """"""\n    Activation Normalization\n    Initialize the bias and scale with a given minibatch,\n    so that the output per-channel have zero mean and unit variance for that.\n\n    After initialization, `bias` and `logs` will be trained as parameters.\n    """"""\n\n    def __init__(self, num_features, scale=1.):\n        super().__init__()\n        # register mean and scale\n        size = [1, num_features, 1, 1]\n        self.register_parameter(""bias"", nn.Parameter(torch.zeros(*size)))\n        self.register_parameter(""logs"", nn.Parameter(torch.zeros(*size)))\n        self.num_features = num_features\n        self.scale = float(scale)\n        self.inited = False\n\n    def _check_input_dim(self, input):\n        return NotImplemented\n\n    def initialize_parameters(self, input):\n        self._check_input_dim(input)\n        if not self.training:\n            return\n        assert input.device == self.bias.device\n        with torch.no_grad():\n            bias = thops.mean(input.clone(), dim=[0, 2, 3], keepdim=True) * -1.0\n            vars = thops.mean((input.clone() + bias) ** 2, dim=[0, 2, 3], keepdim=True)\n            logs = torch.log(self.scale/(torch.sqrt(vars)+1e-6))\n            self.bias.data.copy_(bias.data)\n            self.logs.data.copy_(logs.data)\n            self.inited = True\n\n    def _center(self, input, reverse=False):\n        if not reverse:\n            return input + self.bias\n        else:\n            return input - self.bias\n\n    def _scale(self, input, logdet=None, reverse=False):\n        logs = self.logs\n        if not reverse:\n            input = input * torch.exp(logs)\n        else:\n            input = input * torch.exp(-logs)\n        if logdet is not None:\n            """"""\n            logs is log_std of `mean of channels`\n            so we need to multiply pixels\n            """"""\n            dlogdet = thops.sum(logs) * thops.pixels(input)\n            if reverse:\n                dlogdet *= -1\n            logdet = logdet + dlogdet\n        return input, logdet\n\n    def forward(self, input, logdet=None, reverse=False):\n        if not self.inited:\n            self.initialize_parameters(input)\n        self._check_input_dim(input)\n        # no need to permute dims as old version\n        if not reverse:\n            # center and scale\n            input = self._center(input, reverse)\n            input, logdet = self._scale(input, logdet, reverse)\n        else:\n            # scale and center\n            input, logdet = self._scale(input, logdet, reverse)\n            input = self._center(input, reverse)\n        return input, logdet\n\n\nclass ActNorm2d(_ActNorm):\n    def __init__(self, num_features, scale=1.):\n        super().__init__(num_features, scale)\n\n    def _check_input_dim(self, input):\n        assert len(input.size()) == 4\n        assert input.size(1) == self.num_features, (\n            ""[ActNorm]: input should be in shape as `BCHW`,""\n            "" channels should be {} rather than {}"".format(\n                self.num_features, input.size()))\n\n\nclass LinearZeros(nn.Linear):\n    def __init__(self, in_channels, out_channels, logscale_factor=3):\n        super().__init__(in_channels, out_channels)\n        self.logscale_factor = logscale_factor\n        # set logs parameter\n        self.register_parameter(""logs"", nn.Parameter(torch.zeros(out_channels)))\n        # init\n        self.weight.data.zero_()\n        self.bias.data.zero_()\n\n    def forward(self, input):\n        output = super().forward(input)\n        return output * torch.exp(self.logs * self.logscale_factor)\n\n\nclass Conv2d(nn.Conv2d):\n    pad_dict = {\n        ""same"": lambda kernel, stride: [((k - 1) * s + 1) // 2 for k, s in zip(kernel, stride)],\n        ""valid"": lambda kernel, stride: [0 for _ in kernel]\n    }\n\n    @staticmethod\n    def get_padding(padding, kernel_size, stride):\n        # make paddding\n        if isinstance(padding, str):\n            if isinstance(kernel_size, int):\n                kernel_size = [kernel_size, kernel_size]\n            if isinstance(stride, int):\n                stride = [stride, stride]\n            padding = padding.lower()\n            try:\n                padding = Conv2d.pad_dict[padding](kernel_size, stride)\n            except KeyError:\n                raise ValueError(""{} is not supported"".format(padding))\n        return padding\n\n    def __init__(self, in_channels, out_channels,\n                 kernel_size=[3, 3], stride=[1, 1],\n                 padding=""same"", do_actnorm=True, weight_std=0.05):\n        padding = Conv2d.get_padding(padding, kernel_size, stride)\n        super().__init__(in_channels, out_channels, kernel_size, stride,\n                         padding, bias=(not do_actnorm))\n        # init weight with std\n        self.weight.data.normal_(mean=0.0, std=weight_std)\n        if not do_actnorm:\n            self.bias.data.zero_()\n        else:\n            self.actnorm = ActNorm2d(out_channels)\n        self.do_actnorm = do_actnorm\n\n    def forward(self, input):\n        x = super().forward(input)\n        if self.do_actnorm:\n            x, _ = self.actnorm(x)\n        return x\n\n\nclass Conv2dZeros(nn.Conv2d):\n    def __init__(self, in_channels, out_channels,\n                 kernel_size=[3, 3], stride=[1, 1],\n                 padding=""same"", logscale_factor=3):\n        padding = Conv2d.get_padding(padding, kernel_size, stride)\n        super().__init__(in_channels, out_channels, kernel_size, stride, padding)\n        # logscale_factor\n        self.logscale_factor = logscale_factor\n        self.register_parameter(""logs"", nn.Parameter(torch.zeros(out_channels, 1, 1)))\n        # init\n        self.weight.data.zero_()\n        self.bias.data.zero_()\n\n    def forward(self, input):\n        output = super().forward(input)\n        return output * torch.exp(self.logs * self.logscale_factor)\n\n\nclass Permute2d(nn.Module):\n    def __init__(self, num_channels, shuffle):\n        super().__init__()\n        self.num_channels = num_channels\n        self.indices = np.arange(self.num_channels - 1, -1, -1).astype(np.long)\n        self.indices_inverse = np.zeros((self.num_channels), dtype=np.long)\n        for i in range(self.num_channels):\n            self.indices_inverse[self.indices[i]] = i\n        if shuffle:\n            self.reset_indices()\n\n    def reset_indices(self):\n        np.random.shuffle(self.indices)\n        for i in range(self.num_channels):\n            self.indices_inverse[self.indices[i]] = i\n\n    def forward(self, input, reverse=False):\n        assert len(input.size()) == 4\n        if not reverse:\n            return input[:, self.indices, :, :]\n        else:\n            return input[:, self.indices_inverse, :, :]\n\n\nclass InvertibleConv1x1(nn.Module):\n    def __init__(self, num_channels, LU_decomposed=False):\n        super().__init__()\n        w_shape = [num_channels, num_channels]\n        w_init = np.linalg.qr(np.random.randn(*w_shape))[0].astype(np.float32)\n        if not LU_decomposed:\n            # Sample a random orthogonal matrix:\n            self.register_parameter(""weight"", nn.Parameter(torch.Tensor(w_init)))\n        else:\n            np_p, np_l, np_u = scipy.linalg.lu(w_init)\n            np_s = np.diag(np_u)\n            np_sign_s = np.sign(np_s)\n            np_log_s = np.log(np.abs(np_s))\n            np_u = np.triu(np_u, k=1)\n            l_mask = np.tril(np.ones(w_shape, dtype=np.float32), -1)\n            eye = np.eye(*w_shape, dtype=np.float32)\n\n            self.register_buffer(\'p\', torch.Tensor(np_p.astype(np.float32)))\n            self.register_buffer(\'sign_s\', torch.Tensor(np_sign_s.astype(np.float32)))\n            self.l = nn.Parameter(torch.Tensor(np_l.astype(np.float32)))\n            self.log_s = nn.Parameter(torch.Tensor(np_log_s.astype(np.float32)))\n            self.u = nn.Parameter(torch.Tensor(np_u.astype(np.float32)))\n            self.l_mask = torch.Tensor(l_mask)\n            self.eye = torch.Tensor(eye)\n        self.w_shape = w_shape\n        self.LU = LU_decomposed\n\n    def get_weight(self, input, reverse):\n        w_shape = self.w_shape\n        if not self.LU:\n            pixels = thops.pixels(input)\n            dlogdet = torch.slogdet(self.weight)[1] * pixels\n            if not reverse:\n                weight = self.weight.view(w_shape[0], w_shape[1], 1, 1)\n            else:\n                weight = torch.inverse(self.weight.double()).float()\\\n                              .view(w_shape[0], w_shape[1], 1, 1)\n            return weight, dlogdet\n        else:\n            self.p = self.p.to(input.device)\n            self.sign_s = self.sign_s.to(input.device)\n            self.l_mask = self.l_mask.to(input.device)\n            self.eye = self.eye.to(input.device)\n            l = self.l * self.l_mask + self.eye\n            u = self.u * self.l_mask.transpose(0, 1).contiguous() + torch.diag(self.sign_s * torch.exp(self.log_s))\n            dlogdet = thops.sum(self.log_s) * thops.pixels(input)\n            if not reverse:\n                w = torch.matmul(self.p, torch.matmul(l, u))\n            else:\n                l = torch.inverse(l.double()).float()\n                u = torch.inverse(u.double()).float()\n                w = torch.matmul(u, torch.matmul(l, self.p.inverse()))\n            return w.view(w_shape[0], w_shape[1], 1, 1), dlogdet\n\n    def forward(self, input, logdet=None, reverse=False):\n        """"""\n        log-det = log|abs(|W|)| * pixels\n        """"""\n        weight, dlogdet = self.get_weight(input, reverse)\n        if not reverse:\n            z = F.conv2d(input, weight)\n            if logdet is not None:\n                logdet = logdet + dlogdet\n            return z, logdet\n        else:\n            z = F.conv2d(input, weight)\n            if logdet is not None:\n                logdet = logdet - dlogdet\n            return z, logdet\n\n\nclass GaussianDiag:\n    Log2PI = float(np.log(2 * np.pi))\n\n    @staticmethod\n    def likelihood(mean, logs, x):\n        """"""\n        lnL = -1/2 * { ln|Var| + ((X - Mu)^T)(Var^-1)(X - Mu) + kln(2*PI) }\n              k = 1 (Independent)\n              Var = logs ** 2\n        """"""\n        return -0.5 * (logs * 2. + ((x - mean) ** 2) / torch.exp(logs * 2.) + GaussianDiag.Log2PI)\n\n    @staticmethod\n    def logp(mean, logs, x):\n        likelihood = GaussianDiag.likelihood(mean, logs, x)\n        return thops.sum(likelihood, dim=[1, 2, 3])\n\n    @staticmethod\n    def sample(mean, logs, eps_std=None):\n        eps_std = eps_std or 1\n        eps = torch.normal(mean=torch.zeros_like(mean),\n                           std=torch.ones_like(logs) * eps_std)\n        return mean + torch.exp(logs) * eps\n\n\nclass Split2d(nn.Module):\n    def __init__(self, num_channels):\n        super().__init__()\n        self.conv = Conv2dZeros(num_channels // 2, num_channels)\n\n    def split2d_prior(self, z):\n        h = self.conv(z)\n        return thops.split_feature(h, ""cross"")\n\n    def forward(self, input, logdet=0., reverse=False, eps_std=None):\n        if not reverse:\n            z1, z2 = thops.split_feature(input, ""split"")\n            mean, logs = self.split2d_prior(z1)\n            logdet = GaussianDiag.logp(mean, logs, z2) + logdet\n            return z1, logdet\n        else:\n            z1 = input\n            mean, logs = self.split2d_prior(z1)\n            z2 = GaussianDiag.sample(mean, logs, eps_std)\n            z = thops.cat_feature(z1, z2)\n            return z, logdet\n\n\ndef squeeze2d(input, factor=2):\n    assert factor >= 1 and isinstance(factor, int)\n    if factor == 1:\n        return input\n    size = input.size()\n    B = size[0]\n    C = size[1]\n    H = size[2]\n    W = size[3]\n    assert H % factor == 0 and W % factor == 0, ""{}"".format((H, W))\n    x = input.view(B, C, H // factor, factor, W // factor, factor)\n    x = x.permute(0, 1, 3, 5, 2, 4).contiguous()\n    x = x.view(B, C * factor * factor, H // factor, W // factor)\n    return x\n\n\ndef unsqueeze2d(input, factor=2):\n    assert factor >= 1 and isinstance(factor, int)\n    factor2 = factor ** 2\n    if factor == 1:\n        return input\n    size = input.size()\n    B = size[0]\n    C = size[1]\n    H = size[2]\n    W = size[3]\n    assert C % (factor2) == 0, ""{}"".format(C)\n    x = input.view(B, C // factor2, factor, factor, H, W)\n    x = x.permute(0, 1, 4, 2, 5, 3).contiguous()\n    x = x.view(B, C // (factor2), H * factor, W * factor)\n    return x\n\n\nclass SqueezeLayer(nn.Module):\n    def __init__(self, factor):\n        super().__init__()\n        self.factor = factor\n\n    def forward(self, input, logdet=None, reverse=False):\n        if not reverse:\n            output = squeeze2d(input, self.factor)\n            return output, logdet\n        else:\n            output = unsqueeze2d(input, self.factor)\n            return output, logdet\n'"
glow/thops.py,4,"b'import torch\n\n\ndef onehot(y, num_classes):\n    y_onehot = torch.zeros(y.size(0), num_classes).to(y.device)\n    if len(y.size()) == 1:\n        y_onehot = y_onehot.scatter_(1, y.unsqueeze(-1), 1)\n    elif len(y.size()) == 2:\n        y_onehot = y_onehot.scatter_(1, y, 1)\n    else:\n        raise ValueError(""[onehot]: y should be in shape [B], or [B, C]"")\n    return y_onehot\n\n\ndef sum(tensor, dim=None, keepdim=False):\n    if dim is None:\n        # sum up all dim\n        return torch.sum(tensor)\n    else:\n        if isinstance(dim, int):\n            dim = [dim]\n        dim = sorted(dim)\n        for d in dim:\n            tensor = tensor.sum(dim=d, keepdim=True)\n        if not keepdim:\n            for i, d in enumerate(dim):\n                tensor.squeeze_(d-i)\n        return tensor\n\n\ndef mean(tensor, dim=None, keepdim=False):\n    if dim is None:\n        # mean all dim\n        return torch.mean(tensor)\n    else:\n        if isinstance(dim, int):\n            dim = [dim]\n        dim = sorted(dim)\n        for d in dim:\n            tensor = tensor.mean(dim=d, keepdim=True)\n        if not keepdim:\n            for i, d in enumerate(dim):\n                tensor.squeeze_(d-i)\n        return tensor\n\n\ndef split_feature(tensor, type=""split""):\n    """"""\n    type = [""split"", ""cross""]\n    """"""\n    C = tensor.size(1)\n    if type == ""split"":\n        return tensor[:, :C // 2, ...], tensor[:, C // 2:, ...]\n    elif type == ""cross"":\n        return tensor[:, 0::2, ...], tensor[:, 1::2, ...]\n\n\ndef cat_feature(tensor_a, tensor_b):\n    return torch.cat((tensor_a, tensor_b), dim=1)\n\n\ndef pixels(tensor):\n    return int(tensor.size(2) * tensor.size(3))\n'"
glow/trainer.py,12,"b'import re\nimport os\nimport torch\nimport torch.nn.functional as F\nimport datetime\nimport numpy as np\nfrom tqdm import tqdm\nfrom tensorboardX import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom .utils import save, load, plot_prob\nfrom .config import JsonConfig\nfrom .models import Glow\nfrom . import thops\n\n\nclass Trainer(object):\n    def __init__(self, graph, optim, lrschedule, loaded_step,\n                 devices, data_device,\n                 dataset, hparams):\n        if isinstance(hparams, str):\n            hparams = JsonConfig(hparams)\n        # set members\n        # append date info\n        date = str(datetime.datetime.now())\n        date = date[:date.rfind("":"")].replace(""-"", """")\\\n                                     .replace("":"", """")\\\n                                     .replace("" "", ""_"")\n        self.log_dir = os.path.join(hparams.Dir.log_root, ""log_"" + date)\n        self.checkpoints_dir = os.path.join(self.log_dir, ""checkpoints"")\n        if not os.path.exists(self.log_dir):\n            os.makedirs(self.log_dir)\n        # write hparams\n        hparams.dump(self.log_dir)\n        if not os.path.exists(self.checkpoints_dir):\n            os.makedirs(self.checkpoints_dir)\n        self.checkpoints_gap = hparams.Train.checkpoints_gap\n        self.max_checkpoints = hparams.Train.max_checkpoints\n        # model relative\n        self.graph = graph\n        self.optim = optim\n        self.weight_y = hparams.Train.weight_y\n        # grad operation\n        self.max_grad_clip = hparams.Train.max_grad_clip\n        self.max_grad_norm = hparams.Train.max_grad_norm\n        # copy devices from built graph\n        self.devices = devices\n        self.data_device = data_device\n        # number of training batches\n        self.batch_size = hparams.Train.batch_size\n        self.data_loader = DataLoader(dataset,\n                                      batch_size=self.batch_size,\n                                    #   num_workers=8,\n                                      shuffle=True,\n                                      drop_last=True)\n        self.n_epoches = (hparams.Train.num_batches+len(self.data_loader)-1)\n        self.n_epoches = self.n_epoches // len(self.data_loader)\n        self.global_step = 0\n        # lr schedule\n        self.lrschedule = lrschedule\n        self.loaded_step = loaded_step\n        # data relative\n        self.y_classes = hparams.Glow.y_classes\n        self.y_condition = hparams.Glow.y_condition\n        self.y_criterion = hparams.Criterion.y_condition\n        assert self.y_criterion in [""multi-classes"", ""single-class""]\n\n        # log relative\n        # tensorboard\n        self.writer = SummaryWriter(log_dir=self.log_dir)\n        self.scalar_log_gaps = hparams.Train.scalar_log_gap\n        self.plot_gaps = hparams.Train.plot_gap\n        self.inference_gap = hparams.Train.inference_gap\n\n    def train(self):\n        # set to training state\n        self.graph.train()\n        self.global_step = self.loaded_step\n        # begin to train\n        for epoch in range(self.n_epoches):\n            print(""epoch"", epoch)\n            progress = tqdm(self.data_loader)\n            for i_batch, batch in enumerate(progress):\n                # update learning rate\n                lr = self.lrschedule[""func""](global_step=self.global_step,\n                                             **self.lrschedule[""args""])\n                for param_group in self.optim.param_groups:\n                    param_group[\'lr\'] = lr\n                self.optim.zero_grad()\n                if self.global_step % self.scalar_log_gaps == 0:\n                    self.writer.add_scalar(""lr/lr"", lr, self.global_step)\n                # get batch data\n                for k in batch:\n                    batch[k] = batch[k].to(self.data_device)\n                x = batch[""x""]\n                y = None\n                y_onehot = None\n                if self.y_condition:\n                    if self.y_criterion == ""multi-classes"":\n                        assert ""y_onehot"" in batch, ""multi-classes ask for `y_onehot` (torch.FloatTensor onehot)""\n                        y_onehot = batch[""y_onehot""]\n                    elif self.y_criterion == ""single-class"":\n                        assert ""y"" in batch, ""single-class ask for `y` (torch.LongTensor indexes)""\n                        y = batch[""y""]\n                        y_onehot = thops.onehot(y, num_classes=self.y_classes)\n\n                # at first time, initialize ActNorm\n                if self.global_step == 0:\n                    self.graph(x[:self.batch_size // len(self.devices), ...],\n                               y_onehot[:self.batch_size // len(self.devices), ...] if y_onehot is not None else None)\n                # parallel\n                if len(self.devices) > 1 and not hasattr(self.graph, ""module""):\n                    print(""[Parallel] move to {}"".format(self.devices))\n                    self.graph = torch.nn.parallel.DataParallel(self.graph, self.devices, self.devices[0])\n                # forward phase\n                z, nll, y_logits = self.graph(x=x, y_onehot=y_onehot)\n\n                # loss\n                loss_generative = Glow.loss_generative(nll)\n                loss_classes = 0\n                if self.y_condition:\n                    loss_classes = (Glow.loss_multi_classes(y_logits, y_onehot)\n                                    if self.y_criterion == ""multi-classes"" else\n                                    Glow.loss_class(y_logits, y))\n                if self.global_step % self.scalar_log_gaps == 0:\n                    self.writer.add_scalar(""loss/loss_generative"", loss_generative, self.global_step)\n                    if self.y_condition:\n                        self.writer.add_scalar(""loss/loss_classes"", loss_classes, self.global_step)\n                loss = loss_generative + loss_classes * self.weight_y\n\n                # backward\n                self.graph.zero_grad()\n                self.optim.zero_grad()\n                loss.backward()\n                # operate grad\n                if self.max_grad_clip is not None and self.max_grad_clip > 0:\n                    torch.nn.utils.clip_grad_value_(self.graph.parameters(), self.max_grad_clip)\n                if self.max_grad_norm is not None and self.max_grad_norm > 0:\n                    grad_norm = torch.nn.utils.clip_grad_norm_(self.graph.parameters(), self.max_grad_norm)\n                    if self.global_step % self.scalar_log_gaps == 0:\n                        self.writer.add_scalar(""grad_norm/grad_norm"", grad_norm, self.global_step)\n                # step\n                self.optim.step()\n\n                # checkpoints\n                if self.global_step % self.checkpoints_gap == 0 and self.global_step > 0:\n                    save(global_step=self.global_step,\n                         graph=self.graph,\n                         optim=self.optim,\n                         pkg_dir=self.checkpoints_dir,\n                         is_best=True,\n                         max_checkpoints=self.max_checkpoints)\n                if self.global_step % self.plot_gaps == 0:\n                    img = self.graph(z=z, y_onehot=y_onehot, reverse=True)\n                    # img = torch.clamp(img, min=0, max=1.0)\n                    if self.y_condition:\n                        if self.y_criterion == ""multi-classes"":\n                            y_pred = torch.sigmoid(y_logits)\n                        elif self.y_criterion == ""single-class"":\n                            y_pred = thops.onehot(torch.argmax(F.softmax(y_logits, dim=1), dim=1, keepdim=True),\n                                                  self.y_classes)\n                        y_true = y_onehot\n                    for bi in range(min([len(img), 4])):\n                        self.writer.add_image(""0_reverse/{}"".format(bi), torch.cat((img[bi], batch[""x""][bi]), dim=1), self.global_step)\n                        if self.y_condition:\n                            self.writer.add_image(""1_prob/{}"".format(bi), plot_prob([y_pred[bi], y_true[bi]], [""pred"", ""true""]), self.global_step)\n\n                # inference\n                if hasattr(self, ""inference_gap""):\n                    if self.global_step % self.inference_gap == 0:\n                        img = self.graph(z=None, y_onehot=y_onehot, eps_std=0.5, reverse=True)\n                        # img = torch.clamp(img, min=0, max=1.0)\n                        for bi in range(min([len(img), 4])):\n                            self.writer.add_image(""2_sample/{}"".format(bi), img[bi], self.global_step)\n\n                # global step\n                self.global_step += 1\n\n        self.writer.export_scalars_to_json(os.path.join(self.log_dir, ""all_scalars.json""))\n        self.writer.close()\n'"
glow/utils.py,4,"b'import os\nimport re\nimport copy\nimport torch\nimport numpy as np\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\nfrom shutil import copyfile\n\n\ndef get_proper_cuda_device(device, verbose=True):\n    if not isinstance(device, list):\n        device = [device]\n    count = torch.cuda.device_count()\n    if verbose:\n        print(""[Builder]: Found {} gpu"".format(count))\n    for i in range(len(device)):\n        d = device[i]\n        did = None\n        if isinstance(d, str):\n            if re.search(""cuda:[\\d]+"", d):\n                did = int(d[5:])\n        elif isinstance(d, int):\n            did = d\n        if did is None:\n            raise ValueError(""[Builder]: Wrong cuda id {}"".format(d))\n        if did < 0 or did >= count:\n            if verbose:\n                print(""[Builder]: {} is not found, ignore."".format(d))\n            device[i] = None\n        else:\n            device[i] = did\n    device = [d for d in device if d is not None]\n    return device\n\n\ndef get_proper_device(devices, verbose=True):\n    origin = copy.copy(devices)\n    devices = copy.copy(devices)\n    if not isinstance(devices, list):\n        devices = [devices]\n    use_cpu = any([d.find(""cpu"")>=0 for d in devices])\n    use_gpu = any([(d.find(""cuda"")>=0 or isinstance(d, int)) for d in devices])\n    assert not (use_cpu and use_gpu), ""{} contains cpu and cuda device."".format(devices)\n    if use_gpu:\n        devices = get_proper_cuda_device(devices, verbose)\n        if len(devices) == 0:\n            if verbose:\n                print(""[Builder]: Failed to find any valid gpu in {}, use `cpu`."".format(origin))\n            devices = [""cpu""]\n    return devices\n\n\ndef _file_at_step(step):\n    return ""save_{}k{}.pkg"".format(int(step // 1000), int(step % 1000))\n\n\ndef _file_best():\n    return ""trained.pkg""\n\n\ndef save(global_step, graph, optim, criterion_dict=None, pkg_dir="""", is_best=False, max_checkpoints=None):\n    if optim is None:\n        raise ValueError(""cannot save without optimzier"")\n    state = {\n        ""global_step"": global_step,\n        # DataParallel wrap model in attr `module`.\n        ""graph"": graph.module.state_dict() if hasattr(graph, ""module"") else graph.state_dict(),\n        ""optim"": optim.state_dict(),\n        ""criterion"": {}\n    }\n    if criterion_dict is not None:\n        for k in criterion_dict:\n            state[""criterion""][k] = criterion_dict[k].state_dict()\n    save_path = os.path.join(pkg_dir, _file_at_step(global_step))\n    best_path = os.path.join(pkg_dir, _file_best())\n    torch.save(state, save_path)\n    if is_best:\n        copyfile(save_path, best_path)\n    if max_checkpoints is not None:\n        history = []\n        for file_name in os.listdir(pkg_dir):\n            if re.search(""save_\\d*k\\d*\\.pkg"", file_name):\n                digits = file_name.replace(""save_"", """").replace("".pkg"", """").split(""k"")\n                number = int(digits[0]) * 1000 + int(digits[1])\n                history.append(number)\n        history.sort()\n        while len(history) > max_checkpoints:\n            path = os.path.join(pkg_dir, _file_at_step(history[0]))\n            print(""[Checkpoint]: remove {} to keep {} checkpoints"".format(path, max_checkpoints))\n            if os.path.exists(path):\n                os.remove(path)\n            history.pop(0)\n\n\ndef load(step_or_path, graph, optim=None, criterion_dict=None, pkg_dir="""", device=None):\n    step = step_or_path\n    save_path = None\n    if isinstance(step, int):\n        save_path = os.path.join(pkg_dir, _file_at_step(step))\n    if isinstance(step, str):\n        if pkg_dir is not None:\n            if step == ""best"":\n                save_path = os.path.join(pkg_dir, _file_best())\n            else:\n                save_path = os.path.join(pkg_dir, step)\n        else:\n            save_path = step\n    if save_path is not None and not os.path.exists(save_path):\n        print(""[Checkpoint]: Failed to find {}"".format(save_path))\n        return\n    if save_path is None:\n        print(""[Checkpoint]: Cannot load the checkpoint with given step or filename or `best`"")\n        return\n\n    # begin to load\n    state = torch.load(save_path, map_location=device)\n    global_step = state[""global_step""]\n    graph.load_state_dict(state[""graph""])\n    if optim is not None:\n        optim.load_state_dict(state[""optim""])\n    if criterion_dict is not None:\n        for k in criterion_dict:\n            criterion_dict[k].load_state_dict(state[""criterion""][k])\n\n    graph.set_actnorm_init(inited=True)\n\n    print(""[Checkpoint]: Load {} successfully"".format(save_path))\n    return global_step\n\n\ndef __save_figure_to_numpy(fig):\n    # save it to a numpy array.\n    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\'\')\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    return data\n\n\ndef __to_ndarray_list(tensors, titles):\n    if not isinstance(tensors, list):\n        tensors = [tensors]\n        titles = [titles]\n    assert len(titles) == len(tensors),\\\n        ""[visualizer]: {} titles are not enough for {} tensors"".format(\n            len(titles), len(tensors))\n    for i in range(len(tensors)):\n        if torch.is_tensor(tensors[i]):\n            tensors[i] = tensors[i].cpu().detach().numpy()\n    return tensors, titles\n\n\ndef __get_figures(num_tensors, figsize):\n    fig, axes = plt.subplots(num_tensors, 1, figsize=figsize)\n    if not isinstance(axes, np.ndarray):\n        axes = np.asarray([axes])\n    return fig, axes\n\n\ndef __make_dir(file_name, plot_dir):\n    if file_name is not None and not os.path.exists(plot_dir):\n        os.makedirs(plot_dir)\n\n\ndef __draw(fig, file_name, plot_dir):\n    if file_name is not None:\n        plt.savefig(\'{}/{}.png\'.format(plot_dir, file_name), format=\'png\')\n        plt.close(fig)\n        return None\n    else:\n        fig.tight_layout()\n        fig.canvas.draw()\n        data = __save_figure_to_numpy(fig)\n        plt.close(fig)\n        return data\n\n\ndef __get_size_for_spec(tensors):\n    spectrogram = tensors[0]\n    fig_w = np.min([int(np.ceil(spectrogram.shape[1] / 10.0)), 10])\n    fig_w = np.max([fig_w, 3])\n    fig_h = np.max([3 * len(tensors), 3])\n    return (fig_w, fig_h)\n\n\ndef __get_aspect(spectrogram):\n    fig_w = np.min([int(np.ceil(spectrogram.shape[1] / 10.0)), 10])\n    fig_w = np.max([fig_w, 3])\n    aspect = 3.0 / fig_w\n    if spectrogram.shape[1] > 50:\n        aspect = aspect * spectrogram.shape[1] / spectrogram.shape[0]\n    else:\n        aspect = aspect * spectrogram.shape[1] / (spectrogram.shape[0])\n    return aspect\n\n\ndef plot_prob(done, title="""", file_name=None, plot_dir=None):\n    __make_dir(file_name, plot_dir)\n\n    done, title = __to_ndarray_list(done, title)\n    for i in range(len(done)):\n        done[i] = np.reshape(done[i], (-1, done[i].shape[-1]))\n    figsize = (5, 5 * len(done))\n    fig, axes = __get_figures(len(done), figsize)\n    for ax, d, t in zip(axes, done, title):\n        im = ax.imshow(d, vmin=0, vmax=1, cmap=""Blues"", aspect=d.shape[1]/d.shape[0])\n        ax.set_title(t)\n        ax.set_yticks(np.arange(d.shape[0]))\n        lables = [""Frame{}"".format(i+1) for i in range(d.shape[0])]\n        ax.set_yticklabels(lables)\n        ax.set_yticks(np.arange(d.shape[0])-.5, minor=True)\n        ax.grid(which=""minor"", color=""g"", linestyle=\'-.\', linewidth=1)\n        ax.invert_yaxis()\n    return __draw(fig, file_name, plot_dir)\n'"
vision/__init__.py,0,"b'from .datasets import CelebADataset\n\nDatasets = {\n    ""celeba"": CelebADataset\n}\n'"
vision/datasets/__init__.py,0,b'from .celeba import CelebADataset\n\n'
vision/datasets/celeba.py,1,"b'import os\nimport re\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\n\nIMAGE_EXTENSTOINS = ["".png"", "".jpg"", "".jpeg"", "".bmp""]\nATTR_ANNO = ""list_attr_celeba.txt""\n\ndef _is_image(fname):\n    _, ext = os.path.splitext(fname)\n    return ext.lower() in IMAGE_EXTENSTOINS\n\n\ndef _find_images_and_annotation(root_dir):\n    images = {}\n    attr = None\n    assert os.path.exists(root_dir), ""{} not exists"".format(root_dir)\n    for root, _, fnames in sorted(os.walk(root_dir)):\n        for fname in sorted(fnames):\n            if _is_image(fname):\n                path = os.path.join(root, fname)\n                images[os.path.splitext(fname)[0]] = path\n            elif fname.lower() == ATTR_ANNO:\n                attr = os.path.join(root, fname)\n\n    assert attr is not None, ""Failed to find `list_attr_celeba.txt`""\n\n    # begin to parse all image\n    print(""Begin to parse all image attrs"")\n    final = []\n    with open(attr, ""r"") as fin:\n        image_total = 0\n        attrs = []\n        for i_line, line in enumerate(fin):\n            line = line.strip()\n            if i_line == 0:\n                image_total = int(line)\n            elif i_line == 1:\n                attrs = line.split("" "")\n            else:\n                line = re.sub(""[ ]+"", "" "", line)\n                line = line.split("" "")\n                fname = os.path.splitext(line[0])[0]\n                onehot = [int(int(d) > 0) for d in line[1:]]\n                assert len(onehot) == len(attrs), ""{} only has {} attrs < {}"".format(\n                    fname, len(onehot), len(attrs))\n                final.append({\n                    ""path"": images[fname],\n                    ""attr"": onehot\n                })\n    print(""Find {} images, with {} attrs"".format(len(final), len(attrs)))\n    return final, attrs\n\n\nclass CelebADataset(Dataset):\n    def __init__(self, root_dir, transform=transforms.Compose([\n                                           transforms.CenterCrop(160),\n                                           transforms.Resize(32),\n                                           transforms.ToTensor()])):\n        super().__init__()\n        dicts, attrs = _find_images_and_annotation(root_dir)\n        self.data = dicts\n        self.attrs = attrs\n        self.transform = transform\n\n    def __getitem__(self, index):\n        data = self.data[index]\n        path = data[""path""]\n        attr = data[""attr""]\n        image= Image.open(path).convert(""RGB"")\n        if self.transform is not None:\n            image = self.transform(image)\n        return {\n            ""x"": image,\n            ""y_onehot"": np.asarray(attr, dtype=np.float32)\n        }\n\n    def __len__(self):\n        return len(self.data)\n\n\nif __name__ == ""__main__"":\n    import cv2\n    celeba = CelebADataset(""/home/chaiyujin/Downloads/Dataset/CelebA"")\n    d = celeba[0]\n    print(d[""x""].size())\n    img = d[""x""].permute(1, 2, 0).contiguous().numpy()\n    print(np.min(img), np.max(img))\n    cv2.imshow(""img"", img)\n    cv2.waitKey()\n'"
