file_path,api_count,code
demo.py,9,"b""# coding:utf-8\n\n'''\nMarch 2019 by Chen Jun\nhttps://github.com/chenjun2hao/Attention_ocr.pytorch\n\n'''\n\nimport torch\nfrom torch.autograd import Variable\nimport utils\nimport dataset\nfrom PIL import Image\nfrom utils import alphabet\nimport models.crnn_lang as crnn\n\nuse_gpu = True\n\nencoder_path = './expr/attentioncnn/encoder_5.pth'\n# decoder_path = './expr/attentioncnn/decoder_5.pth'\nimg_path = './test_img/20441531_4212871437.jpg'\nmax_length = 15                          # \xe6\x9c\x80\xe9\x95\xbf\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\nEOS_TOKEN = 1\n\nnclass = len(alphabet) + 3\nencoder = crnn.CNN(32, 1, 256)          # \xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8\n# decoder = crnn.decoder(256, nclass)     # seq to seq\xe7\x9a\x84\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8, nclass\xe5\x9c\xa8decoder\xe4\xb8\xad\xe8\xbf\x98\xe5\x8a\xa0\xe4\xba\x862\ndecoder = crnn.decoderV2(256, nclass)\n\n\nif encoder_path and decoder_path:\n    print('loading pretrained models ......')\n    encoder.load_state_dict(torch.load(encoder_path))\n    decoder.load_state_dict(torch.load(decoder_path))\nif torch.cuda.is_available() and use_gpu:\n    encoder = encoder.cuda()\n    decoder = decoder.cuda()\n\n\nconverter = utils.strLabelConverterForAttention(alphabet)\n\ntransformer = dataset.resizeNormalize((280, 32))\nimage = Image.open(img_path).convert('L')\nimage = transformer(image)\nif torch.cuda.is_available() and use_gpu:\n    image = image.cuda()\nimage = image.view(1, *image.size())\nimage = Variable(image)\n\nencoder.eval()\ndecoder.eval()\nencoder_out = encoder(image)\n\ndecoded_words = []\nprob = 1.0\ndecoder_attentions = torch.zeros(max_length, 71)\ndecoder_input = torch.zeros(1).long()      # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96decoder\xe7\x9a\x84\xe5\xbc\x80\xe5\xa7\x8b,\xe4\xbb\x8e0\xe5\xbc\x80\xe5\xa7\x8b\xe8\xbe\x93\xe5\x87\xba\ndecoder_hidden = decoder.initHidden(1)\nif torch.cuda.is_available() and use_gpu:\n    decoder_input = decoder_input.cuda()\n    decoder_hidden = decoder_hidden.cuda()\nloss = 0.0\n# \xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe9\x87\x87\xe7\x94\xa8\xe9\x9d\x9e\xe5\xbc\xba\xe5\x88\xb6\xe7\xad\x96\xe7\x95\xa5\xef\xbc\x8c\xe5\xb0\x86\xe5\x89\x8d\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x8b\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe7\x9b\xb4\xe5\x88\xb0\xe6\xa0\x87\xe7\xad\xbe\xe4\xb8\xbaEOS_TOKEN\xe6\x97\xb6\xe5\x81\x9c\xe6\xad\xa2\nfor di in range(max_length):  # \xe6\x9c\x80\xe5\xa4\xa7\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n    decoder_output, decoder_hidden, decoder_attention = decoder(\n        decoder_input, decoder_hidden, encoder_out)\n    probs = torch.exp(decoder_output)\n    decoder_attentions[di] = decoder_attention.data\n    topv, topi = decoder_output.data.topk(1)\n    ni = topi.squeeze(1)\n    decoder_input = ni\n    prob *= probs[:, ni]\n    if ni == EOS_TOKEN:\n        # decoded_words.append('<EOS>')\n        break\n    else:\n        decoded_words.append(converter.decode(ni))\n\nwords = ''.join(decoded_words)\nprob = prob.item()\nprint('predict_str:%-20s => prob:%-20s' % (words, prob))\n"""
train.py,19,"b'# coding:utf-8\nfrom __future__ import print_function\nimport argparse\nimport random\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport numpy as np\nimport os\nimport src.utils as utils\nimport src.dataset as dataset\nimport time\nfrom src.utils import alphabet\nfrom src.utils import weights_init\n\nimport models.crnn_lang as crnn\nprint(crnn.__name__)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--trainlist\',  default=\'./data/ch_train.txt\')\nparser.add_argument(\'--vallist\',  default=\'./data/ch_test.txt\')\nparser.add_argument(\'--workers\', type=int, help=\'number of data loading workers\', default=2)\nparser.add_argument(\'--batchSize\', type=int, default=4, help=\'input batch size\')\nparser.add_argument(\'--imgH\', type=int, default=32, help=\'the height of the input image to network\')\nparser.add_argument(\'--imgW\', type=int, default=280, help=\'the width of the input image to network\')\nparser.add_argument(\'--nh\', type=int, default=256, help=\'size of the lstm hidden state\')\nparser.add_argument(\'--niter\', type=int, default=21, help=\'number of epochs to train for\')\nparser.add_argument(\'--lr\', type=float, default=0.001, help=\'learning rate for Critic, default=0.00005\')\nparser.add_argument(\'--beta1\', type=float, default=0.5, help=\'beta1 for adam. default=0.5\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\', default=True)\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'number of GPUs to use\')\nparser.add_argument(\'--encoder\', type=str, default=\'./expr/attentioncnn/encoder_600.pth\', help=""path to encoder (to continue training)"")\nparser.add_argument(\'--decoder\', type=str, default=\'\', help=\'path to decoder (to continue training)\')\nparser.add_argument(\'--experiment\', default=\'./expr/attentioncnn\', help=\'Where to store samples and models\')\nparser.add_argument(\'--displayInterval\', type=int, default=10, help=\'Interval to be displayed\')\nparser.add_argument(\'--valInterval\', type=int, default=1, help=\'Interval to be displayed\')\nparser.add_argument(\'--saveInterval\', type=int, default=10, help=\'Interval to be displayed\')\nparser.add_argument(\'--adam\', default=True, action=\'store_true\', help=\'Whether to use adam (default is rmsprop)\')\nparser.add_argument(\'--adadelta\', action=\'store_true\', help=\'Whether to use adadelta (default is rmsprop)\')\nparser.add_argument(\'--keep_ratio\', action=\'store_true\', help=\'whether to keep ratio for image resize\')\nparser.add_argument(\'--random_sample\', default=True, action=\'store_true\', help=\'whether to sample the dataset with random sampler\')\nparser.add_argument(\'--teaching_forcing_prob\', type=float, default=0.5, help=\'where to use teach forcing\')\nparser.add_argument(\'--max_width\', type=int, default=71, help=\'the width of the featuremap out from cnn\')\nopt = parser.parse_args()\nprint(opt)\n\nSOS_token = 0\nEOS_TOKEN = 1              # \xe7\xbb\x93\xe6\x9d\x9f\xe6\xa0\x87\xe5\xbf\x97\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\nBLANK = 2                  # blank for padding\n\n\nif opt.experiment is None:\n    opt.experiment = \'expr\'\nos.system(\'mkdir -p {0}\'.format(opt.experiment))        # \xe5\x88\x9b\xe5\xbb\xba\xe5\xa4\x9a\xe7\xba\xa7\xe7\x9b\xae\xe5\xbd\x95\n\nopt.manualSeed = random.randint(1, 10000)  # fix seed\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\nnp.random.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\n\ncudnn.benchmark = True\n\nif torch.cuda.is_available() and not opt.cuda:\n    print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n\ntransform = None\ntrain_dataset = dataset.listDataset(list_file =opt.trainlist, transform=transform)\nassert train_dataset\nif not opt.random_sample:\n    sampler = dataset.randomSequentialSampler(train_dataset, opt.batchSize)\nelse:\n    sampler = None\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=opt.batchSize,\n    shuffle=False, sampler=sampler,\n    num_workers=int(opt.workers),\n    collate_fn=dataset.alignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio=opt.keep_ratio))\n\ntest_dataset = dataset.listDataset(list_file =opt.vallist, transform=dataset.resizeNormalize((opt.imgW, opt.imgH)))\n\nnclass = len(alphabet) + 3          # decoder\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0,3 for SOS,EOS\xe5\x92\x8cblank \nnc = 1\n\nconverter = utils.strLabelConverterForAttention(alphabet)\n# criterion = torch.nn.CrossEntropyLoss()\ncriterion = torch.nn.NLLLoss()              # \xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe8\xa6\x81\xe4\xb8\xbalog_softmax\n\n\nencoder = crnn.CNN(opt.imgH, nc, opt.nh)\n# decoder = crnn.decoder(opt.nh, nclass, dropout_p=0.1, max_length=opt.max_width)        # max_length:w/4,\xe4\xb8\xbaencoder\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\xe4\xb9\x8b\xe5\x90\x8e\xe5\xae\xbd\xe5\xba\xa6\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6\ndecoder = crnn.decoderV2(opt.nh, nclass, dropout_p=0.1)        # For prediction of an indefinite long sequence\nencoder.apply(weights_init)\ndecoder.apply(weights_init)\n# continue training or use the pretrained model to initial the parameters of the encoder and decoder\nif opt.encoder:\n    print(\'loading pretrained encoder model from %s\' % opt.encoder)\n    encoder.load_state_dict(torch.load(opt.encoder))\nif opt.decoder:\n    print(\'loading pretrained encoder model from %s\' % opt.decoder)\n    encoder.load_state_dict(torch.load(opt.encoder))\nprint(encoder)\nprint(decoder)\n\nimage = torch.FloatTensor(opt.batchSize, 3, opt.imgH, opt.imgH)\ntext = torch.LongTensor(opt.batchSize * 5)\nlength = torch.IntTensor(opt.batchSize)\n\nif opt.cuda:\n    encoder.cuda()\n    decoder.cuda()\n    # encoder = torch.nn.DataParallel(encoder, device_ids=range(opt.ngpu))\n    # decoder = torch.nn.DataParallel(decoder, device_ids=range(opt.ngpu))\n    image = image.cuda()\n    text = text.cuda()\n    criterion = criterion.cuda()\n\n# loss averager\nloss_avg = utils.averager()\n\n# setup optimizer\nif opt.adam:\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=opt.lr,\n                           betas=(opt.beta1, 0.999))\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=opt.lr,\n                        betas=(opt.beta1, 0.999))\nelif opt.adadelta:\n    optimizer = optim.Adadelta(encoder.parameters(), lr=opt.lr)\nelse:\n    encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=opt.lr)\n    decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=opt.lr)\n\n\ndef val(encoder, decoder, criterion, batchsize, dataset, teach_forcing=False, max_iter=100):\n    print(\'Start val\')\n\n    for e, d in zip(encoder.parameters(), decoder.parameters()):\n        e.requires_grad = False\n        d.requires_grad = False\n\n    encoder.eval()\n    decoder.eval()\n    data_loader = torch.utils.data.DataLoader(\n        dataset, shuffle=False, batch_size=batchsize, num_workers=int(opt.workers))\n    val_iter = iter(data_loader)\n\n    n_correct = 0\n    n_total = 0\n    loss_avg = utils.averager()\n\n    max_iter = min(max_iter, len(data_loader))\n    # max_iter = len(data_loader) - 1\n    for i in range(max_iter):\n        data = val_iter.next()\n        i += 1\n        cpu_images, cpu_texts = data\n        b = cpu_images.size(0)\n        utils.loadData(image, cpu_images)\n\n        target_variable = converter.encode(cpu_texts)\n        n_total += len(cpu_texts[0]) + 1                       # \xe8\xbf\x98\xe8\xa6\x81\xe5\x87\x86\xe7\xa1\xae\xe9\xa2\x84\xe6\xb5\x8b\xe5\x87\xbaEOS\xe5\x81\x9c\xe6\xad\xa2\xe4\xbd\x8d\n\n        decoded_words = []\n        decoded_label = []\n        decoder_attentions = torch.zeros(len(cpu_texts[0]) + 1, opt.max_width)\n        encoder_outputs = encoder(image)            # cnn+biLstm\xe5\x81\x9a\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\n        target_variable = target_variable.cuda()\n        decoder_input = target_variable[0].cuda()   # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96decoder\xe7\x9a\x84\xe5\xbc\x80\xe5\xa7\x8b,\xe4\xbb\x8e0\xe5\xbc\x80\xe5\xa7\x8b\xe8\xbe\x93\xe5\x87\xba\n        decoder_hidden = decoder.initHidden(b).cuda()\n        loss = 0.0\n        if not teach_forcing:\n            # \xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe9\x87\x87\xe7\x94\xa8\xe9\x9d\x9e\xe5\xbc\xba\xe5\x88\xb6\xe7\xad\x96\xe7\x95\xa5\xef\xbc\x8c\xe5\xb0\x86\xe5\x89\x8d\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x8b\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe7\x9b\xb4\xe5\x88\xb0\xe6\xa0\x87\xe7\xad\xbe\xe4\xb8\xbaEOS_TOKEN\xe6\x97\xb6\xe5\x81\x9c\xe6\xad\xa2\n            for di in range(1, target_variable.shape[0]):  # \xe6\x9c\x80\xe5\xa4\xa7\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n                decoder_output, decoder_hidden, decoder_attention = decoder(\n                    decoder_input, decoder_hidden, encoder_outputs)\n                loss += criterion(decoder_output, target_variable[di])  # \xe6\xaf\x8f\xe6\xac\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x97\xe7\xac\xa6\n                loss_avg.add(loss)\n                decoder_attentions[di-1] = decoder_attention.data\n                topv, topi = decoder_output.data.topk(1)\n                ni = topi.squeeze(1)\n                decoder_input = ni\n                if ni == EOS_TOKEN:\n                    decoded_words.append(\'<EOS>\')\n                    decoded_label.append(EOS_TOKEN)\n                    break\n                else:\n                    decoded_words.append(converter.decode(ni))\n                    decoded_label.append(ni)\n\n        # \xe8\xae\xa1\xe7\xae\x97\xe6\xad\xa3\xe7\xa1\xae\xe4\xb8\xaa\xe6\x95\xb0\n        for pred, target in zip(decoded_label, target_variable[1:,:]):\n            if pred == target:\n                n_correct += 1\n\n        if i % 100 == 0:                 # \xe6\xaf\x8f100\xe6\xac\xa1\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x80\xe6\xac\xa1\n            texts = cpu_texts[0]\n            print(\'pred:%-20s, gt: %-20s\' % (decoded_words, texts))\n\n    accuracy = n_correct / float(n_total)\n    print(\'Test loss: %f, accuray: %f\' % (loss_avg.val(), accuracy))\n\n\ndef trainBatch(encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, teach_forcing_prob=1):\n    \'\'\'\n        target_label:\xe9\x87\x87\xe7\x94\xa8\xe5\x90\x8e\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\x96\xe7\xa0\x81\xe5\x92\x8c\xe5\xaf\xb9\xe9\xbd\x90\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe8\xbf\x9b\xe8\xa1\x8cbatch\xe8\xae\xad\xe7\xbb\x83\n    \'\'\'\n    data = train_iter.next()\n    cpu_images, cpu_texts = data\n    b = cpu_images.size(0)\n    target_variable = converter.encode(cpu_texts)\n    utils.loadData(image, cpu_images)\n\n    encoder_outputs = encoder(image)               # cnn+biLstm\xe5\x81\x9a\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\n    target_variable = target_variable.cuda()\n    decoder_input = target_variable[0].cuda()      # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96decoder\xe7\x9a\x84\xe5\xbc\x80\xe5\xa7\x8b,\xe4\xbb\x8e0\xe5\xbc\x80\xe5\xa7\x8b\xe8\xbe\x93\xe5\x87\xba\n    decoder_hidden = decoder.initHidden(b).cuda()\n    loss = 0.0\n    teach_forcing = True if random.random() > teach_forcing_prob else False\n    if teach_forcing:\n        # \xe6\x95\x99\xe5\xb8\x88\xe5\xbc\xba\xe5\x88\xb6\xef\xbc\x9a\xe5\xb0\x86\xe7\x9b\xae\xe6\xa0\x87label\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x85\xa5\n        for di in range(1, target_variable.shape[0]):           # \xe6\x9c\x80\xe5\xa4\xa7\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            loss += criterion(decoder_output, target_variable[di])          # \xe6\xaf\x8f\xe6\xac\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x97\xe7\xac\xa6\n            decoder_input = target_variable[di]  # Teacher forcing/\xe5\x89\x8d\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n    else:\n        for di in range(1, target_variable.shape[0]):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            loss += criterion(decoder_output, target_variable[di])  # \xe6\xaf\x8f\xe6\xac\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x97\xe7\xac\xa6\n            topv, topi = decoder_output.data.topk(1)\n            ni = topi.squeeze()\n            decoder_input = ni\n    encoder.zero_grad()\n    decoder.zero_grad()\n    loss.backward()\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n    return loss\n\n\nif __name__ == \'__main__\':\n    t0 = time.time()\n    for epoch in range(opt.niter):\n        train_iter = iter(train_loader)\n        i = 0\n        while i < len(train_loader)-1:\n            for e, d in zip(encoder.parameters(), decoder.parameters()):\n                e.requires_grad = True\n                d.requires_grad = True\n            encoder.train()\n            decoder.train()\n            cost = trainBatch(encoder, decoder, criterion, encoder_optimizer, \n                              decoder_optimizer, teach_forcing_prob=opt.teaching_forcing_prob)\n            loss_avg.add(cost)\n            i += 1\n\n            if i % opt.displayInterval == 0:\n                print(\'[%d/%d][%d/%d] Loss: %f\' %\n                    (epoch, opt.niter, i, len(train_loader), loss_avg.val()), end=\' \')\n                loss_avg.reset()\n                t1 = time.time()\n                print(\'time elapsed %d\' % (t1-t0))\n                t0 = time.time()\n\n        # do checkpointing\n        if epoch % opt.saveInterval == 0:\n            val(encoder, decoder, criterion, 1, dataset=test_dataset, teach_forcing=False)            # batchsize:1\n            torch.save(\n                encoder.state_dict(), \'{0}/encoder_{1}.pth\'.format(opt.experiment, epoch))\n            torch.save(\n                decoder.state_dict(), \'{0}/decoder_{1}.pth\'.format(opt.experiment, epoch))'"
models/__init__.py,0,b''
models/crnn.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nclass BidirectionalLSTM(nn.Module):\n\n    def __init__(self, nIn, nHidden, nOut):\n        super(BidirectionalLSTM, self).__init__()\n\n        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n        self.embedding = nn.Linear(nHidden * 2, nOut)\n\n    def forward(self, input):\n        recurrent, _ = self.rnn(input)\n        T, b, h = recurrent.size()\n        t_rec = recurrent.view(T * b, h)\n\n        output = self.embedding(t_rec)  # [T * b, nOut]\n        output = output.view(T, b, -1)\n\n        return output\n\nclass AttentionCell(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(AttentionCell, self).__init__()\n        self.i2h = nn.Linear(input_size, hidden_size,bias=False)\n        self.h2h = nn.Linear(hidden_size, hidden_size)\n        self.score = nn.Linear(hidden_size, 1, bias=False)\n        self.rnn = nn.GRUCell(input_size, hidden_size)\n        self.hidden_size = hidden_size\n        self.input_size = input_size\n        self.processed_batches = 0\n\n    def forward(self, prev_hidden, feats):\n        self.processed_batches = self.processed_batches + 1\n        nT = feats.size(0)\n        nB = feats.size(1)\n        nC = feats.size(2)\n        hidden_size = self.hidden_size\n        input_size = self.input_size\n\n        feats_proj = self.i2h(feats.view(-1,nC))\n        prev_hidden_proj = self.h2h(prev_hidden).view(1,nB, hidden_size).expand(nT, nB, hidden_size).contiguous().view(-1, hidden_size)\n        emition = self.score(torch.tanh(feats_proj + prev_hidden_proj).view(-1, hidden_size)).view(nT,nB).transpose(0,1)\n        alpha = F.softmax(emition, dim=1) # nB * nT\n\n        if self.processed_batches % 10000 == 0:\n            print(\'emition \', list(emition.data[0]))\n            print(\'alpha \', list(alpha.data[0]))\n\n        context = (feats * alpha.transpose(0,1).contiguous().view(nT,nB,1).expand(nT, nB, nC)).sum(0).squeeze(0)\n        cur_hidden = self.rnn(context, prev_hidden)\n        return cur_hidden, alpha\n\nclass Attention(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Attention, self).__init__()\n        self.attention_cell = AttentionCell(input_size, hidden_size)\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.generator = nn.Linear(hidden_size, num_classes)\n        self.processed_batches = 0\n\n    def forward(self, feats, text_length):\n        self.processed_batches = self.processed_batches + 1\n        nT = feats.size(0)\n        nB = feats.size(1)\n        nC = feats.size(2)\n        hidden_size = self.hidden_size\n        input_size = self.input_size\n        assert(input_size == nC)\n        assert(nB == text_length.numel())\n\n        num_steps = text_length.data.max()\n        num_labels = text_length.data.sum()\n\n        output_hiddens = Variable(torch.zeros(num_steps, nB, hidden_size).type_as(feats.data))\n        hidden = Variable(torch.zeros(nB,hidden_size).type_as(feats.data))\n        max_locs = torch.zeros(num_steps, nB)\n        max_vals = torch.zeros(num_steps, nB)\n        for i in range(num_steps):\n            hidden, alpha = self.attention_cell(hidden, feats)\n            output_hiddens[i] = hidden\n            if self.processed_batches % 500 == 0:\n                max_val, max_loc = alpha.data.max(1)\n                max_locs[i] = max_loc.cpu()\n                max_vals[i] = max_val.cpu()\n        if self.processed_batches % 500 == 0:\n            print(\'max_locs\', list(max_locs[0:text_length.data[0],0]))\n            print(\'max_vals\', list(max_vals[0:text_length.data[0],0]))\n        new_hiddens = Variable(torch.zeros(num_labels, hidden_size).type_as(feats.data))\n        b = 0\n        start = 0\n        for length in text_length.data:\n            new_hiddens[start:start+length] = output_hiddens[0:length,b,:]\n            start = start + length\n            b = b + 1\n        probs = self.generator(new_hiddens)\n        return probs\n\nclass CRNN(nn.Module):\n\n    def __init__(self, imgH, nc, nclass, nh, n_rnn=2, leakyRelu=False):\n        super(CRNN, self).__init__()\n        assert imgH % 16 == 0, \'imgH has to be a multiple of 16\'\n\n        self.cnn = nn.Sequential(\n                      nn.Conv2d(nc, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2), # 64x16x50\n                      nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2), # 128x8x25\n                      nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True), # 256x8x25\n                      nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2,2), (2,1), (0,1)), # 256x4x25\n                      nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True), # 512x4x25\n                      nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2,2), (2,1), (0,1)), # 512x2x25\n                      nn.Conv2d(512, 512, 2, 1, 0), nn.BatchNorm2d(512), nn.ReLU(True)) # 512x1x25\n        #self.cnn = cnn\n        self.rnn = nn.Sequential(\n            BidirectionalLSTM(512, nh, nh),\n            BidirectionalLSTM(nh, nh, nh))\n        self.attention = Attention(nh, nh, nclass)\n\n    def forward(self, input, length):\n        # conv features\n        conv = self.cnn(input)\n        b, c, h, w = conv.size()\n        assert h == 1, ""the height of conv must be 1""\n        conv = conv.squeeze(2)\n        conv = conv.permute(2, 0, 1)  # [w, b, c]\n\n        # rnn features\n        rnn = self.rnn(conv)\n        output = self.attention(rnn, length)\n\n        return output\n'"
models/crnn_lang.py,19,"b'# coding:utf-8\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\n\nGO = 0\nEOS_TOKEN = 1              # \xe7\xbb\x93\xe6\x9d\x9f\xe6\xa0\x87\xe5\xbf\x97\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\n\nclass BidirectionalLSTM(nn.Module):\n\n    def __init__(self, nIn, nHidden, nOut):\n        super(BidirectionalLSTM, self).__init__()\n\n        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n        self.embedding = nn.Linear(nHidden * 2, nOut)\n\n    def forward(self, input):\n        recurrent, _ = self.rnn(input)\n        T, b, h = recurrent.size()\n        t_rec = recurrent.view(T * b, h)\n\n        output = self.embedding(t_rec)  # [T * b, nOut]\n        output = output.view(T, b, -1)\n\n        return output\n   \n\nclass AttentionCell(nn.Module):\n    def __init__(self, input_size, hidden_size, num_embeddings=128):\n        super(AttentionCell, self).__init__()\n        self.i2h = nn.Linear(input_size, hidden_size,bias=False)\n        self.h2h = nn.Linear(hidden_size, hidden_size)\n        self.score = nn.Linear(hidden_size, 1, bias=False)\n        self.rnn = nn.GRUCell(input_size+num_embeddings, hidden_size)\n        self.hidden_size = hidden_size\n        self.input_size = input_size\n        self.num_embeddings = num_embeddings\n        self.processed_batches = 0\n\n    def forward(self, prev_hidden, feats, cur_embeddings):\n        nT = feats.size(0)\n        nB = feats.size(1)\n        nC = feats.size(2)\n        hidden_size = self.hidden_size\n        input_size = self.input_size\n\n        feats_proj = self.i2h(feats.view(-1,nC))\n        prev_hidden_proj = self.h2h(prev_hidden).view(1,nB, hidden_size).expand(nT, nB, hidden_size).contiguous().view(-1, hidden_size)\n        emition = self.score(torch.tanh(feats_proj + prev_hidden_proj).view(-1, hidden_size)).view(nT,nB).transpose(0,1)\n        self.processed_batches = self.processed_batches + 1\n\n        if self.processed_batches % 10000 == 0:\n            print(\'processed_batches = %d\' %(self.processed_batches))\n\n        alpha = F.softmax(emition) # nB * nT\n        if self.processed_batches % 10000 == 0:\n            print(\'emition \', list(emition.data[0]))\n            print(\'alpha \', list(alpha.data[0]))\n        context = (feats * alpha.transpose(0,1).contiguous().view(nT,nB,1).expand(nT, nB, nC)).sum(0).squeeze(0) # nB * nC//\xe6\x84\x9f\xe8\xa7\x89\xe4\xb8\x8d\xe5\xba\x94\xe8\xaf\xa5sum\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba4\xc3\x97256\n        context = torch.cat([context, cur_embeddings], 1)\n        cur_hidden = self.rnn(context, prev_hidden)\n        return cur_hidden, alpha\n\n\nclass DecoderRNN(nn.Module):\n    """"""\n        \xe9\x87\x87\xe7\x94\xa8RNN\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa7\xa3\xe7\xa0\x81\n    """"""\n    def __init__(self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        output = self.embedding(input).view(1, 1, -1)\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n        output = self.softmax(self.out(output[0]))\n        return output, hidden\n\n    def initHidden(self):\n        result = Variable(torch.zeros(1, 1, self.hidden_size))\n\n        return result\n\n\nclass Attentiondecoder(nn.Module):\n    """"""\n        \xe9\x87\x87\xe7\x94\xa8attention\xe6\xb3\xa8\xe6\x84\x8f\xe5\x8a\x9b\xe6\x9c\xba\xe5\x88\xb6\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa7\xa3\xe7\xa0\x81\n    """"""\n    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=71):\n        super(Attentiondecoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n\n    def forward(self, input, hidden, encoder_outputs):\n        # calculate the attention weight and weight * encoder_output feature\n        embedded = self.embedding(input)         # \xe5\x89\x8d\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xaf\x8d\xe5\xb5\x8c\xe5\x85\xa5\n        embedded = self.dropout(embedded)\n\n        attn_weights = F.softmax(\n            self.attn(torch.cat((embedded, hidden[0]), 1)), dim=1)        # \xe4\xb8\x8a\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x92\x8c\xe9\x9a\x90\xe8\x97\x8f\xe7\x8a\xb6\xe6\x80\x81\xe6\xb1\x82\xe5\x87\xba\xe6\x9d\x83\xe9\x87\x8d, \xe4\xb8\xbb\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaalinear layer\xe4\xbb\x8e512\xe7\xbb\xb4\xe5\x88\xb071\xe7\xbb\xb4\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\xaa\xe8\x83\xbd\xe5\xa4\x84\xe7\x90\x86\xe5\x9b\xba\xe5\xae\x9a\xe5\xae\xbd\xe5\xba\xa6\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\n        attn_applied = torch.matmul(attn_weights.unsqueeze(1),\n                                 encoder_outputs.permute((1, 0, 2)))      # \xe7\x9f\xa9\xe9\x98\xb5\xe4\xb9\x98\xe6\xb3\x95\xef\xbc\x8cbmm\xef\xbc\x888\xc3\x971\xc3\x9756\xef\xbc\x8c8\xc3\x9756\xc3\x97256\xef\xbc\x89=8\xc3\x971\xc3\x97256\n\n        output = torch.cat((embedded, attn_applied.squeeze(1) ), 1)       # \xe4\xb8\x8a\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x92\x8cattention feature\xe5\x81\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe8\x9e\x8d\xe5\x90\x88\xef\xbc\x8c\xe5\x86\x8d\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaalinear layer\n        output = self.attn_combine(output).unsqueeze(0)\n\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)                         # just as sequence to sequence decoder\n\n        output = F.log_softmax(self.out(output[0]), dim=1)          # use log_softmax for nllloss\n        return output, hidden, attn_weights\n\n    def initHidden(self, batch_size):\n        result = Variable(torch.zeros(1, batch_size, self.hidden_size))\n\n        return result\n\n\ndef target_txt_decode(batch_size, text_length, text):\n    \'\'\'\n        \xe5\xaf\xb9target txt\xe6\xaf\x8f\xe4\xb8\xaa\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe7\x9a\x84\xe5\xbc\x80\xe5\xa7\x8b\xe5\x8a\xa0\xe4\xb8\x8aGO\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe5\x8a\xa0\xe4\xb8\x8aEOS\xef\xbc\x8c\xe5\xb9\xb6\xe7\x94\xa8\xe6\x9c\x80\xe9\x95\xbf\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe5\x81\x9a\xe5\xaf\xb9\xe9\xbd\x90\n    return:\n        targets: num_steps+1 * batch_size\n    \'\'\'\n    nB = batch_size      # batch\n\n    # \xe5\xb0\x86text\xe5\x88\x86\xe7\xa6\xbb\xe5\x87\xba\xe6\x9d\xa5\n    num_steps = text_length.data.max()\n    num_steps = int(num_steps.cpu().numpy())\n    targets = torch.ones(nB, num_steps + 2) * 2                 # \xe7\x94\xa8$\xe7\xac\xa6\xe5\x8f\xb7\xe5\xa1\xab\xe5\x85\x85\xe8\xbe\x83\xe7\x9f\xad\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2, \xe5\x9c\xa8\xe6\x9c\x80\xe5\xbc\x80\xe5\xa7\x8b\xe5\x8a\xa0\xe4\xb8\x8aGO,\xe7\xbb\x93\xe6\x9d\x9f\xe5\x8a\xa0\xe4\xb8\x8aEOS_TOKEN\n    targets = targets.long().cuda()        # \xe7\x94\xa8\n    start_id = 0\n    for i in range(nB):\n        targets[i][0] = GO    # \xe5\x9c\xa8\xe5\xbc\x80\xe5\xa7\x8b\xe7\x9a\x84\xe5\x8a\xa0\xe4\xb8\x8a\xe5\xbc\x80\xe5\xa7\x8b\xe6\xa0\x87\xe7\xad\xbe\n        targets[i][1:text_length.data[i] + 1] = text.data[start_id:start_id+text_length.data[i]]       # \xe6\x98\xaf\xe5\x90\xa6\xe8\xa6\x81\xe5\x8a\xa01\n        targets[i][text_length.data[i] + 1] = EOS_TOKEN         # \xe5\x8a\xa0\xe4\xb8\x8a\xe7\xbb\x93\xe6\x9d\x9f\xe6\xa0\x87\xe7\xad\xbe\n        start_id = start_id+text_length.data[i]                 # \xe6\x8b\x86\xe5\x88\x86\xe6\xaf\x8f\xe4\xb8\xaa\xe7\x9b\xae\xe6\xa0\x87\xe7\x9a\x84target label\xef\xbc\x8c\xe4\xb8\xba\xef\xbc\x9abatch\xc3\x97\xe6\x9c\x80\xe9\x95\xbf\xe5\xad\x97\xe7\xac\xa6\xe7\x9a\x84numel\n    targets = Variable(targets.transpose(0, 1).contiguous())\n    return targets\n    \n\nclass CNN(nn.Module):\n    \'\'\'\n        CNN+BiLstm\xe5\x81\x9a\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\n    \'\'\'\n    def __init__(self, imgH, nc, nh):\n        super(CNN, self).__init__()\n        assert imgH % 16 == 0, \'imgH has to be a multiple of 16\'\n\n        self.cnn = nn.Sequential(\n                      nn.Conv2d(nc, 64, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2), # 64x16x50\n                      nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d(2, 2), # 128x8x25\n                      nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(True), # 256x8x25\n                      nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2,2), (2,1), (0,1)), # 256x4x25\n                      nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(True), # 512x4x25\n                      nn.Conv2d(512, 512, 3, 1, 1), nn.ReLU(True), nn.MaxPool2d((2,2), (2,1), (0,1)), # 512x2x25\n                      nn.Conv2d(512, 512, 2, 1, 0), nn.BatchNorm2d(512), nn.ReLU(True)) # 512x1x25\n        self.rnn = nn.Sequential(\n            BidirectionalLSTM(512, nh, nh),\n            BidirectionalLSTM(nh, nh, nh))\n\n    def forward(self, input):\n        # conv features\n        conv = self.cnn(input)\n        b, c, h, w = conv.size()\n        assert h == 1, ""the height of conv must be 1""\n        conv = conv.squeeze(2)\n        conv = conv.permute(2, 0, 1)  # [w, b, c]\n\n        # rnn features calculate\n        encoder_outputs = self.rnn(conv)          # seq * batch * n_classes// 25 \xc3\x97 batchsize \xc3\x97 256\xef\xbc\x88\xe9\x9a\x90\xe8\x97\x8f\xe8\x8a\x82\xe7\x82\xb9\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x89\n        \n        return encoder_outputs\n\n\nclass decoder(nn.Module):\n    \'\'\'\n        decoder from image features\n    \'\'\'\n    def __init__(self, nh=256, nclass=13, dropout_p=0.1, max_length=71):\n        super(decoder, self).__init__()\n        self.hidden_size = nh\n        self.decoder = Attentiondecoder(nh, nclass, dropout_p, max_length)\n\n    def forward(self, input, hidden, encoder_outputs):\n        return self.decoder(input, hidden, encoder_outputs)\n\n    def initHidden(self, batch_size):\n        result = Variable(torch.zeros(1, batch_size, self.hidden_size))\n        return result\n\n\nclass AttentiondecoderV2(nn.Module):\n    """"""\n        \xe9\x87\x87\xe7\x94\xa8seq to seq\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe4\xbf\xae\xe6\x94\xb9\xe6\xb3\xa8\xe6\x84\x8f\xe5\x8a\x9b\xe6\x9d\x83\xe9\x87\x8d\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe6\x96\xb9\xe5\xbc\x8f\n    """"""\n    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n        super(AttentiondecoderV2, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n\n        # test\n        self.vat = nn.Linear(hidden_size, 1)\n\n    def forward(self, input, hidden, encoder_outputs):\n        embedded = self.embedding(input)         # \xe5\x89\x8d\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xaf\x8d\xe5\xb5\x8c\xe5\x85\xa5\n        embedded = self.dropout(embedded)\n\n        # test\n        batch_size = encoder_outputs.shape[1]\n        alpha = hidden + encoder_outputs         # \xe7\x89\xb9\xe5\xbe\x81\xe8\x9e\x8d\xe5\x90\x88\xe9\x87\x87\xe7\x94\xa8+/concat\xe5\x85\xb6\xe5\xae\x9e\xe9\x83\xbd\xe5\x8f\xaf\xe4\xbb\xa5\n        alpha = alpha.view(-1, alpha.shape[-1])\n        attn_weights = self.vat( torch.tanh(alpha))                       # \xe5\xb0\x86encoder_output:batch*seq*features,\xe5\xb0\x86features\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe9\x99\x8d\xe4\xb8\xba1\n        attn_weights = attn_weights.view(-1, 1, batch_size).permute((2,1,0))\n        attn_weights = F.softmax(attn_weights, dim=2)\n\n        # attn_weights = F.softmax(\n        #     self.attn(torch.cat((embedded, hidden[0]), 1)), dim=1)        # \xe4\xb8\x8a\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x92\x8c\xe9\x9a\x90\xe8\x97\x8f\xe7\x8a\xb6\xe6\x80\x81\xe6\xb1\x82\xe5\x87\xba\xe6\x9d\x83\xe9\x87\x8d\n\n        attn_applied = torch.matmul(attn_weights,\n                                 encoder_outputs.permute((1, 0, 2)))      # \xe7\x9f\xa9\xe9\x98\xb5\xe4\xb9\x98\xe6\xb3\x95\xef\xbc\x8cbmm\xef\xbc\x888\xc3\x971\xc3\x9756\xef\xbc\x8c8\xc3\x9756\xc3\x97256\xef\xbc\x89=8\xc3\x971\xc3\x97256\n        output = torch.cat((embedded, attn_applied.squeeze(1) ), 1)       # \xe4\xb8\x8a\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x92\x8cattention feature\xef\xbc\x8c\xe5\x81\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe7\xba\xbf\xe6\x80\xa7+GRU\n        output = self.attn_combine(output).unsqueeze(0)\n\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n\n        output = F.log_softmax(self.out(output[0]), dim=1)          # \xe6\x9c\x80\xe5\x90\x8e\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa6\x82\xe7\x8e\x87\n        return output, hidden, attn_weights\n\n    def initHidden(self, batch_size):\n        result = Variable(torch.zeros(1, batch_size, self.hidden_size))\n\n        return result\n\n\nclass decoderV2(nn.Module):\n    \'\'\'\n        decoder from image features\n    \'\'\'\n\n    def __init__(self, nh=256, nclass=13, dropout_p=0.1):\n        super(decoderV2, self).__init__()\n        self.hidden_size = nh\n        self.decoder = AttentiondecoderV2(nh, nclass, dropout_p)\n\n    def forward(self, input, hidden, encoder_outputs):\n        return self.decoder(input, hidden, encoder_outputs)\n\n    def initHidden(self, batch_size):\n        result = Variable(torch.zeros(1, batch_size, self.hidden_size))\n        return result\n'"
src/__init__.py,0,b''
src/class_attention.py,6,"b""# coding:utf-8\nimport utils\nimport torch\nimport cv2\nimport numpy as np \nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport models.crnn_lang as crnn\n\nclass attention_ocr():\n    '''\xe4\xbd\xbf\xe7\x94\xa8attention_ocr\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xad\x97\xe7\xac\xa6\xe8\xaf\x86\xe5\x88\xab\n    \xe8\xbf\x94\xe5\x9b\x9e\xef\xbc\x9a\n        ocr\xe8\xaf\xbb\xe6\x95\xb0\xef\xbc\x8c\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\n    '''\n    def __init__(self):\n        encoder_path = './expr/attentioncnn/encoder_600.pth'\n        decoder_path = './expr/attentioncnn/decoder_600.pth'\n        self.alphabet = '0123456789'\n        self.max_length = 7                          # \xe6\x9c\x80\xe9\x95\xbf\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n        self.EOS_TOKEN = 1\n        self.use_gpu = True\n        self.max_width = 220\n        self.converter = utils.strLabelConverterForAttention(self.alphabet)\n        self.transform = transforms.ToTensor()\n\n        nclass = len(self.alphabet) + 3\n        encoder = crnn.CNN(32, 1, 256)          # \xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8\n        decoder = crnn.decoder(256, nclass)  # seq to seq\xe7\x9a\x84\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8, nclass\xe5\x9c\xa8decoder\xe4\xb8\xad\xe8\xbf\x98\xe5\x8a\xa0\xe4\xba\x862\n\n        if encoder_path and decoder_path:\n            print('loading pretrained models ......')\n            encoder.load_state_dict(torch.load(encoder_path))\n            decoder.load_state_dict(torch.load(decoder_path))\n        if torch.cuda.is_available() and self.use_gpu:\n            encoder = encoder.cuda()\n            decoder = decoder.cuda()\n        self.encoder = encoder.eval()\n        self.decoder = decoder.eval()\n\n    def constant_pad(self, img_crop):\n        '''\xe6\x8a\x8a\xe5\x9b\xbe\xe7\x89\x87\xe7\xad\x89\xe6\xaf\x94\xe4\xbe\x8b\xe7\xbc\xa9\xe6\x94\xbe\xe5\x88\xb0\xe9\xab\x98\xe5\xba\xa6\xef\xbc\x9a32,\xe5\x86\x8d\xe6\x88\x96resize\xe5\xa1\xab\xe5\x85\x85\xe5\x88\xb0220\xe5\xae\xbd\xe5\xba\xa6\n        img_crop\xef\xbc\x9a\n            cv2\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8crgb\xe9\xa1\xba\xe5\xba\x8f\n        \xe8\xbf\x94\xe5\x9b\x9e\xef\xbc\x9a\n            img tensor\n        '''\n        h, w, c = img_crop.shape\n        ratio = h / 32\n        new_w = int(w / ratio)\n        new_img = cv2.resize(img_crop,(new_w, 32))\n        container = np.ones((32, self.max_width, 3), dtype=np.uint8) * new_img[-3,-3,:]\n        if new_w <= self.max_width:\n            container[:,:new_w,:] = new_img\n        elif new_w > self.max_width:\n            container = cv2.resize(new_img, (self.max_width, 32))\n\n        img = Image.fromarray(container.astype('uint8')).convert('L')\n        img = self.transform(img)\n        img.sub_(0.5).div_(0.5)\n        if self.use_gpu:\n            img = img.cuda()\n        return img.unsqueeze(0)\n    \n    def predict(self, img_crop):\n        '''attention ocr \xe5\x81\x9a\xe6\x96\x87\xe5\xad\x97\xe8\xaf\x86\xe5\x88\xab\n        img_crop:\n            cv2\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8crgb\xe9\xa1\xba\xe5\xba\x8f\n        \xe8\xbf\x94\xe5\x9b\x9e\xef\xbc\x9a\n            ocr\xe8\xaf\xbb\xe6\x95\xb0\xef\xbc\x8cprob\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\n        '''\n        img_tensor = self.constant_pad(img_crop)\n        encoder_out = self.encoder(img_tensor)\n\n        decoded_words = []\n        prob = 1.0\n        decoder_input = torch.zeros(1).long()      # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96decoder\xe7\x9a\x84\xe5\xbc\x80\xe5\xa7\x8b,\xe4\xbb\x8e0\xe5\xbc\x80\xe5\xa7\x8b\xe8\xbe\x93\xe5\x87\xba\n        decoder_hidden = self.decoder.initHidden(1)\n        if torch.cuda.is_available() and self.use_gpu:\n            decoder_input = decoder_input.cuda()\n            decoder_hidden = decoder_hidden.cuda()\n        # \xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe9\x87\x87\xe7\x94\xa8\xe9\x9d\x9e\xe5\xbc\xba\xe5\x88\xb6\xe7\xad\x96\xe7\x95\xa5\xef\xbc\x8c\xe5\xb0\x86\xe5\x89\x8d\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x8b\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe7\x9b\xb4\xe5\x88\xb0\xe6\xa0\x87\xe7\xad\xbe\xe4\xb8\xbaEOS_TOKEN\xe6\x97\xb6\xe5\x81\x9c\xe6\xad\xa2\n        for di in range(self.max_length):  # \xe6\x9c\x80\xe5\xa4\xa7\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n            decoder_output, decoder_hidden, decoder_attention = self.decoder(\n                decoder_input, decoder_hidden, encoder_out)\n            probs = torch.exp(decoder_output)\n            # decoder_attentions[di] = decoder_attention.data\n            topv, topi = decoder_output.data.topk(1)\n            ni = topi.squeeze(1)\n            decoder_input = ni\n            prob *= probs[:, ni]\n            if ni == self.EOS_TOKEN:\n                # decoded_words.append('<EOS>')\n                break\n            else:\n                decoded_words.append(self.converter.decode(ni))\n\n        words = ''.join(decoded_words)\n        prob = prob.item()\n\n        return words, prob\n\nif __name__ == '__main__':\n    path = './test_img/00027_299021_27.jpg'\n    img = cv2.imread(path)\n    attention = attention_ocr()\n    res = attention.predict(img)\n    print(res)"""
src/dataset.py,6,"b""#!/usr/bin/python\n# encoding: utf-8\n\nimport random\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import sampler\nimport torchvision.transforms as transforms\n# import lmdb\nimport six\nimport sys\nfrom PIL import Image\nimport numpy as np\n\n\nclass listDataset(Dataset):\n    def __init__(self, list_file=None, transform=None, target_transform=None):\n        self.list_file = list_file\n        with open(list_file) as fp:\n            self.lines = fp.readlines()\n            self.nSamples = len(self.lines)\n\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return self.nSamples\n\n    def __getitem__(self, index):\n        assert index <= len(self), 'index range error'\n\n        line_splits = self.lines[index].strip().split(' ')\n        imgpath = line_splits[0]\n        try:\n            if 'train' in self.list_file:\n                img = Image.open(imgpath).convert('L')\n            else:\n                img = Image.open(imgpath).convert('L')\n        except IOError:\n            print('Corrupted image for %d' % index)\n            return self[index + 1]\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        label = line_splits[1].decode('utf-8')\n\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n\n        return (img, label)\n\nclass resizeNormalize(object):\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n        self.toTensor = transforms.ToTensor()\n\n    def __call__(self, img):\n        img = img.resize(self.size, self.interpolation)\n        img = self.toTensor(img)\n        img.sub_(0.5).div_(0.5)\n        return img\n\n\nclass randomSequentialSampler(sampler.Sampler):\n\n    def __init__(self, data_source, batch_size):\n        self.num_samples = len(data_source)\n        self.batch_size = batch_size\n\n    def __iter__(self):\n        n_batch = len(self) // self.batch_size\n        tail = len(self) % self.batch_size\n        index = torch.LongTensor(len(self)).fill_(0)\n        for i in range(n_batch):\n            random_start = random.randint(0, len(self) - self.batch_size)\n            batch_index = random_start + torch.arange(0, self.batch_size)\n            index[i * self.batch_size:(i + 1) * self.batch_size] = batch_index\n        # deal with tail\n        if tail:\n            random_start = random.randint(0, len(self) - self.batch_size)\n            tail_index = random_start + torch.arange(0, tail)\n            index[(i + 1) * self.batch_size:] = tail_index\n\n        return iter(index)\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass alignCollate(object):\n\n    def __init__(self, imgH=32, imgW=100, keep_ratio=False, min_ratio=1):\n        self.imgH = imgH\n        self.imgW = imgW\n        self.keep_ratio = keep_ratio\n        self.min_ratio = min_ratio\n\n    def __call__(self, batch):\n        images, labels = zip(*batch)\n\n        imgH = self.imgH\n        imgW = self.imgW\n        if self.keep_ratio:\n            ratios = []\n            for image in images:\n                w, h = image.size\n                ratios.append(w / float(h))\n            ratios.sort()\n            max_ratio = ratios[-1]\n            imgW = int(np.floor(max_ratio * imgH))\n            imgW = max(imgH * self.min_ratio, imgW)  # assure imgH >= imgW\n\n        transform = resizeNormalize((imgW, imgH))\n        images = [transform(image) for image in images]\n        images = torch.cat([t.unsqueeze(0) for t in images], 0)\n\n        return images, labels\n"""
src/utils.py,16,"b'#!/usr/bin/python\n# encoding: utf-8\n#  -*- encoding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport collections\nfrom PIL import Image, ImageFilter\nimport math\nimport random\nimport numpy as np\nimport cv2\n\nwith open(\'./data/char_std_5990.txt\') as f:\n    data = f.readlines()\n    alphabet = [x.rstrip() for x in data]\n    alphabet = \'\'.join(alphabet).decode(\'utf-8\')        # python2\xe4\xb8\x8d\xe5\x8a\xa0decode\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xbc\x9a\xe4\xb9\xb1\xe7\xa0\x81\n\n\nclass strLabelConverterForAttention(object):\n    """"""Convert between str and label.\n\n    NOTE:\n        Insert `EOS` to the alphabet for attention.\n\n    Args:\n        alphabet (str): set of the possible characters.\n        ignore_case (bool, default=True): whether or not to ignore all of the case.\n    """"""\n\n    def __init__(self, alphabet):\n        self.alphabet = alphabet\n\n        self.dict = {}\n        self.dict[\'SOS\'] = 0       # \xe5\xbc\x80\xe5\xa7\x8b\n        self.dict[\'EOS\'] = 1       # \xe7\xbb\x93\xe6\x9d\x9f\n        self.dict[\'$\'] = 2         # blank\xe6\xa0\x87\xe8\xaf\x86\xe7\xac\xa6\n        for i, item in enumerate(self.alphabet):\n            # NOTE: 0 is reserved for \'blank\' required by wrap_ctc\n            self.dict[item] = i + 3                     # \xe4\xbb\x8e3\xe5\xbc\x80\xe5\xa7\x8b\xe7\xbc\x96\xe7\xa0\x81\n\n    def encode(self, text):\n        """"""\xe5\xaf\xb9target_label\xe5\x81\x9a\xe7\xbc\x96\xe7\xa0\x81\xe5\x92\x8c\xe5\xaf\xb9\xe9\xbd\x90\n        \xe5\xaf\xb9target txt\xe6\xaf\x8f\xe4\xb8\xaa\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe7\x9a\x84\xe5\xbc\x80\xe5\xa7\x8b\xe5\x8a\xa0\xe4\xb8\x8aGO\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe5\x8a\xa0\xe4\xb8\x8aEOS\xef\xbc\x8c\xe5\xb9\xb6\xe7\x94\xa8\xe6\x9c\x80\xe9\x95\xbf\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe5\x81\x9a\xe5\xaf\xb9\xe9\xbd\x90\n        Args:\n            text (str or list of str): texts to convert.\n\n        Returns:\n            torch.IntTensor targets:max_length \xc3\x97 batch_size\n        """"""\n        if isinstance(text, unicode):\n            text = [self.dict[item] for item in text]\n        elif isinstance(text, collections.Iterable):\n            text = [self.encode(s) for s in text]           # \xe7\xbc\x96\xe7\xa0\x81\n\n            max_length = max([len(x) for x in text])        # \xe5\xaf\xb9\xe9\xbd\x90\n            nb = len(text)\n            targets = torch.ones(nb, max_length + 2) * 2              # use \xe2\x80\x98blank\xe2\x80\x99 for pading\n            for i in range(nb):\n                targets[i][0] = 0                           # \xe5\xbc\x80\xe5\xa7\x8b\n                targets[i][1:len(text[i]) + 1] = text[i]\n                targets[i][len(text[i]) + 1] = 1\n            text = targets.transpose(0, 1).contiguous()\n            text = text.long()\n        return torch.LongTensor(text)\n\n    def decode(self, t):\n        """"""Decode encoded texts back into strs.\n\n        Args:\n            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n            torch.IntTensor [n]: length of each text.\n\n        Raises:\n            AssertionError: when the texts and its length does not match.\n\n        Returns:\n            text (str or list of str): texts to convert.\n        """"""\n\n        texts = list(self.dict.keys())[list(self.dict.values()).index(t)]\n        return texts\n\nclass strLabelConverterForCTC(object):\n    """"""Convert between str and label.\n\n    NOTE:\n        Insert `blank` to the alphabet for CTC.\n\n    Args:\n        alphabet (str): set of the possible characters.\n        ignore_case (bool, default=True): whether or not to ignore all of the case.\n    """"""\n\n    def __init__(self, alphabet, sep):\n        self.sep = sep\n        self.alphabet = alphabet.split(sep)\n        self.alphabet.append(\'-\')  # for `-1` index\n\n        self.dict = {}\n        for i, item in enumerate(self.alphabet):\n            # NOTE: 0 is reserved for \'blank\' required by wrap_ctc\n            self.dict[item] = i + 1\n\n    def encode(self, text):\n        """"""Support batch or single str.\n\n        Args:\n            text (str or list of str): texts to convert.\n\n        Returns:\n            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n            torch.IntTensor [n]: length of each text.\n        """"""\n        if isinstance(text, str):\n            text = text.split(self.sep)\n            text = [self.dict[item] for item in text]\n            length = [len(text)]\n        elif isinstance(text, collections.Iterable):\n            length = [len(s.split(self.sep)) for s in text]\n            text = self.sep.join(text)\n            text, _ = self.encode(text)\n        return (torch.IntTensor(text), torch.IntTensor(length))\n\n    def decode(self, t, length, raw=False):\n        """"""Decode encoded texts back into strs.\n\n        Args:\n            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n            torch.IntTensor [n]: length of each text.\n\n        Raises:\n            AssertionError: when the texts and its length does not match.\n\n        Returns:\n            text (str or list of str): texts to convert.\n        """"""\n        if length.numel() == 1:\n            length = length[0]\n            assert t.numel() == length, ""text with length: {} does not match declared length: {}"".format(t.numel(), length)\n            if raw:\n                return \'\'.join([self.alphabet[i - 1] for i in t])\n            else:\n                char_list = []\n                for i in range(length):\n                    if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):\n                        char_list.append(self.alphabet[t[i] - 1])\n                return \'\'.join(char_list)\n        else:\n            # batch mode\n            assert t.numel() == length.sum(), ""texts with length: {} does not match declared length: {}"".format(t.numel(), length.sum())\n            texts = []\n            index = 0\n            for i in range(length.numel()):\n                l = length[i]\n                texts.append(\n                    self.decode(\n                        t[index:index + l], torch.IntTensor([l]), raw=raw))\n                index += l\n            return texts\n\n\nclass averager(object):\n    """"""Compute average for `torch.Variable` and `torch.Tensor`. """"""\n\n    def __init__(self):\n        self.reset()\n\n    def add(self, v):\n        if isinstance(v, Variable):\n            count = v.data.numel()\n            v = v.data.sum()\n        elif isinstance(v, torch.Tensor):\n            count = v.numel()\n            v = v.sum()\n\n        self.n_count += count\n        self.sum += v\n\n    def reset(self):\n        self.n_count = 0\n        self.sum = 0\n\n    def val(self):\n        res = 0\n        if self.n_count != 0:\n            res = self.sum / float(self.n_count)\n        return res\n\n\ndef oneHot(v, v_length, nc):\n    batchSize = v_length.size(0)\n    maxLength = v_length.max()\n    v_onehot = torch.FloatTensor(batchSize, maxLength, nc).fill_(0)\n    acc = 0\n    for i in range(batchSize):\n        length = v_length[i]\n        label = v[acc:acc + length].view(-1, 1).long()\n        v_onehot[i, :length].scatter_(1, label, 1.0)\n        acc += length\n    return v_onehot\n\n\ndef loadData(v, data):\n    v.data.resize_(data.size()).copy_(data)\n\n\ndef prettyPrint(v):\n    print(\'Size {0}, Type: {1}\'.format(str(v.size()), v.data.type()))\n    print(\'| Max: %f | Min: %f | Mean: %f\' % (v.max().data[0], v.min().data[0],\n                                              v.mean().data[0]))\n\n\ndef assureRatio(img):\n    """"""Ensure imgH <= imgW.""""""\n    b, c, h, w = img.size()\n    if h > w:\n        main = nn.UpsamplingBilinear2d(size=(h, h), scale_factor=None)\n        img = main(img)\n    return img\n\n\nclass halo():\n    \'\'\'\n    u:\xe9\xab\x98\xe6\x96\xaf\xe5\x88\x86\xe5\xb8\x83\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\n    sigma:\xe6\x96\xb9\xe5\xb7\xae\n    nums:\xe5\x9c\xa8\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x87\xa0\xe4\xb8\xaa\xe5\x85\x89\xe7\x82\xb9\n    prob:\xe4\xbd\xbf\xe7\x94\xa8halo\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\n    \'\'\'\n\n    def __init__(self, nums, u=0, sigma=0.2, prob=0.5):\n        self.u = u  # \xe5\x9d\x87\xe5\x80\xbc\xce\xbc\n        self.sig = math.sqrt(sigma)  # \xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\xce\xb4\n        self.nums = nums\n        self.prob = prob\n\n    def create_kernel(self, maxh=32, maxw=50):\n        height_scope = [10, maxh]  # \xe9\xab\x98\xe5\xba\xa6\xe8\x8c\x83\xe5\x9b\xb4     \xe9\x9a\x8f\xe6\x9c\xba\xe7\x94\x9f\xe6\x88\x90\xe9\xab\x98\xe6\x96\xaf\n        weight_scope = [20, maxw]  # \xe5\xae\xbd\xe5\xba\xa6\xe8\x8c\x83\xe5\x9b\xb4\n\n        x = np.linspace(self.u - 3 * self.sig, self.u + 3 * self.sig, random.randint(*height_scope))\n        y = np.linspace(self.u - 3 * self.sig, self.u + 3 * self.sig, random.randint(*weight_scope))\n        Gauss_map = np.zeros((len(x), len(y)))\n        for i in range(len(x)):\n            for j in range(len(y)):\n                Gauss_map[i, j] = np.exp(-((x[i] - self.u) ** 2 + (y[j] - self.u) ** 2) / (2 * self.sig ** 2)) / (\n                        math.sqrt(2 * math.pi) * self.sig)\n\n        return Gauss_map\n\n    def __call__(self, img):\n        if random.random() < self.prob:\n            Gauss_map = self.create_kernel(32, 60)  # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\x80\xe4\xb8\xaa\xe9\xab\x98\xe6\x96\xaf\xe6\xa0\xb8,32\xe4\xb8\xba\xe9\xab\x98\xe5\xba\xa6\xe6\x96\xb9\xe5\x90\x91\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xef\xbc\x8c60\xe4\xb8\xbaw\xe6\x96\xb9\xe5\x90\x91\n            img1 = np.asarray(img)\n            img1.flags.writeable = True  # \xe5\xb0\x86\xe6\x95\xb0\xe7\xbb\x84\xe6\x94\xb9\xe4\xb8\xba\xe8\xaf\xbb\xe5\x86\x99\xe6\xa8\xa1\xe5\xbc\x8f\n            nums = random.randint(1, self.nums)  # \xe9\x9a\x8f\xe6\x9c\xba\xe7\x94\x9f\xe6\x88\x90nums\xe4\xb8\xaa\xe5\x85\x89\xe7\x82\xb9\n            img1 = img1.astype(np.float)\n            # print(nums)\n            for i in range(nums):\n                img_h, img_w = img1.shape\n                pointx = random.randint(0, img_h - 10)  # \xe5\x9c\xa8\xe5\x8e\x9f\xe5\x9b\xbe\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe6\x89\xbe\xe4\xb8\x80\xe4\xb8\xaa\xe7\x82\xb9\n                pointy = random.randint(0, img_w - 10)\n\n                h, w = Gauss_map.shape  # \xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe5\x90\xa6\xe8\xb6\x85\xe9\x99\x90\n                endx = pointx + h\n                endy = pointy + w\n\n                if pointx + h > img_h:\n                    endx = img_h\n                    Gauss_map = Gauss_map[1:img_h - pointx + 1, :]\n                if img_w < pointy + w:\n                    endy = img_w\n                    Gauss_map = Gauss_map[:, 1:img_w - pointy + 1]\n\n                # \xe5\x8a\xa0\xe4\xb8\x8a\xe4\xb8\x8d\xe5\x9d\x87\xe5\x8c\x80\xe5\x85\x89\xe7\x85\xa7\n                img1[pointx:endx, pointy:endy] = img1[pointx:endx, pointy:endy] + Gauss_map * 255.0\n            img1[img1 > 255.0] = 255.0  # \xe8\xbf\x9b\xe8\xa1\x8c\xe9\x99\x90\xe5\xb9\x85\xef\xbc\x8c\xe4\xb8\x8d\xe7\x84\xb6uint8\xe4\xbc\x9a\xe4\xbb\x8e0\xe5\xbc\x80\xe5\xa7\x8b\xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xa1\xe6\x95\xb0\n            img = img1\n        return Image.fromarray(np.uint8(img))\n\n\nclass MyGaussianBlur(ImageFilter.Filter):\n    name = ""GaussianBlur""\n\n    def __init__(self, radius=2, bounds=None):\n        self.radius = radius\n        self.bounds = bounds\n\n    def filter(self, image):\n        if self.bounds:\n            clips = image.crop(self.bounds).gaussian_blur(self.radius)\n            image.paste(clips, self.bounds)\n            return image\n        else:\n            return image.gaussian_blur(self.radius)\n\n\nclass GBlur(object):\n    def __init__(self, radius=2, prob=0.5):\n        radius = random.randint(0, radius)\n        self.blur = MyGaussianBlur(radius=radius)\n        self.prob = prob\n\n    def __call__(self, img):\n        if random.random() < self.prob:\n            img = img.filter(self.blur)\n        return img\n\n\nclass RandomBrightness(object):\n    """"""\xe9\x9a\x8f\xe6\x9c\xba\xe6\x94\xb9\xe5\x8f\x98\xe4\xba\xae\xe5\xba\xa6\n        pil:pil\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\n    """"""\n\n    def __init__(self, prob=1.5):\n        self.prob = prob\n\n    def __call__(self, pil):\n        rgb = np.asarray(pil)\n        if random.random() < self.prob:\n            hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n            h, s, v = cv2.split(hsv)\n            adjust = random.choice([0.5, 0.7, 0.9, 1.2, 1.5, 1.7])  # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\n            # adjust = random.choice([1.2, 1.5, 1.7, 2.0])      # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\n            v = v * adjust\n            v = np.clip(v, 0, 255).astype(hsv.dtype)\n            hsv = cv2.merge((h, s, v))\n            rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n        return Image.fromarray(np.uint8(rgb)).convert(\'L\')\n\n\nclass randapply(object):\n    """"""\xe9\x9a\x8f\xe6\x9c\xba\xe5\x86\xb3\xe5\xae\x9a\xe6\x98\xaf\xe5\x90\xa6\xe5\xba\x94\xe7\x94\xa8\xe5\x85\x89\xe6\x99\x95\xe3\x80\x81\xe6\xa8\xa1\xe7\xb3\x8a\xe6\x88\x96\xe8\x80\x85\xe4\xba\x8c\xe8\x80\x85\xe9\x83\xbd\xe7\x94\xa8\n\n    Args:\n        transforms (list or tuple): list of transformations\n    """"""\n\n    def __init__(self, transforms):\n        assert isinstance(transforms, (list, tuple))\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \'(\'\n        format_string += \'\\n    p={}\'.format(self.p)\n        for t in self.transforms:\n            format_string += \'\\n\'\n            format_string += \'    {0}\'.format(t)\n        format_string += \'\\n)\'\n        return format_string\n\n\ndef weights_init(model):\n    # Official init from torch repo.\n    for m in model.modules():\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight)\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            nn.init.constant_(m.bias, 0)'"
