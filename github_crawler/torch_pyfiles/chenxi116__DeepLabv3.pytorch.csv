file_path,api_count,code
cityscapes.py,1,"b""from __future__ import print_function\n\nimport torch.utils.data as data\nimport os\nimport random\nimport glob\nfrom PIL import Image\nfrom utils import preprocess\n\n_FOLDERS_MAP = {\n    'image': 'leftImg8bit',\n    'label': 'gtFine',\n}\n\n_POSTFIX_MAP = {\n    'image': '_leftImg8bit',\n    'label': '_gtFine_labelTrainIds',\n}\n\n_DATA_FORMAT_MAP = {\n    'image': 'png',\n    'label': 'png',\n}\n\n\nclass Cityscapes(data.Dataset):\n  CLASSES = [\n      'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n      'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car',\n      'truck', 'bus', 'train', 'motorcycle', 'bicycle'\n  ]\n\n  def __init__(self, root, train=True, transform=None, target_transform=None, download=False, crop_size=None):\n    self.root = root\n    self.transform = transform\n    self.target_transform = target_transform\n    self.train = train\n    self.crop_size = crop_size\n\n    if download:\n      self.download()\n\n    dataset_split = 'train' if self.train else 'val'\n    self.images = self._get_files('image', dataset_split)\n    self.masks = self._get_files('label', dataset_split)\n\n  def __getitem__(self, index):\n    _img = Image.open(self.images[index]).convert('RGB')\n    _target = Image.open(self.masks[index])\n\n    _img, _target = preprocess(_img, _target,\n                               flip=True if self.train else False,\n                               scale=(0.5, 2.0) if self.train else None,\n                               crop=(self.crop_size, self.crop_size) if self.train else (1025, 2049))\n\n    if self.transform is not None:\n      _img = self.transform(_img)\n\n    if self.target_transform is not None:\n      _target = self.target_transform(_target)\n\n    return _img, _target\n\n  def _get_files(self, data, dataset_split):\n    pattern = '*%s.%s' % (_POSTFIX_MAP[data], _DATA_FORMAT_MAP[data])\n    search_files = os.path.join(\n        self.root, _FOLDERS_MAP[data], dataset_split, '*', pattern)\n    filenames = glob.glob(search_files)\n    return sorted(filenames)\n\n  def __len__(self):\n    return len(self.images)\n\n  def download(self):\n    raise NotImplementedError('Automatic download not yet implemented.')\n"""
deeplab.py,8,"b'import torch\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nfrom torch.nn import functional as F\n\n\n__all__ = [\'ResNet\', \'resnet50\', \'resnet101\', \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\nclass Conv2d(nn.Conv2d):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        super(Conv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, groups, bias)\n\n    def forward(self, x):\n        # return super(Conv2d, self).forward(x)\n        weight = self.weight\n        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2,\n                                  keepdim=True).mean(dim=3, keepdim=True)\n        weight = weight - weight_mean\n        std = weight.view(weight.size(0), -1).std(dim=1).view(-1, 1, 1, 1) + 1e-5\n        weight = weight / std.expand_as(weight)\n        return F.conv2d(x, weight, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n\nclass ASPP(nn.Module):\n\n    def __init__(self, C, depth, num_classes, conv=nn.Conv2d, norm=nn.BatchNorm2d, momentum=0.0003, mult=1):\n        super(ASPP, self).__init__()\n        self._C = C\n        self._depth = depth\n        self._num_classes = num_classes\n\n        self.global_pooling = nn.AdaptiveAvgPool2d(1)\n        self.relu = nn.ReLU(inplace=True)\n        self.aspp1 = conv(C, depth, kernel_size=1, stride=1, bias=False)\n        self.aspp2 = conv(C, depth, kernel_size=3, stride=1,\n                               dilation=int(6*mult), padding=int(6*mult),\n                               bias=False)\n        self.aspp3 = conv(C, depth, kernel_size=3, stride=1,\n                               dilation=int(12*mult), padding=int(12*mult),\n                               bias=False)\n        self.aspp4 = conv(C, depth, kernel_size=3, stride=1,\n                               dilation=int(18*mult), padding=int(18*mult),\n                               bias=False)\n        self.aspp5 = conv(C, depth, kernel_size=1, stride=1, bias=False)\n        self.aspp1_bn = norm(depth, momentum)\n        self.aspp2_bn = norm(depth, momentum)\n        self.aspp3_bn = norm(depth, momentum)\n        self.aspp4_bn = norm(depth, momentum)\n        self.aspp5_bn = norm(depth, momentum)\n        self.conv2 = conv(depth * 5, depth, kernel_size=1, stride=1,\n                               bias=False)\n        self.bn2 = norm(depth, momentum)\n        self.conv3 = nn.Conv2d(depth, num_classes, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        x1 = self.aspp1(x)\n        x1 = self.aspp1_bn(x1)\n        x1 = self.relu(x1)\n        x2 = self.aspp2(x)\n        x2 = self.aspp2_bn(x2)\n        x2 = self.relu(x2)\n        x3 = self.aspp3(x)\n        x3 = self.aspp3_bn(x3)\n        x3 = self.relu(x3)\n        x4 = self.aspp4(x)\n        x4 = self.aspp4_bn(x4)\n        x4 = self.relu(x4)\n        x5 = self.global_pooling(x)\n        x5 = self.aspp5(x5)\n        x5 = self.aspp5_bn(x5)\n        x5 = self.relu(x5)\n        x5 = nn.Upsample((x.shape[2], x.shape[3]), mode=\'bilinear\',\n                         align_corners=True)(x5)\n        x = torch.cat((x1, x2, x3, x4, x5), 1)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n\n        return x\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1, conv=None, norm=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = norm(planes)\n        self.conv2 = conv(planes, planes, kernel_size=3, stride=stride,\n                               dilation=dilation, padding=dilation, bias=False)\n        self.bn2 = norm(planes)\n        self.conv3 = conv(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = norm(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes, num_groups=None, weight_std=False, beta=False):\n        self.inplanes = 64\n        self.norm = lambda planes, momentum=0.05: nn.BatchNorm2d(planes, momentum=momentum) if num_groups is None else nn.GroupNorm(num_groups, planes)\n        self.conv = Conv2d if weight_std else nn.Conv2d\n\n        super(ResNet, self).__init__()\n        if not beta:\n            self.conv1 = self.conv(3, 64, kernel_size=7, stride=2, padding=3,\n                                   bias=False)\n        else:\n            self.conv1 = nn.Sequential(\n                self.conv(3, 64, 3, stride=2, padding=1, bias=False),\n                self.conv(64, 64, 3, stride=1, padding=1, bias=False),\n                self.conv(64, 64, 3, stride=1, padding=1, bias=False))\n        self.bn1 = self.norm(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                       dilation=2)\n        self.aspp = ASPP(512 * block.expansion, 256, num_classes, conv=self.conv, norm=self.norm)\n\n        for m in self.modules():\n            if isinstance(m, self.conv):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.GroupNorm):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or dilation != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                self.conv(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, dilation=max(1, dilation/2), bias=False),\n                self.norm(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, dilation=max(1, dilation/2), conv=self.conv, norm=self.norm))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation, conv=self.conv, norm=self.norm))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        size = (x.shape[2], x.shape[3])\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.aspp(x)\n        x = nn.Upsample(size, mode=\'bilinear\', align_corners=True)(x)\n        return x\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, num_groups=None, weight_std=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_groups=num_groups, weight_std=weight_std, **kwargs)\n    if pretrained:\n        model_dict = model.state_dict()\n        if num_groups and weight_std:\n            pretrained_dict = torch.load(\'data/R-101-GN-WS.pth.tar\')\n            overlap_dict = {k[7:]: v for k, v in pretrained_dict.items() if k[7:] in model_dict}\n            assert len(overlap_dict) == 312\n        elif not num_groups and not weight_std:\n            pretrained_dict = model_zoo.load_url(model_urls[\'resnet101\'])\n            overlap_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        else:\n            raise ValueError(\'Currently only support BN or GN+WS\')\n        model_dict.update(overlap_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n'"
main.py,11,"b'import argparse\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pdb\nfrom PIL import Image\nfrom scipy.io import loadmat\nfrom torch.autograd import Variable\nfrom torchvision import transforms\n\nimport deeplab\nfrom pascal import VOCSegmentation\nfrom cityscapes import Cityscapes\nfrom utils import AverageMeter, inter_and_union\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--train\', action=\'store_true\', default=False,\n                    help=\'training mode\')\nparser.add_argument(\'--exp\', type=str, required=True,\n                    help=\'name of experiment\')\nparser.add_argument(\'--gpu\', type=int, default=0,\n                    help=\'test time gpu device id\')\nparser.add_argument(\'--backbone\', type=str, default=\'resnet101\',\n                    help=\'resnet101\')\nparser.add_argument(\'--dataset\', type=str, default=\'pascal\',\n                    help=\'pascal or cityscapes\')\nparser.add_argument(\'--groups\', type=int, default=None, \n                    help=\'num of groups for group normalization\')\nparser.add_argument(\'--epochs\', type=int, default=30,\n                    help=\'num of training epochs\')\nparser.add_argument(\'--batch_size\', type=int, default=16,\n                    help=\'batch size\')\nparser.add_argument(\'--base_lr\', type=float, default=0.00025,\n                    help=\'base learning rate\')\nparser.add_argument(\'--last_mult\', type=float, default=1.0,\n                    help=\'learning rate multiplier for last layers\')\nparser.add_argument(\'--scratch\', action=\'store_true\', default=False,\n                    help=\'train from scratch\')\nparser.add_argument(\'--freeze_bn\', action=\'store_true\', default=False,\n                    help=\'freeze batch normalization parameters\')\nparser.add_argument(\'--weight_std\', action=\'store_true\', default=False,\n                    help=\'weight standardization\')\nparser.add_argument(\'--beta\', action=\'store_true\', default=False,\n                    help=\'resnet101 beta\')\nparser.add_argument(\'--crop_size\', type=int, default=513,\n                    help=\'image crop size\')\nparser.add_argument(\'--resume\', type=str, default=None,\n                    help=\'path to checkpoint to resume from\')\nparser.add_argument(\'--workers\', type=int, default=4,\n                    help=\'number of data loading workers\')\nargs = parser.parse_args()\n\n\ndef main():\n  assert torch.cuda.is_available()\n  torch.backends.cudnn.benchmark = True\n  model_fname = \'data/deeplab_{0}_{1}_v3_{2}_epoch%d.pth\'.format(\n      args.backbone, args.dataset, args.exp)\n  if args.dataset == \'pascal\':\n    dataset = VOCSegmentation(\'data/VOCdevkit\',\n        train=args.train, crop_size=args.crop_size)\n  elif args.dataset == \'cityscapes\':\n    dataset = Cityscapes(\'data/cityscapes\',\n        train=args.train, crop_size=args.crop_size)\n  else:\n    raise ValueError(\'Unknown dataset: {}\'.format(args.dataset))\n  if args.backbone == \'resnet101\':\n    model = getattr(deeplab, \'resnet101\')(\n        pretrained=(not args.scratch),\n        num_classes=len(dataset.CLASSES),\n        num_groups=args.groups,\n        weight_std=args.weight_std,\n        beta=args.beta)\n  else:\n    raise ValueError(\'Unknown backbone: {}\'.format(args.backbone))\n\n  if args.train:\n    criterion = nn.CrossEntropyLoss(ignore_index=255)\n    model = nn.DataParallel(model).cuda()\n    model.train()\n    if args.freeze_bn:\n      for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n          m.eval()\n          m.weight.requires_grad = False\n          m.bias.requires_grad = False\n    backbone_params = (\n        list(model.module.conv1.parameters()) +\n        list(model.module.bn1.parameters()) +\n        list(model.module.layer1.parameters()) +\n        list(model.module.layer2.parameters()) +\n        list(model.module.layer3.parameters()) +\n        list(model.module.layer4.parameters()))\n    last_params = list(model.module.aspp.parameters())\n    optimizer = optim.SGD([\n      {\'params\': filter(lambda p: p.requires_grad, backbone_params)},\n      {\'params\': filter(lambda p: p.requires_grad, last_params)}],\n      lr=args.base_lr, momentum=0.9, weight_decay=0.0001)\n    dataset_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=args.batch_size, shuffle=args.train,\n        pin_memory=True, num_workers=args.workers)\n    max_iter = args.epochs * len(dataset_loader)\n    losses = AverageMeter()\n    start_epoch = 0\n\n    if args.resume:\n      if os.path.isfile(args.resume):\n        print(\'=> loading checkpoint {0}\'.format(args.resume))\n        checkpoint = torch.load(args.resume)\n        start_epoch = checkpoint[\'epoch\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        print(\'=> loaded checkpoint {0} (epoch {1})\'.format(\n          args.resume, checkpoint[\'epoch\']))\n      else:\n        print(\'=> no checkpoint found at {0}\'.format(args.resume))\n\n    for epoch in range(start_epoch, args.epochs):\n      for i, (inputs, target) in enumerate(dataset_loader):\n        cur_iter = epoch * len(dataset_loader) + i\n        lr = args.base_lr * (1 - float(cur_iter) / max_iter) ** 0.9\n        optimizer.param_groups[0][\'lr\'] = lr\n        optimizer.param_groups[1][\'lr\'] = lr * args.last_mult\n\n        inputs = Variable(inputs.cuda())\n        target = Variable(target.cuda())\n        outputs = model(inputs)\n        loss = criterion(outputs, target)\n        if np.isnan(loss.item()) or np.isinf(loss.item()):\n          pdb.set_trace()\n        losses.update(loss.item(), args.batch_size)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        print(\'epoch: {0}\\t\'\n              \'iter: {1}/{2}\\t\'\n              \'lr: {3:.6f}\\t\'\n              \'loss: {loss.val:.4f} ({loss.ema:.4f})\'.format(\n              epoch + 1, i + 1, len(dataset_loader), lr, loss=losses))\n\n      if epoch % 10 == 9:\n        torch.save({\n          \'epoch\': epoch + 1,\n          \'state_dict\': model.state_dict(),\n          \'optimizer\': optimizer.state_dict(),\n          }, model_fname % (epoch + 1))\n\n  else:\n    torch.cuda.set_device(args.gpu)\n    model = model.cuda()\n    model.eval()\n    checkpoint = torch.load(model_fname % args.epochs)\n    state_dict = {k[7:]: v for k, v in checkpoint[\'state_dict\'].items() if \'tracked\' not in k}\n    model.load_state_dict(state_dict)\n    cmap = loadmat(\'data/pascal_seg_colormap.mat\')[\'colormap\']\n    cmap = (cmap * 255).astype(np.uint8).flatten().tolist()\n\n    inter_meter = AverageMeter()\n    union_meter = AverageMeter()\n    for i in range(len(dataset)):\n      inputs, target = dataset[i]\n      inputs = Variable(inputs.cuda())\n      outputs = model(inputs.unsqueeze(0))\n      _, pred = torch.max(outputs, 1)\n      pred = pred.data.cpu().numpy().squeeze().astype(np.uint8)\n      mask = target.numpy().astype(np.uint8)\n      imname = dataset.masks[i].split(\'/\')[-1]\n      mask_pred = Image.fromarray(pred)\n      mask_pred.putpalette(cmap)\n      mask_pred.save(os.path.join(\'data/val\', imname))\n      print(\'eval: {0}/{1}\'.format(i + 1, len(dataset)))\n\n      inter, union = inter_and_union(pred, mask, len(dataset.CLASSES))\n      inter_meter.update(inter)\n      union_meter.update(union)\n\n    iou = inter_meter.sum / (union_meter.sum + 1e-10)\n    for i, val in enumerate(iou):\n      print(\'IoU {0}: {1:.2f}\'.format(dataset.CLASSES[i], val * 100))\n    print(\'Mean IoU: {0:.2f}\'.format(iou.mean() * 100))\n\n\nif __name__ == ""__main__"":\n  main()\n'"
pascal.py,1,"b""from __future__ import print_function\n\nimport torch.utils.data as data\nimport os\nfrom PIL import Image\nfrom utils import preprocess\n\n\nclass VOCSegmentation(data.Dataset):\n  CLASSES = [\n      'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n      'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n      'motorbike', 'person', 'potted-plant', 'sheep', 'sofa', 'train',\n      'tv/monitor'\n  ]\n\n  def __init__(self, root, train=True, transform=None, target_transform=None, download=False, crop_size=None):\n    self.root = root\n    _voc_root = os.path.join(self.root, 'VOC2012')\n    _list_dir = os.path.join(_voc_root, 'list')\n    self.transform = transform\n    self.target_transform = target_transform\n    self.train = train\n    self.crop_size = crop_size\n\n    if download:\n      self.download()\n\n    if self.train:\n      _list_f = os.path.join(_list_dir, 'train_aug.txt')\n    else:\n      _list_f = os.path.join(_list_dir, 'val.txt')\n    self.images = []\n    self.masks = []\n    with open(_list_f, 'r') as lines:\n      for line in lines:\n        _image = _voc_root + line.split()[0]\n        _mask = _voc_root + line.split()[1]\n        assert os.path.isfile(_image)\n        assert os.path.isfile(_mask)\n        self.images.append(_image)\n        self.masks.append(_mask)\n\n  def __getitem__(self, index):\n    _img = Image.open(self.images[index]).convert('RGB')\n    _target = Image.open(self.masks[index])\n\n    _img, _target = preprocess(_img, _target,\n                               flip=True if self.train else False,\n                               scale=(0.5, 2.0) if self.train else None,\n                               crop=(self.crop_size, self.crop_size))\n\n    if self.transform is not None:\n      _img = self.transform(_img)\n\n    if self.target_transform is not None:\n      _target = self.target_transform(_target)\n\n    return _img, _target\n\n  def __len__(self):\n    return len(self.images)\n\n  def download(self):\n    raise NotImplementedError('Automatic download not yet implemented.')\n\n"""
utils.py,3,"b'import math\nimport random\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\n\nclass AverageMeter(object):\n  def __init__(self):\n    self.val = None\n    self.sum = None\n    self.cnt = None\n    self.avg = None\n    self.ema = None\n    self.initialized = False\n\n  def update(self, val, n=1):\n    if not self.initialized:\n      self.initialize(val, n)\n    else:\n      self.add(val, n)\n\n  def initialize(self, val, n):\n    self.val = val\n    self.sum = val * n\n    self.cnt = n\n    self.avg = val\n    self.ema = val\n    self.initialized = True\n\n  def add(self, val, n):\n    self.val = val\n    self.sum += val * n\n    self.cnt += n\n    self.avg = self.sum / self.cnt\n    self.ema = self.ema * 0.99 + self.val * 0.01\n\n\ndef inter_and_union(pred, mask, num_class):\n  pred = np.asarray(pred, dtype=np.uint8).copy()\n  mask = np.asarray(mask, dtype=np.uint8).copy()\n\n  # 255 -> 0\n  pred += 1\n  mask += 1\n  pred = pred * (mask > 0)\n\n  inter = pred * (pred == mask)\n  (area_inter, _) = np.histogram(inter, bins=num_class, range=(1, num_class))\n  (area_pred, _) = np.histogram(pred, bins=num_class, range=(1, num_class))\n  (area_mask, _) = np.histogram(mask, bins=num_class, range=(1, num_class))\n  area_union = area_pred + area_mask - area_inter\n\n  return (area_inter, area_union)\n\n\ndef preprocess(image, mask, flip=False, scale=None, crop=None):\n  if flip:\n    if random.random() < 0.5:\n      image = image.transpose(Image.FLIP_LEFT_RIGHT)\n      mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n  if scale:\n    w, h = image.size\n    rand_log_scale = math.log(scale[0], 2) + random.random() * (math.log(scale[1], 2) - math.log(scale[0], 2))\n    random_scale = math.pow(2, rand_log_scale)\n    new_size = (int(round(w * random_scale)), int(round(h * random_scale)))\n    image = image.resize(new_size, Image.ANTIALIAS)\n    mask = mask.resize(new_size, Image.NEAREST)\n\n  data_transforms = transforms.Compose([\n      transforms.ToTensor(),\n      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n  image = data_transforms(image)\n  mask = torch.LongTensor(np.array(mask).astype(np.int64))\n\n  if crop:\n    h, w = image.shape[1], image.shape[2]\n    pad_tb = max(0, crop[0] - h)\n    pad_lr = max(0, crop[1] - w)\n    image = torch.nn.ZeroPad2d((0, pad_lr, 0, pad_tb))(image)\n    mask = torch.nn.ConstantPad2d((0, pad_lr, 0, pad_tb), 255)(mask)\n\n    h, w = image.shape[1], image.shape[2]\n    i = random.randint(0, h - crop[0])\n    j = random.randint(0, w - crop[1])\n    image = image[:, i:i + crop[0], j:j + crop[1]]\n    mask = mask[i:i + crop[0], j:j + crop[1]]\n\n  return image, mask\n\n'"
