file_path,api_count,code
convert.py,4,"b""import numpy as np\nfrom PIL import Image\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom model import NetworkImageNet\nfrom genotypes import PNASNet\nfrom operations import *\nfrom utils import preprocess_for_eval\n\nimport sys\nimport os\nsys.path.append('../PNASNet.TF')\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nimport tensorflow as tf\nfrom pnasnet import build_pnasnet_large, pnasnet_large_arg_scope\nslim = tf.contrib.slim\n\n\nclass ConvertPNASNet(object):\n\n  def __init__(self):\n    self.image = Image.open('data/cat.jpg')\n    self.read_tf_weight()\n    self.write_pytorch_weight()\n\n  def read_tf_weight(self):\n    self.weight_dict = {}\n    image_ph = tf.placeholder(tf.uint8, (None, None, 3))\n    image_proc = preprocess_for_eval(image_ph, 323, 323)\n    with slim.arg_scope(pnasnet_large_arg_scope()):\n      logits, end_points = build_pnasnet_large(\n          tf.expand_dims(image_proc, 0), num_classes=1001, is_training=False)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n    ckpt_restorer = tf.train.Saver()\n    ckpt_restorer.restore(sess, '../PNASNet.TF/data/model.ckpt')\n\n    weight_keys = [var.name[:-2] for var in tf.global_variables()]\n    weight_vals = sess.run(tf.global_variables())\n    for weight_key, weight_val in zip(weight_keys, weight_vals):\n      self.weight_dict[weight_key] = weight_val\n\n    self.tf_logits, self.tf_end_points, self.tf_image_proc = sess.run(\n        [logits, end_points, image_proc], feed_dict={image_ph: self.image})\n\n  def write_pytorch_weight(self):\n    model = NetworkImageNet(216, 1001, 12, False, PNASNet)\n    model.drop_path_prob = 0\n    model.eval()\n\n    self.used_keys = []\n    self.convert_conv(model.conv0, 'conv0/weights')\n    self.convert_bn(model.conv0_bn, 'conv0_bn/gamma', 'conv0_bn/beta',\n        'conv0_bn/moving_mean', 'conv0_bn/moving_variance')\n    self.convert_cell(model.stem1, 'cell_stem_0/')\n    self.convert_cell(model.stem2, 'cell_stem_1/')\n\n    for i in range(12):\n      self.convert_cell(model.cells[i], 'cell_{}/'.format(i))\n    \n    self.convert_fc(model.classifier, 'final_layer/FC/weights',\n        'final_layer/FC/biases')\n\n    print('Conversion complete!')\n    print('Check 1: whether all TF variables are used...')\n    assert len(self.weight_dict) == len(self.used_keys)\n    print('Pass!')\n\n    model = model.cuda()\n    image = self.tf_image_proc.transpose((2, 0, 1))\n    image = Variable(self.Tensor(image)).cuda()\n    logits, _ = model(image.unsqueeze(0))\n    self.pytorch_logits = logits.data.cpu().numpy()\n\n    print('Check 2: whether logits have small diff...')\n    assert np.max(np.abs(self.tf_logits - self.pytorch_logits)) < 1e-5\n    print('Pass!')\n\n    model_path = 'data/PNASNet-5_Large.pth'\n    torch.save(model.state_dict(), model_path)\n    print('PyTorch model saved to {}'.format(model_path))\n\n  def convert_cell(self, cell, name):\n    # cell.preprocess0\n    assert isinstance(cell.preprocess0, FactorizedReduce) or isinstance(cell.preprocess0, ReLUConvBN) or isinstance(cell.preprocess0, Identity)\n    if isinstance(cell.preprocess0, FactorizedReduce):\n      self.convert_conv(cell.preprocess0.conv_1, name + 'path1_conv/weights')\n      self.convert_conv(cell.preprocess0.conv_2, name + 'path2_conv/weights')\n      self.convert_bn(cell.preprocess0.bn, name + 'final_path_bn/gamma',\n          name + 'final_path_bn/beta', name + 'final_path_bn/moving_mean',\n          name + 'final_path_bn/moving_variance')\n    else:\n      if name + 'prev_1x1/weights' in self.weight_dict:\n        self.convert_conv(cell.preprocess0.op[1], name + 'prev_1x1/weights')\n        self.convert_bn(cell.preprocess0.op[2], name + 'prev_bn/gamma',\n            name + 'prev_bn/beta', name + 'prev_bn/moving_mean',\n            name + 'prev_bn/moving_variance')\n      # else preprocess0 is Identity or = preprocess1; do nothing\n\n    # cell.preprocess1\n    assert isinstance(cell.preprocess1, ReLUConvBN)\n    self.convert_conv(cell.preprocess1.op[1], name + '1x1/weights')\n    self.convert_bn(cell.preprocess1.op[2], name + 'beginning_bn/gamma',\n        name + 'beginning_bn/beta', name + 'beginning_bn/moving_mean',\n        name + 'beginning_bn/moving_variance')\n\n    # cell._ops\n    for i in range(len(cell._ops)):\n      side = 'left/' if i % 2 == 0 else 'right/'\n      prefix = name + 'comb_iter_{}/'.format(i // 2) + side\n      if isinstance(cell._ops[i], SepConv):\n        suffix = '{0}x{0}'.format(cell._ops[i].op[1].kernel_size[0])\n        \n        self.convert_conv(cell._ops[i].op[1], \n            prefix + 'separable_' + suffix + '_1/depthwise_weights', sep=True)\n        self.convert_conv(cell._ops[i].op[2],\n            prefix + 'separable_' + suffix + '_1/pointwise_weights', sep=False)\n        self.convert_bn(cell._ops[i].op[3],\n            prefix + 'bn_sep_' + suffix + '_1/gamma',\n            prefix + 'bn_sep_' + suffix + '_1/beta',\n            prefix + 'bn_sep_' + suffix + '_1/moving_mean',\n            prefix + 'bn_sep_' + suffix + '_1/moving_variance')\n        self.convert_conv(cell._ops[i].op[5],\n            prefix + 'separable_' + suffix + '_2/depthwise_weights', sep=True)\n        self.convert_conv(cell._ops[i].op[6],\n            prefix + 'separable_' + suffix + '_2/pointwise_weights', sep=False)\n        self.convert_bn(cell._ops[i].op[7],\n            prefix + 'bn_sep_' + suffix + '_2/gamma',\n            prefix + 'bn_sep_' + suffix + '_2/beta',\n            prefix + 'bn_sep_' + suffix + '_2/moving_mean',\n            prefix + 'bn_sep_' + suffix + '_2/moving_variance')\n      elif isinstance(cell._ops[i], ReLUConvBN):\n        # skip_connect with stride > 1\n        self.convert_conv(cell._ops[i].op[1], prefix + '1x1/weights')\n        self.convert_bn(cell._ops[i].op[2],\n            prefix + 'bn_1/gamma', prefix + 'bn_1/beta',\n            prefix + 'bn_1/moving_mean', prefix + 'bn_1/moving_variance')\n      elif isinstance(cell._ops[i], nn.Sequential):\n        # max_pool or avg_pool with C_in != C_out\n        self.convert_conv(cell._ops[i][1], prefix + '1x1/weights')\n        self.convert_bn(cell._ops[i][2], \n            prefix + 'bn_1/gamma', prefix + 'bn_1/beta',\n            prefix + 'bn_1/moving_mean', prefix + 'bn_1/moving_variance')\n\n  def convert_conv(self, conv2d, weights_key, sep=False):\n    weights = self.weight_dict[weights_key]\n    if sep:\n      # TF: [filter_height, filter_width, in_channels, channel_multiplier]\n      # TF: [1, 1, channel_multiplier * in_channels, channel_multiplier]\n      # PyTorch: [out_channels, in_channels // groups, *kernel_size]\n      weights = np.transpose(weights, (2, 3, 0, 1))\n    else:\n      # TF: [filter_height, filter_width, in_channels, out_channels]\n      # PyTorch: [out_channels, in_channels, *kernel_size]\n      weights = np.transpose(weights, (3, 2, 0, 1))\n    assert conv2d.weight.shape == self.Param(weights).shape, '{0} vs {1}'.format(conv2d.weight.shape, self.Param(weights).shape)\n    conv2d.weight = self.Param(weights)\n    self.used_keys += [weights_key]\n\n  def convert_bn(self, bn, gamma_key, beta_key, moving_mean_key, moving_var_key):\n    gamma = self.weight_dict[gamma_key]\n    beta = self.weight_dict[beta_key]\n    moving_mean = self.weight_dict[moving_mean_key]\n    moving_var = self.weight_dict[moving_var_key]\n    assert bn.weight.shape == self.Param(gamma).shape\n    assert bn.bias.shape == self.Param(beta).shape\n    assert bn.running_mean.shape == self.Tensor(moving_mean).shape\n    assert bn.running_var.shape == self.Tensor(moving_var).shape\n    bn.weight = self.Param(gamma)\n    bn.bias = self.Param(beta)\n    bn.running_mean = self.Tensor(moving_mean)\n    bn.running_var = self.Tensor(moving_var)\n    self.used_keys += [gamma_key, beta_key, moving_mean_key, moving_var_key]\n\n  def convert_fc(self, fc, weights_key, biases_key):\n    weights = self.weight_dict[weights_key]\n    biases = self.weight_dict[biases_key]\n    weights = np.transpose(weights)\n    assert fc.weight.shape == self.Param(weights).shape\n    assert fc.bias.shape == self.Param(biases).shape\n    fc.weight = self.Param(weights)\n    fc.bias = self.Param(biases)\n    self.used_keys += [weights_key, biases_key]\n\n  def Param(self, x):\n    return torch.nn.Parameter(torch.from_numpy(x))\n\n  def Tensor(self, x):\n    return torch.from_numpy(x)\n\n\nif __name__ == '__main__':\n  ConvertPNASNet()\n"""
genotypes.py,0,"b""from collections import namedtuple\n\nGenotype = namedtuple('Genotype', 'normal normal_concat reduce reduce_concat')\n\nPNASNet = Genotype(\n  normal = [\n    ('sep_conv_5x5', 0),\n    ('max_pool_3x3', 0),\n    ('sep_conv_7x7', 1),\n    ('max_pool_3x3', 1),\n    ('sep_conv_5x5', 1),\n    ('sep_conv_3x3', 1),\n    ('sep_conv_3x3', 4),\n    ('max_pool_3x3', 1),\n    ('sep_conv_3x3', 0),\n    ('skip_connect', 1),\n  ],\n  normal_concat = [2, 3, 4, 5, 6],\n  reduce = [\n    ('sep_conv_5x5', 0),\n    ('max_pool_3x3', 0),\n    ('sep_conv_7x7', 1),\n    ('max_pool_3x3', 1),\n    ('sep_conv_5x5', 1),\n    ('sep_conv_3x3', 1),\n    ('sep_conv_3x3', 4),\n    ('max_pool_3x3', 1),\n    ('sep_conv_3x3', 0),\n    ('skip_connect', 1),\n  ],\n  reduce_concat = [2, 3, 4, 5, 6],\n)\n\n"""
main.py,4,"b""import argparse\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nimport tensorflow as tf\nimport torch\nimport torchvision.datasets as datasets\nfrom torch.autograd import Variable\nfrom model import NetworkImageNet\nfrom genotypes import PNASNet\nfrom utils import preprocess_for_eval\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--valdir', type=str, default='data/val',\n                    help='path to ImageNet val folder')\nparser.add_argument('--image_size', type=int, default=331,\n                    help='image size')\nparser.add_argument('--num_conv_filters', type=int, default=216,\n                    help='number of filters')\nparser.add_argument('--num_classes', type=int, default=1001,\n                    help='number of categories')\nparser.add_argument('--num_cells', type=int, default=12,\n                    help='number of cells')\n\n\ndef main():\n  args = parser.parse_args()\n  assert torch.cuda.is_available()\n\n  image_ph = tf.placeholder(tf.uint8, (None, None, 3))\n  image_proc = preprocess_for_eval(image_ph, args.image_size, args.image_size)\n  config = tf.ConfigProto()\n  config.gpu_options.allow_growth = True\n  sess = tf.Session(config=config)\n\n  model = NetworkImageNet(args.num_conv_filters, args.num_classes,\n                          args.num_cells, False, PNASNet)\n  model.drop_path_prob = 0\n  model.eval()\n  model.load_state_dict(torch.load('data/PNASNet-5_Large.pth'))\n  model = model.cuda()\n\n  c1, c5 = 0, 0\n  val_dataset = datasets.ImageFolder(args.valdir)\n  for i, (image, label) in enumerate(val_dataset):\n    tf_image_proc = sess.run(image_proc, feed_dict={image_ph: image})\n    image = torch.from_numpy(tf_image_proc.transpose((2, 0, 1)))\n    image = Variable(image).cuda()\n    logits, _ = model(image.unsqueeze(0))\n    top5 = logits.data.cpu().numpy().squeeze().argsort()[::-1][:5]\n    top1 = top5[0]\n    if label + 1 == top1:\n      c1 += 1\n    if label + 1 in top5:\n      c5 += 1\n    print('Test: [{0}/{1}]\\t'\n          'Prec@1 {2:.3f}\\t'\n          'Prec@5 {3:.3f}\\t'.format(\n          i + 1, len(val_dataset), c1 / (i + 1.), c5 / (i + 1.)))\n\n\nif __name__ == '__main__':\n  main()\n"""
model.py,3,"b'import torch\nimport torch.nn as nn\nfrom operations import *\nfrom torch.autograd import Variable\n# from utils import drop_path\n\n\nclass Cell(nn.Module):\n\n  def __init__(self, genotype, C_prev_prev, C_prev, C, reduction, reduction_prev):\n    super(Cell, self).__init__()\n    print(C_prev_prev, C_prev, C)\n    self.reduction = reduction\n\n    if reduction_prev is None:\n      self.preprocess0 = Identity()\n    elif reduction_prev is True:\n      self.preprocess0 = FactorizedReduce(C_prev_prev, C)\n    else:\n      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0)\n    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0)\n    \n    if reduction:\n      op_names, indices = zip(*genotype.reduce)\n      concat = genotype.reduce_concat\n    else:\n      op_names, indices = zip(*genotype.normal)\n      concat = genotype.normal_concat\n\n    assert len(op_names) == len(indices)\n    self._steps = len(op_names) // 2\n    self._concat = concat\n    self.multiplier = len(concat)\n\n    self._ops = nn.ModuleList()\n    for name, index in zip(op_names, indices):\n      stride = 2 if reduction and index < 2 else 1\n      if reduction_prev is None and index == 0:\n        op = OPS[name](C_prev_prev, C, stride, True)\n      else:\n        op = OPS[name](C, C, stride, True)\n      self._ops += [op]\n    self._indices = indices\n\n  def forward(self, s0, s1, drop_prob):\n    s0 = self.preprocess0(s0)\n    s1 = self.preprocess1(s1)\n\n    states = [s0, s1]\n    for i in range(self._steps):\n      h1 = states[self._indices[2*i]]\n      h2 = states[self._indices[2*i+1]]\n      op1 = self._ops[2*i]\n      op2 = self._ops[2*i+1]\n      h1 = op1(h1)\n      h2 = op2(h2)\n      # if self.training and drop_prob > 0.:\n      #   if not isinstance(op1, Identity):\n      #     h1 = drop_path(h1, drop_prob)\n      #   if not isinstance(op2, Identity):\n      #     h2 = drop_path(h2, drop_prob)\n      s = h1 + h2\n      states += [s]\n    return torch.cat([states[i] for i in self._concat], dim=1)\n\n\nclass AuxiliaryHeadImageNet(nn.Module):\n\n  def __init__(self, C, num_classes):\n    """"""assuming input size 14x14""""""\n    super(AuxiliaryHeadImageNet, self).__init__()\n    self.features = nn.Sequential(\n      nn.ReLU(inplace=True),\n      nn.AvgPool2d(5, stride=2, padding=0, count_include_pad=False),\n      nn.Conv2d(C, 128, 1, bias=False),\n      nn.BatchNorm2d(128),\n      nn.ReLU(inplace=True),\n      nn.Conv2d(128, 768, 2, bias=False),\n      nn.BatchNorm2d(768),\n      nn.ReLU(inplace=True)\n    )\n    self.classifier = nn.Linear(768, num_classes)\n\n  def forward(self, x):\n    x = self.features(x)\n    x = self.classifier(x.view(x.size(0),-1))\n    return x\n\n\nclass NetworkImageNet(nn.Module):\n\n  def __init__(self, C, num_classes, layers, auxiliary, genotype):\n    super(NetworkImageNet, self).__init__()\n    self._layers = layers\n    self._auxiliary = auxiliary\n\n    self.conv0 = nn.Conv2d(3, 96, kernel_size=3, stride=2, padding=0, bias=False)\n    self.conv0_bn = nn.BatchNorm2d(96, eps=1e-3)\n    self.stem1 = Cell(genotype, 96, 96, C // 4, True, None)\n    self.stem2 = Cell(genotype, 96, C * self.stem1.multiplier // 4, C // 2, True, True)\n\n    C_prev_prev, C_prev, C_curr = C * self.stem1.multiplier // 4, C * self.stem2.multiplier // 2, C\n\n    self.cells = nn.ModuleList()\n    reduction_prev = True\n    for i in xrange(layers):\n      if i in [layers // 3, 2 * layers // 3]:\n        C_curr *= 2\n        reduction = True\n      else:\n        reduction = False\n      cell = Cell(genotype, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\n      reduction_prev = reduction\n      self.cells += [cell]\n      C_prev_prev, C_prev = C_prev, cell.multiplier * C_curr\n      if i == 2 * layers // 3:\n        C_to_auxiliary = C_prev\n\n    if auxiliary:\n      self.auxiliary_head = AuxiliaryHeadImageNet(C_to_auxiliary, num_classes)\n    self.relu = nn.ReLU(inplace=False)\n    self.global_pooling = nn.AdaptiveAvgPool2d(1)\n    self.classifier = nn.Linear(C_prev, num_classes)\n\n  def forward(self, input):\n    logits_aux = None\n    s0 = self.conv0(input)\n    s0 = self.conv0_bn(s0)\n    s1 = self.stem1(s0, s0, self.drop_path_prob)\n    s0, s1 = s1, self.stem2(s0, s1, self.drop_path_prob)\n    for i, cell in enumerate(self.cells):\n      s0, s1 = s1, cell(s0, s1, self.drop_path_prob)\n      if i == 2 * self._layers // 3:\n        if self._auxiliary and self.training:\n          logits_aux = self.auxiliary_head(s1)\n    s1 = self.relu(s1)\n    out = self.global_pooling(s1)\n    logits = self.classifier(out.view(out.size(0), -1))\n    return logits, logits_aux\n\n'"
operations.py,2,"b""import torch\nimport torch.nn as nn\n\nOPS = {\n  'none' : lambda C_in, C_out, stride, affine: Zero(stride),\n  'avg_pool_3x3' : lambda C_in, C_out, stride, affine: nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False) if C_in == C_out else nn.Sequential(\n    nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False),\n    nn.Conv2d(C_in, C_out, 1, stride=1, padding=0, bias=False),\n    nn.BatchNorm2d(C_out, eps=1e-3, affine=affine)\n    ),\n  'max_pool_3x3' : lambda C_in, C_out, stride, affine: nn.MaxPool2d(3, stride=stride, padding=1) if C_in == C_out else nn.Sequential(\n    nn.MaxPool2d(3, stride=stride, padding=1),\n    nn.Conv2d(C_in, C_out, 1, stride=1, padding=0, bias=False),\n    nn.BatchNorm2d(C_out, eps=1e-3, affine=affine)\n    ),\n  'skip_connect' : lambda C_in, C_out, stride, affine: Identity() if stride == 1 else ReLUConvBN(C_in, C_out, 1, stride, 0, affine=affine),\n  'sep_conv_3x3' : lambda C_in, C_out, stride, affine: SepConv(C_in, C_out, 3, stride, 1, affine=affine),\n  'sep_conv_5x5' : lambda C_in, C_out, stride, affine: SepConv(C_in, C_out, 5, stride, 2, affine=affine),\n  'sep_conv_7x7' : lambda C_in, C_out, stride, affine: SepConv(C_in, C_out, 7, stride, 3, affine=affine),\n  'dil_conv_3x3' : lambda C_in, C_out, stride, affine: DilConv(C_in, C_out, 3, stride, 2, 2, affine=affine),\n  'dil_conv_5x5' : lambda C_in, C_out, stride, affine: DilConv(C_in, C_out, 5, stride, 4, 2, affine=affine),\n  'conv_7x1_1x7' : lambda C_in, C_out, stride, affine: nn.Sequential(\n    nn.ReLU(inplace=False),\n    nn.Conv2d(C_in, C_in, (1,7), stride=(1, stride), padding=(0, 3), bias=False),\n    nn.Conv2d(C_in, C_out, (7,1), stride=(stride, 1), padding=(3, 0), bias=False),\n    nn.BatchNorm2d(C_out, eps=1e-3, affine=affine)\n    ),\n}\n\nclass ReLUConvBN(nn.Module):\n\n  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n    super(ReLUConvBN, self).__init__()\n    self.op = nn.Sequential(\n      nn.ReLU(inplace=False),\n      nn.Conv2d(C_in, C_out, kernel_size, stride=stride, padding=padding, bias=False),\n      nn.BatchNorm2d(C_out, eps=1e-3, affine=affine)\n    )\n\n  def forward(self, x):\n    return self.op(x)\n\nclass DilConv(nn.Module):\n    \n  def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):\n    super(DilConv, self).__init__()\n    self.op = nn.Sequential(\n      nn.ReLU(inplace=False),\n      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=C_in, bias=False),\n      nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\n      nn.BatchNorm2d(C_out, eps=1e-3, affine=affine),\n      )\n\n  def forward(self, x):\n    return self.op(x)\n\n\nclass SepConv(nn.Module):\n    \n  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n    super(SepConv, self).__init__()\n    self.op = nn.Sequential(\n      nn.ReLU(inplace=False),\n      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=C_in, bias=False),\n      nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\n      nn.BatchNorm2d(C_out, eps=1e-3, affine=affine),\n      nn.ReLU(inplace=False),\n      nn.Conv2d(C_out, C_out, kernel_size=kernel_size, stride=1, padding=padding, groups=C_out, bias=False),\n      nn.Conv2d(C_out, C_out, kernel_size=1, padding=0, bias=False),\n      nn.BatchNorm2d(C_out, eps=1e-3, affine=affine),\n      )\n\n  def forward(self, x):\n    return self.op(x)\n\n\nclass Identity(nn.Module):\n\n  def __init__(self):\n    super(Identity, self).__init__()\n\n  def forward(self, x):\n    return x\n\n\nclass Zero(nn.Module):\n\n  def __init__(self, stride):\n    super(Zero, self).__init__()\n    self.stride = stride\n\n  def forward(self, x):\n    if self.stride == 1:\n      return x.mul(0.)\n    return x[:,:,::self.stride,::self.stride].mul(0.)\n\n\nclass FactorizedReduce(nn.Module):\n\n  def __init__(self, C_in, C_out, affine=True):\n    super(FactorizedReduce, self).__init__()\n    assert C_out % 2 == 0\n    self.relu = nn.ReLU(inplace=False)\n    self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n    self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False) \n    self.bn = nn.BatchNorm2d(C_out, eps=1e-3, affine=affine)\n    self.pad = nn.ConstantPad2d((0, 1, 0, 1), 0)\n\n  def forward(self, x):\n    x = self.relu(x)\n    y = self.pad(x)\n    out = torch.cat([self.conv_1(x), self.conv_2(y[:,:,1:,1:])], dim=1)\n    out = self.bn(out)\n    return out\n\n"""
utils.py,0,"b'import tensorflow as tf\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would crop the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n    return image\n'"
