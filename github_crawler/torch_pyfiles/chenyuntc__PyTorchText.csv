file_path,api_count,code
config.py,0,"b'#coding:utf8\nimport time\nimport warnings\n\ntfmt = \'%m%d_%H%M%S\'\nclass Config(object):\n    \'\'\'\n    \xe5\xb9\xb6\xe4\xb8\x8d\xe6\x98\xaf\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae\xe9\x83\xbd\xe7\x94\x9f\xe6\x95\x88,\xe5\xae\x9e\xe9\x99\x85\xe8\xbf\x90\xe8\xa1\x8c\xe4\xb8\xad\xe5\x8f\xaa\xe6\xa0\xb9\xe6\x8d\xae\xe9\x9c\x80\xe6\xb1\x82\xe8\x8e\xb7\xe5\x8f\x96\xe8\x87\xaa\xe5\xb7\xb1\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n    \'\'\'\n\n    loss = \'multilabelloss\'\n    model=\'CNNText\' \n    title_dim = 100 # \xe6\xa0\x87\xe9\xa2\x98\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x95\xb0\n    content_dim = 200 #\xe6\x8f\x8f\xe8\xbf\xb0\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x95\xb0\n    num_classes = 1999 # \xe7\xb1\xbb\xe5\x88\xab\n    embedding_dim = 256 # embedding\xe5\xa4\xa7\xe5\xb0\x8f\n    linear_hidden_size = 2000 # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe9\x9a\x90\xe8\x97\x8f\xe5\x85\x83\xe6\x95\xb0\xe7\x9b\xae\n    kmax_pooling = 2# k\n    hidden_size = 256 #LSTM hidden size\n    num_layers=2 #LSTM layers\n    inception_dim = 512 #inception\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x95\xb0\n    \n    # vocab_size = 11973 # num of chars\n    vocab_size = 411720 # num of words \n    kernel_size = 3 #\xe5\x8d\x95\xe5\xb0\xba\xe5\xba\xa6\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\n    kernel_sizes = [2,3,4] #\xe5\xa4\x9a\xe5\xb0\xba\xe5\xba\xa6\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\n    title_seq_len = 50 # \xe6\xa0\x87\xe9\xa2\x98\xe9\x95\xbf\xe5\xba\xa6,word\xe4\xb8\xba30 char\xe4\xb8\xba50\n    content_seq_len = 250 #\xe6\x8f\x8f\xe8\xbf\xb0\xe9\x95\xbf\xe5\xba\xa6 word\xe4\xb8\xba120 char\xe4\xb8\xba250\n    type_=\'word\' #word \xe5\x92\x8cchar\n    all=False # \xe6\xa8\xa1\xe5\x9e\x8b\xe5\x90\x8c\xe6\x97\xb6\xe8\xae\xad\xe7\xbb\x83char\xe5\x92\x8cword\n\n    embedding_path = \'/mnt/7/zhihu/ieee_zhihu_cup/data/char_embedding.npz\' # Embedding\n    train_data_path = \'/mnt/7/zhihu/ieee_zhihu_cup/data/train.npz\' # train\n    labels_path = \'/mnt/7/zhihu/ieee_zhihu_cup/data/labels.json\' # labels    \n    test_data_path=\'/mnt/7/zhihu/ieee_zhihu_cup/data/test.npz\' # test\n    result_path=\'csv/\'+time.strftime(tfmt)+\'.csv\'\n    shuffle = True # \xe6\x98\xaf\xe5\x90\xa6\xe9\x9c\x80\xe8\xa6\x81\xe6\x89\x93\xe4\xb9\xb1\xe6\x95\xb0\xe6\x8d\xae\n    num_workers = 4 # \xe5\xa4\x9a\xe7\xba\xbf\xe7\xa8\x8b\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x89\x80\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe7\xba\xbf\xe7\xa8\x8b\xe6\x95\xb0\xe7\x9b\xae\n    pin_memory =  True # \xe6\x95\xb0\xe6\x8d\xae\xe4\xbb\x8eCPU->pin_memory\xe2\x80\x94>GPU\xe5\x8a\xa0\xe9\x80\x9f\n    batch_size = 128\n\n    env = time.strftime(tfmt) # Visdom env\n    plot_every = 10 # \xe6\xaf\x8f10\xe4\xb8\xaabatch\xef\xbc\x8c\xe6\x9b\xb4\xe6\x96\xb0visdom\xe7\xad\x89\n\n    max_epoch=100\n    lr = 5e-3 # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n    lr2 = 1e-3 # embedding\xe5\xb1\x82\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n    min_lr = 1e-5 # \xe5\xbd\x93\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe4\xbd\x8e\xe4\xba\x8e\xe8\xbf\x99\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x8c\xe5\xb0\xb1\xe9\x80\x80\xe5\x87\xba\xe8\xae\xad\xe7\xbb\x83\n    lr_decay = 0.99 # \xe5\xbd\x93\xe4\xb8\x80\xe4\xb8\xaaepoch\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\xbc\x80\xe5\xa7\x8b\xe4\xb8\x8a\xe5\x8d\x87lr = lr*lr_decay \n    weight_decay = 0 #2e-5 # \xe6\x9d\x83\xe9\x87\x8d\xe8\xa1\xb0\xe5\x87\x8f\n    weight = 1 # \xe6\xad\xa3\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84weight\n    decay_every = 3000 #\xe6\xaf\x8f\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaabatch \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8bscore,\xe5\xb9\xb6\xe9\x9a\x8f\xe4\xb9\x8b\xe4\xbf\xae\xe6\x94\xb9\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n\n    model_path = None # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x9c\x89 \xe5\xb0\xb1\xe5\x8a\xa0\xe8\xbd\xbd\n    optimizer_path=\'optimizer.pth\' # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\x9a\x84\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xb0\xe5\x9d\x80\n\n    debug_file = \'/tmp/debug2\' #\xe8\x8b\xa5\xe8\xaf\xa5\xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe5\x9c\xa8\xe5\x88\x99\xe8\xbf\x9b\xe5\x85\xa5debug\xe6\xa8\xa1\xe5\xbc\x8f\n    debug=False\n    \n    gpu1 = False #\xe5\xa6\x82\xe6\x9e\x9c\xe5\x9c\xa8GPU1\xe4\xb8\x8a\xe8\xbf\x90\xe8\xa1\x8c\xe4\xbb\xa3\xe7\xa0\x81,\xe5\x88\x99\xe9\x9c\x80\xe8\xa6\x81\xe4\xbf\xae\xe6\x94\xb9\xe6\x95\xb0\xe6\x8d\xae\xe5\xad\x98\xe6\x94\xbe\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n    floyd=False # \xe6\x9c\x8d\xe5\x8a\xa1\xe5\xa6\x82\xe6\x9e\x9c\xe5\x9c\xa8floyd\xe4\xb8\x8a\xe8\xbf\x90\xe8\xa1\x8c\xe9\x9c\x80\xe8\xa6\x81\xe4\xbf\xae\xe6\x94\xb9\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n    zhuge=False # \xe6\x9c\x8d\xe5\x8a\xa1\xe5\xa6\x82\xe6\x9e\x9c\xe5\x9c\xa8zhuge\xe4\xb8\x8a\xe8\xbf\x90\xe8\xa1\x8c,\xe4\xbf\xae\xe6\x94\xb9\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n\n    ### multimode \xe7\x94\xa8\xe5\x88\xb0\xe7\x9a\x84\n    model_names=[\'MultiCNNTextBNDeep\',\'CNNText_inception\',\'RCNN\',\'LSTMText\',\'CNNText_inception\']\n    model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/CNNText_tmp_0.380390420742\',\'checkpoints/RCNN_word_0.373609030286\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/CNNText_tmp_0.376364647145\']#,\'checkpoints/CNNText_tmp_0.402429167301\']\n    static=False # \xe6\x98\xaf\xe5\x90\xa6\xe8\xae\xad\xe7\xbb\x83embedding\n    val=False # \xe8\xb7\x91\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe8\xbf\x98\xe6\x98\xaf\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86?\n    \n    fold = 1 # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86fold, 0\xe6\x88\x961 \xe8\xa7\x81 data/fold_dataset.py\n    augument=True # \xe6\x98\xaf\xe5\x90\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\n\n    ###stack\n    model_num=7\n    data_root=""/data/text/zhihu/result/""\n    labels_file=""/home/a/code/pytorch/zhihu/ddd/labels.json""\n    val=""/home/a/code/pytorch/zhihu/ddd/val.npz""\n\ndef parse(self,kwargs,print_=True):\n        \'\'\'\n        \xe6\xa0\xb9\xe6\x8d\xae\xe5\xad\x97\xe5\x85\xb8kwargs \xe6\x9b\xb4\xe6\x96\xb0 config\xe5\x8f\x82\xe6\x95\xb0\n        \'\'\'\n        for k,v in kwargs.iteritems():\n            if not hasattr(self,k):\n                raise Exception(""opt has not attribute <%s>"" %k)\n            setattr(self,k,v) \n\n        ###### \xe6\xa0\xb9\xe6\x8d\xae\xe7\xa8\x8b\xe5\xba\x8f\xe5\x9c\xa8\xe5\x93\xaa\xe5\x8f\xb0\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\xe8\xbf\x90\xe8\xa1\x8c,\xe8\x87\xaa\xe5\x8a\xa8\xe4\xbf\xae\xe6\xad\xa3\xe6\x95\xb0\xe6\x8d\xae\xe5\xad\x98\xe6\x94\xbe\xe8\xb7\xaf\xe5\xbe\x84 ######\n        if self.gpu1:\n            self.train_data_path=\'/mnt/zhihu/data/train.npz\'\n            self.test_data_path=\'/mnt/zhihu/data/%s.npz\' %(\'val\' if self.val else \'test\')\n            self.labels_path=\'/mnt/zhihu/data/labels.json\'\n            self.embedding_path=self.embedding_path.replace(\'/mnt/7/zhihu/ieee_zhihu_cup/\',\'/mnt/zhihu/\')\n        \n        if self.floyd:\n            self.train_data_path=\'/data/train.npz\'\n            self.test_data_path=\'/data/%s.npz\' %(\'val\' if self.val else \'test\')\n            self.labels_path=\'/data/labels.json\'\n            self.embedding_path=\'/data/char_embedding.npz\'\n        if self.zhuge:\n            self.train_data_path=\'./ddd/train.npz\'\n            self.test_data_path=\'./ddd/%s.npz\' %(\'val\' if self.val else \'test\')\n            self.labels_path=\'./ddd/labels.json\'\n            self.embedding_path=\'./ddd/char_embedding.npz\'\n\n        ### word\xe5\x92\x8cchar\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\x8d\xe4\xb8\x80\xe6\xa0\xb7 ##\n        if self.type_==\'word\':\n            self.vocab_size = 411720 # num of words \n            self.title_seq_len = 30\n            self.content_seq_len = 120 \n            self.embedding_path=self.embedding_path.replace(\'char\',\'word\') if self.embedding_path is not None else None\n            \n        if self.type_==\'char\':\n            self.vocab_size = 11973 # num of words\n            self.title_seq_len = 50\n            self.content_seq_len = 250 \n        \n        if self.model_path:\n            self.embedding_path=None\n        \n        if print_:\n            print(\'user config:\')\n            print(\'#################################\')\n            for k in dir(self):\n                if not k.startswith(\'_\') and k!=\'parse\' and k!=\'state_dict\':\n                    print k,getattr(self,k)\n            print(\'#################################\')\n        return self\n\ndef state_dict(self):\n    return  {k:getattr(self,k) for k in dir(self) if not k.startswith(\'_\') and k!=\'parse\' and k!=\'state_dict\' }\n\n\nConfig.parse = parse\nConfig.state_dict = state_dict\nopt = Config()\n'"
main-all.1.py,2,"b'#coding:utf8\nfrom config import opt\nimport models\nimport os\nimport tqdm\nfrom data.dataset import ZhihuData,ZhihuALLData\nimport torch as t\nimport time\nimport fire\nimport torchnet as tnt\nfrom torch.utils import data\nfrom torch.autograd import Variable\nfrom utils.visualize import Visualizer\nfrom utils import get_score#,get_optimizer \nvis = Visualizer(opt.env)\n\'\'\'\n\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xae\xad\xe7\xbb\x83\xe6\x9c\x80\xe5\xa5\xbd\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84attention stack\n\'\'\'\ndef hook():pass\n\ndef val(model,dataset):\n\n    dataset.train(False)\n    model.eval()\n\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = False,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n    \n    predict_label_and_marked_label_list=[]\n    for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n        title,content,label = (Variable(title[0].cuda()),Variable(title[1].cuda())),(Variable(content[0].cuda()),Variable(content[1].cuda())),Variable(label.cuda())\n        score = model(title,content)\n        # !TODO: \xe4\xbc\x98\xe5\x8c\x96\xe6\xad\xa4\xe5\xa4\x84\xe4\xbb\xa3\xe7\xa0\x81\n        #       1. append\n        #       2. for\xe5\xbe\xaa\xe7\x8e\xaf\n        #       3. topk \xe4\xbb\xa3\xe6\x9b\xbfsort\n\n        predict = score.data.topk(5,dim=1)[1].cpu().tolist()\n        true_target = label.data.float().topk(5,dim=1)#[1].cpu().tolist()#sort(dim=1,descending=True)\n        true_index=true_target[1][:,:5]\n        true_label=true_target[0][:,:5]\n        tmp= []\n\n        for jj in range(label.size(0)):\n            true_index_=true_index[jj]\n            true_label_=true_label[jj]\n            true=true_index_[true_label_>0]\n            tmp.append((predict[jj],true.tolist()))\n        \n        predict_label_and_marked_label_list.extend(tmp)\n    del score\n\n    dataset.train(True)\n    model.train()\n    \n    scores,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n    return (scores,prec_,recall_,_ss)\n\n\n\n\ndef main(**kwargs):\n    #\xe5\x8a\xa8\xe6\x80\x81\xe5\x8a\xa0\xe5\x85\xa8\xe8\x81\x8c\xe8\xa1\xb0\xe5\x87\x8f\n    opt.parse(kwargs,print_=False)\n    if opt.debug:import ipdb;ipdb.set_trace()\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'RCNN\',\'LSTMText\',\'CNNText_inception\',\'RCNN\',\'CNNText_inception\',\'LSTMText\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.41124002492\',\'checkpoints/RCNN_word_0.411511574999\',\'checkpoints/LSTMText_word_0.411994005382\',\'checkpoints/CNNText_tmp_char_0.402429167301\',\'checkpoints/RCNN_char_0.403710422571\',\'checkpoints/CNNText_tmp_word_0.41096749885\',\'checkpoints/LSTMText_char_0.403192339135\',]#\'checkpoints/FastText_word_0.400391584867\']\n##################iMultiModelAll2_word_0.425600838271################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'RCNN\',\'LSTMText\',\'RCNN\',\'CNNText_inception\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.41124002492\',\'checkpoints/RCNN_word_0.411511574999\',\'checkpoints/LSTMText_word_0.411994005382\',\'checkpoints/RCNN_char_0.403710422571\',\'checkpoints/CNNText_tmp_char_0.402429167301\']\n#####-------------------------------------------------------#####\n\n#############################################################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'RCNN\',\'LSTMText\',\'RCNN\',\'CNNText_inception\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/RCNN_word_0.373609030286\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/RCNN_char_0.3456599248\',\'checkpoints/CNNText_tmp_0.352036505041\']\n#####-------------------------------------------------------#####\n\n    # opt.model_names=[\'LSTMText\',\'MultiCNNTextBNDeep\']\n    # opt.model_paths=[\'checkpoints/LSTMText_word_0.396765494482\',\'checkpoints/MultiCNNTextBNDeep_word_0.391018392216\']\n    # opt.fold=1\n    # from data.dataset import ALLFoldData as ZhihuALLData\n########################################################################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'RCNN\',\'LSTMText\',\'RCNN\',\'CNNText_inception\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/RCNN_word_0.373609030286\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/RCNN_char_0.3456599248\',\'checkpoints/CNNText_tmp_0.352036505041\']\n\n#######################################0.41884129858126845-force#####################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'RCNN\',\'LSTMText\',\'RCNN\',\'MultiCNNTextBNDeep\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410011182415\',\'checkpoints/RCNN_word_0.413446202556\',\'checkpoints/LSTMText_word_0.413681107036\',\'checkpoints/RCNN_char_0.398655349075\',\'checkpoints/MultiCNNTextBNDeep_char_0.38666657051\']\n#######################################################################\n\n#############################################MultiModelallfast_0.419088#####################################\n    opt.model_names=[\'MultiCNNTextBNDeep\',\'FastText3\',\'LSTMText\',\'CNNText_inception\']\n    opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.41124002492\',\'checkpoints/FastText3_word_0.40810787337\',\'checkpoints/LSTMText_word_0.413681107036\',\'checkpoints/CNNText_tmp_char_0.402429167301\'\n########################################################################################3\n    model = getattr(models,opt.model)(opt).cuda()\n    if opt.model_path:\n        model.load(opt.model_path)\n    print(model)\n\n    opt.parse(kwargs,print_=True)\n\n    vis.reinit(opt.env)\n    pre_loss=1.0\n    lr,lr2=opt.lr,opt.lr2\n    loss_function = getattr(models,opt.loss)()  \n    if opt.all:dataset = ZhihuALLData(opt.train_data_path,opt.labels_path,type_=opt.type_,augument=opt.augument)\n    # else :dataset = ZhihuData(opt.train_data_path,opt.labels_path,type_=opt.type_)\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = opt.shuffle,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n\n    optimizer = model.get_optimizer(opt.lr,opt.lr2)\n    loss_meter = tnt.meter.AverageValueMeter()\n    score_meter=tnt.meter.AverageValueMeter()\n    best_score = 0\n    # pre_score = 0\n\n    for epoch in range(opt.max_epoch):\n        loss_meter.reset()\n        score_meter.reset()\n        for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n            title,content,label = (Variable(title[0].cuda()),Variable(title[1].cuda())),(Variable(content[0].cuda()),Variable(content[1].cuda())),Variable(label.cuda())\n            optimizer.zero_grad()\n            score = model(title,content)\n            loss = loss_function(score,label.float())\n            loss_meter.add(loss.data[0])\n            loss.backward()\n            optimizer.step()\n\n            if ii%opt.plot_every ==opt.plot_every-1:\n                if os.path.exists(opt.debug_file):\n                    import ipdb\n                    ipdb.set_trace()\n\n                predict = score.data.topk(5,dim=1)[1].cpu().tolist()#(dim=1,descending=True)[1][:,:5].tolist()\n                true_target = label.data.float().topk(5,dim=1)#[1].cpu().tolist()#sort(dim=1,descending=True)\n                true_index=true_target[1][:,:5]\n                true_label=true_target[0][:,:5]\n                predict_label_and_marked_label_list=[]\n                for jj in range(label.size(0)):\n                    true_index_=true_index[jj]\n                    true_label_=true_label[jj]\n                    true=true_index_[true_label_>0]\n                    predict_label_and_marked_label_list.append((predict[jj],true.tolist()))\n                score_,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n                score_meter.add(score_)\n                vis.vis.text(\'prec:%s,recall:%s,score:%s,a:%s\' %(prec_,recall_,score_,_ss),win=\'tmp\')\n                vis.plot(\'scores\', score_meter.value()[0])\n                vis.plot(\'loss\', loss_meter.value()[0])\n                   \n            if ii%opt.decay_every == opt.decay_every-1:   \n                del loss\n                scores,prec_,recall_ ,_ss= val(model,dataset)\n                vis.log({\' epoch:\':epoch,\' lr: \':lr,\'scores\':scores,\'prec\':prec_,\'recall\':recall_,\'ss\':_ss,\'scores_train\':score_meter.value()[0],\'loss\':loss_meter.value()[0]})\n\n                if scores>best_score:\n                    best_score = scores\n                    best_path = model.save(name = str(scores),new=True)\n               \n                if scores < best_score:\n                    model.load(best_path,change_opt=False)\n                    lr = lr * opt.lr_decay\n                    if lr2==0:lr2=1e-4\n                    else : lr2 = lr2*0.5\n                    optimizer = model.get_optimizer(lr,lr2,0)\n                             \n                pre_loss = loss_meter.value()[0]\n                loss_meter.reset()\n                score_meter.reset()\n                \n\nif __name__==""__main__"":\n    fire.Fire()  \n'"
main-all.py,2,"b'#coding:utf8\nfrom config import opt\nimport models\nimport os\nimport tqdm\nfrom data.dataset import ZhihuData,ZhihuALLData\nimport torch as t\nimport time\nimport fire\nimport torchnet as tnt\nfrom torch.utils import data\nfrom torch.autograd import Variable\nfrom utils.visualize import Visualizer\nfrom utils import get_score#,get_optimizer \nvis = Visualizer(opt.env)\n\'\'\'\n\xe8\xae\xad\xe7\xbb\x83 stack-attention \xe4\xb8\x8d\xe8\xbf\x87\xe8\xbf\x99\xe4\xba\x9b\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xaf\x94\xe8\xbe\x83\xe5\xb7\xae \xe6\xb2\xa1\xe7\x94\xa8\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\n\'\'\'\ndef hook():pass\n\ndef val(model,dataset):\n\n    dataset.train(False)\n    model.eval()\n\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = False,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n    \n    predict_label_and_marked_label_list=[]\n    for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n        title,content,label = (Variable(title[0].cuda()),Variable(title[1].cuda())),(Variable(content[0].cuda()),Variable(content[1].cuda())),Variable(label.cuda())\n        score = model(title,content)\n        # !TODO: \xe4\xbc\x98\xe5\x8c\x96\xe6\xad\xa4\xe5\xa4\x84\xe4\xbb\xa3\xe7\xa0\x81\n        #       1. append\n        #       2. for\xe5\xbe\xaa\xe7\x8e\xaf\n        #       3. topk \xe4\xbb\xa3\xe6\x9b\xbfsort\n\n        predict = score.data.topk(5,dim=1)[1].cpu().tolist()\n        true_target = label.data.float().topk(5,dim=1)#[1].cpu().tolist()#sort(dim=1,descending=True)\n        true_index=true_target[1][:,:5]\n        true_label=true_target[0][:,:5]\n        tmp= []\n\n        for jj in range(label.size(0)):\n            true_index_=true_index[jj]\n            true_label_=true_label[jj]\n            true=true_index_[true_label_>0]\n            tmp.append((predict[jj],true.tolist()))\n        \n        predict_label_and_marked_label_list.extend(tmp)\n    del score\n\n    dataset.train(True)\n    model.train()\n    \n    scores,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n    return (scores,prec_,recall_,_ss)\n\n\n\n\ndef main(**kwargs):\n    #\xe5\x8a\xa8\xe6\x80\x81\xe5\x8a\xa0\xe5\x85\xa8\xe8\x81\x8c\xe8\xa1\xb0\xe5\x87\x8f\n    origin_weight_decay=1e-5\n\n    opt.parse(kwargs,print_=False)\n    if opt.debug:import ipdb;ipdb.set_trace()\n\n#####################MultiModelALL0.4198########################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'CNNText_inception\',\'RCNN\',\'LSTMText\',\'CNNText_inception\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/CNNText_tmp_0.3803NTextBNDeep_0.37125473788\',\'checkpoints/CNNText_tmp_0.380390420742\',\'checkpoints/RCNN_word_0.373609030286\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/CNNText_tmp_0.376364647145\']\n#####-------------------------------------------------------#####\n\n##########################MultiModelALL0.42169202532381134###################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'CNNText_inception\',\n    # #\'RCNN\',\n    # \'LSTMText\',\'CNNText_inception\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410330780091\',\'checkpoints/CNNText_tmp_word_0.41096749885\',\n    # #\'checkpoints/RCNN_word_0.411511574999\',\n    # \'checkpoints/LSTMText_word_0.411994005382\',\'checkpoints/CNNText_tmp_char_0.402429167301\']\n#####-------------------------------------------------------#####\n\n\n############################MultiModelAll2w2c##################################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\n    # \'LSTMText\',\n    # \'CNNText_inception\',\n    # \'RCNN\',\n    # ]\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410330780091\',\n    # # \'checkpoints/CNNText_tmp_word_0.41096749885\',\n    # #\'checkpoints/RCNN_word_0.411511574999\',\n    # \'checkpoints/LSTMText_word_0.411994005382\',\n    # \'checkpoints/CNNText_tmp_char_0.402429167301\',\n    # \'checkpoints/RCNN_char_0.403710422571\'\n    # ]\n#####-------------------------------------------------------#####\n\n#########################MultiModel_0.4171859###############################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'LSTMText\',\'CNNText_inception\',\'RCNN\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/CNNText_tmp_0.376364647145\',\'checkpoints/RCNN_char_0.3456599248\']\n#####-------------------------------------------------------##### \n\n#################IForgot########################################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'LSTMText\',\'CNNText_inception\',\'RCNN\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/CNNText_tmp_0.376364647145\',\'checkpoints/RCNN_char_0.3456599248\']\n#####-------------------------------------------------------#####\n\n############augment##MultiModelAll_0.423535867989##########\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'RCNN\',\n    # #\'RCNN\',\n    # \'LSTMText\',\'RCNN\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410011182415\',\'checkpoints/RCNN_word_0.413446202556\',\n    # \'checkpoints/LSTMText_word_0.413681107036\',\n    # #\'checkpoints/RCNN_word_0.411511574999\',\n    # \'checkpoints/RCNN_char_0.398378946148\']\n#####-------------------------------------------------------#####\n\n##################MultiModelallfast_0.41652_val###############################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'FastText3\',\'LSTMText\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410011182415\',\'checkpoints/FastText3_word_0.40810787337\',\n    # \'checkpoints/LSTMText_word_0.413681107036\']\n    #\'checkpoints/RCNN_word_0.411511574999\']\n#####-------------------------------------------------------#####\n\n##############################MultiModelall-(\xe6\x9c\xaa\xe4\xbd\xbf\xe7\x94\xa8)###################################\n    # opt.model_names=[\'CNNText_inception\',\'FastText3\',\'RCNN\']\n    # opt.model_paths = [\'checkpoints/CNNText_tmp_word_0.41254624328\',\'checkpoints/FastText3_word_0.409160833419\',\n    # \'checkpoints/RCNN_word_0.413446202556\']\n#####-------------------------------------------------------#####\n\n    model = getattr(models,opt.model)(opt).cuda()\n    if opt.model_path:\n        model.load(opt.model_path)\n    print(model)\n\n    opt.parse(kwargs,print_=True)\n\n    vis.reinit(opt.env)\n    pre_loss=1.0\n    lr,lr2=opt.lr,opt.lr2\n    loss_function = getattr(models,opt.loss)()  \n    if opt.all:dataset = ZhihuALLData(opt.train_data_path,opt.labels_path,type_=opt.type_,augument=opt.augument)\n    # else :dataset = ZhihuData(opt.train_data_path,opt.labels_path,type_=opt.type_)\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = opt.shuffle,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n\n    optimizer = model.get_optimizer(opt.lr,opt.lr2,0)\n    loss_meter = tnt.meter.AverageValueMeter()\n    score_meter=tnt.meter.AverageValueMeter()\n    best_score = 0\n    # pre_score = 0\n\n    for epoch in range(opt.max_epoch):\n        loss_meter.reset()\n        score_meter.reset()\n        for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n            title,content,label = (Variable(title[0].cuda()),Variable(title[1].cuda())),(Variable(content[0].cuda()),Variable(content[1].cuda())),Variable(label.cuda())\n            optimizer.zero_grad()\n            score = model(title,content)\n            loss = loss_function(score,opt.weight*label.float())\n            loss_meter.add(loss.data[0])\n            loss.backward()\n            optimizer.step()\n\n            if ii%opt.plot_every ==opt.plot_every-1:\n                if os.path.exists(opt.debug_file):\n                    import ipdb\n                    ipdb.set_trace()\n\n                predict = score.data.topk(5,dim=1)[1].cpu().tolist()#(dim=1,descending=True)[1][:,:5].tolist()\n                true_target = label.data.float().topk(5,dim=1)#[1].cpu().tolist()#sort(dim=1,descending=True)\n                true_index=true_target[1][:,:5]\n                true_label=true_target[0][:,:5]\n                predict_label_and_marked_label_list=[]\n                for jj in range(label.size(0)):\n                    true_index_=true_index[jj]\n                    true_label_=true_label[jj]\n                    true=true_index_[true_label_>0]\n                    predict_label_and_marked_label_list.append((predict[jj],true.tolist()))\n                score_,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n                score_meter.add(score_)\n                vis.vis.text(\'prec:%s,recall:%s,score:%s,a:%s\' %(prec_,recall_,score_,_ss),win=\'tmp\')\n                vis.plot(\'scores\', score_meter.value()[0])\n                \n                #eval()\n                vis.plot(\'loss\', loss_meter.value()[0])\n                # \xe9\x9a\x8f\xe6\x9c\xba\xe5\xb1\x95\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\n                k = t.randperm(label.size(0))[0]\n                output=t.nn.functional.sigmoid(score)\n                # vis.vis.histogram(\n                #     output.data[k].view(-1).cpu(), win=u\'output_hist\', opts=dict\n                #     (title=\'output_hist\'))\n                # print ""epoch:%4d/%4d,time: %.8f,loss: %.8f "" %(epoch,ii,time.time()-start,loss_meter.value()[0])\n                   \n            if ii%opt.decay_every == opt.decay_every-1:   \n                del loss\n                scores,prec_,recall_ ,_ss= val(model,dataset)\n                if scores>best_score:\n                    best_score = scores\n                    best_path = model.save(name = str(scores),new=True)\n               \n                vis.log({\' epoch:\':epoch,\' lr: \':lr,\'scores\':scores,\'prec\':prec_,\'recall\':recall_,\'ss\':_ss,\'scores_train\':score_meter.value()[0],\'loss\':loss_meter.value()[0]})\n                if scores < best_score:\n                    model.load(best_path,change_opt=False)\n                    #lr = lr*opt.lr_decay\n                    #optimizer = model.get_optimizer(lr)\n                    lr = lr * opt.lr_decay\n                    # \xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe9\x99\x8d\xe4\xbd\x8e\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95:\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x9c\x89moment\xe7\xad\x89\xe7\x9a\x84\xe4\xb8\xa2\xe5\xa4\xb1\n                    if lr2==0:lr2=2e-4\n                    else : lr2 = lr2*opt.lr_decay\n                    optimizer = model.get_optimizer(lr,lr2,0)\n                    origin_weight_decay=5*origin_weight_decay\n                    # optimizer = model.get_optimizer(lr,lr2,0,weight_decay=origin_weight_decay)\n                    # origin_weight_decay=5*origin_weight_decay\n                    # for param_group in optimizer.param_groups:\n                    #     param_group[\'lr\']  *= opt.lr_decay\n                    #     if param_group[\'lr\'] ==0:\n                    #         param_group[\'lr\'] = 1e-4\n                        \n                \n                pre_loss = loss_meter.value()[0]\n                # pre_score = score_meter.value()[0]    \n                # pre_score = scores\n                loss_meter.reset()\n                score_meter.reset()\n                \n                if lr < opt.min_lr:\n                    break \n                 # model.save() \n\nif __name__==""__main__"":\n    fire.Fire()  \n'"
main.py,2,"b'#coding:utf8\nfrom config import opt\nimport models\nimport os\nimport tqdm\nfrom data.dataset import ZhihuData\nimport torch as t\nimport time\nimport fire\nimport torchnet as tnt\nfrom torch.utils import data\nfrom torch.autograd import Variable\nfrom utils.visualize import Visualizer\nfrom utils import get_score\nvis = Visualizer(opt.env)\n\ndef hook():pass\n\ndef val(model,dataset):\n    \'\'\'\n    \xe8\xae\xa1\xe7\xae\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0\n    \'\'\'\n\n    dataset.train(False)\n    model.eval()\n\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = False,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n    \n    predict_label_and_marked_label_list=[]\n    for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n        title,content,label = Variable(title.cuda(),volatile=True),\\\n                              Variable(content.cuda(),volatile=True),\\\n                              Variable(label.cuda(),volatile=True)\n        score = model(title,content)\n        # !TODO: \xe4\xbc\x98\xe5\x8c\x96\xe6\xad\xa4\xe5\xa4\x84\xe4\xbb\xa3\xe7\xa0\x81\n        #       1. append\n        #       2. for\xe5\xbe\xaa\xe7\x8e\xaf\n        #       3. topk \xe4\xbb\xa3\xe6\x9b\xbfsort\n\n        predict = score.data.topk(5,dim=1)[1].cpu().tolist()\n        true_target = label.data.float().topk(5,dim=1)\n        true_index=true_target[1][:,:5]\n        true_label=true_target[0][:,:5]\n        tmp= []\n\n        for jj in range(label.size(0)):\n            true_index_=true_index[jj]\n            true_label_=true_label[jj]\n            true=true_index_[true_label_>0]\n            tmp.append((predict[jj],true.tolist()))\n        \n        predict_label_and_marked_label_list.extend(tmp)\n    del score\n\n    dataset.train(True)\n    model.train()\n    \n    scores,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n    return (scores,prec_,recall_,_ss)\n\ndef main(**kwargs):\n    \'\'\'\n    \xe8\xae\xad\xe7\xbb\x83\xe5\x85\xa5\xe5\x8f\xa3\n    \'\'\'\n\n    opt.parse(kwargs,print_=False)\n    if opt.debug:import ipdb;ipdb.set_trace()\n\n    model = getattr(models,opt.model)(opt).cuda()\n    if opt.model_path:\n        model.load(opt.model_path)\n    print(model)\n\n    opt.parse(kwargs,print_=True)\n\n    vis.reinit(opt.env)\n    pre_loss=1.0\n    lr,lr2=opt.lr,opt.lr2\n    loss_function = getattr(models,opt.loss)()  \n\n    dataset = ZhihuData(opt.train_data_path,opt.labels_path,type_=opt.type_,augument=opt.augument)\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = opt.shuffle,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n\n    optimizer = model.get_optimizer(lr,opt.lr2,opt.weight_decay)\n    loss_meter = tnt.meter.AverageValueMeter()\n    score_meter=tnt.meter.AverageValueMeter()\n    best_score = 0\n\n    for epoch in range(opt.max_epoch):\n        loss_meter.reset()\n        score_meter.reset()\n        for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n            # \xe8\xae\xad\xe7\xbb\x83 \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n            title,content,label = Variable(title.cuda()),Variable(content.cuda()),Variable(label.cuda())\n            optimizer.zero_grad()\n            score = model(title,content)\n            loss = loss_function(score,opt.weight*label.float())\n            loss_meter.add(loss.data[0])\n            loss.backward()\n            optimizer.step()\n\n            if ii%opt.plot_every ==opt.plot_every-1:\n                ### \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n                if os.path.exists(opt.debug_file):\n                    import ipdb\n                    ipdb.set_trace()\n\n                predict = score.data.topk(5,dim=1)[1].cpu().tolist()\n                true_target = label.data.float().cpu().topk(5,dim=1)\n                true_index=true_target[1][:,:5]\n                true_label=true_target[0][:,:5]\n                predict_label_and_marked_label_list=[]\n                for jj in range(label.size(0)):\n                    true_index_=true_index[jj]\n                    true_label_=true_label[jj]\n                    true=true_index_[true_label_>0]\n                    predict_label_and_marked_label_list.append((predict[jj],true.tolist()))\n                score_,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n                score_meter.add(score_)\n                vis.vis.text(\'prec:%s,recall:%s,score:%s,a:%s\' %(prec_,recall_,score_,_ss),win=\'tmp\')\n                vis.plot(\'scores\', score_meter.value()[0])\n                \n                #eval()\n                vis.plot(\'loss\', loss_meter.value()[0])\n                k = t.randperm(label.size(0))[0]\n\n            if ii%opt.decay_every == opt.decay_every-1:   \n                # \xe8\xae\xa1\xe7\xae\x97\xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0,\xe5\xb9\xb6\xe7\x9b\xb8\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe8\xb0\x83\xe6\x95\xb4\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n                del loss\n                scores,prec_,recall_ ,_ss= val(model,dataset)\n                vis.log({\' epoch:\':epoch,\' lr: \':lr,\'scores\':scores,\'prec\':prec_,\'recall\':recall_,\'ss\':_ss,\'scores_train\':score_meter.value()[0],\'loss\':loss_meter.value()[0]})\n                \n                if scores>best_score:\n                    best_score = scores\n                    best_path = model.save(name = str(scores),new=True)\n               \n                if scores < best_score:\n                    model.load(best_path,change_opt=False)\n                    lr = lr * opt.lr_decay\n                    lr2= 2e-4 if lr2==0 else  lr2*0.8\n                    optimizer = model.get_optimizer(lr,lr2,0)                        \n                \n                pre_loss = loss_meter.value()[0]\n                loss_meter.reset()\n                score_meter.reset()\n\nif __name__==""__main__"":\n    fire.Fire()  \n'"
rep.py,2,"b'#encoding:utf-8\n# from torch.utils import data\nimport torch as t\nimport numpy as np\nfrom config import opt\nimport models\nimport json\nimport fire\nimport csv\nimport tqdm\nfrom torch.autograd import Variable\ndef load_data(type_=\'char\'):\n    id2label = t.load(opt.id2label)\n    question_d = np.load(opt.test_data_path)\n    index2qid = question_d[\'index2qid\'].item()\n    return (question_d[\'title_char\'],question_d[\'content_char\']),( question_d[\'title_word\'],question_d[\'content_word\']),index2qid,id2label\ndef write_csv(result,index2qid,labels):\n    f=open(opt.result_path, ""wa"")\n    csv_writer = csv.writer(f, dialect=""excel"")\n    rows=[0 for _ in range(result.shape[0])]\n    for i in range(result.shape[0]):\n        row=[index2qid[i]]+[labels[str(int(i_))] for i_ in result[i]]\n        rows[i]=row\n    csv_writer.writerows(rows)\ndef dotest(model,title,content):\n    title,content = (Variable(t.from_numpy(title[0]).long().cuda(),volatile=True),Variable(t.from_numpy(title[1]).long().cuda(),volatile=True)),(Variable(t.from_numpy(content[0]).long().cuda(),volatile=True),Variable(t.from_numpy(content[1]).long().cuda(),volatile=True))\n    score = model(title,content)\n    probs=t.nn.functional.sigmoid(score)\n    return probs.data.cpu().numpy()\n    \ndef test_one(file,data_):\n    test_data_title,test_data_content,index2qid,labels = data_\n    opt.model_path = file\n    model= models.MultiModelAll4zhihu(opt).cuda().eval()\n    \n    Num=len(test_data_title[0])\n    result=np.zeros((Num,1999))\n    for i in tqdm.tqdm(range(Num)):\n        if i%opt.batch_size==0 and i>0:\n            title=np.array(test_data_title[0][i-opt.batch_size:i]),np.array(test_data_title[1][i-opt.batch_size:i])\n            content=np.array(test_data_content[0][i-opt.batch_size:i]),np.array(test_data_content[1][i-opt.batch_size:i])\n            result[i-opt.batch_size:i,:]=dotest(model,title,content)  \n    if Num%opt.batch_size!=0:\n        title=np.array(test_data_title[0][opt.batch_size*(Num/opt.batch_size):]),np.array(test_data_title[1][opt.batch_size*(Num/opt.batch_size):])\n        content=np.array(test_data_content[0][opt.batch_size*(Num/opt.batch_size):]) ,np.array(test_data_content[1][opt.batch_size*(Num/opt.batch_size):]) \n        result[opt.batch_size*(Num/opt.batch_size):,:]=dotest(model,title,content) \n    # t.save(t.from_numpy(result).float(),opt.result_path)\n    return t.from_numpy(result).float()\n\nif __name__==\'__main__\':\n    \n    # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xb7\xaf\xe5\xbe\x84\xe4\xb8\x8d\xe4\xb8\x80\xe8\x87\xb4\xe8\xae\xb0\xe5\xbe\x97\xe4\xbf\xae\xe6\x94\xb9\xe8\xbf\x99\xe9\x87\x8c\n    opt.result_path = \'submission.csv\'\n    opt.test_data_path=\'test.npz\'\n    opt.id2label=\'id2label.json\'\n    \n    data_=load_data(type_=\'all\')\n    test_data_title,test_data_content,index2qid,labels = data_ \n    label2qid = labels\n    # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xb7\xaf\xe5\xbe\x84\xe4\xb8\x8d\xe4\xb8\x80\xe8\x87\xb4\xe8\xae\xb0\xe5\xbe\x97\xe4\xbf\xae\xe6\x94\xb9\xe8\xbf\x99\xe9\x87\x8c\n    model_paths = [  \n            \'MultiModelAll_word_0.416523282174\',\n            \'MultiModelAll_word_0.417185977233\',\n            \'MultiModelAll_word_0.419866393964\',\n            \'MultiModelAll_word_0.421331405593\',\n            \'MultiModelAll_word_0.421692025324\',\n            \'MultiModelAll_word_0.423535867989\',\n            \'MultiModelAll_word_0.419245894992\'\n            ] \n    \n    result= 0\n\n\n    for model_path in model_paths:\n        result += test_one(model_path,data_)\n    result_labels=(result).topk(5,1)[1]\n\n    ## \xe5\x86\x99csv \xe6\x8f\x90\xe4\xba\xa4\xe7\xbb\x93\xe6\x9e\x9c\n    rows = range(result.size(0))\n    for ii,item in enumerate(result_labels):\n        rows[ii] = [index2qid[ii]] + [label2qid[str(_)] for _ in item ]\n    import csv\n    with open(opt.result_path,\'w\') as f:\n        writer = csv.writer(f)\n        writer.writerows(rows)\n        \n        \n'"
test.1.py,2,"b'#encoding:utf-8\nfrom torch.utils import data\nimport torch as t\nimport numpy as np\nfrom config import opt\nimport models\nimport json\nimport fire\nimport csv\nimport tqdm\nfrom torch.autograd import Variable\ndef load_data(type_=\'char\'):\n    with open(opt.labels_path) as f:\n        labels_ = json.load(f)\n    print ""data_path: "",opt.test_data_path\n    question_d = np.load(opt.test_data_path)\n    if type_ == \'char\':\n        test_data_title,test_data_content =\\\n             question_d[\'title_char\'],question_d[\'content_char\']\n\n    elif type_ == \'word\':\n        test_data_title,test_data_content =\\\n             question_d[\'title_word\'],question_d[\'content_word\']\n\n    index2qid = question_d[\'index2qid\'].item()\n    return test_data_title,test_data_content,index2qid,labels_[\'id2label\']\ndef write_csv(result,index2qid,labels):\n    f=open(opt.result_path, ""wa"")\n    csv_writer = csv.writer(f, dialect=""excel"")\n    rows=[0 for _ in range(result.shape[0])]\n    for i in range(result.shape[0]):\n        row=[index2qid[i]]+[labels[str(int(i_))] for i_ in result[i]]\n        rows[i]=row\n    csv_writer.writerows(rows)\ndef dotest(model,title,content):\n    title,content = Variable(t.from_numpy(title).long().cuda(),volatile=True),Variable(t.from_numpy(content).long().cuda(),volatile=True)\n    score = model(title,content)\n    probs=t.nn.functional.sigmoid(score)\n    return probs.data.cpu().numpy()\n    \ndef main(**kwargs):\n    opt.parse(kwargs)\n    model = getattr(models,opt.model)(opt).cuda().eval()\n    if opt.model_path is not None:\n        model.load(opt.model_path)\n    opt.parse(kwargs)\n    \n    model = model.eval()\n    \n    test_data_title,test_data_content,index2qid,labels=load_data(type_=opt.type_)\n    Num=len(test_data_title)\n    print ""Num: "",Num\n    result=np.zeros((Num,1999))\n    for i in tqdm.tqdm(range(Num)):\n        if i%opt.batch_size==0 and i>0:\n            # import ipdb;ipdb.set_trace()\n            title=np.array(test_data_title[i-opt.batch_size:i])\n            content=np.array(test_data_content[i-opt.batch_size:i])\n            result[i-opt.batch_size:i,:]=dotest(model,title,content)  \n    if Num%opt.batch_size!=0:\n        title=np.array(test_data_title[opt.batch_size*(Num/opt.batch_size):])\n        content=np.array(test_data_content[opt.batch_size*(Num/opt.batch_size):]) \n        result[opt.batch_size*(Num/opt.batch_size):,:]=dotest(model,title,content) \n    t.save(t.from_numpy(result).float(),opt.result_path)\n    \nif __name__==\'__main__\':\n    fire.Fire()\n    \n    '"
test.3.py,2,"b'#encoding:utf-8\n# from torch.utils import data\n\n\'\'\'\n\xe4\xb8\x93\xe9\x97\xa8\xe4\xb8\xbamultimodel\xe8\x80\x8c\xe5\x86\x99 \xe7\x94\x9f\xe6\x88\x90pth\n\'\'\'\nimport torch as t\nimport numpy as np\nfrom config import opt\nimport models\nimport json\nimport fire\nimport csv\nimport tqdm\nfrom torch.autograd import Variable\ndef load_data(type_=\'char\'):\n    with open(opt.labels_path) as f:\n        labels_ = json.load(f)\n    question_d = np.load(opt.test_data_path)\n    # if type_ == \'char\':\n    #     test_data_title,test_data_content =\\\n    #          question_d[\'title_char\'],question_d[\'content_char\']\n\n    # elif type_ == \'word\':\n    #     test_data_title,test_data_content =\\\n    #          question_d[\'title_word\'],question_d[\'content_word\']\n\n    index2qid = question_d[\'index2qid\'].item()\n    return (question_d[\'title_char\'],question_d[\'content_char\']),( question_d[\'title_word\'],question_d[\'content_word\']),index2qid,labels_[\'id2label\']\ndef write_csv(result,index2qid,labels):\n    f=open(opt.result_path, ""wa"")\n    csv_writer = csv.writer(f, dialect=""excel"")\n    rows=[0 for _ in range(result.shape[0])]\n    for i in range(result.shape[0]):\n        row=[index2qid[i]]+[labels[str(int(i_))] for i_ in result[i]]\n        rows[i]=row\n    csv_writer.writerows(rows)\ndef dotest(model,title,content):\n    title,content = (Variable(t.from_numpy(title[0]).long().cuda(),volatile=True),Variable(t.from_numpy(title[1]).long().cuda(),volatile=True)),(Variable(t.from_numpy(content[0]).long().cuda(),volatile=True),Variable(t.from_numpy(content[1]).long().cuda(),volatile=True))\n    score = model(title,content)\n    probs=t.nn.functional.sigmoid(score)\n    return probs.data.cpu().numpy()\n    \ndef main(**kwargs):\n    opt.parse(kwargs)\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'LSTMText\',\'CNNText_inception\',\'RCNN\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/CNNText_tmp_0.376364647145\',\'checkpoints/RCNN_char_0.3456599248\']\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'CNNText_inception\',\n    # #\'RCNN\',\n    # \'LSTMText\',\'CNNText_inception\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410330780091\',\'checkpoints/CNNText_tmp_word_0.41096749885\',\n    # #\'checkpoints/RCNN_word_0.411511574999\',\n    # \'checkpoints/LSTMText_word_0.411994005382\',\'checkpoints/CNNText_tmp_char_0.402429167301\'] \n\n\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'RCNN\',\'LSTMText\',\'RCNN\',\'CNNText_inception\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.41124002492\',\'checkpoints/RCNN_word_0.411511574999\',\'checkpoints/LSTMText_word_0.411994005382\',\'checkpoints/RCNN_char_0.403710422571\',\'checkpoints/CNNText_tmp_char_0.402429167301\']\n    # opt.model_path=\'checkpoints/MultiModelAll2_word_0.425600838271\'\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\n    # \'LSTMText\',\n    # \'CNNText_inception\',\n    # \'RCNN\',\n    # ]\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410330780091\',\n    # # \'checkpoints/CNNText_tmp_word_0.41096749885\',\n    # #\'checkpoints/RCNN_word_0.411511574999\',\n    # \'checkpoints/LSTMText_word_0.411994005382\',\n    # \'checkpoints/CNNText_tmp_char_0.402429167301\',\n    # \'checkpoints/RCNN_char_0.403710422571\'\n    # ]\n    # opt.model_path=\'checkpoints/MultiModelAll_word_0.421331405593\'\n    #############################################################################################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'RCNN\',\n    # #\'RCNN\',\n    # \'LSTMText\',\'RCNN\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410011182415\',\'checkpoints/RCNN_word_0.413446202556\',\n    # \'checkpoints/LSTMText_word_0.413681107036\',\n    # #\'checkpoints/RCNN_word_0.411511574999\',\n    # \'checkpoints/RCNN_char_0.398378946148\'] \n\n    # opt.model_path=\'checkpoints/MultiModelAll_word_0.423535867989\'\n#########################################################################################################################\n\n\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'FastText3\',\'LSTMText\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410011182415\',\'checkpoints/FastText3_word_0.40810787337\',\n    # \'checkpoints/LSTMText_word_0.413681107036\']\n    # opt.model_path=\'checkpoints/MultiModelAll_word_0.416523282174\'\n\n\n\n\n################################################################################3\n\n    \n\n #   opt.model_names=[\'MultiCNNTextBNDeep\',\'FastText3\',\'LSTMText\',\'CNNText_inception\']\n  #  opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.41124002492\',\'checkpoints/FastText3_word_0.40810787337\',\'checkpoints/LSTMText_word_0.413681107036\',\'checkpoints/CNNText_tmp_char_0.402429167301\']\n  #  opt.model_path=\'checkpoints/MultiModelAll2_word_0.419088407885\'\n#####################################################################\n\n    opt.model_names=[\'CNNText_inception\',\'FastText3\',\'RCNN\']\n    opt.model_paths = [\'checkpoints/CNNText_tmp_word_0.41254624328\',\'checkpoints/FastText3_word_0.409160833419\',\n    \'checkpoints/RCNN_word_0.413446202556\']\n    opt.model_path=\'checkpoints/MultiModelAll_word_0.419245894992\'\n    \n    ################################################################\n    model = getattr(models,opt.model)(opt).cuda().eval()\n    if opt.model_path is not None:\n        model.load(opt.model_path)\n    model=model.eval()\n    opt.parse(kwargs)\n    \n    test_data_title,test_data_content,index2qid,labels=load_data(type_=opt.type_)\n    Num=len(test_data_title[0])\n    result=np.zeros((Num,1999))\n    for i in tqdm.tqdm(range(Num)):\n        if i%opt.batch_size==0 and i>0:\n            title=np.array(test_data_title[0][i-opt.batch_size:i]),np.array(test_data_title[1][i-opt.batch_size:i])\n            content=np.array(test_data_content[0][i-opt.batch_size:i]),np.array(test_data_content[1][i-opt.batch_size:i])\n            result[i-opt.batch_size:i,:]=dotest(model,title,content)  \n    if Num%opt.batch_size!=0:\n        title=np.array(test_data_title[0][opt.batch_size*(Num/opt.batch_size):]),np.array(test_data_title[1][opt.batch_size*(Num/opt.batch_size):])\n        content=np.array(test_data_content[0][opt.batch_size*(Num/opt.batch_size):]) ,np.array(test_data_content[1][opt.batch_size*(Num/opt.batch_size):]) \n        result[opt.batch_size*(Num/opt.batch_size):,:]=dotest(model,title,content) \n    t.save(t.from_numpy(result).float(),opt.result_path)\n    \nif __name__==\'__main__\':\n    fire.Fire()\n    \n    '"
data/__init__.py,0,b'from .fold_dataset import FoldData '
data/dataset.1.py,1,"b'#encoding:utf-8\nfrom torch.utils import data\nimport torch as t\nimport numpy as np\nimport random\nfrom glob import glob\nclass StackData(data.Dataset):\n    def __init__(self,data_root,labels_file):\n        self.data_files_path=glob(data_root+""*val.pth"")\n        self.model_num=len(self.data_files_path)\n        self.label_file_path=labels_file\n        self.data=t.zeros(100,1999*self.model_num)\n        for i in range(self.model_num):\n            self.data[:,i*1999:i*1999+1999]=t.sigmoid(t.load(self.data_files_path[i]).float()[:100]) \n        print self.data.size()\n        \nclass ZhihuData(data.Dataset):\n\n    def __init__(self,train_root,labels_file,type_=\'char\'):\n        \'\'\'\n        Dataset(\'/mnt/7/zhihu/ieee_zhihu_cup/train.npz\',\'/mnt/7/zhihu/ieee_zhihu_cup/a.json\')\n        \'\'\'\n        import json\n        with open(labels_file) as f:\n            labels_ = json.load(f)\n\n        # embedding_d = np.load(embedding_root)[\'vector\']\n        question_d = np.load(train_root)\n        self.type_=type_\n        if type_ == \'char\':\n            all_data_title,all_data_content =\\\n                 question_d[\'title_char\'],question_d[\'content_char\']\n\n        elif type_ == \'word\':\n            all_data_title,all_data_content =\\\n                 question_d[\'title_word\'],question_d[\'content_word\']\n\n        self.train_data = all_data_title[:-20000],all_data_content[:-20000]\n        self.val_data = all_data_title[-20000:],all_data_content[-20000:]\n\n        self.all_num = len(all_data_content)\n        # del all_data_title,all_data_content\n        \n        self.data_title,self.data_content = self.train_data\n        self.len_ = len(self.data_title)\n\n        self.index2qid = question_d[\'index2qid\'].item()\n        self.l_end=0\n        self.labels = labels_[\'d\']\n\n    # def augument(self,d):\n    #     \'\'\'\n    #     \xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xe4\xb9\x8b:   \xe9\x9a\x8f\xe6\x9c\xba\xe5\x81\x8f\xe7\xa7\xbb\n    #     \'\'\'\n    #     if self.type_==\'char\':\n    #         _index = (-8,8)\n    #     else :_index =(-5,5)\n    #     r = d.new(d.size()).fill_(0)\n    #     index = random.randint(-3,4)\n    #     if _index >0:\n    #         r[index:] = d[:-index]\n    #     else:\n    #         r[:-index] = d[index:]\n    #     return r\n\n    # def augument(self,d,type_=1):\n    #     if type_==1:\n    #         return self.shuffle(d)\n    #     else :\n    #         if self.type_==\'char\':\n    #             return self.dropout(d,p=0.6)\n\n    def shuffle(self,d):\n        return np.random.permutation(d.tolist())\n\n    def dropout(self,d,p=0.5):\n        len_ = len(d)\n        index = np.random.choice(len_,int(len_*p))\n        d[index]=0\n        return d     \n\n    def train(self, train=True):\n        if train:\n            self.data_title,self.data_content = self.train_data\n            self.l_end = 0\n        else:\n            self.data_title,self.data_content = self.val_data\n            self.l_end = self.all_num-200000\n        self.len_ = len(self.data_content)\n        return self\n\n    def __getitem__(self,index):\n        \'\'\'\n        for (title,content),label in dataloader:\n            train\n`       \n        \xe5\xbd\x93\xe4\xbd\xbf\xe7\x94\xa8char\xe6\x97\xb6\n        title: (50,)\n        content: (250,)\n        labels\xef\xbc\x9a(1999,)\n        \'\'\'\n        title,content =  self.data_title[index],self.data_content[index]\n        qid = self.index2qid[index+self.l_end]\n        labels = self.labels[qid]\n        data = (t.from_numpy(title).long(),t.from_numpy(content).long())\n        label_tensor = t.zeros(1999).scatter_(0,t.LongTensor(labels),1).long()\n        return data,label_tensor\n\n    def __len__(self):\n        return self.len_\n\nclass ZhihuALLData(data.Dataset):\n\n    def __init__(self,train_root,labels_file,type_=\'char\'):\n        \'\'\'\n        Dataset(\'/mnt/7/zhihu/ieee_zhihu_cup/train.npz\',\'/mnt/7/zhihu/ieee_zhihu_cup/a.json\')\n        \'\'\'\n        import json\n        with open(labels_file) as f:\n            labels_ = json.load(f)\n\n        # embedding_d = np.load(embedding_root)[\'vector\']\n        question_d = np.load(train_root)\n\n            # all_data_title,all_data_content =\\\n        all_char_title,all_char_content=      question_d[\'title_char\'],question_d[\'content_char\']\n            # all_data_title,all_data_content =\\\n        all_word_title,all_word_content=     question_d[\'title_word\'],question_d[\'content_word\']\n\n        self.train_data = (all_char_title[:-20000],all_char_content[:-20000]),( all_word_title[:-20000],all_word_content[:-20000])\n        self.val_data = (all_char_title[-20000:],all_char_content[-20000:]), (all_word_title[-20000:],all_word_content[-20000:])\n        self.all_num = len(all_char_title)\n        # del all_data_title,all_data_content\n        \n        self.data_title,self.data_content = self.train_data\n        self.len_ = len(self.data_title[0])\n\n        self.index2qid = question_d[\'index2qid\'].item()\n        self.l_end=0\n        self.labels = labels_[\'d\']\n\n\n    def train(self, train=True):\n        if train:\n            self.data_title,self.data_content = self.train_data\n            self.l_end = 0\n        else:\n            self.data_title,self.data_content = self.val_data\n            self.l_end = self.all_num-20000\n        self.len_ = len(self.data_content[0])\n        return self\n\n    def __getitem__(self,index):\n        \'\'\'\n        for (title,content),label in dataloader:\n            train\n`       \n        \xe5\xbd\x93\xe4\xbd\xbf\xe7\x94\xa8char\xe6\x97\xb6\n        title: (50,)\n        content: (250,)\n        labels\xef\xbc\x9a(1999,)\n        \'\'\'\n        char,word =  (self.data_titl'"
data/dataset.py,1,"b'#encoding:utf-8\nfrom torch.utils import data\nimport torch as t\nimport numpy as np\nimport random\nfrom glob import glob\nimport json\nclass StackData(data.Dataset):\n    def __init__(self,data_root,labels_file,val):\n        \'\'\'\n        data_root=""/data/text/zhihu/result/""\n        labels_file=""/home/a/code/pytorch/zhihu/ddd/labels.json""\n        val=""/home/a/code/pytorch/zhihu/ddd/val.npz""\n        \'\'\'\n        self.data_files_path=glob(data_root+""*val.pth"")\n        print ""model num: "",len(self.data_files_path)\n        self.model_num=len(self.data_files_path)\n        self.all_num=200000\n        self.val_num=20000\n        with open(labels_file) as f:\n            self.label_file= json.load(f)[\'d\']\n        self.val=np.load(val)[\'index2qid\'].item()\n        self.train_data=t.zeros(self.all_num-self.val_num,1999*self.model_num)\n        self.val_data=t.zeros(self.val_num,1999*self.model_num)\n        for i in range(self.model_num):\n            tmpdata=t.load(self.data_files_path[i]).float()\n            self.train_data[:,i*1999:i*1999+1999]=tmpdata[:self.all_num-self.val_num]\n            self.val_data[:,i*1999:i*1999+1999]=tmpdata[-1*self.val_num:]\n        self.data=self.train_data\n        self.len_=self.train_data.size(0)\n        self.len_end=0\n    def train(self, train=True):\n        if train:\n            self.data = self.train_data\n            self.len_end = 0\n        else:\n            self.data = self.val_data\n            self.len_end = self.all_num-self.val_num\n        self.len_ = len(self.data)\n        return self\n    def __getitem__(self,index):\n        qid=self.val[index+self.len_end+2999967-200000]\n        data=self.data[index]\n        label=self.label_file[qid]\n        data = data.float()\n        label_tensor = t.zeros(1999).scatter_(0,t.LongTensor(label),1).long()\n        return data,label_tensor\n    def __len__(self):\n        return self.len_  \n    \n    \nclass ZhihuData(data.Dataset):\n    \'\'\'\n    \xe4\xb8\xbb\xe8\xa6\x81\xe7\x94\xa8\xe5\x88\xb0\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    \'\'\'\n\n    def __init__(self,train_root,labels_file,type_=\'char\',augument=True):\n        \'\'\'\n        Dataset(\'/mnt/7/zhihu/ieee_zhihu_cup/train.npz\',\'/mnt/7/zhihu/ieee_zhihu_cup/a.json\')\n        \'\'\'\n        import json\n        with open(labels_file) as f:\n            labels_ = json.load(f)\n\n        # embedding_d = np.load(embedding_root)[\'vector\']\n        self.augument=augument\n        question_d = np.load(train_root)\n        self.type_=type_\n        if type_ == \'char\':\n            all_data_title,all_data_content =\\\n                 question_d[\'title_char\'],question_d[\'content_char\']\n\n        elif type_ == \'word\':\n            all_data_title,all_data_content =\\\n                 question_d[\'title_word\'],question_d[\'content_word\']\n\n        self.train_data = all_data_title[:-200000],all_data_content[:-200000]\n        self.val_data = all_data_title[-200000:],all_data_content[-200000:]\n\n        self.all_num = len(all_data_content)\n        # del all_data_title,all_data_content\n        \n        self.data_title,self.data_content = self.train_data\n        self.len_ = len(self.data_title)\n\n        self.index2qid = question_d[\'index2qid\'].item()\n        self.l_end=0\n        self.labels = labels_[\'d\']\n        \n        self.training=True\n\n   \n    def shuffle(self,d):\n        return np.random.permutation(d.tolist())\n\n    def dropout(self,d,p=0.5):\n        len_ = len(d)\n        index = np.random.choice(len_,int(len_*p))\n        d[index]=0\n        return d     \n\n    def train(self, train=True):\n        if train:\n            self.training=True\n            self.data_title,self.data_content = self.train_data\n            self.l_end = 0\n        else:\n            self.training=False\n            self.data_title,self.data_content = self.val_data\n            self.l_end = self.all_num-200000\n        self.len_ = len(self.data_content)\n        return self\n\n    def __getitem__(self,index):\n        title,content =  self.data_title[index],self.data_content[index]\n    \n        if self.training and self.augument :\n            augument=random.random()\n\n            if augument>0.5:\n                title = self.dropout(title,p=0.3)\n                content = self.dropout(content,p=0.7)\n            else:\n                title = self.shuffle(title)\n                content = self.shuffle(content)\n\n        qid = self.index2qid[index+self.l_end]\n        labels = self.labels[qid]\n        data = (t.from_numpy(title).long(),t.from_numpy(content).long())\n        label_tensor = t.zeros(1999).scatter_(0,t.LongTensor(labels),1).long()\n        return data,label_tensor\n\n    def __len__(self):\n        return self.len_\n\nclass ZhihuALLData(data.Dataset):\n    \'\'\'\n    \xe5\x90\x8c\xe6\x97\xb6\xe8\xbf\x94\xe5\x9b\x9eword\xe5\x92\x8cchar\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n    \'\'\'\n    def __init__(self,train_root,labels_file,type_=\'char\',augument=True):\n        self.augument=augument\n        import json\n        with open(labels_file) as f:\n            labels_ = json.load(f)\n\n        # embedding_d = np.load(embedding_root)[\'vector\']\n        question_d = np.load(train_root)\n\n            # all_data_title,all_data_content =\\\n        all_char_title,all_char_content=      question_d[\'title_char\'],question_d[\'content_char\']\n            # all_data_title,all_data_content =\\\n        all_word_title,all_word_content=     question_d[\'title_word\'],question_d[\'content_word\']\n\n        self.train_data = (all_char_title[:-200000],all_char_content[:-200000]),( all_word_title[:-200000],all_word_content[:-200000])\n        self.val_data = (all_char_title[-200000:],all_char_content[-200000:]), (all_word_title[-200000:],all_word_content[-200000:])\n        self.all_num = len(all_char_title)\n        # del all_data_title,all_data_content\n        \n        self.data_title,self.data_content = self.train_data\n        self.len_ = len(self.data_title[0])\n\n        self.index2qid = question_d[\'index2qid\'].item()\n        self.l_end=0\n        self.labels = labels_[\'d\']\n\n        self.training=True\n\n\n    def train(self, train=True):\n        if train:\n            self.training=True\n            self.data_title,self.data_content = self.train_data\n            self.l_end = 0\n        else:\n            self.training=False\n            self.data_title,self.data_content = self.val_data\n            self.l_end = self.all_num-200000\n        self.len_ = len(self.data_content[0])\n        return self\n    \n    def shuffle(self,d):\n        return np.random.permutation(d.tolist())\n\n    def dropout(self,d,p=0.5):\n        len_ = len(d)\n        index = np.random.choice(len_,int(len_*p))\n        d[index]=0\n        return d   \n\n    def __getitem__(self,index):\n        char,word =  (self.data_title[0][index],self.data_title[1][index]),  (self.data_content[0][index],self.data_content[1][index])\n\n        if self.training and self.augument :\n            augument=random.random()\n\n            if augument>0.5:\n                char = (self.dropout(char[0],p=0.3), self.dropout(char[1],p=0.7))\n                word =  (self.dropout(word[0],p=0.3), self.dropout(word[1],p=0.7))\n            else:\n                char = (self.shuffle(char[0]), self.shuffle(char[1]))\n                word =  (self.shuffle(word[0]), self.shuffle(word[1]))\n                # title = self.shuffle(title)\n                # content = self.shuffle(content)\n\n        qid = self.index2qid[index+self.l_end]\n        labels = self.labels[qid]\n        data = ((t.from_numpy(char[0]).long(),t.from_numpy(char[1]).long()),(t.from_numpy(word[0]).long(),t.from_numpy(word[1]).long()))\n        label_tensor = t.zeros(1999).scatter_(0,t.LongTensor(labels),1).long()\n        return data,label_tensor\n\n    def __len__(self):\n        return self.len_\n\n\nclass ALLFoldData(data.Dataset):\n    \'\'\'\n    \xe6\xb2\xa1\xe4\xbb\x80\xe4\xb9\x88\xe7\x94\xa8\n    \'\'\'\n\n    def __init__(self,train_root,labels_file,type_=\'char\',fold=0):\n        \'\'\'\n        Dataset(\'/mnt/7/zhihu/ieee_zhihu_cup/train.npz\',\'/mnt/7/zhihu/ieee_zhihu_cup/a.json\')\n        \'\'\'\n        import json\n        with open(labels_file) as f:\n            labels_ = json.load(f)\n        self.fold=fold \n        # embedding_d = np.load(embedding_root)[\'vector\']\n        question_d = np.load(train_root)\n\n            # all_data_title,all_data_content =\\\n        all_char_title,all_char_content=      question_d[\'title_char\'],question_d[\'content_char\']\n            # all_data_title,all_data_content =\\\n        all_word_title,all_word_content=     question_d[\'title_word\'],question_d[\'content_word\']\n\n        self.train_data = (all_char_title[:-200000],all_char_content[:-200000]),( all_word_title[:-200000],all_word_content[:-200000])\n        self.val_data = (all_char_title[-200000:],all_char_content[-200000:]), (all_word_title[-200000:],all_word_content[-200000:])\n        self.all_num = len(all_char_title)\n        # del all_data_title,all_data_content\n        \n        self.data_title,self.data_content = self.train_data\n        self.len_ = len(self.data_title[0])\n        self.training=True\n        self.index2qid = question_d[\'index2qid\'].item()\n        self.l_end=0\n        self.labels = labels_[\'d\']\n\n\n    def train(self, train=True):\n        if train:\n            self.training=True\n            self.data_title,self.data_content = self.train_data\n            self.l_end = 0\n        else:\n            self.training=False\n            self.data_title,self.data_content = self.val_data\n            self.l_end = self.all_num-200000\n        self.len_ = len(self.data_content[0])\n        return self\n\n    def __getitem__(self,index):\n        \'\'\'\n        for (title,content),label in dataloader:\n            train\n`       \n        \xe5\xbd\x93\xe4\xbd\xbf\xe7\x94\xa8char\xe6\x97\xb6\n        title: (50,)\n        content: (250,)\n        labels\xef\xbc\x9a(1999,)\n        \'\'\'\n        if self.training:\n            index = int(index/2)*2+self.fold\n            index = index%self.len_\n        char,word =  (self.data_title[0][index],self.data_title[1][index]),  (self.data_content[0][index],self.data_content[1][index])\n        qid = self.index2qid[index+self.l_end]\n        labels = self.labels[qid]\n        data = ((t.from_numpy(char[0]).long(),t.from_numpy(char[1]).long()),(t.from_numpy(word[0]).long(),t.from_numpy(word[1]).long()))\n        label_tensor = t.zeros(1999).scatter_(0,t.LongTensor(labels),1).long()\n        return data,label_tensor\n\n    def __len__(self):\n        return self.len_\n\nif __name__==""__main__"":\n    data_root=""/data/text/zhihu/result/""\n    labels_file=""/home/a/code/pytorch/zhihu/ddd/labels.json""\n    val_label=""/home/a/code/pytorch/zhihu/ddd/val.npz""\n    sb=StackData(data_root,labels_file,val_label)\n    for i in range(10):\n        print sb[i][0].size()\n        print sb[i][1]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
data/fold_dataset.py,1,"b'#encoding:utf-8\nfrom torch.utils import data\nimport torch as t\nimport numpy as np\nimport random\nfrom glob import glob\nimport json\n    \n    \nclass FoldData(data.Dataset):\n    \'\'\'\xe6\xb2\xa1\xe4\xbb\x80\xe4\xb9\x88\xe7\x94\xa8\'\'\'\n\n    def __init__(self,train_root,labels_file,type_=\'char\',fold=0):\n        \'\'\'\n        Dataset(\'/mnt/7/zhihu/ieee_zhihu_cup/train.npz\',\'/mnt/7/zhihu/ieee_zhihu_cup/a.json\')\n        \'\'\'\n        import json\n        with open(labels_file) as f:\n            labels_ = json.load(f)\n        self.fold =fold\n\n        # embedding_d = np.load(embedding_root)[\'vector\']\n        question_d = np.load(train_root)\n        self.type_=type_\n        if type_ == \'char\':\n            all_data_title,all_data_content =\\\n                 question_d[\'title_char\'],question_d[\'content_char\']\n\n        elif type_ == \'word\':\n            all_data_title,all_data_content =\\\n                 question_d[\'title_word\'],question_d[\'content_word\']\n\n        self.train_data = all_data_title[:-200000],all_data_content[:-200000]\n        self.val_data = all_data_title[-200000:],all_data_content[-200000:]\n\n        self.all_num = len(all_data_content)\n        # del all_data_title,all_data_content\n        \n        self.data_title,self.data_content = self.train_data\n        self.len_ = len(self.data_title)\n        \n        self.training=True\n\n        self.index2qid = question_d[\'index2qid\'].item()\n        self.l_end=0\n        self.labels = labels_[\'d\']\n\n    def shuffle(self,d):\n        return np.random.permutation(d.tolist())\n\n    def dropout(self,d,p=0.5):\n        len_ = len(d)\n        index = np.random.choice(len_,int(len_*p))\n        d[index]=0\n        return d     \n\n    def train(self, train=True):\n        if train:\n            self.training=True\n            self.data_title,self.data_content = self.train_data\n            self.l_end = 0\n        else:\n            self.training=False\n            self.data_title,self.data_content = self.val_data\n            self.l_end = self.all_num-200000\n        self.len_ = len(self.data_content)\n        return self\n\n    def __getitem__(self,index):\n        \'\'\'\n        for (title,content),label in dataloader:\n            train\n`       \n        \xe5\xbd\x93\xe4\xbd\xbf\xe7\x94\xa8char\xe6\x97\xb6\n        title: (50,)\n        content: (250,)\n        labels\xef\xbc\x9a(1999,)\n        \'\'\'\n        augument_type=0\n        if self.training: \n            index  = int(index/2)*2+self.fold\n            index = index%self.len_\n            augument=(index%2)        \n\n        title,content =  self.data_title[index],self.data_content[index]\n        qid = self.index2qid[index+self.l_end]\n        labels = self.labels[qid]\n        if self.training :\n            if augument==0:\n                title = self.dropout(title,p=0.3)\n                content = self.dropout(content,p=0.7)\n            else:\n                title = self.shuffle(title)\n                content = self.shuffle(content)\n        data = (t.from_numpy(title).long(),t.from_numpy(content).long())\n        label_tensor = t.zeros(1999).scatter_(0,t.LongTensor(labels),1).long()\n        return data,label_tensor\n\n    def __len__(self):\n        return self.len_\n\nclass ZhihuALLData(data.Dataset):\n\n    def __init__(self,train_root,labels_file,type_=\'char\'):\n        \'\'\'\n        Dataset(\'/mnt/7/zhihu/ieee_zhihu_cup/train.npz\',\'/mnt/7/zhihu/ieee_zhihu_cup/a.json\')\n        \'\'\'\n        import json\n        with open(labels_file) as f:\n            labels_ = json.load(f)\n\n        # embedding_d = np.load(embedding_root)[\'vector\']\n        question_d = np.load(train_root)\n\n            # all_data_title,all_data_content =\\\n        all_char_title,all_char_content=      question_d[\'title_char\'],question_d[\'content_char\']\n            # all_data_title,all_data_content =\\\n        all_word_title,all_word_content=     question_d[\'title_word\'],question_d[\'content_word\']\n\n        self.train_data = (all_char_title[:-200000],all_char_content[:-200000]),( all_word_title[:-200000],all_word_content[:-200000])\n        self.val_data = (all_char_title[-200000:],all_char_content[-200000:]), (all_word_title[-200000:],all_word_content[-200000:])\n        self.all_num = len(all_char_title)\n        # del all_data_title,all_data_content\n        \n        self.data_title,self.data_content = self.train_data\n        self.len_ = len(self.data_title[0])\n\n        self.index2qid = question_d[\'index2qid\'].item()\n        self.l_end=0\n        self.labels = labels_[\'d\']\n\n\n    def train(self, train=True):\n        if train:\n            self.data_title,self.data_content = self.train_data\n            self.l_end = 0\n        else:\n            self.data_title,self.data_content = self.val_data\n            self.l_end = self.all_num-200000\n        self.len_ = len(self.data_content[0])\n        return self\n\n    def __getitem__(self,index):\n        \'\'\'\n        for (title,content),label in dataloader:\n            train\n`       \n        \xe5\xbd\x93\xe4\xbd\xbf\xe7\x94\xa8char\xe6\x97\xb6\n        title: (50,)\n        content: (250,)\n        labels\xef\xbc\x9a(1999,)\n        \'\'\'\n        char,word =  (self.data_title[0][index],self.data_title[1][index]),  (self.data_content[0][index],self.data_content[1][index])\n        qid = self.index2qid[index+self.l_end]\n        labels = self.labels[qid]\n        data = ((t.from_numpy(char[0]).long(),t.from_numpy(char[1]).long()),(t.from_numpy(word[0]).long(),t.from_numpy(word[1]).long()))\n        label_tensor = t.zeros(1999).scatter_(0,t.LongTensor(labels),1).long()\n        return data,label_tensor\n\n    def __len__(self):\n        return self.len_\nif __name__==""__main__"":\n    data_root=""/data/text/zhihu/result/""\n    labels_file=""/home/a/code/pytorch/zhihu/ddd/labels.json""\n    val_label=""/home/a/code/pytorch/zhihu/ddd/val.npz""\n    sb=StackData(data_root,labels_file,val_label)\n    for i in range(10):\n        print sb[i][0].size()\n        print sb[i][1]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
del/main-all.back.py,2,"b'#coding:utf8\nfrom config import opt\nimport models\nimport os\nimport tqdm\nfrom data.dataset import ZhihuData,ZhihuALLData\nimport torch as t\nimport time\nimport fire\nimport torchnet as tnt\nfrom torch.utils import data\nfrom torch.autograd import Variable\nfrom utils.visualize import Visualizer\nfrom utils import get_score#,get_optimizer \nvis = Visualizer(opt.env)\n\'\'\'\n\xe8\xae\xad\xe7\xbb\x83 stack-attention \xe4\xb8\x8d\xe8\xbf\x87\xe8\xbf\x99\xe4\xba\x9b\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xaf\x94\xe8\xbe\x83\xe5\xb7\xae \xe6\xb2\xa1\xe7\x94\xa8\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\n\'\'\'\ndef hook():pass\n\ndef val(model,dataset):\n\n    dataset.train(False)\n    model.eval()\n\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = False,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n    \n    predict_label_and_marked_label_list=[]\n    for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n        title,content,label = (Variable(title[0].cuda()),Variable(title[1].cuda())),(Variable(content[0].cuda()),Variable(content[1].cuda())),Variable(label.cuda())\n        score = model(title,content)\n        # !TODO: \xe4\xbc\x98\xe5\x8c\x96\xe6\xad\xa4\xe5\xa4\x84\xe4\xbb\xa3\xe7\xa0\x81\n        #       1. append\n        #       2. for\xe5\xbe\xaa\xe7\x8e\xaf\n        #       3. topk \xe4\xbb\xa3\xe6\x9b\xbfsort\n\n        predict = score.data.topk(5,dim=1)[1].cpu().tolist()\n        true_target = label.data.float().topk(5,dim=1)#[1].cpu().tolist()#sort(dim=1,descending=True)\n        true_index=true_target[1][:,:5]\n        true_label=true_target[0][:,:5]\n        tmp= []\n\n        for jj in range(label.size(0)):\n            true_index_=true_index[jj]\n            true_label_=true_label[jj]\n            true=true_index_[true_label_>0]\n            tmp.append((predict[jj],true.tolist()))\n        \n        predict_label_and_marked_label_list.extend(tmp)\n    del score\n\n    dataset.train(True)\n    model.train()\n    \n    scores,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n    return (scores,prec_,recall_,_ss)\n\n\n\n\ndef main(**kwargs):\n    #\xe5\x8a\xa8\xe6\x80\x81\xe5\x8a\xa0\xe5\x85\xa8\xe8\x81\x8c\xe8\xa1\xb0\xe5\x87\x8f\n    origin_weight_decay=1e-5\n\n    opt.parse(kwargs,print_=False)\n    if opt.debug:import ipdb;ipdb.set_trace()\n####################################MultiModelALL0.4198###############################################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'CNNText_inception\',\'RCNN\',\'LSTMText\',\'CNNText_inception\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/CNNText_tmp_0.3803NTextBNDeep_0.37125473788\',\'checkpoints/CNNText_tmp_0.380390420742\',\'checkpoints/RCNN_word_0.373609030286\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/CNNText_tmp_0.376364647145\']\n######################################################################################################\n##########################0.42169202532381134#########\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'CNNText_inception\',\n    # #\'RCNN\',\n    # \'LSTMText\',\'CNNText_inception\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410330780091\',\'checkpoints/CNNText_tmp_word_0.41096749885\',\n    # #\'checkpoints/RCNN_word_0.411511574999\',\n    # \'checkpoints/LSTMText_word_0.411994005382\',\'checkpoints/CNNText_tmp_char_0.402429167301\']\n######################################\n\n\n############################2w2c##################################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\n    # \'LSTMText\',\n    # \'CNNText_inception\',\n    # \'RCNN\',\n    # ]\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410330780091\',\n    # # \'checkpoints/CNNText_tmp_word_0.41096749885\',\n    # #\'checkpoints/RCNN_word_0.411511574999\',\n    # \'checkpoints/LSTMText_word_0.411994005382\',\n    # \'checkpoints/CNNText_tmp_char_0.402429167301\',\n    # \'checkpoints/RCNN_char_0.403710422571\'\n    # ]\n#################################################################3\n\n#####################################0.41718################################################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'LSTMText\',\'CNNText_inception\',\'RCNN\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/CNNText_tmp_0.376364647145\',\'checkpoints/RCNN_char_0.3456599248\']\n################################################################################################# \n\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'LSTMText\',\'CNNText_inception\',\'RCNN\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/CNNText_tmp_0.376364647145\',\'checkpoints/RCNN_char_0.3456599248\']\n\n\n###############################################augment##MultiModelAll_word_0.423535867989#############################3\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'RCNN\',\n    # #\'RCNN\',\n    # \'LSTMText\',\'RCNN\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410011182415\',\'checkpoints/RCNN_word_0.413446202556\',\n    # \'checkpoints/LSTMText_word_0.413681107036\',\n    # #\'checkpoints/RCNN_word_0.411511574999\',\n    # \'checkpoints/RCNN_char_0.398378946148\']\n#######################################################333\n#######################################multmodelall-fast#########################33\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'FastText3\',\'LSTMText\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410011182415\',\'checkpoints/FastText3_word_0.40810787337\',\n    # \'checkpoints/LSTMText_word_0.413681107036\']\n    #\'checkpoints/RCNN_word_0.411511574999\']\n#########################################################################\n\n    opt.model_names=[\'CNNText_inception\',\'FastText3\',\'LSTMText\']\n    opt.model_paths = [\'checkpoints/CNNText_tmp_char_0.402429167301\',\'checkpoints/FastText3_char_0.379036104524\',\n    \'checkpoints/LSTMText_char_0.403192339135\']\n\n\n    model = getattr(models,opt.model)(opt).cuda()\n    if opt.model_path:\n        model.load(opt.model_path)\n    print(model)\n\n    opt.parse(kwargs,print_=True)\n\n    vis.reinit(opt.env)\n    pre_loss=1.0\n    lr,lr2=opt.lr,opt.lr2\n    loss_function = getattr(models,opt.loss)()  \n    if opt.all:dataset = ZhihuALLData(opt.train_data_path,opt.labels_path,type_=opt.type_,augument=opt.augument)\n    # else :dataset = ZhihuData(opt.train_data_path,opt.labels_path,type_=opt.type_)\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = opt.shuffle,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n\n    optimizer = model.get_optimizer(opt.lr,opt.lr2,0)\n    loss_meter = tnt.meter.AverageValueMeter()\n    score_meter=tnt.meter.AverageValueMeter()\n    best_score = 0\n    # pre_score = 0\n\n    for epoch in range(opt.max_epoch):\n        loss_meter.reset()\n        score_meter.reset()\n        for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n            title,content,label = (Variable(title[0].cuda()),Variable(title[1].cuda())),(Variable(content[0].cuda()),Variable(content[1].cuda())),Variable(label.cuda())\n            optimizer.zero_grad()\n            score = model(title,content)\n            loss = loss_function(score,opt.weight*label.float())\n            loss_meter.add(loss.data[0])\n            loss.backward()\n            optimizer.step()\n\n            if ii%opt.plot_every ==opt.plot_every-1:\n                if os.path.exists(opt.debug_file):\n                    import ipdb\n                    ipdb.set_trace()\n\n                predict = score.data.topk(5,dim=1)[1].cpu().tolist()#(dim=1,descending=True)[1][:,:5].tolist()\n                true_target = label.data.float().topk(5,dim=1)#[1].cpu().tolist()#sort(dim=1,descending=True)\n                true_index=true_target[1][:,:5]\n                true_label=true_target[0][:,:5]\n                predict_label_and_marked_label_list=[]\n                for jj in range(label.size(0)):\n                    true_index_=true_index[jj]\n                    true_label_=true_label[jj]\n                    true=true_index_[true_label_>0]\n                    predict_label_and_marked_label_list.append((predict[jj],true.tolist()))\n                score_,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n                score_meter.add(score_)\n                vis.vis.text(\'prec:%s,recall:%s,score:%s,a:%s\' %(prec_,recall_,score_,_ss),win=\'tmp\')\n                vis.plot(\'scores\', score_meter.value()[0])\n                \n                #eval()\n                vis.plot(\'loss\', loss_meter.value()[0])\n                # \xe9\x9a\x8f\xe6\x9c\xba\xe5\xb1\x95\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\n                k = t.randperm(label.size(0))[0]\n                output=t.nn.functional.sigmoid(score)\n                # vis.vis.histogram(\n                #     output.data[k].view(-1).cpu(), win=u\'output_hist\', opts=dict\n                #     (title=\'output_hist\'))\n                # print ""epoch:%4d/%4d,time: %.8f,loss: %.8f "" %(epoch,ii,time.time()-start,loss_meter.value()[0])\n                   \n            if ii%opt.decay_every == opt.decay_every-1:   \n                del loss\n                scores,prec_,recall_ ,_ss= val(model,dataset)\n                if scores>best_score:\n                    best_score = scores\n                    best_path = model.save(name = str(scores),new=True)\n               \n                vis.log({\' epoch:\':epoch,\' lr: \':lr,\'scores\':scores,\'prec\':prec_,\'recall\':recall_,\'ss\':_ss,\'scores_train\':score_meter.value()[0],\'loss\':loss_meter.value()[0]})\n                if scores < best_score:\n                    model.load(best_path,change_opt=False)\n                    #lr = lr*opt.lr_decay\n                    #optimizer = model.get_optimizer(lr)\n                    lr = lr * opt.lr_decay\n                    # \xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe9\x99\x8d\xe4\xbd\x8e\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95:\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x9c\x89moment\xe7\xad\x89\xe7\x9a\x84\xe4\xb8\xa2\xe5\xa4\xb1\n                    if lr2==0:lr2=2e-4\n                    else : lr2 = lr2*opt.lr_decay\n                    optimizer = model.get_optimizer(lr,lr2,0)\n                    origin_weight_decay=5*origin_weight_decay\n                    # optimizer = model.get_optimizer(lr,lr2,0,weight_decay=origin_weight_decay)\n                    # origin_weight_decay=5*origin_weight_decay\n                    # for param_group in optimizer.param_groups:\n                    #     param_group[\'lr\']  *= opt.lr_decay\n                    #     if param_group[\'lr\'] ==0:\n                    #         param_group[\'lr\'] = 1e-4\n                        \n                \n                pre_loss = loss_meter.value()[0]\n                # pre_score = score_meter.value()[0]    \n                # pre_score = scores\n                loss_meter.reset()\n                score_meter.reset()\n                \n                if lr < opt.min_lr:\n                    break \n                 # model.save() \n\nif __name__==""__main__"":\n    fire.Fire()  \n'"
del/main4model.py,2,"b'#coding:utf8\nfrom config import opt\nimport models\nimport os\nimport tqdm\nfrom data.dataset import ZhihuData,ZhihuALLData\nimport torch as t\nimport time\nimport fire\nimport torchnet as tnt\nfrom torch.utils import data\nfrom torch.autograd import Variable\nfrom utils.visualize import Visualizer\nfrom utils import get_score#,get_optimizer \nvis = Visualizer(opt.env)\n\'\'\'\n\xe8\xae\xad\xe7\xbb\x83 stack-attention \xe4\xb8\x8d\xe8\xbf\x87\xe8\xbf\x99\xe4\xba\x9b\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xaf\x94\xe8\xbe\x83\xe5\xb7\xae \xe6\xb2\xa1\xe7\x94\xa8\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\n\'\'\'\ndef hook():pass\n\ndef val(model,dataset):\n\n    dataset.train(False)\n    model.eval()\n\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = False,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n    \n    predict_label_and_marked_label_list=[]\n    for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n        title,content,label = (Variable(title[0].cuda()),Variable(title[1].cuda())),(Variable(content[0].cuda()),Variable(content[1].cuda())),Variable(label.cuda())\n        score = model(title,content)\n        # !TODO: \xe4\xbc\x98\xe5\x8c\x96\xe6\xad\xa4\xe5\xa4\x84\xe4\xbb\xa3\xe7\xa0\x81\n        #       1. append\n        #       2. for\xe5\xbe\xaa\xe7\x8e\xaf\n        #       3. topk \xe4\xbb\xa3\xe6\x9b\xbfsort\n\n        predict = score.data.topk(5,dim=1)[1].cpu().tolist()\n        true_target = label.data.float().topk(5,dim=1)#[1].cpu().tolist()#sort(dim=1,descending=True)\n        true_index=true_target[1][:,:5]\n        true_label=true_target[0][:,:5]\n        tmp= []\n\n        for jj in range(label.size(0)):\n            true_index_=true_index[jj]\n            true_label_=true_label[jj]\n            true=true_index_[true_label_>0]\n            tmp.append((predict[jj],true.tolist()))\n        \n        predict_label_and_marked_label_list.extend(tmp)\n    del score\n\n    dataset.train(True)\n    model.train()\n    \n    scores,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n    return (scores,prec_,recall_,_ss)\n\n\n\n\ndef main(**kwargs):\n    #\xe5\x8a\xa8\xe6\x80\x81\xe5\x8a\xa0\xe5\x85\xa8\xe8\x81\x8c\xe8\xa1\xb0\xe5\x87\x8f\n    origin_weight_decay=1e-5\n\n    opt.parse(kwargs,print_=False)\n    if opt.debug:import ipdb;ipdb.set_trace()\n####################################MultiModelALL0.4198###############################################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'CNNText_inception\',\'RCNN\',\'LSTMText\',\'CNNText_inception\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/CNNText_tmp_0.380390420742\',\'checkpoints/RCNN_word_0.373609030286\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/CNNText_tmp_0.376364647145\']\n######################################################################################################\n \n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'LSTMText\',\'CNNText_inception\',\'RCNN\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/CNNText_tmp_0.376364647145\',\'checkpoints/RCNN_char_0.3456599248\']\n    \n\n\n    model = getattr(models,opt.model)(opt).cuda()\n    if opt.model_path:\n        model.load(opt.model_path)\n    print(model)\n\n    opt.parse(kwargs,print_=True)\n\n    vis.reinit(opt.env)\n    pre_loss=1.0\n    lr,lr2=opt.lr,opt.lr2\n    loss_function = getattr(models,opt.loss)()  \n    if opt.all:dataset = ZhihuALLData(opt.train_data_path,opt.labels_path,type_=opt.type_)\n    else :dataset = ZhihuData(opt.train_data_path,opt.labels_path,type_=opt.type_)\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = opt.shuffle,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n\n    optimizer = model.get_optimizer(opt.lr,opt.lr2,0)\n    loss_meter = tnt.meter.AverageValueMeter()\n    score_meter=tnt.meter.AverageValueMeter()\n    best_score = 0\n    # pre_score = 0\n\n    for epoch in range(opt.max_epoch):\n        loss_meter.reset()\n        score_meter.reset()\n        for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n            title,content,label = (Variable(title[0].cuda()),Variable(title[1].cuda())),(Variable(content[0].cuda()),Variable(content[1].cuda())),Variable(label.cuda())\n            optimizer.zero_grad()\n            score = model(title,content)\n            loss = loss_function(score,opt.weight*label.float())\n            loss_meter.add(loss.data[0])\n            loss.backward()\n            optimizer.step()\n\n            if ii%opt.plot_every ==opt.plot_every-1:\n                if os.path.exists(opt.debug_file):\n                    import ipdb\n                    ipdb.set_trace()\n\n                predict = score.data.topk(5,dim=1)[1].cpu().tolist()#(dim=1,descending=True)[1][:,:5].tolist()\n                true_target = label.data.float().topk(5,dim=1)#[1].cpu().tolist()#sort(dim=1,descending=True)\n                true_index=true_target[1][:,:5]\n                true_label=true_target[0][:,:5]\n                predict_label_and_marked_label_list=[]\n                for jj in range(label.size(0)):\n                    true_index_=true_index[jj]\n                    true_label_=true_label[jj]\n                    true=true_index_[true_label_>0]\n                    predict_label_and_marked_label_list.append((predict[jj],true.tolist()))\n                score_,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n                score_meter.add(score_)\n                vis.vis.text(\'prec:%s,recall:%s,score:%s,a:%s\' %(prec_,recall_,score_,_ss),win=\'tmp\')\n                vis.plot(\'scores\', score_meter.value()[0])\n                \n                #eval()\n                vis.plot(\'loss\', loss_meter.value()[0])\n                # \xe9\x9a\x8f\xe6\x9c\xba\xe5\xb1\x95\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\n                k = t.randperm(label.size(0))[0]\n                output=t.nn.functional.sigmoid(score)\n                vis.vis.histogram(\n                    output.data[k].view(-1).cpu(), win=u\'output_hist\', opts=dict\n                    (title=\'output_hist\'))\n                # print ""epoch:%4d/%4d,time: %.8f,loss: %.8f "" %(epoch,ii,time.time()-start,loss_meter.value()[0])\n                   \n            if ii%opt.decay_every == opt.decay_every-1:   \n                del loss\n                scores,prec_,recall_ ,_ss= val(model,dataset)\n                if scores>best_score:\n                    best_score = scores\n                    best_path = model.save(name = str(scores),new=True)\n               \n                vis.log({\' epoch:\':epoch,\' lr: \':lr,\'scores\':scores,\'prec\':prec_,\'recall\':recall_,\'ss\':_ss,\'scores_train\':score_meter.value()[0],\'loss\':loss_meter.value()[0]})\n                if scores < best_score:\n                    model.load(best_path,change_opt=False)\n                    #lr = lr*opt.lr_decay\n                    #optimizer = model.get_optimizer(lr)\n                    lr = lr * opt.lr_decay\n                    # \xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe9\x99\x8d\xe4\xbd\x8e\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95:\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x9c\x89moment\xe7\xad\x89\xe7\x9a\x84\xe4\xb8\xa2\xe5\xa4\xb1\n                    if lr2==0:lr2=4e-4\n                    else : lr2 = lr2*opt.lr_decay\n                    optimizer = model.get_optimizer(lr,lr2,0)\n                    origin_weight_decay=5*origin_weight_decay\n                    # optimizer = model.get_optimizer(lr,lr2,0,weight_decay=origin_weight_decay)\n                    # origin_weight_decay=5*origin_weight_decay\n                    # for param_group in optimizer.param_groups:\n                    #     param_group[\'lr\']  *= opt.lr_decay\n                    #     if param_group[\'lr\'] ==0:\n                    #         param_group[\'lr\'] = 1e-4\n                        \n                \n                pre_loss = loss_meter.value()[0]\n                # pre_score = score_meter.value()[0]    \n                # pre_score = scores\n                loss_meter.reset()\n                score_meter.reset()\n                \n                if lr < opt.min_lr:\n                    break \n                 # model.save() \n\nif __name__==""__main__"":\n    fire.Fire()  \n'"
del/main_boost.py,2,"b'#coding:utf8\nfrom config import opt\nimport models\nimport os\nimport tqdm\nfrom data.dataset import ZhihuData,ZhihuALLData\nimport torch as t\nimport time\nimport fire\nimport torchnet as tnt\nfrom torch.utils import data\nfrom torch.autograd import Variable\nfrom utils.visualize import Visualizer\nfrom utils import get_score#,get_optimizer \nvis = Visualizer(opt.env)\n\'\'\'\n\xe8\xae\xad\xe7\xbb\x83 stack-attention \xe4\xb8\x8d\xe8\xbf\x87\xe8\xbf\x99\xe4\xba\x9b\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xaf\x94\xe8\xbe\x83\xe5\xb7\xae \xe6\xb2\xa1\xe7\x94\xa8\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\n\'\'\'\ndef hook():pass\n\ndef val(model,dataset):\n\n    dataset.train(False)\n    model.eval()\n\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = False,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n    \n    predict_label_and_marked_label_list=[]\n    for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n        title,content,label = (Variable(title[0].cuda()),Variable(title[1].cuda())),(Variable(content[0].cuda()),Variable(content[1].cuda())),Variable(label.cuda())\n        score = model(title,content)\n        # !TODO: \xe4\xbc\x98\xe5\x8c\x96\xe6\xad\xa4\xe5\xa4\x84\xe4\xbb\xa3\xe7\xa0\x81\n        #       1. append\n        #       2. for\xe5\xbe\xaa\xe7\x8e\xaf\n        #       3. topk \xe4\xbb\xa3\xe6\x9b\xbfsort\n\n        predict = score.data.topk(5,dim=1)[1].cpu().tolist()\n        true_target = label.data.float().topk(5,dim=1)#[1].cpu().tolist()#sort(dim=1,descending=True)\n        true_index=true_target[1][:,:5]\n        true_label=true_target[0][:,:5]\n        tmp= []\n\n        for jj in range(label.size(0)):\n            true_index_=true_index[jj]\n            true_label_=true_label[jj]\n            true=true_index_[true_label_>0]\n            tmp.append((predict[jj],true.tolist()))\n        \n        predict_label_and_marked_label_list.extend(tmp)\n    del score\n\n    dataset.train(True)\n    model.train()\n    \n    scores,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n    return (scores,prec_,recall_,_ss)\n\n\n\n\ndef main(**kwargs):\n    #\xe5\x8a\xa8\xe6\x80\x81\xe5\x8a\xa0\xe5\x85\xa8\xe8\x81\x8c\xe8\xa1\xb0\xe5\x87\x8f\n    origin_weight_decay=1e-5\n\n    opt.parse(kwargs,print_=False)\n    if opt.debug:import ipdb;ipdb.set_trace()\n\n###################################\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'CNNText_inception\',\n    # #\'RCNN\',\n    # \'LSTMText\',\'CNNText_inception\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410330780091\',\'checkpoints/CNNText_tmp_word_0.41096749885\',\n    # #\'checkpoints/RCNN_word_0.411511574999\',\n    # \'checkpoints/LSTMText_word_0.411994005382\',\'checkpoints/CNNText_tmp_char_0.402429167301\']\n######################################\n#     opt.model_names=[\'MultiCNNTextBNDeep\',\n#  #\'CNNText_inception\',\n#     #\'RCNN\',\n#     \'LSTMText_boost\',\n#     #\'CNNText_inception_boost\'\n#     ]\n#     opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410330780091\',\n#     # \'checkpoints/CNNText_tmp_word_0.41096749885\',\n#     #\'checkpoints/RCNN_word_0.411511574999\',\n#     \'checkpoints/LSTMText_word_0.381833388089\',\n#     #\'checkpoints/CNNText_tmp_0.376364647145\'\n#     ]\n#####################################################3\n    opt.model_names=[\'MultiCNNTextBNDeep\',\n \n    #\'RCNN\',\n    \'LSTMText\',\n    \'CNNText_inception\',\n    \'CNNText_inception-boost\'\n    ]\n    opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410330780091\',\n     \'checkpoints/LSTMText_word_0.381833388089\',\n    \'checkpoints/CNNText_tmp_0.380390420742\',\n    #\'checkpoints/RCNN_word_0.411511574999\',\n   \n    \'checkpoints/CNNText_tmp_0.376364647145\'\n    ]\n    # opt.model_path=\'checkpoints/BoostModel_word_0.412524727048\'\n\n#**************************************\n#############################################3\n\n    opt.model_names=[\'MultiCNNTextBNDeep\',\n    \'LSTMText\',\n    \'MultiCNNTextBNDeep-boost\'\n    ]\n    opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.410330780091\',\n     \'checkpoints/LSTMText_word_0.411994005382\',\n     None\n    ]\n    opt.model_path=\'checkpoints/BoostModel2_word_0.410618920827\'\n#*********************************************\n\n\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'LSTMText\',\'CNNText_inception\',\'RCNN\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/CNNText_tmp_0.376364647145\',\'checkpoints/RCNN_char_0.3456599248\']\n\n\n    model = getattr(models,opt.model)(opt).cuda()\n    # if opt.model_path:\n        # model.load(opt.model_path)\n    print(model)\n\n    opt.parse(kwargs,print_=True)\n\n    vis.reinit(opt.env)\n    pre_loss=1.0\n    lr,lr2=opt.lr,opt.lr2\n    loss_function = getattr(models,opt.loss)()  \n    if opt.all:dataset = ZhihuALLData(opt.train_data_path,opt.labels_path,type_=opt.type_)\n    else :dataset = ZhihuData(opt.train_data_path,opt.labels_path,type_=opt.type_)\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = opt.shuffle,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n\n    optimizer = model.get_optimizer(opt.lr,opt.lr2,0)\n    loss_meter = tnt.meter.AverageValueMeter()\n    score_meter=tnt.meter.AverageValueMeter()\n    best_score = 0\n    # pre_score = 0\n\n    for epoch in range(opt.max_epoch):\n        loss_meter.reset()\n        score_meter.reset()\n        for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n            title,content,label = (Variable(title[0].cuda()),Variable(title[1].cuda())),(Variable(content[0].cuda()),Variable(content[1].cuda())),Variable(label.cuda())\n            optimizer.zero_grad()\n            score = model(title,content)\n            loss = loss_function(score,opt.weight*label.float())\n            loss_meter.add(loss.data[0])\n            loss.backward()\n            optimizer.step()\n\n            if ii%opt.plot_every ==opt.plot_every-1:\n                if os.path.exists(opt.debug_file):\n                    import ipdb\n                    ipdb.set_trace()\n\n                predict = score.data.topk(5,dim=1)[1].cpu().tolist()#(dim=1,descending=True)[1][:,:5].tolist()\n                true_target = label.data.float().topk(5,dim=1)#[1].cpu().tolist()#sort(dim=1,descending=True)\n                true_index=true_target[1][:,:5]\n                true_label=true_target[0][:,:5]\n                predict_label_and_marked_label_list=[]\n                for jj in range(label.size(0)):\n                    true_index_=true_index[jj]\n                    true_label_=true_label[jj]\n                    true=true_index_[true_label_>0]\n                    predict_label_and_marked_label_list.append((predict[jj],true.tolist()))\n                score_,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n                score_meter.add(score_)\n                vis.vis.text(\'prec:%s,recall:%s,score:%s,a:%s\' %(prec_,recall_,score_,_ss),win=\'tmp\')\n                vis.plot(\'scores\', score_meter.value()[0])\n                \n                #eval()\n                vis.plot(\'loss\', loss_meter.value()[0])\n                # \xe9\x9a\x8f\xe6\x9c\xba\xe5\xb1\x95\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\n                k = t.randperm(label.size(0))[0]\n                output=t.nn.functional.sigmoid(score)\n                # vis.vis.histogram(\n                #     output.data[k].view(-1).cpu(), win=u\'output_hist\', opts=dict\n                #     (title=\'output_hist\'))\n                # print ""epoch:%4d/%4d,time: %.8f,loss: %.8f "" %(epoch,ii,time.time()-start,loss_meter.value()[0])\n                   \n            if ii%opt.decay_every == opt.decay_every-1:   \n                del loss\n                scores,prec_,recall_ ,_ss= val(model,dataset)\n                if scores>best_score:\n                    best_score = scores\n                    best_path = model.save(name = str(scores),new=True)\n               \n                vis.log({\' epoch:\':epoch,\' lr: \':lr,\'scores\':scores,\'prec\':prec_,\'recall\':recall_,\'ss\':_ss,\'scores_train\':score_meter.value()[0],\'loss\':loss_meter.value()[0]})\n                if scores < best_score:\n                    model.load(best_path,change_opt=False)\n                    #lr = lr*opt.lr_decay\n                    #optimizer = model.get_optimizer(lr)\n                    lr = lr * opt.lr_decay\n                    # \xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe9\x99\x8d\xe4\xbd\x8e\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95:\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x9c\x89moment\xe7\xad\x89\xe7\x9a\x84\xe4\xb8\xa2\xe5\xa4\xb1\n                    if lr2==0:lr2=2e-4\n                    else : lr2 = lr2*opt.lr_decay\n                    optimizer = model.get_optimizer(lr,lr2,0)\n                    origin_weight_decay=5*origin_weight_decay\n                    # optimizer = model.get_optimizer(lr,lr2,0,weight_decay=origin_weight_decay)\n                    # origin_weight_decay=5*origin_weight_decay\n                    # for param_group in optimizer.param_groups:\n                    #     param_group[\'lr\']  *= opt.lr_decay\n                    #     if param_group[\'lr\'] ==0:\n                    #         param_group[\'lr\'] = 1e-4\n                        \n                \n                pre_loss = loss_meter.value()[0]\n                # pre_score = score_meter.value()[0]    \n                # pre_score = scores\n                loss_meter.reset()\n                score_meter.reset()\n                \n                if lr < opt.min_lr:\n                    break \n                 # model.save() \n\nif __name__==""__main__"":\n    fire.Fire()  \n'"
del/main_fold.py,2,"b'#coding:utf8\nfrom config import opt\nimport models\nimport os\nimport tqdm\nfrom data import FoldData \nimport torch as t\nimport time\nimport fire\nimport torchnet as tnt\nfrom torch.utils import data\nfrom torch.autograd import Variable\nfrom utils.visualize import Visualizer\nfrom utils import get_score#,get_optimizer \nvis = Visualizer(opt.env)\n\ndef hook():pass\n\ndef val(model,dataset):\n\n    dataset.train(False)\n    model.eval()\n\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = False,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n    \n    predict_label_and_marked_label_list=[]\n    for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n        title,content,label = Variable(title.cuda(),volatile=True),\\\n                              Variable(content.cuda(),volatile=True),\\\n                              Variable(label.cuda(),volatile=True)\n        score = model(title,content)\n        # !TODO: \xe4\xbc\x98\xe5\x8c\x96\xe6\xad\xa4\xe5\xa4\x84\xe4\xbb\xa3\xe7\xa0\x81\n        #       1. append\n        #       2. for\xe5\xbe\xaa\xe7\x8e\xaf\n        #       3. topk \xe4\xbb\xa3\xe6\x9b\xbfsort\n\n        predict = score.data.topk(5,dim=1)[1].cpu().tolist()\n        true_target = label.data.float().topk(5,dim=1)#[1].cpu().tolist()#sort(dim=1,descending=True)\n        true_index=true_target[1][:,:5]\n        true_label=true_target[0][:,:5]\n        tmp= []\n\n        for jj in range(label.size(0)):\n            true_index_=true_index[jj]\n            true_label_=true_label[jj]\n            true=true_index_[true_label_>0]\n            tmp.append((predict[jj],true.tolist()))\n        \n        predict_label_and_marked_label_list.extend(tmp)\n    del score\n\n    dataset.train(True)\n    model.train()\n    \n    scores,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n    return (scores,prec_,recall_,_ss)\n\n\n\n\ndef main(**kwargs):\n    #\xe5\x8a\xa8\xe6\x80\x81\xe5\x8a\xa0\xe5\x85\xa8\xe8\x81\x8c\xe8\xa1\xb0\xe5\x87\x8f\n    origin_weight_decay=1e-5\n\n    opt.parse(kwargs,print_=False)\n    if opt.debug:import ipdb;ipdb.set_trace()\n\n    model = getattr(models,opt.model)(opt).cuda()\n    if opt.model_path:\n        model.load(opt.model_path)\n    print(model)\n\n    opt.parse(kwargs,print_=True)\n\n    vis.reinit(opt.env)\n    pre_loss=1.0\n    lr,lr2=opt.lr,opt.lr2\n    loss_function = getattr(models,opt.loss)()  \n    # if opt.all:dataset = ZhihuALLData(opt.train_data_path,opt.labels_path,type_=opt.type_)\n    dataset = FoldData(opt.train_data_path,opt.labels_path,type_=opt.type_,fold=opt.fold)\n    dataloader = data.DataLoader(dataset,\n                    batch_size = opt.batch_size,\n                    shuffle = opt.shuffle,\n                    num_workers = opt.num_workers,\n                    pin_memory = True\n                    )\n\n    optimizer = model.get_optimizer(lr,opt.lr2,opt.weight_decay)\n    loss_meter = tnt.meter.AverageValueMeter()\n    score_meter=tnt.meter.AverageValueMeter()\n    best_score = 0\n    # pre_score = 0\n\n    for epoch in range(opt.max_epoch):\n        loss_meter.reset()\n        score_meter.reset()\n        for ii,((title,content),label) in tqdm.tqdm(enumerate(dataloader)):\n            title,content,label = Variable(title.cuda()),Variable(content.cuda()),Variable(label.cuda())\n            optimizer.zero_grad()\n            score = model(title,content)\n            loss = loss_function(score,opt.weight*label.float())\n            loss_meter.add(loss.data[0])\n            loss.backward()\n            optimizer.step()\n\n            if ii%opt.plot_every ==opt.plot_every-1:\n                if os.path.exists(opt.debug_file):\n                    import ipdb\n                    ipdb.set_trace()\n\n                predict = score.data.topk(5,dim=1)[1].cpu().tolist()#(dim=1,descending=True)[1][:,:5].tolist()\n                true_target = label.data.float().topk(5,dim=1)#[1].cpu().tolist()#sort(dim=1,descending=True)\n                true_index=true_target[1][:,:5]\n                true_label=true_target[0][:,:5]\n                predict_label_and_marked_label_list=[]\n                for jj in range(label.size(0)):\n                    true_index_=true_index[jj]\n                    true_label_=true_label[jj]\n                    true=true_index_[true_label_>0]\n                    predict_label_and_marked_label_list.append((predict[jj],true.tolist()))\n                score_,prec_,recall_,_ss=get_score(predict_label_and_marked_label_list)\n                score_meter.add(score_)\n                vis.vis.text(\'prec:%s,recall:%s,score:%s,a:%s\' %(prec_,recall_,score_,_ss),win=\'tmp\')\n                vis.plot(\'scores\', score_meter.value()[0])\n                \n                #eval()\n                vis.plot(\'loss\', loss_meter.value()[0])\n                # \xe9\x9a\x8f\xe6\x9c\xba\xe5\xb1\x95\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\n                k = t.randperm(label.size(0))[0]\n                output=t.nn.functional.sigmoid(score)\n                # vis.vis.histogram(\n                #     output.data[k].view(-1).cpu(), win=u\'output_hist\', opts=dict\n                #     (title=\'output_hist\'))\n                # print ""epoch:%4d/%4d,time: %.8f,loss: %.8f "" %(epoch,ii,time.time()-start,loss_meter.value()[0])\n                   \n            if ii%opt.decay_every == opt.decay_every-1:   \n                del loss\n                scores,prec_,recall_ ,_ss= val(model,dataset)\n                if scores>best_score:\n                    best_score = scores\n                    best_path = model.save(name = str(scores),new=True)\n               \n                vis.log({\' epoch:\':epoch,\' lr: \':lr,\'scores\':scores,\'prec\':prec_,\'recall\':recall_,\'ss\':_ss,\'scores_train\':score_meter.value()[0],\'loss\':loss_meter.value()[0]})\n                if scores < best_score:\n                    model.load(best_path,change_opt=False)\n                    #lr = lr*opt.lr_decay\n                    #optimizer = model.get_optimizer(lr)\n                    lr = lr * opt.lr_decay\n                    # \xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe9\x99\x8d\xe4\xbd\x8e\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95:\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x9c\x89moment\xe7\xad\x89\xe7\x9a\x84\xe4\xb8\xa2\xe5\xa4\xb1\n                    if lr2==0:lr2=2e-4\n                    else : lr2 = lr2*0.8\n                    optimizer = model.get_optimizer(lr,lr2,0)\n                    origin_weight_decay=5*origin_weight_decay\n                    # optimizer = model.get_optimizer(lr,lr2,0,weight_decay=origin_weight_decay)\n                    # origin_weight_decay=5*origin_weight_decay\n                    # for param_group in optimizer.param_groups:\n                    #     param_group[\'lr\']  *= opt.lr_decay\n                    #     if param_group[\'lr\'] ==0:\n                    #         param_group[\'lr\'] = 1e-4\n                        \n                \n                pre_loss = loss_meter.value()[0]\n                # pre_score = score_meter.value()[0]    \n                # pre_score = scores\n                loss_meter.reset()\n                score_meter.reset()\n                \n                if lr < opt.min_lr:\n                    break \n                 # model.save() \n\nif __name__==""__main__"":\n    fire.Fire()  \n'"
del/search2.1.py,0,"b""import json\nimport os\nimport sys\nfrom utils import get_score\nimport torch as t \nimport numpy as np\nfile = '/mnt/zhihu/data/val_best.pth'\nr = t.load(file)\n# b = t.load(file2)\n# c = t.load(file3)\nimport time\ntest_data_path='/mnt/zhihu/data/val.npz'\nindex2qid = np.load(test_data_path)['index2qid'].item()\nlabel_path='/mnt/zhihu/data/labels.json'\nwith open(label_path) as f: \n      labels_info = json.load(f)\nqid2label = labels_info['d']\ntrue_labels = [qid2label[index2qid[2999967-200000+ii]] for ii in range(len(r))]\nlen(true_labels)\nprevious_best_score=0.42\n# large_index=[560, 753, 1208, 609, 741, 915, 879, 458, 1443, 1783]\nlarge_index=[458, 1443, 1783]\ndef target(args):\n        weight = args\n        aaa = t.ones(1999)\n        for ii,_ in enumerate(large_index):\n              aaa[_] = args[ii]\n      #   aaa[0],aaa[1],aaa[2],aaa[3],aaa[4] = args\n        weight = aaa.view(1,-1).expand(200000,1999)\n        r2 = weight*(r.float())\n        result = r2.topk(5,1)[1]\n        predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result,true_labels)]\n        score,_,_,_ = get_score(predict_label_and_marked_label_list)\n      #   if score>previous_best_score:\n        print score\n            # previous_best_score = score\n            # with open(str(score) ,'wb') as f:\n            #     pickle.dump(args,f)\n            \n        return -score\nfrom hyperopt import hp, fmin, rand, tpe, space_eval\nlist_space = [hp.uniform('b%s'%_,0.5,2) for _ in range(10)]\nbest = fmin(target,list_space,algo=tpe.suggest,max_evals=10)\nbest = fmin(target,list_space,algo=tpe.suggest,max_evals=10)\nlist_space = [hp.uniform('w1',0,2),hp.uniform('w2',0,2)]\nbest = fmin(target,list_space,algo=tpe.suggest,max_evals=50)\n%hist -f search2.py\n\n\n{'w0': 0.7261578854014094,\n 'w1': 0.6729932326871956,\n 'w2': 0.9624749042037957,\n 'w3': 0.8998425892602284,\n 'w4': 0.6488650207895496,\n 'w5': 0.5219741148509414,\n 'w6': 0.6845486024566358}\n\nIn [26]: val\nOut[26]: \n['CNNText_tmp0.4024_char_val.pth',\n 'DeepText0.4103_word_val.pth',\n 'CNNText_tmp0.4109_word_val.pth',\n 'LSTMText0.4119_word_val.pth',\n 'RCNN_0.4037_char_val.pth',\n 'LSTMText0.4031_char_val.pth',\n 'RCNN_0.4115_word_val.pth']\n"""
del/search2.py,0,"b""import json\nimport os\nimport sys\nfrom utils import get_score\nimport torch as t \nimport numpy as np\nfile1='/mnt/zhihu/data/rcnndeep4115_word_val.pth'\nfile2='/mnt/zhihu/data/rccndeep4037_char_val.pth'\nfile3='/mnt/zhihu/data/multicnntextbndeep40705_word_val.pth'\na = t.sigmoid(t.load(file1).float())\nb = t.sigmoid(t.load(file2).float())\nc = t.sigmoid(t.load(file3).float())\nimport time\ntest_data_path='/mnt/zhihu/data/val.npz'\nindex2qid = np.load(test_data_path)['index2qid'].item()\nlabel_path='/mnt/zhihu/data/labels.json'\nwith open(label_path) as f: \n      labels_info = json.load(f)\nqid2label = labels_info['d']\ntrue_labels = [qid2label[index2qid[2999967-200000+ii]] for ii in range(len(a))]\nlen(true_labels)\ndef target(args):\n        w1,w2,w3 = args\n        r = a + b*w1 +c*w2 + d*w3\n        result = r.topk(5,1)[1]\n        predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result,true_labels)]\n        score,_,_,_ = get_score(predict_label_and_marked_label_list)\n        print (args,score,_)#list_space = [hp.uniform('a',0,1),hp.uniform('b',0,1)]\n        return -score\nfrom hyperopt import hp, fmin, rand, tpe, space_eval\nlist_space = [hp.uniform('a',0,1),hp.uniform('b',0,1)]\nbest = fmin(target,list_space,algo=tpe.suggest,max_evals=10)\nbest = fmin(target,list_space,algo=tpe.suggest,max_evals=10)\nlist_space = [hp.uniform('w1',0,2),hp.uniform('w2',0,2)]\nbest = fmin(new_target,list_space,algo=tpe.suggest,max_evals=50)\n%hist -f search2.py\n# ((0.9737992669297721, 0.17478779579719494), 0.4223127848108588, [116814, 64995, 42622, 29421, 21724])\n# 0.42238923442022624,\n#((0.8951200869708624, 0.13488758907249945), 0.42242919538129015, [116791, 64994, 42467, 29659, 21763])\n# ([0.9903428611407182, 0.8213389405505421, 1.032499220786336], 0.4241382314016921, [117305, 65592, 42492, 29488, 21876])\n"""
del/search_all.py,0,"b'import json\nimport os\nimport sys\nfrom utils import get_score\nimport torch as t \nimport numpy as np\nfrom glob import glob\nimport pickle\nfrom glob import glob\ndata_root=""/data_ssd/zhihu/result/search_result/""\nfiles_path=glob(data_root+""*_first.pth"")\nfiles_path.sort()\nprint len(files_path)\nfiles_weight1=[]\ninitial_weight=[]\nvar=[]\nfor file in files_path:\n    if \'multimodel\' in file: \n        files_weight1.append(file)\n        initial_weight.append(10)\n        var.append(2)\n        #initial_weight.append(1)\n    elif ""weight5"" in file:\n        files_weight1.append(file)\n        initial_weight.append(0.15)\n        var.append(0.05)\n    else:\n        files_weight1.append(file)\n        initial_weight.append(1)\n        var.append(0.2)\n        \nprint len(files_weight1)\nfor f,w,vr in zip(files_weight1,initial_weight,var):\n    print f,w,vr\nmodel_num=6 if len(files_weight1)>6 else len(files_weight1) \nprobs=[t.load(r).float() for r in files_weight1[:model_num]]\ntest_data_path=\'/home/a/code/pytorch/zhihu/ddd/val.npz\'\nindex2qid = np.load(test_data_path)[\'index2qid\'].item()\nlabel_path=""/home/a/code/pytorch/zhihu/ddd/labels.json""\nwith open(label_path) as f: \n      labels_info = json.load(f)\nqid2label = labels_info[\'d\']\ntrue_labels = [qid2label[index2qid[2999967-200000+ii]] for ii in range(200000)]\ndel labels_info\ndel qid2label\ndel index2qid\ndef target(args):\n    r=0\n    for r_,k_ in enumerate(args):\n        if r_<model_num:\n            r +=k_*probs[r_]\n        else:\n            tmp=t.load(files_path[r_]).cuda().float()\n            r=r+k_*tmp.cpu()\n    result = r.topk(5,1)[1]\n    predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result,true_labels)]\n    score,_,_,rrs = get_score(predict_label_and_marked_label_list)\n    print (args,score,rrs)#list_space = [hp.uniform(\'a\',0,1),hp.uniform(\'b\',0,1)]\n    return -rrs[0]\nmax_evals=250\nfrom hyperopt import hp, fmin, rand, tpe, space_eval\nlist_space = [hp.normal(\'a\'+str(rr),initial_weight[rr],var[rr]) for rr in range(len(files_weight1))]\nfrom hyperopt import Trials\ntrials_to_keep=Trials()\nbest = fmin(target,list_space,algo=tpe.suggest,max_evals=max_evals, trials = trials_to_keep)\noutput = open(\'trials_to_keep_all_first\'+\'.pkl\', \'wb\')\npickle.dump(trials_to_keep, output)\noutput.close()\n\nprint best\nresults=0\nresult_path=""/data_ssd/zhihu/result/search_result/""\nfor ii in range(len(files_weight1)):\n    if ii<model_num:\n        results +=probs[ii]*best[\'a\'+str(ii)]\n    else:\n        tmp=t.load(files_path[ii]).cuda().float()\n        results +=tmp*best[\'a\'+str(ii)]\nt.save(results.float(),result_path+""search_all_first.pth"")\n\n'"
del/search_aug_noMultimodel_weight1.py,0,"b'import json\nimport os\nimport sys\nfrom utils import get_score\nimport torch as t \nimport numpy as np\nfrom glob import glob\nimport pickle\nfrom glob import glob\ndata_root=""/data_ssd/zhihu/result/tmp/""\nfiles_path=glob(data_root+""*val.pth"")\nfiles_path.sort()\nprint len(files_path)\nfiles_weight1=[]\ninitial_weight=[]\nfor file in files_path:\n    if \'MultiModel\' not in file and \'weight5\' not in file:\n        files_weight1.append(file)\n        initial_weight.append(1)\nprint len(files_weight1)\nfor f,w in zip(files_weight1,initial_weight):\n    print f,w\nmodel_num=10\nprobs=[t.load(r).float() for r in files_weight1[:model_num]]\ntest_data_path=\'/home/a/code/pytorch/zhihu/ddd/val.npz\'\nindex2qid = np.load(test_data_path)[\'index2qid\'].item()\nlabel_path=""/home/a/code/pytorch/zhihu/ddd/labels.json""\nwith open(label_path) as f: \n      labels_info = json.load(f)\nqid2label = labels_info[\'d\']\ntrue_labels = [qid2label[index2qid[2999967-200000+ii]] for ii in range(200000)]\ndel labels_info\ndel qid2label\ndel index2qid\ndef target(args):\n    r=0\n    for r_,k_ in enumerate(args):\n        if r_<model_num:\n            r +=k_*probs[r_]\n        else:\n            tmp=t.load(files_path[r_]).cuda().float()\n            r=r+k_*tmp.cpu()\n    result = r.topk(5,1)[1]\n    predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result,true_labels)]\n    score,_,_,rrs= get_score(predict_label_and_marked_label_list)\n    print (args,score,rrs)#list_space = [hp.uniform(\'a\',0,1),hp.uniform(\'b\',0,1)]\n    return -rrs[0]\nmax_evals=100\nfrom hyperopt import hp, fmin, rand, tpe, space_eval\nlist_space = [hp.normal(\'a\'+str(rr),1,0.2) for rr in range(len(files_weight1))]\nfrom hyperopt import Trials\ntrials_to_keep=Trials()\nbest = fmin(target,list_space,algo=tpe.suggest,max_evals=max_evals, trials = trials_to_keep)\noutput = open(\'trials_to_keep__aug_nomulti_weight1\'+\'.pkl\', \'wb\')\npickle.dump(trials_to_keep, output)\noutput.close()\n\nprint best\nresults=0\nresult_path=""/data_ssd/zhihu/result/search_result/""\nfor ii in range(len(files_weight1)):\n    if ii<model_num:\n        results +=probs[ii]*best[\'a\'+str(ii)]\n    else:\n        tmp=t.load(files_path[ii]).cuda().float()\n        results +=tmp*best[\'a\'+str(ii)]\nt.save(results.float(),result_path+""search_nomulti_weight1_result_first.pth"")\n\n'"
del/search_multimodel.py,0,"b'import json\nimport os\nimport sys\nfrom utils import get_score\nimport torch as t \nimport numpy as np\nfrom glob import glob\nimport pickle\nfrom glob import glob\ndata_root=""/data_ssd/zhihu/result/tmp/""\nfiles_path=glob(data_root+""*val.pth"")\nfiles_path.sort()\nprint len(files_path)\nfiles_weight1=[]\ninitial_weight=[]\nfor file in files_path:\n     if \'MultiModel\' in file: \n        files_weight1.append(file)\n        initial_weight.append(1)\nprint len(files_weight1)\nfor f,w in zip(files_weight1,initial_weight):\n    print f,w\nmodel_num=10 if len(files_weight1)>10 else len(files_weight1) \nprobs=[t.load(r).float() for r in files_weight1[:model_num]]\ntest_data_path=\'/home/a/code/pytorch/zhihu/ddd/val.npz\'\nindex2qid = np.load(test_data_path)[\'index2qid\'].item()\nlabel_path=""/home/a/code/pytorch/zhihu/ddd/labels.json""\nwith open(label_path) as f: \n      labels_info = json.load(f)\nqid2label = labels_info[\'d\']\ntrue_labels = [qid2label[index2qid[2999967-200000+ii]] for ii in range(200000)]\ndel labels_info\ndel qid2label\ndel index2qid\ndef target(args):\n    r=0\n    for r_,k_ in enumerate(args):\n        if r_<model_num:\n            r +=k_*probs[r_]\n        else:\n            tmp=t.load(files_path[r_]).cuda().float()\n            r=r+k_*tmp.cpu()\n    result = r.topk(5,1)[1]\n    predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result,true_labels)]\n    score,_,_,rrs = get_score(predict_label_and_marked_label_list)\n    print (args,score,rrs)#list_space = [hp.uniform(\'a\',0,1),hp.uniform(\'b\',0,1)]\n    return -score\nmax_evals=100\nfrom hyperopt import hp, fmin, rand, tpe, space_eval\nlist_space = [hp.normal(\'a\'+str(rr),initial_weight[rr],0.2) for rr in range(len(files_weight1))]\nfrom hyperopt import Trials\ntrials_to_keep=Trials()\nbest = fmin(target,list_space,algo=tpe.suggest,max_evals=max_evals, trials = trials_to_keep)\noutput = open(\'trials_to_keep_multimodel_score\'+\'.pkl\', \'wb\')\npickle.dump(trials_to_keep, output)\noutput.close()\n\nprint best\nresults=0\nresult_path=""/data_ssd/zhihu/result/search_result/""\nfor ii in range(len(files_weight1)):\n    if ii<model_num:\n        results +=probs[ii]*best[\'a\'+str(ii)]\n    else:\n        tmp=t.load(files_path[ii]).cuda().float()\n        results +=tmp*best[\'a\'+str(ii)]\nt.save(results.float(),result_path+""search_multimodel_result_score.pth"")\n\n'"
del/search_paris.py,0,"b'import json\nimport os\nimport sys\nfrom utils import get_score\nimport torch as t \nimport numpy as np\nfrom glob import glob\nimport pickle\nfrom glob import glob\npre=""search_nomulti_weight1""\ndata_root=""/data_ssd/zhihu/result/search_result/""\nfiles_path=glob(data_root+pre+""*.pth"")\nfiles_path.sort()\nprint files_path\nprobs=[t.load(r).float() for r in files_path]\ntest_data_path=\'/home/a/code/pytorch/zhihu/ddd/val.npz\'\nindex2qid = np.load(test_data_path)[\'index2qid\'].item()\nlabel_path=""/home/a/code/pytorch/zhihu/ddd/labels.json""\nwith open(label_path) as f: \n      labels_info = json.load(f)\nqid2label = labels_info[\'d\']\ntrue_labels = [qid2label[index2qid[2999967-200000+ii]] for ii in range(200000)]\ndel labels_info\ndel qid2label\ndel index2qid\n\ndef target(args):\n    r=0\n    for r_,k_ in zip(args,probs):\n        r=r+r_*k_\n    result = r.topk(5,1)[1]\n    predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result,true_labels)]\n    score,_,_,_ = get_score(predict_label_and_marked_label_list)\n    print (args,score,_)#list_space = [hp.uniform(\'a\',0,1),hp.uniform(\'b\',0,1)]\n    return -score\nmax_evals=50\nfrom hyperopt import hp, fmin, rand, tpe, space_eval\nlist_space = [hp.normal(\'a\'+str(rr),1,0.5) for rr in range(len(files_path))]\nfrom hyperopt import Trials\ntrials_to_keep=Trials()\nbest = fmin(target,list_space,algo=tpe.suggest,max_evals=max_evals, trials = trials_to_keep)\noutput = open(\'trials_to_keep__\'+pre+\'pairs_.pkl\', \'wb\')\npickle.dump(trials_to_keep, output)\noutput.close()\n\n\nprint best\nresults=0\nresult_path=""/data_ssd/zhihu/result/search_result/""\nfor ii in range(len(files_path)):\n    results +=probs[ii]*best[\'a\'+str(ii)]\nt.save(results.float(),result_path+pre+""_pairs.pth"")\n\n\n\n\n    '"
del/search_test.py,0,"b'#coding:utf-8\nimport json\nimport os\nimport sys\nfrom utils import get_score\nimport torch as t \nimport numpy as np\nfrom glob import glob\nimport pickle\nfrom glob import glob\nnoaug=True\nno_mul_w1=True\nmultimodel=True\nweight5=True\nallmoel=True\npre=""first""\ndef write_csv(result,index2qid,labels,filename):\n    path=""/data_ssd/zhihu/result/search_result/""\n    f=open(path+filename+"".csv"", ""wa"")\n    csv_writer = csv.writer(f, dialect=""excel"")\n    rows=[0 for _ in range(result.size(0))]\n    for i in range(result.size(0)):\n        tmp=result[i].sort(dim=0,descending=True)\n        tmp=tmp[1][:5]\n        row=[index2qid[i]]+[labels[str(int(i_))] for i_ in tmp]\n        rows[i]=row\n    csv_writer.writerows(rows) \ndef load_data(test,labels_file):\n    index2qid=np.load(test)[\'index2qid\'].item()\n    with open(labels_file) as f:\n        labels= json.load(f)[\'id2label\']\n    return index2qid,labels\n#labels_file=""/home/a/code/pytorch/zhihu/ddd/labels.json""\n#test=""/home/a/code/pytorch/zhihu/ddd/test.npz""\n#index2qid,labels=load_data(test,labels_file)\n################################################\n#no aug  result\xe7\x9b\xae\xe5\xbd\x95\xe4\xb8\x8b\xe9\x9d\x9e\xe5\xa2\x9e\xe5\xbc\xba\xe6\xb5\x8b\xe8\xaf\x95\nif noaug:\n    data_root=""/data_ssd/zhihu/result/""\n    files_path=glob(data_root+""*val.pth"")\n    files_path.sort()\n    files=[]\n    for file in files_path:\n         if \'MultiModel\' not in file:\n            files.append(file)        \n    print len(files_path)\n    pkl_file=open(""trials_to_keep__weight1_noaug.pkl"",\'rb\')\n    trials=pickle.load(pkl_file)\n    model_num=len(files)\n    params=[trials.best_trial[\'misc\'][\'vals\'][\'a\'+str(rr)]for rr in range(8)]\n    result_1=0\n    for i,pa in enumerate (params):\n            print ""load "",files[i],pa\n            result_1=result_1+pa[0]*t.load(files[i].replace(""val"",""test"")).float()\n    t.save(result_1.float(),""/data_ssd/zhihu/result/search_result/test_noaug_first.pth"")\n    #write_csv(result,index2qid,labels,""test_noaug"")\n################################################\n# aug  no multimodel weight1   \xe7\x9b\xae\xe5\xbd\x95\xe4\xb8\x8b\xe9\x9d\x9e\xe5\xa2\x9e\xe5\xbc\xba\xe6\xb5\x8b\xe8\xaf\x95\nif no_mul_w1:\n    data_root=""/data_ssd/zhihu/result/tmp/""\n    files_path=glob(data_root+""*val.pth"")\n    files_path.sort()\n    files=[]\n    for file in files_path:\n        if \'MultiModel\' not in file and \'weight5\' not in file:\n            files.append(file)        \n    print files\n    print len(files)\n    pkl_file=open(""trials_to_keep__aug_nomulti_weight1.pkl"",\'rb\')\n    trials=pickle.load(pkl_file)\n    model_num=len(files)\n    params=[trials.best_trial[\'misc\'][\'vals\'][\'a\'+str(rr)]for rr in range(len(files))]\n    result_1=0\n    for i,pa in enumerate (params):\n            print ""load "",files[i],pa\n            result_1=result_1+pa[0]*t.load(files[i].replace(""val"",""test"")).float()\n    #write_csv(result,index2qid,labels,""test_no_mul_w1"")\n    t.save(result_1.float(),""/data_ssd/zhihu/result/search_result/test_test_nomultiw1_first.pth"")\n################################################\n# multimodel    \xe7\x9b\xae\xe5\xbd\x95\xe4\xb8\x8b\xe9\x9d\x9e\xe5\xa2\x9e\xe5\xbc\xba\xe6\xb5\x8b\xe8\xaf\x95\nif multimodel:\n    data_root=""/data_ssd/zhihu/result/tmp/""\n    files_path=glob(data_root+""*val.pth"")\n    files_path.sort()\n    files=[]\n    for file in files_path:\n        if \'MultiModel\' in file: \n            files.append(file)        \n    print len(files)\n    pkl_file=open(""trials_to_keep_multimodel.pkl"",\'rb\')\n    trials=pickle.load(pkl_file)\n    model_num=len(files)\n    params=[trials.best_trial[\'misc\'][\'vals\'][\'a\'+str(rr)]for rr in range(len(files))]\n    result_1=0\n    for i,pa in enumerate (params):\n            print ""load "",files[i],pa\n            result_1=result_1+pa[0]*t.load(files[i].replace(""val"",""test"")).float()\n    t.save(result_1.float(),""/data_ssd/zhihu/result/search_result/test_multimodel_first.pth"")\n    #write_csv(result,index2qid,labels,""test_multimodel"")\n\n################################################\n# weight5    \xe7\x9b\xae\xe5\xbd\x95\xe4\xb8\x8b\xe9\x9d\x9e\xe5\xa2\x9e\xe5\xbc\xba\xe6\xb5\x8b\xe8\xaf\x95\nif weight5:\n    data_root=""/data_ssd/zhihu/result/tmp/""\n    files_path=glob(data_root+""*val.pth"")\n    files_path.sort()\n    files=[]\n    for file in files_path:\n        if \'weight5\' in file:\n            files.append(file)\n    print len(files)\n    pkl_file=open(""trials_to_keep__weight5_only.pkl"",\'rb\')\n    trials=pickle.load(pkl_file)\n    model_num=len(files)\n    params=[trials.best_trial[\'misc\'][\'vals\'][\'a\'+str(rr)]for rr in range(len(files))]\n    result_1=0\n    for i,pa in enumerate (params):\n            print ""load "",files[i],pa\n            result_1=result_1+pa[0]*t.load(files[i].replace(""val"",""test"")).float()\n    t.save(result_1.float(),""/data_ssd/zhihu/result/search_result/test_weight5_first.pth"")\n    #write_csv(result,index2qid,labels,""test_weight5"") \n    \n    \n################################################\n# allmodel    \xe5\x9b\x9b\xe5\xa4\xa7\xe7\xb1\xbb\xe8\x9e\x8d\xe5\x90\x88\nif allmoel:\n    data_root=""/data_ssd/zhihu/result/search_result/""\n    files_path=glob(data_root+""test*first.pth"")\n    files_path.sort()\n    files= files_path      \n    print len(files_path)\n    pkl_file=open(""trials_to_keep_all_first.pkl"",\'rb\')\n    trials=pickle.load(pkl_file)\n    model_num=len(files)\n    params=[trials.best_trial[\'misc\'][\'vals\'][\'a\'+str(rr)]for rr in range(len(files))]\n    result_1=0\n    for i,pa in enumerate (params):\n            print ""load "",files[i],pa\n            result_1=result_1+pa[0]*t.load(files[i]).float()\n    t.save(result_1.float(),""/data_ssd/zhihu/result/search_result/test_all_first.pth"")\n    #write_csv(result,index2qid,labels,""test_weight5"") \n\n\n\n'"
del/search_weight5.py,0,"b'import json\nimport os\nimport sys\nfrom utils import get_score\nimport torch as t \nimport numpy as np\nfrom glob import glob\nimport pickle\nfrom glob import glob\ndata_root=""/data_ssd/zhihu/result/tmp/""\nfiles_path=glob(data_root+""*val.pth"")\nfiles_path.sort()\nprint len(files_path)\nfiles_weight1=[]\ninitial_weight=[]\nfor file in files_path:\n    if \'weight5\' in file:\n        files_weight1.append(file)\n        if \'MultiModel\' in file:\n            initial_weight.append(5)\n        else:\n            initial_weight.append(1)\nprint len(files_weight1)\nfor f,w in zip(files_weight1,initial_weight):\n    print f,w\nmodel_num=10 if len(files_weight1)>10 else len(files_weight1) \nprobs=[t.load(r).float() for r in files_weight1[:model_num]]\ntest_data_path=\'/home/a/code/pytorch/zhihu/ddd/val.npz\'\nindex2qid = np.load(test_data_path)[\'index2qid\'].item()\nlabel_path=""/home/a/code/pytorch/zhihu/ddd/labels.json""\nwith open(label_path) as f: \n      labels_info = json.load(f)\nqid2label = labels_info[\'d\']\ntrue_labels = [qid2label[index2qid[2999967-200000+ii]] for ii in range(200000)]\ndel labels_info\ndel qid2label\ndel index2qid\ndef target(args):\n    r=0\n    for r_,k_ in enumerate(args):\n        if r_<model_num:\n            r +=k_*probs[r_]\n        else:\n            tmp=t.load(files_path[r_]).cuda().float()\n            r=r+k_*tmp.cpu()\n    result = r.topk(5,1)[1]\n    predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result,true_labels)]\n    score,_,_,rrs = get_score(predict_label_and_marked_label_list)\n    print (args,score,rrs)#list_space = [hp.uniform(\'a\',0,1),hp.uniform(\'b\',0,1)]\n    return -rrs[0]\nmax_evals=150\nfrom hyperopt import hp, fmin, rand, tpe, space_eval\nlist_space = [hp.normal(\'a\'+str(rr),initial_weight[rr],0.2) for rr in range(len(files_weight1))]\nfrom hyperopt import Trials\ntrials_to_keep=Trials()\nbest = fmin(target,list_space,algo=tpe.suggest,max_evals=max_evals, trials = trials_to_keep)\noutput = open(\'trials_to_keep__weight5_only\'+\'.pkl\', \'wb\')\npickle.dump(trials_to_keep, output)\noutput.close()\n\n\nprint best\nresults=0\nresult_path=""/data_ssd/zhihu/result/search_result/""\nfor ii in range(len(files_weight1)):\n    if ii<model_num:\n        results +=probs[ii]*best[\'a\'+str(ii)]\n    else:\n        tmp=t.load(files_path[ii]).cuda().float()\n        results +=tmp*best[\'a\'+str(ii)]\nt.save(results.float(),result_path+""search_weight5_result_first.pth"")\n\n\n'"
del/searchstack.py,0,"b'import json\nimport os\nimport sys\nfrom utils import get_score\nimport torch as t \nimport numpy as np\nfrom glob import glob\nimport pickle\nmax_evals=100\n#data_root=""/home/a/code/pytorch/zhihu/""\ndata_root=""/data/text/zhihu/result/tmp/""\npre=""LSTM""\nfiles_path=glob(data_root+pre+""*val.pth"")\nfiles_path.sort()\nmodel_num=len(files_path)\nprint files_path[:model_num]\nprobs=[t.load(r).float() for r in files_path[:model_num]]\n\ntest_data_path=\'/home/a/code/pytorch/zhihu/ddd/val.npz\'\nindex2qid = np.load(test_data_path)[\'index2qid\'].item()\nlabel_path=""/home/a/code/pytorch/zhihu/ddd/labels.json""\nwith open(label_path) as f: \n      labels_info = json.load(f)\nqid2label = labels_info[\'d\']\ntrue_labels = [qid2label[index2qid[2999967-200000+ii]] for ii in range(len(probs[0]))]\ndel labels_info\ndel qid2label\ndel index2qid\ndef target(args):\n    r=0\n    for r_,k_ in zip(args,probs):\n        r=r+r_*k_\n    result = r.topk(5,1)[1]\n    predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result,true_labels)]\n    score,_,_,_ = get_score(predict_label_and_marked_label_list)\n    print (args,score,_)#list_space = [hp.uniform(\'a\',0,1),hp.uniform(\'b\',0,1)]\n    return -score\n\nfrom hyperopt import hp, fmin, rand, tpe, space_eval\nlist_space = [hp.normal(\'a\'+str(rr),1,0.2) for rr in range(model_num)]\nfrom hyperopt import Trials\ntrials_to_keep=Trials()\nbest = fmin(target,list_space,algo=tpe.suggest,max_evals=max_evals, trials = trials_to_keep)\noutput = open(\'trials_to_keep__\'+pre+\'.pkl\', \'wb\')\npickle.dump(trials_to_keep, output)\n\n    \n\n\n'"
del/searchstack_new.py,0,"b'import json\nimport os\nimport sys\nfrom utils import get_score\nimport torch as t \nimport numpy as np\nfrom glob import glob\nimport pickle\nmax_evals=100\n#data_root=""/home/a/code/pytorch/zhihu/""\ndata_root=""/data/text/zhihu/result/tmp/""\nfiles_path=glob(data_root+""*val.pth"")\nfiles_path.sort()\nmodel_num=10#len(files_path)\nprint files_path[:model_num]\nprobs=[t.load(r).float() for r in files_path[:model_num]]\n\ntest_data_path=\'/home/a/code/pytorch/zhihu/ddd/val.npz\'\nindex2qid = np.load(test_data_path)[\'index2qid\'].item()\nlabel_path=""/home/a/code/pytorch/zhihu/ddd/labels.json""\nwith open(label_path) as f: \n      labels_info = json.load(f)\nqid2label = labels_info[\'d\']\ntrue_labels = [qid2label[index2qid[2999967-200000+ii]] for ii in range(200000)]\ndel labels_info\ndel qid2label\ndel index2qid\ndef target(args):\n    r=0\n    for r_,k_ in enumerate(args):\n        if r_<model_num:\n            r +=k_*probs[r_]\n        else:\n            tmp=t.load(files_path[r_]).cuda().float()\n            r=r+k_*tmp.cpu()\n    result = r.topk(5,1)[1]\n    predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result,true_labels)]\n    score,_,_,_ = get_score(predict_label_and_marked_label_list)\n    print (args,score,_)#list_space = [hp.uniform(\'a\',0,1),hp.uniform(\'b\',0,1)]\n    return -score\n\nfrom hyperopt import hp, fmin, rand, tpe, space_eval\nlist_space = [hp.normal(\'a\'+str(rr),1,0.2) for rr in range(len(files_path))]\nfrom hyperopt import Trials\ntrials_to_keep=Trials()\nbest = fmin(target,list_space,algo=tpe.suggest,max_evals=max_evals, trials = trials_to_keep)\noutput = open(\'trials_to_keep__\'+\'.pkl\', \'wb\')\npickle.dump(trials_to_keep, output)\n\n    \n\n\n'"
del/test.2.py,2,"b'#encoding:utf-8\n\'\'\'\n\xe7\x9b\xb4\xe6\x8e\xa5\xe6\xa0\xb9\xe6\x8d\xae\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x94\x9f\xe6\x88\x90\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8f\x90\xe4\xba\xa4\xe7\x9a\x84csv\n\xe4\xb8\x93\xe9\x97\xa8\xe4\xb8\xbamultimodel \xe8\x80\x8c\xe5\x86\x99\ndataset\xe5\x90\x8c\xe6\x97\xb6\xe9\x80\x81\xe8\xbf\x9b\xe5\x8e\xbbchar\xe5\x92\x8cword\n\'\'\'\nfrom torch.utils import data\nimport torch as t\nimport numpy as np\nfrom config import opt\nimport models\nimport json\nimport fire\nimport csv\nfrom torch.autograd import Variable\nimport tqdm\n\ndef load_data(type_=\'char\'):\n    with open(opt.labels_path) as f:\n        labels_ = json.load(f)\n    question_d = np.load(opt.test_data_path)\n    # if type_ == \'char\':\n    #     test_data_title,test_data_content =\\\n    #          question_d[\'title_char\'],question_d[\'content_char\']\n\n    # elif type_ == \'word\':\n    #     test_data_title,test_data_content =\\\n    #          question_d[\'title_word\'],question_d[\'content_word\']\n\n    index2qid = question_d[\'index2qid\'].item()\n\n    return (question_d[\'title_word\'],question_d[\'content_word\']),(question_d[\'title_char\'],question_d[\'content_char\']),index2qid,labels_[\'id2label\']\ndef write_csv(result,index2qid,labels):\n    f=open(opt.result_path, ""wa"")\n    csv_writer = csv.writer(f, dialect=""excel"")\n    rows=[0 for _ in range(result.shape[0])]\n    for i in range(result.shape[0]):\n        row=[index2qid[i]]+[labels[str(int(i_))] for i_ in result[i]]\n        rows[i]=row\n    csv_writer.writerows(rows)\ndef dotest(model,title,content):\n    title,content = (Variable(t.from_numpy(title[0]).long().cuda(),volatile=True), Variable(t.from_numpy(title[1]).long().cuda(),volatile=True)),(Variable(t.from_numpy(content[0]).long().cuda(),volatile=True),Variable(t.from_numpy(content[1]).long().cuda(),volatile=True))\n    score = model(content,title)\n    probs=t.nn.functional.sigmoid(score)\n    probs_ordered = probs.sort(dim=1,descending=True)\n    tmp=probs_ordered[1][:,:5]\n    return tmp.data.cpu().numpy()\n    \ndef main(**kwargs):\n    opt.parse(kwargs)\n    # opt.model_names=[\'MultiCNNTextBNDeep\',\'RCNN\',\'LSTMText\',\'CNNText_inception\',\'RCNN\',\'CNNText_inception\',\'LSTMText\']\n    # opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_word_0.41124002492\',\'checkpoints/RCNN_word_0.411511574999\',\'checkpoints/LSTMText_word_0.411994005382\',\'checkpoints/CNNText_tmp_char_0.402429167301\',\'checkpoints/RCNN_char_0.403710422571\',\'checkpoints/CNNText_tmp_word_0.41096749885\',\'checkpoints/LSTMText_char_0.403192339135\',]\n    model = getattr(models,opt.model)(opt).cuda().eval()\n    if opt.model_path is not None:\n        model.load(opt.model_path)\n    model=model.eval()\n    test_data_title,test_data_content,index2qid,labels=load_data(type_=opt.type_)\n    Num=len(test_data_title[0])\n    result=np.zeros((Num,5))\n    for i in tqdm.tqdm(range(Num)):\n        if i%opt.batch_size==0 and i>0:\n            title=np.array(test_data_title[0][i-opt.batch_size:i]),np.array(test_data_title[1][i-opt.batch_size:i])\n            content=np.array(test_data_content[0][i-opt.batch_size:i]),np.array(test_data_content[1][i-opt.batch_size:i])\n            result[i-opt.batch_size:i,:]=dotest(model,title,content)  \n        # if i%opt.batch_size ==0:\n        #     print i,""have done""\n\n\n    result[opt.batch_size*(Num/opt.batch_size):,:]=dotest(model,title,content) \n    write_csv(result,index2qid,labels)\n    \n\n    \n\n    \nif __name__==\'__main__\':\n    fire.Fire()\n    \n    '"
del/test.py,2,"b'#encoding:utf-8\nfrom torch.utils import data\nimport torch as t\nimport numpy as np\nfrom config import opt\nimport models\nimport json\nimport fire\nimport csv\nfrom glob import glob\nfrom torch.autograd import Variable\nimport tqdm\n\ndef load_data_stack(data_root,test,labels_file):\n    \'\'\'\n    data_root=""/data/text/zhihu/result/*_test.pth""\n    labels_file=""/home/a/code/pytorch/zhihu/ddd/labels.json""\n    test=""/home/a/code/pytorch/zhihu/ddd/test.npz""\n    \'\'\'\n    result_files_path=glob(data_root+""*test.pth"")\n    index2qid=np.load(test)[\'index2qid\'].item()\n    with open(labels_file) as f:\n        labels= json.load(f)[\'id2label\']\n    test_data_num=217360\n    model_num=len(result_files_path)\n    test_data=t.zeros(test_data_num,1999*model_num)\n    for i in range(model_num):\n        test_data[:,i*1999:i*1999+1999]=t.load(result_files_path[i]).float()\n    return test_data,index2qid,labels\ndef load_data(type_=\'char\'):\n    with open(opt.labels_path) as f:\n        labels_ = json.load(f)\n    question_d = np.load(opt.test_data_path)\n    if type_ == \'char\':\n        test_data_title,test_data_content =\\\n             question_d[\'title_char\'],question_d[\'content_char\']\n\n    elif type_ == \'word\':\n        test_data_title,test_data_content =\\\n             question_d[\'title_word\'],question_d[\'content_word\']\n\n    index2qid = question_d[\'index2qid\'].item()\n    return test_data_title,test_data_content,index2qid,labels_[\'id2label\']\ndef write_csv(result,index2qid,labels):\n    f=open(opt.result_path, ""wa"")\n    csv_writer = csv.writer(f, dialect=""excel"")\n    rows=[0 for _ in range(result.shape[0])]\n    for i in range(result.shape[0]):\n        row=[index2qid[i]]+[labels[str(int(i_))] for i_ in result[i]]\n        rows[i]=row\n    csv_writer.writerows(rows)\ndef dotest(model,title,content):\n    title,content = Variable(t.from_numpy(title).long().cuda(),volatile=True),Variable(t.from_numpy(content).long().cuda(),volatile=True)\n    score = model(title,content)\n    probs=t.nn.functional.sigmoid(score)\n    probs_ordered = probs.sort(dim=1,descending=True)\n    tmp=probs_ordered[1][:,:5]\n    return tmp.data.cpu().numpy()\n    \ndef main(**kwargs):\n    opt.parse(kwargs)\n    model = getattr(models,opt.model)(opt).cuda().eval()\n    if opt.model_path is not None:\n        model.load(opt.model_path)\n    test_data_title,test_data_content,index2qid,labels=load_data_stack(type_=opt.type_)\n    Num=len(test_data_title)\n    result=np.zeros((Num,5))\n    for i in tqdm.tqdm(range(Num)):\n        if i%opt.batch_size==0 and i>0:\n            title=np.array(test_data_title[i-opt.batch_size:i])\n            content=np.array(test_data_content[i-opt.batch_size:i])\n            result[i-opt.batch_size:i,:]=dotest(model,title,content)  \n        if i%opt.batch_size ==0:\n            print i,""have done""\n    title=np.array(test_data_title[opt.batch_size*(Num/opt.batch_size):])\n    content=np.array(test_data_content[opt.batch_size*(Num/opt.batch_size):]) \n    result[opt.batch_size*(Num/opt.batch_size):,:]=dotest(model,title,content) \n    write_csv(result,index2qid,labels)\n    \ndef main_stack(**kwargs):\n    opt.parse(kwargs)\n    model = getattr(models,opt.model)(opt).cuda().eval()\n    if opt.model_path is not None:\n        model.load(opt.model_path)\n    data_root=""/data/text/zhihu/result/""\n    labels_file=""/home/a/code/pytorch/zhihu/ddd/labels.json""\n    test=""/home/a/code/pytorch/zhihu/ddd/test.npz""\n    test_data,index2qid,labels=load_data_stack(data_root,test,labels_file)\n    Num=len(test_data)\n    result=np.zeros((Num,5))\n    for i in tqdm.tqdm(range(Num)):\n        if i%opt.batch_size==0 and i>0:\n            title=np.array(test_data_title[i-opt.batch_size:i])\n            content=np.array(test_data_content[i-opt.batch_size:i])\n            result[i-opt.batch_size:i,:]=dotest(model,title,content)  \n        if i%opt.batch_size ==0:\n            print i,""have done""\n    title=np.array(test_data_title[opt.batch_size*(Num/opt.batch_size):])\n    content=np.array(test_data_content[opt.batch_size*(Num/opt.batch_size):]) \n    result[opt.batch_size*(Num/opt.batch_size):,:]=dotest(model,title,content) \n    write_csv(result,index2qid,labels)\n    \n\n    \nif __name__==\'__main__\':\n    fire.Fire()\n    \n    '"
del/test_aug_multimodel.py,2,"b'#encoding:utf-8\n\n\'\'\'\n\xe6\xb5\x8b\xe8\xaf\x95\xe6\x97\xb6\xe5\xaf\xb9\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xa2\x9e\xe5\xbc\xba\xef\xbc\x8c\xe5\x88\x86\xe5\x88\xab\xe5\x85\x83\xe6\x95\xb0\xe6\x8d\xae shuffle  drop\xe4\xb8\x89\xe7\xa7\x8d\n\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8b\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87hyper search\xe8\xbf\x9b\xe8\xa1\x8c\xe8\x9e\x8d\xe5\x90\x88\n\'\'\'\nfrom utils import get_score\nfrom torch.utils import data\nimport torch as t\nimport numpy as np\nfrom config import opt\nimport models\nimport json\nimport os\nimport sys\nimport fire\nfrom glob import glob\nimport csv\nimport tqdm\nimport pickle\nfrom torch.autograd import Variable\ndef load_data(type_=\'char\'):\n    with open(opt.labels_path) as f:\n        labels_ = json.load(f)\n    question_d = np.load(opt.test_data_path)\n    print ""test_path: "",opt.test_data_path\n    index2qid = question_d[\'index2qid\'].item()\n    index=np.arange(len(question_d[\'title_char\']))\n    np.random.shuffle(index)\n    char_data=(question_d[\'title_char\'],question_d[\'content_char\'])\n    word_data=(question_d[\'title_word\'],question_d[\'content_word\'])\n    return  char_data,word_data,index2qid,labels_,index\ndef write_csv(result,index2qid,labels):\n    f=open(opt.result_path, ""wa"")\n    csv_writer = csv.writer(f, dialect=""excel"")\n    rows=[0 for _ in range(result.size(0))]\n    for i in range(result.size(0)):\n        tmp=result[i].sort(dim=0,descending=True)\n        tmp=tmp[1][:5]\n        row=[index2qid[i]]+[labels[str(int(i_))] for i_ in tmp]\n        rows[i]=row\n    csv_writer.writerows(rows)  \n\n\ndef dotest(model,title,content):\n    title,content = (Variable(t.from_numpy(title[0]).long().cuda(),volatile=True),Variable(t.from_numpy(title[1]).long().cuda(),volatile=True)),(Variable(t.from_numpy(content[0]).long().cuda(),volatile=True),Variable(t.from_numpy(content[1]).long().cuda(),volatile=True))\n    score = model(title,content)\n    probs=t.nn.functional.sigmoid(score)\n    return probs.data.cpu().numpy()\ndef dropout(d,p=0.5):\n        len_ = d.shape[1]\n        batch_=d.shape[0]\n        for ii in range(batch_):\n            index = np.random.choice(len_,int(len_*p))\n            d[ii,index]=0\n        return d \ndef test_val():\n    #####\n    #####\xe6\xb5\x8b\xe8\xaf\x95\xe5\xa2\x9e\xe5\xbc\xba\xe5\x89\x8d\xe7\x9a\x84\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\xbb\x93\xe6\x9e\x9c\n    #####\n    data_path=\'/data_ssd/zhihu/result/tmp/\'\n    result_path=[\'inception0.41254_aug_word_val.pth\',\'LSTMText0.41368_aug_word_val.pth\',\'DeepText0.38738_aug_char_val.pth\',\n                 \' RCNN0.39854_aug_char_val.pth\',\'RCNN0.41344_aug_word_val.pth\']#,\'LSTMText0.4120_aug_word_val.pth\']\n    result=0\n    for i in range(len(result_path)):\n        path=data_path+result_path[i]\n        print ""loading"",path\n        result +=t.load(path.replace(\' \',\'\')).float()\n    test_data_path=\'/home/a/code/pytorch/zhihu/ddd/val.npz\'\n    index2qid = np.load(test_data_path)[\'index2qid\'].item()\n    label_path=""/home/a/code/pytorch/zhihu/ddd/labels.json""\n    with open(label_path) as f: \n          labels_info = json.load(f)\n    qid2label = labels_info[\'d\']\n    true_labels = [qid2label[index2qid[2999967-200000+ii]] for ii in range(200000)]\n    result_ = result.topk(5,1)[1]\n    predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result_,true_labels)]\n    score,_,_,_ = get_score(predict_label_and_marked_label_list)\n    print (score,_)\ndef test_val_aug():\n    #####\n    #####\xe6\xb5\x8b\xe8\xaf\x95\xe5\xa2\x9e\xe5\xbc\xba\xe5\x90\x8e\xe7\x9a\x84\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\xbb\x93\xe6\x9e\x9c\n    #####\n    data_path=""/data_ssd/zhihu/result/test_aug/""\n    result_path=glob(data_path+""*.npz"")\n    result_path.sort()\n    result=0\n    for i in range(len(result_path)):\n        print ""loading"",result_path[i]\n        result +=np.load(result_path[i])[\'result_prob\']\n    test_data_path=\'/home/a/code/pytorch/zhihu/ddd/val.npz\'\n    index2qid = np.load(test_data_path)[\'index2qid\'].item()\n    label_path=""/home/a/code/pytorch/zhihu/ddd/labels.json""\n    with open(label_path) as f: \n          labels_info = json.load(f)\n    qid2label = labels_info[\'d\']\n    true_labels = [qid2label[index2qid[2999967-200000+ii]] for ii in range(200000)]\n    result_ = t.from_numpy(result).topk(5,1)[1]\n    predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result_,true_labels)]\n    score,_,_,_ = get_score(predict_label_and_marked_label_list)\n    print (score,_)\n\n    \ndef main(**kwargs):\n    opt.parse(kwargs)\n    ####################### MultiModelAll_word_0.417185977233#################\n    opt.model_names=[\'MultiCNNTextBNDeep\',\'LSTMText\',\'CNNText_inception\',\'RCNN\']\n    opt.model_paths = [\'checkpoints/MultiCNNTextBNDeep_0.37125473788\',\'checkpoints/LSTMText_word_0.381833388089\',\'checkpoints/CNNText_tmp_0.376364647145\',\'checkpoints/RCNN_char_0.3456599248\']\n    #########################################################################\n    \n    \n    model = getattr(models,opt.model)(opt).cuda().eval()\n    if opt.model_path is not None:\n        model.load(opt.model_path)\n    opt.parse(kwargs)\n    \n    model = model.eval()\n    \n    test_data_title,test_data_content,index2qid,labels,index=load_data(type_=opt.type_)\n    Num=len(test_data_title[0])\n    print ""Num: "",Num\n    result1=np.zeros((Num,1999))\n    result2=np.zeros((Num,1999))\n    result3=np.zeros((Num,1999))\n    for i in tqdm.tqdm(range(Num)):\n        if i%opt.batch_size==0 and i>0:\n            # import ipdb;ipdb.set_trace()\n            title1=np.array(test_data_title[0][i-opt.batch_size:i]),np.array(test_data_title[1][i-opt.batch_size:i])\n            content1=np.array(test_data_content[0][i-opt.batch_size:i]),np.array(test_data_content[1][i-opt.batch_size:i])\n            result1[i-opt.batch_size:i,:]=dotest(model,title1,content1) \n            \n            title3=dropout(title1[0],0.3),dropout(title1[1],0.3)\n            content3=dropout(content1[0],0.7),dropout(content1[1],0.7)\n            result3[i-opt.batch_size:i,:]=dotest(model,title3,content3)\n            \n            title2=np.array(test_data_title[0][index[i-opt.batch_size:i]]),np.array(test_data_title[1][index[i-opt.batch_size:i]])\n            content2=np.array(test_data_content[0][index[i-opt.batch_size:i]]),np.array(test_data_content[1][index[i-opt.batch_size:i]])\n            result2[index[i-opt.batch_size:i]]=dotest(model,title2,content2)\n            \n    if Num%opt.batch_size!=0:\n        title1=np.array(test_data_title[0][opt.batch_size*(Num/opt.batch_size):]),np.array(test_data_title[1][opt.batch_size*(Num/opt.batch_size):])\n        content1=np.array(test_data_content[0][opt.batch_size*(Num/opt.batch_size):]),np.array(test_data_content[1][opt.batch_size*(Num/opt.batch_size):])\n        result1[opt.batch_size*(Num/opt.batch_size):,:]=dotest(model,title1,content1) \n        \n        title3=dropout(title1[0],0.3),dropout(title1[1],0.3)\n        content3=dropout(content1[0],0.7),dropout(content1[1],0.7)\n        result3[opt.batch_size*(Num/opt.batch_size):]=dotest(model,title3,content3)\n\n        title2=np.array(test_data_title[0][index[opt.batch_size*(Num/opt.batch_size):]]),np.array(test_data_title[1][index[opt.batch_size*(Num/opt.batch_size):]])\n        content2=np.array(test_data_content[0][index[opt.batch_size*(Num/opt.batch_size):]]),np.array(test_data_content[1][index[opt.batch_size*(Num/opt.batch_size):]])\n        result2[index[opt.batch_size*(Num/opt.batch_size):]]=dotest(model,title2,content2)\n        \n        \n        \n    #######\n    #hyper search\n    ######\n    if opt.val:\n        probs=[result1,result2,result3]\n        def target(args):\n            r=0\n            for r_,k_ in zip(args,probs):\n                r=r+r_*k_\n            result = t.from_numpy(r).topk(5,1)[1]\n            predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result,true_labels)]\n            score,_,_,_ = get_score(predict_label_and_marked_label_list)\n            print (args,score,_)#list_space = [hp.uniform(\'a\',0,1),hp.uniform(\'b\',0,1)]\n            return -score\n\n\n        true_labels = [labels[\'d\'][index2qid[2999967-200000+ii]] for ii in range(200000)]\n        from hyperopt import hp, fmin, rand, tpe, space_eval\n        list_space = [hp.normal(\'a0\',1,0.8),hp.normal(\'a1\',0.5,0.3),hp.normal(\'a2\',0.2,0.2)]\n        from hyperopt import Trials\n        trials_to_keep=Trials()\n        best = fmin(target,list_space,algo=tpe.suggest,max_evals=100, trials = trials_to_keep)\n        best_params=[best[\'a0\'],best[\'a1\'],best[\'a2\']]\n        sums=best[\'a0\']+best[\'a1\']+best[\'a2\']\n        best_score=trials_to_keep.best_trial[\'result\'][\'loss\']*(-1)\n        result=(result1*best[\'a0\']+result2*best[\'a1\']+result3*best[\'a2\'])/sums\n        output = open(\'trials_to_keep_\'+opt.model+""_""+str(best_score)+\'.pkl\', \'wb\')\n        pickle.dump(trials_to_keep, output)\n        npz_file=\'/data_ssd/zhihu/result/test_aug/\'+opt.model+""_""+str(best_score)+\'.npz\'\n        np.savez_compressed(npz_file, weights=np.array(best_params), result_prob=result)\n    else:\n        a1=1.08507012#1.1419278470112755\n        a2=-1.04290239#-1.0272661867201016\n        a3=1.16344534#1.1450416449670469\n        result=(a1*result1+a2*result2+a3*result3)/(a1+a2+a3)\n        labels__=labels[\'id2label\']\n        #result_top=t.from_numpy(result).topk(5,1)[1]\n        write_csv(t.from_numpy(result),index2qid,labels__)\n    #t.save(t.from_numpy(result1).float(),""test_aug_1.pth"")\n    #t.save(t.from_numpy(result2).float(),""test_aug_2.pth"")\n    #t.save(t.from_numpy(result3).float(),""test_aug_3.pth"")\n    #if opt.val:\n    #    true_labels = [labels[index2qid[2999967-200000+ii]] for ii in range(len(test_data_title))]\n    #    result_ = t.from_numpy(result).topk(5,1)[1]\n    #    predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result_,true_labels)]\n    #    score,_,_,_ = get_score(predict_label_and_marked_label_list)\n    #    print score\n    del result1\n    del result2\n    del result3\n    del result\n    \n    \nif __name__==\'__main__\':\n    fire.Fire()\n    \n    '"
del/test_stack.py,2,"b'#encoding:utf-8\nfrom torch.utils import data\nimport torch as t\nimport numpy as np\nfrom config import opt\nimport models\nimport json\nimport fire\nimport csv\nfrom torch.autograd import Variable\nimport tqdm\nfrom glob import glob\ndef load_data(test,labels_file):\n    \'\'\'\n    data_root=""/data/text/zhihu/result/*_test.pth""\n    labels_file=""/home/a/code/pytorch/zhihu/ddd/labels.json""\n    test=""/home/a/code/pytorch/zhihu/ddd/test.npz""\n    \'\'\'\n    index2qid=np.load(test)[\'index2qid\'].item()\n    with open(labels_file) as f:\n        labels= json.load(f)[\'id2label\']\n    #test_data_num=217360\n    #model_num=len(result_files_path)\n    #test_data=t.zeros(test_data_num,1999*model_num)\n    #for i in range(model_num):\n    #    test_data[:,i*1999:i*1999+1999]=t.load(result_files_path[i]).float()\n    return index2qid,labels\n\ndef test_hyper():\n    data_root=""/data/text/zhihu/result/tmp/""\n    labels_file=""/home/a/code/pytorch/zhihu/ddd/labels.json""\n    test=""/home/a/code/pytorch/zhihu/ddd/test.npz""\n    index2qid,labels=load_data(test,labels_file)\n    result_files_path=glob(data_root+""*val.pth"")\n    result_files_path.sort()\n    result=0\n    #a=[0.19447573541703533, 0.4302956826328484, 0.24415100069297443, 3.1941015222067826, 1.2995016739712848, 0.8448979155201735, 1.54270375208129]\n    # 0.4292096367086292\n    #a=[1.7212561973117408,1.7058806674841556,1.3764529360325304,1.1685564476646357,1.7403842367575626,0.7891955839006122, 1.6554675840993296]\n    # \xe5\xa4\xa7\xe7\xba\xa60.4271\n    a=[0.22478083908058233,0.6963877252668917,0.056146692013221464,2.76910389573104318311,2.842203642680702,1.1596530417845,3.0584950428073454]\n    # \xe5\xa4\xa7\xe7\xba\xa60.4293322872472573 \n    #a=[1,1,1,1,1,1,1]\n    for i,a_ in enumerate (a):\n        print ""load "",result_files_path[i],a_\n        result=result+a_*t.load(result_files_path[i].replace(""val"",""test"")).float()\n    write_csv(result,index2qid,labels)\n        \n        \ndef main(**kwargs):\n    opt.parse(kwargs,print_=False)\n    model = getattr(models,opt.model)(opt).cuda().eval()\n    if opt.model_path is not None:\n        print ""loding model"",opt.model_path\n        model.load(opt.model_path)\n    #model=model.eval()\n    opt.parse(kwargs,print_=False)\n    data_root=""/data/text/zhihu/result/""\n    labels_file=""/home/a/code/pytorch/zhihu/ddd/labels.json""\n    test=""/home/a/code/pytorch/zhihu/ddd/test.npz""\n    index2qid,labels=load_data(test,labels_file)\n    result_files_path=glob(data_root+""*val.pth"")\n    weights=t.nn.functional.softmax(model.weights)\n    print ""max of weights"",weights.max()\n    print ""min of weights"",weights.min()\n    print ""mean of weights"",weights.mean()\n    Num=217360\n    tmp_result=t.zeros((Num,1999))\n    print result_files_path\n    for i in range(len(result_files_path)):\n        tmpdata=t.load(result_files_path[i].replace(""val"",""test"")).float()\n        for j in tqdm.tqdm(range(Num)):\n            if j%opt.batch_size==0 and j>0:\n                data=tmpdata[j-opt.batch_size:j] \n                weights_=weights[:,i].contiguous().view(1,1999).expand_as(data).data.cpu()\n                tmp_result[j-opt.batch_size:j,:]=tmp_result[j-opt.batch_size:j,:]+weights_*data \n            if j%Num-1 ==0:\n                print j,""have done""\n        data=tmpdata[j-opt.batch_size:j] \n        weights_=weights[:,i].contiguous().view(1,1999).expand_as(data).data.cpu()\n        tmp_result[j-opt.batch_size:j,:]=tmp_result[j-opt.batch_size:j,:]+weights_*data\n    write_csv(tmp_result,index2qid,labels)\ndef write_csv(result,index2qid,labels):\n    #result_=result.sort(dim=1,descending=True)\n    #del result\n    f=open(opt.result_path, ""wa"")\n    csv_writer = csv.writer(f, dialect=""excel"")\n    rows=[0 for _ in range(result.size(0))]\n    #result_=result.numpy()\n    for i in range(result.size(0)):\n        tmp=result[i].sort(dim=0,descending=True)\n        tmp=tmp[1][:5]\n        row=[index2qid[i]]+[labels[str(int(i_))] for i_ in tmp]\n        rows[i]=row\n    csv_writer.writerows(rows)    \ndef dotest(weights,data,i):\n    return weights[:,i].contiguous().view(1,1999).expand_as(data).data.cpu()*data\n    \n    \n    \nif __name__==\'__main__\':\n    fire.Fire()\n\n\n    '"
models/BasicModule.py,0,"b""#coding:utf8\n\nimport torch as t\nimport time\n\nclass BasicModule(t.nn.Module):\n    '''\n    \xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86nn.Module,\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86save\xe5\x92\x8cload\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x96\xb9\xe6\xb3\x95\n    '''\n\n    def __init__(self):\n        super(BasicModule,self).__init__()\n        self.model_name=str(type(self))# \xe9\xbb\x98\xe8\xae\xa4\xe5\x90\x8d\xe5\xad\x97\n\n    def load(self, path,change_opt=True):\n        print path\n        data = t.load(path)\n        if 'opt' in data:\n            # old_opt_stats = self.opt.state_dict() \n            if change_opt:\n                \n                self.opt.parse(data['opt'],print_=False)\n                self.opt.embedding_path=None\n                self.__init__(self.opt)\n            # self.opt.parse(old_opt_stats,print_=False)\n            self.load_state_dict(data['d'])\n        else:\n            self.load_state_dict(data)\n        return self.cuda()\n\n    def save(self, name=None,new=False):\n        prefix = 'checkpoints/' + self.model_name + '_' +self.opt.type_+'_'\n        if name is None:\n            name = time.strftime('%m%d_%H:%M:%S.pth')\n        path = prefix+name\n\n        if new:\n            data = {'opt':self.opt.state_dict(),'d':self.state_dict()}\n        else:\n            data=self.state_dict()\n\n        t.save(data, path)\n        return path\n\n    def get_optimizer(self,lr1,lr2=0,weight_decay = 0):\n        ignored_params = list(map(id, self.encoder.parameters()))\n        base_params = filter(lambda p: id(p) not in ignored_params,\n                        self.parameters())\n        if lr2 is None: lr2 = lr1*0.5 \n        optimizer = t.optim.Adam([\n                dict(params=base_params,weight_decay = weight_decay,lr=lr1),\n                {'params': self.encoder.parameters(), 'lr': lr2}\n            ])\n        return optimizer\n \n    # def save2(self,name=None):\n    #     prefix = 'checkpoints/' + self.model_name + '_'\n    #     if name is None:\n    #         name = time.strftime('%m%d_%H:%M:%S.pth')\n    #     path = prefix+name\n    #     data = {'opt':self.opt.state_dict(),'d':self.state_dict()}\n    #     t.save(data, path)\n    #     return path\n    # # def load2(self,path):\n    # #     data = t.load(path)\n    # #     self.__init__(data['opt'])\n    # #     self.load_state_dict(data['d'])"""
models/CNNText_inception.py,2,"b""from .BasicModule import BasicModule\nimport torch as t\nimport torch\nimport numpy as np\nfrom torch import nn\nfrom collections import OrderedDict\n\nclass Inception(nn.Module):\n    def __init__(self,cin,co,relu=True,norm=True):\n        super(Inception, self).__init__()\n        assert(co%4==0)\n        cos=[co/4]*4\n        self.activa=nn.Sequential()\n        if norm:self.activa.add_module('norm',nn.BatchNorm1d(co))\n        if relu:self.activa.add_module('relu',nn.ReLU(True))\n        self.branch1 =nn.Sequential(OrderedDict([\n            ('conv1', nn.Conv1d(cin,cos[0], 1,stride=1)),\n            ])) \n        self.branch2 =nn.Sequential(OrderedDict([\n            ('conv1', nn.Conv1d(cin,cos[1], 1)),\n            ('norm1', nn.BatchNorm1d(cos[1])),\n            ('relu1', nn.ReLU(inplace=True)),\n            ('conv3', nn.Conv1d(cos[1],cos[1], 3,stride=1,padding=1)),\n            ]))\n        self.branch3 =nn.Sequential(OrderedDict([\n            ('conv1', nn.Conv1d(cin,cos[2], 3,padding=1)),\n            ('norm1', nn.BatchNorm1d(cos[2])),\n            ('relu1', nn.ReLU(inplace=True)),\n            ('conv3', nn.Conv1d(cos[2],cos[2], 5,stride=1,padding=2)),\n            ]))\n        self.branch4 =nn.Sequential(OrderedDict([\n            #('pool',nn.MaxPool1d(2)),\n            ('conv3', nn.Conv1d(cin,cos[3], 3,stride=1,padding=1)),\n            ]))\n    def forward(self,x):\n        branch1=self.branch1(x)\n        branch2=self.branch2(x)\n        branch3=self.branch3(x)\n        branch4=self.branch4(x)\n        result=self.activa(torch.cat((branch1,branch2,branch3,branch4),1))\n        return result\nclass CNNText_inception(BasicModule):\n    def __init__(self, opt ):\n        super(CNNText_inception, self).__init__()\n        incept_dim=opt.inception_dim\n        self.model_name = 'CNNText_inception'\n        self.opt=opt\n        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n        self.title_conv=nn.Sequential(\n            Inception(opt.embedding_dim,incept_dim),#(batch_size,64,opt.title_seq_len)->(batch_size,32,(opt.title_seq_len)/2)\n            Inception(incept_dim,incept_dim),\n            nn.MaxPool1d(opt.title_seq_len)\n        )\n        self.content_conv=nn.Sequential(\n            Inception(opt.embedding_dim,incept_dim),#(batch_size,64,opt.content_seq_len)->(batch_size,64,(opt.content_seq_len)/2)\n            #Inception(incept_dim,incept_dim),#(batch_size,64,opt.content_seq_len/2)->(batch_size,32,(opt.content_seq_len)/4)\n            Inception(incept_dim,incept_dim),\n            nn.MaxPool1d(opt.content_seq_len)\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(incept_dim*2,opt.linear_hidden_size),\n            nn.BatchNorm1d(opt.linear_hidden_size),\n            nn.ReLU(inplace=True),\n            nn.Linear(opt.linear_hidden_size,opt.num_classes)\n        )\n        if opt.embedding_path:\n            print('load embedding')\n            self.encoder.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path)['vector']))\n \n    def forward(self,title,content):\n        title = self.encoder(title)\n        content=self.encoder(content)\n        if self.opt.static:\n            title=title.detach()\n            content=content.detach(0)\n        title_out=self.title_conv(title.permute(0,2,1))\n        content_out=self.content_conv(content.permute(0,2,1))\n        out=torch.cat((title_out,content_out),1).view(content_out.size(0), -1)\n        out=self.fc(out)\n        return out\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        """
models/FastText3.py,1,"b""from .BasicModule import BasicModule\r\nimport torch as t\r\nimport torch\r\nimport numpy as np\r\nfrom torch import nn\r\nfrom collections import OrderedDict\r\nclass FastText3(BasicModule):\r\n    def __init__(self, opt ):\r\n        super(FastText3, self).__init__()\r\n        self.model_name = 'FastText3'\r\n        self.opt=opt\r\n        self.pre1 = nn.Sequential(\r\n            nn.Linear(opt.embedding_dim,opt.embedding_dim*2),\r\n            nn.BatchNorm1d(opt.embedding_dim*2),\r\n            nn.ReLU(True)\r\n        )\r\n\r\n        self.pre2 = nn.Sequential(\r\n            nn.Linear(opt.embedding_dim,opt.embedding_dim*2),\r\n            nn.BatchNorm1d(opt.embedding_dim*2),\r\n            nn.ReLU(True)\r\n        )\r\n        # self.pre_fc = nn.Linear(opt.embedding_dim,opt.embedding_dim*2)\r\n        # self.bn = nn.BatchNorm1d(opt.embedding_dim*2)\r\n        # self.pre_fc2 = nn.Linear(opt.embedding_dim,opt.embedding_dim*2)\r\n        # self.bn2 = nn.BatchNorm1d(opt.embedding_dim*2) \r\n\r\n        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\r\n        self.fc = nn.Sequential(\r\n            nn.Linear(opt.embedding_dim*4,opt.linear_hidden_size),\r\n            nn.BatchNorm1d(opt.linear_hidden_size),\r\n            nn.ReLU(inplace=True),\r\n            nn.Linear(opt.linear_hidden_size,opt.num_classes)\r\n        )\r\n        if opt.embedding_path:\r\n            print('load embedding')\r\n            self.encoder.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path)['vector']))\r\n \r\n    def forward(self,title,content):\r\n        title_em = self.encoder(title)\r\n        content_em = self.encoder(content)\r\n        title_size = title_em.size()\r\n        content_size = content_em.size()\r\n        \r\n \r\n    \r\n        title_2 = self.pre1(title_em.contiguous().view(-1,256)).view(title_size[0],title_size[1],-1)\r\n        content_2 = self.pre2(content_em.contiguous().view(-1,256)).view(content_size[0],content_size[1],-1)\r\n\r\n\r\n        title_ = t.mean(title_2,dim=1)\r\n        content_ = t.mean(content_2,dim=1)\r\n        inputs=t.cat((title_.squeeze(),content_.squeeze()),1)\r\n        out=self.fc(inputs)\r\n        # content_out=self.content_fc(content.view(content.size(0),-1))\r\n        # out=torch.cat((title_out,content_out),1)\r\n        # out=self.fc(out)\r\n        return out\r\n        \r\n     \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n """
models/LSTMText.py,0,"b""from .BasicModule import BasicModule\nimport torch as t\nimport numpy as np\nfrom torch import nn\n\n\ndef kmax_pooling(x, dim, k):\n    index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n    return x.gather(dim, index)\n\nclass LSTMText(BasicModule): \n    def __init__(self, opt ):\n        super(LSTMText, self).__init__()\n        self.model_name = 'LSTMText'\n        self.opt=opt\n\n        kernel_size = opt.kernel_size\n        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n\n        self.title_lstm = nn.LSTM(input_size = opt.embedding_dim,\\\n                            hidden_size = opt.hidden_size,\n                            num_layers = opt.num_layers,\n                            bias = True,\n                            batch_first = False,\n                            # dropout = 0.5,\n                            bidirectional = True\n                            )\n        self.content_lstm =nn.LSTM(  input_size = opt.embedding_dim,\\\n                            hidden_size = opt.hidden_size,\n                            num_layers = opt.num_layers,\n                            bias = True,\n                            batch_first = False,\n                            # dropout = 0.5,\n                            bidirectional = True\n                            )\n\n        # self.dropout = nn.Dropout()\n        self.fc = nn.Sequential(\n            nn.Linear(opt.kmax_pooling*(opt.hidden_size*2*2),opt.linear_hidden_size),\n            nn.BatchNorm1d(opt.linear_hidden_size),\n            nn.ReLU(inplace=True),\n            nn.Linear(opt.linear_hidden_size,opt.num_classes)\n        )\n        # self.fc = nn.Linear(3 * (opt.title_dim+opt.content_dim), opt.num_classes)\n        if opt.embedding_path:\n            self.encoder.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path)['vector']))\n \n    def forward(self, title, content):\n        title = self.encoder(title)\n        content = self.encoder(content)\n        if self.opt.static:\n            title=title.detach()\n            content=content.detach()\n        \n        title_out = self.title_lstm(title.permute(1,0,2))[0].permute(1,2,0) \n\n        content_out = self.content_lstm(content.permute(1,0,2))[0].permute(1,2,0)\n\n\n        title_conv_out = kmax_pooling((title_out),2,self.opt.kmax_pooling)\n        content_conv_out = kmax_pooling((content_out),2,self.opt.kmax_pooling)\n\n        conv_out = t.cat((title_conv_out,content_conv_out),dim=1)\n        reshaped = conv_out.view(conv_out.size(0), -1)\n        logits = self.fc((reshaped))\n        return logits\n\n    # def get_optimizer(self):  \n    #    return  t.optim.Adam([\n    #             {'params': self.title_conv.parameters()},\n    #             {'params': self.content_conv.parameters()},\n    #             {'params': self.fc.parameters()},\n    #             {'params': self.encoder.parameters(), 'lr': 5e-4}\n    #         ], lr=self.opt.lr)\n    # # end method forward\n\n\n \nif __name__ == '__main__':\n    from ..config import opt\n    m = CNNText(opt)\n    title = t.autograd.Variable(t.arange(0,500).view(10,50)).long()\n    content = t.autograd.Variable(t.arange(0,2500).view(10,250)).long()\n    o = m(title,content)\n    print(o.size())\n"""
models/MultiCNNTextBNDeep.py,0,"b""from .BasicModule import BasicModule\nimport torch as t\nimport numpy as np\nfrom torch import nn\n\nkernel_sizes =  [1,2,3,4]\nkernel_sizes2 = [1,2,3,4]\nclass MultiCNNTextBNDeep(BasicModule): \n    def __init__(self, opt ):\n        super(MultiCNNTextBNDeep, self).__init__()\n        self.model_name = 'MultiCNNTextBNDeep'\n        self.opt=opt\n        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n\n        title_convs = [ nn.Sequential(\n                                nn.Conv1d(in_channels = opt.embedding_dim,\n                                        out_channels = opt.title_dim,\n                                        kernel_size = kernel_size),\n                                nn.BatchNorm1d(opt.title_dim),\n                                nn.ReLU(inplace=True),\n\n                                nn.Conv1d(in_channels = opt.title_dim,\n                                out_channels = opt.title_dim,\n                                kernel_size = kernel_size),\n                                nn.BatchNorm1d(opt.title_dim),\n                                nn.ReLU(inplace=True),\n                                nn.MaxPool1d(kernel_size = (opt.title_seq_len - kernel_size*2 + 2))\n                            )\n         for kernel_size in kernel_sizes]\n\n        content_convs = [ nn.Sequential(\n                                nn.Conv1d(in_channels = opt.embedding_dim,\n                                        out_channels = opt.content_dim,\n                                        kernel_size = kernel_size),\n                                nn.BatchNorm1d(opt.content_dim),\n                                nn.ReLU(inplace=True),\n\n                                nn.Conv1d(in_channels = opt.content_dim,\n                                        out_channels = opt.content_dim,\n                                        kernel_size = kernel_size),\n                                nn.BatchNorm1d(opt.content_dim),\n                                nn.ReLU(inplace=True),\n                                nn.MaxPool1d(kernel_size = (opt.content_seq_len - kernel_size*2 + 2))\n                            )\n            for kernel_size in kernel_sizes ]\n\n        self.title_convs = nn.ModuleList(title_convs)\n        self.content_convs = nn.ModuleList(content_convs)\n\n        self.fc = nn.Sequential(\n            nn.Linear(len(kernel_sizes)*(opt.title_dim+opt.content_dim),opt.linear_hidden_size),\n            nn.BatchNorm1d(opt.linear_hidden_size),\n            nn.ReLU(inplace=True),\n            nn.Linear(opt.linear_hidden_size,opt.num_classes)\n        )\n        \n\n        if opt.embedding_path:\n            self.encoder.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path)['vector']))\n\n    def forward(self, title, content):\n        title = self.encoder(title)\n        content = self.encoder(content)\n\n        if self.opt.static:\n            title.detach()\n            content.detach()\n\n        title_out = [ title_conv(title.permute(0, 2, 1)) for title_conv in self.title_convs]\n        content_out = [ content_conv(content.permute(0,2,1)) for content_conv in self.content_convs]\n        conv_out = t.cat((title_out+content_out),dim=1)\n        reshaped = conv_out.view(conv_out.size(0), -1)\n        logits = self.fc((reshaped))\n        return logits\n\n    # def get_optimizer(self):  \n    #    return  t.optim.Adam([\n    #             {'params': self.title_conv.parameters()},\n    #             {'params': self.content_conv.parameters()},\n    #             {'params': self.fc.parameters()},\n    #             {'params': self.encoder.parameters(), 'lr': 5e-4}\n    #         ], lr=self.opt.lr)\n    # # end method forward\n\n\n \nif __name__ == '__main__':\n    from ..config import opt\n    opt.content_dim = 128\n    opt.title_dim = 100\n    m = MultiCNNText(opt)\n    title = t.autograd.Variable(t.arange(0,500).view(10,50)).long()\n    content = t.autograd.Variable(t.arange(0,2500).view(10,250)).long()\n    o = m(title,content)\n    print(o.size())\n"""
models/MultiModelAll.py,0,"b""#coding:utf8\nfrom .BasicModule import BasicModule\nimport torch as t\nimport numpy as np\nfrom torch import nn\nimport models\nfrom config import Config \n\nclass MultiModelAll(BasicModule): \n    def __init__(self, opt ):\n        super(MultiModelAll, self).__init__()\n        self.model_name = 'MultiModelAll'\n        self.opt=opt\n        # self.char_models = []\n        self.models = []\n        self.word_embedding=nn.Embedding(411720,256)\n        self.char_embedding=nn.Embedding(11973,256)\n        if opt.embedding_path:\n            self.word_embedding.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path.replace('char','word'))['vector']))\n            self.char_embedding.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path.replace('word','char'))['vector']))\n\n        for _name,_path in zip(opt.model_names, opt.model_paths):\n            tmp_config = Config().parse(opt.state_dict(),print_=False)\n            tmp_config.embedding_path=None\n            _model = getattr(models,_name)(tmp_config)\n            # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe5\xad\x90\xe6\xa8\xa1\xe5\x9e\x8b\n            if _path is not None:\n                _model.load(_path)\n            # \xe6\x89\x80\xe6\x9c\x89\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe5\xad\x90\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84embedding\xe5\x85\xb1\xe4\xba\xab\n            _model.encoder=(self.char_embedding if _model.opt.type_=='char' else self.word_embedding)\n            self.models.append(_model)\n        self.models = nn.ModuleList(self.models)\n        \n        self.model_num = len(self.models)\n        self.weights = nn.Parameter(t.ones(opt.num_classes,self.model_num))\n        assert self.opt.loss=='bceloss'\n        # self.weight =[nn.Parameter(t.ones(self.model_num)/self.model_num) for _ in range(self.model_num)]\n        # self.label_weight = nn.Parameter(t.eye(opt.num_classes))\n    def reinit(self):\n        pass\n    def load(self,path,**kwargs):\n        self.load_state_dict(t.load(path)['d'])\n    def forward(self, char, word):\n        weights = t.nn.functional.softmax(self.weights)\n        outs =[]\n        for ii,model in enumerate(self.models):\n            if model.opt.type_=='char':\n                out = t.sigmoid(model(*char))\n            else:\n                out=t.sigmoid(model(*word))\n\n            out = out*(weights[:,ii].contiguous().view(1,-1).expand_as(out))\n            outs.append(out)\n            # outs = [t.sigmoid(model(title,content))*weight  for model in  self.models]\n\n        # outs = [model(title,content)*weight.view(1,1).expand(title.size(0),self.opt.num_classes).mm(self.label_weight) for model,weight in zip(self.models,self.weight)]\n        return sum(outs)\n\n \n    def get_optimizer(self,lr1=1e-3,lr2=3e-4,lr3=3e-4,weight_decay = 0):\n        encoders = list(self.char_embedding.parameters())+list(self.word_embedding.parameters())\n        other_params = [ param_ for model_ in self.models \n                                for name_,param_ in model_.named_parameters()\n                                if name_.find('encoder')==-1] \n        # new_params = [self.weights ,self.label_weight]\n        new_params = [self.weights]\n\n        optimizer = t.optim.Adam([\n                dict(params=other_params,weight_decay = weight_decay,lr=lr1),# conv \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5 \n                dict(params=encoders,weight_decay = weight_decay,lr=lr2), # embedding\n                dict(params=new_params,weight_decay = weight_decay,lr=lr3), # \xe6\x9d\x83\xe9\x87\x8d\n            ])\n        return optimizer\n \n    # def get_optimizer(self):  \n    #    return  t.optim.Adam([\n    #             {'params': self.title_conv.parameters()},\n    #             {'params': self.content_conv.parameters()},\n    #             {'params': self.fc.parameters()},\n    #             {'params': self.encoder.parameters(), 'lr': 5e-4}\n    #         ], lr=self.opt.lr)\n    # # end method forward\n\n\n \nif __name__ == '__main__':\n    from ..config import opt\n    m = CNNText(opt)\n    title = t.autograd.Variable(t.arange(0,500).view(10,50)).long()\n    content = t.autograd.Variable(t.arange(0,2500).view(10,250)).long()\n    o = m(title,content)\n    print(o.size())\n"""
models/MultiModelAll2.py,0,"b""#coding:utf8\nfrom .BasicModule import BasicModule\nimport torch as t\nimport numpy as np\nfrom torch import nn\nimport models\nfrom config import Config \n\nclass MultiModelAll2(BasicModule): \n    def __init__(self, opt ):\n        super(MultiModelAll2, self).__init__()\n        self.model_name = 'MultiModelAll2'\n        self.opt=opt\n        self.models = []\n\n        for _name,_path in zip(opt.model_names, opt.model_paths):\n            tmp_config = Config().parse(opt.state_dict(),print_=False)\n            # tmp_config.static=True\n            tmp_config.embedding_path=None\n            _model = getattr(models,_name)(tmp_config)\n            if _path is not None:\n                _model.load(_path)\n            self.models.append(_model)\n\n        self.models = nn.ModuleList(self.models)\n        self.model_num = len(self.models)\n        self.weights = nn.Parameter(t.ones(opt.num_classes,self.model_num))\n        assert self.opt.loss=='bceloss'\n\n        self.eval()\n\n    def reinit(self):\n        pass\n\n\n    def forward(self, char, word):\n        weights = t.nn.functional.softmax(self.weights)\n        outs =[]\n        for ii,model in enumerate(self.models):\n            if model.opt.type_=='char':\n                out = t.sigmoid(model(*char))\n            else:\n                out=t.sigmoid(model(*word))\n            if self.opt.static:     out = out.detach()\n            out = out*(weights[:,ii].contiguous().view(1,-1).expand_as(out))\n            outs.append(out)\n            # outs = [t.sigmoid(model(title,content))*weight  for model in  self.models]\n\n        # outs = [model(title,content)*weight.view(1,1).expand(title.size(0),self.opt.num_classes).mm(self.label_weight) for model,weight in zip(self.models,self.weight)]\n        return sum(outs)\n\n \n    def get_optimizer(self,lr1=1e-4,lr2=1e-4,lr3=0,weight_decay = 0):\n        # encoders = list(self.char_embedding.parameters())+list(self.word_embedding.parameters())\n        other_params = [ param_ for model_ in self.models \n                                for name_,param_ in model_.named_parameters()\n                                if name_.find('encoder')==-1] \n        encoders = [ param_ for model_ in self.models \n                                for name_,param_ in model_.named_parameters()\n                                if name_.find('encoder')!=-1] \n        new_params = [self.weights]\n\n        optimizer = t.optim.Adam([\n                dict(params=other_params,weight_decay = weight_decay,lr=lr1),# conv \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5 \n                dict(params=encoders,weight_decay = weight_decay,lr=lr2), # embedding\n                dict(params=new_params,weight_decay = weight_decay,lr=lr3), # \xe6\x9d\x83\xe9\x87\x8d\n            ])\n        return optimizer\n \n    # def get_optimizer(self):  \n    #    return  t.optim.Adam([\n    #             {'params': self.title_conv.parameters()},\n    #             {'params': self.content_conv.parameters()},\n    #             {'params': self.fc.parameters()},\n    #             {'params': self.encoder.parameters(), 'lr': 5e-4}\n    #         ], lr=self.opt.lr)\n    # # end method forward\n\n\n \nif __name__ == '__main__':\n    from ..config import opt\n    m = CNNText(opt)\n    title = t.autograd.Variable(t.arange(0,500).view(10,50)).long()\n    content = t.autograd.Variable(t.arange(0,2500).view(10,250)).long()\n    o = m(title,content)\n    print(o.size())\n"""
models/MultiModelAll4zhihu.py,0,"b""#coding:utf8\nfrom .BasicModule import BasicModule\nimport torch as t\nimport numpy as np\nfrom torch import nn\nimport models\nfrom config import Config \n\nclass MultiModelAll4zhihu(BasicModule): \n    def __init__(self, opt ):\n        super(MultiModelAll4zhihu, self).__init__()\n        self.model_name = 'MultiModelAll4zhihu'\n        self.opt=opt\n        self.models = []\n        self.word_embedding=nn.Embedding(411720,256)\n        self.char_embedding=nn.Embedding(11973,256)\n        model_opts = t.load(opt.model_path+'.json')\n\n        for _name,_path,model_opt_ in zip(opt.model_names, opt.model_paths,model_opts):\n            tmp_config = Config().parse(model_opt_,print_=False)\n            tmp_config.embedding_path=None\n            _model = getattr(models,_name)(tmp_config)\n            _model.encoder=(self.char_embedding if _model.opt.type_=='char' else self.word_embedding)\n            self.models.append(_model)\n            \n        self.models = nn.ModuleList(self.models)\n        self.model_num = len(self.models)\n        self.weights = nn.Parameter(t.ones(opt.num_classes,self.model_num))\n        self.load(opt.model_path)\n\n\n    def load(self,path,**kwargs):\n        self.load_state_dict(t.load(path)['d'])\n    def forward(self, char, word):\n        weights = t.nn.functional.softmax(self.weights)\n        outs =[]\n        for ii,model in enumerate(self.models):\n            if model.opt.type_=='char':\n                out = t.sigmoid(model(*char))\n            else:\n                out=t.sigmoid(model(*word))\n\n            out = out*(weights[:,ii].contiguous().view(1,-1).expand_as(out))\n            outs.append(out)\n\n        return sum(outs)\n\n \n    def get_optimizer(self,lr1=1e-3,lr2=3e-4,lr3=3e-4,weight_decay = 0):\n        encoders = list(self.char_embedding.parameters())+list(self.word_embedding.parameters())\n        other_params = [ param_ for model_ in self.models \n                                for name_,param_ in model_.named_parameters()\n                                if name_.find('encoder')==-1] \n\n        new_params = [self.weights]\n\n        optimizer = t.optim.Adam([\n                dict(params=other_params,weight_decay = weight_decay,lr=lr1),# conv \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5 \n                dict(params=encoders,weight_decay = weight_decay,lr=lr2), # embedding\n                dict(params=new_params,weight_decay = weight_decay,lr=lr3), # \xe6\x9d\x83\xe9\x87\x8d\n            ])\n        return optimizer\n \n\n\n \nif __name__ == '__main__':\n    from ..config import opt\n    m = CNNText(opt)\n    title = t.autograd.Variable(t.arange(0,500).view(10,50)).long()\n    content = t.autograd.Variable(t.arange(0,2500).view(10,250)).long()\n    o = m(title,content)\n    print(o.size())\n"""
models/RCNN.py,0,"b""from .BasicModule import BasicModule\nimport torch as t\nimport numpy as np\nfrom torch import nn\n\n\ndef kmax_pooling(x, dim, k):\n    index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n    return x.gather(dim, index)\n\nclass RCNN(BasicModule): \n    def __init__(self, opt ):\n        super(RCNN, self).__init__()\n        self.model_name = 'RCNN'\n        self.opt=opt\n\n        kernel_size = opt.kernel_size\n        self.encoder = nn.Embedding(opt.vocab_size,opt.embedding_dim)\n\n        self.title_lstm = nn.LSTM(input_size = opt.embedding_dim,\\\n                            hidden_size = opt.hidden_size,\n                            num_layers = opt.num_layers,\n\n                            bias = True,\n                            batch_first = False,\n                            # dropout = 0.5,\n                            bidirectional = True\n                            )\n        self.title_conv = nn.Sequential(\n                                nn.Conv1d(in_channels = opt.hidden_size*2 + opt.embedding_dim,\n                                        out_channels = opt.title_dim,\n                                        kernel_size = kernel_size),\n                                nn.BatchNorm1d(opt.title_dim),\n                                nn.ReLU(inplace=True),\n                                nn.Conv1d(in_channels = opt.title_dim,\n                                        out_channels = opt.title_dim,\n                                        kernel_size = kernel_size),\n                                nn.BatchNorm1d(opt.title_dim),\n                                nn.ReLU(inplace=True),\n                                # nn.MaxPool1d(kernel_size = (opt.title_seq_len - kernel_size + 1))\n                            )\n\n        self.content_lstm =nn.LSTM(  input_size = opt.embedding_dim,\\\n                            hidden_size = opt.hidden_size,\n                            num_layers = opt.num_layers,\n                            bias = True,\n                            batch_first = False,\n                            # dropout = 0.5,\n                            bidirectional = True\n                            )\n\n        self.content_conv = nn.Sequential(\n            nn.Conv1d(in_channels = opt.hidden_size*2 + opt.embedding_dim,\n                      out_channels = opt.content_dim,\n                      kernel_size =  kernel_size),\n            nn.BatchNorm1d(opt.content_dim),\n            nn.ReLU(inplace=True),\n\n            nn.Conv1d(in_channels = opt.content_dim,\n                      out_channels = opt.content_dim,\n                      kernel_size =  kernel_size),\n            nn.BatchNorm1d(opt.content_dim),\n            nn.ReLU(inplace=True),\n\n            \n            # nn.MaxPool1d(kernel_size = (opt.content_seq_len - opt.kernel_size + 1))\n        )\n        # self.dropout = nn.Dropout()\n        self.fc = nn.Sequential(\n            nn.Linear(opt.kmax_pooling*(opt.title_dim+opt.content_dim),opt.linear_hidden_size),\n            nn.BatchNorm1d(opt.linear_hidden_size),\n            nn.ReLU(inplace=True),\n            nn.Linear(opt.linear_hidden_size,opt.num_classes)\n        )\n        # self.fc = nn.Linear(3 * (opt.title_dim+opt.content_dim), opt.num_classes)\n        if opt.embedding_path:\n            self.encoder.weight.data.copy_(t.from_numpy(np.load(opt.embedding_path)['vector']))\n \n    def forward(self, title, content):\n        title = self.encoder(title)\n        content = self.encoder(content)\n        \n        if self.opt.static:\n            title.detach()\n            content.detach()\n\n        title_out = self.title_lstm(title.permute(1,0,2))[0].permute(1,2,0) \n        title_em = title.permute(0,2,1)\n        title_out = t.cat((title_out,title_em),dim=1)\n\n        content_out = self.content_lstm(content.permute(1,0,2))[0].permute(1,2,0)\n        content_em = (content).permute(0,2,1)\n        content_out = t.cat((content_out,content_em),dim=1)\n\n\n        title_conv_out = kmax_pooling(self.title_conv(title_out),2,self.opt.kmax_pooling)\n        content_conv_out = kmax_pooling(self.content_conv(content_out),2,self.opt.kmax_pooling)\n\n        conv_out = t.cat((title_conv_out,content_conv_out),dim=1)\n        reshaped = conv_out.view(conv_out.size(0), -1)\n        logits = self.fc((reshaped))\n        return logits\n\n    # def get_optimizer(self):  \n    #    return  t.optim.Adam([\n    #             {'params': self.title_conv.parameters()},\n    #             {'params': self.content_conv.parameters()},\n    #             {'params': self.fc.parameters()},\n    #             {'params': self.encoder.parameters(), 'lr': 5e-4}\n    #         ], lr=self.opt.lr)\n    # # end method forward\n\n\n \nif __name__ == '__main__':\n    from ..config import opt\n    m = CNNText(opt)\n    title = t.autograd.Variable(t.arange(0,500).view(10,50)).long()\n    content = t.autograd.Variable(t.arange(0,2500).view(10,250)).long()\n    o = m(title,content)\n    print(o.size())\n"""
models/__init__.py,0,"b'from .loss import multilabelloss,bceloss,multilabel_marginloss\nfrom .CNNText import CNNText\nfrom .MultiCNNText import MultiCNNText\nfrom .MultiCNNTextMultiLayer import MultiText \nfrom .RNNText import RNNText\nfrom .RNNText2 import RNNText2\nfrom .RNNTextBN import RNNTextBN\nfrom .MultiCNNTextBN import MultiCNNTextBN\nfrom .MultiCNNTextBNDeep import MultiCNNTextBNDeep\nfrom .MultiCNNTextBNDeep_Copy1 import MultiCNNTextBNDeep as MultiCNNTextBNDeep_copy\nfrom .RCNN import RCNN\n# from .CNNText_tmp import CNNText_tmp\nfrom .CNNText_inception import CNNText_inception\nfrom .LSTMText import LSTMText\n# from .InceptionText import InceptionText\nfrom .MultiModel import MultiModel\nfrom .FastText import FastText\nfrom .MultiModelAll import MultiModelAll\nfrom .MultiModelAll2 import MultiModelAll2\nfrom .StackLayer2 import StackLayer2\nfrom .CNN_Inception_new import CNNTextInception_new\nfrom .FourModel import FourModel\nfrom .BoostModel import BoostModel\n# from .BoostMod\nfrom .FastText2 import FastText2\nfrom .FastText3 import FastText3'"
models/alias_multinomial.py,4,"b""import torch\r\nimport numpy as np\r\n\r\nclass AliasMethod(object):\r\n    '''\r\n        From: https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\r\n    '''\r\n    def __init__(self, probs):\r\n\r\n        K = len(probs)\r\n        self.prob = torch.zeros(K)\r\n        self.alias = torch.LongTensor([0]*K)\r\n\r\n        # Sort the data into the outcomes with probabilities\r\n        # that are larger and smaller than 1/K.\r\n        smaller = []\r\n        larger = []\r\n        for kk, prob in enumerate(probs):\r\n            self.prob[kk] = K*prob\r\n            if self.prob[kk] < 1.0:\r\n                smaller.append(kk)\r\n            else:\r\n                larger.append(kk)\r\n\r\n        # Loop though and create little binary mixtures that\r\n        # appropriately allocate the larger outcomes over the\r\n        # overall uniform mixture.\r\n        while len(smaller) > 0 and len(larger) > 0:\r\n            small = smaller.pop()\r\n            large = larger.pop()\r\n\r\n            self.alias[small] = large\r\n            self.prob[large] = (self.prob[large] - 1.0) + self.prob[small]\r\n\r\n            if self.prob[large] < 1.0:\r\n                smaller.append(large)\r\n            else:\r\n                larger.append(large)\r\n\r\n        for last_one in smaller+larger:\r\n            self.prob[last_one] = 1\r\n\r\n    def draw(self, N):\r\n        '''\r\n            Draw N samples from multinomial\r\n        '''\r\n        K = self.alias.size(0)\r\n\r\n        kk = torch.LongTensor(np.random.randint(0,K, size=N))\r\n        prob = self.prob.index_select(0, kk)\r\n        alias = self.alias.index_select(0, kk)\r\n        # b is whether a random number is greater than q\r\n        b = torch.bernoulli(prob)\r\n        oq = kk.mul(b.long())\r\n        oj = alias.mul((1-b).long())\r\n\r\n        return oq + oj"""
models/loss.py,0,"b'import torch as t\ndef multilabelloss():\n    return  t.nn.MultiLabelSoftMarginLoss()\ndef multilabel_marginloss():\n    return t.nn.MultiLabelMarginLoss()\ndef bceloss():\n    return t.nn.BCELoss() \n\ndef weight_loss():\n    pass\n\n\ndef identityloss():\n    class Loss:\n        def __call__(self,x):\n            return x\n    return Loss()'"
models/nce.py,8,"b'# the NCE module written for pytorch\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nfrom alias_multinomial import AliasMethod\r\n\r\n\r\nclass NCELoss(nn.Module):\r\n    """"""Noise Contrastive Estimation\r\n    NCE is to eliminate the computational cost of softmax\r\n    normalization.\r\n    Ref:\r\n        X.Chen etal Recurrent neural network language\r\n        model training with noise contrastive estimation\r\n        for speech recognition\r\n        https://core.ac.uk/download/pdf/42338485.pdf\r\n    Attributes:\r\n        nhidden: hidden size of LSTM(a.k.a the output size)\r\n        ntokens: vocabulary size\r\n        noise: the distribution of noise\r\n        noise_ratio: $\\frac{#noises}{#real data samples}$ (k in paper)\r\n        norm_term: the normalization term (lnZ in paper)\r\n        size_average: average the loss by batch size\r\n        decoder: the decoder matrix\r\n    Shape:\r\n        - noise: :math:`(V)` where `V = vocabulary size`\r\n        - decoder: :math:`(E, V)` where `E = embedding size`\r\n    """"""\r\n\r\n    def __init__(self,\r\n                 ntokens,\r\n                 nhidden,\r\n                 noise,\r\n                 noise_ratio=10,\r\n                 norm_term=9,\r\n                 size_average=True,\r\n                 decoder_weight=None,\r\n                 ):\r\n        super(NCELoss, self).__init__()\r\n\r\n        self.noise = noise\r\n        self.alias = AliasMethod(noise)\r\n        self.noise_ratio = noise_ratio\r\n        self.norm_term = norm_term\r\n        self.ntokens = ntokens\r\n        self.size_average = size_average\r\n        self.decoder = IndexLinear(nhidden, ntokens)\r\n        # Weight tying\r\n        if decoder_weight:\r\n            self.decoder.weight = decoder_weight\r\n\r\n    def forward(self, input, target=None):\r\n        """"""compute the loss with output and the desired target\r\n        Parameters:\r\n            input: the output of the RNN model, being an predicted embedding\r\n            target: the supervised training label.\r\n        Shape:\r\n            - input: :math:`(N, E)` where `N = number of tokens, E = embedding size`\r\n            - target: :math:`(N)`\r\n        Return:\r\n            the scalar NCELoss Variable ready for backward\r\n        """"""\r\n\r\n        length = target.size(0)\r\n        if self.training:\r\n            assert input.size(0) == target.size(0)\r\n\r\n            noise_samples = self.alias.draw(self.noise_ratio).cuda().unsqueeze(0).repeat(length, 1)\r\n            data_prob, noise_in_data_probs = self._get_prob(input, target.data, noise_samples)\r\n            noise_probs = Variable(\r\n                self.noise[noise_samples.view(-1)].view_as(noise_in_data_probs)\r\n            )\r\n\r\n            rnn_loss = torch.log(data_prob / (\r\n                data_prob + self.noise_ratio * Variable(self.noise[target.data]\r\n            )))\r\n\r\n            noise_loss = torch.sum(\r\n                torch.log((self.noise_ratio * noise_probs) / (noise_in_data_probs + self.noise_ratio * noise_probs)), 1\r\n            )\r\n\r\n            loss = -1 * torch.sum(rnn_loss + noise_loss)\r\n\r\n        else:\r\n            out = self.decoder(input, indices=target.unsqueeze(1))\r\n            nll = out.sub(self.norm_term)\r\n            loss = -1 * nll.sum()\r\n\r\n        if self.size_average:\r\n            loss = loss / length\r\n        return loss\r\n\r\n    def _get_prob(self, embedding, target_idx, noise_idx):\r\n        """"""Get the NCE estimated probability for target and noise\r\n        Shape:\r\n            - Embedding: :math:`(N, E)`\r\n            - Target_idx: :math:`(N)`\r\n            - Noise_idx: :math:`(N, N_r)` where `N_r = noise ratio`\r\n        """"""\r\n\r\n        embedding = embedding\r\n        indices = Variable(\r\n            torch.cat([target_idx.unsqueeze(1), noise_idx], dim=1)\r\n        )\r\n        probs = self.decoder(embedding, indices)\r\n\r\n        probs = probs.sub(self.norm_term).exp()\r\n        return probs[:,0], probs[:,1:]\r\n\r\n\r\nclass IndexLinear(nn.Linear):\r\n    """"""A linear layer that only decodes the results of provided indices\r\n    Args:\r\n        input: the list of embedding\r\n        indices: the indices of interests.\r\n    Shape:\r\n        - Input :math:`(N, in\\_features)`\r\n        - Indices :math:`(N, 1+N_r)` where `max(M) <= N`\r\n    Return:\r\n        - out :math:`(N, 1+N_r)`\r\n    """"""\r\n\r\n    def forward(self, input, indices=None):\r\n        """"""\r\n        Shape:\r\n            - target_batch :math:`(N, E, 1+N_r)`where `N = length, E = embedding size, N_r = noise ratio`\r\n        """"""\r\n\r\n        if indices is None:\r\n            return super(IndexLinear, self).forward(input)\r\n        # the pytorch\'s [] operator BP can\'t correctly\r\n        input = input.unsqueeze(1)\r\n        target_batch = self.weight.index_select(0, indices.view(-1)).view(indices.size(0), indices.size(1), -1).transpose(1,2)\r\n        bias = self.bias.index_select(0, indices.view(-1)).view(indices.size(0), 1, indices.size(1))\r\n        out = torch.baddbmm(1, bias, 1, input, target_batch)\r\n        return out.squeeze()\r\n\r\n    def reset_parameters(self):\r\n        init_range = 0.1\r\n        self.bias.data.fill_(0)\r\n        self.weight.data.uniform_(-init_range, init_range)'"
scripts/ensamble.py,0,"b""#coding:utf8\n\nimport json\nlabel_path =   '/mnt/7/zhihu/ieee_zhihu_cup/data/labels.json'\ntest_data_path='/mnt/7/zhihu/ieee_zhihu_cup/data/test.npz' \ndef ensamble(file1,file2,label_path=label_path,     test_data_path=test_data_path,result_csv=None):\n    import torch as t \n    import numpy as np\n    if result_csv is None:\n        import time\n        result_csv = time.strftime('%y%m%d_%H%M%S.csv')\n    a = t.load(file1)\n    b = t.load(file2)\n    r = 9.0*a+b\n    result = r.topk(5,1)[1]\n    \n    index2qid = np.load(test_data_path)['index2qid'].item()\n    with open(label_path) as f:   label2qid = json.load(f)['id2label']\n    rows = range(result.size(0))\n    for ii,item in enumerate(result):\n        rows[ii] = [index2qid[ii]] + [label2qid[str(_)] for _ in item ]\n    import csv\n    with open(result_csv,'w') as f:\n        writer = csv.writer(f)\n        writer.writerows(rows)\n    \n\nif __name__ == '__main__':\n    import fire\n    fire.Fire()\n\nfiles = ['CNNText_tmp0.4024_char_test.pth',\n 'CNNText_tmp0.4024_char_val.pth',\n 'DeepText0.4103_word_test.pth',\n 'Inception0.4110_word.pth',\n 'LSTMText0.4119_word.pth',\n 'LSTMText0.4031_char_test.pth',\n 'LSTMText0.4119_word_test.pth',\n 'DeepText0.4103_word_val.pth',\n 'CNNText_tmp0.4109_word_val.pth',\n 'LSTMText0.4119_word_val.pth',\n 'RCNN_0.4115_word_test.pth',\n 'RCNN_0.4037_char_val.pth',\n 'LSTMText0.4031_char_val.pth',\n 'RCNN_0.4115_word_val.pth',\n 'RCNN_0.4037_char_test.pth',\n 'CNNText_tmp0.4109_word_test.pth']\n"""
scripts/graph2vec.py,0,"b'# encoding: utf-8\n\'\'\'\nWord2Vec\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xae\x9e\xe7\x8e\xb0\n\'\'\'\nfrom gensim.models import Word2Vec\nimport gensim\nimport re, logging\n\nlogging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n\ndef sentence2words(sentence, stopWords=False, stopWords_set=None):\n    """"""\n    split a sentence into words based on jieba\n    """"""\n    return sentence.split(\',\')\n\nclass MySentences(object):\n    def __init__(self, list_csv):\n        with open(list_csv, \'r\') as f:\n            self.fns = [line.strip() for line in f]\n    def __iter__(self):\n        for line in self.fns:\n            content = line#.replace(""-"","""").replace("" "","""")\n            if len(content) != 0:\n                yield sentence2words(content.strip())\n\n    def train_save(self, list_csv):\n        sentences = MySentences(list_csv)\n        num_features = 256\n        min_word_count = 1\n        num_workers = 20\n        context = 5\n        epoch = 20\n        sample = 1e-5\n        model = Word2Vec(\n            sentences,\n            size=num_features,\n            min_count=min_word_count,\n            workers=num_workers,\n            sample=sample,\n            window=context,\n            iter=epoch,\n        )\n        #model.save(model_fn)\n        return model\n\nif __name__ == ""__main__"":\n    ms = MySentences(\'../ddd/topic_graph.txt\')\n    model = ms.train_save(\'../ddd/topic_graph.txt\')\n    model.save(\'../ddd/topic_vec\')\n    output=open(r\'../ddd/topic_vec.txt\',\'a+\')\n    model.wv.save_word2vec_format(output, binary=False)\n'"
scripts/mer_csv.py,0,"b'#coding:utf8\ndef merge(file1,file2,res=\'result.csv\',k = 1):\n    \'\'\'\n    file1 \xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe9\xab\x98 \xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe4\xb8\xaa\xe6\x95\xb0\xe5\xa4\x9a\n    file2 \xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\xe9\xab\x98 \xe6\x89\x80\xe6\x9c\x89\xe4\xbd\x8d\xe7\xbd\xae\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xe5\xa4\x9a\n    \'\'\'\n    with open(file1) as f:\n        lines1 = f.readlines()\n    with open(file2) as f:\n        lines2 = f.readlines()\n    # import ipdb;\n    # ipdb.set_trace()\n    d1 = {_.split(\',\')[0]:_.strip().split(\',\')[1:] for _ in lines1}\n    d2 = {_.split(\',\')[0]:_.strip().split(\',\')[1:] for _ in lines2}\n    d = {}\n    for key in d1:\n        i1 = d1[key][:k]\n        i2 = [_ for _ in d2[key] if _ not in i1][:(5-k)]\n        d[key] = i1+i2\n    \n    lines = [[_] + _2 for _,_2 in d.iteritems()] \n    write_csv(res,lines)\n    # rows=[0 for _ in range(result.shape[0])]\ndef merge2(file1,file2,res=\'result.csv\',k = 1):\n    \'\'\'\n    file1 \xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe9\xab\x98\n    file2 \xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\xe9\xab\x98\n    \'\'\'\n    with open(file1) as f:\n        lines1 = f.readlines()\n    with open(file2) as f:\n        lines2 = f.readlines()\n    # import ipdb;\n    # ipdb.set_trace()\n    d1 = {_.split(\',\')[0]:_.strip().split(\',\')[1:] for _ in lines1}\n    d2 = {_.split(\',\')[0]:_.strip().split(\',\')[1:] for _ in lines2}\n    d = {}\n    for key in d1:\n        _c1 = d1[key]\n        _c2 = d2[key]\n        i1 = d1[key][:k]\n        i2 = [_ for _ in d2[key] if _ not in i1][:(5-k)]\n        d[key] = i1+i2\n    \n    lines = [[_] + _2 for _,_2 in d.iteritems()] \n    write_csv(res,lines)\n\ndef write_csv(res,lines):\n    import csv\n\n    with open(res, ""w"") as f:\n        csv_writer = csv.writer(f, dialect=""excel"")\n        csv_writer.writerows(lines)\n\n\nif __name__==\'__main__\':\n    import fire\n    fire.Fire()'"
scripts/search.py,0,"b""#coding:utf8\nimport sys\nsys.path.append('../')\nfrom utils import get_score\nimport json\nimport pickle\n\nfile1='/mnt/zhihu/data/RCNN_deep_word_val_4115'\nfile2='/mnt/zhihu/data/rccndeep_char_val_4037.pth'\nfile3='/mnt/zhihu/data/multicnntextbndeep40705_val_word.pth'\n\nlabel_path =   '/mnt/zhihu/data/labels.json'\n# test_data_path='/mnt/zhihu/data/test.npz'\ndef ensamble(file1,file2,file3,label_path=label_path,test_data_path=test_data_path,result_csv=None):\n    import torch as t \n    import numpy as np\n    if result_csv is None:\n        import time\n        result_csv = time.strftime('%y%m%d_%H%M%S.csv')\n    a = t.load(file1)\n    b = t.load(file2)\n    c = t.load(file3)\n\n    index2qid = np.load(test_data_path)['index2qid'].item()\n    with open(label_path) as f: \n          labels_info = json.load(f)\n    qid2label = labels_info['d']\n    # with open(label_path) as f:   label2qid = json.load(f)['id2label']\n    true_labels = [qid2label[index2qid[2999967-200000+ii]] for ii in range(len(a))]\n    # for ii,item in enumerate(result):\n    #     rows[ii] = [index2qid[ii]] + [label2qid[str(_)] for _ in item ]\n\n    previous_best_score = 0.42\n    def target(args):\n        w1,w2 = args\n        r = a + b*w1 +c*w2\n        result = r.topk(5,1)[1]\n        predict_label_and_marked_label_list = [[_1,_2] for _1,_2 in zip(result,true_labels)]\n        score,_,_,_ = get_score(predict_label_and_marked_label_list)\n        print (args,score,_)\n        if score>previous_best_score:\n            previous_best_score = score\n            with open(str(score) ,'wb') as f:\n                pickle.dump(args,f)\n             \n        return -score\n    list_space = [hp.uniform('w1',0,2),hp.uniform('w2',0,2)]\n    best = fmin(new_target,list_space,algo=tpe.suggest,max_evals=50)\n    print best\n    # import csv\n    # with open(result_csv,'w') as f:\n    #     writer = csv.writer(f)\n    #     writer.writerows(rows)\n\n\n\n\nif __name__ == '__main__':\n    import fire\n    fire.Fire()\n"""
utils/__init__.py,0,b'from .calculate_score import get_score\nfrom .optimizer import get_optimizer'
utils/calculate_score.py,0,"b'#coding:utf8\nimport math\ndef get_score(predict_label_and_marked_label_list):\n    """"""\n    :param predict_label_and_marked_label_list: \xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xbb\x84\xe5\x88\x97\xe8\xa1\xa8\xe3\x80\x82\xe4\xbe\x8b\xe5\xa6\x82\n    [ ([1, 2, 3, 4, 5], [4, 5, 6, 7]),\n      ([3, 2, 1, 4, 7], [5, 7, 3])\n     ]\n    \xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xe8\xbf\x99\xe9\x87\x8c predict_label \xe6\x98\xaf\xe5\x8e\xbb\xe9\x87\x8d\xe5\xa4\x8d\xe7\x9a\x84\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82 [1,2,3,2,4,1,6]\xef\xbc\x8c\xe5\x8e\xbb\xe9\x87\x8d\xe5\x90\x8e\xe5\x8f\x98\xe6\x88\x90[1,2,3,4,6]\n    \n    marked_label_list \xe6\x9c\xac\xe8\xba\xab\xe6\xb2\xa1\xe6\x9c\x89\xe9\xa1\xba\xe5\xba\x8f\xe6\x80\xa7\xef\xbc\x8c\xe4\xbd\x86\xe6\x8f\x90\xe4\xba\xa4\xe7\xbb\x93\xe6\x9e\x9c\xe6\x9c\x89\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82\xe4\xb8\x8a\xe4\xbe\x8b\xe7\x9a\x84\xe5\x91\xbd\xe4\xb8\xad\xe6\x83\x85\xe5\x86\xb5\xe5\x88\x86\xe5\x88\xab\xe4\xb8\xba\n    [0\xef\xbc\x8c0\xef\xbc\x8c0\xef\xbc\x8c1\xef\xbc\x8c1]   (4\xef\xbc\x8c5\xe5\x91\xbd\xe4\xb8\xad)\n    [1\xef\xbc\x8c0\xef\xbc\x8c0\xef\xbc\x8c0\xef\xbc\x8c1]   (3\xef\xbc\x8c7\xe5\x91\xbd\xe4\xb8\xad)\n\n    """"""\n    right_label_num = 0  #\xe6\x80\xbb\xe5\x91\xbd\xe4\xb8\xad\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe9\x87\x8f\n    right_label_at_pos_num = [0, 0, 0, 0, 0]  #\xe5\x9c\xa8\xe5\x90\x84\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe4\xb8\x8a\xe6\x80\xbb\xe5\x91\xbd\xe4\xb8\xad\xe6\x95\xb0\xe9\x87\x8f\n    sample_num = 0   #\xe6\x80\xbb\xe9\x97\xae\xe9\xa2\x98\xe6\x95\xb0\xe9\x87\x8f\n    all_marked_label_num = 0    #\xe6\x80\xbb\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe9\x87\x8f\n    for predict_labels, marked_labels in predict_label_and_marked_label_list:\n        sample_num += 1\n        marked_label_set = set(marked_labels)\n        all_marked_label_num += len(marked_label_set)\n        for pos, label in zip(range(0, min(len(predict_labels), 5)), predict_labels):\n            if label in marked_label_set:     #\xe5\x91\xbd\xe4\xb8\xad\n                right_label_num += 1\n                right_label_at_pos_num[pos] += 1\n\n    precision = 0.0\n    for pos, right_num in zip(range(0, 5), right_label_at_pos_num):\n        precision += ((right_num / float(sample_num))) / math.log(2.0 + pos)  # \xe4\xb8\x8b\xe6\xa0\x870-4 \xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 pos1-5 + 1\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x9c\x80\xe7\xbb\x88+2\n    recall = float(right_label_num) / all_marked_label_num\n\n    return (precision * recall) / (precision + recall +0.0000000000001),precision,recall,right_label_at_pos_num\n'"
utils/optimizer.py,0,"b""import torch as t\ndef get_optimizer(model,lr1,lr2=0,weight_decay = 0):\n    ignored_params = list(map(id, model.encoder.parameters()))\n    base_params = filter(lambda p: id(p) not in ignored_params,\n                     model.parameters())\n    if lr2 is None: lr2 = lr1*0.5 \n    optimizer = t.optim.Adam([\n            dict(params=base_params,weight_decay = weight_decay,lr=lr1),\n            {'params': model.encoder.parameters(), 'lr': lr2}\n        ])\n    return optimizer\n\n# def get_optimizer(self):  \n#    return  t.optim.Adam([\n#             {'params': self.title_conv.parameters()},\n#             {'params': self.content_conv.parameters()},\n#             {'params': self.fc.parameters()},\n#             {'params': self.encoder.parameters(), 'lr': 5e-4}\n#         ], lr=self.opt.lr)\n# # end method forward\n\n"""
utils/visualize.py,0,"b""#coding:utf8\nfrom itertools import chain\nimport visdom\nimport torch\nimport time\nimport torchvision as tv\nimport numpy as np\n\nclass Visualizer():\n    '''\n    \xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86visdom\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xbd\xa0\xe4\xbb\x8d\xe7\x84\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87`self.vis.function`\n    \xe8\xb0\x83\xe7\x94\xa8\xe5\x8e\x9f\xe7\x94\x9f\xe7\x9a\x84visdom\xe6\x8e\xa5\xe5\x8f\xa3\n    '''\n\n    def __init__(self, env='default', **kwargs):\n        import visdom\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        \n        # \xe7\x94\xbb\xe7\x9a\x84\xe7\xac\xac\xe5\x87\xa0\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\xa8\xaa\xe5\xba\xa7\xe6\xa0\x87\n        # \xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x88\xe2\x80\x99loss',23\xef\xbc\x89 \xe5\x8d\xb3loss\xe7\x9a\x84\xe7\xac\xac23\xe4\xb8\xaa\xe7\x82\xb9\n        self.index = {} \n        self.log_text = ''\n    def reinit(self,env='default',**kwargs):\n        '''\n        \xe4\xbf\xae\xe6\x94\xb9visdom\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae\n        '''\n        self.vis = visdom.Visdom(env=env,**kwargs)\n        return self\n\n    def plot_many(self, d):\n        '''\n        \xe4\xb8\x80\xe6\xac\xa1plot\xe5\xa4\x9a\xe4\xb8\xaa\n        @params d: dict (name,value) i.e. ('loss',0.11)\n        '''\n        for k, v in d.iteritems():\n            self.plot(k, v)\n\n    def img_many(self, d):\n        for k, v in d.iteritems():\n            self.img(k, v)\n\n    def plot(self, name, y):\n        '''\n        self.plot('loss',1.00)\n        '''\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=unicode(name),\n                      opts=dict(title=name),\n                      update=None if x == 0 else 'append'\n                      )\n        self.index[name] = x + 1\n\n    def img(self, name, img_):\n        '''\n        self.img('input_img',t.Tensor(64,64))\n        '''\n         \n        if len(img_.size())<3:\n            img_ = img_.cpu().unsqueeze(0) \n        self.vis.image(img_.cpu(),\n                       win=unicode(name),\n                       opts=dict(title=name)\n                       )\n    def img_grid_many(self,d):\n        for k, v in d.iteritems():\n            self.img_grid(k, v)\n\n    def img_grid(self, name, input_3d):\n        '''\n        \xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe8\xbd\xac\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe5\x9b\xbe\xef\xbc\x8ci.e. input\xef\xbc\x8836\xef\xbc\x8c64\xef\xbc\x8c64\xef\xbc\x89\n        \xe4\xbc\x9a\xe5\x8f\x98\xe6\x88\x90 6*6 \xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe5\x9b\xbe\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xbc\xe5\xad\x90\xe5\xa4\xa7\xe5\xb0\x8f64*64\n        '''\n        self.img(name, tv.utils.make_grid(\n            input_3d.cpu()[0].unsqueeze(1).clamp(max=1,min=0)))\n\n    def log(self,info,win='log_text'):\n        '''\n        self.log({'loss':1,'lr':0.0001})\n        '''\n\n        self.log_text += ('[{time}] {info} <br>'.format(\n                            time=time.strftime('%m%d_%H%M%S'),\\\n                            info=info)) \n        self.vis.text(self.log_text,win='log_text')   \n\n    def __getattr__(self, name):\n        return getattr(self.vis, name)\n\n"""
scripts/data_process/embedding2matrix.py,0,"b""#coding:utf8\n\n'''\n\xe5\xb0\x86embedding.txt \xe8\xbd\xac\xe6\x88\x90numpy\xe7\x9f\xa9\xe9\x98\xb5\n'''\n\n\nimport word2vec\nimport numpy as np\n\ndef main(em_file, em_result):\n    '''\n    embedding ->numpy\n    '''\n    em = word2vec.load(em_file)\n    vec = (em.vectors)\n    word2id = em.vocab_hash\n    # d = dict(vector = vec, word2id = word2id)\n    # t.save(d,em_result)\n    np.savez_compressed(em_result,vector=vec,word2id=word2id)\n\nif __name__ == '__main__':\n    import fire\n    fire.Fire()"""
scripts/data_process/get_val.py,0,"b""#coding:utf8\n'''\n\xe4\xbb\x8etrian.npz\xe4\xb8\xad\xe6\x8f\x90\xe5\x8f\x96\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84200000\xe6\x9d\xa1\xe4\xbd\x9c\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n'''\n\nimport numpy as np\ntrain = np.load('/mnt/zhihu/data/train.npz')\ncontent_word=train['content_word'][-200000:]\ntitle_word = train['title_word'][-200000:]\ntitle_char = train['title_char'][-200000:]\ncontent_char = train['content_char'][-200000:]\n\nindex2qid = train['index2qid']\nnp.savez_compressed('/mnt/zhihu/data/val.npz',\n                        title_char = title_char,\n                        title_word = title_word, \\\n                        content_char = content_char,\n                        content_word = content_word,\n                        index2qid = index2qid\n            )\n%hist -f get_val.py\n"""
scripts/data_process/label2id.py,0,"b""#coding:utf8\n\n'''\n\xe7\x94\x9f\xe6\x88\x90\xe6\xaf\x8f\xe4\xb8\xaaqid\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84label,\xe4\xbf\x9d\xe5\xad\x98\xe6\x88\x90json\n'''\ndef main(file,outfile):\n\n    with open(file) as f:\n        lines = f.readlines()\n\n    def process(line):\n        qid,labels = line.replace('\\n','').split()\n        labels = labels.split(',')\n        return qid,labels\n    results = list( map(process, lines))\n    import ipdb;ipdb.set_trace()\n    all_labels = { _ for ii,jj in results for _ in jj }\n    sorted_labels = sorted(all_labels)\n    label2id = {l_:ii for ii,l_ in enumerate(sorted_labels)}#-3239204820424->1\n    id2label = {ii:l_ for ii,l_ in enumerate(sorted_labels)}\n\n    d = {ii:[label2id[jj] for jj in labels ]  for ii,labels in results}\n\n    data = dict(d=d,label2id=label2id,id2label=id2label)\n    import json\n    with open(outfile,'w') as f:\n        json.dump(data,f) \n\nif __name__=='__main__':\n    import fire\n    fire.Fire()"""
scripts/data_process/question2array.py,0,"b""#encoding:utf-8\n'''\n\xe5\xb0\x86\xe9\x97\xae\xe9\xa2\x98\xe7\x94\x9f\xe6\x88\x90train.npz\n'''\n\nimport numpy as np\nimport tqdm\nimport tensorflow as tf\n\n\n\ndef main(question_file,outfile, b_=50,c_=30,d_=250,e_=120):\n    with open(question_file ) as f:\n        lines = f.readlines()\n\n    results= [0 for _ in range(len(lines))]\n   \n\n    char2id = np.load('/mnt/7/zhihu/ieee_zhihu_cup/data/char_embedding.npz')['word2id'].item()\n    word2id = np.load('/mnt/7/zhihu/ieee_zhihu_cup/data/word_embedding.npz')['word2id'].item()\n\n    char_keys = set(char2id.keys())\n    word_keys = set(word2id.keys())\n    # import ipdb;ipdb.set_trace()\n    def process(line):\n        a,b,c,d,e = line.replace('\\n','').split('\\t')\n        b,c,d,e = [_.split(',') for _ in [b,c,d,e]]\n        b,c,d,e = [[int(word[1:]) if word!='' else -1  for word in  sen  ] for sen in [b,c,d,e] ]\n        return b,c,d,e,a\n    for ii,line in tqdm.tqdm(enumerate(lines)):\n        \n        results[ii] = process(line)\n    \n    del lines\n\n    pad_sequence = tf.contrib.keras.preprocessing.sequence.pad_sequences\n\n    bs = [[char2id['c'+str(_) if 'c'+str(_)  in char_keys else '</s>'] for _ in line[0]] for line in results]\n\n    b_len = np.array([len(_) for _ in bs])\n    bs_packed = pad_sequence(bs,maxlen=b_,padding='pre',truncating='pre',value = 0)\n    print('a')\n    del bs\n     \n    cs =  [[word2id['w'+str(_) if 'w'+str(_)  in word_keys else '</s>'] for _ in line[1]] for line in results]\n    c_len = np.array([len(_) for _ in cs])\n    cs_packed = pad_sequence(cs,maxlen=c_,padding='pre',truncating='pre',value = 0)\n    print('b')\n    del cs\n\n    ds =  [[char2id['c'+str(_) if 'c'+str(_)  in char_keys else '</s>'] for _ in line[2]] for line in results]\n    d_len = np.array([len(_) for _ in ds])\n    ds_packed = pad_sequence(ds,maxlen=d_,padding='pre',truncating='pre',value = 0)\n    print('c')\n    del ds\n\n    es =  [[word2id['w'+str(_) if 'w'+str(_)  in word_keys else '</s>'] for _ in line[3]] for line in results]\n    e_len =  np.array([len(_) for _ in es])\n    es_packed = pad_sequence(es,maxlen=e_,padding='pre',truncating='pre',value = 0)\n    print('d')\n    del es\n\n    qids = [_[4] for _ in results]\n    index2qid = {ii:jj for ii,jj in enumerate(qids)}\n\n    np.savez_compressed(outfile,\n                        title_char = bs_packed,\n                        title_word = cs_packed, \\\n                        content_char = ds_packed,\n                        content_word = es_packed,\n                        title_char_len = b_len,\n                        title_word_len = c_len,\n                        content_char_len = d_len,\n                        content_word_len = e_len,\n                        index2qid = index2qid\n            )\n\n# \xe6\xa0\x87\xe9\xa2\x98\xe5\xb9\xb3\xe5\x9d\x87\xe6\x9c\x89 22.335409689506584 \xe5\xad\x97 ->50\n# \xe6\xa0\x87\xe9\xa2\x98\xe5\xb9\xb3\xe5\x9d\x87\xe6\x9c\x89 12.90899899898899 \xe8\xaf\x8d ->35\n# \xe6\x8f\x8f\xe8\xbf\xb0\xe5\xb9\xb3\xe5\x9d\x87\xe6\x9c\x89 117.67666210994987 \xe5\xad\x97 ->250 \n# \xe6\x8f\x8f\xe8\xbf\xb0\xe5\xb9\xb3\xe5\x9d\x87\xe6\x9c\x89 58.563685338852 \xe8\xaf\x8d ->120\n\nif __name__=='__main__':\n    import fire\n    fire.Fire()"""
