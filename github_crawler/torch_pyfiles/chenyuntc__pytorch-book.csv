file_path,api_count,code
chapter06-best_practice/config.py,0,"b'# coding:utf8\nimport warnings\nimport torch as t\n\nclass DefaultConfig(object):\n    env = \'default\'  # visdom \xe7\x8e\xaf\xe5\xa2\x83\n    vis_port =8097 # visdom \xe7\xab\xaf\xe5\x8f\xa3\n    model = \'SqueezeNet\'  # \xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x90\x8d\xe5\xad\x97\xe5\xbf\x85\xe9\xa1\xbb\xe4\xb8\x8emodels/__init__.py\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\xe4\xb8\x80\xe8\x87\xb4\n\n    train_data_root = \'./data/train/\'  # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\xad\x98\xe6\x94\xbe\xe8\xb7\xaf\xe5\xbe\x84\n    test_data_root = \'./data/test1\'  # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe5\xad\x98\xe6\x94\xbe\xe8\xb7\xaf\xe5\xbe\x84\n    load_model_path = None  # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe4\xb8\xbaNone\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x8d\xe5\x8a\xa0\xe8\xbd\xbd\n\n    batch_size = 32  # batch size\n    use_gpu = True  # user GPU or not\n    num_workers = 4  # how many workers for loading data\n    print_freq = 20  # print info every N batch\n\n    debug_file = \'/tmp/debug\'  # if os.path.exists(debug_file): enter ipdb\n    result_file = \'result.csv\'\n\n    max_epoch = 10\n    lr = 0.001  # initial learning rate\n    lr_decay = 0.5  # when val_loss increase, lr = lr*lr_decay\n    weight_decay = 0e-5  # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n\n\n    def _parse(self, kwargs):\n        """"""\n        \xe6\xa0\xb9\xe6\x8d\xae\xe5\xad\x97\xe5\x85\xb8kwargs \xe6\x9b\xb4\xe6\x96\xb0 config\xe5\x8f\x82\xe6\x95\xb0\n        """"""\n        for k, v in kwargs.items():\n            if not hasattr(self, k):\n                warnings.warn(""Warning: opt has not attribut %s"" % k)\n            setattr(self, k, v)\n        \n        opt.device =t.device(\'cuda\') if opt.use_gpu else t.device(\'cpu\')\n\n\n        print(\'user config:\')\n        for k, v in self.__class__.__dict__.items():\n            if not k.startswith(\'_\'):\n                print(k, getattr(self, k))\n\nopt = DefaultConfig()\n'"
chapter06-best_practice/main.py,1,"b'#coding:utf8\nfrom config import opt\nimport os\nimport torch as t\nimport models\nfrom data.dataset import DogCat\nfrom torch.utils.data import DataLoader\nfrom torchnet import meter\nfrom utils.visualize import Visualizer\nfrom tqdm import tqdm\n\n\n@t.no_grad() # pytorch>=0.5\ndef test(**kwargs):\n    opt._parse(kwargs)\n\n    # configure model\n    model = getattr(models, opt.model)().eval()\n    if opt.load_model_path:\n        model.load(opt.load_model_path)\n    model.to(opt.device)\n\n    # data\n    train_data = DogCat(opt.test_data_root,test=True)\n    test_dataloader = DataLoader(train_data,batch_size=opt.batch_size,shuffle=False,num_workers=opt.num_workers)\n    results = []\n    for ii,(data,path) in tqdm(enumerate(test_dataloader)):\n        input = data.to(opt.device)\n        score = model(input)\n        probability = t.nn.functional.softmax(score,dim=1)[:,0].detach().tolist()\n        # label = score.max(dim = 1)[1].detach().tolist()\n        \n        batch_results = [(path_.item(),probability_) for path_,probability_ in zip(path,probability) ]\n\n        results += batch_results\n    write_csv(results,opt.result_file)\n\n    return results\n\ndef write_csv(results,file_name):\n    import csv\n    with open(file_name,\'w\') as f:\n        writer = csv.writer(f)\n        writer.writerow([\'id\',\'label\'])\n        writer.writerows(results)\n    \ndef train(**kwargs):\n    opt._parse(kwargs)\n    vis = Visualizer(opt.env,port = opt.vis_port)\n\n    # step1: configure model\n    model = getattr(models, opt.model)()\n    if opt.load_model_path:\n        model.load(opt.load_model_path)\n    model.to(opt.device)\n\n    # step2: data\n    train_data = DogCat(opt.train_data_root,train=True)\n    val_data = DogCat(opt.train_data_root,train=False)\n    train_dataloader = DataLoader(train_data,opt.batch_size,\n                        shuffle=True,num_workers=opt.num_workers)\n    val_dataloader = DataLoader(val_data,opt.batch_size,\n                        shuffle=False,num_workers=opt.num_workers)\n    \n    # step3: criterion and optimizer\n    criterion = t.nn.CrossEntropyLoss()\n    lr = opt.lr\n    optimizer = model.get_optimizer(lr, opt.weight_decay)\n        \n    # step4: meters\n    loss_meter = meter.AverageValueMeter()\n    confusion_matrix = meter.ConfusionMeter(2)\n    previous_loss = 1e10\n\n    # train\n    for epoch in range(opt.max_epoch):\n        \n        loss_meter.reset()\n        confusion_matrix.reset()\n\n        for ii,(data,label) in tqdm(enumerate(train_dataloader)):\n\n            # train model \n            input = data.to(opt.device)\n            target = label.to(opt.device)\n\n\n            optimizer.zero_grad()\n            score = model(input)\n            loss = criterion(score,target)\n            loss.backward()\n            optimizer.step()\n            \n            \n            # meters update and visualize\n            loss_meter.add(loss.item())\n            # detach \xe4\xb8\x80\xe4\xb8\x8b\xe6\x9b\xb4\xe5\xae\x89\xe5\x85\xa8\xe4\xbf\x9d\xe9\x99\xa9\n            confusion_matrix.add(score.detach(), target.detach()) \n\n            if (ii + 1)%opt.print_freq == 0:\n                vis.plot(\'loss\', loss_meter.value()[0])\n                \n                # \xe8\xbf\x9b\xe5\x85\xa5debug\xe6\xa8\xa1\xe5\xbc\x8f\n                if os.path.exists(opt.debug_file):\n                    import ipdb;\n                    ipdb.set_trace()\n\n\n        model.save()\n\n        # validate and visualize\n        val_cm,val_accuracy = val(model,val_dataloader)\n\n        vis.plot(\'val_accuracy\',val_accuracy)\n        vis.log(""epoch:{epoch},lr:{lr},loss:{loss},train_cm:{train_cm},val_cm:{val_cm}"".format(\n                    epoch = epoch,loss = loss_meter.value()[0],val_cm = str(val_cm.value()),train_cm=str(confusion_matrix.value()),lr=lr))\n        \n        # update learning rate\n        if loss_meter.value()[0] > previous_loss:          \n            lr = lr * opt.lr_decay\n            # \xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe9\x99\x8d\xe4\xbd\x8e\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95:\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x9c\x89moment\xe7\xad\x89\xe4\xbf\xa1\xe6\x81\xaf\xe7\x9a\x84\xe4\xb8\xa2\xe5\xa4\xb1\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = lr\n        \n\n        previous_loss = loss_meter.value()[0]\n\n@t.no_grad()\ndef val(model,dataloader):\n    """"""\n    \xe8\xae\xa1\xe7\xae\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe7\xad\x89\xe4\xbf\xa1\xe6\x81\xaf\n    """"""\n    model.eval()\n    confusion_matrix = meter.ConfusionMeter(2)\n    for ii, (val_input, label) in tqdm(enumerate(dataloader)):\n        val_input = val_input.to(opt.device)\n        score = model(val_input)\n        confusion_matrix.add(score.detach().squeeze(), label.type(t.LongTensor))\n\n    model.train()\n    cm_value = confusion_matrix.value()\n    accuracy = 100. * (cm_value[0][0] + cm_value[1][1]) / (cm_value.sum())\n    return confusion_matrix, accuracy\n\ndef help():\n    """"""\n    \xe6\x89\x93\xe5\x8d\xb0\xe5\xb8\xae\xe5\x8a\xa9\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x9a python file.py help\n    """"""\n    \n    print(""""""\n    usage : python file.py <function> [--args=value]\n    <function> := train | test | help\n    example: \n            python {0} train --env=\'env0701\' --lr=0.01\n            python {0} test --dataset=\'path/to/dataset/root/\'\n            python {0} help\n    avaiable args:"""""".format(__file__))\n\n    from inspect import getsource\n    source = (getsource(opt.__class__))\n    print(source)\n\nif __name__==\'__main__\':\n    import fire\n    fire.Fire()\n'"
chapter07-AnimeGAN/dataset.py,0,b'from torch import utils\n\n\nclass FaceDataset(utils.data.Dataset):\n    pass\n'
chapter07-AnimeGAN/main.py,0,"b'# coding:utf8\nimport os\nimport ipdb\nimport torch as t\nimport torchvision as tv\nimport tqdm\nfrom model import NetG, NetD\nfrom torchnet.meter import AverageValueMeter\n\n\nclass Config(object):\n    data_path = \'data/\'  # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\xad\x98\xe6\x94\xbe\xe8\xb7\xaf\xe5\xbe\x84\n    num_workers = 4  # \xe5\xa4\x9a\xe8\xbf\x9b\xe7\xa8\x8b\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe6\x89\x80\xe7\x94\xa8\xe7\x9a\x84\xe8\xbf\x9b\xe7\xa8\x8b\xe6\x95\xb0\n    image_size = 96  # \xe5\x9b\xbe\xe7\x89\x87\xe5\xb0\xba\xe5\xaf\xb8\n    batch_size = 256\n    max_epoch = 200\n    lr1 = 2e-4  # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n    lr2 = 2e-4  # \xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n    beta1 = 0.5  # Adam\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\x9a\x84beta1\xe5\x8f\x82\xe6\x95\xb0\n    gpu = True  # \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8GPU\n    nz = 100  # \xe5\x99\xaa\xe5\xa3\xb0\xe7\xbb\xb4\xe5\xba\xa6\n    ngf = 64  # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8feature map\xe6\x95\xb0\n    ndf = 64  # \xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8feature map\xe6\x95\xb0\n\n    save_path = \'imgs/\'  # \xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\n\n    vis = True  # \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8visdom\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n    env = \'GAN\'  # visdom\xe7\x9a\x84env\n    plot_every = 20  # \xe6\xaf\x8f\xe9\x97\xb4\xe9\x9a\x9420 batch\xef\xbc\x8cvisdom\xe7\x94\xbb\xe5\x9b\xbe\xe4\xb8\x80\xe6\xac\xa1\n\n    debug_file = \'/tmp/debuggan\'  # \xe5\xad\x98\xe5\x9c\xa8\xe8\xaf\xa5\xe6\x96\x87\xe4\xbb\xb6\xe5\x88\x99\xe8\xbf\x9b\xe5\x85\xa5debug\xe6\xa8\xa1\xe5\xbc\x8f\n    d_every = 1  # \xe6\xaf\x8f1\xe4\xb8\xaabatch\xe8\xae\xad\xe7\xbb\x83\xe4\xb8\x80\xe6\xac\xa1\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\n    g_every = 5  # \xe6\xaf\x8f5\xe4\xb8\xaabatch\xe8\xae\xad\xe7\xbb\x83\xe4\xb8\x80\xe6\xac\xa1\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\n    save_every = 10  # \xe6\xb2\xa110\xe4\xb8\xaaepoch\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe6\xac\xa1\xe6\xa8\xa1\xe5\x9e\x8b\n    netd_path = None  # \'checkpoints/netd_.pth\' #\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\n    netg_path = None  # \'checkpoints/netg_211.pth\'\n\n    # \xe5\x8f\xaa\xe6\xb5\x8b\xe8\xaf\x95\xe4\xb8\x8d\xe8\xae\xad\xe7\xbb\x83\n    gen_img = \'result.png\'\n    # \xe4\xbb\x8e512\xe5\xbc\xa0\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\xe4\xbf\x9d\xe5\xad\x98\xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x8464\xe5\xbc\xa0\n    gen_num = 64\n    gen_search_num = 512\n    gen_mean = 0  # \xe5\x99\xaa\xe5\xa3\xb0\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\n    gen_std = 1  # \xe5\x99\xaa\xe5\xa3\xb0\xe7\x9a\x84\xe6\x96\xb9\xe5\xb7\xae\n\n\nopt = Config()\n\n\ndef train(**kwargs):\n    for k_, v_ in kwargs.items():\n        setattr(opt, k_, v_)\n\n    device=t.device(\'cuda\') if opt.gpu else t.device(\'cpu\')\n    if opt.vis:\n        from visualize import Visualizer\n        vis = Visualizer(opt.env)\n\n    # \xe6\x95\xb0\xe6\x8d\xae\n    transforms = tv.transforms.Compose([\n        tv.transforms.Resize(opt.image_size),\n        tv.transforms.CenterCrop(opt.image_size),\n        tv.transforms.ToTensor(),\n        tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    dataset = tv.datasets.ImageFolder(opt.data_path, transform=transforms)\n    dataloader = t.utils.data.DataLoader(dataset,\n                                         batch_size=opt.batch_size,\n                                         shuffle=True,\n                                         num_workers=opt.num_workers,\n                                         drop_last=True\n                                         )\n\n    # \xe7\xbd\x91\xe7\xbb\x9c\n    netg, netd = NetG(opt), NetD(opt)\n    map_location = lambda storage, loc: storage\n    if opt.netd_path:\n        netd.load_state_dict(t.load(opt.netd_path, map_location=map_location))\n    if opt.netg_path:\n        netg.load_state_dict(t.load(opt.netg_path, map_location=map_location))\n    netd.to(device)\n    netg.to(device)\n\n\n    # \xe5\xae\x9a\xe4\xb9\x89\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe5\x92\x8c\xe6\x8d\x9f\xe5\xa4\xb1\n    optimizer_g = t.optim.Adam(netg.parameters(), opt.lr1, betas=(opt.beta1, 0.999))\n    optimizer_d = t.optim.Adam(netd.parameters(), opt.lr2, betas=(opt.beta1, 0.999))\n    criterion = t.nn.BCELoss().to(device)\n\n    # \xe7\x9c\x9f\xe5\x9b\xbe\xe7\x89\x87label\xe4\xb8\xba1\xef\xbc\x8c\xe5\x81\x87\xe5\x9b\xbe\xe7\x89\x87label\xe4\xb8\xba0\n    # noises\xe4\xb8\xba\xe7\x94\x9f\xe6\x88\x90\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\n    true_labels = t.ones(opt.batch_size).to(device)\n    fake_labels = t.zeros(opt.batch_size).to(device)\n    fix_noises = t.randn(opt.batch_size, opt.nz, 1, 1).to(device)\n    noises = t.randn(opt.batch_size, opt.nz, 1, 1).to(device)\n\n    errord_meter = AverageValueMeter()\n    errorg_meter = AverageValueMeter()\n\n\n    epochs = range(opt.max_epoch)\n    for epoch in iter(epochs):\n        for ii, (img, _) in tqdm.tqdm(enumerate(dataloader)):\n            real_img = img.to(device)\n\n            if ii % opt.d_every == 0:\n                # \xe8\xae\xad\xe7\xbb\x83\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\n                optimizer_d.zero_grad()\n                ## \xe5\xb0\xbd\xe5\x8f\xaf\xe8\x83\xbd\xe7\x9a\x84\xe6\x8a\x8a\xe7\x9c\x9f\xe5\x9b\xbe\xe7\x89\x87\xe5\x88\xa4\xe5\x88\xab\xe4\xb8\xba\xe6\xad\xa3\xe7\xa1\xae\n                output = netd(real_img)\n                error_d_real = criterion(output, true_labels)\n                error_d_real.backward()\n\n                ## \xe5\xb0\xbd\xe5\x8f\xaf\xe8\x83\xbd\xe6\x8a\x8a\xe5\x81\x87\xe5\x9b\xbe\xe7\x89\x87\xe5\x88\xa4\xe5\x88\xab\xe4\xb8\xba\xe9\x94\x99\xe8\xaf\xaf\n                noises.data.copy_(t.randn(opt.batch_size, opt.nz, 1, 1))\n                fake_img = netg(noises).detach()  # \xe6\xa0\xb9\xe6\x8d\xae\xe5\x99\xaa\xe5\xa3\xb0\xe7\x94\x9f\xe6\x88\x90\xe5\x81\x87\xe5\x9b\xbe\n                output = netd(fake_img)\n                error_d_fake = criterion(output, fake_labels)\n                error_d_fake.backward()\n                optimizer_d.step()\n\n                error_d = error_d_fake + error_d_real\n\n                errord_meter.add(error_d.item())\n\n            if ii % opt.g_every == 0:\n                # \xe8\xae\xad\xe7\xbb\x83\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\n                optimizer_g.zero_grad()\n                noises.data.copy_(t.randn(opt.batch_size, opt.nz, 1, 1))\n                fake_img = netg(noises)\n                output = netd(fake_img)\n                error_g = criterion(output, true_labels)\n                error_g.backward()\n                optimizer_g.step()\n                errorg_meter.add(error_g.item())\n\n            if opt.vis and ii % opt.plot_every == opt.plot_every - 1:\n                ## \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n                if os.path.exists(opt.debug_file):\n                    ipdb.set_trace()\n                fix_fake_imgs = netg(fix_noises)\n                vis.images(fix_fake_imgs.detach().cpu().numpy()[:64] * 0.5 + 0.5, win=\'fixfake\')\n                vis.images(real_img.data.cpu().numpy()[:64] * 0.5 + 0.5, win=\'real\')\n                vis.plot(\'errord\', errord_meter.value()[0])\n                vis.plot(\'errorg\', errorg_meter.value()[0])\n\n        if (epoch+1) % opt.save_every == 0:\n            # \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x81\xe5\x9b\xbe\xe7\x89\x87\n            tv.utils.save_image(fix_fake_imgs.data[:64], \'%s/%s.png\' % (opt.save_path, epoch), normalize=True,\n                                range=(-1, 1))\n            t.save(netd.state_dict(), \'checkpoints/netd_%s.pth\' % epoch)\n            t.save(netg.state_dict(), \'checkpoints/netg_%s.pth\' % epoch)\n            errord_meter.reset()\n            errorg_meter.reset()\n\n\n@t.no_grad()\ndef generate(**kwargs):\n    """"""\n    \xe9\x9a\x8f\xe6\x9c\xba\xe7\x94\x9f\xe6\x88\x90\xe5\x8a\xa8\xe6\xbc\xab\xe5\xa4\xb4\xe5\x83\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe6\xa0\xb9\xe6\x8d\xaenetd\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0\xe9\x80\x89\xe6\x8b\xa9\xe8\xbe\x83\xe5\xa5\xbd\xe7\x9a\x84\n    """"""\n    for k_, v_ in kwargs.items():\n        setattr(opt, k_, v_)\n    \n    device=t.device(\'cuda\') if opt.gpu else t.device(\'cpu\')\n\n    netg, netd = NetG(opt).eval(), NetD(opt).eval()\n    noises = t.randn(opt.gen_search_num, opt.nz, 1, 1).normal_(opt.gen_mean, opt.gen_std)\n    noises = noises.to(device)\n\n    map_location = lambda storage, loc: storage\n    netd.load_state_dict(t.load(opt.netd_path, map_location=map_location))\n    netg.load_state_dict(t.load(opt.netg_path, map_location=map_location))\n    netd.to(device)\n    netg.to(device)\n\n\n    # \xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\xb9\xb6\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe7\x89\x87\xe5\x9c\xa8\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0\n    fake_img = netg(noises)\n    scores = netd(fake_img).detach()\n\n    # \xe6\x8c\x91\xe9\x80\x89\xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x84\xe6\x9f\x90\xe5\x87\xa0\xe5\xbc\xa0\n    indexs = scores.topk(opt.gen_num)[1]\n    result = []\n    for ii in indexs:\n        result.append(fake_img.data[ii])\n    # \xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe7\x89\x87\n    tv.utils.save_image(t.stack(result), opt.gen_img, normalize=True, range=(-1, 1))\n\n\nif __name__ == \'__main__\':\n    import fire\n    fire.Fire()\n'"
chapter07-AnimeGAN/model.py,0,"b'# coding:utf8\nfrom torch import nn\n\n\nclass NetG(nn.Module):\n    """"""\n    \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe5\xae\x9a\xe4\xb9\x89\n    """"""\n\n    def __init__(self, opt):\n        super(NetG, self).__init__()\n        ngf = opt.ngf  # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8feature map\xe6\x95\xb0\n\n        self.main = nn.Sequential(\n            # \xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaanz\xe7\xbb\xb4\xe5\xba\xa6\xe7\x9a\x84\xe5\x99\xaa\xe5\xa3\xb0\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xa4\xe4\xb8\xba\xe5\xae\x83\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa1*1*nz\xe7\x9a\x84feature map\n            nn.ConvTranspose2d(opt.nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # \xe4\xb8\x8a\xe4\xb8\x80\xe6\xad\xa5\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x9a(ngf*8) x 4 x 4\n\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # \xe4\xb8\x8a\xe4\xb8\x80\xe6\xad\xa5\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x9a (ngf*4) x 8 x 8\n\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # \xe4\xb8\x8a\xe4\xb8\x80\xe6\xad\xa5\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x9a (ngf*2) x 16 x 16\n\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # \xe4\xb8\x8a\xe4\xb8\x80\xe6\xad\xa5\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x9a(ngf) x 32 x 32\n\n            nn.ConvTranspose2d(ngf, 3, 5, 3, 1, bias=False),\n            nn.Tanh()  # \xe8\xbe\x93\xe5\x87\xba\xe8\x8c\x83\xe5\x9b\xb4 -1~1 \xe6\x95\x85\xe8\x80\x8c\xe9\x87\x87\xe7\x94\xa8Tanh\n            # \xe8\xbe\x93\xe5\x87\xba\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x9a3 x 96 x 96\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n\nclass NetD(nn.Module):\n    """"""\n    \xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe5\xae\x9a\xe4\xb9\x89\n    """"""\n\n    def __init__(self, opt):\n        super(NetD, self).__init__()\n        ndf = opt.ndf\n        self.main = nn.Sequential(\n            # \xe8\xbe\x93\xe5\x85\xa5 3 x 96 x 96\n            nn.Conv2d(3, ndf, 5, 3, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # \xe8\xbe\x93\xe5\x87\xba (ndf) x 32 x 32\n\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # \xe8\xbe\x93\xe5\x87\xba (ndf*2) x 16 x 16\n\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # \xe8\xbe\x93\xe5\x87\xba (ndf*4) x 8 x 8\n\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # \xe8\xbe\x93\xe5\x87\xba (ndf*8) x 4 x 4\n\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()  # \xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0(\xe6\xa6\x82\xe7\x8e\x87)\n        )\n\n    def forward(self, input):\n        return self.main(input).view(-1)\n'"
chapter07-AnimeGAN/visualize.py,0,"b'# coding:utf8\nfrom itertools import chain\nimport visdom\nimport torch\nimport time\nimport torchvision as tv\nimport numpy as np\n\n\nclass Visualizer():\n    """"""\n    \xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86visdom\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xbd\xa0\xe4\xbb\x8d\xe7\x84\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87`self.vis.function`\n    \xe8\xb0\x83\xe7\x94\xa8\xe5\x8e\x9f\xe7\x94\x9f\xe7\x9a\x84visdom\xe6\x8e\xa5\xe5\x8f\xa3\n    """"""\n\n    def __init__(self, env=\'default\', **kwargs):\n        import visdom\n        self.vis = visdom.Visdom(env=env, use_incoming_socket=False,**kwargs)\n\n        # \xe7\x94\xbb\xe7\x9a\x84\xe7\xac\xac\xe5\x87\xa0\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\xa8\xaa\xe5\xba\xa7\xe6\xa0\x87\n        # \xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x88\xe2\x80\x99loss\',23\xef\xbc\x89 \xe5\x8d\xb3loss\xe7\x9a\x84\xe7\xac\xac23\xe4\xb8\xaa\xe7\x82\xb9\n        self.index = {}\n        self.log_text = \'\'\n\n    def reinit(self, env=\'default\', **kwargs):\n        """"""\n        \xe4\xbf\xae\xe6\x94\xb9visdom\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae\n        """"""\n        self.vis = visdom.Visdom(env=env,use_incoming_socket=False, **kwargs)\n        return self\n\n    def plot_many(self, d):\n        """"""\n        \xe4\xb8\x80\xe6\xac\xa1plot\xe5\xa4\x9a\xe4\xb8\xaa\n        @params d: dict (name,value) i.e. (\'loss\',0.11)\n        """"""\n        for k, v in d.items():\n            self.plot(k, v)\n\n    def img_many(self, d):\n        for k, v in d.items():\n            self.img(k, v)\n\n    def plot(self, name, y):\n        """"""\n        self.plot(\'loss\',1.00)\n        """"""\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=(name),\n                      opts=dict(title=name),\n                      update=None if x == 0 else \'append\'\n                      )\n        self.index[name] = x + 1\n\n    def img(self, name, img_):\n        """"""\n        self.img(\'input_img\',t.Tensor(64,64))\n        """"""\n\n        if len(img_.size()) < 3:\n            img_ = img_.cpu().unsqueeze(0)\n        self.vis.image(img_.cpu(),\n                       win=(name),\n                       opts=dict(title=name)\n                       )\n\n    def img_grid_many(self, d):\n        for k, v in d.items():\n            self.img_grid(k, v)\n\n    def img_grid(self, name, input_3d):\n        """"""\n        \xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe8\xbd\xac\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe5\x9b\xbe\xef\xbc\x8ci.e. input\xef\xbc\x8836\xef\xbc\x8c64\xef\xbc\x8c64\xef\xbc\x89\n        \xe4\xbc\x9a\xe5\x8f\x98\xe6\x88\x90 6*6 \xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe5\x9b\xbe\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xbc\xe5\xad\x90\xe5\xa4\xa7\xe5\xb0\x8f64*64\n        """"""\n        self.img(name, tv.utils.make_grid(\n            input_3d.cpu()[0].unsqueeze(1).clamp(max=1, min=0)))\n\n    def log(self, info, win=\'log_text\'):\n        """"""\n        self.log({\'loss\':1,\'lr\':0.0001})\n        """"""\n\n        self.log_text += (\'[{time}] {info} <br>\'.format(\n            time=time.strftime(\'%m%d_%H%M%S\'),\n            info=info))\n        self.vis.text(self.log_text, win=win)\n\n    def __getattr__(self, name):\n        return getattr(self.vis, name)\n'"
chapter08-neural_style/PackedVGG.py,2,"b'# coding:utf8\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import vgg16\nfrom collections import namedtuple\n\n\nclass Vgg16(torch.nn.Module):\n    def __init__(self):\n        super(Vgg16, self).__init__()\n        features = list(vgg16(pretrained=True).features)[:23]\n        # the 3rd, 8th, 15th and 22nd layer of \\ \n        # self.features are: relu1_2,relu2_2,relu3_3,relu4_3\n        self.features = nn.ModuleList(features).eval()\n\n    def forward(self, x):\n        results = []\n        for ii, model in enumerate(self.features):\n            x = model(x)\n            if ii in {3, 8, 15, 22}:\n                results.append(x)\n\n        vgg_outputs = namedtuple(""VggOutputs"", [\'relu1_2\', \'relu2_2\', \'relu3_3\', \'relu4_3\'])\n        return vgg_outputs(*results)\n'"
chapter08-neural_style/main.py,2,"b'# coding:utf8\n\nimport torch as t\nimport torchvision as tv\nimport torchnet as tnt\n\nfrom torch.utils import data\nfrom transformer_net import TransformerNet\nimport utils\nfrom PackedVGG import Vgg16\nfrom torch.nn import functional as F\nimport tqdm\nimport os\nimport ipdb\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n\nclass Config(object):\n    # General Args\n    use_gpu = True\n    model_path = None # pretrain model path (for resume training or test)\n    \n    # Train Args\n    image_size = 256 # image crop_size for training\n    batch_size = 8  \n    data_root = \'data/\' # dataset root\xef\xbc\x9a$data_root/coco/a.jpg\n    num_workers = 4 # dataloader num of workers\n    \n    lr = 1e-3\n    epoches = 2 # total epoch to train\n    content_weight = 1e5 # weight of content_loss  \n    style_weight = 1e10 # weight of style_loss\n\n    style_path= \'style.jpg\' # style image path\n    env = \'neural-style\' # visdom env\n    plot_every = 10 # visualize in visdom for every 10 batch\n\n    debug_file = \'/tmp/debugnn\' # touch $debug_fie to interrupt and enter ipdb \n\n    # Test Args\n    content_path = \'input.png\' # input file to do style transfer [for test]\n    result_path = \'output.png\' # style transfer result [for test]\n\n\ndef train(**kwargs):\n    opt = Config()\n    for k_, v_ in kwargs.items():\n        setattr(opt, k_, v_)\n    \n    device=t.device(\'cuda\') if opt.use_gpu else t.device(\'cpu\')\n    vis = utils.Visualizer(opt.env)\n\n    # Data loading\n    transfroms = tv.transforms.Compose([\n        tv.transforms.Resize(opt.image_size),\n        tv.transforms.CenterCrop(opt.image_size),\n        tv.transforms.ToTensor(),\n        tv.transforms.Lambda(lambda x: x * 255)\n    ])\n    dataset = tv.datasets.ImageFolder(opt.data_root, transfroms)\n    dataloader = data.DataLoader(dataset, opt.batch_size)\n\n    # style transformer network\n    transformer = TransformerNet()\n    if opt.model_path:\n        transformer.load_state_dict(t.load(opt.model_path, map_location=lambda _s, _: _s))\n    transformer.to(device)\n\n    # Vgg16 for Perceptual Loss\n    vgg = Vgg16().eval()\n    vgg.to(device)\n    for param in vgg.parameters():\n        param.requires_grad = False\n\n    # Optimizer\n    optimizer = t.optim.Adam(transformer.parameters(), opt.lr)\n\n    # Get style image\n    style = utils.get_style_data(opt.style_path)\n    vis.img(\'style\', (style.data[0] * 0.225 + 0.45).clamp(min=0, max=1))\n    style = style.to(device)\n\n\n    # gram matrix for style image\n    with t.no_grad():\n        features_style = vgg(style)\n        gram_style = [utils.gram_matrix(y) for y in features_style]\n\n    # Loss meter\n    style_meter = tnt.meter.AverageValueMeter()\n    content_meter = tnt.meter.AverageValueMeter()\n\n    for epoch in range(opt.epoches):\n        content_meter.reset()\n        style_meter.reset()\n\n        for ii, (x, _) in tqdm.tqdm(enumerate(dataloader)):\n\n            # Train\n            optimizer.zero_grad()\n            x = x.to(device)\n            y = transformer(x)\n            y = utils.normalize_batch(y)\n            x = utils.normalize_batch(x)\n            features_y = vgg(y)\n            features_x = vgg(x)\n\n            # content loss\n            content_loss = opt.content_weight * F.mse_loss(features_y.relu2_2, features_x.relu2_2)\n\n            # style loss\n            style_loss = 0.\n            for ft_y, gm_s in zip(features_y, gram_style):\n                gram_y = utils.gram_matrix(ft_y)\n                style_loss += F.mse_loss(gram_y, gm_s.expand_as(gram_y))\n            style_loss *= opt.style_weight\n\n            total_loss = content_loss + style_loss\n            total_loss.backward()\n            optimizer.step()\n\n            # Loss smooth for visualization\n            content_meter.add(content_loss.item())\n            style_meter.add(style_loss.item())\n\n            if (ii + 1) % opt.plot_every == 0:\n                if os.path.exists(opt.debug_file):\n                    ipdb.set_trace()\n\n                # visualization\n                vis.plot(\'content_loss\', content_meter.value()[0])\n                vis.plot(\'style_loss\', style_meter.value()[0])\n                # denorm input/output, since we have applied (utils.normalize_batch)\n                vis.img(\'output\', (y.data.cpu()[0] * 0.225 + 0.45).clamp(min=0, max=1))\n                vis.img(\'input\', (x.data.cpu()[0] * 0.225 + 0.45).clamp(min=0, max=1))\n\n        # save checkpoint\n        vis.save([opt.env])\n        t.save(transformer.state_dict(), \'checkpoints/%s_style.pth\' % epoch)\n\n@t.no_grad()\ndef stylize(**kwargs):\n    """"""\n    perform style transfer\n    """"""\n    opt = Config()\n\n    for k_, v_ in kwargs.items():\n        setattr(opt, k_, v_)\n    device=t.device(\'cuda\') if opt.use_gpu else t.device(\'cpu\')\n    \n    # input image preprocess\n    content_image = tv.datasets.folder.default_loader(opt.content_path)\n    content_transform = tv.transforms.Compose([\n        tv.transforms.ToTensor(),\n        tv.transforms.Lambda(lambda x: x.mul(255))\n    ])\n    content_image = content_transform(content_image)\n    content_image = content_image.unsqueeze(0).to(device).detach()\n\n    # model setup\n    style_model = TransformerNet().eval()\n    style_model.load_state_dict(t.load(opt.model_path, map_location=lambda _s, _: _s))\n    style_model.to(device)\n\n    # style transfer and save output\n    output = style_model(content_image)\n    output_data = output.cpu().data[0]\n    tv.utils.save_image(((output_data / 255)).clamp(min=0, max=1), opt.result_path)\n\n\nif __name__ == \'__main__\':\n    import fire\n\n    fire.Fire()\n'"
chapter08-neural_style/transformer_net.py,1,"b'# coding:utf8\n""""""\ncode refer to https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/transformer_net.py\n""""""\nimport torch as t\nfrom torch import nn\nimport numpy as np\n\n\nclass TransformerNet(nn.Module):\n    def __init__(self):\n        super(TransformerNet, self).__init__()\n\n        # Down sample layers\n        self.initial_layers = nn.Sequential(\n            ConvLayer(3, 32, kernel_size=9, stride=1),\n            nn.InstanceNorm2d(32, affine=True),\n            nn.ReLU(True),\n            ConvLayer(32, 64, kernel_size=3, stride=2),\n            nn.InstanceNorm2d(64, affine=True),\n            nn.ReLU(True),\n            ConvLayer(64, 128, kernel_size=3, stride=2),\n            nn.InstanceNorm2d(128, affine=True),\n            nn.ReLU(True),\n        )\n\n        # Residual layers\n        self.res_layers = nn.Sequential(\n            ResidualBlock(128),\n            ResidualBlock(128),\n            ResidualBlock(128),\n            ResidualBlock(128),\n            ResidualBlock(128)\n        )\n\n        # Upsampling Layers\n        self.upsample_layers = nn.Sequential(\n            UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2),\n            nn.InstanceNorm2d(64, affine=True),\n            nn.ReLU(True),\n            UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2),\n            nn.InstanceNorm2d(32, affine=True),\n            nn.ReLU(True),\n            ConvLayer(32, 3, kernel_size=9, stride=1)\n        )\n\n    def forward(self, x):\n        x = self.initial_layers(x)\n        x = self.res_layers(x)\n        x = self.upsample_layers(x)\n        return x\n\n\nclass ConvLayer(nn.Module):\n    """"""\n    add ReflectionPad for Conv\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride):\n        super(ConvLayer, self).__init__()\n        reflection_padding = int(np.floor(kernel_size / 2))\n        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n\n    def forward(self, x):\n        out = self.reflection_pad(x)\n        out = self.conv2d(out)\n        return out\n\n\nclass UpsampleConvLayer(nn.Module):\n    """"""UpsampleConvLayer\n    instead of ConvTranspose2d, we do UpSample + Conv2d\n    see ref for why.\n    ref: http://distill.pub/2016/deconv-checkerboard/\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n        super(UpsampleConvLayer, self).__init__()\n        self.upsample = upsample\n        reflection_padding = int(np.floor(kernel_size / 2))\n        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n\n    def forward(self, x):\n        x_in = x\n        if self.upsample:\n            x_in = t.nn.functional.interpolate(x_in, scale_factor=self.upsample)\n        out = self.reflection_pad(x_in)\n        out = self.conv2d(out)\n        return out\n\n\nclass ResidualBlock(nn.Module):\n    """"""ResidualBlock\n    introduced in: https://arxiv.org/abs/1512.03385\n    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n    """"""\n\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        out = self.relu(self.in1(self.conv1(x)))\n        out = self.in2(self.conv2(out))\n        out = out + residual\n        return out\n'"
chapter08-neural_style/utils.py,0,"b'# coding:utf8\nfrom itertools import chain\nimport visdom\nimport torch as t\nimport time\nimport torchvision as tv\nimport numpy as np\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\n\ndef gram_matrix(y):\n    """"""\n    Input shape: b,c,h,w\n    Output shape: b,c,c\n    """"""\n    (b, ch, h, w) = y.size()\n    features = y.view(b, ch, w * h)\n    features_t = features.transpose(1, 2)\n    gram = features.bmm(features_t) / (ch * h * w)\n    return gram\n\n\nclass Visualizer():\n    """"""\n    wrapper on visdom, but you may still call native visdom by `self.vis.function`\n    """"""\n\n    def __init__(self, env=\'default\', **kwargs):\n        import visdom\n        self.vis = visdom.Visdom(env=env, use_incoming_socket=False, **kwargs)\n\n        self.index = {}\n        self.log_text = \'\'\n\n    def reinit(self, env=\'default\', **kwargs):\n        """"""\n        \n        """"""\n        self.vis = visdom.Visdom(env=env,use_incoming_socket=False,  **kwargs)\n        return self\n\n    def plot_many(self, d):\n        """"""\n        plot multi values in a time\n        @params d: dict (name,value) i.e. (\'loss\',0.11)\n        """"""\n        for k, v in d.items():\n            self.plot(k, v)\n\n    def img_many(self, d):\n        for k, v in d.items():\n            self.img(k, v)\n\n    def plot(self, name, y):\n        """"""\n        self.plot(\'loss\',1.00)\n        """"""\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=name,\n                      opts=dict(title=name),\n                      update=None if x == 0 else \'append\'\n                      )\n        self.index[name] = x + 1\n\n    def img(self, name, img_):\n        """"""\n        self.img(\'input_img\',t.Tensor(64,64))\n        """"""\n\n        if len(img_.size()) < 3:\n            img_ = img_.cpu().unsqueeze(0)\n        self.vis.image(img_.cpu(),\n                       win=name,\n                       opts=dict(title=name)\n                       )\n\n    def img_grid_many(self, d):\n        for k, v in d.items():\n            self.img_grid(k, v)\n\n    def img_grid(self, name, input_3d):\n        """"""\n        convert batch images to grid of images\n        i.e. input\xef\xbc\x8836\xef\xbc\x8c64\xef\xbc\x8c64\xef\xbc\x89 ->  6*6 grid\xef\xbc\x8ceach grid is an image of size 64*64\n        """"""\n        self.img(name, tv.utils.make_grid(\n            input_3d.cpu()[0].unsqueeze(1).clamp(max=1, min=0)))\n\n    def log(self, info, win=\'log_text\'):\n        """"""\n        self.log({\'loss\':1,\'lr\':0.0001})\n        """"""\n\n        self.log_text += (\'[{time}] {info} <br>\'.format(\n            time=time.strftime(\'%m%d_%H%M%S\'),\n            info=info))\n        self.vis.text(self.log_text, win=win)\n\n    def __getattr__(self, name):\n        return getattr(self.vis, name)\n\n\ndef get_style_data(path):\n    """"""\n    load style image\xef\xbc\x8c\n    Return\xef\xbc\x9a tensor shape 1*c*h*w, normalized\n    """"""\n    style_transform = tv.transforms.Compose([\n        tv.transforms.ToTensor(),\n        tv.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ])\n\n    style_image = tv.datasets.folder.default_loader(path)\n    style_tensor = style_transform(style_image)\n    return style_tensor.unsqueeze(0)\n\n\ndef normalize_batch(batch):\n    """"""\n    Input: b,ch,h,w  0~255\n    Output: b,ch,h,w  -2~2\n    """"""\n    mean = batch.data.new(IMAGENET_MEAN).view(1, -1, 1, 1)\n    std = batch.data.new(IMAGENET_STD).view(1, -1, 1, 1)\n    mean = (mean.expand_as(batch.data))\n    std = (std.expand_as(batch.data))\n    return (batch / 255.0 - mean) / std\n'"
chapter09-neural_poet_RNN/data.py,0,"b'# coding:utf-8\nimport sys\nimport os\nimport json\nimport re\nimport numpy as np\n\n\ndef _parseRawData(author=None, constrain=None, src=\'./chinese-poetry/json/simplified\', category=""poet.tang""):\n    """"""\n    code from https://github.com/justdark/pytorch-poetry-gen/blob/master/dataHandler.py\n    \xe5\xa4\x84\xe7\x90\x86json\xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe8\xaf\x97\xe6\xad\x8c\xe5\x86\x85\xe5\xae\xb9\n    @param: author\xef\xbc\x9a \xe4\xbd\x9c\xe8\x80\x85\xe5\x90\x8d\xe5\xad\x97\n    @param: constrain: \xe9\x95\xbf\xe5\xba\xa6\xe9\x99\x90\xe5\x88\xb6\n    @param: src: json \xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe6\x94\xbe\xe8\xb7\xaf\xe5\xbe\x84\n    @param: category: \xe7\xb1\xbb\xe5\x88\xab\xef\xbc\x8c\xe6\x9c\x89poet.song \xe5\x92\x8c poet.tang\n\n    \xe8\xbf\x94\xe5\x9b\x9e data\xef\xbc\x9alist\n        [\'\xe5\xba\x8a\xe5\x89\x8d\xe6\x98\x8e\xe6\x9c\x88\xe5\x85\x89\xef\xbc\x8c\xe7\x96\x91\xe6\x98\xaf\xe5\x9c\xb0\xe4\xb8\x8a\xe9\x9c\x9c\xef\xbc\x8c\xe4\xb8\xbe\xe5\xa4\xb4\xe6\x9c\x9b\xe6\x98\x8e\xe6\x9c\x88\xef\xbc\x8c\xe4\xbd\x8e\xe5\xa4\xb4\xe6\x80\x9d\xe6\x95\x85\xe4\xb9\xa1\xe3\x80\x82\',\n         \'\xe4\xb8\x80\xe5\x8e\xbb\xe4\xba\x8c\xe4\xb8\x89\xe9\x87\x8c\xef\xbc\x8c\xe7\x83\x9f\xe6\x9d\x91\xe5\x9b\x9b\xe4\xba\x94\xe5\xae\xb6\xef\xbc\x8c\xe4\xba\xad\xe5\x8f\xb0\xe5\x85\xad\xe4\xb8\x83\xe5\xba\xa7\xef\xbc\x8c\xe5\x85\xab\xe4\xb9\x9d\xe5\x8d\x81\xe6\x94\xaf\xe8\x8a\xb1\xe3\x80\x82\',\n        .........\n        ]\n    """"""\n\n    def sentenceParse(para):\n        # para \xe5\xbd\xa2\xe5\xa6\x82 ""-181-\xe6\x9d\x91\xe6\xa9\x8b\xe8\xb7\xaf\xe4\xb8\x8d\xe7\xab\xaf\xef\xbc\x8c\xe6\x95\xb8\xe9\x87\x8c\xe5\xb0\xb1\xe8\xbf\xb4\xe6\xb9\x8d\xe3\x80\x82\xe7\xa9\x8d\xe5\xa3\xa4\xe9\x80\xa3\xe6\xb6\x87\xe8\x84\x89\xef\xbc\x8c\xe9\xab\x98\xe6\x9e\x97\xe4\xb8\x8a\xe7\xac\x8b\xe7\xab\xbf\xe3\x80\x82\xe6\x97\xa9\xe5\x98\x97\xe7\x94\x98\xe8\x94\x97\xe6\xb7\xa1\xef\xbc\x8c\n        # \xe7\x94\x9f\xe6\x91\x98\xe7\x90\xb5\xe7\x90\xb6\xe9\x85\xb8\xe3\x80\x82\xef\xbc\x88\xe3\x80\x8c\xe7\x90\xb5\xe7\x90\xb6\xe3\x80\x8d\xef\xbc\x8c\xe5\x9a\xb4\xe5\xa3\xbd\xe6\xbe\x84\xe6\xa0\xa1\xe3\x80\x8a\xe5\xbc\xb5\xe7\xa5\x9c\xe8\xa9\xa9\xe9\x9b\x86\xe3\x80\x8b\xe4\xba\x91\xef\xbc\x9a\xe7\x96\x91\xe3\x80\x8c\xe6\x9e\x87\xe6\x9d\xb7\xe3\x80\x8d\xe4\xb9\x8b\xe8\xaa\xa4\xe3\x80\x82\xef\xbc\x89\n        # \xe5\xa5\xbd\xe6\x98\xaf\xe5\x8e\xbb\xe5\xa1\xb5\xe4\xbf\x97\xef\xbc\x8c\xe7\x85\x99\xe8\x8a\xb1\xe9\x95\xb7\xe4\xb8\x80\xe6\xac\x84\xe3\x80\x82""\n        result, number = re.subn(u""\xef\xbc\x88.*\xef\xbc\x89"", """", para)\n        result, number = re.subn(u""{.*}"", """", result)\n        result, number = re.subn(u""\xe3\x80\x8a.*\xe3\x80\x8b"", """", result)\n        result, number = re.subn(u""\xe3\x80\x8a.*\xe3\x80\x8b"", """", result)\n        result, number = re.subn(u""[\\]\\[]"", """", result)\n        r = """"\n        for s in result:\n            if s not in set(\'0123456789-\'):\n                r += s\n        r, number = re.subn(u""\xe3\x80\x82\xe3\x80\x82"", u""\xe3\x80\x82"", r)\n        return r\n\n    def handleJson(file):\n        # print file\n        rst = []\n        data = json.loads(open(file).read())\n        for poetry in data:\n            pdata = """"\n            if (author is not None and poetry.get(""author"") != author):\n                continue\n            p = poetry.get(""paragraphs"")\n            flag = False\n            for s in p:\n                sp = re.split(u""[\xef\xbc\x8c\xef\xbc\x81\xe3\x80\x82]"", s)\n                for tr in sp:\n                    if constrain is not None and len(tr) != constrain and len(tr) != 0:\n                        flag = True\n                        break\n                    if flag:\n                        break\n            if flag:\n                continue\n            for sentence in poetry.get(""paragraphs""):\n                pdata += sentence\n            pdata = sentenceParse(pdata)\n            if pdata != """":\n                rst.append(pdata)\n        return rst\n\n    data = []\n    for filename in os.listdir(src):\n        if filename.startswith(category):\n            data.extend(handleJson(src + filename))\n    return data\n\n\ndef pad_sequences(sequences,\n                  maxlen=None,\n                  dtype=\'int32\',\n                  padding=\'pre\',\n                  truncating=\'pre\',\n                  value=0.):\n    """"""\n    code from keras\n    Pads each sequence to the same length (length of the longest sequence).\n    If maxlen is provided, any sequence longer\n    than maxlen is truncated to maxlen.\n    Truncation happens off either the beginning (default) or\n    the end of the sequence.\n    Supports post-padding and pre-padding (default).\n    Arguments:\n        sequences: list of lists where each element is a sequence\n        maxlen: int, maximum length\n        dtype: type to cast the resulting sequence.\n        padding: \'pre\' or \'post\', pad either before or after each sequence.\n        truncating: \'pre\' or \'post\', remove values from sequences larger than\n            maxlen either in the beginning or in the end of the sequence\n        value: float, value to pad the sequences to the desired value.\n    Returns:\n        x: numpy array with dimensions (number_of_sequences, maxlen)\n    Raises:\n        ValueError: in case of invalid values for `truncating` or `padding`,\n            or in case of invalid shape for a `sequences` entry.\n    """"""\n    if not hasattr(sequences, \'__len__\'):\n        raise ValueError(\'`sequences` must be iterable.\')\n    lengths = []\n    for x in sequences:\n        if not hasattr(x, \'__len__\'):\n            raise ValueError(\'`sequences` must be a list of iterables. \'\n                             \'Found non-iterable: \' + str(x))\n        lengths.append(len(x))\n\n    num_samples = len(sequences)\n    if maxlen is None:\n        maxlen = np.max(lengths)\n\n    # take the sample shape from the first non empty sequence\n    # checking for consistency in the main loop below.\n    sample_shape = tuple()\n    for s in sequences:\n        if len(s) > 0:  # pylint: disable=g-explicit-length-test\n            sample_shape = np.asarray(s).shape[1:]\n            break\n\n    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n    for idx, s in enumerate(sequences):\n        if not len(s):  # pylint: disable=g-explicit-length-test\n            continue  # empty list/array was found\n        if truncating == \'pre\':\n            trunc = s[-maxlen:]  # pylint: disable=invalid-unary-operand-type\n        elif truncating == \'post\':\n            trunc = s[:maxlen]\n        else:\n            raise ValueError(\'Truncating type ""%s"" not understood\' % truncating)\n\n        # check `trunc` has expected shape\n        trunc = np.asarray(trunc, dtype=dtype)\n        if trunc.shape[1:] != sample_shape:\n            raise ValueError(\n                \'Shape of sample %s of sequence at position %s is different from \'\n                \'expected shape %s\'\n                % (trunc.shape[1:], idx, sample_shape))\n\n        if padding == \'post\':\n            x[idx, :len(trunc)] = trunc\n        elif padding == \'pre\':\n            x[idx, -len(trunc):] = trunc\n        else:\n            raise ValueError(\'Padding type ""%s"" not understood\' % padding)\n    return x\n\n\ndef get_data(opt):\n    """"""\n    @param opt \xe9\x85\x8d\xe7\xbd\xae\xe9\x80\x89\xe9\xa1\xb9 Config\xe5\xaf\xb9\xe8\xb1\xa1\n    @return word2ix: dict,\xe6\xaf\x8f\xe4\xb8\xaa\xe5\xad\x97\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x8c\xe5\xbd\xa2\xe5\xa6\x82u\'\xe6\x9c\x88\'->100\n    @return ix2word: dict,\xe6\xaf\x8f\xe4\xb8\xaa\xe5\xba\x8f\xe5\x8f\xb7\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xad\x97\xef\xbc\x8c\xe5\xbd\xa2\xe5\xa6\x82\'100\'->u\'\xe6\x9c\x88\'\n    @return data: numpy\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\x80\xe8\xa1\x8c\xe6\x98\xaf\xe4\xb8\x80\xe9\xa6\x96\xe8\xaf\x97\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xad\x97\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\n    """"""\n    if os.path.exists(opt.pickle_path):\n        data = np.load(opt.pickle_path, allow_pickle=True)\n        data, word2ix, ix2word = data[\'data\'], data[\'word2ix\'].item(), data[\'ix2word\'].item()\n        return data, word2ix, ix2word\n\n    # \xe5\xa6\x82\xe6\x9e\x9c\xe6\xb2\xa1\xe6\x9c\x89\xe5\xa4\x84\xe7\x90\x86\xe5\xa5\xbd\xe7\x9a\x84\xe4\xba\x8c\xe8\xbf\x9b\xe5\x88\xb6\xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8c\xe5\x88\x99\xe5\xa4\x84\xe7\x90\x86\xe5\x8e\x9f\xe5\xa7\x8b\xe7\x9a\x84json\xe6\x96\x87\xe4\xbb\xb6\n    data = _parseRawData(opt.author, opt.constrain, opt.data_path, opt.category)\n    words = {_word for _sentence in data for _word in _sentence}\n    word2ix = {_word: _ix for _ix, _word in enumerate(words)}\n    word2ix[\'<EOP>\'] = len(word2ix)  # \xe7\xbb\x88\xe6\xad\xa2\xe6\xa0\x87\xe8\xaf\x86\xe7\xac\xa6\n    word2ix[\'<START>\'] = len(word2ix)  # \xe8\xb5\xb7\xe5\xa7\x8b\xe6\xa0\x87\xe8\xaf\x86\xe7\xac\xa6\n    word2ix[\'</s>\'] = len(word2ix)  # \xe7\xa9\xba\xe6\xa0\xbc\n    ix2word = {_ix: _word for _word, _ix in list(word2ix.items())}\n\n    # \xe4\xb8\xba\xe6\xaf\x8f\xe9\xa6\x96\xe8\xaf\x97\xe6\xad\x8c\xe5\x8a\xa0\xe4\xb8\x8a\xe8\xb5\xb7\xe5\xa7\x8b\xe7\xac\xa6\xe5\x92\x8c\xe7\xbb\x88\xe6\xad\xa2\xe7\xac\xa6\n    for i in range(len(data)):\n        data[i] = [""<START>""] + list(data[i]) + [""<EOP>""]\n\n    # \xe5\xb0\x86\xe6\xaf\x8f\xe9\xa6\x96\xe8\xaf\x97\xe6\xad\x8c\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xe7\x94\xb1\xe2\x80\x98\xe5\xad\x97\xe2\x80\x99\xe5\x8f\x98\xe6\x88\x90\xe2\x80\x98\xe6\x95\xb0\xe2\x80\x99\n    # \xe5\xbd\xa2\xe5\xa6\x82[\xe6\x98\xa5,\xe6\xb1\x9f,\xe8\x8a\xb1,\xe6\x9c\x88,\xe5\xa4\x9c]\xe5\x8f\x98\xe6\x88\x90[1,2,3,4,5]\n    new_data = [[word2ix[_word] for _word in _sentence]\n                for _sentence in data]\n\n    # \xe8\xaf\x97\xe6\xad\x8c\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\x8d\xe5\xa4\x9fopt.maxlen\xe7\x9a\x84\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe8\xa1\xa5\xe7\xa9\xba\xe6\xa0\xbc\xef\xbc\x8c\xe8\xb6\x85\xe8\xbf\x87\xe7\x9a\x84\xef\xbc\x8c\xe5\x88\xa0\xe9\x99\xa4\xe6\x9c\xab\xe5\xb0\xbe\xe7\x9a\x84\n    pad_data = pad_sequences(new_data,\n                             maxlen=opt.maxlen,\n                             padding=\'pre\',\n                             truncating=\'post\',\n                             value=len(word2ix) - 1)\n\n    # \xe4\xbf\x9d\xe5\xad\x98\xe6\x88\x90\xe4\xba\x8c\xe8\xbf\x9b\xe5\x88\xb6\xe6\x96\x87\xe4\xbb\xb6\n    np.savez_compressed(opt.pickle_path,\n                        data=pad_data,\n                        word2ix=word2ix,\n                        ix2word=ix2word)\n    return pad_data, word2ix, ix2word\n'"
chapter09-neural_poet_RNN/main.py,0,"b'# coding:utf8\nimport sys, os\nimport torch as t\nfrom data import get_data\nfrom model import PoetryModel\nfrom torch import nn\nfrom utils import Visualizer\nimport tqdm\nfrom torchnet import meter\nimport ipdb\n\n\nclass Config(object):\n    data_path = \'data/\'  # \xe8\xaf\x97\xe6\xad\x8c\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\xe6\x96\x87\xe4\xbb\xb6\xe5\xad\x98\xe6\x94\xbe\xe8\xb7\xaf\xe5\xbe\x84\n    pickle_path = \'tang.npz\'  # \xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe5\xa5\xbd\xe7\x9a\x84\xe4\xba\x8c\xe8\xbf\x9b\xe5\x88\xb6\xe6\x96\x87\xe4\xbb\xb6\n    author = None  # \xe5\x8f\xaa\xe5\xad\xa6\xe4\xb9\xa0\xe6\x9f\x90\xe4\xbd\x8d\xe4\xbd\x9c\xe8\x80\x85\xe7\x9a\x84\xe8\xaf\x97\xe6\xad\x8c\n    constrain = None  # \xe9\x95\xbf\xe5\xba\xa6\xe9\x99\x90\xe5\x88\xb6\n    category = \'poet.tang\'  # \xe7\xb1\xbb\xe5\x88\xab\xef\xbc\x8c\xe5\x94\x90\xe8\xaf\x97\xe8\xbf\x98\xe6\x98\xaf\xe5\xae\x8b\xe8\xaf\x97\xe6\xad\x8c(poet.song)\n    lr = 1e-3\n    weight_decay = 1e-4\n    use_gpu = True\n    epoch = 20\n    batch_size = 128\n    maxlen = 125  # \xe8\xb6\x85\xe8\xbf\x87\xe8\xbf\x99\xe4\xb8\xaa\xe9\x95\xbf\xe5\xba\xa6\xe7\x9a\x84\xe4\xb9\x8b\xe5\x90\x8e\xe5\xad\x97\xe8\xa2\xab\xe4\xb8\xa2\xe5\xbc\x83\xef\xbc\x8c\xe5\xb0\x8f\xe4\xba\x8e\xe8\xbf\x99\xe4\xb8\xaa\xe9\x95\xbf\xe5\xba\xa6\xe7\x9a\x84\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe8\xa1\xa5\xe7\xa9\xba\xe6\xa0\xbc\n    plot_every = 20  # \xe6\xaf\x8f20\xe4\xb8\xaabatch \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe4\xb8\x80\xe6\xac\xa1\n    # use_env = True # \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8visodm\n    env = \'poetry\'  # visdom env\n    max_gen_len = 200  # \xe7\x94\x9f\xe6\x88\x90\xe8\xaf\x97\xe6\xad\x8c\xe6\x9c\x80\xe9\x95\xbf\xe9\x95\xbf\xe5\xba\xa6\n    debug_file = \'/tmp/debugp\'\n    model_path = None  # \xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb7\xaf\xe5\xbe\x84\n    prefix_words = \'\xe7\xbb\x86\xe9\x9b\xa8\xe9\xb1\xbc\xe5\x84\xbf\xe5\x87\xba,\xe5\xbe\xae\xe9\xa3\x8e\xe7\x87\x95\xe5\xad\x90\xe6\x96\x9c\xe3\x80\x82\'  # \xe4\xb8\x8d\xe6\x98\xaf\xe8\xaf\x97\xe6\xad\x8c\xe7\x9a\x84\xe7\xbb\x84\xe6\x88\x90\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe7\x94\xa8\xe6\x9d\xa5\xe6\x8e\xa7\xe5\x88\xb6\xe7\x94\x9f\xe6\x88\x90\xe8\xaf\x97\xe6\xad\x8c\xe7\x9a\x84\xe6\x84\x8f\xe5\xa2\x83\n    start_words = \'\xe9\x97\xb2\xe4\xba\x91\xe6\xbd\xad\xe5\xbd\xb1\xe6\x97\xa5\xe6\x82\xa0\xe6\x82\xa0\'  # \xe8\xaf\x97\xe6\xad\x8c\xe5\xbc\x80\xe5\xa7\x8b\n    acrostic = False  # \xe6\x98\xaf\xe5\x90\xa6\xe6\x98\xaf\xe8\x97\x8f\xe5\xa4\xb4\xe8\xaf\x97\n    model_prefix = \'checkpoints/tang\'  # \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\n\n\nopt = Config()\n\n\ndef generate(model, start_words, ix2word, word2ix, prefix_words=None):\n    """"""\n    \xe7\xbb\x99\xe5\xae\x9a\xe5\x87\xa0\xe4\xb8\xaa\xe8\xaf\x8d\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xae\xe8\xbf\x99\xe5\x87\xa0\xe4\xb8\xaa\xe8\xaf\x8d\xe6\x8e\xa5\xe7\x9d\x80\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x80\xe9\xa6\x96\xe5\xae\x8c\xe6\x95\xb4\xe7\x9a\x84\xe8\xaf\x97\xe6\xad\x8c\n    start_words\xef\xbc\x9au\'\xe6\x98\xa5\xe6\xb1\x9f\xe6\xbd\xae\xe6\xb0\xb4\xe8\xbf\x9e\xe6\xb5\xb7\xe5\xb9\xb3\'\n    \xe6\xaf\x94\xe5\xa6\x82start_words \xe4\xb8\xba \xe6\x98\xa5\xe6\xb1\x9f\xe6\xbd\xae\xe6\xb0\xb4\xe8\xbf\x9e\xe6\xb5\xb7\xe5\xb9\xb3\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x94\x9f\xe6\x88\x90\xef\xbc\x9a\n\n    """"""\n    \n    results = list(start_words)\n    start_word_len = len(start_words)\n    # \xe6\x89\x8b\xe5\x8a\xa8\xe8\xae\xbe\xe7\xbd\xae\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d\xe4\xb8\xba<START>\n    input = t.Tensor([word2ix[\'<START>\']]).view(1, 1).long()\n    if opt.use_gpu: input = input.cuda()\n    hidden = None\n\n    if prefix_words:\n        for word in prefix_words:\n            output, hidden = model(input, hidden)\n            input = input.data.new([word2ix[word]]).view(1, 1)\n\n    for i in range(opt.max_gen_len):\n        output, hidden = model(input, hidden)\n\n        if i < start_word_len:\n            w = results[i]\n            input = input.data.new([word2ix[w]]).view(1, 1)\n        else:\n            top_index = output.data[0].topk(1)[1][0].item()\n            w = ix2word[top_index]\n            results.append(w)\n            input = input.data.new([top_index]).view(1, 1)\n        if w == \'<EOP>\':\n            del results[-1]\n            break\n    return results\n\n\ndef gen_acrostic(model, start_words, ix2word, word2ix, prefix_words=None):\n    """"""\n    \xe7\x94\x9f\xe6\x88\x90\xe8\x97\x8f\xe5\xa4\xb4\xe8\xaf\x97\n    start_words : u\'\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\'\n    \xe7\x94\x9f\xe6\x88\x90\xef\xbc\x9a\n    \xe6\xb7\xb1\xe6\x9c\xa8\xe9\x80\x9a\xe4\xb8\xad\xe5\xb2\xb3\xef\xbc\x8c\xe9\x9d\x92\xe8\x8b\x94\xe5\x8d\x8a\xe6\x97\xa5\xe8\x84\x82\xe3\x80\x82\n    \xe5\xba\xa6\xe5\xb1\xb1\xe5\x88\x86\xe5\x9c\xb0\xe9\x99\xa9\xef\xbc\x8c\xe9\x80\x86\xe6\xb5\xaa\xe5\x88\xb0\xe5\x8d\x97\xe5\xb7\xb4\xe3\x80\x82\n    \xe5\xad\xa6\xe9\x81\x93\xe5\x85\xb5\xe7\x8a\xb9\xe6\xaf\x92\xef\xbc\x8c\xe5\xbd\x93\xe6\x97\xb6\xe7\x87\x95\xe4\xb8\x8d\xe7\xa7\xbb\xe3\x80\x82\n    \xe4\xb9\xa0\xe6\xa0\xb9\xe9\x80\x9a\xe5\x8f\xa4\xe5\xb2\xb8\xef\xbc\x8c\xe5\xbc\x80\xe9\x95\x9c\xe5\x87\xba\xe6\xb8\x85\xe7\xbe\xb8\xe3\x80\x82\n    """"""\n    results = []\n    start_word_len = len(start_words)\n    input = (t.Tensor([word2ix[\'<START>\']]).view(1, 1).long())\n    if opt.use_gpu: input = input.cuda()\n    hidden = None\n\n    index = 0  # \xe7\x94\xa8\xe6\x9d\xa5\xe6\x8c\x87\xe7\xa4\xba\xe5\xb7\xb2\xe7\xbb\x8f\xe7\x94\x9f\xe6\x88\x90\xe4\xba\x86\xe5\xa4\x9a\xe5\xb0\x91\xe5\x8f\xa5\xe8\x97\x8f\xe5\xa4\xb4\xe8\xaf\x97\n    # \xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d\n    pre_word = \'<START>\'\n\n    if prefix_words:\n        for word in prefix_words:\n            output, hidden = model(input, hidden)\n            input = (input.data.new([word2ix[word]])).view(1, 1)\n\n    for i in range(opt.max_gen_len):\n        output, hidden = model(input, hidden)\n        top_index = output.data[0].topk(1)[1][0].item()\n        w = ix2word[top_index]\n\n        if (pre_word in {u\'\xe3\x80\x82\', u\'\xef\xbc\x81\', \'<START>\'}):\n            # \xe5\xa6\x82\xe6\x9e\x9c\xe9\x81\x87\xe5\x88\xb0\xe5\x8f\xa5\xe5\x8f\xb7\xef\xbc\x8c\xe8\x97\x8f\xe5\xa4\xb4\xe7\x9a\x84\xe8\xaf\x8d\xe9\x80\x81\xe8\xbf\x9b\xe5\x8e\xbb\xe7\x94\x9f\xe6\x88\x90\n\n            if index == start_word_len:\n                # \xe5\xa6\x82\xe6\x9e\x9c\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe8\xaf\x97\xe6\xad\x8c\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x8c\x85\xe5\x90\xab\xe5\x85\xa8\xe9\x83\xa8\xe8\x97\x8f\xe5\xa4\xb4\xe7\x9a\x84\xe8\xaf\x8d\xef\xbc\x8c\xe5\x88\x99\xe7\xbb\x93\xe6\x9d\x9f\n                break\n            else:\n                # \xe6\x8a\x8a\xe8\x97\x8f\xe5\xa4\xb4\xe7\x9a\x84\xe8\xaf\x8d\xe4\xbd\x9c\xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x81\xe5\x85\xa5\xe6\xa8\xa1\xe5\x9e\x8b\n                w = start_words[index]\n                index += 1\n                input = (input.data.new([word2ix[w]])).view(1, 1)\n        else:\n            # \xe5\x90\xa6\xe5\x88\x99\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe6\x8a\x8a\xe4\xb8\x8a\xe4\xb8\x80\xe6\xac\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe6\x98\xaf\xe8\xaf\x8d\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d\xe8\xbe\x93\xe5\x85\xa5\n            input = (input.data.new([word2ix[w]])).view(1, 1)\n        results.append(w)\n        pre_word = w\n    return results\n\n\ndef train(**kwargs):\n    for k, v in kwargs.items():\n        setattr(opt, k, v)\n\n    opt.device=t.device(\'cuda\') if opt.use_gpu else t.device(\'cpu\')\n    device = opt.device\n    vis = Visualizer(env=opt.env)\n\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\n    data, word2ix, ix2word = get_data(opt)\n    data = t.from_numpy(data)\n    dataloader = t.utils.data.DataLoader(data,\n                                         batch_size=opt.batch_size,\n                                         shuffle=True,\n                                         num_workers=1)\n\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe5\xae\x9a\xe4\xb9\x89\n    model = PoetryModel(len(word2ix), 128, 256)\n    optimizer = t.optim.Adam(model.parameters(), lr=opt.lr)\n    criterion = nn.CrossEntropyLoss()\n    if opt.model_path:\n        model.load_state_dict(t.load(opt.model_path))\n    model.to(device)\n\n    loss_meter = meter.AverageValueMeter()\n    for epoch in range(opt.epoch):\n        loss_meter.reset()\n        for ii, data_ in tqdm.tqdm(enumerate(dataloader)):\n\n            # \xe8\xae\xad\xe7\xbb\x83\n            data_ = data_.long().transpose(1, 0).contiguous()\n            data_ = data_.to(device)\n            optimizer.zero_grad()\n            input_, target = data_[:-1, :], data_[1:, :]\n            output, _ = model(input_)\n            loss = criterion(output, target.view(-1))\n            loss.backward()\n            optimizer.step()\n\n            loss_meter.add(loss.item())\n\n            # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n            if (1 + ii) % opt.plot_every == 0:\n\n                if os.path.exists(opt.debug_file):\n                    ipdb.set_trace()\n\n                vis.plot(\'loss\', loss_meter.value()[0])\n\n                # \xe8\xaf\x97\xe6\xad\x8c\xe5\x8e\x9f\xe6\x96\x87\n                poetrys = [[ix2word[_word] for _word in data_[:, _iii].tolist()]\n                           for _iii in range(data_.shape[1])][:16]\n                vis.text(\'</br>\'.join([\'\'.join(poetry) for poetry in poetrys]), win=u\'origin_poem\')\n\n                gen_poetries = []\n                # \xe5\x88\x86\xe5\x88\xab\xe4\xbb\xa5\xe8\xbf\x99\xe5\x87\xa0\xe4\xb8\xaa\xe5\xad\x97\xe4\xbd\x9c\xe4\xb8\xba\xe8\xaf\x97\xe6\xad\x8c\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x97\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x908\xe9\xa6\x96\xe8\xaf\x97\n                for word in list(u\'\xe6\x98\xa5\xe6\xb1\x9f\xe8\x8a\xb1\xe6\x9c\x88\xe5\xa4\x9c\xe5\x87\x89\xe5\xa6\x82\xe6\xb0\xb4\'):\n                    gen_poetry = \'\'.join(generate(model, word, ix2word, word2ix))\n                    gen_poetries.append(gen_poetry)\n                vis.text(\'</br>\'.join([\'\'.join(poetry) for poetry in gen_poetries]), win=u\'gen_poem\')\n\n        t.save(model.state_dict(), \'%s_%s.pth\' % (opt.model_prefix, epoch))\n\n\ndef gen(**kwargs):\n    """"""\n    \xe6\x8f\x90\xe4\xbe\x9b\xe5\x91\xbd\xe4\xbb\xa4\xe8\xa1\x8c\xe6\x8e\xa5\xe5\x8f\xa3\xef\xbc\x8c\xe7\x94\xa8\xe4\xbb\xa5\xe7\x94\x9f\xe6\x88\x90\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe8\xaf\x97\n    """"""\n\n    for k, v in kwargs.items():\n        setattr(opt, k, v)\n    data, word2ix, ix2word = get_data(opt)\n    model = PoetryModel(len(word2ix), 128, 256);\n    map_location = lambda s, l: s\n    state_dict = t.load(opt.model_path, map_location=map_location)\n    model.load_state_dict(state_dict)\n\n    if opt.use_gpu:\n        model.cuda()\n\n    # python2\xe5\x92\x8cpython3 \xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe5\x85\xbc\xe5\xae\xb9\n    if sys.version_info.major == 3:\n        if opt.start_words.isprintable():\n            start_words = opt.start_words\n            prefix_words = opt.prefix_words if opt.prefix_words else None\n        else:\n            start_words = opt.start_words.encode(\'ascii\', \'surrogateescape\').decode(\'utf8\')\n            prefix_words = opt.prefix_words.encode(\'ascii\', \'surrogateescape\').decode(\n                \'utf8\') if opt.prefix_words else None\n    else:\n        start_words = opt.start_words.decode(\'utf8\')\n        prefix_words = opt.prefix_words.decode(\'utf8\') if opt.prefix_words else None\n\n    start_words = start_words.replace(\',\', u\'\xef\xbc\x8c\') \\\n        .replace(\'.\', u\'\xe3\x80\x82\') \\\n        .replace(\'?\', u\'\xef\xbc\x9f\')\n\n    gen_poetry = gen_acrostic if opt.acrostic else generate\n    result = gen_poetry(model, start_words, ix2word, word2ix, prefix_words)\n    print(\'\'.join(result))\n\n\nif __name__ == \'__main__\':\n    import fire\n\n    fire.Fire()\n'"
chapter09-neural_poet_RNN/model.py,4,"b'# coding:utf8\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PoetryModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super(PoetryModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=2)\n        self.linear1 = nn.Linear(self.hidden_dim, vocab_size)\n\n    def forward(self, input, hidden=None):\n        seq_len, batch_size = input.size()\n        if hidden is None:\n            #  h_0 = 0.01*torch.Tensor(2, batch_size, self.hidden_dim).normal_().cuda()\n            #  c_0 = 0.01*torch.Tensor(2, batch_size, self.hidden_dim).normal_().cuda()\n            h_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n            c_0 = input.data.new(2, batch_size, self.hidden_dim).fill_(0).float()\n        else:\n            h_0, c_0 = hidden\n        # size: (seq_len,batch_size,embeding_dim)\n        embeds = self.embeddings(input)\n        # output size: (seq_len,batch_size,hidden_dim)\n        output, hidden = self.lstm(embeds, (h_0, c_0))\n\n        # size: (seq_len*batch_size,vocab_size)\n        output = self.linear1(output.view(seq_len * batch_size, -1))\n        return output, hidden\n'"
chapter09-neural_poet_RNN/utils.py,0,"b'# coding:utf8\nimport visdom\nimport torch as t\nimport time\nimport torchvision as tv\nimport numpy as np\n\n\nclass Visualizer():\n    """"""\n    \xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86visdom\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xbd\xa0\xe4\xbb\x8d\xe7\x84\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87`self.vis.function`\n    \xe8\xb0\x83\xe7\x94\xa8\xe5\x8e\x9f\xe7\x94\x9f\xe7\x9a\x84visdom\xe6\x8e\xa5\xe5\x8f\xa3\n    """"""\n\n    def __init__(self, env=\'default\', **kwargs):\n        import visdom\n        self.vis = visdom.Visdom(env=env, use_incoming_socket=False, **kwargs)\n\n        # \xe7\x94\xbb\xe7\x9a\x84\xe7\xac\xac\xe5\x87\xa0\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\xa8\xaa\xe5\xba\xa7\xe6\xa0\x87\n        # \xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x88\xe2\x80\x99loss\',23\xef\xbc\x89 \xe5\x8d\xb3loss\xe7\x9a\x84\xe7\xac\xac23\xe4\xb8\xaa\xe7\x82\xb9\n        self.index = {}\n        self.log_text = \'\'\n\n    def reinit(self, env=\'default\', **kwargs):\n        """"""\n        \xe4\xbf\xae\xe6\x94\xb9visdom\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae\n        """"""\n        self.vis = visdom.Visdom(env=env,use_incoming_socket=False, **kwargs)\n        return self\n\n    def plot_many(self, d):\n        """"""\n        \xe4\xb8\x80\xe6\xac\xa1plot\xe5\xa4\x9a\xe4\xb8\xaa\n        @params d: dict (name,value) i.e. (\'loss\',0.11)\n        """"""\n        for k, v in d.items():\n            self.plot(k, v)\n\n    def img_many(self, d):\n        for k, v in d.items():\n            self.img(k, v)\n\n    def plot(self, name, y):\n        """"""\n        self.plot(\'loss\',1.00)\n        """"""\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=name,\n                      opts=dict(title=name),\n                      update=None if x == 0 else \'append\'\n                      )\n        self.index[name] = x + 1\n\n    def img(self, name, img_):\n        """"""\n        self.img(\'input_img\',t.Tensor(64,64))\n        """"""\n\n        if len(img_.size()) < 3:\n            img_ = img_.cpu().unsqueeze(0)\n        self.vis.image(img_.cpu(),\n                       win=name,\n                       opts=dict(title=name)\n                       )\n\n    def img_grid_many(self, d):\n        for k, v in d.items():\n            self.img_grid(k, v)\n\n    def img_grid(self, name, input_3d):\n        """"""\n        \xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe8\xbd\xac\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe5\x9b\xbe\xef\xbc\x8ci.e. input\xef\xbc\x8836\xef\xbc\x8c64\xef\xbc\x8c64\xef\xbc\x89\n        \xe4\xbc\x9a\xe5\x8f\x98\xe6\x88\x90 6*6 \xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe5\x9b\xbe\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xbc\xe5\xad\x90\xe5\xa4\xa7\xe5\xb0\x8f64*64\n        """"""\n        self.img(name, tv.utils.make_grid(\n            input_3d.cpu()[0].unsqueeze(1).clamp(max=1, min=0)))\n\n    def log(self, info, win=\'log_text\'):\n        """"""\n        self.log({\'loss\':1,\'lr\':0.0001})\n        """"""\n\n        self.log_text += (\'[{time}] {info} <br>\'.format(\n            time=time.strftime(\'%m%d_%H%M%S\'),\n            info=info))\n        self.vis.text(self.log_text, win=win)\n\n    def __getattr__(self, name):\n        return getattr(self.vis, name)\n'"
chapter10-image_caption/config.py,0,"b""# coding:utf8\n\n\nclass Config:\n    caption_data_path = 'caption.pth'  # \xe7\xbb\x8f\xe8\xbf\x87\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe5\x90\x8e\xe7\x9a\x84\xe4\xba\xba\xe5\xb7\xa5\xe6\x8f\x8f\xe8\xbf\xb0\xe4\xbf\xa1\xe6\x81\xaf\n    img_path = '/home/cy/caption_data/'\n    # img_path='/mnt/ht/aichallenger/raw/ai_challenger_caption_train_20170902/caption_train_images_20170902/'\n    img_feature_path = 'results.pth'  # \xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84features,20w*2048\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\n    scale_size = 300\n    img_size = 224\n    batch_size = 8\n    shuffle = True\n    num_workers = 4\n    rnn_hidden = 256\n    embedding_dim = 256\n    num_layers = 2\n    share_embedding_weights = False\n\n    prefix = 'checkpoints/caption'  # \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe5\x89\x8d\xe7\xbc\x80\n\n    env = 'caption'\n    plot_every = 10\n    debug_file = '/tmp/debugc'\n\n    model_ckpt = None  # \xe6\xa8\xa1\xe5\x9e\x8b\xe6\x96\xad\xe7\x82\xb9\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\n    lr = 1e-3\n    use_gpu = True\n    epoch = 1\n\n    test_img = 'img/example.jpeg'\n"""
chapter10-image_caption/data.py,1,"b'# coding:utf8\nimport torch as t\nfrom torch.utils import data\nimport os\nfrom PIL import Image\nimport torchvision as tv\nimport numpy as np\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\n\n# - \xe5\x8c\xba\xe5\x88\x86\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n# - \xe4\xb8\x8d\xe6\x98\xaf\xe9\x9a\x8f\xe6\x9c\xba\xe8\xbf\x94\xe5\x9b\x9e\xe6\xaf\x8f\xe5\x8f\xa5\xe8\xaf\x9d\xef\xbc\x8c\xe8\x80\x8c\xe6\x98\xaf\xe6\xa0\xb9\xe6\x8d\xaeindex%5\n# - \n\n# def create_collate_fn():\n#     def collate_fn():\n#         pass\n#     return collate_fn\n\ndef create_collate_fn(padding, eos, max_length=50):\n    def collate_fn(img_cap):\n        """"""\n        \xe5\xb0\x86\xe5\xa4\x9a\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe6\x8b\xbc\xe6\x8e\xa5\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaabatch\n        \xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x9a list of data\xef\xbc\x8c\xe5\xbd\xa2\xe5\xa6\x82\n        [(img1, cap1, index1), (img2, cap2, index2) ....]\n        \n        \xe6\x8b\xbc\xe6\x8e\xa5\xe7\xad\x96\xe7\x95\xa5\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n        - batch\xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe6\x8f\x8f\xe8\xbf\xb0\xe9\x95\xbf\xe5\xba\xa6\xe9\x83\xbd\xe6\x98\xaf\xe5\x9c\xa8\xe5\x8f\x98\xe5\x8c\x96\xe7\x9a\x84\xef\xbc\x8c\xe4\xb8\x8d\xe4\xb8\xa2\xe5\xbc\x83\xe4\xbb\xbb\xe4\xbd\x95\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d\\\n          \xe9\x80\x89\xe5\x8f\x96\xe9\x95\xbf\xe5\xba\xa6\xe6\x9c\x80\xe9\x95\xbf\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe5\xb0\x86\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\xa5\xe5\xad\x90pad\xe6\x88\x90\xe4\xb8\x80\xe6\xa0\xb7\xe9\x95\xbf\n        - \xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\x8d\xe5\xa4\x9f\xe7\x9a\x84\xe7\x94\xa8</PAD>\xe5\x9c\xa8\xe7\xbb\x93\xe5\xb0\xbePAD\n        - \xe6\xb2\xa1\xe6\x9c\x89START\xe6\xa0\x87\xe8\xaf\x86\xe7\xac\xa6\n        - \xe5\xa6\x82\xe6\x9e\x9c\xe9\x95\xbf\xe5\xba\xa6\xe5\x88\x9a\xe5\xa5\xbd\xe5\x92\x8c\xe8\xaf\x8d\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\xb0\xb1\xe6\xb2\xa1\xe6\x9c\x89</EOS>\n        \n        \xe8\xbf\x94\xe5\x9b\x9e\xef\xbc\x9a\n        - imgs(Tensor): batch_sie*2048\n        - cap_tensor(Tensor): batch_size*max_length\n        - lengths(list of int): \xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xbabatch_size\n        - index(list of int): \xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xbabatch_size\n        """"""\n        img_cap.sort(key=lambda p: len(p[1]), reverse=True)\n        imgs, caps, indexs = zip(*img_cap)\n        imgs = t.cat([img.unsqueeze(0) for img in imgs], 0)\n        lengths = [min(len(c) + 1, max_length) for c in caps]\n        batch_length = max(lengths)\n        cap_tensor = t.LongTensor(batch_length, len(caps)).fill_(padding)\n        for i, c in enumerate(caps):\n            end_cap = lengths[i] - 1\n            if end_cap < batch_length:\n                cap_tensor[end_cap, i] = eos\n            cap_tensor[:end_cap, i].copy_(c[:end_cap])\n        return (imgs, (cap_tensor, lengths), indexs)\n\n    return collate_fn\n\n\nclass CaptionDataset(data.Dataset):\n\n    def __init__(self, opt):\n        """"""\n        Attributes:\n            _data (dict): \xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x8c\x85\xe6\x8b\xac\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xef\xbc\x8c\xe4\xbb\xa5\xe5\x8f\x8a\xe5\xa4\x84\xe7\x90\x86\xe8\xbf\x87\xe5\x90\x8e\xe7\x9a\x84\xe6\x8f\x8f\xe8\xbf\xb0\n            all_imgs (tensor): \xe5\x88\xa9\xe7\x94\xa8resnet50\xe6\x8f\x90\xe5\x8f\x96\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x8c\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x88200000\xef\xbc\x8c2048\xef\xbc\x89\n            caption(list): \xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba20\xe4\xb8\x87\xe7\x9a\x84list\xef\xbc\x8c\xe5\x8c\x85\xe6\x8b\xac\xe6\xaf\x8f\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x96\x87\xe5\xad\x97\xe6\x8f\x8f\xe8\xbf\xb0\n            ix2id(dict): \xe6\x8c\x87\xe5\xae\x9a\xe5\xba\x8f\xe5\x8f\xb7\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\n            start_(int): \xe8\xb5\xb7\xe5\xa7\x8b\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x8c\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe7\x9a\x84\xe8\xb5\xb7\xe5\xa7\x8b\xe5\xba\x8f\xe5\x8f\xb7\xe6\x98\xaf0\xef\xbc\x8c\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\x9a\x84\xe8\xb5\xb7\xe5\xa7\x8b\xe5\xba\x8f\xe5\x8f\xb7\xe6\x98\xaf190000\xef\xbc\x8c\xe5\x8d\xb3\n                \xe5\x89\x8d190000\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe6\x98\xaf\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xef\xbc\x8c\xe5\x89\xa9\xe4\xb8\x8b\xe7\x9a\x8410000\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe6\x98\xaf\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n            len_(init): \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xef\xbc\x8c\xe9\x95\xbf\xe5\xba\xa6\xe5\xb0\xb1\xe6\x98\xaf190000\xef\xbc\x8c\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba10000\n            traininig(bool): \xe6\x98\xaf\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86(True),\xe8\xbf\x98\xe6\x98\xaf\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86(False)\n        """"""\n        self.opt = opt\n        data = t.load(opt.caption_data_path)\n        word2ix = data[\'word2ix\']\n        self.captions = data[\'caption\']\n        self.padding = word2ix.get(data.get(\'padding\'))\n        self.end = word2ix.get(data.get(\'end\'))\n        self._data = data\n        self.ix2id = data[\'ix2id\']\n        self.all_imgs = t.load(opt.img_feature_path)\n\n    def __getitem__(self, index):\n        """"""\n        \xe8\xbf\x94\xe5\x9b\x9e\xef\xbc\x9a\n        - img: \xe5\x9b\xbe\xe5\x83\x8ffeatures 2048\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\n        - caption: \xe6\x8f\x8f\xe8\xbf\xb0\xef\xbc\x8c\xe5\xbd\xa2\xe5\xa6\x82LongTensor([1,3,5,2]),\xe9\x95\xbf\xe5\xba\xa6\xe5\x8f\x96\xe5\x86\xb3\xe4\xba\x8e\xe6\x8f\x8f\xe8\xbf\xb0\xe9\x95\xbf\xe5\xba\xa6\n        - index: \xe4\xb8\x8b\xe6\xa0\x87\xef\xbc\x8c\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87ix2id[index]\xe8\x8e\xb7\xe5\x8f\x96\xe5\xaf\xb9\xe5\xba\x94\xe5\x9b\xbe\xe7\x89\x87\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\n        """"""\n        img = self.all_imgs[index]\n\n        caption = self.captions[index]\n        # 5\xe5\x8f\xa5\xe6\x8f\x8f\xe8\xbf\xb0\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe4\xb8\x80\xe5\x8f\xa5\n        rdn_index = np.random.choice(len(caption), 1)[0]\n        caption = caption[rdn_index]\n        return img, t.LongTensor(caption), index\n\n    def __len__(self):\n        return len(self.ix2id)\n\n\ndef get_dataloader(opt):\n    dataset = CaptionDataset(opt)\n    dataloader = data.DataLoader(dataset,\n                                 batch_size=opt.batch_size,\n                                 shuffle=opt.shuffle,\n                                 num_workers=opt.num_workers,\n                                 collate_fn=create_collate_fn(dataset.padding, dataset.end))\n    return dataloader\n\n\nif __name__ == \'__main__\':\n    from config import Config\n\n    opt = Config()\n    dataloader = get_dataloader(opt)\n    for ii, data in enumerate(dataloader):\n        print(ii, data)\n        break\n'"
chapter10-image_caption/data_preprocess.py,0,"b'# coding:utf8\nimport torch as t\nimport numpy as np\nimport json\nimport jieba\nimport tqdm\n\n\nclass Config:\n    annotation_file = \'caption_train_annotations_20170902.json\'\n    unknown = \'</UNKNOWN>\'\n    end = \'</EOS>\'\n    padding = \'</PAD>\'\n    max_words = 10000\n    min_appear = 2\n    save_path = \'caption.pth\'\n\n\n# START=\'</START>\'\n# MAX_LENS = 25,\n\ndef process(**kwargs):\n    opt = Config()\n    for k, v in kwargs.items():\n        setattr(opt, k, v)\n\n    with open(opt.annotation_file) as f:\n        data = json.load(f)\n\n    # 8f00f3d0f1008e085ab660e70dffced16a8259f6.jpg -> 0\n    id2ix = {item[\'image_id\']: ix for ix, item in enumerate(data)}\n    # 0-> 8f00f3d0f1008e085ab660e70dffced16a8259f6.jpg\n    ix2id = {ix: id for id, ix in (id2ix.items())}\n    assert id2ix[ix2id[10]] == 10\n\n    captions = [item[\'caption\'] for item in data]\n    # \xe5\x88\x86\xe8\xaf\x8d\xe7\xbb\x93\xe6\x9e\x9c\n    cut_captions = [[list(jieba.cut(ii, cut_all=False)) for ii in item] for item in tqdm.tqdm(captions)]\n\n    word_nums = {}  # \'\xe5\xbf\xab\xe4\xb9\x90\'-> 10000 (\xe6\xac\xa1)\n\n    def update(word_nums):\n        def fun(word):\n            word_nums[word] = word_nums.get(word, 0) + 1\n            return None\n\n        return fun\n\n    lambda_ = update(word_nums)\n    _ = {lambda_(word) for sentences in cut_captions for sentence in sentences for word in sentence}\n\n    # [ (10000,u\'\xe5\xbf\xab\xe4\xb9\x90\')\xef\xbc\x8c(9999,u\'\xe5\xbc\x80\xe5\xbf\x83\') ...]\n    word_nums_list = sorted([(num, word) for word, num in word_nums.items()], reverse=True)\n\n    #### \xe4\xbb\xa5\xe4\xb8\x8a\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe6\x98\xaf\xe6\x97\xa0\xe6\x8d\x9f\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x86\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c###############################\n    # **********\xe4\xbb\xa5\xe4\xb8\x8b\xe4\xbc\x9a\xe5\x88\xa0\xe9\x99\xa4\xe4\xb8\x80\xe4\xba\x9b\xe4\xbf\xa1\xe6\x81\xaf******************\n\n    # 1. \xe4\xb8\xa2\xe5\xbc\x83\xe8\xaf\x8d\xe9\xa2\x91\xe4\xb8\x8d\xe5\xa4\x9f\xe7\x9a\x84\xe8\xaf\x8d\n    # 2. ~~\xe4\xb8\xa2\xe5\xbc\x83\xe9\x95\xbf\xe5\xba\xa6\xe8\xbf\x87\xe9\x95\xbf\xe7\x9a\x84\xe8\xaf\x8d~~\n\n    words = [word[1] for word in word_nums_list[:opt.max_words] if word[0] >= opt.min_appear]\n    words = [opt.unknown, opt.padding, opt.end] + words\n    word2ix = {word: ix for ix, word in enumerate(words)}\n    ix2word = {ix: word for word, ix in word2ix.items()}\n    assert word2ix[ix2word[123]] == 123\n\n    ix_captions = [[[word2ix.get(word, word2ix.get(opt.unknown)) for word in sentence]\n                    for sentence in item]\n                   for item in cut_captions]\n    readme = u""""""\n    word\xef\xbc\x9a\xe8\xaf\x8d\n    ix:index\n    id:\xe5\x9b\xbe\xe7\x89\x87\xe5\x90\x8d\n    caption: \xe5\x88\x86\xe8\xaf\x8d\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe6\x8f\x8f\xe8\xbf\xb0\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87ix2word\xe5\x8f\xaf\xe4\xbb\xa5\xe8\x8e\xb7\xe5\xbe\x97\xe5\x8e\x9f\xe5\xa7\x8b\xe4\xb8\xad\xe6\x96\x87\xe8\xaf\x8d\n    """"""\n    results = {\n        \'caption\': ix_captions,\n        \'word2ix\': word2ix,\n        \'ix2word\': ix2word,\n        \'ix2id\': ix2id,\n        \'id2ix\': id2ix,\n        \'padding\': \'</PAD>\',\n        \'end\': \'</EOS>\',\n        \'readme\': readme\n    }\n    t.save(results, opt.save_path)\n    print(\'save file in %s\' % opt.save_path)\n\n    def test(ix, ix2=4):\n        results = t.load(opt.save_path)\n        ix2word = results[\'ix2word\']\n        examples = results[\'caption\'][ix][4]\n        sentences_p = (\'\'.join([ix2word[ii] for ii in examples]))\n        sentences_r = data[ix][\'caption\'][ix2]\n        assert sentences_p == sentences_r, \'test failed\'\n\n    test(1000)\n    print(\'test success\')\n\n\nif __name__ == \'__main__\':\n    import fire\n\n    fire.Fire()\n    # python data_preprocess.py process --annotation-file=/data/annotation.json --max-words=5000\n'"
chapter10-image_caption/feature_extract.py,2,"b'# coding:utf8\n""""""\n\xe5\x88\xa9\xe7\x94\xa8resnet50\xe6\x8f\x90\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe8\xaf\xad\xe4\xb9\x89\xe4\xbf\xa1\xe6\x81\xaf\n\xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe5\xb1\x82results.pth\n""""""\nfrom config import Config\nimport tqdm\nimport torch as t\nfrom torch.autograd import Variable\nimport torchvision as tv\nfrom torch.utils import data\nimport os\nfrom PIL import Image\nimport numpy as np\n\nt.set_grad_enabled(False)\nopt = Config()\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\nnormalize = tv.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n\n\nclass CaptionDataset(data.Dataset):\n\n    def __init__(self, caption_data_path):\n        self.transforms = tv.transforms.Compose([\n            tv.transforms.Resize(256),\n            tv.transforms.CenterCrop(256),\n            tv.transforms.ToTensor(),\n            normalize\n        ])\n\n        data = t.load(caption_data_path)\n        self.ix2id = data[\'ix2id\']\n        # \xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n        self.imgs = [os.path.join(opt.img_path, self.ix2id[_]) \\\n                     for _ in range(len(self.ix2id))]\n\n    def __getitem__(self, index):\n        img = Image.open(self.imgs[index]).convert(\'RGB\')\n        img = self.transforms(img)\n        return img, index\n\n    def __len__(self):\n        return len(self.imgs)\n\n\ndef get_dataloader(opt):\n    dataset = CaptionDataset(opt.caption_data_path)\n    dataloader = data.DataLoader(dataset,\n                                 batch_size=opt.batch_size,\n                                 shuffle=False,\n                                 num_workers=opt.num_workers,\n                                 )\n    return dataloader\n\n\n# \xe6\x95\xb0\xe6\x8d\xae\nopt.batch_size = 256\ndataloader = get_dataloader(opt)\nresults = t.Tensor(len(dataloader.dataset), 2048).fill_(0)\nbatch_size = opt.batch_size\n\n# \xe6\xa8\xa1\xe5\x9e\x8b\nresnet50 = tv.models.resnet50(pretrained=True)\ndel resnet50.fc\nresnet50.fc = lambda x: x\nresnet50.cuda()\n\n# \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x88\x86\xe6\x95\xb0\nfor ii, (imgs, indexs) in tqdm.tqdm(enumerate(dataloader)):\n    # \xe7\xa1\xae\xe4\xbf\x9d\xe5\xba\x8f\xe5\x8f\xb7\xe6\xb2\xa1\xe6\x9c\x89\xe5\xaf\xb9\xe5\xba\x94\xe9\x94\x99\n    assert indexs[0] == batch_size * ii\n    imgs = imgs.cuda()\n    features = resnet50(imgs)\n    results[ii * batch_size:(ii + 1) * batch_size] = features.data.cpu()\n\n# 200000*2048 20\xe4\xb8\x87\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe6\xaf\x8f\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x872048\xe7\xbb\xb4\xe7\x9a\x84feature\nt.save(results, \'results.pth\')\n'"
chapter10-image_caption/main.py,1,"b""# coding:utf8\nimport os  # ,ipdb\nimport torch as t\nimport torchvision as tv\nfrom torchnet import meter\nimport tqdm\n\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom model import CaptionModel\nfrom config import Config\nfrom utils import Visualizer\nfrom data import get_dataloader\nfrom PIL import Image\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\n\ndef generate(**kwargs):\n    opt = Config()\n    for k, v in kwargs.items():\n        setattr(opt, k, v)\n    device=t.device('cuda') if opt.use_gpu else t.device('cpu')\n\n    # \xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n    data = t.load(opt.caption_data_path, map_location=lambda s, l: s)\n    word2ix, ix2word = data['word2ix'], data['ix2word']\n\n    normalize = tv.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n    transforms = tv.transforms.Compose([\n        tv.transforms.Resize(opt.scale_size),\n        tv.transforms.CenterCrop(opt.img_size),\n        tv.transforms.ToTensor(),\n        normalize\n    ])\n    img = Image.open(opt.test_img)\n    img = transforms(img).unsqueeze(0)\n\n    # \xe7\x94\xa8resnet50\xe6\x9d\xa5\xe6\x8f\x90\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\xe7\x89\xb9\xe5\xbe\x81\n    resnet50 = tv.models.resnet50(True).eval()\n    del resnet50.fc\n    resnet50.fc = lambda x: x\n    resnet50.to(device)\n    img = img.to(device)\n    img_feats = resnet50(img).detach()\n\n    # Caption\xe6\xa8\xa1\xe5\x9e\x8b\n    model = CaptionModel(opt, word2ix, ix2word)\n    model = model.load(opt.model_ckpt).eval()\n    model.to(device)\n\n    results = model.generate(img_feats.data[0])\n    print('\\r\\n'.join(results))\n\n\ndef train(**kwargs):\n    opt = Config()\n    for k, v in kwargs.items():\n        setattr(opt, k, v)\n    device=t.device('cuda') if opt.use_gpu else t.device('cpu')\n\n    opt.caption_data_path = 'caption.pth'  # \xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\n    opt.test_img = ''  # \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\n    # opt.model_ckpt='caption_0914_1947' # \xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n\n    # \xe6\x95\xb0\xe6\x8d\xae\n    vis = Visualizer(env=opt.env)\n    dataloader = get_dataloader(opt)\n    _data = dataloader.dataset._data\n    word2ix, ix2word = _data['word2ix'], _data['ix2word']\n\n    # \xe6\xa8\xa1\xe5\x9e\x8b\n    model = CaptionModel(opt, word2ix, ix2word)\n    if opt.model_ckpt:\n        model.load(opt.model_ckpt)\n    optimizer = model.get_optimizer(opt.lr)\n    criterion = t.nn.CrossEntropyLoss()\n   \n    model.to(device)\n\n    # \xe7\xbb\x9f\xe8\xae\xa1\n    loss_meter = meter.AverageValueMeter()\n\n    for epoch in range(opt.epoch):\n        loss_meter.reset()\n        for ii, (imgs, (captions, lengths), indexes) in tqdm.tqdm(enumerate(dataloader)):\n            # \xe8\xae\xad\xe7\xbb\x83\n            optimizer.zero_grad()\n            imgs = imgs.to(device)\n            captions = captions.to(device)\n            input_captions = captions[:-1]\n            target_captions = pack_padded_sequence(captions, lengths)[0]\n            score, _ = model(imgs, input_captions, lengths)\n            loss = criterion(score, target_captions)\n            loss.backward()\n            optimizer.step()\n            loss_meter.add(loss.item())\n\n            # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n            if (ii + 1) % opt.plot_every == 0:\n                if os.path.exists(opt.debug_file):\n                    ipdb.set_trace()\n\n                vis.plot('loss', loss_meter.value()[0])\n\n                # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe5\x8e\x9f\xe5\xa7\x8b\xe5\x9b\xbe\xe7\x89\x87 + \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe4\xba\xba\xe5\xb7\xa5\xe7\x9a\x84\xe6\x8f\x8f\xe8\xbf\xb0\xe8\xaf\xad\xe5\x8f\xa5\n                raw_img = _data['ix2id'][indexes[0]]\n                img_path = opt.img_path + raw_img\n                raw_img = Image.open(img_path).convert('RGB')\n                raw_img = tv.transforms.ToTensor()(raw_img)\n\n                raw_caption = captions.data[:, 0]\n                raw_caption = ''.join([_data['ix2word'][ii] for ii in raw_caption])\n                vis.text(raw_caption, u'raw_caption')\n                vis.img('raw', raw_img, caption=raw_caption)\n\n                # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe7\xbd\x91\xe7\xbb\x9c\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe6\x8f\x8f\xe8\xbf\xb0\xe8\xaf\xad\xe5\x8f\xa5\n                results = model.generate(imgs.data[0])\n                vis.text('</br>'.join(results), u'caption')\n        model.save()\n\n\nif __name__ == '__main__':\n    import fire\n\n    fire.Fire()\n"""
chapter10-image_caption/model.py,1,"b'# coding:utf8\nimport torch as t\nfrom torch import nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom utils.beam_search import CaptionGenerator\nimport time\n\n\nclass CaptionModel(nn.Module):\n    def __init__(self, opt, word2ix, ix2word):\n        super(CaptionModel, self).__init__()\n        self.ix2word = ix2word\n        self.word2ix = word2ix\n        self.opt = opt\n        self.fc = nn.Linear(2048, opt.rnn_hidden)\n\n        self.rnn = nn.LSTM(opt.embedding_dim, opt.rnn_hidden, num_layers=opt.num_layers)\n        self.classifier = nn.Linear(opt.rnn_hidden, len(word2ix))\n        self.embedding = nn.Embedding(len(word2ix), opt.embedding_dim)\n        # if opt.share_embedding_weights:\n        #     # rnn_hidden=embedding_dim\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe6\x89\x8d\xe5\x8f\xaf\xe4\xbb\xa5\n        #     self.embedding.weight\n\n    def forward(self, img_feats, captions, lengths):\n        embeddings = self.embedding(captions)\n        # img_feats\xe6\x98\xaf2048\xe7\xbb\xb4\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f,\xe9\x80\x9a\xe8\xbf\x87\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe8\xbd\xac\xe4\xb8\xba256\xe7\xbb\xb4\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f,\xe5\x92\x8c\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe4\xb8\x80\xe6\xa0\xb7\n        img_feats = self.fc(img_feats).unsqueeze(0)\n        # \xe5\xb0\x86img_feats\xe7\x9c\x8b\xe6\x88\x90\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d\xe7\x9a\x84\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f \n        embeddings = t.cat([img_feats, embeddings], 0)\n        # PackedSequence\n        packed_embeddings = pack_padded_sequence(embeddings, lengths)\n        outputs, state = self.rnn(packed_embeddings)\n        # lstm\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe7\x89\xb9\xe5\xbe\x81\xe7\x94\xa8\xe6\x9d\xa5\xe5\x88\x86\xe7\xb1\xbb\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\n        # \xe5\x9b\xa0\xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xafPackedSequence,\xe6\x89\x80\xe4\xbb\xa5\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84output\xe4\xb9\x9f\xe6\x98\xafPackedSequence\n        # PackedSequence\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe6\x98\xafVariable,\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe6\x98\xafbatch_sizes,\n        # \xe5\x8d\xb3batch\xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n        pred = self.classifier(outputs[0])\n        return pred, state\n\n    def generate(self, img, eos_token=\'</EOS>\',\n                 beam_size=3,\n                 max_caption_length=30,\n                 length_normalization_factor=0.0):\n        """"""\n        \xe6\xa0\xb9\xe6\x8d\xae\xe5\x9b\xbe\xe7\x89\x87\xe7\x94\x9f\xe6\x88\x90\xe6\x8f\x8f\xe8\xbf\xb0,\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8beam search\xe7\xae\x97\xe6\xb3\x95\xe4\xbb\xa5\xe5\xbe\x97\xe5\x88\xb0\xe6\x9b\xb4\xe5\xa5\xbd\xe7\x9a\x84\xe6\x8f\x8f\xe8\xbf\xb0\n        """"""\n        cap_gen = CaptionGenerator(embedder=self.embedding,\n                                   rnn=self.rnn,\n                                   classifier=self.classifier,\n                                   eos_id=self.word2ix[eos_token],\n                                   beam_size=beam_size,\n                                   max_caption_length=max_caption_length,\n                                   length_normalization_factor=length_normalization_factor)\n        if next(self.parameters()).is_cuda:\n            img = img.cuda()\n        img =img.unsqueeze(0)\n        img = self.fc(img).unsqueeze(0)\n        sentences, score = cap_gen.beam_search(img)\n        sentences = [\' \'.join([self.ix2word[idx] for idx in sent])\n                     for sent in sentences]\n        return sentences\n\n    def states(self):\n        opt_state_dict = {attr: getattr(self.opt, attr)\n                          for attr in dir(self.opt)\n                          if not attr.startswith(\'__\')}\n        return {\n            \'state_dict\': self.state_dict(),\n            \'opt\': opt_state_dict\n        }\n\n    def save(self, path=None, **kwargs):\n        if path is None:\n            path = \'{prefix}_{time}\'.format(prefix=self.opt.prefix,\n                                            time=time.strftime(\'%m%d_%H%M\'))\n        states = self.states()\n        states.update(kwargs)\n        t.save(states, path)\n        return path\n\n    def load(self, path, load_opt=False):\n        data = t.load(path, map_location=lambda s, l: s)\n        state_dict = data[\'state_dict\']\n        self.load_state_dict(state_dict)\n\n        if load_opt:\n            for k, v in data[\'opt\'].items():\n                setattr(self.opt, k, v)\n\n        return self\n\n    def get_optimizer(self, lr):\n        return t.optim.Adam(self.parameters(), lr=lr)\n'"
chapter11-speech_recognition/BeamSearch.py,0,"b'#encoding=utf-8\n\nimport math\n\nLOG_ZERO = -99999999.0\nLOG_ONE = 0.0\n\nclass BeamEntry:\n    ""information about one single beam at specific time-step""\n    def __init__(self):\n        self.prTotal=LOG_ZERO       # blank and non-blank\n        self.prNonBlank=LOG_ZERO    # non-blank\n        self.prBlank=LOG_ZERO       # blank\n        self.y=()                   # labelling at current time-step\n\nclass BeamState:\n    ""information about beams at specific time-step""\n    def __init__(self):\n        self.entries={}\n\n    def norm(self):\n        ""length-normalise probabilities to avoid penalising long labellings""\n        for (k,v) in self.entries.items():\n            labellingLen=len(self.entries[k].y)\n            self.entries[k].prTotal=self.entries[k].prTotal*(1.0/(labellingLen if labellingLen else 1))\n\n    def sort(self):\n        ""return beams sorted by probability""\n        u=[v for (k,v) in self.entries.items()]\n        s=sorted(u, reverse=True, key=lambda x:x.prTotal)\n        return [x.y for x in s]\n\nclass ctcBeamSearch(object):\n    def __init__(self, classes, beam_width, blank_index=0):\n        self.classes = classes\n        self.beamWidth = beam_width\n        self.blank_index = blank_index\n        \n    def log_add_prob(self, log_x, log_y):\n        if log_x <= LOG_ZERO:\n            return log_y\n        if log_y <= LOG_ZERO:\n            return log_x\n        if (log_y - log_x) > 0.0:\n            log_y, log_x = log_x, log_y\n        return log_x + math.log(1 + math.exp(log_y - log_x))\n    \n    def calcExtPr(self, k, y, t, mat, beamState):\n        ""probability for extending labelling y to y+k""\n        # optical model (RNN)\n        if len(y) and y[-1]==k and mat[t-1, self.blank_index] < 0.9:\n            return math.log(mat[t, k]) + beamState.entries[y].prBlank\n        else:\n            return math.log(mat[t, k]) + beamState.entries[y].prTotal\n    \n    def addLabelling(self, beamState, y):\n        ""adds labelling if it does not exist yet""\n        if y not in beamState.entries:\n            beamState.entries[y]=BeamEntry()\n    \n    def decode(self, inputs, inputs_list):\n        """"""\n        Args: \n            inputs(FloatTesnor) :  Output of CTC(batch * timesteps * class)\n            inputs_list(list)   :  the frames of each sample\n        Returns:\n            res(list)           :  Result of beam search\n        """"""\n        batches, maxT, maxC = inputs.size()\n        res = []\n        \n        for batch in range(batches):\n            mat = inputs[batch].numpy()\n            # Initialise beam state\n            last=BeamState()\n            y=()\n            last.entries[y]=BeamEntry()\n            last.entries[y].prBlank=LOG_ONE\n            last.entries[y].prTotal=LOG_ONE\n            \n            # go over all time-steps\n            for t in range(inputs_list[batch]):\n                curr=BeamState()\n                \n                #\xe8\xb7\xb3\xe8\xbf\x87\xe6\xa6\x82\xe7\x8e\x87\xe5\xbe\x88\xe6\x8e\xa5\xe8\xbf\x911\xe7\x9a\x84blank\xe5\xb8\xa7\xef\xbc\x8c\xe5\xa2\x9e\xe5\x8a\xa0\xe8\xa7\xa3\xe7\xa0\x81\xe9\x80\x9f\xe5\xba\xa6\n                if (1 - mat[t, self.blank_index]) < 0.1:\n                    continue\n\n                #\xe5\x8f\x96\xe5\x89\x8dbeam\xe4\xb8\xaa\xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n                BHat=last.sort()[0:self.beamWidth]\n                # go over best labellings\n                for y in BHat:\n                    prNonBlank=LOG_ZERO\n                    # if nonempty labelling\n                    if len(y)>0:\n                        #\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84y\xe4\xb8\xa4\xe7\xa7\x8d\xe5\x8f\xaf\xe8\x83\xbd\xef\xbc\x8c\xe5\x8a\xa0\xe5\x85\xa5\xe9\x87\x8d\xe5\xa4\x8d\xe6\x88\x96\xe8\x80\x85\xe5\x8a\xa0\xe5\x85\xa5\xe7\xa9\xba\xe7\x99\xbd,\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb9\x8b\xe5\x89\x8d\xe6\xb2\xa1\xe6\x9c\x89\xe5\xad\x97\xe7\xac\xa6\xef\xbc\x8c\xe5\x9c\xa8NonBlank\xe6\xa6\x82\xe7\x8e\x87\xe4\xb8\xba0\n                        prNonBlank=last.entries[y].prNonBlank + math.log(mat[t, y[-1]])     \n                            \n                    # calc probabilities\n                    prBlank = (last.entries[y].prTotal) + math.log(mat[t, self.blank_index])\n                    # save result\n                    self.addLabelling(curr, y)\n                    curr.entries[y].y=y\n                    curr.entries[y].prNonBlank = self.log_add_prob(curr.entries[y].prNonBlank, prNonBlank)\n                    curr.entries[y].prBlank = self.log_add_prob(curr.entries[y].prBlank, prBlank)\n                    prTotal = self.log_add_prob(prBlank, prNonBlank)\n                    curr.entries[y].prTotal = self.log_add_prob(curr.entries[y].prTotal, prTotal)\n                            \n                    #t\xe6\x97\xb6\xe5\x88\xbb\xe5\x8a\xa0\xe5\x85\xa5\xe5\x85\xb6\xe5\xae\x83\xe7\x9a\x84label,\xe6\xad\xa4\xe6\x97\xb6Blank\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe4\xb8\xba0\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe5\x8a\xa0\xe5\x85\xa5\xe7\x9a\x84label\xe4\xb8\x8e\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe7\x9b\xb8\xe5\x90\x8c\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe4\xb8\x8d\xe8\x83\xbd\xe9\x87\x8d\xe5\xa4\x8d\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\x80\xe5\xae\x9a\xe6\x98\xafblank\n                    for k in range(maxC):                                         \n                        if k != self.blank_index:\n                            newY=y+(k,)\n                            prNonBlank=self.calcExtPr(k, y, t, mat, last)\n                                    \n                            # save result\n                            self.addLabelling(curr, newY)\n                            curr.entries[newY].y=newY\n                            curr.entries[newY].prNonBlank = self.log_add_prob(curr.entries[newY].prNonBlank, prNonBlank)\n                            curr.entries[newY].prTotal = self.log_add_prob(curr.entries[newY].prTotal, prNonBlank)\n                    \n                    # set new beam state\n                last=curr\n                    \n            # normalise probabilities according to labelling length\n            last.norm() \n            \n            # sort by probability\n            bestLabelling=last.sort()[0] # get most probable labelling\n            \n            # map labels to chars\n            res_b =\'\'.join([self.classes[l] for l in bestLabelling])\n            res.append(res_b)\n        return res\n\n'"
chapter11-speech_recognition/data.py,7,"b'#encoding=utf-8\n\n#\xe6\x9c\xac\xe6\x96\x87\xe4\xbb\xb6\xe7\xbb\xa7\xe6\x89\xbf\xe6\x9e\x84\xe5\xbb\xba\xe4\xba\x86Dataset\xe7\xb1\xbb\xe5\x92\x8cDataLoader\xe7\xb1\xbb\xef\xbc\x8c\xe7\x94\xa8\xe6\x9d\xa5\xe5\xa4\x84\xe7\x90\x86\xe9\x9f\xb3\xe9\xa2\x91\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\xe6\x96\x87\xe4\xbb\xb6\n#\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\n\nimport os\nimport torch\nimport scipy.signal\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom utils import parse_audio, process_label_file\n\nwindows = {\'hamming\':scipy.signal.hamming, \'hann\':scipy.signal.hann, \'blackman\':scipy.signal.blackman,\n            \'bartlett\':scipy.signal.bartlett}\naudio_conf = {""sample_rate"":16000, \'window_size\':0.025, \'window_stride\':0.01, \'window\': \'hamming\'}\nint2char = [""_"", ""\'"", ""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", ""h"", ""i"", ""j"", ""k"", ""l"", ""m"", ""n"", ""o"", ""p"",\n            ""q"", ""r"", ""s"", ""t"", ""u"", ""v"", ""w"", ""x"", ""y"", ""z"", "" ""]\n\nclass SpeechDataset(Dataset):\n    def __init__(self, data_dir, data_set=\'train\', normalize=True):\n        self.data_set = data_set\n        self.normalize = normalize\n        self.char2int = {}\n        self.n_feats = int(audio_conf[\'sample_rate\']*audio_conf[\'window_size\']/2+1)\n        for i in range(len(int2char)):\n            self.char2int[int2char[i]] = i\n        \n        wav_path = os.path.join(data_dir, data_set+\'_wav.scp\')\n        label_file = os.path.join(data_dir, data_set+\'.text\')\n        self.process_audio(wav_path, label_file)\n        \n    def process_audio(self, wav_path, label_file):\n        #read the label file\n        self.label = process_label_file(label_file, self.char2int)\n        \n        #read the path file\n        self.path  = []\n        with open(wav_path, \'r\') as f:\n            for line in f.readlines():\n                utt, path = line.strip().split()\n                self.path.append(path)\n        \n        #ensure the same samples of input and label\n        assert len(self.label) == len(self.path)\n\n    def __getitem__(self, idx):\n        return parse_audio(self.path[idx], audio_conf, windows, normalize=self.normalize), self.label[idx]\n\n    def __len__(self):\n        return len(self.path) \n\ndef collate_fn(batch):\n    #\xe5\xb0\x86\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe5\x8f\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84batch\n    #batch :     batch_size * (seq_len * nfeats, target_length)\n    def func(p):\n        return p[0].size(0)\n    \n    #sort batch according to the frame nums\n    batch = sorted(batch, reverse=True, key=func)\n    longest_sample = batch[0][0]\n    feat_size = longest_sample.size(1)\n    max_length = longest_sample.size(0)\n    batch_size = len(batch)\n    \n    inputs = torch.zeros(batch_size, max_length, feat_size)   #\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x85\xa5,\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\x8d\xe7\xad\x89\xe7\x9a\x84\xe8\xa1\xa50\n    input_sizes = torch.IntTensor(batch_size)                 #\xe8\xbe\x93\xe5\x85\xa5\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe5\x8d\xb3\xe5\xb8\xa7\xe6\x95\xb0\n    target_sizes = torch.IntTensor(batch_size)                #\xe6\xaf\x8f\xe5\x8f\xa5\xe6\xa0\x87\xe7\xad\xbe\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n    targets = []\n    input_size_list = []\n    \n    for x in range(batch_size):\n        sample = batch[x]\n        feature = sample[0]\n        label = sample[1]\n        seq_length = feature.size(0)\n        inputs[x].narrow(0, 0, seq_length).copy_(feature)\n        input_sizes[x] = seq_length\n        input_size_list.append(seq_length)\n        target_sizes[x] = len(label)\n        targets.extend(label)\n    targets = torch.IntTensor(targets)\n    return inputs, targets, input_sizes, input_size_list, target_sizes\n\n""""""\nclass torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, \n                                        sampler=None, batch_sampler=None, num_workers=0, \n                                        collate_fn=<function default_collate>, \n                                        pin_memory=False, drop_last=False)\n""""""\nclass SpeechDataLoader(DataLoader):\n    def __init__(self, *args, **kwargs):\n        super(SpeechDataLoader, self).__init__(*args, **kwargs)\n        self.collate_fn = collate_fn\n\n\n'"
chapter11-speech_recognition/decoder.py,1,"b'#encoding=utf-8\n\n#greedy decoder and beamsearch decoder for ctc\n\nimport torch\n\nclass Decoder(object):\n    ""\xe8\xa7\xa3\xe7\xa0\x81\xe5\x99\xa8\xe5\x9f\xba\xe7\xb1\xbb\xe5\xae\x9a\xe4\xb9\x89\xef\xbc\x8c\xe4\xbd\x9c\xe7\x94\xa8\xe6\x98\xaf\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe6\x96\x87\xe6\x9c\xac\xe4\xbd\xbf\xe5\x85\xb6\xe8\x83\xbd\xe5\xa4\x9f\xe4\xb8\x8e\xe6\xa0\x87\xe7\xad\xbe\xe8\xae\xa1\xe7\xae\x97\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87""\n    def __init__(self, int2char, space_idx = 28, blank_index = 0):\n        """"""\n        int2char     :     \xe5\xb0\x86\xe7\xb1\xbb\xe5\x88\xab\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe5\xad\x97\xe7\xac\xa6\xe6\xa0\x87\xe7\xad\xbe\n        space_idx    :     \xe7\xa9\xba\xe6\xa0\xbc\xe7\xac\xa6\xe5\x8f\xb7\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\xba\xe4\xb8\xba-1\xef\xbc\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe7\xa9\xba\xe6\xa0\xbc\xe4\xb8\x8d\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\n        blank_index  :     \xe7\xa9\xba\xe7\x99\xbd\xe7\xb1\xbb\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa4\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba0\n        """"""\n        self.int_to_char = int2char\n        self.space_idx = space_idx\n        self.blank_index = blank_index\n        self.num_word = 0\n        self.num_char = 0\n\n    def decode(self):\n        ""\xe8\xa7\xa3\xe7\xa0\x81\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x9c\xa8GreedyDecoder\xe5\x92\x8cBeamDecoder\xe7\xbb\xa7\xe6\x89\xbf\xe7\xb1\xbb\xe4\xb8\xad\xe5\xae\x9e\xe7\x8e\xb0""\n        raise NotImplementedError;\n\n    def phone_word_error(self, prob_tensor, frame_seq_len, targets, target_sizes):\n        """"""\xe8\xae\xa1\xe7\xae\x97\xe8\xaf\x8d\xe9\x94\x99\xe7\x8e\x87\xe5\x92\x8c\xe5\xad\x97\xe7\xac\xa6\xe9\x94\x99\xe8\xaf\xaf\xe7\x8e\x87\n        Args:\n            prob_tensor     :   \xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n            frame_seq_len   :   \xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe5\xb8\xa7\xe9\x95\xbf\n            targets         :   \xe6\xa0\xb7\xe6\x9c\xac\xe6\xa0\x87\xe7\xad\xbe\n            target_sizes    :   \xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe6\xa0\x87\xe7\xad\xbe\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n        Returns:\n            wer             :   \xe8\xaf\x8d\xe9\x94\x99\xe7\x8e\x87\xef\xbc\x8c\xe4\xbb\xa5space\xe4\xb8\xba\xe9\x97\xb4\xe9\x9a\x94\xe5\x88\x86\xe5\xbc\x80\xe4\xbd\x9c\xe4\xb8\xba\xe8\xaf\x8d\n            cer             :   \xe5\xad\x97\xe7\xac\xa6\xe9\x94\x99\xe8\xaf\xaf\xe7\x8e\x87\n        """"""\n        strings = self.decode(prob_tensor, frame_seq_len)\n        targets = self._unflatten_targets(targets, target_sizes)\n        target_strings = self._process_strings(self._convert_to_strings(targets))\n        \n        cer = 0\n        wer = 0\n        for x in range(len(target_strings)):\n            cer += self.cer(strings[x], target_strings[x])\n            wer += self.wer(strings[x], target_strings[x])\n            self.num_word += len(target_strings[x].split())\n            self.num_char += len(target_strings[x])\n        return cer, wer\n\n    def _unflatten_targets(self, targets, target_sizes):\n        """"""\xe5\xb0\x86\xe6\xa0\x87\xe7\xad\xbe\xe6\x8c\x89\xe7\x85\xa7\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xe9\x95\xbf\xe5\xba\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe5\x89\xb2\n        Args:\n            targets        :    \xe6\x95\xb0\xe5\xad\x97\xe8\xa1\xa8\xe7\xa4\xba\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\n            target_sizes   :    \xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe6\xa0\x87\xe7\xad\xbe\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n        Returns:\n            split_targets  :    \xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe5\x88\x86\xe5\x89\xb2\xe5\x90\x8e\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\n        """"""\n        split_targets = []\n        offset = 0\n        for size in target_sizes:\n            split_targets.append(targets[offset : offset + size])\n            offset += size\n        return split_targets\n\n    def _process_strings(self, seqs, remove_rep = False):\n        """"""\xe5\xa4\x84\xe7\x90\x86\xe8\xbd\xac\xe5\x8c\x96\xe5\x90\x8e\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe5\xba\x8f\xe5\x88\x97\xef\xbc\x8c\xe5\x8c\x85\xe6\x8b\xac\xe5\x8e\xbb\xe9\x87\x8d\xe5\xa4\x8d\xe7\xad\x89\xef\xbc\x8c\xe5\xb0\x86list\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbastring\n        Args:\n            seqs       :   \xe5\xbe\x85\xe5\xa4\x84\xe7\x90\x86\xe5\xba\x8f\xe5\x88\x97\n            remove_rep :   \xe6\x98\xaf\xe5\x90\xa6\xe5\x8e\xbb\xe9\x87\x8d\xe5\xa4\x8d\n        Returns:\n            processed_strings  :  \xe5\xa4\x84\xe7\x90\x86\xe5\x90\x8e\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe5\xba\x8f\xe5\x88\x97\n        """"""\n        processed_strings = []\n        for seq in seqs:\n            string = self._process_string(seq, remove_rep)\n            processed_strings.append(string)\n        return processed_strings\n   \n    def _process_string(self, seq, remove_rep = False):\n        string = \'\'\n        for i, char in enumerate(seq):\n            if char != self.int_to_char[self.blank_index]:\n                if remove_rep and i != 0 and char == seq[i - 1]: #remove dumplicates\n                    pass\n                elif self.space_idx == -1:\n                    string = string + \' \'+ char\n                elif char == self.int_to_char[self.space_idx]:\n                    string += \' \'\n                else:\n                    string = string + char\n        return string\n\n    def _convert_to_strings(self, seq, sizes=None):\n        """"""\xe5\xb0\x86\xe6\x95\xb0\xe5\xad\x97\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe5\xad\x97\xe7\xac\xa6\xe5\xba\x8f\xe5\x88\x97\n        Args:\n            seqs       :   \xe5\xbe\x85\xe8\xbd\xac\xe5\x8c\x96\xe5\xba\x8f\xe5\x88\x97\n            sizes      :   \xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n        Returns:\n            strings  :  \xe8\xbd\xac\xe5\x8c\x96\xe5\x90\x8e\xe7\x9a\x84\xe5\xad\x97\xe7\xac\xa6\xe5\xba\x8f\xe5\x88\x97\n        """"""\n        strings = []\n        for x in range(len(seq)):\n            seq_len = sizes[x] if sizes is not None else len(seq[x])\n            string = self._convert_to_string(seq[x], seq_len)\n            strings.append(string)\n        return strings\n\n    def _convert_to_string(self, seq, sizes):\n        result = []\n        for i in range(sizes):\n            result.append(self.int_to_char[seq[i]])\n        if self.space_idx == -1:\n            return result\n        else:\n            return \'\'.join(result)\n \n    def wer(self, s1, s2):\n        ""\xe5\xb0\x86\xe7\xa9\xba\xe6\xa0\xbc\xe4\xbd\x9c\xe4\xb8\xba\xe5\x88\x86\xe5\x89\xb2\xe8\xae\xa1\xe7\xae\x97\xe8\xaf\x8d\xe9\x94\x99\xe8\xaf\xaf\xe7\x8e\x87""\n        b = set(s1.split() + s2.split())\n        word2int = dict(zip(b, range(len(b))))\n\n        w1 = [word2int[w] for w in s1.split()]\n        w2 = [word2int[w] for w in s2.split()]\n        return self._edit_distance(w1, w2)\n\n    def cer(self, s1, s2):\n        ""\xe8\xae\xa1\xe7\xae\x97\xe5\xad\x97\xe7\xac\xa6\xe9\x94\x99\xe8\xaf\xaf\xe7\x8e\x87""\n        return self._edit_distance(s1, s2)\n    \n    def _edit_distance(self, src_seq, tgt_seq):\n        ""\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\xa4\xe4\xb8\xaa\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe7\xbc\x96\xe8\xbe\x91\xe8\xb7\x9d\xe7\xa6\xbb\xef\xbc\x8c\xe7\x94\xa8\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97\xe5\xad\x97\xe7\xac\xa6\xe9\x94\x99\xe8\xaf\xaf\xe7\x8e\x87""\n        L1, L2 = len(src_seq), len(tgt_seq)\n        if L1 == 0: return L2\n        if L2 == 0: return L1\n        # construct matrix of size (L1 + 1, L2 + 1)\n        dist = [[0] * (L2 + 1) for i in range(L1 + 1)]\n        for i in range(1, L2 + 1):\n            dist[0][i] = dist[0][i-1] + 1\n        for i in range(1, L1 + 1):\n            dist[i][0] = dist[i-1][0] + 1\n        for i in range(1, L1 + 1):\n            for j in range(1, L2 + 1):\n                if src_seq[i - 1] == tgt_seq[j - 1]:\n                    cost = 0\n                else:\n                    cost = 1\n                dist[i][j] = min(dist[i][j-1] + 1, dist[i-1][j] + 1, dist[i-1][j-1] + cost)\n        return dist[L1][L2]\n\n\nclass GreedyDecoder(Decoder):\n    ""\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xa7\xa3\xe7\xa0\x81\xef\xbc\x8c\xe6\x8a\x8a\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb8\xa7\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe6\xa6\x82\xe7\x8e\x87\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xe4\xbd\x9c\xe4\xb8\xba\xe8\xbe\x93\xe5\x87\xba\xe5\x80\xbc\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8d\xe6\x98\xaf\xe6\x95\xb4\xe4\xb8\xaa\xe5\xba\x8f\xe5\x88\x97\xe6\xa6\x82\xe7\x8e\x87\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc""\n    def decode(self, prob_tensor, frame_seq_len):\n        """"""\xe8\xa7\xa3\xe7\xa0\x81\xe5\x87\xbd\xe6\x95\xb0\n        Args:\n            prob_tensor   :   \xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbe\x93\xe5\x87\xba\n            frame_seq_len :   \xe6\xaf\x8f\xe4\xb8\x80\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe5\xb8\xa7\xe6\x95\xb0\n        Returns:\n            \xe8\xa7\xa3\xe7\xa0\x81\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84string\xef\xbc\x8c\xe5\x8d\xb3\xe8\xaf\x86\xe5\x88\xab\xe7\xbb\x93\xe6\x9e\x9c\n        """"""\n        prob_tensor = prob_tensor.transpose(0,1)\n        _, decoded = torch.max(prob_tensor, 2)\n        decoded = decoded.view(decoded.size(0), decoded.size(1))\n        decoded = self._convert_to_strings(decoded, frame_seq_len)     # convert digit idx to chars\n        return self._process_strings(decoded, remove_rep=True)\n\n\nclass BeamDecoder(Decoder):\n    ""Beam search \xe8\xa7\xa3\xe7\xa0\x81\xe3\x80\x82\xe8\xa7\xa3\xe7\xa0\x81\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\xba\xe6\x95\xb4\xe4\xb8\xaa\xe5\xba\x8f\xe5\x88\x97\xe6\xa6\x82\xe7\x8e\x87\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc""\n    def __init__(self, int2char, beam_width = 200, blank_index = 0, space_idx = 28):\n        self.beam_width = beam_width\n        super(BeamDecoder, self).__init__(int2char, space_idx=space_idx, blank_index=blank_index)\n\n        import BeamSearch\n        self._decoder = BeamSearch.ctcBeamSearch(int2char, beam_width, blank_index = blank_index)\n\n    def decode(self, prob_tensor, frame_seq_len=None):\n        """"""\xe8\xa7\xa3\xe7\xa0\x81\xe5\x87\xbd\xe6\x95\xb0\n        Args:\n            prob_tensor   :   \xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbe\x93\xe5\x87\xba\n            frame_seq_len :   \xe6\xaf\x8f\xe4\xb8\x80\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe5\xb8\xa7\xe6\x95\xb0\n        Returns:\n            res           :   \xe8\xa7\xa3\xe7\xa0\x81\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84string\xef\xbc\x8c\xe5\x8d\xb3\xe8\xaf\x86\xe5\x88\xab\xe7\xbb\x93\xe6\x9e\x9c\n        """"""\n        probs = prob_tensor.transpose(0, 1)\n        res = self._decoder.decode(probs, frame_seq_len)\n        return res\n\n'"
chapter11-speech_recognition/model.py,3,"b'#encoding=utf-8\n\n#\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x96\x87\xe4\xbb\xb6\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nclass SequenceWise(nn.Module):\n    """"""\xe8\xb0\x83\xe6\x95\xb4\xe8\xbe\x93\xe5\x85\xa5\xe6\xbb\xa1\xe8\xb6\xb3module\xe7\x9a\x84\xe9\x9c\x80\xe6\xb1\x82\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe5\xa4\x9a\xe6\xac\xa1\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\xa8\xa1\xe5\x9d\x97\xe5\x8c\x96\xe6\x9e\x84\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xb1\xbb\n    \xe9\x80\x82\xe7\x94\xa8\xe4\xba\x8e\xe5\xb0\x86LSTM\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe8\xbf\x87batchnorm\xe6\x88\x96\xe8\x80\x85Linear\xe5\xb1\x82\n    """"""\n    def __init__(self, module):\n        super(SequenceWise, self).__init__()\n        self.module = module\n\n    def forward(self, x):\n        """"""\n        Args:\n            x :    PackedSequence\n        """"""\n        x, batch_size_len = x.data, x.batch_sizes\n        #x.data:    sum(x_len) * num_features\n        x = self.module(x)\n        x = nn.utils.rnn.PackedSequence(x, batch_size_len)\n        return x\n\n    def __repr__(self):\n        tmpstr = self.__class__.__name__ + \' (\\n\'\n        tmpstr += self.module.__repr__()\n        tmpstr += \')\'\n        return tmpstr\n\nclass BatchSoftmax(nn.Module):\n    """"""\n    The layer to add softmax for a sequence, which is the output of rnn\n    Which state use its own softmax, and concat the result\n    """"""\n    def forward(self, x):\n        #x: seq_len * batch_size * num\n        if not self.training:\n            seq_len = x.size()[0]\n            return torch.stack([F.softmax(x[i], dim=1) for i in range(seq_len)], 0)\n        else:\n            return x\n\nclass BatchRNN(nn.Module):\n    """"""\n    Add BatchNorm before rnn to generate a batchrnn layer\n    """"""\n    def __init__(self, input_size, hidden_size, rnn_type=nn.LSTM, \n                    bidirectional=False, batch_norm=True, dropout=0.1):\n        super(BatchRNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bidirectional = bidirectional\n        self.batch_norm = SequenceWise(nn.BatchNorm1d(input_size)) if batch_norm else None\n        self.rnn = rnn_type(input_size=input_size, hidden_size=hidden_size,\n                                bidirectional=bidirectional, dropout = dropout, bias=False)\n        \n    def forward(self, x):\n        if self.batch_norm is not None:\n            x = self.batch_norm(x)\n        x, _ = self.rnn(x)\n        #self.rnn.flatten_parameters()\n        return x\n\nclass CTC_Model(nn.Module):\n    def __init__(self, rnn_param=None, num_class=48, drop_out=0.1):\n        """"""\n        rnn_param(dict)  :  the dict of rnn parameters\n                            rnn_param = {""rnn_input_size"":201, ""rnn_hidden_size"":256, ....}\n        num_class(int)   :  the number of units, add one for blank to be the classes to classify\n        drop_out(float)  :  drop_out paramteter for all place where need drop_out\n        """"""\n        super(CTC_Model, self).__init__()\n        if rnn_param is None or type(rnn_param) != dict:\n            raise ValueError(""rnn_param need to be a dict to contain all params of rnn!"")\n        self.rnn_param = rnn_param\n        self.num_class = num_class\n        self.num_directions = 2 if rnn_param[""bidirectional""] else 1\n        self._drop_out = drop_out\n        \n        rnn_input_size = rnn_param[""rnn_input_size""]\n        rnns = []\n        \n        rnn_hidden_size = rnn_param[""rnn_hidden_size""]\n        rnn_type = rnn_param[""rnn_type""]\n        rnn_layers = rnn_param[""rnn_layers""]\n        bidirectional = rnn_param[""bidirectional""]\n        batch_norm = rnn_param[""batch_norm""]\n        \n        rnn = BatchRNN(input_size=rnn_input_size, hidden_size=rnn_hidden_size, \n                        rnn_type=rnn_type, bidirectional=bidirectional, dropout=drop_out,\n                        batch_norm=False)\n        \n        rnns.append((\'0\', rnn))\n        #\xe5\xa0\x86\xe5\x8f\xa0RNN,\xe9\x99\xa4\xe4\xba\x86\xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8batchnorm\xef\xbc\x8c\xe5\x85\xb6\xe4\xbb\x96\xe5\xb1\x82RNN\xe9\x83\xbd\xe5\x8a\xa0\xe5\x85\xa5BachNorm\n        for i in range(rnn_layers - 1):\n            rnn = BatchRNN(input_size=self.num_directions*rnn_hidden_size, \n                            hidden_size=rnn_hidden_size, rnn_type=rnn_type, \n                            bidirectional=bidirectional, dropout=drop_out, batch_norm=batch_norm)\n            rnns.append((\'%d\' % (i+1), rnn))\n\n        self.rnns = nn.Sequential(OrderedDict(rnns))\n\n        if batch_norm:\n            fc = nn.Sequential(nn.BatchNorm1d(self.num_directions*rnn_hidden_size),\n                                nn.Linear(self.num_directions*rnn_hidden_size, num_class+1, bias=False),)\n        else:\n            fc = nn.Linear(self.num_directions*rnn_hidden_size, num_class+1, bias=False)\n        \n        self.fc = SequenceWise(fc)\n        self.inference_softmax = BatchSoftmax()\n    \n    def forward(self, x, dev=False):\n        x = self.rnns(x)\n        x = self.fc(x)\n        x, batch_seq = nn.utils.rnn.pad_packed_sequence(x, batch_first=False)\n            \n        out = self.inference_softmax(x)\n        if dev:\n            return x, out         #\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe5\x90\x8c\xe6\x97\xb6\xe8\xbf\x94\xe5\x9b\x9ex\xe8\xae\xa1\xe7\xae\x97loss\xe5\x92\x8cout\xe8\xbf\x9b\xe8\xa1\x8cwer\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\n        return out\n\n    @staticmethod\n    def save_package(model, optimizer=None, decoder=None, epoch=None, loss_results=None, dev_loss_results=None, dev_cer_results=None):\n        package = {\n                \'rnn_param\': model.rnn_param,\n                \'num_class\': model.num_class,\n                \'_drop_out\': model._drop_out,\n                \'state_dict\': model.state_dict()\n                }\n        if optimizer is not None:\n            package[\'optim_dict\'] = optimizer.state_dict()\n        if decoder is not None:\n            package[\'decoder\'] = decoder\n        if epoch is not None:\n            package[\'epoch\'] = epoch\n        if loss_results is not None:\n            package[\'loss_results\'] = loss_results\n            package[\'dev_loss_results\'] = dev_loss_results\n            package[\'dev_cer_results\'] = dev_cer_results\n        return package\n\n'"
chapter11-speech_recognition/test.py,3,"b'#encoding=utf-8\n\n#\xe6\x9c\xac\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\xb5\x8b\xe8\xaf\x95\xe6\x96\x87\xe4\xbb\xb6\n#\xe8\xa7\xa3\xe7\xa0\x81\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x9c\xa8run.sh\xe4\xb8\xad\xe5\xae\x9a\xe4\xb9\x89\n\nimport time\nimport torch\nimport argparse\nimport ConfigParser\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom model import *\nfrom decoder import GreedyDecoder, BeamDecoder\nfrom data  import int2char, SpeechDataset, SpeechDataLoader\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--conf\', help=\'conf file for training\')\ndef test():\n    args = parser.parse_args()\n    cf = ConfigParser.ConfigParser()\n    cf.read(args.conf)\n    USE_CUDA = cf.getboolean(\'Training\', \'USE_CUDA\')\n    model_path = cf.get(\'Model\', \'model_file\')\n    data_dir = cf.get(\'Data\', \'data_dir\')\n    beam_width = cf.getint(\'Decode\', \'beam_width\')\n    package = torch.load(model_path)\n    \n    rnn_param = package[""rnn_param""]\n    num_class = package[""num_class""]\n    n_feats = package[\'epoch\'][\'n_feats\']\n    drop_out = package[\'_drop_out\']\n\n    decoder_type =  cf.get(\'Decode\', \'decoder_type\')\n    data_set = cf.get(\'Decode\', \'eval_dataset\')\n\n    test_dataset = SpeechDataset(data_dir, data_set=data_set)\n    \n    model = CTC_Model(rnn_param=rnn_param, num_class=num_class, drop_out=drop_out)\n        \n    test_loader = SpeechDataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=False)\n    \n    model.load_state_dict(package[\'state_dict\'])\n    model.eval()\n    \n    if USE_CUDA:\n        model = model.cuda()\n\n    if decoder_type == \'Greedy\':\n        decoder  = GreedyDecoder(int2char, space_idx=len(int2char) - 1, blank_index = 0)\n    else:\n        decoder = BeamDecoder(int2char, beam_width=beam_width, blank_index = 0, space_idx = len(int2char) - 1)    \n\n    total_wer = 0\n    total_cer = 0\n    start = time.time()\n    for data in test_loader:\n        inputs, target, input_sizes, input_size_list, target_sizes = data \n        inputs = inputs.transpose(0,1)\n        inputs = Variable(inputs, volatile=True, requires_grad=False)\n        \n        if USE_CUDA:\n            inputs = inputs.cuda()\n        \n        inputs = nn.utils.rnn.pack_padded_sequence(inputs, input_size_list)\n        probs = model(inputs)\n\n        probs = probs.data.cpu()\n        decoded = decoder.decode(probs, input_size_list)\n        targets = decoder._unflatten_targets(target, target_sizes)\n        labels = decoder._process_strings(decoder._convert_to_strings(targets))\n\n        for x in range(len(labels)):\n            print(""origin : "" + labels[x])\n            print(""decoded: "" + decoded[x])\n        cer = 0\n        wer = 0\n        for x in range(len(labels)):\n            cer += decoder.cer(decoded[x], labels[x])\n            wer += decoder.wer(decoded[x], labels[x])\n            decoder.num_word += len(labels[x].split())\n            decoder.num_char += len(labels[x])\n        total_cer += cer\n        total_wer += wer\n    CER = (1 - float(total_cer) / decoder.num_char)*100\n    WER = (1 - float(total_wer) / decoder.num_word)*100\n    print(""Character error rate on test set: %.4f"" % CER)\n    print(""Word error rate on test set: %.4f"" % WER)\n    end = time.time()\n    time_used = (end - start) / 60.0\n    print(""time used for decode %d sentences: %.4f minutes."" % (len(test_dataset), time_used))\n\nif __name__ == ""__main__"":\n    test()\n'"
chapter11-speech_recognition/train.py,7,"b'#encoding=utf-8\n\n#\xe6\x9c\xac\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe8\xae\xad\xe7\xbb\x83\xe5\xa3\xb0\xe5\xad\xa6\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x96\x87\xe4\xbb\xb6\n\nimport os\nimport sys\nimport time\nimport copy\nimport torch\nimport argparse\nimport numpy as np\ntry:\n    import ConfigParser\nexcept:\n    import configparser as ConfigParser\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom model import *\nfrom decoder import GreedyDecoder\nfrom warpctc_pytorch import CTCLoss\nfrom data import int2char, SpeechDataset, SpeechDataLoader\n\n#\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84rnn\xe7\xb1\xbb\xe5\x9e\x8b\nRNN = {\'lstm\': nn.LSTM, \'rnn\':nn.RNN, \'gru\':nn.GRU }\n\nparser = argparse.ArgumentParser(description=\'lstm_ctc\')\nparser.add_argument(\'--conf\', default=\'./conf/ctc_model_setting.conf\' , help=\'conf file with Argument of LSTM and training\')\n\ndef train(model, train_loader, loss_fn, optimizer, logger, print_every=20, USE_CUDA=True):\n    """"""\xe8\xae\xad\xe7\xbb\x83\xe4\xb8\x80\xe4\xb8\xaaepoch\xef\xbc\x8c\xe5\x8d\xb3\xe5\xb0\x86\xe6\x95\xb4\xe4\xb8\xaa\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe8\xb7\x91\xe4\xb8\x80\xe6\xac\xa1\n    Args:\n        model         :  \xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\n        train_loader  :  \xe5\x8a\xa0\xe8\xbd\xbd\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe7\x9a\x84\xe7\xb1\xbb\xe5\xaf\xb9\xe8\xb1\xa1\n        loss_fn       :  \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe4\xb8\xbaCTCLoss\n        optimizer     :  \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\xb1\xbb\xe5\xaf\xb9\xe8\xb1\xa1\n        logger        :  \xe6\x97\xa5\xe5\xbf\x97\xe7\xb1\xbb\xe5\xaf\xb9\xe8\xb1\xa1\n        print_every   :  \xe6\xaf\x8f20\xe4\xb8\xaabatch\xe6\x89\x93\xe5\x8d\xb0\xe4\xb8\x80\xe6\xac\xa1loss\n        USE_CUDA      :  \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8GPU\n    Returns:\n        average_loss  :  \xe4\xb8\x80\xe4\xb8\xaaepoch\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87loss\n    """"""\n    model.train()\n    \n    total_loss = 0\n    print_loss = 0\n    i = 0\n    for data in train_loader:\n        inputs, targets, input_sizes, input_sizes_list, target_sizes = data\n        batch_size = inputs.size(0)\n        inputs = inputs.transpose(0, 1)\n        \n        inputs = Variable(inputs, requires_grad=False)\n        input_sizes = Variable(input_sizes, requires_grad=False)\n        targets = Variable(targets, requires_grad=False)\n        target_sizes = Variable(target_sizes, requires_grad=False)\n\n        if USE_CUDA:\n            inputs = inputs.cuda()\n        \n        inputs = nn.utils.rnn.pack_padded_sequence(inputs, input_sizes_list)\n        \n        out = model(inputs)\n        loss = loss_fn(out, targets, input_sizes, target_sizes)\n        loss /= batch_size\n        print_loss += loss.data[0]\n\n        if (i + 1) % print_every == 0:\n            print(\'batch = %d, loss = %.4f\' % (i+1, print_loss / print_every))\n            logger.debug(\'batch = %d, loss = %.4f\' % (i+1, print_loss / print_every))\n            print_loss = 0\n        \n        total_loss += loss.data[0]\n        optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm(model.parameters(), 400)\n        optimizer.step()\n        i += 1\n    average_loss = total_loss / i\n    print(""Epoch done, average loss: %.4f"" % average_loss)\n    logger.info(""Epoch done, average loss: %.4f"" % average_loss)\n    return average_loss\n\ndef dev(model, dev_loader, loss_fn, decoder, logger, USE_CUDA=True):\n    """"""\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe8\xbf\x87\xe7\xa8\x8b\xef\xbc\x8c\xe4\xb8\x8etrain()\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe6\x98\xaf\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe9\x9c\x80\xe8\xa6\x81\xe8\xae\xa1\xe7\xae\x97\xe5\xad\x97\xe7\xac\xa6\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\n    Args:\n        model       :   \xe6\xa8\xa1\xe5\x9e\x8b\n        dev_loader  :   \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\x9a\x84\xe7\xb1\xbb\xe5\xaf\xb9\xe8\xb1\xa1\n        loss_fn     :   \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n        decoder     :   \xe8\xa7\xa3\xe7\xa0\x81\xe7\xb1\xbb\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe5\x8d\xb3\xe5\xb0\x86\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe8\xa7\xa3\xe7\xa0\x81\xe6\x88\x90\xe6\x96\x87\xe6\x9c\xac\n        logger      :   \xe6\x97\xa5\xe5\xbf\x97\xe7\xb1\xbb\xe5\xaf\xb9\xe8\xb1\xa1\n        USE_CUDA    :   \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8GPU\n    Returns:\n        acc * 100    :   \xe5\xad\x97\xe7\xac\xa6\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9cspace\xe4\xb8\x8d\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\x87\xe7\xad\xbe\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe5\x88\x99\xe4\xb8\xba\xe8\xaf\x8d\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\n        average_loss :   \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87loss\n    """"""\n    model.eval()\n    total_cer = 0\n    total_tokens = 0\n    total_loss = 0\n    i = 0\n\n    for data in dev_loader:\n        inputs, targets, input_sizes, input_sizes_list, target_sizes = data\n        batch_size = inputs.size(0)\n        inputs = inputs.transpose(0, 1)\n\n        inputs = Variable(inputs, requires_grad=False)\n        input_sizes = Variable(input_sizes, requires_grad=False)\n        targets = Variable(targets, requires_grad=False)\n        target_sizes = Variable(target_sizes, requires_grad=False)\n\n        if USE_CUDA:\n            inputs = inputs.cuda()\n        \n        inputs = nn.utils.rnn.pack_padded_sequence(inputs, input_sizes_list)\n        out, probs = model(inputs, dev=True)\n        \n        loss = loss_fn(out, targets, input_sizes, target_sizes)\n        loss /= batch_size\n        total_loss += loss.data[0]\n        \n        probs = probs.data.cpu()\n        targets = targets.data\n        target_sizes = target_sizes.data\n\n        if decoder.space_idx == -1:\n            total_cer += decoder.phone_word_error(probs, input_sizes_list, targets, target_sizes)[1]\n        else:\n            total_cer += decoder.phone_word_error(probs, input_sizes_list, targets, target_sizes)[0]\n        total_tokens += sum(target_sizes)\n        i += 1\n    acc = 1 - float(total_cer) / total_tokens\n    average_loss = total_loss / i\n    return acc * 100, average_loss\n\ndef init_logger(log_file):\n    """"""\xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe6\x97\xa5\xe5\xbf\x97\xe7\x9a\x84\xe7\xb1\xbb\xe5\xaf\xb9\xe8\xb1\xa1\n    Args:\n        log_file   :  \xe6\x97\xa5\xe5\xbf\x97\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\n    Returns:\n        logger     :  \xe6\x97\xa5\xe5\xbf\x97\xe7\xb1\xbb\xe5\xaf\xb9\xe8\xb1\xa1\n    """"""\n    import logging\n    from logging.handlers import RotatingFileHandler\n\n    logger = logging.getLogger()\n    hdl = RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=10)\n    formatter=logging.Formatter(\'%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s\')\n    hdl.setFormatter(formatter)\n    logger.addHandler(hdl)\n    logger.setLevel(logging.DEBUG)\n    return logger\n\ndef main():\n    args = parser.parse_args()\n    cf = ConfigParser.ConfigParser()\n    try:\n        cf.read(args.conf)\n    except:\n        print(""conf file not exists"")\n        sys.exit(1)\n    USE_CUDA = cf.getboolean(\'Training\', \'use_cuda\')\n    try:\n        seed = long(cf.get(\'Training\', \'seed\'))\n    except:\n        seed = torch.cuda.initial_seed()\n        cf.set(\'Training\', \'seed\', seed)\n        cf.write(open(args.conf, \'w\'))\n    \n    torch.manual_seed(seed)\n    if USE_CUDA:\n        torch.cuda.manual_seed(seed)\n    \n    log_dir = cf.get(\'Data\', \'log_dir\')\n    log_file = os.path.join(log_dir, cf.get(\'Data\', \'log_file\'))\n    logger = init_logger(log_file)\n    \n    #Define Model\n    rnn_input_size = cf.getint(\'Model\', \'rnn_input_size\')\n    rnn_hidden_size = cf.getint(\'Model\', \'rnn_hidden_size\')\n    rnn_layers = cf.getint(\'Model\', \'rnn_layers\')\n    rnn_type = RNN[cf.get(\'Model\', \'rnn_type\')]\n    bidirectional = cf.getboolean(\'Model\', \'bidirectional\')\n    batch_norm = cf.getboolean(\'Model\', \'batch_norm\')\n    rnn_param = {""rnn_input_size"":rnn_input_size, ""rnn_hidden_size"":rnn_hidden_size, ""rnn_layers"":rnn_layers, \n                    ""rnn_type"":rnn_type, ""bidirectional"":bidirectional, ""batch_norm"":batch_norm}\n    num_class = cf.getint(\'Model\', \'num_class\')\n    drop_out = cf.getfloat(\'Model\', \'drop_out\')\n\n    model = CTC_Model(rnn_param=rnn_param, num_class=num_class, drop_out=drop_out)\n    print(""Model Structure:"")\n    logger.info(""Model Structure:"")\n    for idx, m in enumerate(model.children()):\n        print(idx, m)\n        logger.info(str(idx) + ""->"" + str(m))\n    \n    data_dir = cf.get(\'Data\', \'data_dir\')\n    batch_size = cf.getint(""Training"", \'batch_size\')\n    \n    #Data Loader\n    train_dataset = SpeechDataset(data_dir, data_set=\'train\')\n    dev_dataset = SpeechDataset(data_dir, data_set=""dev"")\n    train_loader = SpeechDataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                                        num_workers=4, pin_memory=False)\n    dev_loader = SpeechDataLoader(dev_dataset, batch_size=batch_size, shuffle=False,\n                                            num_workers=4, pin_memory=False)\n    \n    #ensure the feats is equal to the rnn_input_Size\n    assert train_dataset.n_feats == rnn_input_size\n    \n    #decoder for dev set\n    decoder = GreedyDecoder(int2char, space_idx=len(int2char) - 1, blank_index=0)\n        \n    #Training\n    init_lr = cf.getfloat(\'Training\', \'init_lr\')\n    num_epoches = cf.getint(\'Training\', \'num_epoches\')\n    end_adjust_acc = cf.getfloat(\'Training\', \'end_adjust_acc\')\n    decay = cf.getfloat(""Training"", \'lr_decay\')\n    weight_decay = cf.getfloat(""Training"", \'weight_decay\')\n    \n    params = { \'num_epoches\':num_epoches, \'end_adjust_acc\':end_adjust_acc, \'seed\':seed,\n            \'decay\':decay, \'learning_rate\':init_lr, \'weight_decay\':weight_decay, \'batch_size\':batch_size, \'n_feats\':train_dataset.n_feats }\n    print(params)\n    \n    if USE_CUDA:\n        model = model.cuda()\n    \n    loss_fn = CTCLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr, weight_decay=weight_decay)\n\n    #visualization for training\n    from visdom import Visdom\n    viz = Visdom()\n    title = \'TIMIT LSTM_CTC Acoustic Model\'\n\n    opts = [dict(title=title+"" Loss"", ylabel = \'Loss\', xlabel = \'Epoch\'),\n            dict(title=title+"" Loss on Dev"", ylabel = \'DEV Loss\', xlabel = \'Epoch\'),\n            dict(title=title+\' CER on DEV\', ylabel = \'DEV CER\', xlabel = \'Epoch\')]\n    viz_window = [None, None, None]\n    \n    count = 0\n    learning_rate = init_lr\n    loss_best = 1000\n    loss_best_true = 1000\n    adjust_rate_flag = False\n    stop_train = False\n    adjust_time = 0\n    acc_best = 0\n    start_time = time.time()\n    loss_results = []\n    dev_loss_results = []\n    dev_cer_results = []\n    \n    while not stop_train:\n        if count >= num_epoches:\n            break\n        count += 1\n        \n        if adjust_rate_flag:\n            learning_rate *= decay\n            adjust_rate_flag = False\n            for param in optimizer.param_groups:\n                param[\'lr\'] *= decay\n        \n        print(""Start training epoch: %d, learning_rate: %.5f"" % (count, learning_rate))\n        logger.info(""Start training epoch: %d, learning_rate: %.5f"" % (count, learning_rate))\n        \n        loss = train(model, train_loader, loss_fn, optimizer, logger, print_every=20, USE_CUDA=USE_CUDA)\n        loss_results.append(loss)\n        acc, dev_loss = dev(model, dev_loader, loss_fn, decoder, logger, USE_CUDA=USE_CUDA)\n        print(""loss on dev set is %.4f"" % dev_loss)\n        logger.info(""loss on dev set is %.4f"" % dev_loss)\n        dev_loss_results.append(dev_loss)\n        dev_cer_results.append(acc)\n        \n        #adjust learning rate by dev_loss\n        #adjust_rate_count  :  \xe8\xa1\xa8\xe7\xa4\xba\xe8\xbf\x9e\xe7\xbb\xad\xe8\xb6\x85\xe8\xbf\x87count\xe4\xb8\xaaepoch\xe7\x9a\x84loss\xe5\x9c\xa8end_adjust_acc\xe5\x8c\xba\xe9\x97\xb4\xe5\x86\x85\xe8\xae\xa4\xe4\xb8\xba\xe7\xa8\xb3\xe5\xae\x9a\n        if dev_loss < (loss_best - end_adjust_acc):\n            loss_best = dev_loss\n            loss_best_true = dev_loss\n            adjust_rate_count = 0\n            acc_best = acc\n            best_model_state = copy.deepcopy(model.state_dict())\n            best_op_state = copy.deepcopy(optimizer.state_dict())\n        elif (dev_loss < loss_best + end_adjust_acc):\n            adjust_rate_count += 1\n            if dev_loss < loss_best and dev_loss < loss_best_true:\n                loss_best_true = dev_loss\n                acc_best = acc\n                best_model_state = copy.deepcopy(model.state_dict())\n                best_op_state = copy.deepcopy(optimizer.state_dict())\n        else:\n            adjust_rate_count = 10\n        \n        print(""adjust_rate_count: %d"" % adjust_rate_count)\n        print(\'adjust_time: %d\' % adjust_time)\n        logger.info(""adjust_rate_count: %d"" % adjust_rate_count)\n        logger.info(\'adjust_time: %d\' % adjust_time)\n\n        if adjust_rate_count == 10:\n            adjust_rate_flag = True\n            adjust_time += 1\n            adjust_rate_count = 0\n            if loss_best > loss_best_true:\n                loss_best = loss_best_true\n            model.load_state_dict(best_model_state)\n            optimizer.load_state_dict(best_op_state)\n\n        if adjust_time == 8:\n            stop_train = True\n        \n        time_used = (time.time() - start_time) / 60\n        print(""epoch %d done, dev acc is: %.4f, time_used: %.4f minutes"" % (count, acc, time_used))\n        logger.info(""epoch %d done, dev acc is: %.4f, time_used: %.4f minutes"" % (count, acc, time_used))\n        \n        x_axis = range(count)\n        y_axis = [loss_results[0:count], dev_loss_results[0:count], dev_cer_results[0:count]]\n        for x in range(len(viz_window)):\n            if viz_window[x] is None:\n                viz_window[x] = viz.line(X = np.array(x_axis), Y = np.array(y_axis[x]), opts = opts[x],)\n            else:\n                viz.line(X = np.array(x_axis), Y = np.array(y_axis[x]), win = viz_window[x], update = \'replace\',)\n\n    print(""End training, best dev loss is: %.4f, acc is: %.4f"" % (loss_best_true, acc_best))\n    logger.info(""End training, best dev loss acc is: %.4f, acc is: %.4f"" % (loss_best_true, acc_best)) \n    model.load_state_dict(best_model_state)\n    optimizer.load_state_dict(best_op_state)\n    best_path = os.path.join(log_dir, \'best_model\'+\'_dev\'+str(acc_best)+\'.pkl\')\n    cf.set(\'Model\', \'model_file\', best_path)\n    cf.write(open(args.conf, \'w\'))\n    params[\'epoch\']=count\n\n    torch.save(CTC_Model.save_package(model, optimizer=optimizer, epoch=params, loss_results=loss_results, dev_loss_results=dev_loss_results, dev_cer_results=dev_cer_results), best_path)\n\nif __name__ == \'__main__\':\n    main()\n\n\n'"
chapter11-speech_recognition/utils.py,1,"b'#encoding=utf-8\n\n#\xe6\x9c\xac\xe6\x96\x87\xe4\xbb\xb6\xe6\x8f\x90\xe4\xbe\x9b\xe8\xbd\xbd\xe5\x85\xa5\xe9\x9f\xb3\xe9\xa2\x91\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0,\xe6\x8f\x90\xe5\x8f\x96\xe9\x9f\xb3\xe9\xa2\x91\xe5\xaf\xb9\xe6\x95\xb0\xe5\xb9\x85\xe5\xba\xa6\xe8\xb0\xb1\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe4\xbb\xa5\xe5\x8f\x8a\xe5\xa4\x84\xe7\x90\x86\xe6\x96\x87\xe6\x9c\xac\xe6\xa0\x87\xe7\xad\xbe\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\n#\xe8\xaf\xad\xe9\x9f\xb3\xe7\x9a\x84\xe5\xaf\xb9\xe6\x95\xb0\xe9\xa2\x91\xe8\xb0\xb1\xe4\xbd\x9c\xe4\xb8\xba\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\n\nimport torch\nimport librosa\nimport torchaudio\n\ndef load_audio(path):\n    """"""\xe4\xbd\xbf\xe7\x94\xa8torchaudio\xe8\xaf\xbb\xe5\x8f\x96\xe9\x9f\xb3\xe9\xa2\x91\n    Args:\n        path(string)            : \xe9\x9f\xb3\xe9\xa2\x91\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n    Returns:\n        sound(numpy.ndarray)    : \xe5\x8d\x95\xe5\xa3\xb0\xe9\x81\x93\xe9\x9f\xb3\xe9\xa2\x91\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe5\xa4\x9a\xe5\xa3\xb0\xe9\x81\x93\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xb9\xb3\xe5\x9d\x87(Samples * 1 channel)\n    """"""\n    sound, _ = torchaudio.load(path)\n    sound = sound.numpy()\n    if len(sound.shape) > 1:\n        if sound.shape[1] == 1:\n            sound = sound.squeeze()\n        else:\n            sound = sound.mean(axis = 1)\n    return sound\n\ndef parse_audio(path, audio_conf, windows, normalize=False):\n    """"""\xe4\xbd\xbf\xe7\x94\xa8librosa\xe8\xae\xa1\xe7\xae\x97\xe9\x9f\xb3\xe9\xa2\x91\xe7\x9a\x84\xe5\xaf\xb9\xe6\x95\xb0\xe5\xb9\x85\xe5\xba\xa6\xe8\xb0\xb1\n    Args:\n        path(string)       : \xe9\x9f\xb3\xe9\xa2\x91\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n        audio_conf(dict)   : \xe6\xb1\x82\xe9\xa2\x91\xe8\xb0\xb1\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n        windows(dict)      : \xe5\x8a\xa0\xe7\xaa\x97\xe7\xb1\xbb\xe5\x9e\x8b\n    Returns:\n        spect(FloatTensor) : \xe9\x9f\xb3\xe9\xa2\x91\xe7\x9a\x84\xe5\xaf\xb9\xe6\x95\xb0\xe5\xb9\x85\xe5\xba\xa6\xe8\xb0\xb1(numFrames * nFeatures)\n                             nFeatures = n_fft / 2 + 1\n    """"""\n    y = load_audio(path)\n    n_fft = int(audio_conf[\'sample_rate\']*audio_conf[""window_size""])\n    win_length = n_fft\n    hop_length = int(audio_conf[\'sample_rate\']*audio_conf[\'window_stride\'])\n    window = windows[audio_conf[\'window\']]\n    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n                        win_length=win_length, window=window)\n    spect, phase = librosa.magphase(D) \n    spect = torch.FloatTensor(spect)\n    spect = spect.log1p()\n    \n    #\xe6\xaf\x8f\xe5\x8f\xa5\xe8\xaf\x9d\xe8\x87\xaa\xe5\xb7\xb1\xe5\x81\x9a\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\n    if normalize:\n        mean = spect.mean()\n        std = spect.std()\n        spect.add_(-mean)\n        spect.div_(std)  \n    return spect.transpose(0,1)\n\ndef process_label_file(label_file, char2int):\n    """"""\xe5\xb0\x86\xe6\x96\x87\xe6\x9c\xac\xe6\xa0\x87\xe7\xad\xbe\xe5\xa4\x84\xe7\x90\x86\xe4\xb8\xba\xe6\x95\xb0\xe5\xad\x97\xef\xbc\x8c\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbanumpy\xe7\xb1\xbb\xe5\x9e\x8b\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe5\xad\x98\xe5\x82\xa8\xe4\xb8\xbah5py\xe6\x96\x87\xe4\xbb\xb6\n    Args:\n        label_file(string)  :  \xe6\xa0\x87\xe7\xad\xbe\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n        char2int(dict)      :  \xe6\xa0\x87\xe7\xad\xbe\xe5\x88\xb0\xe6\x95\xb0\xe5\xad\x97\xe7\x9a\x84\xe6\x98\xa0\xe5\xb0\x84\xe5\x85\xb3\xe7\xb3\xbb ""_\'abcdefghijklmnopqrstuvwxyz""\n    Output:\n        label_dict(list)    :  \xe6\x89\x80\xe6\x9c\x89\xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe6\x98\xaflist\xe7\xb1\xbb\xe5\x9e\x8b\n    """"""\n    label_all = []\n    with open(label_file, \'r\') as f:\n        for label in f.readlines():\n            label = label.strip()\n            label_list = []\n            utt = label.split(\'\\t\', 1)[0]\n            label = label.split(\'\\t\', 1)[1]\n            for i in range(len(label)):\n                if label[i].lower() in char2int:\n                    label_list.append(char2int[label[i].lower()])\n                else:\n                    print(""%s not in the label map list"" % label[i].lower())\n            label_all.append(label_list)\n    return label_all\n\n'"
chapter06-best_practice/data/__init__.py,0,b''
chapter06-best_practice/data/dataset.py,1,"b'# coding:utf8\nimport os\nfrom PIL import Image\nfrom torch.utils import data\nimport numpy as np\nfrom torchvision import transforms as T\n\n\nclass DogCat(data.Dataset):\n\n    def __init__(self, root, transforms=None, train=True, test=False):\n        """"""\n        \xe4\xb8\xbb\xe8\xa6\x81\xe7\x9b\xae\xe6\xa0\x87\xef\xbc\x9a \xe8\x8e\xb7\xe5\x8f\x96\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x9c\xb0\xe5\x9d\x80\xef\xbc\x8c\xe5\xb9\xb6\xe6\xa0\xb9\xe6\x8d\xae\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe5\x88\x92\xe5\x88\x86\xe6\x95\xb0\xe6\x8d\xae\n        """"""\n        self.test = test\n        imgs = [os.path.join(root, img) for img in os.listdir(root)]\n\n        # test1: data/test1/8973.jpg\n        # train: data/train/cat.10004.jpg \n        if self.test:\n            imgs = sorted(imgs, key=lambda x: int(x.split(\'.\')[-2].split(\'/\')[-1]))\n        else:\n            imgs = sorted(imgs, key=lambda x: int(x.split(\'.\')[-2]))\n\n        imgs_num = len(imgs)\n\n        if self.test:\n            self.imgs = imgs\n        elif train:\n            self.imgs = imgs[:int(0.7 * imgs_num)]\n        else:\n            self.imgs = imgs[int(0.7 * imgs_num):]\n\n        if transforms is None:\n            normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n                                    std=[0.229, 0.224, 0.225])\n\n            if self.test or not train:\n                self.transforms = T.Compose([\n                    T.Resize(224),\n                    T.CenterCrop(224),\n                    T.ToTensor(),\n                    normalize\n                ])\n            else:\n                self.transforms = T.Compose([\n                    T.Resize(256),\n                    T.RandomReSizedCrop(224),\n                    T.RandomHorizontalFlip(),\n                    T.ToTensor(),\n                    normalize\n                ])\n\n    def __getitem__(self, index):\n        """"""\n        \xe4\xb8\x80\xe6\xac\xa1\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n        """"""\n        img_path = self.imgs[index]\n        if self.test:\n            label = int(self.imgs[index].split(\'.\')[-2].split(\'/\')[-1])\n        else:\n            label = 1 if \'dog\' in img_path.split(\'/\')[-1] else 0\n        data = Image.open(img_path)\n        data = self.transforms(data)\n        return data, label\n\n    def __len__(self):\n        return len(self.imgs)\n'"
chapter06-best_practice/models/__init__.py,0,b'from .alexnet import AlexNet\nfrom .resnet34 import ResNet34\nfrom .squeezenet import SqueezeNet\n# from torchvision.models import InceptinV3\n# from torchvision.models import alexnet as AlexNet\n'
chapter06-best_practice/models/alexnet.py,0,"b'# coding:utf8\nfrom torch import nn\nfrom .basic_module import BasicModule\n\n\nclass AlexNet(BasicModule):\n    """"""\n    code from torchvision/models/alexnet.py\n    \xe7\xbb\x93\xe6\x9e\x84\xe5\x8f\x82\xe8\x80\x83 <https://arxiv.org/abs/1404.5997>\n    """"""\n\n    def __init__(self, num_classes=2):\n        super(AlexNet, self).__init__()\n\n        self.model_name = \'alexnet\'\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), 256 * 6 * 6)\n        x = self.classifier(x)\n        return x\n'"
chapter06-best_practice/models/basic_module.py,0,"b'#coding:utf8\nimport torch as t\nimport time\n\n\nclass BasicModule(t.nn.Module):\n    """"""\n    \xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86nn.Module,\xe4\xb8\xbb\xe8\xa6\x81\xe6\x98\xaf\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86save\xe5\x92\x8cload\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x96\xb9\xe6\xb3\x95\n    """"""\n\n    def __init__(self):\n        super(BasicModule,self).__init__()\n        self.model_name=str(type(self))# \xe9\xbb\x98\xe8\xae\xa4\xe5\x90\x8d\xe5\xad\x97\n\n    def load(self, path):\n        """"""\n        \xe5\x8f\xaf\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x8c\x87\xe5\xae\x9a\xe8\xb7\xaf\xe5\xbe\x84\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        """"""\n        self.load_state_dict(t.load(path))\n\n    def save(self, name=None):\n        """"""\n        \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa4\xe4\xbd\xbf\xe7\x94\xa8\xe2\x80\x9c\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x90\x8d\xe5\xad\x97+\xe6\x97\xb6\xe9\x97\xb4\xe2\x80\x9d\xe4\xbd\x9c\xe4\xb8\xba\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\n        """"""\n        if name is None:\n            prefix = \'checkpoints/\' + self.model_name + \'_\'\n            name = time.strftime(prefix + \'%m%d_%H:%M:%S.pth\')\n        t.save(self.state_dict(), name)\n        return name\n\n    def get_optimizer(self, lr, weight_decay):\n        return t.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n\n\nclass Flat(t.nn.Module):\n    """"""\n    \xe6\x8a\x8a\xe8\xbe\x93\xe5\x85\xa5reshape\xe6\x88\x90\xef\xbc\x88batch_size,dim_length\xef\xbc\x89\n    """"""\n\n    def __init__(self):\n        super(Flat, self).__init__()\n        #self.size = size\n\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n'"
chapter06-best_practice/models/resnet34.py,1,"b'# coding:utf8\nfrom .basic_module import BasicModule\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass ResidualBlock(nn.Module):\n    """"""\n    \xe5\xae\x9e\xe7\x8e\xb0\xe5\xad\x90module: Residual Block\n    """"""\n\n    def __init__(self, inchannel, outchannel, stride=1, shortcut=None):\n        super(ResidualBlock, self).__init__()\n        self.left = nn.Sequential(\n            nn.Conv2d(inchannel, outchannel, 3, stride, 1, bias=False),\n            nn.BatchNorm2d(outchannel),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(outchannel, outchannel, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(outchannel))\n        self.right = shortcut\n\n    def forward(self, x):\n        out = self.left(x)\n        residual = x if self.right is None else self.right(x)\n        out += residual\n        return F.relu(out)\n\n\nclass ResNet34(BasicModule):\n    """"""\n    \xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\xbbmodule\xef\xbc\x9aResNet34\n    ResNet34\xe5\x8c\x85\xe5\x90\xab\xe5\xa4\x9a\xe4\xb8\xaalayer\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaalayer\xe5\x8f\x88\xe5\x8c\x85\xe5\x90\xab\xe5\xa4\x9a\xe4\xb8\xaaResidual block\n    \xe7\x94\xa8\xe5\xad\x90module\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0Residual block\xef\xbc\x8c\xe7\x94\xa8_make_layer\xe5\x87\xbd\xe6\x95\xb0\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0layer\n    """"""\n\n    def __init__(self, num_classes=2):\n        super(ResNet34, self).__init__()\n        self.model_name = \'resnet34\'\n\n        # \xe5\x89\x8d\xe5\x87\xa0\xe5\xb1\x82: \xe5\x9b\xbe\xe5\x83\x8f\xe8\xbd\xac\xe6\x8d\xa2\n        self.pre = nn.Sequential(\n            nn.Conv2d(3, 64, 7, 2, 3, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(3, 2, 1))\n\n        # \xe9\x87\x8d\xe5\xa4\x8d\xe7\x9a\x84layer\xef\xbc\x8c\xe5\x88\x86\xe5\x88\xab\xe6\x9c\x893\xef\xbc\x8c4\xef\xbc\x8c6\xef\xbc\x8c3\xe4\xb8\xaaresidual block\n        self.layer1 = self._make_layer(64, 128, 3)\n        self.layer2 = self._make_layer(128, 256, 4, stride=2)\n        self.layer3 = self._make_layer(256, 512, 6, stride=2)\n        self.layer4 = self._make_layer(512, 512, 3, stride=2)\n\n        # \xe5\x88\x86\xe7\xb1\xbb\xe7\x94\xa8\xe7\x9a\x84\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\n        self.fc = nn.Linear(512, num_classes)\n\n    def _make_layer(self, inchannel, outchannel, block_num, stride=1):\n        """"""\n        \xe6\x9e\x84\xe5\xbb\xbalayer,\xe5\x8c\x85\xe5\x90\xab\xe5\xa4\x9a\xe4\xb8\xaaresidual block\n        """"""\n        shortcut = nn.Sequential(\n            nn.Conv2d(inchannel, outchannel, 1, stride, bias=False),\n            nn.BatchNorm2d(outchannel))\n\n        layers = []\n        layers.append(ResidualBlock(inchannel, outchannel, stride, shortcut))\n\n        for i in range(1, block_num):\n            layers.append(ResidualBlock(outchannel, outchannel))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.pre(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = F.avg_pool2d(x, 7)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n'"
chapter06-best_practice/models/squeezenet.py,1,"b""from torchvision.models import  squeezenet1_1\nfrom models.basic_module import  BasicModule\nfrom torch import nn\nfrom torch.optim import Adam\n\nclass SqueezeNet(BasicModule):\n    def __init__(self, num_classes=2):\n        super(SqueezeNet, self).__init__()\n        self.model_name = 'squeezenet'\n        self.model = squeezenet1_1(pretrained=True)\n        # \xe4\xbf\xae\xe6\x94\xb9 \xe5\x8e\x9f\xe5\xa7\x8b\xe7\x9a\x84num_class: \xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x98\xaf1000\xe5\x88\x86\xe7\xb1\xbb\n        self.model.num_classes = num_classes\n        self.model.classifier =   nn.Sequential(\n            nn.Dropout(p=0.5),\n            nn.Conv2d(512, num_classes, 1),\n            nn.ReLU(inplace=True),\n            nn.AvgPool2d(13, stride=1)\n        )\n\n    def forward(self,x):\n        return self.model(x)\n\n    def get_optimizer(self, lr, weight_decay):\n        # \xe5\x9b\xa0\xe4\xb8\xba\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe8\xae\xad\xe7\xbb\x83\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\n        # \xe5\x89\x8d\xe9\x9d\xa2\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\xe9\x83\xa8\xe5\x88\x86\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbf\x9d\xe6\x8c\x81\xe4\xb8\x8d\xe5\x8f\x98\n        return Adam(self.model.classifier.parameters(), lr, weight_decay=weight_decay) """
chapter06-best_practice/utils/__init__.py,0,b'from .visualize import Visualizer'
chapter06-best_practice/utils/visualize.py,0,"b'# coding:utf8\nimport visdom\nimport time\nimport numpy as np\n\n\nclass Visualizer(object):\n    """"""\n    \xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86visdom\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xbd\xa0\xe4\xbb\x8d\xe7\x84\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87`self.vis.function`\n    \xe8\xb0\x83\xe7\x94\xa8\xe5\x8e\x9f\xe7\x94\x9f\xe7\x9a\x84visdom\xe6\x8e\xa5\xe5\x8f\xa3\n    """"""\n\n    def __init__(self, env=\'default\', **kwargs):\n        self.vis = visdom.Visdom(env=env,use_incoming_socket=False, **kwargs)\n\n        # \xe7\x94\xbb\xe7\x9a\x84\xe7\xac\xac\xe5\x87\xa0\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\xa8\xaa\xe5\xba\xa7\xe6\xa0\x87\n        # \xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x88\xe2\x80\x99loss\',23\xef\xbc\x89 \xe5\x8d\xb3loss\xe7\x9a\x84\xe7\xac\xac23\xe4\xb8\xaa\xe7\x82\xb9\n        self.index = {}\n        self.log_text = \'\'\n\n    def reinit(self, env=\'default\', **kwargs):\n        """"""\n        \xe4\xbf\xae\xe6\x94\xb9visdom\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae\n        """"""\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        return self\n\n    def plot_many(self, d):\n        """"""\n        \xe4\xb8\x80\xe6\xac\xa1plot\xe5\xa4\x9a\xe4\xb8\xaa\n        @params d: dict (name,value) i.e. (\'loss\',0.11)\n        """"""\n        for k, v in d.items():\n            self.plot(k, v)\n\n    def img_many(self, d):\n        for k, v in d.items():\n            self.img(k, v)\n\n    def plot(self, name, y, **kwargs):\n        """"""\n        self.plot(\'loss\',1.00)\n        """"""\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=name,\n                      opts=dict(title=name),\n                      update=None if x == 0 else \'append\',\n                      **kwargs\n                      )\n        self.index[name] = x + 1\n\n    def img(self, name, img_, **kwargs):\n        """"""\n        self.img(\'input_img\',t.Tensor(64,64))\n        self.img(\'input_imgs\',t.Tensor(3,64,64))\n        self.img(\'input_imgs\',t.Tensor(100,1,64,64))\n        self.img(\'input_imgs\',t.Tensor(100,3,64,64),nrows=10)\n\n        \xef\xbc\x81\xef\xbc\x81\xef\xbc\x81don\xe2\x80\x98t ~~self.img(\'input_imgs\',t.Tensor(100,64,64),nrows=10)~~\xef\xbc\x81\xef\xbc\x81\xef\xbc\x81\n        """"""\n        self.vis.images(img_.cpu().numpy(),\n                        win=name,\n                        opts=dict(title=name),\n                        **kwargs\n                        )\n\n    def log(self, info, win=\'log_text\'):\n        """"""\n        self.log({\'loss\':1,\'lr\':0.0001})\n        """"""\n\n        self.log_text += (\'[{time}] {info} <br>\'.format(\n            time=time.strftime(\'%m%d_%H%M%S\'),\n            info=info))\n        self.vis.text(self.log_text, win)\n\n    def __getattr__(self, name):\n        return getattr(self.vis, name)\n'"
chapter10-image_caption/utils/__init__.py,0,b'from .beam_search import CaptionGenerator\nfrom .visualize import Visualizer\n'
chapter10-image_caption/utils/beam_search.py,6,"b'#coding:utf8\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Class for generating captions from an image-to-text model.\nAdapted from https://github.com/tensorflow/models/blob/master/im2txt/im2txt/inference_utils/caption_generator.py""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch.nn.functional import log_softmax\nimport heapq\n\n\nclass Caption(object):\n    """"""Represents a complete or partial caption.""""""\n\n    def __init__(self, sentence, state, logprob, score, metadata=None):\n        """"""Initializes the Caption.\n\n        Args:\n          sentence: List of word ids in the caption.\n          state: Model state after generating the previous word.\n          logprob: Log-probability of the caption.\n          score: Score of the caption.\n          metadata: Optional metadata associated with the partial sentence. If not\n            None, a list of strings with the same length as \'sentence\'.\n        """"""\n        self.sentence = sentence\n        self.state = state\n        self.logprob = logprob\n        self.score = score\n        self.metadata = metadata\n\n    def __cmp__(self, other):\n        """"""Compares Captions by score.""""""\n        assert isinstance(other, Caption)\n        if self.score == other.score:\n            return 0\n        elif self.score < other.score:\n            return -1\n        else:\n            return 1\n\n    # For Python 3 compatibility (__cmp__ is deprecated).\n    def __lt__(self, other):\n        assert isinstance(other, Caption)\n        return self.score < other.score\n\n    # Also for Python 3 compatibility.\n    def __eq__(self, other):\n        assert isinstance(other, Caption)\n        return self.score == other.score\n\n\nclass TopN(object):\n    """"""Maintains the top n elements of an incrementally provided set.""""""\n\n    def __init__(self, n):\n        self._n = n\n        self._data = []\n\n    def size(self):\n        assert self._data is not None\n        return len(self._data)\n\n    def push(self, x):\n        """"""Pushes a new element.""""""\n        assert self._data is not None\n        if len(self._data) < self._n:\n            heapq.heappush(self._data, x)\n        else:\n            heapq.heappushpop(self._data, x)\n\n    def extract(self, sort=False):\n        """"""Extracts all elements from the TopN. This is a destructive operation.\n\n        The only method that can be called immediately after extract() is reset().\n\n        Args:\n          sort: Whether to return the elements in descending sorted order.\n\n        Returns:\n          A list of data; the top n elements provided to the set.\n        """"""\n        assert self._data is not None\n        data = self._data\n        self._data = None\n        if sort:\n            data.sort(reverse=True)\n        return data\n\n    def reset(self):\n        """"""Returns the TopN to an empty state.""""""\n        self._data = []\n\n\nclass CaptionGenerator(object):\n    """"""Class to generate captions from an image-to-text model.""""""\n\n    def __init__(self,\n                 embedder,\n                 rnn,\n                 classifier,\n                 eos_id,\n                 beam_size=3,\n                 max_caption_length=20,\n                 length_normalization_factor=0.0):\n        """"""Initializes the generator.\n\n        Args:\n          model: recurrent model, with inputs: (input, state) and outputs len(vocab) values\n          beam_size: Beam size to use when generating captions.\n          max_caption_length: The maximum caption length before stopping the search.\n          length_normalization_factor: If != 0, a number x such that captions are\n            scored by logprob/length^x, rather than logprob. This changes the\n            relative scores of captions depending on their lengths. For example, if\n            x > 0 then longer captions will be favored.\n        """"""\n        self.embedder = embedder\n        self.rnn = rnn\n        self.classifier = classifier\n        self.eos_id = eos_id\n        self.beam_size = beam_size\n        self.max_caption_length = max_caption_length\n        self.length_normalization_factor = length_normalization_factor\n\n    def beam_search(self, rnn_input, initial_state=None):\n        """"""Runs beam search caption generation on a single image.\n\n        Args:\n          initial_state: An initial state for the recurrent model\n\n        Returns:\n          A list of Caption sorted by descending score.\n        """"""\n\n        def get_topk_words(embeddings, state):\n            output, new_states = self.rnn(embeddings, state)\n            output = self.classifier(output.squeeze(0))\n            logprobs = log_softmax(output, dim=1)\n            logprobs, words = logprobs.topk(self.beam_size, 1)\n            return words.data, logprobs.data, new_states\n\n        partial_captions = TopN(self.beam_size)\n        complete_captions = TopN(self.beam_size)\n\n        words, logprobs, new_state = get_topk_words(rnn_input, initial_state)\n        for k in range(self.beam_size):\n            cap = Caption(\n                sentence=[words[0, k]],\n                state=new_state,\n                logprob=logprobs[0, k],\n                score=logprobs[0, k])\n            partial_captions.push(cap)\n\n        # Run beam search.\n        for _ in range(self.max_caption_length - 1):\n            partial_captions_list = partial_captions.extract()\n            partial_captions.reset()\n            input_feed = torch.LongTensor([c.sentence[-1]\n                                           for c in partial_captions_list])\n            if rnn_input.is_cuda:\n                input_feed = input_feed.cuda()\n            input_feed = Variable(input_feed, volatile=True)\n            state_feed = [c.state for c in partial_captions_list]\n            if isinstance(state_feed[0], tuple):\n                state_feed_h, state_feed_c = zip(*state_feed)\n                state_feed = (torch.cat(state_feed_h, 1),\n                              torch.cat(state_feed_c, 1))\n            else:\n                state_feed = torch.cat(state_feed, 1)\n\n            embeddings = self.embedder(input_feed).view(1, len(input_feed), -1)\n            words, logprobs, new_states = get_topk_words(\n                embeddings, state_feed)\n            for i, partial_caption in enumerate(partial_captions_list):\n                if isinstance(new_states, tuple):\n                    state = (new_states[0].narrow(1, i, 1),\n                             new_states[1].narrow(1, i, 1))\n                else:\n                    state = new_states[i]\n                for k in range(self.beam_size):\n                    w = words[i, k]\n                    sentence = partial_caption.sentence + [w]\n                    logprob = partial_caption.logprob + logprobs[i, k]\n                    score = logprob\n                    if w == self.eos_id:\n                        if self.length_normalization_factor > 0:\n                            score /= len(sentence)**self.length_normalization_factor\n                        beam = Caption(sentence, state, logprob, score)\n                        complete_captions.push(beam)\n                    else:\n                        beam = Caption(sentence, state, logprob, score)\n                        partial_captions.push(beam)\n            if partial_captions.size() == 0:\n                # We have run out of partial candidates; happens when beam_size\n                # = 1.\n                break\n\n        # If we have no complete captions then fall back to the partial captions.\n        # But never output a mixture of complete and partial captions because a\n        # partial caption could have a higher score than all the complete\n        # captions.\n        if not complete_captions.size():\n            complete_captions = partial_captions\n\n        caps = complete_captions.extract(sort=True)\n\n        return [c.sentence for c in caps], [c.score for c in caps]\n'"
chapter10-image_caption/utils/visualize.py,0,"b'# coding:utf8\nimport visdom\nimport time\nimport torchvision as tv\nimport numpy as np\n\n\nclass Visualizer():\n    """"""\n    \xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86visdom\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xbd\xa0\xe4\xbb\x8d\xe7\x84\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87`self.vis.function`\n    \xe8\xb0\x83\xe7\x94\xa8\xe5\x8e\x9f\xe7\x94\x9f\xe7\x9a\x84visdom\xe6\x8e\xa5\xe5\x8f\xa3\n    """"""\n\n    def __init__(self, env=\'default\', **kwargs):\n        import visdom\n        self.vis = visdom.Visdom(env=env, **kwargs)\n\n        # \xe7\x94\xbb\xe7\x9a\x84\xe7\xac\xac\xe5\x87\xa0\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\xa8\xaa\xe5\xba\xa7\xe6\xa0\x87\n        # \xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x88\xe2\x80\x99loss\',23\xef\xbc\x89 \xe5\x8d\xb3loss\xe7\x9a\x84\xe7\xac\xac23\xe4\xb8\xaa\xe7\x82\xb9\n        self.index = {}\n        self.log_text = \'\'\n\n    def reinit(self, env=\'default\', **kwargs):\n        """"""\n        \xe4\xbf\xae\xe6\x94\xb9visdom\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae\n        """"""\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        return self\n\n    def plot_many(self, d):\n        """"""\n        \xe4\xb8\x80\xe6\xac\xa1plot\xe5\xa4\x9a\xe4\xb8\xaa\n        @params d: dict (name,value) i.e. (\'loss\',0.11)\n        """"""\n        for k, v in d.items():\n            self.plot(k, v)\n\n    def img_many(self, d):\n        for k, v in d.items():\n            self.img(k, v)\n\n    def plot(self, name, y):\n        """"""\n        self.plot(\'loss\',1.00)\n        """"""\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=name,\n                      opts=dict(title=name),\n                      update=None if x == 0 else \'append\'\n                      )\n        self.index[name] = x + 1\n\n    def img(self, name, img_, caption=None):\n        """"""\n        self.img(\'input_img\',t.Tensor(64,64))\n        """"""\n\n        if len(img_.size()) < 3:\n            img_ = img_.cpu().unsqueeze(0)\n        self.vis.image(img_.cpu(),\n                       win=name,\n                       opts=dict(title=name, caption=caption)\n                       )\n\n    def img_grid_many(self, d):\n        for k, v in d.items():\n            self.img_grid(k, v)\n\n    def img_grid(self, name, input_3d):\n        """"""\n        \xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe8\xbd\xac\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe5\x9b\xbe\xef\xbc\x8ci.e. input\xef\xbc\x8836\xef\xbc\x8c64\xef\xbc\x8c64\xef\xbc\x89\n        \xe4\xbc\x9a\xe5\x8f\x98\xe6\x88\x90 6*6 \xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe5\x9b\xbe\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xbc\xe5\xad\x90\xe5\xa4\xa7\xe5\xb0\x8f64*64\n        """"""\n        self.img(name, tv.utils.make_grid(\n            input_3d.cpu()[0].unsqueeze(1).clamp(max=1, min=0)))\n\n    def log(self, info, win=\'log_text\'):\n        """"""\n        self.log({\'loss\':1,\'lr\':0.0001})\n        """"""\n\n        self.log_text += (\'[{time}] {info} <br>\'.format(\n            time=time.strftime(\'%m%d_%H%M%S\'),\n            info=info))\n        self.vis.text(self.log_text, win=win)\n\n    def __getattr__(self, name):\n        return getattr(self.vis, name)\n'"
