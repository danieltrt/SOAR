file_path,api_count,code
main.py,4,"b'import argparse\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom model import GGNN\nfrom utils.train import train\nfrom utils.test import test\nfrom utils.data.dataset import bAbIDataset\nfrom utils.data.dataloader import bAbIDataloader\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--task_id\', type=int, default=4, help=\'bAbI task id\')\nparser.add_argument(\'--question_id\', type=int, default=0, help=\'question types\')\nparser.add_argument(\'--workers\', type=int, help=\'number of data loading workers\', default=2)\nparser.add_argument(\'--batchSize\', type=int, default=10, help=\'input batch size\')\nparser.add_argument(\'--state_dim\', type=int, default=4, help=\'GGNN hidden state size\')\nparser.add_argument(\'--n_steps\', type=int, default=5, help=\'propogation steps number of GGNN\')\nparser.add_argument(\'--niter\', type=int, default=10, help=\'number of epochs to train for\')\nparser.add_argument(\'--lr\', type=float, default=0.01, help=\'learning rate\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--verbal\', action=\'store_true\', help=\'print training info or not\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\n\nopt = parser.parse_args()\nprint(opt)\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\n\nopt.dataroot = \'babi_data/processed_1/train/%d_graphs.txt\' % opt.task_id\n\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ndef main(opt):\n    train_dataset = bAbIDataset(opt.dataroot, opt.question_id, True)\n    train_dataloader = bAbIDataloader(train_dataset, batch_size=opt.batchSize, \\\n                                      shuffle=True, num_workers=2)\n\n    test_dataset = bAbIDataset(opt.dataroot, opt.question_id, False)\n    test_dataloader = bAbIDataloader(test_dataset, batch_size=opt.batchSize, \\\n                                     shuffle=False, num_workers=2)\n\n    opt.annotation_dim = 1  # for bAbI\n    opt.n_edge_types = train_dataset.n_edge_types\n    opt.n_node = train_dataset.n_node\n\n    net = GGNN(opt)\n    net.double()\n    print(net)\n\n    criterion = nn.CrossEntropyLoss()\n\n    if opt.cuda:\n        net.cuda()\n        criterion.cuda()\n\n    optimizer = optim.Adam(net.parameters(), lr=opt.lr)\n\n    for epoch in range(0, opt.niter):\n        train(epoch, train_dataloader, net, criterion, optimizer, opt)\n        test(test_dataloader, net, criterion, optimizer, opt)\n\n\nif __name__ == ""__main__"":\n    main(opt)\n\n'"
model.py,9,"b'import torch\nimport torch.nn as nn\n\nclass AttrProxy(object):\n    """"""\n    Translates index lookups into attribute lookups.\n    To implement some trick which able to use list of nn.Module in a nn.Module\n    see https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219/2\n    """"""\n    def __init__(self, module, prefix):\n        self.module = module\n        self.prefix = prefix\n\n    def __getitem__(self, i):\n        return getattr(self.module, self.prefix + str(i))\n\n\nclass Propogator(nn.Module):\n    """"""\n    Gated Propogator for GGNN\n    Using LSTM gating mechanism\n    """"""\n    def __init__(self, state_dim, n_node, n_edge_types):\n        super(Propogator, self).__init__()\n\n        self.n_node = n_node\n        self.n_edge_types = n_edge_types\n\n        self.reset_gate = nn.Sequential(\n            nn.Linear(state_dim*3, state_dim),\n            nn.Sigmoid()\n        )\n        self.update_gate = nn.Sequential(\n            nn.Linear(state_dim*3, state_dim),\n            nn.Sigmoid()\n        )\n        self.tansform = nn.Sequential(\n            nn.Linear(state_dim*3, state_dim),\n            nn.Tanh()\n        )\n\n    def forward(self, state_in, state_out, state_cur, A):\n        A_in = A[:, :, :self.n_node*self.n_edge_types]\n        A_out = A[:, :, self.n_node*self.n_edge_types:]\n\n        a_in = torch.bmm(A_in, state_in)\n        a_out = torch.bmm(A_out, state_out)\n        a = torch.cat((a_in, a_out, state_cur), 2)\n\n        r = self.reset_gate(a)\n        z = self.update_gate(a)\n        joined_input = torch.cat((a_in, a_out, r * state_cur), 2)\n        h_hat = self.tansform(joined_input)\n\n        output = (1 - z) * state_cur + z * h_hat\n\n        return output\n\n\nclass GGNN(nn.Module):\n    """"""\n    Gated Graph Sequence Neural Networks (GGNN)\n    Mode: SelectNode\n    Implementation based on https://arxiv.org/abs/1511.05493\n    """"""\n    def __init__(self, opt):\n        super(GGNN, self).__init__()\n\n        assert (opt.state_dim >= opt.annotation_dim,  \\\n                \'state_dim must be no less than annotation_dim\')\n\n        self.state_dim = opt.state_dim\n        self.annotation_dim = opt.annotation_dim\n        self.n_edge_types = opt.n_edge_types\n        self.n_node = opt.n_node\n        self.n_steps = opt.n_steps\n\n        for i in range(self.n_edge_types):\n            # incoming and outgoing edge embedding\n            in_fc = nn.Linear(self.state_dim, self.state_dim)\n            out_fc = nn.Linear(self.state_dim, self.state_dim)\n            self.add_module(""in_{}"".format(i), in_fc)\n            self.add_module(""out_{}"".format(i), out_fc)\n\n        self.in_fcs = AttrProxy(self, ""in_"")\n        self.out_fcs = AttrProxy(self, ""out_"")\n\n        # Propogation Model\n        self.propogator = Propogator(self.state_dim, self.n_node, self.n_edge_types)\n\n        # Output Model\n        self.out = nn.Sequential(\n            nn.Linear(self.state_dim + self.annotation_dim, self.state_dim),\n            nn.Tanh(),\n            nn.Linear(self.state_dim, 1)\n        )\n\n        self._initialization()\n\n    def _initialization(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                m.weight.data.normal_(0.0, 0.02)\n                m.bias.data.fill_(0)\n\n    def forward(self, prop_state, annotation, A):\n        for i_step in range(self.n_steps):\n            in_states = []\n            out_states = []\n            for i in range(self.n_edge_types):\n                in_states.append(self.in_fcs[i](prop_state))\n                out_states.append(self.out_fcs[i](prop_state))\n            in_states = torch.stack(in_states).transpose(0, 1).contiguous()\n            in_states = in_states.view(-1, self.n_node*self.n_edge_types, self.state_dim)\n            out_states = torch.stack(out_states).transpose(0, 1).contiguous()\n            out_states = out_states.view(-1, self.n_node*self.n_edge_types, self.state_dim)\n\n            prop_state = self.propogator(in_states, out_states, prop_state, A)\n\n        join_state = torch.cat((prop_state, annotation), 2)\n\n        output = self.out(join_state)\n        output = output.sum(2)\n\n        return output\n'"
utils/test.py,3,"b""import torch\nfrom torch.autograd import Variable\n\ndef test(dataloader, net, criterion, optimizer, opt):\n    test_loss = 0\n    correct = 0\n    net.eval()\n    for i, (adj_matrix, annotation, target) in enumerate(dataloader, 0):\n        padding = torch.zeros(len(annotation), opt.n_node, opt.state_dim - opt.annotation_dim).double()\n        init_input = torch.cat((annotation, padding), 2)\n        if opt.cuda:\n            init_input = init_input.cuda()\n            adj_matrix = adj_matrix.cuda()\n            annotation = annotation.cuda()\n            target = target.cuda()\n\n        init_input = Variable(init_input)\n        adj_matrix = Variable(adj_matrix)\n        annotation = Variable(annotation)\n        target = Variable(target)\n\n        output = net(init_input, annotation, adj_matrix)\n\n        test_loss += criterion(output, target).data[0]\n        pred = output.data.max(1, keepdim=True)[1]\n\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(dataloader.dataset)\n    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n        test_loss, correct, len(dataloader.dataset),\n        100. * correct / len(dataloader.dataset)))\n"""
utils/train.py,3,"b""import torch\nfrom torch.autograd import Variable\n\ndef train(epoch, dataloader, net, criterion, optimizer, opt):\n    net.train()\n    for i, (adj_matrix, annotation, target) in enumerate(dataloader, 0):\n        net.zero_grad()\n\n        padding = torch.zeros(len(annotation), opt.n_node, opt.state_dim - opt.annotation_dim).double()\n        init_input = torch.cat((annotation, padding), 2)\n        if opt.cuda:\n            init_input = init_input.cuda()\n            adj_matrix = adj_matrix.cuda()\n            annotation = annotation.cuda()\n            target = target.cuda()\n\n        init_input = Variable(init_input)\n        adj_matrix = Variable(adj_matrix)\n        annotation = Variable(annotation)\n        target = Variable(target)\n\n        output = net(init_input, annotation, adj_matrix)\n\n        loss = criterion(output, target)\n\n        loss.backward()\n        optimizer.step()\n\n        if i % int(len(dataloader) / 10 + 1) == 0 and opt.verbal:\n            print('[%d/%d][%d/%d] Loss: %.4f' % (epoch, opt.niter, i, len(dataloader), loss.data[0]))\n"""
utils/data/dataloader.py,1,"b'from torch.utils.data import DataLoader\n\nclass bAbIDataloader(DataLoader):\n\n    def __init__(self, *args, **kwargs):\n        super(bAbIDataloader, self).__init__(*args, **kwargs)\n'"
