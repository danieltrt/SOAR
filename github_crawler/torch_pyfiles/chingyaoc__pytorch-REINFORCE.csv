file_path,api_count,code
main.py,6,"b'import argparse, math, os\nimport numpy as np\nimport gym\nfrom gym import wrappers\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.utils as utils\n\nfrom normalized_actions import NormalizedActions\n\nparser = argparse.ArgumentParser(description=\'PyTorch REINFORCE example\')\nparser.add_argument(\'--env_name\', type=str, default=\'CartPole-v0\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, metavar=\'G\',\n                    help=\'discount factor for reward (default: 0.99)\')\nparser.add_argument(\'--exploration_end\', type=int, default=100, metavar=\'N\',\n                    help=\'number of episodes with noise (default: 100)\')\nparser.add_argument(\'--seed\', type=int, default=123, metavar=\'N\',\n                    help=\'random seed (default: 123)\')\nparser.add_argument(\'--num_steps\', type=int, default=1000, metavar=\'N\',\n                    help=\'max episode length (default: 1000)\')\nparser.add_argument(\'--num_episodes\', type=int, default=2000, metavar=\'N\',\n                    help=\'number of episodes (default: 2000)\')\nparser.add_argument(\'--hidden_size\', type=int, default=128, metavar=\'N\',\n                    help=\'number of episodes (default: 128)\')\nparser.add_argument(\'--render\', action=\'store_true\',\n                    help=\'render the environment\')\nparser.add_argument(\'--ckpt_freq\', type=int, default=100, \n\t\t    help=\'model saving frequency\')\nparser.add_argument(\'--display\', type=bool, default=False,\n                    help=\'display or not\')\nargs = parser.parse_args()\n\nenv_name = args.env_name\nenv = gym.make(env_name)\nif type(env.action_space) != gym.spaces.discrete.Discrete:\n    from reinforce_continuous import REINFORCE\n    env = NormalizedActions(gym.make(env_name))\nelse:\n    from reinforce_discrete import REINFORCE\n\nif args.display:\n    env = wrappers.Monitor(env, \'/tmp/{}-experiment\'.format(env_name), force=True)\n\nenv.seed(args.seed)\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\n\nagent = REINFORCE(args.hidden_size, env.observation_space.shape[0], env.action_space)\n\ndir = \'ckpt_\' + env_name\nif not os.path.exists(dir):    \n    os.mkdir(dir)\n\nfor i_episode in range(args.num_episodes):\n    state = torch.Tensor([env.reset()])\n    entropies = []\n    log_probs = []\n    rewards = []\n    for t in range(args.num_steps):\n        action, log_prob, entropy = agent.select_action(state)\n        action = action.cpu()\n\n        next_state, reward, done, _ = env.step(action.numpy()[0])\n\n        entropies.append(entropy)\n        log_probs.append(log_prob)\n        rewards.append(reward)\n        state = torch.Tensor([next_state])\n\n        if done:\n            break\n\n    agent.update_parameters(rewards, log_probs, entropies, args.gamma)\n\n\n    if i_episode%args.ckpt_freq == 0:\n\ttorch.save(agent.model.state_dict(), os.path.join(dir, \'reinforce-\'+str(i_episode)+\'.pkl\'))\n\n    print(""Episode: {}, reward: {}"".format(i_episode, np.sum(rewards)))\n\t\nenv.close()\n'"
normalized_actions.py,0,"b'import gym\n\n\nclass NormalizedActions(gym.ActionWrapper):\n\n    def _action(self, action):\n        action = (action + 1) / 2  # [-1, 1] => [0, 1]\n        action *= (self.action_space.high - self.action_space.low)\n        action += self.action_space.low\n        return action\n\n    def _reverse_action(self, action):\n        action -= self.action_space.low\n        action /= (self.action_space.high - self.action_space.low)\n        action = action * 2 - 1\n        return actions\n'"
reinforce_continuous.py,9,"b'import sys\nimport math\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.nn.utils as utils\nimport torchvision.transforms as T\nfrom torch.autograd import Variable\n\npi = Variable(torch.FloatTensor([math.pi])).cuda()\n\ndef normal(x, mu, sigma_sq):\n    a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()\n    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n    return a*b\n\n\nclass Policy(nn.Module):\n    def __init__(self, hidden_size, num_inputs, action_space):\n        super(Policy, self).__init__()\n        self.action_space = action_space\n        num_outputs = action_space.shape[0]\n\n        self.linear1 = nn.Linear(num_inputs, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, num_outputs)\n        self.linear2_ = nn.Linear(hidden_size, num_outputs)\n\n    def forward(self, inputs):\n        x = inputs\n        x = F.relu(self.linear1(x))\n        mu = self.linear2(x)\n        sigma_sq = self.linear2_(x)\n\n        return mu, sigma_sq\n\n\nclass REINFORCE:\n    def __init__(self, hidden_size, num_inputs, action_space):\n        self.action_space = action_space\n        self.model = Policy(hidden_size, num_inputs, action_space)\n\tself.model = self.model.cuda()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n\tself.model.train()\n\n    def select_action(self, state):\n        mu, sigma_sq = self.model(Variable(state).cuda())\n        sigma_sq = F.softplus(sigma_sq)\n\n        eps = torch.randn(mu.size())\n        # calculate the probability\n        action = (mu + sigma_sq.sqrt()*Variable(eps).cuda()).data\n        prob = normal(action, mu, sigma_sq)\n        entropy = -0.5*((sigma_sq+2*pi.expand_as(sigma_sq)).log()+1)\n\n        log_prob = prob.log()\n        return action, log_prob, entropy\n\n    def update_parameters(self, rewards, log_probs, entropies, gamma):\n        R = torch.zeros(1, 1)\n        loss = 0\n        for i in reversed(range(len(rewards))):\n            R = gamma * R + rewards[i]\n            loss = loss - (log_probs[i]*(Variable(R).expand_as(log_probs[i])).cuda()).sum() - (0.0001*entropies[i].cuda()).sum()\n        loss = loss / len(rewards)\n\t\t\n        self.optimizer.zero_grad()\n        loss.backward()\n\tutils.clip_grad_norm(self.model.parameters(), 40)\n        self.optimizer.step()\n'"
reinforce_discrete.py,7,"b'import sys\nimport math\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.nn.utils as utils\nimport torchvision.transforms as T\nfrom torch.autograd import Variable\nimport pdb\n\nclass Policy(nn.Module):\n    def __init__(self, hidden_size, num_inputs, action_space):\n        super(Policy, self).__init__()\n        self.action_space = action_space\n        num_outputs = action_space.n\n\n        self.linear1 = nn.Linear(num_inputs, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, num_outputs)\n\n    def forward(self, inputs):\n        x = inputs\n        x = F.relu(self.linear1(x))\n        action_scores = self.linear2(x)\n        return F.softmax(action_scores)\n\n\nclass REINFORCE:\n    def __init__(self, hidden_size, num_inputs, action_space):\n        self.action_space = action_space\n        self.model = Policy(hidden_size, num_inputs, action_space)\n\tself.model = self.model.cuda()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n\tself.model.train()\n\n    def select_action(self, state):\n        probs = self.model(Variable(state).cuda())       \n        action = probs.multinomial().data\n        prob = probs[:, action[0,0]].view(1, -1)\n        log_prob = prob.log()\n        entropy = - (probs*probs.log()).sum()\n\n        return action[0], log_prob, entropy\n\n    def update_parameters(self, rewards, log_probs, entropies, gamma):\n        R = torch.zeros(1, 1)\n        loss = 0\n        for i in reversed(range(len(rewards))):\n            R = gamma * R + rewards[i]\n            loss = loss - (log_probs[i]*(Variable(R).expand_as(log_probs[i])).cuda()).sum() - (0.0001*entropies[i].cuda()).sum()\n        loss = loss / len(rewards)\n\t\t\n        self.optimizer.zero_grad()\n        loss.backward()\n\tutils.clip_grad_norm(self.model.parameters(), 40)\n        self.optimizer.step()\n'"
