file_path,api_count,code
config.py,0,"b'import argparse\n\narg_lists = []\nparser = argparse.ArgumentParser()\n\n\ndef add_argument_group(name):\n  arg = parser.add_argument_group(name)\n  arg_lists.append(arg)\n  return arg\n\n\ndef str2bool(v):\n  return v.lower() in (\'true\', \'1\')\n\n\nlogging_arg = add_argument_group(\'Logging\')\nlogging_arg.add_argument(\'--out_dir\', type=str, default=\'outputs\')\n\ntrainer_arg = add_argument_group(\'Trainer\')\ntrainer_arg.add_argument(\'--trainer\', type=str, default=\'HardestContrastiveLossTrainer\')\ntrainer_arg.add_argument(\'--save_freq_epoch\', type=int, default=1)\ntrainer_arg.add_argument(\'--batch_size\', type=int, default=4)\ntrainer_arg.add_argument(\'--val_batch_size\', type=int, default=1)\n\n# Hard negative mining\ntrainer_arg.add_argument(\'--use_hard_negative\', type=str2bool, default=True)\ntrainer_arg.add_argument(\'--hard_negative_sample_ratio\', type=int, default=0.05)\ntrainer_arg.add_argument(\'--hard_negative_max_num\', type=int, default=3000)\ntrainer_arg.add_argument(\'--num_pos_per_batch\', type=int, default=1024)\ntrainer_arg.add_argument(\'--num_hn_samples_per_batch\', type=int, default=256)\n\n# Metric learning loss\ntrainer_arg.add_argument(\'--neg_thresh\', type=float, default=1.4)\ntrainer_arg.add_argument(\'--pos_thresh\', type=float, default=0.1)\ntrainer_arg.add_argument(\'--neg_weight\', type=float, default=1)\n\n# Data augmentation\ntrainer_arg.add_argument(\'--use_random_scale\', type=str2bool, default=False)\ntrainer_arg.add_argument(\'--min_scale\', type=float, default=0.8)\ntrainer_arg.add_argument(\'--max_scale\', type=float, default=1.2)\ntrainer_arg.add_argument(\'--use_random_rotation\', type=str2bool, default=True)\ntrainer_arg.add_argument(\'--rotation_range\', type=float, default=360)\n\n# Data loader configs\ntrainer_arg.add_argument(\'--train_phase\', type=str, default=""train"")\ntrainer_arg.add_argument(\'--val_phase\', type=str, default=""val"")\ntrainer_arg.add_argument(\'--test_phase\', type=str, default=""test"")\n\ntrainer_arg.add_argument(\'--stat_freq\', type=int, default=40)\ntrainer_arg.add_argument(\'--test_valid\', type=str2bool, default=True)\ntrainer_arg.add_argument(\'--val_max_iter\', type=int, default=400)\ntrainer_arg.add_argument(\'--val_epoch_freq\', type=int, default=1)\ntrainer_arg.add_argument(\n    \'--positive_pair_search_voxel_size_multiplier\', type=float, default=1.5)\n\ntrainer_arg.add_argument(\'--hit_ratio_thresh\', type=float, default=0.1)\n\n# Triplets\ntrainer_arg.add_argument(\'--triplet_num_pos\', type=int, default=256)\ntrainer_arg.add_argument(\'--triplet_num_hn\', type=int, default=512)\ntrainer_arg.add_argument(\'--triplet_num_rand\', type=int, default=1024)\n\n# dNetwork specific configurations\nnet_arg = add_argument_group(\'Network\')\nnet_arg.add_argument(\'--model\', type=str, default=\'ResUNetBN2C\')\nnet_arg.add_argument(\'--model_n_out\', type=int, default=32, help=\'Feature dimension\')\nnet_arg.add_argument(\'--conv1_kernel_size\', type=int, default=5)\nnet_arg.add_argument(\'--normalize_feature\', type=str2bool, default=True)\nnet_arg.add_argument(\'--dist_type\', type=str, default=\'L2\')\nnet_arg.add_argument(\'--best_val_metric\', type=str, default=\'feat_match_ratio\')\n\n# Optimizer arguments\nopt_arg = add_argument_group(\'Optimizer\')\nopt_arg.add_argument(\'--optimizer\', type=str, default=\'SGD\')\nopt_arg.add_argument(\'--max_epoch\', type=int, default=100)\nopt_arg.add_argument(\'--lr\', type=float, default=1e-1)\nopt_arg.add_argument(\'--momentum\', type=float, default=0.8)\nopt_arg.add_argument(\'--sgd_momentum\', type=float, default=0.9)\nopt_arg.add_argument(\'--sgd_dampening\', type=float, default=0.1)\nopt_arg.add_argument(\'--adam_beta1\', type=float, default=0.9)\nopt_arg.add_argument(\'--adam_beta2\', type=float, default=0.999)\nopt_arg.add_argument(\'--weight_decay\', type=float, default=1e-4)\nopt_arg.add_argument(\'--iter_size\', type=int, default=1, help=\'accumulate gradient\')\nopt_arg.add_argument(\'--bn_momentum\', type=float, default=0.05)\nopt_arg.add_argument(\'--exp_gamma\', type=float, default=0.99)\nopt_arg.add_argument(\'--scheduler\', type=str, default=\'ExpLR\')\nopt_arg.add_argument(\n    \'--icp_cache_path\', type=str, default=""/home/chrischoy/datasets/FCGF/kitti/icp/"")\n\nmisc_arg = add_argument_group(\'Misc\')\nmisc_arg.add_argument(\'--use_gpu\', type=str2bool, default=True)\nmisc_arg.add_argument(\'--weights\', type=str, default=None)\nmisc_arg.add_argument(\'--weights_dir\', type=str, default=None)\nmisc_arg.add_argument(\'--resume\', type=str, default=None)\nmisc_arg.add_argument(\'--resume_dir\', type=str, default=None)\nmisc_arg.add_argument(\'--train_num_thread\', type=int, default=2)\nmisc_arg.add_argument(\'--val_num_thread\', type=int, default=1)\nmisc_arg.add_argument(\'--test_num_thread\', type=int, default=2)\nmisc_arg.add_argument(\'--fast_validation\', type=str2bool, default=False)\nmisc_arg.add_argument(\n    \'--nn_max_n\',\n    type=int,\n    default=500,\n    help=\'The maximum number of features to find nearest neighbors in batch\')\n\n# Dataset specific configurations\ndata_arg = add_argument_group(\'Data\')\ndata_arg.add_argument(\'--dataset\', type=str, default=\'ThreeDMatchPairDataset\')\ndata_arg.add_argument(\'--voxel_size\', type=float, default=0.025)\ndata_arg.add_argument(\n    \'--threed_match_dir\', type=str, default=""/home/chrischoy/datasets/FCGF/threedmatch"")\ndata_arg.add_argument(\n    \'--kitti_root\', type=str, default=""/home/chrischoy/datasets/FCGF/kitti/"")\ndata_arg.add_argument(\n    \'--kitti_max_time_diff\',\n    type=int,\n    default=3,\n    help=\'max time difference between pairs (non inclusive)\')\ndata_arg.add_argument(\'--kitti_date\', type=str, default=\'2011_09_26\')\n\n\ndef get_config():\n  args = parser.parse_args()\n  return args\n'"
train.py,2,"b'# -*- coding: future_fstrings -*-\nimport open3d as o3d  # prevent loading error\n\nimport sys\nimport json\nimport logging\nimport torch\nfrom easydict import EasyDict as edict\n\nfrom lib.data_loaders import make_data_loader\nfrom config import get_config\n\nfrom lib.trainer import ContrastiveLossTrainer, HardestContrastiveLossTrainer, \\\n    TripletLossTrainer, HardestTripletLossTrainer\n\nch = logging.StreamHandler(sys.stdout)\nlogging.getLogger().setLevel(logging.INFO)\nlogging.basicConfig(\n    format=\'%(asctime)s %(message)s\', datefmt=\'%m/%d %H:%M:%S\', handlers=[ch])\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\n\nlogging.basicConfig(level=logging.INFO, format="""")\n\n\ndef get_trainer(trainer):\n  if trainer == \'ContrastiveLossTrainer\':\n    return ContrastiveLossTrainer\n  elif trainer == \'HardestContrastiveLossTrainer\':\n    return HardestContrastiveLossTrainer\n  elif trainer == \'TripletLossTrainer\':\n    return TripletLossTrainer\n  elif trainer == \'HardestTripletLossTrainer\':\n    return HardestTripletLossTrainer\n  else:\n    raise ValueError(f\'Trainer {trainer} not found\')\n\n\ndef main(config, resume=False):\n  train_loader = make_data_loader(\n      config,\n      config.train_phase,\n      config.batch_size,\n      num_threads=config.train_num_thread)\n\n  if config.test_valid:\n    val_loader = make_data_loader(\n        config,\n        config.val_phase,\n        config.val_batch_size,\n        num_threads=config.val_num_thread)\n  else:\n    val_loader = None\n\n  Trainer = get_trainer(config.trainer)\n  trainer = Trainer(\n      config=config,\n      data_loader=train_loader,\n      val_data_loader=val_loader,\n  )\n\n  trainer.train()\n\n\nif __name__ == ""__main__"":\n  logger = logging.getLogger()\n  config = get_config()\n\n  dconfig = vars(config)\n  if config.resume_dir:\n    resume_config = json.load(open(config.resume_dir + \'/config.json\', \'r\'))\n    for k in dconfig:\n      if k not in [\'resume_dir\'] and k in resume_config:\n        dconfig[k] = resume_config[k]\n    dconfig[\'resume\'] = resume_config[\'out_dir\'] + \'/checkpoint.pth\'\n\n  logging.info(\'===> Configurations\')\n  for k in dconfig:\n    logging.info(\'    {}: {}\'.format(k, dconfig[k]))\n\n  # Convert to dict\n  config = edict(dconfig)\n  main(config)\n'"
lib/__init__.py,0,b''
lib/data_loaders.py,18,"b'# -*- coding: future_fstrings -*-\n#\n# Written by Chris Choy <chrischoy@ai.stanford.edu>\n# Distributed under MIT License\nimport logging\nimport random\nimport torch\nimport torch.utils.data\nimport numpy as np\nimport glob\nimport os\nfrom scipy.linalg import expm, norm\nimport pathlib\n\nfrom util.pointcloud import get_matching_indices, make_open3d_point_cloud\nimport lib.transforms as t\n\nimport MinkowskiEngine as ME\n\nimport open3d as o3d\n\nkitti_cache = {}\nkitti_icp_cache = {}\n\n\ndef collate_pair_fn(list_data):\n  xyz0, xyz1, coords0, coords1, feats0, feats1, matching_inds, trans = list(\n      zip(*list_data))\n  xyz_batch0, xyz_batch1 = [], []\n  matching_inds_batch, trans_batch, len_batch = [], [], []\n\n  batch_id = 0\n  curr_start_inds = np.zeros((1, 2))\n\n  def to_tensor(x):\n    if isinstance(x, torch.Tensor):\n      return x\n    elif isinstance(x, np.ndarray):\n      return torch.from_numpy(x)\n    else:\n      raise ValueError(f\'Can not convert to torch tensor, {x}\')\n\n  for batch_id, _ in enumerate(coords0):\n    N0 = coords0[batch_id].shape[0]\n    N1 = coords1[batch_id].shape[0]\n\n    xyz_batch0.append(to_tensor(xyz0[batch_id]))\n    xyz_batch1.append(to_tensor(xyz1[batch_id]))\n\n    trans_batch.append(to_tensor(trans[batch_id]))\n\n    matching_inds_batch.append(\n        torch.from_numpy(np.array(matching_inds[batch_id]) + curr_start_inds))\n    len_batch.append([N0, N1])\n\n    # Move the head\n    curr_start_inds[0, 0] += N0\n    curr_start_inds[0, 1] += N1\n\n  coords_batch0, feats_batch0 = ME.utils.sparse_collate(coords0, feats0)\n  coords_batch1, feats_batch1 = ME.utils.sparse_collate(coords1, feats1)\n\n  # Concatenate all lists\n  xyz_batch0 = torch.cat(xyz_batch0, 0).float()\n  xyz_batch1 = torch.cat(xyz_batch1, 0).float()\n  trans_batch = torch.cat(trans_batch, 0).float()\n  matching_inds_batch = torch.cat(matching_inds_batch, 0).int()\n\n  return {\n      \'pcd0\': xyz_batch0,\n      \'pcd1\': xyz_batch1,\n      \'sinput0_C\': coords_batch0,\n      \'sinput0_F\': feats_batch0.float(),\n      \'sinput1_C\': coords_batch1,\n      \'sinput1_F\': feats_batch1.float(),\n      \'correspondences\': matching_inds_batch,\n      \'T_gt\': trans_batch,\n      \'len_batch\': len_batch\n  }\n\n\n# Rotation matrix along axis with angle theta\ndef M(axis, theta):\n  return expm(np.cross(np.eye(3), axis / norm(axis) * theta))\n\n\ndef sample_random_trans(pcd, randg, rotation_range=360):\n  T = np.eye(4)\n  R = M(randg.rand(3) - 0.5, rotation_range * np.pi / 180.0 * (randg.rand(1) - 0.5))\n  T[:3, :3] = R\n  T[:3, 3] = R.dot(-np.mean(pcd, axis=0))\n  return T\n\n\nclass PairDataset(torch.utils.data.Dataset):\n  AUGMENT = None\n\n  def __init__(self,\n               phase,\n               transform=None,\n               random_rotation=True,\n               random_scale=True,\n               manual_seed=False,\n               config=None):\n    self.phase = phase\n    self.files = []\n    self.data_objects = []\n    self.transform = transform\n    self.voxel_size = config.voxel_size\n    self.matching_search_voxel_size = \\\n        config.voxel_size * config.positive_pair_search_voxel_size_multiplier\n\n    self.random_scale = random_scale\n    self.min_scale = config.min_scale\n    self.max_scale = config.max_scale\n    self.random_rotation = random_rotation\n    self.rotation_range = config.rotation_range\n    self.randg = np.random.RandomState()\n    if manual_seed:\n      self.reset_seed()\n\n  def reset_seed(self, seed=0):\n    logging.info(f""Resetting the data loader seed to {seed}"")\n    self.randg.seed(seed)\n\n  def apply_transform(self, pts, trans):\n    R = trans[:3, :3]\n    T = trans[:3, 3]\n    pts = pts @ R.T + T\n    return pts\n\n  def __len__(self):\n    return len(self.files)\n\n\nclass ThreeDMatchTestDataset(PairDataset):\n  DATA_FILES = {\n      \'test\': \'./config/test_3dmatch.txt\'\n  }\n\n  def __init__(self,\n               phase,\n               transform=None,\n               random_rotation=True,\n               random_scale=True,\n               manual_seed=False,\n               scene_id=None,\n               config=None,\n               return_ply_names=False):\n\n    PairDataset.__init__(self, phase, transform, random_rotation, random_scale,\n                         manual_seed, config)\n    assert phase == \'test\', ""Supports only the test set.""\n\n    self.root = config.threed_match_dir\n\n    subset_names = open(self.DATA_FILES[phase]).read().split()\n    if scene_id is not None:\n      subset_names = [subset_names[scene_id]]\n    for sname in subset_names:\n      traj_file = os.path.join(self.root, sname + \'-evaluation/gt.log\')\n      assert os.path.exists(traj_file)\n      traj = read_trajectory(traj_file)\n      for ctraj in traj:\n        i = ctraj.metadata[0]\n        j = ctraj.metadata[1]\n        T_gt = ctraj.pose\n        self.files.append((sname, i, j, T_gt))\n\n    self.return_ply_names = return_ply_names\n\n  def __getitem__(self, pair_index):\n    sname, i, j, T_gt = self.files[pair_index]\n    ply_name0 = os.path.join(self.root, sname, f\'cloud_bin_{i}.ply\')\n    ply_name1 = os.path.join(self.root, sname, f\'cloud_bin_{j}.ply\')\n\n    if self.return_ply_names:\n      return sname, ply_name0, ply_name1, T_gt\n\n    pcd0 = o3d.io.read_point_cloud(ply_name0)\n    pcd1 = o3d.io.read_point_cloud(ply_name1)\n    pcd0 = np.asarray(pcd0.points)\n    pcd1 = np.asarray(pcd1.points)\n    return sname, pcd0, pcd1, T_gt\n\n\nclass IndoorPairDataset(PairDataset):\n  OVERLAP_RATIO = None\n  AUGMENT = None\n\n  def __init__(self,\n               phase,\n               transform=None,\n               random_rotation=True,\n               random_scale=True,\n               manual_seed=False,\n               config=None):\n    PairDataset.__init__(self, phase, transform, random_rotation, random_scale,\n                         manual_seed, config)\n    self.root = root = config.threed_match_dir\n    logging.info(f""Loading the subset {phase} from {root}"")\n\n    subset_names = open(self.DATA_FILES[phase]).read().split()\n    for name in subset_names:\n      fname = name + ""*%.2f.txt"" % self.OVERLAP_RATIO\n      fnames_txt = glob.glob(root + ""/"" + fname)\n      assert len(fnames_txt) > 0, f""Make sure that the path {root} has data {fname}""\n      for fname_txt in fnames_txt:\n        with open(fname_txt) as f:\n          content = f.readlines()\n        fnames = [x.strip().split() for x in content]\n        for fname in fnames:\n          self.files.append([fname[0], fname[1]])\n\n  def __getitem__(self, idx):\n    file0 = os.path.join(self.root, self.files[idx][0])\n    file1 = os.path.join(self.root, self.files[idx][1])\n    data0 = np.load(file0)\n    data1 = np.load(file1)\n    xyz0 = data0[""pcd""]\n    xyz1 = data1[""pcd""]\n    color0 = data0[""color""]\n    color1 = data1[""color""]\n    matching_search_voxel_size = self.matching_search_voxel_size\n\n    if self.random_scale and random.random() < 0.95:\n      scale = self.min_scale + \\\n          (self.max_scale - self.min_scale) * random.random()\n      matching_search_voxel_size *= scale\n      xyz0 = scale * xyz0\n      xyz1 = scale * xyz1\n\n    if self.random_rotation:\n      T0 = sample_random_trans(xyz0, self.randg, self.rotation_range)\n      T1 = sample_random_trans(xyz1, self.randg, self.rotation_range)\n      trans = T1 @ np.linalg.inv(T0)\n\n      xyz0 = self.apply_transform(xyz0, T0)\n      xyz1 = self.apply_transform(xyz1, T1)\n    else:\n      trans = np.identity(4)\n\n    # Voxelization\n    sel0 = ME.utils.sparse_quantize(xyz0 / self.voxel_size, return_index=True)\n    sel1 = ME.utils.sparse_quantize(xyz1 / self.voxel_size, return_index=True)\n\n    # Make point clouds using voxelized points\n    pcd0 = make_open3d_point_cloud(xyz0)\n    pcd1 = make_open3d_point_cloud(xyz1)\n\n    # Select features and points using the returned voxelized indices\n    pcd0.colors = o3d.utility.Vector3dVector(color0[sel0])\n    pcd1.colors = o3d.utility.Vector3dVector(color1[sel1])\n    pcd0.points = o3d.utility.Vector3dVector(np.array(pcd0.points)[sel0])\n    pcd1.points = o3d.utility.Vector3dVector(np.array(pcd1.points)[sel1])\n    # Get matches\n    matches = get_matching_indices(pcd0, pcd1, trans, matching_search_voxel_size)\n\n    # Get features\n    npts0 = len(pcd0.colors)\n    npts1 = len(pcd1.colors)\n\n    feats_train0, feats_train1 = [], []\n\n    feats_train0.append(np.ones((npts0, 1)))\n    feats_train1.append(np.ones((npts1, 1)))\n\n    feats0 = np.hstack(feats_train0)\n    feats1 = np.hstack(feats_train1)\n\n    # Get coords\n    xyz0 = np.array(pcd0.points)\n    xyz1 = np.array(pcd1.points)\n\n    coords0 = np.floor(xyz0 / self.voxel_size)\n    coords1 = np.floor(xyz1 / self.voxel_size)\n\n    if self.transform:\n      coords0, feats0 = self.transform(coords0, feats0)\n      coords1, feats1 = self.transform(coords1, feats1)\n\n    return (xyz0, xyz1, coords0, coords1, feats0, feats1, matches, trans)\n\n\nclass KITTIPairDataset(PairDataset):\n  AUGMENT = None\n  DATA_FILES = {\n      \'train\': \'./config/train_kitti.txt\',\n      \'val\': \'./config/val_kitti.txt\',\n      \'test\': \'./config/test_kitti.txt\'\n  }\n  TEST_RANDOM_ROTATION = False\n  IS_ODOMETRY = True\n\n  def __init__(self,\n               phase,\n               transform=None,\n               random_rotation=True,\n               random_scale=True,\n               manual_seed=False,\n               config=None):\n    # For evaluation, use the odometry dataset training following the 3DFeat eval method\n    if self.IS_ODOMETRY:\n      self.root = root = config.kitti_root + \'/dataset\'\n      random_rotation = self.TEST_RANDOM_ROTATION\n    else:\n      self.date = config.kitti_date\n      self.root = root = os.path.join(config.kitti_root, self.date)\n\n    self.icp_path = os.path.join(config.kitti_root, \'icp\')\n    pathlib.Path(self.icp_path).mkdir(parents=True, exist_ok=True)\n\n    PairDataset.__init__(self, phase, transform, random_rotation, random_scale,\n                         manual_seed, config)\n\n    logging.info(f""Loading the subset {phase} from {root}"")\n    # Use the kitti root\n    self.max_time_diff = max_time_diff = config.kitti_max_time_diff\n\n    subset_names = open(self.DATA_FILES[phase]).read().split()\n    for dirname in subset_names:\n      drive_id = int(dirname)\n      inames = self.get_all_scan_ids(drive_id)\n      for start_time in inames:\n        for time_diff in range(2, max_time_diff):\n          pair_time = time_diff + start_time\n          if pair_time in inames:\n            self.files.append((drive_id, start_time, pair_time))\n\n  def get_all_scan_ids(self, drive_id):\n    if self.IS_ODOMETRY:\n      fnames = glob.glob(self.root + \'/sequences/%02d/velodyne/*.bin\' % drive_id)\n    else:\n      fnames = glob.glob(self.root + \'/\' + self.date +\n                         \'_drive_%04d_sync/velodyne_points/data/*.bin\' % drive_id)\n    assert len(\n        fnames) > 0, f""Make sure that the path {self.root} has drive id: {drive_id}""\n    inames = [int(os.path.split(fname)[-1][:-4]) for fname in fnames]\n    return inames\n\n  @property\n  def velo2cam(self):\n    try:\n      velo2cam = self._velo2cam\n    except AttributeError:\n      R = np.array([\n          7.533745e-03, -9.999714e-01, -6.166020e-04, 1.480249e-02, 7.280733e-04,\n          -9.998902e-01, 9.998621e-01, 7.523790e-03, 1.480755e-02\n      ]).reshape(3, 3)\n      T = np.array([-4.069766e-03, -7.631618e-02, -2.717806e-01]).reshape(3, 1)\n      velo2cam = np.hstack([R, T])\n      self._velo2cam = np.vstack((velo2cam, [0, 0, 0, 1])).T\n    return self._velo2cam\n\n  def get_video_odometry(self, drive, indices=None, ext=\'.txt\', return_all=False):\n    if self.IS_ODOMETRY:\n      data_path = self.root + \'/poses/%02d.txt\' % drive\n      if data_path not in kitti_cache:\n        kitti_cache[data_path] = np.genfromtxt(data_path)\n      if return_all:\n        return kitti_cache[data_path]\n      else:\n        return kitti_cache[data_path][indices]\n    else:\n      data_path = self.root + \'/\' + self.date + \'_drive_%04d_sync/oxts/data\' % drive\n      odometry = []\n      if indices is None:\n        fnames = glob.glob(self.root + \'/\' + self.date +\n                           \'_drive_%04d_sync/velodyne_points/data/*.bin\' % drive)\n        indices = sorted([int(os.path.split(fname)[-1][:-4]) for fname in fnames])\n\n      for index in indices:\n        filename = os.path.join(data_path, \'%010d%s\' % (index, ext))\n        if filename not in kitti_cache:\n          kitti_cache[filename] = np.genfromtxt(filename)\n        odometry.append(kitti_cache[filename])\n\n      odometry = np.array(odometry)\n      return odometry\n\n  def odometry_to_positions(self, odometry):\n    if self.IS_ODOMETRY:\n      T_w_cam0 = odometry.reshape(3, 4)\n      T_w_cam0 = np.vstack((T_w_cam0, [0, 0, 0, 1]))\n      return T_w_cam0\n    else:\n      lat, lon, alt, roll, pitch, yaw = odometry.T[:6]\n\n      R = 6378137  # Earth\'s radius in metres\n\n      # convert to metres\n      lat, lon = np.deg2rad(lat), np.deg2rad(lon)\n      mx = R * lon * np.cos(lat)\n      my = R * lat\n\n      times = odometry.T[-1]\n      return np.vstack([mx, my, alt, roll, pitch, yaw, times]).T\n\n  def rot3d(self, axis, angle):\n    ei = np.ones(3, dtype=\'bool\')\n    ei[axis] = 0\n    i = np.nonzero(ei)[0]\n    m = np.eye(3)\n    c, s = np.cos(angle), np.sin(angle)\n    m[i[0], i[0]] = c\n    m[i[0], i[1]] = -s\n    m[i[1], i[0]] = s\n    m[i[1], i[1]] = c\n    return m\n\n  def pos_transform(self, pos):\n    x, y, z, rx, ry, rz, _ = pos[0]\n    RT = np.eye(4)\n    RT[:3, :3] = np.dot(np.dot(self.rot3d(0, rx), self.rot3d(1, ry)), self.rot3d(2, rz))\n    RT[:3, 3] = [x, y, z]\n    return RT\n\n  def get_position_transform(self, pos0, pos1, invert=False):\n    T0 = self.pos_transform(pos0)\n    T1 = self.pos_transform(pos1)\n    return (np.dot(T1, np.linalg.inv(T0)).T if not invert else np.dot(\n        np.linalg.inv(T1), T0).T)\n\n  def _get_velodyne_fn(self, drive, t):\n    if self.IS_ODOMETRY:\n      fname = self.root + \'/sequences/%02d/velodyne/%06d.bin\' % (drive, t)\n    else:\n      fname = self.root + \\\n          \'/\' + self.date + \'_drive_%04d_sync/velodyne_points/data/%010d.bin\' % (\n              drive, t)\n    return fname\n\n  def __getitem__(self, idx):\n    drive = self.files[idx][0]\n    t0, t1 = self.files[idx][1], self.files[idx][2]\n    all_odometry = self.get_video_odometry(drive, [t0, t1])\n    positions = [self.odometry_to_positions(odometry) for odometry in all_odometry]\n    fname0 = self._get_velodyne_fn(drive, t0)\n    fname1 = self._get_velodyne_fn(drive, t1)\n\n    # XYZ and reflectance\n    xyzr0 = np.fromfile(fname0, dtype=np.float32).reshape(-1, 4)\n    xyzr1 = np.fromfile(fname1, dtype=np.float32).reshape(-1, 4)\n\n    xyz0 = xyzr0[:, :3]\n    xyz1 = xyzr1[:, :3]\n\n    key = \'%d_%d_%d\' % (drive, t0, t1)\n    filename = self.icp_path + \'/\' + key + \'.npy\'\n    if key not in kitti_icp_cache:\n      if not os.path.exists(filename):\n        # work on the downsampled xyzs, 0.05m == 5cm\n        sel0 = ME.utils.sparse_quantize(xyz0 / 0.05, return_index=True)\n        sel1 = ME.utils.sparse_quantize(xyz1 / 0.05, return_index=True)\n\n        M = (self.velo2cam @ positions[0].T @ np.linalg.inv(positions[1].T)\n             @ np.linalg.inv(self.velo2cam)).T\n        xyz0_t = self.apply_transform(xyz0[sel0], M)\n        pcd0 = make_open3d_point_cloud(xyz0_t)\n        pcd1 = make_open3d_point_cloud(xyz1[sel1])\n        reg = o3d.registration.registration_icp(\n            pcd0, pcd1, 0.2, np.eye(4),\n            o3d.registration.TransformationEstimationPointToPoint(),\n            o3d.registration.ICPConvergenceCriteria(max_iteration=200))\n        pcd0.transform(reg.transformation)\n        # pcd0.transform(M2) or self.apply_transform(xyz0, M2)\n        M2 = M @ reg.transformation\n        # o3d.draw_geometries([pcd0, pcd1])\n        # write to a file\n        np.save(filename, M2)\n      else:\n        M2 = np.load(filename)\n      kitti_icp_cache[key] = M2\n    else:\n      M2 = kitti_icp_cache[key]\n\n    if self.random_rotation:\n      T0 = sample_random_trans(xyz0, self.randg, np.pi / 4)\n      T1 = sample_random_trans(xyz1, self.randg, np.pi / 4)\n      trans = T1 @ M2 @ np.linalg.inv(T0)\n\n      xyz0 = self.apply_transform(xyz0, T0)\n      xyz1 = self.apply_transform(xyz1, T1)\n    else:\n      trans = M2\n\n    matching_search_voxel_size = self.matching_search_voxel_size\n    if self.random_scale and random.random() < 0.95:\n      scale = self.min_scale + \\\n          (self.max_scale - self.min_scale) * random.random()\n      matching_search_voxel_size *= scale\n      xyz0 = scale * xyz0\n      xyz1 = scale * xyz1\n\n    # Voxelization\n    xyz0_th = torch.from_numpy(xyz0)\n    xyz1_th = torch.from_numpy(xyz1)\n\n    sel0 = ME.utils.sparse_quantize(xyz0_th / self.voxel_size, return_index=True)\n    sel1 = ME.utils.sparse_quantize(xyz1_th / self.voxel_size, return_index=True)\n\n    # Make point clouds using voxelized points\n    pcd0 = make_open3d_point_cloud(xyz0[sel0])\n    pcd1 = make_open3d_point_cloud(xyz1[sel1])\n\n    # Get matches\n    matches = get_matching_indices(pcd0, pcd1, trans, matching_search_voxel_size)\n    if len(matches) < 1000:\n      raise ValueError(f""{drive}, {t0}, {t1}"")\n\n    # Get features\n    npts0 = len(sel0)\n    npts1 = len(sel1)\n\n    feats_train0, feats_train1 = [], []\n\n    unique_xyz0_th = xyz0_th[sel0]\n    unique_xyz1_th = xyz1_th[sel1]\n\n    feats_train0.append(torch.ones((npts0, 1)))\n    feats_train1.append(torch.ones((npts1, 1)))\n\n    feats0 = torch.cat(feats_train0, 1)\n    feats1 = torch.cat(feats_train1, 1)\n\n    coords0 = torch.floor(unique_xyz0_th / self.voxel_size)\n    coords1 = torch.floor(unique_xyz1_th / self.voxel_size)\n\n    if self.transform:\n      coords0, feats0 = self.transform(coords0, feats0)\n      coords1, feats1 = self.transform(coords1, feats1)\n\n    return (unique_xyz0_th.float(), unique_xyz1_th.float(), coords0.int(),\n            coords1.int(), feats0.float(), feats1.float(), matches, trans)\n\n\nclass KITTINMPairDataset(KITTIPairDataset):\n  r""""""\n  Generate KITTI pairs within N meter distance\n  """"""\n  MIN_DIST = 10\n\n  def __init__(self,\n               phase,\n               transform=None,\n               random_rotation=True,\n               random_scale=True,\n               manual_seed=False,\n               config=None):\n    if self.IS_ODOMETRY:\n      self.root = root = os.path.join(config.kitti_root, \'dataset\')\n      random_rotation = self.TEST_RANDOM_ROTATION\n    else:\n      self.date = config.kitti_date\n      self.root = root = os.path.join(config.kitti_root, self.date)\n\n    self.icp_path = os.path.join(config.kitti_root, \'icp\')\n    pathlib.Path(self.icp_path).mkdir(parents=True, exist_ok=True)\n\n    PairDataset.__init__(self, phase, transform, random_rotation, random_scale,\n                         manual_seed, config)\n\n    logging.info(f""Loading the subset {phase} from {root}"")\n\n    subset_names = open(self.DATA_FILES[phase]).read().split()\n    if self.IS_ODOMETRY:\n      for dirname in subset_names:\n        drive_id = int(dirname)\n        fnames = glob.glob(root + \'/sequences/%02d/velodyne/*.bin\' % drive_id)\n        assert len(fnames) > 0, f""Make sure that the path {root} has data {dirname}""\n        inames = sorted([int(os.path.split(fname)[-1][:-4]) for fname in fnames])\n\n        all_odo = self.get_video_odometry(drive_id, return_all=True)\n        all_pos = np.array([self.odometry_to_positions(odo) for odo in all_odo])\n        Ts = all_pos[:, :3, 3]\n        pdist = (Ts.reshape(1, -1, 3) - Ts.reshape(-1, 1, 3))**2\n        pdist = np.sqrt(pdist.sum(-1))\n        valid_pairs = pdist > self.MIN_DIST\n        curr_time = inames[0]\n        while curr_time in inames:\n          # Find the min index\n          next_time = np.where(valid_pairs[curr_time][curr_time:curr_time + 100])[0]\n          if len(next_time) == 0:\n            curr_time += 1\n          else:\n            # Follow https://github.com/yewzijian/3DFeatNet/blob/master/scripts_data_processing/kitti/process_kitti_data.m#L44\n            next_time = next_time[0] + curr_time - 1\n\n          if next_time in inames:\n            self.files.append((drive_id, curr_time, next_time))\n            curr_time = next_time + 1\n    else:\n      for dirname in subset_names:\n        drive_id = int(dirname)\n        fnames = glob.glob(root + \'/\' + self.date +\n                           \'_drive_%04d_sync/velodyne_points/data/*.bin\' % drive_id)\n        assert len(fnames) > 0, f""Make sure that the path {root} has data {dirname}""\n        inames = sorted([int(os.path.split(fname)[-1][:-4]) for fname in fnames])\n\n        all_odo = self.get_video_odometry(drive_id, return_all=True)\n        all_pos = np.array([self.odometry_to_positions(odo) for odo in all_odo])\n        Ts = all_pos[:, 0, :3]\n\n        pdist = (Ts.reshape(1, -1, 3) - Ts.reshape(-1, 1, 3))**2\n        pdist = np.sqrt(pdist.sum(-1))\n\n        for start_time in inames:\n          pair_time = np.where(\n              pdist[start_time][start_time:start_time + 100] > self.MIN_DIST)[0]\n          if len(pair_time) == 0:\n            continue\n          else:\n            pair_time = pair_time[0] + start_time\n\n          if pair_time in inames:\n            self.files.append((drive_id, start_time, pair_time))\n\n    if self.IS_ODOMETRY:\n      # Remove problematic sequence\n      for item in [\n          (8, 15, 58),\n      ]:\n        if item in self.files:\n          self.files.pop(self.files.index(item))\n\n\nclass ThreeDMatchPairDataset(IndoorPairDataset):\n  OVERLAP_RATIO = 0.3\n  DATA_FILES = {\n      \'train\': \'./config/train_3dmatch.txt\',\n      \'val\': \'./config/val_3dmatch.txt\',\n      \'test\': \'./config/test_3dmatch.txt\'\n  }\n\n\nALL_DATASETS = [ThreeDMatchPairDataset, KITTIPairDataset, KITTINMPairDataset]\ndataset_str_mapping = {d.__name__: d for d in ALL_DATASETS}\n\n\ndef make_data_loader(config, phase, batch_size, num_threads=0, shuffle=None):\n  assert phase in [\'train\', \'trainval\', \'val\', \'test\']\n  if shuffle is None:\n    shuffle = phase != \'test\'\n\n  if config.dataset not in dataset_str_mapping.keys():\n    logging.error(f\'Dataset {config.dataset}, does not exists in \' +\n                  \', \'.join(dataset_str_mapping.keys()))\n\n  Dataset = dataset_str_mapping[config.dataset]\n\n  use_random_scale = False\n  use_random_rotation = False\n  transforms = []\n  if phase in [\'train\', \'trainval\']:\n    use_random_rotation = config.use_random_rotation\n    use_random_scale = config.use_random_scale\n    transforms += [t.Jitter()]\n\n  dset = Dataset(\n      phase,\n      transform=t.Compose(transforms),\n      random_scale=use_random_scale,\n      random_rotation=use_random_rotation,\n      config=config)\n\n  loader = torch.utils.data.DataLoader(\n      dset,\n      batch_size=batch_size,\n      shuffle=shuffle,\n      num_workers=num_threads,\n      collate_fn=collate_pair_fn,\n      pin_memory=False,\n      drop_last=True)\n\n  return loader\n'"
lib/eval.py,2,"b""import torch\nimport numpy as np\nimport open3d as o3d\n\nfrom lib.metrics import pdist\nfrom scipy.spatial import cKDTree\n\n\ndef find_nn_cpu(feat0, feat1, return_distance=False):\n  feat1tree = cKDTree(feat1)\n  dists, nn_inds = feat1tree.query(feat0, k=1, n_jobs=-1)\n  if return_distance:\n    return nn_inds, dists\n  else:\n    return nn_inds\n\n\ndef find_nn_gpu(F0, F1, nn_max_n=-1, return_distance=False, dist_type='SquareL2'):\n  # Too much memory if F0 or F1 large. Divide the F0\n  if nn_max_n > 1:\n    N = len(F0)\n    C = int(np.ceil(N / nn_max_n))\n    stride = nn_max_n\n    dists, inds = [], []\n    for i in range(C):\n      dist = pdist(F0[i * stride:(i + 1) * stride], F1, dist_type=dist_type)\n      min_dist, ind = dist.min(dim=1)\n      dists.append(min_dist.detach().unsqueeze(1).cpu())\n      inds.append(ind.cpu())\n\n    if C * stride < N:\n      dist = pdist(F0[C * stride:], F1, dist_type=dist_type)\n      min_dist, ind = dist.min(dim=1)\n      dists.append(min_dist.detach().unsqueeze(1).cpu())\n      inds.append(ind.cpu())\n\n    dists = torch.cat(dists)\n    inds = torch.cat(inds)\n    assert len(inds) == N\n  else:\n    dist = pdist(F0, F1, dist_type=dist_type)\n    min_dist, inds = dist.min(dim=1)\n    dists = min_dist.detach().unsqueeze(1).cpu()\n    inds = inds.cpu()\n  if return_distance:\n    return inds, dists\n  else:\n    return inds\n"""
lib/metrics.py,5,"b""import numpy as np\n\nimport torch\nimport torch.functional as F\n\n\ndef eval_metrics(output, target):\n  output = (F.sigmoid(output) > 0.5).cpu().data.numpy()\n  target = target.cpu().data.numpy()\n  return np.linalg.norm(output - target)\n\n\ndef corr_dist(est, gth, xyz0, xyz1, weight=None, max_dist=1):\n  xyz0_est = xyz0 @ est[:3, :3].t() + est[:3, 3]\n  xyz0_gth = xyz0 @ gth[:3, :3].t() + gth[:3, 3]\n  dists = torch.clamp(torch.sqrt(((xyz0_est - xyz0_gth).pow(2)).sum(1)), max=max_dist)\n  if weight is not None:\n    dists = weight * dists\n  return dists.mean()\n\n\ndef pdist(A, B, dist_type='L2'):\n  if dist_type == 'L2':\n    D2 = torch.sum((A.unsqueeze(1) - B.unsqueeze(0)).pow(2), 2)\n    return torch.sqrt(D2 + 1e-7)\n  elif dist_type == 'SquareL2':\n    return torch.sum((A.unsqueeze(1) - B.unsqueeze(0)).pow(2), 2)\n  else:\n    raise NotImplementedError('Not implemented')\n\n\ndef get_loss_fn(loss):\n  if loss == 'corr_dist':\n    return corr_dist\n  else:\n    raise ValueError(f'Loss {loss}, not defined')\n"""
lib/timer.py,0,"b'import time\n\n\nclass AverageMeter(object):\n  """"""Computes and stores the average and current value""""""\n\n  def __init__(self):\n    self.reset()\n\n  def reset(self):\n    self.val = 0\n    self.avg = 0\n    self.sum = 0.0\n    self.sq_sum = 0.0\n    self.count = 0\n\n  def update(self, val, n=1):\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count\n    self.sq_sum += val**2 * n\n    self.var = self.sq_sum / self.count - self.avg ** 2\n\n\nclass Timer(object):\n  """"""A simple timer.""""""\n\n  def __init__(self):\n    self.total_time = 0.\n    self.calls = 0\n    self.start_time = 0.\n    self.diff = 0.\n    self.avg = 0.\n\n  def reset(self):\n    self.total_time = 0\n    self.calls = 0\n    self.start_time = 0\n    self.diff = 0\n    self.avg = 0\n\n  def tic(self):\n    # using time.time instead of time.clock because time time.clock\n    # does not normalize for multithreading\n    self.start_time = time.time()\n\n  def toc(self, average=True):\n    self.diff = time.time() - self.start_time\n    self.total_time += self.diff\n    self.calls += 1\n    self.avg = self.total_time / self.calls\n    if average:\n      return self.avg\n    else:\n      return self.diff\n'"
lib/trainer.py,27,"b'# -*- coding: future_fstrings -*-\n#\n# Written by Chris Choy <chrischoy@ai.stanford.edu>\n# Distributed under MIT License\nimport os\nimport os.path as osp\nimport gc\nimport logging\nimport numpy as np\nimport json\n\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tensorboardX import SummaryWriter\n\nfrom model import load_model\nimport util.transform_estimation as te\nfrom lib.metrics import pdist, corr_dist\nfrom lib.timer import Timer, AverageMeter\nfrom lib.eval import find_nn_gpu\n\nfrom util.file import ensure_dir\nfrom util.misc import _hash\n\nimport MinkowskiEngine as ME\n\n\nclass AlignmentTrainer:\n\n  def __init__(\n      self,\n      config,\n      data_loader,\n      val_data_loader=None,\n  ):\n    num_feats = 1  # occupancy only for 3D Match dataset. For ScanNet, use RGB 3 channels.\n\n    # Model initialization\n    Model = load_model(config.model)\n    model = Model(\n        num_feats,\n        config.model_n_out,\n        bn_momentum=config.bn_momentum,\n        normalize_feature=config.normalize_feature,\n        conv1_kernel_size=config.conv1_kernel_size,\n        D=3)\n\n    if config.weights:\n      checkpoint = torch.load(config.weights)\n      model.load_state_dict(checkpoint[\'state_dict\'])\n\n    logging.info(model)\n\n    self.config = config\n    self.model = model\n    self.max_epoch = config.max_epoch\n    self.save_freq = config.save_freq_epoch\n    self.val_max_iter = config.val_max_iter\n    self.val_epoch_freq = config.val_epoch_freq\n\n    self.best_val_metric = config.best_val_metric\n    self.best_val_epoch = -np.inf\n    self.best_val = -np.inf\n\n    if config.use_gpu and not torch.cuda.is_available():\n      logging.warning(\'Warning: There\\\'s no CUDA support on this machine, \'\n                      \'training is performed on CPU.\')\n      raise ValueError(\'GPU not available, but cuda flag set\')\n\n    self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    self.optimizer = getattr(optim, config.optimizer)(\n        model.parameters(),\n        lr=config.lr,\n        momentum=config.momentum,\n        weight_decay=config.weight_decay)\n\n    self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, config.exp_gamma)\n\n    self.start_epoch = 1\n    self.checkpoint_dir = config.out_dir\n\n    ensure_dir(self.checkpoint_dir)\n    json.dump(\n        config,\n        open(os.path.join(self.checkpoint_dir, \'config.json\'), \'w\'),\n        indent=4,\n        sort_keys=False)\n\n    self.iter_size = config.iter_size\n    self.batch_size = data_loader.batch_size\n    self.data_loader = data_loader\n    self.val_data_loader = val_data_loader\n\n    self.test_valid = True if self.val_data_loader is not None else False\n    self.log_step = int(np.sqrt(self.config.batch_size))\n    self.model = self.model.to(self.device)\n    self.writer = SummaryWriter(logdir=config.out_dir)\n\n    if config.resume is not None:\n      if osp.isfile(config.resume):\n        logging.info(""=> loading checkpoint \'{}\'"".format(config.resume))\n        state = torch.load(config.resume)\n        self.start_epoch = state[\'epoch\']\n        model.load_state_dict(state[\'state_dict\'])\n        self.scheduler.load_state_dict(state[\'scheduler\'])\n        self.optimizer.load_state_dict(state[\'optimizer\'])\n\n        if \'best_val\' in state.keys():\n          self.best_val = state[\'best_val\']\n          self.best_val_epoch = state[\'best_val_epoch\']\n          self.best_val_metric = state[\'best_val_metric\']\n      else:\n        raise ValueError(f""=> no checkpoint found at \'{config.resume}\'"")\n\n  def train(self):\n    """"""\n    Full training logic\n    """"""\n    # Baseline random feature performance\n    if self.test_valid:\n      with torch.no_grad():\n        val_dict = self._valid_epoch()\n\n      for k, v in val_dict.items():\n        self.writer.add_scalar(f\'val/{k}\', v, 0)\n\n    for epoch in range(self.start_epoch, self.max_epoch + 1):\n      lr = self.scheduler.get_lr()\n      logging.info(f"" Epoch: {epoch}, LR: {lr}"")\n      self._train_epoch(epoch)\n      self._save_checkpoint(epoch)\n      self.scheduler.step()\n\n      if self.test_valid and epoch % self.val_epoch_freq == 0:\n        with torch.no_grad():\n          val_dict = self._valid_epoch()\n\n        for k, v in val_dict.items():\n          self.writer.add_scalar(f\'val/{k}\', v, epoch)\n        if self.best_val < val_dict[self.best_val_metric]:\n          logging.info(\n              f\'Saving the best val model with {self.best_val_metric}: {val_dict[self.best_val_metric]}\'\n          )\n          self.best_val = val_dict[self.best_val_metric]\n          self.best_val_epoch = epoch\n          self._save_checkpoint(epoch, \'best_val_checkpoint\')\n        else:\n          logging.info(\n              f\'Current best val model with {self.best_val_metric}: {self.best_val} at epoch {self.best_val_epoch}\'\n          )\n\n  def _save_checkpoint(self, epoch, filename=\'checkpoint\'):\n    state = {\n        \'epoch\': epoch,\n        \'state_dict\': self.model.state_dict(),\n        \'optimizer\': self.optimizer.state_dict(),\n        \'scheduler\': self.scheduler.state_dict(),\n        \'config\': self.config,\n        \'best_val\': self.best_val,\n        \'best_val_epoch\': self.best_val_epoch,\n        \'best_val_metric\': self.best_val_metric\n    }\n    filename = os.path.join(self.checkpoint_dir, f\'{filename}.pth\')\n    logging.info(""Saving checkpoint: {} ..."".format(filename))\n    torch.save(state, filename)\n\n\nclass ContrastiveLossTrainer(AlignmentTrainer):\n\n  def __init__(\n      self,\n      config,\n      data_loader,\n      val_data_loader=None,\n  ):\n    if val_data_loader is not None:\n      assert val_data_loader.batch_size == 1, ""Val set batch size must be 1 for now.""\n    AlignmentTrainer.__init__(self, config, data_loader, val_data_loader)\n    self.neg_thresh = config.neg_thresh\n    self.pos_thresh = config.pos_thresh\n    self.neg_weight = config.neg_weight\n\n  def apply_transform(self, pts, trans):\n    R = trans[:3, :3]\n    T = trans[:3, 3]\n    return pts @ R.t() + T\n\n  def generate_rand_negative_pairs(self, positive_pairs, hash_seed, N0, N1, N_neg=0):\n    """"""\n    Generate random negative pairs\n    """"""\n    if not isinstance(positive_pairs, np.ndarray):\n      positive_pairs = np.array(positive_pairs, dtype=np.int64)\n    if N_neg < 1:\n      N_neg = positive_pairs.shape[0] * 2\n    pos_keys = _hash(positive_pairs, hash_seed)\n\n    neg_pairs = np.floor(np.random.rand(int(N_neg), 2) * np.array([[N0, N1]])).astype(\n        np.int64)\n    neg_keys = _hash(neg_pairs, hash_seed)\n    mask = np.isin(neg_keys, pos_keys, assume_unique=False)\n    return neg_pairs[np.logical_not(mask)]\n\n  def _train_epoch(self, epoch):\n    gc.collect()\n    self.model.train()\n    # Epoch starts from 1\n    total_loss = 0\n    total_num = 0.0\n\n    data_loader = self.data_loader\n    data_loader_iter = self.data_loader.__iter__()\n\n    iter_size = self.iter_size\n    start_iter = (epoch - 1) * (len(data_loader) // iter_size)\n\n    data_meter, data_timer, total_timer = AverageMeter(), Timer(), Timer()\n\n    # Main training\n    for curr_iter in range(len(data_loader) // iter_size):\n      self.optimizer.zero_grad()\n      batch_pos_loss, batch_neg_loss, batch_loss = 0, 0, 0\n\n      data_time = 0\n      total_timer.tic()\n      for iter_idx in range(iter_size):\n        # Caffe iter size\n        data_timer.tic()\n        input_dict = data_loader_iter.next()\n        data_time += data_timer.toc(average=False)\n\n        # pairs consist of (xyz1 index, xyz0 index)\n        sinput0 = ME.SparseTensor(\n            input_dict[\'sinput0_F\'], coords=input_dict[\'sinput0_C\']).to(self.device)\n        F0 = self.model(sinput0).F\n\n        sinput1 = ME.SparseTensor(\n            input_dict[\'sinput1_F\'], coords=input_dict[\'sinput1_C\']).to(self.device)\n        F1 = self.model(sinput1).F\n\n        N0, N1 = len(sinput0), len(sinput1)\n\n        pos_pairs = input_dict[\'correspondences\']\n        neg_pairs = self.generate_rand_negative_pairs(pos_pairs, max(N0, N1), N0, N1)\n        pos_pairs = pos_pairs.long().to(self.device)\n        neg_pairs = torch.from_numpy(neg_pairs).long().to(self.device)\n\n        neg0 = F0.index_select(0, neg_pairs[:, 0])\n        neg1 = F1.index_select(0, neg_pairs[:, 1])\n        pos0 = F0.index_select(0, pos_pairs[:, 0])\n        pos1 = F1.index_select(0, pos_pairs[:, 1])\n\n        # Positive loss\n        pos_loss = (pos0 - pos1).pow(2).sum(1)\n\n        # Negative loss\n        neg_loss = F.relu(self.neg_thresh -\n                          ((neg0 - neg1).pow(2).sum(1) + 1e-4).sqrt()).pow(2)\n\n        pos_loss_mean = pos_loss.mean() / iter_size\n        neg_loss_mean = neg_loss.mean() / iter_size\n\n        # Weighted loss\n        loss = pos_loss_mean + self.neg_weight * neg_loss_mean\n        loss.backward(\n        )  # To accumulate gradient, zero gradients only at the begining of iter_size\n        batch_loss += loss.item()\n        batch_pos_loss += pos_loss_mean.item()\n        batch_neg_loss += neg_loss_mean.item()\n\n      self.optimizer.step()\n\n      torch.cuda.empty_cache()\n\n      total_loss += batch_loss\n      total_num += 1.0\n      total_timer.toc()\n      data_meter.update(data_time)\n\n      # Print logs\n      if curr_iter % self.config.stat_freq == 0:\n        self.writer.add_scalar(\'train/loss\', batch_loss, start_iter + curr_iter)\n        self.writer.add_scalar(\'train/pos_loss\', batch_pos_loss, start_iter + curr_iter)\n        self.writer.add_scalar(\'train/neg_loss\', batch_neg_loss, start_iter + curr_iter)\n        logging.info(\n            ""Train Epoch: {} [{}/{}], Current Loss: {:.3e} Pos: {:.3f} Neg: {:.3f}""\n            .format(epoch, curr_iter,\n                    len(self.data_loader) //\n                    iter_size, batch_loss, batch_pos_loss, batch_neg_loss) +\n            ""\\tData time: {:.4f}, Train time: {:.4f}, Iter time: {:.4f}"".format(\n                data_meter.avg, total_timer.avg - data_meter.avg, total_timer.avg))\n        data_meter.reset()\n        total_timer.reset()\n\n  def _valid_epoch(self):\n    # Change the network to evaluation mode\n    self.model.eval()\n    self.val_data_loader.dataset.reset_seed(0)\n    num_data = 0\n    hit_ratio_meter, feat_match_ratio, loss_meter, rte_meter, rre_meter = AverageMeter(\n    ), AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter()\n    data_timer, feat_timer, matching_timer = Timer(), Timer(), Timer()\n    tot_num_data = len(self.val_data_loader.dataset)\n    if self.val_max_iter > 0:\n      tot_num_data = min(self.val_max_iter, tot_num_data)\n    data_loader_iter = self.val_data_loader.__iter__()\n\n    for batch_idx in range(tot_num_data):\n      data_timer.tic()\n      input_dict = data_loader_iter.next()\n      data_timer.toc()\n\n      # pairs consist of (xyz1 index, xyz0 index)\n      feat_timer.tic()\n      sinput0 = ME.SparseTensor(\n          input_dict[\'sinput0_F\'], coords=input_dict[\'sinput0_C\']).to(self.device)\n      F0 = self.model(sinput0).F\n\n      sinput1 = ME.SparseTensor(\n          input_dict[\'sinput1_F\'], coords=input_dict[\'sinput1_C\']).to(self.device)\n      F1 = self.model(sinput1).F\n      feat_timer.toc()\n\n      matching_timer.tic()\n      xyz0, xyz1, T_gt = input_dict[\'pcd0\'], input_dict[\'pcd1\'], input_dict[\'T_gt\']\n      xyz0_corr, xyz1_corr = self.find_corr(xyz0, xyz1, F0, F1, subsample_size=5000)\n      T_est = te.est_quad_linear_robust(xyz0_corr, xyz1_corr)\n\n      loss = corr_dist(T_est, T_gt, xyz0, xyz1, weight=None)\n      loss_meter.update(loss)\n\n      rte = np.linalg.norm(T_est[:3, 3] - T_gt[:3, 3])\n      rte_meter.update(rte)\n      rre = np.arccos((np.trace(T_est[:3, :3].t() @ T_gt[:3, :3]) - 1) / 2)\n      if not np.isnan(rre):\n        rre_meter.update(rre)\n\n      hit_ratio = self.evaluate_hit_ratio(\n          xyz0_corr, xyz1_corr, T_gt, thresh=self.config.hit_ratio_thresh)\n      hit_ratio_meter.update(hit_ratio)\n      feat_match_ratio.update(hit_ratio > 0.05)\n      matching_timer.toc()\n\n      num_data += 1\n      torch.cuda.empty_cache()\n\n      if batch_idx % 100 == 0 and batch_idx > 0:\n        logging.info(\' \'.join([\n            f""Validation iter {num_data} / {tot_num_data} : Data Loading Time: {data_timer.avg:.3f},"",\n            f""Feature Extraction Time: {feat_timer.avg:.3f}, Matching Time: {matching_timer.avg:.3f},"",\n            f""Loss: {loss_meter.avg:.3f}, RTE: {rte_meter.avg:.3f}, RRE: {rre_meter.avg:.3f},"",\n            f""Hit Ratio: {hit_ratio_meter.avg:.3f}, Feat Match Ratio: {feat_match_ratio.avg:.3f}""\n        ]))\n        data_timer.reset()\n\n    logging.info(\' \'.join([\n        f""Final Loss: {loss_meter.avg:.3f}, RTE: {rte_meter.avg:.3f}, RRE: {rre_meter.avg:.3f},"",\n        f""Hit Ratio: {hit_ratio_meter.avg:.3f}, Feat Match Ratio: {feat_match_ratio.avg:.3f}""\n    ]))\n    return {\n        ""loss"": loss_meter.avg,\n        ""rre"": rre_meter.avg,\n        ""rte"": rte_meter.avg,\n        \'feat_match_ratio\': feat_match_ratio.avg,\n        \'hit_ratio\': hit_ratio_meter.avg\n    }\n\n  def find_corr(self, xyz0, xyz1, F0, F1, subsample_size=-1):\n    subsample = len(F0) > subsample_size\n    if subsample_size > 0 and subsample:\n      N0 = min(len(F0), subsample_size)\n      N1 = min(len(F1), subsample_size)\n      inds0 = np.random.choice(len(F0), N0, replace=False)\n      inds1 = np.random.choice(len(F1), N1, replace=False)\n      F0, F1 = F0[inds0], F1[inds1]\n\n    # Compute the nn\n    nn_inds = find_nn_gpu(F0, F1, nn_max_n=self.config.nn_max_n)\n    if subsample_size > 0 and subsample:\n      return xyz0[inds0], xyz1[inds1[nn_inds]]\n    else:\n      return xyz0, xyz1[nn_inds]\n\n  def evaluate_hit_ratio(self, xyz0, xyz1, T_gth, thresh=0.1):\n    xyz0 = self.apply_transform(xyz0, T_gth)\n    dist = np.sqrt(((xyz0 - xyz1)**2).sum(1) + 1e-6)\n    return (dist < thresh).float().mean().item()\n\n\nclass HardestContrastiveLossTrainer(ContrastiveLossTrainer):\n\n  def contrastive_hardest_negative_loss(self,\n                                        F0,\n                                        F1,\n                                        positive_pairs,\n                                        num_pos=5192,\n                                        num_hn_samples=2048,\n                                        thresh=None):\n    """"""\n    Generate negative pairs\n    """"""\n    N0, N1 = len(F0), len(F1)\n    N_pos_pairs = len(positive_pairs)\n    hash_seed = max(N0, N1)\n    sel0 = np.random.choice(N0, min(N0, num_hn_samples), replace=False)\n    sel1 = np.random.choice(N1, min(N1, num_hn_samples), replace=False)\n\n    if N_pos_pairs > num_pos:\n      pos_sel = np.random.choice(N_pos_pairs, num_pos, replace=False)\n      sample_pos_pairs = positive_pairs[pos_sel]\n    else:\n      sample_pos_pairs = positive_pairs\n\n    # Find negatives for all F1[positive_pairs[:, 1]]\n    subF0, subF1 = F0[sel0], F1[sel1]\n\n    pos_ind0 = sample_pos_pairs[:, 0].long()\n    pos_ind1 = sample_pos_pairs[:, 1].long()\n    posF0, posF1 = F0[pos_ind0], F1[pos_ind1]\n\n    D01 = pdist(posF0, subF1, dist_type=\'L2\')\n    D10 = pdist(posF1, subF0, dist_type=\'L2\')\n\n    D01min, D01ind = D01.min(1)\n    D10min, D10ind = D10.min(1)\n\n    if not isinstance(positive_pairs, np.ndarray):\n      positive_pairs = np.array(positive_pairs, dtype=np.int64)\n\n    pos_keys = _hash(positive_pairs, hash_seed)\n\n    D01ind = sel1[D01ind.cpu().numpy()]\n    D10ind = sel0[D10ind.cpu().numpy()]\n    neg_keys0 = _hash([pos_ind0.numpy(), D01ind], hash_seed)\n    neg_keys1 = _hash([D10ind, pos_ind1.numpy()], hash_seed)\n\n    mask0 = torch.from_numpy(\n        np.logical_not(np.isin(neg_keys0, pos_keys, assume_unique=False)))\n    mask1 = torch.from_numpy(\n        np.logical_not(np.isin(neg_keys1, pos_keys, assume_unique=False)))\n    pos_loss = F.relu((posF0 - posF1).pow(2).sum(1) - self.pos_thresh)\n    neg_loss0 = F.relu(self.neg_thresh - D01min[mask0]).pow(2)\n    neg_loss1 = F.relu(self.neg_thresh - D10min[mask1]).pow(2)\n    return pos_loss.mean(), (neg_loss0.mean() + neg_loss1.mean()) / 2\n\n  def _train_epoch(self, epoch):\n    gc.collect()\n    self.model.train()\n    # Epoch starts from 1\n    total_loss = 0\n    total_num = 0.0\n    data_loader = self.data_loader\n    data_loader_iter = self.data_loader.__iter__()\n    iter_size = self.iter_size\n    data_meter, data_timer, total_timer = AverageMeter(), Timer(), Timer()\n    start_iter = (epoch - 1) * (len(data_loader) // iter_size)\n    for curr_iter in range(len(data_loader) // iter_size):\n      self.optimizer.zero_grad()\n      batch_pos_loss, batch_neg_loss, batch_loss = 0, 0, 0\n\n      data_time = 0\n      total_timer.tic()\n      for iter_idx in range(iter_size):\n        data_timer.tic()\n        input_dict = data_loader_iter.next()\n        data_time += data_timer.toc(average=False)\n\n        sinput0 = ME.SparseTensor(\n            input_dict[\'sinput0_F\'], coords=input_dict[\'sinput0_C\']).to(self.device)\n        F0 = self.model(sinput0).F\n\n        sinput1 = ME.SparseTensor(\n            input_dict[\'sinput1_F\'], coords=input_dict[\'sinput1_C\']).to(self.device)\n\n        F1 = self.model(sinput1).F\n\n        pos_pairs = input_dict[\'correspondences\']\n        pos_loss, neg_loss = self.contrastive_hardest_negative_loss(\n            F0,\n            F1,\n            pos_pairs,\n            num_pos=self.config.num_pos_per_batch * self.config.batch_size,\n            num_hn_samples=self.config.num_hn_samples_per_batch *\n            self.config.batch_size)\n\n        pos_loss /= iter_size\n        neg_loss /= iter_size\n        loss = pos_loss + self.neg_weight * neg_loss\n        loss.backward()\n\n        batch_loss += loss.item()\n        batch_pos_loss += pos_loss.item()\n        batch_neg_loss += neg_loss.item()\n\n      self.optimizer.step()\n      gc.collect()\n\n      torch.cuda.empty_cache()\n\n      total_loss += batch_loss\n      total_num += 1.0\n      total_timer.toc()\n      data_meter.update(data_time)\n\n      if curr_iter % self.config.stat_freq == 0:\n        self.writer.add_scalar(\'train/loss\', batch_loss, start_iter + curr_iter)\n        self.writer.add_scalar(\'train/pos_loss\', batch_pos_loss, start_iter + curr_iter)\n        self.writer.add_scalar(\'train/neg_loss\', batch_neg_loss, start_iter + curr_iter)\n        logging.info(\n            ""Train Epoch: {} [{}/{}], Current Loss: {:.3e} Pos: {:.3f} Neg: {:.3f}""\n            .format(epoch, curr_iter,\n                    len(self.data_loader) //\n                    iter_size, batch_loss, batch_pos_loss, batch_neg_loss) +\n            ""\\tData time: {:.4f}, Train time: {:.4f}, Iter time: {:.4f}"".format(\n                data_meter.avg, total_timer.avg - data_meter.avg, total_timer.avg))\n        data_meter.reset()\n        total_timer.reset()\n\n\nclass TripletLossTrainer(ContrastiveLossTrainer):\n\n  def triplet_loss(self,\n                   F0,\n                   F1,\n                   positive_pairs,\n                   num_pos=1024,\n                   num_hn_samples=None,\n                   num_rand_triplet=1024):\n    """"""\n    Generate negative pairs\n    """"""\n    N0, N1 = len(F0), len(F1)\n    num_pos_pairs = len(positive_pairs)\n    hash_seed = max(N0, N1)\n\n    if num_pos_pairs > num_pos:\n      pos_sel = np.random.choice(num_pos_pairs, num_pos, replace=False)\n      sample_pos_pairs = positive_pairs[pos_sel]\n    else:\n      sample_pos_pairs = positive_pairs\n\n    pos_ind0 = sample_pos_pairs[:, 0].long()\n    pos_ind1 = sample_pos_pairs[:, 1].long()\n    posF0, posF1 = F0[pos_ind0], F1[pos_ind1]\n\n    if not isinstance(positive_pairs, np.ndarray):\n      positive_pairs = np.array(positive_pairs, dtype=np.int64)\n\n    pos_keys = _hash(positive_pairs, hash_seed)\n    pos_dist = torch.sqrt((posF0 - posF1).pow(2).sum(1) + 1e-7)\n\n    # Random triplets\n    rand_inds = np.random.choice(\n        num_pos_pairs, min(num_pos_pairs, num_rand_triplet), replace=False)\n    rand_pairs = positive_pairs[rand_inds]\n    negatives = np.random.choice(N1, min(N1, num_rand_triplet), replace=False)\n\n    # Remove positives from negatives\n    rand_neg_keys = _hash([rand_pairs[:, 0], negatives], hash_seed)\n    rand_mask = np.logical_not(np.isin(rand_neg_keys, pos_keys, assume_unique=False))\n    anchors, positives = rand_pairs[torch.from_numpy(rand_mask)].T\n    negatives = negatives[rand_mask]\n\n    rand_pos_dist = torch.sqrt((F0[anchors] - F1[positives]).pow(2).sum(1) + 1e-7)\n    rand_neg_dist = torch.sqrt((F0[anchors] - F1[negatives]).pow(2).sum(1) + 1e-7)\n\n    loss = F.relu(rand_pos_dist + self.neg_thresh - rand_neg_dist).mean()\n\n    return loss, pos_dist.mean(), rand_neg_dist.mean()\n\n  def _train_epoch(self, epoch):\n    config = self.config\n\n    gc.collect()\n    self.model.train()\n\n    # Epoch starts from 1\n    total_loss = 0\n    total_num = 0.0\n    data_loader = self.data_loader\n    data_loader_iter = self.data_loader.__iter__()\n    iter_size = self.iter_size\n    data_meter, data_timer, total_timer = AverageMeter(), Timer(), Timer()\n    pos_dist_meter, neg_dist_meter = AverageMeter(), AverageMeter()\n    start_iter = (epoch - 1) * (len(data_loader) // iter_size)\n    for curr_iter in range(len(data_loader) // iter_size):\n      self.optimizer.zero_grad()\n      batch_loss = 0\n      data_time = 0\n      total_timer.tic()\n      for iter_idx in range(iter_size):\n        data_timer.tic()\n        input_dict = data_loader_iter.next()\n        data_time += data_timer.toc(average=False)\n\n        # pairs consist of (xyz1 index, xyz0 index)\n        sinput0 = ME.SparseTensor(\n            input_dict[\'sinput0_F\'], coords=input_dict[\'sinput0_C\']).to(self.device)\n        F0 = self.model(sinput0).F\n\n        sinput1 = ME.SparseTensor(\n            input_dict[\'sinput1_F\'], coords=input_dict[\'sinput1_C\']).to(self.device)\n        F1 = self.model(sinput1).F\n\n        pos_pairs = input_dict[\'correspondences\']\n        loss, pos_dist, neg_dist = self.triplet_loss(\n            F0,\n            F1,\n            pos_pairs,\n            num_pos=config.triplet_num_pos * config.batch_size,\n            num_hn_samples=config.triplet_num_hn * config.batch_size,\n            num_rand_triplet=config.triplet_num_rand * config.batch_size)\n        loss /= iter_size\n        loss.backward()\n        batch_loss += loss.item()\n        pos_dist_meter.update(pos_dist)\n        neg_dist_meter.update(neg_dist)\n\n      self.optimizer.step()\n      gc.collect()\n\n      torch.cuda.empty_cache()\n\n      total_loss += batch_loss\n      total_num += 1.0\n      total_timer.toc()\n      data_meter.update(data_time)\n\n      if curr_iter % self.config.stat_freq == 0:\n        self.writer.add_scalar(\'train/loss\', batch_loss, start_iter + curr_iter)\n        logging.info(\n            ""Train Epoch: {} [{}/{}], Current Loss: {:.3e}, Pos dist: {:.3e}, Neg dist: {:.3e}""\n            .format(epoch, curr_iter,\n                    len(self.data_loader) //\n                    iter_size, batch_loss, pos_dist_meter.avg, neg_dist_meter.avg) +\n            ""\\tData time: {:.4f}, Train time: {:.4f}, Iter time: {:.4f}"".format(\n                data_meter.avg, total_timer.avg - data_meter.avg, total_timer.avg))\n        pos_dist_meter.reset()\n        neg_dist_meter.reset()\n        data_meter.reset()\n        total_timer.reset()\n\n\nclass HardestTripletLossTrainer(TripletLossTrainer):\n\n  def triplet_loss(self,\n                   F0,\n                   F1,\n                   positive_pairs,\n                   num_pos=1024,\n                   num_hn_samples=512,\n                   num_rand_triplet=1024):\n    """"""\n    Generate negative pairs\n    """"""\n    N0, N1 = len(F0), len(F1)\n    num_pos_pairs = len(positive_pairs)\n    hash_seed = max(N0, N1)\n    sel0 = np.random.choice(N0, min(N0, num_hn_samples), replace=False)\n    sel1 = np.random.choice(N1, min(N1, num_hn_samples), replace=False)\n\n    if num_pos_pairs > num_pos:\n      pos_sel = np.random.choice(num_pos_pairs, num_pos, replace=False)\n      sample_pos_pairs = positive_pairs[pos_sel]\n    else:\n      sample_pos_pairs = positive_pairs\n\n    # Find negatives for all F1[positive_pairs[:, 1]]\n    subF0, subF1 = F0[sel0], F1[sel1]\n\n    pos_ind0 = sample_pos_pairs[:, 0].long()\n    pos_ind1 = sample_pos_pairs[:, 1].long()\n    posF0, posF1 = F0[pos_ind0], F1[pos_ind1]\n\n    D01 = pdist(posF0, subF1, dist_type=\'L2\')\n    D10 = pdist(posF1, subF0, dist_type=\'L2\')\n\n    D01min, D01ind = D01.min(1)\n    D10min, D10ind = D10.min(1)\n\n    if not isinstance(positive_pairs, np.ndarray):\n      positive_pairs = np.array(positive_pairs, dtype=np.int64)\n\n    pos_keys = _hash(positive_pairs, hash_seed)\n\n    D01ind = sel1[D01ind.cpu().numpy()]\n    D10ind = sel0[D10ind.cpu().numpy()]\n    neg_keys0 = _hash([pos_ind0.numpy(), D01ind], hash_seed)\n    neg_keys1 = _hash([D10ind, pos_ind1.numpy()], hash_seed)\n\n    mask0 = torch.from_numpy(\n        np.logical_not(np.isin(neg_keys0, pos_keys, assume_unique=False)))\n    mask1 = torch.from_numpy(\n        np.logical_not(np.isin(neg_keys1, pos_keys, assume_unique=False)))\n    pos_dist = torch.sqrt((posF0 - posF1).pow(2).sum(1) + 1e-7)\n\n    # Random triplets\n    rand_inds = np.random.choice(\n        num_pos_pairs, min(num_pos_pairs, num_rand_triplet), replace=False)\n    rand_pairs = positive_pairs[rand_inds]\n    negatives = np.random.choice(N1, min(N1, num_rand_triplet), replace=False)\n\n    # Remove positives from negatives\n    rand_neg_keys = _hash([rand_pairs[:, 0], negatives], hash_seed)\n    rand_mask = np.logical_not(np.isin(rand_neg_keys, pos_keys, assume_unique=False))\n    anchors, positives = rand_pairs[torch.from_numpy(rand_mask)].T\n    negatives = negatives[rand_mask]\n\n    rand_pos_dist = torch.sqrt((F0[anchors] - F1[positives]).pow(2).sum(1) + 1e-7)\n    rand_neg_dist = torch.sqrt((F0[anchors] - F1[negatives]).pow(2).sum(1) + 1e-7)\n\n    loss = F.relu(\n        torch.cat([\n            rand_pos_dist + self.neg_thresh - rand_neg_dist,\n            pos_dist[mask0] + self.neg_thresh - D01min[mask0],\n            pos_dist[mask1] + self.neg_thresh - D10min[mask1]\n        ])).mean()\n\n    return loss, pos_dist.mean(), (D01min.mean() + D10min.mean()).item() / 2\n'"
lib/transforms.py,1,"b'import random\nimport numpy as np\n\nimport torch\n\n\nclass Compose:\n\n  def __init__(self, transforms):\n    self.transforms = transforms\n\n  def __call__(self, coords, feats):\n    for transform in self.transforms:\n      coords, feats = transform(coords, feats)\n    return coords, feats\n\n\nclass Jitter:\n\n  def __init__(self, mu=0, sigma=0.01):\n    self.mu = mu\n    self.sigma = sigma\n\n  def __call__(self, coords, feats):\n    if random.random() < 0.95:\n      if isinstance(feats, np.ndarray):\n        feats += np.random.normal(self.mu, self.sigma, (feats.shape[0], feats.shape[1]))\n      else:\n        feats += (torch.randn_like(feats) * self.sigma) + self.mu\n    return coords, feats\n\n\nclass ChromaticShift:\n\n  def __init__(self, mu=0, sigma=0.1):\n    self.mu = mu\n    self.sigma = sigma\n\n  def __call__(self, coords, feats):\n    if random.random() < 0.95:\n      feats[:, :3] += np.random.normal(self.mu, self.sigma, (1, 3))\n    return coords, feats\n'"
model/__init__.py,0,"b""import logging\nimport model.simpleunet as simpleunets\nimport model.resunet as resunets\n\nMODELS = []\n\n\ndef add_models(module):\n  MODELS.extend([getattr(module, a) for a in dir(module) if 'Net' in a or 'MLP' in a])\n\n\nadd_models(simpleunets)\nadd_models(resunets)\n\n\ndef load_model(name):\n  '''Creates and returns an instance of the model given its class name.\n  '''\n  # Find the model class from its name\n  all_models = MODELS\n  mdict = {model.__name__: model for model in all_models}\n  if name not in mdict:\n    logging.info(f'Invalid model index. You put {name}. Options are:')\n    # Display a list of valid model names\n    for model in all_models:\n      logging.info('\\t* {}'.format(model.__name__))\n    return None\n  NetClass = mdict[name]\n\n  return NetClass\n"""
model/common.py,0,"b""import MinkowskiEngine as ME\n\n\ndef get_norm(norm_type, num_feats, bn_momentum=0.05, D=-1):\n  if norm_type == 'BN':\n    return ME.MinkowskiBatchNorm(num_feats, momentum=bn_momentum)\n  elif norm_type == 'IN':\n    return ME.MinkowskiInstanceNorm(num_feats, dimension=D)\n  else:\n    raise ValueError(f'Type {norm_type}, not defined')\n"""
model/residual_block.py,1,"b""import torch.nn as nn\n\nfrom model.common import get_norm\n\nimport MinkowskiEngine as ME\nimport MinkowskiEngine.MinkowskiFunctional as MEF\n\n\nclass BasicBlockBase(nn.Module):\n  expansion = 1\n  NORM_TYPE = 'BN'\n\n  def __init__(self,\n               inplanes,\n               planes,\n               stride=1,\n               dilation=1,\n               downsample=None,\n               bn_momentum=0.1,\n               D=3):\n    super(BasicBlockBase, self).__init__()\n\n    self.conv1 = ME.MinkowskiConvolution(\n        inplanes, planes, kernel_size=3, stride=stride, dimension=D)\n    self.norm1 = get_norm(self.NORM_TYPE, planes, bn_momentum=bn_momentum, D=D)\n    self.conv2 = ME.MinkowskiConvolution(\n        planes,\n        planes,\n        kernel_size=3,\n        stride=1,\n        dilation=dilation,\n        has_bias=False,\n        dimension=D)\n    self.norm2 = get_norm(self.NORM_TYPE, planes, bn_momentum=bn_momentum, D=D)\n    self.downsample = downsample\n\n  def forward(self, x):\n    residual = x\n\n    out = self.conv1(x)\n    out = self.norm1(out)\n    out = MEF.relu(out)\n\n    out = self.conv2(out)\n    out = self.norm2(out)\n\n    if self.downsample is not None:\n      residual = self.downsample(x)\n\n    out += residual\n    out = MEF.relu(out)\n\n    return out\n\n\nclass BasicBlockBN(BasicBlockBase):\n  NORM_TYPE = 'BN'\n\n\nclass BasicBlockIN(BasicBlockBase):\n  NORM_TYPE = 'IN'\n\n\ndef get_block(norm_type,\n              inplanes,\n              planes,\n              stride=1,\n              dilation=1,\n              downsample=None,\n              bn_momentum=0.1,\n              D=3):\n  if norm_type == 'BN':\n    return BasicBlockBN(inplanes, planes, stride, dilation, downsample, bn_momentum, D)\n  elif norm_type == 'IN':\n    return BasicBlockIN(inplanes, planes, stride, dilation, downsample, bn_momentum, D)\n  else:\n    raise ValueError(f'Type {norm_type}, not defined')\n"""
model/resunet.py,1,"b""# -*- coding: future_fstrings -*-\nimport torch\nimport MinkowskiEngine as ME\nimport MinkowskiEngine.MinkowskiFunctional as MEF\nfrom model.common import get_norm\n\nfrom model.residual_block import get_block\n\n\nclass ResUNet2(ME.MinkowskiNetwork):\n  NORM_TYPE = None\n  BLOCK_NORM_TYPE = 'BN'\n  CHANNELS = [None, 32, 64, 128, 256]\n  TR_CHANNELS = [None, 32, 64, 64, 128]\n\n  # To use the model, must call initialize_coords before forward pass.\n  # Once data is processed, call clear to reset the model before calling initialize_coords\n  def __init__(self,\n               in_channels=3,\n               out_channels=32,\n               bn_momentum=0.1,\n               normalize_feature=None,\n               conv1_kernel_size=None,\n               D=3):\n    ME.MinkowskiNetwork.__init__(self, D)\n    NORM_TYPE = self.NORM_TYPE\n    BLOCK_NORM_TYPE = self.BLOCK_NORM_TYPE\n    CHANNELS = self.CHANNELS\n    TR_CHANNELS = self.TR_CHANNELS\n    self.normalize_feature = normalize_feature\n    self.conv1 = ME.MinkowskiConvolution(\n        in_channels=in_channels,\n        out_channels=CHANNELS[1],\n        kernel_size=conv1_kernel_size,\n        stride=1,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm1 = get_norm(NORM_TYPE, CHANNELS[1], bn_momentum=bn_momentum, D=D)\n\n    self.block1 = get_block(\n        BLOCK_NORM_TYPE, CHANNELS[1], CHANNELS[1], bn_momentum=bn_momentum, D=D)\n\n    self.conv2 = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[1],\n        out_channels=CHANNELS[2],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm2 = get_norm(NORM_TYPE, CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n    self.block2 = get_block(\n        BLOCK_NORM_TYPE, CHANNELS[2], CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n    self.conv3 = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[2],\n        out_channels=CHANNELS[3],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm3 = get_norm(NORM_TYPE, CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n    self.block3 = get_block(\n        BLOCK_NORM_TYPE, CHANNELS[3], CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n    self.conv4 = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[3],\n        out_channels=CHANNELS[4],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm4 = get_norm(NORM_TYPE, CHANNELS[4], bn_momentum=bn_momentum, D=D)\n\n    self.block4 = get_block(\n        BLOCK_NORM_TYPE, CHANNELS[4], CHANNELS[4], bn_momentum=bn_momentum, D=D)\n\n    self.conv4_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels=CHANNELS[4],\n        out_channels=TR_CHANNELS[4],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm4_tr = get_norm(NORM_TYPE, TR_CHANNELS[4], bn_momentum=bn_momentum, D=D)\n\n    self.block4_tr = get_block(\n        BLOCK_NORM_TYPE, TR_CHANNELS[4], TR_CHANNELS[4], bn_momentum=bn_momentum, D=D)\n\n    self.conv3_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels=CHANNELS[3] + TR_CHANNELS[4],\n        out_channels=TR_CHANNELS[3],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm3_tr = get_norm(NORM_TYPE, TR_CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n    self.block3_tr = get_block(\n        BLOCK_NORM_TYPE, TR_CHANNELS[3], TR_CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n    self.conv2_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels=CHANNELS[2] + TR_CHANNELS[3],\n        out_channels=TR_CHANNELS[2],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm2_tr = get_norm(NORM_TYPE, TR_CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n    self.block2_tr = get_block(\n        BLOCK_NORM_TYPE, TR_CHANNELS[2], TR_CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n    self.conv1_tr = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[1] + TR_CHANNELS[2],\n        out_channels=TR_CHANNELS[1],\n        kernel_size=1,\n        stride=1,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n\n    # self.block1_tr = BasicBlockBN(TR_CHANNELS[1], TR_CHANNELS[1], bn_momentum=bn_momentum, D=D)\n\n    self.final = ME.MinkowskiConvolution(\n        in_channels=TR_CHANNELS[1],\n        out_channels=out_channels,\n        kernel_size=1,\n        stride=1,\n        dilation=1,\n        has_bias=True,\n        dimension=D)\n\n  def forward(self, x):\n    out_s1 = self.conv1(x)\n    out_s1 = self.norm1(out_s1)\n    out_s1 = self.block1(out_s1)\n    out = MEF.relu(out_s1)\n\n    out_s2 = self.conv2(out)\n    out_s2 = self.norm2(out_s2)\n    out_s2 = self.block2(out_s2)\n    out = MEF.relu(out_s2)\n\n    out_s4 = self.conv3(out)\n    out_s4 = self.norm3(out_s4)\n    out_s4 = self.block3(out_s4)\n    out = MEF.relu(out_s4)\n\n    out_s8 = self.conv4(out)\n    out_s8 = self.norm4(out_s8)\n    out_s8 = self.block4(out_s8)\n    out = MEF.relu(out_s8)\n\n    out = self.conv4_tr(out)\n    out = self.norm4_tr(out)\n    out = self.block4_tr(out)\n    out_s4_tr = MEF.relu(out)\n\n    out = ME.cat(out_s4_tr, out_s4)\n\n    out = self.conv3_tr(out)\n    out = self.norm3_tr(out)\n    out = self.block3_tr(out)\n    out_s2_tr = MEF.relu(out)\n\n    out = ME.cat(out_s2_tr, out_s2)\n\n    out = self.conv2_tr(out)\n    out = self.norm2_tr(out)\n    out = self.block2_tr(out)\n    out_s1_tr = MEF.relu(out)\n\n    out = ME.cat(out_s1_tr, out_s1)\n    out = self.conv1_tr(out)\n    out = MEF.relu(out)\n    out = self.final(out)\n\n    if self.normalize_feature:\n      return ME.SparseTensor(\n          out.F / torch.norm(out.F, p=2, dim=1, keepdim=True),\n          coords_key=out.coords_key,\n          coords_manager=out.coords_man)\n    else:\n      return out\n\n\nclass ResUNetBN2(ResUNet2):\n  NORM_TYPE = 'BN'\n\n\nclass ResUNetBN2B(ResUNet2):\n  NORM_TYPE = 'BN'\n  CHANNELS = [None, 32, 64, 128, 256]\n  TR_CHANNELS = [None, 64, 64, 64, 64]\n\n\nclass ResUNetBN2C(ResUNet2):\n  NORM_TYPE = 'BN'\n  CHANNELS = [None, 32, 64, 128, 256]\n  TR_CHANNELS = [None, 64, 64, 64, 128]\n\n\nclass ResUNetBN2D(ResUNet2):\n  NORM_TYPE = 'BN'\n  CHANNELS = [None, 32, 64, 128, 256]\n  TR_CHANNELS = [None, 64, 64, 128, 128]\n\n\nclass ResUNetBN2E(ResUNet2):\n  NORM_TYPE = 'BN'\n  CHANNELS = [None, 128, 128, 128, 256]\n  TR_CHANNELS = [None, 64, 128, 128, 128]\n\n\nclass ResUNetIN2(ResUNet2):\n  NORM_TYPE = 'BN'\n  BLOCK_NORM_TYPE = 'IN'\n\n\nclass ResUNetIN2B(ResUNetBN2B):\n  NORM_TYPE = 'BN'\n  BLOCK_NORM_TYPE = 'IN'\n\n\nclass ResUNetIN2C(ResUNetBN2C):\n  NORM_TYPE = 'BN'\n  BLOCK_NORM_TYPE = 'IN'\n\n\nclass ResUNetIN2D(ResUNetBN2D):\n  NORM_TYPE = 'BN'\n  BLOCK_NORM_TYPE = 'IN'\n\n\nclass ResUNetIN2E(ResUNetBN2E):\n  NORM_TYPE = 'BN'\n  BLOCK_NORM_TYPE = 'IN'\n"""
model/simpleunet.py,3,"b""# -*- coding: future_fstrings -*-\nimport torch\nimport MinkowskiEngine as ME\nimport MinkowskiEngine.MinkowskiFunctional as MEF\nfrom model.common import get_norm\n\n\nclass SimpleNet(ME.MinkowskiNetwork):\n  NORM_TYPE = None\n  CHANNELS = [None, 32, 64, 128]\n  TR_CHANNELS = [None, 32, 32, 64]\n\n  # To use the model, must call initialize_coords before forward pass.\n  # Once data is processed, call clear to reset the model before calling initialize_coords\n  def __init__(self,\n               in_channels=3,\n               out_channels=32,\n               bn_momentum=0.1,\n               normalize_feature=None,\n               conv1_kernel_size=None,\n               D=3):\n    super(SimpleNet, self).__init__(D)\n    NORM_TYPE = self.NORM_TYPE\n    CHANNELS = self.CHANNELS\n    TR_CHANNELS = self.TR_CHANNELS\n    self.normalize_feature = normalize_feature\n    self.conv1 = ME.MinkowskiConvolution(\n        in_channels=in_channels,\n        out_channels=CHANNELS[1],\n        kernel_size=conv1_kernel_size,\n        stride=1,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm1 = get_norm(NORM_TYPE, CHANNELS[1], bn_momentum=bn_momentum, D=D)\n\n    self.conv2 = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[1],\n        out_channels=CHANNELS[2],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm2 = get_norm(NORM_TYPE, CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n    self.conv3 = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[2],\n        out_channels=CHANNELS[3],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm3 = get_norm(NORM_TYPE, CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n    self.conv3_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels=CHANNELS[3],\n        out_channels=TR_CHANNELS[3],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm3_tr = get_norm(NORM_TYPE, TR_CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n    self.conv2_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels=CHANNELS[2] + TR_CHANNELS[3],\n        out_channels=TR_CHANNELS[2],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm2_tr = get_norm(NORM_TYPE, TR_CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n    self.conv1_tr = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[1] + TR_CHANNELS[2],\n        out_channels=TR_CHANNELS[1],\n        kernel_size=3,\n        stride=1,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm1_tr = get_norm(NORM_TYPE, TR_CHANNELS[1], bn_momentum=bn_momentum, D=D)\n\n    self.final = ME.MinkowskiConvolution(\n        in_channels=TR_CHANNELS[1],\n        out_channels=out_channels,\n        kernel_size=1,\n        stride=1,\n        dilation=1,\n        has_bias=True,\n        dimension=D)\n\n  def forward(self, x):\n    out_s1 = self.conv1(x)\n    out_s1 = self.norm1(out_s1)\n    out = MEF.relu(out_s1)\n\n    out_s2 = self.conv2(out)\n    out_s2 = self.norm2(out_s2)\n    out = MEF.relu(out_s2)\n\n    out_s4 = self.conv3(out)\n    out_s4 = self.norm3(out_s4)\n    out = MEF.relu(out_s4)\n\n    out = self.conv3_tr(out)\n    out = self.norm3_tr(out)\n    out_s2_tr = MEF.relu(out)\n\n    out = ME.cat(out_s2_tr, out_s2)\n\n    out = self.conv2_tr(out)\n    out = self.norm2_tr(out)\n    out_s1_tr = MEF.relu(out)\n\n    out = ME.cat(out_s1_tr, out_s1)\n    out = self.conv1_tr(out)\n    out = self.norm1_tr(out)\n    out = MEF.relu(out)\n\n    out = self.final(out)\n\n    if self.normalize_feature:\n      return ME.SparseTensor(\n          out.F / torch.norm(out.F, p=2, dim=1, keepdim=True),\n          coords_key=out.coords_key,\n          coords_manager=out.coords_man)\n    else:\n      return out\n\n\nclass SimpleNetIN(SimpleNet):\n  NORM_TYPE = 'IN'\n\n\nclass SimpleNetBN(SimpleNet):\n  NORM_TYPE = 'BN'\n\n\nclass SimpleNetBNE(SimpleNetBN):\n  CHANNELS = [None, 16, 32, 32]\n  TR_CHANNELS = [None, 16, 16, 32]\n\n\nclass SimpleNetINE(SimpleNetBNE):\n  NORM_TYPE = 'IN'\n\n\nclass SimpleNet2(ME.MinkowskiNetwork):\n  NORM_TYPE = None\n  CHANNELS = [None, 32, 64, 128, 256]\n  TR_CHANNELS = [None, 32, 32, 64, 64]\n\n  # To use the model, must call initialize_coords before forward pass.\n  # Once data is processed, call clear to reset the model before calling initialize_coords\n  def __init__(self, in_channels=3, out_channels=32, bn_momentum=0.1, D=3, config=None):\n    ME.MinkowskiNetwork.__init__(self, D)\n    NORM_TYPE = self.NORM_TYPE\n    bn_momentum = config.bn_momentum\n    CHANNELS = self.CHANNELS\n    TR_CHANNELS = self.TR_CHANNELS\n    self.normalize_feature = config.normalize_feature\n    self.conv1 = ME.MinkowskiConvolution(\n        in_channels=in_channels,\n        out_channels=CHANNELS[1],\n        kernel_size=config.conv1_kernel_size,\n        stride=1,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm1 = get_norm(NORM_TYPE, CHANNELS[1], bn_momentum=bn_momentum, D=D)\n\n    self.conv2 = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[1],\n        out_channels=CHANNELS[2],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm2 = get_norm(NORM_TYPE, CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n    self.conv3 = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[2],\n        out_channels=CHANNELS[3],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm3 = get_norm(NORM_TYPE, CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n    self.conv4 = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[3],\n        out_channels=CHANNELS[4],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm4 = get_norm(NORM_TYPE, CHANNELS[4], bn_momentum=bn_momentum, D=D)\n\n    self.conv4_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels=CHANNELS[4],\n        out_channels=TR_CHANNELS[4],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm4_tr = get_norm(NORM_TYPE, TR_CHANNELS[4], bn_momentum=bn_momentum, D=D)\n\n    self.conv3_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels=CHANNELS[3] + TR_CHANNELS[4],\n        out_channels=TR_CHANNELS[3],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm3_tr = get_norm(NORM_TYPE, TR_CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n    self.conv2_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels=CHANNELS[2] + TR_CHANNELS[3],\n        out_channels=TR_CHANNELS[2],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm2_tr = get_norm(NORM_TYPE, TR_CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n    self.conv1_tr = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[1] + TR_CHANNELS[2],\n        out_channels=TR_CHANNELS[1],\n        kernel_size=3,\n        stride=1,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm1_tr = get_norm(NORM_TYPE, TR_CHANNELS[1], bn_momentum=bn_momentum, D=D)\n\n    self.final = ME.MinkowskiConvolution(\n        in_channels=TR_CHANNELS[1],\n        out_channels=out_channels,\n        kernel_size=1,\n        stride=1,\n        dilation=1,\n        has_bias=True,\n        dimension=D)\n\n  def forward(self, x):\n    out_s1 = self.conv1(x)\n    out_s1 = self.norm1(out_s1)\n    out = MEF.relu(out_s1)\n\n    out_s2 = self.conv2(out)\n    out_s2 = self.norm2(out_s2)\n    out = MEF.relu(out_s2)\n\n    out_s4 = self.conv3(out)\n    out_s4 = self.norm3(out_s4)\n    out = MEF.relu(out_s4)\n\n    out_s8 = self.conv4(out)\n    out_s8 = self.norm4(out_s8)\n    out = MEF.relu(out_s8)\n\n    out = self.conv4_tr(out)\n    out = self.norm4_tr(out)\n    out_s4_tr = MEF.relu(out)\n\n    out = ME.cat(out_s4_tr, out_s4)\n\n    out = self.conv3_tr(out)\n    out = self.norm3_tr(out)\n    out_s2_tr = MEF.relu(out)\n\n    out = ME.cat(out_s2_tr, out_s2)\n\n    out = self.conv2_tr(out)\n    out = self.norm2_tr(out)\n    out_s1_tr = MEF.relu(out)\n\n    out = ME.cat(out_s1_tr, out_s1)\n    out = self.conv1_tr(out)\n    out = self.norm1_tr(out)\n    out = MEF.relu(out)\n\n    out = self.final(out)\n\n    if self.normalize_feature:\n      return ME.SparseTensor(\n          out.F / torch.norm(out.F, p=2, dim=1, keepdim=True),\n          coords_key=out.coords_key,\n          coords_manager=out.coords_man)\n    else:\n      return out\n\n\nclass SimpleNetIN2(SimpleNet2):\n  NORM_TYPE = 'IN'\n\n\nclass SimpleNetBN2(SimpleNet2):\n  NORM_TYPE = 'BN'\n\n\nclass SimpleNetBN2B(SimpleNet2):\n  NORM_TYPE = 'BN'\n  CHANNELS = [None, 32, 64, 128, 256]\n  TR_CHANNELS = [None, 64, 64, 64, 64]\n\n\nclass SimpleNetBN2C(SimpleNet2):\n  NORM_TYPE = 'BN'\n  CHANNELS = [None, 32, 64, 128, 256]\n  TR_CHANNELS = [None, 32, 64, 64, 128]\n\n\nclass SimpleNetBN2D(SimpleNet2):\n  NORM_TYPE = 'BN'\n  CHANNELS = [None, 32, 64, 128, 256]\n  TR_CHANNELS = [None, 32, 64, 64, 128]\n\n\nclass SimpleNetBN2E(SimpleNet2):\n  NORM_TYPE = 'BN'\n  CHANNELS = [None, 16, 32, 64, 128]\n  TR_CHANNELS = [None, 16, 32, 32, 64]\n\n\nclass SimpleNetIN2E(SimpleNetBN2E):\n  NORM_TYPE = 'IN'\n\n\nclass SimpleNet3(ME.MinkowskiNetwork):\n  NORM_TYPE = None\n  CHANNELS = [None, 32, 64, 128, 256, 512]\n  TR_CHANNELS = [None, 32, 32, 64, 64, 128]\n\n  # To use the model, must call initialize_coords before forward pass.\n  # Once data is processed, call clear to reset the model before calling initialize_coords\n  def __init__(self, in_channels=3, out_channels=32, bn_momentum=0.1, D=3, config=None):\n    ME.MinkowskiNetwork.__init__(self, D)\n    NORM_TYPE = self.NORM_TYPE\n    bn_momentum = config.bn_momentum\n    CHANNELS = self.CHANNELS\n    TR_CHANNELS = self.TR_CHANNELS\n    self.normalize_feature = config.normalize_feature\n    self.conv1 = ME.MinkowskiConvolution(\n        in_channels=in_channels,\n        out_channels=CHANNELS[1],\n        kernel_size=config.conv1_kernel_size,\n        stride=1,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm1 = get_norm(NORM_TYPE, CHANNELS[1], bn_momentum=bn_momentum, D=D)\n\n    self.conv2 = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[1],\n        out_channels=CHANNELS[2],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm2 = get_norm(NORM_TYPE, CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n    self.conv3 = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[2],\n        out_channels=CHANNELS[3],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm3 = get_norm(NORM_TYPE, CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n    self.conv4 = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[3],\n        out_channels=CHANNELS[4],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm4 = get_norm(NORM_TYPE, CHANNELS[4], bn_momentum=bn_momentum, D=D)\n\n    self.conv5 = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[4],\n        out_channels=CHANNELS[5],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm5 = get_norm(NORM_TYPE, CHANNELS[5], bn_momentum=bn_momentum, D=D)\n\n    self.conv5_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels=CHANNELS[5],\n        out_channels=TR_CHANNELS[5],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm5_tr = get_norm(NORM_TYPE, TR_CHANNELS[5], bn_momentum=bn_momentum, D=D)\n\n    self.conv4_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels=CHANNELS[4] + TR_CHANNELS[5],\n        out_channels=TR_CHANNELS[4],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm4_tr = get_norm(NORM_TYPE, TR_CHANNELS[4], bn_momentum=bn_momentum, D=D)\n\n    self.conv3_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels=CHANNELS[3] + TR_CHANNELS[4],\n        out_channels=TR_CHANNELS[3],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm3_tr = get_norm(NORM_TYPE, TR_CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n    self.conv2_tr = ME.MinkowskiConvolutionTranspose(\n        in_channels=CHANNELS[2] + TR_CHANNELS[3],\n        out_channels=TR_CHANNELS[2],\n        kernel_size=3,\n        stride=2,\n        dilation=1,\n        has_bias=False,\n        dimension=D)\n    self.norm2_tr = get_norm(NORM_TYPE, TR_CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n    self.conv1_tr = ME.MinkowskiConvolution(\n        in_channels=CHANNELS[1] + TR_CHANNELS[2],\n        out_channels=TR_CHANNELS[1],\n        kernel_size=1,\n        stride=1,\n        dilation=1,\n        has_bias=True,\n        dimension=D)\n\n  def forward(self, x):\n    out_s1 = self.conv1(x)\n    out_s1 = self.norm1(out_s1)\n    out = MEF.relu(out_s1)\n\n    out_s2 = self.conv2(out)\n    out_s2 = self.norm2(out_s2)\n    out = MEF.relu(out_s2)\n\n    out_s4 = self.conv3(out)\n    out_s4 = self.norm3(out_s4)\n    out = MEF.relu(out_s4)\n\n    out_s8 = self.conv4(out)\n    out_s8 = self.norm4(out_s8)\n    out = MEF.relu(out_s8)\n\n    out_s16 = self.conv5(out)\n    out_s16 = self.norm5(out_s16)\n    out = MEF.relu(out_s16)\n\n    out = self.conv5_tr(out)\n    out = self.norm5_tr(out)\n    out_s8_tr = MEF.relu(out)\n\n    out = ME.cat(out_s8_tr, out_s8)\n\n    out = self.conv4_tr(out)\n    out = self.norm4_tr(out)\n    out_s4_tr = MEF.relu(out)\n\n    out = ME.cat(out_s4_tr, out_s4)\n\n    out = self.conv3_tr(out)\n    out = self.norm3_tr(out)\n    out_s2_tr = MEF.relu(out)\n\n    out = ME.cat(out_s2_tr, out_s2)\n\n    out = self.conv2_tr(out)\n    out = self.norm2_tr(out)\n    out_s1_tr = MEF.relu(out)\n\n    out = ME.cat(out_s1_tr, out_s1)\n    out = self.conv1_tr(out)\n\n    if self.normalize_feature:\n      return ME.SparseTensor(\n          out.F / torch.norm(out.F, p=2, dim=1, keepdim=True),\n          coords_key=out.coords_key,\n          coords_manager=out.coords_man)\n    else:\n      return out\n\n\nclass SimpleNetIN3(SimpleNet3):\n  NORM_TYPE = 'IN'\n\n\nclass SimpleNetBN3(SimpleNet3):\n  NORM_TYPE = 'BN'\n\n\nclass SimpleNetBN3B(SimpleNet3):\n  NORM_TYPE = 'BN'\n  CHANNELS = [None, 32, 64, 128, 256, 512]\n  TR_CHANNELS = [None, 32, 64, 64, 64, 128]\n\n\nclass SimpleNetBN3C(SimpleNet3):\n  NORM_TYPE = 'BN'\n  CHANNELS = [None, 32, 64, 128, 256, 512]\n  TR_CHANNELS = [None, 32, 32, 64, 128, 128]\n\n\nclass SimpleNetBN3D(SimpleNet3):\n  NORM_TYPE = 'BN'\n  CHANNELS = [None, 32, 64, 128, 256, 512]\n  TR_CHANNELS = [None, 32, 64, 64, 128, 128]\n\n\nclass SimpleNetBN3E(SimpleNet3):\n  NORM_TYPE = 'BN'\n  CHANNELS = [None, 16, 32, 64, 128, 256]\n  TR_CHANNELS = [None, 16, 32, 32, 64, 128]\n\n\nclass SimpleNetIN3E(SimpleNetBN3E):\n  NORM_TYPE = 'IN'\n"""
scripts/benchmark_3dmatch.py,5,"b'""""""\nA collection of unrefactored functions.\n""""""\nimport os\nimport sys\nimport numpy as np\nimport argparse\nimport logging\nimport open3d as o3d\n\nfrom lib.timer import Timer, AverageMeter\n\nfrom util.misc import extract_features\n\nfrom model import load_model\nfrom util.file import ensure_dir, get_folder_list, get_file_list\nfrom util.trajectory import read_trajectory, write_trajectory\nfrom util.pointcloud import make_open3d_point_cloud, evaluate_feature_3dmatch\nfrom scripts.benchmark_util import do_single_pair_matching, gen_matching_pair, gather_results\n\nimport torch\n\nimport MinkowskiEngine as ME\n\nch = logging.StreamHandler(sys.stdout)\nlogging.getLogger().setLevel(logging.INFO)\nlogging.basicConfig(\n    format=\'%(asctime)s %(message)s\', datefmt=\'%m/%d %H:%M:%S\', handlers=[ch])\n\no3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)\n\n\ndef extract_features_batch(model, config, source_path, target_path, voxel_size, device):\n\n  folders = get_folder_list(source_path)\n  assert len(folders) > 0, f""Could not find 3DMatch folders under {source_path}""\n  logging.info(folders)\n  list_file = os.path.join(target_path, ""list.txt"")\n  f = open(list_file, ""w"")\n  timer, tmeter = Timer(), AverageMeter()\n  num_feat = 0\n  model.eval()\n\n  for fo in folders:\n    if \'evaluation\' in fo:\n      continue\n    files = get_file_list(fo, "".ply"")\n    fo_base = os.path.basename(fo)\n    f.write(""%s %d\\n"" % (fo_base, len(files)))\n    for i, fi in enumerate(files):\n      # Extract features from a file\n      pcd = o3d.io.read_point_cloud(fi)\n      save_fn = ""%s_%03d"" % (fo_base, i)\n      if i % 100 == 0:\n        logging.info(f""{i} / {len(files)}: {save_fn}"")\n\n      timer.tic()\n      xyz_down, feature = extract_features(\n          model,\n          xyz=np.array(pcd.points),\n          rgb=None,\n          normal=None,\n          voxel_size=voxel_size,\n          device=device,\n          skip_check=True)\n      t = timer.toc()\n      if i > 0:\n        tmeter.update(t)\n        num_feat += len(xyz_down)\n\n      np.savez_compressed(\n          os.path.join(target_path, save_fn),\n          points=np.array(pcd.points),\n          xyz=xyz_down,\n          feature=feature.detach().cpu().numpy())\n      if i % 20 == 0 and i > 0:\n        logging.info(\n            f\'Average time: {tmeter.avg}, FPS: {num_feat / tmeter.sum}, time / feat: {tmeter.sum / num_feat}, \'\n        )\n\n  f.close()\n\n\ndef registration(feature_path, voxel_size):\n  """"""\n  Gather .log files produced in --target folder and run this Matlab script\n  https://github.com/andyzeng/3dmatch-toolbox#geometric-registration-benchmark\n  (see Geometric Registration Benchmark section in\n  http://3dmatch.cs.princeton.edu/)\n  """"""\n  # List file from the extract_features_batch function\n  with open(os.path.join(feature_path, ""list.txt"")) as f:\n    sets = f.readlines()\n    sets = [x.strip().split() for x in sets]\n  for s in sets:\n    set_name = s[0]\n    pts_num = int(s[1])\n    matching_pairs = gen_matching_pair(pts_num)\n    results = []\n    for m in matching_pairs:\n      results.append(do_single_pair_matching(feature_path, set_name, m, voxel_size))\n    traj = gather_results(results)\n    logging.info(f""Writing the trajectory to {feature_path}/{set_name}.log"")\n    write_trajectory(traj, ""%s.log"" % (os.path.join(feature_path, set_name)))\n\n\ndef do_single_pair_evaluation(feature_path,\n                              set_name,\n                              traj,\n                              voxel_size,\n                              tau_1=0.1,\n                              tau_2=0.05,\n                              num_rand_keypoints=-1):\n  trans_gth = np.linalg.inv(traj.pose)\n  i = traj.metadata[0]\n  j = traj.metadata[1]\n  name_i = ""%s_%03d"" % (set_name, i)\n  name_j = ""%s_%03d"" % (set_name, j)\n\n  # coord and feat form a sparse tensor.\n  data_i = np.load(os.path.join(feature_path, name_i + "".npz""))\n  coord_i, points_i, feat_i = data_i[\'xyz\'], data_i[\'points\'], data_i[\'feature\']\n  data_j = np.load(os.path.join(feature_path, name_j + "".npz""))\n  coord_j, points_j, feat_j = data_j[\'xyz\'], data_j[\'points\'], data_j[\'feature\']\n\n  # use the keypoints in 3DMatch\n  if num_rand_keypoints > 0:\n    # Randomly subsample N points\n    Ni, Nj = len(points_i), len(points_j)\n    inds_i = np.random.choice(Ni, min(Ni, num_rand_keypoints), replace=False)\n    inds_j = np.random.choice(Nj, min(Nj, num_rand_keypoints), replace=False)\n\n    sample_i, sample_j = points_i[inds_i], points_j[inds_j]\n\n    key_points_i = ME.utils.fnv_hash_vec(np.floor(sample_i / voxel_size))\n    key_points_j = ME.utils.fnv_hash_vec(np.floor(sample_j / voxel_size))\n\n    key_coords_i = ME.utils.fnv_hash_vec(np.floor(coord_i / voxel_size))\n    key_coords_j = ME.utils.fnv_hash_vec(np.floor(coord_j / voxel_size))\n\n    inds_i = np.where(np.isin(key_coords_i, key_points_i))[0]\n    inds_j = np.where(np.isin(key_coords_j, key_points_j))[0]\n\n    coord_i, feat_i = coord_i[inds_i], feat_i[inds_i]\n    coord_j, feat_j = coord_j[inds_j], feat_j[inds_j]\n\n  coord_i = make_open3d_point_cloud(coord_i)\n  coord_j = make_open3d_point_cloud(coord_j)\n\n  hit_ratio = evaluate_feature_3dmatch(coord_i, coord_j, feat_i, feat_j, trans_gth,\n                                       tau_1)\n\n  # logging.info(f""Hit ratio of {name_i}, {name_j}: {hit_ratio}, {hit_ratio >= tau_2}"")\n  if hit_ratio >= tau_2:\n    return True\n  else:\n    return False\n\n\ndef feature_evaluation(source_path, feature_path, voxel_size, num_rand_keypoints=-1):\n  with open(os.path.join(feature_path, ""list.txt"")) as f:\n    sets = f.readlines()\n    sets = [x.strip().split() for x in sets]\n\n  assert len(\n      sets\n  ) > 0, ""Empty list file. Makesure to run the feature extraction first with --do_extract_feature.""\n\n  tau_1 = 0.1  # 10cm\n  tau_2 = 0.05  # 5% inlier\n  logging.info(""%f %f"" % (tau_1, tau_2))\n  recall = []\n  for s in sets:\n    set_name = s[0]\n    traj = read_trajectory(os.path.join(source_path, set_name + ""_gt.log""))\n    assert len(traj) > 0, ""Empty trajectory file""\n    results = []\n    for i in range(len(traj)):\n      results.append(\n          do_single_pair_evaluation(feature_path, set_name, traj[i], voxel_size, tau_1,\n                                    tau_2, num_rand_keypoints))\n\n    mean_recall = np.array(results).mean()\n    std_recall = np.array(results).std()\n    recall.append([set_name, mean_recall, std_recall])\n    logging.info(f\'{set_name}: {mean_recall} +- {std_recall}\')\n  for r in recall:\n    logging.info(""%s : %.4f"" % (r[0], r[1]))\n  scene_r = np.array([r[1] for r in recall])\n  logging.info(""average : %.4f +- %.4f"" % (scene_r.mean(), scene_r.std()))\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'--source\', default=None, type=str, help=\'path to 3dmatch test dataset\')\n  parser.add_argument(\n      \'--source_high_res\',\n      default=None,\n      type=str,\n      help=\'path to high_resolution point cloud\')\n  parser.add_argument(\n      \'--target\', default=None, type=str, help=\'path to produce generated data\')\n  parser.add_argument(\n      \'-m\',\n      \'--model\',\n      default=None,\n      type=str,\n      help=\'path to latest checkpoint (default: None)\')\n  parser.add_argument(\n      \'--voxel_size\',\n      default=0.05,\n      type=float,\n      help=\'voxel size to preprocess point cloud\')\n  parser.add_argument(\'--extract_features\', action=\'store_true\')\n  parser.add_argument(\'--evaluate_feature_match_recall\', action=\'store_true\')\n  parser.add_argument(\n      \'--evaluate_registration\',\n      action=\'store_true\',\n      help=\'The target directory must contain extracted features\')\n  parser.add_argument(\'--with_cuda\', action=\'store_true\')\n  parser.add_argument(\n      \'--num_rand_keypoints\',\n      type=int,\n      default=5000,\n      help=\'Number of random keypoints for each scene\')\n\n  args = parser.parse_args()\n\n  device = torch.device(\'cuda\' if args.with_cuda else \'cpu\')\n\n  if args.extract_features:\n    assert args.model is not None\n    assert args.source is not None\n    assert args.target is not None\n\n    ensure_dir(args.target)\n    checkpoint = torch.load(args.model)\n    config = checkpoint[\'config\']\n\n    num_feats = 1\n    Model = load_model(config.model)\n    model = Model(\n        num_feats,\n        config.model_n_out,\n        bn_momentum=0.05,\n        normalize_feature=config.normalize_feature,\n        conv1_kernel_size=config.conv1_kernel_size,\n        D=3)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n    model.eval()\n\n    model = model.to(device)\n\n    with torch.no_grad():\n      extract_features_batch(model, config, args.source, args.target, config.voxel_size,\n                             device)\n\n  if args.evaluate_feature_match_recall:\n    assert (args.target is not None)\n    with torch.no_grad():\n      feature_evaluation(args.source, args.target, args.voxel_size,\n                         args.num_rand_keypoints)\n\n  if args.evaluate_registration:\n    assert (args.target is not None)\n    with torch.no_grad():\n      registration(args.target, args.voxel_size)\n'"
scripts/benchmark_util.py,0,"b'import open3d as o3d\nimport os\nimport logging\nimport numpy as np\n\nfrom util.trajectory import CameraPose\nfrom util.pointcloud import compute_overlap_ratio, \\\n    make_open3d_point_cloud, make_open3d_feature_from_numpy\n\n\ndef run_ransac(xyz0, xyz1, feat0, feat1, voxel_size):\n  distance_threshold = voxel_size * 1.5\n  result_ransac = o3d.registration.registration_ransac_based_on_feature_matching(\n      xyz0, xyz1, feat0, feat1, distance_threshold,\n      o3d.registration.TransformationEstimationPointToPoint(False), 4, [\n          o3d.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n          o3d.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)\n      ], o3d.registration.RANSACConvergenceCriteria(4000000, 500))\n  return result_ransac.transformation\n\n\ndef gather_results(results):\n  traj = []\n  for r in results:\n    success = r[0]\n    if success:\n      traj.append(CameraPose([r[1], r[2], r[3]], r[4]))\n  return traj\n\n\ndef gen_matching_pair(pts_num):\n  matching_pairs = []\n  for i in range(pts_num):\n    for j in range(i + 1, pts_num):\n      matching_pairs.append([i, j, pts_num])\n  return matching_pairs\n\n\ndef read_data(feature_path, name):\n  data = np.load(os.path.join(feature_path, name + "".npz""))\n  xyz = make_open3d_point_cloud(data[\'xyz\'])\n  feat = make_open3d_feature_from_numpy(data[\'feature\'])\n  return data[\'points\'], xyz, feat\n\n\ndef do_single_pair_matching(feature_path, set_name, m, voxel_size):\n  i, j, s = m\n  name_i = ""%s_%03d"" % (set_name, i)\n  name_j = ""%s_%03d"" % (set_name, j)\n  logging.info(""matching %s %s"" % (name_i, name_j))\n  points_i, xyz_i, feat_i = read_data(feature_path, name_i)\n  points_j, xyz_j, feat_j = read_data(feature_path, name_j)\n  if len(xyz_i.points) < len(xyz_j.points):\n    trans = run_ransac(xyz_i, xyz_j, feat_i, feat_j, voxel_size)\n  else:\n    trans = run_ransac(xyz_j, xyz_i, feat_j, feat_i, voxel_size)\n    trans = np.linalg.inv(trans)\n  ratio = compute_overlap_ratio(xyz_i, xyz_j, trans, voxel_size)\n  logging.info(f""{ratio}"")\n  if ratio > 0.3:\n    return [True, i, j, s, np.linalg.inv(trans)]\n  else:\n    return [False, i, j, s, np.identity(4)]\n'"
scripts/test_kitti.py,4,"b'import open3d as o3d  # prevent loading error\n\nimport sys\nimport logging\nimport json\nimport argparse\nimport numpy as np\nfrom easydict import EasyDict as edict\n\nimport torch\nfrom model import load_model\n\nfrom lib.data_loaders import make_data_loader\nfrom util.pointcloud import make_open3d_point_cloud, make_open3d_feature\nfrom lib.timer import AverageMeter, Timer\n\nimport MinkowskiEngine as ME\n\nch = logging.StreamHandler(sys.stdout)\nlogging.getLogger().setLevel(logging.INFO)\nlogging.basicConfig(\n    format=\'%(asctime)s %(message)s\', datefmt=\'%m/%d %H:%M:%S\', handlers=[ch])\n\n\ndef main(config):\n  test_loader = make_data_loader(\n      config, config.test_phase, 1, num_threads=config.test_num_workers, shuffle=True)\n\n  num_feats = 1\n\n  device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n  Model = load_model(config.model)\n  model = Model(\n      num_feats,\n      config.model_n_out,\n      bn_momentum=config.bn_momentum,\n      conv1_kernel_size=config.conv1_kernel_size,\n      normalize_feature=config.normalize_feature)\n  checkpoint = torch.load(config.save_dir + \'/checkpoint.pth\')\n  model.load_state_dict(checkpoint[\'state_dict\'])\n  model = model.to(device)\n  model.eval()\n\n  success_meter, rte_meter, rre_meter = AverageMeter(), AverageMeter(), AverageMeter()\n  data_timer, feat_timer, reg_timer = Timer(), Timer(), Timer()\n\n  test_iter = test_loader.__iter__()\n  N = len(test_iter)\n  n_gpu_failures = 0\n\n  # downsample_voxel_size = 2 * config.voxel_size\n\n  for i in range(len(test_iter)):\n    data_timer.tic()\n    try:\n      data_dict = test_iter.next()\n    except ValueError:\n      n_gpu_failures += 1\n      logging.info(f""# Erroneous GPU Pair {n_gpu_failures}"")\n      continue\n    data_timer.toc()\n    xyz0, xyz1 = data_dict[\'pcd0\'], data_dict[\'pcd1\']\n    T_gth = data_dict[\'T_gt\']\n    xyz0np, xyz1np = xyz0.numpy(), xyz1.numpy()\n\n    pcd0 = make_open3d_point_cloud(xyz0np)\n    pcd1 = make_open3d_point_cloud(xyz1np)\n\n    with torch.no_grad():\n      feat_timer.tic()\n      sinput0 = ME.SparseTensor(\n          data_dict[\'sinput0_F\'], coords=data_dict[\'sinput0_C\']).to(device)\n      F0 = model(sinput0).F.detach()\n      sinput1 = ME.SparseTensor(\n          data_dict[\'sinput1_F\'], coords=data_dict[\'sinput1_C\']).to(device)\n      F1 = model(sinput1).F.detach()\n      feat_timer.toc()\n\n    feat0 = make_open3d_feature(F0, 32, F0.shape[0])\n    feat1 = make_open3d_feature(F1, 32, F1.shape[0])\n\n    reg_timer.tic()\n    distance_threshold = config.voxel_size * 1.0\n    ransac_result = o3d.registration.registration_ransac_based_on_feature_matching(\n        pcd0, pcd1, feat0, feat1, distance_threshold,\n        o3d.registration.TransformationEstimationPointToPoint(False), 4, [\n            o3d.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n            o3d.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)\n        ], o3d.registration.RANSACConvergenceCriteria(4000000, 10000))\n    T_ransac = torch.from_numpy(ransac_result.transformation.astype(np.float32))\n    reg_timer.toc()\n\n    # Translation error\n    rte = np.linalg.norm(T_ransac[:3, 3] - T_gth[:3, 3])\n    rre = np.arccos((np.trace(T_ransac[:3, :3].t() @ T_gth[:3, :3]) - 1) / 2)\n\n    # Check if the ransac was successful. successful if rte < 2m and rre < 5\xe2\x97\xa6\n    # http://openaccess.thecvf.com/content_ECCV_2018/papers/Zi_Jian_Yew_3DFeat-Net_Weakly_Supervised_ECCV_2018_paper.pdf\n    if rte < 2:\n      rte_meter.update(rte)\n\n    if not np.isnan(rre) and rre < np.pi / 180 * 5:\n      rre_meter.update(rre)\n\n    if rte < 2 and not np.isnan(rre) and rre < np.pi / 180 * 5:\n      success_meter.update(1)\n    else:\n      success_meter.update(0)\n      logging.info(f""Failed with RTE: {rte}, RRE: {rre}"")\n\n    if i % 10 == 0:\n      logging.info(\n          f""{i} / {N}: Data time: {data_timer.avg}, Feat time: {feat_timer.avg},"" +\n          f"" Reg time: {reg_timer.avg}, RTE: {rte_meter.avg},"" +\n          f"" RRE: {rre_meter.avg}, Success: {success_meter.sum} / {success_meter.count}""\n          + f"" ({success_meter.avg * 100} %)"")\n      data_timer.reset()\n      feat_timer.reset()\n      reg_timer.reset()\n\n  logging.info(\n      f""RTE: {rte_meter.avg}, var: {rte_meter.var},"" +\n      f"" RRE: {rre_meter.avg}, var: {rre_meter.var}, Success: {success_meter.sum} "" +\n      f""/ {success_meter.count} ({success_meter.avg * 100} %)"")\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--save_dir\', default=None, type=str)\n  parser.add_argument(\'--test_phase\', default=\'test\', type=str)\n  parser.add_argument(\'--test_num_thread\', default=5, type=int)\n  parser.add_argument(\'--kitti_root\', type=str, default=""/data/kitti/"")\n  args = parser.parse_args()\n\n  config = json.load(open(args.save_dir + \'/config.json\', \'r\'))\n  config = edict(config)\n  config.save_dir = args.save_dir\n  config.test_phase = args.test_phase\n  config.kitti_root = args.kitti_root\n  config.kitti_odometry_root = args.kitti_root + \'/dataset\'\n  config.test_num_thread = args.test_num_thread\n\n  main(config)\n'"
util/__init__.py,0,b''
util/file.py,0,"b'import os\nimport re\nfrom os import listdir\nfrom os.path import isfile, isdir, join, splitext\n\n\ndef read_txt(path):\n  """"""Read txt file into lines.\n  """"""\n  with open(path) as f:\n    lines = f.readlines()\n  lines = [x.strip() for x in lines]\n  return lines\n\n\ndef ensure_dir(path):\n  if not os.path.exists(path):\n    os.makedirs(path, mode=0o755)\n\n\ndef sorted_alphanum(file_list_ordered):\n\n  def convert(text):\n    return int(text) if text.isdigit() else text\n\n  def alphanum_key(key):\n    return [convert(c) for c in re.split(\'([0-9]+)\', key)]\n\n  return sorted(file_list_ordered, key=alphanum_key)\n\n\ndef get_file_list(path, extension=None):\n  if extension is None:\n    file_list = [join(path, f) for f in listdir(path) if isfile(join(path, f))]\n  else:\n    file_list = [\n        join(path, f)\n        for f in listdir(path)\n        if isfile(join(path, f)) and splitext(f)[1] == extension\n    ]\n  file_list = sorted_alphanum(file_list)\n  return file_list\n\n\ndef get_file_list_specific(path, color_depth, extension=None):\n  if extension is None:\n    file_list = [join(path, f) for f in listdir(path) if isfile(join(path, f))]\n  else:\n    file_list = [\n        join(path, f)\n        for f in listdir(path)\n        if isfile(join(path, f)) and color_depth in f and splitext(f)[1] == extension\n    ]\n    file_list = sorted_alphanum(file_list)\n  return file_list\n\n\ndef get_folder_list(path):\n  folder_list = [join(path, f) for f in listdir(path) if isdir(join(path, f))]\n  folder_list = sorted_alphanum(folder_list)\n  return folder_list\n'"
util/misc.py,3,"b'import torch\nimport numpy as np\nimport MinkowskiEngine as ME\n\n\ndef _hash(arr, M):\n  if isinstance(arr, np.ndarray):\n    N, D = arr.shape\n  else:\n    N, D = len(arr[0]), len(arr)\n\n  hash_vec = np.zeros(N, dtype=np.int64)\n  for d in range(D):\n    if isinstance(arr, np.ndarray):\n      hash_vec += arr[:, d] * M**d\n    else:\n      hash_vec += arr[d] * M**d\n  return hash_vec\n\n\ndef extract_features(model,\n                     xyz,\n                     rgb=None,\n                     normal=None,\n                     voxel_size=0.05,\n                     device=None,\n                     skip_check=False,\n                     is_eval=True):\n  \'\'\'\n  xyz is a N x 3 matrix\n  rgb is a N x 3 matrix and all color must range from [0, 1] or None\n  normal is a N x 3 matrix and all normal range from [-1, 1] or None\n\n  if both rgb and normal are None, we use Nx1 one vector as an input\n\n  if device is None, it tries to use gpu by default\n\n  if skip_check is True, skip rigorous checks to speed up\n\n  model = model.to(device)\n  xyz, feats = extract_features(model, xyz)\n  \'\'\'\n  if is_eval:\n    model.eval()\n\n  if not skip_check:\n    assert xyz.shape[1] == 3\n\n    N = xyz.shape[0]\n    if rgb is not None:\n      assert N == len(rgb)\n      assert rgb.shape[1] == 3\n      if np.any(rgb > 1):\n        raise ValueError(\'Invalid color. Color must range from [0, 1]\')\n\n    if normal is not None:\n      assert N == len(normal)\n      assert normal.shape[1] == 3\n      if np.any(normal > 1):\n        raise ValueError(\'Invalid normal. Normal must range from [-1, 1]\')\n\n  if device is None:\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n  feats = []\n  if rgb is not None:\n    # [0, 1]\n    feats.append(rgb - 0.5)\n\n  if normal is not None:\n    # [-1, 1]\n    feats.append(normal / 2)\n\n  if rgb is None and normal is None:\n    feats.append(np.ones((len(xyz), 1)))\n\n  feats = np.hstack(feats)\n\n  # Voxelize xyz and feats\n  coords = np.floor(xyz / voxel_size)\n  inds = ME.utils.sparse_quantize(coords, return_index=True)\n  coords = coords[inds]\n  # Convert to batched coords compatible with ME\n  coords = ME.utils.batched_coordinates([coords])\n  return_coords = xyz[inds]\n\n  feats = feats[inds]\n\n  feats = torch.tensor(feats, dtype=torch.float32)\n  coords = torch.tensor(coords, dtype=torch.int32)\n\n  stensor = ME.SparseTensor(feats, coords=coords).to(device)\n\n  return return_coords, model(stensor).F\n'"
util/pointcloud.py,0,"b'import copy\nimport numpy as np\nimport math\n\nimport open3d as o3d\nfrom lib.eval import find_nn_cpu\n\n\ndef make_open3d_point_cloud(xyz, color=None):\n  pcd = o3d.geometry.PointCloud()\n  pcd.points = o3d.utility.Vector3dVector(xyz)\n  if color is not None:\n    pcd.colors = o3d.utility.Vector3dVector(color)\n  return pcd\n\n\ndef make_open3d_feature(data, dim, npts):\n  feature = o3d.registration.Feature()\n  feature.resize(dim, npts)\n  feature.data = data.cpu().numpy().astype(\'d\').transpose()\n  return feature\n\n\ndef make_open3d_feature_from_numpy(data):\n  assert isinstance(data, np.ndarray)\n  assert data.ndim == 2\n\n  feature = o3d.registration.Feature()\n  feature.resize(data.shape[1], data.shape[0])\n  feature.data = data.astype(\'d\').transpose()\n  return feature\n\n\ndef prepare_pointcloud(filename, voxel_size):\n  pcd = o3d.io.read_point_cloud(filename)\n  T = get_random_transformation(pcd)\n  pcd.transform(T)\n  pcd_down = pcd.voxel_down_sample(voxel_size)\n  return pcd_down, T\n\n\ndef compute_overlap_ratio(pcd0, pcd1, trans, voxel_size):\n  pcd0_down = pcd0.voxel_down_sample(voxel_size)\n  pcd1_down = pcd1.voxel_down_sample(voxel_size)\n  matching01 = get_matching_indices(pcd0_down, pcd1_down, trans, voxel_size, 1)\n  matching10 = get_matching_indices(pcd1_down, pcd0_down, np.linalg.inv(trans),\n                                    voxel_size, 1)\n  overlap0 = len(matching01) / len(pcd0_down.points)\n  overlap1 = len(matching10) / len(pcd1_down.points)\n  return max(overlap0, overlap1)\n\n\ndef get_matching_indices(source, target, trans, search_voxel_size, K=None):\n  source_copy = copy.deepcopy(source)\n  target_copy = copy.deepcopy(target)\n  source_copy.transform(trans)\n  pcd_tree = o3d.geometry.KDTreeFlann(target_copy)\n\n  match_inds = []\n  for i, point in enumerate(source_copy.points):\n    [_, idx, _] = pcd_tree.search_radius_vector_3d(point, search_voxel_size)\n    if K is not None:\n      idx = idx[:K]\n    for j in idx:\n      match_inds.append((i, j))\n  return match_inds\n\n\ndef evaluate_feature(pcd0, pcd1, feat0, feat1, trans_gth, search_voxel_size):\n  match_inds = get_matching_indices(pcd0, pcd1, trans_gth, search_voxel_size)\n  pcd_tree = o3d.geometry.KDTreeFlann(feat1)\n  dist = []\n  for ind in match_inds:\n    k, idx, _ = pcd_tree.search_knn_vector_xd(feat0.data[:, ind[0]], 1)\n    dist.append(\n        np.clip(\n            np.power(pcd1.points[ind[1]] - pcd1.points[idx[0]], 2),\n            a_min=0.0,\n            a_max=1.0))\n  return np.mean(dist)\n\n\ndef valid_feat_ratio(pcd0, pcd1, feat0, feat1, trans_gth, thresh=0.1):\n  pcd0_copy = copy.deepcopy(pcd0)\n  pcd0_copy.transform(trans_gth)\n  inds = find_nn_cpu(feat0, feat1, return_distance=False)\n  dist = np.sqrt(((np.array(pcd0_copy.points) - np.array(pcd1.points)[inds])**2).sum(1))\n  return np.mean(dist < thresh)\n\n\ndef evaluate_feature_3dmatch(pcd0, pcd1, feat0, feat1, trans_gth, inlier_thresh=0.1):\n  r""""""Return the hit ratio (ratio of inlier correspondences and all correspondences).\n\n  inliear_thresh is the inlier_threshold in meter.\n  """"""\n  if len(pcd0.points) < len(pcd1.points):\n    hit = valid_feat_ratio(pcd0, pcd1, feat0, feat1, trans_gth, inlier_thresh)\n  else:\n    hit = valid_feat_ratio(pcd1, pcd0, feat1, feat0, np.linalg.inv(trans_gth), inlier_thresh)\n  return hit\n\n\ndef get_matching_matrix(source, target, trans, voxel_size, debug_mode):\n  source_copy = copy.deepcopy(source)\n  target_copy = copy.deepcopy(target)\n  source_copy.transform(trans)\n  pcd_tree = o3d.geometry.KDTreeFlann(target_copy)\n  matching_matrix = np.zeros((len(source_copy.points), len(target_copy.points)))\n\n  for i, point in enumerate(source_copy.points):\n    [k, idx, _] = pcd_tree.search_radius_vector_3d(point, voxel_size * 1.5)\n    if k >= 1:\n      matching_matrix[i, idx[0]] = 1  # TODO: only the cloest?\n\n  return matching_matrix\n\n\ndef get_random_transformation(pcd_input):\n\n  def rot_x(x):\n    out = np.zeros((3, 3))\n    c = math.cos(x)\n    s = math.sin(x)\n    out[0, 0] = 1\n    out[1, 1] = c\n    out[1, 2] = -s\n    out[2, 1] = s\n    out[2, 2] = c\n    return out\n\n  def rot_y(x):\n    out = np.zeros((3, 3))\n    c = math.cos(x)\n    s = math.sin(x)\n    out[0, 0] = c\n    out[0, 2] = s\n    out[1, 1] = 1\n    out[2, 0] = -s\n    out[2, 2] = c\n    return out\n\n  def rot_z(x):\n    out = np.zeros((3, 3))\n    c = math.cos(x)\n    s = math.sin(x)\n    out[0, 0] = c\n    out[0, 1] = -s\n    out[1, 0] = s\n    out[1, 1] = c\n    out[2, 2] = 1\n    return out\n\n  pcd_output = copy.deepcopy(pcd_input)\n  mean = np.mean(np.asarray(pcd_output.points), axis=0).transpose()\n  xyz = np.random.uniform(0, 2 * math.pi, 3)\n  R = np.dot(np.dot(rot_x(xyz[0]), rot_y(xyz[1])), rot_z(xyz[2]))\n  T = np.zeros((4, 4))\n  T[:3, :3] = R\n  T[:3, 3] = np.dot(-R, mean)\n  T[3, 3] = 1\n  return T\n'"
util/trajectory.py,0,"b'import os\nimport numpy as np\n\n\nclass CameraPose:\n\n  def __init__(self, meta, mat):\n    self.metadata = meta\n    self.pose = mat\n\n  def __str__(self):\n    return \'metadata : \' + \' \'.join(map(str, self.metadata)) + \'\\n\' + \\\n        ""pose : "" + ""\\n"" + np.array_str(self.pose)\n\n\ndef read_trajectory(filename, dim=4):\n  traj = []\n  assert os.path.exists(filename)\n  with open(filename, \'r\') as f:\n    metastr = f.readline()\n    while metastr:\n      metadata = list(map(int, metastr.split()))\n      mat = np.zeros(shape=(dim, dim))\n      for i in range(dim):\n        matstr = f.readline()\n        mat[i, :] = np.fromstring(matstr, dtype=float, sep=\' \\t\')\n      traj.append(CameraPose(metadata, mat))\n      metastr = f.readline()\n    return traj\n\n\ndef write_trajectory(traj, filename, dim=4):\n  with open(filename, \'w\') as f:\n    for x in traj:\n      p = x.pose.tolist()\n      f.write(\' \'.join(map(str, x.metadata)) + \'\\n\')\n      f.write(\'\\n\'.join(\' \'.join(map(\'{0:.12f}\'.format, p[i])) for i in range(dim)))\n      f.write(\'\\n\')\n'"
util/transform_estimation.py,21,"b""import torch\nimport MinkowskiEngine as ME\n\n\ndef rot_x(x):\n  out = torch.zeros((3, 3))\n  c = torch.cos(x)\n  s = torch.sin(x)\n  out[0, 0] = 1\n  out[1, 1] = c\n  out[1, 2] = -s\n  out[2, 1] = s\n  out[2, 2] = c\n  return out\n\n\ndef rot_y(x):\n  out = torch.zeros((3, 3))\n  c = torch.cos(x)\n  s = torch.sin(x)\n  out[0, 0] = c\n  out[0, 2] = s\n  out[1, 1] = 1\n  out[2, 0] = -s\n  out[2, 2] = c\n  return out\n\n\ndef rot_z(x):\n  out = torch.zeros((3, 3))\n  c = torch.cos(x)\n  s = torch.sin(x)\n  out[0, 0] = c\n  out[0, 1] = -s\n  out[1, 0] = s\n  out[1, 1] = c\n  out[2, 2] = 1\n  return out\n\n\ndef get_trans(x):\n  trans = torch.eye(4)\n  trans[:3, :3] = rot_z(x[2]).mm(rot_y(x[1])).mm(rot_x(x[0]))\n  trans[:3, 3] = x[3:, 0]\n  return trans\n\n\ndef update_pcd(pts, trans):\n  R = trans[:3, :3]\n  T = trans[:3, 3]\n  # pts = R.mm(pts.t()).t() + T.unsqueeze(1).t().expand_as(pts)\n  pts = torch.t(R @ torch.t(pts)) + T\n  return pts\n\n\ndef build_linear_system(pts0, pts1, weight):\n  npts0 = pts0.shape[0]\n  A0 = torch.zeros((npts0, 6))\n  A1 = torch.zeros((npts0, 6))\n  A2 = torch.zeros((npts0, 6))\n  A0[:, 1] = pts0[:, 2]\n  A0[:, 2] = -pts0[:, 1]\n  A0[:, 3] = 1\n  A1[:, 0] = -pts0[:, 2]\n  A1[:, 2] = pts0[:, 0]\n  A1[:, 4] = 1\n  A2[:, 0] = pts0[:, 1]\n  A2[:, 1] = -pts0[:, 0]\n  A2[:, 5] = 1\n  ww1 = weight.repeat(3, 6)\n  ww2 = weight.repeat(3, 1)\n  A = ww1 * torch.cat((A0, A1, A2), 0)\n  b = ww2 * torch.cat(\n      (pts1[:, 0] - pts0[:, 0], pts1[:, 1] - pts0[:, 1], pts1[:, 2] - pts0[:, 2]),\n      0,\n  ).unsqueeze(1)\n  return A, b\n\n\ndef solve_linear_system(A, b):\n  temp = torch.inverse(A.t().mm(A))\n  return temp.mm(A.t()).mm(b)\n\n\ndef compute_weights(pts0, pts1, par):\n  return par / (torch.norm(pts0 - pts1, dim=1).unsqueeze(1) + par)\n\n\ndef est_quad_linear_robust(pts0, pts1, weight=None):\n  # TODO: 2. residual scheduling\n  pts0_curr = pts0\n  trans = torch.eye(4)\n\n  par = 1.0  # todo: need to decide\n  if weight is None:\n    weight = torch.ones(pts0.size()[0], 1)\n\n  for i in range(20):\n    if i > 0 and i % 5 == 0:\n      par /= 2.0\n\n    # compute weights\n    A, b = build_linear_system(pts0_curr, pts1, weight)\n    x = solve_linear_system(A, b)\n\n    # TODO: early termination\n    # residual = np.linalg.norm(A@x - b)\n    # print(residual)\n\n    # x = torch.empty(6, 1).uniform_(0, 1)\n    trans_curr = get_trans(x)\n    pts0_curr = update_pcd(pts0_curr, trans_curr)\n    weight = compute_weights(pts0_curr, pts1, par)\n    trans = trans_curr.mm(trans)\n\n  return trans\n\n\ndef pose_estimation(model,\n                    device,\n                    xyz0,\n                    xyz1,\n                    coord0,\n                    coord1,\n                    feats0,\n                    feats1,\n                    return_corr=False):\n  sinput0 = ME.SparseTensor(feats0, coords=coord0).to(device)\n  F0 = model(sinput0).F\n\n  sinput1 = ME.SparseTensor(feats1, coords=coord1).to(device)\n  F1 = model(sinput1).F\n\n  corr = F0.mm(F1.t())\n  weight, inds = corr.max(dim=1)\n  weight = weight.unsqueeze(1).cpu()\n  xyz1_corr = xyz1[inds, :]\n\n  trans = est_quad_linear_robust(xyz0, xyz1_corr, weight)  # let's do this later\n\n  if return_corr:\n    return trans, weight, corr\n  else:\n    return trans, weight\n"""
util/visualization.py,0,"b'import copy\nimport open3d as o3d\nimport numpy as np\n\nfrom sklearn.manifold import TSNE\nfrom matplotlib import pyplot as plt\n\n\ndef get_color_map(x):\n  colours = plt.cm.Spectral(x)\n  return colours[:, :3]\n\n\ndef mesh_sphere(pcd, voxel_size, sphere_size=0.6):\n  # Create a mesh sphere\n  spheres = o3d.geometry.TriangleMesh()\n  s = o3d.geometry.TriangleMesh.create_sphere(radius=voxel_size * sphere_size)\n  s.compute_vertex_normals()\n\n  for i, p in enumerate(pcd.points):\n    si = copy.deepcopy(s)\n    trans = np.identity(4)\n    trans[:3, 3] = p\n    si.transform(trans)\n    si.paint_uniform_color(pcd.colors[i])\n    spheres += si\n  return spheres\n\n\ndef get_colored_point_cloud_feature(pcd, feature, voxel_size):\n  tsne_results = embed_tsne(feature)\n\n  color = get_color_map(tsne_results)\n  pcd.colors = o3d.utility.Vector3dVector(color)\n  spheres = mesh_sphere(pcd, voxel_size)\n\n  return spheres\n\n\ndef embed_tsne(data):\n  """"""\n  N x D np.array data\n  """"""\n  tsne = TSNE(n_components=1, verbose=1, perplexity=40, n_iter=300, random_state=0)\n  tsne_results = tsne.fit_transform(data)\n  tsne_results = np.squeeze(tsne_results)\n  tsne_min = np.min(tsne_results)\n  tsne_max = np.max(tsne_results)\n  return (tsne_results - tsne_min) / (tsne_max - tsne_min)\n'"
