file_path,api_count,code
convert_torch.py,12,"b'from __future__ import print_function\n\nimport os\nimport math\nimport torch\nimport argparse\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.legacy.nn as lnn\nimport torch.nn.functional as F\n\nfrom functools import reduce\nfrom torch.autograd import Variable\nfrom torch.utils.serialization import load_lua\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        # result is Variables list [Variable1, Variable2, ...]\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        # result is a Variable\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\n\ndef copy_param(m,n):\n    if m.weight is not None: n.weight.data.copy_(m.weight)\n    if m.bias is not None: n.bias.data.copy_(m.bias)\n    if hasattr(n,\'running_mean\'): n.running_mean.copy_(m.running_mean)\n    if hasattr(n,\'running_var\'): n.running_var.copy_(m.running_var)\n\ndef add_submodule(seq, *args):\n    for n in args:\n        seq.add_module(str(len(seq._modules)),n)\n\ndef lua_recursive_model(module,seq):\n    for m in module.modules:\n        name = type(m).__name__\n        real = m\n        if name == \'TorchObject\':\n            name = m._typename.replace(\'cudnn.\',\'\')\n            m = m._obj\n\n        if name == \'SpatialConvolution\' or name == \'nn.SpatialConvolutionMM\':\n            if not hasattr(m,\'groups\') or m.groups is None: m.groups=1\n            n = nn.Conv2d(m.nInputPlane,m.nOutputPlane,(m.kW,m.kH),(m.dW,m.dH),(m.padW,m.padH),1,m.groups,bias=(m.bias is not None))\n            copy_param(m,n)\n            add_submodule(seq,n)\n        elif name == \'SpatialBatchNormalization\':\n            n = nn.BatchNorm2d(m.running_mean.size(0), m.eps, m.momentum, m.affine)\n            copy_param(m,n)\n            add_submodule(seq,n)\n        elif name == \'VolumetricBatchNormalization\':\n            n = nn.BatchNorm3d(m.running_mean.size(0), m.eps, m.momentum, m.affine)\n            copy_param(m, n)\n            add_submodule(seq, n)\n        elif name == \'ReLU\':\n            n = nn.ReLU()\n            add_submodule(seq,n)\n        elif name == \'Sigmoid\':\n            n = nn.Sigmoid()\n            add_submodule(seq,n)\n        elif name == \'SpatialMaxPooling\':\n            n = nn.MaxPool2d((m.kW,m.kH),(m.dW,m.dH),(m.padW,m.padH),ceil_mode=m.ceil_mode)\n            add_submodule(seq,n)\n        elif name == \'SpatialAveragePooling\':\n            n = nn.AvgPool2d((m.kW,m.kH),(m.dW,m.dH),(m.padW,m.padH),ceil_mode=m.ceil_mode)\n            add_submodule(seq,n)\n        elif name == \'SpatialUpSamplingNearest\':\n            n = nn.UpsamplingNearest2d(scale_factor=m.scale_factor)\n            add_submodule(seq,n)\n        elif name == \'View\':\n            n = Lambda(lambda x: x.view(x.size(0),-1))\n            add_submodule(seq,n)\n        elif name == \'Reshape\':\n            n = Lambda(lambda x: x.view(x.size(0),-1))\n            add_submodule(seq,n)\n        elif name == \'Linear\':\n            # Linear in pytorch only accept 2D input\n            n1 = Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x )\n            n2 = nn.Linear(m.weight.size(1),m.weight.size(0),bias=(m.bias is not None))\n            copy_param(m,n2)\n            n = nn.Sequential(n1,n2)\n            add_submodule(seq,n)\n        elif name == \'Dropout\':\n            m.inplace = False\n            n = nn.Dropout(m.p)\n            add_submodule(seq,n)\n        elif name == \'SoftMax\':\n            n = nn.Softmax()\n            add_submodule(seq,n)\n        elif name == \'Identity\':\n            n = Lambda(lambda x: x) # do nothing\n            add_submodule(seq,n)\n        elif name == \'SpatialFullConvolution\':\n            n = nn.ConvTranspose2d(m.nInputPlane,m.nOutputPlane,(m.kW,m.kH),(m.dW,m.dH),(m.padW,m.padH),(m.adjW,m.adjH))\n            copy_param(m,n)\n            add_submodule(seq,n)\n        elif name == \'VolumetricFullConvolution\':\n            n = nn.ConvTranspose3d(m.nInputPlane,m.nOutputPlane,(m.kT,m.kW,m.kH),(m.dT,m.dW,m.dH),(m.padT,m.padW,m.padH),(m.adjT,m.adjW,m.adjH),m.groups)\n            copy_param(m,n)\n            add_submodule(seq, n)\n        elif name == \'SpatialReplicationPadding\':\n            n = nn.ReplicationPad2d((m.pad_l,m.pad_r,m.pad_t,m.pad_b))\n            add_submodule(seq,n)\n        elif name == \'SpatialReflectionPadding\':\n            n = nn.ReflectionPad2d((m.pad_l,m.pad_r,m.pad_t,m.pad_b))\n            add_submodule(seq,n)\n        elif name == \'Copy\':\n            n = Lambda(lambda x: x) # do nothing\n            add_submodule(seq,n)\n        elif name == \'Narrow\':\n            n = Lambda(lambda x,a=(m.dimension,m.index,m.length): x.narrow(*a))\n            add_submodule(seq,n)\n        elif name == \'SpatialCrossMapLRN\':\n            lrn = lnn.SpatialCrossMapLRN(m.size,m.alpha,m.beta,m.k)\n            n = Lambda(lambda x,lrn=lrn: Variable(lrn.forward(x.data)))\n            add_submodule(seq,n)\n        elif name == \'Sequential\':\n            n = nn.Sequential()\n            lua_recursive_model(m,n)\n            add_submodule(seq,n)\n        elif name == \'ConcatTable\': # output is list\n            n = LambdaMap(lambda x: x)\n            lua_recursive_model(m,n)\n            add_submodule(seq,n)\n        elif name == \'CAddTable\': # input is list\n            n = LambdaReduce(lambda x,y: x+y)\n            add_submodule(seq,n)\n        elif name == \'Concat\':\n            dim = m.dimension\n            n = LambdaReduce(lambda x,y,dim=dim: torch.cat((x,y),dim))\n            lua_recursive_model(m,n)\n            add_submodule(seq,n)\n        elif name == \'TorchObject\':\n            print(\'Not Implement\',name,real._typename)\n        else:\n            print(\'Not Implement\',name)\n\n\ndef lua_recursive_source(module):\n    s = []\n    for m in module.modules:\n        name = type(m).__name__\n        real = m\n        if name == \'TorchObject\':\n            name = m._typename.replace(\'cudnn.\',\'\')\n            m = m._obj\n\n        if name == \'SpatialConvolution\' or name == \'nn.SpatialConvolutionMM\':\n            if not hasattr(m,\'groups\') or m.groups is None: m.groups=1\n            s += [\'nn.Conv2d({},{},{},{},{},{},{},bias={}),#Conv2d\'.format(m.nInputPlane,\n                m.nOutputPlane,(m.kW,m.kH),(m.dW,m.dH),(m.padW,m.padH),1,m.groups,m.bias is not None)]\n        elif name == \'SpatialBatchNormalization\':\n            s += [\'nn.BatchNorm2d({},{},{},{}),#BatchNorm2d\'.format(m.running_mean.size(0), m.eps, m.momentum, m.affine)]\n        elif name == \'VolumetricBatchNormalization\':\n            s += [\'nn.BatchNorm3d({},{},{},{}),#BatchNorm3d\'.format(m.running_mean.size(0), m.eps, m.momentum, m.affine)]\n        elif name == \'ReLU\':\n            s += [\'nn.ReLU()\']\n        elif name == \'Sigmoid\':\n            s += [\'nn.Sigmoid()\']\n        elif name == \'SpatialMaxPooling\':\n            s += [\'nn.MaxPool2d({},{},{},ceil_mode={}),#MaxPool2d\'.format((m.kW,m.kH),(m.dW,m.dH),(m.padW,m.padH),m.ceil_mode)]\n        elif name == \'SpatialAveragePooling\':\n            s += [\'nn.AvgPool2d({},{},{},ceil_mode={}),#AvgPool2d\'.format((m.kW,m.kH),(m.dW,m.dH),(m.padW,m.padH),m.ceil_mode)]\n        elif name == \'SpatialUpSamplingNearest\':\n            s += [\'nn.UpsamplingNearest2d(scale_factor={})\'.format(m.scale_factor)]\n        elif name == \'View\':\n            s += [\'Lambda(lambda x: x.view(x.size(0),-1)), # View\']\n        elif name == \'Reshape\':\n            s += [\'Lambda(lambda x: x.view(x.size(0),-1)), # Reshape\']\n        elif name == \'Linear\':\n            s1 = \'Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x )\'\n            s2 = \'nn.Linear({},{},bias={})\'.format(m.weight.size(1),m.weight.size(0),(m.bias is not None))\n            s += [\'nn.Sequential({},{}),#Linear\'.format(s1,s2)]\n        elif name == \'Dropout\':\n            s += [\'nn.Dropout({})\'.format(m.p)]\n        elif name == \'SoftMax\':\n            s += [\'nn.Softmax()\']\n        elif name == \'Identity\':\n            s += [\'Lambda(lambda x: x), # Identity\']\n        elif name == \'SpatialFullConvolution\':\n            s += [\'nn.ConvTranspose2d({},{},{},{},{},{})\'.format(m.nInputPlane,\n                m.nOutputPlane,(m.kW,m.kH),(m.dW,m.dH),(m.padW,m.padH),(m.adjW,m.adjH))]\n        elif name == \'VolumetricFullConvolution\':\n            s += [\'nn.ConvTranspose3d({},{},{},{},{},{},{})\'.format(m.nInputPlane,\n                m.nOutputPlane,(m.kT,m.kW,m.kH),(m.dT,m.dW,m.dH),(m.padT,m.padW,m.padH),(m.adjT,m.adjW,m.adjH),m.groups)]\n        elif name == \'SpatialReplicationPadding\':\n            s += [\'nn.ReplicationPad2d({})\'.format((m.pad_l,m.pad_r,m.pad_t,m.pad_b))]\n        elif name == \'SpatialReflectionPadding\':\n            s += [\'nn.ReflectionPad2d({})\'.format((m.pad_l,m.pad_r,m.pad_t,m.pad_b))]\n        elif name == \'Copy\':\n            s += [\'Lambda(lambda x: x), # Copy\']\n        elif name == \'Narrow\':\n            s += [\'Lambda(lambda x,a={}: x.narrow(*a))\'.format((m.dimension,m.index,m.length))]\n        elif name == \'SpatialCrossMapLRN\':\n            lrn = \'lnn.SpatialCrossMapLRN(*{})\'.format((m.size,m.alpha,m.beta,m.k))\n            s += [\'Lambda(lambda x,lrn={}: Variable(lrn.forward(x.data)))\'.format(lrn)]\n\n        elif name == \'Sequential\':\n            s += [\'nn.Sequential( # Sequential\']\n            s += lua_recursive_source(m)\n            s += [\')\']\n        elif name == \'ConcatTable\':\n            s += [\'LambdaMap(lambda x: x, # ConcatTable\']\n            s += lua_recursive_source(m)\n            s += [\')\']\n        elif name == \'CAddTable\':\n            s += [\'LambdaReduce(lambda x,y: x+y), # CAddTable\']\n        elif name == \'Concat\':\n            dim = m.dimension\n            s += [\'LambdaReduce(lambda x,y,dim={}: torch.cat((x,y),dim), # Concat\'.format(m.dimension)]\n            s += lua_recursive_source(m)\n            s += [\')\']\n        else:\n            s += \'# \' + name + \' Not Implement,\\n\'\n    s = map(lambda x: \'\\t{}\'.format(x),s)\n    return s\n\ndef simplify_source(s):\n    s = map(lambda x: x.replace(\',(1, 1),(0, 0),1,1,bias=True),#Conv2d\',\')\'),s)\n    s = map(lambda x: x.replace(\',(0, 0),1,1,bias=True),#Conv2d\',\')\'),s)\n    s = map(lambda x: x.replace(\',1,1,bias=True),#Conv2d\',\')\'),s)\n    s = map(lambda x: x.replace(\',bias=True),#Conv2d\',\')\'),s)\n    s = map(lambda x: x.replace(\'),#Conv2d\',\')\'),s)\n    s = map(lambda x: x.replace(\',1e-05,0.1,True),#BatchNorm2d\',\')\'),s)\n    s = map(lambda x: x.replace(\'),#BatchNorm2d\',\')\'),s)\n    s = map(lambda x: x.replace(\',(0, 0),ceil_mode=False),#MaxPool2d\',\')\'),s)\n    s = map(lambda x: x.replace(\',ceil_mode=False),#MaxPool2d\',\')\'),s)\n    s = map(lambda x: x.replace(\'),#MaxPool2d\',\')\'),s)\n    s = map(lambda x: x.replace(\',(0, 0),ceil_mode=False),#AvgPool2d\',\')\'),s)\n    s = map(lambda x: x.replace(\',ceil_mode=False),#AvgPool2d\',\')\'),s)\n    s = map(lambda x: x.replace(\',bias=True)),#Linear\',\')), # Linear\'),s)\n    s = map(lambda x: x.replace(\')),#Linear\',\')), # Linear\'),s)\n\n    s = map(lambda x: \'{},\\n\'.format(x),s)\n    s = map(lambda x: x[1:],s)\n    s = reduce(lambda x,y: x+y, s)\n    return s\n\ndef torch_to_pytorch(t7_filename,outputname=None):\n    model = load_lua(t7_filename,unknown_classes=True)\n    if type(model).__name__==\'hashable_uniq_dict\': model=model.model\n    model.gradInput = None\n    slist = lua_recursive_source(lnn.Sequential().add(model))\n    s = simplify_source(slist)\n    header = \'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.legacy.nn as lnn\n\nfrom functools import reduce\nfrom torch.autograd import Variable\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\'\'\'\n    varname = t7_filename.replace(\'.t7\',\'\').replace(\'.\',\'_\').replace(\'-\',\'_\')\n    s = \'{}\\n\\n{} = {}\'.format(header,varname,s[:-2])\n\n    if outputname is None: outputname=varname\n    with open(outputname+\'.py\', ""w"") as pyfile:\n        pyfile.write(s)\n\n    n = nn.Sequential()\n    lua_recursive_model(model,n)\n    torch.save(n.state_dict(),outputname+\'.pth\')\n\n\nparser = argparse.ArgumentParser(description=\'Convert torch t7 model to pytorch\')\nparser.add_argument(\'--model\',\'-m\', type=str, required=True,\n                    help=\'torch model file in t7 format\')\nparser.add_argument(\'--output\', \'-o\', type=str, default=None,\n                    help=\'output file name prefix, xxx.py xxx.pth\')\nargs = parser.parse_args()\n\ntorch_to_pytorch(args.model,args.output)\n'"
