file_path,api_count,code
benchmark.py,6,"b""#!/usr/bin/env python3\n# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport torch.backends.cudnn as cudnn\nimport mobilenet_v1\nimport time\nimport numpy as np\n\nfrom benchmark_aflw2000 import calc_nme as calc_nme_alfw2000\nfrom benchmark_aflw2000 import ana as ana_alfw2000\nfrom benchmark_aflw import calc_nme as calc_nme_alfw\nfrom benchmark_aflw import ana as ana_aflw\n\nfrom utils.ddfa import ToTensorGjz, NormalizeGjz, DDFATestDataset, reconstruct_vertex\nimport argparse\n\n\ndef extract_param(checkpoint_fp, root='', filelists=None, arch='mobilenet_1', num_classes=62, device_ids=[0],\n                  batch_size=128, num_workers=4):\n    map_location = {f'cuda:{i}': 'cuda:0' for i in range(8)}\n    checkpoint = torch.load(checkpoint_fp, map_location=map_location)['state_dict']\n    torch.cuda.set_device(device_ids[0])\n    model = getattr(mobilenet_v1, arch)(num_classes=num_classes)\n    model = nn.DataParallel(model, device_ids=device_ids).cuda()\n    model.load_state_dict(checkpoint)\n\n    dataset = DDFATestDataset(filelists=filelists, root=root,\n                              transform=transforms.Compose([ToTensorGjz(), NormalizeGjz(mean=127.5, std=128)]))\n    data_loader = data.DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n\n    cudnn.benchmark = True\n    model.eval()\n\n    end = time.time()\n    outputs = []\n    with torch.no_grad():\n        for _, inputs in enumerate(data_loader):\n            inputs = inputs.cuda()\n            output = model(inputs)\n\n            for i in range(output.shape[0]):\n                param_prediction = output[i].cpu().numpy().flatten()\n\n                outputs.append(param_prediction)\n        outputs = np.array(outputs, dtype=np.float32)\n\n    print(f'Extracting params take {time.time() - end: .3f}s')\n    return outputs\n\n\ndef _benchmark_aflw(outputs):\n    return ana_aflw(calc_nme_alfw(outputs))\n\n\ndef _benchmark_aflw2000(outputs):\n    return ana_alfw2000(calc_nme_alfw2000(outputs))\n\n\ndef benchmark_alfw_params(params):\n    outputs = []\n    for i in range(params.shape[0]):\n        lm = reconstruct_vertex(params[i])\n        outputs.append(lm[:2, :])\n    return _benchmark_aflw(outputs)\n\n\ndef benchmark_aflw2000_params(params):\n    outputs = []\n    for i in range(params.shape[0]):\n        lm = reconstruct_vertex(params[i])\n        outputs.append(lm[:2, :])\n    return _benchmark_aflw2000(outputs)\n\n\ndef benchmark_pipeline(arch, checkpoint_fp):\n    device_ids = [0]\n\n    def aflw():\n        params = extract_param(\n            checkpoint_fp=checkpoint_fp,\n            root='test.data/AFLW_GT_crop',\n            filelists='test.data/AFLW_GT_crop.list',\n            arch=arch,\n            device_ids=device_ids,\n            batch_size=128)\n\n        benchmark_alfw_params(params)\n\n    def aflw2000():\n        params = extract_param(\n            checkpoint_fp=checkpoint_fp,\n            root='test.data/AFLW2000-3D_crop',\n            filelists='test.data/AFLW2000-3D_crop.list',\n            arch=arch,\n            device_ids=device_ids,\n            batch_size=128)\n\n        benchmark_aflw2000_params(params)\n\n    aflw2000()\n    aflw()\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='3DDFA Benchmark')\n    parser.add_argument('--arch', default='mobilenet_1', type=str)\n    parser.add_argument('-c', '--checkpoint-fp', default='models/phase1_wpdc_vdc.pth.tar', type=str)\n    args = parser.parse_args()\n\n    benchmark_pipeline(args.arch, args.checkpoint_fp)\n\n\nif __name__ == '__main__':\n    main()\n"""
benchmark_aflw.py,0,"b""#!/usr/bin/env python3\n# coding: utf-8\n\nimport os.path as osp\nimport numpy as np\nfrom math import sqrt\nfrom utils.io import _load\n\nd = 'test.configs'\nyaw_list = _load(osp.join(d, 'AFLW_GT_crop_yaws.npy'))\nroi_boxs = _load(osp.join(d, 'AFLW_GT_crop_roi_box.npy'))\npts68_all = _load(osp.join(d, 'AFLW_GT_pts68.npy'))\npts21_all = _load(osp.join(d, 'AFLW_GT_pts21.npy'))\n\n\ndef ana(nme_list):\n    yaw_list_abs = np.abs(yaw_list)\n    ind_yaw_1 = yaw_list_abs <= 30\n    ind_yaw_2 = np.bitwise_and(yaw_list_abs > 30, yaw_list_abs <= 60)\n    ind_yaw_3 = yaw_list_abs > 60\n\n    nme_1 = nme_list[ind_yaw_1]\n    nme_2 = nme_list[ind_yaw_2]\n    nme_3 = nme_list[ind_yaw_3]\n\n    mean_nme_1 = np.mean(nme_1) * 100\n    mean_nme_2 = np.mean(nme_2) * 100\n    mean_nme_3 = np.mean(nme_3) * 100\n    # mean_nme_all = np.mean(nme_list) * 100\n\n    std_nme_1 = np.std(nme_1) * 100\n    std_nme_2 = np.std(nme_2) * 100\n    std_nme_3 = np.std(nme_3) * 100\n    # std_nme_all = np.std(nme_list) * 100\n\n    mean_all = [mean_nme_1, mean_nme_2, mean_nme_3]\n    mean = np.mean(mean_all)\n    std = np.std(mean_all)\n\n    s1 = '[ 0, 30]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_1, std_nme_1)\n    s2 = '[30, 60]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_2, std_nme_2)\n    s3 = '[60, 90]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_3, std_nme_3)\n    # s4 = '[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: {:.3f}'.format(mean_nme_all, std_nme_all)\n    s5 = '[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: \\x1b[31m{:.3f}\\x1b[0m'.format(mean, std)\n\n    s = '\\n'.join([s1, s2, s3, s5])\n    print(s)\n\n    return mean_nme_1, mean_nme_2, mean_nme_3, mean, std\n\n\ndef calc_nme(pts68_fit_all):\n    std_size = 120\n    ind_68to21 = [[18], [20], [22], [23], [25], [27], [37], [37, 38, 39, 40, 41, 42], [40], [43],\n                  [43, 44, 45, 46, 47, 48],\n                  [46], [3], [32], [31], [36], [15], [49], [61, 62, 63, 64, 65, 66, 67, 68], [55], [9]]\n    for i in range(len(ind_68to21)):\n        for j in range(len(ind_68to21[i])):\n            ind_68to21[i][j] -= 1\n\n    nme_list = []\n\n    for i in range(len(roi_boxs)):\n        pts68_fit = pts68_fit_all[i]\n        pts68_gt = pts68_all[i]\n        pts21_gt = pts21_all[i]\n\n        # reconstruct 68 pts\n        sx, sy, ex, ey = roi_boxs[i]\n        scale_x = (ex - sx) / std_size\n        scale_y = (ey - sy) / std_size\n        pts68_fit[0, :] = pts68_fit[0, :] * scale_x + sx\n        pts68_fit[1, :] = pts68_fit[1, :] * scale_y + sy\n\n        # pts68 -> pts21\n        pts21_est = np.zeros_like(pts21_gt, dtype=np.float32)\n        for i in range(21):\n            ind = ind_68to21[i]\n            tmp = np.mean(pts68_fit[:, ind], 1)\n            pts21_est[:, i] = tmp\n\n        # build bbox\n        minx, maxx = np.min(pts68_gt[0, :]), np.max(pts68_gt[0, :])\n        miny, maxy = np.min(pts68_gt[1, :]), np.max(pts68_gt[1, :])\n        llength = sqrt((maxx - minx) * (maxy - miny))\n\n        # nme\n        pt_valid = (pts21_gt[0, :] != -1) & (pts21_gt[1, :] != -1)\n        dis = pts21_est[:, pt_valid] - pts21_gt[:, pt_valid]\n        dis = np.sqrt(np.sum(np.power(dis, 2), 0))\n        dis = np.mean(dis)\n        nme = dis / llength\n        nme_list.append(nme)\n\n    nme_list = np.array(nme_list, dtype=np.float32)\n    return nme_list\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"""
benchmark_aflw2000.py,0,"b'#!/usr/bin/env python3\n# coding: utf-8\n\n""""""\nNotation (2019.09.15): two versions of spliting AFLW2000-3D:\n 1) AFLW2000-3D.pose.npy: according to the fitted pose\n 2) AFLW2000-3D-new.pose: according to AFLW labels \nThere is no obvious difference between these two splits.\n""""""\n\nimport os.path as osp\nimport numpy as np\nfrom math import sqrt\nfrom utils.io import _load\n\nd = \'test.configs\'\n\n# [1312, 383, 305], current version\nyaws_list = _load(osp.join(d, \'AFLW2000-3D.pose.npy\'))\n\n# [1306, 462, 232], same as paper\n# yaws_list = _load(osp.join(d, \'AFLW2000-3D-new.pose.npy\'))\n\n# origin\npts68_all_ori = _load(osp.join(d, \'AFLW2000-3D.pts68.npy\'))\n\n# reannonated\npts68_all_re = _load(osp.join(d, \'AFLW2000-3D-Reannotated.pts68.npy\'))\nroi_boxs = _load(osp.join(d, \'AFLW2000-3D_crop.roi_box.npy\'))\n\n\ndef ana(nme_list):\n    yaw_list_abs = np.abs(yaws_list)\n    ind_yaw_1 = yaw_list_abs <= 30\n    ind_yaw_2 = np.bitwise_and(yaw_list_abs > 30, yaw_list_abs <= 60)\n    ind_yaw_3 = yaw_list_abs > 60\n\n    nme_1 = nme_list[ind_yaw_1]\n    nme_2 = nme_list[ind_yaw_2]\n    nme_3 = nme_list[ind_yaw_3]\n\n    mean_nme_1 = np.mean(nme_1) * 100\n    mean_nme_2 = np.mean(nme_2) * 100\n    mean_nme_3 = np.mean(nme_3) * 100\n    # mean_nme_all = np.mean(nme_list) * 100\n\n    std_nme_1 = np.std(nme_1) * 100\n    std_nme_2 = np.std(nme_2) * 100\n    std_nme_3 = np.std(nme_3) * 100\n    # std_nme_all = np.std(nme_list) * 100\n\n    mean_all = [mean_nme_1, mean_nme_2, mean_nme_3]\n    mean = np.mean(mean_all)\n    std = np.std(mean_all)\n\n    s1 = \'[ 0, 30]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}\'.format(mean_nme_1, std_nme_1)\n    s2 = \'[30, 60]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}\'.format(mean_nme_2, std_nme_2)\n    s3 = \'[60, 90]\\tMean: \\x1b[32m{:.3f}\\x1b[0m, Std: {:.3f}\'.format(mean_nme_3, std_nme_3)\n    # s4 = \'[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: {:.3f}\'.format(mean_nme_all, std_nme_all)\n    s5 = \'[ 0, 90]\\tMean: \\x1b[31m{:.3f}\\x1b[0m, Std: \\x1b[31m{:.3f}\\x1b[0m\'.format(mean, std)\n\n    s = \'\\n\'.join([s1, s2, s3, s5])\n    print(s)\n\n    return mean_nme_1, mean_nme_2, mean_nme_3, mean, std\n\n\ndef convert_to_ori(lms, i):\n    std_size = 120\n    sx, sy, ex, ey = roi_boxs[i]\n    scale_x = (ex - sx) / std_size\n    scale_y = (ey - sy) / std_size\n    lms[0, :] = lms[0, :] * scale_x + sx\n    lms[1, :] = lms[1, :] * scale_y + sy\n    return lms\n\n\ndef calc_nme(pts68_fit_all, option=\'ori\'):\n    if option == \'ori\':\n        pts68_all = pts68_all_ori\n    elif option == \'re\':\n        pts68_all = pts68_all_re\n    std_size = 120\n\n    nme_list = []\n\n    for i in range(len(roi_boxs)):\n        pts68_fit = pts68_fit_all[i]\n        pts68_gt = pts68_all[i]\n\n        sx, sy, ex, ey = roi_boxs[i]\n        scale_x = (ex - sx) / std_size\n        scale_y = (ey - sy) / std_size\n        pts68_fit[0, :] = pts68_fit[0, :] * scale_x + sx\n        pts68_fit[1, :] = pts68_fit[1, :] * scale_y + sy\n\n        # build bbox\n        minx, maxx = np.min(pts68_gt[0, :]), np.max(pts68_gt[0, :])\n        miny, maxy = np.min(pts68_gt[1, :]), np.max(pts68_gt[1, :])\n        llength = sqrt((maxx - minx) * (maxy - miny))\n\n        #\n        dis = pts68_fit - pts68_gt[:2, :]\n        dis = np.sqrt(np.sum(np.power(dis, 2), 0))\n        dis = np.mean(dis)\n        nme = dis / llength\n        nme_list.append(nme)\n\n    nme_list = np.array(nme_list, dtype=np.float32)\n    return nme_list\n\n\ndef main():\n    pass\n\n\nif __name__ == \'__main__\':\n    main()\n'"
main.py,4,"b'#!/usr/bin/env python3\n# coding: utf-8\n\n__author__ = \'cleardusk\'\n\n""""""\nThe pipeline of 3DDFA prediction: given one image, predict the 3d face vertices, 68 landmarks and visualization.\n\n[todo]\n1. CPU optimization: https://pmchojnacki.wordpress.com/2018/10/07/slow-pytorch-cpu-performance\n""""""\n\nimport torch\nimport torchvision.transforms as transforms\nimport mobilenet_v1\nimport numpy as np\nimport cv2\nimport dlib\nfrom utils.ddfa import ToTensorGjz, NormalizeGjz, str2bool\nimport scipy.io as sio\nfrom utils.inference import get_suffix, parse_roi_box_from_landmark, crop_img, predict_68pts, dump_to_ply, dump_vertex, \\\n    draw_landmarks, predict_dense, parse_roi_box_from_bbox, get_colors, write_obj_with_colors\nfrom utils.cv_plot import plot_pose_box\nfrom utils.estimate_pose import parse_pose\nfrom utils.render import get_depths_image, cget_depths_image, cpncc\nfrom utils.paf import gen_img_paf\nimport argparse\nimport torch.backends.cudnn as cudnn\n\nSTD_SIZE = 120\n\n\ndef main(args):\n    # 1. load pre-tained model\n    checkpoint_fp = \'models/phase1_wpdc_vdc.pth.tar\'\n    arch = \'mobilenet_1\'\n\n    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)[\'state_dict\']\n    model = getattr(mobilenet_v1, arch)(num_classes=62)  # 62 = 12(pose) + 40(shape) +10(expression)\n\n    model_dict = model.state_dict()\n    # because the model is trained by multiple gpus, prefix module should be removed\n    for k in checkpoint.keys():\n        model_dict[k.replace(\'module.\', \'\')] = checkpoint[k]\n    model.load_state_dict(model_dict)\n    if args.mode == \'gpu\':\n        cudnn.benchmark = True\n        model = model.cuda()\n    model.eval()\n\n    # 2. load dlib model for face detection and landmark used for face cropping\n    if args.dlib_landmark:\n        dlib_landmark_model = \'models/shape_predictor_68_face_landmarks.dat\'\n        face_regressor = dlib.shape_predictor(dlib_landmark_model)\n    if args.dlib_bbox:\n        face_detector = dlib.get_frontal_face_detector()\n\n    # 3. forward\n    tri = sio.loadmat(\'visualize/tri.mat\')[\'tri\']\n    transform = transforms.Compose([ToTensorGjz(), NormalizeGjz(mean=127.5, std=128)])\n    for img_fp in args.files:\n        img_ori = cv2.imread(img_fp)\n        if args.dlib_bbox:\n            rects = face_detector(img_ori, 1)\n        else:\n            rects = []\n\n        if len(rects) == 0:\n            rects = dlib.rectangles()\n            rect_fp = img_fp + \'.bbox\'\n            lines = open(rect_fp).read().strip().split(\'\\n\')[1:]\n            for l in lines:\n                l, r, t, b = [int(_) for _ in l.split(\' \')[1:]]\n                rect = dlib.rectangle(l, r, t, b)\n                rects.append(rect)\n\n        pts_res = []\n        Ps = []  # Camera matrix collection\n        poses = []  # pose collection, [todo: validate it]\n        vertices_lst = []  # store multiple face vertices\n        ind = 0\n        suffix = get_suffix(img_fp)\n        for rect in rects:\n            # whether use dlib landmark to crop image, if not, use only face bbox to calc roi bbox for cropping\n            if args.dlib_landmark:\n                # - use landmark for cropping\n                pts = face_regressor(img_ori, rect).parts()\n                pts = np.array([[pt.x, pt.y] for pt in pts]).T\n                roi_box = parse_roi_box_from_landmark(pts)\n            else:\n                # - use detected face bbox\n                bbox = [rect.left(), rect.top(), rect.right(), rect.bottom()]\n                roi_box = parse_roi_box_from_bbox(bbox)\n\n            img = crop_img(img_ori, roi_box)\n\n            # forward: one step\n            img = cv2.resize(img, dsize=(STD_SIZE, STD_SIZE), interpolation=cv2.INTER_LINEAR)\n            input = transform(img).unsqueeze(0)\n            with torch.no_grad():\n                if args.mode == \'gpu\':\n                    input = input.cuda()\n                param = model(input)\n                param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n\n            # 68 pts\n            pts68 = predict_68pts(param, roi_box)\n\n            # two-step for more accurate bbox to crop face\n            if args.bbox_init == \'two\':\n                roi_box = parse_roi_box_from_landmark(pts68)\n                img_step2 = crop_img(img_ori, roi_box)\n                img_step2 = cv2.resize(img_step2, dsize=(STD_SIZE, STD_SIZE), interpolation=cv2.INTER_LINEAR)\n                input = transform(img_step2).unsqueeze(0)\n                with torch.no_grad():\n                    if args.mode == \'gpu\':\n                        input = input.cuda()\n                    param = model(input)\n                    param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n\n                pts68 = predict_68pts(param, roi_box)\n\n            pts_res.append(pts68)\n            P, pose = parse_pose(param)\n            Ps.append(P)\n            poses.append(pose)\n\n            # dense face 3d vertices\n            if args.dump_ply or args.dump_vertex or args.dump_depth or args.dump_pncc or args.dump_obj:\n                vertices = predict_dense(param, roi_box)\n                vertices_lst.append(vertices)\n            if args.dump_ply:\n                dump_to_ply(vertices, tri, \'{}_{}.ply\'.format(img_fp.replace(suffix, \'\'), ind))\n            if args.dump_vertex:\n                dump_vertex(vertices, \'{}_{}.mat\'.format(img_fp.replace(suffix, \'\'), ind))\n            if args.dump_pts:\n                wfp = \'{}_{}.txt\'.format(img_fp.replace(suffix, \'\'), ind)\n                np.savetxt(wfp, pts68, fmt=\'%.3f\')\n                print(\'Save 68 3d landmarks to {}\'.format(wfp))\n            if args.dump_roi_box:\n                wfp = \'{}_{}.roibox\'.format(img_fp.replace(suffix, \'\'), ind)\n                np.savetxt(wfp, roi_box, fmt=\'%.3f\')\n                print(\'Save roi box to {}\'.format(wfp))\n            if args.dump_paf:\n                wfp_paf = \'{}_{}_paf.jpg\'.format(img_fp.replace(suffix, \'\'), ind)\n                wfp_crop = \'{}_{}_crop.jpg\'.format(img_fp.replace(suffix, \'\'), ind)\n                paf_feature = gen_img_paf(img_crop=img, param=param, kernel_size=args.paf_size)\n\n                cv2.imwrite(wfp_paf, paf_feature)\n                cv2.imwrite(wfp_crop, img)\n                print(\'Dump to {} and {}\'.format(wfp_crop, wfp_paf))\n            if args.dump_obj:\n                wfp = \'{}_{}.obj\'.format(img_fp.replace(suffix, \'\'), ind)\n                colors = get_colors(img_ori, vertices)\n                write_obj_with_colors(wfp, vertices, tri, colors)\n                print(\'Dump obj with sampled texture to {}\'.format(wfp))\n            ind += 1\n\n        if args.dump_pose:\n            # P, pose = parse_pose(param)  # Camera matrix (without scale), and pose (yaw, pitch, roll, to verify)\n            img_pose = plot_pose_box(img_ori, Ps, pts_res)\n            wfp = img_fp.replace(suffix, \'_pose.jpg\')\n            cv2.imwrite(wfp, img_pose)\n            print(\'Dump to {}\'.format(wfp))\n        if args.dump_depth:\n            wfp = img_fp.replace(suffix, \'_depth.png\')\n            # depths_img = get_depths_image(img_ori, vertices_lst, tri-1)  # python version\n            depths_img = cget_depths_image(img_ori, vertices_lst, tri - 1)  # cython version\n            cv2.imwrite(wfp, depths_img)\n            print(\'Dump to {}\'.format(wfp))\n        if args.dump_pncc:\n            wfp = img_fp.replace(suffix, \'_pncc.png\')\n            pncc_feature = cpncc(img_ori, vertices_lst, tri - 1)  # cython version\n            cv2.imwrite(wfp, pncc_feature[:, :, ::-1])  # cv2.imwrite will swap RGB -> BGR\n            print(\'Dump to {}\'.format(wfp))\n        if args.dump_res:\n            draw_landmarks(img_ori, pts_res, wfp=img_fp.replace(suffix, \'_3DDFA.jpg\'), show_flg=args.show_flg)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'3DDFA inference pipeline\')\n    parser.add_argument(\'-f\', \'--files\', nargs=\'+\',\n                        help=\'image files paths fed into network, single or multiple images\')\n    parser.add_argument(\'-m\', \'--mode\', default=\'cpu\', type=str, help=\'gpu or cpu mode\')\n    parser.add_argument(\'--show_flg\', default=\'true\', type=str2bool, help=\'whether show the visualization result\')\n    parser.add_argument(\'--bbox_init\', default=\'one\', type=str,\n                        help=\'one|two: one-step bbox initialization or two-step\')\n    parser.add_argument(\'--dump_res\', default=\'true\', type=str2bool, help=\'whether write out the visualization image\')\n    parser.add_argument(\'--dump_vertex\', default=\'false\', type=str2bool,\n                        help=\'whether write out the dense face vertices to mat\')\n    parser.add_argument(\'--dump_ply\', default=\'true\', type=str2bool)\n    parser.add_argument(\'--dump_pts\', default=\'true\', type=str2bool)\n    parser.add_argument(\'--dump_roi_box\', default=\'false\', type=str2bool)\n    parser.add_argument(\'--dump_pose\', default=\'true\', type=str2bool)\n    parser.add_argument(\'--dump_depth\', default=\'true\', type=str2bool)\n    parser.add_argument(\'--dump_pncc\', default=\'true\', type=str2bool)\n    parser.add_argument(\'--dump_paf\', default=\'false\', type=str2bool)\n    parser.add_argument(\'--paf_size\', default=3, type=int, help=\'PAF feature kernel size\')\n    parser.add_argument(\'--dump_obj\', default=\'true\', type=str2bool)\n    parser.add_argument(\'--dlib_bbox\', default=\'true\', type=str2bool, help=\'whether use dlib to predict bbox\')\n    parser.add_argument(\'--dlib_landmark\', default=\'true\', type=str2bool,\n                        help=\'whether use dlib landmark to crop image\')\n\n    args = parser.parse_args()\n    main(args)\n'"
mobilenet_v1.py,1,"b'#!/usr/bin/env python3\n# coding: utf-8\n\nfrom __future__ import division\n\n"""""" \nCreates a MobileNet Model as defined in:\nAndrew G. Howard Menglong Zhu Bo Chen, et.al. (2017). \nMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. \nCopyright (c) Yang Lu, 2017\n\nModified By cleardusk\n""""""\nimport math\nimport torch.nn as nn\n\n__all__ = [\'mobilenet_2\', \'mobilenet_1\', \'mobilenet_075\', \'mobilenet_05\', \'mobilenet_025\']\n\n\nclass DepthWiseBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, prelu=False):\n        super(DepthWiseBlock, self).__init__()\n        inplanes, planes = int(inplanes), int(planes)\n        self.conv_dw = nn.Conv2d(inplanes, inplanes, kernel_size=3, padding=1, stride=stride, groups=inplanes,\n                                 bias=False)\n        self.bn_dw = nn.BatchNorm2d(inplanes)\n        self.conv_sep = nn.Conv2d(inplanes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn_sep = nn.BatchNorm2d(planes)\n        if prelu:\n            self.relu = nn.PReLU()\n        else:\n            self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        out = self.conv_dw(x)\n        out = self.bn_dw(out)\n        out = self.relu(out)\n\n        out = self.conv_sep(out)\n        out = self.bn_sep(out)\n        out = self.relu(out)\n\n        return out\n\n\nclass MobileNet(nn.Module):\n    def __init__(self, widen_factor=1.0, num_classes=1000, prelu=False, input_channel=3):\n        """""" Constructor\n        Args:\n            widen_factor: config of widen_factor\n            num_classes: number of classes\n        """"""\n        super(MobileNet, self).__init__()\n\n        block = DepthWiseBlock\n        self.conv1 = nn.Conv2d(input_channel, int(32 * widen_factor), kernel_size=3, stride=2, padding=1,\n                               bias=False)\n\n        self.bn1 = nn.BatchNorm2d(int(32 * widen_factor))\n        if prelu:\n            self.relu = nn.PReLU()\n        else:\n            self.relu = nn.ReLU(inplace=True)\n\n        self.dw2_1 = block(32 * widen_factor, 64 * widen_factor, prelu=prelu)\n        self.dw2_2 = block(64 * widen_factor, 128 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw3_1 = block(128 * widen_factor, 128 * widen_factor, prelu=prelu)\n        self.dw3_2 = block(128 * widen_factor, 256 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw4_1 = block(256 * widen_factor, 256 * widen_factor, prelu=prelu)\n        self.dw4_2 = block(256 * widen_factor, 512 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw5_1 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_2 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_3 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_4 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_5 = block(512 * widen_factor, 512 * widen_factor, prelu=prelu)\n        self.dw5_6 = block(512 * widen_factor, 1024 * widen_factor, stride=2, prelu=prelu)\n\n        self.dw6 = block(1024 * widen_factor, 1024 * widen_factor, prelu=prelu)\n\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(int(1024 * widen_factor), num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.dw2_1(x)\n        x = self.dw2_2(x)\n        x = self.dw3_1(x)\n        x = self.dw3_2(x)\n        x = self.dw4_1(x)\n        x = self.dw4_2(x)\n        x = self.dw5_1(x)\n        x = self.dw5_2(x)\n        x = self.dw5_3(x)\n        x = self.dw5_4(x)\n        x = self.dw5_5(x)\n        x = self.dw5_6(x)\n        x = self.dw6(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef mobilenet(widen_factor=1.0, num_classes=1000):\n    """"""\n    Construct MobileNet.\n    widen_factor=1.0  for mobilenet_1\n    widen_factor=0.75 for mobilenet_075\n    widen_factor=0.5  for mobilenet_05\n    widen_factor=0.25 for mobilenet_025\n    """"""\n    model = MobileNet(widen_factor=widen_factor, num_classes=num_classes)\n    return model\n\n\ndef mobilenet_2(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=2.0, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_1(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=1.0, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_075(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.75, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_05(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.5, num_classes=num_classes, input_channel=input_channel)\n    return model\n\n\ndef mobilenet_025(num_classes=62, input_channel=3):\n    model = MobileNet(widen_factor=0.25, num_classes=num_classes, input_channel=input_channel)\n    return model\n'"
speed_cpu.py,2,"b""#!/usr/bin/env python3\n# coding: utf-8\n\nimport timeit\nimport numpy as np\n\nSETUP_CODE = '''\nimport mobilenet_v1\nimport torch\n\nmodel = mobilenet_v1.mobilenet_1()\nmodel.eval()\ndata = torch.rand(1, 3, 120, 120)\n'''\n\nTEST_CODE = '''\nwith torch.no_grad():\n    model(data)\n'''\n\n\ndef main():\n    repeat, number = 5, 100\n    res = timeit.repeat(setup=SETUP_CODE,\n                        stmt=TEST_CODE,\n                        repeat=repeat,\n                        number=number)\n    res = np.array(res, dtype=np.float32)\n    res /= number\n    mean, var = np.mean(res), np.std(res)\n    print('Inference speed: {:.2f}\xc2\xb1{:.2f} ms'.format(mean * 1000, var * 1000))\n\n\nif __name__ == '__main__':\n    main()\n"""
train.py,9,"b'#!/usr/bin/env python3\n# coding: utf-8\n\nimport os.path as osp\nfrom pathlib import Path\nimport numpy as np\nimport argparse\nimport time\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport mobilenet_v1\nimport torch.backends.cudnn as cudnn\n\nfrom utils.ddfa import DDFADataset, ToTensorGjz, NormalizeGjz\nfrom utils.ddfa import str2bool, AverageMeter\nfrom utils.io import mkdir\nfrom vdc_loss import VDCLoss\nfrom wpdc_loss import WPDCLoss\n\n# global args (configuration)\nargs = None\nlr = None\narch_choices = [\'mobilenet_2\', \'mobilenet_1\', \'mobilenet_075\', \'mobilenet_05\', \'mobilenet_025\']\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'3DMM Fitting\')\n    parser.add_argument(\'-j\', \'--workers\', default=6, type=int)\n    parser.add_argument(\'--epochs\', default=40, type=int)\n    parser.add_argument(\'--start-epoch\', default=1, type=int)\n    parser.add_argument(\'-b\', \'--batch-size\', default=128, type=int)\n    parser.add_argument(\'-vb\', \'--val-batch-size\', default=32, type=int)\n    parser.add_argument(\'--base-lr\', \'--learning-rate\', default=0.001, type=float)\n    parser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                        help=\'momentum\')\n    parser.add_argument(\'--weight-decay\', \'--wd\', default=5e-4, type=float)\n    parser.add_argument(\'--print-freq\', \'-p\', default=20, type=int)\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\')\n    parser.add_argument(\'--devices-id\', default=\'0,1\', type=str)\n    parser.add_argument(\'--filelists-train\',\n                        default=\'\', type=str)\n    parser.add_argument(\'--filelists-val\',\n                        default=\'\', type=str)\n    parser.add_argument(\'--root\', default=\'\')\n    parser.add_argument(\'--snapshot\', default=\'\', type=str)\n    parser.add_argument(\'--log-file\', default=\'output.log\', type=str)\n    parser.add_argument(\'--log-mode\', default=\'w\', type=str)\n    parser.add_argument(\'--size-average\', default=\'true\', type=str2bool)\n    parser.add_argument(\'--num-classes\', default=62, type=int)\n    parser.add_argument(\'--arch\', default=\'mobilenet_1\', type=str,\n                        choices=arch_choices)\n    parser.add_argument(\'--frozen\', default=\'false\', type=str2bool)\n    parser.add_argument(\'--milestones\', default=\'15,25,30\', type=str)\n    parser.add_argument(\'--task\', default=\'all\', type=str)\n    parser.add_argument(\'--test_initial\', default=\'false\', type=str2bool)\n    parser.add_argument(\'--warmup\', default=-1, type=int)\n    parser.add_argument(\'--param-fp-train\',\n                        default=\'\',\n                        type=str)\n    parser.add_argument(\'--param-fp-val\',\n                        default=\'\')\n    parser.add_argument(\'--opt-style\', default=\'resample\', type=str)  # resample\n    parser.add_argument(\'--resample-num\', default=132, type=int)\n    parser.add_argument(\'--loss\', default=\'vdc\', type=str)\n\n    global args\n    args = parser.parse_args()\n\n    # some other operations\n    args.devices_id = [int(d) for d in args.devices_id.split(\',\')]\n    args.milestones = [int(m) for m in args.milestones.split(\',\')]\n\n    snapshot_dir = osp.split(args.snapshot)[0]\n    mkdir(snapshot_dir)\n\n\ndef print_args(args):\n    for arg in vars(args):\n        s = arg + \': \' + str(getattr(args, arg))\n        logging.info(s)\n\n\ndef adjust_learning_rate(optimizer, epoch, milestones=None):\n    """"""Sets the learning rate: milestone is a list/tuple""""""\n\n    def to(epoch):\n        if epoch <= args.warmup:\n            return 1\n        elif args.warmup < epoch <= milestones[0]:\n            return 0\n        for i in range(1, len(milestones)):\n            if milestones[i - 1] < epoch <= milestones[i]:\n                return i\n        return len(milestones)\n\n    n = to(epoch)\n\n    global lr\n    lr = args.base_lr * (0.2 ** n)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef save_checkpoint(state, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, filename)\n    logging.info(f\'Save checkpoint to {filename}\')\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    model.train()\n\n    end = time.time()\n    # loader is batch style\n    # for i, (input, target) in enumerate(train_loader):\n    for i, (input, target) in enumerate(train_loader):\n        target.requires_grad = False\n        target = target.cuda(non_blocking=True)\n        output = model(input)\n\n        data_time.update(time.time() - end)\n\n        if args.loss.lower() == \'vdc\':\n            loss = criterion(output, target)\n        elif args.loss.lower() == \'wpdc\':\n            loss = criterion(output, target)\n        elif args.loss.lower() == \'pdc\':\n            loss = criterion(output, target)\n        else:\n            raise Exception(f\'Unknown loss {args.loss}\')\n\n        losses.update(loss.item(), input.size(0))\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # log\n        if i % args.print_freq == 0:\n            logging.info(f\'Epoch: [{epoch}][{i}/{len(train_loader)}]\\t\'\n                         f\'LR: {lr:8f}\\t\'\n                         f\'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                         # f\'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                         f\'Loss {losses.val:.4f} ({losses.avg:.4f})\')\n\n\ndef validate(val_loader, model, criterion, epoch):\n    model.eval()\n\n    end = time.time()\n    with torch.no_grad():\n        losses = []\n        for i, (input, target) in enumerate(val_loader):\n            # compute output\n            target.requires_grad = False\n            target = target.cuda(non_blocking=True)\n            output = model(input)\n\n            loss = criterion(output, target)\n            losses.append(loss.item())\n\n        elapse = time.time() - end\n        loss = np.mean(losses)\n        logging.info(f\'Val: [{epoch}][{len(val_loader)}]\\t\'\n                     f\'Loss {loss:.4f}\\t\'\n                     f\'Time {elapse:.3f}\')\n\n\ndef main():\n    parse_args()  # parse global argsl\n\n    # logging setup\n    logging.basicConfig(\n        format=\'[%(asctime)s] [p%(process)s] [%(pathname)s:%(lineno)d] [%(levelname)s] %(message)s\',\n        level=logging.INFO,\n        handlers=[\n            logging.FileHandler(args.log_file, mode=args.log_mode),\n            logging.StreamHandler()\n        ]\n    )\n\n    print_args(args)  # print args\n\n    # step1: define the model structure\n    model = getattr(mobilenet_v1, args.arch)(num_classes=args.num_classes)\n\n    torch.cuda.set_device(args.devices_id[0])  # fix bug for `ERROR: all tensors must be on devices[0]`\n\n    model = nn.DataParallel(model, device_ids=args.devices_id).cuda()  # -> GPU\n\n    # step2: optimization: loss and optimization method\n    # criterion = nn.MSELoss(size_average=args.size_average).cuda()\n    if args.loss.lower() == \'wpdc\':\n        print(args.opt_style)\n        criterion = WPDCLoss(opt_style=args.opt_style).cuda()\n        logging.info(\'Use WPDC Loss\')\n    elif args.loss.lower() == \'vdc\':\n        criterion = VDCLoss(opt_style=args.opt_style).cuda()\n        logging.info(\'Use VDC Loss\')\n    elif args.loss.lower() == \'pdc\':\n        criterion = nn.MSELoss(size_average=args.size_average).cuda()\n        logging.info(\'Use PDC loss\')\n    else:\n        raise Exception(f\'Unknown Loss {args.loss}\')\n\n    optimizer = torch.optim.SGD(model.parameters(),\n                                lr=args.base_lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay,\n                                nesterov=True)\n    # step 2.1 resume\n    if args.resume:\n        if Path(args.resume).is_file():\n            logging.info(f\'=> loading checkpoint {args.resume}\')\n\n            checkpoint = torch.load(args.resume, map_location=lambda storage, loc: storage)[\'state_dict\']\n            # checkpoint = torch.load(args.resume)[\'state_dict\']\n            model.load_state_dict(checkpoint)\n\n        else:\n            logging.info(f\'=> no checkpoint found at {args.resume}\')\n\n    # step3: data\n    normalize = NormalizeGjz(mean=127.5, std=128)  # may need optimization\n\n    train_dataset = DDFADataset(\n        root=args.root,\n        filelists=args.filelists_train,\n        param_fp=args.param_fp_train,\n        transform=transforms.Compose([ToTensorGjz(), normalize])\n    )\n    val_dataset = DDFADataset(\n        root=args.root,\n        filelists=args.filelists_val,\n        param_fp=args.param_fp_val,\n        transform=transforms.Compose([ToTensorGjz(), normalize])\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.workers,\n                              shuffle=True, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_dataset, batch_size=args.val_batch_size, num_workers=args.workers,\n                            shuffle=False, pin_memory=True)\n\n    # step4: run\n    cudnn.benchmark = True\n    if args.test_initial:\n        logging.info(\'Testing from initial\')\n        validate(val_loader, model, criterion, args.start_epoch)\n\n    for epoch in range(args.start_epoch, args.epochs + 1):\n        # adjust learning rate\n        adjust_learning_rate(optimizer, epoch, args.milestones)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n        filename = f\'{args.snapshot}_checkpoint_epoch_{epoch}.pth.tar\'\n        save_checkpoint(\n            {\n                \'epoch\': epoch,\n                \'state_dict\': model.state_dict(),\n                # \'optimizer\': optimizer.state_dict()\n            },\n            filename\n        )\n\n        validate(val_loader, model, criterion, epoch)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
vdc_loss.py,6,"b'#!/usr/bin/env python3\n# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nfrom utils.io import _load, _numpy_to_cuda, _numpy_to_tensor\nfrom utils.params import *\n\n_to_tensor = _numpy_to_cuda  # gpu\n\n\ndef _parse_param_batch(param):\n    """"""Work for both numpy and tensor""""""\n    N = param.shape[0]\n    p_ = param[:, :12].view(N, 3, -1)\n    p = p_[:, :, :3]\n    offset = p_[:, :, -1].view(N, 3, 1)\n    alpha_shp = param[:, 12:52].view(N, -1, 1)\n    alpha_exp = param[:, 52:].view(N, -1, 1)\n    return p, offset, alpha_shp, alpha_exp\n\n\nclass VDCLoss(nn.Module):\n    def __init__(self, opt_style=\'all\'):\n        super(VDCLoss, self).__init__()\n\n        self.u = _to_tensor(u)\n        self.param_mean = _to_tensor(param_mean)\n        self.param_std = _to_tensor(param_std)\n        self.w_shp = _to_tensor(w_shp)\n        self.w_exp = _to_tensor(w_exp)\n\n        self.keypoints = _to_tensor(keypoints)\n        self.u_base = self.u[self.keypoints]\n        self.w_shp_base = self.w_shp[self.keypoints]\n        self.w_exp_base = self.w_exp[self.keypoints]\n\n        self.w_shp_length = self.w_shp.shape[0] // 3\n\n        self.opt_style = opt_style\n\n    def reconstruct_and_parse(self, input, target):\n        # reconstruct\n        param = input * self.param_std + self.param_mean\n        param_gt = target * self.param_std + self.param_mean\n\n        # parse param\n        p, offset, alpha_shp, alpha_exp = _parse_param_batch(param)\n        pg, offsetg, alpha_shpg, alpha_expg = _parse_param_batch(param_gt)\n\n        return (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg)\n\n    def forward_all(self, input, target):\n        (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg) \\\n            = self.reconstruct_and_parse(input, target)\n\n        N = input.shape[0]\n        offset[:, -1] = offsetg[:, -1]\n        gt_vertex = pg @ (self.u + self.w_shp @ alpha_shpg + self.w_exp @ alpha_expg) \\\n            .view(N, -1, 3).permute(0, 2, 1) + offsetg\n        vertex = p @ (self.u + self.w_shp @ alpha_shp + self.w_exp @ alpha_exp) \\\n            .view(N, -1, 3).permute(0, 2, 1) + offset\n\n        diff = (gt_vertex - vertex) ** 2\n        loss = torch.mean(diff)\n        return loss\n\n    def forward_resample(self, input, target, resample_num=132):\n        (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg) \\\n            = self.reconstruct_and_parse(input, target)\n\n        # resample index\n        index = torch.randperm(self.w_shp_length)[:resample_num].reshape(-1, 1)\n        keypoints_resample = torch.cat((3 * index, 3 * index + 1, 3 * index + 2), dim=1).view(-1).cuda()\n        keypoints_mix = torch.cat((self.keypoints, keypoints_resample))\n        w_shp_base = self.w_shp[keypoints_mix]\n        u_base = self.u[keypoints_mix]\n        w_exp_base = self.w_exp[keypoints_mix]\n\n        offset[:, -1] = offsetg[:, -1]\n\n        N = input.shape[0]\n        gt_vertex = pg @ (u_base + w_shp_base @ alpha_shpg + w_exp_base @ alpha_expg) \\\n            .view(N, -1, 3).permute(0, 2, 1) + offsetg\n        vertex = p @ (u_base + w_shp_base @ alpha_shp + w_exp_base @ alpha_exp) \\\n            .view(N, -1, 3).permute(0, 2, 1) + offset\n        diff = (gt_vertex - vertex) ** 2\n        loss = torch.mean(diff)\n        return loss\n\n    def forward(self, input, target):\n        if self.opt_style == \'all\':\n            return self.forward_all(input, target)\n        elif self.opt_style == \'resample\':\n            return self.forward_resample(input, target)\n        else:\n            raise Exception(f\'Unknown opt style: f{opt_style}\')\n\n\nif __name__ == \'__main__\':\n    pass\n'"
video_demo.py,3,"b""#!/usr/bin/env python3\n# coding: utf-8\nimport torch\nimport torchvision.transforms as transforms\nimport mobilenet_v1\nimport numpy as np\nimport cv2\nimport dlib\nfrom utils.ddfa import ToTensorGjz, NormalizeGjz\nimport scipy.io as sio\nfrom utils.inference import (\n    parse_roi_box_from_landmark,\n    crop_img,\n    predict_68pts,\n    predict_dense,\n)\nfrom utils.cv_plot import plot_kpt\nfrom utils.render import get_depths_image, cget_depths_image, cpncc\nfrom utils.paf import gen_img_paf\nimport argparse\nimport torch.backends.cudnn as cudnn\n\nSTD_SIZE = 120\n\n\ndef main(args):\n    # 0. open video\n    # vc = cv2.VideoCapture(str(args.video) if len(args.video) == 1 else args.video)\n    vc = cv2.VideoCapture(args.video if int(args.video) != 0 else 0)\n\n    # 1. load pre-tained model\n    checkpoint_fp = 'models/phase1_wpdc_vdc.pth.tar'\n    arch = 'mobilenet_1'\n\n    tri = sio.loadmat('visualize/tri.mat')['tri']\n    transform = transforms.Compose([ToTensorGjz(), NormalizeGjz(mean=127.5, std=128)])\n\n    checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)[\n        'state_dict'\n    ]\n    model = getattr(mobilenet_v1, arch)(\n        num_classes=62\n    )  # 62 = 12(pose) + 40(shape) +10(expression)\n\n    model_dict = model.state_dict()\n    # because the model is trained by multiple gpus, prefix module should be removed\n    for k in checkpoint.keys():\n        model_dict[k.replace('module.', '')] = checkpoint[k]\n    model.load_state_dict(model_dict)\n    if args.mode == 'gpu':\n        cudnn.benchmark = True\n        model = model.cuda()\n    model.eval()\n\n    # 2. load dlib model for face detection and landmark used for face cropping\n    dlib_landmark_model = 'models/shape_predictor_68_face_landmarks.dat'\n    face_regressor = dlib.shape_predictor(dlib_landmark_model)\n    face_detector = dlib.get_frontal_face_detector()\n\n    # 3. forward\n    success, frame = vc.read()\n    last_frame_pts = []\n\n    while success:\n        if len(last_frame_pts) == 0:\n            rects = face_detector(frame, 1)\n            for rect in rects:\n                pts = face_regressor(frame, rect).parts()\n                pts = np.array([[pt.x, pt.y] for pt in pts]).T\n                last_frame_pts.append(pts)\n\n        vertices_lst = []\n        for lmk in last_frame_pts:\n            roi_box = parse_roi_box_from_landmark(lmk)\n            img = crop_img(frame, roi_box)\n            img = cv2.resize(\n                img, dsize=(STD_SIZE, STD_SIZE), interpolation=cv2.INTER_LINEAR\n            )\n            input = transform(img).unsqueeze(0)\n            with torch.no_grad():\n                if args.mode == 'gpu':\n                    input = input.cuda()\n                param = model(input)\n                param = param.squeeze().cpu().numpy().flatten().astype(np.float32)\n            pts68 = predict_68pts(param, roi_box)\n            vertex = predict_dense(param, roi_box)\n            lmk[:] = pts68[:2]\n            vertices_lst.append(vertex)\n\n        pncc = cpncc(frame, vertices_lst, tri - 1) / 255.0\n        frame = frame / 255.0 * (1.0 - pncc)\n        cv2.imshow('3ddfa', frame)\n        cv2.waitKey(1)\n        success, frame = vc.read()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='3DDFA inference pipeline')\n    parser.add_argument(\n        '-v',\n        '--video',\n        default='0',\n        type=str,\n        help='video file path or opencv cam index',\n    )\n    parser.add_argument('-m', '--mode', default='cpu', type=str, help='gpu or cpu mode')\n\n    args = parser.parse_args()\n    main(args)\n"""
visualize.py,0,"b'#!/usr/bin/env python3\n# coding: utf-8\n\nfrom benchmark import extract_param\nfrom utils.ddfa import reconstruct_vertex\nfrom utils.io import _dump, _load\nimport os.path as osp\nfrom skimage import io\nimport matplotlib.pyplot as plt\nfrom benchmark_aflw2000 import convert_to_ori\nimport scipy.io as sio\n\n\ndef aflw2000():\n    arch = \'mobilenet_1\'\n    device_ids = [0]\n    checkpoint_fp = \'models/phase1_wpdc_vdc.pth.tar\'\n\n    params = extract_param(\n        checkpoint_fp=checkpoint_fp,\n        root=\'test.data/AFLW2000-3D_crop\',\n        filelists=\'test.data/AFLW2000-3D_crop.list\',\n        arch=arch,\n        device_ids=device_ids,\n        batch_size=128)\n    _dump(\'res/params_aflw2000.npy\', params)\n\n\ndef draw_landmarks():\n    filelists = \'test.data/AFLW2000-3D_crop.list\'\n    root = \'AFLW-2000-3D/\'\n    fns = open(filelists).read().strip().split(\'\\n\')\n    params = _load(\'res/params_aflw2000.npy\')\n\n    for i in range(2000):\n        plt.close()\n        img_fp = osp.join(root, fns[i])\n        img = io.imread(img_fp)\n        lms = reconstruct_vertex(params[i], dense=False)\n        lms = convert_to_ori(lms, i)\n\n        # print(lms.shape)\n        fig = plt.figure(figsize=plt.figaspect(.5))\n        # fig = plt.figure(figsize=(8, 4))\n        ax = fig.add_subplot(1, 2, 1)\n        ax.imshow(img)\n\n        alpha = 0.8\n        markersize = 4\n        lw = 1.5\n        color = \'w\'\n        markeredgecolor = \'black\'\n\n        nums = [0, 17, 22, 27, 31, 36, 42, 48, 60, 68]\n        for ind in range(len(nums) - 1):\n            l, r = nums[ind], nums[ind + 1]\n            ax.plot(lms[0, l:r], lms[1, l:r], color=color, lw=lw, alpha=alpha - 0.1)\n\n            ax.plot(lms[0, l:r], lms[1, l:r], marker=\'o\', linestyle=\'None\', markersize=markersize, color=color,\n                    markeredgecolor=markeredgecolor, alpha=alpha)\n\n        ax.axis(\'off\')\n\n        # 3D\n        ax = fig.add_subplot(1, 2, 2, projection=\'3d\')\n        lms[1] = img.shape[1] - lms[1]\n        lms[2] = -lms[2]\n\n        # print(lms)\n        ax.scatter(lms[0], lms[2], lms[1], c=""cyan"", alpha=1.0, edgecolor=\'b\')\n\n        for ind in range(len(nums) - 1):\n            l, r = nums[ind], nums[ind + 1]\n            ax.plot3D(lms[0, l:r], lms[2, l:r], lms[1, l:r], color=\'blue\')\n\n        ax.view_init(elev=5., azim=-95)\n        # ax.set_xlabel(\'x\')\n        # ax.set_ylabel(\'y\')\n        # ax.set_zlabel(\'z\')\n\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        ax.set_zticklabels([])\n\n        plt.tight_layout()\n        # plt.show()\n\n        wfp = f\'res/AFLW-2000-3D/{osp.basename(img_fp)}\'\n        plt.savefig(wfp, dpi=200)\n\n\ndef gen_3d_vertex():\n    filelists = \'test.data/AFLW2000-3D_crop.list\'\n    root = \'AFLW-2000-3D/\'\n    fns = open(filelists).read().strip().split(\'\\n\')\n    params = _load(\'res/params_aflw2000.npy\')\n\n    sel = [\'00427\', \'00439\', \'00475\', \'00477\', \'00497\', \'00514\', \'00562\', \'00623\', \'01045\', \'01095\', \'01104\', \'01506\',\n           \'01621\', \'02214\', \'02244\', \'03906\', \'04157\']\n    sel = list(map(lambda x: f\'image{x}.jpg\', sel))\n    for i in range(2000):\n        fn = fns[i]\n        if fn in sel:\n            vertex = reconstruct_vertex(params[i], dense=True)\n            wfp = osp.join(\'res/AFLW-2000-3D_vertex/\', fn.replace(\'.jpg\', \'.mat\'))\n            print(wfp)\n            sio.savemat(wfp, {\'vertex\': vertex})\n\n\ndef main():\n    # step1: extract params\n    # aflw2000()\n\n    # step2: draw landmarks\n    # draw_landmarks()\n\n    # step3: visual 3d vertex\n    gen_3d_vertex()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
wpdc_loss.py,12,"b'#!/usr/bin/env python3\n# coding: utf-8\n\nimport torch\nimport torch.nn as nn\nfrom math import sqrt\nfrom utils.io import _numpy_to_cuda\nfrom utils.params import *\n\n_to_tensor = _numpy_to_cuda  # gpu\n\n\ndef _parse_param_batch(param):\n    """"""Work for both numpy and tensor""""""\n    N = param.shape[0]\n    p_ = param[:, :12].view(N, 3, -1)\n    p = p_[:, :, :3]\n    offset = p_[:, :, -1].view(N, 3, 1)\n    alpha_shp = param[:, 12:52].view(N, -1, 1)\n    alpha_exp = param[:, 52:].view(N, -1, 1)\n    return p, offset, alpha_shp, alpha_exp\n\n\nclass WPDCLoss(nn.Module):\n    """"""Input and target are all 62-d param""""""\n\n    def __init__(self, opt_style=\'resample\', resample_num=132):\n        super(WPDCLoss, self).__init__()\n        self.opt_style = opt_style\n        self.param_mean = _to_tensor(param_mean)\n        self.param_std = _to_tensor(param_std)\n\n        self.u = _to_tensor(u)\n        self.w_shp = _to_tensor(w_shp)\n        self.w_exp = _to_tensor(w_exp)\n        self.w_norm = _to_tensor(w_norm)\n\n        self.w_shp_length = self.w_shp.shape[0] // 3\n        self.keypoints = _to_tensor(keypoints)\n        self.resample_num = resample_num\n\n    def reconstruct_and_parse(self, input, target):\n        # reconstruct\n        param = input * self.param_std + self.param_mean\n        param_gt = target * self.param_std + self.param_mean\n\n        # parse param\n        p, offset, alpha_shp, alpha_exp = _parse_param_batch(param)\n        pg, offsetg, alpha_shpg, alpha_expg = _parse_param_batch(param_gt)\n\n        return (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg)\n\n    def _calc_weights_resample(self, input_, target_):\n        # resample index\n        if self.resample_num <= 0:\n            keypoints_mix = self.keypoints\n        else:\n            index = torch.randperm(self.w_shp_length)[:self.resample_num].reshape(-1, 1)\n            keypoints_resample = torch.cat((3 * index, 3 * index + 1, 3 * index + 2), dim=1).view(-1).cuda()\n            keypoints_mix = torch.cat((self.keypoints, keypoints_resample))\n        w_shp_base = self.w_shp[keypoints_mix]\n        u_base = self.u[keypoints_mix]\n        w_exp_base = self.w_exp[keypoints_mix]\n\n        input = torch.tensor(input_.data.clone(), requires_grad=False)\n        target = torch.tensor(target_.data.clone(), requires_grad=False)\n\n        (p, offset, alpha_shp, alpha_exp), (pg, offsetg, alpha_shpg, alpha_expg) \\\n            = self.reconstruct_and_parse(input, target)\n\n        input = self.param_std * input + self.param_mean\n        target = self.param_std * target + self.param_mean\n\n        N = input.shape[0]\n\n        offset[:, -1] = offsetg[:, -1]\n\n        weights = torch.zeros_like(input, dtype=torch.float)\n        tmpv = (u_base + w_shp_base @ alpha_shp + w_exp_base @ alpha_exp).view(N, -1, 3).permute(0, 2, 1)\n\n        tmpv_norm = torch.norm(tmpv, dim=2)\n        offset_norm = sqrt(w_shp_base.shape[0] // 3)\n\n        # for pose\n        param_diff_pose = torch.abs(input[:, :11] - target[:, :11])\n        for ind in range(11):\n            if ind in [0, 4, 8]:\n                weights[:, ind] = param_diff_pose[:, ind] * tmpv_norm[:, 0]\n            elif ind in [1, 5, 9]:\n                weights[:, ind] = param_diff_pose[:, ind] * tmpv_norm[:, 1]\n            elif ind in [2, 6, 10]:\n                weights[:, ind] = param_diff_pose[:, ind] * tmpv_norm[:, 2]\n            else:\n                weights[:, ind] = param_diff_pose[:, ind] * offset_norm\n\n        ## This is the optimizest version\n        # for shape_exp\n        magic_number = 0.00057339936  # scale\n        param_diff_shape_exp = torch.abs(input[:, 12:] - target[:, 12:])\n        # weights[:, 12:] = magic_number * param_diff_shape_exp * self.w_norm\n        w = torch.cat((w_shp_base, w_exp_base), dim=1)\n        w_norm = torch.norm(w, dim=0)\n        # print(\'here\')\n        weights[:, 12:] = magic_number * param_diff_shape_exp * w_norm\n\n        eps = 1e-6\n        weights[:, :11] += eps\n        weights[:, 12:] += eps\n\n        # normalize the weights\n        maxes, _ = weights.max(dim=1)\n        maxes = maxes.view(-1, 1)\n        weights /= maxes\n\n        # zero the z\n        weights[:, 11] = 0\n\n        return weights\n\n    def forward(self, input, target, weights_scale=10):\n        if self.opt_style == \'resample\':\n            weights = self._calc_weights_resample(input, target)\n            loss = weights * (input - target) ** 2\n            return loss.mean()\n        else:\n            raise Exception(f\'Unknown opt style: {self.opt_style}\')\n\n\nif __name__ == \'__main__\':\n    pass\n'"
demo@obama/convert_imgs_to_video.py,0,"b""#!/usr/bin/env python3\n# coding: utf-8\n\nimport os\nimport os.path as osp\nimport sys\nfrom glob import glob\nimport imageio\n\n\ndef main():\n    assert len(sys.argv) >= 2\n    d = sys.argv[1]\n\n    fps = glob(osp.join(d, '*.jpg'))\n    fps = sorted(fps, key=lambda x: int(x.split('/')[-1].replace('.jpg', '')))\n\n    imgs = []\n    for fp in fps:\n        img = imageio.imread(fp)\n        imgs.append(img)\n\n    if len(sys.argv) >= 3:\n        imageio.mimwrite(sys.argv[2], imgs, fps=24, macro_block_size=None)\n    else:\n        imageio.mimwrite(osp.basename(d) + '.mp4', imgs, fps=24, macro_block_size=None)\n\n\nif __name__ == '__main__':\n    main()\n"""
demo@obama/rendering.py,0,"b""#!/usr/bin/env python3\n# coding: utf-8\n\nimport sys\n\nsys.path.append('../')\nimport os\nimport os.path as osp\nfrom glob import glob\n\nfrom utils.lighting import RenderPipeline\nimport numpy as np\nimport scipy.io as sio\nimport imageio\n\ncfg = {\n    'intensity_ambient': 0.3,\n    'color_ambient': (1, 1, 1),\n    'intensity_directional': 0.6,\n    'color_directional': (1, 1, 1),\n    'intensity_specular': 0.1,\n    'specular_exp': 5,\n    'light_pos': (0, 0, 5),\n    'view_pos': (0, 0, 5)\n}\n\n\ndef _to_ctype(arr):\n    if not arr.flags.c_contiguous:\n        return arr.copy(order='C')\n    return arr\n\n\ndef obama_demo():\n    wd = 'obama_res@dense_py'\n    if not osp.exists(wd):\n        os.mkdir(wd)\n\n    app = RenderPipeline(**cfg)\n    img_fps = sorted(glob('obama/*.jpg'))\n    triangles = sio.loadmat('tri_refine.mat')['tri']  # mx3\n    triangles = _to_ctype(triangles).astype(np.int32)  # for type compatible\n\n    for img_fp in img_fps[:]:\n        vertices = sio.loadmat(img_fp.replace('.jpg', '_0.mat'))['vertex'].T  # mx3\n        img = imageio.imread(img_fp).astype(np.float32) / 255.\n\n        # end = time.clock()\n        img_render = app(vertices, triangles, img)\n        # print('Elapse: {:.1f}ms'.format((time.clock() - end) * 1000))\n\n        img_wfp = osp.join(wd, osp.basename(img_fp))\n        imageio.imwrite(img_wfp, img_render)\n        print('Writing to {}'.format(img_wfp))\n\n\nif __name__ == '__main__':\n    obama_demo()\n"""
demo@obama/rendering_demo.py,0,"b'#!/usr/bin/env python3\n# coding: utf-8\n\n""""""\nA demo for rendering mesh generated by `main.py`\n""""""\n\nfrom rendering import cfg, _to_ctype, RenderPipeline\nimport scipy.io as sio\nimport imageio\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef test():\n    # 1. first, using main.py to generate dense vertices, like emma_input_0.mat\n    fp = \'../samples/emma_input_0.mat\'\n    vertices = sio.loadmat(fp)[\'vertex\'].T  # 3xm\n    print(vertices.shape)\n    img = imageio.imread(\'../samples/emma_input.jpg\').astype(np.float32) / 255.\n\n    # 2. render it\n    # triangles = sio.loadmat(\'tri_refine.mat\')[\'tri\']  # mx3\n    triangles = sio.loadmat(\'../visualize/tri.mat\')[\'tri\'].T - 1  # mx3\n    print(triangles.shape)\n    triangles = _to_ctype(triangles).astype(np.int32)  # for type compatible\n    app = RenderPipeline(**cfg)\n    img_render = app(vertices, triangles, img)\n\n    plt.imshow(img_render)\n    plt.show()\n\n\ndef main():\n    test()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
training/train.py,9,"b'#!/usr/bin/env python3\n# coding: utf-8\n\nimport os.path as osp\nfrom pathlib import Path\nimport numpy as np\nimport argparse\nimport time\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport mobilenet_v1\nimport torch.backends.cudnn as cudnn\n\nfrom utils.ddfa import DDFADataset, ToTensorGjz, NormalizeGjz\nfrom utils.ddfa import str2bool, AverageMeter\nfrom utils.io import mkdir\nfrom vdc_loss import VDCLoss\nfrom wpdc_loss import WPDCLoss\n\n# global args (configuration)\nargs = None\nlr = None\narch_choices = [\'mobilenet_2\', \'mobilenet_1\', \'mobilenet_075\', \'mobilenet_05\', \'mobilenet_025\']\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'3DMM Fitting\')\n    parser.add_argument(\'-j\', \'--workers\', default=6, type=int)\n    parser.add_argument(\'--epochs\', default=40, type=int)\n    parser.add_argument(\'--start-epoch\', default=1, type=int)\n    parser.add_argument(\'-b\', \'--batch-size\', default=128, type=int)\n    parser.add_argument(\'-vb\', \'--val-batch-size\', default=32, type=int)\n    parser.add_argument(\'--base-lr\', \'--learning-rate\', default=0.001, type=float)\n    parser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                        help=\'momentum\')\n    parser.add_argument(\'--weight-decay\', \'--wd\', default=5e-4, type=float)\n    parser.add_argument(\'--print-freq\', \'-p\', default=20, type=int)\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\')\n    parser.add_argument(\'--devices-id\', default=\'0,1\', type=str)\n    parser.add_argument(\'--filelists-train\',\n                        default=\'\', type=str)\n    parser.add_argument(\'--filelists-val\',\n                        default=\'\', type=str)\n    parser.add_argument(\'--root\', default=\'\')\n    parser.add_argument(\'--snapshot\', default=\'\', type=str)\n    parser.add_argument(\'--log-file\', default=\'output.log\', type=str)\n    parser.add_argument(\'--log-mode\', default=\'w\', type=str)\n    parser.add_argument(\'--size-average\', default=\'true\', type=str2bool)\n    parser.add_argument(\'--num-classes\', default=62, type=int)\n    parser.add_argument(\'--arch\', default=\'mobilenet_1\', type=str,\n                        choices=arch_choices)\n    parser.add_argument(\'--frozen\', default=\'false\', type=str2bool)\n    parser.add_argument(\'--milestones\', default=\'15,25,30\', type=str)\n    parser.add_argument(\'--task\', default=\'all\', type=str)\n    parser.add_argument(\'--test_initial\', default=\'false\', type=str2bool)\n    parser.add_argument(\'--warmup\', default=-1, type=int)\n    parser.add_argument(\'--param-fp-train\',\n                        default=\'\',\n                        type=str)\n    parser.add_argument(\'--param-fp-val\',\n                        default=\'\')\n    parser.add_argument(\'--opt-style\', default=\'resample\', type=str)  # resample\n    parser.add_argument(\'--resample-num\', default=132, type=int)\n    parser.add_argument(\'--loss\', default=\'vdc\', type=str)\n\n    global args\n    args = parser.parse_args()\n\n    # some other operations\n    args.devices_id = [int(d) for d in args.devices_id.split(\',\')]\n    args.milestones = [int(m) for m in args.milestones.split(\',\')]\n\n    snapshot_dir = osp.split(args.snapshot)[0]\n    mkdir(snapshot_dir)\n\n\ndef print_args(args):\n    for arg in vars(args):\n        s = arg + \': \' + str(getattr(args, arg))\n        logging.info(s)\n\n\ndef adjust_learning_rate(optimizer, epoch, milestones=None):\n    """"""Sets the learning rate: milestone is a list/tuple""""""\n\n    def to(epoch):\n        if epoch <= args.warmup:\n            return 1\n        elif args.warmup < epoch <= milestones[0]:\n            return 0\n        for i in range(1, len(milestones)):\n            if milestones[i - 1] < epoch <= milestones[i]:\n                return i\n        return len(milestones)\n\n    n = to(epoch)\n\n    global lr\n    lr = args.base_lr * (0.2 ** n)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef save_checkpoint(state, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, filename)\n    logging.info(f\'Save checkpoint to {filename}\')\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    model.train()\n\n    end = time.time()\n    # loader is batch style\n    # for i, (input, target) in enumerate(train_loader):\n    for i, (input, target) in enumerate(train_loader):\n        target.requires_grad = False\n        target = target.cuda(non_blocking=True)\n        output = model(input)\n\n        data_time.update(time.time() - end)\n\n        if args.loss.lower() == \'vdc\':\n            loss = criterion(output, target)\n        elif args.loss.lower() == \'wpdc\':\n            loss = criterion(output, target)\n        elif args.loss.lower() == \'pdc\':\n            loss = criterion(output, target)\n        else:\n            raise Exception(f\'Unknown loss {args.loss}\')\n\n        losses.update(loss.item(), input.size(0))\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # log\n        if i % args.print_freq == 0:\n            logging.info(f\'Epoch: [{epoch}][{i}/{len(train_loader)}]\\t\'\n                         f\'LR: {lr:8f}\\t\'\n                         f\'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                         # f\'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                         f\'Loss {losses.val:.4f} ({losses.avg:.4f})\')\n\n\ndef validate(val_loader, model, criterion, epoch):\n    model.eval()\n\n    end = time.time()\n    with torch.no_grad():\n        losses = []\n        for i, (input, target) in enumerate(val_loader):\n            # compute output\n            target.requires_grad = False\n            target = target.cuda(non_blocking=True)\n            output = model(input)\n\n            loss = criterion(output, target)\n            losses.append(loss.item())\n\n        elapse = time.time() - end\n        loss = np.mean(losses)\n        logging.info(f\'Val: [{epoch}][{len(val_loader)}]\\t\'\n                     f\'Loss {loss:.4f}\\t\'\n                     f\'Time {elapse:.3f}\')\n\n\ndef main():\n    parse_args()  # parse global argsl\n\n    # logging setup\n    logging.basicConfig(\n        format=\'[%(asctime)s] [p%(process)s] [%(pathname)s:%(lineno)d] [%(levelname)s] %(message)s\',\n        level=logging.INFO,\n        handlers=[\n            logging.FileHandler(args.log_file, mode=args.log_mode),\n            logging.StreamHandler()\n        ]\n    )\n\n    print_args(args)  # print args\n\n    # step1: define the model structure\n    model = getattr(mobilenet_v1, args.arch)(num_classes=args.num_classes)\n\n    torch.cuda.set_device(args.devices_id[0])  # fix bug for `ERROR: all tensors must be on devices[0]`\n\n    model = nn.DataParallel(model, device_ids=args.devices_id).cuda()  # -> GPU\n\n    # step2: optimization: loss and optimization method\n    # criterion = nn.MSELoss(size_average=args.size_average).cuda()\n    if args.loss.lower() == \'wpdc\':\n        print(args.opt_style)\n        criterion = WPDCLoss(opt_style=args.opt_style).cuda()\n        logging.info(\'Use WPDC Loss\')\n    elif args.loss.lower() == \'vdc\':\n        criterion = VDCLoss(opt_style=args.opt_style).cuda()\n        logging.info(\'Use VDC Loss\')\n    elif args.loss.lower() == \'pdc\':\n        criterion = nn.MSELoss(size_average=args.size_average).cuda()\n        logging.info(\'Use PDC loss\')\n    else:\n        raise Exception(f\'Unknown Loss {args.loss}\')\n\n    optimizer = torch.optim.SGD(model.parameters(),\n                                lr=args.base_lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay,\n                                nesterov=True)\n    # step 2.1 resume\n    if args.resume:\n        if Path(args.resume).is_file():\n            logging.info(f\'=> loading checkpoint {args.resume}\')\n\n            checkpoint = torch.load(args.resume, map_location=lambda storage, loc: storage)[\'state_dict\']\n            # checkpoint = torch.load(args.resume)[\'state_dict\']\n            model.load_state_dict(checkpoint)\n\n        else:\n            logging.info(f\'=> no checkpoint found at {args.resume}\')\n\n    # step3: data\n    normalize = NormalizeGjz(mean=127.5, std=128)  # may need optimization\n\n    train_dataset = DDFADataset(\n        root=args.root,\n        filelists=args.filelists_train,\n        param_fp=args.param_fp_train,\n        transform=transforms.Compose([ToTensorGjz(), normalize])\n    )\n    val_dataset = DDFADataset(\n        root=args.root,\n        filelists=args.filelists_val,\n        param_fp=args.param_fp_val,\n        transform=transforms.Compose([ToTensorGjz(), normalize])\n    )\n\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.workers,\n                              shuffle=True, pin_memory=True, drop_last=True)\n    val_loader = DataLoader(val_dataset, batch_size=args.val_batch_size, num_workers=args.workers,\n                            shuffle=False, pin_memory=True)\n\n    # step4: run\n    cudnn.benchmark = True\n    if args.test_initial:\n        logging.info(\'Testing from initial\')\n        validate(val_loader, model, criterion, args.start_epoch)\n\n    for epoch in range(args.start_epoch, args.epochs + 1):\n        # adjust learning rate\n        adjust_learning_rate(optimizer, epoch, args.milestones)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n        filename = f\'{args.snapshot}_checkpoint_epoch_{epoch}.pth.tar\'\n        save_checkpoint(\n            {\n                \'epoch\': epoch,\n                \'state_dict\': model.state_dict(),\n                # \'optimizer\': optimizer.state_dict()\n            },\n            filename\n        )\n\n        validate(val_loader, model, criterion, epoch)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/__init__.py,0,b''
utils/cv_plot.py,0,"b'#!/usr/bin/env python3\n# coding: utf-8\n\n\n""""""\nModified from: https://sourcegraph.com/github.com/YadiraF/PRNet@master/-/blob/utils/cv_plot.py\n""""""\n\nimport numpy as np\nimport cv2\n\nfrom utils.inference import calc_hypotenuse\n\nend_list = np.array([17, 22, 27, 42, 48, 31, 36, 68], dtype=np.int32) - 1\n\n\ndef plot_kpt(image, kpt):\n    \'\'\' Draw 68 key points\n    Args:\n        image: the input image\n        kpt: (68, 3).\n    \'\'\'\n    image = image.copy()\n    kpt = np.round(kpt).astype(np.int32)\n    for i in range(kpt.shape[0]):\n        st = kpt[i, :2]\n        image = cv2.circle(image, (st[0], st[1]), 1, (0, 0, 255), 2)\n        if i in end_list:\n            continue\n        ed = kpt[i + 1, :2]\n        image = cv2.line(image, (st[0], st[1]), (ed[0], ed[1]), (255, 255, 255), 1)\n    return image\n\n\ndef build_camera_box(rear_size=90):\n    point_3d = []\n    rear_depth = 0\n    point_3d.append((-rear_size, -rear_size, rear_depth))\n    point_3d.append((-rear_size, rear_size, rear_depth))\n    point_3d.append((rear_size, rear_size, rear_depth))\n    point_3d.append((rear_size, -rear_size, rear_depth))\n    point_3d.append((-rear_size, -rear_size, rear_depth))\n\n    front_size = int(4 / 3 * rear_size)\n    front_depth = int(4 / 3 * rear_size)\n    point_3d.append((-front_size, -front_size, front_depth))\n    point_3d.append((-front_size, front_size, front_depth))\n    point_3d.append((front_size, front_size, front_depth))\n    point_3d.append((front_size, -front_size, front_depth))\n    point_3d.append((-front_size, -front_size, front_depth))\n    point_3d = np.array(point_3d, dtype=np.float).reshape(-1, 3)\n\n    return point_3d\n\n\ndef plot_pose_box(image, Ps, pts68s, color=(40, 255, 0), line_width=2):\n    \'\'\' Draw a 3D box as annotation of pose. Ref:https://github.com/yinguobing/head-pose-estimation/blob/master/pose_estimator.py\n    Args:\n        image: the input image\n        P: (3, 4). Affine Camera Matrix.\n        kpt: (2, 68) or (3, 68)\n    \'\'\'\n    image = image.copy()\n    if not isinstance(pts68s, list):\n        pts68s = [pts68s]\n    if not isinstance(Ps, list):\n        Ps = [Ps]\n    for i in range(len(pts68s)):\n        pts68 = pts68s[i]\n        llength = calc_hypotenuse(pts68)\n        point_3d = build_camera_box(llength)\n        P = Ps[i]\n\n        # Map to 2d image points\n        point_3d_homo = np.hstack((point_3d, np.ones([point_3d.shape[0], 1])))  # n x 4\n        point_2d = point_3d_homo.dot(P.T)[:, :2]\n\n        point_2d[:, 1] = - point_2d[:, 1]\n        point_2d[:, :2] = point_2d[:, :2] - np.mean(point_2d[:4, :2], 0) + np.mean(pts68[:2, :27], 1)\n        point_2d = np.int32(point_2d.reshape(-1, 2))\n\n        # Draw all the lines\n        cv2.polylines(image, [point_2d], True, color, line_width, cv2.LINE_AA)\n        cv2.line(image, tuple(point_2d[1]), tuple(\n            point_2d[6]), color, line_width, cv2.LINE_AA)\n        cv2.line(image, tuple(point_2d[2]), tuple(\n            point_2d[7]), color, line_width, cv2.LINE_AA)\n        cv2.line(image, tuple(point_2d[3]), tuple(\n            point_2d[8]), color, line_width, cv2.LINE_AA)\n\n    return image\n\n\ndef main():\n    pass\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/ddfa.py,2,"b'#!/usr/bin/env python3\n# coding: utf-8\n\nimport os.path as osp\nfrom pathlib import Path\nimport numpy as np\n\nimport torch\nimport torch.utils.data as data\nimport cv2\nimport pickle\nimport argparse\nfrom .io import _numpy_to_tensor, _load_cpu, _load_gpu\nfrom .params import *\n\n\ndef _parse_param(param):\n    """"""Work for both numpy and tensor""""""\n    p_ = param[:12].reshape(3, -1)\n    p = p_[:, :3]\n    offset = p_[:, -1].reshape(3, 1)\n    alpha_shp = param[12:52].reshape(-1, 1)\n    alpha_exp = param[52:].reshape(-1, 1)\n    return p, offset, alpha_shp, alpha_exp\n\n\ndef reconstruct_vertex(param, whitening=True, dense=False, transform=True):\n    """"""Whitening param -> 3d vertex, based on the 3dmm param: u_base, w_shp, w_exp\n    dense: if True, return dense vertex, else return 68 sparse landmarks. All dense or sparse vertex is transformed to\n    image coordinate space, but without alignment caused by face cropping.\n    transform: whether transform to image space\n    """"""\n    if len(param) == 12:\n        param = np.concatenate((param, [0] * 50))\n    if whitening:\n        if len(param) == 62:\n            param = param * param_std + param_mean\n        else:\n            param = np.concatenate((param[:11], [0], param[11:]))\n            param = param * param_std + param_mean\n\n    p, offset, alpha_shp, alpha_exp = _parse_param(param)\n\n    if dense:\n        vertex = p @ (u + w_shp @ alpha_shp + w_exp @ alpha_exp).reshape(3, -1, order=\'F\') + offset\n\n        if transform:\n            # transform to image coordinate space\n            vertex[1, :] = std_size + 1 - vertex[1, :]\n    else:\n        """"""For 68 pts""""""\n        vertex = p @ (u_base + w_shp_base @ alpha_shp + w_exp_base @ alpha_exp).reshape(3, -1, order=\'F\') + offset\n\n        if transform:\n            # transform to image coordinate space\n            vertex[1, :] = std_size + 1 - vertex[1, :]\n\n    return vertex\n\n\ndef img_loader(path):\n    return cv2.imread(path, cv2.IMREAD_COLOR)\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected\')\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass ToTensorGjz(object):\n    def __call__(self, pic):\n        if isinstance(pic, np.ndarray):\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n            return img.float()\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'()\'\n\n\nclass NormalizeGjz(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        tensor.sub_(self.mean).div_(self.std)\n        return tensor\n\n\nclass DDFADataset(data.Dataset):\n    def __init__(self, root, filelists, param_fp, transform=None, **kargs):\n        self.root = root\n        self.transform = transform\n        self.lines = Path(filelists).read_text().strip().split(\'\\n\')\n        self.params = _numpy_to_tensor(_load_cpu(param_fp))\n        self.img_loader = img_loader\n\n    def _target_loader(self, index):\n        target = self.params[index]\n\n        return target\n\n    def __getitem__(self, index):\n        path = osp.join(self.root, self.lines[index])\n        img = self.img_loader(path)\n\n        target = self._target_loader(index)\n\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, target\n\n    def __len__(self):\n        return len(self.lines)\n\n\nclass DDFATestDataset(data.Dataset):\n    def __init__(self, filelists, root=\'\', transform=None):\n        self.root = root\n        self.transform = transform\n        self.lines = Path(filelists).read_text().strip().split(\'\\n\')\n\n    def __getitem__(self, index):\n        path = osp.join(self.root, self.lines[index])\n        img = img_loader(path)\n\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\n    def __len__(self):\n        return len(self.lines)\n'"
utils/estimate_pose.py,0,"b'#!/usr/bin/env python3\n# coding: utf-8\n\n""""""\nReference: https://github.com/YadiraF/PRNet/blob/master/utils/estimate_pose.py\n""""""\n\nfrom math import cos, sin, atan2, asin, sqrt\nimport numpy as np\nfrom .params import param_mean, param_std\n\n\ndef parse_pose(param):\n    param = param * param_std + param_mean\n    Ps = param[:12].reshape(3, -1)  # camera matrix\n    # R = P[:, :3]\n    s, R, t3d = P2sRt(Ps)\n    P = np.concatenate((R, t3d.reshape(3, -1)), axis=1)  # without scale\n    # P = Ps / s\n    pose = matrix2angle(R)  # yaw, pitch, roll\n    # offset = p_[:, -1].reshape(3, 1)\n    return P, pose\n\n\ndef matrix2angle(R):\n    \'\'\' compute three Euler angles from a Rotation Matrix. Ref: http://www.gregslabaugh.net/publications/euler.pdf\n    Args:\n        R: (3,3). rotation matrix\n    Returns:\n        x: yaw\n        y: pitch\n        z: roll\n    \'\'\'\n    # assert(isRotationMatrix(R))\n\n    if R[2, 0] != 1 and R[2, 0] != -1:\n        x = asin(R[2, 0])\n        y = atan2(R[2, 1] / cos(x), R[2, 2] / cos(x))\n        z = atan2(R[1, 0] / cos(x), R[0, 0] / cos(x))\n\n    else:  # Gimbal lock\n        z = 0  # can be anything\n        if R[2, 0] == -1:\n            x = np.pi / 2\n            y = z + atan2(R[0, 1], R[0, 2])\n        else:\n            x = -np.pi / 2\n            y = -z + atan2(-R[0, 1], -R[0, 2])\n\n    return x, y, z\n\n\ndef P2sRt(P):\n    \'\'\' decompositing camera matrix P.\n    Args:\n        P: (3, 4). Affine Camera Matrix.\n    Returns:\n        s: scale factor.\n        R: (3, 3). rotation matrix.\n        t2d: (2,). 2d translation.\n    \'\'\'\n    t3d = P[:, 3]\n    R1 = P[0:1, :3]\n    R2 = P[1:2, :3]\n    s = (np.linalg.norm(R1) + np.linalg.norm(R2)) / 2.0\n    r1 = R1 / np.linalg.norm(R1)\n    r2 = R2 / np.linalg.norm(R2)\n    r3 = np.cross(r1, r2)\n\n    R = np.concatenate((r1, r2, r3), 0)\n    return s, R, t3d\n\n\ndef main():\n    pass\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/inference.py,0,"b'#!/usr/bin/env python3\n# coding: utf-8\n__author__ = \'cleardusk\'\n\nimport numpy as np\nfrom math import sqrt\nimport scipy.io as sio\nimport matplotlib.pyplot as plt\nfrom .ddfa import reconstruct_vertex\n\n\ndef get_suffix(filename):\n    """"""a.jpg -> jpg""""""\n    pos = filename.rfind(\'.\')\n    if pos == -1:\n        return \'\'\n    return filename[pos:]\n\n\ndef crop_img(img, roi_box):\n    h, w = img.shape[:2]\n\n    sx, sy, ex, ey = [int(round(_)) for _ in roi_box]\n    dh, dw = ey - sy, ex - sx\n    if len(img.shape) == 3:\n        res = np.zeros((dh, dw, 3), dtype=np.uint8)\n    else:\n        res = np.zeros((dh, dw), dtype=np.uint8)\n    if sx < 0:\n        sx, dsx = 0, -sx\n    else:\n        dsx = 0\n\n    if ex > w:\n        ex, dex = w, dw - (ex - w)\n    else:\n        dex = dw\n\n    if sy < 0:\n        sy, dsy = 0, -sy\n    else:\n        dsy = 0\n\n    if ey > h:\n        ey, dey = h, dh - (ey - h)\n    else:\n        dey = dh\n\n    res[dsy:dey, dsx:dex] = img[sy:ey, sx:ex]\n    return res\n\n\ndef calc_hypotenuse(pts):\n    bbox = [min(pts[0, :]), min(pts[1, :]), max(pts[0, :]), max(pts[1, :])]\n    center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n    radius = max(bbox[2] - bbox[0], bbox[3] - bbox[1]) / 2\n    bbox = [center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius]\n    llength = sqrt((bbox[2] - bbox[0]) ** 2 + (bbox[3] - bbox[1]) ** 2)\n    return llength / 3\n\n\ndef parse_roi_box_from_landmark(pts):\n    """"""calc roi box from landmark""""""\n    bbox = [min(pts[0, :]), min(pts[1, :]), max(pts[0, :]), max(pts[1, :])]\n    center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n    radius = max(bbox[2] - bbox[0], bbox[3] - bbox[1]) / 2\n    bbox = [center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius]\n\n    llength = sqrt((bbox[2] - bbox[0]) ** 2 + (bbox[3] - bbox[1]) ** 2)\n    center_x = (bbox[2] + bbox[0]) / 2\n    center_y = (bbox[3] + bbox[1]) / 2\n\n    roi_box = [0] * 4\n    roi_box[0] = center_x - llength / 2\n    roi_box[1] = center_y - llength / 2\n    roi_box[2] = roi_box[0] + llength\n    roi_box[3] = roi_box[1] + llength\n\n    return roi_box\n\n\ndef parse_roi_box_from_bbox(bbox):\n    left, top, right, bottom = bbox\n    old_size = (right - left + bottom - top) / 2\n    center_x = right - (right - left) / 2.0\n    center_y = bottom - (bottom - top) / 2.0 + old_size * 0.14\n    size = int(old_size * 1.58)\n    roi_box = [0] * 4\n    roi_box[0] = center_x - size / 2\n    roi_box[1] = center_y - size / 2\n    roi_box[2] = roi_box[0] + size\n    roi_box[3] = roi_box[1] + size\n    return roi_box\n\n\ndef dump_to_ply(vertex, tri, wfp):\n    header = """"""ply\n    format ascii 1.0\n    element vertex {}\n    property float x\n    property float y\n    property float z\n    element face {}\n    property list uchar int vertex_indices\n    end_header""""""\n\n    n_vertex = vertex.shape[1]\n    n_face = tri.shape[1]\n    header = header.format(n_vertex, n_face)\n\n    with open(wfp, \'w\') as f:\n        f.write(header + \'\\n\')\n        for i in range(n_vertex):\n            x, y, z = vertex[:, i]\n            f.write(\'{:.4f} {:.4f} {:.4f}\\n\'.format(x, y, z))\n        for i in range(n_face):\n            idx1, idx2, idx3 = tri[:, i]\n            f.write(\'3 {} {} {}\\n\'.format(idx1 - 1, idx2 - 1, idx3 - 1))\n    print(\'Dump tp {}\'.format(wfp))\n\n\ndef dump_vertex(vertex, wfp):\n    sio.savemat(wfp, {\'vertex\': vertex})\n    print(\'Dump to {}\'.format(wfp))\n\n\ndef _predict_vertices(param, roi_bbox, dense, transform=True):\n    vertex = reconstruct_vertex(param, dense=dense)\n    sx, sy, ex, ey = roi_bbox\n    scale_x = (ex - sx) / 120\n    scale_y = (ey - sy) / 120\n    vertex[0, :] = vertex[0, :] * scale_x + sx\n    vertex[1, :] = vertex[1, :] * scale_y + sy\n\n    s = (scale_x + scale_y) / 2\n    vertex[2, :] *= s\n\n    return vertex\n\n\ndef predict_68pts(param, roi_box):\n    return _predict_vertices(param, roi_box, dense=False)\n\n\ndef predict_dense(param, roi_box):\n    return _predict_vertices(param, roi_box, dense=True)\n\n\ndef draw_landmarks(img, pts, style=\'fancy\', wfp=None, show_flg=False, **kwargs):\n    """"""Draw landmarks using matplotlib""""""\n    height, width = img.shape[:2]\n    plt.figure(figsize=(12, height / width * 12))\n    plt.imshow(img[:, :, ::-1])\n    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n    plt.axis(\'off\')\n\n    if not type(pts) in [tuple, list]:\n        pts = [pts]\n    for i in range(len(pts)):\n        if style == \'simple\':\n            plt.plot(pts[i][0, :], pts[i][1, :], \'o\', markersize=4, color=\'g\')\n\n        elif style == \'fancy\':\n            alpha = 0.8\n            markersize = 4\n            lw = 1.5\n            color = kwargs.get(\'color\', \'w\')\n            markeredgecolor = kwargs.get(\'markeredgecolor\', \'black\')\n\n            nums = [0, 17, 22, 27, 31, 36, 42, 48, 60, 68]\n\n            # close eyes and mouths\n            plot_close = lambda i1, i2: plt.plot([pts[i][0, i1], pts[i][0, i2]], [pts[i][1, i1], pts[i][1, i2]],\n                                                 color=color, lw=lw, alpha=alpha - 0.1)\n            plot_close(41, 36)\n            plot_close(47, 42)\n            plot_close(59, 48)\n            plot_close(67, 60)\n\n            for ind in range(len(nums) - 1):\n                l, r = nums[ind], nums[ind + 1]\n                plt.plot(pts[i][0, l:r], pts[i][1, l:r], color=color, lw=lw, alpha=alpha - 0.1)\n\n                plt.plot(pts[i][0, l:r], pts[i][1, l:r], marker=\'o\', linestyle=\'None\', markersize=markersize,\n                         color=color,\n                         markeredgecolor=markeredgecolor, alpha=alpha)\n\n    if wfp is not None:\n        plt.savefig(wfp, dpi=200)\n        print(\'Save visualization result to {}\'.format(wfp))\n    if show_flg:\n        plt.show()\n\n\ndef get_colors(image, vertices):\n    [h, w, _] = image.shape\n    vertices[0, :] = np.minimum(np.maximum(vertices[0, :], 0), w - 1)  # x\n    vertices[1, :] = np.minimum(np.maximum(vertices[1, :], 0), h - 1)  # y\n    ind = np.round(vertices).astype(np.int32)\n    colors = image[ind[1, :], ind[0, :], :]  # n x 3\n\n    return colors\n\n\ndef write_obj_with_colors(obj_name, vertices, triangles, colors):\n    triangles = triangles.copy() # meshlab start with 1\n\n    if obj_name.split(\'.\')[-1] != \'obj\':\n        obj_name = obj_name + \'.obj\'\n\n    # write obj\n    with open(obj_name, \'w\') as f:\n        # write vertices & colors\n        for i in range(vertices.shape[1]):\n            s = \'v {:.4f} {:.4f} {:.4f} {} {} {}\\n\'.format(vertices[1, i], vertices[0, i], vertices[2, i], colors[i, 2],\n                                               colors[i, 1], colors[i, 0])\n            f.write(s)\n\n        # write f: ver ind/ uv ind\n        for i in range(triangles.shape[1]):\n            s = \'f {} {} {}\\n\'.format(triangles[0, i], triangles[1, i], triangles[2, i])\n            f.write(s)\n\n\ndef main():\n    pass\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/io.py,5,"b'#!/usr/bin/env python3\n# coding: utf-8\n\nimport os\nimport numpy as np\nimport torch\nimport pickle\nimport scipy.io as sio\n\n\ndef mkdir(d):\n    """"""only works on *nix system""""""\n    if not os.path.isdir(d) and not os.path.exists(d):\n        os.system(\'mkdir -p {}\'.format(d))\n\n\ndef _get_suffix(filename):\n    """"""a.jpg -> jpg""""""\n    pos = filename.rfind(\'.\')\n    if pos == -1:\n        return \'\'\n    return filename[pos + 1:]\n\n\ndef _load(fp):\n    suffix = _get_suffix(fp)\n    if suffix == \'npy\':\n        return np.load(fp)\n    elif suffix == \'pkl\':\n        return pickle.load(open(fp, \'rb\'))\n\n\ndef _dump(wfp, obj):\n    suffix = _get_suffix(wfp)\n    if suffix == \'npy\':\n        np.save(wfp, obj)\n    elif suffix == \'pkl\':\n        pickle.dump(obj, open(wfp, \'wb\'))\n    else:\n        raise Exception(\'Unknown Type: {}\'.format(suffix))\n\n\ndef _load_tensor(fp, mode=\'cpu\'):\n    if mode.lower() == \'cpu\':\n        return torch.from_numpy(_load(fp))\n    elif mode.lower() == \'gpu\':\n        return torch.from_numpy(_load(fp)).cuda()\n\n\ndef _tensor_to_cuda(x):\n    if x.is_cuda:\n        return x\n    else:\n        return x.cuda()\n\n\ndef _load_gpu(fp):\n    return torch.from_numpy(_load(fp)).cuda()\n\n\ndef load_bfm(model_path):\n    suffix = _get_suffix(model_path)\n    if suffix == \'mat\':\n        C = sio.loadmat(model_path)\n        model = C[\'model_refine\']\n        model = model[0, 0]\n\n        model_new = {}\n        w_shp = model[\'w\'].astype(np.float32)\n        model_new[\'w_shp_sim\'] = w_shp[:, :40]\n        w_exp = model[\'w_exp\'].astype(np.float32)\n        model_new[\'w_exp_sim\'] = w_exp[:, :10]\n\n        u_shp = model[\'mu_shape\']\n        u_exp = model[\'mu_exp\']\n        u = (u_shp + u_exp).astype(np.float32)\n        model_new[\'mu\'] = u\n        model_new[\'tri\'] = model[\'tri\'].astype(np.int32) - 1\n\n        # flatten it, pay attention to index value\n        keypoints = model[\'keypoints\'].astype(np.int32) - 1\n        keypoints = np.concatenate((3 * keypoints, 3 * keypoints + 1, 3 * keypoints + 2), axis=0)\n\n        model_new[\'keypoints\'] = keypoints.T.flatten()\n\n        #\n        w = np.concatenate((w_shp, w_exp), axis=1)\n        w_base = w[keypoints]\n        w_norm = np.linalg.norm(w, axis=0)\n        w_base_norm = np.linalg.norm(w_base, axis=0)\n\n        dim = w_shp.shape[0] // 3\n        u_base = u[keypoints].reshape(-1, 1)\n        w_shp_base = w_shp[keypoints]\n        w_exp_base = w_exp[keypoints]\n\n        model_new[\'w_norm\'] = w_norm\n        model_new[\'w_base_norm\'] = w_base_norm\n        model_new[\'dim\'] = dim\n        model_new[\'u_base\'] = u_base\n        model_new[\'w_shp_base\'] = w_shp_base\n        model_new[\'w_exp_base\'] = w_exp_base\n\n        _dump(model_path.replace(\'.mat\', \'.pkl\'), model_new)\n        return model_new\n    else:\n        return _load(model_path)\n\n\n_load_cpu = _load\n_numpy_to_tensor = lambda x: torch.from_numpy(x)\n_tensor_to_numpy = lambda x: x.cpu()\n_numpy_to_cuda = lambda x: _tensor_to_cuda(torch.from_numpy(x))\n_cuda_to_tensor = lambda x: x.cpu()\n_cuda_to_numpy = lambda x: x.cpu().numpy()\n'"
utils/lighting.py,0,"b""#!/usr/bin/env python3\n# coding: utf-8\n\nimport sys\n\nsys.path.append('../')\nimport numpy as np\nfrom utils import render\nfrom utils.cython import mesh_core_cython\n\n_norm = lambda arr: arr / np.sqrt(np.sum(arr ** 2, axis=1))[:, None]\n\n\ndef norm_vertices(vertices):\n    vertices -= vertices.min(0)[None, :]\n    vertices /= vertices.max()\n    vertices *= 2\n    vertices -= vertices.max(0)[None, :] / 2\n    return vertices\n\n\ndef convert_type(obj):\n    if isinstance(obj, tuple) or isinstance(obj, list):\n        return np.array(obj, dtype=np.float32)[None, :]\n    return obj\n\n\nclass RenderPipeline(object):\n    def __init__(self, **kwargs):\n        self.intensity_ambient = convert_type(kwargs.get('intensity_ambient', 0.3))\n        self.intensity_directional = convert_type(kwargs.get('intensity_directional', 0.6))\n        self.intensity_specular = convert_type(kwargs.get('intensity_specular', 0.9))\n        self.specular_exp = kwargs.get('specular_exp', 5)\n        self.color_ambient = convert_type(kwargs.get('color_ambient', (1, 1, 1)))\n        self.color_directional = convert_type(kwargs.get('color_directional', (1, 1, 1)))\n        self.light_pos = convert_type(kwargs.get('light_pos', (0, 0, 1)))\n        self.view_pos = convert_type(kwargs.get('view_pos', (0, 0, 1)))\n\n    def update_light_pos(self, light_pos):\n        self.light_pos = convert_type(light_pos)\n\n    def __call__(self, vertices, triangles, background):\n        height, width = background.shape[:2]\n\n        # 1. compute triangle/face normals and vertex normals\n        # ## Old style: very slow\n        # normal = np.zeros((vertices.shape[0], 3), dtype=np.float32)\n        # # surface_count = np.zeros((vertices.shape[0], 1))\n        # for i in range(triangles.shape[0]):\n        #     i1, i2, i3 = triangles[i, :]\n        #     v1, v2, v3 = vertices[[i1, i2, i3], :]\n        #     surface_normal = np.cross(v2 - v1, v3 - v1)\n        #     normal[[i1, i2, i3], :] += surface_normal\n        #     # surface_count[[i1, i2, i3], :] += 1\n        #\n        # # normal /= surface_count\n        # # normal /= np.linalg.norm(normal, axis=1, keepdims=True)\n        # normal = _norm(normal)\n\n        # Cython style\n        normal = np.zeros((vertices.shape[0], 3), dtype=np.float32)\n        mesh_core_cython.get_normal(normal, vertices, triangles, vertices.shape[0], triangles.shape[0])\n\n        # 2. lighting\n        color = np.zeros_like(vertices, dtype=np.float32)\n        # ambient component\n        if self.intensity_ambient > 0:\n            color += self.intensity_ambient * self.color_ambient\n\n        vertices_n = norm_vertices(vertices.copy())\n        if self.intensity_directional > 0:\n            # diffuse component\n            direction = _norm(self.light_pos - vertices_n)\n            cos = np.sum(normal * direction, axis=1)[:, None]\n            # cos = np.clip(cos, 0, 1)\n            #  todo: check below\n            color += self.intensity_directional * (self.color_directional * np.clip(cos, 0, 1))\n\n            # specular component\n            if self.intensity_specular > 0:\n                v2v = _norm(self.view_pos - vertices_n)\n                reflection = 2 * cos * normal - direction\n                spe = np.sum((v2v * reflection) ** self.specular_exp, axis=1)[:, None]\n                spe = np.where(cos != 0, np.clip(spe, 0, 1), np.zeros_like(spe))\n                color += self.intensity_specular * self.color_directional * np.clip(spe, 0, 1)\n        color = np.clip(color, 0, 1)\n\n        # 2. rasterization, [0, 1]\n        render_img = render.crender_colors(vertices, triangles, color, height, width, BG=background)\n        render_img = (render_img * 255).astype(np.uint8)\n        return render_img\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"""
utils/paf.py,0,"b'#!/usr/bin/env python3\n# coding: utf-8\n\nimport numpy as np\nfrom .ddfa import _parse_param\nfrom .params import u_filter, w_filter, w_exp_filter, std_size, param_mean, param_std\n\n\ndef reconstruct_paf_anchor(param, whitening=True):\n    if whitening:\n        param = param * param_std + param_mean\n    p, offset, alpha_shp, alpha_exp = _parse_param(param)\n    anchor = p @ (u_filter + w_filter @ alpha_shp + w_exp_filter @ alpha_exp).reshape(3, -1, order=\'F\') + offset\n    anchor[1, :] = std_size + 1 - anchor[1, :]\n    return anchor[:2, :]\n\n\ndef gen_offsets(kernel_size):\n    offsets = np.zeros((2, kernel_size * kernel_size), dtype=np.int)\n    ind = 0\n    delta = (kernel_size - 1) // 2\n    for i in range(kernel_size):\n        y = i - delta\n        for j in range(kernel_size):\n            x = j - delta\n            offsets[0, ind] = x\n            offsets[1, ind] = y\n            ind += 1\n    return offsets\n\n\ndef gen_img_paf(img_crop, param, kernel_size=3):\n    """"""Generate PAF image\n    img_crop: 120x120\n    kernel_size: kernel_size for convolution, should be even number like 3 or 5 or ...\n    """"""\n    anchor = reconstruct_paf_anchor(param)\n    anchor = np.round(anchor).astype(np.int)\n    delta = (kernel_size - 1) // 2\n    anchor[anchor < delta] = delta\n    anchor[anchor >= std_size - delta - 1] = std_size - delta - 1\n\n    img_paf = np.zeros((64 * kernel_size, 64 * kernel_size, 3), dtype=np.uint8)\n    offsets = gen_offsets(kernel_size)\n    for i in range(kernel_size * kernel_size):\n        ox, oy = offsets[:, i]\n        index0 = anchor[0] + ox\n        index1 = anchor[1] + oy\n        p = img_crop[index1, index0].reshape(64, 64, 3).transpose(1, 0, 2)\n\n        img_paf[oy + delta::kernel_size, ox + delta::kernel_size] = p\n\n    return img_paf\n\n\ndef main():\n    pass\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/params.py,0,"b""#!/usr/bin/env python3\n# coding: utf-8\n\nimport os.path as osp\nimport numpy as np\nfrom .io import _load\n\n\ndef make_abs_path(d):\n    return osp.join(osp.dirname(osp.realpath(__file__)), d)\n\n\nd = make_abs_path('../train.configs')\nkeypoints = _load(osp.join(d, 'keypoints_sim.npy'))\nw_shp = _load(osp.join(d, 'w_shp_sim.npy'))\nw_exp = _load(osp.join(d, 'w_exp_sim.npy'))  # simplified version\nmeta = _load(osp.join(d, 'param_whitening.pkl'))\n# param_mean and param_std are used for re-whitening\nparam_mean = meta.get('param_mean')\nparam_std = meta.get('param_std')\nu_shp = _load(osp.join(d, 'u_shp.npy'))\nu_exp = _load(osp.join(d, 'u_exp.npy'))\nu = u_shp + u_exp\nw = np.concatenate((w_shp, w_exp), axis=1)\nw_base = w[keypoints]\nw_norm = np.linalg.norm(w, axis=0)\nw_base_norm = np.linalg.norm(w_base, axis=0)\n\n# for inference\ndim = w_shp.shape[0] // 3\nu_base = u[keypoints].reshape(-1, 1)\nw_shp_base = w_shp[keypoints]\nw_exp_base = w_exp[keypoints]\nstd_size = 120\n\n# for paf (pac)\npaf = _load(osp.join(d, 'Model_PAF.pkl'))\nu_filter = paf.get('mu_filter')\nw_filter = paf.get('w_filter')\nw_exp_filter = paf.get('w_exp_filter')\n\n# pncc code (mean shape)\npncc_code = _load(osp.join(d, 'pncc_code.npy'))\n"""
utils/render.py,0,"b'#!/usr/bin/env python3\n# coding: utf-8\n\n\n""""""\nModified from https://raw.githubusercontent.com/YadiraF/PRNet/master/utils/render.py\n""""""\n\n__author__ = \'cleardusk\'\n\nimport numpy as np\nfrom .cython import mesh_core_cython\nfrom .params import pncc_code\n\n\ndef is_point_in_tri(point, tri_points):\n    \'\'\' Judge whether the point is in the triangle\n    Method:\n        http://blackpawn.com/texts/pointinpoly/\n    Args:\n        point: [u, v] or [x, y]\n        tri_points: three vertices(2d points) of a triangle. 2 coords x 3 vertices\n    Returns:\n        bool: true for in triangle\n    \'\'\'\n    tp = tri_points\n\n    # vectors\n    v0 = tp[:, 2] - tp[:, 0]\n    v1 = tp[:, 1] - tp[:, 0]\n    v2 = point - tp[:, 0]\n\n    # dot products\n    dot00 = np.dot(v0.T, v0)\n    dot01 = np.dot(v0.T, v1)\n    dot02 = np.dot(v0.T, v2)\n    dot11 = np.dot(v1.T, v1)\n    dot12 = np.dot(v1.T, v2)\n\n    # barycentric coordinates\n    if dot00 * dot11 - dot01 * dot01 == 0:\n        inverDeno = 0\n    else:\n        inverDeno = 1 / (dot00 * dot11 - dot01 * dot01)\n\n    u = (dot11 * dot02 - dot01 * dot12) * inverDeno\n    v = (dot00 * dot12 - dot01 * dot02) * inverDeno\n\n    # check if point in triangle\n    return (u >= 0) & (v >= 0) & (u + v < 1)\n\n\ndef render_colors(vertices, colors, tri, h, w, c=3):\n    """""" render mesh by z buffer\n    Args:\n        vertices: 3 x nver\n        colors: 3 x nver\n        tri: 3 x ntri\n        h: height\n        w: width\n    """"""\n    # initial\n    image = np.zeros((h, w, c))\n\n    depth_buffer = np.zeros([h, w]) - 999999.\n    # triangle depth: approximate the depth to the average value of z in each vertex(v0, v1, v2), since the vertices are closed to each other\n    tri_depth = (vertices[2, tri[0, :]] + vertices[2, tri[1, :]] + vertices[2, tri[2, :]]) / 3.\n    tri_tex = (colors[:, tri[0, :]] + colors[:, tri[1, :]] + colors[:, tri[2, :]]) / 3.\n\n    for i in range(tri.shape[1]):\n        tri_idx = tri[:, i]  # 3 vertex indices\n\n        # the inner bounding box\n        umin = max(int(np.ceil(np.min(vertices[0, tri_idx]))), 0)\n        umax = min(int(np.floor(np.max(vertices[0, tri_idx]))), w - 1)\n\n        vmin = max(int(np.ceil(np.min(vertices[1, tri_idx]))), 0)\n        vmax = min(int(np.floor(np.max(vertices[1, tri_idx]))), h - 1)\n\n        if umax < umin or vmax < vmin:\n            continue\n\n        for u in range(umin, umax + 1):\n            for v in range(vmin, vmax + 1):\n                if tri_depth[i] > depth_buffer[v, u] and is_point_in_tri([u, v], vertices[:2, tri_idx]):\n                    depth_buffer[v, u] = tri_depth[i]\n                    image[v, u, :] = tri_tex[:, i]\n    return image\n\n\ndef get_depths_image(img, vertices_lst, tri):\n    h, w = img.shape[:2]\n    c = 1\n\n    depths_img = np.zeros((h, w, c))\n    for i in range(len(vertices_lst)):\n        vertices = vertices_lst[i]\n\n        z = vertices[2, :]\n        z_min, z_max = min(z), max(z)\n        vertices[2, :] = (z - z_min) / (z_max - z_min)\n\n        z = vertices[2:, :]\n        depth_img = render_colors(vertices.T, z.T, tri.T, h, w, 1)\n        depths_img[depth_img > 0] = depth_img[depth_img > 0]\n\n    depths_img = depths_img.squeeze() * 255\n    return depths_img\n\n\ndef crender_colors(vertices, triangles, colors, h, w, c=3, BG=None):\n    """""" render mesh with colors\n    Args:\n        vertices: [nver, 3]\n        triangles: [ntri, 3]\n        colors: [nver, 3]\n        h: height\n        w: width\n        c: channel\n        BG: background image\n    Returns:\n        image: [h, w, c]. rendered image./rendering.\n    """"""\n\n    if BG is None:\n        image = np.zeros((h, w, c), dtype=np.float32)\n    else:\n        assert BG.shape[0] == h and BG.shape[1] == w and BG.shape[2] == c\n        image = BG.astype(np.float32).copy(order=\'C\')\n    depth_buffer = np.zeros([h, w], dtype=np.float32, order=\'C\') - 999999.\n\n    # to C order\n    vertices = vertices.astype(np.float32).copy(order=\'C\')\n    triangles = triangles.astype(np.int32).copy(order=\'C\')\n    colors = colors.astype(np.float32).copy(order=\'C\')\n\n    mesh_core_cython.render_colors_core(\n        image, vertices, triangles,\n        colors,\n        depth_buffer,\n        vertices.shape[0], triangles.shape[0],\n        h, w, c\n    )\n    return image\n\n\ndef cget_depths_image(img, vertices_lst, tri):\n    """"""cython version for depth image render""""""\n    h, w = img.shape[:2]\n    c = 1\n\n    depths_img = np.zeros((h, w, c))\n    for i in range(len(vertices_lst)):\n        vertices = vertices_lst[i]\n\n        z = vertices[2, :]\n        z_min, z_max = min(z), max(z)\n        vertices[2, :] = (z - z_min) / (z_max - z_min)\n        z = vertices[2:, :]\n\n        depth_img = crender_colors(vertices.T, tri.T, z.T, h, w, 1)\n        depths_img[depth_img > 0] = depth_img[depth_img > 0]\n\n    depths_img = depths_img.squeeze() * 255\n    return depths_img\n\n\ndef ncc(vertices):\n    ## simple version\n    # ncc_vertices = np.zeros_like(vertices)\n    # x = vertices[0, :]\n    # y = vertices[1, :]\n    # z = vertices[2, :]\n    #\n    # ncc_vertices[0, :] = (x - min(x)) / (max(x) - min(x))\n    # ncc_vertices[1, :] = (y - min(y)) / (max(y) - min(y))\n    # ncc_vertices[2, :] = (z - min(z)) / (max(z) - min(z))\n\n    # matrix version\n    v_min = np.min(vertices, axis=1).reshape(-1, 1)\n    v_max = np.max(vertices, axis=1).reshape(-1, 1)\n    ncc_vertices = (vertices - v_min) / (v_max - v_min)\n\n    return ncc_vertices\n\n\ndef cpncc(img, vertices_lst, tri):\n    """"""cython version for PNCC render: original paper""""""\n    h, w = img.shape[:2]\n    c = 3\n\n    pnccs_img = np.zeros((h, w, c))\n    for i in range(len(vertices_lst)):\n        vertices = vertices_lst[i]\n        pncc_img = crender_colors(vertices.T, tri.T, pncc_code.T, h, w, c)\n        pnccs_img[pncc_img > 0] = pncc_img[pncc_img > 0]\n\n    pnccs_img = pnccs_img.squeeze() * 255\n    return pnccs_img\n\n\ndef cpncc_v2(img, vertices_lst, tri):\n    """"""cython version for PNCC render""""""\n    h, w = img.shape[:2]\n    c = 3\n\n    pnccs_img = np.zeros((h, w, c))\n    for i in range(len(vertices_lst)):\n        vertices = vertices_lst[i]\n        ncc_vertices = ncc(vertices)\n        pncc_img = crender_colors(vertices.T, tri.T, ncc_vertices.T, h, w, c)\n        pnccs_img[pncc_img > 0] = pncc_img[pncc_img > 0]\n\n    pnccs_img = pnccs_img.squeeze() * 255\n    return pnccs_img\n\n\ndef main():\n    pass\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/cython/__init__.py,0,b''
utils/cython/setup.py,0,"b'\'\'\'\npython setup.py build_ext -i\nto compile\n\'\'\'\n\n# setup.py\nfrom distutils.core import setup, Extension\n# from Cython.Build import cythonize\nfrom Cython.Distutils import build_ext\nimport numpy\n\nsetup(\n    name=\'mesh_core_cython\',\n    cmdclass={\'build_ext\': build_ext},\n    ext_modules=[Extension(""mesh_core_cython"",\n                           sources=[""mesh_core_cython.pyx"", ""mesh_core.cpp""],\n                           language=\'c++\',\n                           include_dirs=[numpy.get_include()])],\n)\n'"
