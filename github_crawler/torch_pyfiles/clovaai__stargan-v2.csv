file_path,api_count,code
main.py,2,"b'""""""\nStarGAN v2\nCopyright (c) 2020-present NAVER Corp.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial\n4.0 International License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n""""""\n\nimport os\nimport argparse\n\nfrom munch import Munch\nfrom torch.backends import cudnn\nimport torch\n\nfrom core.data_loader import get_train_loader\nfrom core.data_loader import get_test_loader\nfrom core.solver import Solver\n\n\ndef str2bool(v):\n    return v.lower() in (\'true\')\n\n\ndef subdirs(dname):\n    return [d for d in os.listdir(dname)\n            if os.path.isdir(os.path.join(dname, d))]\n\n\ndef main(args):\n    print(args)\n    cudnn.benchmark = True\n    torch.manual_seed(args.seed)\n\n    solver = Solver(args)\n\n    if args.mode == \'train\':\n        assert len(subdirs(args.train_img_dir)) == args.num_domains\n        assert len(subdirs(args.val_img_dir)) == args.num_domains\n        loaders = Munch(src=get_train_loader(root=args.train_img_dir,\n                                             which=\'source\',\n                                             img_size=args.img_size,\n                                             batch_size=args.batch_size,\n                                             prob=args.randcrop_prob,\n                                             num_workers=args.num_workers),\n                        ref=get_train_loader(root=args.train_img_dir,\n                                             which=\'reference\',\n                                             img_size=args.img_size,\n                                             batch_size=args.batch_size,\n                                             prob=args.randcrop_prob,\n                                             num_workers=args.num_workers),\n                        val=get_test_loader(root=args.val_img_dir,\n                                            img_size=args.img_size,\n                                            batch_size=args.val_batch_size,\n                                            shuffle=True,\n                                            num_workers=args.num_workers))\n        solver.train(loaders)\n    elif args.mode == \'sample\':\n        assert len(subdirs(args.src_dir)) == args.num_domains\n        assert len(subdirs(args.ref_dir)) == args.num_domains\n        loaders = Munch(src=get_test_loader(root=args.src_dir,\n                                            img_size=args.img_size,\n                                            batch_size=args.val_batch_size,\n                                            shuffle=False,\n                                            num_workers=args.num_workers),\n                        ref=get_test_loader(root=args.ref_dir,\n                                            img_size=args.img_size,\n                                            batch_size=args.val_batch_size,\n                                            shuffle=False,\n                                            num_workers=args.num_workers))\n        solver.sample(loaders)\n    elif args.mode == \'eval\':\n        solver.evaluate()\n    elif args.mode == \'align\':\n        from core.wing import align_faces\n        align_faces(args, args.inp_dir, args.out_dir)\n    else:\n        raise NotImplementedError\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n\n    # model arguments\n    parser.add_argument(\'--img_size\', type=int, default=256,\n                        help=\'Image resolution\')\n    parser.add_argument(\'--num_domains\', type=int, default=2,\n                        help=\'Number of domains\')\n    parser.add_argument(\'--latent_dim\', type=int, default=16,\n                        help=\'Latent vector dimension\')\n    parser.add_argument(\'--hidden_dim\', type=int, default=512,\n                        help=\'Hidden dimension of mapping network\')\n    parser.add_argument(\'--style_dim\', type=int, default=64,\n                        help=\'Style code dimension\')\n\n    # weight for objective functions\n    parser.add_argument(\'--lambda_reg\', type=float, default=1,\n                        help=\'Weight for R1 regularization\')\n    parser.add_argument(\'--lambda_cyc\', type=float, default=1,\n                        help=\'Weight for cyclic consistency loss\')\n    parser.add_argument(\'--lambda_sty\', type=float, default=1,\n                        help=\'Weight for style reconstruction loss\')\n    parser.add_argument(\'--lambda_ds\', type=float, default=1,\n                        help=\'Weight for diversity sensitive loss\')\n    parser.add_argument(\'--ds_iter\', type=int, default=100000,\n                        help=\'Number of iterations to optimize diversity sensitive loss\')\n    parser.add_argument(\'--w_hpf\', type=float, default=1,\n                        help=\'weight for high-pass filtering\')\n\n    # training arguments\n    parser.add_argument(\'--randcrop_prob\', type=float, default=0.5,\n                        help=\'Probabilty of using random-resized cropping\')\n    parser.add_argument(\'--total_iters\', type=int, default=100000,\n                        help=\'Number of total iterations\')\n    parser.add_argument(\'--resume_iter\', type=int, default=0,\n                        help=\'Iterations to resume training/testing\')\n    parser.add_argument(\'--batch_size\', type=int, default=8,\n                        help=\'Batch size for training\')\n    parser.add_argument(\'--val_batch_size\', type=int, default=32,\n                        help=\'Batch size for validation\')\n    parser.add_argument(\'--lr\', type=float, default=1e-4,\n                        help=\'Learning rate for D, E and G\')\n    parser.add_argument(\'--f_lr\', type=float, default=1e-6,\n                        help=\'Learning rate for F\')\n    parser.add_argument(\'--beta1\', type=float, default=0.0,\n                        help=\'Decay rate for 1st moment of Adam\')\n    parser.add_argument(\'--beta2\', type=float, default=0.99,\n                        help=\'Decay rate for 2nd moment of Adam\')\n    parser.add_argument(\'--weight_decay\', type=float, default=1e-4,\n                        help=\'Weight decay for optimizer\')\n    parser.add_argument(\'--num_outs_per_domain\', type=int, default=10,\n                        help=\'Number of generated images per domain during sampling\')\n\n    # misc\n    parser.add_argument(\'--mode\', type=str, required=True,\n                        choices=[\'train\', \'sample\', \'eval\', \'align\'],\n                        help=\'This argument is used in solver\')\n    parser.add_argument(\'--num_workers\', type=int, default=4,\n                        help=\'Number of workers used in DataLoader\')\n    parser.add_argument(\'--seed\', type=int, default=777,\n                        help=\'Seed for random number generator\')\n\n    # directory for training\n    parser.add_argument(\'--train_img_dir\', type=str, default=\'data/celeba_hq/train\',\n                        help=\'Directory containing training images\')\n    parser.add_argument(\'--val_img_dir\', type=str, default=\'data/celeba_hq/val\',\n                        help=\'Directory containing validation images\')\n    parser.add_argument(\'--sample_dir\', type=str, default=\'expr/samples\',\n                        help=\'Directory for saving generated images\')\n    parser.add_argument(\'--checkpoint_dir\', type=str, default=\'expr/checkpoints\',\n                        help=\'Directory for saving network checkpoints\')\n\n    # directory for calculating metrics\n    parser.add_argument(\'--eval_dir\', type=str, default=\'expr/eval\',\n                        help=\'Directory for saving metrics, i.e., FID and LPIPS\')\n\n    # directory for testing\n    parser.add_argument(\'--result_dir\', type=str, default=\'expr/results\',\n                        help=\'Directory for saving generated images and videos\')\n    parser.add_argument(\'--src_dir\', type=str, default=\'assets/representative/celeba_hq/src\',\n                        help=\'Directory containing input source images\')\n    parser.add_argument(\'--ref_dir\', type=str, default=\'assets/representative/celeba_hq/ref\',\n                        help=\'Directory containing input reference images\')\n    parser.add_argument(\'--inp_dir\', type=str, default=\'assets/representative/custom/female\',\n                        help=\'input directory when aligning faces\')\n    parser.add_argument(\'--out_dir\', type=str, default=\'assets/representative/celeba_hq/src/female\',\n                        help=\'output directory when aligning faces\')\n\n    # face alignment\n    parser.add_argument(\'--wing_path\', type=str, default=\'expr/checkpoints/wing.ckpt\')\n    parser.add_argument(\'--lm_path\', type=str, default=\'expr/checkpoints/celeba_lm_mean.npz\')\n\n    # step size\n    parser.add_argument(\'--print_every\', type=int, default=10)\n    parser.add_argument(\'--sample_every\', type=int, default=5000)\n    parser.add_argument(\'--save_every\', type=int, default=10000)\n    parser.add_argument(\'--eval_every\', type=int, default=50000)\n\n    args = parser.parse_args()\n    main(args)\n'"
core/__init__.py,0,b''
core/checkpoint.py,4,"b'""""""\nStarGAN v2\nCopyright (c) 2020-present NAVER Corp.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial\n4.0 International License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n""""""\n\nimport os\nimport torch\n\n\nclass CheckpointIO(object):\n    def __init__(self, fname_template, **kwargs):\n        os.makedirs(os.path.dirname(fname_template), exist_ok=True)\n        self.fname_template = fname_template\n        self.module_dict = kwargs\n\n    def register(self, **kwargs):\n        self.module_dict.update(kwargs)\n\n    def save(self, step):\n        fname = self.fname_template.format(step)\n        print(\'Saving checkpoint into %s...\' % fname)\n        outdict = {}\n        for name, module in self.module_dict.items():\n            outdict[name] = module.state_dict()\n        torch.save(outdict, fname)\n\n    def load(self, step):\n        fname = self.fname_template.format(step)\n        assert os.path.exists(fname), fname + \' does not exist!\'\n        print(\'Loading checkpoint from %s...\' % fname)\n        if torch.cuda.is_available():\n            module_dict = torch.load(fname)\n        else:\n            module_dict = torch.load(fname, map_location=torch.device(\'cpu\'))\n        for name, module in self.module_dict.items():\n            module.load_state_dict(module_dict[name])\n'"
core/data_loader.py,5,"b'""""""\nStarGAN v2\nCopyright (c) 2020-present NAVER Corp.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial\n4.0 International License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n""""""\n\nfrom pathlib import Path\nfrom itertools import chain\nimport os\nimport random\n\nfrom munch import Munch\nfrom PIL import Image\nimport numpy as np\n\nimport torch\nfrom torch.utils import data\nfrom torch.utils.data.sampler import WeightedRandomSampler\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\n\n\ndef listdir(dname):\n    fnames = list(chain(*[list(Path(dname).rglob(\'*.\' + ext))\n                          for ext in [\'png\', \'jpg\', \'jpeg\', \'JPG\']]))\n    return fnames\n\n\nclass DefaultDataset(data.Dataset):\n    def __init__(self, root, transform=None):\n        self.samples = listdir(root)\n        self.samples.sort()\n        self.transform = transform\n        self.targets = None\n\n    def __getitem__(self, index):\n        fname = self.samples[index]\n        img = Image.open(fname).convert(\'RGB\')\n        if self.transform is not None:\n            img = self.transform(img)\n        return img\n\n    def __len__(self):\n        return len(self.samples)\n\n\nclass ReferenceDataset(data.Dataset):\n    def __init__(self, root, transform=None):\n        self.samples, self.targets = self._make_dataset(root)\n        self.transform = transform\n\n    def _make_dataset(self, root):\n        domains = os.listdir(root)\n        fnames, fnames2, labels = [], [], []\n        for idx, domain in enumerate(sorted(domains)):\n            class_dir = os.path.join(root, domain)\n            cls_fnames = listdir(class_dir)\n            fnames += cls_fnames\n            fnames2 += random.sample(cls_fnames, len(cls_fnames))\n            labels += [idx] * len(cls_fnames)\n        return list(zip(fnames, fnames2)), labels\n\n    def __getitem__(self, index):\n        fname, fname2 = self.samples[index]\n        label = self.targets[index]\n        img = Image.open(fname).convert(\'RGB\')\n        img2 = Image.open(fname2).convert(\'RGB\')\n        if self.transform is not None:\n            img = self.transform(img)\n            img2 = self.transform(img2)\n        return img, img2, label\n\n    def __len__(self):\n        return len(self.targets)\n\n\ndef _make_balanced_sampler(labels):\n    class_counts = np.bincount(labels)\n    class_weights = 1. / class_counts\n    weights = class_weights[labels]\n    return WeightedRandomSampler(weights, len(weights))\n\n\ndef get_train_loader(root, which=\'source\', img_size=256,\n                     batch_size=8, prob=0.5, num_workers=4):\n    print(\'Preparing DataLoader to fetch %s images \'\n          \'during the training phase...\' % which)\n\n    crop = transforms.RandomResizedCrop(\n        img_size, scale=[0.8, 1.0], ratio=[0.9, 1.1])\n    rand_crop = transforms.Lambda(\n        lambda x: crop(x) if random.random() < prob else x)\n\n    transform = transforms.Compose([\n        rand_crop,\n        transforms.Resize([img_size, img_size]),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                             std=[0.5, 0.5, 0.5]),\n    ])\n\n    if which == \'source\':\n        dataset = ImageFolder(root, transform)\n    elif which == \'reference\':\n        dataset = ReferenceDataset(root, transform)\n    else:\n        raise NotImplementedError\n\n    sampler = _make_balanced_sampler(dataset.targets)\n    return data.DataLoader(dataset=dataset,\n                           batch_size=batch_size,\n                           sampler=sampler,\n                           num_workers=num_workers,\n                           pin_memory=True,\n                           drop_last=True)\n\n\ndef get_eval_loader(root, img_size=256, batch_size=32,\n                    imagenet_normalize=True, shuffle=True,\n                    num_workers=4, drop_last=False):\n    print(\'Preparing DataLoader for the evaluation phase...\')\n    if imagenet_normalize:\n        height, width = 299, 299\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n    else:\n        height, width = img_size, img_size\n        mean = [0.5, 0.5, 0.5]\n        std = [0.5, 0.5, 0.5]\n\n    transform = transforms.Compose([\n        transforms.Resize([img_size, img_size]),\n        transforms.Resize([height, width]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=std)\n    ])\n\n    dataset = DefaultDataset(root, transform=transform)\n    return data.DataLoader(dataset=dataset,\n                           batch_size=batch_size,\n                           shuffle=shuffle,\n                           num_workers=num_workers,\n                           pin_memory=True,\n                           drop_last=drop_last)\n\n\ndef get_test_loader(root, img_size=256, batch_size=32,\n                    shuffle=True, num_workers=4):\n    print(\'Preparing DataLoader for the generation phase...\')\n    transform = transforms.Compose([\n        transforms.Resize([img_size, img_size]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                             std=[0.5, 0.5, 0.5]),\n    ])\n\n    dataset = ImageFolder(root, transform)\n    return data.DataLoader(dataset=dataset,\n                           batch_size=batch_size,\n                           shuffle=shuffle,\n                           num_workers=num_workers,\n                           pin_memory=True)\n\n\nclass InputFetcher:\n    def __init__(self, loader, loader_ref=None, latent_dim=16, mode=\'\'):\n        self.loader = loader\n        self.loader_ref = loader_ref\n        self.latent_dim = latent_dim\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n        self.mode = mode\n\n    def _fetch_inputs(self):\n        try:\n            x, y = next(self.iter)\n        except (AttributeError, StopIteration):\n            self.iter = iter(self.loader)\n            x, y = next(self.iter)\n        return x, y\n\n    def _fetch_refs(self):\n        try:\n            x, x2, y = next(self.iter_ref)\n        except (AttributeError, StopIteration):\n            self.iter_ref = iter(self.loader_ref)\n            x, x2, y = next(self.iter_ref)\n        return x, x2, y\n\n    def __next__(self):\n        x, y = self._fetch_inputs()\n        if self.mode == \'train\':\n            x_ref, x_ref2, y_ref = self._fetch_refs()\n            z_trg = torch.randn(x.size(0), self.latent_dim)\n            z_trg2 = torch.randn(x.size(0), self.latent_dim)\n            inputs = Munch(x_src=x, y_src=y, y_ref=y_ref,\n                           x_ref=x_ref, x_ref2=x_ref2,\n                           z_trg=z_trg, z_trg2=z_trg2)\n        elif self.mode == \'val\':\n            x_ref, y_ref = self._fetch_inputs()\n            inputs = Munch(x_src=x, y_src=y,\n                           x_ref=x_ref, y_ref=y_ref)\n        elif self.mode == \'test\':\n            inputs = Munch(x=x, y=y)\n        else:\n            raise NotImplementedError\n\n        return Munch({k: v.to(self.device)\n                      for k, v in inputs.items()})'"
core/model.py,11,"b'""""""\nStarGAN v2\nCopyright (c) 2020-present NAVER Corp.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial\n4.0 International License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n""""""\n\nimport copy\nimport math\n\nfrom munch import Munch\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.wing import FAN\n\n\nclass ResBlk(nn.Module):\n    def __init__(self, dim_in, dim_out, actv=nn.LeakyReLU(0.2),\n                 normalize=False, downsample=False):\n        super().__init__()\n        self.actv = actv\n        self.normalize = normalize\n        self.downsample = downsample\n        self.learned_sc = dim_in != dim_out\n        self._build_weights(dim_in, dim_out)\n\n    def _build_weights(self, dim_in, dim_out):\n        self.conv1 = nn.Conv2d(dim_in, dim_in, 3, 1, 1)\n        self.conv2 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n        if self.normalize:\n            self.norm1 = nn.InstanceNorm2d(dim_in, affine=True)\n            self.norm2 = nn.InstanceNorm2d(dim_in, affine=True)\n        if self.learned_sc:\n            self.conv1x1 = nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False)\n\n    def _shortcut(self, x):\n        if self.learned_sc:\n            x = self.conv1x1(x)\n        if self.downsample:\n            x = F.avg_pool2d(x, 2)\n        return x\n\n    def _residual(self, x):\n        if self.normalize:\n            x = self.norm1(x)\n        x = self.actv(x)\n        x = self.conv1(x)\n        if self.downsample:\n            x = F.avg_pool2d(x, 2)\n        if self.normalize:\n            x = self.norm2(x)\n        x = self.actv(x)\n        x = self.conv2(x)\n        return x\n\n    def forward(self, x):\n        x = self._shortcut(x) + self._residual(x)\n        return x / math.sqrt(2)  # unit variance\n\n\nclass AdaIN(nn.Module):\n    def __init__(self, style_dim, num_features):\n        super().__init__()\n        self.norm = nn.InstanceNorm2d(num_features, affine=False)\n        self.fc = nn.Linear(style_dim, num_features*2)\n\n    def forward(self, x, s):\n        h = self.fc(s)\n        h = h.view(h.size(0), h.size(1), 1, 1)\n        gamma, beta = torch.chunk(h, chunks=2, dim=1)\n        return (1 + gamma) * self.norm(x) + beta\n\n\nclass AdainResBlk(nn.Module):\n    def __init__(self, dim_in, dim_out, style_dim=64, w_hpf=0,\n                 actv=nn.LeakyReLU(0.2), upsample=False):\n        super().__init__()\n        self.w_hpf = w_hpf\n        self.actv = actv\n        self.upsample = upsample\n        self.learned_sc = dim_in != dim_out\n        self._build_weights(dim_in, dim_out, style_dim)\n\n    def _build_weights(self, dim_in, dim_out, style_dim=64):\n        self.conv1 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n        self.conv2 = nn.Conv2d(dim_out, dim_out, 3, 1, 1)\n        self.norm1 = AdaIN(style_dim, dim_in)\n        self.norm2 = AdaIN(style_dim, dim_out)\n        if self.learned_sc:\n            self.conv1x1 = nn.Conv2d(dim_in, dim_out, 1, 1, 0, bias=False)\n\n    def _shortcut(self, x):\n        if self.upsample:\n            x = F.interpolate(x, scale_factor=2, mode=\'nearest\')\n        if self.learned_sc:\n            x = self.conv1x1(x)\n        return x\n\n    def _residual(self, x, s):\n        x = self.norm1(x, s)\n        x = self.actv(x)\n        if self.upsample:\n            x = F.interpolate(x, scale_factor=2, mode=\'nearest\')\n        x = self.conv1(x)\n        x = self.norm2(x, s)\n        x = self.actv(x)\n        x = self.conv2(x)\n        return x\n\n    def forward(self, x, s):\n        out = self._residual(x, s)\n        if self.w_hpf == 0:\n            out = (out + self._shortcut(x)) / math.sqrt(2)\n        return out\n\n\nclass HighPass(nn.Module):\n    def __init__(self, w_hpf, device):\n        super(HighPass, self).__init__()\n        self.filter = torch.tensor([[-1, -1, -1],\n                                    [-1, 8., -1],\n                                    [-1, -1, -1]]).to(device) / w_hpf\n\n    def forward(self, x):\n        filter = self.filter.unsqueeze(0).unsqueeze(1).repeat(x.size(1), 1, 1, 1)\n        return F.conv2d(x, filter, padding=1, groups=x.size(1))\n\n\nclass Generator(nn.Module):\n    def __init__(self, img_size=256, style_dim=64, max_conv_dim=512, w_hpf=1):\n        super().__init__()\n        dim_in = 2**14 // img_size\n        self.img_size = img_size\n        self.from_rgb = nn.Conv2d(3, dim_in, 3, 1, 1)\n        self.encode = nn.ModuleList()\n        self.decode = nn.ModuleList()\n        self.to_rgb = nn.Sequential(\n            nn.InstanceNorm2d(dim_in, affine=True),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(dim_in, 3, 1, 1, 0))\n\n        # down/up-sampling blocks\n        repeat_num = int(np.log2(img_size)) - 4\n        if w_hpf > 0:\n            repeat_num += 1\n        for _ in range(repeat_num):\n            dim_out = min(dim_in*2, max_conv_dim)\n            self.encode.append(\n                ResBlk(dim_in, dim_out, normalize=True, downsample=True))\n            self.decode.insert(\n                0, AdainResBlk(dim_out, dim_in, style_dim,\n                               w_hpf=w_hpf, upsample=True))  # stack-like\n            dim_in = dim_out\n\n        # bottleneck blocks\n        for _ in range(2):\n            self.encode.append(\n                ResBlk(dim_out, dim_out, normalize=True))\n            self.decode.insert(\n                0, AdainResBlk(dim_out, dim_out, style_dim, w_hpf=w_hpf))\n\n        if w_hpf > 0:\n            device = torch.device(\n                \'cuda\' if torch.cuda.is_available() else \'cpu\')\n            self.hpf = HighPass(w_hpf, device)\n\n    def forward(self, x, s, masks=None):\n        x = self.from_rgb(x)\n        cache = {}\n        for block in self.encode:\n            if (masks is not None) and (x.size(2) in [32, 64, 128]):\n                cache[x.size(2)] = x\n            x = block(x)\n        for block in self.decode:\n            x = block(x, s)\n            if (masks is not None) and (x.size(2) in [32, 64, 128]):\n                mask = masks[0] if x.size(2) in [32] else masks[1]\n                mask = F.interpolate(mask, size=x.size(2), mode=\'bilinear\')\n                x = x + self.hpf(mask * cache[x.size(2)])\n        return self.to_rgb(x)\n\n\nclass MappingNetwork(nn.Module):\n    def __init__(self, latent_dim=16, style_dim=64, num_domains=2):\n        super().__init__()\n        layers = []\n        layers += [nn.Linear(latent_dim, 512)]\n        layers += [nn.ReLU()]\n        for _ in range(3):\n            layers += [nn.Linear(512, 512)]\n            layers += [nn.ReLU()]\n        self.shared = nn.Sequential(*layers)\n\n        self.unshared = nn.ModuleList()\n        for _ in range(num_domains):\n            self.unshared += [nn.Sequential(nn.Linear(512, 512),\n                                            nn.ReLU(),\n                                            nn.Linear(512, 512),\n                                            nn.ReLU(),\n                                            nn.Linear(512, 512),\n                                            nn.ReLU(),\n                                            nn.Linear(512, style_dim))]\n\n    def forward(self, z, y):\n        h = self.shared(z)\n        out = []\n        for layer in self.unshared:\n            out += [layer(h)]\n        out = torch.stack(out, dim=1)  # (batch, num_domains, style_dim)\n        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n        s = out[idx, y]  # (batch, style_dim)\n        return s\n\n\nclass StyleEncoder(nn.Module):\n    def __init__(self, img_size=256, style_dim=64, num_domains=2, max_conv_dim=512):\n        super().__init__()\n        dim_in = 2**14 // img_size\n        blocks = []\n        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n\n        repeat_num = int(np.log2(img_size)) - 2\n        for _ in range(repeat_num):\n            dim_out = min(dim_in*2, max_conv_dim)\n            blocks += [ResBlk(dim_in, dim_out, downsample=True)]\n            dim_in = dim_out\n\n        blocks += [nn.LeakyReLU(0.2)]\n        blocks += [nn.Conv2d(dim_out, dim_out, 4, 1, 0)]\n        blocks += [nn.LeakyReLU(0.2)]\n        self.shared = nn.Sequential(*blocks)\n\n        self.unshared = nn.ModuleList()\n        for _ in range(num_domains):\n            self.unshared += [nn.Linear(dim_out, style_dim)]\n\n    def forward(self, x, y):\n        h = self.shared(x)\n        h = h.view(h.size(0), -1)\n        out = []\n        for layer in self.unshared:\n            out += [layer(h)]\n        out = torch.stack(out, dim=1)  # (batch, num_domains, style_dim)\n        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n        s = out[idx, y]  # (batch, style_dim)\n        return s\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_size=256, num_domains=2, max_conv_dim=512):\n        super().__init__()\n        dim_in = 2**14 // img_size\n        blocks = []\n        blocks += [nn.Conv2d(3, dim_in, 3, 1, 1)]\n\n        repeat_num = int(np.log2(img_size)) - 2\n        for _ in range(repeat_num):\n            dim_out = min(dim_in*2, max_conv_dim)\n            blocks += [ResBlk(dim_in, dim_out, downsample=True)]\n            dim_in = dim_out\n\n        blocks += [nn.LeakyReLU(0.2)]\n        blocks += [nn.Conv2d(dim_out, dim_out, 4, 1, 0)]\n        blocks += [nn.LeakyReLU(0.2)]\n        blocks += [nn.Conv2d(dim_out, num_domains, 1, 1, 0)]\n        self.main = nn.Sequential(*blocks)\n\n    def forward(self, x, y):\n        out = self.main(x)\n        out = out.view(out.size(0), -1)  # (batch, num_domains)\n        idx = torch.LongTensor(range(y.size(0))).to(y.device)\n        out = out[idx, y]  # (batch)\n        return out\n\n\ndef build_model(args):\n    generator = Generator(args.img_size, args.style_dim, w_hpf=args.w_hpf)\n    mapping_network = MappingNetwork(args.latent_dim, args.style_dim, args.num_domains)\n    style_encoder = StyleEncoder(args.img_size, args.style_dim, args.num_domains)\n    discriminator = Discriminator(args.img_size, args.num_domains)\n    generator_ema = copy.deepcopy(generator)\n    mapping_network_ema = copy.deepcopy(mapping_network)\n    style_encoder_ema = copy.deepcopy(style_encoder)\n\n    nets = Munch(generator=generator,\n                 mapping_network=mapping_network,\n                 style_encoder=style_encoder,\n                 discriminator=discriminator)\n    nets_ema = Munch(generator=generator_ema,\n                     mapping_network=mapping_network_ema,\n                     style_encoder=style_encoder_ema)\n\n    if args.w_hpf > 0:\n        fan = FAN(fname_pretrained=args.wing_path).eval()\n        nets.fan = fan\n        nets_ema.fan = fan\n\n    return nets, nets_ema'"
core/solver.py,13,"b'""""""\nStarGAN v2\nCopyright (c) 2020-present NAVER Corp.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial\n4.0 International License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n""""""\n\nimport os\nfrom os.path import join as ospj\nimport time\nimport datetime\nfrom munch import Munch\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.model import build_model\nfrom core.checkpoint import CheckpointIO\nfrom core.data_loader import InputFetcher\nimport core.utils as utils\nfrom metrics.eval import calculate_metrics\n\n\nclass Solver(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n        self.nets, self.nets_ema = build_model(args)\n        # below setattrs are to make networks be children of Solver, e.g., for self.to(self.device)\n        for name, module in self.nets.items():\n            utils.print_network(module, name)\n            setattr(self, name, module)\n        for name, module in self.nets_ema.items():\n            setattr(self, name + \'_ema\', module)\n\n        if args.mode == \'train\':\n            self.optims = Munch()\n            for net in self.nets.keys():\n                if net == \'fan\':\n                    continue\n                self.optims[net] = torch.optim.Adam(\n                    params=self.nets[net].parameters(),\n                    lr=args.f_lr if net == \'mapping_network\' else args.lr,\n                    betas=[args.beta1, args.beta2],\n                    weight_decay=args.weight_decay)\n\n            self.ckptios = [\n                CheckpointIO(ospj(args.checkpoint_dir, \'{:06d}_nets.ckpt\'), **self.nets),\n                CheckpointIO(ospj(args.checkpoint_dir, \'{:06d}_nets_ema.ckpt\'), **self.nets_ema),\n                CheckpointIO(ospj(args.checkpoint_dir, \'{:06d}_optims.ckpt\'), **self.optims)]\n        else:\n            self.ckptios = [CheckpointIO(ospj(args.checkpoint_dir, \'{:06d}_nets_ema.ckpt\'), **self.nets_ema)]\n\n        self.to(self.device)\n        for name, network in self.named_children():\n            # Do not initialize the FAN parameters\n            if (\'ema\' not in name) and (\'fan\' not in name):\n                print(\'Initializing %s...\' % name)\n                network.apply(utils.he_init)\n\n    def _save_checkpoint(self, step):\n        for ckptio in self.ckptios:\n            ckptio.save(step)\n\n    def _load_checkpoint(self, step):\n        for ckptio in self.ckptios:\n            ckptio.load(step)\n\n    def _reset_grad(self):\n        for optim in self.optims.values():\n            optim.zero_grad()\n\n    def train(self, loaders):\n        args = self.args\n        nets = self.nets\n        nets_ema = self.nets_ema\n        optims = self.optims\n\n        # fetch random validation images for debugging\n        fetcher = InputFetcher(loaders.src, loaders.ref, args.latent_dim, \'train\')\n        fetcher_val = InputFetcher(loaders.val, None, args.latent_dim, \'val\')\n        inputs_val = next(fetcher_val)\n\n        # resume training if necessary\n        if args.resume_iter > 0:\n            self._load_checkpoint(args.resume_iter)\n\n        # remember the initial value of ds weight\n        initial_lambda_ds = args.lambda_ds\n\n        print(\'Start training...\')\n        start_time = time.time()\n        for i in range(args.resume_iter, args.total_iters):\n            # fetch images and labels\n            inputs = next(fetcher)\n            x_real, y_org = inputs.x_src, inputs.y_src\n            x_ref, x_ref2, y_trg = inputs.x_ref, inputs.x_ref2, inputs.y_ref\n            z_trg, z_trg2 = inputs.z_trg, inputs.z_trg2\n\n            masks = nets.fan.get_heatmap(x_real) if args.w_hpf > 0 else None\n\n            # train the discriminator\n            d_loss, d_losses_latent = compute_d_loss(\n                nets, args, x_real, y_org, y_trg, z_trg=z_trg, masks=masks)\n            self._reset_grad()\n            d_loss.backward()\n            optims.discriminator.step()\n\n            d_loss, d_losses_ref = compute_d_loss(\n                nets, args, x_real, y_org, y_trg, x_ref=x_ref, masks=masks)\n            self._reset_grad()\n            d_loss.backward()\n            optims.discriminator.step()\n\n            # train the generator\n            g_loss, g_losses_latent = compute_g_loss(\n                nets, args, x_real, y_org, y_trg, z_trgs=[z_trg, z_trg2], masks=masks)\n            self._reset_grad()\n            g_loss.backward()\n            optims.generator.step()\n            optims.mapping_network.step()\n            optims.style_encoder.step()\n\n            g_loss, g_losses_ref = compute_g_loss(\n                nets, args, x_real, y_org, y_trg, x_refs=[x_ref, x_ref2], masks=masks)\n            self._reset_grad()\n            g_loss.backward()\n            optims.generator.step()\n\n            # compute moving average of network parameters\n            moving_average(nets.generator, nets_ema.generator, beta=0.999)\n            moving_average(nets.mapping_network, nets_ema.mapping_network, beta=0.999)\n            moving_average(nets.style_encoder, nets_ema.style_encoder, beta=0.999)\n\n            # decay weight for diversity sensitive loss\n            if args.lambda_ds > 0:\n                args.lambda_ds -= (initial_lambda_ds / args.ds_iter)\n\n            # print out log info\n            if (i+1) % args.print_every == 0:\n                elapsed = time.time() - start_time\n                elapsed = str(datetime.timedelta(seconds=elapsed))[:-7]\n                log = ""Elapsed time [%s], Iteration [%i/%i], "" % (elapsed, i+1, args.total_iters)\n                all_losses = dict()\n                for loss, prefix in zip([d_losses_latent, d_losses_ref, g_losses_latent, g_losses_ref],\n                                        [\'D/latent_\', \'D/ref_\', \'G/latent_\', \'G/ref_\']):\n                    for key, value in loss.items():\n                        all_losses[prefix + key] = value\n                all_losses[\'G/lambda_ds\'] = args.lambda_ds\n                log += \' \'.join([\'%s: [%.4f]\' % (key, value) for key, value in all_losses.items()])\n                print(log)\n\n            # generate images for debugging\n            if (i+1) % args.sample_every == 0:\n                os.makedirs(args.sample_dir, exist_ok=True)\n                utils.debug_image(nets_ema, args, inputs=inputs_val, step=i+1)\n\n            # save model checkpoints\n            if (i+1) % args.save_every == 0:\n                self._save_checkpoint(step=i+1)\n\n            # compute FID and LPIPS if necessary\n            if (i+1) % args.eval_every == 0:\n                calculate_metrics(nets_ema, args, i+1, mode=\'latent\')\n                calculate_metrics(nets_ema, args, i+1, mode=\'reference\')\n\n    @torch.no_grad()\n    def sample(self, loaders):\n        args = self.args\n        nets_ema = self.nets_ema\n        os.makedirs(args.result_dir, exist_ok=True)\n        self._load_checkpoint(args.resume_iter)\n\n        src = next(InputFetcher(loaders.src, None, args.latent_dim, \'test\'))\n        ref = next(InputFetcher(loaders.ref, None, args.latent_dim, \'test\'))\n\n        fname = ospj(args.result_dir, \'reference.jpg\')\n        print(\'Working on {}...\'.format(fname))\n        utils.translate_using_reference(nets_ema, args, src.x, ref.x, ref.y, fname)\n\n        fname = ospj(args.result_dir, \'video_ref.mp4\')\n        print(\'Working on {}...\'.format(fname))\n        utils.video_ref(nets_ema, args, src.x, ref.x, ref.y, fname)\n\n    @torch.no_grad()\n    def evaluate(self):\n        args = self.args\n        nets_ema = self.nets_ema\n        resume_iter = args.resume_iter\n        self._load_checkpoint(args.resume_iter)\n        calculate_metrics(nets_ema, args, step=resume_iter, mode=\'latent\')\n        calculate_metrics(nets_ema, args, step=resume_iter, mode=\'reference\')\n\n\ndef compute_d_loss(nets, args, x_real, y_org, y_trg, z_trg=None, x_ref=None, masks=None):\n    assert (z_trg is None) != (x_ref is None)\n    # with real images\n    x_real.requires_grad_()\n    out = nets.discriminator(x_real, y_org)\n    loss_real = adv_loss(out, 1)\n    loss_reg = r1_reg(out, x_real)\n\n    # with fake images\n    with torch.no_grad():\n        if z_trg is not None:\n            s_trg = nets.mapping_network(z_trg, y_trg)\n        else:  # x_ref is not None\n            s_trg = nets.style_encoder(x_ref, y_trg)\n\n        x_fake = nets.generator(x_real, s_trg, masks=masks)\n    out = nets.discriminator(x_fake, y_trg)\n    loss_fake = adv_loss(out, 0)\n\n    loss = loss_real + loss_fake + args.lambda_reg * loss_reg\n    return loss, Munch(real=loss_real.item(),\n                       fake=loss_fake.item(),\n                       reg=loss_reg.item())\n\n\ndef compute_g_loss(nets, args, x_real, y_org, y_trg, z_trgs=None, x_refs=None, masks=None):\n    assert (z_trgs is None) != (x_refs is None)\n    if z_trgs is not None:\n        z_trg, z_trg2 = z_trgs\n    if x_refs is not None:\n        x_ref, x_ref2 = x_refs\n\n    # adversarial loss\n    if z_trgs is not None:\n        s_trg = nets.mapping_network(z_trg, y_trg)\n    else:\n        s_trg = nets.style_encoder(x_ref, y_trg)\n\n    x_fake = nets.generator(x_real, s_trg, masks=masks)\n    out = nets.discriminator(x_fake, y_trg)\n    loss_adv = adv_loss(out, 1)\n\n    # style reconstruction loss\n    s_pred = nets.style_encoder(x_fake, y_trg)\n    loss_sty = torch.mean(torch.abs(s_pred - s_trg))\n\n    # diversity sensitive loss\n    if z_trgs is not None:\n        s_trg2 = nets.mapping_network(z_trg2, y_trg)\n    else:\n        s_trg2 = nets.style_encoder(x_ref2, y_trg)\n    x_fake2 = nets.generator(x_real, s_trg2, masks=masks)\n    x_fake2 = x_fake2.detach()\n    loss_ds = torch.mean(torch.abs(x_fake - x_fake2))\n\n    # cycle-consistency loss\n    masks = nets.fan.get_heatmap(x_fake) if args.w_hpf > 0 else None\n    s_org = nets.style_encoder(x_real, y_org)\n    x_rec = nets.generator(x_fake, s_org, masks=masks)\n    loss_cyc = torch.mean(torch.abs(x_rec - x_real))\n\n    loss = loss_adv + args.lambda_sty * loss_sty \\\n        - args.lambda_ds * loss_ds + args.lambda_cyc * loss_cyc\n    return loss, Munch(adv=loss_adv.item(),\n                       sty=loss_sty.item(),\n                       ds=loss_ds.item(),\n                       cyc=loss_cyc.item())\n\n\ndef moving_average(model, model_test, beta=0.999):\n    for param, param_test in zip(model.parameters(), model_test.parameters()):\n        param_test.data = torch.lerp(param.data, param_test.data, beta)\n\n\ndef adv_loss(logits, target):\n    assert target in [1, 0]\n    targets = torch.full_like(logits, fill_value=target)\n    loss = F.binary_cross_entropy_with_logits(logits, targets)\n    return loss\n\n\ndef r1_reg(d_out, x_in):\n    # zero-centered gradient penalty for real images\n    batch_size = x_in.size(0)\n    grad_dout = torch.autograd.grad(\n        outputs=d_out.sum(), inputs=x_in,\n        create_graph=True, retain_graph=True, only_inputs=True\n    )[0]\n    grad_dout2 = grad_dout.pow(2)\n    assert(grad_dout2.size() == x_in.size())\n    reg = 0.5 * grad_dout2.view(batch_size, -1).sum(1).mean(0)\n    return reg'"
core/utils.py,33,"b'""""""\nStarGAN v2\nCopyright (c) 2020-present NAVER Corp.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial\n4.0 International License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n""""""\n\nimport os\nfrom os.path import join as ospj\nimport json\nimport glob\nfrom shutil import copyfile\n\nfrom tqdm import tqdm\nimport ffmpeg\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.utils as vutils\n\n\ndef save_json(json_file, filename):\n    with open(filename, \'w\') as f:\n        json.dump(json_file, f, indent=4, sort_keys=False)\n\n\ndef print_network(network, name):\n    num_params = 0\n    for p in network.parameters():\n        num_params += p.numel()\n    # print(network)\n    print(""Number of parameters of %s: %i"" % (name, num_params))\n\n\ndef he_init(module):\n    if isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight, mode=\'fan_in\', nonlinearity=\'relu\')\n        if module.bias is not None:\n            nn.init.constant_(module.bias, 0)\n    if isinstance(module, nn.Linear):\n        nn.init.kaiming_normal_(module.weight, mode=\'fan_in\', nonlinearity=\'relu\')\n        if module.bias is not None:\n            nn.init.constant_(module.bias, 0)\n\n\ndef denormalize(x):\n    out = (x + 1) / 2\n    return out.clamp_(0, 1)\n\n\ndef save_image(x, ncol, filename):\n    x = denormalize(x)\n    vutils.save_image(x.cpu(), filename, nrow=ncol, padding=0)\n\n\n@torch.no_grad()\ndef translate_and_reconstruct(nets, args, x_src, y_src, x_ref, y_ref, filename):\n    N, C, H, W = x_src.size()\n    s_ref = nets.style_encoder(x_ref, y_ref)\n    masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n    x_fake = nets.generator(x_src, s_ref, masks=masks)\n    s_src = nets.style_encoder(x_src, y_src)\n    masks = nets.fan.get_heatmap(x_fake) if args.w_hpf > 0 else None\n    x_rec = nets.generator(x_fake, s_src, masks=masks)\n    x_concat = [x_src, x_ref, x_fake, x_rec]\n    x_concat = torch.cat(x_concat, dim=0)\n    save_image(x_concat, N, filename)\n    del x_concat\n\n\n@torch.no_grad()\ndef translate_using_latent(nets, args, x_src, y_trg_list, z_trg_list, psi, filename):\n    N, C, H, W = x_src.size()\n    latent_dim = z_trg_list[0].size(1)\n    x_concat = [x_src]\n    masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n\n    for i, y_trg in enumerate(y_trg_list):\n        z_many = torch.randn(10000, latent_dim).to(x_src.device)\n        y_many = torch.LongTensor(10000).to(x_src.device).fill_(y_trg[0])\n        s_many = nets.mapping_network(z_many, y_many)\n        s_avg = torch.mean(s_many, dim=0, keepdim=True)\n        s_avg = s_avg.repeat(N, 1)\n\n        for z_trg in z_trg_list:\n            s_trg = nets.mapping_network(z_trg, y_trg)\n            s_trg = torch.lerp(s_avg, s_trg, psi)\n            x_fake = nets.generator(x_src, s_trg, masks=masks)\n            x_concat += [x_fake]\n\n    x_concat = torch.cat(x_concat, dim=0)\n    save_image(x_concat, N, filename)\n\n\n@torch.no_grad()\ndef translate_using_reference(nets, args, x_src, x_ref, y_ref, filename):\n    N, C, H, W = x_src.size()\n    wb = torch.ones(1, C, H, W).to(x_src.device)\n    x_src_with_wb = torch.cat([wb, x_src], dim=0)\n\n    masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n    s_ref = nets.style_encoder(x_ref, y_ref)\n    s_ref_list = s_ref.unsqueeze(1).repeat(1, N, 1)\n    x_concat = [x_src_with_wb]\n    for i, s_ref in enumerate(s_ref_list):\n        x_fake = nets.generator(x_src, s_ref, masks=masks)\n        x_fake_with_ref = torch.cat([x_ref[i:i+1], x_fake], dim=0)\n        x_concat += [x_fake_with_ref]\n\n    x_concat = torch.cat(x_concat, dim=0)\n    save_image(x_concat, N+1, filename)\n    del x_concat\n\n\n@torch.no_grad()\ndef debug_image(nets, args, inputs, step):\n    x_src, y_src = inputs.x_src, inputs.y_src\n    x_ref, y_ref = inputs.x_ref, inputs.y_ref\n\n    device = inputs.x_src.device\n    N = inputs.x_src.size(0)\n\n    # translate and reconstruct (reference-guided)\n    filename = ospj(args.sample_dir, \'%06d_cycle_consistency.jpg\' % (step))\n    translate_and_reconstruct(nets, args, x_src, y_src, x_ref, y_ref, filename)\n\n    # latent-guided image synthesis\n    y_trg_list = [torch.tensor(y).repeat(N).to(device)\n                  for y in range(min(args.num_domains, 5))]\n    z_trg_list = torch.randn(args.num_outs_per_domain, 1, args.latent_dim).repeat(1, N, 1).to(device)\n    for psi in [0.5, 0.7, 1.0]:\n        filename = ospj(args.sample_dir, \'%06d_latent_psi_%.1f.jpg\' % (step, psi))\n        translate_using_latent(nets, args, x_src, y_trg_list, z_trg_list, psi, filename)\n\n    # reference-guided image synthesis\n    filename = ospj(args.sample_dir, \'%06d_reference.jpg\' % (step))\n    translate_using_reference(nets, args, x_src, x_ref, y_ref, filename)\n\n\n# ======================= #\n# Video-related functions #\n# ======================= #\n\n\ndef sigmoid(x, w=1):\n    return 1. / (1 + np.exp(-w * x))\n\n\ndef get_alphas(start=-5, end=5, step=0.5, len_tail=10):\n    return [0] + [sigmoid(alpha) for alpha in np.arange(start, end, step)] + [1] * len_tail\n\n\ndef interpolate(nets, args, x_src, s_prev, s_next):\n    \'\'\' returns T x C x H x W \'\'\'\n    B = x_src.size(0)\n    frames = []\n    masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n    alphas = get_alphas()\n\n    for alpha in alphas:\n        s_ref = torch.lerp(s_prev, s_next, alpha)\n        x_fake = nets.generator(x_src, s_ref, masks=masks)\n        entries = torch.cat([x_src.cpu(), x_fake.cpu()], dim=2)\n        frame = torchvision.utils.make_grid(entries, nrow=B, padding=0, pad_value=-1).unsqueeze(0)\n        frames.append(frame)\n    frames = torch.cat(frames)\n    return frames\n\n\ndef slide(entries, margin=32):\n    """"""Returns a sliding reference window.\n    Args:\n        entries: a list containing two reference images, x_prev and x_next, \n                 both of which has a shape (1, 3, 256, 256)\n    Returns:\n        canvas: output slide of shape (num_frames, 3, 256*2, 256+margin)\n    """"""\n    _, C, H, W = entries[0].shape\n    alphas = get_alphas()\n    T = len(alphas) # number of frames\n\n    canvas = - torch.ones((T, C, H*2, W + margin))\n    merged = torch.cat(entries, dim=2)  # (1, 3, 512, 256)\n    for t, alpha in enumerate(alphas):\n        top = int(H * (1 - alpha))  # top, bottom for canvas\n        bottom = H * 2\n        m_top = 0  # top, bottom for merged\n        m_bottom = 2 * H - top\n        canvas[t, :, top:bottom, :W] = merged[:, :, m_top:m_bottom, :]\n    return canvas\n\n\n@torch.no_grad()\ndef video_ref(nets, args, x_src, x_ref, y_ref, fname):\n    video = []\n    s_ref = nets.style_encoder(x_ref, y_ref)\n    s_prev = None\n    for data_next in tqdm(zip(x_ref, y_ref, s_ref), \'video_ref\', len(x_ref)):\n        x_next, y_next, s_next = [d.unsqueeze(0) for d in data_next]\n        if s_prev is None:\n            x_prev, y_prev, s_prev = x_next, y_next, s_next\n            continue\n        if y_prev != y_next:\n            x_prev, y_prev, s_prev = x_next, y_next, s_next\n            continue\n\n        interpolated = interpolate(nets, args, x_src, s_prev, s_next)\n        entries = [x_prev, x_next]\n        slided = slide(entries)  # (T, C, 256*2, 256)\n        frames = torch.cat([slided, interpolated], dim=3).cpu()  # (T, C, 256*2, 256*(batch+1))\n        video.append(frames)\n        x_prev, y_prev, s_prev = x_next, y_next, s_next\n\n    # append last frame 10 time\n    for _ in range(10):\n        video.append(frames[-1:])\n    video = tensor2ndarray255(torch.cat(video))\n    save_video(fname, video)\n\n\n@torch.no_grad()\ndef video_latent(nets, args, x_src, y_list, z_list, psi, fname):\n    latent_dim = z_list[0].size(1)\n    s_list = []\n    for i, y_trg in enumerate(y_list):\n        z_many = torch.randn(10000, latent_dim).to(x_src.device)\n        y_many = torch.LongTensor(10000).to(x_src.device).fill_(y_trg[0])\n        s_many = nets.mapping_network(z_many, y_many)\n        s_avg = torch.mean(s_many, dim=0, keepdim=True)\n        s_avg = s_avg.repeat(x_src.size(0), 1)\n\n        for z_trg in z_list:\n            s_trg = nets.mapping_network(z_trg, y_trg)\n            s_trg = torch.lerp(s_avg, s_trg, psi)\n            s_list.append(s_trg)\n\n    s_prev = None\n    video = []\n    # fetch reference images\n    for idx_ref, s_next in enumerate(tqdm(s_list, \'video_latent\', len(s_list))):\n        if s_prev is None:\n            s_prev = s_next\n            continue\n        if idx_ref % len(z_list) == 0:\n            s_prev = s_next\n            continue\n        frames = interpolate(nets, args, x_src, s_prev, s_next).cpu()\n        video.append(frames)\n        s_prev = s_next\n    for _ in range(10):\n        video.append(frames[-1:])\n    video = tensor2ndarray255(torch.cat(video))\n    save_video(fname, video)\n\n\ndef save_video(fname, images, output_fps=30, vcodec=\'libx264\', filters=\'\'):\n    assert isinstance(images, np.ndarray), ""images should be np.array: NHWC""\n    num_frames, height, width, channels = images.shape\n    stream = ffmpeg.input(\'pipe:\', format=\'rawvideo\', \n                          pix_fmt=\'rgb24\', s=\'{}x{}\'.format(width, height))\n    stream = ffmpeg.filter(stream, \'setpts\', \'2*PTS\')  # 2*PTS is for slower playback\n    stream = ffmpeg.output(stream, fname, pix_fmt=\'yuv420p\', vcodec=vcodec, r=output_fps)\n    stream = ffmpeg.overwrite_output(stream)\n    process = ffmpeg.run_async(stream, pipe_stdin=True)\n    for frame in tqdm(images, desc=\'writing video to %s\' % fname):\n        process.stdin.write(frame.astype(np.uint8).tobytes())\n    process.stdin.close()\n    process.wait()\n\n\ndef tensor2ndarray255(images):\n    images = torch.clamp(images * 0.5 + 0.5, 0, 1)\n    return images.cpu().numpy().transpose(0, 2, 3, 1) * 255'"
core/wing.py,43,"b'""""""\nStarGAN v2\nCopyright (c) 2020-present NAVER Corp.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial\n4.0 International License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n\nLines (19 to 80) were adapted from https://github.com/1adrianb/face-alignment\nLines (83 to 235) were adapted from https://github.com/protossw512/AdaptiveWingLoss\n""""""\n\nfrom collections import namedtuple\nfrom copy import deepcopy\nfrom functools import partial\n\nfrom munch import Munch\nimport numpy as np\nimport cv2\nfrom skimage.filters import gaussian\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef get_preds_fromhm(hm):\n    max, idx = torch.max(\n        hm.view(hm.size(0), hm.size(1), hm.size(2) * hm.size(3)), 2)\n    idx += 1\n    preds = idx.view(idx.size(0), idx.size(1), 1).repeat(1, 1, 2).float()\n    preds[..., 0].apply_(lambda x: (x - 1) % hm.size(3) + 1)\n    preds[..., 1].add_(-1).div_(hm.size(2)).floor_().add_(1)\n\n    for i in range(preds.size(0)):\n        for j in range(preds.size(1)):\n            hm_ = hm[i, j, :]\n            pX, pY = int(preds[i, j, 0]) - 1, int(preds[i, j, 1]) - 1\n            if pX > 0 and pX < 63 and pY > 0 and pY < 63:\n                diff = torch.FloatTensor(\n                    [hm_[pY, pX + 1] - hm_[pY, pX - 1],\n                     hm_[pY + 1, pX] - hm_[pY - 1, pX]])\n                preds[i, j].add_(diff.sign_().mul_(.25))\n\n    preds.add_(-0.5)\n    return preds\n\n\nclass HourGlass(nn.Module):\n    def __init__(self, num_modules, depth, num_features, first_one=False):\n        super(HourGlass, self).__init__()\n        self.num_modules = num_modules\n        self.depth = depth\n        self.features = num_features\n        self.coordconv = CoordConvTh(64, 64, True, True, 256, first_one,\n                                     out_channels=256,\n                                     kernel_size=1, stride=1, padding=0)\n        self._generate_network(self.depth)\n\n    def _generate_network(self, level):\n        self.add_module(\'b1_\' + str(level), ConvBlock(256, 256))\n        self.add_module(\'b2_\' + str(level), ConvBlock(256, 256))\n        if level > 1:\n            self._generate_network(level - 1)\n        else:\n            self.add_module(\'b2_plus_\' + str(level), ConvBlock(256, 256))\n        self.add_module(\'b3_\' + str(level), ConvBlock(256, 256))\n\n    def _forward(self, level, inp):\n        up1 = inp\n        up1 = self._modules[\'b1_\' + str(level)](up1)\n        low1 = F.avg_pool2d(inp, 2, stride=2)\n        low1 = self._modules[\'b2_\' + str(level)](low1)\n\n        if level > 1:\n            low2 = self._forward(level - 1, low1)\n        else:\n            low2 = low1\n            low2 = self._modules[\'b2_plus_\' + str(level)](low2)\n        low3 = low2\n        low3 = self._modules[\'b3_\' + str(level)](low3)\n        up2 = F.interpolate(low3, scale_factor=2, mode=\'nearest\')\n\n        return up1 + up2\n\n    def forward(self, x, heatmap):\n        x, last_channel = self.coordconv(x, heatmap)\n        return self._forward(self.depth, x), last_channel\n\n\nclass AddCoordsTh(nn.Module):\n    def __init__(self, height=64, width=64, with_r=False, with_boundary=False):\n        super(AddCoordsTh, self).__init__()\n        self.with_r = with_r\n        self.with_boundary = with_boundary\n        device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n        with torch.no_grad():\n            x_coords = torch.arange(height).unsqueeze(1).expand(height, width).float()\n            y_coords = torch.arange(width).unsqueeze(0).expand(height, width).float()\n            x_coords = (x_coords / (height - 1)) * 2 - 1\n            y_coords = (y_coords / (width - 1)) * 2 - 1\n            coords = torch.stack([x_coords, y_coords], dim=0)  # (2, height, width)\n\n            if self.with_r:\n                rr = torch.sqrt(torch.pow(x_coords, 2) + torch.pow(y_coords, 2))  # (height, width)\n                rr = (rr / torch.max(rr)).unsqueeze(0)\n                coords = torch.cat([coords, rr], dim=0)\n\n            self.coords = coords.unsqueeze(0).to(device)  # (1, 2 or 3, height, width)\n            self.x_coords = x_coords.to(device)\n            self.y_coords = y_coords.to(device)\n\n    def forward(self, x, heatmap=None):\n        """"""\n        x: (batch, c, x_dim, y_dim)\n        """"""\n        coords = self.coords.repeat(x.size(0), 1, 1, 1)\n\n        if self.with_boundary and heatmap is not None:\n            boundary_channel = torch.clamp(heatmap[:, -1:, :, :], 0.0, 1.0)\n            zero_tensor = torch.zeros_like(self.x_coords)\n            xx_boundary_channel = torch.where(boundary_channel > 0.05, self.x_coords, zero_tensor).to(zero_tensor.device)\n            yy_boundary_channel = torch.where(boundary_channel > 0.05, self.y_coords, zero_tensor).to(zero_tensor.device)\n            coords = torch.cat([coords, xx_boundary_channel, yy_boundary_channel], dim=1)\n\n        x_and_coords = torch.cat([x, coords], dim=1)\n        return x_and_coords\n\n\nclass CoordConvTh(nn.Module):\n    """"""CoordConv layer as in the paper.""""""\n    def __init__(self, height, width, with_r, with_boundary,\n                 in_channels, first_one=False, *args, **kwargs):\n        super(CoordConvTh, self).__init__()\n        self.addcoords = AddCoordsTh(height, width, with_r, with_boundary)\n        in_channels += 2\n        if with_r:\n            in_channels += 1\n        if with_boundary and not first_one:\n            in_channels += 2\n        self.conv = nn.Conv2d(in_channels=in_channels, *args, **kwargs)\n\n    def forward(self, input_tensor, heatmap=None):\n        ret = self.addcoords(input_tensor, heatmap)\n        last_channel = ret[:, -2:, :, :]\n        ret = self.conv(ret)\n        return ret, last_channel\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_planes, out_planes):\n        super(ConvBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        conv3x3 = partial(nn.Conv2d, kernel_size=3, stride=1, padding=1, bias=False, dilation=1)\n        self.conv1 = conv3x3(in_planes, int(out_planes / 2))\n        self.bn2 = nn.BatchNorm2d(int(out_planes / 2))\n        self.conv2 = conv3x3(int(out_planes / 2), int(out_planes / 4))\n        self.bn3 = nn.BatchNorm2d(int(out_planes / 4))\n        self.conv3 = conv3x3(int(out_planes / 4), int(out_planes / 4))\n\n        self.downsample = None\n        if in_planes != out_planes:\n            self.downsample = nn.Sequential(nn.BatchNorm2d(in_planes),\n                                            nn.ReLU(True),\n                                            nn.Conv2d(in_planes, out_planes, 1, 1, bias=False))\n\n    def forward(self, x):\n        residual = x\n\n        out1 = self.bn1(x)\n        out1 = F.relu(out1, True)\n        out1 = self.conv1(out1)\n\n        out2 = self.bn2(out1)\n        out2 = F.relu(out2, True)\n        out2 = self.conv2(out2)\n\n        out3 = self.bn3(out2)\n        out3 = F.relu(out3, True)\n        out3 = self.conv3(out3)\n\n        out3 = torch.cat((out1, out2, out3), 1)\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n        out3 += residual\n        return out3\n\n\nclass FAN(nn.Module):\n    def __init__(self, num_modules=1, end_relu=False, num_landmarks=98, fname_pretrained=None):\n        super(FAN, self).__init__()\n        self.num_modules = num_modules\n        self.end_relu = end_relu\n\n        # Base part\n        self.conv1 = CoordConvTh(256, 256, True, False,\n                                 in_channels=3, out_channels=64,\n                                 kernel_size=7, stride=2, padding=3)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = ConvBlock(64, 128)\n        self.conv3 = ConvBlock(128, 128)\n        self.conv4 = ConvBlock(128, 256)\n\n        # Stacking part\n        self.add_module(\'m0\', HourGlass(1, 4, 256, first_one=True))\n        self.add_module(\'top_m_0\', ConvBlock(256, 256))\n        self.add_module(\'conv_last0\', nn.Conv2d(256, 256, 1, 1, 0))\n        self.add_module(\'bn_end0\', nn.BatchNorm2d(256))\n        self.add_module(\'l0\', nn.Conv2d(256, num_landmarks+1, 1, 1, 0))\n\n        if fname_pretrained is not None:\n            self.load_pretrained_weights(fname_pretrained)\n\n    def load_pretrained_weights(self, fname):\n        if torch.cuda.is_available():\n            checkpoint = torch.load(fname)\n        else:\n            checkpoint = torch.load(fname, map_location=torch.device(\'cpu\'))\n        model_weights = self.state_dict()\n        model_weights.update({k: v for k, v in checkpoint[\'state_dict\'].items()\n                              if k in model_weights})\n        self.load_state_dict(model_weights)\n\n    def forward(self, x):\n        x, _ = self.conv1(x)\n        x = F.relu(self.bn1(x), True)\n        x = F.avg_pool2d(self.conv2(x), 2, stride=2)\n        x = self.conv3(x)\n        x = self.conv4(x)\n\n        outputs = []\n        boundary_channels = []\n        tmp_out = None\n        ll, boundary_channel = self._modules[\'m0\'](x, tmp_out)\n        ll = self._modules[\'top_m_0\'](ll)\n        ll = F.relu(self._modules[\'bn_end0\']\n                    (self._modules[\'conv_last0\'](ll)), True)\n\n        # Predict heatmaps\n        tmp_out = self._modules[\'l0\'](ll)\n        if self.end_relu:\n            tmp_out = F.relu(tmp_out)  # HACK: Added relu\n        outputs.append(tmp_out)\n        boundary_channels.append(boundary_channel)\n        return outputs, boundary_channels\n\n    @torch.no_grad()\n    def get_heatmap(self, x, b_preprocess=True):\n        \'\'\' outputs 0-1 normalized heatmap \'\'\'\n        x = F.interpolate(x, size=256, mode=\'bilinear\')\n        x_01 = x*0.5 + 0.5\n        outputs, _ = self(x_01)\n        heatmaps = outputs[-1][:, :-1, :, :]\n        scale_factor = x.size(2) // heatmaps.size(2)\n        if b_preprocess:\n            heatmaps = F.interpolate(heatmaps, scale_factor=scale_factor,\n                                     mode=\'bilinear\', align_corners=True)\n            heatmaps = preprocess(heatmaps)\n        return heatmaps\n\n    @torch.no_grad()\n    def get_landmark(self, x):\n        \'\'\' outputs landmarks of x.shape \'\'\'\n        heatmaps = self.get_heatmap(x, b_preprocess=False)\n        landmarks = []\n        for i in range(x.size(0)):\n            pred_landmarks = get_preds_fromhm(heatmaps[i].cpu().unsqueeze(0))\n            landmarks.append(pred_landmarks)\n        scale_factor = x.size(2) // heatmaps.size(2)\n        landmarks = torch.cat(landmarks) * scale_factor\n        return landmarks\n\n\n# ========================== #\n#   Align related functions  #\n# ========================== #\n\n\ndef tensor2numpy255(tensor):\n    """"""Converts torch tensor to numpy array.""""""\n    return ((tensor.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5) * 255).astype(\'uint8\')\n\n\ndef np2tensor(image):\n    """"""Converts numpy array to torch tensor.""""""\n    return torch.FloatTensor(image).permute(2, 0, 1) / 255 * 2 - 1\n\n\nclass FaceAligner():\n    def __init__(self, fname_wing, fname_celeba_mean, output_size):\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n        self.fan = FAN(fname_pretrained=fname_wing).to(self.device).eval()\n        scale = output_size // 256\n        self.CELEB_REF = np.float32(np.load(fname_celeba_mean)[\'mean\']) * scale\n        self.xaxis_ref = landmarks2xaxis(self.CELEB_REF)\n        self.output_size = output_size\n\n    def align(self, imgs, output_size=256):\n        \'\'\' imgs = torch.CUDATensor of BCHW \'\'\'\n        imgs = imgs.to(self.device)\n        landmarkss = self.fan.get_landmark(imgs).cpu().numpy()\n        for i, (img, landmarks) in enumerate(zip(imgs, landmarkss)):\n            img_np = tensor2numpy255(img)\n            img_np, landmarks = pad_mirror(img_np, landmarks)\n            transform = self.landmarks2mat(landmarks)\n            rows, cols, _ = img_np.shape\n            rows = max(rows, self.output_size)\n            cols = max(cols, self.output_size)\n            aligned = cv2.warpPerspective(img_np, transform, (cols, rows), flags=cv2.INTER_LANCZOS4)\n            imgs[i] = np2tensor(aligned[:self.output_size, :self.output_size, :])\n        return imgs\n\n    def landmarks2mat(self, landmarks):\n        T_origin = points2T(landmarks, \'from\')\n        xaxis_src = landmarks2xaxis(landmarks)\n        R = vecs2R(xaxis_src, self.xaxis_ref)\n        S = landmarks2S(landmarks, self.CELEB_REF)\n        T_ref = points2T(self.CELEB_REF, \'to\')\n        matrix = np.dot(T_ref, np.dot(S, np.dot(R, T_origin)))\n        return matrix\n\n\ndef points2T(point, direction):\n    point_mean = point.mean(axis=0)\n    T = np.eye(3)\n    coef = -1 if direction == \'from\' else 1\n    T[:2, 2] = coef * point_mean\n    return T\n\n\ndef landmarks2eyes(landmarks):\n    idx_left = np.array(list(range(60, 67+1)) + [96])\n    idx_right = np.array(list(range(68, 75+1)) + [97])\n    left = landmarks[idx_left]\n    right = landmarks[idx_right]\n    return left.mean(axis=0), right.mean(axis=0)\n\n\ndef landmarks2mouthends(landmarks):\n    left = landmarks[76]\n    right = landmarks[82]\n    return left, right\n\n\ndef rotate90(vec):\n    x, y = vec\n    return np.array([y, -x])\n\n\ndef landmarks2xaxis(landmarks):\n    eye_left, eye_right = landmarks2eyes(landmarks)\n    mouth_left, mouth_right = landmarks2mouthends(landmarks)\n    xp = eye_right - eye_left  # x\' in pggan\n    eye_center = (eye_left + eye_right) * 0.5\n    mouth_center = (mouth_left + mouth_right) * 0.5\n    yp = eye_center - mouth_center\n    xaxis = xp - rotate90(yp)\n    return xaxis / np.linalg.norm(xaxis)\n\n\ndef vecs2R(vec_x, vec_y):\n    vec_x = vec_x / np.linalg.norm(vec_x)\n    vec_y = vec_y / np.linalg.norm(vec_y)\n    c = np.dot(vec_x, vec_y)\n    s = np.sqrt(1 - c * c) * np.sign(np.cross(vec_x, vec_y))\n    R = np.array(((c, -s, 0), (s, c, 0), (0, 0, 1)))\n    return R\n\n\ndef landmarks2S(x, y):\n    x_mean = x.mean(axis=0).squeeze()\n    y_mean = y.mean(axis=0).squeeze()\n    # vectors = mean -> each point\n    x_vectors = x - x_mean\n    y_vectors = y - y_mean\n\n    x_norms = np.linalg.norm(x_vectors, axis=1)\n    y_norms = np.linalg.norm(y_vectors, axis=1)\n\n    indices = [96, 97, 76, 82]  # indices for eyes, lips\n    scale = (y_norms / x_norms)[indices].mean()\n\n    S = np.eye(3)\n    S[0, 0] = S[1, 1] = scale\n    return S\n\n\ndef pad_mirror(img, landmarks):\n    H, W, _ = img.shape\n    img = np.pad(img, ((H//2, H//2), (W//2, W//2), (0, 0)), \'reflect\')\n    small_blurred = gaussian(cv2.resize(img, (W, H)), H//100, multichannel=True)\n    blurred = cv2.resize(small_blurred, (W * 2, H * 2)) * 255\n\n    H, W, _ = img.shape\n    coords = np.meshgrid(np.arange(H), np.arange(W), indexing=""ij"")\n    weight_y = np.clip(coords[0] / (H//4), 0, 1)\n    weight_x = np.clip(coords[1] / (H//4), 0, 1)\n    weight_y = np.minimum(weight_y, np.flip(weight_y, axis=0))\n    weight_x = np.minimum(weight_x, np.flip(weight_x, axis=1))\n    weight = np.expand_dims(np.minimum(weight_y, weight_x), 2)**4\n    img = img * weight + blurred * (1 - weight)\n    landmarks += np.array([W//4, H//4])\n    return img, landmarks\n\n\ndef align_faces(args, input_dir, output_dir):\n    import os\n    from torchvision import transforms\n    from PIL import Image\n    from core.utils import save_image\n\n    aligner = FaceAligner(args.wing_path, args.lm_path, args.img_size)\n    transform = transforms.Compose([\n        transforms.Resize((args.img_size, args.img_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                             std=[0.5, 0.5, 0.5]),\n    ])\n\n    fnames = os.listdir(input_dir)\n    os.makedirs(output_dir, exist_ok=True)\n    fnames.sort()\n    for fname in fnames:\n        image = Image.open(os.path.join(input_dir, fname)).convert(\'RGB\')\n        x = transform(image).unsqueeze(0)\n        x_aligned = aligner.align(x)\n        save_image(x_aligned, 1, filename=os.path.join(output_dir, fname))\n        print(\'Saved the aligned image to %s...\' % fname)\n\n\n# ========================== #\n#   Mask related functions   #\n# ========================== #\n\n\ndef normalize(x, eps=1e-6):\n    """"""Apply min-max normalization.""""""\n    x = x.contiguous()\n    N, C, H, W = x.size()\n    x_ = x.view(N*C, -1)\n    max_val = torch.max(x_, dim=1, keepdim=True)[0]\n    min_val = torch.min(x_, dim=1, keepdim=True)[0]\n    x_ = (x_ - min_val) / (max_val - min_val + eps)\n    out = x_.view(N, C, H, W)\n    return out\n\n\ndef truncate(x, thres=0.1):\n    """"""Remove small values in heatmaps.""""""\n    return torch.where(x < thres, torch.zeros_like(x), x)\n\n\ndef resize(x, p=2):\n    """"""Resize heatmaps.""""""\n    return x**p\n\n\ndef shift(x, N):\n    """"""Shift N pixels up or down.""""""\n    up = N >= 0\n    N = abs(N)\n    _, _, H, W = x.size()\n    head = torch.arange(N)\n    tail = torch.arange(H-N)\n\n    if up:\n        head = torch.arange(H-N)+N\n        tail = torch.arange(N)\n    else:\n        head = torch.arange(N) + (H-N)\n        tail = torch.arange(H-N)\n\n    # permutation indices\n    perm = torch.cat([head, tail]).to(x.device)\n    out = x[:, :, perm, :]\n    return out\n\n\nIDXPAIR = namedtuple(\'IDXPAIR\', \'start end\')\nindex_map = Munch(chin=IDXPAIR(0 + 8, 33 - 8),\n                  eyebrows=IDXPAIR(33, 51),\n                  eyebrowsedges=IDXPAIR(33, 46),\n                  nose=IDXPAIR(51, 55),\n                  nostrils=IDXPAIR(55, 60),\n                  eyes=IDXPAIR(60, 76),\n                  lipedges=IDXPAIR(76, 82),\n                  lipupper=IDXPAIR(77, 82),\n                  liplower=IDXPAIR(83, 88),\n                  lipinner=IDXPAIR(88, 96))\nOPPAIR = namedtuple(\'OPPAIR\', \'shift resize\')\n\n\ndef preprocess(x):\n    """"""Preprocess 98-dimensional heatmaps.""""""\n    N, C, H, W = x.size()\n    x = truncate(x)\n    x = normalize(x)\n\n    sw = H // 256\n    operations = Munch(chin=OPPAIR(0, 3),\n                       eyebrows=OPPAIR(-7*sw, 2),\n                       nostrils=OPPAIR(8*sw, 4),\n                       lipupper=OPPAIR(-8*sw, 4),\n                       liplower=OPPAIR(8*sw, 4),\n                       lipinner=OPPAIR(-2*sw, 3))\n\n    for part, ops in operations.items():\n        start, end = index_map[part]\n        x[:, start:end] = resize(shift(x[:, start:end], ops.shift), ops.resize)\n\n    zero_out = torch.cat([torch.arange(0, index_map.chin.start),\n                          torch.arange(index_map.chin.end, 33),\n                          torch.LongTensor([index_map.eyebrowsedges.start,\n                                            index_map.eyebrowsedges.end,\n                                            index_map.lipedges.start,\n                                            index_map.lipedges.end])])\n    x[:, zero_out] = 0\n\n    start, end = index_map.nose\n    x[:, start+1:end] = shift(x[:, start+1:end], 4*sw)\n    x[:, start:end] = resize(x[:, start:end], 1)\n\n    start, end = index_map.eyes\n    x[:, start:end] = resize(x[:, start:end], 1)\n    x[:, start:end] = resize(shift(x[:, start:end], -8), 3) + \\\n        shift(x[:, start:end], -24)\n\n    # Second-level mask\n    x2 = deepcopy(x)\n    x2[:, index_map.chin.start:index_map.chin.end] = 0  # start:end was 0:33\n    x2[:, index_map.lipedges.start:index_map.lipinner.end] = 0  # start:end was 76:96\n    x2[:, index_map.eyebrows.start:index_map.eyebrows.end] = 0  # start:end was 33:51\n\n    x = torch.sum(x, dim=1, keepdim=True)  # (N, 1, H, W)\n    x2 = torch.sum(x2, dim=1, keepdim=True)  # mask without faceline and mouth\n\n    x[x != x] = 0  # set nan to zero\n    x2[x != x] = 0  # set nan to zero\n    return x.clamp_(0, 1), x2.clamp_(0, 1)'"
metrics/__init__.py,0,b''
metrics/eval.py,4,"b'""""""\nStarGAN v2\nCopyright (c) 2020-present NAVER Corp.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial\n4.0 International License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n""""""\n\nimport os\nimport shutil\nfrom collections import OrderedDict\nfrom tqdm import tqdm\n\nimport numpy as np\nimport torch\n\nfrom metrics.fid import calculate_fid_given_paths\nfrom metrics.lpips import calculate_lpips_given_images\nfrom core.data_loader import get_eval_loader\nfrom core import utils\n\n\n@torch.no_grad()\ndef calculate_metrics(nets, args, step, mode):\n    print(\'Calculating evaluation metrics...\')\n    assert mode in [\'latent\', \'reference\']\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    domains = os.listdir(args.val_img_dir)\n    domains.sort()\n    num_domains = len(domains)\n    print(\'Number of domains: %d\' % num_domains)\n\n    lpips_dict = OrderedDict()\n    for trg_idx, trg_domain in enumerate(domains):\n        src_domains = [x for x in domains if x != trg_domain]\n\n        if mode == \'reference\':\n            path_ref = os.path.join(args.val_img_dir, trg_domain)\n            loader_ref = get_eval_loader(root=path_ref,\n                                         img_size=args.img_size,\n                                         batch_size=args.val_batch_size,\n                                         imagenet_normalize=False,\n                                         drop_last=True)\n\n        for src_idx, src_domain in enumerate(src_domains):\n            path_src = os.path.join(args.val_img_dir, src_domain)\n            loader_src = get_eval_loader(root=path_src,\n                                         img_size=args.img_size,\n                                         batch_size=args.val_batch_size,\n                                         imagenet_normalize=False)\n\n            task = \'%s2%s\' % (src_domain, trg_domain)\n            path_fake = os.path.join(args.eval_dir, task)\n            shutil.rmtree(path_fake, ignore_errors=True)\n            os.makedirs(path_fake)\n\n            lpips_values = []\n            print(\'Generating images and calculating LPIPS for %s...\' % task)\n            for i, x_src in enumerate(tqdm(loader_src, total=len(loader_src))):\n                N = x_src.size(0)\n                x_src = x_src.to(device)\n                y_trg = torch.tensor([trg_idx] * N).to(device)\n                masks = nets.fan.get_heatmap(x_src) if args.w_hpf > 0 else None\n\n                # generate 10 outputs from the same input\n                group_of_images = []\n                for j in range(args.num_outs_per_domain):\n                    if mode == \'latent\':\n                        z_trg = torch.randn(N, args.latent_dim).to(device)\n                        s_trg = nets.mapping_network(z_trg, y_trg)\n                    else:\n                        try:\n                            x_ref = next(iter_ref).to(device)\n                        except:\n                            iter_ref = iter(loader_ref)\n                            x_ref = next(iter_ref).to(device)\n\n                        if x_ref.size(0) > N:\n                            x_ref = x_ref[:N]\n                        s_trg = nets.style_encoder(x_ref, y_trg)\n\n                    x_fake = nets.generator(x_src, s_trg, masks=masks)\n                    group_of_images.append(x_fake)\n\n                    # save generated images to calculate FID later\n                    for k in range(N):\n                        filename = os.path.join(\n                            path_fake,\n                            \'%.4i_%.2i.png\' % (i*args.val_batch_size+(k+1), j+1))\n                        utils.save_image(x_fake[k], ncol=1, filename=filename)\n\n                lpips_value = calculate_lpips_given_images(group_of_images)\n                lpips_values.append(lpips_value)\n\n            # calculate LPIPS for each task (e.g. cat2dog, dog2cat)\n            lpips_mean = np.array(lpips_values).mean()\n            lpips_dict[\'LPIPS_%s/%s\' % (mode, task)] = lpips_mean\n\n        # delete dataloaders\n        del loader_src\n        if mode == \'reference\':\n            del loader_ref\n            del iter_ref\n\n    # calculate the average LPIPS for all tasks\n    lpips_mean = 0\n    for _, value in lpips_dict.items():\n        lpips_mean += value / len(lpips_dict)\n    lpips_dict[\'LPIPS_%s/mean\' % mode] = lpips_mean\n\n    # report LPIPS values\n    filename = os.path.join(args.eval_dir, \'LPIPS_%.5i_%s.json\' % (step, mode))\n    utils.save_json(lpips_dict, filename)\n\n    # calculate and report fid values\n    calculate_fid_for_all_tasks(args, domains, step=step, mode=mode)\n\n\ndef calculate_fid_for_all_tasks(args, domains, step, mode):\n    print(\'Calculating FID for all tasks...\')\n    fid_values = OrderedDict()\n    for trg_domain in domains:\n        src_domains = [x for x in domains if x != trg_domain]\n\n        for src_domain in src_domains:\n            task = \'%s2%s\' % (src_domain, trg_domain)\n            path_real = os.path.join(args.train_img_dir, trg_domain)\n            path_fake = os.path.join(args.eval_dir, task)\n            print(\'Calculating FID for %s...\' % task)\n            fid_value = calculate_fid_given_paths(\n                paths=[path_real, path_fake],\n                img_size=args.img_size,\n                batch_size=args.val_batch_size)\n            fid_values[\'FID_%s/%s\' % (mode, task)] = fid_value\n\n    # calculate the average FID for all tasks\n    fid_mean = 0\n    for _, value in fid_values.items():\n        fid_mean += value / len(fid_values)\n    fid_values[\'FID_%s/mean\' % mode] = fid_mean\n\n    # report FID values\n    filename = os.path.join(args.eval_dir, \'FID_%.5i_%s.json\' % (step, mode))\n    utils.save_json(fid_values, filename)\n'"
metrics/fid.py,4,"b'""""""\nStarGAN v2\nCopyright (c) 2020-present NAVER Corp.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial\n4.0 International License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n""""""\n\nimport os\nimport argparse\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torchvision import models\nfrom scipy import linalg\nfrom core.data_loader import get_eval_loader\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(x): return x\n\n\nclass InceptionV3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        inception = models.inception_v3(pretrained=True)\n        self.block1 = nn.Sequential(\n            inception.Conv2d_1a_3x3, inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2))\n        self.block2 = nn.Sequential(\n            inception.Conv2d_3b_1x1, inception.Conv2d_4a_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2))\n        self.block3 = nn.Sequential(\n            inception.Mixed_5b, inception.Mixed_5c,\n            inception.Mixed_5d, inception.Mixed_6a,\n            inception.Mixed_6b, inception.Mixed_6c,\n            inception.Mixed_6d, inception.Mixed_6e)\n        self.block4 = nn.Sequential(\n            inception.Mixed_7a, inception.Mixed_7b,\n            inception.Mixed_7c,\n            nn.AdaptiveAvgPool2d(output_size=(1, 1)))\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        return x.view(x.size(0), -1)\n\n\ndef frechet_distance(mu, cov, mu2, cov2):\n    cc, _ = linalg.sqrtm(np.dot(cov, cov2), disp=False)\n    dist = np.sum((mu -mu2)**2) + np.trace(cov + cov2 - 2*cc)\n    return np.real(dist)\n\n\n@torch.no_grad()\ndef calculate_fid_given_paths(paths, img_size=256, batch_size=50):\n    print(\'Calculating FID given paths %s and %s...\' % (paths[0], paths[1]))\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    inception = InceptionV3().eval().to(device)\n    loaders = [get_eval_loader(path, img_size, batch_size) for path in paths]\n\n    mu, cov = [], []\n    for loader in loaders:\n        actvs = []\n        for x in tqdm(loader, total=len(loader)):\n            actv = inception(x.to(device))\n            actvs.append(actv)\n        actvs = torch.cat(actvs, dim=0).cpu().detach().numpy()\n        mu.append(np.mean(actvs, axis=0))\n        cov.append(np.cov(actvs, rowvar=False))\n    fid_value = frechet_distance(mu[0], cov[0], mu[1], cov[1])\n    return fid_value\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--paths\', type=str, nargs=2, help=\'paths to real and fake images\')\n    parser.add_argument(\'--img_size\', type=int, default=256, help=\'image resolution\')\n    parser.add_argument(\'--batch_size\', type=int, default=64, help=\'batch size to use\')\n    args = parser.parse_args()\n    fid_value = calculate_fid_given_paths(args.paths, args.img_size, args.batch_size)\n    print(\'FID: \', fid_value)\n\n# python -m metrics.fid --paths PATH_REAL PATH_FAKE'"
metrics/lpips.py,13,"b'""""""\nStarGAN v2\nCopyright (c) 2020-present NAVER Corp.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial\n4.0 International License. To view a copy of this license, visit\nhttp://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\nCreative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n""""""\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\n\n\ndef normalize(x, eps=1e-10):\n    return x * torch.rsqrt(torch.sum(x**2, dim=1, keepdim=True) + eps)\n\n\nclass AlexNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = models.alexnet(pretrained=True).features\n        self.channels = []\n        for layer in self.layers:\n            if isinstance(layer, nn.Conv2d):\n                self.channels.append(layer.out_channels)\n\n    def forward(self, x):\n        fmaps = []\n        for layer in self.layers:\n            x = layer(x)\n            if isinstance(layer, nn.ReLU):\n                fmaps.append(x)\n        return fmaps\n\n\nclass Conv1x1(nn.Module):\n    def __init__(self, in_channels, out_channels=1):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False))\n\n    def forward(self, x):\n        return self.main(x)\n\n\nclass LPIPS(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.alexnet = AlexNet()\n        self.lpips_weights = nn.ModuleList()\n        for channels in self.alexnet.channels:\n            self.lpips_weights.append(Conv1x1(channels, 1))\n        self._load_lpips_weights()\n        # imagenet normalization for range [-1, 1]\n        self.mu = torch.tensor([-0.03, -0.088, -0.188]).view(1, 3, 1, 1).cuda()\n        self.sigma = torch.tensor([0.458, 0.448, 0.450]).view(1, 3, 1, 1).cuda()\n\n    def _load_lpips_weights(self):\n        own_state_dict = self.state_dict()\n        if torch.cuda.is_available():\n            state_dict = torch.load(\'metrics/lpips_weights.ckpt\')\n        else:\n            state_dict = torch.load(\'metrics/lpips_weights.ckpt\',\n                                    map_location=torch.device(\'cpu\'))\n        for name, param in state_dict.items():\n            if name in own_state_dict:\n                own_state_dict[name].copy_(param)\n\n    def forward(self, x, y):\n        x = (x - self.mu) / self.sigma\n        y = (y - self.mu) / self.sigma\n        x_fmaps = self.alexnet(x)\n        y_fmaps = self.alexnet(y)\n        lpips_value = 0\n        for x_fmap, y_fmap, conv1x1 in zip(x_fmaps, y_fmaps, self.lpips_weights):\n            x_fmap = normalize(x_fmap)\n            y_fmap = normalize(y_fmap)\n            lpips_value += torch.mean(conv1x1((x_fmap - y_fmap)**2))\n        return lpips_value\n\n\n@torch.no_grad()\ndef calculate_lpips_given_images(group_of_images):\n    # group_of_images = [torch.randn(N, C, H, W) for _ in range(10)]\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    lpips = LPIPS().eval().to(device)\n    lpips_values = []\n    num_rand_outputs = len(group_of_images)\n\n    # calculate the average of pairwise distances among all random outputs\n    for i in range(num_rand_outputs-1):\n        for j in range(i+1, num_rand_outputs):\n            lpips_values.append(lpips(group_of_images[i], group_of_images[j]))\n    lpips_value = torch.mean(torch.stack(lpips_values, dim=0))\n    return lpips_value.item()'"
