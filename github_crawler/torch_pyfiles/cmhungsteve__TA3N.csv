file_path,api_count,code
TRNmodule.py,5,"b""import torch\nimport torch.nn as nn\nimport numpy as np\nfrom math import ceil\n\nclass RelationModule(torch.nn.Module):\n    # this is the naive implementation of the n-frame relation module, as num_frames == num_frames_relation\n    def __init__(self, img_feature_dim, num_bottleneck, num_frames):\n        super(RelationModule, self).__init__()\n        self.num_frames = num_frames\n        self.img_feature_dim = img_feature_dim\n        self.num_bottleneck = num_bottleneck\n        self.classifier = self.fc_fusion()\n    def fc_fusion(self):\n        # naive concatenate\n        classifier = nn.Sequential(\n                nn.ReLU(),\n                nn.Linear(self.num_frames * self.img_feature_dim, self.num_bottleneck),\n                nn.ReLU(),\n                )\n        return classifier\n    def forward(self, input):\n        input = input.view(input.size(0), self.num_frames*self.img_feature_dim)\n        input = self.classifier(input)\n        return input\n\nclass RelationModuleMultiScale(torch.nn.Module):\n    # Temporal Relation module in multiply scale, suming over [2-frame relation, 3-frame relation, ..., n-frame relation]\n\n    def __init__(self, img_feature_dim, num_bottleneck, num_frames):\n        super(RelationModuleMultiScale, self).__init__()\n        self.subsample_num = 3 # how many relations selected to sum up\n        self.img_feature_dim = img_feature_dim\n        self.scales = [i for i in range(num_frames, 1, -1)] # generate the multiple frame relations\n\n        self.relations_scales = []\n        self.subsample_scales = []\n        for scale in self.scales:\n            relations_scale = self.return_relationset(num_frames, scale)\n            self.relations_scales.append(relations_scale)\n            self.subsample_scales.append(min(self.subsample_num, len(relations_scale))) # how many samples of relation to select in each forward pass\n\n        # self.num_class = num_class\n        self.num_frames = num_frames\n        self.fc_fusion_scales = nn.ModuleList() # high-tech modulelist\n        for i in range(len(self.scales)):\n            scale = self.scales[i]\n            fc_fusion = nn.Sequential(\n                        nn.ReLU(),\n                        nn.Linear(scale * self.img_feature_dim, num_bottleneck),\n                        nn.ReLU(),\n                        )\n\n            self.fc_fusion_scales += [fc_fusion]\n\n        print('Multi-Scale Temporal Relation Network Module in use', ['%d-frame relation' % i for i in self.scales])\n\n    def forward(self, input):\n        # the first one is the largest scale\n        act_scale_1 = input[:, self.relations_scales[0][0] , :]\n        act_scale_1 = act_scale_1.view(act_scale_1.size(0), self.scales[0] * self.img_feature_dim)\n        act_scale_1 = self.fc_fusion_scales[0](act_scale_1)\n        act_scale_1 = act_scale_1.unsqueeze(1) # add one dimension for the later concatenation\n        act_all = act_scale_1.clone()\n\n        for scaleID in range(1, len(self.scales)):\n            act_relation_all = torch.zeros_like(act_scale_1)\n            # iterate over the scales\n            num_total_relations = len(self.relations_scales[scaleID])\n            num_select_relations = self.subsample_scales[scaleID]\n            idx_relations_evensample = [int(ceil(i * num_total_relations / num_select_relations)) for i in range(num_select_relations)]\n\n            #for idx in idx_relations_randomsample:\n            for idx in idx_relations_evensample:\n                act_relation = input[:, self.relations_scales[scaleID][idx], :]\n                act_relation = act_relation.view(act_relation.size(0), self.scales[scaleID] * self.img_feature_dim)\n                act_relation = self.fc_fusion_scales[scaleID](act_relation)\n                act_relation = act_relation.unsqueeze(1)  # add one dimension for the later concatenation\n                act_relation_all += act_relation\n\n            act_all = torch.cat((act_all, act_relation_all), 1)\n        return act_all\n\n    def return_relationset(self, num_frames, num_frames_relation):\n        import itertools\n        return list(itertools.combinations([i for i in range(num_frames)], num_frames_relation))\n"""
dataset.py,5,"b'import torch.utils.data as data\n\nimport os\nimport os.path\nimport numpy as np\nfrom numpy.random import randint\nimport torch\n\nfrom colorama import init\nfrom colorama import Fore, Back, Style\n\ninit(autoreset=True)\n\nclass VideoRecord(object):\n    def __init__(self, row):\n        self._data = row\n\n    @property\n    def path(self):\n        return self._data[0]\n\n    @property\n    def num_frames(self):\n        return int(self._data[1])\n\n    @property\n    def label(self):\n        return int(self._data[2])\n\n\nclass TSNDataSet(data.Dataset):\n    def __init__(self, root_path, list_file, num_dataload,\n                 num_segments=3, new_length=1, modality=\'RGB\',\n                 image_tmpl=\'img_{:05d}.t7\', transform=None,\n                 force_grayscale=False, random_shift=True, test_mode=False):\n\n        self.root_path = root_path\n        self.list_file = list_file\n        self.num_segments = num_segments\n        self.new_length = new_length\n        self.modality = modality\n        self.image_tmpl = image_tmpl\n        self.transform = transform\n        self.random_shift = random_shift\n        self.test_mode = test_mode\n        self.num_dataload = num_dataload\n\n        if self.modality == \'RGBDiff\' or self.modality == \'RGBDiff2\' or self.modality == \'RGBDiffplus\':\n            self.new_length += 1 # Diff needs one more image to calculate diff\n\n        self._parse_list() # read all the video files\n\n    def _load_feature(self, directory, idx):\n        if self.modality == \'RGB\' or self.modality == \'RGBDiff\' or self.modality == \'RGBDiff2\' or self.modality == \'RGBDiffplus\':\n            feat_path = os.path.join(directory, self.image_tmpl.format(idx))\n            try:\n                feat = [torch.load(feat_path)]\n            except:\n                print(Back.RED + feat_path)\n            return feat\n\n        elif self.modality == \'Flow\':\n            x_feat = torch.load(os.path.join(directory, self.image_tmpl.format(\'x\', idx)))\n            y_feat = torch.load(os.path.join(directory, self.image_tmpl.format(\'y\', idx)))\n\n            return [x_feat, y_feat]\n\n\n    def _parse_list(self):\n        self.video_list = [VideoRecord(x.strip().split(\' \')) for x in open(self.list_file)]\n        # repeat the list if the length is less than num_dataload (especially for target data)\n        n_repeat = self.num_dataload//len(self.video_list)\n        n_left = self.num_dataload%len(self.video_list)\n        self.video_list = self.video_list*n_repeat + self.video_list[:n_left]\n\n    def _sample_indices(self, record):\n        """"""\n\n        :param record: VideoRecord\n        :return: list\n        """"""\n        #np.random.seed(1)\n        average_duration = (record.num_frames - self.new_length + 1) // self.num_segments\n        if average_duration > 0:\n            offsets = np.multiply(list(range(self.num_segments)), average_duration) + randint(average_duration, size=self.num_segments)\n        elif record.num_frames > self.num_segments:\n            offsets = np.sort(randint(record.num_frames - self.new_length + 1, size=self.num_segments))\n        else:\n            offsets = np.zeros((self.num_segments,))\n        return offsets + 1\n\n    def _get_val_indices(self, record):\n        num_min = self.num_segments + self.new_length - 1\n        num_select = record.num_frames - self.new_length + 1\n\n        if record.num_frames >= num_min:\n            tick = float(num_select) / float(self.num_segments)\n            offsets = np.array([int(tick / 2.0 + tick * float(x)) for x in range(self.num_segments)])\n        else:\n            offsets = np.zeros((self.num_segments,))\n        return offsets + 1\n\n    def _get_test_indices(self, record):\n        num_min = self.num_segments + self.new_length - 1\n        num_select = record.num_frames - self.new_length + 1\n\n        if record.num_frames >= num_min:\n            tick = float(num_select) / float(self.num_segments)\n            offsets = np.array([int(tick / 2.0 + tick * float(x)) for x in range(self.num_segments)]) # pick the central frame in each segment\n        else: # the video clip is too short --> duplicate the last frame\n            id_select = np.array([x for x in range(num_select)])\n            # expand to the length of self.num_segments with the last element\n            id_expand = np.ones(self.num_segments-num_select,dtype=int)*id_select[id_select[0]-1]\n            offsets = np.append(id_select, id_expand)\n\n        return offsets + 1\n\n    def __getitem__(self, index):\n        record = self.video_list[index]\n\n        if not self.test_mode:\n            segment_indices = self._sample_indices(record) if self.random_shift else self._get_val_indices(record)\n        else:\n            segment_indices = self._get_test_indices(record)\n\n        return self.get(record, segment_indices)\n\n    def get(self, record, indices):\n\n        frames = list()\n\n        for seg_ind in indices:\n            p = int(seg_ind)\n            for i in range(self.new_length):\n                seg_feats = self._load_feature(record.path, p)\n                frames.extend(seg_feats)\n\n                if p < record.num_frames:\n                    p += 1\n\n        # process_data = self.transform(frames)\n        process_data = torch.stack(frames)\n\n        return process_data, record.label\n\n    def __len__(self):\n        return len(self.video_list)\n'"
loss.py,12,"b""# list all the additional loss functions\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n################## entropy loss (continuous target) #####################\ndef cross_entropy_soft(pred):\n    softmax = nn.Softmax(dim=1)\n    logsoftmax = nn.LogSoftmax(dim=1)\n    loss = torch.mean(torch.sum(-softmax(pred) * logsoftmax(pred), 1))\n    return loss\n\n################## attentive entropy loss (source + target) #####################\ndef attentive_entropy(pred, pred_domain):\n    softmax = nn.Softmax(dim=1)\n    logsoftmax = nn.LogSoftmax(dim=1)\n\n    # attention weight\n    entropy = torch.sum(-softmax(pred_domain) * logsoftmax(pred_domain), 1)\n    weights = 1 + entropy\n\n    # attentive entropy\n    loss = torch.mean(weights * torch.sum(-softmax(pred) * logsoftmax(pred), 1))\n    return loss\n\n################## ensemble-based loss #####################\n# discrepancy loss used in MCD (CVPR 18)\ndef dis_MCD(out1, out2):\n    return torch.mean(torch.abs(F.softmax(out1,dim=1) - F.softmax(out2, dim=1)))\n\n################## MMD-based loss #####################\ndef mmd_linear(f_of_X, f_of_Y):\n    # Consider linear time MMD with a linear kernel:\n    # K(f(x), f(y)) = f(x)^Tf(y)\n    # h(z_i, z_j) = k(x_i, x_j) + k(y_i, y_j) - k(x_i, y_j) - k(x_j, y_i)\n    #             = [f(x_i) - f(y_i)]^T[f(x_j) - f(y_j)]\n    #\n    # f_of_X: batch_size * k\n    # f_of_Y: batch_size * k\n\n    delta = f_of_X - f_of_Y\n    loss = torch.mean(torch.mm(delta, torch.transpose(delta, 0, 1)))\n    return loss\n\ndef guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n    n_samples = int(source.size()[0])+int(target.size()[0])\n    total = torch.cat([source, target], dim=0)\n    total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n    total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n    L2_distance = ((total0-total1)**2).sum(2)\n    if fix_sigma:\n        bandwidth = fix_sigma\n    else:\n        bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n    bandwidth /= kernel_mul ** (kernel_num // 2)\n    bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n    kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]\n    return sum(kernel_val)\n\ndef mmd_rbf(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None, ver=2):\n    batch_size = int(source.size()[0])\n    kernels = guassian_kernel(source, target, kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)\n\n    loss = 0\n\n    if ver==1:\n        for i in range(batch_size):\n            s1, s2 = i, (i + 1) % batch_size\n            t1, t2 = s1 + batch_size, s2 + batch_size\n            loss += kernels[s1, s2] + kernels[t1, t2]\n            loss -= kernels[s1, t2] + kernels[s2, t1]\n        loss = loss.abs_() / float(batch_size)\n    elif ver==2:\n        XX = kernels[:batch_size, :batch_size]\n        YY = kernels[batch_size:, batch_size:]\n        XY = kernels[:batch_size, batch_size:]\n        YX = kernels[batch_size:, :batch_size]\n        loss = torch.mean(XX + YY - XY - YX)\n    else:\n        raise ValueError('ver == 1 or 2')\n\n    return loss\n\ndef JAN(source_list, target_list, kernel_muls=[2.0, 2.0], kernel_nums=[2, 5], fix_sigma_list=[None, None], ver=2):\n    batch_size = int(source_list[0].size()[0])\n    layer_num = len(source_list)\n    joint_kernels = None\n    for i in range(layer_num):\n        source = source_list[i]\n        target = target_list[i]\n        kernel_mul = kernel_muls[i]\n        kernel_num = kernel_nums[i]\n        fix_sigma = fix_sigma_list[i]\n        kernels = guassian_kernel(source, target,\n                                  kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)\n        if joint_kernels is not None:\n            joint_kernels = joint_kernels * kernels\n        else:\n            joint_kernels = kernels\n\n    loss = 0\n\n    if ver==1:\n        for i in range(batch_size):\n            s1, s2 = i, (i + 1) % batch_size\n            t1, t2 = s1 + batch_size, s2 + batch_size\n            loss += joint_kernels[s1, s2] + joint_kernels[t1, t2]\n            loss -= joint_kernels[s1, t2] + joint_kernels[s2, t1]\n        loss = loss.abs_() / float(batch_size)\n    elif ver==2:\n        XX = joint_kernels[:batch_size, :batch_size]\n        YY = joint_kernels[batch_size:, batch_size:]\n        XY = joint_kernels[:batch_size, batch_size:]\n        YX = joint_kernels[batch_size:, :batch_size]\n        loss = torch.mean(XX + YY - XY - YX)\n    else:\n        raise ValueError('ver == 1 or 2')\n\n    return loss"""
main.py,72,"b'# import argparse\nimport os\nimport time\nimport shutil\nimport torch\nimport torch.nn.parallel\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom dataset import TSNDataSet\nfrom models import VideoModel\nfrom loss import *\nfrom opts import parser\nfrom utils.utils import randSelectBatch\nimport math\n\nfrom colorama import init\nfrom colorama import Fore, Back, Style\nimport numpy as np\nfrom tensorboardX import SummaryWriter\n\nnp.random.seed(1)\ntorch.manual_seed(1)\ntorch.cuda.manual_seed_all(1)\n\ninit(autoreset=True)\n\nbest_prec1 = 0\ngpu_count = torch.cuda.device_count()\n\ndef main():\n\tglobal args, best_prec1, writer\n\targs = parser.parse_args()\n\n\tprint(Fore.GREEN + \'Baseline:\', args.baseline_type)\n\tprint(Fore.GREEN + \'Frame aggregation method:\', args.frame_aggregation)\n\n\tprint(Fore.GREEN + \'target data usage:\', args.use_target)\n\tif args.use_target == \'none\':\n\t\tprint(Fore.GREEN + \'no Domain Adaptation\')\n\telse:\n\t\tif args.dis_DA != \'none\':\n\t\t\tprint(Fore.GREEN + \'Apply the discrepancy-based Domain Adaptation approach:\', args.dis_DA)\n\t\t\tif len(args.place_dis) != args.add_fc + 2:\n\t\t\t\traise ValueError(Back.RED + \'len(place_dis) should be equal to add_fc + 2\')\n\n\t\tif args.adv_DA != \'none\':\n\t\t\tprint(Fore.GREEN + \'Apply the adversarial-based Domain Adaptation approach:\', args.adv_DA)\n\n\t\tif args.use_bn != \'none\':\n\t\t\tprint(Fore.GREEN + \'Apply the adaptive normalization approach:\', args.use_bn)\n\n\t# determine the categories\n\tclass_names = [line.strip().split(\' \', 1)[1] for line in open(args.class_file)]\n\tnum_class = len(class_names)\n\n\t#=== check the folder existence ===#\n\tpath_exp = args.exp_path + args.modality + \'/\'\n\tif not os.path.isdir(path_exp):\n\t\tos.makedirs(path_exp)\n\n\tif args.tensorboard:\n\t\twriter = SummaryWriter(path_exp + \'/tensorboard\')  # for tensorboardX\n\n\t#=== initialize the model ===#\n\tprint(Fore.CYAN + \'preparing the model......\')\n\tmodel = VideoModel(num_class, args.baseline_type, args.frame_aggregation, args.modality,\n\t\t\t\ttrain_segments=args.num_segments, val_segments=args.val_segments, \n\t\t\t\tbase_model=args.arch, path_pretrained=args.pretrained,\n\t\t\t\tadd_fc=args.add_fc, fc_dim = args.fc_dim,\n\t\t\t\tdropout_i=args.dropout_i, dropout_v=args.dropout_v, partial_bn=not args.no_partialbn,\n\t\t\t\tuse_bn=args.use_bn if args.use_target != \'none\' else \'none\', ens_DA=args.ens_DA if args.use_target != \'none\' else \'none\',\n\t\t\t\tn_rnn=args.n_rnn, rnn_cell=args.rnn_cell, n_directions=args.n_directions, n_ts=args.n_ts,\n\t\t\t\tuse_attn=args.use_attn, n_attn=args.n_attn, use_attn_frame=args.use_attn_frame,\n\t\t\t\tverbose=args.verbose, share_params=args.share_params)\n\n\tmodel = torch.nn.DataParallel(model, args.gpus).cuda()\n\n\tif args.optimizer == \'SGD\':\n\t\tprint(Fore.YELLOW + \'using SGD\')\n\t\toptimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)\n\telif args.optimizer == \'Adam\':\n\t\tprint(Fore.YELLOW + \'using Adam\')\n\t\toptimizer = torch.optim.Adam(model.parameters(), args.lr, weight_decay=args.weight_decay)\n\telse:\n\t\tprint(Back.RED + \'optimizer not support or specified!!!\')\n\t\texit()\n\n\t#=== check point ===#\n\tstart_epoch = 1\n\tprint(Fore.CYAN + \'checking the checkpoint......\')\n\tif args.resume:\n\t\tif os.path.isfile(args.resume):\n\t\t\tcheckpoint = torch.load(args.resume)\n\t\t\tstart_epoch = checkpoint[\'epoch\'] + 1\n\t\t\tbest_prec1 = checkpoint[\'best_prec1\']\n\t\t\tmodel.load_state_dict(checkpoint[\'state_dict\'])\n\t\t\tprint((""=> loaded checkpoint \'{}\' (epoch {})""\n\t\t\t\t  .format(args.resume, checkpoint[\'epoch\'])))\n\t\t\tif args.resume_hp:\n\t\t\t\tprint(""=> loaded checkpoint hyper-parameters"")\n\t\t\t\toptimizer.load_state_dict(checkpoint[\'optimizer\'])\n\t\telse:\n\t\t\tprint(Back.RED + ""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n\tcudnn.benchmark = True\n\n\t#--- open log files ---#\n\tif not args.evaluate:\n\t\tif args.resume:\n\t\t\ttrain_file = open(path_exp + \'train.log\', \'a\')\n\t\t\ttrain_short_file = open(path_exp + \'train_short.log\', \'a\')\n\t\t\tval_file = open(path_exp + \'val.log\', \'a\')\n\t\t\tval_short_file = open(path_exp + \'val_short.log\', \'a\')\n\t\t\ttrain_file.write(\'========== start: \' + str(start_epoch) + \'\\n\')  # separation line\n\t\t\ttrain_short_file.write(\'========== start: \' + str(start_epoch) + \'\\n\')\n\t\t\tval_file.write(\'========== start: \' + str(start_epoch) + \'\\n\')\n\t\t\tval_short_file.write(\'========== start: \' + str(start_epoch) + \'\\n\')\n\t\telse:\n\t\t\ttrain_short_file = open(path_exp + \'train_short.log\', \'w\')\n\t\t\tval_short_file = open(path_exp + \'val_short.log\', \'w\')\n\t\t\ttrain_file = open(path_exp + \'train.log\', \'w\')\n\t\t\tval_file = open(path_exp + \'val.log\', \'w\')\n\n\t\tval_best_file = open(args.save_best_log, \'a\')\n\n\telse:\n\t\ttest_short_file = open(path_exp + \'test_short.log\', \'w\')\n\t\ttest_file = open(path_exp + \'test.log\', \'w\')\n\n\t#=== Data loading ===#\n\tprint(Fore.CYAN + \'loading data......\')\n\n\tif args.use_opencv:\n\t\tprint(""use opencv functions"")\n\n\tif args.modality == \'RGB\':\n\t\tdata_length = 1\n\telif args.modality in [\'Flow\', \'RGBDiff\', \'RGBDiff2\', \'RGBDiffplus\']:\n\t\tdata_length = 5\n\n\t# calculate the number of videos to load for training in each list ==> make sure the iteration # of source & target are same\n\tnum_source = sum(1 for i in open(args.train_source_list))\n\tnum_target = sum(1 for i in open(args.train_target_list))\n\tnum_val = sum(1 for i in open(args.val_list))\n\n\tnum_iter_source = num_source / args.batch_size[0]\n\tnum_iter_target = num_target / args.batch_size[1]\n\tnum_max_iter = max(num_iter_source, num_iter_target)\n\tnum_source_train = round(num_max_iter*args.batch_size[0]) if args.copy_list[0] == \'Y\' else num_source\n\tnum_target_train = round(num_max_iter*args.batch_size[1]) if args.copy_list[1] == \'Y\' else num_target\n\n\t# calculate the weight for each class\n\tclass_id_list = [int(line.strip().split(\' \')[2]) for line in open(args.train_source_list)]\n\tclass_id, class_data_counts = np.unique(np.array(class_id_list), return_counts=True)\n\tclass_freq = (class_data_counts / class_data_counts.sum()).tolist()\n\n\tweight_source_class = torch.ones(num_class).cuda()\n\tweight_domain_loss = torch.Tensor([1, 1]).cuda()\n\n\tif args.weighted_class_loss == \'Y\':\n\t\tweight_source_class = 1 / torch.Tensor(class_freq).cuda()\n\n\tif args.weighted_class_loss_DA == \'Y\':\n\t\tweight_domain_loss = torch.Tensor([1/num_source_train, 1/num_target_train]).cuda()\n\n\t# data loading (always need to load the testing data)\n\tval_segments = args.val_segments if args.val_segments > 0 else args.num_segments\n\tval_set = TSNDataSet("""", args.val_list, num_dataload=num_val, num_segments=val_segments,\n\t\t\t\t\t\t new_length=data_length, modality=args.modality,\n\t\t\t\t\t\t image_tmpl=""img_{:05d}.t7"" if args.modality in [""RGB"", ""RGBDiff"", ""RGBDiff2"",\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  ""RGBDiffplus""] else args.flow_prefix + ""{}_{:05d}.t7"",\n\t\t\t\t\t\t random_shift=False,\n\t\t\t\t\t\t test_mode=True,\n\t\t\t\t\t\t )\n\tval_loader = torch.utils.data.DataLoader(val_set, batch_size=args.batch_size[2], shuffle=False,\n\t\t\t\t\t\t\t\t\t\t\t num_workers=args.workers, pin_memory=True)\n\n\tif not args.evaluate:\n\t\tsource_set = TSNDataSet("""", args.train_source_list, num_dataload=num_source_train, num_segments=args.num_segments,\n\t\t\t\t\t\t\t\tnew_length=data_length, modality=args.modality,\n\t\t\t\t\t\t\t\timage_tmpl=""img_{:05d}.t7"" if args.modality in [""RGB"", ""RGBDiff"", ""RGBDiff2"", ""RGBDiffplus""] else args.flow_prefix+""{}_{:05d}.t7"",\n\t\t\t\t\t\t\t\trandom_shift=False,\n\t\t\t\t\t\t\t\ttest_mode=True,\n\t\t\t\t\t\t\t\t)\n\n\t\tsource_sampler = torch.utils.data.sampler.RandomSampler(source_set)\n\t\tsource_loader = torch.utils.data.DataLoader(source_set, batch_size=args.batch_size[0], shuffle=False, sampler=source_sampler, num_workers=args.workers, pin_memory=True)\n\n\t\ttarget_set = TSNDataSet("""", args.train_target_list, num_dataload=num_target_train, num_segments=args.num_segments,\n\t\t\t\t\t\t\t\tnew_length=data_length, modality=args.modality,\n\t\t\t\t\t\t\t\timage_tmpl=""img_{:05d}.t7"" if args.modality in [""RGB"", ""RGBDiff"", ""RGBDiff2"", ""RGBDiffplus""] else args.flow_prefix + ""{}_{:05d}.t7"",\n\t\t\t\t\t\t\t\trandom_shift=False,\n\t\t\t\t\t\t\t\ttest_mode=True,\n\t\t\t\t\t\t\t\t)\n\n\t\ttarget_sampler = torch.utils.data.sampler.RandomSampler(target_set)\n\t\ttarget_loader = torch.utils.data.DataLoader(target_set, batch_size=args.batch_size[1], shuffle=False, sampler=target_sampler, num_workers=args.workers, pin_memory=True)\n\n\t# --- Optimizer ---#\n\t# define loss function (criterion) and optimizer\n\tif args.loss_type == \'nll\':\n\t\tcriterion = torch.nn.CrossEntropyLoss(weight=weight_source_class).cuda()\n\t\tcriterion_domain = torch.nn.CrossEntropyLoss(weight=weight_domain_loss).cuda()\n\telse:\n\t\traise ValueError(""Unknown loss type"")\n\n\tif args.evaluate:\n\t\tprint(Fore.CYAN + \'evaluation only......\')\n\t\tprec1 = validate(val_loader, model, criterion, num_class, 0, test_file)\n\t\ttest_short_file.write(\'%.3f\\n\' % prec1)\n\t\treturn\n\n\t#=== Training ===#\n\tstart_train = time.time()\n\tprint(Fore.CYAN + \'start training......\')\n\tbeta = args.beta\n\tgamma = args.gamma\n\tmu = args.mu\n\tloss_c_current = 999 # random large number\n\tloss_c_previous = 999 # random large number\n\n\tattn_source_all = torch.Tensor()\n\tattn_target_all = torch.Tensor()\n\n\tfor epoch in range(start_epoch, args.epochs+1):\n\n\t\t## schedule for parameters\n\t\talpha = 2 / (1 + math.exp(-1 * (epoch) / args.epochs)) - 1 if args.alpha < 0 else args.alpha\n\n\t\t## schedule for learning rate\n\t\tif args.lr_adaptive == \'loss\':\n\t\t\tadjust_learning_rate_loss(optimizer, args.lr_decay, loss_c_current, loss_c_previous, \'>\')\n\t\telif args.lr_adaptive == \'none\' and epoch in args.lr_steps:\n\t\t\tadjust_learning_rate(optimizer, args.lr_decay)\n\n\t\t# train for one epoch\n\t\tloss_c, attn_epoch_source, attn_epoch_target = train(num_class, source_loader, target_loader, model, criterion, criterion_domain, optimizer, epoch, train_file, train_short_file, alpha, beta, gamma, mu)\n\t\t\n\t\tif args.save_attention >= 0:\n\t\t\tattn_source_all = torch.cat((attn_source_all, attn_epoch_source.unsqueeze(0)))  # save the attention values\n\t\t\tattn_target_all = torch.cat((attn_target_all, attn_epoch_target.unsqueeze(0)))  # save the attention values\n\n\t\t# update the recorded loss_c\n\t\tloss_c_previous = loss_c_current\n\t\tloss_c_current = loss_c\n\n\t\t# evaluate on validation set\n\t\tif epoch % args.eval_freq == 0 or epoch == args.epochs:\n\t\t\tprec1 = validate(val_loader, model, criterion, num_class, epoch, val_file)\n\n\t\t\t# remember best prec@1 and save checkpoint\n\t\t\tis_best = prec1 > best_prec1\n\t\t\tline_update = \' ==> updating the best accuracy\' if is_best else \'\'\n\t\t\tline_best = ""Best score {} vs current score {}"".format(best_prec1, prec1) + line_update\n\t\t\tprint(Fore.YELLOW + line_best)\n\t\t\tval_short_file.write(\'%.3f\\n\' % prec1)\n\n\t\t\tbest_prec1 = max(prec1, best_prec1)\n\n\t\t\tif args.tensorboard:\n\t\t\t\twriter.add_text(\'Best_Accuracy\', str(best_prec1), epoch)\n\n\t\t\tif args.save_model:\n\t\t\t\tsave_checkpoint({\n\t\t\t\t\t\'epoch\': epoch,\n\t\t\t\t\t\'arch\': args.arch,\n\t\t\t\t\t\'state_dict\': model.state_dict(),\n\t\t\t\t\t\'optimizer\' : optimizer.state_dict(),\n\t\t\t\t\t\'best_prec1\': best_prec1,\n\t\t\t\t\t\'prec1\': prec1,\n\t\t\t\t}, is_best, path_exp)\n\t\n\tend_train = time.time()\n\tprint(Fore.CYAN + \'total training time:\', end_train - start_train)\n\tval_best_file.write(\'%.3f\\n\' % best_prec1)\n\n\t# --- write the total time to log files ---#\n\tline_time = \'total time: {:.3f} \'.format(end_train - start_train)\n\tif not args.evaluate:\n\t\ttrain_file.write(line_time)\n\t\ttrain_short_file.write(line_time)\n\t\tval_file.write(line_time)\n\t\tval_short_file.write(line_time)\n\telse:\n\t\ttest_file.write(line_time)\n\t\ttest_short_file.write(line_time)\n\n\t#--- close log files ---#\n\tif not args.evaluate:\n\t\ttrain_file.close()\n\t\ttrain_short_file.close()\n\t\tval_file.close()\n\t\tval_short_file.close()\n\telse:\n\t\ttest_file.close()\n\t\ttest_short_file.close()\n\n\tif args.tensorboard:\n\t\twriter.close()\n\n\tif args.save_attention >= 0:\n\t\tnp.savetxt(\'attn_source_\' + str(args.save_attention) + \'.log\', attn_source_all.cpu().detach().numpy(), fmt=""%s"")\n\t\tnp.savetxt(\'attn_target_\' + str(args.save_attention) + \'.log\', attn_target_all.cpu().detach().numpy(), fmt=""%s"")\n\n\ndef train(num_class, source_loader, target_loader, model, criterion, criterion_domain, optimizer, epoch, log, log_short, alpha, beta, gamma, mu):\n\tbatch_time = AverageMeter()\n\tdata_time = AverageMeter()\n\tlosses_a = AverageMeter()  # adversarial loss\n\tlosses_d = AverageMeter()  # discrepancy loss\n\tlosses_e = AverageMeter()  # entropy loss\n\tlosses_s = AverageMeter()  # ensemble loss\n\tlosses_c = AverageMeter()  # classification loss\n\tlosses = AverageMeter()\n\ttop1 = AverageMeter()\n\ttop5 = AverageMeter()\n\n\tif args.no_partialbn:\n\t\tmodel.module.partialBN(False)\n\telse:\n\t\tmodel.module.partialBN(True)\n\n\t# switch to train mode\n\tmodel.train()\n\n\tend = time.time()\n\tdata_loader = enumerate(zip(source_loader, target_loader))\n\n\t# step info\n\tstart_steps = epoch * len(source_loader)\n\ttotal_steps = args.epochs * len(source_loader)\n\n\t# initialize the embedding\n\tif args.tensorboard:\n\t\tfeat_source_display = None\n\t\tlabel_source_display = None\n\t\tlabel_source_domain_display = None\n\n\t\tfeat_target_display = None\n\t\tlabel_target_display = None\n\t\tlabel_target_domain_display = None\n\n\tattn_epoch_source = torch.Tensor()\n\tattn_epoch_target = torch.Tensor()\n\tfor i, ((source_data, source_label),(target_data, target_label)) in data_loader:\n\t\t# setup hyperparameters\n\t\tp = float(i + start_steps) / total_steps\n\t\tbeta_dann = 2. / (1. + np.exp(-10 * p)) - 1\n\t\tbeta = [beta_dann if beta[i] < 0 else beta[i] for i in range(len(beta))] # replace the default beta if value < 0\n\n\t\tsource_size_ori = source_data.size()  # original shape\n\t\ttarget_size_ori = target_data.size()  # original shape\n\t\tbatch_source_ori = source_size_ori[0]\n\t\tbatch_target_ori = target_size_ori[0]\n\t\t# add dummy tensors to keep the same batch size for each epoch (for the last epoch)\n\t\tif batch_source_ori < args.batch_size[0]:\n\t\t\tsource_data_dummy = torch.zeros(args.batch_size[0] - batch_source_ori, source_size_ori[1], source_size_ori[2])\n\t\t\tsource_data = torch.cat((source_data, source_data_dummy))\n\t\tif batch_target_ori < args.batch_size[1]:\n\t\t\ttarget_data_dummy = torch.zeros(args.batch_size[1] - batch_target_ori, target_size_ori[1], target_size_ori[2])\n\t\t\ttarget_data = torch.cat((target_data, target_data_dummy))\n\n\t\t# add dummy tensors to make sure batch size can be divided by gpu #\n\t\tif source_data.size(0) % gpu_count != 0:\n\t\t\tsource_data_dummy = torch.zeros(gpu_count - source_data.size(0) % gpu_count, source_data.size(1), source_data.size(2))\n\t\t\tsource_data = torch.cat((source_data, source_data_dummy))\n\t\tif target_data.size(0) % gpu_count != 0:\n\t\t\ttarget_data_dummy = torch.zeros(gpu_count - target_data.size(0) % gpu_count, target_data.size(1), target_data.size(2))\n\t\t\ttarget_data = torch.cat((target_data, target_data_dummy))\n\n\t\t# measure data loading time\n\t\tdata_time.update(time.time() - end)\n\n\t\tsource_label = source_label.cuda(non_blocking=True) # pytorch 0.4.X\n\t\ttarget_label = target_label.cuda(non_blocking=True) # pytorch 0.4.X\n\n\t\tif args.baseline_type == \'frame\':\n\t\t\tsource_label_frame = source_label.unsqueeze(1).repeat(1,args.num_segments).view(-1) # expand the size for all the frames\n\t\t\ttarget_label_frame = target_label.unsqueeze(1).repeat(1, args.num_segments).view(-1)\n\n\t\tlabel_source = source_label_frame if args.baseline_type == \'frame\' else source_label  # determine the label for calculating the loss function\n\t\tlabel_target = target_label_frame if args.baseline_type == \'frame\' else target_label\n\n\t\t#====== pre-train source data ======#\n\t\tif args.pretrain_source:\n\t\t\t#------ forward pass data again ------#\n\t\t\t_, out_source, out_source_2, _, _, _, _, _, _, _ = model(source_data, target_data, beta, mu, is_train=True, reverse=False)\n\n\t\t\t# ignore dummy tensors\n\t\t\tout_source = out_source[:batch_source_ori]\n\t\t\tout_source_2 = out_source_2[:batch_source_ori]\n\n\t\t\t#------ calculate the loss function ------#\n\t\t\t# 1. calculate the classification loss\n\t\t\tout = out_source\n\t\t\tlabel = label_source\n\n\t\t\tloss = criterion(out, label)\n\t\t\tif args.ens_DA == \'MCD\' and args.use_target != \'none\':\n\t\t\t\tloss += criterion(out_source_2, label)\n\n\t\t\t# compute gradient and do SGD step\n\t\t\toptimizer.zero_grad()\n\t\t\tloss.backward()\n\n\t\t\tif args.clip_gradient is not None:\n\t\t\t\ttotal_norm = clip_grad_norm_(model.parameters(), args.clip_gradient)\n\t\t\t\tif total_norm > args.clip_gradient and args.verbose:\n\t\t\t\t\tprint(""clipping gradient: {} with coef {}"".format(total_norm, args.clip_gradient / total_norm))\n\n\t\t\toptimizer.step()\n\n\n\t\t#====== forward pass data ======#\n\t\tattn_source, out_source, out_source_2, pred_domain_source, feat_source, attn_target, out_target, out_target_2, pred_domain_target, feat_target = model(source_data, target_data, beta, mu, is_train=True, reverse=False)\n\n\t\t# ignore dummy tensors\n\t\tattn_source, out_source, out_source_2, pred_domain_source, feat_source = removeDummy(attn_source, out_source, out_source_2, pred_domain_source, feat_source, batch_source_ori)\n\t\tattn_target, out_target, out_target_2, pred_domain_target, feat_target = removeDummy(attn_target, out_target, out_target_2, pred_domain_target, feat_target, batch_target_ori)\n\n\t\tif args.pred_normalize == \'Y\': # use the uncertainly method (in contruction...)\n\t\t\tout_source = out_source / out_source.var().log()\n\t\t\tout_target = out_target / out_target.var().log()\n\n\t\t# store the embedding\n\t\tif args.tensorboard:\n\t\t\tfeat_source_display = feat_source[1] if i==0 else torch.cat((feat_source_display, feat_source[1]), 0)\n\t\t\tlabel_source_display = label_source if i==0 else torch.cat((label_source_display, label_source), 0)\n\t\t\tlabel_source_domain_display = torch.zeros(label_source.size(0)) if i==0 else torch.cat((label_source_domain_display, torch.zeros(label_source.size(0))), 0)\n\t\t\tfeat_target_display = feat_target[1] if i==0 else torch.cat((feat_target_display, feat_target[1]), 0)\n\t\t\tlabel_target_display = label_target if i==0 else torch.cat((label_target_display, label_target), 0)\n\t\t\tlabel_target_domain_display = torch.ones(label_target.size(0)) if i==0 else torch.cat((label_target_domain_display, torch.ones(label_target.size(0))), 0)\n\n\t\t#====== calculate the loss function ======#\n\t\t# 1. calculate the classification loss\n\t\tout = out_source\n\t\tlabel = label_source\n\n\t\tif args.use_target == \'Sv\':\n\t\t\tout = torch.cat((out, out_target))\n\t\t\tlabel = torch.cat((label, label_target))\n\n\t\tloss_classification = criterion(out, label)\n\t\tif args.ens_DA == \'MCD\' and args.use_target != \'none\':\n\t\t\tloss_classification += criterion(out_source_2, label)\n\n\t\tlosses_c.update(loss_classification.item(), out_source.size(0)) # pytorch 0.4.X\n\t\tloss = loss_classification\n\n\t\t# 2. calculate the loss for DA\n\t\t# (I) discrepancy-based approach: discrepancy loss\n\t\tif args.dis_DA != \'none\' and args.use_target != \'none\':\n\t\t\tloss_discrepancy = 0\n\n\t\t\tkernel_muls = [2.0]*2\n\t\t\tkernel_nums = [2, 5]\n\t\t\tfix_sigma_list = [None]*2\n\n\t\t\tif args.dis_DA == \'JAN\':\n\t\t\t\t# ignore the features from shared layers\n\t\t\t\tfeat_source_sel = feat_source[:-args.add_fc]\n\t\t\t\tfeat_target_sel = feat_target[:-args.add_fc]\n\n\t\t\t\tsize_loss = min(feat_source_sel[0].size(0), feat_target_sel[0].size(0))  # choose the smaller number\n\t\t\t\tfeat_source_sel = [feat[:size_loss] for feat in feat_source_sel]\n\t\t\t\tfeat_target_sel = [feat[:size_loss] for feat in feat_target_sel]\n\n\t\t\t\tloss_discrepancy += JAN(feat_source_sel, feat_target_sel, kernel_muls=kernel_muls, kernel_nums=kernel_nums, fix_sigma_list=fix_sigma_list, ver=2)\n\n\t\t\telse:\n\t\t\t\t# extend the parameter list for shared layers\n\t\t\t\tkernel_muls.extend([kernel_muls[-1]]*args.add_fc)\n\t\t\t\tkernel_nums.extend([kernel_nums[-1]]*args.add_fc)\n\t\t\t\tfix_sigma_list.extend([fix_sigma_list[-1]]*args.add_fc)\n\n\t\t\t\tfor l in range(0, args.add_fc + 2):  # loss from all the features (+2 because of frame-aggregation layer + final fc layer)\n\t\t\t\t\tif args.place_dis[l] == \'Y\':\n\t\t\t\t\t\t# select the data for calculating the loss (make sure source # == target #)\n\t\t\t\t\t\tsize_loss = min(feat_source[l].size(0), feat_target[l].size(0)) # choose the smaller number\n\t\t\t\t\t\t# select\n\t\t\t\t\t\tfeat_source_sel = feat_source[l][:size_loss]\n\t\t\t\t\t\tfeat_target_sel = feat_target[l][:size_loss]\n\n\t\t\t\t\t\t# break into multiple batches to avoid ""out of memory"" issue\n\t\t\t\t\t\tsize_batch = min(256,feat_source_sel.size(0))\n\t\t\t\t\t\tfeat_source_sel = feat_source_sel.view((-1,size_batch) + feat_source_sel.size()[1:])\n\t\t\t\t\t\tfeat_target_sel = feat_target_sel.view((-1,size_batch) + feat_target_sel.size()[1:])\n\n\t\t\t\t\t\tif args.dis_DA == \'CORAL\':\n\t\t\t\t\t\t\tlosses_coral = [CORAL(feat_source_sel[t], feat_target_sel[t]) for t in range(feat_source_sel.size(0))]\n\t\t\t\t\t\t\tloss_coral = sum(losses_coral)/len(losses_coral)\n\t\t\t\t\t\t\tloss_discrepancy += loss_coral\n\t\t\t\t\t\telif args.dis_DA == \'DAN\':\n\t\t\t\t\t\t\tlosses_mmd = [mmd_rbf(feat_source_sel[t], feat_target_sel[t], kernel_mul=kernel_muls[l], kernel_num=kernel_nums[l], fix_sigma=fix_sigma_list[l], ver=2) for t in range(feat_source_sel.size(0))]\n\t\t\t\t\t\t\tloss_mmd = sum(losses_mmd) / len(losses_mmd)\n\n\t\t\t\t\t\t\tloss_discrepancy += loss_mmd\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\traise NameError(\'not in dis_DA!!!\')\n\n\t\t\tlosses_d.update(loss_discrepancy.item(), feat_source[0].size(0))\n\t\t\tloss += alpha * loss_discrepancy\n\n\t\t# (II) adversarial discriminative model: adversarial loss\n\t\tif args.adv_DA != \'none\' and args.use_target != \'none\':\n\t\t\tloss_adversarial = 0\n\t\t\tpred_domain_all = []\n\t\t\tpred_domain_target_all = []\n\n\t\t\tfor l in range(len(args.place_adv)):\n\t\t\t\tif args.place_adv[l] == \'Y\':\n\n\t\t\t\t\t# reshape the features (e.g. 128x5x2 --> 640x2)\n\t\t\t\t\tpred_domain_source_single = pred_domain_source[l].view(-1, pred_domain_source[l].size()[-1])\n\t\t\t\t\tpred_domain_target_single = pred_domain_target[l].view(-1, pred_domain_target[l].size()[-1])\n\n\t\t\t\t\t# prepare domain labels\n\t\t\t\t\tsource_domain_label = torch.zeros(pred_domain_source_single.size(0)).long()\n\t\t\t\t\ttarget_domain_label = torch.ones(pred_domain_target_single.size(0)).long()\n\t\t\t\t\tdomain_label = torch.cat((source_domain_label,target_domain_label),0)\n\n\t\t\t\t\tdomain_label = domain_label.cuda(non_blocking=True)\n\n\t\t\t\t\tpred_domain = torch.cat((pred_domain_source_single, pred_domain_target_single),0)\n\t\t\t\t\tpred_domain_all.append(pred_domain)\n\t\t\t\t\tpred_domain_target_all.append(pred_domain_target_single)\n\n\t\t\t\t\tif args.pred_normalize == \'Y\':  # use the uncertainly method (in construction......)\n\t\t\t\t\t\tpred_domain = pred_domain / pred_domain.var().log()\n\t\t\t\t\tloss_adversarial_single = criterion_domain(pred_domain, domain_label)\n\n\t\t\t\t\tloss_adversarial += loss_adversarial_single\n\n\t\t\tlosses_a.update(loss_adversarial.item(), pred_domain.size(0))\n\t\t\tloss += loss_adversarial\n\n\t\t# (III) other loss\n\t\t# 1. entropy loss for target data\n\t\tif args.add_loss_DA == \'target_entropy\' and args.use_target != \'none\':\n\t\t\tloss_entropy = cross_entropy_soft(out_target)\n\t\t\tlosses_e.update(loss_entropy.item(), out_target.size(0))\n\t\t\tloss += gamma * loss_entropy\n\n\t\t# 2. discrepancy loss for MCD (CVPR 18)\n\t\tif args.ens_DA == \'MCD\' and args.use_target != \'none\':\n\t\t\t_, _, _, _, _, attn_target, out_target, out_target_2, pred_domain_target, feat_target = model(source_data, target_data, beta, mu, is_train=True, reverse=True)\n\n\t\t\t# ignore dummy tensors\n\t\t\t_, out_target, out_target_2, _, _ = removeDummy(attn_target, out_target, out_target_2, pred_domain_target, feat_target, batch_target_ori)\n\n\t\t\tloss_dis = -dis_MCD(out_target, out_target_2)\n\t\t\tlosses_s.update(loss_dis.item(), out_target.size(0))\n\t\t\tloss += loss_dis\n\n\t\t# 3. attentive entropy loss\n\t\tif args.add_loss_DA == \'attentive_entropy\' and args.use_attn != \'none\' and args.use_target != \'none\':\n\t\t\tloss_entropy = attentive_entropy(torch.cat((out_source, out_target),0), pred_domain_all[1])\n\t\t\tlosses_e.update(loss_entropy.item(), out_target.size(0))\n\t\t\tloss += gamma * loss_entropy\n\n\t\t# measure accuracy and record loss\n\t\tpred = out\n\n\t\tprec1, prec5 = accuracy(pred.data, label, topk=(1, 5))\n\n\t\tlosses.update(loss.item())\n\t\ttop1.update(prec1.item(), out_source.size(0))\n\t\ttop5.update(prec5.item(), out_source.size(0))\n\n\t\t# compute gradient and do SGD step\n\t\toptimizer.zero_grad()\n\n\t\tloss.backward()\n\n\t\tif args.clip_gradient is not None:\n\t\t\ttotal_norm = clip_grad_norm_(model.parameters(), args.clip_gradient)\n\t\t\tif total_norm > args.clip_gradient and args.verbose:\n\t\t\t\tprint(""clipping gradient: {} with coef {}"".format(total_norm, args.clip_gradient / total_norm))\n\n\t\toptimizer.step()\n\n\t\t# measure elapsed time\n\t\tbatch_time.update(time.time() - end)\n\t\tend = time.time()\n\n\t\tif i % args.print_freq == 0:\n\t\t\tline = \'Train: [{0}][{1}/{2}], lr: {lr:.5f}\\t\' + \\\n\t\t\t\t   \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\' + \\\n\t\t\t\t   \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\' + \\\n\t\t\t\t   \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\' + \\\n\t\t\t\t   \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\\t\' + \\\n\t\t\t\t   \'Loss {loss.val:.4f} ({loss.avg:.4f})   loss_c {loss_c.avg:.4f}\\t\'\n\n\t\t\tif args.dis_DA != \'none\' and args.use_target != \'none\':\n\t\t\t\tline += \'alpha {alpha:.3f}  loss_d {loss_d.avg:.4f}\\t\'\n\n\t\t\tif args.adv_DA != \'none\' and args.use_target != \'none\':\n\t\t\t\tline += \'beta {beta[0]:.3f}, {beta[1]:.3f}, {beta[2]:.3f}  loss_a {loss_a.avg:.4f}\\t\'\n\n\t\t\tif args.add_loss_DA != \'none\' and args.use_target != \'none\':\n\t\t\t\tline += \'gamma {gamma:.6f}  loss_e {loss_e.avg:.4f}\\t\'\n\n\t\t\tif args.ens_DA != \'none\' and args.use_target != \'none\':\n\t\t\t\tline += \'mu {mu:.6f}  loss_s {loss_s.avg:.4f}\\t\'\n\n\t\t\tline = line.format(\n\t\t\t\tepoch, i, len(source_loader), batch_time=batch_time, data_time=data_time, alpha=alpha, beta=beta, gamma=gamma, mu=mu,\n\t\t\t\tloss=losses, loss_c=losses_c, loss_d=losses_d, loss_a=losses_a, loss_e=losses_e, loss_s=losses_s, top1=top1, top5=top5,\n\t\t\t\tlr=optimizer.param_groups[0][\'lr\'])\n\n\t\t\tif i % args.show_freq == 0:\n\t\t\t\tprint(line)\n\n\t\t\tlog.write(\'%s\\n\' % line)\n\n\t\t# adjust the learning rate for ech step (e.g. DANN)\n\t\tif args.lr_adaptive == \'dann\':\n\t\t\tadjust_learning_rate_dann(optimizer, p)\n\n\t\t# save attention values w/ the selected class\n\t\tif args.save_attention >= 0:\n\t\t\tattn_source = attn_source[source_label==args.save_attention]\n\t\t\tattn_target = attn_target[target_label==args.save_attention]\n\t\t\tattn_epoch_source = torch.cat((attn_epoch_source, attn_source.cpu()))\n\t\t\tattn_epoch_target = torch.cat((attn_epoch_target, attn_target.cpu()))\n\n\t# update the embedding every epoch\n\tif args.tensorboard:\n\t\tn_iter_train = epoch * len(source_loader) # calculate the total iteration\n\t\t# embedding\n\t\t# see source and target separately\n\t\twriter.add_embedding(feat_source_display, metadata=label_source_display.data, global_step=n_iter_train, tag=\'train_source\')\n\t\twriter.add_embedding(feat_target_display, metadata=label_target_display.data, global_step=n_iter_train, tag=\'train_target\')\n\n\t\t# mix source and target\n\t\tfeat_all_display = torch.cat((feat_source_display, feat_target_display), 0)\n\t\tlabel_all_domain_display = torch.cat((label_source_domain_display, label_target_domain_display), 0)\n\t\twriter.add_embedding(feat_all_display, metadata=label_all_domain_display.data, global_step=n_iter_train, tag=\'train_DA\')\n\n\t\t# emphazise some classes (1, 3, 11 here)\n\t\tlabel_source_1 = 1 * torch.eq(label_source_display, torch.cuda.LongTensor([1]).repeat(label_source_display.size(0))).long().cuda(non_blocking=True)\n\t\tlabel_source_3 = 2 * torch.eq(label_source_display, torch.cuda.LongTensor([3]).repeat(label_source_display.size(0))).long().cuda(non_blocking=True)\n\t\tlabel_source_11 = 3 * torch.eq(label_source_display, torch.cuda.LongTensor([11]).repeat(label_source_display.size(0))).long().cuda(non_blocking=True)\n\n\t\tlabel_target_1 = 4 * torch.eq(label_target_display, torch.cuda.LongTensor([1]).repeat(label_target_display.size(0))).long().cuda(non_blocking=True)\n\t\tlabel_target_3 = 5 * torch.eq(label_target_display, torch.cuda.LongTensor([3]).repeat(label_target_display.size(0))).long().cuda(non_blocking=True)\n\t\tlabel_target_11 = 6 * torch.eq(label_target_display, torch.cuda.LongTensor([11]).repeat(label_target_display.size(0))).long().cuda(non_blocking=True)\n\n\t\tlabel_source_display_new = label_source_1 + label_source_3 + label_source_11\n\t\tid_source_show = ~torch.eq(label_source_display_new, 0).cuda(non_blocking=True)\n\t\tlabel_source_display_new = label_source_display_new[id_source_show]\n\t\tfeat_source_display_new = feat_source_display[id_source_show]\n\n\t\tlabel_target_display_new = label_target_1 + label_target_3 + label_target_11\n\t\tid_target_show = ~torch.eq(label_target_display_new, 0).cuda(non_blocking=True)\n\t\tlabel_target_display_new = label_target_display_new[id_target_show]\n\t\tfeat_target_display_new = feat_target_display[id_target_show]\n\n\t\tfeat_all_display_new = torch.cat((feat_source_display_new, feat_target_display_new), 0)\n\t\tlabel_all_display_new = torch.cat((label_source_display_new, label_target_display_new), 0)\n\t\twriter.add_embedding(feat_all_display_new, metadata=label_all_display_new.data, global_step=n_iter_train, tag=\'train_DA_labels\')\n\n\tlog_short.write(\'%s\\n\' % line)\n\treturn losses_c.avg, attn_epoch_source.mean(0), attn_epoch_target.mean(0)\n\ndef validate(val_loader, model, criterion, num_class, epoch, log):\n\tbatch_time = AverageMeter()\n\tlosses = AverageMeter()\n\ttop1 = AverageMeter()\n\ttop5 = AverageMeter()\n\n\t# switch to evaluate mode\n\tmodel.eval()\n\n\tend = time.time()\n\n\t# initialize the embedding\n\tif args.tensorboard:\n\t\tfeat_val_display = None\n\t\tlabel_val_display = None\n\n\tfor i, (val_data, val_label) in enumerate(val_loader):\n\n\t\tval_size_ori = val_data.size()  # original shape\n\t\tbatch_val_ori = val_size_ori[0]\n\n\t\t# add dummy tensors to keep the same batch size for each epoch (for the last epoch)\n\t\tif batch_val_ori < args.batch_size[2]:\n\t\t\tval_data_dummy = torch.zeros(args.batch_size[2] - batch_val_ori, val_size_ori[1], val_size_ori[2])\n\t\t\tval_data = torch.cat((val_data, val_data_dummy))\n\n\t\t# add dummy tensors to make sure batch size can be divided by gpu #\n\t\tif val_data.size(0) % gpu_count != 0:\n\t\t\tval_data_dummy = torch.zeros(gpu_count - val_data.size(0) % gpu_count, val_data.size(1), val_data.size(2))\n\t\t\tval_data = torch.cat((val_data, val_data_dummy))\n\n\t\tval_label = val_label.cuda(non_blocking=True)\n\t\twith torch.no_grad():\n\n\t\t\tif args.baseline_type == \'frame\':\n\t\t\t\tval_label_frame = val_label.unsqueeze(1).repeat(1,args.num_segments).view(-1) # expand the size for all the frames\n\n\t\t\t# compute output\n\t\t\t_, _, _, _, _, attn_val, out_val, out_val_2, pred_domain_val, feat_val = model(val_data, val_data, [0]*len(args.beta), 0, is_train=False, reverse=False)\n\n\t\t\t# ignore dummy tensors\n\t\t\tattn_val, out_val, out_val_2, pred_domain_val, feat_val = removeDummy(attn_val, out_val, out_val_2, pred_domain_val, feat_val, batch_val_ori)\n\n\t\t\t# measure accuracy and record loss\n\t\t\tlabel = val_label_frame if args.baseline_type == \'frame\' else val_label\n\n\t\t\t# store the embedding\n\t\t\tif args.tensorboard:\n\t\t\t\tfeat_val_display = feat_val[1] if i == 0 else torch.cat((feat_val_display, feat_val[1]), 0)\n\t\t\t\tlabel_val_display = label if i == 0 else torch.cat((label_val_display, label), 0)\n\n\t\t\tpred = out_val\n\n\t\t\tif args.baseline_type == \'tsn\':\n\t\t\t\tpred = pred.view(val_label.size(0), -1, num_class).mean(dim=1) # average all the segments (needed when num_segments != val_segments)\n\n\t\t\tloss = criterion(pred, label)\n\t\t\tprec1, prec5 = accuracy(pred.data, label, topk=(1, 5))\n\n\t\t\tlosses.update(loss.item(), out_val.size(0))\n\t\t\ttop1.update(prec1.item(), out_val.size(0))\n\t\t\ttop5.update(prec5.item(), out_val.size(0))\n\n\t\t\t# measure elapsed time\n\t\t\tbatch_time.update(time.time() - end)\n\t\t\tend = time.time()\n\n\t\t\tif i % args.print_freq == 0:\n\t\t\t\tline = \'Test: [{0}][{1}/{2}]\\t\' + \\\n\t\t\t\t\t  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\' + \\\n\t\t\t\t\t  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\' + \\\n\t\t\t\t\t  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\' + \\\n\t\t\t\t\t  \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\\t\'\n\n\t\t\t\tline = line.format(\n\t\t\t\t\t   epoch, i, len(val_loader), batch_time=batch_time, loss=losses,\n\t\t\t\t\t   top1=top1, top5=top5)\n\n\t\t\t\tif i % args.show_freq == 0:\n\t\t\t\t\tprint(line)\n\n\t\t\t\tlog.write(\'%s\\n\' % line)\n\n\tif args.tensorboard:  # update the embedding every iteration\n\t\t# embedding\n\t\tn_iter_val = epoch * len(val_loader)\n\n\t\twriter.add_embedding(feat_val_display, metadata=label_val_display.data, global_step=n_iter_val, tag=\'validation\')\n\n\tprint((\'Testing Results: Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Loss {loss.avg:.5f}\'\n\t\t  .format(top1=top1, top5=top5, loss=losses)))\n\n\treturn top1.avg\n\n\ndef save_checkpoint(state, is_best, path_exp, filename=\'checkpoint.pth.tar\'):\n\n\tpath_file = path_exp + filename\n\ttorch.save(state, path_file)\n\tif is_best:\n\t\tpath_best = path_exp + \'model_best.pth.tar\'\n\t\tshutil.copyfile(path_file, path_best)\n\nclass AverageMeter(object):\n\t""""""Computes and stores the average and current value""""""\n\tdef __init__(self):\n\t\tself.reset()\n\n\tdef reset(self):\n\t\tself.val = 0\n\t\tself.avg = 0\n\t\tself.sum = 0\n\t\tself.count = 0\n\n\tdef update(self, val, n=1):\n\t\tself.val = val\n\t\tself.sum += val * n\n\t\tself.count += n\n\t\tself.avg = self.sum / self.count\n\ndef adjust_learning_rate(optimizer, decay):\n\t""""""Sets the learning rate to the initial LR decayed by 10 """"""\n\tfor param_group in optimizer.param_groups:\n\t\tparam_group[\'lr\'] /= decay\n\ndef adjust_learning_rate_loss(optimizer, decay, stat_current, stat_previous, op):\n\tops = {\'>\': (lambda x, y: x > y), \'<\': (lambda x, y: x < y), \'>=\': (lambda x, y: x >= y), \'<=\': (lambda x, y: x <= y)}\n\tif ops[op](stat_current, stat_previous):\n\t\tfor param_group in optimizer.param_groups:\n\t\t\tparam_group[\'lr\'] /= decay\n\ndef adjust_learning_rate_dann(optimizer, p):\n\tfor param_group in optimizer.param_groups:\n\t\tparam_group[\'lr\'] = args.lr / (1. + 10 * p) ** 0.75\n\ndef loss_adaptive_weight(loss, pred):\n\tweight = 1 / pred.var().log()\n\tconstant = pred.std().log()\n\treturn loss * weight + constant\n\ndef accuracy(output, target, topk=(1,)):\n\t""""""Computes the precision@k for the specified values of k""""""\n\tmaxk = max(topk)\n\tbatch_size = target.size(0)\n\n\t_, pred = output.topk(maxk, 1, True, True)\n\tpred = pred.t()\n\tcorrect = pred.eq(target.view(1, -1).expand_as(pred))\n\n\tres = []\n\tfor k in topk:\n\t\tcorrect_k = correct[:k].view(-1).float().sum(0)\n\t\tres.append(correct_k.mul_(100.0 / batch_size))\n\treturn res\n\n# remove dummy tensors\ndef removeDummy(attn, out_1, out_2, pred_domain, feat, batch_size):\n\tattn = attn[:batch_size]\n\tout_1 = out_1[:batch_size]\n\tout_2 = out_2[:batch_size]\n\tpred_domain = [pred[:batch_size] for pred in pred_domain]\n\tfeat = [f[:batch_size] for f in feat]\n\n\treturn attn, out_1, out_2, pred_domain, feat\n\nif __name__ == \'__main__\':\n\tmain()\n'"
models.py,17,"b'from torch import nn\n\nfrom torch.nn.init import *\nfrom torch.autograd import Function\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport TRNmodule\nimport math\n\nfrom colorama import init\nfrom colorama import Fore, Back, Style\n\ntorch.manual_seed(1)\ntorch.cuda.manual_seed_all(1)\n\ninit(autoreset=True)\n\n# definition of Gradient Reversal Layer\nclass GradReverse(Function):\n\t@staticmethod\n\tdef forward(ctx, x, beta):\n\t\tctx.beta = beta\n\t\treturn x.view_as(x)\n\n\t@staticmethod\n\tdef backward(ctx, grad_output):\n\t\tgrad_input = grad_output.neg() * ctx.beta\n\t\treturn grad_input, None\n\n# definition of Gradient Scaling Layer\nclass GradScale(Function):\n\t@staticmethod\n\tdef forward(ctx, x, beta):\n\t\tctx.beta = beta\n\t\treturn x.view_as(x)\n\n\t@staticmethod\n\tdef backward(ctx, grad_output):\n\t\tgrad_input = grad_output * ctx.beta\n\t\treturn grad_input, None\n\n# definition of Temporal-ConvNet Layer\nclass TCL(nn.Module):\n\tdef __init__(self, conv_size, dim):\n\t\tsuper(TCL, self).__init__()\n\n\t\tself.conv2d = nn.Conv2d(dim, dim, kernel_size=(conv_size,1), padding=(conv_size//2,0))\n\n\t\t# initialization\n\t\tkaiming_normal_(self.conv2d.weight)\n\n\tdef\tforward(self, x):\n\t\tx = self.conv2d(x)\n\n\t\treturn x\n\nclass VideoModel(nn.Module):\n\tdef __init__(self, num_class, baseline_type, frame_aggregation, modality,\n\t\t\t\ttrain_segments=5, val_segments=25,\n\t\t\t\tbase_model=\'resnet101\', path_pretrained=\'\', new_length=None,\n\t\t\t\tbefore_softmax=True,\n\t\t\t\tdropout_i=0.5, dropout_v=0.5, use_bn=\'none\', ens_DA=\'none\',\n\t\t\t\tcrop_num=1, partial_bn=True, verbose=True, add_fc=1, fc_dim=1024,\n\t\t\t\tn_rnn=1, rnn_cell=\'LSTM\', n_directions=1, n_ts=5,\n\t\t\t\tuse_attn=\'TransAttn\', n_attn=1, use_attn_frame=\'none\',\n\t\t\t\tshare_params=\'Y\'):\n\t\tsuper(VideoModel, self).__init__()\n\t\tself.modality = modality\n\t\tself.train_segments = train_segments\n\t\tself.val_segments = val_segments\n\t\tself.baseline_type = baseline_type\n\t\tself.frame_aggregation = frame_aggregation\n\t\tself.reshape = True\n\t\tself.before_softmax = before_softmax\n\t\tself.dropout_rate_i = dropout_i\n\t\tself.dropout_rate_v = dropout_v\n\t\tself.use_bn = use_bn\n\t\tself.ens_DA = ens_DA\n\t\tself.crop_num = crop_num\n\t\tself.add_fc = add_fc\n\t\tself.fc_dim = fc_dim\n\t\tself.share_params = share_params\n\n\t\t# RNN\n\t\tself.n_layers = n_rnn\n\t\tself.rnn_cell = rnn_cell\n\t\tself.n_directions = n_directions\n\t\tself.n_ts = n_ts # temporal segment\n\n\t\t# Attention\n\t\tself.use_attn = use_attn \n\t\tself.n_attn = n_attn\n\t\tself.use_attn_frame = use_attn_frame\n\n\t\tif new_length is None:\n\t\t\tself.new_length = 1 if modality == ""RGB"" else 5\n\t\telse:\n\t\t\tself.new_length = new_length\n\n\t\tif verbose:\n\t\t\tprint((""""""\n\t\t\t\tInitializing TSN with base model: {}.\n\t\t\t\tTSN Configurations:\n\t\t\t\tinput_modality:     {}\n\t\t\t\tnum_segments:       {}\n\t\t\t\tnew_length:         {}\n\t\t\t\t"""""".format(base_model, self.modality, self.train_segments, self.new_length)))\n\n\t\tself._prepare_DA(num_class, base_model)\n\n\t\tif not self.before_softmax:\n\t\t\tself.softmax = nn.Softmax()\n\n\t\tself._enable_pbn = partial_bn\n\t\tif partial_bn:\n\t\t\tself.partialBN(True)\n\n\tdef _prepare_DA(self, num_class, base_model): # convert the model to DA framework\n\t\tif base_model == \'c3d\': # C3D mode: in construction...\n\t\t\tfrom C3D_model import C3D\n\t\t\tmodel_test = C3D()\n\t\t\tself.feature_dim = model_test.fc7.in_features\n\t\telse:\n\t\t\tmodel_test = getattr(torchvision.models, base_model)(True) # model_test is only used for getting the dim #\n\t\t\tself.feature_dim = model_test.fc.in_features\n\n\t\tstd = 0.001\n\t\tfeat_shared_dim = min(self.fc_dim, self.feature_dim) if self.add_fc > 0 and self.fc_dim > 0 else self.feature_dim\n\t\tfeat_frame_dim = feat_shared_dim\n\n\t\tself.relu = nn.ReLU(inplace=True)\n\t\tself.dropout_i = nn.Dropout(p=self.dropout_rate_i)\n\t\tself.dropout_v = nn.Dropout(p=self.dropout_rate_v)\n\n\t\t#------ frame-level layers (shared layers + source layers + domain layers) ------#\n\t\tif self.add_fc < 1:\n\t\t\traise ValueError(Back.RED + \'add at least one fc layer\')\n\n\t\t# 1. shared feature layers\n\t\tself.fc_feature_shared_source = nn.Linear(self.feature_dim, feat_shared_dim)\n\t\tnormal_(self.fc_feature_shared_source.weight, 0, std)\n\t\tconstant_(self.fc_feature_shared_source.bias, 0)\n\n\t\tif self.add_fc > 1:\n\t\t\tself.fc_feature_shared_2_source = nn.Linear(feat_shared_dim, feat_shared_dim)\n\t\t\tnormal_(self.fc_feature_shared_2_source.weight, 0, std)\n\t\t\tconstant_(self.fc_feature_shared_2_source.bias, 0)\n\n\t\tif self.add_fc > 2:\n\t\t\tself.fc_feature_shared_3_source = nn.Linear(feat_shared_dim, feat_shared_dim)\n\t\t\tnormal_(self.fc_feature_shared_3_source.weight, 0, std)\n\t\t\tconstant_(self.fc_feature_shared_3_source.bias, 0)\n\n\t\t# 2. frame-level feature layers\n\t\tself.fc_feature_source = nn.Linear(feat_shared_dim, feat_frame_dim)\n\t\tnormal_(self.fc_feature_source.weight, 0, std)\n\t\tconstant_(self.fc_feature_source.bias, 0)\n\n\t\t# 3. domain feature layers (frame-level)\n\t\tself.fc_feature_domain = nn.Linear(feat_shared_dim, feat_frame_dim)\n\t\tnormal_(self.fc_feature_domain.weight, 0, std)\n\t\tconstant_(self.fc_feature_domain.bias, 0)\n\n\t\t# 4. classifiers (frame-level)\n\t\tself.fc_classifier_source = nn.Linear(feat_frame_dim, num_class)\n\t\tnormal_(self.fc_classifier_source.weight, 0, std)\n\t\tconstant_(self.fc_classifier_source.bias, 0)\n\n\t\tself.fc_classifier_domain = nn.Linear(feat_frame_dim, 2)\n\t\tnormal_(self.fc_classifier_domain.weight, 0, std)\n\t\tconstant_(self.fc_classifier_domain.bias, 0)\n\n\t\tif self.share_params == \'N\':\n\t\t\tself.fc_feature_shared_target = nn.Linear(self.feature_dim, feat_shared_dim)\n\t\t\tnormal_(self.fc_feature_shared_target.weight, 0, std)\n\t\t\tconstant_(self.fc_feature_shared_target.bias, 0)\n\t\t\tif self.add_fc > 1:\n\t\t\t\tself.fc_feature_shared_2_target = nn.Linear(feat_shared_dim, feat_shared_dim)\n\t\t\t\tnormal_(self.fc_feature_shared_2_target.weight, 0, std)\n\t\t\t\tconstant_(self.fc_feature_shared_2_target.bias, 0)\n\t\t\tif self.add_fc > 2:\n\t\t\t\tself.fc_feature_shared_3_target = nn.Linear(feat_shared_dim, feat_shared_dim)\n\t\t\t\tnormal_(self.fc_feature_shared_3_target.weight, 0, std)\n\t\t\t\tconstant_(self.fc_feature_shared_3_target.bias, 0)\n\n\t\t\tself.fc_feature_target = nn.Linear(feat_shared_dim, feat_frame_dim)\n\t\t\tnormal_(self.fc_feature_target.weight, 0, std)\n\t\t\tconstant_(self.fc_feature_target.bias, 0)\n\t\t\tself.fc_classifier_target = nn.Linear(feat_frame_dim, num_class)\n\t\t\tnormal_(self.fc_classifier_target.weight, 0, std)\n\t\t\tconstant_(self.fc_classifier_target.bias, 0)\n\n\t\t# BN for the above layers\n\t\tif self.use_bn != \'none\':  # S & T: use AdaBN (ICLRW 2017) approach\n\t\t\tself.bn_shared_S = nn.BatchNorm1d(feat_shared_dim)  # BN for the shared layers\n\t\t\tself.bn_shared_T = nn.BatchNorm1d(feat_shared_dim)\n\t\t\tself.bn_source_S = nn.BatchNorm1d(feat_frame_dim)  # BN for the source feature layers\n\t\t\tself.bn_source_T = nn.BatchNorm1d(feat_frame_dim)\n\n\t\t#------ aggregate frame-based features (frame feature --> video feature) ------#\n\t\tif self.frame_aggregation == \'rnn\': # 2. rnn\n\t\t\tself.hidden_dim = feat_frame_dim\n\t\t\tif self.rnn_cell == \'LSTM\':\n\t\t\t\tself.rnn = nn.LSTM(feat_frame_dim, self.hidden_dim//self.n_directions, self.n_layers, batch_first=True, bidirectional=bool(int(self.n_directions/2)))\n\t\t\telif self.rnn_cell == \'GRU\':\n\t\t\t\tself.rnn = nn.GRU(feat_frame_dim, self.hidden_dim//self.n_directions, self.n_layers, batch_first=True, bidirectional=bool(int(self.n_directions/2)))\n\n\t\t\t# initialization\n\t\t\tfor p in range(self.n_layers):\n\t\t\t\tkaiming_normal_(self.rnn.all_weights[p][0])\n\t\t\t\tkaiming_normal_(self.rnn.all_weights[p][1])\n\n\t\t\tself.bn_before_rnn = nn.BatchNorm2d(1)\n\t\t\tself.bn_after_rnn = nn.BatchNorm2d(1)\n\n\t\telif self.frame_aggregation == \'trn\': # 4. TRN (ECCV 2018) ==> fix segment # for both train/val\n\t\t\tself.num_bottleneck = 512\n\t\t\tself.TRN = TRNmodule.RelationModule(feat_shared_dim, self.num_bottleneck, self.train_segments)\n\t\t\tself.bn_trn_S = nn.BatchNorm1d(self.num_bottleneck)\n\t\t\tself.bn_trn_T = nn.BatchNorm1d(self.num_bottleneck)\n\t\telif self.frame_aggregation == \'trn-m\':  # 4. TRN (ECCV 2018) ==> fix segment # for both train/val\n\t\t\tself.num_bottleneck = 256\n\t\t\tself.TRN = TRNmodule.RelationModuleMultiScale(feat_shared_dim, self.num_bottleneck, self.train_segments)\n\t\t\tself.bn_trn_S = nn.BatchNorm1d(self.num_bottleneck)\n\t\t\tself.bn_trn_T = nn.BatchNorm1d(self.num_bottleneck)\n\n\t\telif self.frame_aggregation == \'temconv\': # 3. temconv\n\n\t\t\tself.tcl_3_1 = TCL(3, 1)\n\t\t\tself.tcl_5_1 = TCL(5, 1)\n\t\t\tself.bn_1_S = nn.BatchNorm1d(feat_frame_dim)\n\t\t\tself.bn_1_T = nn.BatchNorm1d(feat_frame_dim)\n\n\t\t\tself.tcl_3_2 = TCL(3, 1)\n\t\t\tself.tcl_5_2 = TCL(5, 2)\n\t\t\tself.bn_2_S = nn.BatchNorm1d(feat_frame_dim)\n\t\t\tself.bn_2_T = nn.BatchNorm1d(feat_frame_dim)\n\n\t\t\tself.conv_fusion = nn.Sequential(\n\t\t\t\tnn.Conv2d(2, 1, kernel_size=(1, 1), padding=(0, 0)),\n\t\t\t\tnn.ReLU(inplace=True),\n\t\t\t)\n\n\t\t# ------ video-level layers (source layers + domain layers) ------#\n\t\tif self.frame_aggregation == \'avgpool\': # 1. avgpool\n\t\t\tfeat_aggregated_dim = feat_shared_dim\n\t\tif \'trn\' in self.frame_aggregation : # 4. trn\n\t\t\tfeat_aggregated_dim = self.num_bottleneck\n\t\telif self.frame_aggregation == \'rnn\': # 2. rnn\n\t\t\tfeat_aggregated_dim = self.hidden_dim\n\t\telif self.frame_aggregation == \'temconv\': # 3. temconv\n\t\t\tfeat_aggregated_dim = feat_shared_dim\n\n\t\tfeat_video_dim = feat_aggregated_dim\n\n\t\t# 1. source feature layers (video-level)\n\t\tself.fc_feature_video_source = nn.Linear(feat_aggregated_dim, feat_video_dim)\n\t\tnormal_(self.fc_feature_video_source.weight, 0, std)\n\t\tconstant_(self.fc_feature_video_source.bias, 0)\n\n\t\tself.fc_feature_video_source_2 = nn.Linear(feat_video_dim, feat_video_dim)\n\t\tnormal_(self.fc_feature_video_source_2.weight, 0, std)\n\t\tconstant_(self.fc_feature_video_source_2.bias, 0)\n\n\t\t# 2. domain feature layers (video-level)\n\t\tself.fc_feature_domain_video = nn.Linear(feat_aggregated_dim, feat_video_dim)\n\t\tnormal_(self.fc_feature_domain_video.weight, 0, std)\n\t\tconstant_(self.fc_feature_domain_video.bias, 0)\n\n\t\t# 3. classifiers (video-level)\n\t\tself.fc_classifier_video_source = nn.Linear(feat_video_dim, num_class)\n\t\tnormal_(self.fc_classifier_video_source.weight, 0, std)\n\t\tconstant_(self.fc_classifier_video_source.bias, 0)\n\n\t\tif self.ens_DA == \'MCD\':\n\t\t\tself.fc_classifier_video_source_2 = nn.Linear(feat_video_dim, num_class) # second classifier for self-ensembling\n\t\t\tnormal_(self.fc_classifier_video_source_2.weight, 0, std)\n\t\t\tconstant_(self.fc_classifier_video_source_2.bias, 0)\n\n\t\tself.fc_classifier_domain_video = nn.Linear(feat_video_dim, 2)\n\t\tnormal_(self.fc_classifier_domain_video.weight, 0, std)\n\t\tconstant_(self.fc_classifier_domain_video.bias, 0)\n\n\t\t# domain classifier for TRN-M\n\t\tif self.frame_aggregation == \'trn-m\':\n\t\t\tself.relation_domain_classifier_all = nn.ModuleList()\n\t\t\tfor i in range(self.train_segments-1):\n\t\t\t\trelation_domain_classifier = nn.Sequential(\n\t\t\t\t\tnn.Linear(feat_aggregated_dim, feat_video_dim),\n\t\t\t\t\tnn.ReLU(),\n\t\t\t\t\tnn.Linear(feat_video_dim, 2)\n\t\t\t\t)\n\t\t\t\tself.relation_domain_classifier_all += [relation_domain_classifier]\n\n\t\tif self.share_params == \'N\':\n\t\t\tself.fc_feature_video_target = nn.Linear(feat_aggregated_dim, feat_video_dim)\n\t\t\tnormal_(self.fc_feature_video_target.weight, 0, std)\n\t\t\tconstant_(self.fc_feature_video_target.bias, 0)\n\t\t\tself.fc_feature_video_target_2 = nn.Linear(feat_video_dim, feat_video_dim)\n\t\t\tnormal_(self.fc_feature_video_target_2.weight, 0, std)\n\t\t\tconstant_(self.fc_feature_video_target_2.bias, 0)\n\t\t\tself.fc_classifier_video_target = nn.Linear(feat_video_dim, num_class)\n\t\t\tnormal_(self.fc_classifier_video_target.weight, 0, std)\n\t\t\tconstant_(self.fc_classifier_video_target.bias, 0)\n\n\t\t# BN for the above layers\n\t\tif self.use_bn != \'none\':  # S & T: use AdaBN (ICLRW 2017) approach\n\t\t\tself.bn_source_video_S = nn.BatchNorm1d(feat_video_dim)\n\t\t\tself.bn_source_video_T = nn.BatchNorm1d(feat_video_dim)\n\t\t\tself.bn_source_video_2_S = nn.BatchNorm1d(feat_video_dim)\n\t\t\tself.bn_source_video_2_T = nn.BatchNorm1d(feat_video_dim)\n\n\t\tself.alpha = torch.ones(1)\n\t\tif self.use_bn == \'AutoDIAL\':\n\t\t\tself.alpha = nn.Parameter(self.alpha)\n\n\t\t# ------ attention mechanism ------#\n\t\t# conventional attention\n\t\tif self.use_attn == \'general\':\n\t\t\tself.attn_layer = nn.Sequential(\n\t\t\t\tnn.Linear(feat_aggregated_dim, feat_aggregated_dim),\n\t\t\t\tnn.Tanh(),\n\t\t\t\tnn.Linear(feat_aggregated_dim, 1)\n\t\t\t\t)\n\n\n\tdef train(self, mode=True):\n\t\t# not necessary in our setting\n\t\t""""""\n\t\tOverride the default train() to freeze the BN parameters\n\t\t:return:\n\t\t""""""\n\t\tsuper(VideoModel, self).train(mode)\n\t\tcount = 0\n\t\tif self._enable_pbn:\n\t\t\tprint(""Freezing BatchNorm2D except the first one."")\n\t\t\tfor m in self.base_model.modules():\n\t\t\t\tif isinstance(m, nn.BatchNorm2d):\n\t\t\t\t\tcount += 1\n\t\t\t\t\tif count >= (2 if self._enable_pbn else 1):\n\t\t\t\t\t\tm.eval()\n\n\t\t\t\t\t\t# shutdown update in frozen mode\n\t\t\t\t\t\tm.weight.requires_grad = False\n\t\t\t\t\t\tm.bias.requires_grad = False\n\n\tdef partialBN(self, enable):\n\t\tself._enable_pbn = enable\n\n\tdef get_trans_attn(self, pred_domain):\n\t\tsoftmax = nn.Softmax(dim=1)\n\t\tlogsoftmax = nn.LogSoftmax(dim=1)\n\t\tentropy = torch.sum(-softmax(pred_domain) * logsoftmax(pred_domain), 1)\n\t\tweights = 1 - entropy\n\n\t\treturn weights\n\n\tdef get_general_attn(self, feat):\n\t\tnum_segments = feat.size()[1]\n\t\tfeat = feat.view(-1, feat.size()[-1]) # reshape features: 128x4x256 --> (128x4)x256\n\t\tweights = self.attn_layer(feat) # e.g. (128x4)x1\n\t\tweights = weights.view(-1, num_segments, weights.size()[-1]) # reshape attention weights: (128x4)x1 --> 128x4x1\n\t\tweights = F.softmax(weights, dim=1)  # softmax over segments ==> 128x4x1\n\n\t\treturn weights\n\n\tdef get_attn_feat_frame(self, feat_fc, pred_domain): # not used for now\n\t\tif self.use_attn == \'TransAttn\':\n\t\t\tweights_attn = self.get_trans_attn(pred_domain)\n\t\telif self.use_attn == \'general\':\n\t\t\tweights_attn = self.get_general_attn(feat_fc)\n\n\t\tweights_attn = weights_attn.view(-1, 1).repeat(1,feat_fc.size()[-1]) # reshape & repeat weights (e.g. 16 x 512)\n\t\tfeat_fc_attn = (weights_attn+1) * feat_fc\n\n\t\treturn feat_fc_attn\n\n\tdef get_attn_feat_relation(self, feat_fc, pred_domain, num_segments):\n\t\tif self.use_attn == \'TransAttn\':\n\t\t\tweights_attn = self.get_trans_attn(pred_domain)\n\t\telif self.use_attn == \'general\':\n\t\t\tweights_attn = self.get_general_attn(feat_fc)\n\n\t\tweights_attn = weights_attn.view(-1, num_segments-1, 1).repeat(1,1,feat_fc.size()[-1]) # reshape & repeat weights (e.g. 16 x 4 x 256)\n\t\tfeat_fc_attn = (weights_attn+1) * feat_fc\n\n\t\treturn feat_fc_attn, weights_attn[:,:,0]\n\n\tdef aggregate_frames(self, feat_fc, num_segments, pred_domain):\n\t\tfeat_fc_video = None\n\t\tif self.frame_aggregation == \'rnn\':\n\t\t\t# 2. RNN\n\t\t\tfeat_fc_video = feat_fc.view((-1, num_segments) + feat_fc.size()[-1:])  # reshape for RNN\n\n\t\t\t# temporal segments and pooling\n\t\t\tlen_ts = round(num_segments/self.n_ts)\n\t\t\tnum_extra_f = len_ts*self.n_ts-num_segments\n\t\t\tif num_extra_f < 0: # can remove last frame-level features\n\t\t\t\tfeat_fc_video = feat_fc_video[:, :len_ts * self.n_ts, :]  # make the temporal length can be divided by n_ts (16 x 25 x 512 --> 16 x 24 x 512)\n\t\t\telif num_extra_f > 0: # need to repeat last frame-level features\n\t\t\t\tfeat_fc_video = torch.cat((feat_fc_video, feat_fc_video[:,-1:,:].repeat(1,num_extra_f,1)), 1) # make the temporal length can be divided by n_ts (16 x 5 x 512 --> 16 x 6 x 512)\n\n\t\t\tfeat_fc_video = feat_fc_video.view(\n\t\t\t\t(-1, self.n_ts, len_ts) + feat_fc_video.size()[2:])  # 16 x 6 x 512 --> 16 x 3 x 2 x 512\n\t\t\tfeat_fc_video = nn.MaxPool2d(kernel_size=(len_ts, 1))(\n\t\t\t\tfeat_fc_video)  # 16 x 3 x 2 x 512 --> 16 x 3 x 1 x 512\n\t\t\tfeat_fc_video = feat_fc_video.squeeze(2)  # 16 x 3 x 1 x 512 --> 16 x 3 x 512\n\n\t\t\thidden_temp = torch.zeros(self.n_layers * self.n_directions, feat_fc_video.size(0),\n\t\t\t\t\t\t\t\t\t  self.hidden_dim // self.n_directions).cuda()\n\n\t\t\tif self.rnn_cell == \'LSTM\':\n\t\t\t\thidden_init = (hidden_temp, hidden_temp)\n\t\t\telif self.rnn_cell == \'GRU\':\n\t\t\t\thidden_init = hidden_temp\n\n\t\t\tself.rnn.flatten_parameters()\n\t\t\tfeat_fc_video, hidden_final = self.rnn(feat_fc_video, hidden_init)  # e.g. 16 x 25 x 512\n\n\t\t\t# get the last feature vector\n\t\t\tfeat_fc_video = feat_fc_video[:, -1, :]\n\n\t\telse:\n\t\t\t# 1. averaging\n\t\t\tfeat_fc_video = feat_fc.view((-1, 1, num_segments) + feat_fc.size()[-1:])  # reshape based on the segments (e.g. 16 x 1 x 5 x 512)\n\t\t\tif self.use_attn == \'TransAttn\': # get the attention weighting\n\t\t\t\tweights_attn = self.get_trans_attn(pred_domain)\n\t\t\t\tweights_attn = weights_attn.view(-1, 1, num_segments,1).repeat(1,1,1,feat_fc.size()[-1]) # reshape & repeat weights (e.g. 16 x 1 x 5 x 512)\n\t\t\t\tfeat_fc_video = (weights_attn+1) * feat_fc_video\n\n\t\t\tfeat_fc_video = nn.AvgPool2d([num_segments, 1])(feat_fc_video)  # e.g. 16 x 1 x 1 x 512\n\t\t\tfeat_fc_video = feat_fc_video.squeeze(1).squeeze(1)  # e.g. 16 x 512\n\n\t\treturn feat_fc_video\n\n\tdef final_output(self, pred, pred_video, num_segments):\n\t\tif self.baseline_type == \'video\':\n\t\t\tbase_out = pred_video\n\t\telse:\n\t\t\tbase_out = pred\n\n\t\tif not self.before_softmax:\n\t\t\tbase_out = self.softmax(base_out)\n\n\t\toutput = base_out\n\n\t\tif self.baseline_type == \'tsn\':\n\t\t\tif self.reshape:\n\t\t\t\tbase_out = base_out.view((-1, num_segments) + base_out.size()[1:]) # e.g. 16 x 3 x 12 (3 segments)\n\n\t\t\toutput = base_out.mean(1) # e.g. 16 x 12\n\n\t\treturn output\n\n\tdef domain_classifier_frame(self, feat, beta):\n\t\tfeat_fc_domain_frame = GradReverse.apply(feat, beta[2])\n\t\tfeat_fc_domain_frame = self.fc_feature_domain(feat_fc_domain_frame)\n\t\tfeat_fc_domain_frame = self.relu(feat_fc_domain_frame)\n\t\tpred_fc_domain_frame = self.fc_classifier_domain(feat_fc_domain_frame)\n\n\t\treturn pred_fc_domain_frame\n\n\tdef domain_classifier_video(self, feat_video, beta):\n\t\tfeat_fc_domain_video = GradReverse.apply(feat_video, beta[1])\n\t\tfeat_fc_domain_video = self.fc_feature_domain_video(feat_fc_domain_video)\n\t\tfeat_fc_domain_video = self.relu(feat_fc_domain_video)\n\t\tpred_fc_domain_video = self.fc_classifier_domain_video(feat_fc_domain_video)\n\n\t\treturn pred_fc_domain_video\n\n\tdef domain_classifier_relation(self, feat_relation, beta):\n\t\t# 128x4x256 --> (128x4)x2\n\t\tpred_fc_domain_relation_video = None\n\t\tfor i in range(len(self.relation_domain_classifier_all)):\n\t\t\tfeat_relation_single = feat_relation[:,i,:].squeeze(1) # 128x1x256 --> 128x256\n\t\t\tfeat_fc_domain_relation_single = GradReverse.apply(feat_relation_single, beta[0]) # the same beta for all relations (for now)\n\n\t\t\tpred_fc_domain_relation_single = self.relation_domain_classifier_all[i](feat_fc_domain_relation_single)\n\t\n\t\t\tif pred_fc_domain_relation_video is None:\n\t\t\t\tpred_fc_domain_relation_video = pred_fc_domain_relation_single.view(-1,1,2)\n\t\t\telse:\n\t\t\t\tpred_fc_domain_relation_video = torch.cat((pred_fc_domain_relation_video, pred_fc_domain_relation_single.view(-1,1,2)), 1)\n\t\t\n\t\tpred_fc_domain_relation_video = pred_fc_domain_relation_video.view(-1,2)\n\n\t\treturn pred_fc_domain_relation_video\n\n\tdef domainAlign(self, input_S, input_T, is_train, name_layer, alpha, num_segments, dim):\n\t\tinput_S = input_S.view((-1, dim, num_segments) + input_S.size()[-1:])  # reshape based on the segments (e.g. 80 x 512 --> 16 x 1 x 5 x 512)\n\t\tinput_T = input_T.view((-1, dim, num_segments) + input_T.size()[-1:])  # reshape based on the segments\n\n\t\t# clamp alpha\n\t\talpha = max(alpha,0.5)\n\n\t\t# rearange source and target data\n\t\tnum_S_1 = int(round(input_S.size(0) * alpha))\n\t\tnum_S_2 = input_S.size(0) - num_S_1\n\t\tnum_T_1 = int(round(input_T.size(0) * alpha))\n\t\tnum_T_2 = input_T.size(0) - num_T_1\n\n\t\tif is_train and num_S_2 > 0 and num_T_2 > 0:\n\t\t\tinput_source = torch.cat((input_S[:num_S_1], input_T[-num_T_2:]), 0)\n\t\t\tinput_target = torch.cat((input_T[:num_T_1], input_S[-num_S_2:]), 0)\n\t\telse:\n\t\t\tinput_source = input_S\n\t\t\tinput_target = input_T\n\n\t\t# adaptive BN\n\t\tinput_source = input_source.view((-1, ) + input_source.size()[-1:]) # reshape to feed BN (e.g. 16 x 1 x 5 x 512 --> 80 x 512)\n\t\tinput_target = input_target.view((-1, ) + input_target.size()[-1:])\n\n\t\tif name_layer == \'shared\':\n\t\t\tinput_source_bn = self.bn_shared_S(input_source)\n\t\t\tinput_target_bn = self.bn_shared_T(input_target)\n\t\telif \'trn\' in name_layer:\n\t\t\tinput_source_bn = self.bn_trn_S(input_source)\n\t\t\tinput_target_bn = self.bn_trn_T(input_target)\n\t\telif name_layer == \'temconv_1\':\n\t\t\tinput_source_bn = self.bn_1_S(input_source)\n\t\t\tinput_target_bn = self.bn_1_T(input_target)\n\t\telif name_layer == \'temconv_2\':\n\t\t\tinput_source_bn = self.bn_2_S(input_source)\n\t\t\tinput_target_bn = self.bn_2_T(input_target)\n\n\t\tinput_source_bn = input_source_bn.view((-1, dim, num_segments) + input_source_bn.size()[-1:])  # reshape back (e.g. 80 x 512 --> 16 x 1 x 5 x 512)\n\t\tinput_target_bn = input_target_bn.view((-1, dim, num_segments) + input_target_bn.size()[-1:])  #\n\n\t\t# rearange back to the original order of source and target data (since target may be unlabeled)\n\t\tif is_train and num_S_2 > 0 and num_T_2 > 0:\n\t\t\tinput_source_bn = torch.cat((input_source_bn[:num_S_1], input_target_bn[-num_S_2:]), 0)\n\t\t\tinput_target_bn = torch.cat((input_target_bn[:num_T_1], input_source_bn[-num_T_2:]), 0)\n\n\t\t# reshape for frame-level features\n\t\tif name_layer == \'shared\' or name_layer == \'trn_sum\':\n\t\t\tinput_source_bn = input_source_bn.view((-1,) + input_source_bn.size()[-1:])  # (e.g. 16 x 1 x 5 x 512 --> 80 x 512)\n\t\t\tinput_target_bn = input_target_bn.view((-1,) + input_target_bn.size()[-1:])\n\t\telif name_layer == \'trn\':\n\t\t\tinput_source_bn = input_source_bn.view((-1, num_segments) + input_source_bn.size()[-1:])  # (e.g. 16 x 1 x 5 x 512 --> 80 x 512)\n\t\t\tinput_target_bn = input_target_bn.view((-1, num_segments) + input_target_bn.size()[-1:])\n\n\t\treturn input_source_bn, input_target_bn\n\n\tdef forward(self, input_source, input_target, beta, mu, is_train, reverse):\n\t\tbatch_source = input_source.size()[0]\n\t\tbatch_target = input_target.size()[0]\n\t\tnum_segments = self.train_segments if is_train else self.val_segments\n\t\t# sample_len = (3 if self.modality == ""RGB"" else 2) * self.new_length\n\t\tsample_len = self.new_length\n\t\tfeat_all_source = []\n\t\tfeat_all_target = []\n\t\tpred_domain_all_source = []\n\t\tpred_domain_all_target = []\n\n\t\t# input_data is a list of tensors --> need to do pre-processing\n\t\tfeat_base_source = input_source.view(-1, input_source.size()[-1]) # e.g. 256 x 25 x 2048 --> 6400 x 2048\n\t\tfeat_base_target = input_target.view(-1, input_target.size()[-1])  # e.g. 256 x 25 x 2048 --> 6400 x 2048\n\n\t\t#=== shared layers ===#\n\t\t# need to separate BN for source & target ==> otherwise easy to overfit to source data\n\t\tif self.add_fc < 1:\n\t\t\traise ValueError(Back.RED + \'not enough fc layer\')\n\n\t\tfeat_fc_source = self.fc_feature_shared_source(feat_base_source)\n\t\tfeat_fc_target = self.fc_feature_shared_target(feat_base_target) if self.share_params == \'N\' else self.fc_feature_shared_source(feat_base_target)\n\n\t\t# adaptive BN\n\t\tif self.use_bn != \'none\':\n\t\t\tfeat_fc_source, feat_fc_target = self.domainAlign(feat_fc_source, feat_fc_target, is_train, \'shared\', self.alpha.item(), num_segments, 1)\n\n\t\tfeat_fc_source = self.relu(feat_fc_source)\n\t\tfeat_fc_target = self.relu(feat_fc_target)\n\t\tfeat_fc_source = self.dropout_i(feat_fc_source)\n\t\tfeat_fc_target = self.dropout_i(feat_fc_target)\n\n\t\t# feat_fc = self.dropout_i(feat_fc)\n\t\tfeat_all_source.append(feat_fc_source.view((batch_source, num_segments) + feat_fc_source.size()[-1:])) # reshape ==> 1st dim is the batch size\n\t\tfeat_all_target.append(feat_fc_target.view((batch_target, num_segments) + feat_fc_target.size()[-1:]))\n\n\t\tif self.add_fc > 1: \n\t\t\tfeat_fc_source = self.fc_feature_shared_2_source(feat_fc_source)\n\t\t\tfeat_fc_target = self.fc_feature_shared_2_target(feat_fc_target) if self.share_params == \'N\' else self.fc_feature_shared_2_source(feat_fc_target)\n\n\t\t\tfeat_fc_source = self.relu(feat_fc_source)\n\t\t\tfeat_fc_target = self.relu(feat_fc_target)\n\t\t\tfeat_fc_source = self.dropout_i(feat_fc_source)\n\t\t\tfeat_fc_target = self.dropout_i(feat_fc_target)\n\n\t\t\tfeat_all_source.append(feat_fc_source.view((batch_source, num_segments) + feat_fc_source.size()[-1:])) # reshape ==> 1st dim is the batch size\n\t\t\tfeat_all_target.append(feat_fc_target.view((batch_target, num_segments) + feat_fc_target.size()[-1:]))\n\n\t\tif self.add_fc > 2: \n\t\t\tfeat_fc_source = self.fc_feature_shared_3_source(feat_fc_source)\n\t\t\tfeat_fc_target = self.fc_feature_shared_3_target(feat_fc_target) if self.share_params == \'N\' else self.fc_feature_shared_3_source(feat_fc_target)\n\n\t\t\tfeat_fc_source = self.relu(feat_fc_source)\n\t\t\tfeat_fc_target = self.relu(feat_fc_target)\n\t\t\tfeat_fc_source = self.dropout_i(feat_fc_source)\n\t\t\tfeat_fc_target = self.dropout_i(feat_fc_target)\n\n\t\t\tfeat_all_source.append(feat_fc_source.view((batch_source, num_segments) + feat_fc_source.size()[-1:])) # reshape ==> 1st dim is the batch size\n\t\t\tfeat_all_target.append(feat_fc_target.view((batch_target, num_segments) + feat_fc_target.size()[-1:]))\n\n\t\t# === adversarial branch (frame-level) ===#\n\t\tpred_fc_domain_frame_source = self.domain_classifier_frame(feat_fc_source, beta)\n\t\tpred_fc_domain_frame_target = self.domain_classifier_frame(feat_fc_target, beta)\n\n\t\tpred_domain_all_source.append(pred_fc_domain_frame_source.view((batch_source, num_segments) + pred_fc_domain_frame_source.size()[-1:]))\n\t\tpred_domain_all_target.append(pred_fc_domain_frame_target.view((batch_target, num_segments) + pred_fc_domain_frame_target.size()[-1:]))\n\n\t\tif self.use_attn_frame != \'none\': # attend the frame-level features only\n\t\t\tfeat_fc_source = self.get_attn_feat_frame(feat_fc_source, pred_fc_domain_frame_source)\n\t\t\tfeat_fc_target = self.get_attn_feat_frame(feat_fc_target, pred_fc_domain_frame_target)\n\n\t\t#=== source layers (frame-level) ===#\n\t\tpred_fc_source = self.fc_classifier_source(feat_fc_source)\n\t\tpred_fc_target = self.fc_classifier_target(feat_fc_target) if self.share_params == \'N\' else self.fc_classifier_source(feat_fc_target)\n\t\tif self.baseline_type == \'frame\':\n\t\t\tfeat_all_source.append(pred_fc_source.view((batch_source, num_segments) + pred_fc_source.size()[-1:])) # reshape ==> 1st dim is the batch size\n\t\t\tfeat_all_target.append(pred_fc_target.view((batch_target, num_segments) + pred_fc_target.size()[-1:]))\n\n\t\t### aggregate the frame-based features to video-based features ###\n\t\tif self.frame_aggregation == \'avgpool\' or self.frame_aggregation == \'rnn\':\n\t\t\tfeat_fc_video_source = self.aggregate_frames(feat_fc_source, num_segments, pred_fc_domain_frame_source)\n\t\t\tfeat_fc_video_target = self.aggregate_frames(feat_fc_target, num_segments, pred_fc_domain_frame_target)\n\n\t\t\tattn_relation_source = feat_fc_video_source[:,0] # assign random tensors to attention values to avoid runtime error\n\t\t\tattn_relation_target = feat_fc_video_target[:,0] # assign random tensors to attention values to avoid runtime error\n\n\t\telif \'trn\' in self.frame_aggregation:\n\t\t\tfeat_fc_video_source = feat_fc_source.view((-1, num_segments) + feat_fc_source.size()[-1:])  # reshape based on the segments (e.g. 640x512 --> 128x5x512)\n\t\t\tfeat_fc_video_target = feat_fc_target.view((-1, num_segments) + feat_fc_target.size()[-1:])  # reshape based on the segments (e.g. 640x512 --> 128x5x512)\n\n\t\t\tfeat_fc_video_relation_source = self.TRN(feat_fc_video_source) # 128x5x512 --> 128x5x256 (256-dim. relation feature vectors x 5)\n\t\t\tfeat_fc_video_relation_target = self.TRN(feat_fc_video_target)\n\n\t\t\t# adversarial branch\n\t\t\tpred_fc_domain_video_relation_source = self.domain_classifier_relation(feat_fc_video_relation_source, beta)\n\t\t\tpred_fc_domain_video_relation_target = self.domain_classifier_relation(feat_fc_video_relation_target, beta)\n\n\t\t\t# transferable attention\n\t\t\tif self.use_attn != \'none\': # get the attention weighting\n\t\t\t\tfeat_fc_video_relation_source, attn_relation_source = self.get_attn_feat_relation(feat_fc_video_relation_source, pred_fc_domain_video_relation_source, num_segments)\n\t\t\t\tfeat_fc_video_relation_target, attn_relation_target = self.get_attn_feat_relation(feat_fc_video_relation_target, pred_fc_domain_video_relation_target, num_segments)\n\t\t\telse:\n\t\t\t\tattn_relation_source = feat_fc_video_relation_source[:,:,0] # assign random tensors to attention values to avoid runtime error\n\t\t\t\tattn_relation_target = feat_fc_video_relation_target[:,:,0] # assign random tensors to attention values to avoid runtime error\n\n\t\t\t# sum up relation features (ignore 1-relation)\n\t\t\tfeat_fc_video_source = torch.sum(feat_fc_video_relation_source, 1)\n\t\t\tfeat_fc_video_target = torch.sum(feat_fc_video_relation_target, 1)\n\n\t\telif self.frame_aggregation == \'temconv\': # DA operation inside temconv\n\t\t\tfeat_fc_video_source = feat_fc_source.view((-1, 1, num_segments) + feat_fc_source.size()[-1:])  # reshape based on the segments\n\t\t\tfeat_fc_video_target = feat_fc_target.view((-1, 1, num_segments) + feat_fc_target.size()[-1:])  # reshape based on the segments\n\n\t\t\t# 1st TCL\n\t\t\tfeat_fc_video_source_3_1 = self.tcl_3_1(feat_fc_video_source)\n\t\t\tfeat_fc_video_target_3_1 = self.tcl_3_1(feat_fc_video_target)\n\n\t\t\tif self.use_bn != \'none\':\n\t\t\t\tfeat_fc_video_source_3_1, feat_fc_video_target_3_1 = self.domainAlign(feat_fc_video_source_3_1, feat_fc_video_target_3_1, is_train, \'temconv_1\', self.alpha.item(), num_segments, 1)\n\n\t\t\tfeat_fc_video_source = self.relu(feat_fc_video_source_3_1)  # 16 x 1 x 5 x 512\n\t\t\tfeat_fc_video_target = self.relu(feat_fc_video_target_3_1)  # 16 x 1 x 5 x 512\n\n\t\t\tfeat_fc_video_source = nn.AvgPool2d(kernel_size=(num_segments, 1))(feat_fc_video_source)  # 16 x 4 x 1 x 512\n\t\t\tfeat_fc_video_target = nn.AvgPool2d(kernel_size=(num_segments, 1))(feat_fc_video_target)  # 16 x 4 x 1 x 512\n\n\t\t\tfeat_fc_video_source = feat_fc_video_source.squeeze(1).squeeze(1)  # e.g. 16 x 512\n\t\t\tfeat_fc_video_target = feat_fc_video_target.squeeze(1).squeeze(1)  # e.g. 16 x 512\n\n\t\tif self.baseline_type == \'video\':\n\t\t\tfeat_all_source.append(feat_fc_video_source.view((batch_source,) + feat_fc_video_source.size()[-1:]))\n\t\t\tfeat_all_target.append(feat_fc_video_target.view((batch_target,) + feat_fc_video_target.size()[-1:]))\n\n\t\t#=== source layers (video-level) ===#\n\t\tfeat_fc_video_source = self.dropout_v(feat_fc_video_source)\n\t\tfeat_fc_video_target = self.dropout_v(feat_fc_video_target)\n\n\t\tif reverse:\n\t\t\tfeat_fc_video_source = GradReverse.apply(feat_fc_video_source, mu)\n\t\t\tfeat_fc_video_target = GradReverse.apply(feat_fc_video_target, mu)\n\n\t\tpred_fc_video_source = self.fc_classifier_video_source(feat_fc_video_source)\n\t\tpred_fc_video_target = self.fc_classifier_video_target(feat_fc_video_target) if self.share_params == \'N\' else self.fc_classifier_video_source(feat_fc_video_target)\n\n\t\tif self.baseline_type == \'video\': # only store the prediction from classifier 1 (for now)\n\t\t\tfeat_all_source.append(pred_fc_video_source.view((batch_source,) + pred_fc_video_source.size()[-1:]))\n\t\t\tfeat_all_target.append(pred_fc_video_target.view((batch_target,) + pred_fc_video_target.size()[-1:]))\n\n\t\t#=== adversarial branch (video-level) ===#\n\t\tpred_fc_domain_video_source = self.domain_classifier_video(feat_fc_video_source, beta)\n\t\tpred_fc_domain_video_target = self.domain_classifier_video(feat_fc_video_target, beta)\n\n\t\tpred_domain_all_source.append(pred_fc_domain_video_source.view((batch_source,) + pred_fc_domain_video_source.size()[-1:]))\n\t\tpred_domain_all_target.append(pred_fc_domain_video_target.view((batch_target,) + pred_fc_domain_video_target.size()[-1:]))\n\n\t\t# video relation-based discriminator\n\t\tif self.frame_aggregation == \'trn-m\':\n\t\t\tnum_relation = feat_fc_video_relation_source.size()[1]\n\t\t\tpred_domain_all_source.append(pred_fc_domain_video_relation_source.view((batch_source, num_relation) + pred_fc_domain_video_relation_source.size()[-1:]))\n\t\t\tpred_domain_all_target.append(pred_fc_domain_video_relation_target.view((batch_target, num_relation) + pred_fc_domain_video_relation_target.size()[-1:]))\n\t\telse:\n\t\t\tpred_domain_all_source.append(pred_fc_domain_video_source) # if not trn-m, add dummy tensors for relation features\n\t\t\tpred_domain_all_target.append(pred_fc_domain_video_target)\n\n\t\t#=== final output ===#\n\t\toutput_source = self.final_output(pred_fc_source, pred_fc_video_source, num_segments) # select output from frame or video prediction\n\t\toutput_target = self.final_output(pred_fc_target, pred_fc_video_target, num_segments)\n\n\t\toutput_source_2 = output_source\n\t\toutput_target_2 = output_target\n\n\t\tif self.ens_DA == \'MCD\':\n\t\t\tpred_fc_video_source_2 = self.fc_classifier_video_source_2(feat_fc_video_source)\n\t\t\tpred_fc_video_target_2 = self.fc_classifier_video_target_2(feat_fc_video_target) if self.share_params == \'N\' else self.fc_classifier_video_source_2(feat_fc_video_target)\n\t\t\toutput_source_2 = self.final_output(pred_fc_source, pred_fc_video_source_2, num_segments)\n\t\t\toutput_target_2 = self.final_output(pred_fc_target, pred_fc_video_target_2, num_segments)\n\n\t\treturn attn_relation_source, output_source, output_source_2, pred_domain_all_source[::-1], feat_all_source[::-1], attn_relation_target, output_target, output_target_2, pred_domain_all_target[::-1], feat_all_target[::-1] # reverse the order of feature list due to some multi-gpu issues\n'"
opts.py,0,"b'import argparse\nparser = argparse.ArgumentParser(description=""PyTorch implementation of Temporal Segment Networks"")\nparser.add_argument(\'class_file\', type=str, default=""classInd.txt"")\nparser.add_argument(\'modality\', type=str, choices=[\'RGB\', \'Flow\', \'RGBDiff\', \'RGBDiff2\', \'RGBDiffplus\'])\nparser.add_argument(\'train_source_list\', type=str)\nparser.add_argument(\'train_target_list\', type=str)\nparser.add_argument(\'val_list\', type=str)\n\n# ========================= Model Configs ==========================\nparser.add_argument(\'--arch\', type=str, default=""resnet101"")\nparser.add_argument(\'--pretrained\', type=str, default=""none"")\nparser.add_argument(\'--num_segments\', type=int, default=5)\nparser.add_argument(\'--val_segments\', type=int, default=-1)\nparser.add_argument(\'--add_fc\', default=1, type=int, metavar=\'M\',\n                    help=\'number of additional fc layers (excluding the last fc layer) (e.g. 0, 1, 2, ...)\')\nparser.add_argument(\'--fc_dim\', type=int, default=1024, help=\'dimension of added fc\')\nparser.add_argument(\'--baseline_type\', type=str, default=\'frame\',\n                    choices=[\'frame\', \'video\', \'tsn\'])\nparser.add_argument(\'--frame_aggregation\', type=str, default=\'avgpool\',\n                    choices=[\'avgpool\', \'rnn\', \'temconv\', \'trn\', \'trn-m\', \'none\'], help=\'aggregation of frame features (none if baseline_type is not video)\')\nparser.add_argument(\'--optimizer\', type=str, default=\'SGD\', choices=[\'SGD\', \'Adam\'])\nparser.add_argument(\'--use_opencv\', default=False, action=""store_true"",\n                    help=\'whether to use the opencv transformation\')\nparser.add_argument(\'--dropout_i\', \'--doi\', default=0.8, type=float,\n                    metavar=\'DOI\', help=\'dropout ratio for frame-level feature (default: 0.5)\')\nparser.add_argument(\'--dropout_v\', \'--dov\', default=0.8, type=float,\n                    metavar=\'DOV\', help=\'dropout ratio for video-level feature (default: 0.5)\')\nparser.add_argument(\'--loss_type\', type=str, default=""nll"",\n                    choices=[\'nll\'])\nparser.add_argument(\'--weighted_class_loss\', type=str, default=\'N\', choices=[\'Y\', \'N\'])\n\n#------ RNN ------\nparser.add_argument(\'--n_rnn\', default=1, type=int, metavar=\'M\',\n                    help=\'number of RNN layers (e.g. 0, 1, 2, ...)\')\nparser.add_argument(\'--rnn_cell\', type=str, default=\'LSTM\', choices=[\'LSTM\', \'GRU\'])\nparser.add_argument(\'--n_directions\', type=int, default=1, choices=[1, 2],\n                    help=\'(bi-) direction RNN\')\nparser.add_argument(\'--n_ts\', type=int, default=5, help=\'number of temporal segments\')\n\n# ========================= DA Configs ==========================\nparser.add_argument(\'--share_params\', type=str, default=\'Y\', choices=[\'Y\', \'N\'])\nparser.add_argument(\'--use_target\', type=str, default=\'none\', choices=[\'none\', \'Sv\', \'uSv\'],\n                    help=\'the method to use target data (not use | supervised | unsupervised)\')\nparser.add_argument(\'--dis_DA\', type=str, default=\'none\', choices=[\'none\', \'DAN\', \'JAN\', \'CORAL\'],\n                    help=\'discrepancy method for DA\')\nparser.add_argument(\'--adv_DA\', type=str, default=\'none\', choices=[\'none\', \'RevGrad\'],\n                    help=\'adversarial method for DA\')\nparser.add_argument(\'--use_bn\', type=str, default=\'none\', choices=[\'none\', \'AdaBN\', \'AutoDIAL\'], help=\'normalization-based methods\')\nparser.add_argument(\'--ens_DA\', type=str, default=\'none\', choices=[\'none\', \'MCD\'], help=\'ensembling-based methods\')\nparser.add_argument(\'--use_attn_frame\', type=str, default=\'none\', choices=[\'none\', \'TransAttn\', \'general\', \'DotProduct\'], help=\'attention-mechanism for frames only\')\nparser.add_argument(\'--use_attn\', type=str, default=\'none\', choices=[\'none\', \'TransAttn\', \'general\', \'DotProduct\'], help=\'attention-mechanism\')\nparser.add_argument(\'--n_attn\', type=int, default=1, help=\'number of discriminators for transferable attention\')\nparser.add_argument(\'--add_loss_DA\', type=str, default=\'none\', choices=[\'none\', \'target_entropy\', \'attentive_entropy\'],\n                    help=\'add more loss functions for DA\')\nparser.add_argument(\'--pred_normalize\', type=str, default=\'N\', choices=[\'Y\', \'N\'])\nparser.add_argument(\'--alpha\', default=1, type=float, metavar=\'M\',\n                    help=\'weighting for the discrepancy loss (use scheduler if < 0)\')\nparser.add_argument(\'--beta\', default=[1, 1, 1], type=float, nargs=""+"", metavar=\'M\',\n                    help=\'weighting for the adversarial loss (use scheduler if < 0; [relation-beta, video-beta, frame-beta])\')\nparser.add_argument(\'--gamma\', default=1, type=float, metavar=\'M\',\n                    help=\'weighting for the entropy loss\')\nparser.add_argument(\'--mu\', default=0, type=float, metavar=\'M\',\n                    help=\'weighting for ensembling loss (e.g. discrepancy)\')\nparser.add_argument(\'--weighted_class_loss_DA\', type=str, default=\'N\', choices=[\'Y\', \'N\'])\nparser.add_argument(\'--place_dis\', default=[\'Y\', \'Y\', \'N\'], type=str, nargs=""+"",\n                    metavar=\'N\', help=\'where to place the discrepancy loss (length = add_fc + 2)\')\nparser.add_argument(\'--place_adv\', default=[\'Y\', \'Y\', \'Y\'], type=str, nargs=""+"",\n                    metavar=\'N\', help=\'[video relation-based adv, video-based adv, frame-based adv]\')\n\n\n# ========================= Learning Configs ==========================\nparser.add_argument(\'--pretrain_source\', default=False, action=""store_true"", help=\'perform source-only training before DA\')\nparser.add_argument(\'--epochs\', default=100, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'-b\', \'--batch_size\', default=[32, 28, 64], type=int, nargs=""+"",\n                    metavar=\'N\', help=\'mini-batch size ([source, target, testing])\')\nparser.add_argument(\'--lr\', \'--learning_rate\', default=0.0001, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--lr_decay\', default=10, type=float, metavar=\'LRDecay\', help=\'decay factor for learning rate\')\nparser.add_argument(\'--lr_adaptive\', type=str, default=\'none\', choices=[\'none\', \'loss\', \'dann\'])\nparser.add_argument(\'--lr_steps\', default=[60, 100], type=float, nargs=""+"",\n                    metavar=\'LRSteps\', help=\'epochs to decay learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight_decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--clip_gradient\', \'--gd\', default=20, type=float,\n                    metavar=\'W\', help=\'gradient norm clipping (default: disabled)\')\nparser.add_argument(\'--no_partialbn\', \'--npb\', default=True, action=""store_true"")\nparser.add_argument(\'--copy_list\', default=[\'N\', \'Y\'], type=str, nargs=""+"",\n                    metavar=\'N\', help=\'duplicate data in case the dataset is relatively small ([copy source list, copy target list])\')\n\n# ========================= Monitor Configs ==========================\nparser.add_argument(\'--print_freq\', \'-pf\', default=10, type=int,\n                    metavar=\'N\', help=\'frequency for printing to text files (default: 10)\')\nparser.add_argument(\'--show_freq\', \'-sf\', default=10, type=int,\n                    metavar=\'N\', help=\'frequency for showing on the screen (default: 10)\')\nparser.add_argument(\'--eval_freq\', \'-ef\', default=1, type=int,\n                    metavar=\'N\', help=\'evaluation frequency (default: 5)\')\nparser.add_argument(\'--verbose\', default=False, action=""store_true"")\n\n# ========================= Runtime Configs ==========================\nparser.add_argument(\'-j\', \'--workers\', default=8, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--resume_hp\', default=False, action=""store_true"",\n                    help=\'whether to use the saved hyper-parameters\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--exp_path\', type=str, default="""", \n                    help=\'full path of the experiment folder\')\nparser.add_argument(\'--gpus\', nargs=\'+\', type=int, default=None)\nparser.add_argument(\'--flow_prefix\', default="""", type=str)\nparser.add_argument(\'--save_model\', default=False, action=""store_true"")\nparser.add_argument(\'--save_best_log\', default=""best.log"", type=str)\nparser.add_argument(\'--save_attention\', type=int, default=-1)\nparser.add_argument(\'--tensorboard\', dest=\'tensorboard\', action=\'store_true\')\n\n\n\n\n\n\n\n\n'"
test_models.py,14,"b'import argparse\nimport time\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim\nfrom torch.autograd import Variable\nfrom sklearn.metrics import confusion_matrix\n\nfrom dataset import TSNDataSet\nfrom models import VideoModel\nfrom utils.utils import plot_confusion_matrix\n\nfrom colorama import init\nfrom colorama import Fore, Back, Style\nfrom tqdm import tqdm\nfrom time import sleep\n\ninit(autoreset=True)\n\n# options\nparser = argparse.ArgumentParser(description=""Standard video-level testing"")\nparser.add_argument(\'class_file\', type=str, default=""classInd.txt"")\nparser.add_argument(\'modality\', type=str, choices=[\'RGB\', \'Flow\', \'RGBDiff\', \'RGBDiff2\', \'RGBDiffplus\'])\nparser.add_argument(\'test_list\', type=str)\nparser.add_argument(\'weights\', type=str)\n\n# ========================= Model Configs ==========================\nparser.add_argument(\'--arch\', type=str, default=""resnet101"")\nparser.add_argument(\'--test_segments\', type=int, default=5)\nparser.add_argument(\'--add_fc\', default=1, type=int, metavar=\'M\', help=\'number of additional fc layers (excluding the last fc layer) (e.g. 0, 1, 2, ...)\')\nparser.add_argument(\'--fc_dim\', type=int, default=512, help=\'dimension of added fc\')\nparser.add_argument(\'--baseline_type\', type=str, default=\'frame\', choices=[\'frame\', \'video\', \'tsn\'])\nparser.add_argument(\'--frame_aggregation\', type=str, default=\'avgpool\', choices=[\'avgpool\', \'rnn\', \'temconv\', \'trn-m\', \'none\'], help=\'aggregation of frame features (none if baseline_type is not video)\')\nparser.add_argument(\'--dropout_i\', type=float, default=0)\nparser.add_argument(\'--dropout_v\', type=float, default=0)\n\n#------ RNN ------\nparser.add_argument(\'--n_rnn\', default=1, type=int, metavar=\'M\',\n                    help=\'number of RNN layers (e.g. 0, 1, 2, ...)\')\nparser.add_argument(\'--rnn_cell\', type=str, default=\'LSTM\', choices=[\'LSTM\', \'GRU\'])\nparser.add_argument(\'--n_directions\', type=int, default=1, choices=[1, 2],\n                    help=\'(bi-) direction RNN\')\nparser.add_argument(\'--n_ts\', type=int, default=5, help=\'number of temporal segments\')\n\n# ========================= DA Configs ==========================\nparser.add_argument(\'--share_params\', type=str, default=\'Y\', choices=[\'Y\', \'N\'])\nparser.add_argument(\'--use_bn\', type=str, default=\'none\', choices=[\'none\', \'AdaBN\', \'AutoDIAL\'])\nparser.add_argument(\'--use_attn_frame\', type=str, default=\'none\', choices=[\'none\', \'TransAttn\', \'general\', \'DotProduct\'], help=\'attention-mechanism for frames only\')\nparser.add_argument(\'--use_attn\', type=str, default=\'none\', choices=[\'none\', \'TransAttn\', \'general\', \'DotProduct\'], help=\'attention-mechanism\')\nparser.add_argument(\'--n_attn\', type=int, default=1, help=\'number of discriminators for transferable attention\')\n\n# ========================= Monitor Configs ==========================\nparser.add_argument(\'--top\', default=[1, 3, 5], nargs=\'+\', type=int, help=\'show top-N categories\')\nparser.add_argument(\'--verbose\', default=False, action=""store_true"")\n\n# ========================= Runtime Configs ==========================\nparser.add_argument(\'--save_confusion\', type=str, default=None)\nparser.add_argument(\'--save_scores\', type=str, default=None)\nparser.add_argument(\'--save_attention\', type=str, default=None)\nparser.add_argument(\'--max_num\', type=int, default=-1, help=\'number of videos to test\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\', help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--bS\', default=2, help=\'batch size\', type=int, required=False)\nparser.add_argument(\'--gpus\', nargs=\'+\', type=int, default=None)\nparser.add_argument(\'--flow_prefix\', type=str, default=\'\')\n\nargs = parser.parse_args()\n\nclass_names = [line.strip().split(\' \', 1)[1] for line in open(args.class_file)]\nnum_class = len(class_names)\n\n#=== Load the network ===#\nprint(Fore.CYAN + \'preparing the model......\')\nnet = VideoModel(num_class, args.baseline_type, args.frame_aggregation, args.modality,\n\t\ttrain_segments=args.test_segments if args.baseline_type == \'video\' else 1, val_segments=args.test_segments if args.baseline_type == \'video\' else 1,\n\t\tbase_model=args.arch, add_fc=args.add_fc, fc_dim=args.fc_dim, share_params=args.share_params,\n\t\tdropout_i=args.dropout_i, dropout_v=args.dropout_v, use_bn=args.use_bn, partial_bn=False,\n\t\tn_rnn=args.n_rnn, rnn_cell=args.rnn_cell, n_directions=args.n_directions, n_ts=args.n_ts,\n\t\tuse_attn=args.use_attn, n_attn=args.n_attn, use_attn_frame=args.use_attn_frame,\n\t\tverbose=args.verbose)\n\ncheckpoint = torch.load(args.weights)\n\nprint(""model epoch {} prec@1: {}"".format(checkpoint[\'epoch\'], checkpoint[\'prec1\']))\n\nbase_dict = {\'.\'.join(k.split(\'.\')[1:]): v for k,v in list(checkpoint[\'state_dict\'].items())}\nnet.load_state_dict(base_dict)\n\n#=== Data loading ===#\nprint(Fore.CYAN + \'loading data......\')\n\ndata_length = 1 if args.modality == ""RGB"" else 5\nnum_test = sum(1 for i in open(args.test_list))\n\ndata_set = TSNDataSet("""", args.test_list, num_dataload=num_test, num_segments=args.test_segments,\n\tnew_length=data_length, modality=args.modality,\n\timage_tmpl=""img_{:05d}.t7"" if args.modality in [\'RGB\', \'RGBDiff\', \'RGBDiff2\', \'RGBDiffplus\'] else args.flow_prefix+""{}_{:05d}.t7"",\n\ttest_mode=True,\n\t)\ndata_loader = torch.utils.data.DataLoader(data_set, batch_size=args.bS, shuffle=False, num_workers=args.workers, pin_memory=True)\n\ndata_gen = tqdm(data_loader)\n\n#--- GPU processing ---#\nnet = torch.nn.DataParallel(net.cuda())\nnet.eval()\n\noutput = []\nattn_values = torch.Tensor()\n\n#############################################################\ndef eval_video(video_data):\n\ti, data, label = video_data\n\n\tdata = data.cuda()\n\tlabel = label.cuda(non_blocking=True) # pytorch 0.4.X\n\n\tnum_crop = 1 \n\n\t# e.g.\n\t# data.shape = [1,sample # x 2048]\n\t# data.view(-1, length, data.size(2), data.size(3)).shape = [sample #,2048]\n\n\twith torch.no_grad():\n\t\t_, _, _, _, _, attn, out, _, _, _ = net(data, data, [0, 0, 0], 0, is_train=False, reverse=False)\n\t\tout = nn.Softmax(dim=1)(out).topk(max(args.top))\n\t\tprob = out[0].data.cpu().numpy().copy() # rst.shape = [sample #, top class #]\n\t\tpred_labels = out[1].data.cpu().numpy().copy() # rst.shape = [sample #, top class #]\n\n\t\tif args.baseline_type == \'video\':\n\t\t\tprob_video = prob.reshape((data.size(0), num_crop, max(args.top))).mean(axis=1).reshape(\n\t\t\t(data.size(0), max(args.top)))\n\t\telse:\n\t\t\tprob_video = prob.reshape((data.size(0), num_crop, args.test_segments, max(args.top))).mean(axis=1).reshape(\n\t\t\t(data.size(0), args.test_segments, max(args.top)))\n\t\t\tprob_video = np.mean(prob_video, axis=1)\n\n\n\t\treturn i, prob_video, pred_labels, label.cpu().numpy(), attn.cpu()\n#############################################################\n\nproc_start_time = time.time()\nmax_num = args.max_num if args.max_num > 0 else len(data_loader.dataset)\n\ncount_correct_topK = [0 for i in range(len(args.top))]\ncount_total = 0\nvideo_pred = [[] for i in range(max(args.top))]\nvideo_labels = []\n\n#=== Testing ===#\nprint(Fore.CYAN + \'start testing......\')\nfor i, (data, label) in enumerate(data_gen):\n\tdata_size_ori = data.size() # original shape\n\tif data_size_ori[0] < args.bS:\n\t\tdata_dummy = torch.zeros(args.bS - data_size_ori[0], data_size_ori[1], data_size_ori[2])\n\t\tdata = torch.cat((data, data_dummy))\n\t\tlabel_dummy = torch.zeros(args.bS - data_size_ori[0]).long()\n\t\tlabel = torch.cat((label, label_dummy))\n\n\tif i >= max_num:\n\t\tbreak\n\trst = eval_video((i, data, label))\n\n\t# remove the dummy part\n\tprobs = rst[1][:data_size_ori[0]] # rst[1].shape = [sample #, top class #]\n\tpreds = rst[2][:data_size_ori[0]]\n\tlabels = rst[3][:data_size_ori[0]]\n\tattn = rst[4][:data_size_ori[0]]\n\n\tattn_values = torch.cat((attn_values, attn))  # save the attention values\n\n\t# accumulate\n\tfor j in range(len(args.top)):\n\t\tfor k in range(args.top[j]):\n\t\t\tcount_correct_topK[j] += ((preds[:,k] == labels) * 1).sum()\n\tcount_total += (preds[:,0].shape)[0]\n\tacc_topK = [float(count_correct_topK[j]) / float(count_total) for j in range(len(args.top))]\n\n\tfor k in range(max(args.top)):\n\t\tvideo_pred[k] += preds[:,k].tolist() # save the top-K prediction\n\n\tvideo_labels += labels.tolist()\n\n\tcnt_time = time.time() - proc_start_time\n\tline_print = \'                                                            \' # leave a large space for the tqdm progess bar\n\tfor j in range(len(args.top)):\n\t\tline_print += \'Pred@%d %f, \' % (args.top[j], acc_topK[j])\n\n\tline_print += \'average %f sec/video\\r\' % (float(cnt_time) / (i + 1) / args.bS)\n\tdata_gen.set_description(line_print)\n\nif args.save_attention:\n\tnp.savetxt(args.save_attention+\'.txt\', attn_values.cpu().numpy(), fmt=""%s"")\n\ncf = [confusion_matrix(video_labels, video_pred[k], labels=list(range(num_class))) for k in range(max(args.top))]\n\nplot_confusion_matrix(args.save_confusion+\'.png\', cf[0], classes=class_names, normalize=True,\n\t\t\t\t\t  title=\'Normalized confusion matrix\')\n\n#--- overall accuracy ---#\ncls_cnt = cf[0].sum(axis=1)\ncls_hit = np.array([np.diag(cf[i]) for i in range(max(args.top))])\ncls_acc_topK = [cls_hit[:j].sum(axis=0) / cls_cnt for j in args.top]\n\nif args.verbose:\n\tfor i in range(len(cls_acc_topK[0])):\n\t\tline_print = \'\'\n\t\tfor j in range(len(args.top)):\n\t\t\tline_print += str(cls_acc_topK[j][i]) + \' \'\n\t\tprint(line_print)\n\nfinal_line = \'\'\nfor j in args.top:\n\tfinal_line += Fore.YELLOW + \'Pred@{:d} {:.02f}% \'.format(j, np.sum(cls_hit[:j].sum(axis=0)) / np.sum(cls_cnt) * 100)\nprint(final_line)\n\nif args.save_confusion:\n\tclass_acc_file = open(args.save_confusion + \'-top\' + str(args.top) + \'.txt\', \'w\')\n\n\tfor i in range(len(cls_acc_topK[0])):\n\t\tline_print = \'\'\n\t\tfor j in range(len(args.top)):\n\t\t\tline_print += str(cls_acc_topK[j][i]) + \' \'\n\t\tclass_acc_file.write(line_print + \'\\n\')\n\n\tclass_acc_file.close()\n\n\nif args.save_scores is not None: \n\t# reorder before saving\n\tname_list = [x.strip().split()[0] for x in open(args.test_list)]\n\n\torder_dict = {e:i for i, e in enumerate(sorted(name_list))}\n\n\treorder_output = [None] * len(output)\n\treorder_label = [None] * len(output)\n\n\tfor i in range(len(output)):\n\t\tidx = order_dict[name_list[i]]\n\t\treorder_output[idx] = output[i]\n\t\treorder_label[idx] = video_labels[i]\n\n\tnp.savez(args.save_scores, scores=reorder_output, labels=reorder_label)\n\t'"
dataset_preparation/C3D_model.py,1,"b'import torch.nn as nn\n\n\nclass C3D(nn.Module):\n    """"""\n    The C3D network as described in [1].\n    """"""\n\n    def __init__(self):\n        super(C3D, self).__init__()\n\n        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n\n        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n\n        self.conv3a = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.conv3b = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n\n        self.conv4a = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.conv4b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n\n        self.conv5a = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.conv5b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))\n\n        self.fc6 = nn.Linear(8192, 4096)\n        self.fc7 = nn.Linear(4096, 4096)\n        self.fc8 = nn.Linear(4096, 487)\n\n        self.dropout = nn.Dropout(p=0.5)\n\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n\n        h = self.relu(self.conv1(x))\n        h = self.pool1(h)\n\n        h = self.relu(self.conv2(h))\n        h = self.pool2(h)\n\n        h = self.relu(self.conv3a(h))\n        h = self.relu(self.conv3b(h))\n        h = self.pool3(h)\n\n        h = self.relu(self.conv4a(h))\n        h = self.relu(self.conv4b(h))\n        h = self.pool4(h)\n\n        h = self.relu(self.conv5a(h))\n        h = self.relu(self.conv5b(h))\n        h = self.pool5(h)\n\n        h = h.view(-1, 8192)\n        h = self.relu(self.fc6(h))\n        h = self.dropout(h)\n        h = self.relu(self.fc7(h))\n        h = self.dropout(h)\n\n        logits = self.fc8(h)\n        probs = self.softmax(logits)\n\n        return probs\n\n""""""\nReferences\n----------\n[1] Tran, Du, et al. ""Learning spatiotemporal features with 3d convolutional networks."" \nProceedings of the IEEE international conference on computer vision. 2015.\n""""""'"
dataset_preparation/dataset2split.py,0,"b'# split the dataset into train/val sets (randomly choose files according to the split ratio)\nimport argparse\nimport os\nimport random\nimport shutil\n\nparser = argparse.ArgumentParser(description=\'Split the dataset\')\nparser.add_argument(\'--data_path\', default=\'data/\', help=\'data path\', type=str, required=False)\nparser.add_argument(\'--folder_in\', default=\'folder_in/\', help=\'input folder\', type=str, required=False)\nparser.add_argument(\'--modality\', type=str, choices=[\'RGB\', \'Flow\'])\nparser.add_argument(\'--folder_out_1\', default=\'folder_out_1/\', help=\'1st output folder\', type=str, required=False)\nparser.add_argument(\'--folder_out_2\', default=\'folder_out_2/\', help=\'2nd output folder\', type=str, required=False)\nparser.add_argument(\'--input_type\', type=str, default=\'video\', choices=[\'video\', \'frames\'], help=\'input types for videos\')\nparser.add_argument(\'--split_ratio\', type=float, required=False, default=0.7, help=\'ratio of train/val for each class\')\nparser.add_argument(\'--split_feat\', default=\'N\', help=\'split the feature vectors as well\', type=str, required=False)\nargs = parser.parse_args()\n\npath_input = args.data_path + args.folder_in # input videos\npath_output_1 = args.data_path + args.folder_out_1\npath_output_2 = args.data_path + args.folder_out_2\n\n# create folders for split videos (or frames) & features\nif not os.path.isdir(path_output_1 + args.modality + \'/\'):\n\tprint(\'create\', path_output_1 + args.modality + \'/\')\n\tos.makedirs(path_output_1 + args.modality + \'/\')\n\nif not os.path.isdir(path_output_2 + args.modality + \'/\'):\n\tprint(\'create\', path_output_2 + args.modality + \'/\')\n\tos.makedirs(path_output_2 + args.modality + \'/\')\n\nif args.split_feat == \'Y\':\n\tif not os.path.isdir(path_output_1 + args.modality + \'-feature/\'):\n\t\tprint(\'create\', path_output_1 + args.modality + \'-feature/\')\n\t\tos.makedirs(path_output_1 + args.modality + \'-feature/\')\n\n\tif not os.path.isdir(path_output_2 + args.modality + \'-feature/\'):\n\t\tprint(\'create\', path_output_2 + args.modality + \'-feature/\')\n\t\tos.makedirs(path_output_2 + args.modality + \'-feature/\')\n\n################### Main Function ###################\ndef copy_files(class_name, files, path):\n\t#--- path ---#\n\tdestination = ""{}{}/{}/"".format(path, args.modality, class_name) # destination location\n\tdestination_feature = ""{}{}-feature/"".format(path, args.modality)  # destination location for features\n\t# remove old images\n\tif os.path.exists(destination):\n\t\tprint(""deleted old {}"".format(destination))\n\t\tshutil.rmtree(destination)\n\tos.makedirs(destination)\n\n\t#--- copy files/folders ---#\n\tfor file in files:\n\t\t# frames/video\n\t\tpath_origin = ""{}{}/{}/{}"".format(path_input, args.modality, class_name, file) # origin location\n\t\tif args.input_type == \'video\':\n\t\t\tshutil.copyfile(path_origin, destination + file)\n\t\telif args.input_type == \'frames\':\n\t\t\tshutil.copytree(path_origin, destination + file)\n\n\t\t# features\n\t\tif args.split_feat == \'Y\':\n\t\t\tfile_name = file.split(\'.\')[0]\n\t\t\tpath_origin_feature = ""{}{}-feature/{}"".format(path_input, args.modality, file_name)  # origin location for features\n\t\t\tshutil.copytree(path_origin_feature, destination_feature + file_name)\n\n################### Main Program ###################\nlist_class = os.listdir(path_input + args.modality)\nlist_class.sort()\n\nfor class_dir in list_class:\n\tfiles = os.listdir(""{}{}/{}"".format(path_input, args.modality, class_dir))  # all the files in this class\n\tnum_files = len(files)  # file #\n\n\tfiles = set(files)  # convert from list to set\n\n\tif args.split_ratio < 0: # split the training/validation sets based on the text file\n\t\tif args.folder_in == \'olympic/\':\n\t\t\tfiles_set1 = [line.strip() + \'.avi\' for line in open(path_input + \'train/\' + class_dir + \'.txt\')]\n\t\t\tfiles_set2 = [line.strip() + \'.avi\' for line in open(path_input + \'test/\' + class_dir + \'.txt\')]\n\t\t\tnum_files_1 = len(files_set1)\n\t\t\tnum_files_2 = len(files_set2)\n\n\t\telse:\n\t\t\traise ValueError(\'The dataset is not listed!!\')\n\n\telse:\n\t\tnum_files_1 = max(int(num_files*args.split_ratio),1)\n\t\tnum_files_2 = num_files - num_files_1\n\t\tfiles_set1 = set(random.sample(files, num_files_1))\n\t\tfiles_set2 = files - files_set1\n\n\tprint(class_dir + "": {}"".format(num_files) + "" --> {}/{}"".format(num_files_1, num_files_2))\n\n\tcopy_files(class_dir, files_set1, path_output_1)\n\tcopy_files(class_dir, files_set2, path_output_2)\n'"
dataset_preparation/list_ucf_hmdb_full2DA.py,0,"b""# generate the file list from video dataset\nimport argparse\nimport os\nfrom colorama import init\nfrom colorama import Fore, Back, Style\ninit(autoreset=True)\n\n###### Flags ######\nparser = argparse.ArgumentParser(description='select DA classes for UCF/HMDB')\nparser.add_argument('dataset', type=str, help='ucf101 | hmdb51')\nparser.add_argument('modality', type=str, help='rgb | flow')\nparser.add_argument('--sp', default=1, help='split #', type=int, required=False)\nparser.add_argument('--data_path', default='data/', help='data path', type=str, required=False)\nparser.add_argument('--frame_in', default='RGB-feature', help='video frame/feature folder name', type=str, required=False)\nparser.add_argument('--method_read', default='video', type=str, choices=['video','frame'], help='approach to load data')\nparser.add_argument('--class_file', type=str, default='class.txt', help='process the classes only in the class_file', required=True)\nparser.add_argument('--suffix', default=None, help='additional string for filename', type=str, required=False)\n\nargs = parser.parse_args()\n\n###### Function for list generation ######\ndef gen_list_DA(path_input_list, class_indices_DA, class_names_DA, list_type):\n\tpath_output = args.data_path + args.dataset + '/' + 'list_' + args.dataset + '_' + list_type + args.suffix + '.txt'\n\tfile_write = open(path_output,'w')\n\tclass_indices_DA_unique = list(set(class_indices_DA))\n\tcount_video = [0 for i in range(len(class_indices_DA_unique))]\n\tfor line in open(path_input_list):\n\t\t# 1. parse [path, length, class_id]\n\t\tpath_video, len_video, id_video = line.strip().split(' ')\n\t\tpath_dataset, frame_in, name_video = path_video.rsplit('/', 2)\n\n\t\t# print(path_video, len_video, id_video)\n\t\t# print(path_dataset, frame_in, name_video)\n\t\t# exit()\n\n\t\tcheck_class = False\n\n\t\tif args.dataset == 'hmdb51':\n\t\t\tname_video_short = name_video.rsplit('_',6)[0] # remove the suffix\n\t\t\tname_str = name_video_short.rsplit('_',2)[-2:] # remove the prefix\n\t\t\tclass_str = '_'.join(name_str) # join the strings\n\t\t\n\t\t\tif class_str.split('_')[1] in class_names_DA:\n\t\t\t\tcheck_class = True\n\t\t\t\tid_video_DA = class_indices_DA[class_names_DA.index(class_str.split('_')[1])]\n\t\t\telif class_str in class_names_DA:\n\t\t\t\tcheck_class = True\n\t\t\t\tid_video_DA = class_indices_DA[class_names_DA.index(class_str)]\n\t\t\n\t\telif args.dataset == 'ucf101':\n\t\t\tclass_str = name_video.split('_')[1]\n\t\t\tif class_str in class_names_DA:\n\t\t\t\tcheck_class = True\n\t\t\t\tid_video_DA = class_indices_DA[class_names_DA.index(class_str)]\n\n\t\tif check_class:\n\t\t\t# 2. rearrange and write a new line\n\t\t\tcount_video[class_indices_DA_unique.index(id_video_DA)] += 1\n\n\t\t\tif args.method_read == 'frame':\n\t\t\t\tlen_video = str(len(os.listdir(path_dataset + '/' + args.frame_in  + '/' + name_video)))\n\n\t\t\tline_new = path_dataset + '/' + args.frame_in  + '/' + name_video + ' ' + len_video + ' ' + str(id_video_DA) + '\\n'\n\t\t\tfile_write.write(line_new)\n\n\tfile_write.close()\n\n\t# print the video # in each class\n\tprint(path_output)\n\tfor j in range(len(count_video)):\n\t\tprint(class_indices_DA_unique[j], count_video[j])\n\n###### data path ######\nprint(Fore.GREEN + 'dataset:', args.dataset)\npath_split_folder = args.data_path + args.dataset + '/' + args.dataset + '_splits/'\npath_train_list = path_split_folder + args.dataset + '_' + args.modality + '_train_split_' + str(args.sp) + '.txt'\npath_val_list = path_split_folder + args.dataset + '_' + args.modality + '_val_split_' + str(args.sp) + '.txt'\n\n###### Load the class list ######\nclass_indices = [int(line.strip().split(' ', 1)[0]) for line in open(args.class_file)]\nclass_names = [line.strip().split(' ', 1)[1] for line in open(args.class_file)]\n# print(class_indices)\n# print(class_names)\n# exit()\n\n###### Generate lists for DA ######\nprint(Fore.CYAN + 'generating lists......')\ngen_list_DA(path_train_list, class_indices, class_names, 'train')\ngen_list_DA(path_val_list, class_indices, class_names, 'val')\n"""
dataset_preparation/video2feature.py,16,"b""import argparse\r\nfrom colorama import init\r\nfrom colorama import Fore, Back, Style\r\nimport os\r\nimport imageio\r\nimport numpy as np\r\nfrom multiprocessing.dummy import Pool as ThreadPool\r\n\r\nimport time\r\n\r\n# for extracting feature vectors\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.backends.cudnn as cudnn\r\nimport torchvision.models as models\r\nimport torchvision.transforms as transforms\r\nfrom torch.autograd import Variable\r\nfrom PIL import Image\r\n\r\nimageio.plugins.ffmpeg.download()\r\ninit(autoreset=True)\r\n\r\n###### Flags ######\r\nparser = argparse.ArgumentParser(description='Dataset Preparation')\r\nparser.add_argument('--data_path', type=str, required=False, default='', help='source path')\r\nparser.add_argument('--video_in', type=str, required=False, default='RGB', help='name of input video dataset')\r\nparser.add_argument('--feature_in', type=str, required=False, default='RGB-feature', help='name of output frame dataset')\r\nparser.add_argument('--input_type', type=str, default='video', choices=['video', 'frames'], help='input types for videos')\r\nparser.add_argument('--structure', type=str, default='tsn', choices=['tsn', 'imagenet'], help='data structure of output frames')\r\nparser.add_argument('--base_model', type=str, required=False, default='resnet101', choices=['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'c3d'])\r\nparser.add_argument('--pretrain_weight', type=str, required=False, default='', help='model weight file path')\r\nparser.add_argument('--num_thread', type=int, required=False, default=-1, help='number of threads for multiprocessing')\r\nparser.add_argument('--batch_size', type=int, required=False, default=1, help='batch size')\r\nparser.add_argument('--start_class', type=int, required=False, default=1, help='the starting class id (start from 1)')\r\nparser.add_argument('--end_class', type=int, required=False, default=-1, help='the end class id')\r\nparser.add_argument('--class_file', type=str, default='class.txt', help='process the classes only in the class_file')\r\n\r\nargs = parser.parse_args()\r\n\r\n# Create thread pool\r\nmax_thread = 8 # there are some issues if too many threads\r\nnum_thread = args.num_thread if args.num_thread>0 and args.num_thread<=max_thread else max_thread\r\nprint(Fore.CYAN + 'thread #:', num_thread)\r\npool = ThreadPool(num_thread)\r\n\r\n###### data path ######\r\npath_input = args.data_path + args.video_in + '/'\r\nfeature_in_type = '.t7'\r\n\r\n#--- create dataset folders\r\n# root folder\r\npath_output = args.data_path + args.feature_in + '_' + args.base_model + '/'\r\nif args.structure != 'tsn':\r\n\tpath_output = args.data_path + args.feature_in + '-' + args.structure + '/'\r\nif not os.path.isdir(path_output):\r\n\tos.makedirs(path_output)\r\n\r\n###### set up the model ######\r\n# Load the pretrained model\r\nprint(Fore.GREEN + 'Pre-trained model:', args.base_model)\r\n\r\nif args.base_model == 'c3d':\r\n\tfrom C3D_model import C3D\r\n\tc3d_clip_size = 16\r\n\tmodel = C3D()\r\n\tmodel.load_state_dict(torch.load(args.pretrain_weight))\r\n    \r\n\tlist_model = list(model.children())\r\n\tlist_conv = list_model[:-6]\r\n\tlist_fc = list_model[-6:-4]\r\n\textractor_conv = nn.Sequential(*list_conv)\r\n\textractor_fc = nn.Sequential(*list_fc)\r\n\r\n\t# multi-gpu\r\n\textractor_conv = torch.nn.DataParallel(extractor_conv.cuda())\r\n\textractor_conv.eval()\r\n\textractor_fc = torch.nn.DataParallel(extractor_fc.cuda())\r\n\textractor_fc.eval()\r\n\r\nelse:\r\n\tmodel = getattr(models, args.base_model)(pretrained=True)\r\n\t# remove the last layer\r\n\tfeature_map = list(model.children())\r\n\tfeature_map.pop()\r\n\textractor = nn.Sequential(*feature_map)\r\n\t# multi-gpu\r\n\textractor = torch.nn.DataParallel(extractor.cuda())\r\n\textractor.eval()\r\n\r\ncudnn.benchmark = True\r\n\r\n#--- data pre-processing\r\nif args.base_model == 'c3d':\r\n\tdata_transform = transforms.Compose([\r\n\t\ttransforms.Resize(112),\r\n\t\ttransforms.CenterCrop(112),\r\n\t\ttransforms.ToTensor(),\r\n\t\t])\r\nelse:\r\n\tdata_transform = transforms.Compose([\r\n\t\ttransforms.Resize(224),\r\n\t\ttransforms.CenterCrop(224),\r\n\t\ttransforms.ToTensor(),\r\n\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\r\n\t\t])\r\n\r\n# read the class files\r\nif args.class_file == 'none':\r\n\tclass_names_proc = ['unlabeled']\r\nelse:\r\n\tclass_names_proc = [line.strip().split(' ', 1)[1] for line in open(args.class_file)]\r\n\r\n################### Main Function ###################\r\ndef im2tensor(im):\r\n\tim = Image.fromarray(im) # convert numpy array to PIL image\r\n\tt_im = data_transform(im) # Create a PyTorch Variable with the transformed image\r\n\treturn t_im\r\n\r\ndef extract_frame_feature_batch(list_tensor):\r\n\twith torch.no_grad():\r\n\t\tbatch_tensor = torch.stack(list_tensor)\r\n\r\n\t\tif args.base_model == 'c3d':\r\n\t\t\tbatch_tensor = convert_c3d_tensor_batch(batch_tensor) # e.g. 113x3x16x112x112\r\n\t\t\tbatch_tensor = Variable(batch_tensor).cuda() # Create a PyTorch Variable\r\n\t\t\tfeatures_conv = extractor_conv(batch_tensor) # e.g. 113x512x1x4x4\r\n\t\t\tfeatures_conv = features_conv.view(features_conv.size(0),-1) # e.g. 113x8192\r\n\t\t\tfeatures = extractor_fc(features_conv)\r\n\t\telse:\r\n\t\t\tbatch_tensor = Variable(batch_tensor).cuda() # Create a PyTorch Variable\r\n\t\t\tfeatures = extractor(batch_tensor)\r\n\t\tfeatures = features.view(features.size(0), -1).cpu()\r\n\t\treturn features\r\n\r\ndef convert_c3d_tensor_batch(batch_tensor): # e.g. 30x3x112x112 --> 15x3x16x112x112\r\n\tbatch_tensor_c3d = torch.Tensor()\r\n\tfor b in range(batch_tensor.size(0)-c3d_clip_size+1):\r\n\t\ttensor_c3d = batch_tensor[b:b+c3d_clip_size,:,:,:]\r\n\t\ttensor_c3d = torch.transpose(tensor_c3d,0,1).unsqueeze(0)\r\n\t\tbatch_tensor_c3d = torch.cat((batch_tensor_c3d, tensor_c3d))\r\n\r\n\tbatch_tensor_c3d = batch_tensor_c3d*255\r\n\treturn batch_tensor_c3d\r\n\r\ndef extract_features(video_file):\r\n\tprint(video_file)\r\n\tvideo_name = os.path.splitext(video_file)[0]\r\n\tif args.structure == 'tsn':  # create the video folder if the data structure is TSN\r\n\t\tif not os.path.isdir(path_output + video_name + '/'):\r\n\t\t\tos.makedirs(path_output + video_name + '/')\r\n\r\n\tnum_exist_files = len(os.listdir(path_output + video_name + '/'))\r\n\r\n\tframes_tensor = []\r\n\t# print(class_name)\r\n\tif args.input_type == 'video':\r\n\t\treader = imageio.get_reader(path_input + class_name + '/' + video_file)\r\n\r\n\t\t#--- collect list of frame tensors\r\n\t\ttry:\r\n\t\t\tfor t, im in enumerate(reader):\r\n\t\t\t\tif np.sum(im.shape) != 0:\r\n\t\t\t\t\tid_frame = t+1\r\n\t\t\t\t\tframes_tensor.append(im2tensor(im))  # include data pre-processing\r\n\t\texcept RuntimeError:\r\n\t\t\tprint(Back.RED + 'Could not read frame', id_frame+1, 'from', video_file)\r\n\telif args.input_type == 'frames':\r\n\t\tlist_frames = os.listdir(path_input + class_name + '/' + video_file)\r\n\t\tlist_frames.sort()\r\n\r\n\t\t# --- collect list of frame tensors\r\n\t\ttry:\r\n\t\t\tfor t in range(len(list_frames)):\r\n\t\t\t\tim = imageio.imread(path_input + class_name + '/' + video_file + '/' + list_frames[t])\r\n\t\t\t\tif np.sum(im.shape) != 0:\r\n\t\t\t\t\tid_frame = t+1\r\n\t\t\t\t\tframes_tensor.append(im2tensor(im))  # include data pre-processing\r\n\t\texcept RuntimeError:\r\n\t\t\tprint(Back.RED + 'Could not read frame', id_frame+1, 'from', video_file)\r\n\r\n\r\n\t#--- divide the list into two parts: major (can de divided by batch size) & the rest (will add dummy tensors)\r\n\tnum_frames = len(frames_tensor)\r\n\tif num_frames == num_exist_files: # skip if the features are already saved\r\n\t\treturn\r\n\r\n\tnum_major = num_frames//args.batch_size*args.batch_size\r\n\tnum_rest = num_frames - num_major\r\n\r\n\t# add dummy tensor to make total size == batch_size*N\r\n\tnum_dummy = args.batch_size - num_rest\r\n\tfor i in range(num_dummy):\r\n\t\tframes_tensor.append(torch.zeros_like(frames_tensor[0]))\r\n\r\n\t#--- extract video features\r\n\tfeatures = torch.Tensor()\r\n\r\n\tfor t in range(0, num_frames+num_dummy, args.batch_size):\r\n\t\tframes_batch = frames_tensor[t:t+args.batch_size]\r\n\t\tfeatures_batch = extract_frame_feature_batch(frames_batch)\r\n\t\tfeatures = torch.cat((features,features_batch))\r\n\r\n\tfeatures = features[:num_frames] # remove the dummy part\r\n\r\n\t#--- save the frame-level feature vectors to files\r\n\tfor t in range(features.size(0)):\r\n\t\tid_frame = t+1\r\n\t\tid_frame_name = str(id_frame).zfill(5)\r\n\t\tif args.structure == 'tsn':\r\n\t\t\tfilename = path_output + video_name + '/' + 'img_' + id_frame_name + feature_in_type\r\n\t\telif args.structure == 'imagenet':\r\n\t\t\tfilename = path_output + class_name + '/' + video_name + '_' + id_frame_name + feature_in_type\r\n\t\telse:\r\n\t\t\traise NameError(Back.RED + 'not valid data structure')\r\n\r\n\t\tif not os.path.exists(filename):\r\n\t\t\ttorch.save(features[t].clone(), filename) # if no clone(), the size of features[t] will be the same as features\r\n\r\n################### Main Program ###################\r\n# parse the classes\r\nlist_class = os.listdir(path_input)\r\nlist_class.sort()\r\n\r\n# for i in range(len(list_class)):\r\n# \tprint(i, list_class[i])\r\n# exit()\r\n\r\nid_class_start = args.start_class-1\r\nid_class_end = len(list_class) if args.end_class <= 0 else args.end_class\r\nstart = time.time()\r\n\r\nfor i in range(id_class_start, id_class_end):\r\n\tstart_class = time.time()\r\n\tclass_name = list_class[i]\r\n\tif class_name in class_names_proc:\r\n\t\tprint(Fore.YELLOW + 'class ' + str(i+1) + ': ' + class_name)\r\n\r\n\t\tif args.structure == 'imagenet': # create the class folder if the data structure is ImageNet\r\n\t\t\tif not os.path.isdir(path_output + class_name + '/'):\r\n\t\t\t\tos.makedirs(path_output + class_name + '/')\r\n\r\n\t\tlist_video = os.listdir(path_input + class_name + '/')\r\n\t\tlist_video.sort()\r\n\r\n\t\tpool.map(extract_features, list_video, chunksize=1)\r\n\r\n\t\tend_class = time.time()\r\n\t\tprint('Elapsed time for ' + class_name + ': ' + str(end_class-start_class))\r\n\telse:\r\n\t\tprint(Fore.RED + class_name + ' is not selected !!')\r\n\r\nend = time.time()\r\nprint('Total elapsed time: ' + str(end-start))\r\nprint(Fore.GREEN + 'All the features are generated for ' + args.video_in)\r\n"""
dataset_preparation/video_dataset2list.py,0,"b'# generate the file list from video dataset (TODO: update the code w/ DA_setting)\r\nimport argparse\r\nimport os\r\n# import imageio\r\nimport numpy as np\r\nimport random\r\nimport cv2\r\nfrom colorama import init\r\nfrom colorama import Fore, Back, Style\r\ninit(autoreset=True)\r\n\r\n###### Flags ######\r\nparser = argparse.ArgumentParser(description=\'Generate train/val splits from dataset \')\r\nparser.add_argument(\'dataset\', type=str, help=\'ucf101 | hmdb51 | ps_train | ps_val | ra_train | ra_val | ps_unlabeled\')\r\nparser.add_argument(\'--class_select\', action=""store_true"", help=\'select some classes only\')\r\nparser.add_argument(\'--data_path\', default=\'data/\', help=\'data path\', type=str, required=False)\r\nparser.add_argument(\'--video_in\', default=\'RGB\', help=\'raw video folder name\', type=str, required=False)\r\nparser.add_argument(\'--frame_in\', default=\'RGB-feature\', help=\'video frame/feature folder name\', type=str, required=False)\r\nparser.add_argument(\'--max_num\', default=-1, help=\'max number of training images/category (-1: all/0: avg #)\', type=int, required=False)\r\nparser.add_argument(\'--random_each_video\', default=\'N\', type=str, choices=[\'Y\',\'N\'], help=\'randomly select videos for each video\')\r\nparser.add_argument(\'--method_read\', default=\'video\', type=str, choices=[\'video\',\'frame\'], help=\'approach to load data\')\r\nparser.add_argument(\'--DA_setting\', default=\'hmdb_ucf\', type=str, choices=[\'hmdb_ucf\', \'hmdb_phav\', \'ps_kinetics\', \'kinetics_phav\', \'ucf_olympic\'], help=\'datasets for DA\')\r\nparser.add_argument(\'--suffix\', default=None, help=\'additional string for filename\', type=str, required=False)\r\n\r\nargs = parser.parse_args()\r\n\r\n###### data path ######\r\nprint(Fore.GREEN + \'dataset:\', args.dataset)\r\npath_frame_dataset = args.data_path + args.dataset + \'/\' + args.frame_in + \'/\'\r\nlist_video = os.listdir(path_frame_dataset)\r\nlist_video.sort()\r\n\r\npath_video_dataset = args.data_path + args.dataset + \'/\' + args.video_in + \'/\'\r\nlist_class = os.listdir(path_video_dataset) # create a list of original categories\r\nlist_class.sort()\r\n\r\n#--- Get the category information ---#\r\n# if args.dataset == \'ucf101\':\r\n\t# if args.class_select:\r\n\t# \tclass_file = \'../data/ucf101_splits/classInd_DA.txt\'\r\n\t# \tclass_id = [int(line.strip().split(\' \')[0]) for line in open(class_file)] # number shown in th text file\r\n\t# else:\r\n\t# \tclass_file = \'../data/ucf101_splits/classInd.txt\'\r\n\t# \tclass_id = [int(line.strip().split(\' \')[0])-1 for line in open(class_file)] # number shown in th text file\r\n\r\n\t# class_names = [line.strip().split(\' \')[1] for line in open(class_file)]\r\n\t\r\nif args.dataset == \'hmdb51\' or args.dataset == \'ucf101\':\r\n\tif args.class_select:\r\n\t\tfile_suffix = \'_\' + args.DA_setting\r\n\telse:\r\n\t\tfile_suffix = \'_full\'\r\n\r\n\tclass_file = \'../data/\' + args.dataset + \'_splits/class_list\' + file_suffix + \'.txt\'\r\n\r\n\tclass_id = [int(line.strip().split(\' \', 1)[0]) for line in open(class_file)]  # number shown in th text file\r\n\tclass_names = [line.strip().split(\' \', 1)[1] for line in open(class_file)]\r\n\r\nelif \'unlabeled\' in args.dataset:\r\n\tclass_id = [-1]  # number shown in th text file\r\n\tclass_names = [\'unlabeled\']\r\n\r\nelif \'ps\' in args.dataset or \'kinetics\' in args.dataset or \'phav\' in args.dataset or \'olympic\' in args.dataset:\r\n# \tif \'ps_\' in args.dataset:\r\n# \t\tname_dataset = \'ps\'\r\n# \telif \'kinetics_\' in args.dataset:\r\n# \t\tname_dataset = \'kinetics\'\r\n# \telif \'phav_\' in args.dataset:\r\n# \t\tname_dataset = \'phav\'\r\n\tname_dataset = args.dataset.split(\'_\')[0]\r\n\r\n\tif args.class_select:\r\n\t\tfile_suffix = \'_\' + args.DA_setting\r\n\telse:\r\n\t\tfile_suffix = \'_full\'\r\n\r\n\tclass_file = \'../data/\' + name_dataset + \'_splits/class_list\' + file_suffix + \'.txt\'\r\n\r\n\tclass_id = [int(line.strip().split(\' \', 1)[0]) for line in open(class_file)]  # number shown in th text file\r\n\tclass_names = [line.strip().split(\' \', 1)[1] for line in open(class_file)]\r\n\r\nelse:\r\n\traise ValueError(\'Unknown dataset \'+args.dataset)\r\n\r\n# print(class_names)\r\n# print(list_class)\r\n# print(class_id)\r\n\r\nnum_class = len(set(class_id)) # get the unique indices\r\nlist_class_video = [[] for i in range(num_class)] # create a list to store video paths in terms of new categories\r\nnum_class_video = np.zeros(num_class, dtype=int) # record the number of data for each class\r\n################### Main Function ###################\r\n#=== store all the video paths ===#\r\nfor i in range(len(class_names)):\r\n\tprint(i, class_names[i])\r\n\tlist_video = os.listdir(path_video_dataset + class_names[i])\r\n\tlist_video.sort()\r\n\tlist_video_name = [v.split(\'.\')[0] for v in list_video]\r\n\tid_category = class_id[i]\r\n\r\n\tif args.method_read == \'video\':\r\n\t\tlines_path = [path_frame_dataset + list_video_name[t] + \' \' + str(int(cv2.VideoCapture(path_video_dataset + class_names[i] + \'/\' + list_video_name[t] + \'.mp4\').get(cv2.CAP_PROP_FRAME_COUNT))) + \' \' + str(id_category) + \'\\n\' for t in range(len(list_video))]\r\n\r\n\telif args.method_read == \'frame\':\r\n\t\tlines_path = [path_frame_dataset + list_video_name[t] + \' \' + str(len(os.listdir(path_frame_dataset + list_video_name[t]))) + \' \' + str(id_category) + \'\\n\' for t in range(len(list_video))]\r\n\r\n\t# print(list_current_class_video)\r\n\tlist_class_video[id_category] = list_class_video[id_category] + lines_path\r\n\tnum_class_video[id_category] += len(list_video)\r\n\r\nnum_avg_class = int(num_class_video.mean())\r\nmax_num = num_avg_class if args.max_num == 0 else args.max_num\r\n\r\n#=== randomly select video paths to write the list ===#\r\nif args.suffix:\r\n\tfile = open(args.data_path + args.dataset + \'/\' + \'list_\' + args.dataset + args.suffix + \'.txt\',\'w\')\r\nelse:\r\n\tfile = open(args.data_path + args.dataset + \'/\' + \'list_\' + args.dataset + \'.txt\',\'w\')\t\r\n\r\nprint(args.video_in, \': \')\r\nfor i in range(len(list_class_video)):\r\n\tlist_video_clips = list_class_video[i]\r\n\tnum_videos = len(list_video_clips)\r\n\tfull_list = range(num_videos)\r\n\r\n\tif args.random_each_video == \'Y\':\r\n\t\t#--- re-arrange the list w/ video names as indices\r\n\t\tlist_video_name = [j.rsplit(\'_\',2)[0] for j in list_video_clips] # remove the frame number\r\n\t\tlist_video_name = list(set(list_video_name)) # unique items only\r\n\r\n\t\tlist_video_clips_nest = [[] for j in range(len(list_video_name))] # 2D array using video name as indices\r\n\t\tlist_video_id_nest = [[] for j in range(len(list_video_name))] # 2D array using video name as indices\r\n\t\tfor j in list_video_clips:\r\n\t\t\tid_unique = list_video_name.index(j.rsplit(\'_\',2)[0])\r\n\t\t\tid_single = list_video_clips.index(j)\r\n\t\t\t\r\n\t\t\tlist_video_clips_nest[id_unique].append(j)\r\n\t\t\tlist_video_id_nest[id_unique].append(id_single)\r\n\r\n\t\t#--- random selection for each sub-list\r\n\t\tselect_list = []\r\n\t\tfor j in list_video_id_nest:\r\n\t\t\tselect_list += random.sample(j, max_num) if max_num>0 and max_num<len(j) else j\r\n\r\n\telse:\r\n\t\tselect_list = random.sample(full_list, max_num) if max_num>0 and max_num<num_videos else full_list\r\n\t\r\n\tfor j in select_list:\r\n\t\tfile.write(list_class_video[i][j])\r\n\r\n\tprint(i, len(full_list), \'-->\', len(select_list)) # print the number of videos in the category\r\n\r\nfile.close()\r\n\r\n'"
tools/video_processing.py,0,"b'import argparse\nfrom colorama import init\nfrom colorama import Fore, Back, Style\nimport os\nimport imageio\nimport numpy as np\nfrom tqdm import tqdm\n\nimport time\n\nimport cv2 # add texts and draw figures on frames\n\nimageio.plugins.ffmpeg.download()\ninit(autoreset=True)\n\n###### Flags ######\nparser = argparse.ArgumentParser(description=\'Video Processing Code\')\n# data\nparser.add_argument(\'--data_path\', type=str, required=False, default=\'\', help=\'source path\')\nparser.add_argument(\'--video_in\', type=str, required=False, default=\'test.mp4\', help=\'name of the input video\')\n# others\nparser.add_argument(\'--verbose\', default=False, action=""store_true"")\nparser.add_argument(\'-w\', \'--write_video\', type=str, required=False, default=\'video_output\', help=\'name of the output folder\')\n\nargs = parser.parse_args()\n\n###### data path ######\npath_video_in = args.data_path + args.video_in\n\n#--- create the output folder\npath_output = args.data_path + args.write_video + \'/\'\nif not os.path.isdir(path_output) and args.write_video:\n\tos.makedirs(path_output)\n\n###### Prepare the video i/o ######\n# meta_data: nframes, fps, size, duration\nprint(\'video:\', path_video_in)\nreader = imageio.get_reader(path_video_in)\nnum_frames_total = reader.get_meta_data()[\'nframes\']\nfps = reader.get_meta_data()[\'fps\']\nvideo_name = args.video_in.split(\'.\')[0]\nif args.write_video:\n\twriter = imageio.get_writer(path_output + video_name + \'_proc.mp4\', fps=fps)\n\n###### Video Processing ######\nstart = time.time()\nprint(Fore.CYAN + \'Process the video......\')\nsize = cv2.getTextSize(\'Frame: \', cv2.FONT_HERSHEY_SIMPLEX, 1, 2)[0]\ntry:\n\tfor t, im in tqdm(enumerate(reader)):\n\t\tif np.sum(im.shape) != 0:\n\t\t\tim_new = im\n\t\t\tline = \'Frame: \' + str(t)\n\t\t\tif args.write_video:\n\t\t\t\tcv2.putText(im_new, line, (10, int(size[1] * 1.5)), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n\t\t\t\twriter.append_data(im_new)\n\nexcept RuntimeError:\n\tprint(Back.RED + \'Could not read frame\', t + 1, \'from\', args.video_in)\n\nif args.write_video:\n\twriter.close()\n\nend = time.time()\nprint(\'Total elapsed time: \' + str(end - start))'"
utils/utils.py,1,"b'import itertools\r\nimport numpy as np\r\nimport matplotlib as mpl\r\nmpl.use(\'Agg\')\r\nimport matplotlib.pyplot as plt\r\nimport torch\r\n\r\ndef randSelectBatch(input, num):\r\n    id_all = torch.randperm(input.size(0)).cuda()\r\n    id = id_all[:num]\r\n    return id, input[id]\r\n\r\ndef plot_confusion_matrix(path, cm, classes,\r\n                          normalize=False,\r\n                          title=\'Confusion matrix\',\r\n                          cmap=plt.cm.Blues):\r\n    """"""\r\n    This function prints and plots the confusion matrix.\r\n    Normalization can be applied by setting `normalize=True`.\r\n    """"""\r\n    num_classlabels = cm.sum(axis=1) # count the number of true labels for all the classes\r\n    np.putmask(num_classlabels, num_classlabels == 0, 1) # avoid zero division\r\n\r\n    if normalize:\r\n        cm = cm.astype(\'float\') / num_classlabels[:, np.newaxis]\r\n        print(""Normalized confusion matrix"")\r\n    else:\r\n        print(\'Confusion matrix, without normalization\')\r\n\r\n    plt.figure(figsize=(13, 10))\r\n    plt.imshow(cm, interpolation=\'nearest\', cmap=cmap)\r\n    plt.title(title)\r\n    plt.colorbar()\r\n    tick_marks = np.arange(len(classes))\r\n    plt.xticks(tick_marks, classes, rotation=90)\r\n    plt.yticks(tick_marks, classes)\r\n\r\n    factor = 100 if normalize else 1\r\n    fmt = \'.0f\' if normalize else \'d\'\r\n    thresh = cm.max() / 2.\r\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\r\n        plt.text(j, i, format(cm[i, j]*factor, fmt),\r\n                 horizontalalignment=""center"",\r\n                 color=""white"" if cm[i, j] > thresh else ""black"")\r\n\r\n    plt.tight_layout()\r\n    plt.ylabel(\'True label\')\r\n    plt.xlabel(\'Predicted label\')\r\n\r\n    plt.savefig(path)\r\n'"
