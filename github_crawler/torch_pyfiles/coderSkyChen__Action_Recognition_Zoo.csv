file_path,api_count,code
average_scores.py,0,"b""# @Author  : Sky chen\n# @Email   : dzhchxk@126.com\n# @Personal homepage  : https://coderskychen.cn\n\nimport numpy as np\nimport pdb\n\ndef valid():\n    files_scores = ['/home/mcg/cxk/action-recognition-zoo/results/tsn-flow/output/flow.npz', '/home/mcg/cxk/action-recognition-zoo/results/tsn-rgb/output/rgb.npz']\n\n    allsum = np.zeros([11522, 174])\n    labels = []\n    for filename in files_scores:\n        print(filename)\n        data = np.load(filename)\n        scores = data['scores']\n        # print(scores.shape)\n        ss = scores[:, 0]\n        ll = scores[:, 1]\n        labels.append(ll)\n        ss = [x.reshape(174) for x in ss]\n\n        allsum += ss\n\n    preds = np.argmax(allsum,axis=1)\n\n    num_correct = np.sum(preds == labels)\n    acc = num_correct * 1.0 / preds.shape[0]\n    print('acc=%.3f' % (acc))\n\n\n\nif __name__ == '__main__':\n    valid()\n"""
dataset.py,1,"b'# @Author  : Sky chen\n# @Email   : dzhchxk@126.com\n# @Personal homepage  : https://coderskychen.cn\n\nimport torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\nimport numpy as np\nfrom numpy.random import randint\nimport random\n\n\nclass VideoRecord(object):\n    def __init__(self, row):\n        self._data = row\n\n    @property\n    def path(self):\n        return self._data[0]\n\n    @property\n    def num_frames(self):\n        return int(self._data[1])\n\n    @property\n    def label(self):\n        if len(self._data) == 2:\n            return -1\n        else:\n            return int(self._data[2])\n\n\nclass TwoStreamDataSet(data.Dataset):\n    def __init__(self, root_path, list_file, num_segments=3,\n                 new_length=1, modality=\'RGB\',\n                 image_tmpl=\'img_{:05d}.jpg\', transform=None,\n                 random_shift=True, test_mode=False):\n\n        self.root_path = root_path\n        self.list_file = list_file\n        self.new_length = new_length\n        self.modality = modality\n        self.image_tmpl = image_tmpl\n        self.transform = transform\n        self.random_shift = random_shift\n        self.test_mode = test_mode\n        self.num_segments = num_segments\n\n        self._parse_list()\n\n    def _load_image(self, directory, idx):\n        if self.modality == \'RGB\':\n            try:\n                return [Image.open(os.path.join(self.root_path, directory, self.image_tmpl.format(idx))).convert(\'RGB\')]\n            except Exception:\n                print(\'error loading image:\', os.path.join(self.root_path, directory, self.image_tmpl.format(idx)))\n                return [Image.open(os.path.join(self.root_path, directory, self.image_tmpl.format(1))).convert(\'RGB\')]\n        elif self.modality == \'Flow\':\n            x_img = Image.open(os.path.join(self.root_path, directory, self.image_tmpl.format(\'x\', idx))).convert(\'L\')\n            y_img = Image.open(os.path.join(self.root_path, directory, self.image_tmpl.format(\'y\', idx))).convert(\'L\')\n            return [x_img, y_img]\n\n    def _parse_list(self):\n        # check the frame number is large >3:\n        # usualy it is [video_id, num_frames, class_idx]\n        if not self.test_mode:\n            tmp = [x.strip().split(\' \') for x in open(self.list_file)]\n            if self.modality == \'Flow\':\n                for item in tmp:\n                    item[1] = int(item[1]) / 2\n            tmp = [item for item in tmp if int(item[1]) >= 3]\n            self.video_list = [VideoRecord(item) for item in tmp]\n            print(\'video number:%d\' % (len(self.video_list)))\n        else:\n            tmp = [x.strip().split() for x in open(self.list_file)]\n            if self.modality == \'Flow\':\n                for item in tmp:\n                    item[1] = int(item[1]) / 2\n            # tmp = [item for item in tmp if int(item[1]) >= 3]\n            self.video_list = [VideoRecord(item) for item in tmp]\n            print(\'video number:%d\' % (len(self.video_list)))\n\n    def _get_val_indices(self, record):\n        if record.num_frames > self.num_segments + self.new_length - 1:\n            tick = (record.num_frames - self.new_length + 1) / float(self.num_segments)\n            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n        else:\n            # offsets = np.zeros((self.num_segments,))\n            offsets = np.sort(randint(record.num_frames - self.new_length + 1, size=self.num_segments))\n        if self.modality == \'Flow\':\n            offsets = offsets * 2 + 1\n        else:\n            offsets += 1\n        return offsets\n\n    def __getitem__(self, index):\n        record = self.video_list[index]\n        # check this is a legit video folder\n        if self.modality == \'RGB\':\n            while not os.path.exists(os.path.join(self.root_path, record.path, self.image_tmpl.format(1))):\n                print(os.path.join(self.root_path, record.path, self.image_tmpl.format(1)) + \' not exists jumpping\')\n                index = np.random.randint(len(self.video_list))\n                record = self.video_list[index]\n        else:\n            while not os.path.exists(os.path.join(self.root_path, record.path, self.image_tmpl.format(\'x\', 1))):\n                print(\n                    os.path.join(self.root_path, record.path, self.image_tmpl.format(\'x\', 1)) + \' not exists jumpping\')\n                index = np.random.randint(len(self.video_list))\n                record = self.video_list[index]\n\n        if not self.test_mode:\n            sample_indice = [randint(low=1, high=record.num_frames + 2 - self.new_length)]\n            if self.modality == \'Flow\':\n                sample_indice = [x * 2 - 1 for x in sample_indice]  # flow index 1 3 5 7 ...\n        else:\n            sample_indice = self._get_val_indices(record)\n\n        return self.get(record, sample_indice)\n\n    def get(self, record, indice):\n\n        images = list()\n\n        for seg_ind in indice:\n            p = int(seg_ind)\n            for i in range(self.new_length):  # for optical flow getting a volumn start with seg_ind\n                img = self._load_image(record.path, p)\n                images.extend(img)\n                if p < record.num_frames:\n                    if self.modality == \'RGB\':\n                        p += 1\n                    else:\n                        p += 2\n\n        # one image: H*W*C\n        process_data = self.transform(images)\n        return process_data, record.label\n\n    def __len__(self):\n        return len(self.video_list)\n\n\nclass TSNDataSet(data.Dataset):\n    def __init__(self, root_path, list_file,\n                 num_segments=3, new_length=1, modality=\'RGB\',\n                 image_tmpl=\'img_{:05d}.jpg\', transform=None,\n                 random_shift=True, test_mode=False):\n\n        self.root_path = root_path\n        self.list_file = list_file\n        self.num_segments = num_segments\n        self.new_length = new_length\n        self.modality = modality\n        self.image_tmpl = image_tmpl\n        self.transform = transform\n        self.random_shift = random_shift\n        self.test_mode = test_mode\n\n        self._parse_list()\n\n    def _load_image(self, directory, idx):\n        if self.modality == \'RGB\':\n            try:\n                return [Image.open(os.path.join(self.root_path, directory, self.image_tmpl.format(idx))).convert(\'RGB\')]\n            except Exception:\n                print(\'error loading image:\', os.path.join(self.root_path, directory, self.image_tmpl.format(idx)))\n                return [Image.open(os.path.join(self.root_path, directory, self.image_tmpl.format(1))).convert(\'RGB\')]\n        elif self.modality == \'Flow\':\n            x_img = Image.open(os.path.join(self.root_path, directory, self.image_tmpl.format(\'x\', idx))).convert(\'L\')\n            y_img = Image.open(os.path.join(self.root_path, directory, self.image_tmpl.format(\'y\', idx))).convert(\'L\')\n            return [x_img, y_img]\n\n    def _parse_list(self):\n        # check the frame number is large >3:\n        # usualy it is [video_id, num_frames, class_idx]\n        if not self.test_mode:\n            tmp = [x.strip().split(\' \') for x in open(self.list_file)]\n            if self.modality == \'Flow\':\n                for item in tmp:\n                    item[1] = int(item[1]) / 2\n            tmp = [item for item in tmp if int(item[1]) >= 3]\n            self.video_list = [VideoRecord(item) for item in tmp]\n            print(\'video number:%d\' % (len(self.video_list)))\n        else:\n            tmp = [x.strip().split() for x in open(self.list_file)]\n            if self.modality == \'Flow\':\n                for item in tmp:\n                    item[1] = int(item[1]) / 2\n            # tmp = [item for item in tmp if int(item[1]) >= 3]\n            self.video_list = [VideoRecord(item) for item in tmp]\n            print(\'video number:%d\' % (len(self.video_list)))\n\n    def _sample_indices(self, record):\n        """"""\n\n        :param record: VideoRecord\n        :return: list\n        """"""\n\n        average_duration = (record.num_frames - self.new_length + 1) // self.num_segments\n        if average_duration > 0:  # random sample\n            offsets = np.multiply(list(range(self.num_segments)), average_duration) + randint(average_duration,\n                                                                                              size=self.num_segments)\n        elif record.num_frames > self.num_segments:  # [0,0,1,1,1,2,2,3]     dense sample\n            offsets = np.sort(randint(record.num_frames - self.new_length + 1, size=self.num_segments))\n        else:\n            offsets = np.zeros((self.num_segments,))\n        if self.modality == \'Flow\':\n            offsets = offsets * 2 + 1\n        else:\n            offsets += 1\n        return offsets\n\n    def _get_val_indices(self, record):\n        if record.num_frames > self.num_segments + self.new_length - 1:\n            tick = (record.num_frames - self.new_length + 1) / float(self.num_segments)\n            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n        else:\n            offsets = np.sort(randint(record.num_frames - self.new_length + 1, size=self.num_segments))\n        if self.modality == \'Flow\':\n            offsets = offsets * 2 + 1\n        else:\n            offsets += 1\n        return offsets\n\n    def _get_test_indices(self, record):\n        if record.num_frames > self.num_segments + self.new_length - 1:\n            tick = (record.num_frames - self.new_length + 1) / float(self.num_segments)\n            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n        else:\n            offsets = np.sort(randint(record.num_frames - self.new_length + 1, size=self.num_segments))\n        if self.modality == \'Flow\':\n            offsets = offsets * 2 + 1\n        else:\n            offsets += 1\n        return offsets\n\n    def __getitem__(self, index):\n        record = self.video_list[index]\n        # check this is a legit video folder\n        if self.modality == \'RGB\':\n            while not os.path.exists(os.path.join(self.root_path, record.path, self.image_tmpl.format(1))):\n                print(os.path.join(self.root_path, record.path, self.image_tmpl.format(1)) + \' not exists jumpping\')\n                index = np.random.randint(len(self.video_list))\n                record = self.video_list[index]\n        else:\n            while not os.path.exists(os.path.join(self.root_path, record.path, self.image_tmpl.format(\'x\', 1))):\n                print(\n                    os.path.join(self.root_path, record.path, self.image_tmpl.format(\'x\', 1)) + \' not exists jumpping\')\n                index = np.random.randint(len(self.video_list))\n                record = self.video_list[index]\n\n        if not self.test_mode:\n            segment_indices = self._sample_indices(record) if self.random_shift else self._get_val_indices(record)\n        else:\n            segment_indices = self._get_test_indices(record)\n\n        return self.get(record, segment_indices)\n\n    def get(self, record, indices):\n\n        images = list()\n\n        for seg_ind in indices:\n            p = int(seg_ind)\n            for i in range(self.new_length):  # for optical flow getting a volumn start with seg_ind\n                img = self._load_image(record.path, p)\n                images.extend(img)\n                if p < record.num_frames:\n                    if self.modality == \'RGB\':\n                        p += 1\n                    else:\n                        p += 2\n\n        # one image: H*W*C\n        process_data = self.transform(images)\n        return process_data, record.label\n\n    def __len__(self):\n        return len(self.video_list)\n\nclass C3DDataSet(data.Dataset):\n    def __init__(self, root_path, list_file,\n                 num_segments=3, new_length=1, modality=\'RGB\',\n                 image_tmpl=\'img_{:05d}.jpg\', transform=None,\n                 random_shift=True, test_mode=False):\n\n        self.root_path = root_path\n        self.list_file = list_file\n        self.num_segments = num_segments\n        self.new_length = new_length\n        self.modality = modality\n        self.image_tmpl = image_tmpl\n        self.transform = transform\n        self.random_shift = random_shift\n        self.test_mode = test_mode\n\n        self._parse_list()\n\n    def _load_image(self, directory, idx):\n        if self.modality == \'RGB\':\n            try:\n                return [Image.open(os.path.join(self.root_path, directory, self.image_tmpl.format(idx))).convert(\'RGB\')]\n            except Exception:\n                print(\'error loading image:\', os.path.join(self.root_path, directory, self.image_tmpl.format(idx)))\n                return [Image.open(os.path.join(self.root_path, directory, self.image_tmpl.format(1))).convert(\'RGB\')]\n        elif self.modality == \'Flow\':\n            x_img = Image.open(os.path.join(self.root_path, directory, self.image_tmpl.format(\'x\', idx))).convert(\'L\')\n            y_img = Image.open(os.path.join(self.root_path, directory, self.image_tmpl.format(\'y\', idx))).convert(\'L\')\n            return [x_img, y_img]\n\n    def _parse_list(self):\n        # check the frame number is large >3:\n        # usualy it is [video_id, num_frames, class_idx]\n        if not self.test_mode:\n            tmp = [x.strip().split(\' \') for x in open(self.list_file)]\n            if self.modality == \'Flow\':\n                for item in tmp:\n                    item[1] = int(item[1]) / 2\n            tmp = [item for item in tmp if int(item[1]) >= 3]\n            self.video_list = [VideoRecord(item) for item in tmp]\n            print(\'video number:%d\' % (len(self.video_list)))\n        else:\n            tmp = [x.strip().split() for x in open(self.list_file)]\n            if self.modality == \'Flow\':\n                for item in tmp:\n                    item[1] = int(item[1]) / 2\n            # tmp = [item for item in tmp if int(item[1]) >= 3]\n            self.video_list = [VideoRecord(item) for item in tmp]\n            print(\'video number:%d\' % (len(self.video_list)))\n\n    def _sample_indices(self, record):\n        """"""\n\n        :param record: VideoRecord\n        :return: list\n        """"""\n\n        average_duration = (record.num_frames - self.new_length + 1) // self.num_segments\n        if average_duration > 0:  # random sample\n            offsets = np.multiply(list(range(self.num_segments)), average_duration) + randint(average_duration,\n                                                                                              size=self.num_segments)\n        elif record.num_frames > self.num_segments:  # [0,0,1,1,1,2,2,3]     dense sample\n            offsets = np.sort(randint(record.num_frames - self.new_length + 1, size=self.num_segments))\n        else:\n            offsets = np.zeros((self.num_segments,))\n        if self.modality == \'Flow\':\n            offsets = offsets * 2 + 1\n        else:\n            offsets += 1\n        return offsets\n\n    def _get_val_indices(self, record):\n        if record.num_frames > self.num_segments + self.new_length - 1:\n            tick = (record.num_frames - self.new_length + 1) / float(self.num_segments)\n            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n        else:\n            offsets = np.sort(randint(record.num_frames - self.new_length + 1, size=self.num_segments))\n        if self.modality == \'Flow\':\n            offsets = offsets * 2 + 1\n        else:\n            offsets += 1\n        return offsets\n\n    def _get_test_indices(self, record):\n        if record.num_frames > self.num_segments + self.new_length - 1:\n            tick = (record.num_frames - self.new_length + 1) / float(self.num_segments)\n            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n        else:\n            offsets = np.sort(randint(record.num_frames - self.new_length + 1, size=self.num_segments))\n\n        offsets += 1\n        return offsets\n\n    def __getitem__(self, index):\n        record = self.video_list[index]\n        # check this is a legit video folder\n        if self.modality == \'RGB\':\n            while not os.path.exists(os.path.join(self.root_path, record.path, self.image_tmpl.format(1))):\n                print(os.path.join(self.root_path, record.path, self.image_tmpl.format(1)) + \' not exists jumpping\')\n                index = np.random.randint(len(self.video_list))\n                record = self.video_list[index]\n        else:\n            while not os.path.exists(os.path.join(self.root_path, record.path, self.image_tmpl.format(\'x\', 1))):\n                print(\n                    os.path.join(self.root_path, record.path, self.image_tmpl.format(\'x\', 1)) + \' not exists jumpping\')\n                index = np.random.randint(len(self.video_list))\n                record = self.video_list[index]\n\n        if not self.test_mode:\n            if record.num_frames > self.new_length:\n                segment_indices = [randint(low=1, high=record.num_frames + 2 - self.new_length)]\n            else:\n                segment_indices = [1]\n\n        else:\n            segment_indices = self._get_test_indices(record)\n\n        return self.get(record, segment_indices)\n\n    def get(self, record, indices):\n\n        images = list()\n\n        for seg_ind in indices:\n            p = int(seg_ind)\n            for i in range(self.new_length):\n                img = self._load_image(record.path, p)\n                images.extend(img)\n                if p < record.num_frames:\n                    if self.modality == \'RGB\':\n                        p += 1\n\n\n        # one image: H*W*C\n        process_data = self.transform(images)\n        return process_data, record.label\n\n    def __len__(self):\n        return len(self.video_list)\n'"
main.py,17,"b'# @Author  : Sky chen\n# @Email   : dzhchxk@126.com\n# @Personal homepage  : https://coderskychen.cn\n\ntry:\n    import tensorflow as tf\nexcept ImportError:\n    print(""Tensorflow not installed; No tensorboard logging."")\n    tf = None\n\nimport argparse\nimport os\nimport time\nimport shutil\nimport torch\nimport torchvision\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nfrom torch.nn.utils import clip_grad_norm\n\nfrom dataset import *\nfrom models import *\nfrom transforms import *\nfrom opts import parser\n\n\ndef add_summary_value(writer, key, value, iteration):\n    summary = tf.Summary(value=[tf.Summary.Value(tag=key, simple_value=value)])\n    writer.add_summary(summary, iteration)\n\n\ndef return_something_path(modality):\n    filename_categories = \'/home/mcg/cxk/dataset/somthing-something/category.txt\'\n    if modality == \'RGB\':\n        root_data = \'/home/mcg/cxk/dataset/somthing-something/something-rgb\'\n        filename_imglist_train = \'/home/mcg/cxk/dataset/somthing-something/train_videofolder_rgb.txt\'\n        filename_imglist_val = \'/home/mcg/cxk/dataset/somthing-something/val_videofolder_rgb.txt\'\n\n        prefix = \'{:05d}.jpg\'\n    else:\n        root_data = \'/home/mcg/cxk/dataset/somthing-something/something-optical-flow\'\n        filename_imglist_train = \'/home/mcg/cxk/dataset/somthing-something/train_videofolder_flow.txt\'\n        filename_imglist_val = \'/home/mcg/cxk/dataset/somthing-something/val_videofolder_flow.txt\'\n\n        prefix = \'{:s}_{:05d}.jpg\'\n\n    with open(filename_categories) as f:\n        lines = f.readlines()\n    categories = [item.rstrip() for item in lines]\n    return categories, filename_imglist_train, filename_imglist_val, root_data, prefix\n\n\nbest_prec1 = 0\n\ndef main():\n    global args, best_prec1\n    args = parser.parse_args()\n    assert len(args.train_id) > 0\n\n    check_rootfolders(args.train_id)\n    summary_w = tf and tf.summary.FileWriter(os.path.join(\'results\', args.train_id, args.root_log))  #tensorboard\n\n    categories, args.train_list, args.val_list, args.root_path, prefix = return_something_path(args.modality)\n    num_class = len(categories)\n\n    args.store_name = \'_\'.join([args.model, args.modality, args.arch])\n    print(\'storing name: \' + args.store_name)\n\n    policies = -1\n    if args.model == \'TwoStream\':\n        model = TwoStream(num_class, args.modality,\n                     base_model=args.arch, dropout=args.dropout,\n                     crop_num=1, partial_bn=not args.no_partialbn)\n        policies = model.get_optim_policies()\n\n    elif args.model == \'TSN\':\n        model = TSN(num_class, args.num_segments, args.modality,\n                          base_model=args.arch, dropout=args.dropout,\n                          crop_num=1, partial_bn=not args.no_partialbn)\n        policies = model.get_optim_policies()\n\n    elif args.model == \'C3D\':\n        model = C3D()\n        model_dict = model.state_dict()\n\n        pretrained_dict = torch.load(\'./model_zoo/c3d.pickle\')\n\n        # 1. filter out unnecessary keys\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(pretrained_dict)\n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n\n        print(\'c3d pretrained model loaded~\')\n    else:\n        print(\'error!\')\n        exit()\n\n    crop_size = model.crop_size\n    scale_size = model.scale_size\n    input_mean = model.input_mean\n    input_std = model.input_std\n    train_augmentation = model.get_augmentation()\n\n    model = torch.nn.DataParallel(model, device_ids=args.gpus).cuda()\n\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print((""=> loading checkpoint \'{}\'"".format(args.resume)))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_prec1 = checkpoint[\'best_prec1\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            print((""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.evaluate, checkpoint[\'epoch\'])))\n        else:\n            print((""=> no checkpoint found at \'{}\'"".format(args.resume)))\n\n    cudnn.benchmark = True\n\n    # Data loading code\n    if args.modality != \'RGBDiff\':\n        normalize = GroupNormalize(input_mean, input_std)\n    else:\n        normalize = IdentityTransform()\n\n    if args.modality == \'RGB\':\n        data_length = 1\n    elif args.modality in [\'Flow\', \'RGBDiff\']:\n        data_length = 5\n\n    if args.modality == \'RGB\' and args.model == \'C3D\':\n        data_length = 16  # clip\n\n    if args.model == \'TwoStream\':\n        datasettrain = TwoStreamDataSet(args.root_path, args.train_list,\n                   new_length=data_length,\n                   modality=args.modality,\n                   image_tmpl=prefix,\n                   transform=torchvision.transforms.Compose([\n                       train_augmentation,\n                       Stack(roll=(args.arch in [\'BNInception\', \'InceptionV3\'])),\n                       ToTorchFormatTensor(div=(args.arch not in [\'BNInception\', \'InceptionV3\'])),\n                       normalize,\n                   ]))\n\n        datasetval = TwoStreamDataSet(args.root_path, args.val_list,\n                   new_length=data_length,\n                   modality=args.modality,\n                   image_tmpl=prefix,\n                   random_shift=False,\n                   transform=torchvision.transforms.Compose([\n                       GroupScale(int(scale_size)),\n                       GroupCenterCrop(crop_size),\n                       Stack(roll=(args.arch in [\'BNInception\', \'InceptionV3\'])),\n                       ToTorchFormatTensor(div=(args.arch not in [\'BNInception\', \'InceptionV3\'])),\n                       normalize,\n                   ]))\n    elif args.model == \'TSN\':\n        datasettrain = TSNDataSet(args.root_path, args.train_list, args.num_segments,\n                                        new_length=data_length,\n                                        modality=args.modality,\n                                        image_tmpl=prefix,\n                                        transform=torchvision.transforms.Compose([\n                                            train_augmentation,\n                                            Stack(roll=(args.arch in [\'BNInception\', \'InceptionV3\'])),\n                                            ToTorchFormatTensor(div=(args.arch not in [\'BNInception\', \'InceptionV3\'])),\n                                            normalize,\n                                        ]))\n\n        datasetval = TSNDataSet(args.root_path, args.val_list, args.num_segments,\n                                      new_length=data_length,\n                                      modality=args.modality,\n                                      image_tmpl=prefix,\n                                      random_shift=False,\n                                      transform=torchvision.transforms.Compose([\n                                          GroupScale(int(scale_size)),\n                                          GroupCenterCrop(crop_size),\n                                          Stack(roll=(args.arch in [\'BNInception\', \'InceptionV3\'])),\n                                          ToTorchFormatTensor(div=(args.arch not in [\'BNInception\', \'InceptionV3\'])),\n                                          normalize,\n                                      ]))\n    elif args.model == \'C3D\':\n        datasettrain = C3DDataSet(args.root_path, args.train_list, 1,\n                                        new_length=data_length,\n                                        modality=args.modality,\n                                        image_tmpl=prefix,\n                                        transform=torchvision.transforms.Compose([\n                                            train_augmentation,\n                                            Stack(roll=(args.arch in [\'BNInception\', \'InceptionV3\'])),\n                                            ToTorchFormatTensor(div=(args.arch not in [\'BNInception\', \'InceptionV3\', \'C3D\'])),\n                                            normalize,\n                                        ]))\n\n        datasetval = C3DDataSet(args.root_path, args.val_list, 1,\n                                        new_length=data_length,\n                                        modality=args.modality,\n                                        image_tmpl=prefix,\n                                        random_shift=False,\n                                        transform=torchvision.transforms.Compose([\n                                            GroupScale(int(scale_size)),\n                                            GroupCenterCrop(crop_size),\n                                            Stack(roll=(args.arch in [\'BNInception\', \'InceptionV3\'])),\n                                            ToTorchFormatTensor(\n                                                div=(args.arch not in [\'BNInception\', \'InceptionV3\', \'C3D\'])),\n                                            normalize,\n                                        ]))\n\n    trainvidnum = len(datasettrain)\n    valvidnum = len(datasetval)\n\n    train_loader = torch.utils.data.DataLoader(\n        datasettrain,\n        batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        datasetval,\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    # define loss function (criterion) and optimizer\n    criterion = torch.nn.CrossEntropyLoss().cuda()\n\n    if policies != -1:\n        for group in policies:\n            print((\'group: {} has {} params, lr_mult: {}, decay_mult: {}\'.format(\n                group[\'name\'], len(group[\'params\']), group[\'lr_mult\'], group[\'decay_mult\'])))\n\n        optimizer = torch.optim.SGD(policies, args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n\n    # log_training = open(os.path.join(args.root_log, \'%s.csv\' % args.store_name), \'w\')\n    for epoch in range(args.start_epoch, args.epochs):\n        adjust_learning_rate(optimizer, epoch, args.lr_steps, args.factor, policies != -1)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch, trainvidnum, summary_w)\n\n        # evaluate on validation set\n        if (epoch + 1) % args.eval_freq == 0 or epoch == args.epochs - 1:\n            prec1 = validate(val_loader, model, criterion, (epoch + 1) * trainvidnum, summary_w)\n            # prec1 = validate(val_loader, model, criterion, (epoch + 1) * len(train_loader), summary_w)\n\n            # remember best prec@1 and save checkpoint\n            is_best = prec1 > best_prec1\n            best_prec1 = max(prec1, best_prec1)\n            save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'arch\': args.arch,\n                \'state_dict\': model.state_dict(),\n                \'best_prec1\': best_prec1,\n            }, is_best)\n\n\ndef train(train_loader, model, criterion, optimizer, epoch, vidnums, summary_w):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    if args.no_partialbn:\n        model.module.partialBN(False)\n    else:\n        # model.partialBN(True)\n        model.module.partialBN(True)\n\n    # switch to train mode\n    model.train()\n\n    samples_have_seen = epoch*vidnums\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n        # if i>5:\n        #     break\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        target = target.cuda(async=True)\n\n        input_var = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n\n        bs = input_var.size(0)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1,5))\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        if args.clip_gradient is not None:\n            total_norm = clip_grad_norm(model.parameters(), args.clip_gradient)\n            if total_norm > args.clip_gradient:\n                print(""clipping gradient: {} with coef {}"".format(total_norm, args.clip_gradient / total_norm))\n\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        samples_have_seen += bs\n\n        if i % args.print_freq == 0:\n            output = (\'Epoch: [{0}][{1}/{2}], lr: {lr:.5f}\\t\'\n                    \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                    \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                    \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                    \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                    \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                        epoch, i, len(train_loader), batch_time=batch_time,\n                        data_time=data_time, loss=losses, top1=top1, top5=top5, lr=optimizer.param_groups[-1][\'lr\']))\n            print(output)\n            add_summary_value(summary_w, \'train_loss\', losses.val, samples_have_seen)\n            add_summary_value(summary_w, \'train_Prec@1\', top1.val, samples_have_seen)\n            add_summary_value(summary_w, \'train_Prec@5\', top5.val, samples_have_seen)\n            add_summary_value(summary_w, \'train_Prec@1_mean\', top1.avg, samples_have_seen)\n            add_summary_value(summary_w, \'train_Prec@5_mean\', top5.avg, samples_have_seen)\n            add_summary_value(summary_w, \'lr\', optimizer.param_groups[-1][\'lr\'], samples_have_seen)\n\n            # log.write(output + \'\\n\')\n            # log.flush()\n\n\n\ndef validate(val_loader, model, criterion, iter, summary_w):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target) in enumerate(val_loader):\n        # if i>5:\n        #     break\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input, volatile=True)\n        target_var = torch.autograd.Variable(target, volatile=True)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output.data, target, topk=(1,5))\n\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        if i % args.print_freq == 0:\n            output = (\'Test: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                   i, len(val_loader), batch_time=batch_time, loss=losses,\n                   top1=top1, top5=top5))\n            print(output)            \n            # log.write(output + \'\\n\')\n            # log.flush()\n\n    output = (\'Testing Results: Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Loss {loss.avg:.5f}\'\n          .format(top1=top1, top5=top5, loss=losses))\n    print(output)\n\n    add_summary_value(summary_w, \'val_loss\', losses.avg, iter)\n    add_summary_value(summary_w, \'val_Prec@1\', top1.avg, iter)\n    add_summary_value(summary_w, \'val_Prec@5\', top5.avg, iter)\n    \n    output_best = \'\\nBest Prec@1: %.3f\'%(best_prec1)\n    print(output_best)\n    # log.write(output + \' \' + output_best + \'\\n\')\n    # log.flush()\n\n    return top1.avg\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, \'./results/%s/%s/%s_checkpoint.pth.tar\' % (args.train_id, args.root_model, args.store_name))\n    if is_best:\n        shutil.copyfile(\'./results/%s/%s/%s_checkpoint.pth.tar\' % (args.train_id, args.root_model, args.store_name), \'./results/%s/%s/%s_best.pth.tar\' % (args.train_id, args.root_model, args.store_name))\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, epoch, lr_steps, factor, with_police=True):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    decay = factor ** (sum(epoch >= np.array(lr_steps)))\n    lr = args.lr * decay\n    decay = args.weight_decay\n    if with_police:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr * param_group[\'lr_mult\']\n            param_group[\'weight_decay\'] = decay * param_group[\'decay_mult\']\n    else:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr\n            param_group[\'weight_decay\'] = decay\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\ndef check_rootfolders(trainid):\n    """"""Create log and model folder""""""\n    folders_util = [args.root_log, args.root_model, args.root_output]\n    if not os.path.exists(\'./results\'):\n        os.makedirs(\'./results\')\n    for folder in folders_util:\n        if not os.path.exists(os.path.join(\'./results\', trainid, folder)):\n            print(\'creating folder \' + folder)\n            os.makedirs(os.path.join(\'./results\', trainid, folder))\n\nif __name__ == \'__main__\':\n    main()\n'"
models.py,10,"b'# @Author  : Sky chen\n# @Email   : dzhchxk@126.com\n# @Personal homepage  : https://coderskychen.cn\n\nfrom torch import nn\n\nfrom transforms import *\nfrom torch.nn.init import normal, constant\n\n\nclass TwoStream(nn.Module):\n    def __init__(self, num_class, modality,\n                 base_model=\'BNInception\', new_length=None,\n                 dropout=0.8,\n                 crop_num=1, partial_bn=True):\n        super(TwoStream, self).__init__()\n        self.modality = modality\n        self.reshape = True\n        self.dropout = dropout\n        self.crop_num = crop_num\n\n        if new_length is None:\n            self.new_length = 1 if modality == ""RGB"" else 5\n        else:\n            self.new_length = new_length\n\n        print((""""""\nInitializing with base model: {}.\nTSN Configurations:\n    input_modality:     {}    \n    new_length:         {}    \n    dropout_ratio:      {}    \n        """""".format(base_model, self.modality, self.new_length, self.dropout)))\n\n        self._prepare_base_model(base_model)\n\n        feature_dim = self._prepare_classifier(num_class)\n\n        if self.modality == \'Flow\':\n            print(""Converting the ImageNet model to a flow init model"")\n            self.base_model = self._construct_flow_model(self.base_model)\n            print(""Done. Flow model ready..."")\n\n\n        self.softmax = nn.Softmax()\n\n        if partial_bn:\n            self.partialBN(True)\n\n    def _prepare_classifier(self, num_class):\n        feature_dim = getattr(self.base_model, self.base_model.last_layer_name).in_features\n\n        setattr(self.base_model, self.base_model.last_layer_name, nn.Dropout(p=self.dropout))\n        self.new_fc = nn.Linear(feature_dim, num_class)\n\n        std = 0.001\n        normal(self.new_fc.weight, 0, std)\n        constant(self.new_fc.bias, 0)\n\n        return feature_dim\n\n    def _prepare_base_model(self, base_model):\n\n        if \'resnet\' in base_model or \'vgg\' in base_model:\n            self.base_model = getattr(torchvision.models, base_model)(True)\n            self.base_model.last_layer_name = \'fc\'\n            self.input_size = 224\n            self.input_mean = [0.485, 0.456, 0.406]\n            self.input_std = [0.229, 0.224, 0.225]\n\n            if self.modality == \'Flow\':\n                self.input_mean = [0.5]\n                self.input_std = [np.mean(self.input_std)]\n            elif self.modality == \'RGBDiff\':\n                self.input_mean = [0.485, 0.456, 0.406] + [0] * 3 * self.new_length\n                self.input_std = self.input_std + [np.mean(self.input_std) * 2] * 3 * self.new_length\n        elif base_model == \'BNInception\':\n            import model_zoo\n            self.base_model = getattr(model_zoo, base_model)()\n            self.base_model.last_layer_name = \'fc\'\n            self.input_size = 224\n            self.input_mean = [104, 117, 128]\n            self.input_std = [1]\n\n            if self.modality == \'Flow\':\n                self.input_mean = [128]\n\n        elif base_model == \'InceptionV3\':\n            import model_zoo\n            self.base_model = getattr(model_zoo, base_model)()\n            self.base_model.last_layer_name = \'top_cls_fc\'\n            self.input_size = 299\n            self.input_mean = [104, 117, 128]\n            self.input_std = [1]\n            if self.modality == \'Flow\':\n                self.input_mean = [128]\n            elif self.modality == \'RGBDiff\':\n                self.input_mean = self.input_mean * (1 + self.new_length)\n\n        elif \'inception\' in base_model:\n            import model_zoo\n            self.base_model = getattr(model_zoo, base_model)()\n            self.base_model.last_layer_name = \'classif\'\n            self.input_size = 299\n            self.input_mean = [0.5]\n            self.input_std = [0.5]\n        else:\n            raise ValueError(\'Unknown base model: {}\'.format(base_model))\n\n    def train(self, mode=True):\n        """"""\n        Override the default train() to freeze the BN parameters\n        :return:\n        """"""\n        super(TwoStream, self).train(mode)\n        count = 0\n        if self._enable_pbn:\n            print(""Freezing BatchNorm2D except the first one."")\n            for m in self.base_model.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    count += 1\n                    if count >= (2 if self._enable_pbn else 1):\n                        m.eval()\n\n                        # shutdown update in frozen mode\n                        m.weight.requires_grad = False\n                        m.bias.requires_grad = False\n\n    def partialBN(self, enable):\n        self._enable_pbn = enable\n\n    def get_optim_policies(self):\n        first_conv_weight = []\n        first_conv_bias = []\n        normal_weight = []\n        normal_bias = []\n        bn = []\n\n        conv_cnt = 0\n        bn_cnt = 0\n        for m in self.modules():\n            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Conv1d):\n                ps = list(m.parameters())\n                conv_cnt += 1\n                if conv_cnt == 1:\n                    first_conv_weight.append(ps[0])\n                    if len(ps) == 2:\n                        first_conv_bias.append(ps[1])\n                else:\n                    normal_weight.append(ps[0])\n                    if len(ps) == 2:\n                        normal_bias.append(ps[1])\n            elif isinstance(m, torch.nn.Linear):\n                ps = list(m.parameters())\n                normal_weight.append(ps[0])\n                if len(ps) == 2:\n                    normal_bias.append(ps[1])\n\n            elif isinstance(m, torch.nn.BatchNorm1d):\n                bn.extend(list(m.parameters()))\n            elif isinstance(m, torch.nn.BatchNorm2d):\n                bn_cnt += 1\n                # later BN\'s are frozen\n                if not self._enable_pbn or bn_cnt == 1:\n                    bn.extend(list(m.parameters()))\n            elif len(m._modules) == 0:\n                if len(list(m.parameters())) > 0:\n                    raise ValueError(""New atomic module type: {}. Need to give it a learning policy"".format(type(m)))\n\n        return [\n            {\'params\': first_conv_weight, \'lr_mult\': 5 if self.modality == \'Flow\' else 1, \'decay_mult\': 1,\n             \'name\': ""first_conv_weight""},\n            {\'params\': first_conv_bias, \'lr_mult\': 10 if self.modality == \'Flow\' else 2, \'decay_mult\': 0,\n             \'name\': ""first_conv_bias""},\n            {\'params\': normal_weight, \'lr_mult\': 1, \'decay_mult\': 1,\n             \'name\': ""normal_weight""},\n            {\'params\': normal_bias, \'lr_mult\': 2, \'decay_mult\': 0,\n             \'name\': ""normal_bias""},\n            {\'params\': bn, \'lr_mult\': 1, \'decay_mult\': 0,\n             \'name\': ""BN scale/shift""},\n        ]\n\n    def forward(self, input):\n        # input: bs* channelss * w *h   channelss=channels*frames\n        sample_len = (3 if self.modality == ""RGB"" else 2) * self.new_length   #channels\n\n        base_out = self.base_model(input.view((-1, sample_len) + input.size()[-2:]))\n        # input:(bs*frames)*channels*w*h\n        # output:(bs*frames)*dim\n\n        base_out = self.new_fc(base_out)\n\n        # if self.reshape:\n        #     base_out = base_out.view((-1, self.num_segments) + base_out.size()[1:])\n            #bs*frames*dim\n\n\n        return base_out    #bs*dim\n        # return output.squeeze(1)    #bs*dim\n\n    def _construct_flow_model(self, base_model):\n        # modify the convolution layers\n        # Torch models are usually defined in a hierarchical way.\n        # nn.modules.children() return all sub modules in a DFS manner\n        modules = list(self.base_model.modules())\n        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n        conv_layer = modules[first_conv_idx]\n        container = modules[first_conv_idx - 1]\n\n        # modify parameters, assume the first blob contains the convolution kernels\n        params = [x.clone() for x in conv_layer.parameters()]\n        kernel_size = params[0].size()\n        new_kernel_size = kernel_size[:1] + (2 * self.new_length,) + kernel_size[2:]\n        new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n\n        new_conv = nn.Conv2d(2 * self.new_length, conv_layer.out_channels,\n                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n                             bias=True if len(params) == 2 else False)\n        new_conv.weight.data = new_kernels\n        if len(params) == 2:\n            new_conv.bias.data = params[1].data  # add bias if neccessary\n        layer_name = list(container.state_dict().keys())[0][:-7]  # remove .weight suffix to get the layer name\n\n        # replace the first convlution layer\n        setattr(container, layer_name, new_conv)\n        return base_model\n\n    @property\n    def crop_size(self):\n        return self.input_size\n\n    @property\n    def scale_size(self):\n        return self.input_size * 256 // 224\n\n    def get_augmentation(self):\n        if self.modality == \'RGB\':\n            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75, .66]),\n                                                   GroupRandomHorizontalFlip(is_flow=False)])\n        elif self.modality == \'Flow\':\n            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n                                                   GroupRandomHorizontalFlip(is_flow=True)])\n        elif self.modality == \'RGBDiff\':\n            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n                                                   GroupRandomHorizontalFlip(is_flow=False)])\n\n\nclass TSN(nn.Module):\n    def __init__(self, num_class, num_segments, modality,\n                 base_model=\'BNInception\', new_length=None,\n                 dropout=0.8,\n                 crop_num=1, partial_bn=True):\n        super(TSN, self).__init__()\n        self.modality = modality\n        self.num_class = num_class\n        self.num_segments = num_segments\n        self.reshape = True\n        self.dropout = dropout\n        self.crop_num = crop_num\n\n        if new_length is None:\n            self.new_length = 1 if modality == ""RGB"" else 5\n        else:\n            self.new_length = new_length\n\n        print((""""""\nInitializing with base model: {}.\nTSN Configurations:\n    input_modality:     {}    \n    new_length:         {}    \n    dropout_ratio:      {}    \n        """""".format(base_model, self.modality, self.new_length, self.dropout)))\n\n        self._prepare_base_model(base_model)\n\n        feature_dim = self._prepare_tsn(num_class)\n\n        if self.modality == \'Flow\':\n            print(""Converting the ImageNet model to a flow init model"")\n            self.base_model = self._construct_flow_model(self.base_model)\n            print(""Done. Flow model ready..."")\n\n\n        self.softmax = nn.Softmax()\n\n        if partial_bn:\n            self.partialBN(True)\n\n    def _prepare_tsn(self, num_class):\n        feature_dim = getattr(self.base_model, self.base_model.last_layer_name).in_features\n\n        setattr(self.base_model, self.base_model.last_layer_name, nn.Dropout(p=self.dropout))\n        self.new_fc = nn.Linear(feature_dim, num_class)\n\n        std = 0.001\n        normal(self.new_fc.weight, 0, std)\n        constant(self.new_fc.bias, 0)\n\n        return feature_dim\n\n    def _prepare_base_model(self, base_model):\n\n        if \'resnet\' in base_model or \'vgg\' in base_model:\n            self.base_model = getattr(torchvision.models, base_model)(True)\n            self.base_model.last_layer_name = \'fc\'\n            self.input_size = 224\n            self.input_mean = [0.485, 0.456, 0.406]\n            self.input_std = [0.229, 0.224, 0.225]\n\n            if self.modality == \'Flow\':\n                self.input_mean = [0.5]\n                self.input_std = [np.mean(self.input_std)]\n            elif self.modality == \'RGBDiff\':\n                self.input_mean = [0.485, 0.456, 0.406] + [0] * 3 * self.new_length\n                self.input_std = self.input_std + [np.mean(self.input_std) * 2] * 3 * self.new_length\n        elif base_model == \'BNInception\':\n            import model_zoo\n            self.base_model = getattr(model_zoo, base_model)()\n            self.base_model.last_layer_name = \'fc\'\n            self.input_size = 224\n            self.input_mean = [104, 117, 128]\n            self.input_std = [1]\n\n            if self.modality == \'Flow\':\n                self.input_mean = [128]\n\n        elif base_model == \'InceptionV3\':\n            import model_zoo\n            self.base_model = getattr(model_zoo, base_model)()\n            self.base_model.last_layer_name = \'top_cls_fc\'\n            self.input_size = 299\n            self.input_mean = [104, 117, 128]\n            self.input_std = [1]\n            if self.modality == \'Flow\':\n                self.input_mean = [128]\n            elif self.modality == \'RGBDiff\':\n                self.input_mean = self.input_mean * (1 + self.new_length)\n\n        elif \'inception\' in base_model:\n            import model_zoo\n            self.base_model = getattr(model_zoo, base_model)()\n            self.base_model.last_layer_name = \'classif\'\n            self.input_size = 299\n            self.input_mean = [0.5]\n            self.input_std = [0.5]\n        else:\n            raise ValueError(\'Unknown base model: {}\'.format(base_model))\n\n    def train(self, mode=True):\n        """"""\n        Override the default train() to freeze the BN parameters\n        :return:\n        """"""\n        super(TSN, self).train(mode)\n        count = 0\n        if self._enable_pbn:\n            print(""Freezing BatchNorm2D except the first one."")\n            for m in self.base_model.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    count += 1\n                    if count >= (2 if self._enable_pbn else 1):\n                        m.eval()\n\n                        # shutdown update in frozen mode\n                        m.weight.requires_grad = False\n                        m.bias.requires_grad = False\n\n    def partialBN(self, enable):\n        self._enable_pbn = enable\n\n    def get_optim_policies(self):\n        first_conv_weight = []\n        first_conv_bias = []\n        normal_weight = []\n        normal_bias = []\n        bn = []\n\n        conv_cnt = 0\n        bn_cnt = 0\n        for m in self.modules():\n            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Conv1d):\n                ps = list(m.parameters())\n                conv_cnt += 1\n                if conv_cnt == 1:\n                    first_conv_weight.append(ps[0])\n                    if len(ps) == 2:\n                        first_conv_bias.append(ps[1])\n                else:\n                    normal_weight.append(ps[0])\n                    if len(ps) == 2:\n                        normal_bias.append(ps[1])\n            elif isinstance(m, torch.nn.Linear):\n                ps = list(m.parameters())\n                normal_weight.append(ps[0])\n                if len(ps) == 2:\n                    normal_bias.append(ps[1])\n\n            elif isinstance(m, torch.nn.BatchNorm1d):\n                bn.extend(list(m.parameters()))\n            elif isinstance(m, torch.nn.BatchNorm2d):\n                bn_cnt += 1\n                # later BN\'s are frozen\n                if not self._enable_pbn or bn_cnt == 1:\n                    bn.extend(list(m.parameters()))\n            elif len(m._modules) == 0:\n                if len(list(m.parameters())) > 0:\n                    raise ValueError(""New atomic module type: {}. Need to give it a learning policy"".format(type(m)))\n\n        return [\n            {\'params\': first_conv_weight, \'lr_mult\': 5 if self.modality == \'Flow\' else 1, \'decay_mult\': 1,\n             \'name\': ""first_conv_weight""},\n            {\'params\': first_conv_bias, \'lr_mult\': 10 if self.modality == \'Flow\' else 2, \'decay_mult\': 0,\n             \'name\': ""first_conv_bias""},\n            {\'params\': normal_weight, \'lr_mult\': 1, \'decay_mult\': 1,\n             \'name\': ""normal_weight""},\n            {\'params\': normal_bias, \'lr_mult\': 2, \'decay_mult\': 0,\n             \'name\': ""normal_bias""},\n            {\'params\': bn, \'lr_mult\': 1, \'decay_mult\': 0,\n             \'name\': ""BN scale/shift""},\n        ]\n\n    def forward(self, input):\n        # input: bs* channelss * w *h   channelss=channels*frames\n        bs = input.size()[0]\n        sample_len = (3 if self.modality == ""RGB"" else 2) * self.new_length   #channels\n\n        base_out = self.base_model(input.view((-1, sample_len) + input.size()[-2:]))\n        # input:(bs*frames)*channels*w*h\n        # output:(bs*frames)*dim\n\n        base_out = self.new_fc(base_out)\n\n        # avg consensus function\n        base_out = base_out.view((bs, self.num_segments, self.num_class))\n        base_out = torch.mean(base_out, 1)\n\n\n        return base_out    #bs*dim\n\n\n    def _construct_flow_model(self, base_model):\n        # modify the convolution layers\n        # Torch models are usually defined in a hierarchical way.\n        # nn.modules.children() return all sub modules in a DFS manner\n        modules = list(self.base_model.modules())\n        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n        conv_layer = modules[first_conv_idx]\n        container = modules[first_conv_idx - 1]\n\n        # modify parameters, assume the first blob contains the convolution kernels\n        params = [x.clone() for x in conv_layer.parameters()]\n        kernel_size = params[0].size()\n        new_kernel_size = kernel_size[:1] + (2 * self.new_length,) + kernel_size[2:]\n        new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n\n        new_conv = nn.Conv2d(2 * self.new_length, conv_layer.out_channels,\n                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n                             bias=True if len(params) == 2 else False)\n        new_conv.weight.data = new_kernels\n        if len(params) == 2:\n            new_conv.bias.data = params[1].data  # add bias if neccessary\n        layer_name = list(container.state_dict().keys())[0][:-7]  # remove .weight suffix to get the layer name\n\n        # replace the first convlution layer\n        setattr(container, layer_name, new_conv)\n        return base_model\n\n    @property\n    def crop_size(self):\n        return self.input_size\n\n    @property\n    def scale_size(self):\n        return self.input_size * 256 // 224\n\n    def get_augmentation(self):\n        if self.modality == \'RGB\':\n            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75, .66]),\n                                                   GroupRandomHorizontalFlip(is_flow=False)])\n        elif self.modality == \'Flow\':\n            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n                                                   GroupRandomHorizontalFlip(is_flow=True)])\n        elif self.modality == \'RGBDiff\':\n            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75]),\n                                                   GroupRandomHorizontalFlip(is_flow=False)])\n\n\n\nclass C3D(nn.Module):\n\n    def __init__(self):\n        super(C3D, self).__init__()\n\n        self.modality = \'RGB\'\n        self.input_size = 112\n        self.input_mean = [104, 117, 128]\n        self.input_std = [1]\n\n        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n\n        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n\n        self.conv3a = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.conv3b = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n\n        self.conv4a = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.conv4b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n\n        self.conv5a = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.conv5b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))\n\n        self.fc6 = nn.Linear(8192, 4096)\n        self.fc7 = nn.Linear(4096, 4096)\n        self.fc8_new = nn.Linear(4096, 174)\n\n        self.dropout = nn.Dropout(p=0.5)\n\n        self.relu = nn.ReLU()\n\n        self.softmax = nn.Softmax()\n    def forward(self, x):\n\n        bs = x.size(0)\n        # print(x.size())\n        x = x.view(-1, 3, 16, 112, 112)\n\n        h = self.relu(self.conv1(x))\n        h = self.pool1(h)\n\n        h = self.relu(self.conv2(h))\n        h = self.pool2(h)\n\n        h = self.relu(self.conv3a(h))\n        h = self.relu(self.conv3b(h))\n        h = self.pool3(h)\n\n        h = self.relu(self.conv4a(h))\n        h = self.relu(self.conv4b(h))\n        h = self.pool4(h)\n\n        h = self.relu(self.conv5a(h))\n        h = self.relu(self.conv5b(h))\n        h = self.pool5(h)\n\n        h = h.view(-1, 8192)\n        h = self.relu(self.fc6(h))\n        h = self.dropout(h)\n        h = self.relu(self.fc7(h))\n        h = self.dropout(h)\n\n        logits = self.fc8_new(h)\n\n        return logits\n\n    def partialBN(self, enable):\n        pass\n\n    @property\n    def crop_size(self):\n        return self.input_size\n\n    @property\n    def scale_size(self):\n        return self.input_size * 128 // 112\n\n    def get_augmentation(self):\n        if self.modality == \'RGB\':\n            return torchvision.transforms.Compose([GroupMultiScaleCrop(self.input_size, [1, .875, .75, .66]),\n                                                   GroupRandomHorizontalFlip(is_flow=False)])\n'"
opts.py,0,"b'# @Author  : Sky chen\n# @Email   : dzhchxk@126.com\n# @Personal homepage  : https://coderskychen.cn\n\nimport argparse\nparser = argparse.ArgumentParser(description=""PyTorch implementation of TwoStream"")\nparser.add_argument(\'model\', type=str, choices=[\'TwoStream\', \'TSN\', \'C3D\'])\nparser.add_argument(\'modality\', type=str, choices=[\'RGB\', \'Flow\'])\nparser.add_argument(\'train_id\', type=str)\nparser.add_argument(\'--num_segments\', type=int, default=3)\nparser.add_argument(\'--train_list\', type=str,default="""")\nparser.add_argument(\'--val_list\', type=str, default="""")\nparser.add_argument(\'--root_path\', type=str, default="""")\nparser.add_argument(\'--store_name\', type=str, default="""")\n# ========================= Model Configs ==========================\nparser.add_argument(\'--arch\', type=str, default=""BNInception"")\nparser.add_argument(\'--dropout\', \'--do\', default=0.8, type=float, metavar=\'DO\', help=\'dropout ratio (default: 0.8)\')\n# ========================= Learning Configs ==========================\nparser.add_argument(\'--epochs\', default=120, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'-b\', \'--batch_size\', default=16, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 256)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.005, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--factor\', \'--factor\', default=0.1, type=float,\n                    help=\'decay factor\')\nparser.add_argument(\'--lr_steps\', default=[30, 55], type=float, nargs=""+"",\n                    metavar=\'LRSteps\', help=\'epochs to decay learning rate by factor\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=5e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 5e-4)\')\nparser.add_argument(\'--clip-gradient\', \'--gd\', default=20, type=float,\n                    metavar=\'W\', help=\'gradient norm clipping (default: disabled)\')\nparser.add_argument(\'--no_partialbn\', \'--npb\', default=False, action=""store_true"")\n# ========================= Monitor Configs ==========================\nparser.add_argument(\'--print-freq\', \'-p\', default=5, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--eval-freq\', \'-ef\', default=1, type=int,\n                    metavar=\'N\', help=\'evaluation frequency (default: 5)\')\n# ========================= Runtime Configs ==========================\nparser.add_argument(\'-j\', \'--workers\', default=30, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--snapshot_pref\', type=str, default="""")\nparser.add_argument(\'--start_epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--gpus\', nargs=\'+\', type=int, default=None)\nparser.add_argument(\'--root_log\',type=str, default=\'log\')\nparser.add_argument(\'--root_model\', type=str, default=\'model\')\nparser.add_argument(\'--root_output\',type=str, default=\'output\')\n\n\n\n'"
process_dataset.py,0,"b""# @Author  : Sky chen\n# @Email   : dzhchxk@126.com\n# @Personal homepage  : https://coderskychen.cn\n\n# processing the raw data of the video datasets (Something-something)\n# generate the meta files:\n#   category.txt:               the list of categories.\n#   train_videofolder.txt:      each row contains [videoname num_frames classIDX]\n#   val_videofolder.txt:        same as above\n#\n\nimport os\n\n\ndef train_and_val_test(modality='rgb'):\n    dataset_name = ''\n    if modality == 'rgb':\n        dataset_name = 'something-rgb'\n    elif modality == 'flow':\n        dataset_name = 'something-optical-flow'\n    else:\n        print('error in modal type!')\n        exit()\n    rootdir = '/home/mcg/cxk/dataset/somthing-something/'\n\n    prefix_name = 'something-something-v1'\n    with open(rootdir + '%s-labels.csv' % prefix_name) as f:\n        lines = f.readlines()\n    categories = []\n    for line in lines:\n        line = line.rstrip()\n        categories.append(line)\n    categories = sorted(categories)\n    with open(rootdir + 'category.txt', 'w') as f:\n        f.write('\\n'.join(categories))\n\n    dict_categories = {}\n    for i, category in enumerate(categories):\n        dict_categories[category] = i\n\n    files_input = [rootdir + '%s-validation.csv' % prefix_name, rootdir + '%s-train.csv' % prefix_name]\n    files_output = [rootdir + 'val_videofolder_%s.txt' % modality, rootdir + 'train_videofolder_%s.txt' % modality]\n    for (filename_input, filename_output) in zip(files_input, files_output):\n        with open(filename_input) as f:\n            lines = f.readlines()\n        folders = []\n        idx_categories = []\n        for line in lines:\n            line = line.rstrip()\n            items = line.split(';')\n            folders.append(items[0])\n            idx_categories.append(os.path.join(dict_categories[items[1]]))\n        output = []\n        for i in range(len(folders)):\n            curFolder = folders[i]\n            curIDX = idx_categories[i]\n            # counting the number of frames in each video folders\n            dir_files = os.listdir(os.path.join(rootdir, dataset_name, curFolder))\n            output.append('%s %d %d' % (curFolder, len(dir_files), curIDX))\n            print('%d/%d' % (i, len(folders)))\n        with open(filename_output, 'w') as f:\n            f.write('\\n'.join(output))\n        # -----------test set-----------\n        with open(rootdir + 'something-something-v1-test.csv') as f:\n            lines = f.readlines()\n        output = []\n        for idx, i in enumerate(lines):\n            floder = i.strip()\n            files = os.listdir(rootdir + dataset_name + '/%s' % floder)\n            output.append('%s %d' % (floder, len(files)))\n            print('%d/%d' % (idx, len(lines)))\n        with open(rootdir + 'test_videofolder_%s.txt' % modality, 'w') as f:\n            f.write('\\n'.join(output))\n\n\nif __name__ == '__main__':\n    train_and_val_test(modality='flow')\n    train_and_val_test(modality='rgb')\n"""
test_models.py,11,"b'# @Author  : Sky chen\n# @Email   : dzhchxk@126.com\n# @Personal homepage  : https://coderskychen.cn\n# Note that when testing TSN, num_segments=1, and num_segments>1 only on traing phrase.\n\nimport os\nimport argparse\nimport time\n\nimport numpy as np\nimport torch.nn.parallel\nimport torch.optim\nfrom sklearn.metrics import confusion_matrix\nfrom models import *\nfrom transforms import *\nfrom dataset import *\nimport pdb\nfrom torch.nn import functional as F\n\n\n# options\nparser = argparse.ArgumentParser(description=""testing on the full validation set"")\nparser.add_argument(\'--model\', type=str, choices=[\'TwoStream\', \'TSN\', \'C3D\'])\nparser.add_argument(\'--modality\', type=str, choices=[\'RGB\', \'Flow\'])\nparser.add_argument(\'--weights\', type=str)\nparser.add_argument(\'--train_id\', type=str)\nparser.add_argument(\'--arch\', type=str, default=""BNInception"")\nparser.add_argument(\'--save_scores\', type=str, default=None)\nparser.add_argument(\'--test_segments\', type=int, default=25)\nparser.add_argument(\'--max_num\', type=int, default=-1)\nparser.add_argument(\'--test_crops\', type=int, default=10)\nparser.add_argument(\'--input_size\', type=int, default=224)\nparser.add_argument(\'--crop_fusion_type\', type=str, default=\'TSN-DI\',\n                    choices=[\'avg\', \'TRN\',\'TRNmultiscale\', \'TSN-DI\'])\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--gpus\', nargs=\'+\', type=int, default=None)\nparser.add_argument(\'--img_feature_dim\',type=int, default=256)\nparser.add_argument(\'--k\', type=int, default=3)\nparser.add_argument(\'--softmax\', type=int, default=0)\n\nargs = parser.parse_args()\n\ndef return_something_path(modality):\n    filename_categories = \'/home/mcg/cxk/dataset/somthing-something/category.txt\'\n    if modality == \'RGB\':\n        root_data = \'/home/mcg/cxk/dataset/somthing-something/something-rgb\'\n        filename_imglist_train = \'/home/mcg/cxk/dataset/somthing-something/train_videofolder_rgb.txt\'\n        filename_imglist_val = \'/home/mcg/cxk/dataset/somthing-something/val_videofolder_rgb.txt\'\n\n        prefix = \'{:05d}.jpg\'\n    else:\n        root_data = \'/home/mcg/cxk/dataset/somthing-something/something-optical-flow\'\n        filename_imglist_train = \'/home/mcg/cxk/dataset/somthing-something/train_videofolder_flow.txt\'\n        filename_imglist_val = \'/home/mcg/cxk/dataset/somthing-something/val_videofolder_flow.txt\'\n\n        prefix = \'{:s}_{:05d}.jpg\'\n\n    with open(filename_categories) as f:\n        lines = f.readlines()\n    categories = [item.rstrip() for item in lines]\n    return categories, filename_imglist_train, filename_imglist_val, root_data, prefix\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 0, True, True)\n    # pred = pred.t()\n    # print(target)\n    # print(pred)\n    correct = pred.eq(target.view(-1).expand(pred.size()))\n    # print(correct)\n    # correct = pred.eq(target.view(1, -1).expand_as(pred))\n    res = []\n    for k in topk:\n         correct_k = correct[:k].view(-1).float().sum(0)\n         res.append(correct_k.mul_(100.0 / (batch_size)))\n    return res\n\n\n# time.sleep(3400)\n\ncategories, args.train_list, args.val_list, args.root_path, prefix = return_something_path(args.modality)\nnum_class = len(categories)\n\nif args.model == \'TwoStream\':\n    net = TwoStream(num_class, args.modality, base_model=args.arch)\nelif args.model == \'TSN\':\n    net = TSN(num_class, 1, args.modality, base_model=args.arch)\nelif args.model == \'C3D\':\n    net = C3D()\n\ncheckpoint = torch.load(os.path.join(\'/home/mcg/cxk/action-recognition-zoo/results\', args.train_id, \'model\', args.weights))\nprint(""model epoch {} best prec@1: {}"".format(checkpoint[\'epoch\'], checkpoint[\'best_prec1\']))\n\nbase_dict = {\'.\'.join(k.split(\'.\')[1:]): v for k,v in list(checkpoint[\'state_dict\'].items())}\nnet.load_state_dict(base_dict)\n\nif args.test_crops == 1:\n    cropping = torchvision.transforms.Compose([\n        GroupScale(net.scale_size),\n        GroupCenterCrop(net.input_size),\n    ])\nelif args.test_crops == 10:\n    cropping = torchvision.transforms.Compose([\n        GroupOverSample(net.input_size, net.scale_size)\n    ])\nelse:\n    raise ValueError(""Only 1 and 10 crops are supported while we got {}"".format(args.test_crops))\nif args.model == \'TwoStream\':\n    data_loader = torch.utils.data.DataLoader(\n            TwoStreamDataSet(args.root_path, args.val_list, num_segments=args.test_segments,\n                       new_length=1 if args.modality == ""RGB"" else 5,\n                       modality=args.modality,\n                       image_tmpl=prefix,\n                       test_mode=True,\n                       transform=torchvision.transforms.Compose([\n                           cropping,\n                           Stack(roll=(args.arch in [\'BNInception\',\'InceptionV3\'])),\n                           ToTorchFormatTensor(div=(args.arch not in [\'BNInception\',\'InceptionV3\'])),\n                           GroupNormalize(net.input_mean, net.input_std),\n                       ])),\n            batch_size=1, shuffle=False,\n            num_workers=args.workers * 2, pin_memory=True)\nelif args.model == \'TSN\':\n    data_loader = torch.utils.data.DataLoader(\n            TSNDataSet(args.root_path, args.val_list, num_segments=args.test_segments,\n                       new_length=1 if args.modality == ""RGB"" else 5,\n                       modality=args.modality,\n                       image_tmpl=prefix,\n                       test_mode=True,\n                       transform=torchvision.transforms.Compose([\n                           cropping,\n                           Stack(roll=(args.arch in [\'BNInception\',\'InceptionV3\'])),\n                           ToTorchFormatTensor(div=(args.arch not in [\'BNInception\',\'InceptionV3\'])),\n                           GroupNormalize(net.input_mean, net.input_std),\n                       ])),\n            batch_size=1, shuffle=False,\n            num_workers=args.workers * 2, pin_memory=True)\nelif args.model == \'C3D\':\n    data_loader = torch.utils.data.DataLoader(\n        C3DDataSet(args.root_path, args.val_list, num_segments=args.test_segments,\n                              new_length=16,\n                              modality=args.modality,\n                              image_tmpl=prefix,\n                              test_mode=True,\n                              random_shift=False,\n                              transform=torchvision.transforms.Compose([\n                                  cropping,\n                                  Stack(roll=(args.arch in [\'BNInception\', \'InceptionV3\'])),\n                                  ToTorchFormatTensor(\n                                      div=(args.arch not in [\'BNInception\', \'InceptionV3\', \'C3D\'])),\n                                  GroupNormalize(net.input_mean, net.input_std),\n                              ])),\n        batch_size=1, shuffle=False,\n        num_workers=args.workers * 2, pin_memory=True)\n\nif args.gpus is not None:\n    devices = [args.gpus[i] for i in range(args.workers)]\nelse:\n    devices = list(range(args.workers))\n\n\n#net = torch.nn.DataParallel(net.cuda(devices[0]), device_ids=devices)\nnet = torch.nn.DataParallel(net.cuda())\n# net=net.cuda()\nnet.eval()\n\ndata_gen = enumerate(data_loader)\n\ntotal_num = len(data_loader.dataset)\noutput = []\n\n\ndef eval_video(video_data):\n    i, data, label = video_data\n\n    num_crop = args.test_crops\n\n    if args.modality == \'RGB\':\n        length = 3\n        if args.model == \'C3D\':\n            length = 16\n    elif args.modality == \'Flow\':\n        length = 10\n    elif args.modality == \'RGBDiff\':\n        length = 18\n    else:\n        raise ValueError(""Unknown modality ""+args.modality)\n\n    # data: bs* channelss * w *h   channelss=channels*frames\n\n    input_var = torch.autograd.Variable(data.view(-1, length, data.size(2), data.size(3)),\n                                        volatile=True)\n    rst = net(input_var.cuda())\n    rst = rst.data.cpu().numpy().copy()\n\n    rst = rst.reshape((num_crop, args.test_segments, num_class)).mean(axis=0).reshape((args.test_segments, num_class)).mean(axis=0).reshape((num_class))\n\n    return i, rst, label[0]\n\n\nproc_start_time = time.time()\nmax_num = args.max_num if args.max_num > 0 else len(data_loader.dataset)\n\ntop1 = AverageMeter()\ntop5 = AverageMeter()\n\nfor i, (data, label) in data_gen:\n    if i >= max_num:\n        break\n    rst = eval_video((i, data, label))\n    output.append(rst[1:])\n    cnt_time = time.time() - proc_start_time\n    prec1, prec5 = accuracy(torch.from_numpy(rst[1]), label, topk=(1, 5))\n    top1.update(prec1[0], 1)\n    top5.update(prec5[0], 1)\n    print(\'video {} done, total {}/{}, average {:.3f} sec/video, moving Prec@1 {:.3f} Prec@5 {:.3f}\'.format(i, i+1,\n                                                                    len(data_loader),\n                                                                    float(cnt_time) / (i+1), top1.avg, top5.avg))\n\nvideo_pred = [np.argmax(np.mean(x[0], axis=0)) for x in output]\n\nvideo_labels = [x[1] for x in output]\n\n\ncf = confusion_matrix(video_labels, video_pred).astype(float)\n\ncls_cnt = cf.sum(axis=1)\ncls_hit = np.diag(cf)\n\ncls_acc = cls_hit / cls_cnt\n\nprint(\'-----Evaluation is finished------\')\nprint(\'Class Accuracy {:.02f}%\'.format(np.mean(cls_acc) * 100))\nprint(\'Overall Prec@1 {:.02f}% Prec@5 {:.02f}%\'.format(top1.avg, top5.avg))\n\nif args.save_scores is not None:\n\n    if args.modality==\'RGB\':\n        test_list = \'/home/mcg/cxk/dataset/somthing-something/val_videofolder_rgb.txt\'\n    elif args.modality==\'Flow\':\n        test_list = \'/home/mcg/cxk/dataset/somthing-something/val_videofolder_flow.txt\'\n    # reorder before saving\n    name_list = [x.strip().split()[0] for x in open(test_list)]\n\n    assert len(output) == len(name_list)\n\n    order_dict = {e:i for i, e in enumerate(sorted(name_list))}\n    reorder_output = [None] * len(name_list)\n    # reorder_label = [None] * len(output)\n    reorder_pred = [None] * len(name_list)\n    output_csv = []\n    for i in range(len(output)):\n        idx = order_dict[name_list[i]]\n        reorder_output[idx] = output[i]\n        # reorder_label[idx] = video_labels[i]\n        reorder_pred[idx] = video_pred[i]\n        output_csv.append(\'%s;%s\'%(name_list[i], categories[video_pred[i]]))\n\n    np.savez(os.path.join(\'/home/mcg/cxk/action-recognition-zoo/results\', args.train_id, \'output\', args.save_scores), scores=reorder_output, predictions=reorder_pred)\n\n    # with open(os.path.join(\'/home/mcg/cxk/action-recognition-zoo/results\', args.train_id, \'output\', args.save_scores+\'.csv\'), \'w\') as f:\n    #     f.write(\'\\n\'.join(output_csv))\n\n\n'"
transforms.py,3,"b'# @Author  : Sky chen\n# @Email   : dzhchxk@126.com\n# @Personal homepage  : https://coderskychen.cn\n\nimport torchvision\nimport random\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport numbers\nimport math\nimport torch\n\n\nclass GroupRandomCrop(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img_group):\n\n        w, h = img_group[0].size\n        th, tw = self.size\n\n        out_images = list()\n\n        x1 = random.randint(0, w - tw)\n        y1 = random.randint(0, h - th)\n\n        for img in img_group:\n            assert(img.size[0] == w and img.size[1] == h)\n            if w == tw and h == th:\n                out_images.append(img)\n            else:\n                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n\n        return out_images\n\n\nclass GroupCenterCrop(object):\n    def __init__(self, size):\n        self.worker = torchvision.transforms.CenterCrop(size)\n\n    def __call__(self, img_group):\n        return [self.worker(img) for img in img_group]\n\n\nclass GroupRandomHorizontalFlip(object):\n    """"""Randomly horizontally flips the given PIL.Image with a probability of 0.5\n    """"""\n    def __init__(self, is_flow=False):\n        self.is_flow = is_flow\n\n    def __call__(self, img_group, is_flow=False):\n        v = random.random()\n        if v < 0.5:\n            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n            if self.is_flow:\n                for i in range(0, len(ret), 2):\n                    ret[i] = ImageOps.invert(ret[i])  # invert flow pixel values when flipping\n            return ret\n        else:\n            return img_group\n\n\nclass GroupNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))\n        rep_std = self.std * (tensor.size()[0]//len(self.std))\n\n        # TODO: make efficient\n        for t, m, s in zip(tensor, rep_mean, rep_std):\n            t.sub_(m).div_(s)\n\n        return tensor\n\n\nclass GroupScale(object):\n    """""" Rescales the input PIL.Image to the given \'size\'.\n    \'size\' will be the size of the smaller edge.\n    For example, if height > width, then image will be\n    rescaled to (size * height / width, size)\n    size: size of the smaller edge\n    interpolation: Default: PIL.Image.BILINEAR\n    """"""\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.worker = torchvision.transforms.Scale(size, interpolation)\n\n    def __call__(self, img_group):\n        return [self.worker(img) for img in img_group]\n\n\nclass GroupOverSample(object):\n    def __init__(self, crop_size, scale_size=None):\n        self.crop_size = crop_size if not isinstance(crop_size, int) else (crop_size, crop_size)\n\n        if scale_size is not None:\n            self.scale_worker = GroupScale(scale_size)\n        else:\n            self.scale_worker = None\n\n    def __call__(self, img_group):\n\n        if self.scale_worker is not None:\n            img_group = self.scale_worker(img_group)\n\n        image_w, image_h = img_group[0].size\n        crop_w, crop_h = self.crop_size\n\n        offsets = GroupMultiScaleCrop.fill_fix_offset(False, image_w, image_h, crop_w, crop_h)\n        oversample_group = list()\n        for o_w, o_h in offsets:\n            normal_group = list()\n            flip_group = list()\n            for i, img in enumerate(img_group):\n                crop = img.crop((o_w, o_h, o_w + crop_w, o_h + crop_h))\n                normal_group.append(crop)\n                flip_crop = crop.copy().transpose(Image.FLIP_LEFT_RIGHT)\n\n                if img.mode == \'L\' and i % 2 == 0:\n                    flip_group.append(ImageOps.invert(flip_crop))\n                else:\n                    flip_group.append(flip_crop)\n\n            oversample_group.extend(normal_group)\n            oversample_group.extend(flip_group)\n        return oversample_group\n\n\nclass GroupMultiScaleCrop(object):\n\n    def __init__(self, input_size, scales=None, max_distort=1, fix_crop=True, more_fix_crop=True):\n        self.scales = scales if scales is not None else [1, 875, .75, .66]\n        self.max_distort = max_distort\n        self.fix_crop = fix_crop\n        self.more_fix_crop = more_fix_crop\n        self.input_size = input_size if not isinstance(input_size, int) else [input_size, input_size]\n        self.interpolation = Image.BILINEAR\n\n    def __call__(self, img_group):\n\n        im_size = img_group[0].size\n\n        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size(im_size)\n        crop_img_group = [img.crop((offset_w, offset_h, offset_w + crop_w, offset_h + crop_h)) for img in img_group]\n        ret_img_group = [img.resize((self.input_size[0], self.input_size[1]), self.interpolation)\n                         for img in crop_img_group]\n        return ret_img_group\n\n    def _sample_crop_size(self, im_size):\n        image_w, image_h = im_size[0], im_size[1]\n\n        # find a crop size\n        base_size = min(image_w, image_h)\n        crop_sizes = [int(base_size * x) for x in self.scales]\n        crop_h = [self.input_size[1] if abs(x - self.input_size[1]) < 3 else x for x in crop_sizes]\n        crop_w = [self.input_size[0] if abs(x - self.input_size[0]) < 3 else x for x in crop_sizes]\n\n        pairs = []\n        for i, h in enumerate(crop_h):\n            for j, w in enumerate(crop_w):\n                if abs(i - j) <= self.max_distort:\n                    pairs.append((w, h))\n\n        crop_pair = random.choice(pairs)\n        if not self.fix_crop:\n            w_offset = random.randint(0, image_w - crop_pair[0])\n            h_offset = random.randint(0, image_h - crop_pair[1])\n        else:\n            w_offset, h_offset = self._sample_fix_offset(image_w, image_h, crop_pair[0], crop_pair[1])\n\n        return crop_pair[0], crop_pair[1], w_offset, h_offset\n\n    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h, crop_w, crop_h)\n        return random.choice(offsets)\n\n    @staticmethod\n    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n        w_step = (image_w - crop_w) // 4\n        h_step = (image_h - crop_h) // 4\n\n        ret = list()\n        ret.append((0, 0))  # upper left\n        ret.append((4 * w_step, 0))  # upper right\n        ret.append((0, 4 * h_step))  # lower left\n        ret.append((4 * w_step, 4 * h_step))  # lower right\n        ret.append((2 * w_step, 2 * h_step))  # center\n\n        if more_fix_crop:\n            ret.append((0, 2 * h_step))  # center left\n            ret.append((4 * w_step, 2 * h_step))  # center right\n            ret.append((2 * w_step, 4 * h_step))  # lower center\n            ret.append((2 * w_step, 0 * h_step))  # upper center\n\n            ret.append((1 * w_step, 1 * h_step))  # upper left quarter\n            ret.append((3 * w_step, 1 * h_step))  # upper right quarter\n            ret.append((1 * w_step, 3 * h_step))  # lower left quarter\n            ret.append((3 * w_step, 3 * h_step))  # lower righ quarter\n\n        return ret\n\n\nclass GroupRandomSizedCrop(object):\n    """"""Random crop the given PIL.Image to a random size of (0.08 to 1.0) of the original size\n    and and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio\n    This is popularly used to train the Inception networks\n    size: size of the smaller edge\n    interpolation: Default: PIL.Image.BILINEAR\n    """"""\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img_group):\n        for attempt in range(10):\n            area = img_group[0].size[0] * img_group[0].size[1]\n            target_area = random.uniform(0.08, 1.0) * area\n            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= img_group[0].size[0] and h <= img_group[0].size[1]:\n                x1 = random.randint(0, img_group[0].size[0] - w)\n                y1 = random.randint(0, img_group[0].size[1] - h)\n                found = True\n                break\n        else:\n            found = False\n            x1 = 0\n            y1 = 0\n\n        if found:\n            out_group = list()\n            for img in img_group:\n                img = img.crop((x1, y1, x1 + w, y1 + h))\n                assert(img.size == (w, h))\n                out_group.append(img.resize((self.size, self.size), self.interpolation))\n            return out_group\n        else:\n            # Fallback\n            scale = GroupScale(self.size, interpolation=self.interpolation)\n            crop = GroupRandomCrop(self.size)\n            return crop(scale(img_group))\n\n\nclass Stack(object):\n\n    def __init__(self, roll=False):\n        self.roll = roll\n\n    def __call__(self, img_group):\n        if img_group[0].mode == \'L\':\n            return np.concatenate([np.expand_dims(x, 2) for x in img_group], axis=2)\n        elif img_group[0].mode == \'RGB\':\n            if self.roll:\n                return np.concatenate([np.array(x)[:, :, ::-1] for x in img_group], axis=2)\n            else:\n                return np.concatenate(img_group, axis=2)\n\n\nclass ToTorchFormatTensor(object):\n    """""" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] """"""\n    def __init__(self, div=True):\n        self.div = div\n\n    def __call__(self, pic):\n        if isinstance(pic, np.ndarray):\n            # handle numpy array\n            img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n        else:\n            # handle PIL Image\n            img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n            img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n            # put it from HWC to CHW format\n            # yikes, this transpose takes 80% of the loading time/CPU\n            img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        return img.float().div(255) if self.div else img.float()\n\n\nclass IdentityTransform(object):\n\n    def __call__(self, data):\n        return data\n\n\nif __name__ == ""__main__"":\n    trans = torchvision.transforms.Compose([\n        GroupScale(256),\n        GroupRandomCrop(224),\n        Stack(),\n        ToTorchFormatTensor(),\n        GroupNormalize(\n            mean=[.485, .456, .406],\n            std=[.229, .224, .225]\n        )]\n    )\n\n    im = Image.open(\'../tensorflow-model-zoo.torch/lena_299.png\')\n\n    color_group = [im] * 3\n    rst = trans(color_group)\n\n    gray_group = [im.convert(\'L\')] * 9\n    gray_rst = trans(gray_group)\n\n    trans2 = torchvision.transforms.Compose([\n        GroupRandomSizedCrop(256),\n        Stack(),\n        ToTorchFormatTensor(),\n        GroupNormalize(\n            mean=[.485, .456, .406],\n            std=[.229, .224, .225])\n    ])\n    print(trans2(color_group))'"
model_zoo/__init__.py,0,"b'from .inceptionresnetv2.pytorch_load import inceptionresnetv2\nfrom .inceptionv4.pytorch_load import inceptionv4\nfrom .bninception.pytorch_load import BNInception, InceptionV3\n'"
model_zoo/bninception/__init__.py,0,b''
model_zoo/bninception/caffe_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: caffe.proto\n\nfrom google.protobuf.internal import enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'caffe.proto\',\n  package=\'caffe\',\n  serialized_pb=\'\\n\\x0b\\x63\\x61\\x66\\x66\\x65.proto\\x12\\x05\\x63\\x61\\x66\\x66\\x65\\""\\x1c\\n\\tBlobShape\\x12\\x0f\\n\\x03\\x64im\\x18\\x01 \\x03(\\x03\\x42\\x02\\x10\\x01\\""\\x9a\\x01\\n\\tBlobProto\\x12\\x1f\\n\\x05shape\\x18\\x07 \\x01(\\x0b\\x32\\x10.caffe.BlobShape\\x12\\x10\\n\\x04\\x64\\x61ta\\x18\\x05 \\x03(\\x02\\x42\\x02\\x10\\x01\\x12\\x10\\n\\x04\\x64iff\\x18\\x06 \\x03(\\x02\\x42\\x02\\x10\\x01\\x12\\x0e\\n\\x03num\\x18\\x01 \\x01(\\x05:\\x01\\x30\\x12\\x13\\n\\x08\\x63hannels\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x11\\n\\x06height\\x18\\x03 \\x01(\\x05:\\x01\\x30\\x12\\x10\\n\\x05width\\x18\\x04 \\x01(\\x05:\\x01\\x30\\""2\\n\\x0f\\x42lobProtoVector\\x12\\x1f\\n\\x05\\x62lobs\\x18\\x01 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\""\\x81\\x01\\n\\x05\\x44\\x61tum\\x12\\x10\\n\\x08\\x63hannels\\x18\\x01 \\x01(\\x05\\x12\\x0e\\n\\x06height\\x18\\x02 \\x01(\\x05\\x12\\r\\n\\x05width\\x18\\x03 \\x01(\\x05\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x04 \\x01(\\x0c\\x12\\r\\n\\x05label\\x18\\x05 \\x01(\\x05\\x12\\x12\\n\\nfloat_data\\x18\\x06 \\x03(\\x02\\x12\\x16\\n\\x07\\x65ncoded\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\x8a\\x02\\n\\x0f\\x46illerParameter\\x12\\x16\\n\\x04type\\x18\\x01 \\x01(\\t:\\x08\\x63onstant\\x12\\x10\\n\\x05value\\x18\\x02 \\x01(\\x02:\\x01\\x30\\x12\\x0e\\n\\x03min\\x18\\x03 \\x01(\\x02:\\x01\\x30\\x12\\x0e\\n\\x03max\\x18\\x04 \\x01(\\x02:\\x01\\x31\\x12\\x0f\\n\\x04mean\\x18\\x05 \\x01(\\x02:\\x01\\x30\\x12\\x0e\\n\\x03std\\x18\\x06 \\x01(\\x02:\\x01\\x31\\x12\\x12\\n\\x06sparse\\x18\\x07 \\x01(\\x05:\\x02-1\\x12\\x42\\n\\rvariance_norm\\x18\\x08 \\x01(\\x0e\\x32#.caffe.FillerParameter.VarianceNorm:\\x06\\x46\\x41N_IN\\""4\\n\\x0cVarianceNorm\\x12\\n\\n\\x06\\x46\\x41N_IN\\x10\\x00\\x12\\x0b\\n\\x07\\x46\\x41N_OUT\\x10\\x01\\x12\\x0b\\n\\x07\\x41VERAGE\\x10\\x02\\""\\xc6\\x02\\n\\x0cNetParameter\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05input\\x18\\x03 \\x03(\\t\\x12%\\n\\x0binput_shape\\x18\\x08 \\x03(\\x0b\\x32\\x10.caffe.BlobShape\\x12\\x11\\n\\tinput_dim\\x18\\x04 \\x03(\\x05\\x12\\x1d\\n\\x0e\\x66orce_backward\\x18\\x05 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1e\\n\\x05state\\x18\\x06 \\x01(\\x0b\\x32\\x0f.caffe.NetState\\x12\\x19\\n\\ndebug_info\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\x12$\\n\\x05layer\\x18\\x64 \\x03(\\x0b\\x32\\x15.caffe.LayerParameter\\x12\\x36\\n\\tmem_param\\x18\\xc8\\x01 \\x01(\\x0b\\x32\\"".caffe.MemoryOptimizationParameter\\x12\\\'\\n\\x06layers\\x18\\x02 \\x03(\\x0b\\x32\\x17.caffe.V1LayerParameter\\""\\xe3\\x08\\n\\x0fSolverParameter\\x12\\x0b\\n\\x03net\\x18\\x18 \\x01(\\t\\x12&\\n\\tnet_param\\x18\\x19 \\x01(\\x0b\\x32\\x13.caffe.NetParameter\\x12\\x11\\n\\ttrain_net\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x08test_net\\x18\\x02 \\x03(\\t\\x12,\\n\\x0ftrain_net_param\\x18\\x15 \\x01(\\x0b\\x32\\x13.caffe.NetParameter\\x12+\\n\\x0etest_net_param\\x18\\x16 \\x03(\\x0b\\x32\\x13.caffe.NetParameter\\x12$\\n\\x0btrain_state\\x18\\x1a \\x01(\\x0b\\x32\\x0f.caffe.NetState\\x12#\\n\\ntest_state\\x18\\x1b \\x03(\\x0b\\x32\\x0f.caffe.NetState\\x12\\x11\\n\\ttest_iter\\x18\\x03 \\x03(\\x05\\x12\\x18\\n\\rtest_interval\\x18\\x04 \\x01(\\x05:\\x01\\x30\\x12 \\n\\x11test_compute_loss\\x18\\x13 \\x01(\\x08:\\x05\\x66\\x61lse\\x12!\\n\\x13test_initialization\\x18  \\x01(\\x08:\\x04true\\x12\\x0f\\n\\x07\\x62\\x61se_lr\\x18\\x05 \\x01(\\x02\\x12\\x0f\\n\\x07\\x64isplay\\x18\\x06 \\x01(\\x05\\x12\\x17\\n\\x0c\\x61verage_loss\\x18! \\x01(\\x05:\\x01\\x31\\x12\\x10\\n\\x08max_iter\\x18\\x07 \\x01(\\x05\\x12\\x14\\n\\titer_size\\x18$ \\x01(\\x05:\\x01\\x31\\x12\\x11\\n\\tlr_policy\\x18\\x08 \\x01(\\t\\x12\\r\\n\\x05gamma\\x18\\t \\x01(\\x02\\x12\\r\\n\\x05power\\x18\\n \\x01(\\x02\\x12\\x10\\n\\x08momentum\\x18\\x0b \\x01(\\x02\\x12\\x14\\n\\x0cweight_decay\\x18\\x0c \\x01(\\x02\\x12\\x1f\\n\\x13regularization_type\\x18\\x1d \\x01(\\t:\\x02L2\\x12\\x10\\n\\x08stepsize\\x18\\r \\x01(\\x05\\x12\\x11\\n\\tstepvalue\\x18\\"" \\x03(\\x05\\x12\\x1a\\n\\x0e\\x63lip_gradients\\x18# \\x01(\\x02:\\x02-1\\x12\\x13\\n\\x08snapshot\\x18\\x0e \\x01(\\x05:\\x01\\x30\\x12\\x17\\n\\x0fsnapshot_prefix\\x18\\x0f \\x01(\\t\\x12\\x1c\\n\\rsnapshot_diff\\x18\\x10 \\x01(\\x08:\\x05\\x66\\x61lse\\x12;\\n\\x0bsolver_mode\\x18\\x11 \\x01(\\x0e\\x32!.caffe.SolverParameter.SolverMode:\\x03GPU\\x12\\x11\\n\\tdevice_id\\x18\\x12 \\x03(\\x05\\x12\\x10\\n\\x08group_id\\x18& \\x03(\\x05\\x12\\x17\\n\\x0brandom_seed\\x18\\x14 \\x01(\\x03:\\x02-1\\x12;\\n\\x0bsolver_type\\x18\\x1e \\x01(\\x0e\\x32!.caffe.SolverParameter.SolverType:\\x03SGD\\x12\\x14\\n\\x05\\x64\\x65lta\\x18\\x1f \\x01(\\x02:\\x05\\x31\\x65-08\\x12\\x19\\n\\ndebug_info\\x18\\x17 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\""\\n\\x14snapshot_after_train\\x18\\x1c \\x01(\\x08:\\x04true\\x12\\x15\\n\\x08richness\\x18% \\x01(\\x05:\\x03\\x33\\x30\\x30\\""\\x1e\\n\\nSolverMode\\x12\\x07\\n\\x03\\x43PU\\x10\\x00\\x12\\x07\\n\\x03GPU\\x10\\x01\\""0\\n\\nSolverType\\x12\\x07\\n\\x03SGD\\x10\\x00\\x12\\x0c\\n\\x08NESTEROV\\x10\\x01\\x12\\x0b\\n\\x07\\x41\\x44\\x41GRAD\\x10\\x02\\""l\\n\\x0bSolverState\\x12\\x0c\\n\\x04iter\\x18\\x01 \\x01(\\x05\\x12\\x13\\n\\x0blearned_net\\x18\\x02 \\x01(\\t\\x12!\\n\\x07history\\x18\\x03 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\x12\\x17\\n\\x0c\\x63urrent_step\\x18\\x04 \\x01(\\x05:\\x01\\x30\\""N\\n\\x08NetState\\x12!\\n\\x05phase\\x18\\x01 \\x01(\\x0e\\x32\\x0c.caffe.Phase:\\x04TEST\\x12\\x10\\n\\x05level\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\r\\n\\x05stage\\x18\\x03 \\x03(\\t\\""s\\n\\x0cNetStateRule\\x12\\x1b\\n\\x05phase\\x18\\x01 \\x01(\\x0e\\x32\\x0c.caffe.Phase\\x12\\x11\\n\\tmin_level\\x18\\x02 \\x01(\\x05\\x12\\x11\\n\\tmax_level\\x18\\x03 \\x01(\\x05\\x12\\r\\n\\x05stage\\x18\\x04 \\x03(\\t\\x12\\x11\\n\\tnot_stage\\x18\\x05 \\x03(\\t\\""\\xa3\\x01\\n\\tParamSpec\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x31\\n\\nshare_mode\\x18\\x02 \\x01(\\x0e\\x32\\x1d.caffe.ParamSpec.DimCheckMode\\x12\\x12\\n\\x07lr_mult\\x18\\x03 \\x01(\\x02:\\x01\\x31\\x12\\x15\\n\\ndecay_mult\\x18\\x04 \\x01(\\x02:\\x01\\x31\\""*\\n\\x0c\\x44imCheckMode\\x12\\n\\n\\x06STRICT\\x10\\x00\\x12\\x0e\\n\\nPERMISSIVE\\x10\\x01\\""\\x90\\x13\\n\\x0eLayerParameter\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04type\\x18\\x02 \\x01(\\t\\x12\\x0e\\n\\x06\\x62ottom\\x18\\x03 \\x03(\\t\\x12\\x0b\\n\\x03top\\x18\\x04 \\x03(\\t\\x12\\x1b\\n\\x05phase\\x18\\n \\x01(\\x0e\\x32\\x0c.caffe.Phase\\x12\\x13\\n\\x0bloss_weight\\x18\\x05 \\x03(\\x02\\x12\\x1f\\n\\x05param\\x18\\x06 \\x03(\\x0b\\x32\\x10.caffe.ParamSpec\\x12\\x1f\\n\\x05\\x62lobs\\x18\\x07 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\x12\\x16\\n\\x0epropagate_down\\x18\\x0b \\x03(\\x08\\x12$\\n\\x07include\\x18\\x08 \\x03(\\x0b\\x32\\x13.caffe.NetStateRule\\x12$\\n\\x07\\x65xclude\\x18\\t \\x03(\\x0b\\x32\\x13.caffe.NetStateRule\\x12\\x37\\n\\x0ftransform_param\\x18\\x64 \\x01(\\x0b\\x32\\x1e.caffe.TransformationParameter\\x12(\\n\\nloss_param\\x18\\x65 \\x01(\\x0b\\x32\\x14.caffe.LossParameter\\x12\\x30\\n\\x0e\\x61\\x63\\x63uracy_param\\x18\\x66 \\x01(\\x0b\\x32\\x18.caffe.AccuracyParameter\\x12,\\n\\x0c\\x61rgmax_param\\x18g \\x01(\\x0b\\x32\\x16.caffe.ArgMaxParameter\\x12%\\n\\x08\\x62n_param\\x18\\x89\\x01 \\x01(\\x0b\\x32\\x12.caffe.BNParameter\\x12,\\n\\x0c\\x63oncat_param\\x18h \\x01(\\x0b\\x32\\x16.caffe.ConcatParameter\\x12?\\n\\x16\\x63ontrastive_loss_param\\x18i \\x01(\\x0b\\x32\\x1f.caffe.ContrastiveLossParameter\\x12\\x36\\n\\x11\\x63onvolution_param\\x18j \\x01(\\x0b\\x32\\x1b.caffe.ConvolutionParameter\\x12(\\n\\ndata_param\\x18k \\x01(\\x0b\\x32\\x14.caffe.DataParameter\\x12.\\n\\rdropout_param\\x18l \\x01(\\x0b\\x32\\x17.caffe.DropoutParameter\\x12\\x33\\n\\x10\\x64ummy_data_param\\x18m \\x01(\\x0b\\x32\\x19.caffe.DummyDataParameter\\x12.\\n\\reltwise_param\\x18n \\x01(\\x0b\\x32\\x17.caffe.EltwiseParameter\\x12&\\n\\texp_param\\x18o \\x01(\\x0b\\x32\\x13.caffe.ExpParameter\\x12/\\n\\rflatten_param\\x18\\x87\\x01 \\x01(\\x0b\\x32\\x17.caffe.FlattenParameter\\x12\\x31\\n\\x0fhdf5_data_param\\x18p \\x01(\\x0b\\x32\\x18.caffe.HDF5DataParameter\\x12\\x35\\n\\x11hdf5_output_param\\x18q \\x01(\\x0b\\x32\\x1a.caffe.HDF5OutputParameter\\x12\\x33\\n\\x10hinge_loss_param\\x18r \\x01(\\x0b\\x32\\x19.caffe.HingeLossParameter\\x12\\x33\\n\\x10image_data_param\\x18s \\x01(\\x0b\\x32\\x19.caffe.ImageDataParameter\\x12\\x39\\n\\x13infogain_loss_param\\x18t \\x01(\\x0b\\x32\\x1c.caffe.InfogainLossParameter\\x12\\x39\\n\\x13inner_product_param\\x18u \\x01(\\x0b\\x32\\x1c.caffe.InnerProductParameter\\x12\\\'\\n\\tlog_param\\x18\\x86\\x01 \\x01(\\x0b\\x32\\x13.caffe.LogParameter\\x12&\\n\\tlrn_param\\x18v \\x01(\\x0b\\x32\\x13.caffe.LRNParameter\\x12\\x35\\n\\x11memory_data_param\\x18w \\x01(\\x0b\\x32\\x1a.caffe.MemoryDataParameter\\x12&\\n\\tmvn_param\\x18x \\x01(\\x0b\\x32\\x13.caffe.MVNParameter\\x12.\\n\\rpooling_param\\x18y \\x01(\\x0b\\x32\\x17.caffe.PoolingParameter\\x12*\\n\\x0bpower_param\\x18z \\x01(\\x0b\\x32\\x15.caffe.PowerParameter\\x12+\\n\\x0bprelu_param\\x18\\x83\\x01 \\x01(\\x0b\\x32\\x15.caffe.PReLUParameter\\x12-\\n\\x0cpython_param\\x18\\x82\\x01 \\x01(\\x0b\\x32\\x16.caffe.PythonParameter\\x12\\x33\\n\\x0freduction_param\\x18\\x88\\x01 \\x01(\\x0b\\x32\\x19.caffe.ReductionParameter\\x12(\\n\\nrelu_param\\x18{ \\x01(\\x0b\\x32\\x14.caffe.ReLUParameter\\x12/\\n\\rreshape_param\\x18\\x85\\x01 \\x01(\\x0b\\x32\\x17.caffe.ReshapeParameter\\x12\\x30\\n\\x0eseg_data_param\\x18\\x8d\\x01 \\x01(\\x0b\\x32\\x17.caffe.SegDataParameter\\x12.\\n\\rsigmoid_param\\x18| \\x01(\\x0b\\x32\\x17.caffe.SigmoidParameter\\x12.\\n\\rsoftmax_param\\x18} \\x01(\\x0b\\x32\\x17.caffe.SoftmaxParameter\\x12\\\'\\n\\tspp_param\\x18\\x84\\x01 \\x01(\\x0b\\x32\\x13.caffe.SPPParameter\\x12*\\n\\x0bslice_param\\x18~ \\x01(\\x0b\\x32\\x15.caffe.SliceParameter\\x12(\\n\\ntanh_param\\x18\\x7f \\x01(\\x0b\\x32\\x14.caffe.TanHParameter\\x12\\x33\\n\\x0fthreshold_param\\x18\\x80\\x01 \\x01(\\x0b\\x32\\x19.caffe.ThresholdParameter\\x12\\x36\\n\\x11window_data_param\\x18\\x81\\x01 \\x01(\\x0b\\x32\\x1a.caffe.WindowDataParameter\\x12\\x34\\n\\x10video_data_param\\x18\\x8c\\x01 \\x01(\\x0b\\x32\\x19.caffe.VideoDataParameter\\x12\\x36\\n\\x11roi_pooling_param\\x18\\x96\\x01 \\x01(\\x0b\\x32\\x1a.caffe.ROIPoolingParameter\\x12+\\n\\x0bscale_param\\x18\\xa0\\x01 \\x01(\\x0b\\x32\\x15.caffe.ScaleParameter\\x12)\\n\\nbias_param\\x18\\xa1\\x01 \\x01(\\x0b\\x32\\x14.caffe.BiasParameter\\x12>\\n\\x15\\x62\\x61tch_reduction_param\\x18\\xa2\\x01 \\x01(\\x0b\\x32\\x1e.caffe.BatchReductionParameter\\""\\xc0\\x03\\n\\x17TransformationParameter\\x12\\x10\\n\\x05scale\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x15\\n\\x06mirror\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x14\\n\\tcrop_size\\x18\\x03 \\x01(\\r:\\x01\\x30\\x12\\x11\\n\\tmean_file\\x18\\x04 \\x01(\\t\\x12\\x12\\n\\nmean_value\\x18\\x05 \\x03(\\x02\\x12\\x1a\\n\\x0b\\x66orce_color\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x19\\n\\nforce_gray\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x17\\n\\x08\\x66ix_crop\\x18\\n \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1c\\n\\rmore_fix_crop\\x18\\x0f \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1a\\n\\x0bmulti_scale\\x18\\x0b \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x14\\n\\x0cscale_ratios\\x18\\x0c \\x03(\\x02\\x12\\x16\\n\\x0bmax_distort\\x18\\r \\x01(\\x05:\\x01\\x31\\x12\\x16\\n\\x07is_flow\\x18\\x0e \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1d\\n\\x0eoriginal_image\\x18\\x14 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x11\\n\\x06stride\\x18\\x10 \\x01(\\x05:\\x01\\x31\\x12\\x12\\n\\nupper_size\\x18\\x11 \\x01(\\x05\\x12\\x14\\n\\x0cupper_height\\x18\\x12 \\x01(\\x05\\x12\\x13\\n\\x0bupper_width\\x18\\x13 \\x01(\\x05\\"">\\n\\rLossParameter\\x12\\x14\\n\\x0cignore_label\\x18\\x01 \\x01(\\x05\\x12\\x17\\n\\tnormalize\\x18\\x02 \\x01(\\x08:\\x04true\\""L\\n\\x11\\x41\\x63\\x63uracyParameter\\x12\\x10\\n\\x05top_k\\x18\\x01 \\x01(\\r:\\x01\\x31\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x31\\x12\\x14\\n\\x0cignore_label\\x18\\x03 \\x01(\\x05\\""?\\n\\x0f\\x41rgMaxParameter\\x12\\x1a\\n\\x0bout_max_val\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x10\\n\\x05top_k\\x18\\x02 \\x01(\\r:\\x01\\x31\\""\\x8b\\x02\\n\\x0b\\x42NParameter\\x12,\\n\\x0cslope_filler\\x18\\x01 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x02 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x15\\n\\x08momentum\\x18\\x03 \\x01(\\x02:\\x03\\x30.9\\x12\\x12\\n\\x03\\x65ps\\x18\\x04 \\x01(\\x02:\\x05\\x31\\x65-05\\x12\\x15\\n\\x06\\x66rozen\\x18\\x05 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x32\\n\\x06\\x65ngine\\x18\\x06 \\x01(\\x0e\\x32\\x19.caffe.BNParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""9\\n\\x0f\\x43oncatParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x31\\x12\\x15\\n\\nconcat_dim\\x18\\x01 \\x01(\\r:\\x01\\x31\\""L\\n\\x18\\x43ontrastiveLossParameter\\x12\\x11\\n\\x06margin\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x1d\\n\\x0elegacy_version\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\xfc\\x03\\n\\x14\\x43onvolutionParameter\\x12\\x12\\n\\nnum_output\\x18\\x01 \\x01(\\r\\x12\\x17\\n\\tbias_term\\x18\\x02 \\x01(\\x08:\\x04true\\x12\\x0e\\n\\x03pad\\x18\\x03 \\x01(\\r:\\x01\\x30\\x12\\x10\\n\\x05pad_h\\x18\\t \\x01(\\r:\\x01\\x30\\x12\\x10\\n\\x05pad_w\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x13\\n\\x0bkernel_size\\x18\\x04 \\x01(\\r\\x12\\x10\\n\\x08kernel_h\\x18\\x0b \\x01(\\r\\x12\\x10\\n\\x08kernel_w\\x18\\x0c \\x01(\\r\\x12\\x10\\n\\x05group\\x18\\x05 \\x01(\\r:\\x01\\x31\\x12\\x11\\n\\x06stride\\x18\\x06 \\x01(\\r:\\x01\\x31\\x12\\x10\\n\\x08stride_h\\x18\\r \\x01(\\r\\x12\\x10\\n\\x08stride_w\\x18\\x0e \\x01(\\r\\x12\\x13\\n\\x08\\x64ilation\\x18\\x10 \\x01(\\r:\\x01\\x31\\x12\\x12\\n\\ndilation_h\\x18\\x11 \\x01(\\r\\x12\\x12\\n\\ndilation_w\\x18\\x12 \\x01(\\r\\x12-\\n\\rweight_filler\\x18\\x07 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x08 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12;\\n\\x06\\x65ngine\\x18\\x0f \\x01(\\x0e\\x32\\"".caffe.ConvolutionParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""\\xa7\\x02\\n\\rDataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nbatch_size\\x18\\x04 \\x01(\\r\\x12\\x14\\n\\trand_skip\\x18\\x07 \\x01(\\r:\\x01\\x30\\x12\\x31\\n\\x07\\x62\\x61\\x63kend\\x18\\x08 \\x01(\\x0e\\x32\\x17.caffe.DataParameter.DB:\\x07LEVELDB\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x11\\n\\tmean_file\\x18\\x03 \\x01(\\t\\x12\\x14\\n\\tcrop_size\\x18\\x05 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\""\\n\\x13\\x66orce_encoded_color\\x18\\t \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x16\\n\\x07shuffle\\x18\\n \\x01(\\x08:\\x05\\x66\\x61lse\\""\\x1b\\n\\x02\\x44\\x42\\x12\\x0b\\n\\x07LEVELDB\\x10\\x00\\x12\\x08\\n\\x04LMDB\\x10\\x01\\"".\\n\\x10\\x44ropoutParameter\\x12\\x1a\\n\\rdropout_ratio\\x18\\x01 \\x01(\\x02:\\x03\\x30.5\\""\\xa0\\x01\\n\\x12\\x44ummyDataParameter\\x12+\\n\\x0b\\x64\\x61ta_filler\\x18\\x01 \\x03(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x1f\\n\\x05shape\\x18\\x06 \\x03(\\x0b\\x32\\x10.caffe.BlobShape\\x12\\x0b\\n\\x03num\\x18\\x02 \\x03(\\r\\x12\\x10\\n\\x08\\x63hannels\\x18\\x03 \\x03(\\r\\x12\\x0e\\n\\x06height\\x18\\x04 \\x03(\\r\\x12\\r\\n\\x05width\\x18\\x05 \\x03(\\r\\""\\xb9\\x01\\n\\x10\\x45ltwiseParameter\\x12\\x39\\n\\toperation\\x18\\x01 \\x01(\\x0e\\x32!.caffe.EltwiseParameter.EltwiseOp:\\x03SUM\\x12\\r\\n\\x05\\x63oeff\\x18\\x02 \\x03(\\x02\\x12\\x1e\\n\\x10stable_prod_grad\\x18\\x03 \\x01(\\x08:\\x04true\\"";\\n\\tEltwiseOp\\x12\\x08\\n\\x04PROD\\x10\\x00\\x12\\x07\\n\\x03SUM\\x10\\x01\\x12\\x07\\n\\x03MAX\\x10\\x02\\x12\\x12\\n\\x0eSTOCHASTIC_SUM\\x10\\x03\\""D\\n\\x0c\\x45xpParameter\\x12\\x10\\n\\x04\\x62\\x61se\\x18\\x01 \\x01(\\x02:\\x02-1\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x05shift\\x18\\x03 \\x01(\\x02:\\x01\\x30\\""9\\n\\x10\\x46lattenParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x01 \\x01(\\x05:\\x01\\x31\\x12\\x14\\n\\x08\\x65nd_axis\\x18\\x02 \\x01(\\x05:\\x02-1\\""O\\n\\x11HDF5DataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nbatch_size\\x18\\x02 \\x01(\\r\\x12\\x16\\n\\x07shuffle\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\""(\\n\\x13HDF5OutputParameter\\x12\\x11\\n\\tfile_name\\x18\\x01 \\x01(\\t\\""^\\n\\x12HingeLossParameter\\x12\\x30\\n\\x04norm\\x18\\x01 \\x01(\\x0e\\x32\\x1e.caffe.HingeLossParameter.Norm:\\x02L1\\""\\x16\\n\\x04Norm\\x12\\x06\\n\\x02L1\\x10\\x01\\x12\\x06\\n\\x02L2\\x10\\x02\\""\\x94\\x02\\n\\x12ImageDataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nbatch_size\\x18\\x04 \\x01(\\r\\x12\\x14\\n\\trand_skip\\x18\\x07 \\x01(\\r:\\x01\\x30\\x12\\x16\\n\\x07shuffle\\x18\\x08 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\nnew_height\\x18\\t \\x01(\\r:\\x01\\x30\\x12\\x14\\n\\tnew_width\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x16\\n\\x08is_color\\x18\\x0b \\x01(\\x08:\\x04true\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x11\\n\\tmean_file\\x18\\x03 \\x01(\\t\\x12\\x14\\n\\tcrop_size\\x18\\x05 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\x0broot_folder\\x18\\x0c \\x01(\\t:\\x00\\""\\xb8\\x03\\n\\x12VideoDataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nbatch_size\\x18\\x04 \\x01(\\r\\x12\\x14\\n\\trand_skip\\x18\\x07 \\x01(\\r:\\x01\\x30\\x12\\x16\\n\\x07shuffle\\x18\\x08 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\nnew_height\\x18\\t \\x01(\\r:\\x01\\x30\\x12\\x14\\n\\tnew_width\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\nnew_length\\x18\\x0b \\x01(\\r:\\x01\\x31\\x12\\x17\\n\\x0cnum_segments\\x18\\x0c \\x01(\\r:\\x01\\x31\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x11\\n\\tmean_file\\x18\\x03 \\x01(\\t\\x12\\x14\\n\\tcrop_size\\x18\\x05 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12:\\n\\x08modality\\x18\\r \\x01(\\x0e\\x32\\"".caffe.VideoDataParameter.Modality:\\x04\\x46LOW\\x12\\x14\\n\\x0cname_pattern\\x18\\x0e \\x01(\\t\\x12\\x16\\n\\x07\\x65ncoded\\x18\\x0f \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x18\\n\\tgrayscale\\x18\\x10 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\x1d\\n\\x08Modality\\x12\\x07\\n\\x03RGB\\x10\\x00\\x12\\x08\\n\\x04\\x46LOW\\x10\\x01\\""\\\'\\n\\x15InfogainLossParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\""\\xb1\\x01\\n\\x15InnerProductParameter\\x12\\x12\\n\\nnum_output\\x18\\x01 \\x01(\\r\\x12\\x17\\n\\tbias_term\\x18\\x02 \\x01(\\x08:\\x04true\\x12-\\n\\rweight_filler\\x18\\x03 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x04 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x05 \\x01(\\x05:\\x01\\x31\\""D\\n\\x0cLogParameter\\x12\\x10\\n\\x04\\x62\\x61se\\x18\\x01 \\x01(\\x02:\\x02-1\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x05shift\\x18\\x03 \\x01(\\x02:\\x01\\x30\\""\\xd6\\x01\\n\\x0cLRNParameter\\x12\\x15\\n\\nlocal_size\\x18\\x01 \\x01(\\r:\\x01\\x35\\x12\\x10\\n\\x05\\x61lpha\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x12\\n\\x04\\x62\\x65ta\\x18\\x03 \\x01(\\x02:\\x04\\x30.75\\x12\\x44\\n\\x0bnorm_region\\x18\\x04 \\x01(\\x0e\\x32\\x1e.caffe.LRNParameter.NormRegion:\\x0f\\x41\\x43ROSS_CHANNELS\\x12\\x0c\\n\\x01k\\x18\\x05 \\x01(\\x02:\\x01\\x31\\""5\\n\\nNormRegion\\x12\\x13\\n\\x0f\\x41\\x43ROSS_CHANNELS\\x10\\x00\\x12\\x12\\n\\x0eWITHIN_CHANNEL\\x10\\x01\\""Z\\n\\x13MemoryDataParameter\\x12\\x12\\n\\nbatch_size\\x18\\x01 \\x01(\\r\\x12\\x10\\n\\x08\\x63hannels\\x18\\x02 \\x01(\\r\\x12\\x0e\\n\\x06height\\x18\\x03 \\x01(\\r\\x12\\r\\n\\x05width\\x18\\x04 \\x01(\\r\\""d\\n\\x0cMVNParameter\\x12 \\n\\x12normalize_variance\\x18\\x01 \\x01(\\x08:\\x04true\\x12\\x1e\\n\\x0f\\x61\\x63ross_channels\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x12\\n\\x03\\x65ps\\x18\\x03 \\x01(\\x02:\\x05\\x31\\x65-09\\""\\xa2\\x03\\n\\x10PoolingParameter\\x12\\x35\\n\\x04pool\\x18\\x01 \\x01(\\x0e\\x32\\"".caffe.PoolingParameter.PoolMethod:\\x03MAX\\x12\\x0e\\n\\x03pad\\x18\\x04 \\x01(\\r:\\x01\\x30\\x12\\x10\\n\\x05pad_h\\x18\\t \\x01(\\r:\\x01\\x30\\x12\\x10\\n\\x05pad_w\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x13\\n\\x0bkernel_size\\x18\\x02 \\x01(\\r\\x12\\x10\\n\\x08kernel_h\\x18\\x05 \\x01(\\r\\x12\\x10\\n\\x08kernel_w\\x18\\x06 \\x01(\\r\\x12\\x11\\n\\x06stride\\x18\\x03 \\x01(\\r:\\x01\\x31\\x12\\x10\\n\\x08stride_h\\x18\\x07 \\x01(\\r\\x12\\x10\\n\\x08stride_w\\x18\\x08 \\x01(\\r\\x12\\x37\\n\\x06\\x65ngine\\x18\\x0b \\x01(\\x0e\\x32\\x1e.caffe.PoolingParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\x12\\x1d\\n\\x0eglobal_pooling\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lse\\"".\\n\\nPoolMethod\\x12\\x07\\n\\x03MAX\\x10\\x00\\x12\\x07\\n\\x03\\x41VE\\x10\\x01\\x12\\x0e\\n\\nSTOCHASTIC\\x10\\x02\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""F\\n\\x0ePowerParameter\\x12\\x10\\n\\x05power\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x05shift\\x18\\x03 \\x01(\\x02:\\x01\\x30\\""C\\n\\x0fPythonParameter\\x12\\x0e\\n\\x06module\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05layer\\x18\\x02 \\x01(\\t\\x12\\x11\\n\\tparam_str\\x18\\x03 \\x01(\\t\\""\\xc5\\x01\\n\\x12ReductionParameter\\x12=\\n\\toperation\\x18\\x01 \\x01(\\x0e\\x32%.caffe.ReductionParameter.ReductionOp:\\x03SUM\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x10\\n\\x05\\x63oeff\\x18\\x03 \\x01(\\x02:\\x01\\x31\\x12\\x0c\\n\\x01k\\x18\\x04 \\x01(\\x05:\\x01\\x31\\""?\\n\\x0bReductionOp\\x12\\x07\\n\\x03SUM\\x10\\x01\\x12\\x08\\n\\x04\\x41SUM\\x10\\x02\\x12\\t\\n\\x05SUMSQ\\x10\\x03\\x12\\x08\\n\\x04MEAN\\x10\\x04\\x12\\x08\\n\\x04TOPK\\x10\\x05\\""\\x8d\\x01\\n\\rReLUParameter\\x12\\x19\\n\\x0enegative_slope\\x18\\x01 \\x01(\\x02:\\x01\\x30\\x12\\x34\\n\\x06\\x65ngine\\x18\\x02 \\x01(\\x0e\\x32\\x1b.caffe.ReLUParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""Z\\n\\x10ReshapeParameter\\x12\\x1f\\n\\x05shape\\x18\\x01 \\x01(\\x0b\\x32\\x10.caffe.BlobShape\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x14\\n\\x08num_axes\\x18\\x03 \\x01(\\x05:\\x02-1\\""d\\n\\x10SegDataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x08root_dir\\x18\\x02 \\x01(\\t\\x12\\x16\\n\\x07shuffle\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x16\\n\\x07\\x62\\x61lance\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse\\""x\\n\\x10SigmoidParameter\\x12\\x37\\n\\x06\\x65ngine\\x18\\x01 \\x01(\\x0e\\x32\\x1e.caffe.SigmoidParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""L\\n\\x0eSliceParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x03 \\x01(\\x05:\\x01\\x31\\x12\\x13\\n\\x0bslice_point\\x18\\x02 \\x03(\\r\\x12\\x14\\n\\tslice_dim\\x18\\x01 \\x01(\\r:\\x01\\x31\\""\\x89\\x01\\n\\x10SoftmaxParameter\\x12\\x37\\n\\x06\\x65ngine\\x18\\x01 \\x01(\\x0e\\x32\\x1e.caffe.SoftmaxParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\x12\\x0f\\n\\x04\\x61xis\\x18\\x02 \\x01(\\x05:\\x01\\x31\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""r\\n\\rTanHParameter\\x12\\x34\\n\\x06\\x65ngine\\x18\\x01 \\x01(\\x0e\\x32\\x1b.caffe.TanHParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""*\\n\\x12ThresholdParameter\\x12\\x14\\n\\tthreshold\\x18\\x01 \\x01(\\x02:\\x01\\x30\\""\\xc1\\x02\\n\\x13WindowDataParameter\\x12\\x0e\\n\\x06source\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x05scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x11\\n\\tmean_file\\x18\\x03 \\x01(\\t\\x12\\x12\\n\\nbatch_size\\x18\\x04 \\x01(\\r\\x12\\x14\\n\\tcrop_size\\x18\\x05 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x19\\n\\x0c\\x66g_threshold\\x18\\x07 \\x01(\\x02:\\x03\\x30.5\\x12\\x19\\n\\x0c\\x62g_threshold\\x18\\x08 \\x01(\\x02:\\x03\\x30.5\\x12\\x19\\n\\x0b\\x66g_fraction\\x18\\t \\x01(\\x02:\\x04\\x30.25\\x12\\x16\\n\\x0b\\x63ontext_pad\\x18\\n \\x01(\\r:\\x01\\x30\\x12\\x17\\n\\tcrop_mode\\x18\\x0b \\x01(\\t:\\x04warp\\x12\\x1b\\n\\x0c\\x63\\x61\\x63he_images\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\x0broot_folder\\x18\\r \\x01(\\t:\\x00\\""\\xeb\\x01\\n\\x0cSPPParameter\\x12\\x16\\n\\x0epyramid_height\\x18\\x01 \\x01(\\r\\x12\\x31\\n\\x04pool\\x18\\x02 \\x01(\\x0e\\x32\\x1e.caffe.SPPParameter.PoolMethod:\\x03MAX\\x12\\x33\\n\\x06\\x65ngine\\x18\\x06 \\x01(\\x0e\\x32\\x1a.caffe.SPPParameter.Engine:\\x07\\x44\\x45\\x46\\x41ULT\\"".\\n\\nPoolMethod\\x12\\x07\\n\\x03MAX\\x10\\x00\\x12\\x07\\n\\x03\\x41VE\\x10\\x01\\x12\\x0e\\n\\nSTOCHASTIC\\x10\\x02\\""+\\n\\x06\\x45ngine\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\t\\n\\x05\\x43\\x41\\x46\\x46\\x45\\x10\\x01\\x12\\t\\n\\x05\\x43UDNN\\x10\\x02\\""Y\\n\\x13ROIPoolingParameter\\x12\\x13\\n\\x08pooled_h\\x18\\x01 \\x01(\\r:\\x01\\x30\\x12\\x13\\n\\x08pooled_w\\x18\\x02 \\x01(\\r:\\x01\\x30\\x12\\x18\\n\\rspatial_scale\\x18\\x03 \\x01(\\x02:\\x01\\x31\\""\\xe0\\x13\\n\\x10V1LayerParameter\\x12\\x0e\\n\\x06\\x62ottom\\x18\\x02 \\x03(\\t\\x12\\x0b\\n\\x03top\\x18\\x03 \\x03(\\t\\x12\\x0c\\n\\x04name\\x18\\x04 \\x01(\\t\\x12$\\n\\x07include\\x18  \\x03(\\x0b\\x32\\x13.caffe.NetStateRule\\x12$\\n\\x07\\x65xclude\\x18! \\x03(\\x0b\\x32\\x13.caffe.NetStateRule\\x12/\\n\\x04type\\x18\\x05 \\x01(\\x0e\\x32!.caffe.V1LayerParameter.LayerType\\x12\\x1f\\n\\x05\\x62lobs\\x18\\x06 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\x12\\x0e\\n\\x05param\\x18\\xe9\\x07 \\x03(\\t\\x12>\\n\\x0f\\x62lob_share_mode\\x18\\xea\\x07 \\x03(\\x0e\\x32$.caffe.V1LayerParameter.DimCheckMode\\x12\\x10\\n\\x08\\x62lobs_lr\\x18\\x07 \\x03(\\x02\\x12\\x14\\n\\x0cweight_decay\\x18\\x08 \\x03(\\x02\\x12\\x13\\n\\x0bloss_weight\\x18# \\x03(\\x02\\x12\\x30\\n\\x0e\\x61\\x63\\x63uracy_param\\x18\\x1b \\x01(\\x0b\\x32\\x18.caffe.AccuracyParameter\\x12,\\n\\x0c\\x61rgmax_param\\x18\\x17 \\x01(\\x0b\\x32\\x16.caffe.ArgMaxParameter\\x12,\\n\\x0c\\x63oncat_param\\x18\\t \\x01(\\x0b\\x32\\x16.caffe.ConcatParameter\\x12?\\n\\x16\\x63ontrastive_loss_param\\x18( \\x01(\\x0b\\x32\\x1f.caffe.ContrastiveLossParameter\\x12\\x36\\n\\x11\\x63onvolution_param\\x18\\n \\x01(\\x0b\\x32\\x1b.caffe.ConvolutionParameter\\x12(\\n\\ndata_param\\x18\\x0b \\x01(\\x0b\\x32\\x14.caffe.DataParameter\\x12.\\n\\rdropout_param\\x18\\x0c \\x01(\\x0b\\x32\\x17.caffe.DropoutParameter\\x12\\x33\\n\\x10\\x64ummy_data_param\\x18\\x1a \\x01(\\x0b\\x32\\x19.caffe.DummyDataParameter\\x12.\\n\\reltwise_param\\x18\\x18 \\x01(\\x0b\\x32\\x17.caffe.EltwiseParameter\\x12&\\n\\texp_param\\x18) \\x01(\\x0b\\x32\\x13.caffe.ExpParameter\\x12\\x31\\n\\x0fhdf5_data_param\\x18\\r \\x01(\\x0b\\x32\\x18.caffe.HDF5DataParameter\\x12\\x35\\n\\x11hdf5_output_param\\x18\\x0e \\x01(\\x0b\\x32\\x1a.caffe.HDF5OutputParameter\\x12\\x33\\n\\x10hinge_loss_param\\x18\\x1d \\x01(\\x0b\\x32\\x19.caffe.HingeLossParameter\\x12\\x33\\n\\x10image_data_param\\x18\\x0f \\x01(\\x0b\\x32\\x19.caffe.ImageDataParameter\\x12\\x39\\n\\x13infogain_loss_param\\x18\\x10 \\x01(\\x0b\\x32\\x1c.caffe.InfogainLossParameter\\x12\\x39\\n\\x13inner_product_param\\x18\\x11 \\x01(\\x0b\\x32\\x1c.caffe.InnerProductParameter\\x12&\\n\\tlrn_param\\x18\\x12 \\x01(\\x0b\\x32\\x13.caffe.LRNParameter\\x12\\x35\\n\\x11memory_data_param\\x18\\x16 \\x01(\\x0b\\x32\\x1a.caffe.MemoryDataParameter\\x12&\\n\\tmvn_param\\x18\\"" \\x01(\\x0b\\x32\\x13.caffe.MVNParameter\\x12.\\n\\rpooling_param\\x18\\x13 \\x01(\\x0b\\x32\\x17.caffe.PoolingParameter\\x12*\\n\\x0bpower_param\\x18\\x15 \\x01(\\x0b\\x32\\x15.caffe.PowerParameter\\x12(\\n\\nrelu_param\\x18\\x1e \\x01(\\x0b\\x32\\x14.caffe.ReLUParameter\\x12.\\n\\rsigmoid_param\\x18& \\x01(\\x0b\\x32\\x17.caffe.SigmoidParameter\\x12.\\n\\rsoftmax_param\\x18\\\' \\x01(\\x0b\\x32\\x17.caffe.SoftmaxParameter\\x12*\\n\\x0bslice_param\\x18\\x1f \\x01(\\x0b\\x32\\x15.caffe.SliceParameter\\x12(\\n\\ntanh_param\\x18% \\x01(\\x0b\\x32\\x14.caffe.TanHParameter\\x12\\x32\\n\\x0fthreshold_param\\x18\\x19 \\x01(\\x0b\\x32\\x19.caffe.ThresholdParameter\\x12\\x35\\n\\x11window_data_param\\x18\\x14 \\x01(\\x0b\\x32\\x1a.caffe.WindowDataParameter\\x12\\x37\\n\\x0ftransform_param\\x18$ \\x01(\\x0b\\x32\\x1e.caffe.TransformationParameter\\x12(\\n\\nloss_param\\x18* \\x01(\\x0b\\x32\\x14.caffe.LossParameter\\x12&\\n\\x05layer\\x18\\x01 \\x01(\\x0b\\x32\\x17.caffe.V0LayerParameter\\""\\xd8\\x04\\n\\tLayerType\\x12\\x08\\n\\x04NONE\\x10\\x00\\x12\\n\\n\\x06\\x41\\x42SVAL\\x10#\\x12\\x0c\\n\\x08\\x41\\x43\\x43URACY\\x10\\x01\\x12\\n\\n\\x06\\x41RGMAX\\x10\\x1e\\x12\\x08\\n\\x04\\x42NLL\\x10\\x02\\x12\\n\\n\\x06\\x43ONCAT\\x10\\x03\\x12\\x14\\n\\x10\\x43ONTRASTIVE_LOSS\\x10%\\x12\\x0f\\n\\x0b\\x43ONVOLUTION\\x10\\x04\\x12\\x08\\n\\x04\\x44\\x41TA\\x10\\x05\\x12\\x11\\n\\rDECONVOLUTION\\x10\\\'\\x12\\x0b\\n\\x07\\x44ROPOUT\\x10\\x06\\x12\\x0e\\n\\nDUMMY_DATA\\x10 \\x12\\x12\\n\\x0e\\x45UCLIDEAN_LOSS\\x10\\x07\\x12\\x0b\\n\\x07\\x45LTWISE\\x10\\x19\\x12\\x07\\n\\x03\\x45XP\\x10&\\x12\\x0b\\n\\x07\\x46LATTEN\\x10\\x08\\x12\\r\\n\\tHDF5_DATA\\x10\\t\\x12\\x0f\\n\\x0bHDF5_OUTPUT\\x10\\n\\x12\\x0e\\n\\nHINGE_LOSS\\x10\\x1c\\x12\\n\\n\\x06IM2COL\\x10\\x0b\\x12\\x0e\\n\\nIMAGE_DATA\\x10\\x0c\\x12\\x11\\n\\rINFOGAIN_LOSS\\x10\\r\\x12\\x11\\n\\rINNER_PRODUCT\\x10\\x0e\\x12\\x07\\n\\x03LRN\\x10\\x0f\\x12\\x0f\\n\\x0bMEMORY_DATA\\x10\\x1d\\x12\\x1d\\n\\x19MULTINOMIAL_LOGISTIC_LOSS\\x10\\x10\\x12\\x07\\n\\x03MVN\\x10\\""\\x12\\x0b\\n\\x07POOLING\\x10\\x11\\x12\\t\\n\\x05POWER\\x10\\x1a\\x12\\x08\\n\\x04RELU\\x10\\x12\\x12\\x0b\\n\\x07SIGMOID\\x10\\x13\\x12\\x1e\\n\\x1aSIGMOID_CROSS_ENTROPY_LOSS\\x10\\x1b\\x12\\x0b\\n\\x07SILENCE\\x10$\\x12\\x0b\\n\\x07SOFTMAX\\x10\\x14\\x12\\x10\\n\\x0cSOFTMAX_LOSS\\x10\\x15\\x12\\t\\n\\x05SPLIT\\x10\\x16\\x12\\t\\n\\x05SLICE\\x10!\\x12\\x08\\n\\x04TANH\\x10\\x17\\x12\\x0f\\n\\x0bWINDOW_DATA\\x10\\x18\\x12\\r\\n\\tTHRESHOLD\\x10\\x1f\\""*\\n\\x0c\\x44imCheckMode\\x12\\n\\n\\x06STRICT\\x10\\x00\\x12\\x0e\\n\\nPERMISSIVE\\x10\\x01\\""\\xfd\\x07\\n\\x10V0LayerParameter\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x0c\\n\\x04type\\x18\\x02 \\x01(\\t\\x12\\x12\\n\\nnum_output\\x18\\x03 \\x01(\\r\\x12\\x16\\n\\x08\\x62iasterm\\x18\\x04 \\x01(\\x08:\\x04true\\x12-\\n\\rweight_filler\\x18\\x05 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12+\\n\\x0b\\x62ias_filler\\x18\\x06 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x0e\\n\\x03pad\\x18\\x07 \\x01(\\r:\\x01\\x30\\x12\\x12\\n\\nkernelsize\\x18\\x08 \\x01(\\r\\x12\\x10\\n\\x05group\\x18\\t \\x01(\\r:\\x01\\x31\\x12\\x11\\n\\x06stride\\x18\\n \\x01(\\r:\\x01\\x31\\x12\\x35\\n\\x04pool\\x18\\x0b \\x01(\\x0e\\x32\\"".caffe.V0LayerParameter.PoolMethod:\\x03MAX\\x12\\x1a\\n\\rdropout_ratio\\x18\\x0c \\x01(\\x02:\\x03\\x30.5\\x12\\x15\\n\\nlocal_size\\x18\\r \\x01(\\r:\\x01\\x35\\x12\\x10\\n\\x05\\x61lpha\\x18\\x0e \\x01(\\x02:\\x01\\x31\\x12\\x12\\n\\x04\\x62\\x65ta\\x18\\x0f \\x01(\\x02:\\x04\\x30.75\\x12\\x0c\\n\\x01k\\x18\\x16 \\x01(\\x02:\\x01\\x31\\x12\\x0e\\n\\x06source\\x18\\x10 \\x01(\\t\\x12\\x10\\n\\x05scale\\x18\\x11 \\x01(\\x02:\\x01\\x31\\x12\\x10\\n\\x08meanfile\\x18\\x12 \\x01(\\t\\x12\\x11\\n\\tbatchsize\\x18\\x13 \\x01(\\r\\x12\\x13\\n\\x08\\x63ropsize\\x18\\x14 \\x01(\\r:\\x01\\x30\\x12\\x15\\n\\x06mirror\\x18\\x15 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1f\\n\\x05\\x62lobs\\x18\\x32 \\x03(\\x0b\\x32\\x10.caffe.BlobProto\\x12\\x10\\n\\x08\\x62lobs_lr\\x18\\x33 \\x03(\\x02\\x12\\x14\\n\\x0cweight_decay\\x18\\x34 \\x03(\\x02\\x12\\x14\\n\\trand_skip\\x18\\x35 \\x01(\\r:\\x01\\x30\\x12\\x1d\\n\\x10\\x64\\x65t_fg_threshold\\x18\\x36 \\x01(\\x02:\\x03\\x30.5\\x12\\x1d\\n\\x10\\x64\\x65t_bg_threshold\\x18\\x37 \\x01(\\x02:\\x03\\x30.5\\x12\\x1d\\n\\x0f\\x64\\x65t_fg_fraction\\x18\\x38 \\x01(\\x02:\\x04\\x30.25\\x12\\x1a\\n\\x0f\\x64\\x65t_context_pad\\x18: \\x01(\\r:\\x01\\x30\\x12\\x1b\\n\\rdet_crop_mode\\x18; \\x01(\\t:\\x04warp\\x12\\x12\\n\\x07new_num\\x18< \\x01(\\x05:\\x01\\x30\\x12\\x17\\n\\x0cnew_channels\\x18= \\x01(\\x05:\\x01\\x30\\x12\\x15\\n\\nnew_height\\x18> \\x01(\\x05:\\x01\\x30\\x12\\x14\\n\\tnew_width\\x18? \\x01(\\x05:\\x01\\x30\\x12\\x1d\\n\\x0eshuffle_images\\x18@ \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x15\\n\\nconcat_dim\\x18\\x41 \\x01(\\r:\\x01\\x31\\x12\\x36\\n\\x11hdf5_output_param\\x18\\xe9\\x07 \\x01(\\x0b\\x32\\x1a.caffe.HDF5OutputParameter\\"".\\n\\nPoolMethod\\x12\\x07\\n\\x03MAX\\x10\\x00\\x12\\x07\\n\\x03\\x41VE\\x10\\x01\\x12\\x0e\\n\\nSTOCHASTIC\\x10\\x02\\""W\\n\\x0ePReLUParameter\\x12&\\n\\x06\\x66iller\\x18\\x01 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x1d\\n\\x0e\\x63hannel_shared\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\xa5\\x01\\n\\x0eScaleParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x01 \\x01(\\x05:\\x01\\x31\\x12\\x13\\n\\x08num_axes\\x18\\x02 \\x01(\\x05:\\x01\\x31\\x12&\\n\\x06\\x66iller\\x18\\x03 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\x12\\x18\\n\\tbias_term\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse\\x12+\\n\\x0b\\x62ias_filler\\x18\\x05 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\""]\\n\\rBiasParameter\\x12\\x0f\\n\\x04\\x61xis\\x18\\x01 \\x01(\\x05:\\x01\\x31\\x12\\x13\\n\\x08num_axes\\x18\\x02 \\x01(\\x05:\\x01\\x31\\x12&\\n\\x06\\x66iller\\x18\\x03 \\x01(\\x0b\\x32\\x16.caffe.FillerParameter\\""p\\n\\x17\\x42\\x61tchReductionParameter\\x12\\r\\n\\x05level\\x18\\x01 \\x03(\\x05\\x12\\x32\\n\\x0freduction_param\\x18\\x02 \\x01(\\x0b\\x32\\x19.caffe.ReductionParameter\\x12\\x12\\n\\x03pos\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\""o\\n\\x1bMemoryOptimizationParameter\\x12\\x1c\\n\\x0eoptimize_train\\x18\\x01 \\x01(\\x08:\\x04true\\x12\\x1c\\n\\roptimize_test\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x14\\n\\x0c\\x65xclude_blob\\x18\\x03 \\x03(\\t*\\x1c\\n\\x05Phase\\x12\\t\\n\\x05TRAIN\\x10\\x00\\x12\\x08\\n\\x04TEST\\x10\\x01\')\n\n_PHASE = _descriptor.EnumDescriptor(\n  name=\'Phase\',\n  full_name=\'caffe.Phase\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'TRAIN\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'TEST\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=15473,\n  serialized_end=15501,\n)\n\nPhase = enum_type_wrapper.EnumTypeWrapper(_PHASE)\nTRAIN = 0\nTEST = 1\n\n\n_FILLERPARAMETER_VARIANCENORM = _descriptor.EnumDescriptor(\n  name=\'VarianceNorm\',\n  full_name=\'caffe.FillerParameter.VarianceNorm\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'FAN_IN\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FAN_OUT\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AVERAGE\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=608,\n  serialized_end=660,\n)\n\n_SOLVERPARAMETER_SOLVERMODE = _descriptor.EnumDescriptor(\n  name=\'SolverMode\',\n  full_name=\'caffe.SolverParameter.SolverMode\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'CPU\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'GPU\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=2035,\n  serialized_end=2065,\n)\n\n_SOLVERPARAMETER_SOLVERTYPE = _descriptor.EnumDescriptor(\n  name=\'SolverType\',\n  full_name=\'caffe.SolverParameter.SolverType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'SGD\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NESTEROV\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ADAGRAD\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=2067,\n  serialized_end=2115,\n)\n\n_PARAMSPEC_DIMCHECKMODE = _descriptor.EnumDescriptor(\n  name=\'DimCheckMode\',\n  full_name=\'caffe.ParamSpec.DimCheckMode\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'STRICT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'PERMISSIVE\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=2546,\n  serialized_end=2588,\n)\n\n_BNPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.BNParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=5924,\n  serialized_end=5967,\n)\n\n_CONVOLUTIONPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.ConvolutionParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=5924,\n  serialized_end=5967,\n)\n\n_DATAPARAMETER_DB = _descriptor.EnumDescriptor(\n  name=\'DB\',\n  full_name=\'caffe.DataParameter.DB\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'LEVELDB\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'LMDB\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=6886,\n  serialized_end=6913,\n)\n\n_ELTWISEPARAMETER_ELTWISEOP = _descriptor.EnumDescriptor(\n  name=\'EltwiseOp\',\n  full_name=\'caffe.EltwiseParameter.EltwiseOp\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'PROD\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SUM\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MAX\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STOCHASTIC_SUM\', index=3, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=7253,\n  serialized_end=7312,\n)\n\n_HINGELOSSPARAMETER_NORM = _descriptor.EnumDescriptor(\n  name=\'Norm\',\n  full_name=\'caffe.HingeLossParameter.Norm\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'L1\', index=0, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'L2\', index=1, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=7638,\n  serialized_end=7660,\n)\n\n_VIDEODATAPARAMETER_MODALITY = _descriptor.EnumDescriptor(\n  name=\'Modality\',\n  full_name=\'caffe.VideoDataParameter.Modality\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'RGB\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FLOW\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=8353,\n  serialized_end=8382,\n)\n\n_LRNPARAMETER_NORMREGION = _descriptor.EnumDescriptor(\n  name=\'NormRegion\',\n  full_name=\'caffe.LRNParameter.NormRegion\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'ACROSS_CHANNELS\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'WITHIN_CHANNEL\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=8837,\n  serialized_end=8890,\n)\n\n_POOLINGPARAMETER_POOLMETHOD = _descriptor.EnumDescriptor(\n  name=\'PoolMethod\',\n  full_name=\'caffe.PoolingParameter.PoolMethod\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'MAX\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AVE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STOCHASTIC\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=9414,\n  serialized_end=9460,\n)\n\n_POOLINGPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.PoolingParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=5924,\n  serialized_end=5967,\n)\n\n_REDUCTIONPARAMETER_REDUCTIONOP = _descriptor.EnumDescriptor(\n  name=\'ReductionOp\',\n  full_name=\'caffe.ReductionParameter.ReductionOp\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'SUM\', index=0, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ASUM\', index=1, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SUMSQ\', index=2, number=3,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MEAN\', index=3, number=4,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'TOPK\', index=4, number=5,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=9783,\n  serialized_end=9846,\n)\n\n_RELUPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.ReLUParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=5924,\n  serialized_end=5967,\n)\n\n_SIGMOIDPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.SigmoidParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=5924,\n  serialized_end=5967,\n)\n\n_SOFTMAXPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.SoftmaxParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=5924,\n  serialized_end=5967,\n)\n\n_TANHPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.TanHParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=5924,\n  serialized_end=5967,\n)\n\n_SPPPARAMETER_POOLMETHOD = _descriptor.EnumDescriptor(\n  name=\'PoolMethod\',\n  full_name=\'caffe.SPPParameter.PoolMethod\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'MAX\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AVE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STOCHASTIC\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=9414,\n  serialized_end=9460,\n)\n\n_SPPPARAMETER_ENGINE = _descriptor.EnumDescriptor(\n  name=\'Engine\',\n  full_name=\'caffe.SPPParameter.Engine\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CAFFE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CUDNN\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=5924,\n  serialized_end=5967,\n)\n\n_V1LAYERPARAMETER_LAYERTYPE = _descriptor.EnumDescriptor(\n  name=\'LayerType\',\n  full_name=\'caffe.V1LayerParameter.LayerType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'NONE\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ABSVAL\', index=1, number=35,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ACCURACY\', index=2, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ARGMAX\', index=3, number=30,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BNLL\', index=4, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CONCAT\', index=5, number=3,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CONTRASTIVE_LOSS\', index=6, number=37,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CONVOLUTION\', index=7, number=4,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DATA\', index=8, number=5,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DECONVOLUTION\', index=9, number=39,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DROPOUT\', index=10, number=6,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'DUMMY_DATA\', index=11, number=32,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'EUCLIDEAN_LOSS\', index=12, number=7,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ELTWISE\', index=13, number=25,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'EXP\', index=14, number=38,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FLATTEN\', index=15, number=8,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'HDF5_DATA\', index=16, number=9,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'HDF5_OUTPUT\', index=17, number=10,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'HINGE_LOSS\', index=18, number=28,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'IM2COL\', index=19, number=11,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'IMAGE_DATA\', index=20, number=12,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'INFOGAIN_LOSS\', index=21, number=13,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'INNER_PRODUCT\', index=22, number=14,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'LRN\', index=23, number=15,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MEMORY_DATA\', index=24, number=29,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MULTINOMIAL_LOGISTIC_LOSS\', index=25, number=16,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'MVN\', index=26, number=34,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'POOLING\', index=27, number=17,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'POWER\', index=28, number=26,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'RELU\', index=29, number=18,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SIGMOID\', index=30, number=19,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SIGMOID_CROSS_ENTROPY_LOSS\', index=31, number=27,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SILENCE\', index=32, number=36,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SOFTMAX\', index=33, number=20,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SOFTMAX_LOSS\', index=34, number=21,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SPLIT\', index=35, number=22,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SLICE\', index=36, number=33,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'TANH\', index=37, number=23,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'WINDOW_DATA\', index=38, number=24,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'THRESHOLD\', index=39, number=31,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=13224,\n  serialized_end=13824,\n)\n\n_V1LAYERPARAMETER_DIMCHECKMODE = _descriptor.EnumDescriptor(\n  name=\'DimCheckMode\',\n  full_name=\'caffe.V1LayerParameter.DimCheckMode\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'STRICT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'PERMISSIVE\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=2546,\n  serialized_end=2588,\n)\n\n_V0LAYERPARAMETER_POOLMETHOD = _descriptor.EnumDescriptor(\n  name=\'PoolMethod\',\n  full_name=\'caffe.V0LayerParameter.PoolMethod\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'MAX\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AVE\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STOCHASTIC\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=9414,\n  serialized_end=9460,\n)\n\n\n_BLOBSHAPE = _descriptor.Descriptor(\n  name=\'BlobShape\',\n  full_name=\'caffe.BlobShape\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dim\', full_name=\'caffe.BlobShape.dim\', index=0,\n      number=1, type=3, cpp_type=2, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), \'\\020\\001\')),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=22,\n  serialized_end=50,\n)\n\n\n_BLOBPROTO = _descriptor.Descriptor(\n  name=\'BlobProto\',\n  full_name=\'caffe.BlobProto\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'caffe.BlobProto.shape\', index=0,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data\', full_name=\'caffe.BlobProto.data\', index=1,\n      number=5, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), \'\\020\\001\')),\n    _descriptor.FieldDescriptor(\n      name=\'diff\', full_name=\'caffe.BlobProto.diff\', index=2,\n      number=6, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), \'\\020\\001\')),\n    _descriptor.FieldDescriptor(\n      name=\'num\', full_name=\'caffe.BlobProto.num\', index=3,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'channels\', full_name=\'caffe.BlobProto.channels\', index=4,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'caffe.BlobProto.height\', index=5,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'caffe.BlobProto.width\', index=6,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=53,\n  serialized_end=207,\n)\n\n\n_BLOBPROTOVECTOR = _descriptor.Descriptor(\n  name=\'BlobProtoVector\',\n  full_name=\'caffe.BlobProtoVector\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'blobs\', full_name=\'caffe.BlobProtoVector.blobs\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=209,\n  serialized_end=259,\n)\n\n\n_DATUM = _descriptor.Descriptor(\n  name=\'Datum\',\n  full_name=\'caffe.Datum\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'channels\', full_name=\'caffe.Datum.channels\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'caffe.Datum.height\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'caffe.Datum.width\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data\', full_name=\'caffe.Datum.data\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value="""",\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'label\', full_name=\'caffe.Datum.label\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'float_data\', full_name=\'caffe.Datum.float_data\', index=5,\n      number=6, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'encoded\', full_name=\'caffe.Datum.encoded\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=262,\n  serialized_end=391,\n)\n\n\n_FILLERPARAMETER = _descriptor.Descriptor(\n  name=\'FillerParameter\',\n  full_name=\'caffe.FillerParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'caffe.FillerParameter.type\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=unicode(""constant"", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'caffe.FillerParameter.value\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min\', full_name=\'caffe.FillerParameter.min\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max\', full_name=\'caffe.FillerParameter.max\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean\', full_name=\'caffe.FillerParameter.mean\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'std\', full_name=\'caffe.FillerParameter.std\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sparse\', full_name=\'caffe.FillerParameter.sparse\', index=6,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=-1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'variance_norm\', full_name=\'caffe.FillerParameter.variance_norm\', index=7,\n      number=8, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _FILLERPARAMETER_VARIANCENORM,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=394,\n  serialized_end=660,\n)\n\n\n_NETPARAMETER = _descriptor.Descriptor(\n  name=\'NetParameter\',\n  full_name=\'caffe.NetParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'caffe.NetParameter.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input\', full_name=\'caffe.NetParameter.input\', index=1,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input_shape\', full_name=\'caffe.NetParameter.input_shape\', index=2,\n      number=8, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input_dim\', full_name=\'caffe.NetParameter.input_dim\', index=3,\n      number=4, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'force_backward\', full_name=\'caffe.NetParameter.force_backward\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'state\', full_name=\'caffe.NetParameter.state\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug_info\', full_name=\'caffe.NetParameter.debug_info\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'layer\', full_name=\'caffe.NetParameter.layer\', index=7,\n      number=100, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mem_param\', full_name=\'caffe.NetParameter.mem_param\', index=8,\n      number=200, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'layers\', full_name=\'caffe.NetParameter.layers\', index=9,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=663,\n  serialized_end=989,\n)\n\n\n_SOLVERPARAMETER = _descriptor.Descriptor(\n  name=\'SolverParameter\',\n  full_name=\'caffe.SolverParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'net\', full_name=\'caffe.SolverParameter.net\', index=0,\n      number=24, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'net_param\', full_name=\'caffe.SolverParameter.net_param\', index=1,\n      number=25, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_net\', full_name=\'caffe.SolverParameter.train_net\', index=2,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_net\', full_name=\'caffe.SolverParameter.test_net\', index=3,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_net_param\', full_name=\'caffe.SolverParameter.train_net_param\', index=4,\n      number=21, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_net_param\', full_name=\'caffe.SolverParameter.test_net_param\', index=5,\n      number=22, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_state\', full_name=\'caffe.SolverParameter.train_state\', index=6,\n      number=26, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_state\', full_name=\'caffe.SolverParameter.test_state\', index=7,\n      number=27, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_iter\', full_name=\'caffe.SolverParameter.test_iter\', index=8,\n      number=3, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_interval\', full_name=\'caffe.SolverParameter.test_interval\', index=9,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_compute_loss\', full_name=\'caffe.SolverParameter.test_compute_loss\', index=10,\n      number=19, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_initialization\', full_name=\'caffe.SolverParameter.test_initialization\', index=11,\n      number=32, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'base_lr\', full_name=\'caffe.SolverParameter.base_lr\', index=12,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'display\', full_name=\'caffe.SolverParameter.display\', index=13,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'average_loss\', full_name=\'caffe.SolverParameter.average_loss\', index=14,\n      number=33, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_iter\', full_name=\'caffe.SolverParameter.max_iter\', index=15,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'iter_size\', full_name=\'caffe.SolverParameter.iter_size\', index=16,\n      number=36, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'lr_policy\', full_name=\'caffe.SolverParameter.lr_policy\', index=17,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'gamma\', full_name=\'caffe.SolverParameter.gamma\', index=18,\n      number=9, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'power\', full_name=\'caffe.SolverParameter.power\', index=19,\n      number=10, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'momentum\', full_name=\'caffe.SolverParameter.momentum\', index=20,\n      number=11, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_decay\', full_name=\'caffe.SolverParameter.weight_decay\', index=21,\n      number=12, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'regularization_type\', full_name=\'caffe.SolverParameter.regularization_type\', index=22,\n      number=29, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=unicode(""L2"", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stepsize\', full_name=\'caffe.SolverParameter.stepsize\', index=23,\n      number=13, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stepvalue\', full_name=\'caffe.SolverParameter.stepvalue\', index=24,\n      number=34, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'clip_gradients\', full_name=\'caffe.SolverParameter.clip_gradients\', index=25,\n      number=35, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=-1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'snapshot\', full_name=\'caffe.SolverParameter.snapshot\', index=26,\n      number=14, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'snapshot_prefix\', full_name=\'caffe.SolverParameter.snapshot_prefix\', index=27,\n      number=15, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'snapshot_diff\', full_name=\'caffe.SolverParameter.snapshot_diff\', index=28,\n      number=16, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'solver_mode\', full_name=\'caffe.SolverParameter.solver_mode\', index=29,\n      number=17, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'device_id\', full_name=\'caffe.SolverParameter.device_id\', index=30,\n      number=18, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'group_id\', full_name=\'caffe.SolverParameter.group_id\', index=31,\n      number=38, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_seed\', full_name=\'caffe.SolverParameter.random_seed\', index=32,\n      number=20, type=3, cpp_type=2, label=1,\n      has_default_value=True, default_value=-1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'solver_type\', full_name=\'caffe.SolverParameter.solver_type\', index=33,\n      number=30, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'delta\', full_name=\'caffe.SolverParameter.delta\', index=34,\n      number=31, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1e-08,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug_info\', full_name=\'caffe.SolverParameter.debug_info\', index=35,\n      number=23, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'snapshot_after_train\', full_name=\'caffe.SolverParameter.snapshot_after_train\', index=36,\n      number=28, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'richness\', full_name=\'caffe.SolverParameter.richness\', index=37,\n      number=37, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=300,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _SOLVERPARAMETER_SOLVERMODE,\n    _SOLVERPARAMETER_SOLVERTYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=992,\n  serialized_end=2115,\n)\n\n\n_SOLVERSTATE = _descriptor.Descriptor(\n  name=\'SolverState\',\n  full_name=\'caffe.SolverState\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'iter\', full_name=\'caffe.SolverState.iter\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'learned_net\', full_name=\'caffe.SolverState.learned_net\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'history\', full_name=\'caffe.SolverState.history\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'current_step\', full_name=\'caffe.SolverState.current_step\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=2117,\n  serialized_end=2225,\n)\n\n\n_NETSTATE = _descriptor.Descriptor(\n  name=\'NetState\',\n  full_name=\'caffe.NetState\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'phase\', full_name=\'caffe.NetState.phase\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'level\', full_name=\'caffe.NetState.level\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stage\', full_name=\'caffe.NetState.stage\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=2227,\n  serialized_end=2305,\n)\n\n\n_NETSTATERULE = _descriptor.Descriptor(\n  name=\'NetStateRule\',\n  full_name=\'caffe.NetStateRule\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'phase\', full_name=\'caffe.NetStateRule.phase\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_level\', full_name=\'caffe.NetStateRule.min_level\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_level\', full_name=\'caffe.NetStateRule.max_level\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stage\', full_name=\'caffe.NetStateRule.stage\', index=3,\n      number=4, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'not_stage\', full_name=\'caffe.NetStateRule.not_stage\', index=4,\n      number=5, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=2307,\n  serialized_end=2422,\n)\n\n\n_PARAMSPEC = _descriptor.Descriptor(\n  name=\'ParamSpec\',\n  full_name=\'caffe.ParamSpec\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'caffe.ParamSpec.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'share_mode\', full_name=\'caffe.ParamSpec.share_mode\', index=1,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'lr_mult\', full_name=\'caffe.ParamSpec.lr_mult\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'decay_mult\', full_name=\'caffe.ParamSpec.decay_mult\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _PARAMSPEC_DIMCHECKMODE,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=2425,\n  serialized_end=2588,\n)\n\n\n_LAYERPARAMETER = _descriptor.Descriptor(\n  name=\'LayerParameter\',\n  full_name=\'caffe.LayerParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'caffe.LayerParameter.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'caffe.LayerParameter.type\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bottom\', full_name=\'caffe.LayerParameter.bottom\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'top\', full_name=\'caffe.LayerParameter.top\', index=3,\n      number=4, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'phase\', full_name=\'caffe.LayerParameter.phase\', index=4,\n      number=10, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loss_weight\', full_name=\'caffe.LayerParameter.loss_weight\', index=5,\n      number=5, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'param\', full_name=\'caffe.LayerParameter.param\', index=6,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'blobs\', full_name=\'caffe.LayerParameter.blobs\', index=7,\n      number=7, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'propagate_down\', full_name=\'caffe.LayerParameter.propagate_down\', index=8,\n      number=11, type=8, cpp_type=7, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'include\', full_name=\'caffe.LayerParameter.include\', index=9,\n      number=8, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'exclude\', full_name=\'caffe.LayerParameter.exclude\', index=10,\n      number=9, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'transform_param\', full_name=\'caffe.LayerParameter.transform_param\', index=11,\n      number=100, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loss_param\', full_name=\'caffe.LayerParameter.loss_param\', index=12,\n      number=101, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'accuracy_param\', full_name=\'caffe.LayerParameter.accuracy_param\', index=13,\n      number=102, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'argmax_param\', full_name=\'caffe.LayerParameter.argmax_param\', index=14,\n      number=103, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bn_param\', full_name=\'caffe.LayerParameter.bn_param\', index=15,\n      number=137, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'concat_param\', full_name=\'caffe.LayerParameter.concat_param\', index=16,\n      number=104, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'contrastive_loss_param\', full_name=\'caffe.LayerParameter.contrastive_loss_param\', index=17,\n      number=105, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'convolution_param\', full_name=\'caffe.LayerParameter.convolution_param\', index=18,\n      number=106, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data_param\', full_name=\'caffe.LayerParameter.data_param\', index=19,\n      number=107, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dropout_param\', full_name=\'caffe.LayerParameter.dropout_param\', index=20,\n      number=108, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dummy_data_param\', full_name=\'caffe.LayerParameter.dummy_data_param\', index=21,\n      number=109, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eltwise_param\', full_name=\'caffe.LayerParameter.eltwise_param\', index=22,\n      number=110, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'exp_param\', full_name=\'caffe.LayerParameter.exp_param\', index=23,\n      number=111, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'flatten_param\', full_name=\'caffe.LayerParameter.flatten_param\', index=24,\n      number=135, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hdf5_data_param\', full_name=\'caffe.LayerParameter.hdf5_data_param\', index=25,\n      number=112, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hdf5_output_param\', full_name=\'caffe.LayerParameter.hdf5_output_param\', index=26,\n      number=113, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hinge_loss_param\', full_name=\'caffe.LayerParameter.hinge_loss_param\', index=27,\n      number=114, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'image_data_param\', full_name=\'caffe.LayerParameter.image_data_param\', index=28,\n      number=115, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'infogain_loss_param\', full_name=\'caffe.LayerParameter.infogain_loss_param\', index=29,\n      number=116, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'inner_product_param\', full_name=\'caffe.LayerParameter.inner_product_param\', index=30,\n      number=117, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'log_param\', full_name=\'caffe.LayerParameter.log_param\', index=31,\n      number=134, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'lrn_param\', full_name=\'caffe.LayerParameter.lrn_param\', index=32,\n      number=118, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'memory_data_param\', full_name=\'caffe.LayerParameter.memory_data_param\', index=33,\n      number=119, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mvn_param\', full_name=\'caffe.LayerParameter.mvn_param\', index=34,\n      number=120, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pooling_param\', full_name=\'caffe.LayerParameter.pooling_param\', index=35,\n      number=121, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'power_param\', full_name=\'caffe.LayerParameter.power_param\', index=36,\n      number=122, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'prelu_param\', full_name=\'caffe.LayerParameter.prelu_param\', index=37,\n      number=131, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'python_param\', full_name=\'caffe.LayerParameter.python_param\', index=38,\n      number=130, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'reduction_param\', full_name=\'caffe.LayerParameter.reduction_param\', index=39,\n      number=136, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'relu_param\', full_name=\'caffe.LayerParameter.relu_param\', index=40,\n      number=123, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'reshape_param\', full_name=\'caffe.LayerParameter.reshape_param\', index=41,\n      number=133, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'seg_data_param\', full_name=\'caffe.LayerParameter.seg_data_param\', index=42,\n      number=141, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sigmoid_param\', full_name=\'caffe.LayerParameter.sigmoid_param\', index=43,\n      number=124, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'softmax_param\', full_name=\'caffe.LayerParameter.softmax_param\', index=44,\n      number=125, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'spp_param\', full_name=\'caffe.LayerParameter.spp_param\', index=45,\n      number=132, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'slice_param\', full_name=\'caffe.LayerParameter.slice_param\', index=46,\n      number=126, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tanh_param\', full_name=\'caffe.LayerParameter.tanh_param\', index=47,\n      number=127, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'threshold_param\', full_name=\'caffe.LayerParameter.threshold_param\', index=48,\n      number=128, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'window_data_param\', full_name=\'caffe.LayerParameter.window_data_param\', index=49,\n      number=129, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'video_data_param\', full_name=\'caffe.LayerParameter.video_data_param\', index=50,\n      number=140, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'roi_pooling_param\', full_name=\'caffe.LayerParameter.roi_pooling_param\', index=51,\n      number=150, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale_param\', full_name=\'caffe.LayerParameter.scale_param\', index=52,\n      number=160, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_param\', full_name=\'caffe.LayerParameter.bias_param\', index=53,\n      number=161, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_reduction_param\', full_name=\'caffe.LayerParameter.batch_reduction_param\', index=54,\n      number=162, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=2591,\n  serialized_end=5039,\n)\n\n\n_TRANSFORMATIONPARAMETER = _descriptor.Descriptor(\n  name=\'TransformationParameter\',\n  full_name=\'caffe.TransformationParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.TransformationParameter.scale\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mirror\', full_name=\'caffe.TransformationParameter.mirror\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_size\', full_name=\'caffe.TransformationParameter.crop_size\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_file\', full_name=\'caffe.TransformationParameter.mean_file\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_value\', full_name=\'caffe.TransformationParameter.mean_value\', index=4,\n      number=5, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'force_color\', full_name=\'caffe.TransformationParameter.force_color\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'force_gray\', full_name=\'caffe.TransformationParameter.force_gray\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fix_crop\', full_name=\'caffe.TransformationParameter.fix_crop\', index=7,\n      number=10, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'more_fix_crop\', full_name=\'caffe.TransformationParameter.more_fix_crop\', index=8,\n      number=15, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'multi_scale\', full_name=\'caffe.TransformationParameter.multi_scale\', index=9,\n      number=11, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale_ratios\', full_name=\'caffe.TransformationParameter.scale_ratios\', index=10,\n      number=12, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_distort\', full_name=\'caffe.TransformationParameter.max_distort\', index=11,\n      number=13, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'is_flow\', full_name=\'caffe.TransformationParameter.is_flow\', index=12,\n      number=14, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'original_image\', full_name=\'caffe.TransformationParameter.original_image\', index=13,\n      number=20, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride\', full_name=\'caffe.TransformationParameter.stride\', index=14,\n      number=16, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'upper_size\', full_name=\'caffe.TransformationParameter.upper_size\', index=15,\n      number=17, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'upper_height\', full_name=\'caffe.TransformationParameter.upper_height\', index=16,\n      number=18, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'upper_width\', full_name=\'caffe.TransformationParameter.upper_width\', index=17,\n      number=19, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=5042,\n  serialized_end=5490,\n)\n\n\n_LOSSPARAMETER = _descriptor.Descriptor(\n  name=\'LossParameter\',\n  full_name=\'caffe.LossParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'ignore_label\', full_name=\'caffe.LossParameter.ignore_label\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'normalize\', full_name=\'caffe.LossParameter.normalize\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=5492,\n  serialized_end=5554,\n)\n\n\n_ACCURACYPARAMETER = _descriptor.Descriptor(\n  name=\'AccuracyParameter\',\n  full_name=\'caffe.AccuracyParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'top_k\', full_name=\'caffe.AccuracyParameter.top_k\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.AccuracyParameter.axis\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ignore_label\', full_name=\'caffe.AccuracyParameter.ignore_label\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=5556,\n  serialized_end=5632,\n)\n\n\n_ARGMAXPARAMETER = _descriptor.Descriptor(\n  name=\'ArgMaxParameter\',\n  full_name=\'caffe.ArgMaxParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'out_max_val\', full_name=\'caffe.ArgMaxParameter.out_max_val\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'top_k\', full_name=\'caffe.ArgMaxParameter.top_k\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=5634,\n  serialized_end=5697,\n)\n\n\n_BNPARAMETER = _descriptor.Descriptor(\n  name=\'BNParameter\',\n  full_name=\'caffe.BNParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'slope_filler\', full_name=\'caffe.BNParameter.slope_filler\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_filler\', full_name=\'caffe.BNParameter.bias_filler\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'momentum\', full_name=\'caffe.BNParameter.momentum\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0.9,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eps\', full_name=\'caffe.BNParameter.eps\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1e-05,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'frozen\', full_name=\'caffe.BNParameter.frozen\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.BNParameter.engine\', index=5,\n      number=6, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _BNPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=5700,\n  serialized_end=5967,\n)\n\n\n_CONCATPARAMETER = _descriptor.Descriptor(\n  name=\'ConcatParameter\',\n  full_name=\'caffe.ConcatParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.ConcatParameter.axis\', index=0,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'concat_dim\', full_name=\'caffe.ConcatParameter.concat_dim\', index=1,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=5969,\n  serialized_end=6026,\n)\n\n\n_CONTRASTIVELOSSPARAMETER = _descriptor.Descriptor(\n  name=\'ContrastiveLossParameter\',\n  full_name=\'caffe.ContrastiveLossParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'margin\', full_name=\'caffe.ContrastiveLossParameter.margin\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'legacy_version\', full_name=\'caffe.ContrastiveLossParameter.legacy_version\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=6028,\n  serialized_end=6104,\n)\n\n\n_CONVOLUTIONPARAMETER = _descriptor.Descriptor(\n  name=\'ConvolutionParameter\',\n  full_name=\'caffe.ConvolutionParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_output\', full_name=\'caffe.ConvolutionParameter.num_output\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_term\', full_name=\'caffe.ConvolutionParameter.bias_term\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad\', full_name=\'caffe.ConvolutionParameter.pad\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_h\', full_name=\'caffe.ConvolutionParameter.pad_h\', index=3,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_w\', full_name=\'caffe.ConvolutionParameter.pad_w\', index=4,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_size\', full_name=\'caffe.ConvolutionParameter.kernel_size\', index=5,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_h\', full_name=\'caffe.ConvolutionParameter.kernel_h\', index=6,\n      number=11, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_w\', full_name=\'caffe.ConvolutionParameter.kernel_w\', index=7,\n      number=12, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'group\', full_name=\'caffe.ConvolutionParameter.group\', index=8,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride\', full_name=\'caffe.ConvolutionParameter.stride\', index=9,\n      number=6, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride_h\', full_name=\'caffe.ConvolutionParameter.stride_h\', index=10,\n      number=13, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride_w\', full_name=\'caffe.ConvolutionParameter.stride_w\', index=11,\n      number=14, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dilation\', full_name=\'caffe.ConvolutionParameter.dilation\', index=12,\n      number=16, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dilation_h\', full_name=\'caffe.ConvolutionParameter.dilation_h\', index=13,\n      number=17, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dilation_w\', full_name=\'caffe.ConvolutionParameter.dilation_w\', index=14,\n      number=18, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_filler\', full_name=\'caffe.ConvolutionParameter.weight_filler\', index=15,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_filler\', full_name=\'caffe.ConvolutionParameter.bias_filler\', index=16,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.ConvolutionParameter.engine\', index=17,\n      number=15, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _CONVOLUTIONPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=6107,\n  serialized_end=6615,\n)\n\n\n_DATAPARAMETER = _descriptor.Descriptor(\n  name=\'DataParameter\',\n  full_name=\'caffe.DataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.DataParameter.source\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'caffe.DataParameter.batch_size\', index=1,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rand_skip\', full_name=\'caffe.DataParameter.rand_skip\', index=2,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'backend\', full_name=\'caffe.DataParameter.backend\', index=3,\n      number=8, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.DataParameter.scale\', index=4,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_file\', full_name=\'caffe.DataParameter.mean_file\', index=5,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_size\', full_name=\'caffe.DataParameter.crop_size\', index=6,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mirror\', full_name=\'caffe.DataParameter.mirror\', index=7,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'force_encoded_color\', full_name=\'caffe.DataParameter.force_encoded_color\', index=8,\n      number=9, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle\', full_name=\'caffe.DataParameter.shuffle\', index=9,\n      number=10, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _DATAPARAMETER_DB,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=6618,\n  serialized_end=6913,\n)\n\n\n_DROPOUTPARAMETER = _descriptor.Descriptor(\n  name=\'DropoutParameter\',\n  full_name=\'caffe.DropoutParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'dropout_ratio\', full_name=\'caffe.DropoutParameter.dropout_ratio\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0.5,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=6915,\n  serialized_end=6961,\n)\n\n\n_DUMMYDATAPARAMETER = _descriptor.Descriptor(\n  name=\'DummyDataParameter\',\n  full_name=\'caffe.DummyDataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'data_filler\', full_name=\'caffe.DummyDataParameter.data_filler\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'caffe.DummyDataParameter.shape\', index=1,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num\', full_name=\'caffe.DummyDataParameter.num\', index=2,\n      number=2, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'channels\', full_name=\'caffe.DummyDataParameter.channels\', index=3,\n      number=3, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'caffe.DummyDataParameter.height\', index=4,\n      number=4, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'caffe.DummyDataParameter.width\', index=5,\n      number=5, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=6964,\n  serialized_end=7124,\n)\n\n\n_ELTWISEPARAMETER = _descriptor.Descriptor(\n  name=\'EltwiseParameter\',\n  full_name=\'caffe.EltwiseParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'operation\', full_name=\'caffe.EltwiseParameter.operation\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'coeff\', full_name=\'caffe.EltwiseParameter.coeff\', index=1,\n      number=2, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stable_prod_grad\', full_name=\'caffe.EltwiseParameter.stable_prod_grad\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _ELTWISEPARAMETER_ELTWISEOP,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=7127,\n  serialized_end=7312,\n)\n\n\n_EXPPARAMETER = _descriptor.Descriptor(\n  name=\'ExpParameter\',\n  full_name=\'caffe.ExpParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'base\', full_name=\'caffe.ExpParameter.base\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=-1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.ExpParameter.scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shift\', full_name=\'caffe.ExpParameter.shift\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=7314,\n  serialized_end=7382,\n)\n\n\n_FLATTENPARAMETER = _descriptor.Descriptor(\n  name=\'FlattenParameter\',\n  full_name=\'caffe.FlattenParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.FlattenParameter.axis\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'end_axis\', full_name=\'caffe.FlattenParameter.end_axis\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=-1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=7384,\n  serialized_end=7441,\n)\n\n\n_HDF5DATAPARAMETER = _descriptor.Descriptor(\n  name=\'HDF5DataParameter\',\n  full_name=\'caffe.HDF5DataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.HDF5DataParameter.source\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'caffe.HDF5DataParameter.batch_size\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle\', full_name=\'caffe.HDF5DataParameter.shuffle\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=7443,\n  serialized_end=7522,\n)\n\n\n_HDF5OUTPUTPARAMETER = _descriptor.Descriptor(\n  name=\'HDF5OutputParameter\',\n  full_name=\'caffe.HDF5OutputParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'file_name\', full_name=\'caffe.HDF5OutputParameter.file_name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=7524,\n  serialized_end=7564,\n)\n\n\n_HINGELOSSPARAMETER = _descriptor.Descriptor(\n  name=\'HingeLossParameter\',\n  full_name=\'caffe.HingeLossParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'norm\', full_name=\'caffe.HingeLossParameter.norm\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _HINGELOSSPARAMETER_NORM,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=7566,\n  serialized_end=7660,\n)\n\n\n_IMAGEDATAPARAMETER = _descriptor.Descriptor(\n  name=\'ImageDataParameter\',\n  full_name=\'caffe.ImageDataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.ImageDataParameter.source\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'caffe.ImageDataParameter.batch_size\', index=1,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rand_skip\', full_name=\'caffe.ImageDataParameter.rand_skip\', index=2,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle\', full_name=\'caffe.ImageDataParameter.shuffle\', index=3,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_height\', full_name=\'caffe.ImageDataParameter.new_height\', index=4,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_width\', full_name=\'caffe.ImageDataParameter.new_width\', index=5,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'is_color\', full_name=\'caffe.ImageDataParameter.is_color\', index=6,\n      number=11, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.ImageDataParameter.scale\', index=7,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_file\', full_name=\'caffe.ImageDataParameter.mean_file\', index=8,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_size\', full_name=\'caffe.ImageDataParameter.crop_size\', index=9,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mirror\', full_name=\'caffe.ImageDataParameter.mirror\', index=10,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'root_folder\', full_name=\'caffe.ImageDataParameter.root_folder\', index=11,\n      number=12, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=7663,\n  serialized_end=7939,\n)\n\n\n_VIDEODATAPARAMETER = _descriptor.Descriptor(\n  name=\'VideoDataParameter\',\n  full_name=\'caffe.VideoDataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.VideoDataParameter.source\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'caffe.VideoDataParameter.batch_size\', index=1,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rand_skip\', full_name=\'caffe.VideoDataParameter.rand_skip\', index=2,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle\', full_name=\'caffe.VideoDataParameter.shuffle\', index=3,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_height\', full_name=\'caffe.VideoDataParameter.new_height\', index=4,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_width\', full_name=\'caffe.VideoDataParameter.new_width\', index=5,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_length\', full_name=\'caffe.VideoDataParameter.new_length\', index=6,\n      number=11, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_segments\', full_name=\'caffe.VideoDataParameter.num_segments\', index=7,\n      number=12, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.VideoDataParameter.scale\', index=8,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_file\', full_name=\'caffe.VideoDataParameter.mean_file\', index=9,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_size\', full_name=\'caffe.VideoDataParameter.crop_size\', index=10,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mirror\', full_name=\'caffe.VideoDataParameter.mirror\', index=11,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'modality\', full_name=\'caffe.VideoDataParameter.modality\', index=12,\n      number=13, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'name_pattern\', full_name=\'caffe.VideoDataParameter.name_pattern\', index=13,\n      number=14, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'encoded\', full_name=\'caffe.VideoDataParameter.encoded\', index=14,\n      number=15, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'grayscale\', full_name=\'caffe.VideoDataParameter.grayscale\', index=15,\n      number=16, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _VIDEODATAPARAMETER_MODALITY,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=7942,\n  serialized_end=8382,\n)\n\n\n_INFOGAINLOSSPARAMETER = _descriptor.Descriptor(\n  name=\'InfogainLossParameter\',\n  full_name=\'caffe.InfogainLossParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.InfogainLossParameter.source\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=8384,\n  serialized_end=8423,\n)\n\n\n_INNERPRODUCTPARAMETER = _descriptor.Descriptor(\n  name=\'InnerProductParameter\',\n  full_name=\'caffe.InnerProductParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_output\', full_name=\'caffe.InnerProductParameter.num_output\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_term\', full_name=\'caffe.InnerProductParameter.bias_term\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_filler\', full_name=\'caffe.InnerProductParameter.weight_filler\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_filler\', full_name=\'caffe.InnerProductParameter.bias_filler\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.InnerProductParameter.axis\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=8426,\n  serialized_end=8603,\n)\n\n\n_LOGPARAMETER = _descriptor.Descriptor(\n  name=\'LogParameter\',\n  full_name=\'caffe.LogParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'base\', full_name=\'caffe.LogParameter.base\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=-1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.LogParameter.scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shift\', full_name=\'caffe.LogParameter.shift\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=8605,\n  serialized_end=8673,\n)\n\n\n_LRNPARAMETER = _descriptor.Descriptor(\n  name=\'LRNParameter\',\n  full_name=\'caffe.LRNParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'local_size\', full_name=\'caffe.LRNParameter.local_size\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=5,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'alpha\', full_name=\'caffe.LRNParameter.alpha\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'beta\', full_name=\'caffe.LRNParameter.beta\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0.75,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'norm_region\', full_name=\'caffe.LRNParameter.norm_region\', index=3,\n      number=4, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'k\', full_name=\'caffe.LRNParameter.k\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _LRNPARAMETER_NORMREGION,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=8676,\n  serialized_end=8890,\n)\n\n\n_MEMORYDATAPARAMETER = _descriptor.Descriptor(\n  name=\'MemoryDataParameter\',\n  full_name=\'caffe.MemoryDataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'caffe.MemoryDataParameter.batch_size\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'channels\', full_name=\'caffe.MemoryDataParameter.channels\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'caffe.MemoryDataParameter.height\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'caffe.MemoryDataParameter.width\', index=3,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=8892,\n  serialized_end=8982,\n)\n\n\n_MVNPARAMETER = _descriptor.Descriptor(\n  name=\'MVNParameter\',\n  full_name=\'caffe.MVNParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'normalize_variance\', full_name=\'caffe.MVNParameter.normalize_variance\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'across_channels\', full_name=\'caffe.MVNParameter.across_channels\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eps\', full_name=\'caffe.MVNParameter.eps\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1e-09,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=8984,\n  serialized_end=9084,\n)\n\n\n_POOLINGPARAMETER = _descriptor.Descriptor(\n  name=\'PoolingParameter\',\n  full_name=\'caffe.PoolingParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'pool\', full_name=\'caffe.PoolingParameter.pool\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad\', full_name=\'caffe.PoolingParameter.pad\', index=1,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_h\', full_name=\'caffe.PoolingParameter.pad_h\', index=2,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_w\', full_name=\'caffe.PoolingParameter.pad_w\', index=3,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_size\', full_name=\'caffe.PoolingParameter.kernel_size\', index=4,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_h\', full_name=\'caffe.PoolingParameter.kernel_h\', index=5,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_w\', full_name=\'caffe.PoolingParameter.kernel_w\', index=6,\n      number=6, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride\', full_name=\'caffe.PoolingParameter.stride\', index=7,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride_h\', full_name=\'caffe.PoolingParameter.stride_h\', index=8,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride_w\', full_name=\'caffe.PoolingParameter.stride_w\', index=9,\n      number=8, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.PoolingParameter.engine\', index=10,\n      number=11, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'global_pooling\', full_name=\'caffe.PoolingParameter.global_pooling\', index=11,\n      number=12, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _POOLINGPARAMETER_POOLMETHOD,\n    _POOLINGPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=9087,\n  serialized_end=9505,\n)\n\n\n_POWERPARAMETER = _descriptor.Descriptor(\n  name=\'PowerParameter\',\n  full_name=\'caffe.PowerParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'power\', full_name=\'caffe.PowerParameter.power\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.PowerParameter.scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shift\', full_name=\'caffe.PowerParameter.shift\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=9507,\n  serialized_end=9577,\n)\n\n\n_PYTHONPARAMETER = _descriptor.Descriptor(\n  name=\'PythonParameter\',\n  full_name=\'caffe.PythonParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'module\', full_name=\'caffe.PythonParameter.module\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'layer\', full_name=\'caffe.PythonParameter.layer\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'param_str\', full_name=\'caffe.PythonParameter.param_str\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=9579,\n  serialized_end=9646,\n)\n\n\n_REDUCTIONPARAMETER = _descriptor.Descriptor(\n  name=\'ReductionParameter\',\n  full_name=\'caffe.ReductionParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'operation\', full_name=\'caffe.ReductionParameter.operation\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.ReductionParameter.axis\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'coeff\', full_name=\'caffe.ReductionParameter.coeff\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'k\', full_name=\'caffe.ReductionParameter.k\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _REDUCTIONPARAMETER_REDUCTIONOP,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=9649,\n  serialized_end=9846,\n)\n\n\n_RELUPARAMETER = _descriptor.Descriptor(\n  name=\'ReLUParameter\',\n  full_name=\'caffe.ReLUParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'negative_slope\', full_name=\'caffe.ReLUParameter.negative_slope\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.ReLUParameter.engine\', index=1,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _RELUPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=9849,\n  serialized_end=9990,\n)\n\n\n_RESHAPEPARAMETER = _descriptor.Descriptor(\n  name=\'ReshapeParameter\',\n  full_name=\'caffe.ReshapeParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'caffe.ReshapeParameter.shape\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.ReshapeParameter.axis\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_axes\', full_name=\'caffe.ReshapeParameter.num_axes\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=-1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=9992,\n  serialized_end=10082,\n)\n\n\n_SEGDATAPARAMETER = _descriptor.Descriptor(\n  name=\'SegDataParameter\',\n  full_name=\'caffe.SegDataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.SegDataParameter.source\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'root_dir\', full_name=\'caffe.SegDataParameter.root_dir\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle\', full_name=\'caffe.SegDataParameter.shuffle\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'balance\', full_name=\'caffe.SegDataParameter.balance\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=10084,\n  serialized_end=10184,\n)\n\n\n_SIGMOIDPARAMETER = _descriptor.Descriptor(\n  name=\'SigmoidParameter\',\n  full_name=\'caffe.SigmoidParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.SigmoidParameter.engine\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _SIGMOIDPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=10186,\n  serialized_end=10306,\n)\n\n\n_SLICEPARAMETER = _descriptor.Descriptor(\n  name=\'SliceParameter\',\n  full_name=\'caffe.SliceParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.SliceParameter.axis\', index=0,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'slice_point\', full_name=\'caffe.SliceParameter.slice_point\', index=1,\n      number=2, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'slice_dim\', full_name=\'caffe.SliceParameter.slice_dim\', index=2,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=10308,\n  serialized_end=10384,\n)\n\n\n_SOFTMAXPARAMETER = _descriptor.Descriptor(\n  name=\'SoftmaxParameter\',\n  full_name=\'caffe.SoftmaxParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.SoftmaxParameter.engine\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.SoftmaxParameter.axis\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _SOFTMAXPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=10387,\n  serialized_end=10524,\n)\n\n\n_TANHPARAMETER = _descriptor.Descriptor(\n  name=\'TanHParameter\',\n  full_name=\'caffe.TanHParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.TanHParameter.engine\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _TANHPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=10526,\n  serialized_end=10640,\n)\n\n\n_THRESHOLDPARAMETER = _descriptor.Descriptor(\n  name=\'ThresholdParameter\',\n  full_name=\'caffe.ThresholdParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'threshold\', full_name=\'caffe.ThresholdParameter.threshold\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=10642,\n  serialized_end=10684,\n)\n\n\n_WINDOWDATAPARAMETER = _descriptor.Descriptor(\n  name=\'WindowDataParameter\',\n  full_name=\'caffe.WindowDataParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.WindowDataParameter.source\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.WindowDataParameter.scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_file\', full_name=\'caffe.WindowDataParameter.mean_file\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'caffe.WindowDataParameter.batch_size\', index=3,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_size\', full_name=\'caffe.WindowDataParameter.crop_size\', index=4,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mirror\', full_name=\'caffe.WindowDataParameter.mirror\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fg_threshold\', full_name=\'caffe.WindowDataParameter.fg_threshold\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0.5,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bg_threshold\', full_name=\'caffe.WindowDataParameter.bg_threshold\', index=7,\n      number=8, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0.5,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fg_fraction\', full_name=\'caffe.WindowDataParameter.fg_fraction\', index=8,\n      number=9, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0.25,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'context_pad\', full_name=\'caffe.WindowDataParameter.context_pad\', index=9,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_mode\', full_name=\'caffe.WindowDataParameter.crop_mode\', index=10,\n      number=11, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=unicode(""warp"", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'cache_images\', full_name=\'caffe.WindowDataParameter.cache_images\', index=11,\n      number=12, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'root_folder\', full_name=\'caffe.WindowDataParameter.root_folder\', index=12,\n      number=13, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=10687,\n  serialized_end=11008,\n)\n\n\n_SPPPARAMETER = _descriptor.Descriptor(\n  name=\'SPPParameter\',\n  full_name=\'caffe.SPPParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'pyramid_height\', full_name=\'caffe.SPPParameter.pyramid_height\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pool\', full_name=\'caffe.SPPParameter.pool\', index=1,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'engine\', full_name=\'caffe.SPPParameter.engine\', index=2,\n      number=6, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _SPPPARAMETER_POOLMETHOD,\n    _SPPPARAMETER_ENGINE,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=11011,\n  serialized_end=11246,\n)\n\n\n_ROIPOOLINGPARAMETER = _descriptor.Descriptor(\n  name=\'ROIPoolingParameter\',\n  full_name=\'caffe.ROIPoolingParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'pooled_h\', full_name=\'caffe.ROIPoolingParameter.pooled_h\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pooled_w\', full_name=\'caffe.ROIPoolingParameter.pooled_w\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'spatial_scale\', full_name=\'caffe.ROIPoolingParameter.spatial_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=11248,\n  serialized_end=11337,\n)\n\n\n_V1LAYERPARAMETER = _descriptor.Descriptor(\n  name=\'V1LayerParameter\',\n  full_name=\'caffe.V1LayerParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'bottom\', full_name=\'caffe.V1LayerParameter.bottom\', index=0,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'top\', full_name=\'caffe.V1LayerParameter.top\', index=1,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'caffe.V1LayerParameter.name\', index=2,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'include\', full_name=\'caffe.V1LayerParameter.include\', index=3,\n      number=32, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'exclude\', full_name=\'caffe.V1LayerParameter.exclude\', index=4,\n      number=33, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'caffe.V1LayerParameter.type\', index=5,\n      number=5, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'blobs\', full_name=\'caffe.V1LayerParameter.blobs\', index=6,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'param\', full_name=\'caffe.V1LayerParameter.param\', index=7,\n      number=1001, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'blob_share_mode\', full_name=\'caffe.V1LayerParameter.blob_share_mode\', index=8,\n      number=1002, type=14, cpp_type=8, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'blobs_lr\', full_name=\'caffe.V1LayerParameter.blobs_lr\', index=9,\n      number=7, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_decay\', full_name=\'caffe.V1LayerParameter.weight_decay\', index=10,\n      number=8, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loss_weight\', full_name=\'caffe.V1LayerParameter.loss_weight\', index=11,\n      number=35, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'accuracy_param\', full_name=\'caffe.V1LayerParameter.accuracy_param\', index=12,\n      number=27, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'argmax_param\', full_name=\'caffe.V1LayerParameter.argmax_param\', index=13,\n      number=23, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'concat_param\', full_name=\'caffe.V1LayerParameter.concat_param\', index=14,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'contrastive_loss_param\', full_name=\'caffe.V1LayerParameter.contrastive_loss_param\', index=15,\n      number=40, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'convolution_param\', full_name=\'caffe.V1LayerParameter.convolution_param\', index=16,\n      number=10, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data_param\', full_name=\'caffe.V1LayerParameter.data_param\', index=17,\n      number=11, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dropout_param\', full_name=\'caffe.V1LayerParameter.dropout_param\', index=18,\n      number=12, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dummy_data_param\', full_name=\'caffe.V1LayerParameter.dummy_data_param\', index=19,\n      number=26, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eltwise_param\', full_name=\'caffe.V1LayerParameter.eltwise_param\', index=20,\n      number=24, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'exp_param\', full_name=\'caffe.V1LayerParameter.exp_param\', index=21,\n      number=41, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hdf5_data_param\', full_name=\'caffe.V1LayerParameter.hdf5_data_param\', index=22,\n      number=13, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hdf5_output_param\', full_name=\'caffe.V1LayerParameter.hdf5_output_param\', index=23,\n      number=14, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hinge_loss_param\', full_name=\'caffe.V1LayerParameter.hinge_loss_param\', index=24,\n      number=29, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'image_data_param\', full_name=\'caffe.V1LayerParameter.image_data_param\', index=25,\n      number=15, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'infogain_loss_param\', full_name=\'caffe.V1LayerParameter.infogain_loss_param\', index=26,\n      number=16, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'inner_product_param\', full_name=\'caffe.V1LayerParameter.inner_product_param\', index=27,\n      number=17, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'lrn_param\', full_name=\'caffe.V1LayerParameter.lrn_param\', index=28,\n      number=18, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'memory_data_param\', full_name=\'caffe.V1LayerParameter.memory_data_param\', index=29,\n      number=22, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mvn_param\', full_name=\'caffe.V1LayerParameter.mvn_param\', index=30,\n      number=34, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pooling_param\', full_name=\'caffe.V1LayerParameter.pooling_param\', index=31,\n      number=19, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'power_param\', full_name=\'caffe.V1LayerParameter.power_param\', index=32,\n      number=21, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'relu_param\', full_name=\'caffe.V1LayerParameter.relu_param\', index=33,\n      number=30, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sigmoid_param\', full_name=\'caffe.V1LayerParameter.sigmoid_param\', index=34,\n      number=38, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'softmax_param\', full_name=\'caffe.V1LayerParameter.softmax_param\', index=35,\n      number=39, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'slice_param\', full_name=\'caffe.V1LayerParameter.slice_param\', index=36,\n      number=31, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tanh_param\', full_name=\'caffe.V1LayerParameter.tanh_param\', index=37,\n      number=37, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'threshold_param\', full_name=\'caffe.V1LayerParameter.threshold_param\', index=38,\n      number=25, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'window_data_param\', full_name=\'caffe.V1LayerParameter.window_data_param\', index=39,\n      number=20, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'transform_param\', full_name=\'caffe.V1LayerParameter.transform_param\', index=40,\n      number=36, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loss_param\', full_name=\'caffe.V1LayerParameter.loss_param\', index=41,\n      number=42, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'layer\', full_name=\'caffe.V1LayerParameter.layer\', index=42,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _V1LAYERPARAMETER_LAYERTYPE,\n    _V1LAYERPARAMETER_DIMCHECKMODE,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=11340,\n  serialized_end=13868,\n)\n\n\n_V0LAYERPARAMETER = _descriptor.Descriptor(\n  name=\'V0LayerParameter\',\n  full_name=\'caffe.V0LayerParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'caffe.V0LayerParameter.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'caffe.V0LayerParameter.type\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_output\', full_name=\'caffe.V0LayerParameter.num_output\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'biasterm\', full_name=\'caffe.V0LayerParameter.biasterm\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_filler\', full_name=\'caffe.V0LayerParameter.weight_filler\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_filler\', full_name=\'caffe.V0LayerParameter.bias_filler\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad\', full_name=\'caffe.V0LayerParameter.pad\', index=6,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernelsize\', full_name=\'caffe.V0LayerParameter.kernelsize\', index=7,\n      number=8, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'group\', full_name=\'caffe.V0LayerParameter.group\', index=8,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride\', full_name=\'caffe.V0LayerParameter.stride\', index=9,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pool\', full_name=\'caffe.V0LayerParameter.pool\', index=10,\n      number=11, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dropout_ratio\', full_name=\'caffe.V0LayerParameter.dropout_ratio\', index=11,\n      number=12, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0.5,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'local_size\', full_name=\'caffe.V0LayerParameter.local_size\', index=12,\n      number=13, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=5,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'alpha\', full_name=\'caffe.V0LayerParameter.alpha\', index=13,\n      number=14, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'beta\', full_name=\'caffe.V0LayerParameter.beta\', index=14,\n      number=15, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0.75,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'k\', full_name=\'caffe.V0LayerParameter.k\', index=15,\n      number=22, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'caffe.V0LayerParameter.source\', index=16,\n      number=16, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'caffe.V0LayerParameter.scale\', index=17,\n      number=17, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'meanfile\', full_name=\'caffe.V0LayerParameter.meanfile\', index=18,\n      number=18, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=unicode("""", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batchsize\', full_name=\'caffe.V0LayerParameter.batchsize\', index=19,\n      number=19, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'cropsize\', full_name=\'caffe.V0LayerParameter.cropsize\', index=20,\n      number=20, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mirror\', full_name=\'caffe.V0LayerParameter.mirror\', index=21,\n      number=21, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'blobs\', full_name=\'caffe.V0LayerParameter.blobs\', index=22,\n      number=50, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'blobs_lr\', full_name=\'caffe.V0LayerParameter.blobs_lr\', index=23,\n      number=51, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_decay\', full_name=\'caffe.V0LayerParameter.weight_decay\', index=24,\n      number=52, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rand_skip\', full_name=\'caffe.V0LayerParameter.rand_skip\', index=25,\n      number=53, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'det_fg_threshold\', full_name=\'caffe.V0LayerParameter.det_fg_threshold\', index=26,\n      number=54, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0.5,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'det_bg_threshold\', full_name=\'caffe.V0LayerParameter.det_bg_threshold\', index=27,\n      number=55, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0.5,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'det_fg_fraction\', full_name=\'caffe.V0LayerParameter.det_fg_fraction\', index=28,\n      number=56, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=0.25,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'det_context_pad\', full_name=\'caffe.V0LayerParameter.det_context_pad\', index=29,\n      number=58, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'det_crop_mode\', full_name=\'caffe.V0LayerParameter.det_crop_mode\', index=30,\n      number=59, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=unicode(""warp"", ""utf-8""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_num\', full_name=\'caffe.V0LayerParameter.new_num\', index=31,\n      number=60, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_channels\', full_name=\'caffe.V0LayerParameter.new_channels\', index=32,\n      number=61, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_height\', full_name=\'caffe.V0LayerParameter.new_height\', index=33,\n      number=62, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_width\', full_name=\'caffe.V0LayerParameter.new_width\', index=34,\n      number=63, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle_images\', full_name=\'caffe.V0LayerParameter.shuffle_images\', index=35,\n      number=64, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'concat_dim\', full_name=\'caffe.V0LayerParameter.concat_dim\', index=36,\n      number=65, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hdf5_output_param\', full_name=\'caffe.V0LayerParameter.hdf5_output_param\', index=37,\n      number=1001, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _V0LAYERPARAMETER_POOLMETHOD,\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=13871,\n  serialized_end=14892,\n)\n\n\n_PRELUPARAMETER = _descriptor.Descriptor(\n  name=\'PReLUParameter\',\n  full_name=\'caffe.PReLUParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'filler\', full_name=\'caffe.PReLUParameter.filler\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'channel_shared\', full_name=\'caffe.PReLUParameter.channel_shared\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=14894,\n  serialized_end=14981,\n)\n\n\n_SCALEPARAMETER = _descriptor.Descriptor(\n  name=\'ScaleParameter\',\n  full_name=\'caffe.ScaleParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.ScaleParameter.axis\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_axes\', full_name=\'caffe.ScaleParameter.num_axes\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'filler\', full_name=\'caffe.ScaleParameter.filler\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_term\', full_name=\'caffe.ScaleParameter.bias_term\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_filler\', full_name=\'caffe.ScaleParameter.bias_filler\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=14984,\n  serialized_end=15149,\n)\n\n\n_BIASPARAMETER = _descriptor.Descriptor(\n  name=\'BiasParameter\',\n  full_name=\'caffe.BiasParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'axis\', full_name=\'caffe.BiasParameter.axis\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_axes\', full_name=\'caffe.BiasParameter.num_axes\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'filler\', full_name=\'caffe.BiasParameter.filler\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=15151,\n  serialized_end=15244,\n)\n\n\n_BATCHREDUCTIONPARAMETER = _descriptor.Descriptor(\n  name=\'BatchReductionParameter\',\n  full_name=\'caffe.BatchReductionParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'level\', full_name=\'caffe.BatchReductionParameter.level\', index=0,\n      number=1, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'reduction_param\', full_name=\'caffe.BatchReductionParameter.reduction_param\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pos\', full_name=\'caffe.BatchReductionParameter.pos\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=15246,\n  serialized_end=15358,\n)\n\n\n_MEMORYOPTIMIZATIONPARAMETER = _descriptor.Descriptor(\n  name=\'MemoryOptimizationParameter\',\n  full_name=\'caffe.MemoryOptimizationParameter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'optimize_train\', full_name=\'caffe.MemoryOptimizationParameter.optimize_train\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'optimize_test\', full_name=\'caffe.MemoryOptimizationParameter.optimize_test\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'exclude_blob\', full_name=\'caffe.MemoryOptimizationParameter.exclude_blob\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  extension_ranges=[],\n  serialized_start=15360,\n  serialized_end=15471,\n)\n\n_BLOBPROTO.fields_by_name[\'shape\'].message_type = _BLOBSHAPE\n_BLOBPROTOVECTOR.fields_by_name[\'blobs\'].message_type = _BLOBPROTO\n_FILLERPARAMETER.fields_by_name[\'variance_norm\'].enum_type = _FILLERPARAMETER_VARIANCENORM\n_FILLERPARAMETER_VARIANCENORM.containing_type = _FILLERPARAMETER;\n_NETPARAMETER.fields_by_name[\'input_shape\'].message_type = _BLOBSHAPE\n_NETPARAMETER.fields_by_name[\'state\'].message_type = _NETSTATE\n_NETPARAMETER.fields_by_name[\'layer\'].message_type = _LAYERPARAMETER\n_NETPARAMETER.fields_by_name[\'mem_param\'].message_type = _MEMORYOPTIMIZATIONPARAMETER\n_NETPARAMETER.fields_by_name[\'layers\'].message_type = _V1LAYERPARAMETER\n_SOLVERPARAMETER.fields_by_name[\'net_param\'].message_type = _NETPARAMETER\n_SOLVERPARAMETER.fields_by_name[\'train_net_param\'].message_type = _NETPARAMETER\n_SOLVERPARAMETER.fields_by_name[\'test_net_param\'].message_type = _NETPARAMETER\n_SOLVERPARAMETER.fields_by_name[\'train_state\'].message_type = _NETSTATE\n_SOLVERPARAMETER.fields_by_name[\'test_state\'].message_type = _NETSTATE\n_SOLVERPARAMETER.fields_by_name[\'solver_mode\'].enum_type = _SOLVERPARAMETER_SOLVERMODE\n_SOLVERPARAMETER.fields_by_name[\'solver_type\'].enum_type = _SOLVERPARAMETER_SOLVERTYPE\n_SOLVERPARAMETER_SOLVERMODE.containing_type = _SOLVERPARAMETER;\n_SOLVERPARAMETER_SOLVERTYPE.containing_type = _SOLVERPARAMETER;\n_SOLVERSTATE.fields_by_name[\'history\'].message_type = _BLOBPROTO\n_NETSTATE.fields_by_name[\'phase\'].enum_type = _PHASE\n_NETSTATERULE.fields_by_name[\'phase\'].enum_type = _PHASE\n_PARAMSPEC.fields_by_name[\'share_mode\'].enum_type = _PARAMSPEC_DIMCHECKMODE\n_PARAMSPEC_DIMCHECKMODE.containing_type = _PARAMSPEC;\n_LAYERPARAMETER.fields_by_name[\'phase\'].enum_type = _PHASE\n_LAYERPARAMETER.fields_by_name[\'param\'].message_type = _PARAMSPEC\n_LAYERPARAMETER.fields_by_name[\'blobs\'].message_type = _BLOBPROTO\n_LAYERPARAMETER.fields_by_name[\'include\'].message_type = _NETSTATERULE\n_LAYERPARAMETER.fields_by_name[\'exclude\'].message_type = _NETSTATERULE\n_LAYERPARAMETER.fields_by_name[\'transform_param\'].message_type = _TRANSFORMATIONPARAMETER\n_LAYERPARAMETER.fields_by_name[\'loss_param\'].message_type = _LOSSPARAMETER\n_LAYERPARAMETER.fields_by_name[\'accuracy_param\'].message_type = _ACCURACYPARAMETER\n_LAYERPARAMETER.fields_by_name[\'argmax_param\'].message_type = _ARGMAXPARAMETER\n_LAYERPARAMETER.fields_by_name[\'bn_param\'].message_type = _BNPARAMETER\n_LAYERPARAMETER.fields_by_name[\'concat_param\'].message_type = _CONCATPARAMETER\n_LAYERPARAMETER.fields_by_name[\'contrastive_loss_param\'].message_type = _CONTRASTIVELOSSPARAMETER\n_LAYERPARAMETER.fields_by_name[\'convolution_param\'].message_type = _CONVOLUTIONPARAMETER\n_LAYERPARAMETER.fields_by_name[\'data_param\'].message_type = _DATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'dropout_param\'].message_type = _DROPOUTPARAMETER\n_LAYERPARAMETER.fields_by_name[\'dummy_data_param\'].message_type = _DUMMYDATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'eltwise_param\'].message_type = _ELTWISEPARAMETER\n_LAYERPARAMETER.fields_by_name[\'exp_param\'].message_type = _EXPPARAMETER\n_LAYERPARAMETER.fields_by_name[\'flatten_param\'].message_type = _FLATTENPARAMETER\n_LAYERPARAMETER.fields_by_name[\'hdf5_data_param\'].message_type = _HDF5DATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'hdf5_output_param\'].message_type = _HDF5OUTPUTPARAMETER\n_LAYERPARAMETER.fields_by_name[\'hinge_loss_param\'].message_type = _HINGELOSSPARAMETER\n_LAYERPARAMETER.fields_by_name[\'image_data_param\'].message_type = _IMAGEDATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'infogain_loss_param\'].message_type = _INFOGAINLOSSPARAMETER\n_LAYERPARAMETER.fields_by_name[\'inner_product_param\'].message_type = _INNERPRODUCTPARAMETER\n_LAYERPARAMETER.fields_by_name[\'log_param\'].message_type = _LOGPARAMETER\n_LAYERPARAMETER.fields_by_name[\'lrn_param\'].message_type = _LRNPARAMETER\n_LAYERPARAMETER.fields_by_name[\'memory_data_param\'].message_type = _MEMORYDATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'mvn_param\'].message_type = _MVNPARAMETER\n_LAYERPARAMETER.fields_by_name[\'pooling_param\'].message_type = _POOLINGPARAMETER\n_LAYERPARAMETER.fields_by_name[\'power_param\'].message_type = _POWERPARAMETER\n_LAYERPARAMETER.fields_by_name[\'prelu_param\'].message_type = _PRELUPARAMETER\n_LAYERPARAMETER.fields_by_name[\'python_param\'].message_type = _PYTHONPARAMETER\n_LAYERPARAMETER.fields_by_name[\'reduction_param\'].message_type = _REDUCTIONPARAMETER\n_LAYERPARAMETER.fields_by_name[\'relu_param\'].message_type = _RELUPARAMETER\n_LAYERPARAMETER.fields_by_name[\'reshape_param\'].message_type = _RESHAPEPARAMETER\n_LAYERPARAMETER.fields_by_name[\'seg_data_param\'].message_type = _SEGDATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'sigmoid_param\'].message_type = _SIGMOIDPARAMETER\n_LAYERPARAMETER.fields_by_name[\'softmax_param\'].message_type = _SOFTMAXPARAMETER\n_LAYERPARAMETER.fields_by_name[\'spp_param\'].message_type = _SPPPARAMETER\n_LAYERPARAMETER.fields_by_name[\'slice_param\'].message_type = _SLICEPARAMETER\n_LAYERPARAMETER.fields_by_name[\'tanh_param\'].message_type = _TANHPARAMETER\n_LAYERPARAMETER.fields_by_name[\'threshold_param\'].message_type = _THRESHOLDPARAMETER\n_LAYERPARAMETER.fields_by_name[\'window_data_param\'].message_type = _WINDOWDATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'video_data_param\'].message_type = _VIDEODATAPARAMETER\n_LAYERPARAMETER.fields_by_name[\'roi_pooling_param\'].message_type = _ROIPOOLINGPARAMETER\n_LAYERPARAMETER.fields_by_name[\'scale_param\'].message_type = _SCALEPARAMETER\n_LAYERPARAMETER.fields_by_name[\'bias_param\'].message_type = _BIASPARAMETER\n_LAYERPARAMETER.fields_by_name[\'batch_reduction_param\'].message_type = _BATCHREDUCTIONPARAMETER\n_BNPARAMETER.fields_by_name[\'slope_filler\'].message_type = _FILLERPARAMETER\n_BNPARAMETER.fields_by_name[\'bias_filler\'].message_type = _FILLERPARAMETER\n_BNPARAMETER.fields_by_name[\'engine\'].enum_type = _BNPARAMETER_ENGINE\n_BNPARAMETER_ENGINE.containing_type = _BNPARAMETER;\n_CONVOLUTIONPARAMETER.fields_by_name[\'weight_filler\'].message_type = _FILLERPARAMETER\n_CONVOLUTIONPARAMETER.fields_by_name[\'bias_filler\'].message_type = _FILLERPARAMETER\n_CONVOLUTIONPARAMETER.fields_by_name[\'engine\'].enum_type = _CONVOLUTIONPARAMETER_ENGINE\n_CONVOLUTIONPARAMETER_ENGINE.containing_type = _CONVOLUTIONPARAMETER;\n_DATAPARAMETER.fields_by_name[\'backend\'].enum_type = _DATAPARAMETER_DB\n_DATAPARAMETER_DB.containing_type = _DATAPARAMETER;\n_DUMMYDATAPARAMETER.fields_by_name[\'data_filler\'].message_type = _FILLERPARAMETER\n_DUMMYDATAPARAMETER.fields_by_name[\'shape\'].message_type = _BLOBSHAPE\n_ELTWISEPARAMETER.fields_by_name[\'operation\'].enum_type = _ELTWISEPARAMETER_ELTWISEOP\n_ELTWISEPARAMETER_ELTWISEOP.containing_type = _ELTWISEPARAMETER;\n_HINGELOSSPARAMETER.fields_by_name[\'norm\'].enum_type = _HINGELOSSPARAMETER_NORM\n_HINGELOSSPARAMETER_NORM.containing_type = _HINGELOSSPARAMETER;\n_VIDEODATAPARAMETER.fields_by_name[\'modality\'].enum_type = _VIDEODATAPARAMETER_MODALITY\n_VIDEODATAPARAMETER_MODALITY.containing_type = _VIDEODATAPARAMETER;\n_INNERPRODUCTPARAMETER.fields_by_name[\'weight_filler\'].message_type = _FILLERPARAMETER\n_INNERPRODUCTPARAMETER.fields_by_name[\'bias_filler\'].message_type = _FILLERPARAMETER\n_LRNPARAMETER.fields_by_name[\'norm_region\'].enum_type = _LRNPARAMETER_NORMREGION\n_LRNPARAMETER_NORMREGION.containing_type = _LRNPARAMETER;\n_POOLINGPARAMETER.fields_by_name[\'pool\'].enum_type = _POOLINGPARAMETER_POOLMETHOD\n_POOLINGPARAMETER.fields_by_name[\'engine\'].enum_type = _POOLINGPARAMETER_ENGINE\n_POOLINGPARAMETER_POOLMETHOD.containing_type = _POOLINGPARAMETER;\n_POOLINGPARAMETER_ENGINE.containing_type = _POOLINGPARAMETER;\n_REDUCTIONPARAMETER.fields_by_name[\'operation\'].enum_type = _REDUCTIONPARAMETER_REDUCTIONOP\n_REDUCTIONPARAMETER_REDUCTIONOP.containing_type = _REDUCTIONPARAMETER;\n_RELUPARAMETER.fields_by_name[\'engine\'].enum_type = _RELUPARAMETER_ENGINE\n_RELUPARAMETER_ENGINE.containing_type = _RELUPARAMETER;\n_RESHAPEPARAMETER.fields_by_name[\'shape\'].message_type = _BLOBSHAPE\n_SIGMOIDPARAMETER.fields_by_name[\'engine\'].enum_type = _SIGMOIDPARAMETER_ENGINE\n_SIGMOIDPARAMETER_ENGINE.containing_type = _SIGMOIDPARAMETER;\n_SOFTMAXPARAMETER.fields_by_name[\'engine\'].enum_type = _SOFTMAXPARAMETER_ENGINE\n_SOFTMAXPARAMETER_ENGINE.containing_type = _SOFTMAXPARAMETER;\n_TANHPARAMETER.fields_by_name[\'engine\'].enum_type = _TANHPARAMETER_ENGINE\n_TANHPARAMETER_ENGINE.containing_type = _TANHPARAMETER;\n_SPPPARAMETER.fields_by_name[\'pool\'].enum_type = _SPPPARAMETER_POOLMETHOD\n_SPPPARAMETER.fields_by_name[\'engine\'].enum_type = _SPPPARAMETER_ENGINE\n_SPPPARAMETER_POOLMETHOD.containing_type = _SPPPARAMETER;\n_SPPPARAMETER_ENGINE.containing_type = _SPPPARAMETER;\n_V1LAYERPARAMETER.fields_by_name[\'include\'].message_type = _NETSTATERULE\n_V1LAYERPARAMETER.fields_by_name[\'exclude\'].message_type = _NETSTATERULE\n_V1LAYERPARAMETER.fields_by_name[\'type\'].enum_type = _V1LAYERPARAMETER_LAYERTYPE\n_V1LAYERPARAMETER.fields_by_name[\'blobs\'].message_type = _BLOBPROTO\n_V1LAYERPARAMETER.fields_by_name[\'blob_share_mode\'].enum_type = _V1LAYERPARAMETER_DIMCHECKMODE\n_V1LAYERPARAMETER.fields_by_name[\'accuracy_param\'].message_type = _ACCURACYPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'argmax_param\'].message_type = _ARGMAXPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'concat_param\'].message_type = _CONCATPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'contrastive_loss_param\'].message_type = _CONTRASTIVELOSSPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'convolution_param\'].message_type = _CONVOLUTIONPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'data_param\'].message_type = _DATAPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'dropout_param\'].message_type = _DROPOUTPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'dummy_data_param\'].message_type = _DUMMYDATAPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'eltwise_param\'].message_type = _ELTWISEPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'exp_param\'].message_type = _EXPPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'hdf5_data_param\'].message_type = _HDF5DATAPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'hdf5_output_param\'].message_type = _HDF5OUTPUTPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'hinge_loss_param\'].message_type = _HINGELOSSPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'image_data_param\'].message_type = _IMAGEDATAPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'infogain_loss_param\'].message_type = _INFOGAINLOSSPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'inner_product_param\'].message_type = _INNERPRODUCTPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'lrn_param\'].message_type = _LRNPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'memory_data_param\'].message_type = _MEMORYDATAPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'mvn_param\'].message_type = _MVNPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'pooling_param\'].message_type = _POOLINGPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'power_param\'].message_type = _POWERPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'relu_param\'].message_type = _RELUPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'sigmoid_param\'].message_type = _SIGMOIDPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'softmax_param\'].message_type = _SOFTMAXPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'slice_param\'].message_type = _SLICEPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'tanh_param\'].message_type = _TANHPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'threshold_param\'].message_type = _THRESHOLDPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'window_data_param\'].message_type = _WINDOWDATAPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'transform_param\'].message_type = _TRANSFORMATIONPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'loss_param\'].message_type = _LOSSPARAMETER\n_V1LAYERPARAMETER.fields_by_name[\'layer\'].message_type = _V0LAYERPARAMETER\n_V1LAYERPARAMETER_LAYERTYPE.containing_type = _V1LAYERPARAMETER;\n_V1LAYERPARAMETER_DIMCHECKMODE.containing_type = _V1LAYERPARAMETER;\n_V0LAYERPARAMETER.fields_by_name[\'weight_filler\'].message_type = _FILLERPARAMETER\n_V0LAYERPARAMETER.fields_by_name[\'bias_filler\'].message_type = _FILLERPARAMETER\n_V0LAYERPARAMETER.fields_by_name[\'pool\'].enum_type = _V0LAYERPARAMETER_POOLMETHOD\n_V0LAYERPARAMETER.fields_by_name[\'blobs\'].message_type = _BLOBPROTO\n_V0LAYERPARAMETER.fields_by_name[\'hdf5_output_param\'].message_type = _HDF5OUTPUTPARAMETER\n_V0LAYERPARAMETER_POOLMETHOD.containing_type = _V0LAYERPARAMETER;\n_PRELUPARAMETER.fields_by_name[\'filler\'].message_type = _FILLERPARAMETER\n_SCALEPARAMETER.fields_by_name[\'filler\'].message_type = _FILLERPARAMETER\n_SCALEPARAMETER.fields_by_name[\'bias_filler\'].message_type = _FILLERPARAMETER\n_BIASPARAMETER.fields_by_name[\'filler\'].message_type = _FILLERPARAMETER\n_BATCHREDUCTIONPARAMETER.fields_by_name[\'reduction_param\'].message_type = _REDUCTIONPARAMETER\nDESCRIPTOR.message_types_by_name[\'BlobShape\'] = _BLOBSHAPE\nDESCRIPTOR.message_types_by_name[\'BlobProto\'] = _BLOBPROTO\nDESCRIPTOR.message_types_by_name[\'BlobProtoVector\'] = _BLOBPROTOVECTOR\nDESCRIPTOR.message_types_by_name[\'Datum\'] = _DATUM\nDESCRIPTOR.message_types_by_name[\'FillerParameter\'] = _FILLERPARAMETER\nDESCRIPTOR.message_types_by_name[\'NetParameter\'] = _NETPARAMETER\nDESCRIPTOR.message_types_by_name[\'SolverParameter\'] = _SOLVERPARAMETER\nDESCRIPTOR.message_types_by_name[\'SolverState\'] = _SOLVERSTATE\nDESCRIPTOR.message_types_by_name[\'NetState\'] = _NETSTATE\nDESCRIPTOR.message_types_by_name[\'NetStateRule\'] = _NETSTATERULE\nDESCRIPTOR.message_types_by_name[\'ParamSpec\'] = _PARAMSPEC\nDESCRIPTOR.message_types_by_name[\'LayerParameter\'] = _LAYERPARAMETER\nDESCRIPTOR.message_types_by_name[\'TransformationParameter\'] = _TRANSFORMATIONPARAMETER\nDESCRIPTOR.message_types_by_name[\'LossParameter\'] = _LOSSPARAMETER\nDESCRIPTOR.message_types_by_name[\'AccuracyParameter\'] = _ACCURACYPARAMETER\nDESCRIPTOR.message_types_by_name[\'ArgMaxParameter\'] = _ARGMAXPARAMETER\nDESCRIPTOR.message_types_by_name[\'BNParameter\'] = _BNPARAMETER\nDESCRIPTOR.message_types_by_name[\'ConcatParameter\'] = _CONCATPARAMETER\nDESCRIPTOR.message_types_by_name[\'ContrastiveLossParameter\'] = _CONTRASTIVELOSSPARAMETER\nDESCRIPTOR.message_types_by_name[\'ConvolutionParameter\'] = _CONVOLUTIONPARAMETER\nDESCRIPTOR.message_types_by_name[\'DataParameter\'] = _DATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'DropoutParameter\'] = _DROPOUTPARAMETER\nDESCRIPTOR.message_types_by_name[\'DummyDataParameter\'] = _DUMMYDATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'EltwiseParameter\'] = _ELTWISEPARAMETER\nDESCRIPTOR.message_types_by_name[\'ExpParameter\'] = _EXPPARAMETER\nDESCRIPTOR.message_types_by_name[\'FlattenParameter\'] = _FLATTENPARAMETER\nDESCRIPTOR.message_types_by_name[\'HDF5DataParameter\'] = _HDF5DATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'HDF5OutputParameter\'] = _HDF5OUTPUTPARAMETER\nDESCRIPTOR.message_types_by_name[\'HingeLossParameter\'] = _HINGELOSSPARAMETER\nDESCRIPTOR.message_types_by_name[\'ImageDataParameter\'] = _IMAGEDATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'VideoDataParameter\'] = _VIDEODATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'InfogainLossParameter\'] = _INFOGAINLOSSPARAMETER\nDESCRIPTOR.message_types_by_name[\'InnerProductParameter\'] = _INNERPRODUCTPARAMETER\nDESCRIPTOR.message_types_by_name[\'LogParameter\'] = _LOGPARAMETER\nDESCRIPTOR.message_types_by_name[\'LRNParameter\'] = _LRNPARAMETER\nDESCRIPTOR.message_types_by_name[\'MemoryDataParameter\'] = _MEMORYDATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'MVNParameter\'] = _MVNPARAMETER\nDESCRIPTOR.message_types_by_name[\'PoolingParameter\'] = _POOLINGPARAMETER\nDESCRIPTOR.message_types_by_name[\'PowerParameter\'] = _POWERPARAMETER\nDESCRIPTOR.message_types_by_name[\'PythonParameter\'] = _PYTHONPARAMETER\nDESCRIPTOR.message_types_by_name[\'ReductionParameter\'] = _REDUCTIONPARAMETER\nDESCRIPTOR.message_types_by_name[\'ReLUParameter\'] = _RELUPARAMETER\nDESCRIPTOR.message_types_by_name[\'ReshapeParameter\'] = _RESHAPEPARAMETER\nDESCRIPTOR.message_types_by_name[\'SegDataParameter\'] = _SEGDATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'SigmoidParameter\'] = _SIGMOIDPARAMETER\nDESCRIPTOR.message_types_by_name[\'SliceParameter\'] = _SLICEPARAMETER\nDESCRIPTOR.message_types_by_name[\'SoftmaxParameter\'] = _SOFTMAXPARAMETER\nDESCRIPTOR.message_types_by_name[\'TanHParameter\'] = _TANHPARAMETER\nDESCRIPTOR.message_types_by_name[\'ThresholdParameter\'] = _THRESHOLDPARAMETER\nDESCRIPTOR.message_types_by_name[\'WindowDataParameter\'] = _WINDOWDATAPARAMETER\nDESCRIPTOR.message_types_by_name[\'SPPParameter\'] = _SPPPARAMETER\nDESCRIPTOR.message_types_by_name[\'ROIPoolingParameter\'] = _ROIPOOLINGPARAMETER\nDESCRIPTOR.message_types_by_name[\'V1LayerParameter\'] = _V1LAYERPARAMETER\nDESCRIPTOR.message_types_by_name[\'V0LayerParameter\'] = _V0LAYERPARAMETER\nDESCRIPTOR.message_types_by_name[\'PReLUParameter\'] = _PRELUPARAMETER\nDESCRIPTOR.message_types_by_name[\'ScaleParameter\'] = _SCALEPARAMETER\nDESCRIPTOR.message_types_by_name[\'BiasParameter\'] = _BIASPARAMETER\nDESCRIPTOR.message_types_by_name[\'BatchReductionParameter\'] = _BATCHREDUCTIONPARAMETER\nDESCRIPTOR.message_types_by_name[\'MemoryOptimizationParameter\'] = _MEMORYOPTIMIZATIONPARAMETER\n\nclass BlobShape(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _BLOBSHAPE\n\n  # @@protoc_insertion_point(class_scope:caffe.BlobShape)\n\nclass BlobProto(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _BLOBPROTO\n\n  # @@protoc_insertion_point(class_scope:caffe.BlobProto)\n\nclass BlobProtoVector(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _BLOBPROTOVECTOR\n\n  # @@protoc_insertion_point(class_scope:caffe.BlobProtoVector)\n\nclass Datum(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _DATUM\n\n  # @@protoc_insertion_point(class_scope:caffe.Datum)\n\nclass FillerParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _FILLERPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.FillerParameter)\n\nclass NetParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _NETPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.NetParameter)\n\nclass SolverParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _SOLVERPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.SolverParameter)\n\nclass SolverState(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _SOLVERSTATE\n\n  # @@protoc_insertion_point(class_scope:caffe.SolverState)\n\nclass NetState(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _NETSTATE\n\n  # @@protoc_insertion_point(class_scope:caffe.NetState)\n\nclass NetStateRule(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _NETSTATERULE\n\n  # @@protoc_insertion_point(class_scope:caffe.NetStateRule)\n\nclass ParamSpec(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _PARAMSPEC\n\n  # @@protoc_insertion_point(class_scope:caffe.ParamSpec)\n\nclass LayerParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _LAYERPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.LayerParameter)\n\nclass TransformationParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _TRANSFORMATIONPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.TransformationParameter)\n\nclass LossParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _LOSSPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.LossParameter)\n\nclass AccuracyParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _ACCURACYPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.AccuracyParameter)\n\nclass ArgMaxParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _ARGMAXPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.ArgMaxParameter)\n\nclass BNParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _BNPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.BNParameter)\n\nclass ConcatParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _CONCATPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.ConcatParameter)\n\nclass ContrastiveLossParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _CONTRASTIVELOSSPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter)\n\nclass ConvolutionParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _CONVOLUTIONPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.ConvolutionParameter)\n\nclass DataParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _DATAPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.DataParameter)\n\nclass DropoutParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _DROPOUTPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.DropoutParameter)\n\nclass DummyDataParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _DUMMYDATAPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.DummyDataParameter)\n\nclass EltwiseParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _ELTWISEPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.EltwiseParameter)\n\nclass ExpParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _EXPPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.ExpParameter)\n\nclass FlattenParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _FLATTENPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.FlattenParameter)\n\nclass HDF5DataParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _HDF5DATAPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.HDF5DataParameter)\n\nclass HDF5OutputParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _HDF5OUTPUTPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter)\n\nclass HingeLossParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _HINGELOSSPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.HingeLossParameter)\n\nclass ImageDataParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _IMAGEDATAPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.ImageDataParameter)\n\nclass VideoDataParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _VIDEODATAPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.VideoDataParameter)\n\nclass InfogainLossParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _INFOGAINLOSSPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.InfogainLossParameter)\n\nclass InnerProductParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _INNERPRODUCTPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.InnerProductParameter)\n\nclass LogParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _LOGPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.LogParameter)\n\nclass LRNParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _LRNPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.LRNParameter)\n\nclass MemoryDataParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _MEMORYDATAPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.MemoryDataParameter)\n\nclass MVNParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _MVNPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.MVNParameter)\n\nclass PoolingParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _POOLINGPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.PoolingParameter)\n\nclass PowerParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _POWERPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.PowerParameter)\n\nclass PythonParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _PYTHONPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.PythonParameter)\n\nclass ReductionParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _REDUCTIONPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.ReductionParameter)\n\nclass ReLUParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _RELUPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.ReLUParameter)\n\nclass ReshapeParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _RESHAPEPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.ReshapeParameter)\n\nclass SegDataParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _SEGDATAPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.SegDataParameter)\n\nclass SigmoidParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _SIGMOIDPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.SigmoidParameter)\n\nclass SliceParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _SLICEPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.SliceParameter)\n\nclass SoftmaxParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _SOFTMAXPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.SoftmaxParameter)\n\nclass TanHParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _TANHPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.TanHParameter)\n\nclass ThresholdParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _THRESHOLDPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.ThresholdParameter)\n\nclass WindowDataParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _WINDOWDATAPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.WindowDataParameter)\n\nclass SPPParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _SPPPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.SPPParameter)\n\nclass ROIPoolingParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _ROIPOOLINGPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.ROIPoolingParameter)\n\nclass V1LayerParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _V1LAYERPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.V1LayerParameter)\n\nclass V0LayerParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _V0LAYERPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.V0LayerParameter)\n\nclass PReLUParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _PRELUPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.PReLUParameter)\n\nclass ScaleParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _SCALEPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.ScaleParameter)\n\nclass BiasParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _BIASPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.BiasParameter)\n\nclass BatchReductionParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _BATCHREDUCTIONPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.BatchReductionParameter)\n\nclass MemoryOptimizationParameter(_message.Message):\n  __metaclass__ = _reflection.GeneratedProtocolMessageType\n  DESCRIPTOR = _MEMORYOPTIMIZATIONPARAMETER\n\n  # @@protoc_insertion_point(class_scope:caffe.MemoryOptimizationParameter)\n\n\n_BLOBSHAPE.fields_by_name[\'dim\'].has_options = True\n_BLOBSHAPE.fields_by_name[\'dim\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), \'\\020\\001\')\n_BLOBPROTO.fields_by_name[\'data\'].has_options = True\n_BLOBPROTO.fields_by_name[\'data\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), \'\\020\\001\')\n_BLOBPROTO.fields_by_name[\'diff\'].has_options = True\n_BLOBPROTO.fields_by_name[\'diff\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), \'\\020\\001\')\n# @@protoc_insertion_point(module_scope)\n'"
model_zoo/bninception/layer_factory.py,0,"b'import torch\nfrom torch import nn\n\n\nLAYER_BUILDER_DICT=dict()\n\n\ndef parse_expr(expr):\n    parts = expr.split(\'<=\')\n    return parts[0].split(\',\'), parts[1], parts[2].split(\',\')\n\n\ndef get_basic_layer(info, channels=None, conv_bias=False):\n    id = info[\'id\']\n    attr = info[\'attrs\'] if \'attrs\' in info else list()\n\n    out, op, in_vars = parse_expr(info[\'expr\'])\n    assert(len(out) == 1)\n    assert(len(in_vars) == 1)\n    mod, out_channel, = LAYER_BUILDER_DICT[op](attr, channels, conv_bias)\n\n    return id, out[0], mod, out_channel, in_vars[0]\n\n\ndef build_conv(attr, channels=None, conv_bias=False):\n    out_channels = attr[\'num_output\']\n    ks = attr[\'kernel_size\'] if \'kernel_size\' in attr else (attr[\'kernel_h\'], attr[\'kernel_w\'])\n    if \'pad\' in attr or \'pad_w\' in attr and \'pad_h\' in attr:\n        padding = attr[\'pad\'] if \'pad\' in attr else (attr[\'pad_h\'], attr[\'pad_w\'])\n    else:\n        padding = 0\n    if \'stride\' in attr or \'stride_w\' in attr and \'stride_h\' in attr:\n        stride = attr[\'stride\'] if \'stride\' in attr else (attr[\'stride_h\'], attr[\'stride_w\'])\n    else:\n        stride = 1\n\n    conv = nn.Conv2d(channels, out_channels, ks, stride, padding, bias=conv_bias)\n\n    return conv, out_channels\n\n\ndef build_pooling(attr, channels=None, conv_bias=False):\n    method = attr[\'mode\']\n    pad = attr[\'pad\'] if \'pad\' in attr else 0\n    if method == \'max\':\n        pool = nn.MaxPool2d(attr[\'kernel_size\'], attr[\'stride\'], pad,\n                            ceil_mode=True) # all Caffe pooling use ceil model\n    elif method == \'ave\':\n        pool = nn.AvgPool2d(attr[\'kernel_size\'], attr[\'stride\'], pad,\n                            ceil_mode=True)  # all Caffe pooling use ceil model\n    else:\n        raise ValueError(""Unknown pooling method: {}"".format(method))\n\n    return pool, channels\n\n\ndef build_relu(attr, channels=None, conv_bias=False):\n    return nn.ReLU(inplace=True), channels\n\n\ndef build_bn(attr, channels=None, conv_bias=False):\n    return nn.BatchNorm2d(channels, momentum=0.1), channels\n\n\ndef build_linear(attr, channels=None, conv_bias=False):\n    return nn.Linear(channels, attr[\'num_output\']), channels\n\n\ndef build_dropout(attr, channels=None, conv_bias=False):\n    return nn.Dropout(p=attr[\'dropout_ratio\']), channels\n\n\nLAYER_BUILDER_DICT[\'Convolution\'] = build_conv\n\nLAYER_BUILDER_DICT[\'Pooling\'] = build_pooling\n\nLAYER_BUILDER_DICT[\'ReLU\'] = build_relu\n\nLAYER_BUILDER_DICT[\'Dropout\'] = build_dropout\n\nLAYER_BUILDER_DICT[\'BN\'] = build_bn\n\nLAYER_BUILDER_DICT[\'InnerProduct\'] = build_linear\n\n'"
model_zoo/bninception/parse_caffe.py,5,"b'#!/usr/bin/env python\n\nimport argparse\n\nparser = argparse.ArgumentParser(description=""Convert a Caffe model and its learned parameters to torch"")\nparser.add_argument(\'model\', help=\'network spec, usually a ProtoBuf text message\')\nparser.add_argument(\'weights\', help=\'network parameters, usually in a name like *.caffemodel \')\nparser.add_argument(\'--model_yaml\', help=""translated model spec yaml file"")\nparser.add_argument(\'--dump_weights\', help=""translated model parameters to be used by torch"")\nparser.add_argument(\'--model_version\', help=""the version of Caffe\'s model spec, usually 2"", default=2)\n\nargs = parser.parse_args()\n\nfrom . import caffe_pb2\nfrom google.protobuf import text_format\nfrom pprint import pprint\nimport yaml\nimport numpy as np\nimport torch\n\n\nclass CaffeVendor(object):\n    def __init__(self, net_name, weight_name, version=2):\n        print(""loading model spec..."")\n        self._net_pb = caffe_pb2.NetParameter()\n        text_format.Merge(open(net_name).read(), self._net_pb)\n        self._weight_dict = {}\n        self._init_dict = []\n\n        if weight_name is not None:\n            print(""loading weights..."")\n            self._weight_pb = caffe_pb2.NetParameter()\n            self._weight_pb.ParseFromString(open(weight_name, \'rb\').read())\n            for l in self._weight_pb.layer:\n                self._weight_dict[l.name] = l\n\n        print(""parsing..."")\n        self._parse_net(version)\n\n    def _parse_net(self, version):\n        self._name = str(self._net_pb.name)\n        self._layers = self._net_pb.layer if version == 2 else self._net_pb.layers\n        self._parsed_layers = [self._layer2dict(x, version) for x in self._layers]\n\n        self._net_dict = {\n            \'name\': self._name,\n            \'inputs\': [],\n            \'layers\': [],\n        }\n\n        self._weight_array_dict = {}\n\n        for info, blob, is_data in self._parsed_layers:\n            if not is_data and info is not None:\n                self._net_dict[\'layers\'].append(info)\n\n            self._weight_array_dict.update(blob)\n\n    @staticmethod\n    def _parse_blob(blob):\n        flat_data = np.array(blob.data)\n        shaped_data = flat_data.reshape(list(blob.shape.dim))\n        return shaped_data\n\n    def _layer2dict(self, layer, version):\n        attr_dict = {}\n        params = []\n        weight_params = []\n        fillers = []\n\n        for field, value in layer.ListFields():\n            if field.name == \'top\':\n                tops = [v.replace(\'-\', \'_\').replace(\'/\', \'_\') for v in value]\n            elif field.name == \'name\':\n                layer_name = str(value).replace(\'-\', \'_\').replace(\'/\', \'_\')\n            elif field.name == \'bottom\':\n                bottoms = [v.replace(\'-\', \'_\').replace(\'/\', \'_\') for v in value]\n            elif field.name == \'include\':\n                if value[0].phase == 1 and op == \'Data\':\n                    print(\'found 1 testing data layer\')\n                    return None, dict(), dict(), False\n            elif field.name == \'type\':\n                if version == 2:\n                    op = value\n                else:\n                    raise NotImplemented\n            elif field.name == \'loss_weight\':\n                pass\n            elif field.name == \'param\':\n                pass\n            else:\n                # other params\n                try:\n                    for f, v in value.ListFields():\n                        if \'filler\' in f.name:\n                            pass\n                        elif f.name == \'pool\':\n                          attr_dict[\'mode\'] = \'max\' if v == 0 else \'ave\'\n                        else:\n                          attr_dict[f.name] = v\n\n                except:\n                    print(field.name, value)\n                    raise\n\n        expr_temp = \'{top}<={op}<={input}\'\n\n        if layer.name in self._weight_dict:\n            blobs = [self._parse_blob(x) for x in self._weight_dict[layer.name].blobs]\n        else:\n            blobs = []\n\n        blob_dict = dict()\n        if len(blobs) > 0:\n            blob_dict[\'{}.weight\'.format(layer_name)] = torch.from_numpy(blobs[0])\n            blob_dict[\'{}.bias\'.format(layer_name)] = torch.from_numpy(blobs[1])\n            if op == \'BN\':\n                blob_dict[\'{}.running_mean\'.format(layer_name)] = torch.from_numpy(blobs[2])\n                blob_dict[\'{}.running_var\'.format(layer_name)] = torch.from_numpy(blobs[3])\n\n        expr = expr_temp.format(top=\',\'.join(tops), input=\',\'.join(bottoms), op=op)\n\n        out_dict = {\n            \'id\': layer_name,\n            \'expr\': expr,\n        }\n\n        if len(attr_dict) > 0:\n            out_dict[\'attrs\'] = attr_dict\n\n        return out_dict, blob_dict, False\n\n    @property\n    def text_form(self):\n        return str(self._net_pb)\n\n    @property\n    def info(self):\n        return {\n            \'name\': self._name,\n            \'layers\': [x.name for x in self._layers]\n        }\n\n    @property\n    def yaml(self):\n        return yaml.dump(self._net_dict)\n\n    def dump_weights(self, filename):\n        # print self._weight_array_dict.keys()\n        torch.save(self._weight_array_dict, open(filename, \'wb\'))\n\n# build output\ncv = CaffeVendor(args.model, args.weights, int(args.model_version))\n\nif args.model_yaml is not None:\n    open(args.model_yaml, \'w\').write(cv.yaml)\n\nif args.dump_weights is not None:\n    cv.dump_weights(args.dump_weights)\n'"
model_zoo/bninception/pytorch_load.py,3,"b""import torch\nfrom torch import nn\nfrom .layer_factory import get_basic_layer, parse_expr\nimport torch.utils.model_zoo as model_zoo\nimport yaml\n\n\nclass BNInception(nn.Module):\n    def __init__(self, model_path='model_zoo/bninception/bn_inception.yaml', num_classes=101,\n                       weight_url='https://yjxiong.blob.core.windows.net/models/bn_inception-9f5701afb96c8044.pth'):\n        super(BNInception, self).__init__()\n\n        manifest = yaml.load(open(model_path))\n\n        layers = manifest['layers']\n\n        self._channel_dict = dict()\n\n        self._op_list = list()\n        for l in layers:\n            out_var, op, in_var = parse_expr(l['expr'])\n            if op != 'Concat':\n                id, out_name, module, out_channel, in_name = get_basic_layer(l,\n                                                                3 if len(self._channel_dict) == 0 else self._channel_dict[in_var[0]],\n                                                                             conv_bias=True)\n\n                self._channel_dict[out_name] = out_channel\n                setattr(self, id, module)\n                self._op_list.append((id, op, out_name, in_name))\n            else:\n                self._op_list.append((id, op, out_var[0], in_var))\n                channel = sum([self._channel_dict[x] for x in in_var])\n                self._channel_dict[out_var[0]] = channel\n\n        self.load_state_dict(torch.utils.model_zoo.load_url(weight_url))\n\n    def forward(self, input):\n        data_dict = dict()\n        data_dict[self._op_list[0][-1]] = input\n\n        def get_hook(name):\n\n            def hook(m, grad_in, grad_out):\n                print(name, grad_out[0].data.abs().mean())\n\n            return hook\n        for op in self._op_list:\n            if op[1] != 'Concat' and op[1] != 'InnerProduct':\n                data_dict[op[2]] = getattr(self, op[0])(data_dict[op[-1]])\n                # getattr(self, op[0]).register_backward_hook(get_hook(op[0]))\n            elif op[1] == 'InnerProduct':\n                x = data_dict[op[-1]]\n                data_dict[op[2]] = getattr(self, op[0])(x.view(x.size(0), -1))\n            else:\n                try:\n                    data_dict[op[2]] = torch.cat(tuple(data_dict[x] for x in op[-1]), 1)\n                except:\n                    for x in op[-1]:\n                        print(x,data_dict[x].size())\n                    raise\n        return data_dict[self._op_list[-1][2]]\n\n\nclass InceptionV3(BNInception):\n    def __init__(self, model_path='model_zoo/bninception/inceptionv3.yaml', num_classes=101,\n                 weight_url='https://yjxiong.blob.core.windows.net/models/inceptionv3-cuhk-0e09b300b493bc74c.pth'):\n        super(InceptionV3, self).__init__(model_path=model_path, weight_url=weight_url, num_classes=num_classes)\n"""
model_zoo/inceptionresnetv2/__init__.py,0,b''
model_zoo/inceptionresnetv2/pytorch_load.py,31,"b'import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport os\nimport sys\n\nmodel_urls = {\n    \'imagenet\': \'http://webia.lip6.fr/~cadene/Downloads/inceptionresnetv2-d579a627.pth\'\n}\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False) # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes, eps=0.001, momentum=0, affine=True)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\nclass Mixed_5b(nn.Module):\n\n    def __init__(self):\n        super(Mixed_5b, self).__init__()\n\n        self.branch0 = BasicConv2d(192, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(192, 48, kernel_size=1, stride=1),\n            BasicConv2d(48, 64, kernel_size=5, stride=1, padding=2)\n        ) \n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(192, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(192, 64, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\nclass Block35(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super(Block35, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(320, 32, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 48, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\nclass Mixed_6a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_6a, self).__init__()\n        \n        self.branch0 = BasicConv2d(320, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\nclass Block17(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super(Block17, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(1088, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 128, kernel_size=1, stride=1),\n            BasicConv2d(128, 160, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(160, 192, kernel_size=(7,1), stride=1, padding=(3,0))\n        )\n\n        self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\nclass Mixed_7a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_7a, self).__init__()\n        \n        self.branch0 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(288, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch3 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\nclass Block8(nn.Module):\n\n    def __init__(self, scale=1.0, noReLU=False):\n        super(Block8, self).__init__()\n\n        self.scale = scale\n        self.noReLU = noReLU\n\n        self.branch0 = BasicConv2d(2080, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(2080, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1,3), stride=1, padding=(0,1)),\n            BasicConv2d(224, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n        )\n\n        self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)\n        if not self.noReLU:\n            self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        if not self.noReLU:\n            out = self.relu(out)\n        return out\n\n\nclass InceptionResnetV2(nn.Module):\n\n    def __init__(self, num_classes=1001):\n        super(InceptionResnetV2, self).__init__()\n        self.conv2d_1a = BasicConv2d(3, 32, kernel_size=3, stride=2)\n        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n        self.maxpool_5a = nn.MaxPool2d(3, stride=2)\n        self.mixed_5b = Mixed_5b()\n        self.repeat = nn.Sequential(\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17)\n        )\n        self.mixed_6a = Mixed_6a()\n        self.repeat_1 = nn.Sequential(\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10)\n        )\n        self.mixed_7a = Mixed_7a()\n        self.repeat_2 = nn.Sequential(\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20)\n        )\n        self.block8 = Block8(noReLU=True)\n        self.conv2d_7b = BasicConv2d(2080, 1536, kernel_size=1, stride=1)\n        self.avgpool_1a = nn.AvgPool2d(8, count_include_pad=False)\n        self.classif = nn.Linear(1536, num_classes)\n\n    def forward(self, x):\n        x = self.conv2d_1a(x)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x = self.maxpool_5a(x)\n        x = self.mixed_5b(x)\n        x = self.repeat(x)\n        x = self.mixed_6a(x)\n        x = self.repeat_1(x)\n        x = self.mixed_7a(x)\n        x = self.repeat_2(x)\n        x = self.block8(x)\n        x = self.conv2d_7b(x)\n        x = self.avgpool_1a(x)\n        x = x.view(x.size(0), -1)\n        x = self.classif(x) \n        return x\n\ndef inceptionresnetv2(pretrained=True):\n    r""""""InceptionResnetV2 model architecture from the\n    `""InceptionV4, Inception-ResNet..."" <https://arxiv.org/abs/1602.07261>`_ paper.\n\n    Args:\n        pretrained (\'string\'): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = InceptionResnetV2()\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'imagenet\']))\n    return model\n\n\n######################################################################\n## Load parameters from HDF5 to Dict\n######################################################################\n\ndef load_conv2d(state_dict, name_pth, name_tf):\n    h5f = h5py.File(\'dump/InceptionResnetV2/\'+name_tf+\'.h5\', \'r\')\n    state_dict[name_pth+\'.conv.weight\'] = torch.from_numpy(h5f[\'weights\'][()]).permute(3, 2, 0, 1)\n    out_planes = state_dict[name_pth+\'.conv.weight\'].size(0)\n    state_dict[name_pth+\'.bn.weight\'] = torch.ones(out_planes)\n    state_dict[name_pth+\'.bn.bias\'] = torch.from_numpy(h5f[\'beta\'][()])\n    state_dict[name_pth+\'.bn.running_mean\'] = torch.from_numpy(h5f[\'mean\'][()])\n    state_dict[name_pth+\'.bn.running_var\'] = torch.from_numpy(h5f[\'var\'][()])\n    h5f.close()\n\ndef load_conv2d_nobn(state_dict, name_pth, name_tf):\n    h5f = h5py.File(\'dump/InceptionResnetV2/\'+name_tf+\'.h5\', \'r\')\n    state_dict[name_pth+\'.weight\'] = torch.from_numpy(h5f[\'weights\'][()]).permute(3, 2, 0, 1)\n    state_dict[name_pth+\'.bias\'] = torch.from_numpy(h5f[\'biases\'][()])\n    h5f.close()\n\ndef load_linear(state_dict, name_pth, name_tf):\n    h5f = h5py.File(\'dump/InceptionResnetV2/\'+name_tf+\'.h5\', \'r\')\n    state_dict[name_pth+\'.weight\'] = torch.from_numpy(h5f[\'weights\'][()]).t()\n    state_dict[name_pth+\'.bias\'] = torch.from_numpy(h5f[\'biases\'][()])\n    h5f.close()\n\ndef load_mixed_5b(state_dict, name_pth, name_tf):\n    load_conv2d(state_dict, name_pth+\'.branch0\', name_tf+\'/Branch_0/Conv2d_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.0\', name_tf+\'/Branch_1/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.1\', name_tf+\'/Branch_1/Conv2d_0b_5x5\')\n    load_conv2d(state_dict, name_pth+\'.branch2.0\', name_tf+\'/Branch_2/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch2.1\', name_tf+\'/Branch_2/Conv2d_0b_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch2.2\', name_tf+\'/Branch_2/Conv2d_0c_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch3.1\', name_tf+\'/Branch_3/Conv2d_0b_1x1\')\n\ndef load_block35(state_dict, name_pth, name_tf):\n    load_conv2d(state_dict, name_pth+\'.branch0\', name_tf+\'/Branch_0/Conv2d_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.0\', name_tf+\'/Branch_1/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.1\', name_tf+\'/Branch_1/Conv2d_0b_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch2.0\', name_tf+\'/Branch_2/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch2.1\', name_tf+\'/Branch_2/Conv2d_0b_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch2.2\', name_tf+\'/Branch_2/Conv2d_0c_3x3\')\n    load_conv2d_nobn(state_dict, name_pth+\'.conv2d\', name_tf+\'/Conv2d_1x1\')\n\ndef load_mixed_6a(state_dict, name_pth, name_tf):\n    load_conv2d(state_dict, name_pth+\'.branch0\', name_tf+\'/Branch_0/Conv2d_1a_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch1.0\', name_tf+\'/Branch_1/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.1\', name_tf+\'/Branch_1/Conv2d_0b_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch1.2\', name_tf+\'/Branch_1/Conv2d_1a_3x3\')\n\ndef load_block17(state_dict, name_pth, name_tf):\n    load_conv2d(state_dict, name_pth+\'.branch0\', name_tf+\'/Branch_0/Conv2d_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.0\', name_tf+\'/Branch_1/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.1\', name_tf+\'/Branch_1/Conv2d_0b_1x7\')\n    load_conv2d(state_dict, name_pth+\'.branch1.2\', name_tf+\'/Branch_1/Conv2d_0c_7x1\')\n    load_conv2d_nobn(state_dict, name_pth+\'.conv2d\', name_tf+\'/Conv2d_1x1\')\n\ndef load_mixed_7a(state_dict, name_pth, name_tf):\n    load_conv2d(state_dict, name_pth+\'.branch0.0\', name_tf+\'/Branch_0/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch0.1\', name_tf+\'/Branch_0/Conv2d_1a_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch1.0\', name_tf+\'/Branch_1/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.1\', name_tf+\'/Branch_1/Conv2d_1a_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch2.0\', name_tf+\'/Branch_2/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch2.1\', name_tf+\'/Branch_2/Conv2d_0b_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch2.2\', name_tf+\'/Branch_2/Conv2d_1a_3x3\')\n\ndef load_block8(state_dict, name_pth, name_tf):\n    load_conv2d(state_dict, name_pth+\'.branch0\', name_tf+\'/Branch_0/Conv2d_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.0\', name_tf+\'/Branch_1/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.1\', name_tf+\'/Branch_1/Conv2d_0b_1x3\')\n    load_conv2d(state_dict, name_pth+\'.branch1.2\', name_tf+\'/Branch_1/Conv2d_0c_3x1\')\n    load_conv2d_nobn(state_dict, name_pth+\'.conv2d\', name_tf+\'/Conv2d_1x1\')\n\n\n\ndef load():\n    state_dict={}\n    \n    load_conv2d(state_dict, name_pth=\'conv2d_1a\', name_tf=\'Conv2d_1a_3x3\')\n    load_conv2d(state_dict, name_pth=\'conv2d_2a\', name_tf=\'Conv2d_2a_3x3\')\n    load_conv2d(state_dict, name_pth=\'conv2d_2b\', name_tf=\'Conv2d_2b_3x3\')\n    \n    load_conv2d(state_dict, name_pth=\'conv2d_3b\', name_tf=\'Conv2d_3b_1x1\')\n    load_conv2d(state_dict, name_pth=\'conv2d_4a\', name_tf=\'Conv2d_4a_3x3\')\n\n    load_mixed_5b(state_dict, name_pth=\'mixed_5b\', name_tf=\'Mixed_5b\')\n\n    for i in range(10):\n        load_block35(state_dict, name_pth=\'repeat.\'+str(i), name_tf=\'Repeat/block35_\'+str(i+1))\n\n    load_mixed_6a(state_dict, name_pth=\'mixed_6a\', name_tf=\'Mixed_6a\')\n\n    for i in range(20):\n        load_block17(state_dict, name_pth=\'repeat_1.\'+str(i), name_tf=\'Repeat_1/block17_\'+str(i+1))\n\n    load_mixed_7a(state_dict, name_pth=\'mixed_7a\', name_tf=\'Mixed_7a\')\n\n    for i in range(9):\n        load_block8(state_dict, name_pth=\'repeat_2.\'+str(i), name_tf=\'Repeat_2/block8_\'+str(i+1))\n\n    load_block8(state_dict, name_pth=\'block8\', name_tf=\'Block8\')\n    load_conv2d(state_dict, name_pth=\'conv2d_7b\', name_tf=\'Conv2d_7b_1x1\')\n    load_linear(state_dict, name_pth=\'classif\', name_tf=\'Logits\')\n\n    return state_dict\n\n######################################################################\n## Test\n######################################################################\n\ndef test(model):\n    from scipy import misc\n    img = misc.imread(\'lena_299.png\')\n    inputs = torch.ones(1,299,299,3)\n    #inputs[0] = torch.from_numpy(img)\n\n    inputs[0,0,0,0] = -1\n    inputs.transpose_(1,3)\n    inputs.transpose_(2,3)\n\n    print(inputs.mean())\n    print(inputs.std())\n\n    #inputs.sub_(0.5).div_(0.5)\n    #inputs.sub_(inputs)\n    # 1, 3, 299, 299\n\n    outputs = model.forward(torch.autograd.Variable(inputs))\n    h5f = h5py.File(\'dump/InceptionResnetV2/Logits.h5\', \'r\')\n    outputs_tf = torch.from_numpy(h5f[\'out\'][()])\n    h5f.close()\n    outputs = torch.nn.functional.softmax(outputs)\n    print(outputs.sum())\n    print(outputs[0])\n    print(outputs_tf.sum())\n    print(outputs_tf[0])\n    print(torch.dist(outputs.data, outputs_tf))\n    return outputs\n \ndef test_conv2d(module, name):\n    #global output_tf\n    h5f = h5py.File(\'dump/InceptionResnetV2/\'+name+\'.h5\', \'r\')\n    output_tf_conv = torch.from_numpy(h5f[\'conv_out\'][()])\n    output_tf_conv.transpose_(1,3)\n    output_tf_conv.transpose_(2,3)\n    output_tf_relu = torch.from_numpy(h5f[\'relu_out\'][()])\n    output_tf_relu.transpose_(1,3)\n    output_tf_relu.transpose_(2,3)\n    h5f.close()\n    def test_dist_conv(self, input, output):\n        print(name, \'conv\', torch.dist(output.data, output_tf_conv))\n    module.conv.register_forward_hook(test_dist_conv)\n    def test_dist_relu(self, input, output):\n        print(name, \'relu\', torch.dist(output.data, output_tf_relu))\n    module.relu.register_forward_hook(test_dist_relu)\n\ndef test_conv2d_nobn(module, name):\n    #global output_tf\n    h5f = h5py.File(\'dump/InceptionResnetV2/\'+name+\'.h5\', \'r\')\n    output_tf = torch.from_numpy(h5f[\'conv_out\'][()])\n    output_tf.transpose_(1,3)\n    output_tf.transpose_(2,3)\n    h5f.close()\n    def test_dist(self, input, output):\n        print(name, \'conv+bias\', torch.dist(output.data, output_tf))\n    module.register_forward_hook(test_dist)\n\ndef test_mixed_5b(module, name):\n    test_conv2d(module.branch0, name+\'/Branch_0/Conv2d_1x1\')\n    test_conv2d(module.branch1[0], name+\'/Branch_1/Conv2d_0a_1x1\')\n    test_conv2d(module.branch1[1], name+\'/Branch_1/Conv2d_0b_5x5\')\n    test_conv2d(module.branch2[0], name+\'/Branch_2/Conv2d_0a_1x1\')\n    test_conv2d(module.branch2[1], name+\'/Branch_2/Conv2d_0b_3x3\')\n    test_conv2d(module.branch2[2], name+\'/Branch_2/Conv2d_0c_3x3\')\n    test_conv2d(module.branch3[1], name+\'/Branch_3/Conv2d_0b_1x1\')\n\ndef test_block35(module, name):\n    test_conv2d(module.branch0, name+\'/Branch_0/Conv2d_1x1\')\n    test_conv2d(module.branch1[0], name+\'/Branch_1/Conv2d_0a_1x1\')\n    test_conv2d(module.branch1[1], name+\'/Branch_1/Conv2d_0b_3x3\')\n    test_conv2d(module.branch2[0], name+\'/Branch_2/Conv2d_0a_1x1\')\n    test_conv2d(module.branch2[1], name+\'/Branch_2/Conv2d_0b_3x3\')\n    test_conv2d(module.branch2[2], name+\'/Branch_2/Conv2d_0c_3x3\')\n    test_conv2d_nobn(module.conv2d, name+\'/Conv2d_1x1\')\n\ndef test_mixed_6a(module, name):\n    test_conv2d(module.branch0, name+\'/Branch_0/Conv2d_1a_3x3\')\n    test_conv2d(module.branch1[0], name+\'/Branch_1/Conv2d_0a_1x1\')\n    test_conv2d(module.branch1[1], name+\'/Branch_1/Conv2d_0b_3x3\')\n    test_conv2d(module.branch1[2], name+\'/Branch_1/Conv2d_1a_3x3\')\n\ndef test_block17(module, name):\n    test_conv2d(module.branch0, name+\'/Branch_0/Conv2d_1x1\')\n    test_conv2d(module.branch1[0], name+\'/Branch_1/Conv2d_0a_1x1\')\n    test_conv2d(module.branch1[1], name+\'/Branch_1/Conv2d_0b_1x7\')\n    test_conv2d(module.branch1[2], name+\'/Branch_1/Conv2d_0c_7x1\')\n    test_conv2d_nobn(module.conv2d, name+\'/Conv2d_1x1\')\n\ndef test_mixed_7a(module, name):\n    test_conv2d(module.branch0[0], name+\'/Branch_0/Conv2d_0a_1x1\')\n    test_conv2d(module.branch0[1], name+\'/Branch_0/Conv2d_1a_3x3\')\n    test_conv2d(module.branch1[0], name+\'/Branch_1/Conv2d_0a_1x1\')\n    test_conv2d(module.branch1[1], name+\'/Branch_1/Conv2d_1a_3x3\')\n    test_conv2d(module.branch2[0], name+\'/Branch_2/Conv2d_0a_1x1\')\n    test_conv2d(module.branch2[1], name+\'/Branch_2/Conv2d_0b_3x3\')\n    test_conv2d(module.branch2[2], name+\'/Branch_2/Conv2d_1a_3x3\')\n\ndef test_block8(module, name):\n    test_conv2d(module.branch0, name+\'/Branch_0/Conv2d_1x1\')\n    test_conv2d(module.branch1[0], name+\'/Branch_1/Conv2d_0a_1x1\')\n    test_conv2d(module.branch1[1], name+\'/Branch_1/Conv2d_0b_1x3\')\n    test_conv2d(module.branch1[2], name+\'/Branch_1/Conv2d_0c_3x1\')\n    test_conv2d_nobn(module.conv2d, name+\'/Conv2d_1x1\')\n\n######################################################################\n## Main\n######################################################################\n\nif __name__ == ""__main__"":\n\n    import h5py\n\n    model = InceptionResnetV2()\n    state_dict = load()\n    model.load_state_dict(state_dict)\n    model.eval()\n\n    os.system(\'mkdir -p save\')\n    torch.save(model, \'save/inceptionresnetv2.pth\')\n    torch.save(state_dict, \'save/inceptionresnetv2_state.pth\')\n\n    test_conv2d(model.conv2d_1a, \'Conv2d_1a_3x3\')\n    test_conv2d(model.conv2d_2a, \'Conv2d_2a_3x3\')\n    test_conv2d(model.conv2d_2b, \'Conv2d_2b_3x3\')\n    test_conv2d(model.conv2d_3b, \'Conv2d_3b_1x1\')\n    test_conv2d(model.conv2d_4a, \'Conv2d_4a_3x3\')\n\n    test_mixed_5b(model.mixed_5b, \'Mixed_5b\')\n\n    for i in range(len(model.repeat._modules)):\n        test_block35(model.repeat[i], \'Repeat/block35_\'+str(i+1))\n\n    test_mixed_6a(model.mixed_6a, \'Mixed_6a\')\n\n    for i in range(len(model.repeat_1._modules)):\n        test_block17(model.repeat_1[i], \'Repeat_1/block17_\'+str(i+1))\n\n    test_mixed_7a(model.mixed_7a, \'Mixed_7a\')\n\n    for i in range(len(model.repeat_2._modules)):\n        test_block8(model.repeat_2[i], \'Repeat_2/block8_\'+str(i+1))\n\n    test_block8(model.block8, \'Block8\')\n\n    test_conv2d(model.conv2d_7b, \'Conv2d_7b_1x1\')\n\n    outputs = test(model)\n    # test_conv2d(model.features[1], \'Conv2d_2a_3x3\')\n    # test_conv2d(model.features[2], \'Conv2d_2b_3x3\')\n    # test_conv2d(model.features[3].conv, \'Mixed_3a/Branch_1/Conv2d_0a_3x3\')\n    #test_mixed_4a_7a(model.features[4], \'Mixed_4a\')\n\n'"
model_zoo/inceptionresnetv2/tensorflow_dump.py,0,"b'# python3\n\n# TensorBoard\n# python ~/.local/lib/python3.5/site-packages/tensorflow/tensorboard/tensorboard.py --logdir=logs --port=6007\n\n#\xc2\xa0python /home/cadene/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/tensorboard/tensorboard.py  --logdir=logs --port=6007\n\n\nimport os\nimport sys\nimport h5py\nimport math\nimport urllib.request\nimport numpy as np\nimport tensorflow as tf\n\nsys.path.append(\'models/slim\')\nfrom datasets import dataset_utils\nfrom datasets import imagenet\nfrom nets import inception\nfrom preprocessing import inception_preprocessing\n\nslim = tf.contrib.slim\n\nimage_size = inception.inception_v3.default_image_size\n\nurl = \'http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz\'\ncheckpoints_dir = \'/tmp/checkpoints/\'\n\ndef make_padding(padding_name, conv_shape):\n  padding_name = padding_name.decode(""utf-8"")\n  if padding_name == ""VALID"":\n    return [0, 0]\n  elif padding_name == ""SAME"":\n    #return [math.ceil(int(conv_shape[0])/2), math.ceil(int(conv_shape[1])/2)]\n    return [math.floor(int(conv_shape[0])/2), math.floor(int(conv_shape[1])/2)]\n  else:\n    sys.exit(\'Invalid padding name \'+padding_name)\n\ndef dump_conv2d(name=\'Conv2d_1a_3x3\'):\n  conv_operation = sess.graph.get_operation_by_name(\'InceptionResnetV2/\'+name+\'/Conv2D\') # remplacer convolution par Conv2D si erreur\n  weights_tensor = sess.graph.get_tensor_by_name(\'InceptionResnetV2/\'+name+\'/weights:0\')\n  weights = weights_tensor.eval()\n  padding = make_padding(conv_operation.get_attr(\'padding\'), weights_tensor.get_shape())\n  strides = conv_operation.get_attr(\'strides\')\n  conv_out = sess.graph.get_operation_by_name(\'InceptionResnetV2/\'+name+\'/Conv2D\').outputs[0].eval() # remplacer convolution par Conv2D si erreur\n\n  beta = sess.graph.get_tensor_by_name(\'InceptionResnetV2/\'+name+\'/BatchNorm/beta:0\').eval()\n  #gamma = sess.graph.get_tensor_by_name(\'InceptionResnetV2/\'+name+\'/BatchNorm/gamma:0\').eval()\n  mean = sess.graph.get_tensor_by_name(\'InceptionResnetV2/\'+name+\'/BatchNorm/moving_mean:0\').eval()\n  var = sess.graph.get_tensor_by_name(\'InceptionResnetV2/\'+name+\'/BatchNorm/moving_variance:0\').eval()\n  \n  relu_out = sess.graph.get_operation_by_name(\'InceptionResnetV2/\'+name+\'/Relu\').outputs[0].eval()\n\n  os.system(\'mkdir -p dump/InceptionResnetV2/\'+name)\n  h5f = h5py.File(\'dump/InceptionResnetV2/\'+name+\'.h5\', \'w\')\n  # conv\n  h5f.create_dataset(""weights"", data=weights)\n  h5f.create_dataset(""strides"", data=strides)\n  h5f.create_dataset(""padding"", data=padding)\n  h5f.create_dataset(""conv_out"", data=conv_out)\n  # batch norm\n  h5f.create_dataset(""beta"", data=beta)\n  #h5f.create_dataset(""gamma"", data=gamma)\n  h5f.create_dataset(""mean"", data=mean)\n  h5f.create_dataset(""var"", data=var)\n  h5f.create_dataset(""relu_out"", data=relu_out)\n  h5f.close()\n\ndef dump_conv2d_nobn(name=\'Conv2d_1x1\'):\n  conv_operation = sess.graph.get_operation_by_name(\'InceptionResnetV2/\'+name+\'/Conv2D\') # remplacer convolution par Conv2D si erreur\n  weights_tensor = sess.graph.get_tensor_by_name(\'InceptionResnetV2/\'+name+\'/weights:0\')\n  weights = weights_tensor.eval()\n  biases_tensor = sess.graph.get_tensor_by_name(\'InceptionResnetV2/\'+name+\'/biases:0\')\n  biases = biases_tensor.eval()\n  padding = make_padding(conv_operation.get_attr(\'padding\'), weights_tensor.get_shape())\n  strides = conv_operation.get_attr(\'strides\')\n  conv_out = sess.graph.get_operation_by_name(\'InceptionResnetV2/\'+name+\'/BiasAdd\').outputs[0].eval() # remplacer convolution par Conv2D si erreur\n\n  os.system(\'mkdir -p dump/InceptionResnetV2/\'+name)\n  h5f = h5py.File(\'dump/InceptionResnetV2/\'+name+\'.h5\', \'w\')\n  # conv\n  h5f.create_dataset(""weights"", data=weights)\n  h5f.create_dataset(""biases"", data=biases)\n  h5f.create_dataset(""strides"", data=strides)\n  h5f.create_dataset(""padding"", data=padding)\n  h5f.create_dataset(""conv_out"", data=conv_out)\n  h5f.close()\n\ndef dump_logits():\n  operation = sess.graph.get_operation_by_name(\'InceptionResnetV2/Logits/Predictions\')\n\n  weights_tensor = sess.graph.get_tensor_by_name(\'InceptionResnetV2/Logits/Logits/weights:0\')\n  weights = weights_tensor.eval()\n\n  biases_tensor = sess.graph.get_tensor_by_name(\'InceptionResnetV2/Logits/Logits/biases:0\')\n  biases = biases_tensor.eval()\n  \n  out = operation.outputs[0].eval()\n  print(out)\n\n  h5f = h5py.File(\'dump/InceptionResnetV2/Logits.h5\', \'w\')\n  h5f.create_dataset(""weights"", data=weights)\n  h5f.create_dataset(""biases"", data=biases)\n  h5f.create_dataset(""out"", data=out)\n  h5f.close()\n\n\n# def dump_avgpool(name=\'Mixed_5b/Branch_3/AvgPool_0a_3x3\'):\n#   operation = sess.graph.get_operation_by_name(\'InceptionResnetV2/InceptionResnetV2/\'+name+\'/AvgPool\')\n#   out = operation.outputs[0].eval()\n#   os.system(\'mkdir -p dump/InceptionResnetV2/\'+name)\n#   h5f = h5py.File(\'dump/InceptionResnetV2/\'+name+\'.h5\', \'w\')\n#   h5f.create_dataset(""out"", data=out)\n#   h5f.close()\n\n# def dump_concats():\n#   operation1 = sess.graph.get_operation_by_name(\'InceptionResnetV2/InceptionResnetV2/Mixed_7b/Branch_1/concat\')\n#   operation2 = sess.graph.get_operation_by_name(\'InceptionResnetV2/InceptionResnetV2/Mixed_7b/Branch_2/concat\')\n#   out1 = operation1.outputs[0].eval()\n#   out2 = operation2.outputs[0].eval()\n#   os.system(\'mkdir -p dump/InceptionResnetV2/Mixed_7b/Branch_1/concat\')\n#   h5f = h5py.File(\'dump/InceptionResnetV2/Mixed_7b/Branch_1/concat.h5\', \'w\')\n#   h5f.create_dataset(""out"", data=out1)\n#   h5f.close()\n#   os.system(\'mkdir -p dump/InceptionResnetV2/Mixed_7b/Branch_2/concat\')\n#   h5f = h5py.File(\'dump/InceptionResnetV2/Mixed_7b/Branch_2/concat.h5\', \'w\')\n#   h5f.create_dataset(""out"", data=out2)\n#   h5f.close()\n\n\ndef dump_mixed_5b(name=\'Mixed_5b\'):\n  dump_conv2d(name=name+\'/Branch_0/Conv2d_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0b_5x5\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0b_3x3\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0c_3x3\')\n  dump_conv2d(name=name+\'/Branch_3/Conv2d_0b_1x1\')\n\ndef dump_block35(name=\'Repeat/block35_1\'):\n  dump_conv2d(name=name+\'/Branch_0/Conv2d_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0b_3x3\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0b_3x3\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0c_3x3\')\n  dump_conv2d_nobn(name=name+\'/Conv2d_1x1\')\n\ndef dump_mixed_6a(name=\'Mixed_6a\'):\n  dump_conv2d(name=name+\'/Branch_0/Conv2d_1a_3x3\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0b_3x3\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_1a_3x3\')\n\ndef dump_block17(name=\'Repeat_1/block17_1\'):\n  dump_conv2d(name=name+\'/Branch_0/Conv2d_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0b_1x7\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0c_7x1\')\n  dump_conv2d_nobn(name=name+\'/Conv2d_1x1\')\n\ndef dump_mixed_7a(name=\'Mixed_7a\'):\n  dump_conv2d(name=name+\'/Branch_0/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_0/Conv2d_1a_3x3\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_1a_3x3\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0b_3x3\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_1a_3x3\')\n\ndef dump_block8(name=\'Repeat_2/block8_1\'):\n  dump_conv2d(name=name+\'/Branch_0/Conv2d_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0b_1x3\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0c_3x1\')\n  dump_conv2d_nobn(name=name+\'/Conv2d_1x1\')\n\n\nif not tf.gfile.Exists(checkpoints_dir+\'inception_resnet_v2_2016_08_30.ckpt\'):\n  tf.gfile.MakeDirs(checkpoints_dir)\n  dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)\n\nwith tf.Graph().as_default():\n\n  # Create model architecture\n\n  from scipy import misc\n  img = misc.imread(\'lena_299.png\')\n  print(img.shape)\n\n  inputs = np.ones((1,299,299,3), dtype=np.float32)\n  inputs[0,0,0,0] = -1\n  #inputs[0] = img\n  print(inputs.mean())\n  print(inputs.std())\n  inputs = tf.pack(inputs)\n  #\xc2\xa0tensorflow normalization\n  #\xc2\xa0https://github.com/tensorflow/models/blob/master/slim/preprocessing/inception_preprocessing.py#L273\n  #inputs = tf.sub(inputs, 0.5) \n  #inputs = tf.mul(inputs, 2.0)\n\n\n  with slim.arg_scope(inception.inception_resnet_v2_arg_scope()):\n    logits, _ = inception.inception_resnet_v2(inputs, num_classes=1001, is_training=False)\n\n  with tf.Session() as sess:\n\n    # Initialize model\n    init_fn = slim.assign_from_checkpoint_fn(\n    os.path.join(checkpoints_dir, \'inception_resnet_v2_2016_08_30.ckpt\'),\n    slim.get_model_variables(\'InceptionResnetV2\'))  \n\n    init_fn(sess)\n\n    # Display model variables\n    for v in slim.get_model_variables():\n      print(\'name = {}, shape = {}\'.format(v.name, v.get_shape()))\n\n    # Create graph\n    os.system(""rm -rf logs"")\n    os.system(""mkdir -p logs"")\n\n    tf.scalar_summary(\'logs\', logits[0][0])\n    summary_op = tf.merge_all_summaries()\n    summary_writer = tf.train.SummaryWriter(""logs"", sess.graph)\n\n    out = sess.run(summary_op)\n    summary_writer.add_summary(out, 0)\n\n    ###############################\n    # Dump parameters and outputs\n\n    dump_conv2d(name=\'Conv2d_1a_3x3\')\n    dump_conv2d(name=\'Conv2d_2a_3x3\')\n    dump_conv2d(name=\'Conv2d_2b_3x3\')\n    # MaxPooling\n\n    dump_conv2d(name=\'Conv2d_3b_1x1\')\n    dump_conv2d(name=\'Conv2d_4a_3x3\')\n    # MaxPooling\n\n    dump_mixed_5b()\n    for i in range(1,11):\n      dump_block35(name=\'Repeat/block35_\'+str(i))\n\n    dump_mixed_6a()\n    for i in range(1,21):\n      dump_block17(name=\'Repeat_1/block17_\'+str(i))\n\n    dump_mixed_7a()\n    for i in range(1,10):\n      dump_block8(name=\'Repeat_2/block8_\'+str(i))\n    \n    dump_block8(name=\'Block8\')\n    dump_conv2d(name=\'Conv2d_7b_1x1\')\n    # AvgPooling\n    \n    dump_logits()'"
model_zoo/inceptionv4/__init__.py,0,b''
model_zoo/inceptionv4/pytorch_load.py,29,"b'import torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport os\nimport sys\n\nmodel_urls = {\n    \'imagenet\': \'http://webia.lip6.fr/~cadene/Downloads/inceptionv4-97ef9c30.pth\'\n}\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False) # verify bias false\n        self.bn = nn.BatchNorm2d(out_planes, eps=0.001, momentum=0, affine=True)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\nclass Mixed_3a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_3a, self).__init__()\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n        self.conv = BasicConv2d(64, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        x0 = self.maxpool(x)\n        x1 = self.conv(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\nclass Mixed_4a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_4a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 64, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(64, 64, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(64, 96, kernel_size=(3,3), stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\nclass Mixed_5a(nn.Module):\n\n    def __init__(self):\n        super(Mixed_5a, self).__init__()\n        self.conv = BasicConv2d(192, 192, kernel_size=3, stride=2)\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.conv(x)\n        x1 = self.maxpool(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\nclass Inception_A(nn.Module):\n\n    def __init__(self):\n        super(Inception_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(384, 96, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\nclass Reduction_A(nn.Module):\n\n    def __init__(self):\n        super(Reduction_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(224, 256, kernel_size=3, stride=2)\n        )\n        \n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\nclass Inception_B(nn.Module):\n\n    def __init__(self):\n        super(Inception_B, self).__init__()\n        self.branch0 = BasicConv2d(1024, 384, kernel_size=1, stride=1)\n        \n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(224, 256, kernel_size=(7,1), stride=1, padding=(3,0))\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(192, 224, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(224, 224, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(224, 256, kernel_size=(1,7), stride=1, padding=(0,3))\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1024, 128, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\nclass Reduction_B(nn.Module):\n\n    def __init__(self):\n        super(Reduction_B, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=(1,7), stride=1, padding=(0,3)),\n            BasicConv2d(256, 320, kernel_size=(7,1), stride=1, padding=(3,0)),\n            BasicConv2d(320, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\nclass Inception_C(nn.Module):\n\n    def __init__(self):\n        super(Inception_C, self).__init__()\n\n        self.branch0 = BasicConv2d(1536, 256, kernel_size=1, stride=1)\n        \n        self.branch1_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch1_1a = BasicConv2d(384, 256, kernel_size=(1,3), stride=1, padding=(0,1))\n        self.branch1_1b = BasicConv2d(384, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n        \n        self.branch2_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch2_1 = BasicConv2d(384, 448, kernel_size=(3,1), stride=1, padding=(1,0))\n        self.branch2_2 = BasicConv2d(448, 512, kernel_size=(1,3), stride=1, padding=(0,1))\n        self.branch2_3a = BasicConv2d(512, 256, kernel_size=(1,3), stride=1, padding=(0,1))\n        self.branch2_3b = BasicConv2d(512, 256, kernel_size=(3,1), stride=1, padding=(1,0))\n        \n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1536, 256, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        \n        x1_0 = self.branch1_0(x)\n        x1_1a = self.branch1_1a(x1_0)\n        x1_1b = self.branch1_1b(x1_0)\n        x1 = torch.cat((x1_1a, x1_1b), 1)\n\n        x2_0 = self.branch2_0(x)\n        x2_1 = self.branch2_1(x2_0)\n        x2_2 = self.branch2_2(x2_1)\n        x2_3a = self.branch2_3a(x2_2)\n        x2_3b = self.branch2_3b(x2_2)\n        x2 = torch.cat((x2_3a, x2_3b), 1)\n\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\nclass InceptionV4(nn.Module):\n\n    def __init__(self, num_classes=1001):\n        super(InceptionV4, self).__init__()\n        self.features = nn.Sequential(\n            BasicConv2d(3, 32, kernel_size=3, stride=2),\n            BasicConv2d(32, 32, kernel_size=3, stride=1),\n            BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            Mixed_3a(),\n            Mixed_4a(),\n            Mixed_5a(),\n            Inception_A(),\n            Inception_A(),\n            Inception_A(),\n            Inception_A(),\n            Reduction_A(), # Mixed_6a\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Reduction_B(), # Mixed_7a\n            Inception_C(),\n            Inception_C(),\n            Inception_C(),\n            nn.AvgPool2d(8, count_include_pad=False)\n        )\n        self.classif = nn.Linear(1536, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classif(x) \n        return x\n\ndef inceptionv4(pretrained=True):\n    model = InceptionV4()\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'imagenet\']))\n    return model\n\n######################################################################\n## Load parameters from HDF5 to Dict\n######################################################################\n\ndef load_conv2d(state_dict, name_pth, name_tf):\n    h5f = h5py.File(\'dump/InceptionV4/\'+name_tf+\'.h5\', \'r\')\n    state_dict[name_pth+\'.conv.weight\'] = torch.from_numpy(h5f[\'weights\'][()]).permute(3, 2, 0, 1)\n    out_planes = state_dict[name_pth+\'.conv.weight\'].size(0)\n    state_dict[name_pth+\'.bn.weight\'] = torch.ones(out_planes)\n    state_dict[name_pth+\'.bn.bias\'] = torch.from_numpy(h5f[\'beta\'][()])\n    state_dict[name_pth+\'.bn.running_mean\'] = torch.from_numpy(h5f[\'mean\'][()])\n    state_dict[name_pth+\'.bn.running_var\'] = torch.from_numpy(h5f[\'var\'][()])\n    h5f.close()\n\ndef load_linear(state_dict, name_pth, name_tf):\n    h5f = h5py.File(\'dump/InceptionV4/\'+name_tf+\'.h5\', \'r\')\n    state_dict[name_pth+\'.weight\'] = torch.from_numpy(h5f[\'weights\'][()]).t()\n    state_dict[name_pth+\'.bias\'] = torch.from_numpy(h5f[\'biases\'][()])\n    h5f.close()\n\ndef load_mixed_4a_7a(state_dict, name_pth, name_tf):\n    load_conv2d(state_dict, name_pth+\'.branch0.0\', name_tf+\'/Branch_0/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch0.1\', name_tf+\'/Branch_0/Conv2d_1a_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch1.0\', name_tf+\'/Branch_1/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.1\', name_tf+\'/Branch_1/Conv2d_0b_1x7\')\n    load_conv2d(state_dict, name_pth+\'.branch1.2\', name_tf+\'/Branch_1/Conv2d_0c_7x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.3\', name_tf+\'/Branch_1/Conv2d_1a_3x3\')\n\ndef load_mixed_5(state_dict, name_pth, name_tf):\n    load_conv2d(state_dict, name_pth+\'.branch0\', name_tf+\'/Branch_0/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.0\', name_tf+\'/Branch_1/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.1\', name_tf+\'/Branch_1/Conv2d_0b_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch2.0\', name_tf+\'/Branch_2/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch2.1\', name_tf+\'/Branch_2/Conv2d_0b_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch2.2\', name_tf+\'/Branch_2/Conv2d_0c_3x3\')\n    load_conv2d(state_dict, name_pth+\'.branch3.1\', name_tf+\'/Branch_3/Conv2d_0b_1x1\')\n\ndef load_mixed_6(state_dict, name_pth, name_tf):\n    load_conv2d(state_dict, name_pth+\'.branch0\', name_tf+\'/Branch_0/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.0\', name_tf+\'/Branch_1/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1.1\', name_tf+\'/Branch_1/Conv2d_0b_1x7\')\n    load_conv2d(state_dict, name_pth+\'.branch1.2\', name_tf+\'/Branch_1/Conv2d_0c_7x1\')\n    load_conv2d(state_dict, name_pth+\'.branch2.0\', name_tf+\'/Branch_2/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch2.1\', name_tf+\'/Branch_2/Conv2d_0b_7x1\')\n    load_conv2d(state_dict, name_pth+\'.branch2.2\', name_tf+\'/Branch_2/Conv2d_0c_1x7\')\n    load_conv2d(state_dict, name_pth+\'.branch2.3\', name_tf+\'/Branch_2/Conv2d_0d_7x1\')\n    load_conv2d(state_dict, name_pth+\'.branch2.4\', name_tf+\'/Branch_2/Conv2d_0e_1x7\')\n    load_conv2d(state_dict, name_pth+\'.branch3.1\', name_tf+\'/Branch_3/Conv2d_0b_1x1\')\n\ndef load_mixed_7(state_dict, name_pth, name_tf):\n    load_conv2d(state_dict, name_pth+\'.branch0\', name_tf+\'/Branch_0/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1_0\', name_tf+\'/Branch_1/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch1_1a\', name_tf+\'/Branch_1/Conv2d_0b_1x3\')\n    load_conv2d(state_dict, name_pth+\'.branch1_1b\', name_tf+\'/Branch_1/Conv2d_0c_3x1\')\n    load_conv2d(state_dict, name_pth+\'.branch2_0\', name_tf+\'/Branch_2/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth+\'.branch2_1\', name_tf+\'/Branch_2/Conv2d_0b_3x1\')\n    load_conv2d(state_dict, name_pth+\'.branch2_2\', name_tf+\'/Branch_2/Conv2d_0c_1x3\')\n    load_conv2d(state_dict, name_pth+\'.branch2_3a\', name_tf+\'/Branch_2/Conv2d_0d_1x3\')\n    load_conv2d(state_dict, name_pth+\'.branch2_3b\', name_tf+\'/Branch_2/Conv2d_0e_3x1\')\n    load_conv2d(state_dict, name_pth+\'.branch3.1\', name_tf+\'/Branch_3/Conv2d_0b_1x1\')\n\n\ndef load():\n    state_dict={}\n    \n    load_conv2d(state_dict, name_pth=\'features.0\', name_tf=\'Conv2d_1a_3x3\')\n    load_conv2d(state_dict, name_pth=\'features.1\', name_tf=\'Conv2d_2a_3x3\')\n    load_conv2d(state_dict, name_pth=\'features.2\', name_tf=\'Conv2d_2b_3x3\')\n    \n    load_conv2d(state_dict, name_pth=\'features.3.conv\', name_tf=\'Mixed_3a/Branch_1/Conv2d_0a_3x3\')\n\n    load_mixed_4a_7a(state_dict, name_pth=\'features.4\', name_tf=\'Mixed_4a\')\n\n    load_conv2d(state_dict, name_pth=\'features.5.conv\', name_tf=\'Mixed_5a/Branch_0/Conv2d_1a_3x3\')\n\n    load_mixed_5(state_dict, name_pth=\'features.6\', name_tf=\'Mixed_5b\')\n    load_mixed_5(state_dict, name_pth=\'features.7\', name_tf=\'Mixed_5c\')\n    load_mixed_5(state_dict, name_pth=\'features.8\', name_tf=\'Mixed_5d\')\n    load_mixed_5(state_dict, name_pth=\'features.9\', name_tf=\'Mixed_5e\')\n\n    load_conv2d(state_dict, name_pth=\'features.10.branch0\', name_tf=\'Mixed_6a/Branch_0/Conv2d_1a_3x3\')\n    load_conv2d(state_dict, name_pth=\'features.10.branch1.0\', name_tf=\'Mixed_6a/Branch_1/Conv2d_0a_1x1\')\n    load_conv2d(state_dict, name_pth=\'features.10.branch1.1\', name_tf=\'Mixed_6a/Branch_1/Conv2d_0b_3x3\')\n    load_conv2d(state_dict, name_pth=\'features.10.branch1.2\', name_tf=\'Mixed_6a/Branch_1/Conv2d_1a_3x3\')\n\n    load_mixed_6(state_dict, name_pth=\'features.11\', name_tf=\'Mixed_6b\')\n    load_mixed_6(state_dict, name_pth=\'features.12\', name_tf=\'Mixed_6c\')\n    load_mixed_6(state_dict, name_pth=\'features.13\', name_tf=\'Mixed_6d\')\n    load_mixed_6(state_dict, name_pth=\'features.14\', name_tf=\'Mixed_6e\')\n    load_mixed_6(state_dict, name_pth=\'features.15\', name_tf=\'Mixed_6f\')\n    load_mixed_6(state_dict, name_pth=\'features.16\', name_tf=\'Mixed_6g\')\n    load_mixed_6(state_dict, name_pth=\'features.17\', name_tf=\'Mixed_6h\')\n\n    load_mixed_4a_7a(state_dict, name_pth=\'features.18\', name_tf=\'Mixed_7a\')\n\n    load_mixed_7(state_dict, name_pth=\'features.19\', name_tf=\'Mixed_7b\')\n    load_mixed_7(state_dict, name_pth=\'features.20\', name_tf=\'Mixed_7c\')\n    load_mixed_7(state_dict, name_pth=\'features.21\', name_tf=\'Mixed_7d\')\n\n    load_linear(state_dict, name_pth=\'classif\', name_tf=\'Logits\')\n\n    return state_dict\n\n######################################################################\n## Test\n######################################################################\n\ndef test(model):\n    model.eval()\n    from scipy import misc\n    img = misc.imread(\'lena_299.png\')\n    inputs = torch.zeros(1,299,299,3)\n    inputs[0] = torch.from_numpy(img)\n    inputs.transpose_(1,3)\n    inputs.transpose_(2,3)\n    # 1, 3, 299, 299\n    outputs = model.forward(torch.autograd.Variable(inputs))\n    h5f = h5py.File(\'dump/InceptionV4/Logits.h5\', \'r\')\n    outputs_tf = torch.from_numpy(h5f[\'out\'][()])\n    h5f.close()\n    outputs = torch.nn.functional.softmax(outputs)\n    print(torch.dist(outputs.data, outputs_tf))\n    return outputs\n \ndef test_conv2d(module, name):\n    #global output_tf\n    h5f = h5py.File(\'dump/InceptionV4/\'+name+\'.h5\', \'r\')\n    output_tf = torch.from_numpy(h5f[\'relu_out\'][()])\n    output_tf.transpose_(1,3)\n    output_tf.transpose_(2,3)\n    h5f.close()\n    def test_dist(self, input, output):\n        print(name, torch.dist(output.data, output_tf))\n    module.register_forward_hook(test_dist)\n\ndef test_mixed_4a_7a(module, name):\n    test_conv2d(module.branch0[0], name+\'/Branch_0/Conv2d_0a_1x1\')\n    test_conv2d(module.branch0[1], name+\'/Branch_0/Conv2d_1a_3x3\')\n    test_conv2d(module.branch1[0], name+\'/Branch_1/Conv2d_0a_1x1\')\n    test_conv2d(module.branch1[1], name+\'/Branch_1/Conv2d_0b_1x7\')\n    test_conv2d(module.branch1[2], name+\'/Branch_1/Conv2d_0c_7x1\')\n    test_conv2d(module.branch1[3], name+\'/Branch_1/Conv2d_1a_3x3\')\n\n######################################################################\n## Main\n######################################################################\n\nif __name__ == ""__main__"":\n\n    import h5py\n\n    model = InceptionV4()\n    state_dict = load()\n    model.load_state_dict(state_dict)\n\n    # test_conv2d(model.features[0], \'Conv2d_1a_3x3\')\n    # test_conv2d(model.features[1], \'Conv2d_2a_3x3\')\n    # test_conv2d(model.features[2], \'Conv2d_2b_3x3\')\n    # test_conv2d(model.features[3].conv, \'Mixed_3a/Branch_1/Conv2d_0a_3x3\')\n    # test_mixed_4a_7a(model.features[4], \'Mixed_4a\')\n    \n    os.system(\'mkdir -p save\')\n    torch.save(model, \'save/inceptionv4.pth\')\n    torch.save(state_dict, \'save/inceptionv4_state.pth\')\n\n    outputs = test(model)\n\n\n'"
model_zoo/inceptionv4/tensorflow_dump.py,0,"b'# python3\n\n# TensorBoard\n# python3 ~/.local/lib/python3.5/site-packages/tensorflow/tensorboard/tensorboard.py --logdir=logs --port=6007\n\nimport os\nimport sys\nimport h5py\nimport math\nimport urllib.request\nimport numpy as np\nimport tensorflow as tf\n\nsys.path.append(\'models/slim\')\nfrom datasets import dataset_utils\nfrom datasets import imagenet\nfrom nets import inception\nfrom preprocessing import inception_preprocessing\n\nslim = tf.contrib.slim\n\nimage_size = inception.inception_v3.default_image_size\n\nurl = \'http://download.tensorflow.org/models/inception_v4_2016_09_09.tar.gz\'\ncheckpoints_dir = \'/tmp/checkpoints/\'\n\ndef make_padding(padding_name, conv_shape):\n  padding_name = padding_name.decode(""utf-8"")\n  if padding_name == ""VALID"":\n    return [0, 0]\n  elif padding_name == ""SAME"":\n    #return [math.ceil(int(conv_shape[0])/2), math.ceil(int(conv_shape[1])/2)]\n    return [math.floor(int(conv_shape[0])/2), math.floor(int(conv_shape[1])/2)]\n  else:\n    sys.exit(\'Invalid padding name \'+padding_name)\n\ndef dump_conv2d(name=\'Conv2d_1a_3x3\'):\n  \n  conv_operation = sess.graph.get_operation_by_name(\'InceptionV4/InceptionV4/\'+name+\'/Conv2D\') # remplacer convolution par Conv2D si erreur\n\n  weights_tensor = sess.graph.get_tensor_by_name(\'InceptionV4/\'+name+\'/weights:0\')\n  weights = weights_tensor.eval()\n\n  padding = make_padding(conv_operation.get_attr(\'padding\'), weights_tensor.get_shape())\n  strides = conv_operation.get_attr(\'strides\')\n\n  conv_out = sess.graph.get_operation_by_name(\'InceptionV4/InceptionV4/\'+name+\'/Conv2D\').outputs[0].eval() # remplacer convolution par Conv2D si erreur\n  \n  beta = sess.graph.get_tensor_by_name(\'InceptionV4/\'+name+\'/BatchNorm/beta:0\').eval()\n  #gamma = sess.graph.get_tensor_by_name(\'InceptionV4/\'+name+\'/BatchNorm/gamma:0\').eval()\n  mean = sess.graph.get_tensor_by_name(\'InceptionV4/\'+name+\'/BatchNorm/moving_mean:0\').eval()\n  var = sess.graph.get_tensor_by_name(\'InceptionV4/\'+name+\'/BatchNorm/moving_variance:0\').eval()\n  \n  relu_out = sess.graph.get_operation_by_name(\'InceptionV4/InceptionV4/\'+name+\'/Relu\').outputs[0].eval()\n\n  os.system(\'mkdir -p dump/InceptionV4/\'+name)\n  h5f = h5py.File(\'dump/InceptionV4/\'+name+\'.h5\', \'w\')\n  # conv\n  h5f.create_dataset(""weights"", data=weights)\n  h5f.create_dataset(""strides"", data=strides)\n  h5f.create_dataset(""padding"", data=padding)\n  h5f.create_dataset(""conv_out"", data=conv_out)\n  # batch norm\n  h5f.create_dataset(""beta"", data=beta)\n  #h5f.create_dataset(""gamma"", data=gamma)\n  h5f.create_dataset(""mean"", data=mean)\n  h5f.create_dataset(""var"", data=var)\n  h5f.create_dataset(""relu_out"", data=relu_out)\n  h5f.close()\n\ndef dump_logits():\n  operation = sess.graph.get_operation_by_name(\'InceptionV4/Logits/Predictions\')\n\n  weights_tensor = sess.graph.get_tensor_by_name(\'InceptionV4/Logits/Logits/weights:0\')\n  weights = weights_tensor.eval()\n\n  biases_tensor = sess.graph.get_tensor_by_name(\'InceptionV4/Logits/Logits/biases:0\')\n  biases = biases_tensor.eval()\n\n  out = operation.outputs[0].eval()\n\n  h5f = h5py.File(\'dump/InceptionV4/Logits.h5\', \'w\')\n  # conv\n  h5f.create_dataset(""weights"", data=weights)\n  h5f.create_dataset(""biases"", data=biases)\n  h5f.create_dataset(""out"", data=out)\n  h5f.close()\n\n\n# def dump_avgpool(name=\'Mixed_5b/Branch_3/AvgPool_0a_3x3\'):\n#   operation = sess.graph.get_operation_by_name(\'InceptionV4/InceptionV4/\'+name+\'/AvgPool\')\n#   out = operation.outputs[0].eval()\n#   os.system(\'mkdir -p dump/InceptionV4/\'+name)\n#   h5f = h5py.File(\'dump/InceptionV4/\'+name+\'.h5\', \'w\')\n#   h5f.create_dataset(""out"", data=out)\n#   h5f.close()\n\n# def dump_concats():\n#   operation1 = sess.graph.get_operation_by_name(\'InceptionV4/InceptionV4/Mixed_7b/Branch_1/concat\')\n#   operation2 = sess.graph.get_operation_by_name(\'InceptionV4/InceptionV4/Mixed_7b/Branch_2/concat\')\n#   out1 = operation1.outputs[0].eval()\n#   out2 = operation2.outputs[0].eval()\n#   os.system(\'mkdir -p dump/InceptionV4/Mixed_7b/Branch_1/concat\')\n#   h5f = h5py.File(\'dump/InceptionV4/Mixed_7b/Branch_1/concat.h5\', \'w\')\n#   h5f.create_dataset(""out"", data=out1)\n#   h5f.close()\n#   os.system(\'mkdir -p dump/InceptionV4/Mixed_7b/Branch_2/concat\')\n#   h5f = h5py.File(\'dump/InceptionV4/Mixed_7b/Branch_2/concat.h5\', \'w\')\n#   h5f.create_dataset(""out"", data=out2)\n#   h5f.close()\n\ndef dump_mixed_4a_7a(name=\'Mixed_4a\'):\n  dump_conv2d(name=name+\'/Branch_0/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_0/Conv2d_1a_3x3\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0b_1x7\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0c_7x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_1a_3x3\')\n\ndef dump_mixed_5(name=\'Mixed_5b\'):\n  dump_conv2d(name=name+\'/Branch_0/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0b_3x3\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0b_3x3\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0c_3x3\')\n  dump_conv2d(name=name+\'/Branch_3/Conv2d_0b_1x1\')\n\ndef dump_mixed_6(name=\'Mixed_6b\'):\n  dump_conv2d(name=name+\'/Branch_0/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0b_1x7\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0c_7x1\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0b_7x1\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0c_1x7\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0d_7x1\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0e_1x7\')\n  dump_conv2d(name=name+\'/Branch_3/Conv2d_0b_1x1\')\n\ndef dump_mixed_7(name=\'Mixed_7b\'):\n  dump_conv2d(name=name+\'/Branch_0/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0b_1x3\')\n  dump_conv2d(name=name+\'/Branch_1/Conv2d_0c_3x1\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0a_1x1\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0b_3x1\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0c_1x3\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0d_1x3\')\n  dump_conv2d(name=name+\'/Branch_2/Conv2d_0e_3x1\')\n  dump_conv2d(name=name+\'/Branch_3/Conv2d_0b_1x1\')\n\n\nif not tf.gfile.Exists(checkpoints_dir+\'inception_v4.ckpt\'):\n  tf.gfile.MakeDirs(checkpoints_dir)\n  dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)\n\nwith tf.Graph().as_default():\n\n  # Create model architecture\n\n  from scipy import misc\n  img = misc.imread(\'lena_299.png\')\n  print(img.shape)\n\n  inputs = np.zeros((1,299,299,3), dtype=np.float32)\n\n  inputs[0] = img\n  inputs = tf.pack(inputs)\n\n  with slim.arg_scope(inception.inception_v4_arg_scope()):\n    logits, _ = inception.inception_v4(inputs, num_classes=1001, is_training=False)\n\n  with tf.Session() as sess:\n\n    # Initialize model\n    init_fn = slim.assign_from_checkpoint_fn(\n    os.path.join(checkpoints_dir, \'inception_v4.ckpt\'),\n    slim.get_model_variables(\'InceptionV4\'))  \n\n    init_fn(sess)\n\n    # Display model variables\n    for v in slim.get_model_variables():\n      print(\'name = {}, shape = {}\'.format(v.name, v.get_shape()))\n\n    # Create graph\n    os.system(""rm -rf logs"")\n    os.system(""mkdir -p logs"")\n\n    tf.scalar_summary(\'logs\', logits[0][0])\n    summary_op = tf.merge_all_summaries()\n    summary_writer = tf.train.SummaryWriter(""logs"", sess.graph)\n\n    out = sess.run(summary_op)\n    summary_writer.add_summary(out, 0)\n\n    # Stem\n    dump_conv2d(name=\'Conv2d_1a_3x3\')\n    dump_conv2d(name=\'Conv2d_2a_3x3\')\n    dump_conv2d(name=\'Conv2d_2b_3x3\')\n\n    dump_conv2d(name=\'Mixed_3a/Branch_1/Conv2d_0a_3x3\')\n    dump_mixed_4a_7a(name=\'Mixed_4a\')\n    dump_conv2d(name=\'Mixed_5a/Branch_0/Conv2d_1a_3x3\')\n\n    # Inception A\n    dump_mixed_5(name=\'Mixed_5b\')\n    dump_mixed_5(name=\'Mixed_5c\')\n    dump_mixed_5(name=\'Mixed_5d\')\n    dump_mixed_5(name=\'Mixed_5e\')\n\n    # Reduction A\n    dump_conv2d(name=\'Mixed_6a/Branch_0/Conv2d_1a_3x3\')\n    dump_conv2d(name=\'Mixed_6a/Branch_1/Conv2d_0a_1x1\')\n    dump_conv2d(name=\'Mixed_6a/Branch_1/Conv2d_0b_3x3\')\n    dump_conv2d(name=\'Mixed_6a/Branch_1/Conv2d_1a_3x3\')\n\n    # Inception B\n    dump_mixed_6(name=\'Mixed_6b\')\n    dump_mixed_6(name=\'Mixed_6c\')\n    dump_mixed_6(name=\'Mixed_6d\')\n    dump_mixed_6(name=\'Mixed_6e\')\n    dump_mixed_6(name=\'Mixed_6f\')\n    dump_mixed_6(name=\'Mixed_6g\')\n    dump_mixed_6(name=\'Mixed_6h\')\n\n    # Reduction B\n    dump_mixed_4a_7a(name=\'Mixed_7a\')\n\n    # Inception C\n    dump_mixed_7(name=\'Mixed_7b\')\n    dump_mixed_7(name=\'Mixed_7c\')\n    dump_mixed_7(name=\'Mixed_7d\')\n\n    dump_logits()\n\n\n    # #dump_concats()\n    # #dump_avgpool(name=\'Mixed_5b/Branch_3/AvgPool_0a_3x3\')'"
model_zoo/models/autoencoder/AdditiveGaussianNoiseAutoencoderRunner.py,0,"b'import numpy as np\n\nimport sklearn.preprocessing as prep\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nfrom autoencoder.autoencoder_models.DenoisingAutoencoder import AdditiveGaussianNoiseAutoencoder\n\nmnist = input_data.read_data_sets(\'MNIST_data\', one_hot = True)\n\ndef standard_scale(X_train, X_test):\n    preprocessor = prep.StandardScaler().fit(X_train)\n    X_train = preprocessor.transform(X_train)\n    X_test = preprocessor.transform(X_test)\n    return X_train, X_test\n\ndef get_random_block_from_data(data, batch_size):\n    start_index = np.random.randint(0, len(data) - batch_size)\n    return data[start_index:(start_index + batch_size)]\n\nX_train, X_test = standard_scale(mnist.train.images, mnist.test.images)\n\nn_samples = int(mnist.train.num_examples)\ntraining_epochs = 20\nbatch_size = 128\ndisplay_step = 1\n\nautoencoder = AdditiveGaussianNoiseAutoencoder(n_input = 784,\n                                               n_hidden = 200,\n                                               transfer_function = tf.nn.softplus,\n                                               optimizer = tf.train.AdamOptimizer(learning_rate = 0.001),\n                                               scale = 0.01)\n\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(n_samples / batch_size)\n    # Loop over all batches\n    for i in range(total_batch):\n        batch_xs = get_random_block_from_data(X_train, batch_size)\n\n        # Fit training using batch data\n        cost = autoencoder.partial_fit(batch_xs)\n        # Compute average loss\n        avg_cost += cost / n_samples * batch_size\n\n    # Display logs per epoch step\n    if epoch % display_step == 0:\n        print ""Epoch:"", \'%04d\' % (epoch + 1), \\\n            ""cost="", ""{:.9f}"".format(avg_cost)\n\nprint ""Total cost: "" + str(autoencoder.calc_total_cost(X_test))\n'"
model_zoo/models/autoencoder/AutoencoderRunner.py,0,"b'import numpy as np\n\nimport sklearn.preprocessing as prep\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nfrom autoencoder.autoencoder_models.Autoencoder import Autoencoder\n\nmnist = input_data.read_data_sets(\'MNIST_data\', one_hot = True)\n\ndef standard_scale(X_train, X_test):\n    preprocessor = prep.StandardScaler().fit(X_train)\n    X_train = preprocessor.transform(X_train)\n    X_test = preprocessor.transform(X_test)\n    return X_train, X_test\n\ndef get_random_block_from_data(data, batch_size):\n    start_index = np.random.randint(0, len(data) - batch_size)\n    return data[start_index:(start_index + batch_size)]\n\nX_train, X_test = standard_scale(mnist.train.images, mnist.test.images)\n\nn_samples = int(mnist.train.num_examples)\ntraining_epochs = 20\nbatch_size = 128\ndisplay_step = 1\n\nautoencoder = Autoencoder(n_input = 784,\n                          n_hidden = 200,\n                          transfer_function = tf.nn.softplus,\n                          optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))\n\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(n_samples / batch_size)\n    # Loop over all batches\n    for i in range(total_batch):\n        batch_xs = get_random_block_from_data(X_train, batch_size)\n\n        # Fit training using batch data\n        cost = autoencoder.partial_fit(batch_xs)\n        # Compute average loss\n        avg_cost += cost / n_samples * batch_size\n\n    # Display logs per epoch step\n    if epoch % display_step == 0:\n        print ""Epoch:"", \'%04d\' % (epoch + 1), \\\n            ""cost="", ""{:.9f}"".format(avg_cost)\n\nprint ""Total cost: "" + str(autoencoder.calc_total_cost(X_test))\n'"
model_zoo/models/autoencoder/MaskingNoiseAutoencoderRunner.py,0,"b'import numpy as np\n\nimport sklearn.preprocessing as prep\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nfrom autoencoder.autoencoder_models.DenoisingAutoencoder import MaskingNoiseAutoencoder\n\nmnist = input_data.read_data_sets(\'MNIST_data\', one_hot = True)\n\ndef standard_scale(X_train, X_test):\n    preprocessor = prep.StandardScaler().fit(X_train)\n    X_train = preprocessor.transform(X_train)\n    X_test = preprocessor.transform(X_test)\n    return X_train, X_test\n\ndef get_random_block_from_data(data, batch_size):\n    start_index = np.random.randint(0, len(data) - batch_size)\n    return data[start_index:(start_index + batch_size)]\n\nX_train, X_test = standard_scale(mnist.train.images, mnist.test.images)\n\n\nn_samples = int(mnist.train.num_examples)\ntraining_epochs = 100\nbatch_size = 128\ndisplay_step = 1\n\nautoencoder = MaskingNoiseAutoencoder(n_input = 784,\n                                      n_hidden = 200,\n                                      transfer_function = tf.nn.softplus,\n                                      optimizer = tf.train.AdamOptimizer(learning_rate = 0.001),\n                                      dropout_probability = 0.95)\n\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(n_samples / batch_size)\n    for i in range(total_batch):\n        batch_xs = get_random_block_from_data(X_train, batch_size)\n\n        cost = autoencoder.partial_fit(batch_xs)\n\n        avg_cost += cost / n_samples * batch_size\n\n    if epoch % display_step == 0:\n        print ""Epoch:"", \'%04d\' % (epoch + 1), \\\n            ""cost="", ""{:.9f}"".format(avg_cost)\n\nprint ""Total cost: "" + str(autoencoder.calc_total_cost(X_test))\n'"
model_zoo/models/autoencoder/Utils.py,0,"b'import numpy as np\nimport tensorflow as tf\n\ndef xavier_init(fan_in, fan_out, constant = 1):\n    low = -constant * np.sqrt(6.0 / (fan_in + fan_out))\n    high = constant * np.sqrt(6.0 / (fan_in + fan_out))\n    return tf.random_uniform((fan_in, fan_out),\n                             minval = low, maxval = high,\n                             dtype = tf.float32)\n'"
model_zoo/models/autoencoder/VariationalAutoencoderRunner.py,0,"b'import numpy as np\n\nimport sklearn.preprocessing as prep\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nfrom autoencoder.autoencoder_models.VariationalAutoencoder import VariationalAutoencoder\n\nmnist = input_data.read_data_sets(\'MNIST_data\', one_hot = True)\n\n\n\ndef min_max_scale(X_train, X_test):\n    preprocessor = prep.MinMaxScaler().fit(X_train)\n    X_train = preprocessor.transform(X_train)\n    X_test = preprocessor.transform(X_test)\n    return X_train, X_test\n\n\ndef get_random_block_from_data(data, batch_size):\n    start_index = np.random.randint(0, len(data) - batch_size)\n    return data[start_index:(start_index + batch_size)]\n\n\nX_train, X_test = min_max_scale(mnist.train.images, mnist.test.images)\n\nn_samples = int(mnist.train.num_examples)\ntraining_epochs = 20\nbatch_size = 128\ndisplay_step = 1\n\nautoencoder = VariationalAutoencoder(n_input = 784,\n                                     n_hidden = 200,\n                                     optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))\n\nfor epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = int(n_samples / batch_size)\n    # Loop over all batches\n    for i in range(total_batch):\n        batch_xs = get_random_block_from_data(X_train, batch_size)\n\n        # Fit training using batch data\n        cost = autoencoder.partial_fit(batch_xs)\n        # Compute average loss\n        avg_cost += cost / n_samples * batch_size\n\n    # Display logs per epoch step\n    if epoch % display_step == 0:\n        print ""Epoch:"", \'%04d\' % (epoch + 1), \\\n            ""cost="", ""{:.9f}"".format(avg_cost)\n\nprint ""Total cost: "" + str(autoencoder.calc_total_cost(X_test))\n'"
model_zoo/models/autoencoder/__init__.py,0,b''
model_zoo/models/compression/decoder.py,0,"b'#!/usr/bin/python\n#\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Neural Network Image Compression Decoder.\n\nDecompress an image from the numpy\'s npz format generated by the encoder.\n\nExample usage:\npython decoder.py --input_codes=output_codes.pkl --iteration=15 \\\n--output_directory=/tmp/compression_output/ --model=residual_gru.pb\n""""""\nimport io\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\ntf.flags.DEFINE_string(\'input_codes\', None, \'Location of binary code file.\')\ntf.flags.DEFINE_integer(\'iteration\', -1, \'The max quality level of \'\n                        \'the images to output. Use -1 to infer from loaded \'\n                        \' codes.\')\ntf.flags.DEFINE_string(\'output_directory\', None, \'Directory to save decoded \'\n                       \'images.\')\ntf.flags.DEFINE_string(\'model\', None, \'Location of compression model.\')\n\nFLAGS = tf.flags.FLAGS\n\n\ndef get_input_tensor_names():\n  name_list = [\'GruBinarizer/SignBinarizer/Sign:0\']\n  for i in xrange(1, 16):\n    name_list.append(\'GruBinarizer/SignBinarizer/Sign_{}:0\'.format(i))\n  return name_list\n\n\ndef get_output_tensor_names():\n  return [\'loop_{0:02d}/add:0\'.format(i) for i in xrange(0, 16)]\n\n\ndef main(_):\n  if (FLAGS.input_codes is None or FLAGS.output_directory is None or\n      FLAGS.model is None):\n    print (\'\\nUsage: python decoder.py --input_codes=output_codes.pkl \'\n           \'--iteration=15 --output_directory=/tmp/compression_output/ \'\n           \'--model=residual_gru.pb\\n\\n\')\n    return\n\n  if FLAGS.iteration < -1 or FLAGS.iteration > 15:\n    print (\'\\n--iteration must be between 0 and 15 inclusive, or -1 to infer \'\n           \'from file.\\n\')\n    return\n  iteration = FLAGS.iteration\n\n  if not tf.gfile.Exists(FLAGS.output_directory):\n    tf.gfile.MkDir(FLAGS.output_directory)\n\n  if not tf.gfile.Exists(FLAGS.input_codes):\n    print \'\\nInput codes not found.\\n\'\n    return\n\n  contents = \'\'\n  with tf.gfile.FastGFile(FLAGS.input_codes, \'r\') as code_file:\n    contents = code_file.read()\n    loaded_codes = np.load(io.BytesIO(contents))\n    assert [\'codes\', \'shape\'] not in loaded_codes.files\n    loaded_shape = loaded_codes[\'shape\']\n    loaded_array = loaded_codes[\'codes\']\n\n    # Unpack and recover code shapes.\n    unpacked_codes = np.reshape(np.unpackbits(loaded_array)\n                                [:np.prod(loaded_shape)],\n                                loaded_shape)\n\n    numpy_int_codes = np.split(unpacked_codes, len(unpacked_codes))\n    if iteration == -1:\n      iteration = len(unpacked_codes) - 1\n    # Convert back to float and recover scale.\n    numpy_codes = [np.squeeze(x.astype(np.float32), 0) * 2 - 1 for x in\n                   numpy_int_codes]\n\n  with tf.Graph().as_default() as graph:\n    # Load the inference model for decoding.\n    with tf.gfile.FastGFile(FLAGS.model, \'rb\') as model_file:\n      graph_def = tf.GraphDef()\n      graph_def.ParseFromString(model_file.read())\n    _ = tf.import_graph_def(graph_def, name=\'\')\n\n    # For encoding the tensors into PNGs.\n    input_image = tf.placeholder(tf.uint8)\n    encoded_image = tf.image.encode_png(input_image)\n\n    input_tensors = [graph.get_tensor_by_name(name) for name in\n                     get_input_tensor_names()][0:iteration+1]\n    outputs = [graph.get_tensor_by_name(name) for name in\n               get_output_tensor_names()][0:iteration+1]\n\n  feed_dict = {key: value for (key, value) in zip(input_tensors,\n                                                  numpy_codes)}\n\n  with tf.Session(graph=graph) as sess:\n    results = sess.run(outputs, feed_dict=feed_dict)\n\n    for index, result in enumerate(results):\n      img = np.uint8(np.clip(result + 0.5, 0, 255))\n      img = img.squeeze()\n      png_img = sess.run(encoded_image, feed_dict={input_image: img})\n\n      with tf.gfile.FastGFile(os.path.join(FLAGS.output_directory,\n                                           \'image_{0:02d}.png\'.format(index)),\n                              \'w\') as output_image:\n        output_image.write(png_img)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/compression/encoder.py,0,"b'#!/usr/bin/python\n#\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Neural Network Image Compression Encoder.\n\nCompresses an image to a binarized numpy array. The image must be padded to a\nmultiple of 32 pixels in height and width.\n\nExample usage:\npython encoder.py --input_image=/your/image/here.png \\\n--output_codes=output_codes.pkl --iteration=15 --model=residual_gru.pb\n""""""\nimport io\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\ntf.flags.DEFINE_string(\'input_image\', None, \'Location of input image. We rely \'\n                       \'on tf.image to decode the image, so only PNG and JPEG \'\n                       \'formats are currently supported.\')\ntf.flags.DEFINE_integer(\'iteration\', 15, \'Quality level for encoding image. \'\n                        \'Must be between 0 and 15 inclusive.\')\ntf.flags.DEFINE_string(\'output_codes\', None, \'File to save output encoding.\')\ntf.flags.DEFINE_string(\'model\', None, \'Location of compression model.\')\n\nFLAGS = tf.flags.FLAGS\n\n\ndef get_output_tensor_names():\n  name_list = [\'GruBinarizer/SignBinarizer/Sign:0\']\n  for i in xrange(1, 16):\n    name_list.append(\'GruBinarizer/SignBinarizer/Sign_{}:0\'.format(i))\n  return name_list\n\n\ndef main(_):\n  if (FLAGS.input_image is None or FLAGS.output_codes is None or\n      FLAGS.model is None):\n    print (\'\\nUsage: python encoder.py --input_image=/your/image/here.png \'\n           \'--output_codes=output_codes.pkl --iteration=15 \'\n           \'--model=residual_gru.pb\\n\\n\')\n    return\n\n  if FLAGS.iteration < 0 or FLAGS.iteration > 15:\n    print \'\\n--iteration must be between 0 and 15 inclusive.\\n\'\n    return\n\n  with tf.gfile.FastGFile(FLAGS.input_image) as input_image:\n    input_image_str = input_image.read()\n\n  with tf.Graph().as_default() as graph:\n    # Load the inference model for encoding.\n    with tf.gfile.FastGFile(FLAGS.model, \'rb\') as model_file:\n      graph_def = tf.GraphDef()\n      graph_def.ParseFromString(model_file.read())\n    _ = tf.import_graph_def(graph_def, name=\'\')\n\n    input_tensor = graph.get_tensor_by_name(\'Placeholder:0\')\n    outputs = [graph.get_tensor_by_name(name) for name in\n               get_output_tensor_names()]\n\n    input_image = tf.placeholder(tf.string)\n    _, ext = os.path.splitext(FLAGS.input_image)\n    if ext == \'.png\':\n      decoded_image = tf.image.decode_png(input_image, channels=3)\n    elif ext == \'.jpeg\' or ext == \'.jpg\':\n      decoded_image = tf.image.decode_jpeg(input_image, channels=3)\n    else:\n      assert False, \'Unsupported file format {}\'.format(ext)\n    decoded_image = tf.expand_dims(decoded_image, 0)\n\n  with tf.Session(graph=graph) as sess:\n    img_array = sess.run(decoded_image, feed_dict={input_image:\n                                                   input_image_str})\n    results = sess.run(outputs, feed_dict={input_tensor: img_array})\n\n  results = results[0:FLAGS.iteration + 1]\n  int_codes = np.asarray([x.astype(np.int8) for x in results])\n\n  # Convert int codes to binary.\n  int_codes = (int_codes + 1)/2\n  export = np.packbits(int_codes.reshape(-1))\n\n  output = io.BytesIO()\n  np.savez_compressed(output, shape=int_codes.shape, codes=export)\n  with tf.gfile.FastGFile(FLAGS.output_codes, \'w\') as code_file:\n    code_file.write(output.getvalue())\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/compression/msssim.py,0,"b'#!/usr/bin/python\n#\n# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Python implementation of MS-SSIM.\n\nUsage:\n\npython msssim.py --original_image=original.png --compared_image=distorted.png\n""""""\nimport numpy as np\nfrom scipy import signal\nfrom scipy.ndimage.filters import convolve\nimport tensorflow as tf\n\n\ntf.flags.DEFINE_string(\'original_image\', None, \'Path to PNG image.\')\ntf.flags.DEFINE_string(\'compared_image\', None, \'Path to PNG image.\')\nFLAGS = tf.flags.FLAGS\n\n\ndef _FSpecialGauss(size, sigma):\n  """"""Function to mimic the \'fspecial\' gaussian MATLAB function.""""""\n  radius = size // 2\n  offset = 0.0\n  start, stop = -radius, radius + 1\n  if size % 2 == 0:\n    offset = 0.5\n    stop -= 1\n  x, y = np.mgrid[offset + start:stop, offset + start:stop]\n  assert len(x) == size\n  g = np.exp(-((x**2 + y**2)/(2.0 * sigma**2)))\n  return g / g.sum()\n\n\ndef _SSIMForMultiScale(img1, img2, max_val=255, filter_size=11,\n                       filter_sigma=1.5, k1=0.01, k2=0.03):\n  """"""Return the Structural Similarity Map between `img1` and `img2`.\n\n  This function attempts to match the functionality of ssim_index_new.m by\n  Zhou Wang: http://www.cns.nyu.edu/~lcv/ssim/msssim.zip\n\n  Arguments:\n    img1: Numpy array holding the first RGB image batch.\n    img2: Numpy array holding the second RGB image batch.\n    max_val: the dynamic range of the images (i.e., the difference between the\n      maximum the and minimum allowed values).\n    filter_size: Size of blur kernel to use (will be reduced for small images).\n    filter_sigma: Standard deviation for Gaussian blur kernel (will be reduced\n      for small images).\n    k1: Constant used to maintain stability in the SSIM calculation (0.01 in\n      the original paper).\n    k2: Constant used to maintain stability in the SSIM calculation (0.03 in\n      the original paper).\n\n  Returns:\n    Pair containing the mean SSIM and contrast sensitivity between `img1` and\n    `img2`.\n\n  Raises:\n    RuntimeError: If input images don\'t have the same shape or don\'t have four\n      dimensions: [batch_size, height, width, depth].\n  """"""\n  if img1.shape != img2.shape:\n    raise RuntimeError(\'Input images must have the same shape (%s vs. %s).\',\n                       img1.shape, img2.shape)\n  if img1.ndim != 4:\n    raise RuntimeError(\'Input images must have four dimensions, not %d\',\n                       img1.ndim)\n\n  img1 = img1.astype(np.float64)\n  img2 = img2.astype(np.float64)\n  _, height, width, _ = img1.shape\n\n  # Filter size can\'t be larger than height or width of images.\n  size = min(filter_size, height, width)\n\n  # Scale down sigma if a smaller filter size is used.\n  sigma = size * filter_sigma / filter_size if filter_size else 0\n\n  if filter_size:\n    window = np.reshape(_FSpecialGauss(size, sigma), (1, size, size, 1))\n    mu1 = signal.fftconvolve(img1, window, mode=\'valid\')\n    mu2 = signal.fftconvolve(img2, window, mode=\'valid\')\n    sigma11 = signal.fftconvolve(img1 * img1, window, mode=\'valid\')\n    sigma22 = signal.fftconvolve(img2 * img2, window, mode=\'valid\')\n    sigma12 = signal.fftconvolve(img1 * img2, window, mode=\'valid\')\n  else:\n    # Empty blur kernel so no need to convolve.\n    mu1, mu2 = img1, img2\n    sigma11 = img1 * img1\n    sigma22 = img2 * img2\n    sigma12 = img1 * img2\n\n  mu11 = mu1 * mu1\n  mu22 = mu2 * mu2\n  mu12 = mu1 * mu2\n  sigma11 -= mu11\n  sigma22 -= mu22\n  sigma12 -= mu12\n\n  # Calculate intermediate values used by both ssim and cs_map.\n  c1 = (k1 * max_val) ** 2\n  c2 = (k2 * max_val) ** 2\n  v1 = 2.0 * sigma12 + c2\n  v2 = sigma11 + sigma22 + c2\n  ssim = np.mean((((2.0 * mu12 + c1) * v1) / ((mu11 + mu22 + c1) * v2)))\n  cs = np.mean(v1 / v2)\n  return ssim, cs\n\n\ndef MultiScaleSSIM(img1, img2, max_val=255, filter_size=11, filter_sigma=1.5,\n                   k1=0.01, k2=0.03, weights=None):\n  """"""Return the MS-SSIM score between `img1` and `img2`.\n\n  This function implements Multi-Scale Structural Similarity (MS-SSIM) Image\n  Quality Assessment according to Zhou Wang\'s paper, ""Multi-scale structural\n  similarity for image quality assessment"" (2003).\n  Link: https://ece.uwaterloo.ca/~z70wang/publications/msssim.pdf\n\n  Author\'s MATLAB implementation:\n  http://www.cns.nyu.edu/~lcv/ssim/msssim.zip\n\n  Arguments:\n    img1: Numpy array holding the first RGB image batch.\n    img2: Numpy array holding the second RGB image batch.\n    max_val: the dynamic range of the images (i.e., the difference between the\n      maximum the and minimum allowed values).\n    filter_size: Size of blur kernel to use (will be reduced for small images).\n    filter_sigma: Standard deviation for Gaussian blur kernel (will be reduced\n      for small images).\n    k1: Constant used to maintain stability in the SSIM calculation (0.01 in\n      the original paper).\n    k2: Constant used to maintain stability in the SSIM calculation (0.03 in\n      the original paper).\n    weights: List of weights for each level; if none, use five levels and the\n      weights from the original paper.\n\n  Returns:\n    MS-SSIM score between `img1` and `img2`.\n\n  Raises:\n    RuntimeError: If input images don\'t have the same shape or don\'t have four\n      dimensions: [batch_size, height, width, depth].\n  """"""\n  if img1.shape != img2.shape:\n    raise RuntimeError(\'Input images must have the same shape (%s vs. %s).\',\n                       img1.shape, img2.shape)\n  if img1.ndim != 4:\n    raise RuntimeError(\'Input images must have four dimensions, not %d\',\n                       img1.ndim)\n\n  # Note: default weights don\'t sum to 1.0 but do match the paper / matlab code.\n  weights = np.array(weights if weights else\n                     [0.0448, 0.2856, 0.3001, 0.2363, 0.1333])\n  levels = weights.size\n  downsample_filter = np.ones((1, 2, 2, 1)) / 4.0\n  im1, im2 = [x.astype(np.float64) for x in [img1, img2]]\n  mssim = np.array([])\n  mcs = np.array([])\n  for _ in xrange(levels):\n    ssim, cs = _SSIMForMultiScale(\n        im1, im2, max_val=max_val, filter_size=filter_size,\n        filter_sigma=filter_sigma, k1=k1, k2=k2)\n    mssim = np.append(mssim, ssim)\n    mcs = np.append(mcs, cs)\n    filtered = [convolve(im, downsample_filter, mode=\'reflect\')\n                for im in [im1, im2]]\n    im1, im2 = [x[:, ::2, ::2, :] for x in filtered]\n  return (np.prod(mcs[0:levels-1] ** weights[0:levels-1]) *\n          (mssim[levels-1] ** weights[levels-1]))\n\n\ndef main(_):\n  if FLAGS.original_image is None or FLAGS.compared_image is None:\n    print (\'\\nUsage: python msssim.py --original_image=original.png \'\n           \'--compared_image=distorted.png\\n\\n\')\n    return\n\n  if not tf.gfile.Exists(FLAGS.original_image):\n    print \'\\nCannot find --original_image.\\n\'\n    return\n\n  if not tf.gfile.Exists(FLAGS.compared_image):\n    print \'\\nCannot find --compared_image.\\n\'\n    return\n\n  with tf.gfile.FastGFile(FLAGS.original_image) as image_file:\n    img1_str = image_file.read()\n  with tf.gfile.FastGFile(FLAGS.compared_image) as image_file:\n    img2_str = image_file.read()\n\n  input_img = tf.placeholder(tf.string)\n  decoded_image = tf.expand_dims(tf.image.decode_png(input_img, channels=3), 0)\n\n  with tf.Session() as sess:\n    img1 = sess.run(decoded_image, feed_dict={input_img: img1_str})\n    img2 = sess.run(decoded_image, feed_dict={input_img: img2_str})\n\n  print MultiScaleSSIM(img1, img2, max_val=255)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/differential_privacy/__init__.py,0,b''
model_zoo/models/lm_1b/data_utils.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A library for loading 1B word benchmark dataset.""""""\n\nimport random\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass Vocabulary(object):\n  """"""Class that holds a vocabulary for the dataset.""""""\n\n  def __init__(self, filename):\n    """"""Initialize vocabulary.\n\n    Args:\n      filename: Vocabulary file name.\n    """"""\n\n    self._id_to_word = []\n    self._word_to_id = {}\n    self._unk = -1\n    self._bos = -1\n    self._eos = -1\n\n    with tf.gfile.Open(filename) as f:\n      idx = 0\n      for line in f:\n        word_name = line.strip()\n        if word_name == \'<S>\':\n          self._bos = idx\n        elif word_name == \'</S>\':\n          self._eos = idx\n        elif word_name == \'<UNK>\':\n          self._unk = idx\n        if word_name == \'!!!MAXTERMID\':\n          continue\n\n        self._id_to_word.append(word_name)\n        self._word_to_id[word_name] = idx\n        idx += 1\n\n  @property\n  def bos(self):\n    return self._bos\n\n  @property\n  def eos(self):\n    return self._eos\n\n  @property\n  def unk(self):\n    return self._unk\n\n  @property\n  def size(self):\n    return len(self._id_to_word)\n\n  def word_to_id(self, word):\n    if word in self._word_to_id:\n      return self._word_to_id[word]\n    return self.unk\n\n  def id_to_word(self, cur_id):\n    if cur_id < self.size:\n      return self._id_to_word[cur_id]\n    return \'ERROR\'\n\n  def decode(self, cur_ids):\n    """"""Convert a list of ids to a sentence, with space inserted.""""""\n    return \' \'.join([self.id_to_word(cur_id) for cur_id in cur_ids])\n\n  def encode(self, sentence):\n    """"""Convert a sentence to a list of ids, with special tokens added.""""""\n    word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split()]\n    return np.array([self.bos] + word_ids + [self.eos], dtype=np.int32)\n\n\nclass CharsVocabulary(Vocabulary):\n  """"""Vocabulary containing character-level information.""""""\n\n  def __init__(self, filename, max_word_length):\n    super(CharsVocabulary, self).__init__(filename)\n    self._max_word_length = max_word_length\n    chars_set = set()\n\n    for word in self._id_to_word:\n      chars_set |= set(word)\n\n    free_ids = []\n    for i in range(256):\n      if chr(i) in chars_set:\n        continue\n      free_ids.append(chr(i))\n\n    if len(free_ids) < 5:\n      raise ValueError(\'Not enough free char ids: %d\' % len(free_ids))\n\n    self.bos_char = free_ids[0]  # <begin sentence>\n    self.eos_char = free_ids[1]  # <end sentence>\n    self.bow_char = free_ids[2]  # <begin word>\n    self.eow_char = free_ids[3]  # <end word>\n    self.pad_char = free_ids[4]  # <padding>\n\n    chars_set |= {self.bos_char, self.eos_char, self.bow_char, self.eow_char,\n                  self.pad_char}\n\n    self._char_set = chars_set\n    num_words = len(self._id_to_word)\n\n    self._word_char_ids = np.zeros([num_words, max_word_length], dtype=np.int32)\n\n    self.bos_chars = self._convert_word_to_char_ids(self.bos_char)\n    self.eos_chars = self._convert_word_to_char_ids(self.eos_char)\n\n    for i, word in enumerate(self._id_to_word):\n      self._word_char_ids[i] = self._convert_word_to_char_ids(word)\n\n  @property\n  def word_char_ids(self):\n    return self._word_char_ids\n\n  @property\n  def max_word_length(self):\n    return self._max_word_length\n\n  def _convert_word_to_char_ids(self, word):\n    code = np.zeros([self.max_word_length], dtype=np.int32)\n    code[:] = ord(self.pad_char)\n\n    if len(word) > self.max_word_length - 2:\n      word = word[:self.max_word_length-2]\n    cur_word = self.bow_char + word + self.eow_char\n    for j in range(len(cur_word)):\n      code[j] = ord(cur_word[j])\n    return code\n\n  def word_to_char_ids(self, word):\n    if word in self._word_to_id:\n      return self._word_char_ids[self._word_to_id[word]]\n    else:\n      return self._convert_word_to_char_ids(word)\n\n  def encode_chars(self, sentence):\n    chars_ids = [self.word_to_char_ids(cur_word)\n                 for cur_word in sentence.split()]\n    return np.vstack([self.bos_chars] + chars_ids + [self.eos_chars])\n\n\ndef get_batch(generator, batch_size, num_steps, max_word_length, pad=False):\n  """"""Read batches of input.""""""\n  cur_stream = [None] * batch_size\n\n  inputs = np.zeros([batch_size, num_steps], np.int32)\n  char_inputs = np.zeros([batch_size, num_steps, max_word_length], np.int32)\n  global_word_ids = np.zeros([batch_size, num_steps], np.int32)\n  targets = np.zeros([batch_size, num_steps], np.int32)\n  weights = np.ones([batch_size, num_steps], np.float32)\n\n  no_more_data = False\n  while True:\n    inputs[:] = 0\n    char_inputs[:] = 0\n    global_word_ids[:] = 0\n    targets[:] = 0\n    weights[:] = 0.0\n\n    for i in range(batch_size):\n      cur_pos = 0\n\n      while cur_pos < num_steps:\n        if cur_stream[i] is None or len(cur_stream[i][0]) <= 1:\n          try:\n            cur_stream[i] = list(generator.next())\n          except StopIteration:\n            # No more data, exhaust current streams and quit\n            no_more_data = True\n            break\n\n        how_many = min(len(cur_stream[i][0]) - 1, num_steps - cur_pos)\n        next_pos = cur_pos + how_many\n\n        inputs[i, cur_pos:next_pos] = cur_stream[i][0][:how_many]\n        char_inputs[i, cur_pos:next_pos] = cur_stream[i][1][:how_many]\n        global_word_ids[i, cur_pos:next_pos] = cur_stream[i][2][:how_many]\n        targets[i, cur_pos:next_pos] = cur_stream[i][0][1:how_many+1]\n        weights[i, cur_pos:next_pos] = 1.0\n\n        cur_pos = next_pos\n        cur_stream[i][0] = cur_stream[i][0][how_many:]\n        cur_stream[i][1] = cur_stream[i][1][how_many:]\n        cur_stream[i][2] = cur_stream[i][2][how_many:]\n\n        if pad:\n          break\n\n    if no_more_data and np.sum(weights) == 0:\n      # There is no more data and this is an empty batch. Done!\n      break\n    yield inputs, char_inputs, global_word_ids, targets, weights\n\n\nclass LM1BDataset(object):\n  """"""Utility class for 1B word benchmark dataset.\n\n  The current implementation reads the data from the tokenized text files.\n  """"""\n\n  def __init__(self, filepattern, vocab):\n    """"""Initialize LM1BDataset reader.\n\n    Args:\n      filepattern: Dataset file pattern.\n      vocab: Vocabulary.\n    """"""\n    self._vocab = vocab\n    self._all_shards = tf.gfile.Glob(filepattern)\n    tf.logging.info(\'Found %d shards at %s\', len(self._all_shards), filepattern)\n\n  def _load_random_shard(self):\n    """"""Randomly select a file and read it.""""""\n    return self._load_shard(random.choice(self._all_shards))\n\n  def _load_shard(self, shard_name):\n    """"""Read one file and convert to ids.\n\n    Args:\n      shard_name: file path.\n\n    Returns:\n      list of (id, char_id, global_word_id) tuples.\n    """"""\n    tf.logging.info(\'Loading data from: %s\', shard_name)\n    with tf.gfile.Open(shard_name) as f:\n      sentences = f.readlines()\n    chars_ids = [self.vocab.encode_chars(sentence) for sentence in sentences]\n    ids = [self.vocab.encode(sentence) for sentence in sentences]\n\n    global_word_ids = []\n    current_idx = 0\n    for word_ids in ids:\n      current_size = len(word_ids) - 1  # without <BOS> symbol\n      cur_ids = np.arange(current_idx, current_idx + current_size)\n      global_word_ids.append(cur_ids)\n      current_idx += current_size\n\n    tf.logging.info(\'Loaded %d words.\', current_idx)\n    tf.logging.info(\'Finished loading\')\n    return zip(ids, chars_ids, global_word_ids)\n\n  def _get_sentence(self, forever=True):\n    while True:\n      ids = self._load_random_shard()\n      for current_ids in ids:\n        yield current_ids\n      if not forever:\n        break\n\n  def get_batch(self, batch_size, num_steps, pad=False, forever=True):\n    return get_batch(self._get_sentence(forever), batch_size, num_steps,\n                     self.vocab.max_word_length, pad=pad)\n\n  @property\n  def vocab(self):\n    return self._vocab\n'"
model_zoo/models/lm_1b/lm_1b_eval.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Eval pre-trained 1 billion word language model.\n""""""\nimport os\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom google.protobuf import text_format\nimport data_utils\n\nFLAGS = tf.flags.FLAGS\n# General flags.\ntf.flags.DEFINE_string(\'mode\', \'eval\',\n                       \'One of [sample, eval, dump_emb, dump_lstm_emb]. \'\n                       \'""sample"" mode samples future word predictions, using \'\n                       \'FLAGS.prefix as prefix (prefix could be left empty). \'\n                       \'""eval"" mode calculates perplexity of the \'\n                       \'FLAGS.input_data. \'\n                       \'""dump_emb"" mode dumps word and softmax embeddings to \'\n                       \'FLAGS.save_dir. embeddings are dumped in the same \'\n                       \'order as words in vocabulary. All words in vocabulary \'\n                       \'are dumped.\'\n                       \'dump_lstm_emb dumps lstm embeddings of FLAGS.sentence \'\n                       \'to FLAGS.save_dir.\')\ntf.flags.DEFINE_string(\'pbtxt\', \'\',\n                       \'GraphDef proto text file used to construct model \'\n                       \'structure.\')\ntf.flags.DEFINE_string(\'ckpt\', \'\',\n                       \'Checkpoint directory used to fill model values.\')\ntf.flags.DEFINE_string(\'vocab_file\', \'\', \'Vocabulary file.\')\ntf.flags.DEFINE_string(\'save_dir\', \'\',\n                       \'Used for ""dump_emb"" mode to save word embeddings.\')\n# sample mode flags.\ntf.flags.DEFINE_string(\'prefix\', \'\',\n                       \'Used for ""sample"" mode to predict next words.\')\ntf.flags.DEFINE_integer(\'max_sample_words\', 100,\n                        \'Sampling stops either when </S> is met or this number \'\n                        \'of steps has passed.\')\ntf.flags.DEFINE_integer(\'num_samples\', 3,\n                        \'Number of samples to generate for the prefix.\')\n# dump_lstm_emb mode flags.\ntf.flags.DEFINE_string(\'sentence\', \'\',\n                       \'Used as input for ""dump_lstm_emb"" mode.\')\n# eval mode flags.\ntf.flags.DEFINE_string(\'input_data\', \'\',\n                       \'Input data files for eval model.\')\ntf.flags.DEFINE_integer(\'max_eval_steps\', 1000000,\n                        \'Maximum mumber of steps to run ""eval"" mode.\')\n\n\n# For saving demo resources, use batch size 1 and step 1.\nBATCH_SIZE = 1\nNUM_TIMESTEPS = 1\nMAX_WORD_LEN = 50\n\n\ndef _LoadModel(gd_file, ckpt_file):\n  """"""Load the model from GraphDef and Checkpoint.\n\n  Args:\n    gd_file: GraphDef proto text file.\n    ckpt_file: TensorFlow Checkpoint file.\n\n  Returns:\n    TensorFlow session and tensors dict.\n  """"""\n  with tf.Graph().as_default():\n    sys.stderr.write(\'Recovering graph.\\n\')\n    with tf.gfile.FastGFile(gd_file, \'r\') as f:\n      s = f.read()\n      gd = tf.GraphDef()\n      text_format.Merge(s, gd)\n\n    tf.logging.info(\'Recovering Graph %s\', gd_file)\n    t = {}\n    [t[\'states_init\'], t[\'lstm/lstm_0/control_dependency\'],\n     t[\'lstm/lstm_1/control_dependency\'], t[\'softmax_out\'], t[\'class_ids_out\'],\n     t[\'class_weights_out\'], t[\'log_perplexity_out\'], t[\'inputs_in\'],\n     t[\'targets_in\'], t[\'target_weights_in\'], t[\'char_inputs_in\'],\n     t[\'all_embs\'], t[\'softmax_weights\'], t[\'global_step\']\n    ] = tf.import_graph_def(gd, {}, [\'states_init\',\n                                     \'lstm/lstm_0/control_dependency:0\',\n                                     \'lstm/lstm_1/control_dependency:0\',\n                                     \'softmax_out:0\',\n                                     \'class_ids_out:0\',\n                                     \'class_weights_out:0\',\n                                     \'log_perplexity_out:0\',\n                                     \'inputs_in:0\',\n                                     \'targets_in:0\',\n                                     \'target_weights_in:0\',\n                                     \'char_inputs_in:0\',\n                                     \'all_embs_out:0\',\n                                     \'Reshape_3:0\',\n                                     \'global_step:0\'], name=\'\')\n\n    sys.stderr.write(\'Recovering checkpoint %s\\n\' % ckpt_file)\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n    sess.run(\'save/restore_all\', {\'save/Const:0\': ckpt_file})\n    sess.run(t[\'states_init\'])\n\n  return sess, t\n\n\ndef _EvalModel(dataset):\n  """"""Evaluate model perplexity using provided dataset.\n\n  Args:\n    dataset: LM1BDataset object.\n  """"""\n  sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)\n\n  current_step = t[\'global_step\'].eval(session=sess)\n  sys.stderr.write(\'Loaded step %d.\\n\' % current_step)\n\n  data_gen = dataset.get_batch(BATCH_SIZE, NUM_TIMESTEPS, forever=False)\n  sum_num = 0.0\n  sum_den = 0.0\n  perplexity = 0.0\n  for i, (inputs, char_inputs, _, targets, weights) in enumerate(data_gen):\n    input_dict = {t[\'inputs_in\']: inputs,\n                  t[\'targets_in\']: targets,\n                  t[\'target_weights_in\']: weights}\n    if \'char_inputs_in\' in t:\n      input_dict[t[\'char_inputs_in\']] = char_inputs\n    log_perp = sess.run(t[\'log_perplexity_out\'], feed_dict=input_dict)\n\n    if np.isnan(log_perp):\n      sys.stderr.error(\'log_perplexity is Nan.\\n\')\n    else:\n      sum_num += log_perp * weights.mean()\n      sum_den += weights.mean()\n    if sum_den > 0:\n      perplexity = np.exp(sum_num / sum_den)\n\n    sys.stderr.write(\'Eval Step: %d, Average Perplexity: %f.\\n\' %\n                     (i, perplexity))\n\n    if i > FLAGS.max_eval_steps:\n      break\n\n\ndef _SampleSoftmax(softmax):\n  return min(np.sum(np.cumsum(softmax) < np.random.rand()), len(softmax) - 1)\n\n\ndef _SampleModel(prefix_words, vocab):\n  """"""Predict next words using the given prefix words.\n\n  Args:\n    prefix_words: Prefix words.\n    vocab: Vocabulary. Contains max word chard id length and converts between\n        words and ids.\n  """"""\n  targets = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  weights = np.ones([BATCH_SIZE, NUM_TIMESTEPS], np.float32)\n\n  sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)\n\n  if prefix_words.find(\'<S>\') != 0:\n    prefix_words = \'<S> \' + prefix_words\n\n  prefix = [vocab.word_to_id(w) for w in prefix_words.split()]\n  prefix_char_ids = [vocab.word_to_char_ids(w) for w in prefix_words.split()]\n  for _ in xrange(FLAGS.num_samples):\n    inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n    char_ids_inputs = np.zeros(\n        [BATCH_SIZE, NUM_TIMESTEPS, vocab.max_word_length], np.int32)\n    samples = prefix[:]\n    char_ids_samples = prefix_char_ids[:]\n    sent = \'\'\n    while True:\n      inputs[0, 0] = samples[0]\n      char_ids_inputs[0, 0, :] = char_ids_samples[0]\n      samples = samples[1:]\n      char_ids_samples = char_ids_samples[1:]\n\n      softmax = sess.run(t[\'softmax_out\'],\n                         feed_dict={t[\'char_inputs_in\']: char_ids_inputs,\n                                    t[\'inputs_in\']: inputs,\n                                    t[\'targets_in\']: targets,\n                                    t[\'target_weights_in\']: weights})\n\n      sample = _SampleSoftmax(softmax[0])\n      sample_char_ids = vocab.word_to_char_ids(vocab.id_to_word(sample))\n\n      if not samples:\n        samples = [sample]\n        char_ids_samples = [sample_char_ids]\n      sent += vocab.id_to_word(samples[0]) + \' \'\n      sys.stderr.write(\'%s\\n\' % sent)\n\n      if (vocab.id_to_word(samples[0]) == \'</S>\' or\n          len(sent) > FLAGS.max_sample_words):\n        break\n\n\ndef _DumpEmb(vocab):\n  """"""Dump the softmax weights and word embeddings to files.\n\n  Args:\n    vocab: Vocabulary. Contains vocabulary size and converts word to ids.\n  """"""\n  assert FLAGS.save_dir, \'Must specify FLAGS.save_dir for dump_emb.\'\n  inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  targets = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  weights = np.ones([BATCH_SIZE, NUM_TIMESTEPS], np.float32)\n\n  sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)\n\n  softmax_weights = sess.run(t[\'softmax_weights\'])\n  fname = FLAGS.save_dir + \'/embeddings_softmax.npy\'\n  with tf.gfile.Open(fname, mode=\'w\') as f:\n    np.save(f, softmax_weights)\n  sys.stderr.write(\'Finished softmax weights\\n\')\n\n  all_embs = np.zeros([vocab.size, 1024])\n  for i in range(vocab.size):\n    input_dict = {t[\'inputs_in\']: inputs,\n                  t[\'targets_in\']: targets,\n                  t[\'target_weights_in\']: weights}\n    if \'char_inputs_in\' in t:\n      input_dict[t[\'char_inputs_in\']] = (\n          vocab.word_char_ids[i].reshape([-1, 1, MAX_WORD_LEN]))\n    embs = sess.run(t[\'all_embs\'], input_dict)\n    all_embs[i, :] = embs\n    sys.stderr.write(\'Finished word embedding %d/%d\\n\' % (i, vocab.size))\n\n  fname = FLAGS.save_dir + \'/embeddings_char_cnn.npy\'\n  with tf.gfile.Open(fname, mode=\'w\') as f:\n    np.save(f, all_embs)\n  sys.stderr.write(\'Embedding file saved\\n\')\n\n\ndef _DumpSentenceEmbedding(sentence, vocab):\n  """"""Predict next words using the given prefix words.\n\n  Args:\n    sentence: Sentence words.\n    vocab: Vocabulary. Contains max word chard id length and converts between\n        words and ids.\n  """"""\n  targets = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  weights = np.ones([BATCH_SIZE, NUM_TIMESTEPS], np.float32)\n\n  sess, t = _LoadModel(FLAGS.pbtxt, FLAGS.ckpt)\n\n  if sentence.find(\'<S>\') != 0:\n    sentence = \'<S> \' + sentence\n\n  word_ids = [vocab.word_to_id(w) for w in sentence.split()]\n  char_ids = [vocab.word_to_char_ids(w) for w in sentence.split()]\n\n  inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)\n  char_ids_inputs = np.zeros(\n      [BATCH_SIZE, NUM_TIMESTEPS, vocab.max_word_length], np.int32)\n  for i in xrange(len(word_ids)):\n    inputs[0, 0] = word_ids[i]\n    char_ids_inputs[0, 0, :] = char_ids[i]\n\n    # Add \'lstm/lstm_0/control_dependency\' if you want to dump previous layer\n    # LSTM.\n    lstm_emb = sess.run(t[\'lstm/lstm_1/control_dependency\'],\n                        feed_dict={t[\'char_inputs_in\']: char_ids_inputs,\n                                   t[\'inputs_in\']: inputs,\n                                   t[\'targets_in\']: targets,\n                                   t[\'target_weights_in\']: weights})\n\n    fname = os.path.join(FLAGS.save_dir, \'lstm_emb_step_%d.npy\' % i)\n    with tf.gfile.Open(fname, mode=\'w\') as f:\n      np.save(f, lstm_emb)\n    sys.stderr.write(\'LSTM embedding step %d file saved\\n\' % i)\n\n\ndef main(unused_argv):\n  vocab = data_utils.CharsVocabulary(FLAGS.vocab_file, MAX_WORD_LEN)\n\n  if FLAGS.mode == \'eval\':\n    dataset = data_utils.LM1BDataset(FLAGS.input_data, vocab)\n    _EvalModel(dataset)\n  elif FLAGS.mode == \'sample\':\n    _SampleModel(FLAGS.prefix, vocab)\n  elif FLAGS.mode == \'dump_emb\':\n    _DumpEmb(vocab)\n  elif FLAGS.mode == \'dump_lstm_emb\':\n    _DumpSentenceEmbedding(FLAGS.sentence, vocab)\n  else:\n    raise Exception(\'Mode not supported.\')\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/namignizer/data_utils.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Utilities for parsing Kaggle baby names files.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\n\n# the default end of name rep will be zero\n_EON = 0\n\n\ndef read_names(names_path):\n    """"""read data from downloaded file. See SmallNames.txt for example format\n    or go to https://www.kaggle.com/kaggle/us-baby-names for full lists\n\n    Args:\n        names_path: path to the csv file similar to the example type\n    Returns:\n        Dataset: a namedtuple of two elements: deduped names and their associated\n            counts. The names contain only 26 chars and are all lower case\n    """"""\n    names_data = pd.read_csv(names_path)\n    names_data.Name = names_data.Name.str.lower()\n\n    name_data = names_data.groupby(by=[""Name""])[""Count""].sum()\n    name_counts = np.array(name_data.tolist())\n    names_deduped = np.array(name_data.index.tolist())\n\n    Dataset = collections.namedtuple(\'Dataset\', [\'Name\', \'Count\'])\n    return Dataset(names_deduped, name_counts)\n\n\ndef _letter_to_number(letter):\n    """"""converts letters to numbers between 1 and 27""""""\n    # ord of lower case \'a\' is 97\n    return ord(letter) - 96\n\n\ndef namignizer_iterator(names, counts, batch_size, num_steps, epoch_size):\n    """"""Takes a list of names and counts like those output from read_names, and\n    makes an iterator yielding a batch_size by num_steps array of random names\n    separated by an end of name token. The names are choosen randomly according\n    to their counts. The batch may end mid-name\n\n    Args:\n        names: a set of lowercase names composed of 26 characters\n        counts: a list of the frequency of those names\n        batch_size: int\n        num_steps: int\n        epoch_size: number of batches to yield\n    Yields:\n        (x, y): a batch_size by num_steps array of ints representing letters, where\n            x will be the input and y will be the target\n    """"""\n    name_distribution = counts / counts.sum()\n\n    for i in range(epoch_size):\n        data = np.zeros(batch_size * num_steps + 1)\n        samples = np.random.choice(names, size=batch_size * num_steps // 2,\n                                   replace=True, p=name_distribution)\n\n        data_index = 0\n        for sample in samples:\n            if data_index >= batch_size * num_steps:\n                break\n            for letter in map(_letter_to_number, sample) + [_EON]:\n                if data_index >= batch_size * num_steps:\n                    break\n                data[data_index] = letter\n                data_index += 1\n\n        x = data[:batch_size * num_steps].reshape((batch_size, num_steps))\n        y = data[1:batch_size * num_steps + 1].reshape((batch_size, num_steps))\n\n        yield (x, y)\n\n\ndef name_to_batch(name, batch_size, num_steps):\n    """""" Takes a single name and fills a batch with it\n\n    Args:\n        name: lowercase composed of 26 characters\n        batch_size: int\n        num_steps: int\n    Returns:\n        x, y: a batch_size by num_steps array of ints representing letters, where\n            x will be the input and y will be the target. The array is filled up\n            to the length of the string, the rest is filled with zeros\n    """"""\n    data = np.zeros(batch_size * num_steps + 1)\n\n    data_index = 0\n    for letter in map(_letter_to_number, name) + [_EON]:\n        data[data_index] = letter\n        data_index += 1\n\n    x = data[:batch_size * num_steps].reshape((batch_size, num_steps))\n    y = data[1:batch_size * num_steps + 1].reshape((batch_size, num_steps))\n\n    return x, y\n'"
model_zoo/models/namignizer/model.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""RNN model with embeddings""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\nclass NamignizerModel(object):\n    """"""The Namignizer model ~ strongly based on PTB""""""\n\n    def __init__(self, is_training, config):\n        self.batch_size = batch_size = config.batch_size\n        self.num_steps = num_steps = config.num_steps\n        size = config.hidden_size\n        # will always be 27\n        vocab_size = config.vocab_size\n\n        # placeholders for inputs\n        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n        # weights for the loss function\n        self._weights = tf.placeholder(tf.float32, [batch_size * num_steps])\n\n        # lstm for our RNN cell (GRU supported too)\n        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=0.0)\n        if is_training and config.keep_prob < 1:\n            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n                lstm_cell, output_keep_prob=config.keep_prob)\n        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)\n\n        self._initial_state = cell.zero_state(batch_size, tf.float32)\n\n        with tf.device(""/cpu:0""):\n            embedding = tf.get_variable(""embedding"", [vocab_size, size])\n            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n\n        if is_training and config.keep_prob < 1:\n            inputs = tf.nn.dropout(inputs, config.keep_prob)\n\n        outputs = []\n        state = self._initial_state\n        with tf.variable_scope(""RNN""):\n            for time_step in range(num_steps):\n                if time_step > 0:\n                    tf.get_variable_scope().reuse_variables()\n                (cell_output, state) = cell(inputs[:, time_step, :], state)\n                outputs.append(cell_output)\n\n        output = tf.reshape(tf.concat(1, outputs), [-1, size])\n        softmax_w = tf.get_variable(""softmax_w"", [size, vocab_size])\n        softmax_b = tf.get_variable(""softmax_b"", [vocab_size])\n        logits = tf.matmul(output, softmax_w) + softmax_b\n        loss = tf.nn.seq2seq.sequence_loss_by_example(\n            [logits],\n            [tf.reshape(self._targets, [-1])],\n            [self._weights])\n        self._loss = loss\n        self._cost = cost = tf.reduce_sum(loss) / batch_size\n        self._final_state = state\n\n        # probabilities of each letter\n        self._activations = tf.nn.softmax(logits)\n\n        # ability to save the model\n        self.saver = tf.train.Saver(tf.all_variables())\n\n        if not is_training:\n            return\n\n        self._lr = tf.Variable(0.0, trainable=False)\n        tvars = tf.trainable_variables()\n        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n                                          config.max_grad_norm)\n        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n    def assign_lr(self, session, lr_value):\n        session.run(tf.assign(self.lr, lr_value))\n\n    @property\n    def input_data(self):\n        return self._input_data\n\n    @property\n    def targets(self):\n        return self._targets\n\n    @property\n    def activations(self):\n        return self._activations\n\n    @property\n    def weights(self):\n        return self._weights\n\n    @property\n    def initial_state(self):\n        return self._initial_state\n\n    @property\n    def cost(self):\n        return self._cost\n\n    @property\n    def loss(self):\n        return self._loss\n\n    @property\n    def final_state(self):\n        return self._final_state\n\n    @property\n    def lr(self):\n        return self._lr\n\n    @property\n    def train_op(self):\n        return self._train_op\n'"
model_zoo/models/namignizer/names.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""A library showing off sequence recognition and generation with the simple\nexample of names.\n\nWe use recurrent neural nets to learn complex functions able to recogize and\ngenerate sequences of a given form. This can be used for natural language\nsyntax recognition, dynamically generating maps or puzzles and of course\nbaby name generation.\n\nBefore using this module, it is recommended to read the Tensorflow tutorial on\nrecurrent neural nets, as it explains the basic concepts of this model, and\nwill show off another module, the PTB module on which this model bases itself.\n\nHere is an overview of the functions available in this module:\n\n* RNN Module for sequence functions based on PTB\n\n* Name recognition specifically for recognizing names, but can be adapted to\n    recognizing sequence patterns\n\n* Name generations specifically for generating names, but can be adapted to\n    generating arbitrary sequence patterns\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport time\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom model import NamignizerModel\nimport data_utils\n\n\nclass SmallConfig(object):\n    """"""Small config.""""""\n    init_scale = 0.1\n    learning_rate = 1.0\n    max_grad_norm = 5\n    num_layers = 2\n    num_steps = 20\n    hidden_size = 200\n    max_epoch = 4\n    max_max_epoch = 13\n    keep_prob = 1.0\n    lr_decay = 0.5\n    batch_size = 20\n    vocab_size = 27\n    epoch_size = 100\n\n\nclass LargeConfig(object):\n    """"""Medium config.""""""\n    init_scale = 0.05\n    learning_rate = 1.0\n    max_grad_norm = 5\n    num_layers = 2\n    num_steps = 35\n    hidden_size = 650\n    max_epoch = 6\n    max_max_epoch = 39\n    keep_prob = 0.5\n    lr_decay = 0.8\n    batch_size = 20\n    vocab_size = 27\n    epoch_size = 100\n\n\nclass TestConfig(object):\n    """"""Tiny config, for testing.""""""\n    init_scale = 0.1\n    learning_rate = 1.0\n    max_grad_norm = 1\n    num_layers = 1\n    num_steps = 2\n    hidden_size = 2\n    max_epoch = 1\n    max_max_epoch = 1\n    keep_prob = 1.0\n    lr_decay = 0.5\n    batch_size = 20\n    vocab_size = 27\n    epoch_size = 100\n\n\ndef run_epoch(session, m, names, counts, epoch_size, eval_op, verbose=False):\n    """"""Runs the model on the given data for one epoch\n\n    Args:\n        session: the tf session holding the model graph\n        m: an instance of the NamignizerModel\n        names: a set of lowercase names of 26 characters\n        counts: a list of the frequency of the above names\n        epoch_size: the number of batches to run\n        eval_op: whether to change the params or not, and how to do it\n    Kwargs:\n        verbose: whether to print out state of training during the epoch\n    Returns:\n        cost: the average cost during the last stage of the epoch\n    """"""\n    start_time = time.time()\n    costs = 0.0\n    iters = 0\n    for step, (x, y) in enumerate(data_utils.namignizer_iterator(names, counts,\n                                                                 m.batch_size, m.num_steps, epoch_size)):\n\n        cost, _ = session.run([m.cost, eval_op],\n                              {m.input_data: x,\n                               m.targets: y,\n                               m.initial_state: m.initial_state.eval(),\n                               m.weights: np.ones(m.batch_size * m.num_steps)})\n        costs += cost\n        iters += m.num_steps\n\n        if verbose and step % (epoch_size // 10) == 9:\n            print(""%.3f perplexity: %.3f speed: %.0f lps"" %\n                  (step * 1.0 / epoch_size, np.exp(costs / iters),\n                   iters * m.batch_size / (time.time() - start_time)))\n\n        if step >= epoch_size:\n            break\n\n    return np.exp(costs / iters)\n\n\ndef train(data_dir, checkpoint_path, config):\n    """"""Trains the model with the given data\n\n    Args:\n        data_dir: path to the data for the model (see data_utils for data\n            format)\n        checkpoint_path: the path to save the trained model checkpoints\n        config: one of the above configs that specify the model and how it\n            should be run and trained\n    Returns:\n        None\n    """"""\n    # Prepare Name data.\n    print(""Reading Name data in %s"" % data_dir)\n    names, counts = data_utils.read_names(data_dir)\n\n    with tf.Graph().as_default(), tf.Session() as session:\n        initializer = tf.random_uniform_initializer(-config.init_scale,\n                                                    config.init_scale)\n        with tf.variable_scope(""model"", reuse=None, initializer=initializer):\n            m = NamignizerModel(is_training=True, config=config)\n\n        tf.initialize_all_variables().run()\n\n        for i in range(config.max_max_epoch):\n            lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n            m.assign_lr(session, config.learning_rate * lr_decay)\n\n            print(""Epoch: %d Learning rate: %.3f"" % (i + 1, session.run(m.lr)))\n            train_perplexity = run_epoch(session, m, names, counts, config.epoch_size, m.train_op,\n                                         verbose=True)\n            print(""Epoch: %d Train Perplexity: %.3f"" %\n                  (i + 1, train_perplexity))\n\n            m.saver.save(session, checkpoint_path, global_step=i)\n\n\ndef namignize(names, checkpoint_path, config):\n    """"""Recognizes names and prints the Perplexity of the model for each names\n    in the list\n\n    Args:\n        names: a list of names in the model format\n        checkpoint_path: the path to restore the trained model from, should not\n            include the model name, just the path to\n        config: one of the above configs that specify the model and how it\n            should be run and trained\n    Returns:\n        None\n    """"""\n    with tf.Graph().as_default(), tf.Session() as session:\n\n        with tf.variable_scope(""model""):\n            m = NamignizerModel(is_training=False, config=config)\n\n        m.saver.restore(session, checkpoint_path)\n\n        for name in names:\n            x, y = data_utils.name_to_batch(name, m.batch_size, m.num_steps)\n\n            cost, loss, _ = session.run([m.cost, m.loss, tf.no_op()],\n                                  {m.input_data: x,\n                                   m.targets: y,\n                                   m.initial_state: m.initial_state.eval(),\n                                   m.weights: np.concatenate((\n                                       np.ones(len(name)), np.zeros(m.batch_size * m.num_steps - len(name))))})\n\n            print(""Name {} gives us a perplexity of {}"".format(\n                name, np.exp(cost)))\n\n\ndef namignator(checkpoint_path, config):\n    """"""Generates names randomly according to a given model\n\n    Args:\n        checkpoint_path: the path to restore the trained model from, should not\n            include the model name, just the path to\n        config: one of the above configs that specify the model and how it\n            should be run and trained\n    Returns:\n        None\n    """"""\n    # mutate the config to become a name generator config\n    config.num_steps = 1\n    config.batch_size = 1\n\n    with tf.Graph().as_default(), tf.Session() as session:\n\n        with tf.variable_scope(""model""):\n            m = NamignizerModel(is_training=False, config=config)\n\n        m.saver.restore(session, checkpoint_path)\n\n        activations, final_state, _ = session.run([m.activations, m.final_state, tf.no_op()],\n                                                  {m.input_data: np.zeros((1, 1)),\n                                                   m.targets: np.zeros((1, 1)),\n                                                   m.initial_state: m.initial_state.eval(),\n                                                   m.weights: np.ones(1)})\n\n        # sample from our softmax activations\n        next_letter = np.random.choice(27, p=activations[0])\n        name = [next_letter]\n        while next_letter != 0:\n            activations, final_state, _ = session.run([m.activations, m.final_state, tf.no_op()],\n                                                      {m.input_data: [[next_letter]],\n                                                       m.targets: np.zeros((1, 1)),\n                                                       m.initial_state: final_state,\n                                                       m.weights: np.ones(1)})\n\n            next_letter = np.random.choice(27, p=activations[0])\n            name += [next_letter]\n\n        print(map(lambda x: chr(x + 96), name))\n\n\nif __name__ == ""__main__"":\n    # train(""data/SmallNames.txt"", ""model/namignizer"", SmallConfig)\n\n    # namignize([""mary"", ""ida"", ""gazorbazorb"", ""mmmhmm"", ""bob""],\n    #     tf.train.latest_checkpoint(""model""), SmallConfig)\n\n    # namignator(tf.train.latest_checkpoint(""model""), SmallConfig)\n'"
model_zoo/models/neural_gpu/data_utils.py,0,"b'# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Convolutional Gated Recurrent Networks for Algorithm Learning.""""""\n\nimport math\nimport random\nimport sys\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\n\nFLAGS = tf.app.flags.FLAGS\n\nbins = [8, 12, 16, 20, 24, 28, 32, 36, 40, 48, 64, 128]\nall_tasks = [""sort"", ""kvsort"", ""id"", ""rev"", ""rev2"", ""incr"", ""add"", ""left"",\n             ""right"", ""left-shift"", ""right-shift"", ""bmul"", ""mul"", ""dup"",\n             ""badd"", ""qadd"", ""search""]\nforward_max = 128\nlog_filename = """"\n\n\ndef pad(l):\n  for b in bins:\n    if b >= l: return b\n  return forward_max\n\n\ntrain_set = {}\ntest_set = {}\nfor some_task in all_tasks:\n  train_set[some_task] = []\n  test_set[some_task] = []\n  for all_max_len in xrange(10000):\n    train_set[some_task].append([])\n    test_set[some_task].append([])\n\n\ndef add(n1, n2, base=10):\n  """"""Add two numbers represented as lower-endian digit lists.""""""\n  k = max(len(n1), len(n2)) + 1\n  d1 = n1 + [0 for _ in xrange(k - len(n1))]\n  d2 = n2 + [0 for _ in xrange(k - len(n2))]\n  res = []\n  carry = 0\n  for i in xrange(k):\n    if d1[i] + d2[i] + carry < base:\n      res.append(d1[i] + d2[i] + carry)\n      carry = 0\n    else:\n      res.append(d1[i] + d2[i] + carry - base)\n      carry = 1\n  while res and res[-1] == 0:\n    res = res[:-1]\n  if res: return res\n  return [0]\n\n\ndef init_data(task, length, nbr_cases, nclass):\n  """"""Data initialization.""""""\n  def rand_pair(l, task):\n    """"""Random data pair for a task. Total length should be <= l.""""""\n    k = (l-1)/2\n    base = 10\n    if task[0] == ""b"": base = 2\n    if task[0] == ""q"": base = 4\n    d1 = [np.random.randint(base) for _ in xrange(k)]\n    d2 = [np.random.randint(base) for _ in xrange(k)]\n    if task in [""add"", ""badd"", ""qadd""]:\n      res = add(d1, d2, base)\n    elif task in [""mul"", ""bmul""]:\n      d1n = sum([d * (base ** i) for i, d in enumerate(d1)])\n      d2n = sum([d * (base ** i) for i, d in enumerate(d2)])\n      if task == ""bmul"":\n        res = [int(x) for x in list(reversed(str(bin(d1n * d2n))))[:-2]]\n      else:\n        res = [int(x) for x in list(reversed(str(d1n * d2n)))]\n    else:\n      sys.exit()\n    sep = [12]\n    if task in [""add"", ""badd"", ""qadd""]: sep = [11]\n    inp = [d + 1 for d in d1] + sep + [d + 1 for d in d2]\n    return inp, [r + 1 for r in res]\n\n  def rand_dup_pair(l):\n    """"""Random data pair for duplication task. Total length should be <= l.""""""\n    k = l/2\n    x = [np.random.randint(nclass - 1) + 1 for _ in xrange(k)]\n    inp = x + [0 for _ in xrange(l - k)]\n    res = x + x + [0 for _ in xrange(l - 2*k)]\n    return inp, res\n\n  def rand_rev2_pair(l):\n    """"""Random data pair for reverse2 task. Total length should be <= l.""""""\n    inp = [(np.random.randint(nclass - 1) + 1,\n            np.random.randint(nclass - 1) + 1) for _ in xrange(l/2)]\n    res = [i for i in reversed(inp)]\n    return [x for p in inp for x in p], [x for p in res for x in p]\n\n  def rand_search_pair(l):\n    """"""Random data pair for search task. Total length should be <= l.""""""\n    inp = [(np.random.randint(nclass - 1) + 1,\n            np.random.randint(nclass - 1) + 1) for _ in xrange(l-1/2)]\n    q = np.random.randint(nclass - 1) + 1\n    res = 0\n    for (k, v) in reversed(inp):\n      if k == q:\n        res = v\n    return [x for p in inp for x in p] + [q], [res]\n\n  def rand_kvsort_pair(l):\n    """"""Random data pair for key-value sort. Total length should be <= l.""""""\n    keys = [(np.random.randint(nclass - 1) + 1, i) for i in xrange(l/2)]\n    vals = [np.random.randint(nclass - 1) + 1 for _ in xrange(l/2)]\n    kv = [(k, vals[i]) for (k, i) in keys]\n    sorted_kv = [(k, vals[i]) for (k, i) in sorted(keys)]\n    return [x for p in kv for x in p], [x for p in sorted_kv for x in p]\n\n  def spec(inp):\n    """"""Return the target given the input for some tasks.""""""\n    if task == ""sort"":\n      return sorted(inp)\n    elif task == ""id"":\n      return inp\n    elif task == ""rev"":\n      return [i for i in reversed(inp)]\n    elif task == ""incr"":\n      carry = 1\n      res = []\n      for i in xrange(len(inp)):\n        if inp[i] + carry < nclass:\n          res.append(inp[i] + carry)\n          carry = 0\n        else:\n          res.append(1)\n          carry = 1\n      return res\n    elif task == ""left"":\n      return [inp[0]]\n    elif task == ""right"":\n      return [inp[-1]]\n    elif task == ""left-shift"":\n      return [inp[l-1] for l in xrange(len(inp))]\n    elif task == ""right-shift"":\n      return [inp[l+1] for l in xrange(len(inp))]\n    else:\n      print_out(""Unknown spec for task "" + str(task))\n      sys.exit()\n\n  l = length\n  cur_time = time.time()\n  total_time = 0.0\n  for case in xrange(nbr_cases):\n    total_time += time.time() - cur_time\n    cur_time = time.time()\n    if l > 10000 and case % 100 == 1:\n      print_out(""  avg gen time %.4f s"" % (total_time / float(case)))\n    if task in [""add"", ""badd"", ""qadd"", ""bmul"", ""mul""]:\n      i, t = rand_pair(l, task)\n      train_set[task][len(i)].append([i, t])\n      i, t = rand_pair(l, task)\n      test_set[task][len(i)].append([i, t])\n    elif task == ""dup"":\n      i, t = rand_dup_pair(l)\n      train_set[task][len(i)].append([i, t])\n      i, t = rand_dup_pair(l)\n      test_set[task][len(i)].append([i, t])\n    elif task == ""rev2"":\n      i, t = rand_rev2_pair(l)\n      train_set[task][len(i)].append([i, t])\n      i, t = rand_rev2_pair(l)\n      test_set[task][len(i)].append([i, t])\n    elif task == ""search"":\n      i, t = rand_search_pair(l)\n      train_set[task][len(i)].append([i, t])\n      i, t = rand_search_pair(l)\n      test_set[task][len(i)].append([i, t])\n    elif task == ""kvsort"":\n      i, t = rand_kvsort_pair(l)\n      train_set[task][len(i)].append([i, t])\n      i, t = rand_kvsort_pair(l)\n      test_set[task][len(i)].append([i, t])\n    else:\n      inp = [np.random.randint(nclass - 1) + 1 for i in xrange(l)]\n      target = spec(inp)\n      train_set[task][l].append([inp, target])\n      inp = [np.random.randint(nclass - 1) + 1 for i in xrange(l)]\n      target = spec(inp)\n      test_set[task][l].append([inp, target])\n\n\ndef to_symbol(i):\n  """"""Covert ids to text.""""""\n  if i == 0: return """"\n  if i == 11: return ""+""\n  if i == 12: return ""*""\n  return str(i-1)\n\n\ndef to_id(s):\n  """"""Covert text to ids.""""""\n  if s == ""+"": return 11\n  if s == ""*"": return 12\n  return int(s) + 1\n\n\ndef get_batch(max_length, batch_size, do_train, task, offset=None, preset=None):\n  """"""Get a batch of data, training or testing.""""""\n  inputs = []\n  targets = []\n  length = max_length\n  if preset is None:\n    cur_set = test_set[task]\n    if do_train: cur_set = train_set[task]\n    while not cur_set[length]:\n      length -= 1\n  pad_length = pad(length)\n  for b in xrange(batch_size):\n    if preset is None:\n      elem = random.choice(cur_set[length])\n      if offset is not None and offset + b < len(cur_set[length]):\n        elem = cur_set[length][offset + b]\n    else:\n      elem = preset\n    inp, target = elem[0], elem[1]\n    assert len(inp) == length\n    inputs.append(inp + [0 for l in xrange(pad_length - len(inp))])\n    targets.append(target + [0 for l in xrange(pad_length - len(target))])\n  res_input = []\n  res_target = []\n  for l in xrange(pad_length):\n    new_input = np.array([inputs[b][l] for b in xrange(batch_size)],\n                         dtype=np.int32)\n    new_target = np.array([targets[b][l] for b in xrange(batch_size)],\n                          dtype=np.int32)\n    res_input.append(new_input)\n    res_target.append(new_target)\n  return res_input, res_target\n\n\ndef print_out(s, newline=True):\n  """"""Print a message out and log it to file.""""""\n  if log_filename:\n    try:\n      with gfile.GFile(log_filename, mode=""a"") as f:\n        f.write(s + (""\\n"" if newline else """"))\n    # pylint: disable=bare-except\n    except:\n      sys.stdout.write(""Error appending to %s\\n"" % log_filename)\n  sys.stdout.write(s + (""\\n"" if newline else """"))\n  sys.stdout.flush()\n\n\ndef decode(output):\n  return [np.argmax(o, axis=1) for o in output]\n\n\ndef accuracy(inpt, output, target, batch_size, nprint):\n  """"""Calculate output accuracy given target.""""""\n  assert nprint < batch_size + 1\n  def task_print(inp, output, target):\n    stop_bound = 0\n    print_len = 0\n    while print_len < len(target) and target[print_len] > stop_bound:\n      print_len += 1\n    print_out(""    i: "" + "" "".join([str(i - 1) for i in inp if i > 0]))\n    print_out(""    o: "" +\n              "" "".join([str(output[l] - 1) for l in xrange(print_len)]))\n    print_out(""    t: "" +\n              "" "".join([str(target[l] - 1) for l in xrange(print_len)]))\n  decoded_target = target\n  decoded_output = decode(output)\n  total = 0\n  errors = 0\n  seq = [0 for b in xrange(batch_size)]\n  for l in xrange(len(decoded_output)):\n    for b in xrange(batch_size):\n      if decoded_target[l][b] > 0:\n        total += 1\n        if decoded_output[l][b] != decoded_target[l][b]:\n          seq[b] = 1\n          errors += 1\n  e = 0  # Previous error index\n  for _ in xrange(min(nprint, sum(seq))):\n    while seq[e] == 0:\n      e += 1\n    task_print([inpt[l][e] for l in xrange(len(inpt))],\n               [decoded_output[l][e] for l in xrange(len(decoded_target))],\n               [decoded_target[l][e] for l in xrange(len(decoded_target))])\n    e += 1\n  for b in xrange(nprint - errors):\n    task_print([inpt[l][b] for l in xrange(len(inpt))],\n               [decoded_output[l][b] for l in xrange(len(decoded_target))],\n               [decoded_target[l][b] for l in xrange(len(decoded_target))])\n  return errors, total, sum(seq)\n\n\ndef safe_exp(x):\n  perp = 10000\n  if x < 100: perp = math.exp(x)\n  if perp > 10000: return 10000\n  return perp\n'"
model_zoo/models/neural_gpu/neural_gpu.py,0,"b'# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The Neural GPU Model.""""""\n\nimport time\n\nimport tensorflow as tf\n\nimport data_utils\n\n\ndef conv_linear(args, kw, kh, nin, nout, do_bias, bias_start, prefix):\n  """"""Convolutional linear map.""""""\n  assert args is not None\n  if not isinstance(args, (list, tuple)):\n    args = [args]\n  with tf.variable_scope(prefix):\n    k = tf.get_variable(""CvK"", [kw, kh, nin, nout])\n    if len(args) == 1:\n      res = tf.nn.conv2d(args[0], k, [1, 1, 1, 1], ""SAME"")\n    else:\n      res = tf.nn.conv2d(tf.concat(3, args), k, [1, 1, 1, 1], ""SAME"")\n    if not do_bias: return res\n    bias_term = tf.get_variable(""CvB"", [nout],\n                                initializer=tf.constant_initializer(0.0))\n    return res + bias_term + bias_start\n\n\ndef sigmoid_cutoff(x, cutoff):\n  """"""Sigmoid with cutoff, e.g., 1.2sigmoid(x) - 0.1.""""""\n  y = tf.sigmoid(x)\n  if cutoff < 1.01: return y\n  d = (cutoff - 1.0) / 2.0\n  return tf.minimum(1.0, tf.maximum(0.0, cutoff * y - d))\n\n\ndef tanh_cutoff(x, cutoff):\n  """"""Tanh with cutoff, e.g., 1.1tanh(x) cut to [-1. 1].""""""\n  y = tf.tanh(x)\n  if cutoff < 1.01: return y\n  d = (cutoff - 1.0) / 2.0\n  return tf.minimum(1.0, tf.maximum(-1.0, (1.0 + d) * y))\n\n\ndef conv_gru(inpts, mem, kw, kh, nmaps, cutoff, prefix):\n  """"""Convolutional GRU.""""""\n  def conv_lin(args, suffix, bias_start):\n    return conv_linear(args, kw, kh, len(args) * nmaps, nmaps, True, bias_start,\n                       prefix + ""/"" + suffix)\n  reset = sigmoid_cutoff(conv_lin(inpts + [mem], ""r"", 1.0), cutoff)\n  # candidate = tanh_cutoff(conv_lin(inpts + [reset * mem], ""c"", 0.0), cutoff)\n  candidate = tf.tanh(conv_lin(inpts + [reset * mem], ""c"", 0.0))\n  gate = sigmoid_cutoff(conv_lin(inpts + [mem], ""g"", 1.0), cutoff)\n  return gate * mem + (1 - gate) * candidate\n\n\n@tf.RegisterGradient(""CustomIdG"")\ndef _custom_id_grad(_, grads):\n  return grads\n\n\ndef quantize(t, quant_scale, max_value=1.0):\n  """"""Quantize a tensor t with each element in [-max_value, max_value].""""""\n  t = tf.minimum(max_value, tf.maximum(t, -max_value))\n  big = quant_scale * (t + max_value) + 0.5\n  with tf.get_default_graph().gradient_override_map({""Floor"": ""CustomIdG""}):\n    res = (tf.floor(big) / quant_scale) - max_value\n  return res\n\n\ndef quantize_weights_op(quant_scale, max_value):\n  ops = [v.assign(quantize(v, quant_scale, float(max_value)))\n         for v in tf.trainable_variables()]\n  return tf.group(*ops)\n\n\ndef relaxed_average(var_name_suffix, rx_step):\n  """"""Calculate the average of relaxed variables having var_name_suffix.""""""\n  relaxed_vars = []\n  for l in xrange(rx_step):\n    with tf.variable_scope(""RX%d"" % l, reuse=True):\n      try:\n        relaxed_vars.append(tf.get_variable(var_name_suffix))\n      except ValueError:\n        pass\n  dsum = tf.add_n(relaxed_vars)\n  avg = dsum / len(relaxed_vars)\n  diff = [v - avg for v in relaxed_vars]\n  davg = tf.add_n([d*d for d in diff])\n  return avg, tf.reduce_sum(davg)\n\n\ndef relaxed_distance(rx_step):\n  """"""Distance between relaxed variables and their average.""""""\n  res, ops, rx_done = [], [], {}\n  for v in tf.trainable_variables():\n    if v.name[0:2] == ""RX"":\n      rx_name = v.op.name[v.name.find(""/"") + 1:]\n      if rx_name not in rx_done:\n        avg, dist_loss = relaxed_average(rx_name, rx_step)\n        res.append(dist_loss)\n        rx_done[rx_name] = avg\n      ops.append(v.assign(rx_done[rx_name]))\n  return tf.add_n(res), tf.group(*ops)\n\n\ndef make_dense(targets, noclass):\n  """"""Move a batch of targets to a dense 1-hot representation.""""""\n  with tf.device(""/cpu:0""):\n    shape = tf.shape(targets)\n    batch_size = shape[0]\n    indices = targets + noclass * tf.range(0, batch_size)\n    length = tf.expand_dims(batch_size * noclass, 0)\n    dense = tf.sparse_to_dense(indices, length, 1.0, 0.0)\n  return tf.reshape(dense, [-1, noclass])\n\n\ndef check_for_zero(sparse):\n  """"""In a sparse batch of ints, make 1.0 if it\'s 0 and 0.0 else.""""""\n  with tf.device(""/cpu:0""):\n    shape = tf.shape(sparse)\n    batch_size = shape[0]\n    sparse = tf.minimum(sparse, 1)\n    indices = sparse + 2 * tf.range(0, batch_size)\n    dense = tf.sparse_to_dense(indices, tf.expand_dims(2 * batch_size, 0),\n                               1.0, 0.0)\n    reshaped = tf.reshape(dense, [-1, 2])\n  return tf.reshape(tf.slice(reshaped, [0, 0], [-1, 1]), [-1])\n\n\nclass NeuralGPU(object):\n  """"""Neural GPU Model.""""""\n\n  def __init__(self, nmaps, vec_size, niclass, noclass, dropout, rx_step,\n               max_grad_norm, cutoff, nconvs, kw, kh, height, mode,\n               learning_rate, pull, pull_incr, min_length, act_noise=0.0):\n    # Feeds for parameters and ops to update them.\n    self.global_step = tf.Variable(0, trainable=False)\n    self.cur_length = tf.Variable(min_length, trainable=False)\n    self.cur_length_incr_op = self.cur_length.assign_add(1)\n    self.lr = tf.Variable(float(learning_rate), trainable=False)\n    self.lr_decay_op = self.lr.assign(self.lr * 0.98)\n    self.pull = tf.Variable(float(pull), trainable=False)\n    self.pull_incr_op = self.pull.assign(self.pull * pull_incr)\n    self.do_training = tf.placeholder(tf.float32, name=""do_training"")\n    self.noise_param = tf.placeholder(tf.float32, name=""noise_param"")\n\n    # Feeds for inputs, targets, outputs, losses, etc.\n    self.input = []\n    self.target = []\n    for l in xrange(data_utils.forward_max + 1):\n      self.input.append(tf.placeholder(tf.int32, name=""inp{0}"".format(l)))\n      self.target.append(tf.placeholder(tf.int32, name=""tgt{0}"".format(l)))\n    self.outputs = []\n    self.losses = []\n    self.grad_norms = []\n    self.updates = []\n\n    # Computation.\n    inp0_shape = tf.shape(self.input[0])\n    batch_size = inp0_shape[0]\n    with tf.device(""/cpu:0""):\n      emb_weights = tf.get_variable(\n          ""embedding"", [niclass, vec_size],\n          initializer=tf.random_uniform_initializer(-1.7, 1.7))\n      e0 = tf.scatter_update(emb_weights,\n                             tf.constant(0, dtype=tf.int32, shape=[1]),\n                             tf.zeros([1, vec_size]))\n\n    adam = tf.train.AdamOptimizer(self.lr, epsilon=1e-4)\n\n    # Main graph creation loop, for every bin in data_utils.\n    self.steps = []\n    for length in sorted(list(set(data_utils.bins + [data_utils.forward_max]))):\n      data_utils.print_out(""Creating model for bin of length %d."" % length)\n      start_time = time.time()\n      if length > data_utils.bins[0]:\n        tf.get_variable_scope().reuse_variables()\n\n      # Embed inputs and calculate mask.\n      with tf.device(""/cpu:0""):\n        with tf.control_dependencies([e0]):\n          embedded = [tf.nn.embedding_lookup(emb_weights, self.input[l])\n                      for l in xrange(length)]\n        # Mask to 0-out padding space in each step.\n        imask = [check_for_zero(self.input[l]) for l in xrange(length)]\n        omask = [check_for_zero(self.target[l]) for l in xrange(length)]\n        mask = [1.0 - (imask[i] * omask[i]) for i in xrange(length)]\n        mask = [tf.reshape(m, [-1, 1]) for m in mask]\n        # Use a shifted mask for step scaling and concatenated for weights.\n        shifted_mask = mask + [tf.zeros_like(mask[0])]\n        scales = [shifted_mask[i] * (1.0 - shifted_mask[i+1])\n                  for i in xrange(length)]\n        scales = [tf.reshape(s, [-1, 1, 1, 1]) for s in scales]\n        mask = tf.concat(1, mask[0:length])  # batch x length\n        weights = mask\n        # Add a height dimension to mask to use later for masking.\n        mask = tf.reshape(mask, [-1, length, 1, 1])\n        mask = tf.concat(2, [mask for _ in xrange(height)]) + tf.zeros(\n            tf.pack([batch_size, length, height, nmaps]), dtype=tf.float32)\n\n      # Start is a length-list of batch-by-nmaps tensors, reshape and concat.\n      start = [tf.tanh(embedded[l]) for l in xrange(length)]\n      start = [tf.reshape(start[l], [-1, 1, nmaps]) for l in xrange(length)]\n      start = tf.reshape(tf.concat(1, start), [-1, length, 1, nmaps])\n\n      # First image comes from start by applying one convolution and adding 0s.\n      first = conv_linear(start, 1, 1, vec_size, nmaps, True, 0.0, ""input"")\n      first = [first] + [tf.zeros(tf.pack([batch_size, length, 1, nmaps]),\n                                  dtype=tf.float32) for _ in xrange(height - 1)]\n      first = tf.concat(2, first)\n\n      # Computation steps.\n      keep_prob = 1.0 - self.do_training * (dropout * 8.0 / float(length))\n      step = [tf.nn.dropout(first, keep_prob) * mask]\n      act_noise_scale = act_noise * self.do_training * self.pull\n      outputs = []\n      for it in xrange(length):\n        with tf.variable_scope(""RX%d"" % (it % rx_step)) as vs:\n          if it >= rx_step:\n            vs.reuse_variables()\n          cur = step[it]\n          # Do nconvs-many CGRU steps.\n          for layer in xrange(nconvs):\n            cur = conv_gru([], cur, kw, kh, nmaps, cutoff, ""cgru_%d"" % layer)\n            cur *= mask\n          outputs.append(tf.slice(cur, [0, 0, 0, 0], [-1, -1, 1, -1]))\n          cur = tf.nn.dropout(cur, keep_prob)\n          if act_noise > 0.00001:\n            cur += tf.truncated_normal(tf.shape(cur)) * act_noise_scale\n          step.append(cur * mask)\n\n      self.steps.append([tf.reshape(s, [-1, length, height * nmaps])\n                         for s in step])\n      # Output is the n-th step output; n = current length, as in scales.\n      output = tf.add_n([outputs[i] * scales[i] for i in xrange(length)])\n      # Final convolution to get logits, list outputs.\n      output = conv_linear(output, 1, 1, nmaps, noclass, True, 0.0, ""output"")\n      output = tf.reshape(output, [-1, length, noclass])\n      external_output = [tf.reshape(o, [-1, noclass])\n                         for o in list(tf.split(1, length, output))]\n      external_output = [tf.nn.softmax(o) for o in external_output]\n      self.outputs.append(external_output)\n\n      # Calculate cross-entropy loss and normalize it.\n      targets = tf.concat(1, [make_dense(self.target[l], noclass)\n                              for l in xrange(length)])\n      targets = tf.reshape(targets, [-1, noclass])\n      xent = tf.reshape(tf.nn.softmax_cross_entropy_with_logits(\n          tf.reshape(output, [-1, noclass]), targets), [-1, length])\n      perp_loss = tf.reduce_sum(xent * weights)\n      perp_loss /= tf.cast(batch_size, dtype=tf.float32)\n      perp_loss /= length\n\n      # Final loss: cross-entropy + shared parameter relaxation part.\n      relax_dist, self.avg_op = relaxed_distance(rx_step)\n      total_loss = perp_loss + relax_dist * self.pull\n      self.losses.append(perp_loss)\n\n      # Gradients and Adam update operation.\n      if length == data_utils.bins[0] or (mode == 0 and\n                                          length < data_utils.bins[-1] + 1):\n        data_utils.print_out(""Creating backward for bin of length %d."" % length)\n        params = tf.trainable_variables()\n        grads = tf.gradients(total_loss, params)\n        grads, norm = tf.clip_by_global_norm(grads, max_grad_norm)\n        self.grad_norms.append(norm)\n        for grad in grads:\n          if isinstance(grad, tf.Tensor):\n            grad += tf.truncated_normal(tf.shape(grad)) * self.noise_param\n        update = adam.apply_gradients(zip(grads, params),\n                                      global_step=self.global_step)\n        self.updates.append(update)\n      data_utils.print_out(""Created model for bin of length %d in""\n                           "" %.2f s."" % (length, time.time() - start_time))\n    self.saver = tf.train.Saver(tf.all_variables())\n\n  def step(self, sess, inp, target, do_backward, noise_param=None,\n           get_steps=False):\n    """"""Run a step of the network.""""""\n    assert len(inp) == len(target)\n    length = len(target)\n    feed_in = {}\n    feed_in[self.noise_param.name] = noise_param if noise_param else 0.0\n    feed_in[self.do_training.name] = 1.0 if do_backward else 0.0\n    feed_out = []\n    index = len(data_utils.bins)\n    if length < data_utils.bins[-1] + 1:\n      index = data_utils.bins.index(length)\n    if do_backward:\n      feed_out.append(self.updates[index])\n      feed_out.append(self.grad_norms[index])\n    feed_out.append(self.losses[index])\n    for l in xrange(length):\n      feed_in[self.input[l].name] = inp[l]\n    for l in xrange(length):\n      feed_in[self.target[l].name] = target[l]\n      feed_out.append(self.outputs[index][l])\n    if get_steps:\n      for l in xrange(length+1):\n        feed_out.append(self.steps[index][l])\n    res = sess.run(feed_out, feed_in)\n    offset = 0\n    norm = None\n    if do_backward:\n      offset = 2\n      norm = res[1]\n    outputs = res[offset + 1:offset + 1 + length]\n    steps = res[offset + 1 + length:] if get_steps else None\n    return res[offset], outputs, norm, steps\n'"
model_zoo/models/neural_gpu/neural_gpu_trainer.py,0,"b'# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Neural GPU for Learning Algorithms.""""""\n\nimport math\nimport os\nimport random\nimport sys\nimport time\n\nimport matplotlib.animation as anim\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\n\nimport data_utils as data\nimport neural_gpu\n\ntf.app.flags.DEFINE_float(""lr"", 0.001, ""Learning rate."")\ntf.app.flags.DEFINE_float(""init_weight"", 1.0, ""Initial weights deviation."")\ntf.app.flags.DEFINE_float(""max_grad_norm"", 1.0, ""Clip gradients to this norm."")\ntf.app.flags.DEFINE_float(""cutoff"", 1.2, ""Cutoff at the gates."")\ntf.app.flags.DEFINE_float(""pull"", 0.0005, ""Starting pull of the relaxations."")\ntf.app.flags.DEFINE_float(""pull_incr"", 1.2, ""Increase pull by that much."")\ntf.app.flags.DEFINE_float(""curriculum_bound"", 0.15, ""Move curriculum < this."")\ntf.app.flags.DEFINE_float(""dropout"", 0.15, ""Dropout that much."")\ntf.app.flags.DEFINE_float(""grad_noise_scale"", 0.0, ""Gradient noise scale."")\ntf.app.flags.DEFINE_integer(""batch_size"", 32, ""Batch size."")\ntf.app.flags.DEFINE_integer(""low_batch_size"", 16, ""Low batch size."")\ntf.app.flags.DEFINE_integer(""steps_per_checkpoint"", 200, ""Steps per epoch."")\ntf.app.flags.DEFINE_integer(""nmaps"", 128, ""Number of floats in each cell."")\ntf.app.flags.DEFINE_integer(""niclass"", 33, ""Number of classes (0 is padding)."")\ntf.app.flags.DEFINE_integer(""noclass"", 33, ""Number of classes (0 is padding)."")\ntf.app.flags.DEFINE_integer(""train_data_size"", 5000, ""Training examples/len."")\ntf.app.flags.DEFINE_integer(""max_length"", 41, ""Maximum length."")\ntf.app.flags.DEFINE_integer(""rx_step"", 6, ""Relax that many recursive steps."")\ntf.app.flags.DEFINE_integer(""random_seed"", 125459, ""Random seed."")\ntf.app.flags.DEFINE_integer(""nconvs"", 2, ""How many convolutions / 1 step."")\ntf.app.flags.DEFINE_integer(""kw"", 3, ""Kernel width."")\ntf.app.flags.DEFINE_integer(""kh"", 3, ""Kernel height."")\ntf.app.flags.DEFINE_integer(""height"", 4, ""Height."")\ntf.app.flags.DEFINE_integer(""forward_max"", 401, ""Maximum forward length."")\ntf.app.flags.DEFINE_integer(""jobid"", -1, ""Task id when running on borg."")\ntf.app.flags.DEFINE_integer(""nprint"", 0, ""How many test examples to print out."")\ntf.app.flags.DEFINE_integer(""mode"", 0, ""Mode: 0-train other-decode."")\ntf.app.flags.DEFINE_bool(""animate"", False, ""Whether to produce an animation."")\ntf.app.flags.DEFINE_bool(""quantize"", False, ""Whether to quantize variables."")\ntf.app.flags.DEFINE_string(""task"", ""rev"", ""Which task are we learning?"")\ntf.app.flags.DEFINE_string(""train_dir"", ""/tmp/"", ""Directory to store models."")\ntf.app.flags.DEFINE_string(""ensemble"", """", ""Model paths for ensemble."")\n\nFLAGS = tf.app.flags.FLAGS\nEXTRA_EVAL = 12\n\n\ndef initialize(sess):\n  """"""Initialize data and model.""""""\n  if FLAGS.jobid >= 0:\n    data.log_filename = os.path.join(FLAGS.train_dir, ""log%d"" % FLAGS.jobid)\n  data.print_out(""NN "", newline=False)\n\n  # Set random seed.\n  seed = FLAGS.random_seed + max(0, FLAGS.jobid)\n  tf.set_random_seed(seed)\n  random.seed(seed)\n  np.random.seed(seed)\n\n  # Check data sizes.\n  assert data.bins\n  min_length = 3\n  max_length = min(FLAGS.max_length, data.bins[-1])\n  assert max_length + 1 > min_length\n  while len(data.bins) > 1 and data.bins[-2] > max_length + EXTRA_EVAL:\n    data.bins = data.bins[:-1]\n  assert data.bins[0] > FLAGS.rx_step\n  data.forward_max = max(FLAGS.forward_max, data.bins[-1])\n  nclass = min(FLAGS.niclass, FLAGS.noclass)\n  data_size = FLAGS.train_data_size if FLAGS.mode == 0 else 1000\n\n  # Initialize data for each task.\n  tasks = FLAGS.task.split(""-"")\n  for t in tasks:\n    for l in xrange(max_length + EXTRA_EVAL - 1):\n      data.init_data(t, l, data_size, nclass)\n    data.init_data(t, data.bins[-2], data_size, nclass)\n    data.init_data(t, data.bins[-1], data_size, nclass)\n    end_size = 4 * 1024 if FLAGS.mode > 0 else 1024\n    data.init_data(t, data.forward_max, end_size, nclass)\n\n  # Print out parameters.\n  curriculum = FLAGS.curriculum_bound\n  msg1 = (""layers %d kw %d h %d kh %d relax %d batch %d noise %.2f task %s""\n          % (FLAGS.nconvs, FLAGS.kw, FLAGS.height, FLAGS.kh, FLAGS.rx_step,\n             FLAGS.batch_size, FLAGS.grad_noise_scale, FLAGS.task))\n  msg2 = ""data %d %s"" % (FLAGS.train_data_size, msg1)\n  msg3 = (""cut %.2f pull %.3f lr %.2f iw %.2f cr %.2f nm %d d%.4f gn %.2f %s"" %\n          (FLAGS.cutoff, FLAGS.pull_incr, FLAGS.lr, FLAGS.init_weight,\n           curriculum, FLAGS.nmaps, FLAGS.dropout, FLAGS.max_grad_norm, msg2))\n  data.print_out(msg3)\n\n  # Create checkpoint directory if it does not exist.\n  checkpoint_dir = os.path.join(FLAGS.train_dir, ""neural_gpu%s""\n                                % ("""" if FLAGS.jobid < 0 else str(FLAGS.jobid)))\n  if not gfile.IsDirectory(checkpoint_dir):\n    data.print_out(""Creating checkpoint directory %s."" % checkpoint_dir)\n    gfile.MkDir(checkpoint_dir)\n\n  # Create model and initialize it.\n  tf.get_variable_scope().set_initializer(\n      tf.uniform_unit_scaling_initializer(factor=1.8 * FLAGS.init_weight))\n  model = neural_gpu.NeuralGPU(\n      FLAGS.nmaps, FLAGS.nmaps, FLAGS.niclass, FLAGS.noclass, FLAGS.dropout,\n      FLAGS.rx_step, FLAGS.max_grad_norm, FLAGS.cutoff, FLAGS.nconvs,\n      FLAGS.kw, FLAGS.kh, FLAGS.height, FLAGS.mode, FLAGS.lr,\n      FLAGS.pull, FLAGS.pull_incr, min_length + 3)\n  data.print_out(""Created model."")\n  sess.run(tf.initialize_all_variables())\n  data.print_out(""Initialized variables."")\n\n  # Load model from parameters if a checkpoint exists.\n  ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n  if ckpt and gfile.Exists(ckpt.model_checkpoint_path):\n    data.print_out(""Reading model parameters from %s""\n                   % ckpt.model_checkpoint_path)\n    model.saver.restore(sess, ckpt.model_checkpoint_path)\n\n  # Check if there are ensemble models and get their checkpoints.\n  ensemble = []\n  ensemble_dir_list = [d for d in FLAGS.ensemble.split("","") if d]\n  for ensemble_dir in ensemble_dir_list:\n    ckpt = tf.train.get_checkpoint_state(ensemble_dir)\n    if ckpt and gfile.Exists(ckpt.model_checkpoint_path):\n      data.print_out(""Found ensemble model %s"" % ckpt.model_checkpoint_path)\n      ensemble.append(ckpt.model_checkpoint_path)\n\n  # Return the model and needed variables.\n  return (model, min_length, max_length, checkpoint_dir, curriculum, ensemble)\n\n\ndef single_test(l, model, sess, task, nprint, batch_size, print_out=True,\n                offset=None, ensemble=None, get_steps=False):\n  """"""Test model on test data of length l using the given session.""""""\n  inpt, target = data.get_batch(l, batch_size, False, task, offset)\n  _, res, _, steps = model.step(sess, inpt, target, False, get_steps=get_steps)\n  errors, total, seq_err = data.accuracy(inpt, res, target, batch_size, nprint)\n  seq_err = float(seq_err) / batch_size\n  if total > 0:\n    errors = float(errors) / total\n  if print_out:\n    data.print_out(""  %s len %d errors %.2f sequence-errors %.2f""\n                   % (task, l, 100*errors, 100*seq_err))\n  # Ensemble eval.\n  if ensemble:\n    results = []\n    for m in ensemble:\n      model.saver.restore(sess, m)\n      _, result, _, _ = model.step(sess, inpt, target, False)\n      m_errors, m_total, m_seq_err = data.accuracy(inpt, result, target,\n                                                   batch_size, nprint)\n      m_seq_err = float(m_seq_err) / batch_size\n      if total > 0:\n        m_errors = float(m_errors) / m_total\n      data.print_out(""     %s len %d m-errors %.2f m-sequence-errors %.2f""\n                     % (task, l, 100*m_errors, 100*m_seq_err))\n      results.append(result)\n    ens = [sum(o) for o in zip(*results)]\n    errors, total, seq_err = data.accuracy(inpt, ens, target,\n                                           batch_size, nprint)\n    seq_err = float(seq_err) / batch_size\n    if total > 0:\n      errors = float(errors) / total\n    if print_out:\n      data.print_out(""  %s len %d ens-errors %.2f ens-sequence-errors %.2f""\n                     % (task, l, 100*errors, 100*seq_err))\n  return errors, seq_err, (steps, inpt, [np.argmax(o, axis=1) for o in res])\n\n\ndef multi_test(l, model, sess, task, nprint, batch_size, offset=None,\n               ensemble=None):\n  """"""Run multiple tests at lower batch size to save memory.""""""\n  errors, seq_err = 0.0, 0.0\n  to_print = nprint\n  low_batch = FLAGS.low_batch_size\n  low_batch = min(low_batch, batch_size)\n  for mstep in xrange(batch_size / low_batch):\n    cur_offset = None if offset is None else offset + mstep * low_batch\n    err, sq_err, _ = single_test(l, model, sess, task, to_print, low_batch,\n                                 False, cur_offset, ensemble=ensemble)\n    to_print = max(0, to_print - low_batch)\n    errors += err\n    seq_err += sq_err\n    if FLAGS.mode > 0:\n      cur_errors = float(low_batch * errors) / ((mstep+1) * low_batch)\n      cur_seq_err = float(low_batch * seq_err) / ((mstep+1) * low_batch)\n      data.print_out(""    %s multitest current errors %.2f sequence-errors %.2f""\n                     % (task, 100*cur_errors, 100*cur_seq_err))\n  errors = float(low_batch) * float(errors) / batch_size\n  seq_err = float(low_batch) * float(seq_err) / batch_size\n  data.print_out(""  %s len %d errors %.2f sequence-errors %.2f""\n                 % (task, l, 100*errors, 100*seq_err))\n  return errors, seq_err\n\n\ndef train():\n  """"""Train the model.""""""\n  batch_size = FLAGS.batch_size\n  tasks = FLAGS.task.split(""-"")\n  with tf.Session() as sess:\n    (model, min_length, max_length, checkpoint_dir,\n     curriculum, _) = initialize(sess)\n    quant_op = neural_gpu.quantize_weights_op(512, 8)\n    max_cur_length = min(min_length + 3, max_length)\n    prev_acc_perp = [1000000 for _ in xrange(3)]\n    prev_seq_err = 1.0\n\n    # Main traning loop.\n    while True:\n      global_step, pull, max_cur_length, learning_rate = sess.run(\n          [model.global_step, model.pull, model.cur_length, model.lr])\n      acc_loss, acc_total, acc_errors, acc_seq_err = 0.0, 0, 0, 0\n      acc_grad_norm, step_count, step_time = 0.0, 0, 0.0\n      for _ in xrange(FLAGS.steps_per_checkpoint):\n        global_step += 1\n        task = random.choice(tasks)\n\n        # Select the length for curriculum learning.\n        l = np.random.randint(max_cur_length - min_length + 1) + min_length\n        # Prefer longer stuff 60% of time.\n        if np.random.randint(100) < 60:\n          l1 = np.random.randint(max_cur_length - min_length+1) + min_length\n          l = max(l, l1)\n        # Mixed curriculum learning: in 25% of cases go to any larger length.\n        if np.random.randint(100) < 25:\n          l1 = np.random.randint(max_length - min_length + 1) + min_length\n          l = max(l, l1)\n\n        # Run a step and time it.\n        start_time = time.time()\n        inp, target = data.get_batch(l, batch_size, True, task)\n        noise_param = math.sqrt(math.pow(global_step, -0.55) *\n                                prev_seq_err) * FLAGS.grad_noise_scale\n        loss, res, gnorm, _ = model.step(sess, inp, target, True, noise_param)\n        step_time += time.time() - start_time\n        acc_grad_norm += float(gnorm)\n\n        # Accumulate statistics only if we did not exceed curriculum length.\n        if l < max_cur_length + 1:\n          step_count += 1\n          acc_loss += loss\n          errors, total, seq_err = data.accuracy(inp, res, target,\n                                                 batch_size, 0)\n          acc_total += total\n          acc_errors += errors\n          acc_seq_err += seq_err\n\n      # Normalize and print out accumulated statistics.\n      acc_loss /= step_count\n      step_time /= FLAGS.steps_per_checkpoint\n      acc_seq_err = float(acc_seq_err) / (step_count * batch_size)\n      prev_seq_err = max(0.0, acc_seq_err - 0.02)  # No noise at error < 2%.\n      acc_errors = float(acc_errors) / acc_total if acc_total > 0 else 1.0\n      msg1 = ""step %d step-time %.2f"" % (global_step, step_time)\n      msg2 = ""lr %.8f pull %.3f"" % (learning_rate, pull)\n      msg3 = (""%s %s grad-norm %.8f""\n              % (msg1, msg2, acc_grad_norm / FLAGS.steps_per_checkpoint))\n      data.print_out(""%s len %d ppx %.8f errors %.2f sequence-errors %.2f"" %\n                     (msg3, max_cur_length, data.safe_exp(acc_loss),\n                      100*acc_errors, 100*acc_seq_err))\n\n      # If errors are below the curriculum threshold, move curriculum forward.\n      if curriculum > acc_seq_err:\n        if FLAGS.quantize:\n          # Quantize weights.\n          data.print_out(""  Quantizing parameters."")\n          sess.run([quant_op])\n        # Increase current length (until the next with training data).\n        do_incr = True\n        while do_incr and max_cur_length < max_length:\n          sess.run(model.cur_length_incr_op)\n          for t in tasks:\n            if data.train_set[t]: do_incr = False\n        # Forget last perplexities if we\'re not yet at the end.\n        if max_cur_length < max_length:\n          prev_acc_perp.append(1000000)\n        # Either increase pull or, if it\'s large, average parameters.\n        if pull < 0.1:\n          sess.run(model.pull_incr_op)\n        else:\n          data.print_out(""  Averaging parameters."")\n          sess.run(model.avg_op)\n          if acc_seq_err < (curriculum / 3.0):\n            sess.run(model.lr_decay_op)\n\n      # Lower learning rate if we\'re worse than the last 3 checkpoints.\n      acc_perp = data.safe_exp(acc_loss)\n      if acc_perp > max(prev_acc_perp[-3:]):\n        sess.run(model.lr_decay_op)\n      prev_acc_perp.append(acc_perp)\n\n      # Save checkpoint.\n      checkpoint_path = os.path.join(checkpoint_dir, ""neural_gpu.ckpt"")\n      model.saver.save(sess, checkpoint_path,\n                       global_step=model.global_step)\n\n      # Run evaluation.\n      bound = data.bins[-1] + 1\n      for t in tasks:\n        l = min_length\n        while l < max_length + EXTRA_EVAL and l < bound:\n          _, seq_err, _ = single_test(l, model, sess, t,\n                                      FLAGS.nprint, batch_size)\n          l += 1\n          while l < bound + 1 and not data.test_set[t][l]:\n            l += 1\n        if seq_err < 0.05:  # Run larger test if we\'re good enough.\n          _, seq_err = multi_test(data.forward_max, model, sess, t,\n                                  FLAGS.nprint, batch_size * 4)\n      if seq_err < 0.01:  # Super-large test on 1-task large-forward models.\n        if data.forward_max > 4000 and len(tasks) == 1:\n          multi_test(data.forward_max, model, sess, tasks[0], FLAGS.nprint,\n                     batch_size * 16, 0)\n\n\ndef animate(l, test_data, anim_size):\n  """"""Create animation for the given data (hacky matplotlib use).""""""\n  xf = 12  # Extra frames to slow down at start and end.\n  fps = 2  # Frames per step.\n\n  # Make the figure.\n  fig = plt.figure(figsize=(16, 9), facecolor=""white"")\n  ax = fig.add_axes([0, 0, 1, 1], frameon=False, zorder=2)\n  ax.set_xticks([i * 24-0.5 for i in xrange(4)])\n  ax.set_xticklabels([])\n  ax.set_yticks([i - 0.5 for i in xrange(l+1)])\n  ax.grid(which=""major"", axis=""both"", linestyle=""-"", color=""black"")\n  # We need text fields.\n  text_fields = []\n  text_size = 24*32/l\n  for y in xrange(l):\n    text_fields.append(ax.text(\n        11.25, y + 0.15, """", color=""g"", ha=""center"", va=""center"",\n        bbox={""facecolor"": ""b"", ""alpha"": 0.01, ""pad"": 24 * text_size},\n        size=text_size - (4 * 32 / l), animated=True))\n  im = ax.imshow(np.zeros_like(test_data[0][0][0]), vmin=-1.0,\n                 vmax=1.0, cmap=""gray"", aspect=""auto"", origin=""upper"",\n                 interpolation=""none"", animated=True)\n  im.set_zorder(1)\n\n  # Main animation step.\n  def animation_update(frame_no, test_data, xf, im, text_fields):\n    """"""Update an animation frame.""""""\n    steps, inpt, out_raw = test_data\n    length = len(steps)\n    batch = frame_no / (fps * (l+4*xf))\n    index = int((frame_no % (fps * (l+4*xf))) / fps)\n    # Cut output after first padding.\n    out = [out_raw[i][batch] for i in xrange(len(text_fields))]\n    if 0 in out:\n      i = out.index(0)\n      out = out[0:i] + [0 for _ in xrange(len(out) - i)]\n    # Show the state after the first frames.\n    if index >= 2*xf:\n      im.set_array(steps[min(length - 1, index - 2*xf)][batch])\n      for i, t in enumerate(text_fields):\n        if index - 2*xf < length:\n          t.set_text("""")\n        else:\n          t.set_text(data.to_symbol(out[i]))\n    else:\n      for i, t in enumerate(text_fields):\n        t.set_text(data.to_symbol(inpt[i][batch]) if index < xf else """")\n      if index < xf:\n        im.set_array(np.zeros_like(steps[0][0]))\n      else:\n        im.set_array(steps[0][batch])\n    return im,\n\n  # Create the animation and save to mp4.\n  animation = anim.FuncAnimation(\n      fig, animation_update, blit=True, frames=(l+4*xf)*anim_size*fps,\n      interval=500/fps, fargs=(test_data, xf, im, text_fields))\n  animation.save(""/tmp/neural_gpu.mp4"", writer=""mencoder"", fps=4*fps, dpi=3*80)\n\n\ndef evaluate():\n  """"""Evaluate an existing model.""""""\n  batch_size = FLAGS.batch_size\n  tasks = FLAGS.task.split(""-"")\n  with tf.Session() as sess:\n    model, min_length, max_length, _, _, ensemble = initialize(sess)\n    bound = data.bins[-1] + 1\n    for t in tasks:\n      l = min_length\n      while l < max_length + EXTRA_EVAL and l < bound:\n        _, seq_err, _ = single_test(l, model, sess, t, FLAGS.nprint,\n                                    batch_size, ensemble=ensemble)\n        l += 1\n        while l < bound + 1 and not data.test_set[t][l]:\n          l += 1\n      # Animate.\n      if FLAGS.animate:\n        anim_size = 2\n        _, _, test_data = single_test(l, model, sess, t, 0, anim_size,\n                                      get_steps=True)\n        animate(l, test_data, anim_size)\n      # More tests.\n      _, seq_err = multi_test(data.forward_max, model, sess, t, FLAGS.nprint,\n                              batch_size * 4, ensemble=ensemble)\n    if seq_err < 0.01:  # Super-test if we\'re very good and in large-test mode.\n      if data.forward_max > 4000 and len(tasks) == 1:\n        multi_test(data.forward_max, model, sess, tasks[0], FLAGS.nprint,\n                   batch_size * 64, 0, ensemble=ensemble)\n\n\ndef interactive():\n  """"""Interactively probe an existing model.""""""\n  with tf.Session() as sess:\n    model, _, _, _, _, _ = initialize(sess)\n    sys.stdout.write(""Input to Neural GPU, e.g., 0 1. Use -1 for PAD.\\n"")\n    sys.stdout.write(""> "")\n    sys.stdout.flush()\n    inpt = sys.stdin.readline()\n    while inpt:\n      ids = [data.to_id(s) for s in inpt.strip().split()]\n      inpt, target = data.get_batch(len(ids), 1, False, """",\n                                    preset=(ids, [0 for _ in ids]))\n      _, res, _, _ = model.step(sess, inpt, target, False)\n      res = [np.argmax(o, axis=1) for o in res]\n      res = [o for o in res[:len(ids)] if o > 0]\n      print ""  "" + "" "".join([data.to_symbol(output[0]) for output in res])\n      sys.stdout.write(""> "")\n      sys.stdout.flush()\n      inpt = sys.stdin.readline()\n\n\ndef main(_):\n  if FLAGS.mode == 0:\n    train()\n  elif FLAGS.mode == 1:\n    evaluate()\n  else:\n    interactive()\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
model_zoo/models/neural_programmer/data_utils.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Functions for constructing vocabulary, converting the examples to integer format and building the required masks for batch computation Author: aneelakantan (Arvind Neelakantan)\n""""""\n\nimport copy\nimport numbers\nimport numpy as np\nimport wiki_data\n\n\ndef return_index(a):\n  for i in range(len(a)):\n    if (a[i] == 1.0):\n      return i\n\n\ndef construct_vocab(data, utility, add_word=False):\n  ans = []\n  for example in data:\n    sent = """"\n    for word in example.question:\n      if (not (isinstance(word, numbers.Number))):\n        sent += word + "" ""\n    example.original_nc = copy.deepcopy(example.number_columns)\n    example.original_wc = copy.deepcopy(example.word_columns)\n    example.original_nc_names = copy.deepcopy(example.number_column_names)\n    example.original_wc_names = copy.deepcopy(example.word_column_names)\n    if (add_word):\n      continue\n    number_found = 0\n    if (not (example.is_bad_example)):\n      for word in example.question:\n        if (isinstance(word, numbers.Number)):\n          number_found += 1\n        else:\n          if (not (utility.word_ids.has_key(word))):\n            utility.words.append(word)\n            utility.word_count[word] = 1\n            utility.word_ids[word] = len(utility.word_ids)\n            utility.reverse_word_ids[utility.word_ids[word]] = word\n          else:\n            utility.word_count[word] += 1\n      for col_name in example.word_column_names:\n        for word in col_name:\n          if (isinstance(word, numbers.Number)):\n            number_found += 1\n          else:\n            if (not (utility.word_ids.has_key(word))):\n              utility.words.append(word)\n              utility.word_count[word] = 1\n              utility.word_ids[word] = len(utility.word_ids)\n              utility.reverse_word_ids[utility.word_ids[word]] = word\n            else:\n              utility.word_count[word] += 1\n      for col_name in example.number_column_names:\n        for word in col_name:\n          if (isinstance(word, numbers.Number)):\n            number_found += 1\n          else:\n            if (not (utility.word_ids.has_key(word))):\n              utility.words.append(word)\n              utility.word_count[word] = 1\n              utility.word_ids[word] = len(utility.word_ids)\n              utility.reverse_word_ids[utility.word_ids[word]] = word\n            else:\n              utility.word_count[word] += 1\n\n\ndef word_lookup(word, utility):\n  if (utility.word_ids.has_key(word)):\n    return word\n  else:\n    return utility.unk_token\n\n\ndef convert_to_int_2d_and_pad(a, utility):\n  ans = []\n  #print a\n  for b in a:\n    temp = []\n    if (len(b) > utility.FLAGS.max_entry_length):\n      b = b[0:utility.FLAGS.max_entry_length]\n    for remaining in range(len(b), utility.FLAGS.max_entry_length):\n      b.append(utility.dummy_token)\n    assert len(b) == utility.FLAGS.max_entry_length\n    for word in b:\n      temp.append(utility.word_ids[word_lookup(word, utility)])\n    ans.append(temp)\n  #print ans\n  return ans\n\n\ndef convert_to_bool_and_pad(a, utility):\n  a = a.tolist()\n  for i in range(len(a)):\n    for j in range(len(a[i])):\n      if (a[i][j] < 1):\n        a[i][j] = False\n      else:\n        a[i][j] = True\n    a[i] = a[i] + [False] * (utility.FLAGS.max_elements - len(a[i]))\n  return a\n\n\nseen_tables = {}\n\n\ndef partial_match(question, table, number):\n  answer = []\n  match = {}\n  for i in range(len(table)):\n    temp = []\n    for j in range(len(table[i])):\n      temp.append(0)\n    answer.append(temp)\n  for i in range(len(table)):\n    for j in range(len(table[i])):\n      for word in question:\n        if (number):\n          if (word == table[i][j]):\n            answer[i][j] = 1.0\n            match[i] = 1.0\n        else:\n          if (word in table[i][j]):\n            answer[i][j] = 1.0\n            match[i] = 1.0\n  return answer, match\n\n\ndef exact_match(question, table, number):\n  #performs exact match operation\n  answer = []\n  match = {}\n  matched_indices = []\n  for i in range(len(table)):\n    temp = []\n    for j in range(len(table[i])):\n      temp.append(0)\n    answer.append(temp)\n  for i in range(len(table)):\n    for j in range(len(table[i])):\n      if (number):\n        for word in question:\n          if (word == table[i][j]):\n            match[i] = 1.0\n            answer[i][j] = 1.0\n      else:\n        table_entry = table[i][j]\n        for k in range(len(question)):\n          if (k + len(table_entry) <= len(question)):\n            if (table_entry == question[k:(k + len(table_entry))]):\n              #if(len(table_entry) == 1):\n              #print ""match: "", table_entry, question\n              match[i] = 1.0\n              answer[i][j] = 1.0\n              matched_indices.append((k, len(table_entry)))\n  return answer, match, matched_indices\n\n\ndef partial_column_match(question, table, number):\n  answer = []\n  for i in range(len(table)):\n    answer.append(0)\n  for i in range(len(table)):\n    for word in question:\n      if (word in table[i]):\n        answer[i] = 1.0\n  return answer\n\n\ndef exact_column_match(question, table, number):\n  #performs exact match on column names\n  answer = []\n  matched_indices = []\n  for i in range(len(table)):\n    answer.append(0)\n  for i in range(len(table)):\n    table_entry = table[i]\n    for k in range(len(question)):\n      if (k + len(table_entry) <= len(question)):\n        if (table_entry == question[k:(k + len(table_entry))]):\n          answer[i] = 1.0\n          matched_indices.append((k, len(table_entry)))\n  return answer, matched_indices\n\n\ndef get_max_entry(a):\n  e = {}\n  for w in a:\n    if (w != ""UNK, ""):\n      if (e.has_key(w)):\n        e[w] += 1\n      else:\n        e[w] = 1\n  if (len(e) > 0):\n    (key, val) = sorted(e.items(), key=lambda x: -1 * x[1])[0]\n    if (val > 1):\n      return key\n    else:\n      return -1.0\n  else:\n    return -1.0\n\n\ndef list_join(a):\n  ans = """"\n  for w in a:\n    ans += str(w) + "", ""\n  return ans\n\n\ndef group_by_max(table, number):\n  #computes the most frequently occuring entry in a column\n  answer = []\n  for i in range(len(table)):\n    temp = []\n    for j in range(len(table[i])):\n      temp.append(0)\n    answer.append(temp)\n  for i in range(len(table)):\n    if (number):\n      curr = table[i]\n    else:\n      curr = [list_join(w) for w in table[i]]\n    max_entry = get_max_entry(curr)\n    #print i, max_entry\n    for j in range(len(curr)):\n      if (max_entry == curr[j]):\n        answer[i][j] = 1.0\n      else:\n        answer[i][j] = 0.0\n  return answer\n\n\ndef pick_one(a):\n  for i in range(len(a)):\n    if (1.0 in a[i]):\n      return True\n  return False\n\n\ndef check_processed_cols(col, utility):\n  return True in [\n      True for y in col\n      if (y != utility.FLAGS.pad_int and y !=\n          utility.FLAGS.bad_number_pre_process)\n  ]\n\n\ndef complete_wiki_processing(data, utility, train=True):\n  #convert to integers and padding\n  processed_data = []\n  num_bad_examples = 0\n  for example in data:\n    number_found = 0\n    if (example.is_bad_example):\n      num_bad_examples += 1\n    if (not (example.is_bad_example)):\n      example.string_question = example.question[:]\n      #entry match\n      example.processed_number_columns = example.processed_number_columns[:]\n      example.processed_word_columns = example.processed_word_columns[:]\n      example.word_exact_match, word_match, matched_indices = exact_match(\n          example.string_question, example.original_wc, number=False)\n      example.number_exact_match, number_match, _ = exact_match(\n          example.string_question, example.original_nc, number=True)\n      if (not (pick_one(example.word_exact_match)) and not (\n          pick_one(example.number_exact_match))):\n        assert len(word_match) == 0\n        assert len(number_match) == 0\n        example.word_exact_match, word_match = partial_match(\n            example.string_question, example.original_wc, number=False)\n      #group by max\n      example.word_group_by_max = group_by_max(example.original_wc, False)\n      example.number_group_by_max = group_by_max(example.original_nc, True)\n      #column name match\n      example.word_column_exact_match, wcol_matched_indices = exact_column_match(\n          example.string_question, example.original_wc_names, number=False)\n      example.number_column_exact_match, ncol_matched_indices = exact_column_match(\n          example.string_question, example.original_nc_names, number=False)\n      if (not (1.0 in example.word_column_exact_match) and not (\n          1.0 in example.number_column_exact_match)):\n        example.word_column_exact_match = partial_column_match(\n            example.string_question, example.original_wc_names, number=False)\n        example.number_column_exact_match = partial_column_match(\n            example.string_question, example.original_nc_names, number=False)\n      if (len(word_match) > 0 or len(number_match) > 0):\n        example.question.append(utility.entry_match_token)\n      if (1.0 in example.word_column_exact_match or\n          1.0 in example.number_column_exact_match):\n        example.question.append(utility.column_match_token)\n      example.string_question = example.question[:]\n      example.number_lookup_matrix = np.transpose(\n          example.number_lookup_matrix)[:]\n      example.word_lookup_matrix = np.transpose(example.word_lookup_matrix)[:]\n      example.columns = example.number_columns[:]\n      example.word_columns = example.word_columns[:]\n      example.len_total_cols = len(example.word_column_names) + len(\n          example.number_column_names)\n      example.column_names = example.number_column_names[:]\n      example.word_column_names = example.word_column_names[:]\n      example.string_column_names = example.number_column_names[:]\n      example.string_word_column_names = example.word_column_names[:]\n      example.sorted_number_index = []\n      example.sorted_word_index = []\n      example.column_mask = []\n      example.word_column_mask = []\n      example.processed_column_mask = []\n      example.processed_word_column_mask = []\n      example.word_column_entry_mask = []\n      example.question_attention_mask = []\n      example.question_number = example.question_number_1 = -1\n      example.question_attention_mask = []\n      example.ordinal_question = []\n      example.ordinal_question_one = []\n      new_question = []\n      if (len(example.number_columns) > 0):\n        example.len_col = len(example.number_columns[0])\n      else:\n        example.len_col = len(example.word_columns[0])\n      for (start, length) in matched_indices:\n        for j in range(length):\n          example.question[start + j] = utility.unk_token\n      #print example.question\n      for word in example.question:\n        if (isinstance(word, numbers.Number) or wiki_data.is_date(word)):\n          if (not (isinstance(word, numbers.Number)) and\n              wiki_data.is_date(word)):\n            word = word.replace(""X"", """").replace(""-"", """")\n          number_found += 1\n          if (number_found == 1):\n            example.question_number = word\n            if (len(example.ordinal_question) > 0):\n              example.ordinal_question[len(example.ordinal_question) - 1] = 1.0\n            else:\n              example.ordinal_question.append(1.0)\n          elif (number_found == 2):\n            example.question_number_1 = word\n            if (len(example.ordinal_question_one) > 0):\n              example.ordinal_question_one[len(example.ordinal_question_one) -\n                                           1] = 1.0\n            else:\n              example.ordinal_question_one.append(1.0)\n        else:\n          new_question.append(word)\n          example.ordinal_question.append(0.0)\n          example.ordinal_question_one.append(0.0)\n      example.question = [\n          utility.word_ids[word_lookup(w, utility)] for w in new_question\n      ]\n      example.question_attention_mask = [0.0] * len(example.question)\n      #when the first question number occurs before a word\n      example.ordinal_question = example.ordinal_question[0:len(\n          example.question)]\n      example.ordinal_question_one = example.ordinal_question_one[0:len(\n          example.question)]\n      #question-padding\n      example.question = [utility.word_ids[utility.dummy_token]] * (\n          utility.FLAGS.question_length - len(example.question)\n      ) + example.question\n      example.question_attention_mask = [-10000.0] * (\n          utility.FLAGS.question_length - len(example.question_attention_mask)\n      ) + example.question_attention_mask\n      example.ordinal_question = [0.0] * (utility.FLAGS.question_length -\n                                          len(example.ordinal_question)\n                                         ) + example.ordinal_question\n      example.ordinal_question_one = [0.0] * (utility.FLAGS.question_length -\n                                              len(example.ordinal_question_one)\n                                             ) + example.ordinal_question_one\n      if (True):\n        #number columns and related-padding\n        num_cols = len(example.columns)\n        start = 0\n        for column in example.number_columns:\n          if (check_processed_cols(example.processed_number_columns[start],\n                                   utility)):\n            example.processed_column_mask.append(0.0)\n          sorted_index = sorted(\n              range(len(example.processed_number_columns[start])),\n              key=lambda k: example.processed_number_columns[start][k],\n              reverse=True)\n          sorted_index = sorted_index + [utility.FLAGS.pad_int] * (\n              utility.FLAGS.max_elements - len(sorted_index))\n          example.sorted_number_index.append(sorted_index)\n          example.columns[start] = column + [utility.FLAGS.pad_int] * (\n              utility.FLAGS.max_elements - len(column))\n          example.processed_number_columns[start] += [utility.FLAGS.pad_int] * (\n              utility.FLAGS.max_elements -\n              len(example.processed_number_columns[start]))\n          start += 1\n          example.column_mask.append(0.0)\n        for remaining in range(num_cols, utility.FLAGS.max_number_cols):\n          example.sorted_number_index.append([utility.FLAGS.pad_int] *\n                                             (utility.FLAGS.max_elements))\n          example.columns.append([utility.FLAGS.pad_int] *\n                                 (utility.FLAGS.max_elements))\n          example.processed_number_columns.append([utility.FLAGS.pad_int] *\n                                                  (utility.FLAGS.max_elements))\n          example.number_exact_match.append([0.0] *\n                                            (utility.FLAGS.max_elements))\n          example.number_group_by_max.append([0.0] *\n                                             (utility.FLAGS.max_elements))\n          example.column_mask.append(-100000000.0)\n          example.processed_column_mask.append(-100000000.0)\n          example.number_column_exact_match.append(0.0)\n          example.column_names.append([utility.dummy_token])\n        #word column  and related-padding\n        start = 0\n        word_num_cols = len(example.word_columns)\n        for column in example.word_columns:\n          if (check_processed_cols(example.processed_word_columns[start],\n                                   utility)):\n            example.processed_word_column_mask.append(0.0)\n          sorted_index = sorted(\n              range(len(example.processed_word_columns[start])),\n              key=lambda k: example.processed_word_columns[start][k],\n              reverse=True)\n          sorted_index = sorted_index + [utility.FLAGS.pad_int] * (\n              utility.FLAGS.max_elements - len(sorted_index))\n          example.sorted_word_index.append(sorted_index)\n          column = convert_to_int_2d_and_pad(column, utility)\n          example.word_columns[start] = column + [[\n              utility.word_ids[utility.dummy_token]\n          ] * utility.FLAGS.max_entry_length] * (utility.FLAGS.max_elements -\n                                                 len(column))\n          example.processed_word_columns[start] += [utility.FLAGS.pad_int] * (\n              utility.FLAGS.max_elements -\n              len(example.processed_word_columns[start]))\n          example.word_column_entry_mask.append([0] * len(column) + [\n              utility.word_ids[utility.dummy_token]\n          ] * (utility.FLAGS.max_elements - len(column)))\n          start += 1\n          example.word_column_mask.append(0.0)\n        for remaining in range(word_num_cols, utility.FLAGS.max_word_cols):\n          example.sorted_word_index.append([utility.FLAGS.pad_int] *\n                                           (utility.FLAGS.max_elements))\n          example.word_columns.append([[utility.word_ids[utility.dummy_token]] *\n                                       utility.FLAGS.max_entry_length] *\n                                      (utility.FLAGS.max_elements))\n          example.word_column_entry_mask.append(\n              [utility.word_ids[utility.dummy_token]] *\n              (utility.FLAGS.max_elements))\n          example.word_exact_match.append([0.0] * (utility.FLAGS.max_elements))\n          example.word_group_by_max.append([0.0] * (utility.FLAGS.max_elements))\n          example.processed_word_columns.append([utility.FLAGS.pad_int] *\n                                                (utility.FLAGS.max_elements))\n          example.word_column_mask.append(-100000000.0)\n          example.processed_word_column_mask.append(-100000000.0)\n          example.word_column_exact_match.append(0.0)\n          example.word_column_names.append([utility.dummy_token] *\n                                           utility.FLAGS.max_entry_length)\n        seen_tables[example.table_key] = 1\n      #convert column and word column names to integers\n      example.column_ids = convert_to_int_2d_and_pad(example.column_names,\n                                                     utility)\n      example.word_column_ids = convert_to_int_2d_and_pad(\n          example.word_column_names, utility)\n      for i_em in range(len(example.number_exact_match)):\n        example.number_exact_match[i_em] = example.number_exact_match[\n            i_em] + [0.0] * (utility.FLAGS.max_elements -\n                             len(example.number_exact_match[i_em]))\n        example.number_group_by_max[i_em] = example.number_group_by_max[\n            i_em] + [0.0] * (utility.FLAGS.max_elements -\n                             len(example.number_group_by_max[i_em]))\n      for i_em in range(len(example.word_exact_match)):\n        example.word_exact_match[i_em] = example.word_exact_match[\n            i_em] + [0.0] * (utility.FLAGS.max_elements -\n                             len(example.word_exact_match[i_em]))\n        example.word_group_by_max[i_em] = example.word_group_by_max[\n            i_em] + [0.0] * (utility.FLAGS.max_elements -\n                             len(example.word_group_by_max[i_em]))\n      example.exact_match = example.number_exact_match + example.word_exact_match\n      example.group_by_max = example.number_group_by_max + example.word_group_by_max\n      example.exact_column_match = example.number_column_exact_match + example.word_column_exact_match\n      #answer and related mask, padding\n      if (example.is_lookup):\n        example.answer = example.calc_answer\n        example.number_print_answer = example.number_lookup_matrix.tolist()\n        example.word_print_answer = example.word_lookup_matrix.tolist()\n        for i_answer in range(len(example.number_print_answer)):\n          example.number_print_answer[i_answer] = example.number_print_answer[\n              i_answer] + [0.0] * (utility.FLAGS.max_elements -\n                                   len(example.number_print_answer[i_answer]))\n        for i_answer in range(len(example.word_print_answer)):\n          example.word_print_answer[i_answer] = example.word_print_answer[\n              i_answer] + [0.0] * (utility.FLAGS.max_elements -\n                                   len(example.word_print_answer[i_answer]))\n        example.number_lookup_matrix = convert_to_bool_and_pad(\n            example.number_lookup_matrix, utility)\n        example.word_lookup_matrix = convert_to_bool_and_pad(\n            example.word_lookup_matrix, utility)\n        for remaining in range(num_cols, utility.FLAGS.max_number_cols):\n          example.number_lookup_matrix.append([False] *\n                                              utility.FLAGS.max_elements)\n          example.number_print_answer.append([0.0] * utility.FLAGS.max_elements)\n        for remaining in range(word_num_cols, utility.FLAGS.max_word_cols):\n          example.word_lookup_matrix.append([False] *\n                                            utility.FLAGS.max_elements)\n          example.word_print_answer.append([0.0] * utility.FLAGS.max_elements)\n        example.print_answer = example.number_print_answer + example.word_print_answer\n      else:\n        example.answer = example.calc_answer\n        example.print_answer = [[0.0] * (utility.FLAGS.max_elements)] * (\n            utility.FLAGS.max_number_cols + utility.FLAGS.max_word_cols)\n      #question_number masks\n      if (example.question_number == -1):\n        example.question_number_mask = np.zeros([utility.FLAGS.max_elements])\n      else:\n        example.question_number_mask = np.ones([utility.FLAGS.max_elements])\n      if (example.question_number_1 == -1):\n        example.question_number_one_mask = -10000.0\n      else:\n        example.question_number_one_mask = np.float64(0.0)\n      if (example.len_col > utility.FLAGS.max_elements):\n        continue\n      processed_data.append(example)\n  return processed_data\n\n\ndef add_special_words(utility):\n  utility.words.append(utility.entry_match_token)\n  utility.word_ids[utility.entry_match_token] = len(utility.word_ids)\n  utility.reverse_word_ids[utility.word_ids[\n      utility.entry_match_token]] = utility.entry_match_token\n  utility.entry_match_token_id = utility.word_ids[utility.entry_match_token]\n  print ""entry match token: "", utility.word_ids[\n      utility.entry_match_token], utility.entry_match_token_id\n  utility.words.append(utility.column_match_token)\n  utility.word_ids[utility.column_match_token] = len(utility.word_ids)\n  utility.reverse_word_ids[utility.word_ids[\n      utility.column_match_token]] = utility.column_match_token\n  utility.column_match_token_id = utility.word_ids[utility.column_match_token]\n  print ""entry match token: "", utility.word_ids[\n      utility.column_match_token], utility.column_match_token_id\n  utility.words.append(utility.dummy_token)\n  utility.word_ids[utility.dummy_token] = len(utility.word_ids)\n  utility.reverse_word_ids[utility.word_ids[\n      utility.dummy_token]] = utility.dummy_token\n  utility.dummy_token_id = utility.word_ids[utility.dummy_token]\n  utility.words.append(utility.unk_token)\n  utility.word_ids[utility.unk_token] = len(utility.word_ids)\n  utility.reverse_word_ids[utility.word_ids[\n      utility.unk_token]] = utility.unk_token\n\n\ndef perform_word_cutoff(utility):\n  if (utility.FLAGS.word_cutoff > 0):\n    for word in utility.word_ids.keys():\n      if (utility.word_count.has_key(word) and utility.word_count[word] <\n          utility.FLAGS.word_cutoff and word != utility.unk_token and\n          word != utility.dummy_token and word != utility.entry_match_token and\n          word != utility.column_match_token):\n        utility.word_ids.pop(word)\n        utility.words.remove(word)\n\n\ndef word_dropout(question, utility):\n  if (utility.FLAGS.word_dropout_prob > 0.0):\n    new_question = []\n    for i in range(len(question)):\n      if (question[i] != utility.dummy_token_id and\n          utility.random.random() > utility.FLAGS.word_dropout_prob):\n        new_question.append(utility.word_ids[utility.unk_token])\n      else:\n        new_question.append(question[i])\n    return new_question\n  else:\n    return question\n\n\ndef generate_feed_dict(data, curr, batch_size, gr, train=False, utility=None):\n  #prepare feed dict dictionary\n  feed_dict = {}\n  feed_examples = []\n  for j in range(batch_size):\n    feed_examples.append(data[curr + j])\n  if (train):\n    feed_dict[gr.batch_question] = [\n        word_dropout(feed_examples[j].question, utility)\n        for j in range(batch_size)\n    ]\n  else:\n    feed_dict[gr.batch_question] = [\n        feed_examples[j].question for j in range(batch_size)\n    ]\n  feed_dict[gr.batch_question_attention_mask] = [\n      feed_examples[j].question_attention_mask for j in range(batch_size)\n  ]\n  feed_dict[\n      gr.batch_answer] = [feed_examples[j].answer for j in range(batch_size)]\n  feed_dict[gr.batch_number_column] = [\n      feed_examples[j].columns for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_processed_number_column] = [\n      feed_examples[j].processed_number_columns for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_processed_sorted_index_number_column] = [\n      feed_examples[j].sorted_number_index for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_processed_sorted_index_word_column] = [\n      feed_examples[j].sorted_word_index for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_question_number] = np.array(\n      [feed_examples[j].question_number for j in range(batch_size)]).reshape(\n          (batch_size, 1))\n  feed_dict[gr.batch_question_number_one] = np.array(\n      [feed_examples[j].question_number_1 for j in range(batch_size)]).reshape(\n          (batch_size, 1))\n  feed_dict[gr.batch_question_number_mask] = [\n      feed_examples[j].question_number_mask for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_question_number_one_mask] = np.array(\n      [feed_examples[j].question_number_one_mask for j in range(batch_size)\n      ]).reshape((batch_size, 1))\n  feed_dict[gr.batch_print_answer] = [\n      feed_examples[j].print_answer for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_exact_match] = [\n      feed_examples[j].exact_match for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_group_by_max] = [\n      feed_examples[j].group_by_max for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_column_exact_match] = [\n      feed_examples[j].exact_column_match for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_ordinal_question] = [\n      feed_examples[j].ordinal_question for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_ordinal_question_one] = [\n      feed_examples[j].ordinal_question_one for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_number_column_mask] = [\n      feed_examples[j].column_mask for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_number_column_names] = [\n      feed_examples[j].column_ids for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_processed_word_column] = [\n      feed_examples[j].processed_word_columns for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_word_column_mask] = [\n      feed_examples[j].word_column_mask for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_word_column_names] = [\n      feed_examples[j].word_column_ids for j in range(batch_size)\n  ]\n  feed_dict[gr.batch_word_column_entry_mask] = [\n      feed_examples[j].word_column_entry_mask for j in range(batch_size)\n  ]\n  return feed_dict\n'"
model_zoo/models/neural_programmer/model.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Author: aneelakantan (Arvind Neelakantan)\n""""""\n\nimport numpy as np\nimport tensorflow as tf\nimport nn_utils\n\n\nclass Graph():\n\n  def __init__(self, utility, batch_size, max_passes, mode=""train""):\n    self.utility = utility\n    self.data_type = self.utility.tf_data_type[self.utility.FLAGS.data_type]\n    self.max_elements = self.utility.FLAGS.max_elements\n    max_elements = self.utility.FLAGS.max_elements\n    self.num_cols = self.utility.FLAGS.max_number_cols\n    self.num_word_cols = self.utility.FLAGS.max_word_cols\n    self.question_length = self.utility.FLAGS.question_length\n    self.batch_size = batch_size\n    self.max_passes = max_passes\n    self.mode = mode\n    self.embedding_dims = self.utility.FLAGS.embedding_dims\n    #input question and a mask\n    self.batch_question = tf.placeholder(tf.int32,\n                                         [batch_size, self.question_length])\n    self.batch_question_attention_mask = tf.placeholder(\n        self.data_type, [batch_size, self.question_length])\n    #ground truth scalar answer and lookup answer\n    self.batch_answer = tf.placeholder(self.data_type, [batch_size])\n    self.batch_print_answer = tf.placeholder(\n        self.data_type,\n        [batch_size, self.num_cols + self.num_word_cols, max_elements])\n    #number columns and its processed version\n    self.batch_number_column = tf.placeholder(\n        self.data_type, [batch_size, self.num_cols, max_elements\n                        ])  #columns with numeric entries\n    self.batch_processed_number_column = tf.placeholder(\n        self.data_type, [batch_size, self.num_cols, max_elements])\n    self.batch_processed_sorted_index_number_column = tf.placeholder(\n        tf.int32, [batch_size, self.num_cols, max_elements])\n    #word columns and its processed version\n    self.batch_processed_word_column = tf.placeholder(\n        self.data_type, [batch_size, self.num_word_cols, max_elements])\n    self.batch_processed_sorted_index_word_column = tf.placeholder(\n        tf.int32, [batch_size, self.num_word_cols, max_elements])\n    self.batch_word_column_entry_mask = tf.placeholder(\n        tf.int32, [batch_size, self.num_word_cols, max_elements])\n    #names of word and number columns along with their mask\n    self.batch_word_column_names = tf.placeholder(\n        tf.int32,\n        [batch_size, self.num_word_cols, self.utility.FLAGS.max_entry_length])\n    self.batch_word_column_mask = tf.placeholder(\n        self.data_type, [batch_size, self.num_word_cols])\n    self.batch_number_column_names = tf.placeholder(\n        tf.int32,\n        [batch_size, self.num_cols, self.utility.FLAGS.max_entry_length])\n    self.batch_number_column_mask = tf.placeholder(self.data_type,\n                                                   [batch_size, self.num_cols])\n    #exact match and group by max operation\n    self.batch_exact_match = tf.placeholder(\n        self.data_type,\n        [batch_size, self.num_cols + self.num_word_cols, max_elements])\n    self.batch_column_exact_match = tf.placeholder(\n        self.data_type, [batch_size, self.num_cols + self.num_word_cols])\n    self.batch_group_by_max = tf.placeholder(\n        self.data_type,\n        [batch_size, self.num_cols + self.num_word_cols, max_elements])\n    #numbers in the question along with their position. This is used to compute arguments to the comparison operations\n    self.batch_question_number = tf.placeholder(self.data_type, [batch_size, 1])\n    self.batch_question_number_one = tf.placeholder(self.data_type,\n                                                    [batch_size, 1])\n    self.batch_question_number_mask = tf.placeholder(\n        self.data_type, [batch_size, max_elements])\n    self.batch_question_number_one_mask = tf.placeholder(self.data_type,\n                                                         [batch_size, 1])\n    self.batch_ordinal_question = tf.placeholder(\n        self.data_type, [batch_size, self.question_length])\n    self.batch_ordinal_question_one = tf.placeholder(\n        self.data_type, [batch_size, self.question_length])\n\n  def LSTM_question_embedding(self, sentence, sentence_length):\n    #LSTM processes the input question\n    lstm_params = ""question_lstm""\n    hidden_vectors = []\n    sentence = self.batch_question\n    question_hidden = tf.zeros(\n        [self.batch_size, self.utility.FLAGS.embedding_dims], self.data_type)\n    question_c_hidden = tf.zeros(\n        [self.batch_size, self.utility.FLAGS.embedding_dims], self.data_type)\n    if (self.utility.FLAGS.rnn_dropout > 0.0):\n      if (self.mode == ""train""):\n        rnn_dropout_mask = tf.cast(\n            tf.random_uniform(\n                tf.shape(question_hidden), minval=0.0, maxval=1.0) <\n            self.utility.FLAGS.rnn_dropout,\n            self.data_type) / self.utility.FLAGS.rnn_dropout\n      else:\n        rnn_dropout_mask = tf.ones_like(question_hidden)\n    for question_iterator in range(self.question_length):\n      curr_word = sentence[:, question_iterator]\n      question_vector = nn_utils.apply_dropout(\n          nn_utils.get_embedding(curr_word, self.utility, self.params),\n          self.utility.FLAGS.dropout, self.mode)\n      question_hidden, question_c_hidden = nn_utils.LSTMCell(\n          question_vector, question_hidden, question_c_hidden, lstm_params,\n          self.params)\n      if (self.utility.FLAGS.rnn_dropout > 0.0):\n        question_hidden = question_hidden * rnn_dropout_mask\n      hidden_vectors.append(tf.expand_dims(question_hidden, 0))\n    hidden_vectors = tf.concat(0, hidden_vectors)\n    return question_hidden, hidden_vectors\n\n  def history_recurrent_step(self, curr_hprev, hprev):\n    #A single RNN step for controller or history RNN\n    return tf.tanh(\n        tf.matmul(\n            tf.concat(1, [hprev, curr_hprev]), self.params[\n                ""history_recurrent""])) + self.params[""history_recurrent_bias""]\n\n  def question_number_softmax(self, hidden_vectors):\n    #Attention on quetsion to decide the question number to passed to comparison ops\n    def compute_ans(op_embedding, comparison):\n      op_embedding = tf.expand_dims(op_embedding, 0)\n      #dot product of operation embedding with hidden state to the left of the number occurence\n      first = tf.transpose(\n          tf.matmul(op_embedding,\n                    tf.transpose(\n                        tf.reduce_sum(hidden_vectors * tf.tile(\n                            tf.expand_dims(\n                                tf.transpose(self.batch_ordinal_question), 2),\n                            [1, 1, self.utility.FLAGS.embedding_dims]), 0))))\n      second = self.batch_question_number_one_mask + tf.transpose(\n          tf.matmul(op_embedding,\n                    tf.transpose(\n                        tf.reduce_sum(hidden_vectors * tf.tile(\n                            tf.expand_dims(\n                                tf.transpose(self.batch_ordinal_question_one), 2\n                            ), [1, 1, self.utility.FLAGS.embedding_dims]), 0))))\n      question_number_softmax = tf.nn.softmax(tf.concat(1, [first, second]))\n      if (self.mode == ""test""):\n        cond = tf.equal(question_number_softmax,\n                        tf.reshape(\n                            tf.reduce_max(question_number_softmax, 1),\n                            [self.batch_size, 1]))\n        question_number_softmax = tf.select(\n            cond,\n            tf.fill(tf.shape(question_number_softmax), 1.0),\n            tf.fill(tf.shape(question_number_softmax), 0.0))\n        question_number_softmax = tf.cast(question_number_softmax,\n                                          self.data_type)\n      ans = tf.reshape(\n          tf.reduce_sum(question_number_softmax * tf.concat(\n              1, [self.batch_question_number, self.batch_question_number_one]),\n                        1), [self.batch_size, 1])\n      return ans\n\n    def compute_op_position(op_name):\n      for i in range(len(self.utility.operations_set)):\n        if (op_name == self.utility.operations_set[i]):\n          return i\n\n    def compute_question_number(op_name):\n      op_embedding = tf.nn.embedding_lookup(self.params_unit,\n                                            compute_op_position(op_name))\n      return compute_ans(op_embedding, op_name)\n\n    curr_greater_question_number = compute_question_number(""greater"")\n    curr_lesser_question_number = compute_question_number(""lesser"")\n    curr_geq_question_number = compute_question_number(""geq"")\n    curr_leq_question_number = compute_question_number(""leq"")\n    return curr_greater_question_number, curr_lesser_question_number, curr_geq_question_number, curr_leq_question_number\n\n  def perform_attention(self, context_vector, hidden_vectors, length, mask):\n    #Performs attention on hiddent_vectors using context vector\n    context_vector = tf.tile(\n        tf.expand_dims(context_vector, 0), [length, 1, 1])  #time * bs * d\n    attention_softmax = tf.nn.softmax(\n        tf.transpose(tf.reduce_sum(context_vector * hidden_vectors, 2)) +\n        mask)  #batch_size * time\n    attention_softmax = tf.tile(\n        tf.expand_dims(tf.transpose(attention_softmax), 2),\n        [1, 1, self.embedding_dims])\n    ans_vector = tf.reduce_sum(attention_softmax * hidden_vectors, 0)\n    return ans_vector\n\n  #computes embeddings for column names using parameters of question module\n  def get_column_hidden_vectors(self):\n    #vector representations for the column names\n    self.column_hidden_vectors = tf.reduce_sum(\n        nn_utils.get_embedding(self.batch_number_column_names, self.utility,\n                               self.params), 2)\n    self.word_column_hidden_vectors = tf.reduce_sum(\n        nn_utils.get_embedding(self.batch_word_column_names, self.utility,\n                               self.params), 2)\n\n  def create_summary_embeddings(self):\n    #embeddings for each text entry in the table using parameters of the question module\n    self.summary_text_entry_embeddings = tf.reduce_sum(\n        tf.expand_dims(self.batch_exact_match, 3) * tf.expand_dims(\n            tf.expand_dims(\n                tf.expand_dims(\n                    nn_utils.get_embedding(self.utility.entry_match_token_id,\n                                           self.utility, self.params), 0), 1),\n            2), 2)\n\n  def compute_column_softmax(self, column_controller_vector, time_step):\n    #compute softmax over all the columns using column controller vector\n    column_controller_vector = tf.tile(\n        tf.expand_dims(column_controller_vector, 1),\n        [1, self.num_cols + self.num_word_cols, 1])  #max_cols * bs * d\n    column_controller_vector = nn_utils.apply_dropout(\n        column_controller_vector, self.utility.FLAGS.dropout, self.mode)\n    self.full_column_hidden_vectors = tf.concat(\n        1, [self.column_hidden_vectors, self.word_column_hidden_vectors])\n    self.full_column_hidden_vectors += self.summary_text_entry_embeddings\n    self.full_column_hidden_vectors = nn_utils.apply_dropout(\n        self.full_column_hidden_vectors, self.utility.FLAGS.dropout, self.mode)\n    column_logits = tf.reduce_sum(\n        column_controller_vector * self.full_column_hidden_vectors, 2) + (\n            self.params[""word_match_feature_column_name""] *\n            self.batch_column_exact_match) + self.full_column_mask\n    column_softmax = tf.nn.softmax(column_logits)  #batch_size * max_cols\n    return column_softmax\n\n  def compute_first_or_last(self, select, first=True):\n    #perform first ot last operation on row select with probabilistic row selection\n    answer = tf.zeros_like(select)\n    running_sum = tf.zeros([self.batch_size, 1], self.data_type)\n    for i in range(self.max_elements):\n      if (first):\n        current = tf.slice(select, [0, i], [self.batch_size, 1])\n      else:\n        current = tf.slice(select, [0, self.max_elements - 1 - i],\n                           [self.batch_size, 1])\n      curr_prob = current * (1 - running_sum)\n      curr_prob = curr_prob * tf.cast(curr_prob >= 0.0, self.data_type)\n      running_sum += curr_prob\n      temp_ans = []\n      curr_prob = tf.expand_dims(tf.reshape(curr_prob, [self.batch_size]), 0)\n      for i_ans in range(self.max_elements):\n        if (not (first) and i_ans == self.max_elements - 1 - i):\n          temp_ans.append(curr_prob)\n        elif (first and i_ans == i):\n          temp_ans.append(curr_prob)\n        else:\n          temp_ans.append(tf.zeros_like(curr_prob))\n      temp_ans = tf.transpose(tf.concat(0, temp_ans))\n      answer += temp_ans\n    return answer\n\n  def make_hard_softmax(self, softmax):\n    #converts soft selection to hard selection. used at test time\n    cond = tf.equal(\n        softmax, tf.reshape(tf.reduce_max(softmax, 1), [self.batch_size, 1]))\n    softmax = tf.select(\n        cond, tf.fill(tf.shape(softmax), 1.0), tf.fill(tf.shape(softmax), 0.0))\n    softmax = tf.cast(softmax, self.data_type)\n    return softmax\n\n  def compute_max_or_min(self, select, maxi=True):\n    #computes the argmax and argmin of a column with probabilistic row selection\n    answer = tf.zeros([\n        self.batch_size, self.num_cols + self.num_word_cols, self.max_elements\n    ], self.data_type)\n    sum_prob = tf.zeros([self.batch_size, self.num_cols + self.num_word_cols],\n                        self.data_type)\n    for j in range(self.max_elements):\n      if (maxi):\n        curr_pos = j\n      else:\n        curr_pos = self.max_elements - 1 - j\n      select_index = tf.slice(self.full_processed_sorted_index_column,\n                              [0, 0, curr_pos], [self.batch_size, -1, 1])\n      select_mask = tf.equal(\n          tf.tile(\n              tf.expand_dims(\n                  tf.tile(\n                      tf.expand_dims(tf.range(self.max_elements), 0),\n                      [self.batch_size, 1]), 1),\n              [1, self.num_cols + self.num_word_cols, 1]), select_index)\n      curr_prob = tf.expand_dims(select, 1) * tf.cast(\n          select_mask, self.data_type) * self.select_bad_number_mask\n      curr_prob = curr_prob * tf.expand_dims((1 - sum_prob), 2)\n      curr_prob = curr_prob * tf.expand_dims(\n          tf.cast((1 - sum_prob) > 0.0, self.data_type), 2)\n      answer = tf.select(select_mask, curr_prob, answer)\n      sum_prob += tf.reduce_sum(curr_prob, 2)\n    return answer\n\n  def perform_operations(self, softmax, full_column_softmax, select,\n                         prev_select_1, curr_pass):\n    #performs all the 15 operations. computes scalar output, lookup answer and row selector\n    column_softmax = tf.slice(full_column_softmax, [0, 0],\n                              [self.batch_size, self.num_cols])\n    word_column_softmax = tf.slice(full_column_softmax, [0, self.num_cols],\n                                   [self.batch_size, self.num_word_cols])\n    init_max = self.compute_max_or_min(select, maxi=True)\n    init_min = self.compute_max_or_min(select, maxi=False)\n    #operations that are column  independent\n    count = tf.reshape(tf.reduce_sum(select, 1), [self.batch_size, 1])\n    select_full_column_softmax = tf.tile(\n        tf.expand_dims(full_column_softmax, 2),\n        [1, 1, self.max_elements\n        ])  #BS * (max_cols + max_word_cols) * max_elements\n    select_word_column_softmax = tf.tile(\n        tf.expand_dims(word_column_softmax, 2),\n        [1, 1, self.max_elements])  #BS * max_word_cols * max_elements\n    select_greater = tf.reduce_sum(\n        self.init_select_greater * select_full_column_softmax,\n        1) * self.batch_question_number_mask  #BS * max_elements\n    select_lesser = tf.reduce_sum(\n        self.init_select_lesser * select_full_column_softmax,\n        1) * self.batch_question_number_mask  #BS * max_elements\n    select_geq = tf.reduce_sum(\n        self.init_select_geq * select_full_column_softmax,\n        1) * self.batch_question_number_mask  #BS * max_elements\n    select_leq = tf.reduce_sum(\n        self.init_select_leq * select_full_column_softmax,\n        1) * self.batch_question_number_mask  #BS * max_elements\n    select_max = tf.reduce_sum(init_max * select_full_column_softmax,\n                               1)  #BS * max_elements\n    select_min = tf.reduce_sum(init_min * select_full_column_softmax,\n                               1)  #BS * max_elements\n    select_prev = tf.concat(1, [\n        tf.slice(select, [0, 1], [self.batch_size, self.max_elements - 1]),\n        tf.cast(tf.zeros([self.batch_size, 1]), self.data_type)\n    ])\n    select_next = tf.concat(1, [\n        tf.cast(tf.zeros([self.batch_size, 1]), self.data_type), tf.slice(\n            select, [0, 0], [self.batch_size, self.max_elements - 1])\n    ])\n    select_last_rs = self.compute_first_or_last(select, False)\n    select_first_rs = self.compute_first_or_last(select, True)\n    select_word_match = tf.reduce_sum(self.batch_exact_match *\n                                      select_full_column_softmax, 1)\n    select_group_by_max = tf.reduce_sum(self.batch_group_by_max *\n                                        select_full_column_softmax, 1)\n    length_content = 1\n    length_select = 13\n    length_print = 1\n    values = tf.concat(1, [count])\n    softmax_content = tf.slice(softmax, [0, 0],\n                               [self.batch_size, length_content])\n    #compute scalar output\n    output = tf.reduce_sum(tf.mul(softmax_content, values), 1)\n    #compute lookup answer\n    softmax_print = tf.slice(softmax, [0, length_content + length_select],\n                             [self.batch_size, length_print])\n    curr_print = select_full_column_softmax * tf.tile(\n        tf.expand_dims(select, 1),\n        [1, self.num_cols + self.num_word_cols, 1\n        ])  #BS * max_cols * max_elements (conisders only column)\n    self.batch_lookup_answer = curr_print * tf.tile(\n        tf.expand_dims(softmax_print, 2),\n        [1, self.num_cols + self.num_word_cols, self.max_elements\n        ])  #BS * max_cols * max_elements\n    self.batch_lookup_answer = self.batch_lookup_answer * self.select_full_mask\n    #compute row select\n    softmax_select = tf.slice(softmax, [0, length_content],\n                              [self.batch_size, length_select])\n    select_lists = [\n        tf.expand_dims(select_prev, 1), tf.expand_dims(select_next, 1),\n        tf.expand_dims(select_first_rs, 1), tf.expand_dims(select_last_rs, 1),\n        tf.expand_dims(select_group_by_max, 1),\n        tf.expand_dims(select_greater, 1), tf.expand_dims(select_lesser, 1),\n        tf.expand_dims(select_geq, 1), tf.expand_dims(select_leq, 1),\n        tf.expand_dims(select_max, 1), tf.expand_dims(select_min, 1),\n        tf.expand_dims(select_word_match, 1),\n        tf.expand_dims(self.reset_select, 1)\n    ]\n    select = tf.reduce_sum(\n        tf.tile(tf.expand_dims(softmax_select, 2), [1, 1, self.max_elements]) *\n        tf.concat(1, select_lists), 1)\n    select = select * self.select_whole_mask\n    return output, select\n\n  def one_pass(self, select, question_embedding, hidden_vectors, hprev,\n               prev_select_1, curr_pass):\n    #Performs one timestep which involves selecting an operation and a column\n    attention_vector = self.perform_attention(\n        hprev, hidden_vectors, self.question_length,\n        self.batch_question_attention_mask)  #batch_size * embedding_dims\n    controller_vector = tf.nn.relu(\n        tf.matmul(hprev, self.params[""controller_prev""]) + tf.matmul(\n            tf.concat(1, [question_embedding, attention_vector]), self.params[\n                ""controller""]))\n    column_controller_vector = tf.nn.relu(\n        tf.matmul(hprev, self.params[""column_controller_prev""]) + tf.matmul(\n            tf.concat(1, [question_embedding, attention_vector]), self.params[\n                ""column_controller""]))\n    controller_vector = nn_utils.apply_dropout(\n        controller_vector, self.utility.FLAGS.dropout, self.mode)\n    self.operation_logits = tf.matmul(controller_vector,\n                                      tf.transpose(self.params_unit))\n    softmax = tf.nn.softmax(self.operation_logits)\n    soft_softmax = softmax\n    #compute column softmax: bs * max_columns\n    weighted_op_representation = tf.transpose(\n        tf.matmul(tf.transpose(self.params_unit), tf.transpose(softmax)))\n    column_controller_vector = tf.nn.relu(\n        tf.matmul(\n            tf.concat(1, [\n                column_controller_vector, weighted_op_representation\n            ]), self.params[""break_conditional""]))\n    full_column_softmax = self.compute_column_softmax(column_controller_vector,\n                                                      curr_pass)\n    soft_column_softmax = full_column_softmax\n    if (self.mode == ""test""):\n      full_column_softmax = self.make_hard_softmax(full_column_softmax)\n      softmax = self.make_hard_softmax(softmax)\n    output, select = self.perform_operations(softmax, full_column_softmax,\n                                             select, prev_select_1, curr_pass)\n    return output, select, softmax, soft_softmax, full_column_softmax, soft_column_softmax\n\n  def compute_lookup_error(self, val):\n    #computes lookup error.\n    cond = tf.equal(self.batch_print_answer, val)\n    inter = tf.select(\n        cond, self.init_print_error,\n        tf.tile(\n            tf.reshape(tf.constant(1e10, self.data_type), [1, 1, 1]), [\n                self.batch_size, self.utility.FLAGS.max_word_cols +\n                self.utility.FLAGS.max_number_cols,\n                self.utility.FLAGS.max_elements\n            ]))\n    return tf.reduce_min(tf.reduce_min(inter, 1), 1) * tf.cast(\n        tf.greater(\n            tf.reduce_sum(tf.reduce_sum(tf.cast(cond, self.data_type), 1), 1),\n            0.0), self.data_type)\n\n  def soft_min(self, x, y):\n    return tf.maximum(-1.0 * (1 / (\n        self.utility.FLAGS.soft_min_value + 0.0)) * tf.log(\n            tf.exp(-self.utility.FLAGS.soft_min_value * x) + tf.exp(\n                -self.utility.FLAGS.soft_min_value * y)), tf.zeros_like(x))\n\n  def error_computation(self):\n    #computes the error of each example in a batch\n    math_error = 0.5 * tf.square(tf.sub(self.scalar_output, self.batch_answer))\n    #scale math error\n    math_error = math_error / self.rows\n    math_error = tf.minimum(math_error, self.utility.FLAGS.max_math_error *\n                            tf.ones(tf.shape(math_error), self.data_type))\n    self.init_print_error = tf.select(\n        self.batch_gold_select, -1 * tf.log(self.batch_lookup_answer + 1e-300 +\n                                            self.invert_select_full_mask), -1 *\n        tf.log(1 - self.batch_lookup_answer)) * self.select_full_mask\n    print_error_1 = self.init_print_error * tf.cast(\n        tf.equal(self.batch_print_answer, 0.0), self.data_type)\n    print_error = tf.reduce_sum(tf.reduce_sum((print_error_1), 1), 1)\n    for val in range(1, 58):\n      print_error += self.compute_lookup_error(val + 0.0)\n    print_error = print_error * self.utility.FLAGS.print_cost / self.num_entries\n    if (self.mode == ""train""):\n      error = tf.select(\n          tf.logical_and(\n              tf.not_equal(self.batch_answer, 0.0),\n              tf.not_equal(\n                  tf.reduce_sum(tf.reduce_sum(self.batch_print_answer, 1), 1),\n                  0.0)),\n          self.soft_min(math_error, print_error),\n          tf.select(\n              tf.not_equal(self.batch_answer, 0.0), math_error, print_error))\n    else:\n      error = tf.select(\n          tf.logical_and(\n              tf.equal(self.scalar_output, 0.0),\n              tf.equal(\n                  tf.reduce_sum(tf.reduce_sum(self.batch_lookup_answer, 1), 1),\n                  0.0)),\n          tf.ones_like(math_error),\n          tf.select(\n              tf.equal(self.scalar_output, 0.0), print_error, math_error))\n    return error\n\n  def batch_process(self):\n    #Computes loss and fraction of correct examples in a batch.\n    self.params_unit = nn_utils.apply_dropout(\n        self.params[""unit""], self.utility.FLAGS.dropout, self.mode)\n    batch_size = self.batch_size\n    max_passes = self.max_passes\n    num_timesteps = 1\n    max_elements = self.max_elements\n    select = tf.cast(\n        tf.fill([self.batch_size, max_elements], 1.0), self.data_type)\n    hprev = tf.cast(\n        tf.fill([self.batch_size, self.embedding_dims], 0.0),\n        self.data_type)  #running sum of the hidden states of the model\n    output = tf.cast(tf.fill([self.batch_size, 1], 0.0),\n                     self.data_type)  #output of the model\n    correct = tf.cast(\n        tf.fill([1], 0.0), self.data_type\n    )  #to compute accuracy, returns number of correct examples for this batch\n    total_error = 0.0\n    prev_select_1 = tf.zeros_like(select)\n    self.create_summary_embeddings()\n    self.get_column_hidden_vectors()\n    #get question embedding\n    question_embedding, hidden_vectors = self.LSTM_question_embedding(\n        self.batch_question, self.question_length)\n    #compute arguments for comparison operation\n    greater_question_number, lesser_question_number, geq_question_number, leq_question_number = self.question_number_softmax(\n        hidden_vectors)\n    self.init_select_greater = tf.cast(\n        tf.greater(self.full_processed_column,\n                   tf.expand_dims(greater_question_number, 2)), self.\n        data_type) * self.select_bad_number_mask  #bs * max_cols * max_elements\n    self.init_select_lesser = tf.cast(\n        tf.less(self.full_processed_column,\n                tf.expand_dims(lesser_question_number, 2)), self.\n        data_type) * self.select_bad_number_mask  #bs * max_cols * max_elements\n    self.init_select_geq = tf.cast(\n        tf.greater_equal(self.full_processed_column,\n                         tf.expand_dims(geq_question_number, 2)), self.\n        data_type) * self.select_bad_number_mask  #bs * max_cols * max_elements\n    self.init_select_leq = tf.cast(\n        tf.less_equal(self.full_processed_column,\n                      tf.expand_dims(leq_question_number, 2)), self.\n        data_type) * self.select_bad_number_mask  #bs * max_cols * max_elements\n    self.init_select_word_match = 0\n    if (self.utility.FLAGS.rnn_dropout > 0.0):\n      if (self.mode == ""train""):\n        history_rnn_dropout_mask = tf.cast(\n            tf.random_uniform(\n                tf.shape(hprev), minval=0.0, maxval=1.0) <\n            self.utility.FLAGS.rnn_dropout,\n            self.data_type) / self.utility.FLAGS.rnn_dropout\n      else:\n        history_rnn_dropout_mask = tf.ones_like(hprev)\n    select = select * self.select_whole_mask\n    self.batch_log_prob = tf.zeros([self.batch_size], dtype=self.data_type)\n    #Perform max_passes and at each  pass select operation and column\n    for curr_pass in range(max_passes):\n      print ""step: "", curr_pass\n      output, select, softmax, soft_softmax, column_softmax, soft_column_softmax = self.one_pass(\n          select, question_embedding, hidden_vectors, hprev, prev_select_1,\n          curr_pass)\n      prev_select_1 = select\n      #compute input to history RNN\n      input_op = tf.transpose(\n          tf.matmul(\n              tf.transpose(self.params_unit), tf.transpose(\n                  soft_softmax)))  #weighted average of emebdding of operations\n      input_col = tf.reduce_sum(\n          tf.expand_dims(soft_column_softmax, 2) *\n          self.full_column_hidden_vectors, 1)\n      history_input = tf.concat(1, [input_op, input_col])\n      history_input = nn_utils.apply_dropout(\n          history_input, self.utility.FLAGS.dropout, self.mode)\n      hprev = self.history_recurrent_step(history_input, hprev)\n      if (self.utility.FLAGS.rnn_dropout > 0.0):\n        hprev = hprev * history_rnn_dropout_mask\n    self.scalar_output = output\n    error = self.error_computation()\n    cond = tf.less(error, 0.0001, name=""cond"")\n    correct_add = tf.select(\n        cond, tf.fill(tf.shape(cond), 1.0), tf.fill(tf.shape(cond), 0.0))\n    correct = tf.reduce_sum(correct_add)\n    error = error / batch_size\n    total_error = tf.reduce_sum(error)\n    total_correct = correct / batch_size\n    return total_error, total_correct\n\n  def compute_error(self):\n    #Sets mask variables and performs batch processing\n    self.batch_gold_select = self.batch_print_answer > 0.0\n    self.full_column_mask = tf.concat(\n        1, [self.batch_number_column_mask, self.batch_word_column_mask])\n    self.full_processed_column = tf.concat(\n        1,\n        [self.batch_processed_number_column, self.batch_processed_word_column])\n    self.full_processed_sorted_index_column = tf.concat(1, [\n        self.batch_processed_sorted_index_number_column,\n        self.batch_processed_sorted_index_word_column\n    ])\n    self.select_bad_number_mask = tf.cast(\n        tf.logical_and(\n            tf.not_equal(self.full_processed_column,\n                         self.utility.FLAGS.pad_int),\n            tf.not_equal(self.full_processed_column,\n                         self.utility.FLAGS.bad_number_pre_process)),\n        self.data_type)\n    self.select_mask = tf.cast(\n        tf.logical_not(\n            tf.equal(self.batch_number_column, self.utility.FLAGS.pad_int)),\n        self.data_type)\n    self.select_word_mask = tf.cast(\n        tf.logical_not(\n            tf.equal(self.batch_word_column_entry_mask,\n                     self.utility.dummy_token_id)), self.data_type)\n    self.select_full_mask = tf.concat(\n        1, [self.select_mask, self.select_word_mask])\n    self.select_whole_mask = tf.maximum(\n        tf.reshape(\n            tf.slice(self.select_mask, [0, 0, 0],\n                     [self.batch_size, 1, self.max_elements]),\n            [self.batch_size, self.max_elements]),\n        tf.reshape(\n            tf.slice(self.select_word_mask, [0, 0, 0],\n                     [self.batch_size, 1, self.max_elements]),\n            [self.batch_size, self.max_elements]))\n    self.invert_select_full_mask = tf.cast(\n        tf.concat(1, [\n            tf.equal(self.batch_number_column, self.utility.FLAGS.pad_int),\n            tf.equal(self.batch_word_column_entry_mask,\n                     self.utility.dummy_token_id)\n        ]), self.data_type)\n    self.batch_lookup_answer = tf.zeros(tf.shape(self.batch_gold_select))\n    self.reset_select = self.select_whole_mask\n    self.rows = tf.reduce_sum(self.select_whole_mask, 1)\n    self.num_entries = tf.reshape(\n        tf.reduce_sum(tf.reduce_sum(self.select_full_mask, 1), 1),\n        [self.batch_size])\n    self.final_error, self.final_correct = self.batch_process()\n    return self.final_error\n\n  def create_graph(self, params, global_step):\n    #Creates the graph to compute error, gradient computation and updates parameters\n    self.params = params\n    batch_size = self.batch_size\n    learning_rate = tf.cast(self.utility.FLAGS.learning_rate, self.data_type)\n    self.total_cost = self.compute_error() \n    optimize_params = self.params.values()\n    optimize_names = self.params.keys()\n    print ""optimize params "", optimize_names\n    if (self.utility.FLAGS.l2_regularizer > 0.0):\n      reg_cost = 0.0\n      for ind_param in self.params.keys():\n        reg_cost += tf.nn.l2_loss(self.params[ind_param])\n      self.total_cost += self.utility.FLAGS.l2_regularizer * reg_cost\n    grads = tf.gradients(self.total_cost, optimize_params, name=""gradients"")\n    grad_norm = 0.0\n    for p, name in zip(grads, optimize_names):\n      print ""grads: "", p, name\n      if isinstance(p, tf.IndexedSlices):\n        grad_norm += tf.reduce_sum(p.values * p.values)\n      elif not (p == None):\n        grad_norm += tf.reduce_sum(p * p)\n    grad_norm = tf.sqrt(grad_norm)\n    max_grad_norm = np.float32(self.utility.FLAGS.clip_gradients).astype(\n        self.utility.np_data_type[self.utility.FLAGS.data_type])\n    grad_scale = tf.minimum(\n        tf.cast(1.0, self.data_type), max_grad_norm / grad_norm)\n    clipped_grads = list()\n    for p in grads:\n      if isinstance(p, tf.IndexedSlices):\n        tmp = p.values * grad_scale\n        clipped_grads.append(tf.IndexedSlices(tmp, p.indices))\n      elif not (p == None):\n        clipped_grads.append(p * grad_scale)\n      else:\n        clipped_grads.append(p)\n    grads = clipped_grads\n    self.global_step = global_step\n    params_list = self.params.values()\n    params_list.append(self.global_step)\n    adam = tf.train.AdamOptimizer(\n        learning_rate,\n        epsilon=tf.cast(self.utility.FLAGS.eps, self.data_type),\n        use_locking=True)\n    self.step = adam.apply_gradients(zip(grads, optimize_params), \n\t\t\t\t\tglobal_step=self.global_step)\n    self.init_op = tf.initialize_all_variables()\n\n'"
model_zoo/models/neural_programmer/neural_programmer.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Implementation of the Neural Programmer model described in https://openreview.net/pdf?id=ry2YOrcge\n\nThis file calls functions to load & pre-process data, construct the TF graph\nand performs training or evaluation as specified by the flag evaluator_job\nAuthor: aneelakantan (Arvind Neelakantan)\n""""""\nimport time\nfrom random import Random\nimport numpy as np\nimport tensorflow as tf\nimport model\nimport wiki_data\nimport parameters\nimport data_utils\n\ntf.flags.DEFINE_integer(""train_steps"", 100001, ""Number of steps to train"")\ntf.flags.DEFINE_integer(""eval_cycle"", 500,\n                        ""Evaluate model at every eval_cycle steps"")\ntf.flags.DEFINE_integer(""max_elements"", 100,\n                        ""maximum rows that are  considered for processing"")\ntf.flags.DEFINE_integer(\n    ""max_number_cols"", 15,\n    ""maximum number columns that are considered for processing"")\ntf.flags.DEFINE_integer(\n    ""max_word_cols"", 25,\n    ""maximum number columns that are considered for processing"")\ntf.flags.DEFINE_integer(""question_length"", 62, ""maximum question length"")\ntf.flags.DEFINE_integer(""max_entry_length"", 1, """")\ntf.flags.DEFINE_integer(""max_passes"", 4, ""number of operation passes"")\ntf.flags.DEFINE_integer(""embedding_dims"", 256, """")\ntf.flags.DEFINE_integer(""batch_size"", 20, """")\ntf.flags.DEFINE_float(""clip_gradients"", 1.0, """")\ntf.flags.DEFINE_float(""eps"", 1e-6, """")\ntf.flags.DEFINE_float(""param_init"", 0.1, """")\ntf.flags.DEFINE_float(""learning_rate"", 0.001, """")\ntf.flags.DEFINE_float(""l2_regularizer"", 0.0001, """")\ntf.flags.DEFINE_float(""print_cost"", 50.0,\n                      ""weighting factor in the objective function"")\ntf.flags.DEFINE_string(""job_id"", ""temp"", """"""job id"""""")\ntf.flags.DEFINE_string(""output_dir"", ""../model/"",\n                       """"""output_dir"""""")\ntf.flags.DEFINE_string(""data_dir"", ""../data/"",\n                       """"""data_dir"""""")\ntf.flags.DEFINE_integer(""write_every"", 500, ""wrtie every N"")\ntf.flags.DEFINE_integer(""param_seed"", 150, """")\ntf.flags.DEFINE_integer(""python_seed"", 200, """")\ntf.flags.DEFINE_float(""dropout"", 0.8, ""dropout keep probability"")\ntf.flags.DEFINE_float(""rnn_dropout"", 0.9,\n                      ""dropout keep probability for rnn connections"")\ntf.flags.DEFINE_float(""pad_int"", -20000.0,\n                      ""number columns are padded with pad_int"")\ntf.flags.DEFINE_string(""data_type"", ""double"", ""float or double"")\ntf.flags.DEFINE_float(""word_dropout_prob"", 0.9, ""word dropout keep prob"")\ntf.flags.DEFINE_integer(""word_cutoff"", 10, """")\ntf.flags.DEFINE_integer(""vocab_size"", 10800, """")\ntf.flags.DEFINE_boolean(""evaluator_job"", False,\n                        ""wehther to run as trainer/evaluator"")\ntf.flags.DEFINE_float(\n    ""bad_number_pre_process"", -200000.0,\n    ""number that is added to a corrupted table entry in a number column"")\ntf.flags.DEFINE_float(""max_math_error"", 3.0,\n                      ""max square loss error that is considered"")\ntf.flags.DEFINE_float(""soft_min_value"", 5.0, """")\nFLAGS = tf.flags.FLAGS\n\n\nclass Utility:\n  #holds FLAGS and other variables that are used in different files\n  def __init__(self):\n    global FLAGS\n    self.FLAGS = FLAGS\n    self.unk_token = ""UNK""\n    self.entry_match_token = ""entry_match""\n    self.column_match_token = ""column_match""\n    self.dummy_token = ""dummy_token""\n    self.tf_data_type = {}\n    self.tf_data_type[""double""] = tf.float64\n    self.tf_data_type[""float""] = tf.float32\n    self.np_data_type = {}\n    self.np_data_type[""double""] = np.float64\n    self.np_data_type[""float""] = np.float32\n    self.operations_set = [""count""] + [\n        ""prev"", ""next"", ""first_rs"", ""last_rs"", ""group_by_max"", ""greater"",\n        ""lesser"", ""geq"", ""leq"", ""max"", ""min"", ""word-match""\n    ] + [""reset_select""] + [""print""]\n    self.word_ids = {}\n    self.reverse_word_ids = {}\n    self.word_count = {}\n    self.random = Random(FLAGS.python_seed)\n\n\ndef evaluate(sess, data, batch_size, graph, i):\n  #computes accuracy\n  num_examples = 0.0\n  gc = 0.0\n  for j in range(0, len(data) - batch_size + 1, batch_size):\n    [ct] = sess.run([graph.final_correct],\n                    feed_dict=data_utils.generate_feed_dict(data, j, batch_size,\n                                                            graph))\n    gc += ct * batch_size\n    num_examples += batch_size\n  print ""dev set accuracy   after "", i, "" : "", gc / num_examples\n  print num_examples, len(data)\n  print ""--------""\n\n\ndef Train(graph, utility, batch_size, train_data, sess, model_dir,\n          saver):\n  #performs training\n  curr = 0\n  train_set_loss = 0.0\n  utility.random.shuffle(train_data)\n  start = time.time()\n  for i in range(utility.FLAGS.train_steps):\n    curr_step = i\n    if (i > 0 and i % FLAGS.write_every == 0):\n      model_file = model_dir + ""/model_"" + str(i)\n      saver.save(sess, model_file)\n    if curr + batch_size >= len(train_data):\n      curr = 0\n      utility.random.shuffle(train_data)\n    step, cost_value = sess.run(\n        [graph.step, graph.total_cost],\n        feed_dict=data_utils.generate_feed_dict(\n            train_data, curr, batch_size, graph, train=True, utility=utility))\n    curr = curr + batch_size\n    train_set_loss += cost_value\n    if (i > 0 and i % FLAGS.eval_cycle == 0):\n      end = time.time()\n      time_taken = end - start\n      print ""step "", i, "" "", time_taken, "" seconds ""\n      start = end\n      print "" printing train set loss: "", train_set_loss / utility.FLAGS.eval_cycle\n      train_set_loss = 0.0\n\n\ndef master(train_data, dev_data, utility):\n  #creates TF graph and calls trainer or evaluator\n  batch_size = utility.FLAGS.batch_size \n  model_dir = utility.FLAGS.output_dir + ""/model"" + utility.FLAGS.job_id + ""/""\n  #create all paramters of the model\n  param_class = parameters.Parameters(utility)\n  params, global_step, init = param_class.parameters(utility)\n  key = ""test"" if (FLAGS.evaluator_job) else ""train""\n  graph = model.Graph(utility, batch_size, utility.FLAGS.max_passes, mode=key)\n  graph.create_graph(params, global_step)\n  prev_dev_error = 0.0\n  final_loss = 0.0\n  final_accuracy = 0.0\n  #start session\n  with tf.Session() as sess:\n    sess.run(init.name)\n    sess.run(graph.init_op.name)\n    to_save = params.copy()\n    saver = tf.train.Saver(to_save, max_to_keep=500)\n    if (FLAGS.evaluator_job):\n      while True:\n        selected_models = {}\n        file_list = tf.gfile.ListDirectory(model_dir)\n        for model_file in file_list:\n          if (""checkpoint"" in model_file or ""index"" in model_file or\n              ""meta"" in model_file):\n            continue\n          if (""data"" in model_file):\n            model_file = model_file.split(""."")[0]\n          model_step = int(\n              model_file.split(""_"")[len(model_file.split(""_"")) - 1])\n          selected_models[model_step] = model_file\n        file_list = sorted(selected_models.items(), key=lambda x: x[0])\n        if (len(file_list) > 0):\n          file_list = file_list[0:len(file_list) - 1]\n\tprint ""list of models: "", file_list\n        for model_file in file_list:\n          model_file = model_file[1]\n          print ""restoring: "", model_file\n          saver.restore(sess, model_dir + ""/"" + model_file)\n          model_step = int(\n              model_file.split(""_"")[len(model_file.split(""_"")) - 1])\n          print ""evaluating on dev "", model_file, model_step\n          evaluate(sess, dev_data, batch_size, graph, model_step)\n    else:\n      ckpt = tf.train.get_checkpoint_state(model_dir)\n      print ""model dir: "", model_dir\n      if (not (tf.gfile.IsDirectory(model_dir))):\n        print ""create dir: "", model_dir\n        tf.gfile.MkDir(model_dir)\n      Train(graph, utility, batch_size, train_data, sess, model_dir,\n            saver)\n\ndef main(args):\n  utility = Utility()\n  train_name = ""random-split-1-train.examples""\n  dev_name = ""random-split-1-dev.examples""\n  test_name = ""pristine-unseen-tables.examples""\n  #load data\n  dat = wiki_data.WikiQuestionGenerator(train_name, dev_name, test_name, FLAGS.data_dir)\n  train_data, dev_data, test_data = dat.load()\n  utility.words = []\n  utility.word_ids = {}\n  utility.reverse_word_ids = {}\n  #construct vocabulary\n  data_utils.construct_vocab(train_data, utility)\n  data_utils.construct_vocab(dev_data, utility, True)\n  data_utils.construct_vocab(test_data, utility, True)\n  data_utils.add_special_words(utility)\n  data_utils.perform_word_cutoff(utility)\n  #convert data to int format and pad the inputs\n  train_data = data_utils.complete_wiki_processing(train_data, utility, True)\n  dev_data = data_utils.complete_wiki_processing(dev_data, utility, False)\n  test_data = data_utils.complete_wiki_processing(test_data, utility, False)\n  print ""# train examples "", len(train_data)\n  print ""# dev examples "", len(dev_data)\n  print ""# test examples "", len(test_data)\n  print ""running open source""\n  #construct TF graph and train or evaluate\n  master(train_data, dev_data, utility)\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
model_zoo/models/neural_programmer/nn_utils.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Author: aneelakantan (Arvind Neelakantan)\n""""""\n\nimport tensorflow as tf\n\ndef get_embedding(word, utility, params):\n  return tf.nn.embedding_lookup(params[""word""], word)\n\n\ndef apply_dropout(x, dropout_rate, mode):\n  if (dropout_rate > 0.0):\n    if (mode == ""train""):\n      x = tf.nn.dropout(x, dropout_rate)\n    else:\n      x = x\n  return x\n\n\ndef LSTMCell(x, mprev, cprev, key, params):\n  """"""Create an LSTM cell.\n\n  Implements the equations in pg.2 from\n  ""Long Short-Term Memory Based Recurrent Neural Network Architectures\n  For Large Vocabulary Speech Recognition"",\n  Hasim Sak, Andrew Senior, Francoise Beaufays.\n\n  Args:\n    w: A dictionary of the weights and optional biases as returned\n      by LSTMParametersSplit().\n    x: Inputs to this cell.\n    mprev: m_{t-1}, the recurrent activations (same as the output)\n      from the previous cell.\n    cprev: c_{t-1}, the cell activations from the previous cell.\n    keep_prob: Keep probability on the input and the outputs of a cell.\n\n  Returns:\n    m: Outputs of this cell.\n    c: Cell Activations.\n    """"""\n\n  i = tf.matmul(x, params[key + ""_ix""]) + tf.matmul(mprev, params[key + ""_im""])\n  i = tf.nn.bias_add(i, params[key + ""_i""])\n  f = tf.matmul(x, params[key + ""_fx""]) + tf.matmul(mprev, params[key + ""_fm""])\n  f = tf.nn.bias_add(f, params[key + ""_f""])\n  c = tf.matmul(x, params[key + ""_cx""]) + tf.matmul(mprev, params[key + ""_cm""])\n  c = tf.nn.bias_add(c, params[key + ""_c""])\n  o = tf.matmul(x, params[key + ""_ox""]) + tf.matmul(mprev, params[key + ""_om""])\n  o = tf.nn.bias_add(o, params[key + ""_o""])\n  i = tf.sigmoid(i, name=""i_gate"")\n  f = tf.sigmoid(f, name=""f_gate"")\n  o = tf.sigmoid(o, name=""o_gate"")\n  c = f * cprev + i * tf.tanh(c)\n  m = o * c\n  return m, c\n'"
model_zoo/models/neural_programmer/parameters.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Author: aneelakantan (Arvind Neelakantan)\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass Parameters:\n\n  def __init__(self, u):\n    self.utility = u\n    self.init_seed_counter = 0\n    self.word_init = {}\n\n  def parameters(self, utility):\n    params = {}\n    inits = []\n    embedding_dims = self.utility.FLAGS.embedding_dims\n    params[""unit""] = tf.Variable(\n        self.RandomUniformInit([len(utility.operations_set), embedding_dims]))\n    params[""word""] = tf.Variable(\n        self.RandomUniformInit([utility.FLAGS.vocab_size, embedding_dims]))\n    params[""word_match_feature_column_name""] = tf.Variable(\n        self.RandomUniformInit([1]))\n    params[""controller""] = tf.Variable(\n        self.RandomUniformInit([2 * embedding_dims, embedding_dims]))\n    params[""column_controller""] = tf.Variable(\n        self.RandomUniformInit([2 * embedding_dims, embedding_dims]))\n    params[""column_controller_prev""] = tf.Variable(\n        self.RandomUniformInit([embedding_dims, embedding_dims]))\n    params[""controller_prev""] = tf.Variable(\n        self.RandomUniformInit([embedding_dims, embedding_dims]))\n    global_step = tf.Variable(1, name=""global_step"")\n    #weigths of question and history RNN (or LSTM)\n    key_list = [""question_lstm""]\n    for key in key_list:\n      # Weights going from inputs to nodes.\n      for wgts in [""ix"", ""fx"", ""cx"", ""ox""]:\n        params[key + ""_"" + wgts] = tf.Variable(\n            self.RandomUniformInit([embedding_dims, embedding_dims]))\n      # Weights going from nodes to nodes.\n      for wgts in [""im"", ""fm"", ""cm"", ""om""]:\n        params[key + ""_"" + wgts] = tf.Variable(\n            self.RandomUniformInit([embedding_dims, embedding_dims]))\n      #Biases for the gates and cell\n      for bias in [""i"", ""f"", ""c"", ""o""]:\n        if (bias == ""f""):\n          print ""forget gate bias""\n          params[key + ""_"" + bias] = tf.Variable(\n              tf.random_uniform([embedding_dims], 1.0, 1.1, self.utility.\n                                tf_data_type[self.utility.FLAGS.data_type]))\n        else:\n          params[key + ""_"" + bias] = tf.Variable(\n              self.RandomUniformInit([embedding_dims]))\n    params[""history_recurrent""] = tf.Variable(\n        self.RandomUniformInit([3 * embedding_dims, embedding_dims]))\n    params[""history_recurrent_bias""] = tf.Variable(\n        self.RandomUniformInit([1, embedding_dims]))\n    params[""break_conditional""] = tf.Variable(\n        self.RandomUniformInit([2 * embedding_dims, embedding_dims]))\n    init = tf.initialize_all_variables()\n    return params, global_step, init\n\n  def RandomUniformInit(self, shape):\n    """"""Returns a RandomUniform Tensor between -param_init and param_init.""""""\n    param_seed = self.utility.FLAGS.param_seed\n    self.init_seed_counter += 1\n    return tf.random_uniform(\n        shape, -1.0 *\n        (np.float32(self.utility.FLAGS.param_init)\n        ).astype(self.utility.np_data_type[self.utility.FLAGS.data_type]),\n        (np.float32(self.utility.FLAGS.param_init)\n        ).astype(self.utility.np_data_type[self.utility.FLAGS.data_type]),\n        self.utility.tf_data_type[self.utility.FLAGS.data_type],\n        param_seed + self.init_seed_counter)\n'"
model_zoo/models/neural_programmer/wiki_data.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Loads the WikiQuestions dataset.\n\nAn example consists of question, table. Additionally, we store the processed\ncolumns which store the entries after performing number, date and other\npreprocessing as done in the baseline.\ncolumns, column names and processed columns are split into word and number\ncolumns.\nlookup answer (or matrix) is also split into number and word lookup matrix\nAuthor: aneelakantan (Arvind Neelakantan)\n""""""\nimport math\nimport os\nimport re\nimport numpy as np\nimport unicodedata as ud\nimport tensorflow as tf\n\nbad_number = -200000.0  #number that is added to a corrupted table entry in a number column\n\ndef is_nan_or_inf(number):\n  return math.isnan(number) or math.isinf(number)\n\ndef strip_accents(s):\n  u = unicode(s, ""utf-8"")\n  u_new = \'\'.join(c for c in ud.normalize(\'NFKD\', u) if ud.category(c) != \'Mn\')\n  return u_new.encode(""utf-8"")\n\n\ndef correct_unicode(string):\n  string = strip_accents(string)\n  string = re.sub(""\\xc2\\xa0"", "" "", string).strip()\n  string = re.sub(""\\xe2\\x80\\x93"", ""-"", string).strip()\n  #string = re.sub(ur\'[\\u0300-\\u036F]\', """", string)\n  string = re.sub(""\xc3\xa2\xe2\x82\xac\xc5\xa1"", "","", string)\n  string = re.sub(""\xc3\xa2\xe2\x82\xac\xc2\xa6"", ""..."", string)\n  #string = re.sub(""[\xc3\x82\xc2\xb7\xc3\xa3\xc6\x92\xc2\xbb]"", ""."", string)\n  string = re.sub(""\xc3\x8b\xe2\x80\xa0"", ""^"", string)\n  string = re.sub(""\xc3\x8b\xc5\x93"", ""~"", string)\n  string = re.sub(""\xc3\xa2\xe2\x82\xac\xc2\xb9"", ""<"", string)\n  string = re.sub(""\xc3\xa2\xe2\x82\xac\xc2\xba"", "">"", string)\n  #string = re.sub(""[\xc3\xa2\xe2\x82\xac\xcb\x9c\xc3\xa2\xe2\x82\xac\xe2\x84\xa2\xc3\x82\xc2\xb4`]"", ""\'"", string)\n  #string = re.sub(""[\xc3\xa2\xe2\x82\xac\xc5\x93\xc3\xa2\xe2\x82\xac\xc2\x9d\xc3\x82\xc2\xab\xc3\x82\xc2\xbb]"", ""\\"""", string)\n  #string = re.sub(""[\xc3\xa2\xe2\x82\xac\xc2\xa2\xc3\xa2\xe2\x82\xac \xc3\xa2\xe2\x82\xac\xc2\xa1]"", """", string)\n  #string = re.sub(""[\xc3\xa2\xe2\x82\xac\xc2\x90\xc3\xa2\xe2\x82\xac\xe2\x80\x98\xc3\xa2\xe2\x82\xac\xe2\x80\x9c\xc3\xa2\xe2\x82\xac\xe2\x80\x9d]"", ""-"", string)\n  string = re.sub(ur\'[\\u2E00-\\uFFFF]\', """", string)\n  string = re.sub(""\\\\s+"", "" "", string).strip()\n  return string\n\n\ndef simple_normalize(string):\n  string = correct_unicode(string)\n  # Citations\n  string = re.sub(""\\[(nb ?)?\\d+\\]"", """", string)\n  string = re.sub(""\\*+$"", """", string)\n  # Year in parenthesis\n  string = re.sub(""\\(\\d* ?-? ?\\d*\\)"", """", string)\n  string = re.sub(""^\\""(.*)\\""$"", """", string)\n  return string\n\n\ndef full_normalize(string):\n  #print ""an: "", string\n  string = simple_normalize(string)\n  # Remove trailing info in brackets\n  string = re.sub(""\\[[^\\]]*\\]"", """", string)\n  # Remove most unicode characters in other languages\n  string = re.sub(ur\'[\\u007F-\\uFFFF]\', """", string.strip())\n  # Remove trailing info in parenthesis\n  string = re.sub(""\\([^)]*\\)$"", """", string.strip())\n  string = final_normalize(string)\n  # Get rid of question marks\n  string = re.sub(""\\?"", """", string).strip()\n  # Get rid of trailing colons (usually occur in column titles)\n  string = re.sub(""\\:$"", "" "", string).strip()\n  # Get rid of slashes\n  string = re.sub(r""/"", "" "", string).strip()\n  string = re.sub(r""\\\\"", "" "", string).strip()\n  # Replace colon, slash, and dash with space\n  # Note: need better replacement for this when parsing time\n  string = re.sub(r""\\:"", "" "", string).strip()\n  string = re.sub(""/"", "" "", string).strip()\n  string = re.sub(""-"", "" "", string).strip()\n  # Convert empty strings to UNK\n  # Important to do this last or near last\n  if not string:\n    string = ""UNK""\n  return string\n\ndef final_normalize(string):\n  # Remove leading and trailing whitespace\n  string = re.sub(""\\\\s+"", "" "", string).strip()\n  # Convert entirely to lowercase\n  string = string.lower()\n  # Get rid of strangely escaped newline characters\n  string = re.sub(""\\\\\\\\n"", "" "", string).strip()\n  # Get rid of quotation marks\n  string = re.sub(r""\\"""", """", string).strip()\n  string = re.sub(r""\\\'"", """", string).strip()\n  string = re.sub(r""`"", """", string).strip()\n  # Get rid of *\n  string = re.sub(""\\*"", """", string).strip()\n  return string\n\ndef is_number(x):\n  try:\n    f = float(x)\n    return not is_nan_or_inf(f)\n  except ValueError:\n    return False\n  except TypeError:\n    return False\n\n\nclass WikiExample(object):\n\n  def __init__(self, id, question, answer, table_key):\n    self.question_id = id\n    self.question = question\n    self.answer = answer\n    self.table_key = table_key\n    self.lookup_matrix = []\n    self.is_bad_example = False\n    self.is_word_lookup = False\n    self.is_ambiguous_word_lookup = False\n    self.is_number_lookup = False\n    self.is_number_calc = False\n    self.is_unknown_answer = False\n\n\nclass TableInfo(object):\n\n  def __init__(self, word_columns, word_column_names, word_column_indices,\n               number_columns, number_column_names, number_column_indices,\n               processed_word_columns, processed_number_columns, orig_columns):\n    self.word_columns = word_columns\n    self.word_column_names = word_column_names\n    self.word_column_indices = word_column_indices\n    self.number_columns = number_columns\n    self.number_column_names = number_column_names\n    self.number_column_indices = number_column_indices\n    self.processed_word_columns = processed_word_columns\n    self.processed_number_columns = processed_number_columns\n    self.orig_columns = orig_columns\n\n\nclass WikiQuestionLoader(object):\n\n  def __init__(self, data_name, root_folder):\n    self.root_folder = root_folder\n    self.data_folder = os.path.join(self.root_folder, ""data"")\n    self.examples = []\n    self.data_name = data_name\n\n  def num_questions(self):\n    return len(self.examples)\n\n  def load_qa(self):\n    data_source = os.path.join(self.data_folder, self.data_name)\n    f = tf.gfile.GFile(data_source, ""r"")\n    id_regex = re.compile(""\\(id ([^\\)]*)\\)"")\n    for line in f:\n      id_match = id_regex.search(line)\n      id = id_match.group(1)\n      self.examples.append(id)\n\n  def load(self):\n    self.load_qa()\n\n\ndef is_date(word):\n  if (not (bool(re.search(""[a-z0-9]"", word, re.IGNORECASE)))):\n    return False\n  if (len(word) != 10):\n    return False\n  if (word[4] != ""-""):\n    return False\n  if (word[7] != ""-""):\n    return False\n  for i in range(len(word)):\n    if (not (word[i] == ""X"" or word[i] == ""x"" or word[i] == ""-"" or re.search(\n        ""[0-9]"", word[i]))):\n      return False\n  return True\n\n\nclass WikiQuestionGenerator(object):\n\n  def __init__(self, train_name, dev_name, test_name, root_folder):\n    self.train_name = train_name\n    self.dev_name = dev_name\n    self.test_name = test_name\n    self.train_loader = WikiQuestionLoader(train_name, root_folder)\n    self.dev_loader = WikiQuestionLoader(dev_name, root_folder)\n    self.test_loader = WikiQuestionLoader(test_name, root_folder)\n    self.bad_examples = 0\n    self.root_folder = root_folder   \n    self.data_folder = os.path.join(self.root_folder, ""annotated/data"")\n    self.annotated_examples = {}\n    self.annotated_tables = {}\n    self.annotated_word_reject = {}\n    self.annotated_word_reject[""-lrb-""] = 1\n    self.annotated_word_reject[""-rrb-""] = 1\n    self.annotated_word_reject[""UNK""] = 1\n\n  def is_money(self, word):\n    if (not (bool(re.search(""[a-z0-9]"", word, re.IGNORECASE)))):\n      return False\n    for i in range(len(word)):\n      if (not (word[i] == ""E"" or word[i] == ""."" or re.search(""[0-9]"",\n                                                             word[i]))):\n        return False\n    return True\n\n  def remove_consecutive(self, ner_tags, ner_values):\n    for i in range(len(ner_tags)):\n      if ((ner_tags[i] == ""NUMBER"" or ner_tags[i] == ""MONEY"" or\n           ner_tags[i] == ""PERCENT"" or ner_tags[i] == ""DATE"") and\n          i + 1 < len(ner_tags) and ner_tags[i] == ner_tags[i + 1] and\n          ner_values[i] == ner_values[i + 1] and ner_values[i] != """"):\n        word = ner_values[i]\n        word = word.replace("">"", """").replace(""<"", """").replace(""="", """").replace(\n            ""%"", """").replace(""~"", """").replace(""$"", """").replace(""\xc2\xa3"", """").replace(\n                ""\xe2\x82\xac"", """")\n        if (re.search(""[A-Z]"", word) and not (is_date(word)) and not (\n            self.is_money(word))):\n          ner_values[i] = ""A""\n        else:\n          ner_values[i] = "",""\n    return ner_tags, ner_values\n\n  def pre_process_sentence(self, tokens, ner_tags, ner_values):\n    sentence = []\n    tokens = tokens.split(""|"")\n    ner_tags = ner_tags.split(""|"")\n    ner_values = ner_values.split(""|"")\n    ner_tags, ner_values = self.remove_consecutive(ner_tags, ner_values)\n    #print ""old: "", tokens\n    for i in range(len(tokens)):\n      word = tokens[i]\n      if (ner_values[i] != """" and\n          (ner_tags[i] == ""NUMBER"" or ner_tags[i] == ""MONEY"" or\n           ner_tags[i] == ""PERCENT"" or ner_tags[i] == ""DATE"")):\n        word = ner_values[i]\n        word = word.replace("">"", """").replace(""<"", """").replace(""="", """").replace(\n            ""%"", """").replace(""~"", """").replace(""$"", """").replace(""\xc2\xa3"", """").replace(\n                ""\xe2\x82\xac"", """")\n        if (re.search(""[A-Z]"", word) and not (is_date(word)) and not (\n            self.is_money(word))):\n          word = tokens[i]\n        if (is_number(ner_values[i])):\n          word = float(ner_values[i])\n        elif (is_number(word)):\n          word = float(word)\n        if (tokens[i] == ""score""):\n          word = ""score""\n      if (is_number(word)):\n        word = float(word)\n      if (not (self.annotated_word_reject.has_key(word))):\n        if (is_number(word) or is_date(word) or self.is_money(word)):\n          sentence.append(word)\n        else:\n          word = full_normalize(word)\n          if (not (self.annotated_word_reject.has_key(word)) and\n              bool(re.search(""[a-z0-9]"", word, re.IGNORECASE))):\n            m = re.search("","", word)\n            sentence.append(word.replace("","", """"))\n    if (len(sentence) == 0):\n      sentence.append(""UNK"")\n    return sentence\n\n  def load_annotated_data(self, in_file):\n    self.annotated_examples = {}\n    self.annotated_tables = {}\n    f = tf.gfile.GFile(in_file, ""r"")\n    counter = 0\n    for line in f:\n      if (counter > 0):\n        line = line.strip()\n        (question_id, utterance, context, target_value, tokens, lemma_tokens,\n         pos_tags, ner_tags, ner_values, target_canon) = line.split(""\\t"")\n        question = self.pre_process_sentence(tokens, ner_tags, ner_values)\n        target_canon = target_canon.split(""|"")\n        self.annotated_examples[question_id] = WikiExample(\n            question_id, question, target_canon, context)\n        self.annotated_tables[context] = []\n      counter += 1\n    print ""Annotated examples loaded "", len(self.annotated_examples)\n    f.close()\n\n  def is_number_column(self, a):\n    for w in a:\n      if (len(w) != 1):\n        return False\n      if (not (is_number(w[0]))):\n        return False\n    return True\n\n  def convert_table(self, table):\n    answer = []\n    for i in range(len(table)):\n      temp = []\n      for j in range(len(table[i])):\n        temp.append("" "".join([str(w) for w in table[i][j]]))\n      answer.append(temp)\n    return answer\n\n  def load_annotated_tables(self):\n    for table in self.annotated_tables.keys():\n      annotated_table = table.replace(""csv"", ""annotated"")\n      orig_columns = []\n      processed_columns = []\n      f = tf.gfile.GFile(os.path.join(self.root_folder, annotated_table), ""r"")\n      counter = 0\n      for line in f:\n        if (counter > 0):\n          line = line.strip()\n          line = line + ""\\t"" * (13 - len(line.split(""\\t"")))\n          (row, col, read_id, content, tokens, lemma_tokens, pos_tags, ner_tags,\n           ner_values, number, date, num2, read_list) = line.split(""\\t"")\n        counter += 1\n      f.close()\n      max_row = int(row)\n      max_col = int(col)\n      for i in range(max_col + 1):\n        orig_columns.append([])\n        processed_columns.append([])\n        for j in range(max_row + 1):\n          orig_columns[i].append(bad_number)\n          processed_columns[i].append(bad_number)\n      #print orig_columns\n      f = tf.gfile.GFile(os.path.join(self.root_folder, annotated_table), ""r"")\n      counter = 0\n      column_names = []\n      for line in f:\n        if (counter > 0):\n          line = line.strip()\n          line = line + ""\\t"" * (13 - len(line.split(""\\t"")))\n          (row, col, read_id, content, tokens, lemma_tokens, pos_tags, ner_tags,\n           ner_values, number, date, num2, read_list) = line.split(""\\t"")\n          entry = self.pre_process_sentence(tokens, ner_tags, ner_values)\n          if (row == ""-1""):\n            column_names.append(entry)\n          else:\n            orig_columns[int(col)][int(row)] = entry\n            if (len(entry) == 1 and is_number(entry[0])):\n              processed_columns[int(col)][int(row)] = float(entry[0])\n            else:\n              for single_entry in entry:\n                if (is_number(single_entry)):\n                  processed_columns[int(col)][int(row)] = float(single_entry)\n                  break\n              nt = ner_tags.split(""|"")\n              nv = ner_values.split(""|"")\n              for i_entry in range(len(tokens.split(""|""))):\n                if (nt[i_entry] == ""DATE"" and\n                    is_number(nv[i_entry].replace(""-"", """").replace(""X"", """"))):\n                  processed_columns[int(col)][int(row)] = float(nv[\n                      i_entry].replace(""-"", """").replace(""X"", """"))\n                  #processed_columns[int(col)][int(row)] =  float(nv[i_entry])\n            if (len(entry) == 1 and (is_number(entry[0]) or is_date(entry[0]) or\n                                     self.is_money(entry[0]))):\n              if (len(entry) == 1 and not (is_number(entry[0])) and\n                  is_date(entry[0])):\n                entry[0] = entry[0].replace(""X"", ""x"")\n        counter += 1\n      word_columns = []\n      processed_word_columns = []\n      word_column_names = []\n      word_column_indices = []\n      number_columns = []\n      processed_number_columns = []\n      number_column_names = []\n      number_column_indices = []\n      for i in range(max_col + 1):\n        if (self.is_number_column(orig_columns[i])):\n          number_column_indices.append(i)\n          number_column_names.append(column_names[i])\n          temp = []\n          for w in orig_columns[i]:\n            if (is_number(w[0])):\n              temp.append(w[0])\n          number_columns.append(temp)\n          processed_number_columns.append(processed_columns[i])\n        else:\n          word_column_indices.append(i)\n          word_column_names.append(column_names[i])\n          word_columns.append(orig_columns[i])\n          processed_word_columns.append(processed_columns[i])\n      table_info = TableInfo(\n          word_columns, word_column_names, word_column_indices, number_columns,\n          number_column_names, number_column_indices, processed_word_columns,\n          processed_number_columns, orig_columns)\n      self.annotated_tables[table] = table_info\n      f.close()\n\n  def answer_classification(self):\n    lookup_questions = 0\n    number_lookup_questions = 0\n    word_lookup_questions = 0\n    ambiguous_lookup_questions = 0\n    number_questions = 0\n    bad_questions = 0\n    ice_bad_questions = 0\n    tot = 0\n    got = 0\n    ice = {}\n    with tf.gfile.GFile(\n        self.root_folder + ""/arvind-with-norms-2.tsv"", mode=""r"") as f:\n      lines = f.readlines()\n      for line in lines:\n        line = line.strip()\n        if (not (self.annotated_examples.has_key(line.split(""\\t"")[0]))):\n          continue\n        if (len(line.split(""\\t"")) == 4):\n          line = line + ""\\t"" * (5 - len(line.split(""\\t"")))\n          if (not (is_number(line.split(""\\t"")[2]))):\n            ice_bad_questions += 1\n        (example_id, ans_index, ans_raw, process_answer,\n         matched_cells) = line.split(""\\t"")\n        if (ice.has_key(example_id)):\n          ice[example_id].append(line.split(""\\t""))\n        else:\n          ice[example_id] = [line.split(""\\t"")]\n    for q_id in self.annotated_examples.keys():\n      tot += 1\n      example = self.annotated_examples[q_id]\n      table_info = self.annotated_tables[example.table_key]\n      # Figure out if the answer is numerical or lookup\n      n_cols = len(table_info.orig_columns)\n      n_rows = len(table_info.orig_columns[0])\n      example.lookup_matrix = np.zeros((n_rows, n_cols))\n      exact_matches = {}\n      for (example_id, ans_index, ans_raw, process_answer,\n           matched_cells) in ice[q_id]:\n        for match_cell in matched_cells.split(""|""):\n          if (len(match_cell.split("","")) == 2):\n            (row, col) = match_cell.split("","")\n            row = int(row)\n            col = int(col)\n            if (row >= 0):\n              exact_matches[ans_index] = 1\n      answer_is_in_table = len(exact_matches) == len(example.answer)\n      if (answer_is_in_table):\n        for (example_id, ans_index, ans_raw, process_answer,\n             matched_cells) in ice[q_id]:\n          for match_cell in matched_cells.split(""|""):\n            if (len(match_cell.split("","")) == 2):\n              (row, col) = match_cell.split("","")\n              row = int(row)\n              col = int(col)\n              example.lookup_matrix[row, col] = float(ans_index) + 1.0\n      example.lookup_number_answer = 0.0\n      if (answer_is_in_table):\n        lookup_questions += 1\n        if len(example.answer) == 1 and is_number(example.answer[0]):\n          example.number_answer = float(example.answer[0])\n          number_lookup_questions += 1\n          example.is_number_lookup = True\n        else:\n          #print ""word lookup""\n          example.calc_answer = example.number_answer = 0.0\n          word_lookup_questions += 1\n          example.is_word_lookup = True\n      else:\n        if (len(example.answer) == 1 and is_number(example.answer[0])):\n          example.number_answer = example.answer[0]\n          example.is_number_calc = True\n        else:\n          bad_questions += 1\n          example.is_bad_example = True\n          example.is_unknown_answer = True\n      example.is_lookup = example.is_word_lookup or example.is_number_lookup\n      if not example.is_word_lookup and not example.is_bad_example:\n        number_questions += 1\n        example.calc_answer = example.answer[0]\n        example.lookup_number_answer = example.calc_answer\n      # Split up the lookup matrix into word part and number part\n      number_column_indices = table_info.number_column_indices\n      word_column_indices = table_info.word_column_indices\n      example.word_columns = table_info.word_columns\n      example.number_columns = table_info.number_columns\n      example.word_column_names = table_info.word_column_names\n      example.processed_number_columns = table_info.processed_number_columns\n      example.processed_word_columns = table_info.processed_word_columns\n      example.number_column_names = table_info.number_column_names\n      example.number_lookup_matrix = example.lookup_matrix[:,\n                                                           number_column_indices]\n      example.word_lookup_matrix = example.lookup_matrix[:, word_column_indices]\n\n  def load(self):\n    train_data = []\n    dev_data = []\n    test_data = []\n    self.load_annotated_data(\n        os.path.join(self.data_folder, ""training.annotated""))\n    self.load_annotated_tables()\n    self.answer_classification()\n    self.train_loader.load()\n    self.dev_loader.load()\n    for i in range(self.train_loader.num_questions()):\n      example = self.train_loader.examples[i]\n      example = self.annotated_examples[example]\n      train_data.append(example)\n    for i in range(self.dev_loader.num_questions()):\n      example = self.dev_loader.examples[i]\n      dev_data.append(self.annotated_examples[example])\n\n    self.load_annotated_data(\n        os.path.join(self.data_folder, ""pristine-unseen-tables.annotated""))\n    self.load_annotated_tables()\n    self.answer_classification()\n    self.test_loader.load()\n    for i in range(self.test_loader.num_questions()):\n      example = self.test_loader.examples[i]\n      test_data.append(self.annotated_examples[example])\n    return train_data, dev_data, test_data\n'"
model_zoo/models/resnet/cifar_input.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""CIFAR dataset input module.\n""""""\n\nimport tensorflow as tf\n\n\ndef build_input(dataset, data_path, batch_size, mode):\n  """"""Build CIFAR image and labels.\n\n  Args:\n    dataset: Either \'cifar10\' or \'cifar100\'.\n    data_path: Filename for data.\n    batch_size: Input batch size.\n    mode: Either \'train\' or \'eval\'.\n  Returns:\n    images: Batches of images. [batch_size, image_size, image_size, 3]\n    labels: Batches of labels. [batch_size, num_classes]\n  Raises:\n    ValueError: when the specified dataset is not supported.\n  """"""\n  image_size = 32\n  if dataset == \'cifar10\':\n    label_bytes = 1\n    label_offset = 0\n    num_classes = 10\n  elif dataset == \'cifar100\':\n    label_bytes = 1\n    label_offset = 1\n    num_classes = 100\n  else:\n    raise ValueError(\'Not supported dataset %s\', dataset)\n\n  depth = 3\n  image_bytes = image_size * image_size * depth\n  record_bytes = label_bytes + label_offset + image_bytes\n\n  data_files = tf.gfile.Glob(data_path)\n  file_queue = tf.train.string_input_producer(data_files, shuffle=True)\n  # Read examples from files in the filename queue.\n  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n  _, value = reader.read(file_queue)\n\n  # Convert these examples to dense labels and processed images.\n  record = tf.reshape(tf.decode_raw(value, tf.uint8), [record_bytes])\n  label = tf.cast(tf.slice(record, [label_offset], [label_bytes]), tf.int32)\n  # Convert from string to [depth * height * width] to [depth, height, width].\n  depth_major = tf.reshape(tf.slice(record, [label_bytes], [image_bytes]),\n                           [depth, image_size, image_size])\n  # Convert from [depth, height, width] to [height, width, depth].\n  image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n\n  if mode == \'train\':\n    image = tf.image.resize_image_with_crop_or_pad(\n        image, image_size+4, image_size+4)\n    image = tf.random_crop(image, [image_size, image_size, 3])\n    image = tf.image.random_flip_left_right(image)\n    # Brightness/saturation/constrast provides small gains .2%~.5% on cifar.\n    # image = tf.image.random_brightness(image, max_delta=63. / 255.)\n    # image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n    # image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n    image = tf.image.per_image_whitening(image)\n\n    example_queue = tf.RandomShuffleQueue(\n        capacity=16 * batch_size,\n        min_after_dequeue=8 * batch_size,\n        dtypes=[tf.float32, tf.int32],\n        shapes=[[image_size, image_size, depth], [1]])\n    num_threads = 16\n  else:\n    image = tf.image.resize_image_with_crop_or_pad(\n        image, image_size, image_size)\n    image = tf.image.per_image_whitening(image)\n\n    example_queue = tf.FIFOQueue(\n        3 * batch_size,\n        dtypes=[tf.float32, tf.int32],\n        shapes=[[image_size, image_size, depth], [1]])\n    num_threads = 1\n\n  example_enqueue_op = example_queue.enqueue([image, label])\n  tf.train.add_queue_runner(tf.train.queue_runner.QueueRunner(\n      example_queue, [example_enqueue_op] * num_threads))\n\n  # Read \'batch\' labels + images from the example queue.\n  images, labels = example_queue.dequeue_many(batch_size)\n  labels = tf.reshape(labels, [batch_size, 1])\n  indices = tf.reshape(tf.range(0, batch_size, 1), [batch_size, 1])\n  labels = tf.sparse_to_dense(\n      tf.concat(1, [indices, labels]),\n      [batch_size, num_classes], 1.0, 0.0)\n\n  assert len(images.get_shape()) == 4\n  assert images.get_shape()[0] == batch_size\n  assert images.get_shape()[-1] == 3\n  assert len(labels.get_shape()) == 2\n  assert labels.get_shape()[0] == batch_size\n  assert labels.get_shape()[1] == num_classes\n\n  # Display the training images in the visualizer.\n  tf.image_summary(\'images\', images)\n  return images, labels\n'"
model_zoo/models/resnet/resnet_main.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""ResNet Train/Eval module.\n""""""\nimport sys\nimport time\n\nimport cifar_input\nimport numpy as np\nimport resnet_model\nimport tensorflow as tf\n\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_string(\'dataset\', \'cifar10\', \'cifar10 or cifar100.\')\ntf.app.flags.DEFINE_string(\'mode\', \'train\', \'train or eval.\')\ntf.app.flags.DEFINE_string(\'train_data_path\', \'\', \'Filepattern for training data.\')\ntf.app.flags.DEFINE_string(\'eval_data_path\', \'\', \'Filepattern for eval data\')\ntf.app.flags.DEFINE_integer(\'image_size\', 32, \'Image side length.\')\ntf.app.flags.DEFINE_string(\'train_dir\', \'\',\n                           \'Directory to keep training outputs.\')\ntf.app.flags.DEFINE_string(\'eval_dir\', \'\',\n                           \'Directory to keep eval outputs.\')\ntf.app.flags.DEFINE_integer(\'eval_batch_count\', 50,\n                            \'Number of batches to eval.\')\ntf.app.flags.DEFINE_bool(\'eval_once\', False,\n                         \'Whether evaluate the model only once.\')\ntf.app.flags.DEFINE_string(\'log_root\', \'\',\n                           \'Directory to keep the checkpoints. Should be a \'\n                           \'parent directory of FLAGS.train_dir/eval_dir.\')\ntf.app.flags.DEFINE_integer(\'num_gpus\', 0,\n                            \'Number of gpus used for training. (0 or 1)\')\n\n\ndef train(hps):\n  """"""Training loop.""""""\n  images, labels = cifar_input.build_input(\n      FLAGS.dataset, FLAGS.train_data_path, hps.batch_size, FLAGS.mode)\n  model = resnet_model.ResNet(hps, images, labels, FLAGS.mode)\n  model.build_graph()\n  summary_writer = tf.train.SummaryWriter(FLAGS.train_dir)\n\n  sv = tf.train.Supervisor(logdir=FLAGS.log_root,\n                           is_chief=True,\n                           summary_op=None,\n                           save_summaries_secs=60,\n                           save_model_secs=300,\n                           global_step=model.global_step)\n  sess = sv.prepare_or_wait_for_session(\n      config=tf.ConfigProto(allow_soft_placement=True))\n\n  step = 0\n  lrn_rate = 0.1\n\n  while not sv.should_stop():\n    (_, summaries, loss, predictions, truth, train_step) = sess.run(\n        [model.train_op, model.summaries, model.cost, model.predictions,\n         model.labels, model.global_step],\n        feed_dict={model.lrn_rate: lrn_rate})\n\n    if train_step < 40000:\n      lrn_rate = 0.1\n    elif train_step < 60000:\n      lrn_rate = 0.01\n    elif train_step < 80000:\n      lrn_rate = 0.001\n    else:\n      lrn_rate = 0.0001\n\n    truth = np.argmax(truth, axis=1)\n    predictions = np.argmax(predictions, axis=1)\n    precision = np.mean(truth == predictions)\n\n    step += 1\n    if step % 100 == 0:\n      precision_summ = tf.Summary()\n      precision_summ.value.add(\n          tag=\'Precision\', simple_value=precision)\n      summary_writer.add_summary(precision_summ, train_step)\n      summary_writer.add_summary(summaries, train_step)\n      tf.logging.info(\'loss: %.3f, precision: %.3f\\n\' % (loss, precision))\n      summary_writer.flush()\n\n  sv.Stop()\n\n\ndef evaluate(hps):\n  """"""Eval loop.""""""\n  images, labels = cifar_input.build_input(\n      FLAGS.dataset, FLAGS.eval_data_path, hps.batch_size, FLAGS.mode)\n  model = resnet_model.ResNet(hps, images, labels, FLAGS.mode)\n  model.build_graph()\n  saver = tf.train.Saver()\n  summary_writer = tf.train.SummaryWriter(FLAGS.eval_dir)\n\n  sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n  tf.train.start_queue_runners(sess)\n\n  best_precision = 0.0\n  while True:\n    time.sleep(60)\n    try:\n      ckpt_state = tf.train.get_checkpoint_state(FLAGS.log_root)\n    except tf.errors.OutOfRangeError as e:\n      tf.logging.error(\'Cannot restore checkpoint: %s\', e)\n      continue\n    if not (ckpt_state and ckpt_state.model_checkpoint_path):\n      tf.logging.info(\'No model to eval yet at %s\', FLAGS.log_root)\n      continue\n    tf.logging.info(\'Loading checkpoint %s\', ckpt_state.model_checkpoint_path)\n    saver.restore(sess, ckpt_state.model_checkpoint_path)\n\n    total_prediction, correct_prediction = 0, 0\n    for _ in xrange(FLAGS.eval_batch_count):\n      (summaries, loss, predictions, truth, train_step) = sess.run(\n          [model.summaries, model.cost, model.predictions,\n           model.labels, model.global_step])\n\n      truth = np.argmax(truth, axis=1)\n      predictions = np.argmax(predictions, axis=1)\n      correct_prediction += np.sum(truth == predictions)\n      total_prediction += predictions.shape[0]\n\n    precision = 1.0 * correct_prediction / total_prediction\n    best_precision = max(precision, best_precision)\n\n    precision_summ = tf.Summary()\n    precision_summ.value.add(\n        tag=\'Precision\', simple_value=precision)\n    summary_writer.add_summary(precision_summ, train_step)\n    best_precision_summ = tf.Summary()\n    best_precision_summ.value.add(\n        tag=\'Best Precision\', simple_value=best_precision)\n    summary_writer.add_summary(best_precision_summ, train_step)\n    summary_writer.add_summary(summaries, train_step)\n    tf.logging.info(\'loss: %.3f, precision: %.3f, best precision: %.3f\\n\' %\n                    (loss, precision, best_precision))\n    summary_writer.flush()\n\n    if FLAGS.eval_once:\n      break\n\n\ndef main(_):\n  if FLAGS.num_gpus == 0:\n    dev = \'/cpu:0\'\n  elif FLAGS.num_gpus == 1:\n    dev = \'/gpu:0\'\n  else:\n    raise ValueError(\'Only support 0 or 1 gpu.\')\n\n  if FLAGS.mode == \'train\':\n    batch_size = 128\n  elif FLAGS.mode == \'eval\':\n    batch_size = 100\n\n  if FLAGS.dataset == \'cifar10\':\n    num_classes = 10\n  elif FLAGS.dataset == \'cifar100\':\n    num_classes = 100\n\n  hps = resnet_model.HParams(batch_size=batch_size,\n                             num_classes=num_classes,\n                             min_lrn_rate=0.0001,\n                             lrn_rate=0.1,\n                             num_residual_units=5,\n                             use_bottleneck=False,\n                             weight_decay_rate=0.0002,\n                             relu_leakiness=0.1,\n                             optimizer=\'mom\')\n\n  with tf.device(dev):\n    if FLAGS.mode == \'train\':\n      train(hps)\n    elif FLAGS.mode == \'eval\':\n      evaluate(hps)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/resnet/resnet_model.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""ResNet model.\n\nRelated papers:\nhttps://arxiv.org/pdf/1603.05027v2.pdf\nhttps://arxiv.org/pdf/1512.03385v1.pdf\nhttps://arxiv.org/pdf/1605.07146v1.pdf\n""""""\nfrom collections import namedtuple\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.training import moving_averages\n\n\nHParams = namedtuple(\'HParams\',\n                     \'batch_size, num_classes, min_lrn_rate, lrn_rate, \'\n                     \'num_residual_units, use_bottleneck, weight_decay_rate, \'\n                     \'relu_leakiness, optimizer\')\n\n\nclass ResNet(object):\n  """"""ResNet model.""""""\n\n  def __init__(self, hps, images, labels, mode):\n    """"""ResNet constructor.\n\n    Args:\n      hps: Hyperparameters.\n      images: Batches of images. [batch_size, image_size, image_size, 3]\n      labels: Batches of labels. [batch_size, num_classes]\n      mode: One of \'train\' and \'eval\'.\n    """"""\n    self.hps = hps\n    self._images = images\n    self.labels = labels\n    self.mode = mode\n\n    self._extra_train_ops = []\n\n  def build_graph(self):\n    """"""Build a whole graph for the model.""""""\n    self.global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n    self._build_model()\n    if self.mode == \'train\':\n      self._build_train_op()\n    self.summaries = tf.merge_all_summaries()\n\n  def _stride_arr(self, stride):\n    """"""Map a stride scalar to the stride array for tf.nn.conv2d.""""""\n    return [1, stride, stride, 1]\n\n  def _build_model(self):\n    """"""Build the core model within the graph.""""""\n    with tf.variable_scope(\'init\'):\n      x = self._images\n      x = self._conv(\'init_conv\', x, 3, 3, 16, self._stride_arr(1))\n\n    strides = [1, 2, 2]\n    activate_before_residual = [True, False, False]\n    if self.hps.use_bottleneck:\n      res_func = self._bottleneck_residual\n      filters = [16, 64, 128, 256]\n    else:\n      res_func = self._residual\n      filters = [16, 16, 32, 64]\n      # Uncomment the following codes to use w28-10 wide residual network.\n      # It is more memory efficient than very deep residual network and has\n      # comparably good performance.\n      # https://arxiv.org/pdf/1605.07146v1.pdf\n      # filters = [16, 160, 320, 640]\n      # Update hps.num_residual_units to 9\n\n    with tf.variable_scope(\'unit_1_0\'):\n      x = res_func(x, filters[0], filters[1], self._stride_arr(strides[0]),\n                   activate_before_residual[0])\n    for i in xrange(1, self.hps.num_residual_units):\n      with tf.variable_scope(\'unit_1_%d\' % i):\n        x = res_func(x, filters[1], filters[1], self._stride_arr(1), False)\n\n    with tf.variable_scope(\'unit_2_0\'):\n      x = res_func(x, filters[1], filters[2], self._stride_arr(strides[1]),\n                   activate_before_residual[1])\n    for i in xrange(1, self.hps.num_residual_units):\n      with tf.variable_scope(\'unit_2_%d\' % i):\n        x = res_func(x, filters[2], filters[2], self._stride_arr(1), False)\n\n    with tf.variable_scope(\'unit_3_0\'):\n      x = res_func(x, filters[2], filters[3], self._stride_arr(strides[2]),\n                   activate_before_residual[2])\n    for i in xrange(1, self.hps.num_residual_units):\n      with tf.variable_scope(\'unit_3_%d\' % i):\n        x = res_func(x, filters[3], filters[3], self._stride_arr(1), False)\n\n    with tf.variable_scope(\'unit_last\'):\n      x = self._batch_norm(\'final_bn\', x)\n      x = self._relu(x, self.hps.relu_leakiness)\n      x = self._global_avg_pool(x)\n\n    with tf.variable_scope(\'logit\'):\n      logits = self._fully_connected(x, self.hps.num_classes)\n      self.predictions = tf.nn.softmax(logits)\n\n    with tf.variable_scope(\'costs\'):\n      xent = tf.nn.softmax_cross_entropy_with_logits(\n          logits, self.labels)\n      self.cost = tf.reduce_mean(xent, name=\'xent\')\n      self.cost += self._decay()\n\n      tf.scalar_summary(\'cost\', self.cost)\n\n  def _build_train_op(self):\n    """"""Build training specific ops for the graph.""""""\n    self.lrn_rate = tf.constant(self.hps.lrn_rate, tf.float32)\n    tf.scalar_summary(\'learning rate\', self.lrn_rate)\n\n    trainable_variables = tf.trainable_variables()\n    grads = tf.gradients(self.cost, trainable_variables)\n\n    if self.hps.optimizer == \'sgd\':\n      optimizer = tf.train.GradientDescentOptimizer(self.lrn_rate)\n    elif self.hps.optimizer == \'mom\':\n      optimizer = tf.train.MomentumOptimizer(self.lrn_rate, 0.9)\n\n    apply_op = optimizer.apply_gradients(\n        zip(grads, trainable_variables),\n        global_step=self.global_step, name=\'train_step\')\n\n    train_ops = [apply_op] + self._extra_train_ops\n    self.train_op = tf.group(*train_ops)\n\n  # TODO(xpan): Consider batch_norm in contrib/layers/python/layers/layers.py\n  def _batch_norm(self, name, x):\n    """"""Batch normalization.""""""\n    with tf.variable_scope(name):\n      params_shape = [x.get_shape()[-1]]\n\n      beta = tf.get_variable(\n          \'beta\', params_shape, tf.float32,\n          initializer=tf.constant_initializer(0.0, tf.float32))\n      gamma = tf.get_variable(\n          \'gamma\', params_shape, tf.float32,\n          initializer=tf.constant_initializer(1.0, tf.float32))\n\n      if self.mode == \'train\':\n        mean, variance = tf.nn.moments(x, [0, 1, 2], name=\'moments\')\n\n        moving_mean = tf.get_variable(\n            \'moving_mean\', params_shape, tf.float32,\n            initializer=tf.constant_initializer(0.0, tf.float32),\n            trainable=False)\n        moving_variance = tf.get_variable(\n            \'moving_variance\', params_shape, tf.float32,\n            initializer=tf.constant_initializer(1.0, tf.float32),\n            trainable=False)\n\n        self._extra_train_ops.append(moving_averages.assign_moving_average(\n            moving_mean, mean, 0.9))\n        self._extra_train_ops.append(moving_averages.assign_moving_average(\n            moving_variance, variance, 0.9))\n      else:\n        mean = tf.get_variable(\n            \'moving_mean\', params_shape, tf.float32,\n            initializer=tf.constant_initializer(0.0, tf.float32),\n            trainable=False)\n        variance = tf.get_variable(\n            \'moving_variance\', params_shape, tf.float32,\n            initializer=tf.constant_initializer(1.0, tf.float32),\n            trainable=False)\n        tf.histogram_summary(mean.op.name, mean)\n        tf.histogram_summary(variance.op.name, variance)\n      # elipson used to be 1e-5. Maybe 0.001 solves NaN problem in deeper net.\n      y = tf.nn.batch_normalization(\n          x, mean, variance, beta, gamma, 0.001)\n      y.set_shape(x.get_shape())\n      return y\n\n  def _residual(self, x, in_filter, out_filter, stride,\n                activate_before_residual=False):\n    """"""Residual unit with 2 sub layers.""""""\n    if activate_before_residual:\n      with tf.variable_scope(\'shared_activation\'):\n        x = self._batch_norm(\'init_bn\', x)\n        x = self._relu(x, self.hps.relu_leakiness)\n        orig_x = x\n    else:\n      with tf.variable_scope(\'residual_only_activation\'):\n        orig_x = x\n        x = self._batch_norm(\'init_bn\', x)\n        x = self._relu(x, self.hps.relu_leakiness)\n\n    with tf.variable_scope(\'sub1\'):\n      x = self._conv(\'conv1\', x, 3, in_filter, out_filter, stride)\n\n    with tf.variable_scope(\'sub2\'):\n      x = self._batch_norm(\'bn2\', x)\n      x = self._relu(x, self.hps.relu_leakiness)\n      x = self._conv(\'conv2\', x, 3, out_filter, out_filter, [1, 1, 1, 1])\n\n    with tf.variable_scope(\'sub_add\'):\n      if in_filter != out_filter:\n        orig_x = tf.nn.avg_pool(orig_x, stride, stride, \'VALID\')\n        orig_x = tf.pad(\n            orig_x, [[0, 0], [0, 0], [0, 0],\n                     [(out_filter-in_filter)//2, (out_filter-in_filter)//2]])\n      x += orig_x\n\n    tf.logging.info(\'image after unit %s\', x.get_shape())\n    return x\n\n  def _bottleneck_residual(self, x, in_filter, out_filter, stride,\n                           activate_before_residual=False):\n    """"""Bottleneck resisual unit with 3 sub layers.""""""\n    if activate_before_residual:\n      with tf.variable_scope(\'common_bn_relu\'):\n        x = self._batch_norm(\'init_bn\', x)\n        x = self._relu(x, self.hps.relu_leakiness)\n        orig_x = x\n    else:\n      with tf.variable_scope(\'residual_bn_relu\'):\n        orig_x = x\n        x = self._batch_norm(\'init_bn\', x)\n        x = self._relu(x, self.hps.relu_leakiness)\n\n    with tf.variable_scope(\'sub1\'):\n      x = self._conv(\'conv1\', x, 1, in_filter, out_filter/4, stride)\n\n    with tf.variable_scope(\'sub2\'):\n      x = self._batch_norm(\'bn2\', x)\n      x = self._relu(x, self.hps.relu_leakiness)\n      x = self._conv(\'conv2\', x, 3, out_filter/4, out_filter/4, [1, 1, 1, 1])\n\n    with tf.variable_scope(\'sub3\'):\n      x = self._batch_norm(\'bn3\', x)\n      x = self._relu(x, self.hps.relu_leakiness)\n      x = self._conv(\'conv3\', x, 1, out_filter/4, out_filter, [1, 1, 1, 1])\n\n    with tf.variable_scope(\'sub_add\'):\n      if in_filter != out_filter:\n        orig_x = self._conv(\'project\', orig_x, 1, in_filter, out_filter, stride)\n      x += orig_x\n\n    tf.logging.info(\'image after unit %s\', x.get_shape())\n    return x\n\n  def _decay(self):\n    """"""L2 weight decay loss.""""""\n    costs = []\n    for var in tf.trainable_variables():\n      if var.op.name.find(r\'DW\') > 0:\n        costs.append(tf.nn.l2_loss(var))\n        # tf.histogram_summary(var.op.name, var)\n\n    return tf.mul(self.hps.weight_decay_rate, tf.add_n(costs))\n\n  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\n    """"""Convolution.""""""\n    with tf.variable_scope(name):\n      n = filter_size * filter_size * out_filters\n      kernel = tf.get_variable(\n          \'DW\', [filter_size, filter_size, in_filters, out_filters],\n          tf.float32, initializer=tf.random_normal_initializer(\n              stddev=np.sqrt(2.0/n)))\n      return tf.nn.conv2d(x, kernel, strides, padding=\'SAME\')\n\n  def _relu(self, x, leakiness=0.0):\n    """"""Relu, with optional leaky support.""""""\n    return tf.select(tf.less(x, 0.0), leakiness * x, x, name=\'leaky_relu\')\n\n  def _fully_connected(self, x, out_dim):\n    """"""FullyConnected layer for final output.""""""\n    x = tf.reshape(x, [self.hps.batch_size, -1])\n    w = tf.get_variable(\n        \'DW\', [x.get_shape()[1], out_dim],\n        initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n    b = tf.get_variable(\'biases\', [out_dim],\n                        initializer=tf.constant_initializer())\n    return tf.nn.xw_plus_b(x, w, b)\n\n  def _global_avg_pool(self, x):\n    assert x.get_shape().ndims == 4\n    return tf.reduce_mean(x, [1, 2])\n'"
model_zoo/models/slim/download_and_convert_data.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts a particular dataset.\n\nUsage:\n```shell\n\n$ python download_and_convert_data.py \\\n    --dataset_name=mnist \\\n    --dataset_dir=/tmp/mnist\n\n$ python download_and_convert_data.py \\\n    --dataset_name=cifar10 \\\n    --dataset_dir=/tmp/cifar10\n\n$ python download_and_convert_data.py \\\n    --dataset_name=flowers \\\n    --dataset_dir=/tmp/flowers\n```\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom datasets import download_and_convert_cifar10\nfrom datasets import download_and_convert_flowers\nfrom datasets import download_and_convert_mnist\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\',\n    None,\n    \'The name of the dataset to convert, one of ""cifar10"", ""flowers"", ""mnist"".\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\',\n    None,\n    \'The directory where the output TFRecords and temporary files are saved.\')\n\n\ndef main(_):\n  if not FLAGS.dataset_name:\n    raise ValueError(\'You must supply the dataset name with --dataset_name\')\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  if FLAGS.dataset_name == \'cifar10\':\n    download_and_convert_cifar10.run(FLAGS.dataset_dir)\n  elif FLAGS.dataset_name == \'flowers\':\n    download_and_convert_flowers.run(FLAGS.dataset_dir)\n  elif FLAGS.dataset_name == \'mnist\':\n    download_and_convert_mnist.run(FLAGS.dataset_dir)\n  else:\n    raise ValueError(\n        \'dataset_name [%s] was not recognized.\' % FLAGS.dataset_dir)\n\nif __name__ == \'__main__\':\n  tf.app.run()\n\n'"
model_zoo/models/slim/eval_image_classifier.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic evaluation script that evaluates a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport tensorflow as tf\n\nfrom datasets import dataset_factory\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 100, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'max_num_batches\', None,\n    \'Max number of batches to evaluate by default use all.\')\n\ntf.app.flags.DEFINE_string(\n    \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', \'/tmp/tfmodel/\',\n    \'The directory where the model was written to or an absolute path to a \'\n    \'checkpoint file.\')\n\ntf.app.flags.DEFINE_string(\n    \'eval_dir\', \'/tmp/tfmodel/\', \'Directory where the results are saved to.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'test\', \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'inception_v3\', \'The name of the architecture to evaluate.\')\n\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\ntf.app.flags.DEFINE_integer(\n    \'eval_image_size\', None, \'Eval image size\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n    tf_global_step = slim.get_or_create_global_step()\n\n    ######################\n    # Select the dataset #\n    ######################\n    dataset = dataset_factory.get_dataset(\n        FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n    ####################\n    # Select the model #\n    ####################\n    network_fn = nets_factory.get_network_fn(\n        FLAGS.model_name,\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\n        is_training=False)\n\n    ##############################################################\n    # Create a dataset provider that loads data from the dataset #\n    ##############################################################\n    provider = slim.dataset_data_provider.DatasetDataProvider(\n        dataset,\n        shuffle=False,\n        common_queue_capacity=2 * FLAGS.batch_size,\n        common_queue_min=FLAGS.batch_size)\n    [image, label] = provider.get([\'image\', \'label\'])\n    label -= FLAGS.labels_offset\n\n    #####################################\n    # Select the preprocessing function #\n    #####################################\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n        preprocessing_name,\n        is_training=False)\n\n    eval_image_size = FLAGS.eval_image_size or network_fn.default_image_size\n\n    image = image_preprocessing_fn(image, eval_image_size, eval_image_size)\n\n    images, labels = tf.train.batch(\n        [image, label],\n        batch_size=FLAGS.batch_size,\n        num_threads=FLAGS.num_preprocessing_threads,\n        capacity=5 * FLAGS.batch_size)\n\n    ####################\n    # Define the model #\n    ####################\n    logits, _ = network_fn(images)\n\n    if FLAGS.moving_average_decay:\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, tf_global_step)\n      variables_to_restore = variable_averages.variables_to_restore(\n          slim.get_model_variables())\n      variables_to_restore[tf_global_step.op.name] = tf_global_step\n    else:\n      variables_to_restore = slim.get_variables_to_restore()\n\n    predictions = tf.argmax(logits, 1)\n    labels = tf.squeeze(labels)\n\n    # Define the metrics:\n    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n        \'Accuracy\': slim.metrics.streaming_accuracy(predictions, labels),\n        \'Recall@5\': slim.metrics.streaming_recall_at_k(\n            logits, labels, 5),\n    })\n\n    # Print the summaries to screen.\n    for name, value in names_to_values.iteritems():\n      summary_name = \'eval/%s\' % name\n      op = tf.scalar_summary(summary_name, value, collections=[])\n      op = tf.Print(op, [value], summary_name)\n      tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)\n\n    # TODO(sguada) use num_epochs=1\n    if FLAGS.max_num_batches:\n      num_batches = FLAGS.max_num_batches\n    else:\n      # This ensures that we make a single pass over all of the data.\n      num_batches = math.ceil(dataset.num_samples / float(FLAGS.batch_size))\n\n    if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n      checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n    else:\n      checkpoint_path = FLAGS.checkpoint_path\n\n    tf.logging.info(\'Evaluating %s\' % checkpoint_path)\n\n    slim.evaluation.evaluate_once(\n        master=FLAGS.master,\n        checkpoint_path=checkpoint_path,\n        logdir=FLAGS.eval_dir,\n        num_evals=num_batches,\n        eval_op=names_to_updates.values(),\n        variables_to_restore=variables_to_restore)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/slim/train_image_classifier.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generic training script that trains a model using a given dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom datasets import dataset_factory\nfrom deployment import model_deploy\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_string(\n    \'master\', \'\', \'The address of the TensorFlow master to use.\')\n\ntf.app.flags.DEFINE_string(\n    \'train_dir\', \'/tmp/tfmodel/\',\n    \'Directory where checkpoints and event logs are written to.\')\n\ntf.app.flags.DEFINE_integer(\'num_clones\', 1,\n                            \'Number of model clones to deploy.\')\n\ntf.app.flags.DEFINE_boolean(\'clone_on_cpu\', False,\n                            \'Use CPUs to deploy clones.\')\n\ntf.app.flags.DEFINE_integer(\'worker_replicas\', 1, \'Number of worker replicas.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_ps_tasks\', 0,\n    \'The number of parameter servers. If the value is 0, then the parameters \'\n    \'are handled locally by the worker.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_readers\', 4,\n    \'The number of parallel readers that read data from the dataset.\')\n\ntf.app.flags.DEFINE_integer(\n    \'num_preprocessing_threads\', 4,\n    \'The number of threads used to create the batches.\')\n\ntf.app.flags.DEFINE_integer(\n    \'log_every_n_steps\', 10,\n    \'The frequency with which logs are print.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_summaries_secs\', 600,\n    \'The frequency with which summaries are saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'save_interval_secs\', 600,\n    \'The frequency with which the model is saved, in seconds.\')\n\ntf.app.flags.DEFINE_integer(\n    \'task\', 0, \'Task id of the replica running the training.\')\n\n######################\n# Optimization Flags #\n######################\n\ntf.app.flags.DEFINE_float(\n    \'weight_decay\', 0.00004, \'The weight decay on the model weights.\')\n\ntf.app.flags.DEFINE_string(\n    \'optimizer\', \'rmsprop\',\n    \'The name of the optimizer, one of ""adadelta"", ""adagrad"", ""adam"",\'\n    \'""ftrl"", ""momentum"", ""sgd"" or ""rmsprop"".\')\n\ntf.app.flags.DEFINE_float(\n    \'adadelta_rho\', 0.95,\n    \'The decay rate for adadelta.\')\n\ntf.app.flags.DEFINE_float(\n    \'adagrad_initial_accumulator_value\', 0.1,\n    \'Starting value for the AdaGrad accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta1\', 0.9,\n    \'The exponential decay rate for the 1st moment estimates.\')\n\ntf.app.flags.DEFINE_float(\n    \'adam_beta2\', 0.999,\n    \'The exponential decay rate for the 2nd moment estimates.\')\n\ntf.app.flags.DEFINE_float(\'opt_epsilon\', 1.0, \'Epsilon term for the optimizer.\')\n\ntf.app.flags.DEFINE_float(\'ftrl_learning_rate_power\', -0.5,\n                          \'The learning rate power.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_initial_accumulator_value\', 0.1,\n    \'Starting value for the FTRL accumulators.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_l1\', 0.0, \'The FTRL l1 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'ftrl_l2\', 0.0, \'The FTRL l2 regularization strength.\')\n\ntf.app.flags.DEFINE_float(\n    \'momentum\', 0.9,\n    \'The momentum for the MomentumOptimizer and RMSPropOptimizer.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_momentum\', 0.9, \'Momentum.\')\n\ntf.app.flags.DEFINE_float(\'rmsprop_decay\', 0.9, \'Decay term for RMSProp.\')\n\n#######################\n# Learning Rate Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'learning_rate_decay_type\',\n    \'exponential\',\n    \'Specifies how the learning rate is decayed. One of ""fixed"", ""exponential"",\'\n    \' or ""polynomial""\')\n\ntf.app.flags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'end_learning_rate\', 0.0001,\n    \'The minimal end learning rate used by a polynomial decay learning rate.\')\n\ntf.app.flags.DEFINE_float(\n    \'label_smoothing\', 0.0, \'The amount of label smoothing.\')\n\ntf.app.flags.DEFINE_float(\n    \'learning_rate_decay_factor\', 0.94, \'Learning rate decay factor.\')\n\ntf.app.flags.DEFINE_float(\n    \'num_epochs_per_decay\', 2.0,\n    \'Number of epochs after which learning rate decays.\')\n\ntf.app.flags.DEFINE_bool(\n    \'sync_replicas\', False,\n    \'Whether or not to synchronize the replicas during training.\')\n\ntf.app.flags.DEFINE_integer(\n    \'replicas_to_aggregate\', 1,\n    \'The Number of gradients to collect before updating params.\')\n\ntf.app.flags.DEFINE_float(\n    \'moving_average_decay\', None,\n    \'The decay to use for the moving average.\'\n    \'If left as None, then moving averages are not used.\')\n\n#######################\n# Dataset Flags #\n#######################\n\ntf.app.flags.DEFINE_string(\n    \'dataset_name\', \'imagenet\', \'The name of the dataset to load.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_split_name\', \'train\', \'The name of the train/test split.\')\n\ntf.app.flags.DEFINE_string(\n    \'dataset_dir\', None, \'The directory where the dataset files are stored.\')\n\ntf.app.flags.DEFINE_integer(\n    \'labels_offset\', 0,\n    \'An offset for the labels in the dataset. This flag is primarily used to \'\n    \'evaluate the VGG and ResNet architectures which do not use a background \'\n    \'class for the ImageNet dataset.\')\n\ntf.app.flags.DEFINE_string(\n    \'model_name\', \'inception_v3\', \'The name of the architecture to train.\')\n\ntf.app.flags.DEFINE_string(\n    \'preprocessing_name\', None, \'The name of the preprocessing to use. If left \'\n    \'as `None`, then the model_name flag is used.\')\n\ntf.app.flags.DEFINE_integer(\n    \'batch_size\', 32, \'The number of samples in each batch.\')\n\ntf.app.flags.DEFINE_integer(\n    \'train_image_size\', None, \'Train image size\')\n\ntf.app.flags.DEFINE_integer(\'max_number_of_steps\', None,\n                            \'The maximum number of training steps.\')\n\n#####################\n# Fine-Tuning Flags #\n#####################\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_path\', None,\n    \'The path to a checkpoint from which to fine-tune.\')\n\ntf.app.flags.DEFINE_string(\n    \'checkpoint_exclude_scopes\', None,\n    \'Comma-separated list of scopes of variables to exclude when restoring \'\n    \'from a checkpoint.\')\n\ntf.app.flags.DEFINE_string(\n    \'trainable_scopes\', None,\n    \'Comma-separated list of scopes to filter the set of variables to train.\'\n    \'By default, None would train all the variables.\')\n\ntf.app.flags.DEFINE_boolean(\n    \'ignore_missing_vars\', False,\n    \'When restoring a checkpoint would ignore missing variables.\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step):\n  """"""Configures the learning rate.\n\n  Args:\n    num_samples_per_epoch: The number of samples in each epoch of training.\n    global_step: The global_step tensor.\n\n  Returns:\n    A `Tensor` representing the learning rate.\n\n  Raises:\n    ValueError: if\n  """"""\n  decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n                    FLAGS.num_epochs_per_decay)\n  if FLAGS.sync_replicas:\n    decay_steps /= FLAGS.replicas_to_aggregate\n\n  if FLAGS.learning_rate_decay_type == \'exponential\':\n    return tf.train.exponential_decay(FLAGS.learning_rate,\n                                      global_step,\n                                      decay_steps,\n                                      FLAGS.learning_rate_decay_factor,\n                                      staircase=True,\n                                      name=\'exponential_decay_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'fixed\':\n    return tf.constant(FLAGS.learning_rate, name=\'fixed_learning_rate\')\n  elif FLAGS.learning_rate_decay_type == \'polynomial\':\n    return tf.train.polynomial_decay(FLAGS.learning_rate,\n                                     global_step,\n                                     decay_steps,\n                                     FLAGS.end_learning_rate,\n                                     power=1.0,\n                                     cycle=False,\n                                     name=\'polynomial_decay_learning_rate\')\n  else:\n    raise ValueError(\'learning_rate_decay_type [%s] was not recognized\',\n                     FLAGS.learning_rate_decay_type)\n\n\ndef _configure_optimizer(learning_rate):\n  """"""Configures the optimizer used for training.\n\n  Args:\n    learning_rate: A scalar or `Tensor` learning rate.\n\n  Returns:\n    An instance of an optimizer.\n\n  Raises:\n    ValueError: if FLAGS.optimizer is not recognized.\n  """"""\n  if FLAGS.optimizer == \'adadelta\':\n    optimizer = tf.train.AdadeltaOptimizer(\n        learning_rate,\n        rho=FLAGS.adadelta_rho,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'adagrad\':\n    optimizer = tf.train.AdagradOptimizer(\n        learning_rate,\n        initial_accumulator_value=FLAGS.adagrad_initial_accumulator_value)\n  elif FLAGS.optimizer == \'adam\':\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate,\n        beta1=FLAGS.adam_beta1,\n        beta2=FLAGS.adam_beta2,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'ftrl\':\n    optimizer = tf.train.FtrlOptimizer(\n        learning_rate,\n        learning_rate_power=FLAGS.ftrl_learning_rate_power,\n        initial_accumulator_value=FLAGS.ftrl_initial_accumulator_value,\n        l1_regularization_strength=FLAGS.ftrl_l1,\n        l2_regularization_strength=FLAGS.ftrl_l2)\n  elif FLAGS.optimizer == \'momentum\':\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate,\n        momentum=FLAGS.momentum,\n        name=\'Momentum\')\n  elif FLAGS.optimizer == \'rmsprop\':\n    optimizer = tf.train.RMSPropOptimizer(\n        learning_rate,\n        decay=FLAGS.rmsprop_decay,\n        momentum=FLAGS.rmsprop_momentum,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == \'sgd\':\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  else:\n    raise ValueError(\'Optimizer [%s] was not recognized\', FLAGS.optimizer)\n  return optimizer\n\n\ndef _add_variables_summaries(learning_rate):\n  summaries = []\n  for variable in slim.get_model_variables():\n    summaries.append(tf.histogram_summary(variable.op.name, variable))\n  summaries.append(tf.scalar_summary(\'training/Learning Rate\', learning_rate))\n  return summaries\n\n\ndef _get_init_fn():\n  """"""Returns a function run by the chief worker to warm-start the training.\n\n  Note that the init_fn is only run when initializing the model during the very\n  first global step.\n\n  Returns:\n    An init function run by the supervisor.\n  """"""\n  if FLAGS.checkpoint_path is None:\n    return None\n\n  # Warn the user if a checkpoint exists in the train_dir. Then we\'ll be\n  # ignoring the checkpoint anyway.\n  if tf.train.latest_checkpoint(FLAGS.train_dir):\n    tf.logging.info(\n        \'Ignoring --checkpoint_path because a checkpoint already exists in %s\'\n        % FLAGS.train_dir)\n    return None\n\n  exclusions = []\n  if FLAGS.checkpoint_exclude_scopes:\n    exclusions = [scope.strip()\n                  for scope in FLAGS.checkpoint_exclude_scopes.split(\',\')]\n\n  # TODO(sguada) variables.filter_variables()\n  variables_to_restore = []\n  for var in slim.get_model_variables():\n    excluded = False\n    for exclusion in exclusions:\n      if var.op.name.startswith(exclusion):\n        excluded = True\n        break\n    if not excluded:\n      variables_to_restore.append(var)\n\n  if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n    checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n  else:\n    checkpoint_path = FLAGS.checkpoint_path\n\n  tf.logging.info(\'Fine-tuning from %s\' % checkpoint_path)\n\n  return slim.assign_from_checkpoint_fn(\n      checkpoint_path,\n      variables_to_restore,\n      ignore_missing_vars=FLAGS.ignore_missing_vars)\n\n\ndef _get_variables_to_train():\n  """"""Returns a list of variables to train.\n\n  Returns:\n    A list of variables to train by the optimizer.\n  """"""\n  if FLAGS.trainable_scopes is None:\n    return tf.trainable_variables()\n  else:\n    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(\',\')]\n\n  variables_to_train = []\n  for scope in scopes:\n    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n    variables_to_train.extend(variables)\n  return variables_to_train\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError(\'You must supply the dataset directory with --dataset_dir\')\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n    ######################\n    # Config model_deploy#\n    ######################\n    deploy_config = model_deploy.DeploymentConfig(\n        num_clones=FLAGS.num_clones,\n        clone_on_cpu=FLAGS.clone_on_cpu,\n        replica_id=FLAGS.task,\n        num_replicas=FLAGS.worker_replicas,\n        num_ps_tasks=FLAGS.num_ps_tasks)\n\n    # Create global_step\n    with tf.device(deploy_config.variables_device()):\n      global_step = slim.create_global_step()\n\n    ######################\n    # Select the dataset #\n    ######################\n    dataset = dataset_factory.get_dataset(\n        FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n    ####################\n    # Select the network #\n    ####################\n    network_fn = nets_factory.get_network_fn(\n        FLAGS.model_name,\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\n        weight_decay=FLAGS.weight_decay,\n        is_training=True)\n\n    #####################################\n    # Select the preprocessing function #\n    #####################################\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n        preprocessing_name,\n        is_training=True)\n\n    ##############################################################\n    # Create a dataset provider that loads data from the dataset #\n    ##############################################################\n    with tf.device(deploy_config.inputs_device()):\n      provider = slim.dataset_data_provider.DatasetDataProvider(\n          dataset,\n          num_readers=FLAGS.num_readers,\n          common_queue_capacity=20 * FLAGS.batch_size,\n          common_queue_min=10 * FLAGS.batch_size)\n      [image, label] = provider.get([\'image\', \'label\'])\n      label -= FLAGS.labels_offset\n\n      train_image_size = FLAGS.train_image_size or network_fn.default_image_size\n\n      image = image_preprocessing_fn(image, train_image_size, train_image_size)\n\n      images, labels = tf.train.batch(\n          [image, label],\n          batch_size=FLAGS.batch_size,\n          num_threads=FLAGS.num_preprocessing_threads,\n          capacity=5 * FLAGS.batch_size)\n      labels = slim.one_hot_encoding(\n          labels, dataset.num_classes - FLAGS.labels_offset)\n      batch_queue = slim.prefetch_queue.prefetch_queue(\n          [images, labels], capacity=2 * deploy_config.num_clones)\n\n    ####################\n    # Define the model #\n    ####################\n    def clone_fn(batch_queue):\n      """"""Allows data parallelism by creating multiple clones of network_fn.""""""\n      images, labels = batch_queue.dequeue()\n      logits, end_points = network_fn(images)\n\n      #############################\n      # Specify the loss function #\n      #############################\n      if \'AuxLogits\' in end_points:\n        slim.losses.softmax_cross_entropy(\n            end_points[\'AuxLogits\'], labels,\n            label_smoothing=FLAGS.label_smoothing, weight=0.4, scope=\'aux_loss\')\n      slim.losses.softmax_cross_entropy(\n          logits, labels, label_smoothing=FLAGS.label_smoothing, weight=1.0)\n      return end_points\n\n    # Gather initial summaries.\n    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n    first_clone_scope = deploy_config.clone_scope(0)\n    # Gather update_ops from the first clone. These contain, for example,\n    # the updates for the batch_norm variables created by network_fn.\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n\n    # Add summaries for end_points.\n    end_points = clones[0].outputs\n    for end_point in end_points:\n      x = end_points[end_point]\n      summaries.add(tf.histogram_summary(\'activations/\' + end_point, x))\n      summaries.add(tf.scalar_summary(\'sparsity/\' + end_point,\n                                      tf.nn.zero_fraction(x)))\n\n    # Add summaries for losses.\n    for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n      summaries.add(tf.scalar_summary(\'losses/%s\' % loss.op.name, loss))\n\n    # Add summaries for variables.\n    for variable in slim.get_model_variables():\n      summaries.add(tf.histogram_summary(variable.op.name, variable))\n\n    #################################\n    # Configure the moving averages #\n    #################################\n    if FLAGS.moving_average_decay:\n      moving_average_variables = slim.get_model_variables()\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, global_step)\n    else:\n      moving_average_variables, variable_averages = None, None\n\n    #########################################\n    # Configure the optimization procedure. #\n    #########################################\n    with tf.device(deploy_config.optimizer_device()):\n      learning_rate = _configure_learning_rate(dataset.num_samples, global_step)\n      optimizer = _configure_optimizer(learning_rate)\n      summaries.add(tf.scalar_summary(\'learning_rate\', learning_rate,\n                                      name=\'learning_rate\'))\n\n    if FLAGS.sync_replicas:\n      # If sync_replicas is enabled, the averaging will be done in the chief\n      # queue runner.\n      optimizer = tf.train.SyncReplicasOptimizer(\n          opt=optimizer,\n          replicas_to_aggregate=FLAGS.replicas_to_aggregate,\n          variable_averages=variable_averages,\n          variables_to_average=moving_average_variables,\n          replica_id=tf.constant(FLAGS.task, tf.int32, shape=()),\n          total_num_replicas=FLAGS.worker_replicas)\n    elif FLAGS.moving_average_decay:\n      # Update ops executed locally by trainer.\n      update_ops.append(variable_averages.apply(moving_average_variables))\n\n    # Variables to train.\n    variables_to_train = _get_variables_to_train()\n\n    #  and returns a train_tensor and summary_op\n    total_loss, clones_gradients = model_deploy.optimize_clones(\n        clones,\n        optimizer,\n        var_list=variables_to_train)\n    # Add total_loss to summary.\n    summaries.add(tf.scalar_summary(\'total_loss\', total_loss,\n                                    name=\'total_loss\'))\n\n    # Create gradient updates.\n    grad_updates = optimizer.apply_gradients(clones_gradients,\n                                             global_step=global_step)\n    update_ops.append(grad_updates)\n\n    update_op = tf.group(*update_ops)\n    train_tensor = control_flow_ops.with_dependencies([update_op], total_loss,\n                                                      name=\'train_op\')\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone_scope))\n\n    # Merge all summaries together.\n    summary_op = tf.merge_summary(list(summaries), name=\'summary_op\')\n\n\n    ###########################\n    # Kicks off the training. #\n    ###########################\n    slim.learning.train(\n        train_tensor,\n        logdir=FLAGS.train_dir,\n        master=FLAGS.master,\n        is_chief=(FLAGS.task == 0),\n        init_fn=_get_init_fn(),\n        summary_op=summary_op,\n        number_of_steps=FLAGS.max_number_of_steps,\n        log_every_n_steps=FLAGS.log_every_n_steps,\n        save_summaries_secs=FLAGS.save_summaries_secs,\n        save_interval_secs=FLAGS.save_interval_secs,\n        sync_optimizer=optimizer if FLAGS.sync_replicas else None)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/swivel/glove_to_shards.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Converts a Glove binary co-occurrence matrix into Swivel shards.\n\nUsage:\n\n  glove_to_shards.py --input <coocs> --vocab <vocab> --output_dir <output_dir>\n\nOptions\n\n  --input <coocs>\n      The Glove co-occurrence file.\n\n  --vocab <vocab>\n      Path to the vocabulary text file, one token per line.\n\n  --output_dir <directory>\n      Specifies the touput directory where the various Swivel data\n      files sohuld be placed.\n\n  --shard_size <int>\n      Specifies the shard size; default 4096.\n""""""\n\nfrom __future__ import print_function\n\nimport itertools\nimport os\nimport struct\nimport sys\n\nimport tensorflow as tf\n\nflags = tf.app.flags\n\nflags.DEFINE_string(\'input\', \'coocurrences.bin\', \'Vocabulary file\')\nflags.DEFINE_string(\'vocab\', \'vocab.txt\', \'Vocabulary file\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/swivel_data\', \'Output directory\')\nflags.DEFINE_integer(\'shard_size\', 4096, \'Shard size\')\n\nFLAGS = tf.app.flags.FLAGS\n\nglove_cooc_fmt = struct.Struct(\'iid\')\nshard_cooc_fmt = struct.Struct(\'if\')\n\n\ndef make_shard_files(coocs, nshards, vocab_sz):\n  """"""Chops the binary Glove co-occurrence matrix into shards.\n\n  This reads the Glove binary co-occurrence file and assigns individual\n  co-occurrence counts to the appropriate Swivel shard.\n\n  Args:\n    coocs: the co-occurrnece file to read\n    nshards: the number of shards along one dimension of the square matrix\n    vocab_sz: the vocabulary size\n\n  Returns:\n    A (shard_table, marginals) tuple.  The shard_table maps the row and column\n    shard ID to a file handle containing the co-occurrences for that shard; the\n    marginals contain the marginal sums.\n  """"""\n  row_sums = [0] * vocab_sz\n  col_sums = [0] * vocab_sz\n\n  coocs.seek(0, os.SEEK_END)\n  ncoocs = coocs.tell() / glove_cooc_fmt.size\n  coocs.seek(0, os.SEEK_SET)\n\n  shard_files = {}\n\n  for row in range(nshards):\n    for col in range(nshards):\n      filename = os.path.join(\n          FLAGS.output_dir, \'shard-%03d-%03d.bin\' % (row, col))\n\n      shard_files[(row, col)] = open(filename, \'w+\')\n\n  for ix in xrange(ncoocs):\n    if ix % 1000000 == 0:\n      sys.stdout.write(\'\\rsharding co-occurrences: %0.1f%% (%d/%d)\' % (\n          100.0 * ix / ncoocs, ix, ncoocs))\n\n      sys.stdout.flush()\n\n    bits = coocs.read(glove_cooc_fmt.size)\n    if not bits:\n      break\n\n    # Glove has 1-indexed IDs.\n    row_id, col_id, cnt = glove_cooc_fmt.unpack(bits)\n    if row_id > vocab_sz or col_id > vocab_sz:\n      continue\n\n    row_id -= 1\n    row_shard = row_id % nshards\n    row_off = row_id / nshards\n\n    col_id -= 1\n    col_shard = col_id % nshards\n    col_off = col_id / nshards\n\n    shard_pos = row_off * FLAGS.shard_size + col_off  # row major\n\n    shard_files[(row_shard, col_shard)].write(\n        shard_cooc_fmt.pack(shard_pos, cnt))\n\n    # Accumulate marginals.\n    row_sums[row_id] += cnt\n    col_sums[col_id] += cnt\n\n  sys.stdout.write(\'\\n\')\n\n  if any(abs(r - c) > 0.1 for r, c in itertools.izip(row_sums, col_sums)):\n    print(\'WARNING! Row and column marginals differ; is your matrix symmetric?\',\n          file=sys.stderr)\n\n  return (shard_files, row_sums)\n\ndef main(_):\n  with open(FLAGS.vocab, \'r\') as lines:\n    orig_vocab_sz = sum(1 for _ in lines)\n\n  shard_sz = FLAGS.shard_size\n  vocab_sz = orig_vocab_sz - orig_vocab_sz % shard_sz\n  nshards = vocab_sz / shard_sz\n\n  print(\'vocab size is %d (originally %d), %d %dx%d-element shards\' % (\n      vocab_sz, orig_vocab_sz, nshards * nshards, shard_sz, shard_sz))\n\n  # Create the output directory, if necessary\n  if FLAGS.output_dir and not os.path.isdir(FLAGS.output_dir):\n    os.makedirs(FLAGS.output_dir)\n\n  with open(FLAGS.input, \'r\') as coocs:\n    shard_files, marginals = make_shard_files(coocs, nshards, vocab_sz)\n\n  # Now sort the shards and write the TFRecords.\n  filename = os.path.join(FLAGS.output_dir, \'shards.recs\')\n  with tf.python_io.TFRecordWriter(filename) as writer:\n    ix = 0\n    for (row, col), fh in shard_files.iteritems():\n      ix += 1\n      sys.stdout.write(\'\\rwriting shard %d/%d\' % (ix, len(shard_files)))\n      sys.stdout.flush()\n\n      fh.seek(0)\n      buf = fh.read()\n      os.unlink(fh.name)\n      fh.close()\n\n      coocs = [\n          shard_cooc_fmt.unpack_from(buf, off)\n          for off in range(0, len(buf), shard_cooc_fmt.size)]\n\n      # N.B. we assume that there aren\'t any duplicates here!\n      coocs.sort(key=lambda kv: kv[0])\n\n      def _int64s(xs):\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=list(xs)))\n\n      def _floats(xs):\n        return tf.train.Feature(float_list=tf.train.FloatList(value=list(xs)))\n\n      example = tf.train.Example(features=tf.train.Features(feature={\n          \'global_row\': _int64s(row + nshards * i for i in range(shard_sz)),\n          \'global_col\': _int64s(col + nshards * i for i in range(shard_sz)),\n          \'sparse_local_row\': _int64s(pos / shard_sz for pos, _ in coocs),\n          \'sparse_local_col\': _int64s(pos % shard_sz for pos, _ in coocs),\n          \'sparse_value\': _floats(cnt for _, cnt in coocs)}))\n\n      writer.write(example.SerializeToString())\n\n  print(\'\\nwriting marginals...\')\n\n  with open(os.path.join(FLAGS.output_dir, \'marginals.txt\'), \'w\') as fh:\n    for cnt in marginals:\n      fh.write(\'%0.1f\\n\' % cnt)\n\n  print(\'done!\')\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/swivel/nearest.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Simple tool for inspecting nearest neighbors and analogies.""""""\n\nimport re\nimport sys\nfrom getopt import GetoptError, getopt\n\nfrom vecs import Vecs\n\ntry:\n  opts, args = getopt(sys.argv[1:], \'v:e:\', [\'vocab=\', \'embeddings=\'])\nexcept GetoptError, e:\n  print >> sys.stderr, e\n  sys.exit(2)\n\nopt_vocab = \'vocab.txt\'\nopt_embeddings = None\n\nfor o, a in opts:\n  if o in (\'-v\', \'--vocab\'):\n    opt_vocab = a\n  if o in (\'-e\', \'--embeddings\'):\n    opt_embeddings = a\n\nvecs = Vecs(opt_vocab, opt_embeddings)\n\nwhile True:\n  sys.stdout.write(\'query> \')\n  sys.stdout.flush()\n\n  query = sys.stdin.readline().strip()\n  if not query:\n    break\n\n  parts = re.split(r\'\\s+\', query)\n\n  if len(parts) == 1:\n    res = vecs.neighbors(parts[0])\n\n  elif len(parts) == 3:\n    vs = [vecs.lookup(w) for w in parts]\n    if any(v is None for v in vs):\n      print \'not in vocabulary: %s\' % (\n          \', \'.join(tok for tok, v in zip(parts, vs) if v is None))\n\n      continue\n\n    res = vecs.neighbors(vs[2] - vs[0] + vs[1])\n\n  else:\n    print \'use a single word to query neighbors, or three words for analogy\'\n    continue\n\n  if not res:\n    continue\n\n  for word, sim in res[:20]:\n    print \'%0.4f: %s\' % (sim, word)\n\n  print\n'"
model_zoo/models/swivel/prep.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Prepare a corpus for processing by swivel.\n\nCreates a sharded word co-occurrence matrix from a text file input corpus.\n\nUsage:\n\n  prep.py --output_dir <output-dir> --input <text-file>\n\nOptions:\n\n  --input <filename>\n      The input text.\n\n  --output_dir <directory>\n      Specifies the output directory where the various Swivel data\n      files should be placed.\n\n  --shard_size <int>\n      Specifies the shard size; default 4096.\n\n  --min_count <int>\n      Specifies the minimum number of times a word should appear\n      to be included in the vocabulary; default 5.\n\n  --max_vocab <int>\n      Specifies the maximum vocabulary size; default shard size\n      times 1024.\n\n  --vocab <filename>\n      Use the specified unigram vocabulary instead of generating\n      it from the corpus.\n\n  --window_size <int>\n      Specifies the window size for computing co-occurrence stats;\n      default 10.\n\n  --bufsz <int>\n      The number of co-occurrences that are buffered; default 16M.\n\n""""""\n\nimport itertools\nimport math\nimport os\nimport struct\nimport sys\n\nimport tensorflow as tf\n\nflags = tf.app.flags\n\nflags.DEFINE_string(\'input\', \'\', \'The input text.\')\nflags.DEFINE_string(\'output_dir\', \'/tmp/swivel_data\',\n                    \'Output directory for Swivel data\')\nflags.DEFINE_integer(\'shard_size\', 4096, \'The size for each shard\')\nflags.DEFINE_integer(\'min_count\', 5,\n                     \'The minimum number of times a word should occur to be \'\n                     \'included in the vocabulary\')\nflags.DEFINE_integer(\'max_vocab\', 4096 * 64, \'The maximum vocabulary size\')\nflags.DEFINE_string(\'vocab\', \'\', \'Vocabulary to use instead of generating one\')\nflags.DEFINE_integer(\'window_size\', 10, \'The window size\')\nflags.DEFINE_integer(\'bufsz\', 16 * 1024 * 1024,\n                     \'The number of co-occurrences to buffer\')\n\nFLAGS = flags.FLAGS\n\nshard_cooc_fmt = struct.Struct(\'iif\')\n\n\ndef words(line):\n  """"""Splits a line of text into tokens.""""""\n  return line.strip().split()\n\n\ndef create_vocabulary(lines):\n  """"""Reads text lines and generates a vocabulary.""""""\n  lines.seek(0, os.SEEK_END)\n  nbytes = lines.tell()\n  lines.seek(0, os.SEEK_SET)\n\n  vocab = {}\n  for lineno, line in enumerate(lines, start=1):\n    for word in words(line):\n      vocab.setdefault(word, 0)\n      vocab[word] += 1\n\n    if lineno % 100000 == 0:\n      pos = lines.tell()\n      sys.stdout.write(\'\\rComputing vocabulary: %0.1f%% (%d/%d)...\' % (\n          100.0 * pos / nbytes, pos, nbytes))\n      sys.stdout.flush()\n\n  sys.stdout.write(\'\\n\')\n\n  vocab = [(tok, n) for tok, n in vocab.iteritems() if n >= FLAGS.min_count]\n  vocab.sort(key=lambda kv: (-kv[1], kv[0]))\n\n  num_words = min(len(vocab), FLAGS.max_vocab)\n  if num_words % FLAGS.shard_size != 0:\n    num_words -= num_words % FLAGS.shard_size\n\n  if not num_words:\n    raise Exception(\'empty vocabulary\')\n\n  print \'vocabulary contains %d tokens\' % num_words\n\n  vocab = vocab[:num_words]\n  return [tok for tok, n in vocab]\n\n\ndef write_vocab_and_sums(vocab, sums, vocab_filename, sums_filename):\n  """"""Writes vocabulary and marginal sum files.""""""\n  with open(os.path.join(FLAGS.output_dir, vocab_filename), \'w\') as vocab_out:\n    with open(os.path.join(FLAGS.output_dir, sums_filename), \'w\') as sums_out:\n      for tok, cnt in itertools.izip(vocab, sums):\n        print >> vocab_out, tok\n        print >> sums_out, cnt\n\n\ndef compute_coocs(lines, vocab):\n  """"""Compute the co-occurrence statistics from the text.\n\n  This generates a temporary file for each shard that contains the intermediate\n  counts from the shard: these counts must be subsequently sorted and collated.\n\n  """"""\n  word_to_id = {tok: idx for idx, tok in enumerate(vocab)}\n\n  lines.seek(0, os.SEEK_END)\n  nbytes = lines.tell()\n  lines.seek(0, os.SEEK_SET)\n\n  num_shards = len(vocab) / FLAGS.shard_size\n\n  shardfiles = {}\n  for row in range(num_shards):\n    for col in range(num_shards):\n      filename = os.path.join(\n          FLAGS.output_dir, \'shard-%03d-%03d.tmp\' % (row, col))\n\n      shardfiles[(row, col)] = open(filename, \'w+\')\n\n  def flush_coocs():\n    for (row_id, col_id), cnt in coocs.iteritems():\n      row_shard = row_id % num_shards\n      row_off = row_id / num_shards\n      col_shard = col_id % num_shards\n      col_off = col_id / num_shards\n\n      # Since we only stored (a, b), we emit both (a, b) and (b, a).\n      shardfiles[(row_shard, col_shard)].write(\n          shard_cooc_fmt.pack(row_off, col_off, cnt))\n\n      shardfiles[(col_shard, row_shard)].write(\n          shard_cooc_fmt.pack(col_off, row_off, cnt))\n\n  coocs = {}\n  sums = [0.0] * len(vocab)\n\n  for lineno, line in enumerate(lines, start=1):\n    # Computes the word IDs for each word in the sentence.  This has the effect\n    # of ""stretching"" the window past OOV tokens.\n    wids = filter(\n        lambda wid: wid is not None,\n        (word_to_id.get(w) for w in words(line)))\n\n    for pos in xrange(len(wids)):\n      lid = wids[pos]\n      window_extent = min(FLAGS.window_size + 1, len(wids) - pos)\n      for off in xrange(1, window_extent):\n        rid = wids[pos + off]\n        pair = (min(lid, rid), max(lid, rid))\n        count = 1.0 / off\n        sums[lid] += count\n        sums[rid] += count\n        coocs.setdefault(pair, 0.0)\n        coocs[pair] += count\n\n      sums[lid] += 1.0\n      pair = (lid, lid)\n      coocs.setdefault(pair, 0.0)\n      coocs[pair] += 0.5  # Only add 1/2 since we output (a, b) and (b, a)\n\n    if lineno % 10000 == 0:\n      pos = lines.tell()\n      sys.stdout.write(\'\\rComputing co-occurrences: %0.1f%% (%d/%d)...\' % (\n          100.0 * pos / nbytes, pos, nbytes))\n      sys.stdout.flush()\n\n      if len(coocs) > FLAGS.bufsz:\n        flush_coocs()\n        coocs = {}\n\n  flush_coocs()\n  sys.stdout.write(\'\\n\')\n\n  return shardfiles, sums\n\n\ndef write_shards(vocab, shardfiles):\n  """"""Processes the temporary files to generate the final shard data.\n\n  The shard data is stored as a tf.Example protos using a TFRecordWriter. The\n  temporary files are removed from the filesystem once they\'ve been processed.\n\n  """"""\n  num_shards = len(vocab) / FLAGS.shard_size\n\n  ix = 0\n  for (row, col), fh in shardfiles.iteritems():\n    ix += 1\n    sys.stdout.write(\'\\rwriting shard %d/%d\' % (ix, len(shardfiles)))\n    sys.stdout.flush()\n\n    # Read the entire binary co-occurrence and unpack it into an array.\n    fh.seek(0)\n    buf = fh.read()\n    os.unlink(fh.name)\n    fh.close()\n\n    coocs = [\n        shard_cooc_fmt.unpack_from(buf, off)\n        for off in range(0, len(buf), shard_cooc_fmt.size)]\n\n    # Sort and merge co-occurrences for the same pairs.\n    coocs.sort()\n\n    if coocs:\n      current_pos = 0\n      current_row_col = (coocs[current_pos][0], coocs[current_pos][1])\n      for next_pos in range(1, len(coocs)):\n        next_row_col = (coocs[next_pos][0], coocs[next_pos][1])\n        if current_row_col == next_row_col:\n          coocs[current_pos] = (\n              coocs[current_pos][0],\n              coocs[current_pos][1],\n              coocs[current_pos][2] + coocs[next_pos][2])\n        else:\n          current_pos += 1\n          if current_pos < next_pos:\n            coocs[current_pos] = coocs[next_pos]\n\n          current_row_col = (coocs[current_pos][0], coocs[current_pos][1])\n\n      coocs = coocs[:(1 + current_pos)]\n\n    # Convert to a TF Example proto.\n    def _int64s(xs):\n      return tf.train.Feature(int64_list=tf.train.Int64List(value=list(xs)))\n\n    def _floats(xs):\n      return tf.train.Feature(float_list=tf.train.FloatList(value=list(xs)))\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'global_row\': _int64s(\n            row + num_shards * i for i in range(FLAGS.shard_size)),\n        \'global_col\': _int64s(\n            col + num_shards * i for i in range(FLAGS.shard_size)),\n\n        \'sparse_local_row\': _int64s(cooc[0] for cooc in coocs),\n        \'sparse_local_col\': _int64s(cooc[1] for cooc in coocs),\n        \'sparse_value\': _floats(cooc[2] for cooc in coocs),\n    }))\n\n    filename = os.path.join(FLAGS.output_dir, \'shard-%03d-%03d.pb\' % (row, col))\n    with open(filename, \'w\') as out:\n      out.write(example.SerializeToString())\n\n  sys.stdout.write(\'\\n\')\n\n\ndef main(_):\n  # Create the output directory, if necessary\n  if FLAGS.output_dir and not os.path.isdir(FLAGS.output_dir):\n    os.makedirs(FLAGS.output_dir)\n\n  # Read the file onces to create the vocabulary.\n  if FLAGS.vocab:\n    with open(FLAGS.vocab, \'r\') as lines:\n      vocab = [line.strip() for line in lines]\n  else:\n    with open(FLAGS.input, \'r\') as lines:\n      vocab = create_vocabulary(lines)\n\n  # Now read the file again to determine the co-occurrence stats.\n  with open(FLAGS.input, \'r\') as lines:\n    shardfiles, sums = compute_coocs(lines, vocab)\n\n  # Collect individual shards into the shards.recs file.\n  write_shards(vocab, shardfiles)\n\n  # Now write the marginals.  They\'re symmetric for this application.\n  write_vocab_and_sums(vocab, sums, \'row_vocab.txt\', \'row_sums.txt\')\n  write_vocab_and_sums(vocab, sums, \'col_vocab.txt\', \'col_sums.txt\')\n\n  print \'done!\'\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/swivel/swivel.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Submatrix-wise Vector Embedding Learner.\n\nImplementation of SwiVel algorithm described at:\nhttp://arxiv.org/abs/1602.02215\n\nThis program expects an input directory that contains the following files.\n\n  row_vocab.txt, col_vocab.txt\n\n    The row an column vocabulary files.  Each file should contain one token per\n    line; these will be used to generate a tab-separate file containing the\n    trained embeddings.\n\n  row_sums.txt, col_sum.txt\n\n    The matrix row and column marginal sums.  Each file should contain one\n    decimal floating point number per line which corresponds to the marginal\n    count of the matrix for that row or column.\n\n  shards.recs\n\n    A file containing the sub-matrix shards, stored as TFRecords.  Each shard is\n    expected to be a serialzed tf.Example protocol buffer with the following\n    properties:\n\n      global_row: the global row indicies contained in the shard\n      global_col: the global column indicies contained in the shard\n      sparse_local_row, sparse_local_col, sparse_value: three parallel arrays\n      that are a sparse representation of the submatrix counts.\n\nIt will generate embeddings, training from the input directory for the specified\nnumber of epochs.  When complete, it will output the trained vectors to a\ntab-separated file that contains one line per embedding.  Row and column\nembeddings are stored in separate files.\n\n""""""\n\nimport argparse\nimport glob\nimport math\nimport os\nimport sys\nimport time\nimport threading\n\nimport numpy as np\nimport tensorflow as tf\n\nflags = tf.app.flags\n\nflags.DEFINE_string(\'input_base_path\', \'/tmp/swivel_data\',\n                    \'Directory containing input shards, vocabularies, \'\n                    \'and marginals.\')\nflags.DEFINE_string(\'output_base_path\', \'/tmp/swivel_data\',\n                    \'Path where to write the trained embeddings.\')\nflags.DEFINE_integer(\'embedding_size\', 300, \'Size of the embeddings\')\nflags.DEFINE_boolean(\'trainable_bias\', False, \'Biases are trainable\')\nflags.DEFINE_integer(\'submatrix_rows\', 4096, \'Rows in each training submatrix. \'\n                     \'This must match the training data.\')\nflags.DEFINE_integer(\'submatrix_cols\', 4096, \'Rows in each training submatrix. \'\n                     \'This must match the training data.\')\nflags.DEFINE_float(\'loss_multiplier\', 1.0 / 4096,\n                   \'constant multiplier on loss.\')\nflags.DEFINE_float(\'confidence_exponent\', 0.5,\n                   \'Exponent for l2 confidence function\')\nflags.DEFINE_float(\'confidence_scale\', 0.25, \'Scale for l2 confidence function\')\nflags.DEFINE_float(\'confidence_base\', 0.1, \'Base for l2 confidence function\')\nflags.DEFINE_float(\'learning_rate\', 1.0, \'Initial learning rate\')\nflags.DEFINE_integer(\'num_concurrent_steps\', 2,\n                     \'Number of threads to train with\')\nflags.DEFINE_float(\'num_epochs\', 40, \'Number epochs to train for\')\nflags.DEFINE_float(\'per_process_gpu_memory_fraction\', 0.25,\n                   \'Fraction of GPU memory to use\')\n\nFLAGS = flags.FLAGS\n\n\ndef embeddings_with_init(vocab_size, embedding_dim, name):\n  """"""Creates and initializes the embedding tensors.""""""\n  return tf.get_variable(name=name,\n                         shape=[vocab_size, embedding_dim],\n                         initializer=tf.random_normal_initializer(\n                             stddev=math.sqrt(1.0 / embedding_dim)))\n\n\ndef count_matrix_input(filenames, submatrix_rows, submatrix_cols):\n  """"""Reads submatrix shards from disk.""""""\n  filename_queue = tf.train.string_input_producer(filenames)\n  reader = tf.WholeFileReader()\n  _, serialized_example = reader.read(filename_queue)\n  features = tf.parse_single_example(\n      serialized_example,\n      features={\n          \'global_row\': tf.FixedLenFeature([submatrix_rows], dtype=tf.int64),\n          \'global_col\': tf.FixedLenFeature([submatrix_cols], dtype=tf.int64),\n          \'sparse_local_row\': tf.VarLenFeature(dtype=tf.int64),\n          \'sparse_local_col\': tf.VarLenFeature(dtype=tf.int64),\n          \'sparse_value\': tf.VarLenFeature(dtype=tf.float32)\n      })\n\n  global_row = features[\'global_row\']\n  global_col = features[\'global_col\']\n\n  sparse_local_row = features[\'sparse_local_row\'].values\n  sparse_local_col = features[\'sparse_local_col\'].values\n  sparse_count = features[\'sparse_value\'].values\n\n  sparse_indices = tf.concat(1, [tf.expand_dims(sparse_local_row, 1),\n                                 tf.expand_dims(sparse_local_col, 1)])\n  count = tf.sparse_to_dense(sparse_indices, [submatrix_rows, submatrix_cols],\n                             sparse_count)\n\n  queued_global_row, queued_global_col, queued_count = tf.train.batch(\n      [global_row, global_col, count],\n      batch_size=1,\n      num_threads=4,\n      capacity=32)\n\n  queued_global_row = tf.reshape(queued_global_row, [submatrix_rows])\n  queued_global_col = tf.reshape(queued_global_col, [submatrix_cols])\n  queued_count = tf.reshape(queued_count, [submatrix_rows, submatrix_cols])\n\n  return queued_global_row, queued_global_col, queued_count\n\n\ndef read_marginals_file(filename):\n  """"""Reads text file with one number per line to an array.""""""\n  with open(filename) as lines:\n    return [float(line) for line in lines]\n\n\ndef write_embedding_tensor_to_disk(vocab_path, output_path, sess, embedding):\n  """"""Writes tensor to output_path as tsv""""""\n  # Fetch the embedding values from the model\n  embeddings = sess.run(embedding)\n\n  with open(output_path, \'w\') as out_f:\n    with open(vocab_path) as vocab_f:\n      for index, word in enumerate(vocab_f):\n        word = word.strip()\n        embedding = embeddings[index]\n        out_f.write(word + \'\\t\' + \'\\t\'.join([str(x) for x in embedding]) + \'\\n\')\n\n\ndef write_embeddings_to_disk(config, model, sess):\n  """"""Writes row and column embeddings disk""""""\n  # Row Embedding\n  row_vocab_path = config.input_base_path + \'/row_vocab.txt\'\n  row_embedding_output_path = config.output_base_path + \'/row_embedding.tsv\'\n  print \'Writing row embeddings to:\', row_embedding_output_path\n  sys.stdout.flush()\n  write_embedding_tensor_to_disk(row_vocab_path, row_embedding_output_path,\n                                 sess, model.row_embedding)\n\n  # Column Embedding\n  col_vocab_path = config.input_base_path + \'/col_vocab.txt\'\n  col_embedding_output_path = config.output_base_path + \'/col_embedding.tsv\'\n  print \'Writing column embeddings to:\', col_embedding_output_path\n  sys.stdout.flush()\n  write_embedding_tensor_to_disk(col_vocab_path, col_embedding_output_path,\n                                 sess, model.col_embedding)\n\n\nclass SwivelModel(object):\n  """"""Small class to gather needed pieces from a Graph being built.""""""\n\n  def __init__(self, config):\n    """"""Construct graph for dmc.""""""\n    self._config = config\n\n    # Create paths to input data files\n    print \'Reading model from:\', config.input_base_path\n    sys.stdout.flush()\n    count_matrix_files = glob.glob(config.input_base_path + \'/shard-*.pb\')\n    row_sums_path = config.input_base_path + \'/row_sums.txt\'\n    col_sums_path = config.input_base_path + \'/col_sums.txt\'\n\n    # Read marginals\n    row_sums = read_marginals_file(row_sums_path)\n    col_sums = read_marginals_file(col_sums_path)\n\n    self.n_rows = len(row_sums)\n    self.n_cols = len(col_sums)\n    print \'Matrix dim: (%d,%d) SubMatrix dim: (%d,%d) \' % (\n        self.n_rows, self.n_cols, config.submatrix_rows, config.submatrix_cols)\n    sys.stdout.flush()\n    self.n_submatrices = (self.n_rows * self.n_cols /\n                          (config.submatrix_rows * config.submatrix_cols))\n    print \'n_submatrices: %d\' % (self.n_submatrices)\n    sys.stdout.flush()\n\n    # ===== CREATE VARIABLES ======\n\n    with tf.device(\'/cpu:0\'):\n      # embeddings\n      self.row_embedding = embeddings_with_init(\n          embedding_dim=config.embedding_size,\n          vocab_size=self.n_rows,\n          name=\'row_embedding\')\n      self.col_embedding = embeddings_with_init(\n          embedding_dim=config.embedding_size,\n          vocab_size=self.n_cols,\n          name=\'col_embedding\')\n      tf.histogram_summary(\'row_emb\', self.row_embedding)\n      tf.histogram_summary(\'col_emb\', self.col_embedding)\n\n      matrix_log_sum = math.log(np.sum(row_sums) + 1)\n      row_bias_init = [math.log(x + 1) for x in row_sums]\n      col_bias_init = [math.log(x + 1) for x in col_sums]\n      self.row_bias = tf.Variable(row_bias_init,\n                                  trainable=config.trainable_bias)\n      self.col_bias = tf.Variable(col_bias_init,\n                                  trainable=config.trainable_bias)\n      tf.histogram_summary(\'row_bias\', self.row_bias)\n      tf.histogram_summary(\'col_bias\', self.col_bias)\n\n    # ===== CREATE GRAPH =====\n\n    # Get input\n    with tf.device(\'/cpu:0\'):\n      global_row, global_col, count = count_matrix_input(\n          count_matrix_files, config.submatrix_rows, config.submatrix_cols)\n\n      # Fetch embeddings.\n      selected_row_embedding = tf.nn.embedding_lookup(self.row_embedding,\n                                                      global_row)\n      selected_col_embedding = tf.nn.embedding_lookup(self.col_embedding,\n                                                      global_col)\n\n      # Fetch biases.\n      selected_row_bias = tf.nn.embedding_lookup([self.row_bias], global_row)\n      selected_col_bias = tf.nn.embedding_lookup([self.col_bias], global_col)\n\n    # Multiply the row and column embeddings to generate predictions.\n    predictions = tf.matmul(\n        selected_row_embedding, selected_col_embedding, transpose_b=True)\n\n    # These binary masks separate zero from non-zero values.\n    count_is_nonzero = tf.to_float(tf.cast(count, tf.bool))\n    count_is_zero = 1 - tf.to_float(tf.cast(count, tf.bool))\n\n    objectives = count_is_nonzero * tf.log(count + 1e-30)\n    objectives -= tf.reshape(selected_row_bias, [config.submatrix_rows, 1])\n    objectives -= selected_col_bias\n    objectives += matrix_log_sum\n\n    err = predictions - objectives\n\n    # The confidence function scales the L2 loss based on the raw co-occurrence\n    # count.\n    l2_confidence = (config.confidence_base + config.confidence_scale * tf.pow(\n        count, config.confidence_exponent))\n\n    l2_loss = config.loss_multiplier * tf.reduce_sum(\n        0.5 * l2_confidence * err * err * count_is_nonzero)\n\n    sigmoid_loss = config.loss_multiplier * tf.reduce_sum(\n        tf.nn.softplus(err) * count_is_zero)\n\n    self.loss = l2_loss + sigmoid_loss\n\n    tf.scalar_summary(""l2_loss"", l2_loss)\n    tf.scalar_summary(""sigmoid_loss"", sigmoid_loss)\n    tf.scalar_summary(""loss"", self.loss)\n\n    # Add optimizer.\n    self.global_step = tf.Variable(0, name=\'global_step\')\n    opt = tf.train.AdagradOptimizer(config.learning_rate)\n    self.train_op = opt.minimize(self.loss, global_step=self.global_step)\n    self.saver = tf.train.Saver(sharded=True)\n\n\ndef main(_):\n  # Create the output path.  If this fails, it really ought to fail\n  # now. :)\n  if not os.path.isdir(FLAGS.output_base_path):\n    os.makedirs(FLAGS.output_base_path)\n\n  # Create and run model\n  with tf.Graph().as_default():\n    model = SwivelModel(FLAGS)\n\n    # Create a session for running Ops on the Graph.\n    gpu_options = tf.GPUOptions(\n        per_process_gpu_memory_fraction=FLAGS.per_process_gpu_memory_fraction)\n    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n\n    # Run the Op to initialize the variables.\n    sess.run(tf.initialize_all_variables())\n\n    # Start feeding input\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    # Calculate how many steps each thread should run\n    n_total_steps = int(FLAGS.num_epochs * model.n_rows * model.n_cols) / (\n        FLAGS.submatrix_rows * FLAGS.submatrix_cols)\n    n_steps_per_thread = n_total_steps / FLAGS.num_concurrent_steps\n    n_submatrices_to_train = model.n_submatrices * FLAGS.num_epochs\n    t0 = [time.time()]\n\n    def TrainingFn():\n      for _ in range(n_steps_per_thread):\n        _, global_step = sess.run([model.train_op, model.global_step])\n        n_steps_between_status_updates = 100\n        if (global_step % n_steps_between_status_updates) == 0:\n          elapsed = float(time.time() - t0[0])\n          print \'%d/%d submatrices trained (%.1f%%), %.1f submatrices/sec\' % (\n              global_step, n_submatrices_to_train,\n              100.0 * global_step / n_submatrices_to_train,\n              n_steps_between_status_updates / elapsed)\n          sys.stdout.flush()\n          t0[0] = time.time()\n\n    # Start training threads\n    train_threads = []\n    for _ in range(FLAGS.num_concurrent_steps):\n      t = threading.Thread(target=TrainingFn)\n      train_threads.append(t)\n      t.start()\n\n    # Wait for threads to finish.\n    for t in train_threads:\n      t.join()\n\n    coord.request_stop()\n    coord.join(threads)\n\n    # Write out vectors\n    write_embeddings_to_disk(FLAGS, model, sess)\n\n    #Shutdown\n    sess.close()\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/swivel/text2bin.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Converts vectors from text to a binary format for quicker manipulation.\n\nUsage:\n\n  text2bin.py -o <out> -v <vocab> vec1.txt [vec2.txt ...]\n\nOptiona:\n\n  -o <filename>, --output <filename>\n    The name of the file into which the binary vectors are written.\n\n  -v <filename>, --vocab <filename>\n    The name of the file into which the vocabulary is written.\n\nDescription\n\nThis program merges one or more whitespace separated vector files into a single\nbinary vector file that can be used by downstream evaluation tools in this\ndirectory (""wordsim.py"" and ""analogy"").\n\nIf more than one vector file is specified, then the files must be aligned\nrow-wise (i.e., each line must correspond to the same embedding), and they must\nhave the same number of columns (i.e., be the same dimension).\n\n""""""\n\nfrom itertools import izip\nfrom getopt import GetoptError, getopt\nimport os\nimport struct\nimport sys\n\ntry:\n  opts, args = getopt(\n      sys.argv[1:], \'o:v:\', [\'output=\', \'vocab=\'])\nexcept GetoptError, e:\n  print >> sys.stderr, e\n  sys.exit(2)\n\nopt_output = \'vecs.bin\'\nopt_vocab = \'vocab.txt\'\nfor o, a in opts:\n  if o in (\'-o\', \'--output\'):\n    opt_output = a\n  if o in (\'-v\', \'--vocab\'):\n    opt_vocab = a\n\ndef go(fhs):\n  fmt = None\n  with open(opt_vocab, \'w\') as vocab_out:\n    with open(opt_output, \'w\') as vecs_out:\n      for lines in izip(*fhs):\n        parts = [line.split() for line in lines]\n        token = parts[0][0]\n        if any(part[0] != token for part in parts[1:]):\n          raise IOError(\'vector files must be aligned\')\n\n        print >> vocab_out, token\n\n        vec = [sum(float(x) for x in xs) for xs in zip(*parts)[1:]]\n        if not fmt:\n          fmt = struct.Struct(\'%df\' % len(vec))\n\n        vecs_out.write(fmt.pack(*vec))\n\nif args:\n  fhs = [open(filename) for filename in args]\n  go(fhs)\n  for fh in fhs:\n    fh.close()\nelse:\n  go([sys.stdin])\n'"
model_zoo/models/swivel/vecs.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport mmap\nimport numpy as np\nimport os\nimport struct\n\nclass Vecs(object):\n  def __init__(self, vocab_filename, rows_filename, cols_filename=None):\n    """"""Initializes the vectors from a text vocabulary and binary data.""""""\n    with open(vocab_filename, \'r\') as lines:\n      self.vocab = [line.split()[0] for line in lines]\n      self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n\n    n = len(self.vocab)\n\n    with open(rows_filename, \'r\') as rows_fh:\n      rows_fh.seek(0, os.SEEK_END)\n      size = rows_fh.tell()\n\n      # Make sure that the file size seems reasonable.\n      if size % (4 * n) != 0:\n        raise IOError(\n            \'unexpected file size for binary vector file %s\' % rows_filename)\n\n      # Memory map the rows.\n      dim = size / (4 * n)\n      rows_mm = mmap.mmap(rows_fh.fileno(), 0, prot=mmap.PROT_READ)\n      rows = np.matrix(\n          np.frombuffer(rows_mm, dtype=np.float32).reshape(n, dim))\n\n      # If column vectors were specified, then open them and add them to the row\n      # vectors.\n      if cols_filename:\n        with open(cols_filename, \'r\') as cols_fh:\n          cols_mm = mmap.mmap(cols_fh.fileno(), 0, prot=mmap.PROT_READ)\n          cols_fh.seek(0, os.SEEK_END)\n          if cols_fh.tell() != size:\n            raise IOError(\'row and column vector files have different sizes\')\n\n          cols = np.matrix(\n              np.frombuffer(cols_mm, dtype=np.float32).reshape(n, dim))\n\n          rows += cols\n          cols_mm.close()\n\n      # Normalize so that dot products are just cosine similarity.\n      self.vecs = rows / np.linalg.norm(rows, axis=1).reshape(n, 1)\n      rows_mm.close()\n\n  def similarity(self, word1, word2):\n    """"""Computes the similarity of two tokens.""""""\n    idx1 = self.word_to_idx.get(word1)\n    idx2 = self.word_to_idx.get(word2)\n    if not idx1 or not idx2:\n      return None\n\n    return float(self.vecs[idx1] * self.vecs[idx2].transpose())\n\n  def neighbors(self, query):\n    """"""Returns the nearest neighbors to the query (a word or vector).""""""\n    if isinstance(query, basestring):\n      idx = self.word_to_idx.get(query)\n      if idx is None:\n        return None\n\n      query = self.vecs[idx]\n\n    neighbors = self.vecs * query.transpose()\n\n    return sorted(\n      zip(self.vocab, neighbors.flat),\n      key=lambda kv: kv[1], reverse=True)\n\n  def lookup(self, word):\n    """"""Returns the embedding for a token, or None if no embedding exists.""""""\n    idx = self.word_to_idx.get(word)\n    return None if idx is None else self.vecs[idx]\n'"
model_zoo/models/swivel/wordsim.py,0,"b'#!/usr/bin/env python\n#\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Computes Spearman\'s rho with respect to human judgements.\n\nGiven a set of row (and potentially column) embeddings, this computes Spearman\'s\nrho between the rank ordering of predicted word similarity and human judgements.\n\nUsage:\n\n  wordim.py --embeddings=<binvecs> --vocab=<vocab> eval1.tab eval2.tab ...\n\nOptions:\n\n  --embeddings=<filename>: the vectors to test\n  --vocab=<filename>: the vocabulary file\n\nEvaluation files are assumed to be tab-separated files with exactly three\ncolumns.  The first two columns contain the words, and the third column contains\nthe scored human judgement.\n\n""""""\n\nimport scipy.stats\nimport sys\nfrom getopt import GetoptError, getopt\n\nfrom vecs import Vecs\n\ntry:\n  opts, args = getopt(sys.argv[1:], \'\', [\'embeddings=\', \'vocab=\'])\nexcept GetoptError, e:\n  print >> sys.stderr, e\n  sys.exit(2)\n\nopt_embeddings = None\nopt_vocab = None\n\nfor o, a in opts:\n  if o == \'--embeddings\':\n    opt_embeddings = a\n  if o == \'--vocab\':\n    opt_vocab = a\n\nif not opt_vocab:\n  print >> sys.stderr, \'please specify a vocabulary file with ""--vocab""\'\n  sys.exit(2)\n\nif not opt_embeddings:\n  print >> sys.stderr, \'please specify the embeddings with ""--embeddings""\'\n  sys.exit(2)\n\ntry:\n  vecs = Vecs(opt_vocab, opt_embeddings)\nexcept IOError, e:\n  print >> sys.stderr, e\n  sys.exit(1)\n\ndef evaluate(lines):\n  acts, preds = [], []\n\n  with open(filename, \'r\') as lines:\n    for line in lines:\n      w1, w2, act = line.strip().split(\'\\t\')\n      pred = vecs.similarity(w1, w2)\n      if pred is None:\n        continue\n\n      acts.append(float(act))\n      preds.append(pred)\n\n  rho, _ = scipy.stats.spearmanr(acts, preds)\n  return rho\n\nfor filename in args:\n  with open(filename, \'r\') as lines:\n    print \'%0.3f %s\' % (evaluate(lines), filename)\n'"
model_zoo/models/textsum/batch_reader.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Batch reader to seq2seq attention model, with bucketing support.""""""\n\nfrom collections import namedtuple\nimport Queue\nfrom random import shuffle\nfrom threading import Thread\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nimport data\n\nModelInput = namedtuple(\'ModelInput\',\n                        \'enc_input dec_input target enc_len dec_len \'\n                        \'origin_article origin_abstract\')\n\nBUCKET_CACHE_BATCH = 100\nQUEUE_NUM_BATCH = 100\n\n\nclass Batcher(object):\n  """"""Batch reader with shuffling and bucketing support.""""""\n\n  def __init__(self, data_path, vocab, hps,\n               article_key, abstract_key, max_article_sentences,\n               max_abstract_sentences, bucketing=True, truncate_input=False):\n    """"""Batcher constructor.\n\n    Args:\n      data_path: tf.Example filepattern.\n      vocab: Vocabulary.\n      hps: Seq2SeqAttention model hyperparameters.\n      article_key: article feature key in tf.Example.\n      abstract_key: abstract feature key in tf.Example.\n      max_article_sentences: Max number of sentences used from article.\n      max_abstract_sentences: Max number of sentences used from abstract.\n      bucketing: Whether bucket articles of similar length into the same batch.\n      truncate_input: Whether to truncate input that is too long. Alternative is\n        to discard such examples.\n    """"""\n    self._data_path = data_path\n    self._vocab = vocab\n    self._hps = hps\n    self._article_key = article_key\n    self._abstract_key = abstract_key\n    self._max_article_sentences = max_article_sentences\n    self._max_abstract_sentences = max_abstract_sentences\n    self._bucketing = bucketing\n    self._truncate_input = truncate_input\n    self._input_queue = Queue.Queue(QUEUE_NUM_BATCH * self._hps.batch_size)\n    self._bucket_input_queue = Queue.Queue(QUEUE_NUM_BATCH)\n    self._input_threads = []\n    for _ in xrange(16):\n      self._input_threads.append(Thread(target=self._FillInputQueue))\n      self._input_threads[-1].daemon = True\n      self._input_threads[-1].start()\n    self._bucketing_threads = []\n    for _ in xrange(4):\n      self._bucketing_threads.append(Thread(target=self._FillBucketInputQueue))\n      self._bucketing_threads[-1].daemon = True\n      self._bucketing_threads[-1].start()\n\n    self._watch_thread = Thread(target=self._WatchThreads)\n    self._watch_thread.daemon = True\n    self._watch_thread.start()\n\n  def NextBatch(self):\n    """"""Returns a batch of inputs for seq2seq attention model.\n\n    Returns:\n      enc_batch: A batch of encoder inputs [batch_size, hps.enc_timestamps].\n      dec_batch: A batch of decoder inputs [batch_size, hps.dec_timestamps].\n      target_batch: A batch of targets [batch_size, hps.dec_timestamps].\n      enc_input_len: encoder input lengths of the batch.\n      dec_input_len: decoder input lengths of the batch.\n      loss_weights: weights for loss function, 1 if not padded, 0 if padded.\n      origin_articles: original article words.\n      origin_abstracts: original abstract words.\n    """"""\n    enc_batch = np.zeros(\n        (self._hps.batch_size, self._hps.enc_timesteps), dtype=np.int32)\n    enc_input_lens = np.zeros(\n        (self._hps.batch_size), dtype=np.int32)\n    dec_batch = np.zeros(\n        (self._hps.batch_size, self._hps.dec_timesteps), dtype=np.int32)\n    dec_output_lens = np.zeros(\n        (self._hps.batch_size), dtype=np.int32)\n    target_batch = np.zeros(\n        (self._hps.batch_size, self._hps.dec_timesteps), dtype=np.int32)\n    loss_weights = np.zeros(\n        (self._hps.batch_size, self._hps.dec_timesteps), dtype=np.float32)\n    origin_articles = [\'None\'] * self._hps.batch_size\n    origin_abstracts = [\'None\'] * self._hps.batch_size\n\n    buckets = self._bucket_input_queue.get()\n    for i in xrange(self._hps.batch_size):\n      (enc_inputs, dec_inputs, targets, enc_input_len, dec_output_len,\n       article, abstract) = buckets[i]\n\n      origin_articles[i] = article\n      origin_abstracts[i] = abstract\n      enc_input_lens[i] = enc_input_len\n      dec_output_lens[i] = dec_output_len\n      enc_batch[i, :] = enc_inputs[:]\n      dec_batch[i, :] = dec_inputs[:]\n      target_batch[i, :] = targets[:]\n      for j in xrange(dec_output_len):\n        loss_weights[i][j] = 1\n    return (enc_batch, dec_batch, target_batch, enc_input_lens, dec_output_lens,\n            loss_weights, origin_articles, origin_abstracts)\n\n  def _FillInputQueue(self):\n    """"""Fill input queue with ModelInput.""""""\n    start_id = self._vocab.WordToId(data.SENTENCE_START)\n    end_id = self._vocab.WordToId(data.SENTENCE_END)\n    pad_id = self._vocab.WordToId(data.PAD_TOKEN)\n    input_gen = self._TextGenerator(data.ExampleGen(self._data_path))\n    while True:\n      (article, abstract) = input_gen.next()\n      article_sentences = [sent.strip() for sent in\n                           data.ToSentences(article, include_token=False)]\n      abstract_sentences = [sent.strip() for sent in\n                            data.ToSentences(abstract, include_token=False)]\n\n      enc_inputs = []\n      # Use the <s> as the <GO> symbol for decoder inputs.\n      dec_inputs = [start_id]\n\n      # Convert first N sentences to word IDs, stripping existing <s> and </s>.\n      for i in xrange(min(self._max_article_sentences,\n                          len(article_sentences))):\n        enc_inputs += data.GetWordIds(article_sentences[i], self._vocab)\n      for i in xrange(min(self._max_abstract_sentences,\n                          len(abstract_sentences))):\n        dec_inputs += data.GetWordIds(abstract_sentences[i], self._vocab)\n\n      # Filter out too-short input\n      if (len(enc_inputs) < self._hps.min_input_len or\n          len(dec_inputs) < self._hps.min_input_len):\n        tf.logging.warning(\'Drop an example - too short.\\nenc:%d\\ndec:%d\',\n                           len(enc_inputs), len(dec_inputs))\n        continue\n\n      # If we\'re not truncating input, throw out too-long input\n      if not self._truncate_input:\n        if (len(enc_inputs) > self._hps.enc_timesteps or\n            len(dec_inputs) > self._hps.dec_timesteps):\n          tf.logging.warning(\'Drop an example - too long.\\nenc:%d\\ndec:%d\',\n                             len(enc_inputs), len(dec_inputs))\n          continue\n      # If we are truncating input, do so if necessary\n      else:\n        if len(enc_inputs) > self._hps.enc_timesteps:\n          enc_inputs = enc_inputs[:self._hps.enc_timesteps]\n        if len(dec_inputs) > self._hps.dec_timesteps:\n          dec_inputs = dec_inputs[:self._hps.dec_timesteps]\n\n      # targets is dec_inputs without <s> at beginning, plus </s> at end\n      targets = dec_inputs[1:]\n      targets.append(end_id)\n\n      # Now len(enc_inputs) should be <= enc_timesteps, and\n      # len(targets) = len(dec_inputs) should be <= dec_timesteps\n\n      enc_input_len = len(enc_inputs)\n      dec_output_len = len(targets)\n\n      # Pad if necessary\n      while len(enc_inputs) < self._hps.enc_timesteps:\n        enc_inputs.append(pad_id)\n      while len(dec_inputs) < self._hps.dec_timesteps:\n        dec_inputs.append(end_id)\n      while len(targets) < self._hps.dec_timesteps:\n        targets.append(end_id)\n\n      element = ModelInput(enc_inputs, dec_inputs, targets, enc_input_len,\n                           dec_output_len, \' \'.join(article_sentences),\n                           \' \'.join(abstract_sentences))\n      self._input_queue.put(element)\n\n  def _FillBucketInputQueue(self):\n    """"""Fill bucketed batches into the bucket_input_queue.""""""\n    while True:\n      inputs = []\n      for _ in xrange(self._hps.batch_size * BUCKET_CACHE_BATCH):\n        inputs.append(self._input_queue.get())\n      if self._bucketing:\n        inputs = sorted(inputs, key=lambda inp: inp.enc_len)\n\n      batches = []\n      for i in xrange(0, len(inputs), self._hps.batch_size):\n        batches.append(inputs[i:i+self._hps.batch_size])\n      shuffle(batches)\n      for b in batches:\n        self._bucket_input_queue.put(b)\n\n  def _WatchThreads(self):\n    """"""Watch the daemon input threads and restart if dead.""""""\n    while True:\n      time.sleep(60)\n      input_threads = []\n      for t in self._input_threads:\n        if t.is_alive():\n          input_threads.append(t)\n        else:\n          tf.logging.error(\'Found input thread dead.\')\n          new_t = Thread(target=self._FillInputQueue)\n          input_threads.append(new_t)\n          input_threads[-1].daemon = True\n          input_threads[-1].start()\n      self._input_threads = input_threads\n\n      bucketing_threads = []\n      for t in self._bucketing_threads:\n        if t.is_alive():\n          bucketing_threads.append(t)\n        else:\n          tf.logging.error(\'Found bucketing thread dead.\')\n          new_t = Thread(target=self._FillBucketInputQueue)\n          bucketing_threads.append(new_t)\n          bucketing_threads[-1].daemon = True\n          bucketing_threads[-1].start()\n      self._bucketing_threads = bucketing_threads\n\n  def _TextGenerator(self, example_gen):\n    """"""Generates article and abstract text from tf.Example.""""""\n    while True:\n      e = example_gen.next()\n      try:\n        article_text = self._GetExFeatureText(e, self._article_key)\n        abstract_text = self._GetExFeatureText(e, self._abstract_key)\n      except ValueError:\n        tf.logging.error(\'Failed to get article or abstract from example\')\n        continue\n\n      yield (article_text, abstract_text)\n\n  def _GetExFeatureText(self, ex, key):\n    """"""Extract text for a feature from td.Example.\n\n    Args:\n      ex: tf.Example.\n      key: key of the feature to be extracted.\n    Returns:\n      feature: a feature text extracted.\n    """"""\n    return ex.features.feature[key].bytes_list.value[0]\n'"
model_zoo/models/textsum/beam_search.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Beam search module.\n\nBeam search takes the top K results from the model, predicts the K results for\neach of the previous K result, getting K*K results. Pick the top K results from\nK*K results, and start over again until certain number of results are fully\ndecoded.\n""""""\n\nimport tensorflow as tf\n\nFLAGS = tf.flags.FLAGS\ntf.flags.DEFINE_bool(\'normalize_by_length\', True, \'Whether normalize\')\n\n\nclass Hypothesis(object):\n  """"""Defines a hypothesis during beam search.""""""\n\n  def __init__(self, tokens, log_prob, state):\n    """"""Hypothesis constructor.\n\n    Args:\n      tokens: start tokens for decoding.\n      log_prob: log prob of the start tokens, usually 1.\n      state: decoder initial states.\n    """"""\n    self.tokens = tokens\n    self.log_prob = log_prob\n    self.state = state\n\n  def Extend(self, token, log_prob, new_state):\n    """"""Extend the hypothesis with result from latest step.\n\n    Args:\n      token: latest token from decoding.\n      log_prob: log prob of the latest decoded tokens.\n      new_state: decoder output state. Fed to the decoder for next step.\n    Returns:\n      New Hypothesis with the results from latest step.\n    """"""\n    return Hypothesis(self.tokens + [token], self.log_prob + log_prob,\n                      new_state)\n\n  @property\n  def latest_token(self):\n    return self.tokens[-1]\n\n  def __str__(self):\n    return (\'Hypothesis(log prob = %.4f, tokens = %s)\' % (self.log_prob,\n                                                          self.tokens))\n\n\nclass BeamSearch(object):\n  """"""Beam search.""""""\n\n  def __init__(self, model, beam_size, start_token, end_token, max_steps):\n    """"""Creates BeamSearch object.\n\n    Args:\n      model: Seq2SeqAttentionModel.\n      beam_size: int.\n      start_token: int, id of the token to start decoding with\n      end_token: int, id of the token that completes an hypothesis\n      max_steps: int, upper limit on the size of the hypothesis\n    """"""\n    self._model = model\n    self._beam_size = beam_size\n    self._start_token = start_token\n    self._end_token = end_token\n    self._max_steps = max_steps\n\n  def BeamSearch(self, sess, enc_inputs, enc_seqlen):\n    """"""Performs beam search for decoding.\n\n    Args:\n      sess: tf.Session, session\n      enc_inputs: ndarray of shape (enc_length, 1), the document ids to encode\n      enc_seqlen: ndarray of shape (1), the length of the sequnce\n\n    Returns:\n      hyps: list of Hypothesis, the best hypotheses found by beam search,\n          ordered by score\n    """"""\n\n    # Run the encoder and extract the outputs and final state.\n    enc_top_states, dec_in_state = self._model.encode_top_state(\n        sess, enc_inputs, enc_seqlen)\n    # Replicate the initial states K times for the first step.\n    hyps = [Hypothesis([self._start_token], 0.0, dec_in_state)\n           ] * self._beam_size\n    results = []\n\n    steps = 0\n    while steps < self._max_steps and len(results) < self._beam_size:\n      latest_tokens = [h.latest_token for h in hyps]\n      states = [h.state for h in hyps]\n\n      topk_ids, topk_log_probs, new_states = self._model.decode_topk(\n          sess, latest_tokens, enc_top_states, states)\n      # Extend each hypothesis.\n      all_hyps = []\n      # The first step takes the best K results from first hyps. Following\n      # steps take the best K results from K*K hyps.\n      num_beam_source = 1 if steps == 0 else len(hyps)\n      for i in xrange(num_beam_source):\n        h, ns = hyps[i], new_states[i]\n        for j in xrange(self._beam_size*2):\n          all_hyps.append(h.Extend(topk_ids[i, j], topk_log_probs[i, j], ns))\n\n      # Filter and collect any hypotheses that have the end token.\n      hyps = []\n      for h in self._BestHyps(all_hyps):\n        if h.latest_token == self._end_token:\n          # Pull the hypothesis off the beam if the end token is reached.\n          results.append(h)\n        else:\n          # Otherwise continue to the extend the hypothesis.\n          hyps.append(h)\n        if len(hyps) == self._beam_size or len(results) == self._beam_size:\n          break\n\n      steps += 1\n\n    if steps == self._max_steps:\n      results.extend(hyps)\n\n    return self._BestHyps(results)\n\n  def _BestHyps(self, hyps):\n    """"""Sort the hyps based on log probs and length.\n\n    Args:\n      hyps: A list of hypothesis.\n    Returns:\n      hyps: A list of sorted hypothesis in reverse log_prob order.\n    """"""\n    # This length normalization is only effective for the final results.\n    if FLAGS.normalize_by_length:\n      return sorted(hyps, key=lambda h: h.log_prob/len(h.tokens), reverse=True)\n    else:\n      return sorted(hyps, key=lambda h: h.log_prob, reverse=True)\n'"
model_zoo/models/textsum/data.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Data batchers for data described in ..//data_prep/README.md.""""""\n\nimport glob\nimport random\nimport struct\nimport sys\n\nfrom tensorflow.core.example import example_pb2\n\n\n# Special tokens\nPARAGRAPH_START = \'<p>\'\nPARAGRAPH_END = \'</p>\'\nSENTENCE_START = \'<s>\'\nSENTENCE_END = \'</s>\'\nUNKNOWN_TOKEN = \'<UNK>\'\nPAD_TOKEN = \'<PAD>\'\nDOCUMENT_START = \'<d>\'\nDOCUMENT_END = \'</d>\'\n\n\nclass Vocab(object):\n  """"""Vocabulary class for mapping words and ids.""""""\n\n  def __init__(self, vocab_file, max_size):\n    self._word_to_id = {}\n    self._id_to_word = {}\n    self._count = 0\n\n    with open(vocab_file, \'r\') as vocab_f:\n      for line in vocab_f:\n        pieces = line.split()\n        if len(pieces) != 2:\n          sys.stderr.write(\'Bad line: %s\\n\' % line)\n          continue\n        if pieces[0] in self._word_to_id:\n          raise ValueError(\'Duplicated word: %s.\' % pieces[0])\n        self._word_to_id[pieces[0]] = self._count\n        self._id_to_word[self._count] = pieces[0]\n        self._count += 1\n        if self._count > max_size:\n          raise ValueError(\'Too many words: >%d.\' % max_size)\n\n  def WordToId(self, word):\n    if word not in self._word_to_id:\n      return self._word_to_id[UNKNOWN_TOKEN]\n    return self._word_to_id[word]\n\n  def IdToWord(self, word_id):\n    if word_id not in self._id_to_word:\n      raise ValueError(\'id not found in vocab: %d.\' % word_id)\n    return self._id_to_word[word_id]\n\n  def NumIds(self):\n    return self._count\n\n\ndef ExampleGen(data_path, num_epochs=None):\n  """"""Generates tf.Examples from path of data files.\n\n    Binary data format: <length><blob>. <length> represents the byte size\n    of <blob>. <blob> is serialized tf.Example proto. The tf.Example contains\n    the tokenized article text and summary.\n\n  Args:\n    data_path: path to tf.Example data files.\n    num_epochs: Number of times to go through the data. None means infinite.\n\n  Yields:\n    Deserialized tf.Example.\n\n  If there are multiple files specified, they accessed in a random order.\n  """"""\n  epoch = 0\n  while True:\n    if num_epochs is not None and epoch >= num_epochs:\n      break\n    filelist = glob.glob(data_path)\n    assert filelist, \'Empty filelist.\'\n    random.shuffle(filelist)\n    for f in filelist:\n      reader = open(f, \'rb\')\n      while True:\n        len_bytes = reader.read(8)\n        if not len_bytes: break\n        str_len = struct.unpack(\'q\', len_bytes)[0]\n        example_str = struct.unpack(\'%ds\' % str_len, reader.read(str_len))[0]\n        yield example_pb2.Example.FromString(example_str)\n\n    epoch += 1\n\n\ndef Pad(ids, pad_id, length):\n  """"""Pad or trim list to len length.\n\n  Args:\n    ids: list of ints to pad\n    pad_id: what to pad with\n    length: length to pad or trim to\n\n  Returns:\n    ids trimmed or padded with pad_id\n  """"""\n  assert pad_id is not None\n  assert length is not None\n\n  if len(ids) < length:\n    a = [pad_id] * (length - len(ids))\n    return ids + a\n  else:\n    return ids[:length]\n\n\ndef GetWordIds(text, vocab, pad_len=None, pad_id=None):\n  """"""Get ids corresponding to words in text.\n\n  Assumes tokens separated by space.\n\n  Args:\n    text: a string\n    vocab: TextVocabularyFile object\n    pad_len: int, length to pad to\n    pad_id: int, word id for pad symbol\n\n  Returns:\n    A list of ints representing word ids.\n  """"""\n  ids = []\n  for w in text.split():\n    i = vocab.WordToId(w)\n    if i >= 0:\n      ids.append(i)\n    else:\n      ids.append(vocab.WordToId(UNKNOWN_TOKEN))\n  if pad_len is not None:\n    return Pad(ids, pad_id, pad_len)\n  return ids\n\n\ndef Ids2Words(ids_list, vocab):\n  """"""Get words from ids.\n\n  Args:\n    ids_list: list of int32\n    vocab: TextVocabulary object\n\n  Returns:\n    List of words corresponding to ids.\n  """"""\n  assert isinstance(ids_list, list), \'%s  is not a list\' % ids_list\n  return [vocab.IdToWord(i) for i in ids_list]\n\n\ndef SnippetGen(text, start_tok, end_tok, inclusive=True):\n  """"""Generates consecutive snippets between start and end tokens.\n\n  Args:\n    text: a string\n    start_tok: a string denoting the start of snippets\n    end_tok: a string denoting the end of snippets\n    inclusive: Whether include the tokens in the returned snippets.\n\n  Yields:\n    String snippets\n  """"""\n  cur = 0\n  while True:\n    try:\n      start_p = text.index(start_tok, cur)\n      end_p = text.index(end_tok, start_p + 1)\n      cur = end_p + len(end_tok)\n      if inclusive:\n        yield text[start_p:cur]\n      else:\n        yield text[start_p+len(start_tok):end_p]\n    except ValueError as e:\n      raise StopIteration(\'no more snippets in text: %s\' % e)\n\n\ndef GetExFeatureText(ex, key):\n  return ex.features.feature[key].bytes_list.value[0]\n\n\ndef ToSentences(paragraph, include_token=True):\n  """"""Takes tokens of a paragraph and returns list of sentences.\n\n  Args:\n    paragraph: string, text of paragraph\n    include_token: Whether include the sentence separation tokens result.\n\n  Returns:\n    List of sentence strings.\n  """"""\n  s_gen = SnippetGen(paragraph, SENTENCE_START, SENTENCE_END, include_token)\n  return [s for s in s_gen]\n'"
model_zoo/models/textsum/data_convert_example.py,0,"b'""""""Example of Converting TextSum model data.\nUsage:\npython data_convert_example.py --command binary_to_text --in_file data/data --out_file data/text_data\npython data_convert_example.py --command text_to_binary --in_file data/text_data --out_file data/binary_data\npython data_convert_example.py --command binary_to_text --in_file data/binary_data --out_file data/text_data2\ndiff data/text_data2 data/text_data\n""""""\n\nimport struct\nimport sys\n\nimport tensorflow as tf\nfrom tensorflow.core.example import example_pb2\n\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_string(\'command\', \'binary_to_text\',\n                           \'Either binary_to_text or text_to_binary.\'\n                           \'Specify FLAGS.in_file accordingly.\')\ntf.app.flags.DEFINE_string(\'in_file\', \'\', \'path to file\')\ntf.app.flags.DEFINE_string(\'out_file\', \'\', \'path to file\')\n\ndef _binary_to_text():\n  reader = open(FLAGS.in_file, \'rb\')\n  writer = open(FLAGS.out_file, \'w\')\n  while True:\n    len_bytes = reader.read(8)\n    if not len_bytes:\n      sys.stderr.write(\'Done reading\\n\')\n      return\n    str_len = struct.unpack(\'q\', len_bytes)[0]\n    tf_example_str = struct.unpack(\'%ds\' % str_len, reader.read(str_len))[0]\n    tf_example = example_pb2.Example.FromString(tf_example_str)\n    examples = []\n    for key in tf_example.features.feature:\n      examples.append(\'%s=%s\' % (key, tf_example.features.feature[key].bytes_list.value[0]))\n    writer.write(\'%s\\n\' % \'\\t\'.join(examples))\n  reader.close()\n  writer.close()\n\n\ndef _text_to_binary():\n  inputs = open(FLAGS.in_file, \'r\').readlines()\n  writer = open(FLAGS.out_file, \'wb\')\n  for inp in inputs:\n    tf_example = example_pb2.Example()\n    for feature in inp.strip().split(\'\\t\'):\n      (k, v) = feature.split(\'=\')\n      tf_example.features.feature[k].bytes_list.value.extend([v])\n    tf_example_str = tf_example.SerializeToString()\n    str_len = len(tf_example_str)\n    writer.write(struct.pack(\'q\', str_len))\n    writer.write(struct.pack(\'%ds\' % str_len, tf_example_str))\n  writer.close()\n\n\ndef main(unused_argv):\n  assert FLAGS.command and FLAGS.in_file and FLAGS.out_file\n  if FLAGS.command == \'binary_to_text\':\n    _binary_to_text()\n  elif FLAGS.command == \'text_to_binary\':\n    _text_to_binary()\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/textsum/seq2seq_attention.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Trains a seq2seq model.\n\nWORK IN PROGRESS.\n\nImplement ""Abstractive Text Summarization using Sequence-to-sequence RNNS and\nBeyond.""\n\n""""""\nimport sys\nimport time\n\nimport tensorflow as tf\nimport batch_reader\nimport data\nimport seq2seq_attention_decode\nimport seq2seq_attention_model\n\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_string(\'data_path\',\n                           \'\', \'Path expression to tf.Example.\')\ntf.app.flags.DEFINE_string(\'vocab_path\',\n                           \'\', \'Path expression to text vocabulary file.\')\ntf.app.flags.DEFINE_string(\'article_key\', \'article\',\n                           \'tf.Example feature key for article.\')\ntf.app.flags.DEFINE_string(\'abstract_key\', \'headline\',\n                           \'tf.Example feature key for abstract.\')\ntf.app.flags.DEFINE_string(\'log_root\', \'\', \'Directory for model root.\')\ntf.app.flags.DEFINE_string(\'train_dir\', \'\', \'Directory for train.\')\ntf.app.flags.DEFINE_string(\'eval_dir\', \'\', \'Directory for eval.\')\ntf.app.flags.DEFINE_string(\'decode_dir\', \'\', \'Directory for decode summaries.\')\ntf.app.flags.DEFINE_string(\'mode\', \'train\', \'train/eval/decode mode\')\ntf.app.flags.DEFINE_integer(\'max_run_steps\', 10000000,\n                            \'Maximum number of run steps.\')\ntf.app.flags.DEFINE_integer(\'max_article_sentences\', 2,\n                            \'Max number of first sentences to use from the \'\n                            \'article\')\ntf.app.flags.DEFINE_integer(\'max_abstract_sentences\', 100,\n                            \'Max number of first sentences to use from the \'\n                            \'abstract\')\ntf.app.flags.DEFINE_integer(\'beam_size\', 4,\n                            \'beam size for beam search decoding.\')\ntf.app.flags.DEFINE_integer(\'eval_interval_secs\', 60, \'How often to run eval.\')\ntf.app.flags.DEFINE_integer(\'checkpoint_secs\', 60, \'How often to checkpoint.\')\ntf.app.flags.DEFINE_bool(\'use_bucketing\', False,\n                         \'Whether bucket articles of similar length.\')\ntf.app.flags.DEFINE_bool(\'truncate_input\', False,\n                         \'Truncate inputs that are too long. If False, \'\n                         \'examples that are too long are discarded.\')\ntf.app.flags.DEFINE_integer(\'num_gpus\', 0, \'Number of gpus used.\')\ntf.app.flags.DEFINE_integer(\'random_seed\', 111, \'A seed value for randomness.\')\n\n\ndef _RunningAvgLoss(loss, running_avg_loss, summary_writer, step, decay=0.999):\n  """"""Calculate the running average of losses.""""""\n  if running_avg_loss == 0:\n    running_avg_loss = loss\n  else:\n    running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n  running_avg_loss = min(running_avg_loss, 12)\n  loss_sum = tf.Summary()\n  loss_sum.value.add(tag=\'running_avg_loss\', simple_value=running_avg_loss)\n  summary_writer.add_summary(loss_sum, step)\n  sys.stdout.write(\'running_avg_loss: %f\\n\' % running_avg_loss)\n  return running_avg_loss\n\n\ndef _Train(model, data_batcher):\n  """"""Runs model training.""""""\n  with tf.device(\'/cpu:0\'):\n    model.build_graph()\n    saver = tf.train.Saver()\n    # Train dir is different from log_root to avoid summary directory\n    # conflict with Supervisor.\n    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir)\n    sv = tf.train.Supervisor(logdir=FLAGS.log_root,\n                             is_chief=True,\n                             saver=saver,\n                             summary_op=None,\n                             save_summaries_secs=60,\n                             save_model_secs=FLAGS.checkpoint_secs,\n                             global_step=model.global_step)\n    sess = sv.prepare_or_wait_for_session(config=tf.ConfigProto(\n        allow_soft_placement=True))\n    running_avg_loss = 0\n    step = 0\n    while not sv.should_stop() and step < FLAGS.max_run_steps:\n      (article_batch, abstract_batch, targets, article_lens, abstract_lens,\n       loss_weights, _, _) = data_batcher.NextBatch()\n      (_, summaries, loss, train_step) = model.run_train_step(\n          sess, article_batch, abstract_batch, targets, article_lens,\n          abstract_lens, loss_weights)\n\n      summary_writer.add_summary(summaries, train_step)\n      running_avg_loss = _RunningAvgLoss(\n          running_avg_loss, loss, summary_writer, train_step)\n      step += 1\n      if step % 100 == 0:\n        summary_writer.flush()\n    sv.Stop()\n    return running_avg_loss\n\n\ndef _Eval(model, data_batcher, vocab=None):\n  """"""Runs model eval.""""""\n  model.build_graph()\n  saver = tf.train.Saver()\n  summary_writer = tf.train.SummaryWriter(FLAGS.eval_dir)\n  sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n  running_avg_loss = 0\n  step = 0\n  while True:\n    time.sleep(FLAGS.eval_interval_secs)\n    try:\n      ckpt_state = tf.train.get_checkpoint_state(FLAGS.log_root)\n    except tf.errors.OutOfRangeError as e:\n      tf.logging.error(\'Cannot restore checkpoint: %s\', e)\n      continue\n\n    if not (ckpt_state and ckpt_state.model_checkpoint_path):\n      tf.logging.info(\'No model to eval yet at %s\', FLAGS.train_dir)\n      continue\n\n    tf.logging.info(\'Loading checkpoint %s\', ckpt_state.model_checkpoint_path)\n    saver.restore(sess, ckpt_state.model_checkpoint_path)\n\n    (article_batch, abstract_batch, targets, article_lens, abstract_lens,\n     loss_weights, _, _) = data_batcher.NextBatch()\n    (summaries, loss, train_step) = model.run_eval_step(\n        sess, article_batch, abstract_batch, targets, article_lens,\n        abstract_lens, loss_weights)\n    tf.logging.info(\n        \'article:  %s\',\n        \' \'.join(data.Ids2Words(article_batch[0][:].tolist(), vocab)))\n    tf.logging.info(\n        \'abstract: %s\',\n        \' \'.join(data.Ids2Words(abstract_batch[0][:].tolist(), vocab)))\n\n    summary_writer.add_summary(summaries, train_step)\n    running_avg_loss = _RunningAvgLoss(\n        running_avg_loss, loss, summary_writer, train_step)\n    if step % 100 == 0:\n      summary_writer.flush()\n\n\ndef main(unused_argv):\n  vocab = data.Vocab(FLAGS.vocab_path, 1000000)\n  # Check for presence of required special tokens.\n  assert vocab.WordToId(data.PAD_TOKEN) > 0\n  assert vocab.WordToId(data.UNKNOWN_TOKEN) >= 0\n  assert vocab.WordToId(data.SENTENCE_START) > 0\n  assert vocab.WordToId(data.SENTENCE_END) > 0\n\n  batch_size = 4\n  if FLAGS.mode == \'decode\':\n    batch_size = FLAGS.beam_size\n\n  hps = seq2seq_attention_model.HParams(\n      mode=FLAGS.mode,  # train, eval, decode\n      min_lr=0.01,  # min learning rate.\n      lr=0.15,  # learning rate\n      batch_size=batch_size,\n      enc_layers=4,\n      enc_timesteps=120,\n      dec_timesteps=30,\n      min_input_len=2,  # discard articles/summaries < than this\n      num_hidden=256,  # for rnn cell\n      emb_dim=128,  # If 0, don\'t use embedding\n      max_grad_norm=2,\n      num_softmax_samples=4096)  # If 0, no sampled softmax.\n\n  batcher = batch_reader.Batcher(\n      FLAGS.data_path, vocab, hps, FLAGS.article_key,\n      FLAGS.abstract_key, FLAGS.max_article_sentences,\n      FLAGS.max_abstract_sentences, bucketing=FLAGS.use_bucketing,\n      truncate_input=FLAGS.truncate_input)\n  tf.set_random_seed(FLAGS.random_seed)\n\n  if hps.mode == \'train\':\n    model = seq2seq_attention_model.Seq2SeqAttentionModel(\n        hps, vocab, num_gpus=FLAGS.num_gpus)\n    _Train(model, batcher)\n  elif hps.mode == \'eval\':\n    model = seq2seq_attention_model.Seq2SeqAttentionModel(\n        hps, vocab, num_gpus=FLAGS.num_gpus)\n    _Eval(model, batcher, vocab=vocab)\n  elif hps.mode == \'decode\':\n    decode_mdl_hps = hps\n    # Only need to restore the 1st step and reuse it since\n    # we keep and feed in state for each step\'s output.\n    decode_mdl_hps = hps._replace(dec_timesteps=1)\n    model = seq2seq_attention_model.Seq2SeqAttentionModel(\n        decode_mdl_hps, vocab, num_gpus=FLAGS.num_gpus)\n    decoder = seq2seq_attention_decode.BSDecoder(model, batcher, hps, vocab)\n    decoder.DecodeLoop()\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/textsum/seq2seq_attention_decode.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Module for decoding.""""""\n\nimport os\nimport time\n\nimport tensorflow as tf\nimport beam_search\nimport data\n\nFLAGS = tf.app.flags.FLAGS\ntf.app.flags.DEFINE_integer(\'max_decode_steps\', 1000000,\n                            \'Number of decoding steps.\')\ntf.app.flags.DEFINE_integer(\'decode_batches_per_ckpt\', 8000,\n                            \'Number of batches to decode before restoring next \'\n                            \'checkpoint\')\n\nDECODE_LOOP_DELAY_SECS = 60\nDECODE_IO_FLUSH_INTERVAL = 100\n\n\nclass DecodeIO(object):\n  """"""Writes the decoded and references to RKV files for Rouge score.\n\n    See nlp/common/utils/internal/rkv_parser.py for detail about rkv file.\n  """"""\n\n  def __init__(self, outdir):\n    self._cnt = 0\n    self._outdir = outdir\n    if not os.path.exists(self._outdir):\n      os.mkdir(self._outdir)\n    self._ref_file = None\n    self._decode_file = None\n\n  def Write(self, reference, decode):\n    """"""Writes the reference and decoded outputs to RKV files.\n\n    Args:\n      reference: The human (correct) result.\n      decode: The machine-generated result\n    """"""\n    self._ref_file.write(\'output=%s\\n\' % reference)\n    self._decode_file.write(\'output=%s\\n\' % decode)\n    self._cnt += 1\n    if self._cnt % DECODE_IO_FLUSH_INTERVAL == 0:\n      self._ref_file.flush()\n      self._decode_file.flush()\n\n  def ResetFiles(self):\n    """"""Resets the output files. Must be called once before Write().""""""\n    if self._ref_file: self._ref_file.close()\n    if self._decode_file: self._decode_file.close()\n    timestamp = int(time.time())\n    self._ref_file = open(\n        os.path.join(self._outdir, \'ref%d\'%timestamp), \'w\')\n    self._decode_file = open(\n        os.path.join(self._outdir, \'decode%d\'%timestamp), \'w\')\n\n\nclass BSDecoder(object):\n  """"""Beam search decoder.""""""\n\n  def __init__(self, model, batch_reader, hps, vocab):\n    """"""Beam search decoding.\n\n    Args:\n      model: The seq2seq attentional model.\n      batch_reader: The batch data reader.\n      hps: Hyperparamters.\n      vocab: Vocabulary\n    """"""\n    self._model = model\n    self._model.build_graph()\n    self._batch_reader = batch_reader\n    self._hps = hps\n    self._vocab = vocab\n    self._saver = tf.train.Saver()\n    self._decode_io = DecodeIO(FLAGS.decode_dir)\n\n  def DecodeLoop(self):\n    """"""Decoding loop for long running process.""""""\n    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n    step = 0\n    while step < FLAGS.max_decode_steps:\n      time.sleep(DECODE_LOOP_DELAY_SECS)\n      if not self._Decode(self._saver, sess):\n        continue\n      step += 1\n\n  def _Decode(self, saver, sess):\n    """"""Restore a checkpoint and decode it.\n\n    Args:\n      saver: Tensorflow checkpoint saver.\n      sess: Tensorflow session.\n    Returns:\n      If success, returns true, otherwise, false.\n    """"""\n    ckpt_state = tf.train.get_checkpoint_state(FLAGS.log_root)\n    if not (ckpt_state and ckpt_state.model_checkpoint_path):\n      tf.logging.info(\'No model to decode yet at %s\', FLAGS.log_root)\n      return False\n\n    tf.logging.info(\'checkpoint path %s\', ckpt_state.model_checkpoint_path)\n    ckpt_path = os.path.join(\n        FLAGS.log_root, os.path.basename(ckpt_state.model_checkpoint_path))\n    tf.logging.info(\'renamed checkpoint path %s\', ckpt_path)\n    saver.restore(sess, ckpt_path)\n\n    self._decode_io.ResetFiles()\n    for _ in xrange(FLAGS.decode_batches_per_ckpt):\n      (article_batch, _, _, article_lens, _, _, origin_articles,\n       origin_abstracts) = self._batch_reader.NextBatch()\n      for i in xrange(self._hps.batch_size):\n        bs = beam_search.BeamSearch(\n            self._model, self._hps.batch_size,\n            self._vocab.WordToId(data.SENTENCE_START),\n            self._vocab.WordToId(data.SENTENCE_END),\n            self._hps.dec_timesteps)\n\n        article_batch_cp = article_batch.copy()\n        article_batch_cp[:] = article_batch[i:i+1]\n        article_lens_cp = article_lens.copy()\n        article_lens_cp[:] = article_lens[i:i+1]\n        best_beam = bs.BeamSearch(sess, article_batch_cp, article_lens_cp)[0]\n        decode_output = [int(t) for t in best_beam.tokens[1:]]\n        self._DecodeBatch(\n            origin_articles[i], origin_abstracts[i], decode_output)\n    return True\n\n  def _DecodeBatch(self, article, abstract, output_ids):\n    """"""Convert id to words and writing results.\n\n    Args:\n      article: The original article string.\n      abstract: The human (correct) abstract string.\n      output_ids: The abstract word ids output by machine.\n    """"""\n    decoded_output = \' \'.join(data.Ids2Words(output_ids, self._vocab))\n    end_p = decoded_output.find(data.SENTENCE_END, 0)\n    if end_p != -1:\n      decoded_output = decoded_output[:end_p]\n    tf.logging.info(\'article:  %s\', article)\n    tf.logging.info(\'abstract: %s\', abstract)\n    tf.logging.info(\'decoded:  %s\', decoded_output)\n    self._decode_io.Write(abstract, decoded_output.strip())\n'"
model_zoo/models/textsum/seq2seq_attention_model.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Sequence-to-Sequence with attention model for text summarization.\n""""""\nfrom collections import namedtuple\n\nimport numpy as np\nimport tensorflow as tf\nimport seq2seq_lib\n\n\nHParams = namedtuple(\'HParams\',\n                     \'mode, min_lr, lr, batch_size, \'\n                     \'enc_layers, enc_timesteps, dec_timesteps, \'\n                     \'min_input_len, num_hidden, emb_dim, max_grad_norm, \'\n                     \'num_softmax_samples\')\n\n\ndef _extract_argmax_and_embed(embedding, output_projection=None,\n                              update_embedding=True):\n  """"""Get a loop_function that extracts the previous symbol and embeds it.\n\n  Args:\n    embedding: embedding tensor for symbols.\n    output_projection: None or a pair (W, B). If provided, each fed previous\n      output will first be multiplied by W and added B.\n    update_embedding: Boolean; if False, the gradients will not propagate\n      through the embeddings.\n\n  Returns:\n    A loop function.\n  """"""\n  def loop_function(prev, _):\n    """"""function that feed previous model output rather than ground truth.""""""\n    if output_projection is not None:\n      prev = tf.nn.xw_plus_b(\n          prev, output_projection[0], output_projection[1])\n    prev_symbol = tf.argmax(prev, 1)\n    # Note that gradients will not propagate through the second parameter of\n    # embedding_lookup.\n    emb_prev = tf.nn.embedding_lookup(embedding, prev_symbol)\n    if not update_embedding:\n      emb_prev = tf.stop_gradient(emb_prev)\n    return emb_prev\n  return loop_function\n\n\nclass Seq2SeqAttentionModel(object):\n  """"""Wrapper for Tensorflow model graph for text sum vectors.""""""\n\n  def __init__(self, hps, vocab, num_gpus=0):\n    self._hps = hps\n    self._vocab = vocab\n    self._num_gpus = num_gpus\n    self._cur_gpu = 0\n\n  def run_train_step(self, sess, article_batch, abstract_batch, targets,\n                     article_lens, abstract_lens, loss_weights):\n    to_return = [self._train_op, self._summaries, self._loss, self.global_step]\n    return sess.run(to_return,\n                    feed_dict={self._articles: article_batch,\n                               self._abstracts: abstract_batch,\n                               self._targets: targets,\n                               self._article_lens: article_lens,\n                               self._abstract_lens: abstract_lens,\n                               self._loss_weights: loss_weights})\n\n  def run_eval_step(self, sess, article_batch, abstract_batch, targets,\n                    article_lens, abstract_lens, loss_weights):\n    to_return = [self._summaries, self._loss, self.global_step]\n    return sess.run(to_return,\n                    feed_dict={self._articles: article_batch,\n                               self._abstracts: abstract_batch,\n                               self._targets: targets,\n                               self._article_lens: article_lens,\n                               self._abstract_lens: abstract_lens,\n                               self._loss_weights: loss_weights})\n\n  def run_decode_step(self, sess, article_batch, abstract_batch, targets,\n                      article_lens, abstract_lens, loss_weights):\n    to_return = [self._outputs, self.global_step]\n    return sess.run(to_return,\n                    feed_dict={self._articles: article_batch,\n                               self._abstracts: abstract_batch,\n                               self._targets: targets,\n                               self._article_lens: article_lens,\n                               self._abstract_lens: abstract_lens,\n                               self._loss_weights: loss_weights})\n\n  def _next_device(self):\n    """"""Round robin the gpu device. (Reserve last gpu for expensive op).""""""\n    if self._num_gpus == 0:\n      return \'\'\n    dev = \'/gpu:%d\' % self._cur_gpu\n    if self._num_gpus > 1:\n      self._cur_gpu = (self._cur_gpu + 1) % (self._num_gpus-1)\n    return dev\n\n  def _get_gpu(self, gpu_id):\n    if self._num_gpus <= 0 or gpu_id >= self._num_gpus:\n      return \'\'\n    return \'/gpu:%d\' % gpu_id\n\n  def _add_placeholders(self):\n    """"""Inputs to be fed to the graph.""""""\n    hps = self._hps\n    self._articles = tf.placeholder(tf.int32,\n                                    [hps.batch_size, hps.enc_timesteps],\n                                    name=\'articles\')\n    self._abstracts = tf.placeholder(tf.int32,\n                                     [hps.batch_size, hps.dec_timesteps],\n                                     name=\'abstracts\')\n    self._targets = tf.placeholder(tf.int32,\n                                   [hps.batch_size, hps.dec_timesteps],\n                                   name=\'targets\')\n    self._article_lens = tf.placeholder(tf.int32, [hps.batch_size],\n                                        name=\'article_lens\')\n    self._abstract_lens = tf.placeholder(tf.int32, [hps.batch_size],\n                                         name=\'abstract_lens\')\n    self._loss_weights = tf.placeholder(tf.float32,\n                                        [hps.batch_size, hps.dec_timesteps],\n                                        name=\'loss_weights\')\n\n  def _add_seq2seq(self):\n    hps = self._hps\n    vsize = self._vocab.NumIds()\n\n    with tf.variable_scope(\'seq2seq\'):\n      encoder_inputs = tf.unpack(tf.transpose(self._articles))\n      decoder_inputs = tf.unpack(tf.transpose(self._abstracts))\n      targets = tf.unpack(tf.transpose(self._targets))\n      loss_weights = tf.unpack(tf.transpose(self._loss_weights))\n      article_lens = self._article_lens\n\n      # Embedding shared by the input and outputs.\n      with tf.variable_scope(\'embedding\'), tf.device(\'/cpu:0\'):\n        embedding = tf.get_variable(\n            \'embedding\', [vsize, hps.emb_dim], dtype=tf.float32,\n            initializer=tf.truncated_normal_initializer(stddev=1e-4))\n        emb_encoder_inputs = [tf.nn.embedding_lookup(embedding, x)\n                              for x in encoder_inputs]\n        emb_decoder_inputs = [tf.nn.embedding_lookup(embedding, x)\n                              for x in decoder_inputs]\n\n      for layer_i in xrange(hps.enc_layers):\n        with tf.variable_scope(\'encoder%d\'%layer_i), tf.device(\n            self._next_device()):\n          cell_fw = tf.nn.rnn_cell.LSTMCell(\n              hps.num_hidden,\n              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=123),\n              state_is_tuple=False)\n          cell_bw = tf.nn.rnn_cell.LSTMCell(\n              hps.num_hidden,\n              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=113),\n              state_is_tuple=False)\n          (emb_encoder_inputs, fw_state, _) = tf.nn.bidirectional_rnn(\n              cell_fw, cell_bw, emb_encoder_inputs, dtype=tf.float32,\n              sequence_length=article_lens)\n      encoder_outputs = emb_encoder_inputs\n\n      with tf.variable_scope(\'output_projection\'):\n        w = tf.get_variable(\n            \'w\', [hps.num_hidden, vsize], dtype=tf.float32,\n            initializer=tf.truncated_normal_initializer(stddev=1e-4))\n        w_t = tf.transpose(w)\n        v = tf.get_variable(\n            \'v\', [vsize], dtype=tf.float32,\n            initializer=tf.truncated_normal_initializer(stddev=1e-4))\n\n      with tf.variable_scope(\'decoder\'), tf.device(self._next_device()):\n        # When decoding, use model output from the previous step\n        # for the next step.\n        loop_function = None\n        if hps.mode == \'decode\':\n          loop_function = _extract_argmax_and_embed(\n              embedding, (w, v), update_embedding=False)\n\n        cell = tf.nn.rnn_cell.LSTMCell(\n            hps.num_hidden,\n            initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=113),\n            state_is_tuple=False)\n\n        encoder_outputs = [tf.reshape(x, [hps.batch_size, 1, 2*hps.num_hidden])\n                           for x in encoder_outputs]\n        self._enc_top_states = tf.concat(1, encoder_outputs)\n        self._dec_in_state = fw_state\n        # During decoding, follow up _dec_in_state are fed from beam_search.\n        # dec_out_state are stored by beam_search for next step feeding.\n        initial_state_attention = (hps.mode == \'decode\')\n        decoder_outputs, self._dec_out_state = tf.nn.seq2seq.attention_decoder(\n            emb_decoder_inputs, self._dec_in_state, self._enc_top_states,\n            cell, num_heads=1, loop_function=loop_function,\n            initial_state_attention=initial_state_attention)\n\n      with tf.variable_scope(\'output\'), tf.device(self._next_device()):\n        model_outputs = []\n        for i in xrange(len(decoder_outputs)):\n          if i > 0:\n            tf.get_variable_scope().reuse_variables()\n          model_outputs.append(\n              tf.nn.xw_plus_b(decoder_outputs[i], w, v))\n\n      if hps.mode == \'decode\':\n        with tf.variable_scope(\'decode_output\'), tf.device(\'/cpu:0\'):\n          best_outputs = [tf.argmax(x, 1) for x in model_outputs]\n          tf.logging.info(\'best_outputs%s\', best_outputs[0].get_shape())\n          self._outputs = tf.concat(\n              1, [tf.reshape(x, [hps.batch_size, 1]) for x in best_outputs])\n\n          self._topk_log_probs, self._topk_ids = tf.nn.top_k(\n              tf.log(tf.nn.softmax(model_outputs[-1])), hps.batch_size*2)\n\n      with tf.variable_scope(\'loss\'), tf.device(self._next_device()):\n        def sampled_loss_func(inputs, labels):\n          with tf.device(\'/cpu:0\'):  # Try gpu.\n            labels = tf.reshape(labels, [-1, 1])\n            return tf.nn.sampled_softmax_loss(w_t, v, inputs, labels,\n                                              hps.num_softmax_samples, vsize)\n\n        if hps.num_softmax_samples != 0 and hps.mode == \'train\':\n          self._loss = seq2seq_lib.sampled_sequence_loss(\n              decoder_outputs, targets, loss_weights, sampled_loss_func)\n        else:\n          self._loss = tf.nn.seq2seq.sequence_loss(\n              model_outputs, targets, loss_weights)\n        tf.scalar_summary(\'loss\', tf.minimum(12.0, self._loss))\n\n  def _add_train_op(self):\n    """"""Sets self._train_op, op to run for training.""""""\n    hps = self._hps\n\n    self._lr_rate = tf.maximum(\n        hps.min_lr,  # min_lr_rate.\n        tf.train.exponential_decay(hps.lr, self.global_step, 30000, 0.98))\n\n    tvars = tf.trainable_variables()\n    with tf.device(self._get_gpu(self._num_gpus-1)):\n      grads, global_norm = tf.clip_by_global_norm(\n          tf.gradients(self._loss, tvars), hps.max_grad_norm)\n    tf.scalar_summary(\'global_norm\', global_norm)\n    optimizer = tf.train.GradientDescentOptimizer(self._lr_rate)\n    tf.scalar_summary(\'learning rate\', self._lr_rate)\n    self._train_op = optimizer.apply_gradients(\n        zip(grads, tvars), global_step=self.global_step, name=\'train_step\')\n\n  def encode_top_state(self, sess, enc_inputs, enc_len):\n    """"""Return the top states from encoder for decoder.\n\n    Args:\n      sess: tensorflow session.\n      enc_inputs: encoder inputs of shape [batch_size, enc_timesteps].\n      enc_len: encoder input length of shape [batch_size]\n    Returns:\n      enc_top_states: The top level encoder states.\n      dec_in_state: The decoder layer initial state.\n    """"""\n    results = sess.run([self._enc_top_states, self._dec_in_state],\n                       feed_dict={self._articles: enc_inputs,\n                                  self._article_lens: enc_len})\n    return results[0], results[1][0]\n\n  def decode_topk(self, sess, latest_tokens, enc_top_states, dec_init_states):\n    """"""Return the topK results and new decoder states.""""""\n    feed = {\n        self._enc_top_states: enc_top_states,\n        self._dec_in_state:\n            np.squeeze(np.array(dec_init_states)),\n        self._abstracts:\n            np.transpose(np.array([latest_tokens])),\n        self._abstract_lens: np.ones([len(dec_init_states)], np.int32)}\n\n    results = sess.run(\n        [self._topk_ids, self._topk_log_probs, self._dec_out_state],\n        feed_dict=feed)\n\n    ids, probs, states = results[0], results[1], results[2]\n    new_states = [s for s in states]\n    return ids, probs, new_states\n\n  def build_graph(self):\n    self._add_placeholders()\n    self._add_seq2seq()\n    self.global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n    if self._hps.mode == \'train\':\n      self._add_train_op()\n    self._summaries = tf.merge_all_summaries()\n'"
model_zoo/models/textsum/seq2seq_lib.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""seq2seq library codes copied from elsewhere for customization.""""""\n\nimport tensorflow as tf\n\n\n# Adapted to support sampled_softmax loss function, which accepts activations\n# instead of logits.\ndef sequence_loss_by_example(inputs, targets, weights, loss_function,\n                             average_across_timesteps=True, name=None):\n  """"""Sampled softmax loss for a sequence of inputs (per example).\n\n  Args:\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\n    targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n    weights: List of 1D batch-sized float-Tensors of the same length as logits.\n    loss_function: Sampled softmax function (inputs, labels) -> loss\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    name: Optional name for this operation, default: \'sequence_loss_by_example\'.\n\n  Returns:\n    1D batch-sized float Tensor: The log-perplexity for each sequence.\n\n  Raises:\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\n  """"""\n  if len(targets) != len(inputs) or len(weights) != len(inputs):\n    raise ValueError(\'Lengths of logits, weights, and targets must be the same \'\n                     \'%d, %d, %d.\' % (len(inputs), len(weights), len(targets)))\n  with tf.op_scope(inputs + targets + weights, name,\n                   \'sequence_loss_by_example\'):\n    log_perp_list = []\n    for inp, target, weight in zip(inputs, targets, weights):\n      crossent = loss_function(inp, target)\n      log_perp_list.append(crossent * weight)\n    log_perps = tf.add_n(log_perp_list)\n    if average_across_timesteps:\n      total_size = tf.add_n(weights)\n      total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n      log_perps /= total_size\n  return log_perps\n\n\ndef sampled_sequence_loss(inputs, targets, weights, loss_function,\n                          average_across_timesteps=True,\n                          average_across_batch=True, name=None):\n  """"""Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\n\n  Args:\n    inputs: List of 2D Tensors of shape [batch_size x hid_dim].\n    targets: List of 1D batch-sized int32 Tensors of the same length as inputs.\n    weights: List of 1D batch-sized float-Tensors of the same length as inputs.\n    loss_function: Sampled softmax function (inputs, labels) -> loss\n    average_across_timesteps: If set, divide the returned cost by the total\n      label weight.\n    average_across_batch: If set, divide the returned cost by the batch size.\n    name: Optional name for this operation, defaults to \'sequence_loss\'.\n\n  Returns:\n    A scalar float Tensor: The average log-perplexity per symbol (weighted).\n\n  Raises:\n    ValueError: If len(inputs) is different from len(targets) or len(weights).\n  """"""\n  with tf.op_scope(inputs + targets + weights, name, \'sampled_sequence_loss\'):\n    cost = tf.reduce_sum(sequence_loss_by_example(\n        inputs, targets, weights, loss_function,\n        average_across_timesteps=average_across_timesteps))\n    if average_across_batch:\n      batch_size = tf.shape(targets[0])[0]\n      return cost / tf.cast(batch_size, tf.float32)\n    else:\n      return cost\n\n\ndef linear(args, output_size, bias, bias_start=0.0, scope=None):\n  """"""Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n    scope: VariableScope for the created subgraph; defaults to ""Linear"".\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  """"""\n  if args is None or (isinstance(args, (list, tuple)) and not args):\n    raise ValueError(\'`args` must be specified\')\n  if not isinstance(args, (list, tuple)):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape().as_list() for a in args]\n  for shape in shapes:\n    if len(shape) != 2:\n      raise ValueError(\'Linear is expecting 2D arguments: %s\' % str(shapes))\n    if not shape[1]:\n      raise ValueError(\'Linear expects shape[1] of arguments: %s\' % str(shapes))\n    else:\n      total_arg_size += shape[1]\n\n  # Now the computation.\n  with tf.variable_scope(scope or \'Linear\'):\n    matrix = tf.get_variable(\'Matrix\', [total_arg_size, output_size])\n    if len(args) == 1:\n      res = tf.matmul(args[0], matrix)\n    else:\n      res = tf.matmul(tf.concat(1, args), matrix)\n    if not bias:\n      return res\n    bias_term = tf.get_variable(\n        \'Bias\', [output_size],\n        initializer=tf.constant_initializer(bias_start))\n  return res + bias_term\n'"
model_zoo/models/transformer/cluttered_mnist.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# =============================================================================\nimport tensorflow as tf\nfrom spatial_transformer import transformer\nimport numpy as np\nfrom tf_utils import weight_variable, bias_variable, dense_to_one_hot\n\n# %% Load data\nmnist_cluttered = np.load(\'./data/mnist_sequence1_sample_5distortions5x5.npz\')\n\nX_train = mnist_cluttered[\'X_train\']\ny_train = mnist_cluttered[\'y_train\']\nX_valid = mnist_cluttered[\'X_valid\']\ny_valid = mnist_cluttered[\'y_valid\']\nX_test = mnist_cluttered[\'X_test\']\ny_test = mnist_cluttered[\'y_test\']\n\n# % turn from dense to one hot representation\nY_train = dense_to_one_hot(y_train, n_classes=10)\nY_valid = dense_to_one_hot(y_valid, n_classes=10)\nY_test = dense_to_one_hot(y_test, n_classes=10)\n\n# %% Graph representation of our network\n\n# %% Placeholders for 40x40 resolution\nx = tf.placeholder(tf.float32, [None, 1600])\ny = tf.placeholder(tf.float32, [None, 10])\n\n# %% Since x is currently [batch, height*width], we need to reshape to a\n# 4-D tensor to use it in a convolutional graph.  If one component of\n# `shape` is the special value -1, the size of that dimension is\n# computed so that the total size remains constant.  Since we haven\'t\n# defined the batch dimension\'s shape yet, we use -1 to denote this\n# dimension should not change size.\nx_tensor = tf.reshape(x, [-1, 40, 40, 1])\n\n# %% We\'ll setup the two-layer localisation network to figure out the\n# %% parameters for an affine transformation of the input\n# %% Create variables for fully connected layer\nW_fc_loc1 = weight_variable([1600, 20])\nb_fc_loc1 = bias_variable([20])\n\nW_fc_loc2 = weight_variable([20, 6])\n# Use identity transformation as starting point\ninitial = np.array([[1., 0, 0], [0, 1., 0]])\ninitial = initial.astype(\'float32\')\ninitial = initial.flatten()\nb_fc_loc2 = tf.Variable(initial_value=initial, name=\'b_fc_loc2\')\n\n# %% Define the two layer localisation network\nh_fc_loc1 = tf.nn.tanh(tf.matmul(x, W_fc_loc1) + b_fc_loc1)\n# %% We can add dropout for regularizing and to reduce overfitting like so:\nkeep_prob = tf.placeholder(tf.float32)\nh_fc_loc1_drop = tf.nn.dropout(h_fc_loc1, keep_prob)\n# %% Second layer\nh_fc_loc2 = tf.nn.tanh(tf.matmul(h_fc_loc1_drop, W_fc_loc2) + b_fc_loc2)\n\n# %% We\'ll create a spatial transformer module to identify discriminative\n# %% patches\nout_size = (40, 40)\nh_trans = transformer(x_tensor, h_fc_loc2, out_size)\n\n# %% We\'ll setup the first convolutional layer\n# Weight matrix is [height x width x input_channels x output_channels]\nfilter_size = 3\nn_filters_1 = 16\nW_conv1 = weight_variable([filter_size, filter_size, 1, n_filters_1])\n\n# %% Bias is [output_channels]\nb_conv1 = bias_variable([n_filters_1])\n\n# %% Now we can build a graph which does the first layer of convolution:\n# we define our stride as batch x height x width x channels\n# instead of pooling, we use strides of 2 and more layers\n# with smaller filters.\n\nh_conv1 = tf.nn.relu(\n    tf.nn.conv2d(input=h_trans,\n                 filter=W_conv1,\n                 strides=[1, 2, 2, 1],\n                 padding=\'SAME\') +\n    b_conv1)\n\n# %% And just like the first layer, add additional layers to create\n# a deep net\nn_filters_2 = 16\nW_conv2 = weight_variable([filter_size, filter_size, n_filters_1, n_filters_2])\nb_conv2 = bias_variable([n_filters_2])\nh_conv2 = tf.nn.relu(\n    tf.nn.conv2d(input=h_conv1,\n                 filter=W_conv2,\n                 strides=[1, 2, 2, 1],\n                 padding=\'SAME\') +\n    b_conv2)\n\n# %% We\'ll now reshape so we can connect to a fully-connected layer:\nh_conv2_flat = tf.reshape(h_conv2, [-1, 10 * 10 * n_filters_2])\n\n# %% Create a fully-connected layer:\nn_fc = 1024\nW_fc1 = weight_variable([10 * 10 * n_filters_2, n_fc])\nb_fc1 = bias_variable([n_fc])\nh_fc1 = tf.nn.relu(tf.matmul(h_conv2_flat, W_fc1) + b_fc1)\n\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n# %% And finally our softmax layer:\nW_fc2 = weight_variable([n_fc, 10])\nb_fc2 = bias_variable([10])\ny_logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n\n# %% Define loss/eval/training functions\ncross_entropy = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits(y_logits, y))\nopt = tf.train.AdamOptimizer()\noptimizer = opt.minimize(cross_entropy)\ngrads = opt.compute_gradients(cross_entropy, [b_fc_loc2])\n\n# %% Monitor accuracy\ncorrect_prediction = tf.equal(tf.argmax(y_logits, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \'float\'))\n\n# %% We now create a new session to actually perform the initialization the\n# variables:\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\n\n# %% We\'ll now train in minibatches and report accuracy, loss:\niter_per_epoch = 100\nn_epochs = 500\ntrain_size = 10000\n\nindices = np.linspace(0, 10000 - 1, iter_per_epoch)\nindices = indices.astype(\'int\')\n\nfor epoch_i in range(n_epochs):\n    for iter_i in range(iter_per_epoch - 1):\n        batch_xs = X_train[indices[iter_i]:indices[iter_i+1]]\n        batch_ys = Y_train[indices[iter_i]:indices[iter_i+1]]\n\n        if iter_i % 10 == 0:\n            loss = sess.run(cross_entropy,\n                            feed_dict={\n                                x: batch_xs,\n                                y: batch_ys,\n                                keep_prob: 1.0\n                            })\n            print(\'Iteration: \' + str(iter_i) + \' Loss: \' + str(loss))\n\n        sess.run(optimizer, feed_dict={\n            x: batch_xs, y: batch_ys, keep_prob: 0.8})\n\n    print(\'Accuracy (%d): \' % epoch_i + str(sess.run(accuracy,\n                                                     feed_dict={\n                                                         x: X_valid,\n                                                         y: Y_valid,\n                                                         keep_prob: 1.0\n                                                     })))\n    # theta = sess.run(h_fc_loc2, feed_dict={\n    #        x: batch_xs, keep_prob: 1.0})\n    # print(theta[0])\n'"
model_zoo/models/transformer/example.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nfrom scipy import ndimage\nimport tensorflow as tf\nfrom spatial_transformer import transformer\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# %% Create a batch of three images (1600 x 1200)\n# %% Image retrieved from:\n# %% https://raw.githubusercontent.com/skaae/transformer_network/master/cat.jpg\nim = ndimage.imread(\'cat.jpg\')\nim = im / 255.\nim = im.reshape(1, 1200, 1600, 3)\nim = im.astype(\'float32\')\n\n# %% Let the output size of the transformer be half the image size.\nout_size = (600, 800)\n\n# %% Simulate batch\nbatch = np.append(im, im, axis=0)\nbatch = np.append(batch, im, axis=0)\nnum_batch = 3\n\nx = tf.placeholder(tf.float32, [None, 1200, 1600, 3])\nx = tf.cast(batch, \'float32\')\n\n# %% Create localisation network and convolutional layer\nwith tf.variable_scope(\'spatial_transformer_0\'):\n\n    # %% Create a fully-connected layer with 6 output nodes\n    n_fc = 6\n    W_fc1 = tf.Variable(tf.zeros([1200 * 1600 * 3, n_fc]), name=\'W_fc1\')\n\n    # %% Zoom into the image\n    initial = np.array([[0.5, 0, 0], [0, 0.5, 0]])\n    initial = initial.astype(\'float32\')\n    initial = initial.flatten()\n\n    b_fc1 = tf.Variable(initial_value=initial, name=\'b_fc1\')\n    h_fc1 = tf.matmul(tf.zeros([num_batch, 1200 * 1600 * 3]), W_fc1) + b_fc1\n    h_trans = transformer(x, h_fc1, out_size)\n\n# %% Run session\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\ny = sess.run(h_trans, feed_dict={x: batch})\n\n# plt.imshow(y[0])\n'"
model_zoo/models/transformer/spatial_transformer.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport tensorflow as tf\n\n\ndef transformer(U, theta, out_size, name=\'SpatialTransformer\', **kwargs):\n    """"""Spatial Transformer Layer\n\n    Implements a spatial transformer layer as described in [1]_.\n    Based on [2]_ and edited by David Dao for Tensorflow.\n\n    Parameters\n    ----------\n    U : float\n        The output of a convolutional net should have the\n        shape [num_batch, height, width, num_channels].\n    theta: float\n        The output of the\n        localisation network should be [num_batch, 6].\n    out_size: tuple of two ints\n        The size of the output of the network (height, width)\n\n    References\n    ----------\n    .. [1]  Spatial Transformer Networks\n            Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu\n            Submitted on 5 Jun 2015\n    .. [2]  https://github.com/skaae/transformer_network/blob/master/transformerlayer.py\n\n    Notes\n    -----\n    To initialize the network to the identity transform init\n    ``theta`` to :\n        identity = np.array([[1., 0., 0.],\n                             [0., 1., 0.]])\n        identity = identity.flatten()\n        theta = tf.Variable(initial_value=identity)\n\n    """"""\n\n    def _repeat(x, n_repeats):\n        with tf.variable_scope(\'_repeat\'):\n            rep = tf.transpose(\n                tf.expand_dims(tf.ones(shape=tf.pack([n_repeats, ])), 1), [1, 0])\n            rep = tf.cast(rep, \'int32\')\n            x = tf.matmul(tf.reshape(x, (-1, 1)), rep)\n            return tf.reshape(x, [-1])\n\n    def _interpolate(im, x, y, out_size):\n        with tf.variable_scope(\'_interpolate\'):\n            # constants\n            num_batch = tf.shape(im)[0]\n            height = tf.shape(im)[1]\n            width = tf.shape(im)[2]\n            channels = tf.shape(im)[3]\n\n            x = tf.cast(x, \'float32\')\n            y = tf.cast(y, \'float32\')\n            height_f = tf.cast(height, \'float32\')\n            width_f = tf.cast(width, \'float32\')\n            out_height = out_size[0]\n            out_width = out_size[1]\n            zero = tf.zeros([], dtype=\'int32\')\n            max_y = tf.cast(tf.shape(im)[1] - 1, \'int32\')\n            max_x = tf.cast(tf.shape(im)[2] - 1, \'int32\')\n\n            # scale indices from [-1, 1] to [0, width/height]\n            x = (x + 1.0)*(width_f) / 2.0\n            y = (y + 1.0)*(height_f) / 2.0\n\n            # do sampling\n            x0 = tf.cast(tf.floor(x), \'int32\')\n            x1 = x0 + 1\n            y0 = tf.cast(tf.floor(y), \'int32\')\n            y1 = y0 + 1\n\n            x0 = tf.clip_by_value(x0, zero, max_x)\n            x1 = tf.clip_by_value(x1, zero, max_x)\n            y0 = tf.clip_by_value(y0, zero, max_y)\n            y1 = tf.clip_by_value(y1, zero, max_y)\n            dim2 = width\n            dim1 = width*height\n            base = _repeat(tf.range(num_batch)*dim1, out_height*out_width)\n            base_y0 = base + y0*dim2\n            base_y1 = base + y1*dim2\n            idx_a = base_y0 + x0\n            idx_b = base_y1 + x0\n            idx_c = base_y0 + x1\n            idx_d = base_y1 + x1\n\n            # use indices to lookup pixels in the flat image and restore\n            # channels dim\n            im_flat = tf.reshape(im, tf.pack([-1, channels]))\n            im_flat = tf.cast(im_flat, \'float32\')\n            Ia = tf.gather(im_flat, idx_a)\n            Ib = tf.gather(im_flat, idx_b)\n            Ic = tf.gather(im_flat, idx_c)\n            Id = tf.gather(im_flat, idx_d)\n\n            # and finally calculate interpolated values\n            x0_f = tf.cast(x0, \'float32\')\n            x1_f = tf.cast(x1, \'float32\')\n            y0_f = tf.cast(y0, \'float32\')\n            y1_f = tf.cast(y1, \'float32\')\n            wa = tf.expand_dims(((x1_f-x) * (y1_f-y)), 1)\n            wb = tf.expand_dims(((x1_f-x) * (y-y0_f)), 1)\n            wc = tf.expand_dims(((x-x0_f) * (y1_f-y)), 1)\n            wd = tf.expand_dims(((x-x0_f) * (y-y0_f)), 1)\n            output = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n            return output\n\n    def _meshgrid(height, width):\n        with tf.variable_scope(\'_meshgrid\'):\n            # This should be equivalent to:\n            #  x_t, y_t = np.meshgrid(np.linspace(-1, 1, width),\n            #                         np.linspace(-1, 1, height))\n            #  ones = np.ones(np.prod(x_t.shape))\n            #  grid = np.vstack([x_t.flatten(), y_t.flatten(), ones])\n            x_t = tf.matmul(tf.ones(shape=tf.pack([height, 1])),\n                            tf.transpose(tf.expand_dims(tf.linspace(-1.0, 1.0, width), 1), [1, 0]))\n            y_t = tf.matmul(tf.expand_dims(tf.linspace(-1.0, 1.0, height), 1),\n                            tf.ones(shape=tf.pack([1, width])))\n\n            x_t_flat = tf.reshape(x_t, (1, -1))\n            y_t_flat = tf.reshape(y_t, (1, -1))\n\n            ones = tf.ones_like(x_t_flat)\n            grid = tf.concat(0, [x_t_flat, y_t_flat, ones])\n            return grid\n\n    def _transform(theta, input_dim, out_size):\n        with tf.variable_scope(\'_transform\'):\n            num_batch = tf.shape(input_dim)[0]\n            height = tf.shape(input_dim)[1]\n            width = tf.shape(input_dim)[2]\n            num_channels = tf.shape(input_dim)[3]\n            theta = tf.reshape(theta, (-1, 2, 3))\n            theta = tf.cast(theta, \'float32\')\n\n            # grid of (x_t, y_t, 1), eq (1) in ref [1]\n            height_f = tf.cast(height, \'float32\')\n            width_f = tf.cast(width, \'float32\')\n            out_height = out_size[0]\n            out_width = out_size[1]\n            grid = _meshgrid(out_height, out_width)\n            grid = tf.expand_dims(grid, 0)\n            grid = tf.reshape(grid, [-1])\n            grid = tf.tile(grid, tf.pack([num_batch]))\n            grid = tf.reshape(grid, tf.pack([num_batch, 3, -1]))\n\n            # Transform A x (x_t, y_t, 1)^T -> (x_s, y_s)\n            T_g = tf.batch_matmul(theta, grid)\n            x_s = tf.slice(T_g, [0, 0, 0], [-1, 1, -1])\n            y_s = tf.slice(T_g, [0, 1, 0], [-1, 1, -1])\n            x_s_flat = tf.reshape(x_s, [-1])\n            y_s_flat = tf.reshape(y_s, [-1])\n\n            input_transformed = _interpolate(\n                input_dim, x_s_flat, y_s_flat,\n                out_size)\n\n            output = tf.reshape(\n                input_transformed, tf.pack([num_batch, out_height, out_width, num_channels]))\n            return output\n\n    with tf.variable_scope(name):\n        output = _transform(theta, U, out_size)\n        return output\n\n\ndef batch_transformer(U, thetas, out_size, name=\'BatchSpatialTransformer\'):\n    """"""Batch Spatial Transformer Layer\n\n    Parameters\n    ----------\n\n    U : float\n        tensor of inputs [num_batch,height,width,num_channels]\n    thetas : float\n        a set of transformations for each input [num_batch,num_transforms,6]\n    out_size : int\n        the size of the output [out_height,out_width]\n\n    Returns: float\n        Tensor of size [num_batch*num_transforms,out_height,out_width,num_channels]\n    """"""\n    with tf.variable_scope(name):\n        num_batch, num_transforms = map(int, thetas.get_shape().as_list()[:2])\n        indices = [[i]*num_transforms for i in xrange(num_batch)]\n        input_repeated = tf.gather(U, tf.reshape(indices, [-1]))\n        return transformer(input_repeated, thetas, out_size)\n'"
model_zoo/models/transformer/tf_utils.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# %% Borrowed utils from here: https://github.com/pkmital/tensorflow_tutorials/\nimport tensorflow as tf\nimport numpy as np\n\ndef conv2d(x, n_filters,\n           k_h=5, k_w=5,\n           stride_h=2, stride_w=2,\n           stddev=0.02,\n           activation=lambda x: x,\n           bias=True,\n           padding=\'SAME\',\n           name=""Conv2D""):\n    """"""2D Convolution with options for kernel size, stride, and init deviation.\n    Parameters\n    ----------\n    x : Tensor\n        Input tensor to convolve.\n    n_filters : int\n        Number of filters to apply.\n    k_h : int, optional\n        Kernel height.\n    k_w : int, optional\n        Kernel width.\n    stride_h : int, optional\n        Stride in rows.\n    stride_w : int, optional\n        Stride in cols.\n    stddev : float, optional\n        Initialization\'s standard deviation.\n    activation : arguments, optional\n        Function which applies a nonlinearity\n    padding : str, optional\n        \'SAME\' or \'VALID\'\n    name : str, optional\n        Variable scope to use.\n    Returns\n    -------\n    x : Tensor\n        Convolved input.\n    """"""\n    with tf.variable_scope(name):\n        w = tf.get_variable(\n            \'w\', [k_h, k_w, x.get_shape()[-1], n_filters],\n            initializer=tf.truncated_normal_initializer(stddev=stddev))\n        conv = tf.nn.conv2d(\n            x, w, strides=[1, stride_h, stride_w, 1], padding=padding)\n        if bias:\n            b = tf.get_variable(\n                \'b\', [n_filters],\n                initializer=tf.truncated_normal_initializer(stddev=stddev))\n            conv = conv + b\n        return conv\n    \ndef linear(x, n_units, scope=None, stddev=0.02,\n           activation=lambda x: x):\n    """"""Fully-connected network.\n    Parameters\n    ----------\n    x : Tensor\n        Input tensor to the network.\n    n_units : int\n        Number of units to connect to.\n    scope : str, optional\n        Variable scope to use.\n    stddev : float, optional\n        Initialization\'s standard deviation.\n    activation : arguments, optional\n        Function which applies a nonlinearity\n    Returns\n    -------\n    x : Tensor\n        Fully-connected output.\n    """"""\n    shape = x.get_shape().as_list()\n\n    with tf.variable_scope(scope or ""Linear""):\n        matrix = tf.get_variable(""Matrix"", [shape[1], n_units], tf.float32,\n                                 tf.random_normal_initializer(stddev=stddev))\n        return activation(tf.matmul(x, matrix))\n    \n# %%\ndef weight_variable(shape):\n    \'\'\'Helper function to create a weight variable initialized with\n    a normal distribution\n    Parameters\n    ----------\n    shape : list\n        Size of weight variable\n    \'\'\'\n    #initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n    initial = tf.zeros(shape)\n    return tf.Variable(initial)\n\n# %%\ndef bias_variable(shape):\n    \'\'\'Helper function to create a bias variable initialized with\n    a constant value.\n    Parameters\n    ----------\n    shape : list\n        Size of weight variable\n    \'\'\'\n    initial = tf.random_normal(shape, mean=0.0, stddev=0.01)\n    return tf.Variable(initial)\n\n# %% \ndef dense_to_one_hot(labels, n_classes=2):\n    """"""Convert class labels from scalars to one-hot vectors.""""""\n    labels = np.array(labels)\n    n_labels = labels.shape[0]\n    index_offset = np.arange(n_labels) * n_classes\n    labels_one_hot = np.zeros((n_labels, n_classes), dtype=np.float32)\n    labels_one_hot.flat[index_offset + labels.ravel()] = 1\n    return labels_one_hot\n'"
model_zoo/models/video_prediction/lstm_ops.py,0,"b'# Copyright 2016 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Convolutional LSTM implementation.""""""\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib.slim import add_arg_scope\nfrom tensorflow.contrib.slim import layers\n\n\ndef init_state(inputs,\n               state_shape,\n               state_initializer=tf.zeros_initializer,\n               dtype=tf.float32):\n  """"""Helper function to create an initial state given inputs.\n\n  Args:\n    inputs: input Tensor, at least 2D, the first dimension being batch_size\n    state_shape: the shape of the state.\n    state_initializer: Initializer(shape, dtype) for state Tensor.\n    dtype: Optional dtype, needed when inputs is None.\n  Returns:\n     A tensors representing the initial state.\n  """"""\n  if inputs is not None:\n    # Handle both the dynamic shape as well as the inferred shape.\n    inferred_batch_size = inputs.get_shape().with_rank_at_least(1)[0]\n    batch_size = tf.shape(inputs)[0]\n    dtype = inputs.dtype\n  else:\n    inferred_batch_size = 0\n    batch_size = 0\n\n  initial_state = state_initializer(\n      tf.pack([batch_size] + state_shape),\n      dtype=dtype)\n  initial_state.set_shape([inferred_batch_size] + state_shape)\n\n  return initial_state\n\n\n@add_arg_scope\ndef basic_conv_lstm_cell(inputs,\n                         state,\n                         num_channels,\n                         filter_size=5,\n                         forget_bias=1.0,\n                         scope=None,\n                         reuse=None):\n  """"""Basic LSTM recurrent network cell, with 2D convolution connctions.\n\n  We add forget_bias (default: 1) to the biases of the forget gate in order to\n  reduce the scale of forgetting in the beginning of the training.\n\n  It does not allow cell clipping, a projection layer, and does not\n  use peep-hole connections: it is the basic baseline.\n\n  Args:\n    inputs: input Tensor, 4D, batch x height x width x channels.\n    state: state Tensor, 4D, batch x height x width x channels.\n    num_channels: the number of output channels in the layer.\n    filter_size: the shape of the each convolution filter.\n    forget_bias: the initial value of the forget biases.\n    scope: Optional scope for variable_scope.\n    reuse: whether or not the layer and the variables should be reused.\n\n  Returns:\n     a tuple of tensors representing output and the new state.\n  """"""\n  spatial_size = inputs.get_shape()[1:3]\n  if state is None:\n    state = init_state(inputs, list(spatial_size) + [2 * num_channels])\n  with tf.variable_scope(scope,\n                         \'BasicConvLstmCell\',\n                         [inputs, state],\n                         reuse=reuse):\n    inputs.get_shape().assert_has_rank(4)\n    state.get_shape().assert_has_rank(4)\n    c, h = tf.split(3, 2, state)\n    inputs_h = tf.concat(3, [inputs, h])\n    # Parameters of gates are concatenated into one conv for efficiency.\n    i_j_f_o = layers.conv2d(inputs_h,\n                            4 * num_channels, [filter_size, filter_size],\n                            stride=1,\n                            activation_fn=None,\n                            scope=\'Gates\')\n\n    # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n    i, j, f, o = tf.split(3, 4, i_j_f_o)\n\n    new_c = c * tf.sigmoid(f + forget_bias) + tf.sigmoid(i) * tf.tanh(j)\n    new_h = tf.tanh(new_c) * tf.sigmoid(o)\n\n    return new_h, tf.concat(3, [new_c, new_h])\n\n\n\n'"
model_zoo/models/video_prediction/prediction_input.py,0,"b'# Copyright 2016 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Code for building the input for the prediction model.""""""\n\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import flags\nfrom tensorflow.python.platform import gfile\n\n\nFLAGS = flags.FLAGS\n\n# Original image dimensions\nORIGINAL_WIDTH = 640\nORIGINAL_HEIGHT = 512\nCOLOR_CHAN = 3\n\n# Default image dimensions.\nIMG_WIDTH = 64\nIMG_HEIGHT = 64\n\n# Dimension of the state and action.\nSTATE_DIM = 5\n\n\ndef build_tfrecord_input(training=True):\n  """"""Create input tfrecord tensors.\n\n  Args:\n    training: training or validation data.\n  Returns:\n    list of tensors corresponding to images, actions, and states. The images\n    tensor is 5D, batch x time x height x width x channels. The state and\n    action tensors are 3D, batch x time x dimension.\n  Raises:\n    RuntimeError: if no files found.\n  """"""\n  filenames = gfile.Glob(os.path.join(FLAGS.data_dir, \'*\'))\n  if not filenames:\n    raise RuntimeError(\'No data files found.\')\n  index = int(np.floor(FLAGS.train_val_split * len(filenames)))\n  if training:\n    filenames = filenames[:index]\n  else:\n    filenames = filenames[index:]\n  filename_queue = tf.train.string_input_producer(filenames, shuffle=True)\n  reader = tf.TFRecordReader()\n  _, serialized_example = reader.read(filename_queue)\n\n  image_seq, state_seq, action_seq = [], [], []\n\n  for i in range(FLAGS.sequence_length):\n    image_name = \'move/\' + str(i) + \'/image/encoded\'\n    action_name = \'move/\' + str(i) + \'/commanded_pose/vec_pitch_yaw\'\n    state_name = \'move/\' + str(i) + \'/endeffector/vec_pitch_yaw\'\n    if FLAGS.use_state:\n      features = {image_name: tf.FixedLenFeature([1], tf.string),\n                  action_name: tf.FixedLenFeature([STATE_DIM], tf.float32),\n                  state_name: tf.FixedLenFeature([STATE_DIM], tf.float32)}\n    else:\n      features = {image_name: tf.FixedLenFeature([1], tf.string)}\n    features = tf.parse_single_example(serialized_example, features=features)\n\n    image_buffer = tf.reshape(features[image_name], shape=[])\n    image = tf.image.decode_jpeg(image_buffer, channels=COLOR_CHAN)\n    image.set_shape([ORIGINAL_HEIGHT, ORIGINAL_WIDTH, COLOR_CHAN])\n\n    if IMG_HEIGHT != IMG_WIDTH:\n      raise ValueError(\'Unequal height and width unsupported\')\n\n    crop_size = min(ORIGINAL_HEIGHT, ORIGINAL_WIDTH)\n    image = tf.image.resize_image_with_crop_or_pad(image, crop_size, crop_size)\n    image = tf.reshape(image, [1, crop_size, crop_size, COLOR_CHAN])\n    image = tf.image.resize_bicubic(image, [IMG_HEIGHT, IMG_WIDTH])\n    image = tf.cast(image, tf.float32) / 255.0\n    image_seq.append(image)\n\n    if FLAGS.use_state:\n      state = tf.reshape(features[state_name], shape=[1, STATE_DIM])\n      state_seq.append(state)\n      action = tf.reshape(features[action_name], shape=[1, STATE_DIM])\n      action_seq.append(action)\n\n  image_seq = tf.concat(0, image_seq)\n\n  if FLAGS.use_state:\n    state_seq = tf.concat(0, state_seq)\n    action_seq = tf.concat(0, action_seq)\n    [image_batch, action_batch, state_batch] = tf.train.batch(\n        [image_seq, action_seq, state_seq],\n        FLAGS.batch_size,\n        num_threads=FLAGS.batch_size,\n        capacity=100 * FLAGS.batch_size)\n    return image_batch, action_batch, state_batch\n  else:\n    image_batch = tf.train.batch(\n        [image_seq],\n        FLAGS.batch_size,\n        num_threads=FLAGS.batch_size,\n        capacity=100 * FLAGS.batch_size)\n    zeros_batch = tf.zeros([FLAGS.batch_size, FLAGS.sequence_length, STATE_DIM])\n    return image_batch, zeros_batch, zeros_batch\n\n'"
model_zoo/models/video_prediction/prediction_model.py,0,"b'# Copyright 2016 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Model architecture for predictive model, including CDNA, DNA, and STP.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.layers.python import layers as tf_layers\nfrom lstm_ops import basic_conv_lstm_cell\n\n# Amount to use when lower bounding tensors\nRELU_SHIFT = 1e-12\n\n# kernel size for DNA and CDNA.\nDNA_KERN_SIZE = 5\n\n\ndef construct_model(images,\n                    actions=None,\n                    states=None,\n                    iter_num=-1.0,\n                    k=-1,\n                    use_state=True,\n                    num_masks=10,\n                    stp=False,\n                    cdna=True,\n                    dna=False,\n                    context_frames=2):\n  """"""Build convolutional lstm video predictor using STP, CDNA, or DNA.\n\n  Args:\n    images: tensor of ground truth image sequences\n    actions: tensor of action sequences\n    states: tensor of ground truth state sequences\n    iter_num: tensor of the current training iteration (for sched. sampling)\n    k: constant used for scheduled sampling. -1 to feed in own prediction.\n    use_state: True to include state and action in prediction\n    num_masks: the number of different pixel motion predictions (and\n               the number of masks for each of those predictions)\n    stp: True to use Spatial Transformer Predictor (STP)\n    cdna: True to use Convoluational Dynamic Neural Advection (CDNA)\n    dna: True to use Dynamic Neural Advection (DNA)\n    context_frames: number of ground truth frames to pass in before\n                    feeding in own predictions\n  Returns:\n    gen_images: predicted future image frames\n    gen_states: predicted future states\n\n  Raises:\n    ValueError: if more than one network option specified or more than 1 mask\n    specified for DNA model.\n  """"""\n  if stp + cdna + dna != 1:\n    raise ValueError(\'More than one, or no network option specified.\')\n  batch_size, img_height, img_width, color_channels = images[0].get_shape()[0:4]\n  lstm_func = basic_conv_lstm_cell\n\n  # Generated robot states and images.\n  gen_states, gen_images = [], []\n  current_state = states[0]\n\n  if k == -1:\n    feedself = True\n  else:\n    # Scheduled sampling:\n    # Calculate number of ground-truth frames to pass in.\n    num_ground_truth = tf.to_int32(\n        tf.round(tf.to_float(batch_size) * (k / (k + tf.exp(iter_num / k)))))\n    feedself = False\n\n  # LSTM state sizes and states.\n  lstm_size = np.int32(np.array([32, 32, 64, 64, 128, 64, 32]))\n  lstm_state1, lstm_state2, lstm_state3, lstm_state4 = None, None, None, None\n  lstm_state5, lstm_state6, lstm_state7 = None, None, None\n\n  for image, action in zip(images[:-1], actions[:-1]):\n    # Reuse variables after the first timestep.\n    reuse = bool(gen_images)\n\n    done_warm_start = len(gen_images) > context_frames - 1\n    with slim.arg_scope(\n        [lstm_func, slim.layers.conv2d, slim.layers.fully_connected,\n         tf_layers.layer_norm, slim.layers.conv2d_transpose],\n        reuse=reuse):\n\n      if feedself and done_warm_start:\n        # Feed in generated image.\n        prev_image = gen_images[-1]\n      elif done_warm_start:\n        # Scheduled sampling\n        prev_image = scheduled_sample(image, gen_images[-1], batch_size,\n                                      num_ground_truth)\n      else:\n        # Always feed in ground_truth\n        prev_image = image\n\n      # Predicted state is always fed back in\n      state_action = tf.concat(1, [action, current_state])\n\n      enc0 = slim.layers.conv2d(\n          prev_image,\n          32, [5, 5],\n          stride=2,\n          scope=\'scale1_conv1\',\n          normalizer_fn=tf_layers.layer_norm,\n          normalizer_params={\'scope\': \'layer_norm1\'})\n\n      hidden1, lstm_state1 = lstm_func(\n          enc0, lstm_state1, lstm_size[0], scope=\'state1\')\n      hidden1 = tf_layers.layer_norm(hidden1, scope=\'layer_norm2\')\n      hidden2, lstm_state2 = lstm_func(\n          hidden1, lstm_state2, lstm_size[1], scope=\'state2\')\n      hidden2 = tf_layers.layer_norm(hidden2, scope=\'layer_norm3\')\n      enc1 = slim.layers.conv2d(\n          hidden2, hidden2.get_shape()[3], [3, 3], stride=2, scope=\'conv2\')\n\n      hidden3, lstm_state3 = lstm_func(\n          enc1, lstm_state3, lstm_size[2], scope=\'state3\')\n      hidden3 = tf_layers.layer_norm(hidden3, scope=\'layer_norm4\')\n      hidden4, lstm_state4 = lstm_func(\n          hidden3, lstm_state4, lstm_size[3], scope=\'state4\')\n      hidden4 = tf_layers.layer_norm(hidden4, scope=\'layer_norm5\')\n      enc2 = slim.layers.conv2d(\n          hidden4, hidden4.get_shape()[3], [3, 3], stride=2, scope=\'conv3\')\n\n      # Pass in state and action.\n      smear = tf.reshape(\n          state_action,\n          [int(batch_size), 1, 1, int(state_action.get_shape()[1])])\n      smear = tf.tile(\n          smear, [1, int(enc2.get_shape()[1]), int(enc2.get_shape()[2]), 1])\n      if use_state:\n        enc2 = tf.concat(3, [enc2, smear])\n      enc3 = slim.layers.conv2d(\n          enc2, hidden4.get_shape()[3], [1, 1], stride=1, scope=\'conv4\')\n\n      hidden5, lstm_state5 = lstm_func(\n          enc3, lstm_state5, lstm_size[4], scope=\'state5\')  # last 8x8\n      hidden5 = tf_layers.layer_norm(hidden5, scope=\'layer_norm6\')\n      enc4 = slim.layers.conv2d_transpose(\n          hidden5, hidden5.get_shape()[3], 3, stride=2, scope=\'convt1\')\n\n      hidden6, lstm_state6 = lstm_func(\n          enc4, lstm_state6, lstm_size[5], scope=\'state6\')  # 16x16\n      hidden6 = tf_layers.layer_norm(hidden6, scope=\'layer_norm7\')\n      # Skip connection.\n      hidden6 = tf.concat(3, [hidden6, enc1])  # both 16x16\n\n      enc5 = slim.layers.conv2d_transpose(\n          hidden6, hidden6.get_shape()[3], 3, stride=2, scope=\'convt2\')\n      hidden7, lstm_state7 = lstm_func(\n          enc5, lstm_state7, lstm_size[6], scope=\'state7\')  # 32x32\n      hidden7 = tf_layers.layer_norm(hidden7, scope=\'layer_norm8\')\n\n      # Skip connection.\n      hidden7 = tf.concat(3, [hidden7, enc0])  # both 32x32\n\n      enc6 = slim.layers.conv2d_transpose(\n          hidden7,\n          hidden7.get_shape()[3], 3, stride=2, scope=\'convt3\',\n          normalizer_fn=tf_layers.layer_norm,\n          normalizer_params={\'scope\': \'layer_norm9\'})\n\n      if dna:\n        # Using largest hidden state for predicting untied conv kernels.\n        enc7 = slim.layers.conv2d_transpose(\n            enc6, DNA_KERN_SIZE**2, 1, stride=1, scope=\'convt4\')\n      else:\n        # Using largest hidden state for predicting a new image layer.\n        enc7 = slim.layers.conv2d_transpose(\n            enc6, color_channels, 1, stride=1, scope=\'convt4\')\n        # This allows the network to also generate one image from scratch,\n        # which is useful when regions of the image become unoccluded.\n        transformed = [tf.nn.sigmoid(enc7)]\n\n      if stp:\n        stp_input0 = tf.reshape(hidden5, [int(batch_size), -1])\n        stp_input1 = slim.layers.fully_connected(\n            stp_input0, 100, scope=\'fc_stp\')\n        transformed += stp_transformation(prev_image, stp_input1, num_masks)\n      elif cdna:\n        cdna_input = tf.reshape(hidden5, [int(batch_size), -1])\n        transformed += cdna_transformation(prev_image, cdna_input, num_masks,\n                                           int(color_channels))\n      elif dna:\n        # Only one mask is supported (more should be unnecessary).\n        if num_masks != 1:\n          raise ValueError(\'Only one mask is supported for DNA model.\')\n        transformed = [dna_transformation(prev_image, enc7)]\n\n      masks = slim.layers.conv2d_transpose(\n          enc6, num_masks + 1, 1, stride=1, scope=\'convt7\')\n      masks = tf.reshape(\n          tf.nn.softmax(tf.reshape(masks, [-1, num_masks + 1])),\n          [int(batch_size), int(img_height), int(img_width), num_masks + 1])\n      mask_list = tf.split(3, num_masks + 1, masks)\n      output = mask_list[0] * prev_image\n      for layer, mask in zip(transformed, mask_list[1:]):\n        output += layer * mask\n      gen_images.append(output)\n\n      current_state = slim.layers.fully_connected(\n          state_action,\n          int(current_state.get_shape()[1]),\n          scope=\'state_pred\',\n          activation_fn=None)\n      gen_states.append(current_state)\n\n  return gen_images, gen_states\n\n\n## Utility functions\ndef stp_transformation(prev_image, stp_input, num_masks):\n  """"""Apply spatial transformer predictor (STP) to previous image.\n\n  Args:\n    prev_image: previous image to be transformed.\n    stp_input: hidden layer to be used for computing STN parameters.\n    num_masks: number of masks and hence the number of STP transformations.\n  Returns:\n    List of images transformed by the predicted STP parameters.\n  """"""\n  # Only import spatial transformer if needed.\n  from spatial_transformer import transformer\n\n  identity_params = tf.convert_to_tensor(\n      np.array([1.0, 0.0, 0.0, 0.0, 1.0, 0.0], np.float32))\n  transformed = []\n  for i in range(num_masks - 1):\n    params = slim.layers.fully_connected(\n        stp_input, 6, scope=\'stp_params\' + str(i),\n        activation_fn=None) + identity_params\n    transformed.append(transformer(prev_image, params))\n\n  return transformed\n\n\ndef cdna_transformation(prev_image, cdna_input, num_masks, color_channels):\n  """"""Apply convolutional dynamic neural advection to previous image.\n\n  Args:\n    prev_image: previous image to be transformed.\n    cdna_input: hidden lyaer to be used for computing CDNA kernels.\n    num_masks: the number of masks and hence the number of CDNA transformations.\n    color_channels: the number of color channels in the images.\n  Returns:\n    List of images transformed by the predicted CDNA kernels.\n  """"""\n  batch_size = int(cdna_input.get_shape()[0])\n\n  # Predict kernels using linear function of last hidden layer.\n  cdna_kerns = slim.layers.fully_connected(\n      cdna_input,\n      DNA_KERN_SIZE * DNA_KERN_SIZE * num_masks,\n      scope=\'cdna_params\',\n      activation_fn=None)\n\n  # Reshape and normalize.\n  cdna_kerns = tf.reshape(\n      cdna_kerns, [batch_size, DNA_KERN_SIZE, DNA_KERN_SIZE, 1, num_masks])\n  cdna_kerns = tf.nn.relu(cdna_kerns - RELU_SHIFT) + RELU_SHIFT\n  norm_factor = tf.reduce_sum(cdna_kerns, [1, 2, 3], keep_dims=True)\n  cdna_kerns /= norm_factor\n\n  cdna_kerns = tf.tile(cdna_kerns, [1, 1, 1, color_channels, 1])\n  cdna_kerns = tf.split(0, batch_size, cdna_kerns)\n  prev_images = tf.split(0, batch_size, prev_image)\n\n  # Transform image.\n  transformed = []\n  for kernel, preimg in zip(cdna_kerns, prev_images):\n    kernel = tf.squeeze(kernel)\n    if len(kernel.get_shape()) == 3:\n      kernel = tf.expand_dims(kernel, -1)\n    transformed.append(\n        tf.nn.depthwise_conv2d(preimg, kernel, [1, 1, 1, 1], \'SAME\'))\n  transformed = tf.concat(0, transformed)\n  transformed = tf.split(3, num_masks, transformed)\n  return transformed\n\n\ndef dna_transformation(prev_image, dna_input):\n  """"""Apply dynamic neural advection to previous image.\n\n  Args:\n    prev_image: previous image to be transformed.\n    dna_input: hidden lyaer to be used for computing DNA transformation.\n  Returns:\n    List of images transformed by the predicted CDNA kernels.\n  """"""\n  # Construct translated images.\n  prev_image_pad = tf.pad(prev_image, [[0, 0], [2, 2], [2, 2], [0, 0]])\n  image_height = int(prev_image.get_shape()[1])\n  image_width = int(prev_image.get_shape()[2])\n\n  inputs = []\n  for xkern in range(DNA_KERN_SIZE):\n    for ykern in range(DNA_KERN_SIZE):\n      inputs.append(\n          tf.expand_dims(\n              tf.slice(prev_image_pad, [0, xkern, ykern, 0],\n                       [-1, image_height, image_width, -1]), [3]))\n  inputs = tf.concat(3, inputs)\n\n  # Normalize channels to 1.\n  kernel = tf.nn.relu(dna_input - RELU_SHIFT) + RELU_SHIFT\n  kernel = tf.expand_dims(\n      kernel / tf.reduce_sum(\n          kernel, [3], keep_dims=True), [4])\n  return tf.reduce_sum(kernel * inputs, [3], keep_dims=False)\n\n\ndef scheduled_sample(ground_truth_x, generated_x, batch_size, num_ground_truth):\n  """"""Sample batch with specified mix of ground truth and generated data points.\n\n  Args:\n    ground_truth_x: tensor of ground-truth data points.\n    generated_x: tensor of generated data points.\n    batch_size: batch size\n    num_ground_truth: number of ground-truth examples to include in batch.\n  Returns:\n    New batch with num_ground_truth sampled from ground_truth_x and the rest\n    from generated_x.\n  """"""\n  idx = tf.random_shuffle(tf.range(int(batch_size)))\n  ground_truth_idx = tf.gather(idx, tf.range(num_ground_truth))\n  generated_idx = tf.gather(idx, tf.range(num_ground_truth, int(batch_size)))\n\n  ground_truth_examps = tf.gather(ground_truth_x, ground_truth_idx)\n  generated_examps = tf.gather(generated_x, generated_idx)\n  return tf.dynamic_stitch([ground_truth_idx, generated_idx],\n                           [ground_truth_examps, generated_examps])\n'"
model_zoo/models/video_prediction/prediction_train.py,0,"b'# Copyright 2016 The TensorFlow Authors All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Code for training the prediction model.""""""\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import app\nfrom tensorflow.python.platform import flags\n\nfrom prediction_input import build_tfrecord_input\nfrom prediction_model import construct_model\n\n# How often to record tensorboard summaries.\nSUMMARY_INTERVAL = 40\n\n# How often to run a batch through the validation model.\nVAL_INTERVAL = 200\n\n# How often to save a model checkpoint\nSAVE_INTERVAL = 2000\n\n# tf record data location:\nDATA_DIR = \'push/push_train\'\n\n# local output directory\nOUT_DIR = \'/tmp/data\'\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'data_dir\', DATA_DIR, \'directory containing data.\')\nflags.DEFINE_string(\'output_dir\', OUT_DIR, \'directory for model checkpoints.\')\nflags.DEFINE_string(\'event_log_dir\', OUT_DIR, \'directory for writing summary.\')\nflags.DEFINE_integer(\'num_iterations\', 100000, \'number of training iterations.\')\nflags.DEFINE_string(\'pretrained_model\', \'\',\n                    \'filepath of a pretrained model to initialize from.\')\n\nflags.DEFINE_integer(\'sequence_length\', 10,\n                     \'sequence length, including context frames.\')\nflags.DEFINE_integer(\'context_frames\', 2, \'# of frames before predictions.\')\nflags.DEFINE_integer(\'use_state\', 1,\n                     \'Whether or not to give the state+action to the model\')\n\nflags.DEFINE_string(\'model\', \'CDNA\',\n                    \'model architecture to use - CDNA, DNA, or STP\')\n\nflags.DEFINE_integer(\'num_masks\', 10,\n                     \'number of masks, usually 1 for DNA, 10 for CDNA, STN.\')\nflags.DEFINE_float(\'schedsamp_k\', 900.0,\n                   \'The k hyperparameter for scheduled sampling,\'\n                   \'-1 for no scheduled sampling.\')\nflags.DEFINE_float(\'train_val_split\', 0.95,\n                   \'The percentage of files to use for the training set,\'\n                   \' vs. the validation set.\')\n\nflags.DEFINE_integer(\'batch_size\', 32, \'batch size for training\')\nflags.DEFINE_float(\'learning_rate\', 0.001,\n                   \'the base learning rate of the generator\')\n\n\n## Helper functions\ndef peak_signal_to_noise_ratio(true, pred):\n  """"""Image quality metric based on maximal signal power vs. power of the noise.\n\n  Args:\n    true: the ground truth image.\n    pred: the predicted image.\n  Returns:\n    peak signal to noise ratio (PSNR)\n  """"""\n  return 10.0 * tf.log(1.0 / mean_squared_error(true, pred)) / tf.log(10.0)\n\n\ndef mean_squared_error(true, pred):\n  """"""L2 distance between tensors true and pred.\n\n  Args:\n    true: the ground truth image.\n    pred: the predicted image.\n  Returns:\n    mean squared error between ground truth and predicted image.\n  """"""\n  return tf.reduce_sum(tf.square(true - pred)) / tf.to_float(tf.size(pred))\n\n\nclass Model(object):\n\n  def __init__(self,\n               images=None,\n               actions=None,\n               states=None,\n               sequence_length=None,\n               reuse_scope=None):\n\n    if sequence_length is None:\n      sequence_length = FLAGS.sequence_length\n\n    self.prefix = prefix = tf.placeholder(tf.string, [])\n    self.iter_num = tf.placeholder(tf.float32, [])\n    summaries = []\n\n    # Split into timesteps.\n    actions = tf.split(1, actions.get_shape()[1], actions)\n    actions = [tf.squeeze(act) for act in actions]\n    states = tf.split(1, states.get_shape()[1], states)\n    states = [tf.squeeze(st) for st in states]\n    images = tf.split(1, images.get_shape()[1], images)\n    images = [tf.squeeze(img) for img in images]\n\n    if reuse_scope is None:\n      gen_images, gen_states = construct_model(\n          images,\n          actions,\n          states,\n          iter_num=self.iter_num,\n          k=FLAGS.schedsamp_k,\n          use_state=FLAGS.use_state,\n          num_masks=FLAGS.num_masks,\n          cdna=FLAGS.model == \'CDNA\',\n          dna=FLAGS.model == \'DNA\',\n          stp=FLAGS.model == \'STP\',\n          context_frames=FLAGS.context_frames)\n    else:  # If it\'s a validation or test model.\n      with tf.variable_scope(reuse_scope, reuse=True):\n        gen_images, gen_states = construct_model(\n            images,\n            actions,\n            states,\n            iter_num=self.iter_num,\n            k=FLAGS.schedsamp_k,\n            use_state=FLAGS.use_state,\n            num_masks=FLAGS.num_masks,\n            cdna=FLAGS.model == \'CDNA\',\n            dna=FLAGS.model == \'DNA\',\n            stp=FLAGS.model == \'STP\',\n            context_frames=FLAGS.context_frames)\n\n    # L2 loss, PSNR for eval.\n    loss, psnr_all = 0.0, 0.0\n    for i, x, gx in zip(\n        range(len(gen_images)), images[FLAGS.context_frames:],\n        gen_images[FLAGS.context_frames - 1:]):\n      recon_cost = mean_squared_error(x, gx)\n      psnr_i = peak_signal_to_noise_ratio(x, gx)\n      psnr_all += psnr_i\n      summaries.append(\n          tf.scalar_summary(prefix + \'_recon_cost\' + str(i), recon_cost))\n      summaries.append(tf.scalar_summary(prefix + \'_psnr\' + str(i), psnr_i))\n      loss += recon_cost\n\n    for i, state, gen_state in zip(\n        range(len(gen_states)), states[FLAGS.context_frames:],\n        gen_states[FLAGS.context_frames - 1:]):\n      state_cost = mean_squared_error(state, gen_state) * 1e-4\n      summaries.append(\n          tf.scalar_summary(prefix + \'_state_cost\' + str(i), state_cost))\n      loss += state_cost\n    summaries.append(tf.scalar_summary(prefix + \'_psnr_all\', psnr_all))\n    self.psnr_all = psnr_all\n\n    self.loss = loss = loss / np.float32(len(images) - FLAGS.context_frames)\n\n    summaries.append(tf.scalar_summary(prefix + \'_loss\', loss))\n\n    self.lr = tf.placeholder_with_default(FLAGS.learning_rate, ())\n\n    self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n    self.summ_op = tf.merge_summary(summaries)\n\n\ndef main(unused_argv):\n\n  print \'Constructing models and inputs.\'\n  with tf.variable_scope(\'model\', reuse=None) as training_scope:\n    images, actions, states = build_tfrecord_input(training=True)\n    model = Model(images, actions, states, FLAGS.sequence_length)\n\n  with tf.variable_scope(\'val_model\', reuse=None):\n    val_images, val_actions, val_states = build_tfrecord_input(training=False)\n    val_model = Model(val_images, val_actions, val_states,\n                      FLAGS.sequence_length, training_scope)\n\n  print \'Constructing saver.\'\n  # Make saver.\n  saver = tf.train.Saver(\n      tf.get_collection(tf.GraphKeys.VARIABLES), max_to_keep=0)\n\n  # Make training session.\n  sess = tf.InteractiveSession()\n  summary_writer = tf.train.SummaryWriter(\n      FLAGS.event_log_dir, graph=sess.graph, flush_secs=10)\n\n  if FLAGS.pretrained_model:\n    saver.restore(sess, FLAGS.pretrained_model)\n\n  tf.train.start_queue_runners(sess)\n  sess.run(tf.initialize_all_variables())\n\n  tf.logging.info(\'iteration number, cost\')\n\n  # Run training.\n  for itr in range(FLAGS.num_iterations):\n    # Generate new batch of data.\n    feed_dict = {model.prefix: \'train\',\n                 model.iter_num: np.float32(itr),\n                 model.lr: FLAGS.learning_rate}\n    cost, _, summary_str = sess.run([model.loss, model.train_op, model.summ_op],\n                                    feed_dict)\n\n    # Print info: iteration #, cost.\n    tf.logging.info(str(itr) + \' \' + str(cost))\n\n    if (itr) % VAL_INTERVAL == 2:\n      # Run through validation set.\n      feed_dict = {val_model.lr: 0.0,\n                   val_model.prefix: \'val\',\n                   val_model.iter_num: np.float32(itr)}\n      _, val_summary_str = sess.run([val_model.train_op, val_model.summ_op],\n                                     feed_dict)\n      summary_writer.add_summary(val_summary_str, itr)\n\n    if (itr) % SAVE_INTERVAL == 2:\n      tf.logging.info(\'Saving model.\')\n      saver.save(sess, FLAGS.output_dir + \'/model\' + str(itr))\n\n    if (itr) % SUMMARY_INTERVAL:\n      summary_writer.add_summary(summary_str, itr)\n\n  tf.logging.info(\'Saving model.\')\n  saver.save(sess, FLAGS.output_dir + \'/model\')\n  tf.logging.info(\'Training complete\')\n  tf.logging.flush()\n\n\nif __name__ == \'__main__\':\n  app.run()\n'"
model_zoo/models/autoencoder/autoencoder_models/Autoencoder.py,0,"b'import tensorflow as tf\nimport numpy as np\nimport autoencoder.Utils\n\nclass Autoencoder(object):\n\n    def __init__(self, n_input, n_hidden, transfer_function=tf.nn.softplus, optimizer = tf.train.AdamOptimizer()):\n        self.n_input = n_input\n        self.n_hidden = n_hidden\n        self.transfer = transfer_function\n\n        network_weights = self._initialize_weights()\n        self.weights = network_weights\n\n        # model\n        self.x = tf.placeholder(tf.float32, [None, self.n_input])\n        self.hidden = self.transfer(tf.add(tf.matmul(self.x, self.weights[\'w1\']), self.weights[\'b1\']))\n        self.reconstruction = tf.add(tf.matmul(self.hidden, self.weights[\'w2\']), self.weights[\'b2\'])\n\n        # cost\n        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.sub(self.reconstruction, self.x), 2.0))\n        self.optimizer = optimizer.minimize(self.cost)\n\n        init = tf.initialize_all_variables()\n        self.sess = tf.Session()\n        self.sess.run(init)\n\n\n    def _initialize_weights(self):\n        all_weights = dict()\n        all_weights[\'w1\'] = tf.Variable(autoencoder.Utils.xavier_init(self.n_input, self.n_hidden))\n        all_weights[\'b1\'] = tf.Variable(tf.zeros([self.n_hidden], dtype=tf.float32))\n        all_weights[\'w2\'] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype=tf.float32))\n        all_weights[\'b2\'] = tf.Variable(tf.zeros([self.n_input], dtype=tf.float32))\n        return all_weights\n\n    def partial_fit(self, X):\n        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict={self.x: X})\n        return cost\n\n    def calc_total_cost(self, X):\n        return self.sess.run(self.cost, feed_dict = {self.x: X})\n\n    def transform(self, X):\n        return self.sess.run(self.hidden, feed_dict={self.x: X})\n\n    def generate(self, hidden = None):\n        if hidden is None:\n            hidden = np.random.normal(size=self.weights[""b1""])\n        return self.sess.run(self.reconstruction, feed_dict={self.hidden: hidden})\n\n    def reconstruct(self, X):\n        return self.sess.run(self.reconstruction, feed_dict={self.x: X})\n\n    def getWeights(self):\n        return self.sess.run(self.weights[\'w1\'])\n\n    def getBiases(self):\n        return self.sess.run(self.weights[\'b1\'])\n\n'"
model_zoo/models/autoencoder/autoencoder_models/DenoisingAutoencoder.py,0,"b'import tensorflow as tf\nimport numpy as np\nimport autoencoder.Utils\n\n\nclass AdditiveGaussianNoiseAutoencoder(object):\n    def __init__(self, n_input, n_hidden, transfer_function = tf.nn.softplus, optimizer = tf.train.AdamOptimizer(),\n                 scale = 0.1):\n        self.n_input = n_input\n        self.n_hidden = n_hidden\n        self.transfer = transfer_function\n        self.scale = tf.placeholder(tf.float32)\n        self.training_scale = scale\n        network_weights = self._initialize_weights()\n        self.weights = network_weights\n\n        # model\n        self.x = tf.placeholder(tf.float32, [None, self.n_input])\n        self.hidden = self.transfer(tf.add(tf.matmul(self.x + scale * tf.random_normal((n_input,)),\n                self.weights[\'w1\']),\n                self.weights[\'b1\']))\n        self.reconstruction = tf.add(tf.matmul(self.hidden, self.weights[\'w2\']), self.weights[\'b2\'])\n\n        # cost\n        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.sub(self.reconstruction, self.x), 2.0))\n        self.optimizer = optimizer.minimize(self.cost)\n\n        init = tf.initialize_all_variables()\n        self.sess = tf.Session()\n        self.sess.run(init)\n\n    def _initialize_weights(self):\n        all_weights = dict()\n        all_weights[\'w1\'] = tf.Variable(autoencoder.Utils.xavier_init(self.n_input, self.n_hidden))\n        all_weights[\'b1\'] = tf.Variable(tf.zeros([self.n_hidden], dtype = tf.float32))\n        all_weights[\'w2\'] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype = tf.float32))\n        all_weights[\'b2\'] = tf.Variable(tf.zeros([self.n_input], dtype = tf.float32))\n        return all_weights\n\n    def partial_fit(self, X):\n        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict = {self.x: X,\n                                                                            self.scale: self.training_scale\n                                                                            })\n        return cost\n\n    def calc_total_cost(self, X):\n        return self.sess.run(self.cost, feed_dict = {self.x: X,\n                                                     self.scale: self.training_scale\n                                                     })\n\n    def transform(self, X):\n        return self.sess.run(self.hidden, feed_dict = {self.x: X,\n                                                       self.scale: self.training_scale\n                                                       })\n\n    def generate(self, hidden = None):\n        if hidden is None:\n            hidden = np.random.normal(size = self.weights[""b1""])\n        return self.sess.run(self.reconstruction, feed_dict = {self.hidden: hidden})\n\n    def reconstruct(self, X):\n        return self.sess.run(self.reconstruction, feed_dict = {self.x: X,\n                                                               self.scale: self.training_scale\n                                                               })\n\n    def getWeights(self):\n        return self.sess.run(self.weights[\'w1\'])\n\n    def getBiases(self):\n        return self.sess.run(self.weights[\'b1\'])\n\n\nclass MaskingNoiseAutoencoder(object):\n    def __init__(self, n_input, n_hidden, transfer_function = tf.nn.softplus, optimizer = tf.train.AdamOptimizer(),\n                 dropout_probability = 0.95):\n        self.n_input = n_input\n        self.n_hidden = n_hidden\n        self.transfer = transfer_function\n        self.dropout_probability = dropout_probability\n        self.keep_prob = tf.placeholder(tf.float32)\n\n        network_weights = self._initialize_weights()\n        self.weights = network_weights\n\n        # model\n        self.x = tf.placeholder(tf.float32, [None, self.n_input])\n        self.hidden = self.transfer(tf.add(tf.matmul(tf.nn.dropout(self.x, self.keep_prob), self.weights[\'w1\']),\n                                           self.weights[\'b1\']))\n        self.reconstruction = tf.add(tf.matmul(self.hidden, self.weights[\'w2\']), self.weights[\'b2\'])\n\n        # cost\n        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.sub(self.reconstruction, self.x), 2.0))\n        self.optimizer = optimizer.minimize(self.cost)\n\n        init = tf.initialize_all_variables()\n        self.sess = tf.Session()\n        self.sess.run(init)\n\n    def _initialize_weights(self):\n        all_weights = dict()\n        all_weights[\'w1\'] = tf.Variable(autoencoder.Utils.xavier_init(self.n_input, self.n_hidden))\n        all_weights[\'b1\'] = tf.Variable(tf.zeros([self.n_hidden], dtype = tf.float32))\n        all_weights[\'w2\'] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype = tf.float32))\n        all_weights[\'b2\'] = tf.Variable(tf.zeros([self.n_input], dtype = tf.float32))\n        return all_weights\n\n    def partial_fit(self, X):\n        cost, opt = self.sess.run((self.cost, self.optimizer),\n                                  feed_dict = {self.x: X, self.keep_prob: self.dropout_probability})\n        return cost\n\n    def calc_total_cost(self, X):\n        return self.sess.run(self.cost, feed_dict = {self.x: X, self.keep_prob: 1.0})\n\n    def transform(self, X):\n        return self.sess.run(self.hidden, feed_dict = {self.x: X, self.keep_prob: 1.0})\n\n    def generate(self, hidden = None):\n        if hidden is None:\n            hidden = np.random.normal(size = self.weights[""b1""])\n        return self.sess.run(self.reconstruction, feed_dict = {self.hidden: hidden})\n\n    def reconstruct(self, X):\n        return self.sess.run(self.reconstruction, feed_dict = {self.x: X, self.keep_prob: 1.0})\n\n    def getWeights(self):\n        return self.sess.run(self.weights[\'w1\'])\n\n    def getBiases(self):\n        return self.sess.run(self.weights[\'b1\'])\n'"
model_zoo/models/autoencoder/autoencoder_models/VariationalAutoencoder.py,0,"b'import tensorflow as tf\nimport numpy as np\nimport autoencoder.Utils\n\nclass VariationalAutoencoder(object):\n\n    def __init__(self, n_input, n_hidden, optimizer = tf.train.AdamOptimizer()):\n        self.n_input = n_input\n        self.n_hidden = n_hidden\n\n        network_weights = self._initialize_weights()\n        self.weights = network_weights\n\n        # model\n        self.x = tf.placeholder(tf.float32, [None, self.n_input])\n        self.z_mean = tf.add(tf.matmul(self.x, self.weights[\'w1\']), self.weights[\'b1\'])\n        self.z_log_sigma_sq = tf.add(tf.matmul(self.x, self.weights[\'log_sigma_w1\']), self.weights[\'log_sigma_b1\'])\n\n        # sample from gaussian distribution\n        eps = tf.random_normal(tf.pack([tf.shape(self.x)[0], self.n_hidden]), 0, 1, dtype = tf.float32)\n        self.z = tf.add(self.z_mean, tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n\n        self.reconstruction = tf.add(tf.matmul(self.z, self.weights[\'w2\']), self.weights[\'b2\'])\n\n        # cost\n        reconstr_loss = 0.5 * tf.reduce_sum(tf.pow(tf.sub(self.reconstruction, self.x), 2.0))\n        latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq\n                                           - tf.square(self.z_mean)\n                                           - tf.exp(self.z_log_sigma_sq), 1)\n        self.cost = tf.reduce_mean(reconstr_loss + latent_loss)\n        self.optimizer = optimizer.minimize(self.cost)\n\n        init = tf.initialize_all_variables()\n        self.sess = tf.Session()\n        self.sess.run(init)\n\n    def _initialize_weights(self):\n        all_weights = dict()\n        all_weights[\'w1\'] = tf.Variable(autoencoder.Utils.xavier_init(self.n_input, self.n_hidden))\n        all_weights[\'log_sigma_w1\'] = tf.Variable(autoencoder.Utils.xavier_init(self.n_input, self.n_hidden))\n        all_weights[\'b1\'] = tf.Variable(tf.zeros([self.n_hidden], dtype=tf.float32))\n        all_weights[\'log_sigma_b1\'] = tf.Variable(tf.zeros([self.n_hidden], dtype=tf.float32))\n        all_weights[\'w2\'] = tf.Variable(tf.zeros([self.n_hidden, self.n_input], dtype=tf.float32))\n        all_weights[\'b2\'] = tf.Variable(tf.zeros([self.n_input], dtype=tf.float32))\n        return all_weights\n\n    def partial_fit(self, X):\n        cost, opt = self.sess.run((self.cost, self.optimizer), feed_dict={self.x: X})\n        return cost\n\n    def calc_total_cost(self, X):\n        return self.sess.run(self.cost, feed_dict = {self.x: X})\n\n    def transform(self, X):\n        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n\n    def generate(self, hidden = None):\n        if hidden is None:\n            hidden = np.random.normal(size=self.weights[""b1""])\n        return self.sess.run(self.reconstruction, feed_dict={self.z_mean: hidden})\n\n    def reconstruct(self, X):\n        return self.sess.run(self.reconstruction, feed_dict={self.x: X})\n\n    def getWeights(self):\n        return self.sess.run(self.weights[\'w1\'])\n\n    def getBiases(self):\n        return self.sess.run(self.weights[\'b1\'])\n\n'"
model_zoo/models/autoencoder/autoencoder_models/__init__.py,0,b''
model_zoo/models/differential_privacy/multiple_teachers/aggregation.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\ndef labels_from_probs(probs):\n  """"""\n  Helper function: computes argmax along last dimension of array to obtain\n  labels (max prob or max logit value)\n  :param probs: numpy array where probabilities or logits are on last dimension\n  :return: array with same shape as input besides last dimension with shape 1\n          now containing the labels\n  """"""\n  # Compute last axis index\n  last_axis = len(np.shape(probs)) - 1\n\n  # Label is argmax over last dimension\n  labels = np.argmax(probs, axis=last_axis)\n\n  # Return as np.int32\n  return np.asarray(labels, dtype=np.int32)\n\n\ndef noisy_max(logits, lap_scale, return_clean_votes=False):\n  """"""\n  This aggregation mechanism takes the softmax/logit output of several models\n  resulting from inference on identical inputs and computes the noisy-max of\n  the votes for candidate classes to select a label for each sample: it\n  adds Laplacian noise to label counts and returns the most frequent label.\n  :param logits: logits or probabilities for each sample\n  :param lap_scale: scale of the Laplacian noise to be added to counts\n  :param return_clean_votes: if set to True, also returns clean votes (without\n                      Laplacian noise). This can be used to perform the\n                      privacy analysis of this aggregation mechanism.\n  :return: pair of result and (if clean_votes is set to True) the clean counts\n           for each class per sample and the the original labels produced by\n           the teachers.\n  """"""\n\n  # Compute labels from logits/probs and reshape array properly\n  labels = labels_from_probs(logits)\n  labels_shape = np.shape(labels)\n  labels = labels.reshape((labels_shape[0], labels_shape[1]))\n\n  # Initialize array to hold final labels\n  result = np.zeros(int(labels_shape[1]))\n\n  if return_clean_votes:\n    # Initialize array to hold clean votes for each sample\n    clean_votes = np.zeros((int(labels_shape[1]), 10))\n\n  # Parse each sample\n  for i in xrange(int(labels_shape[1])):\n    # Count number of votes assigned to each class\n    label_counts = np.bincount(labels[:, i], minlength=10)\n\n    if return_clean_votes:\n      # Store vote counts for export\n      clean_votes[i] = label_counts\n\n    # Cast in float32 to prepare before addition of Laplacian noise\n    label_counts = np.asarray(label_counts, dtype=np.float32)\n\n    # Sample independent Laplacian noise for each class\n    for item in xrange(10):\n      label_counts[item] += np.random.laplace(loc=0.0, scale=float(lap_scale))\n\n    # Result is the most frequent label\n    result[i] = np.argmax(label_counts)\n\n  # Cast labels to np.int32 for compatibility with deep_cnn.py feed dictionaries\n  result = np.asarray(result, dtype=np.int32)\n\n  if return_clean_votes:\n    # Returns several array, which are later saved:\n    # result: labels obtained from the noisy aggregation\n    # clean_votes: the number of teacher votes assigned to each sample and class\n    # labels: the labels assigned by teachers (before the noisy aggregation)\n    return result, clean_votes, labels\n  else:\n    # Only return labels resulting from noisy aggregation\n    return result\n\n\ndef aggregation_most_frequent(logits):\n  """"""\n  This aggregation mechanism takes the softmax/logit output of several models\n  resulting from inference on identical inputs and computes the most frequent\n  label. It is deterministic (no noise injection like noisy_max() above.\n  :param logits: logits or probabilities for each sample\n  :return:\n  """"""\n  # Compute labels from logits/probs and reshape array properly\n  labels = labels_from_probs(logits)\n  labels_shape = np.shape(labels)\n  labels = labels.reshape((labels_shape[0], labels_shape[1]))\n\n  # Initialize array to hold final labels\n  result = np.zeros(int(labels_shape[1]))\n\n  # Parse each sample\n  for i in xrange(int(labels_shape[1])):\n    # Count number of votes assigned to each class\n    label_counts = np.bincount(labels[:, i], minlength=10)\n\n    label_counts = np.asarray(label_counts, dtype=np.int32)\n\n    # Result is the most frequent label\n    result[i] = np.argmax(label_counts)\n\n  return np.asarray(result, dtype=np.int32)\n\n\n'"
model_zoo/models/differential_privacy/multiple_teachers/analysis.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nThis script computes bounds on the privacy cost of training the\nstudent model from noisy aggregation of labels predicted by teachers.\nIt should be used only after training the student (and therefore the\nteachers as well). We however include the label files required to\nreproduce key results from our paper (https://arxiv.org/abs/1610.05755):\nthe epsilon bounds for MNIST and SVHN students.\n\nThe command that computes the epsilon bound associated\nwith the training of the MNIST student model (100 label queries\nwith a (1/20)*2=0.1 epsilon bound each) is:\n\npython analysis.py\n  --counts_file=mnist_250_teachers_labels.npy\n  --indices_file=mnist_250_teachers_100_indices_used_by_student.npy\n\nThe command that computes the epsilon bound associated\nwith the training of the SVHN student model (1000 label queries\nwith a (1/20)*2=0.1 epsilon bound each) is:\n\npython analysis.py\n  --counts_file=svhn_250_teachers_labels.npy\n  --max_examples=1000\n  --delta=1e-6\n""""""\nimport os\nimport math\nimport numpy as np\nimport tensorflow as tf\n\nfrom differential_privacy.multiple_teachers.input import maybe_download\n\n# These parameters can be changed to compute bounds for different failure rates\n# or different model predictions.\n\ntf.flags.DEFINE_integer(""moments"",8, ""Number of moments"")\ntf.flags.DEFINE_float(""noise_eps"", 0.1, ""Eps value for each call to noisymax."")\ntf.flags.DEFINE_float(""delta"", 1e-5, ""Target value of delta."")\ntf.flags.DEFINE_float(""beta"", 0.09, ""Value of beta for smooth sensitivity"")\ntf.flags.DEFINE_string(""counts_file"","""",""Numpy matrix with raw counts"")\ntf.flags.DEFINE_string(""indices_file"","""",\n    ""File containting a numpy matrix with indices used.""\n    ""Optional. Use the first max_examples indices if this is not provided."")\ntf.flags.DEFINE_integer(""max_examples"",1000,\n    ""Number of examples to use. We will use the first""\n    "" max_examples many examples from the counts_file""\n    "" or indices_file to do the privacy cost estimate"")\ntf.flags.DEFINE_float(""too_small"", 1e-10, ""Small threshold to avoid log of 0"")\ntf.flags.DEFINE_bool(""input_is_counts"", False, ""False if labels, True if counts"")\n\nFLAGS = tf.flags.FLAGS\n\n\ndef compute_q_noisy_max(counts, noise_eps):\n  """"""returns ~ Pr[outcome != winner].\n\n  Args:\n    counts: a list of scores\n    noise_eps: privacy parameter for noisy_max\n  Returns:\n    q: the probability that outcome is different from true winner.\n  """"""\n  # For noisy max, we only get an upper bound.\n  # Pr[ j beats i*] \\leq (2+gap(j,i*))/ 4 exp(gap(j,i*)\n  # proof at http://mathoverflow.net/questions/66763/\n  # tight-bounds-on-probability-of-sum-of-laplace-random-variables\n\n  winner = np.argmax(counts)\n  counts_normalized = noise_eps * (counts - counts[winner])\n  counts_rest = np.array(\n      [counts_normalized[i] for i in xrange(len(counts)) if i != winner])\n  q = 0.0\n  for c in counts_rest:\n    gap = -c\n    q += (gap + 2.0) / (4.0 * math.exp(gap))\n  return min(q, 1.0 - (1.0/len(counts)))\n\n\ndef compute_q_noisy_max_approx(counts, noise_eps):\n  """"""returns ~ Pr[outcome != winner].\n\n  Args:\n    counts: a list of scores\n    noise_eps: privacy parameter for noisy_max\n  Returns:\n    q: the probability that outcome is different from true winner.\n  """"""\n  # For noisy max, we only get an upper bound.\n  # Pr[ j beats i*] \\leq (2+gap(j,i*))/ 4 exp(gap(j,i*)\n  # proof at http://mathoverflow.net/questions/66763/\n  # tight-bounds-on-probability-of-sum-of-laplace-random-variables\n  # This code uses an approximation that is faster and easier\n  # to get local sensitivity bound on.\n\n  winner = np.argmax(counts)\n  counts_normalized = noise_eps * (counts - counts[winner])\n  counts_rest = np.array(\n      [counts_normalized[i] for i in xrange(len(counts)) if i != winner])\n  gap = -max(counts_rest)\n  q = (len(counts) - 1) * (gap + 2.0) / (4.0 * math.exp(gap))\n  return min(q, 1.0 - (1.0/len(counts)))\n\n\ndef logmgf_exact(q, priv_eps, l):\n  """"""Computes the logmgf value given q and privacy eps.\n\n  The bound used is the min of three terms. The first term is from\n  https://arxiv.org/pdf/1605.02065.pdf.\n  The second term is based on the fact that when event has probability (1-q) for\n  q close to zero, q can only change by exp(eps), which corresponds to a\n  much smaller multiplicative change in (1-q)\n  The third term comes directly from the privacy guarantee.\n  Args:\n    q: pr of non-optimal outcome\n    priv_eps: eps parameter for DP\n    l: moment to compute.\n  Returns:\n    Upper bound on logmgf\n  """"""\n  if q < 0.5:\n    t_one = (1-q) * math.pow((1-q) / (1 - math.exp(priv_eps) * q), l)\n    t_two = q * math.exp(priv_eps * l)\n    t = t_one + t_two\n    try:\n      log_t = math.log(t)\n    except ValueError:\n      print ""Got ValueError in math.log for values :"" + str((q, priv_eps, l, t))\n      log_t = priv_eps * l\n  else:\n    log_t = priv_eps * l\n\n  return min(0.5 * priv_eps * priv_eps * l * (l + 1), log_t, priv_eps * l)\n\n\ndef logmgf_from_counts(counts, noise_eps, l):\n  """"""\n  ReportNoisyMax mechanism with noise_eps with 2*noise_eps-DP\n  in our setting where one count can go up by one and another\n  can go down by 1.\n  """"""\n\n  q = compute_q_noisy_max(counts, noise_eps)\n  return logmgf_exact(q, 2.0 * noise_eps, l)\n\n\ndef sens_at_k(counts, noise_eps, l, k):\n  """"""Return sensitivity at distane k.\n\n  Args:\n    counts: an array of scores\n    noise_eps: noise parameter used\n    l: moment whose sensitivity is being computed\n    k: distance\n  Returns:\n    sensitivity: at distance k\n  """"""\n  counts_sorted = sorted(counts, reverse=True)\n  if 0.5 * noise_eps * l > 1:\n    print ""l too large to compute sensitivity""\n    return 0\n  # Now we can assume that at k, gap remains positive\n  # or we have reached the point where logmgf_exact is\n  # determined by the first term and ind of q.\n  if counts[0] < counts[1] + k:\n    return 0\n  counts_sorted[0] -= k\n  counts_sorted[1] += k\n  val = logmgf_from_counts(counts_sorted, noise_eps, l)\n  counts_sorted[0] -= 1\n  counts_sorted[1] += 1\n  val_changed = logmgf_from_counts(counts_sorted, noise_eps, l)\n  return val_changed - val\n\n\ndef smoothed_sens(counts, noise_eps, l, beta):\n  """"""Compute beta-smooth sensitivity.\n\n  Args:\n    counts: array of scors\n    noise_eps: noise parameter\n    l: moment of interest\n    beta: smoothness parameter\n  Returns:\n    smooth_sensitivity: a beta smooth upper bound\n  """"""\n  k = 0\n  smoothed_sensitivity = sens_at_k(counts, noise_eps, l, k)\n  while k < max(counts):\n    k += 1\n    sensitivity_at_k = sens_at_k(counts, noise_eps, l, k)\n    smoothed_sensitivity = max(\n        smoothed_sensitivity,\n        math.exp(-beta * k) * sensitivity_at_k)\n    if sensitivity_at_k == 0.0:\n      break\n  return smoothed_sensitivity\n\n\ndef main(unused_argv):\n  ##################################################################\n  # If we are reproducing results from paper https://arxiv.org/abs/1610.05755,\n  # download the required binaries with label information.\n  ##################################################################\n  \n  # Binaries for MNIST results\n  paper_binaries_mnist = \\\n    [""https://github.com/npapernot/multiple-teachers-for-privacy/blob/master/mnist_250_teachers_labels.npy?raw=true"", \n    ""https://github.com/npapernot/multiple-teachers-for-privacy/blob/master/mnist_250_teachers_100_indices_used_by_student.npy?raw=true""]\n  if FLAGS.counts_file == ""mnist_250_teachers_labels.npy"" \\\n    or FLAGS.indices_file == ""mnist_250_teachers_100_indices_used_by_student.npy"":\n    maybe_download(paper_binaries_mnist, os.getcwd())\n\n  # Binaries for SVHN results\n  paper_binaries_svhn = [""https://github.com/npapernot/multiple-teachers-for-privacy/blob/master/svhn_250_teachers_labels.npy?raw=true""]\n  if FLAGS.counts_file == ""svhn_250_teachers_labels.npy"":\n    maybe_download(paper_binaries_svhn, os.getcwd())\n\n  input_mat = np.load(FLAGS.counts_file)\n  if FLAGS.input_is_counts:\n    counts_mat = input_mat\n  else:\n    # In this case, the input is the raw predictions. Transform\n    num_teachers, n = input_mat.shape\n    counts_mat = np.zeros((n, 10)).astype(np.int32)\n    for i in range(n):\n      for j in range(num_teachers):\n        counts_mat[i, input_mat[j, i]] += 1\n  n = counts_mat.shape[0]\n  num_examples = min(n, FLAGS.max_examples)\n\n  if not FLAGS.indices_file:\n    indices = np.array(range(num_examples))\n  else:\n    index_list = np.load(FLAGS.indices_file)\n    indices = index_list[:num_examples]\n\n  l_list = 1.0 + np.array(xrange(FLAGS.moments))\n  beta = FLAGS.beta\n  total_log_mgf_nm = np.array([0.0 for _ in l_list])\n  total_ss_nm = np.array([0.0 for _ in l_list])\n  noise_eps = FLAGS.noise_eps\n  \n  for i in indices:\n    total_log_mgf_nm += np.array(\n        [logmgf_from_counts(counts_mat[i], noise_eps, l)\n         for l in l_list])\n    total_ss_nm += np.array(\n        [smoothed_sens(counts_mat[i], noise_eps, l, beta)\n         for l in l_list])\n  delta = FLAGS.delta\n\n  # We want delta = exp(alpha - eps l).\n  # Solving gives eps = (alpha - ln (delta))/l\n  eps_list_nm = (total_log_mgf_nm - math.log(delta)) / l_list\n\n  print ""Epsilons (Noisy Max): "" + str(eps_list_nm)\n  print ""Smoothed sensitivities (Noisy Max): "" + str(total_ss_nm / l_list)\n\n  # If beta < eps / 2 ln (1/delta), then adding noise Lap(1) * 2 SS/eps\n  # is eps,delta DP\n  # Also if beta < eps / 2(gamma +1), then adding noise 2(gamma+1) SS eta / eps\n  # where eta has density proportional to 1 / (1+|z|^gamma) is eps-DP\n  # Both from Corolloary 2.4 in\n  # http://www.cse.psu.edu/~ads22/pubs/NRS07/NRS07-full-draft-v1.pdf\n  # Print the first one\'s scale\n  ss_eps = 2.0 * beta * math.log(1/delta)\n  ss_scale = 2.0 / ss_eps\n  print ""To get an "" + str(ss_eps) + ""-DP estimate of epsilon, ""\n  print ""..add noise ~ "" + str(ss_scale)\n  print ""... times "" + str(total_ss_nm / l_list)\n  print ""Epsilon = "" + str(min(eps_list_nm)) + "".""\n  if min(eps_list_nm) == eps_list_nm[-1]:\n    print ""Warning: May not have used enough values of l""\n\n  # Data indpendent bound, as mechanism is\n  # 2*noise_eps DP.\n  data_ind_log_mgf = np.array([0.0 for _ in l_list])\n  data_ind_log_mgf += num_examples * np.array(\n      [logmgf_exact(1.0, 2.0 * noise_eps, l) for l in l_list])\n\n  data_ind_eps_list = (data_ind_log_mgf - math.log(delta)) / l_list\n  print ""Data independent bound = "" + str(min(data_ind_eps_list)) + "".""\n\n  return\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
model_zoo/models/differential_privacy/multiple_teachers/deep_cnn.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport math\nimport numpy as np\nimport tensorflow as tf\nimport time\n\nfrom differential_privacy.multiple_teachers import utils\n\nFLAGS = tf.app.flags.FLAGS\n\n# Basic model parameters.\ntf.app.flags.DEFINE_integer(\'dropout_seed\', 123, """"""seed for dropout."""""")\ntf.app.flags.DEFINE_integer(\'batch_size\', 128, """"""Nb of images in a batch."""""")\ntf.app.flags.DEFINE_integer(\'epochs_per_decay\', 350, """"""Nb epochs per decay"""""")\ntf.app.flags.DEFINE_integer(\'learning_rate\', 5, """"""100 * learning rate"""""")\ntf.app.flags.DEFINE_boolean(\'log_device_placement\', False, """"""see TF doc"""""")\n\n\n# Constants describing the training process.\nMOVING_AVERAGE_DECAY = 0.9999     # The decay to use for the moving average.\nLEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.\n\n\ndef _variable_on_cpu(name, shape, initializer):\n  """"""Helper to create a Variable stored on CPU memory.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    initializer: initializer for Variable\n\n  Returns:\n    Variable Tensor\n  """"""\n  with tf.device(\'/cpu:0\'):\n    var = tf.get_variable(name, shape, initializer=initializer)\n  return var\n\n\ndef _variable_with_weight_decay(name, shape, stddev, wd):\n  """"""Helper to create an initialized Variable with weight decay.\n\n  Note that the Variable is initialized with a truncated normal distribution.\n  A weight decay is added only if one is specified.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    stddev: standard deviation of a truncated Gaussian\n    wd: add L2Loss weight decay multiplied by this float. If None, weight\n        decay is not added for this Variable.\n\n  Returns:\n    Variable Tensor\n  """"""\n  var = _variable_on_cpu(name, shape,\n                         tf.truncated_normal_initializer(stddev=stddev))\n  if wd is not None:\n    weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name=\'weight_loss\')\n    tf.add_to_collection(\'losses\', weight_decay)\n  return var\n\n\ndef inference(images, dropout=False):\n  """"""Build the CNN model.\n  Args:\n    images: Images returned from distorted_inputs() or inputs().\n    dropout: Boolean controling whether to use dropout or not\n  Returns:\n    Logits\n  """"""\n  if FLAGS.dataset == \'mnist\':\n    first_conv_shape = [5, 5, 1, 64]\n  else:\n    first_conv_shape = [5, 5, 3, 64]\n\n  # conv1\n  with tf.variable_scope(\'conv1\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\', \n                                         shape=first_conv_shape,\n                                         stddev=1e-4, \n                                         wd=0.0)\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [64], tf.constant_initializer(0.0))\n    bias = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(bias, name=scope.name)\n    if dropout:\n      conv1 = tf.nn.dropout(conv1, 0.3, seed=FLAGS.dropout_seed)\n\n\n  # pool1\n  pool1 = tf.nn.max_pool(conv1, \n                         ksize=[1, 3, 3, 1], \n                         strides=[1, 2, 2, 1],\n                         padding=\'SAME\', \n                         name=\'pool1\')\n  \n  # norm1\n  norm1 = tf.nn.lrn(pool1, \n                    4, \n                    bias=1.0, \n                    alpha=0.001 / 9.0, \n                    beta=0.75,\n                    name=\'norm1\')\n\n  # conv2\n  with tf.variable_scope(\'conv2\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\', \n                                         shape=[5, 5, 64, 128],\n                                         stddev=1e-4, \n                                         wd=0.0)\n    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [128], tf.constant_initializer(0.1))\n    bias = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(bias, name=scope.name)\n    if dropout:\n      conv2 = tf.nn.dropout(conv2, 0.3, seed=FLAGS.dropout_seed)\n\n\n  # norm2\n  norm2 = tf.nn.lrn(conv2, \n                    4, \n                    bias=1.0, \n                    alpha=0.001 / 9.0, \n                    beta=0.75,\n                    name=\'norm2\')\n  \n  # pool2\n  pool2 = tf.nn.max_pool(norm2, \n                         ksize=[1, 3, 3, 1],\n                         strides=[1, 2, 2, 1], \n                         padding=\'SAME\', \n                         name=\'pool2\')\n\n  # local3\n  with tf.variable_scope(\'local3\') as scope:\n    # Move everything into depth so we can perform a single matrix multiply.\n    reshape = tf.reshape(pool2, [FLAGS.batch_size, -1])\n    dim = reshape.get_shape()[1].value\n    weights = _variable_with_weight_decay(\'weights\', \n                                          shape=[dim, 384],\n                                          stddev=0.04, \n                                          wd=0.004)\n    biases = _variable_on_cpu(\'biases\', [384], tf.constant_initializer(0.1))\n    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n    if dropout:\n      local3 = tf.nn.dropout(local3, 0.5, seed=FLAGS.dropout_seed)\n\n  # local4\n  with tf.variable_scope(\'local4\') as scope:\n    weights = _variable_with_weight_decay(\'weights\', \n                                          shape=[384, 192],\n                                          stddev=0.04, \n                                          wd=0.004)\n    biases = _variable_on_cpu(\'biases\', [192], tf.constant_initializer(0.1))\n    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n    if dropout:\n      local4 = tf.nn.dropout(local4, 0.5, seed=FLAGS.dropout_seed)\n\n  # compute logits\n  with tf.variable_scope(\'softmax_linear\') as scope:\n    weights = _variable_with_weight_decay(\'weights\', \n                                          [192, FLAGS.nb_labels],\n                                          stddev=1/192.0, \n                                          wd=0.0)\n    biases = _variable_on_cpu(\'biases\', \n                              [FLAGS.nb_labels],\n                              tf.constant_initializer(0.0))\n    logits = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n\n  return logits\n\n\ndef inference_deeper(images, dropout=False):\n  """"""Build a deeper CNN model.\n  Args:\n    images: Images returned from distorted_inputs() or inputs().\n    dropout: Boolean controling whether to use dropout or not\n  Returns:\n    Logits\n  """"""\n  if FLAGS.dataset == \'mnist\':\n    first_conv_shape = [3, 3, 1, 96]\n  else:\n    first_conv_shape = [3, 3, 3, 96]\n\n  # conv1\n  with tf.variable_scope(\'conv1\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=first_conv_shape,\n                                         stddev=0.05,\n                                         wd=0.0)\n    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [96], tf.constant_initializer(0.0))\n    bias = tf.nn.bias_add(conv, biases)\n    conv1 = tf.nn.relu(bias, name=scope.name)\n\n  # conv2\n  with tf.variable_scope(\'conv2\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=[3, 3, 96, 96],\n                                         stddev=0.05,\n                                         wd=0.0)\n    conv = tf.nn.conv2d(conv1, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [96], tf.constant_initializer(0.0))\n    bias = tf.nn.bias_add(conv, biases)\n    conv2 = tf.nn.relu(bias, name=scope.name)\n\n  # conv3\n  with tf.variable_scope(\'conv3\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=[3, 3, 96, 96],\n                                         stddev=0.05,\n                                         wd=0.0)\n    conv = tf.nn.conv2d(conv2, kernel, [1, 2, 2, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [96], tf.constant_initializer(0.0))\n    bias = tf.nn.bias_add(conv, biases)\n    conv3 = tf.nn.relu(bias, name=scope.name)\n    if dropout:\n      conv3 = tf.nn.dropout(conv3, 0.5, seed=FLAGS.dropout_seed)\n\n  # conv4\n  with tf.variable_scope(\'conv4\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=[3, 3, 96, 192],\n                                         stddev=0.05,\n                                         wd=0.0)\n    conv = tf.nn.conv2d(conv3, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [192], tf.constant_initializer(0.0))\n    bias = tf.nn.bias_add(conv, biases)\n    conv4 = tf.nn.relu(bias, name=scope.name)\n\n  # conv5\n  with tf.variable_scope(\'conv5\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=[3, 3, 192, 192],\n                                         stddev=0.05,\n                                         wd=0.0)\n    conv = tf.nn.conv2d(conv4, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [192], tf.constant_initializer(0.0))\n    bias = tf.nn.bias_add(conv, biases)\n    conv5 = tf.nn.relu(bias, name=scope.name)\n\n  # conv6\n  with tf.variable_scope(\'conv6\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=[3, 3, 192, 192],\n                                         stddev=0.05,\n                                         wd=0.0)\n    conv = tf.nn.conv2d(conv5, kernel, [1, 2, 2, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [192], tf.constant_initializer(0.0))\n    bias = tf.nn.bias_add(conv, biases)\n    conv6 = tf.nn.relu(bias, name=scope.name)\n    if dropout:\n      conv6 = tf.nn.dropout(conv6, 0.5, seed=FLAGS.dropout_seed)\n\n\n  # conv7\n  with tf.variable_scope(\'conv7\') as scope:\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=[5, 5, 192, 192],\n                                         stddev=1e-4,\n                                         wd=0.0)\n    conv = tf.nn.conv2d(conv6, kernel, [1, 1, 1, 1], padding=\'SAME\')\n    biases = _variable_on_cpu(\'biases\', [192], tf.constant_initializer(0.1))\n    bias = tf.nn.bias_add(conv, biases)\n    conv7 = tf.nn.relu(bias, name=scope.name)\n\n\n  # local1\n  with tf.variable_scope(\'local1\') as scope:\n    # Move everything into depth so we can perform a single matrix multiply.\n    reshape = tf.reshape(conv7, [FLAGS.batch_size, -1])\n    dim = reshape.get_shape()[1].value\n    weights = _variable_with_weight_decay(\'weights\',\n                                          shape=[dim, 192],\n                                          stddev=0.05,\n                                          wd=0)\n    biases = _variable_on_cpu(\'biases\', [192], tf.constant_initializer(0.1))\n    local1 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n\n  # local2\n  with tf.variable_scope(\'local2\') as scope:\n    weights = _variable_with_weight_decay(\'weights\',\n                                          shape=[192, 192],\n                                          stddev=0.05,\n                                          wd=0)\n    biases = _variable_on_cpu(\'biases\', [192], tf.constant_initializer(0.1))\n    local2 = tf.nn.relu(tf.matmul(local1, weights) + biases, name=scope.name)\n    if dropout:\n      local2 = tf.nn.dropout(local2, 0.5, seed=FLAGS.dropout_seed)\n\n  # compute logits\n  with tf.variable_scope(\'softmax_linear\') as scope:\n    weights = _variable_with_weight_decay(\'weights\',\n                                          [192, FLAGS.nb_labels],\n                                          stddev=0.05,\n                                          wd=0.0)\n    biases = _variable_on_cpu(\'biases\',\n                              [FLAGS.nb_labels],\n                              tf.constant_initializer(0.0))\n    logits = tf.add(tf.matmul(local2, weights), biases, name=scope.name)\n\n  return logits\n\n\ndef loss_fun(logits, labels):\n  """"""Add L2Loss to all the trainable variables.\n\n  Add summary for ""Loss"" and ""Loss/avg"".\n  Args:\n    logits: Logits from inference().\n    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n            of shape [batch_size]\n    distillation: if set to True, use probabilities and not class labels to\n                  compute softmax loss\n\n  Returns:\n    Loss tensor of type float.\n  """"""\n\n  # Calculate the cross entropy between labels and predictions\n  labels = tf.cast(labels, tf.int64)\n  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n      logits, labels, name=\'cross_entropy_per_example\')\n\n  # Calculate the average cross entropy loss across the batch.\n  cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n\n  # Add to TF collection for losses\n  tf.add_to_collection(\'losses\', cross_entropy_mean)\n\n  # The total loss is defined as the cross entropy loss plus all of the weight\n  # decay terms (L2 loss).\n  return tf.add_n(tf.get_collection(\'losses\'), name=\'total_loss\')\n\n\ndef moving_av(total_loss):\n  """"""\n  Generates moving average for all losses\n\n  Args:\n    total_loss: Total loss from loss().\n  Returns:\n    loss_averages_op: op for generating moving averages of losses.\n  """"""\n  # Compute the moving average of all individual losses and the total loss.\n  loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n  losses = tf.get_collection(\'losses\')\n  loss_averages_op = loss_averages.apply(losses + [total_loss])\n\n  return loss_averages_op\n\n\ndef train_op_fun(total_loss, global_step):\n  """"""Train model.\n\n  Create an optimizer and apply to all trainable variables. Add moving\n  average for all trainable variables.\n\n  Args:\n    total_loss: Total loss from loss().\n    global_step: Integer Variable counting the number of training steps\n      processed.\n  Returns:\n    train_op: op for training.\n  """"""\n  # Variables that affect learning rate.\n  nb_ex_per_train_epoch = int(60000 / FLAGS.nb_teachers)\n  \n  num_batches_per_epoch = nb_ex_per_train_epoch / FLAGS.batch_size\n  decay_steps = int(num_batches_per_epoch * FLAGS.epochs_per_decay)\n\n  initial_learning_rate = float(FLAGS.learning_rate) / 100.0\n\n  # Decay the learning rate exponentially based on the number of steps.\n  lr = tf.train.exponential_decay(initial_learning_rate,\n                                  global_step,\n                                  decay_steps,\n                                  LEARNING_RATE_DECAY_FACTOR,\n                                  staircase=True)\n  tf.scalar_summary(\'learning_rate\', lr)\n\n  # Generate moving averages of all losses and associated summaries.\n  loss_averages_op = moving_av(total_loss)\n\n  # Compute gradients.\n  with tf.control_dependencies([loss_averages_op]):\n    opt = tf.train.GradientDescentOptimizer(lr)\n    grads = opt.compute_gradients(total_loss)\n\n  # Apply gradients.\n  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n  # Add histograms for trainable variables.\n  for var in tf.trainable_variables():\n    tf.histogram_summary(var.op.name, var)\n\n  # Track the moving averages of all trainable variables.\n  variable_averages = tf.train.ExponentialMovingAverage(\n      MOVING_AVERAGE_DECAY, global_step)\n  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n    train_op = tf.no_op(name=\'train\')\n\n  return train_op\n\n\ndef _input_placeholder():\n  """"""\n  This helper function declares a TF placeholder for the graph input data\n  :return: TF placeholder for the graph input data\n  """"""\n  if FLAGS.dataset == \'mnist\':\n    image_size = 28\n    num_channels = 1\n  else:\n    image_size = 32\n    num_channels = 3\n\n  # Declare data placeholder\n  train_node_shape = (FLAGS.batch_size, image_size, image_size, num_channels)\n  return tf.placeholder(tf.float32, shape=train_node_shape)\n\n\ndef train(images, labels, ckpt_path, dropout=False):\n  """"""\n  This function contains the loop that actually trains the model.\n  :param images: a numpy array with the input data\n  :param labels: a numpy array with the output labels\n  :param ckpt_path: a path (including name) where model checkpoints are saved\n  :param dropout: Boolean, whether to use dropout or not\n  :return: True if everything went well\n  """"""\n\n  # Check training data\n  assert len(images) == len(labels)\n  assert images.dtype == np.float32\n  assert labels.dtype == np.int32\n\n  # Set default TF graph\n  with tf.Graph().as_default():\n    global_step = tf.Variable(0, trainable=False)\n\n    # Declare data placeholder\n    train_data_node = _input_placeholder()\n\n    # Create a placeholder to hold labels\n    train_labels_shape = (FLAGS.batch_size,)\n    train_labels_node = tf.placeholder(tf.int32, shape=train_labels_shape)\n\n    print(""Done Initializing Training Placeholders"")\n\n    # Build a Graph that computes the logits predictions from the placeholder\n    if FLAGS.deeper:\n      logits = inference_deeper(train_data_node, dropout=dropout)\n    else:\n      logits = inference(train_data_node, dropout=dropout)\n\n    # Calculate loss\n    loss = loss_fun(logits, train_labels_node)\n\n    # Build a Graph that trains the model with one batch of examples and\n    # updates the model parameters.\n    train_op = train_op_fun(loss, global_step)\n\n    # Create a saver.\n    saver = tf.train.Saver(tf.all_variables())\n\n    print(""Graph constructed and saver created"")\n\n    # Build an initialization operation to run below.\n    init = tf.initialize_all_variables()\n\n    # Create and init sessions\n    sess = tf.Session(config=tf.ConfigProto(log_device_placement=FLAGS.log_device_placement)) #NOLINT(long-line)\n    sess.run(init)\n\n    print(""Session ready, beginning training loop"")\n\n    # Initialize the number of batches\n    data_length = len(images)\n    nb_batches = math.ceil(data_length / FLAGS.batch_size)\n\n    for step in xrange(FLAGS.max_steps):\n      # for debug, save start time\n      start_time = time.time()\n\n      # Current batch number\n      batch_nb = step % nb_batches\n\n      # Current batch start and end indices\n      start, end = utils.batch_indices(batch_nb, data_length, FLAGS.batch_size)\n\n      # Prepare dictionnary to feed the session with\n      feed_dict = {train_data_node: images[start:end],\n                   train_labels_node: labels[start:end]}\n\n      # Run training step\n      _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n\n      # Compute duration of training step\n      duration = time.time() - start_time\n\n      # Sanity check\n      assert not np.isnan(loss_value), \'Model diverged with loss = NaN\'\n\n      # Echo loss once in a while\n      if step % 100 == 0:\n        num_examples_per_step = FLAGS.batch_size\n        examples_per_sec = num_examples_per_step / duration\n        sec_per_batch = float(duration)\n\n        format_str = (\'%s: step %d, loss = %.2f (%.1f examples/sec; %.3f \'\n                      \'sec/batch)\')\n        print (format_str % (datetime.now(), step, loss_value,\n                             examples_per_sec, sec_per_batch))\n\n      # Save the model checkpoint periodically.\n      if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n        saver.save(sess, ckpt_path, global_step=step)\n\n  return True\n\n\ndef softmax_preds(images, ckpt_path, return_logits=False):\n  """"""\n  Compute softmax activations (probabilities) with the model saved in the path\n  specified as an argument\n  :param images: a np array of images\n  :param ckpt_path: a TF model checkpoint\n  :param logits: if set to True, return logits instead of probabilities\n  :return: probabilities (or logits if logits is set to True)\n  """"""\n  # Compute nb samples and deduce nb of batches\n  data_length = len(images)\n  nb_batches = math.ceil(len(images) / FLAGS.batch_size)\n\n  # Declare data placeholder\n  train_data_node = _input_placeholder()\n\n  # Build a Graph that computes the logits predictions from the placeholder\n  if FLAGS.deeper:\n    logits = inference_deeper(train_data_node)\n  else:\n    logits = inference(train_data_node)\n\n  if return_logits:\n    # We are returning the logits directly (no need to apply softmax)\n    output = logits\n  else:\n    # Add softmax predictions to graph: will return probabilities\n    output = tf.nn.softmax(logits)\n\n  # Restore the moving average version of the learned variables for eval.\n  variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n  variables_to_restore = variable_averages.variables_to_restore()\n  saver = tf.train.Saver(variables_to_restore)\n\n  # Will hold the result\n  preds = np.zeros((data_length, FLAGS.nb_labels), dtype=np.float32)\n\n  # Create TF session\n  with tf.Session() as sess:\n    # Restore TF session from checkpoint file\n    saver.restore(sess, ckpt_path)\n\n    # Parse data by batch\n    for batch_nb in xrange(0, int(nb_batches+1)):\n      # Compute batch start and end indices\n      start, end = utils.batch_indices(batch_nb, data_length, FLAGS.batch_size)\n\n      # Prepare feed dictionary\n      feed_dict = {train_data_node: images[start:end]}\n\n      # Run session ([0] because run returns a batch with len 1st dim == 1)\n      preds[start:end, :] = sess.run([output], feed_dict=feed_dict)[0]\n\n  # Reset graph to allow multiple calls\n  tf.reset_default_graph()\n\n  return preds\n\n\n'"
model_zoo/models/differential_privacy/multiple_teachers/input.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cPickle\nimport gzip\nimport math\nimport numpy as np\nimport os\nfrom scipy.io import loadmat as loadmat\nfrom six.moves import urllib\nimport sys\nimport tarfile\n\nimport tensorflow as tf\n\nFLAGS = tf.flags.FLAGS\n\n\ndef create_dir_if_needed(dest_directory):\n  """"""\n  Create directory if doesn\'t exist\n  :param dest_directory:\n  :return: True if everything went well\n  """"""\n  if not tf.gfile.IsDirectory(dest_directory):\n    tf.gfile.MakeDirs(dest_directory)\n\n  return True\n\n\ndef maybe_download(file_urls, directory):\n  """"""\n  Download a set of files in temporary local folder\n  :param directory: the directory where to download \n  :return: a tuple of filepaths corresponding to the files given as input\n  """"""\n  # Create directory if doesn\'t exist\n  assert create_dir_if_needed(directory)\n\n  # This list will include all URLS of the local copy of downloaded files\n  result = []\n\n  # For each file of the dataset\n  for file_url in file_urls:\n    # Extract filename\n    filename = file_url.split(\'/\')[-1]\n\n    # If downloading from GitHub, remove suffix ?raw=True from local filename\n    if filename.endswith(""?raw=true""):\n      filename = filename[:-9]\n\n    # Deduce local file url\n    #filepath = os.path.join(directory, filename)\n    filepath = directory + \'/\' + filename\n\n    # Add to result list\n    result.append(filepath)\n\n    # Test if file already exists\n    if not gfile.Exists(filepath):\n      def _progress(count, block_size, total_size):\n        sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (filename,\n            float(count * block_size) / float(total_size) * 100.0))\n        sys.stdout.flush()\n      filepath, _ = urllib.request.urlretrieve(file_url, filepath, _progress)\n      print()\n      statinfo = os.stat(filepath)\n      print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n\n  return result\n\n\ndef image_whitening(data):\n  """"""\n  Subtracts mean of image and divides by adjusted standard variance (for\n  stability). Operations are per image but performed for the entire array.\n  :param image: 4D array (ID, Height, Weight, Channel)\n  :return: 4D array (ID, Height, Weight, Channel)\n  """"""\n  assert len(np.shape(data)) == 4\n\n  # Compute number of pixels in image\n  nb_pixels = np.shape(data)[1] * np.shape(data)[2] * np.shape(data)[3]\n\n  # Subtract mean\n  mean = np.mean(data, axis=(1,2,3))\n\n  ones = np.ones(np.shape(data)[1:4], dtype=np.float32)\n  for i in xrange(len(data)):\n    data[i, :, :, :] -= mean[i] * ones\n\n  # Compute adjusted standard variance\n  adj_std_var = np.maximum(np.ones(len(data), dtype=np.float32) / math.sqrt(nb_pixels), np.std(data, axis=(1,2,3))) #NOLINT(long-line)\n\n  # Divide image\n  for i in xrange(len(data)):\n    data[i, :, :, :] = data[i, :, :, :] / adj_std_var[i]\n\n  print(np.shape(data))\n\n  return data\n\n\ndef extract_svhn(local_url):\n  """"""\n  Extract a MATLAB matrix into two numpy arrays with data and labels\n  :param local_url:\n  :return:\n  """"""\n\n  with gfile.Open(local_url, mode=\'r\') as file_obj:\n    # Load MATLAB matrix using scipy IO\n    dict = loadmat(file_obj)\n\n    # Extract each dictionary (one for data, one for labels)\n    data, labels = dict[""X""], dict[""y""]\n\n    # Set np type\n    data = np.asarray(data, dtype=np.float32)\n    labels = np.asarray(labels, dtype=np.int32)\n\n    # Transpose data to match TF model input format\n    data = data.transpose(3, 0, 1, 2)\n\n    # Fix the SVHN labels which label 0s as 10s\n    labels[labels == 10] = 0\n\n    # Fix label dimensions\n    labels = labels.reshape(len(labels))\n\n    return data, labels\n\n\ndef unpickle_cifar_dic(file):\n  """"""\n  Helper function: unpickles a dictionary (used for loading CIFAR)\n  :param file: filename of the pickle\n  :return: tuple of (images, labels)\n  """"""\n  fo = open(file, \'rb\')\n  dict = cPickle.load(fo)\n  fo.close()\n  return dict[\'data\'], dict[\'labels\']\n\n\ndef extract_cifar10(local_url, data_dir):\n  """"""\n  Extracts the CIFAR-10 dataset and return numpy arrays with the different sets\n  :param local_url: where the tar.gz archive is located locally\n  :param data_dir: where to extract the archive\'s file\n  :return: a tuple (train data, train labels, test data, test labels)\n  """"""\n  # These numpy dumps can be reloaded to avoid performing the pre-processing\n  # if they exist in the working directory.\n  # Changing the order of this list will ruin the indices below.\n  preprocessed_files = [\'/cifar10_train.npy\',\n                        \'/cifar10_train_labels.npy\',\n                        \'/cifar10_test.npy\',\n                        \'/cifar10_test_labels.npy\']\n\n  all_preprocessed = True\n  for file in preprocessed_files:\n    if not tf.gfile.Exists(data_dir + file):\n      all_preprocessed = False\n      break\n\n  if all_preprocessed:\n    # Reload pre-processed training data from numpy dumps\n    with tf.gfile.Open(data_dir + preprocessed_files[0], mode=\'r\') as file_obj:\n      train_data = np.load(file_obj)\n    with tf.gfile.Open(data_dir + preprocessed_files[1], mode=\'r\') as file_obj:\n      train_labels = np.load(file_obj)\n\n    # Reload pre-processed testing data from numpy dumps\n    with tf.gfile.Open(data_dir + preprocessed_files[2], mode=\'r\') as file_obj:\n      test_data = np.load(file_obj)\n    with tf.gfile.Open(data_dir + preprocessed_files[3], mode=\'r\') as file_obj:\n      test_labels = np.load(file_obj)\n\n  else:\n    # Do everything from scratch\n    # Define lists of all files we should extract\n    train_files = [""data_batch_"" + str(i) for i in xrange(1,6)]\n    test_file = [""test_batch""]\n    cifar10_files = train_files + test_file\n\n    # Check if all files have already been extracted\n    need_to_unpack = False\n    for file in cifar10_files:\n      if not tf.gfile.Exists(file):\n        need_to_unpack = True\n        break\n\n    # We have to unpack the archive\n    if need_to_unpack:\n      tarfile.open(local_url, \'r:gz\').extractall(data_dir)\n\n    # Load training images and labels\n    images = []\n    labels = []\n    for file in train_files:\n      # Construct filename\n      filename = data_dir + ""/cifar-10-batches-py/"" + file\n\n      # Unpickle dictionary and extract images and labels\n      images_tmp, labels_tmp = unpickle_cifar_dic(filename)\n\n      # Append to lists\n      images.append(images_tmp)\n      labels.append(labels_tmp)\n\n    # Convert to numpy arrays and reshape in the expected format\n    train_data = np.asarray(images, dtype=np.float32).reshape((50000,3,32,32))\n    train_data = np.swapaxes(train_data, 1, 3)\n    train_labels = np.asarray(labels, dtype=np.int32).reshape(50000)\n\n    # Save so we don\'t have to do this again\n    np.save(data_dir + preprocessed_files[0], train_data)\n    np.save(data_dir + preprocessed_files[1], train_labels)\n\n    # Construct filename for test file\n    filename = data_dir + ""/cifar-10-batches-py/"" + test_file[0]\n\n    # Load test images and labels\n    test_data, test_images = unpickle_cifar_dic(filename)\n\n    # Convert to numpy arrays and reshape in the expected format\n    test_data = np.asarray(test_data,dtype=np.float32).reshape((10000,3,32,32))\n    test_data = np.swapaxes(test_data, 1, 3)\n    test_labels = np.asarray(test_images, dtype=np.int32).reshape(10000)\n\n    # Save so we don\'t have to do this again\n    np.save(data_dir + preprocessed_files[2], test_data)\n    np.save(data_dir + preprocessed_files[3], test_labels)\n\n  return train_data, train_labels, test_data, test_labels\n\n\ndef extract_mnist_data(filename, num_images, image_size, pixel_depth):\n  """"""\n  Extract the images into a 4D tensor [image index, y, x, channels].\n\n  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n  """"""\n  # if not os.path.exists(file):\n  if not tf.gfile.Exists(filename+"".npy""):\n    with gzip.open(filename) as bytestream:\n      bytestream.read(16)\n      buf = bytestream.read(image_size * image_size * num_images)\n      data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n      data = (data - (pixel_depth / 2.0)) / pixel_depth\n      data = data.reshape(num_images, image_size, image_size, 1)\n      np.save(filename, data)\n      return data\n  else:\n    with tf.gfile.Open(filename+"".npy"", mode=\'r\') as file_obj:\n      return np.load(file_obj)\n\n\ndef extract_mnist_labels(filename, num_images):\n  """"""\n  Extract the labels into a vector of int64 label IDs.\n  """"""\n  # if not os.path.exists(file):\n  if not tf.gfile.Exists(filename+"".npy""):\n    with gzip.open(filename) as bytestream:\n      bytestream.read(8)\n      buf = bytestream.read(1 * num_images)\n      labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int32)\n      np.save(filename, labels)\n    return labels\n  else:\n    with tf.gfile.Open(filename+"".npy"", mode=\'r\') as file_obj:\n      return np.load(file_obj)\n\n\ndef ld_svhn(extended=False, test_only=False):\n  """"""\n  Load the original SVHN data\n  :param extended: include extended training data in the returned array\n  :param test_only: disables loading of both train and extra -> large speed up\n  :return: tuple of arrays which depend on the parameters\n  """"""\n  # Define files to be downloaded\n  # WARNING: changing the order of this list will break indices (cf. below)\n  file_urls = [\'http://ufldl.stanford.edu/housenumbers/train_32x32.mat\',\n               \'http://ufldl.stanford.edu/housenumbers/test_32x32.mat\',\n               \'http://ufldl.stanford.edu/housenumbers/extra_32x32.mat\']\n\n  # Maybe download data and retrieve local storage urls\n  local_urls = maybe_download(file_urls, FLAGS.data_dir)\n\n  # Extra Train, Test, and Extended Train data\n  if not test_only:\n    # Load and applying whitening to train data\n    train_data, train_labels = extract_svhn(local_urls[0])\n    train_data = image_whitening(train_data)\n\n    # Load and applying whitening to extended train data\n    ext_data, ext_labels = extract_svhn(local_urls[2])\n    ext_data = image_whitening(ext_data)\n\n  # Load and applying whitening to test data\n  test_data, test_labels = extract_svhn(local_urls[1])\n  test_data = image_whitening(test_data)\n\n  if test_only:\n    return test_data, test_labels\n  else:\n    if extended:\n      # Stack train data with the extended training data\n      train_data = np.vstack((train_data, ext_data))\n      train_labels = np.hstack((train_labels, ext_labels))\n\n      return train_data, train_labels, test_data, test_labels\n    else:\n      # Return training and extended training data separately\n      return train_data,train_labels, test_data,test_labels, ext_data,ext_labels\n\n\ndef ld_cifar10(test_only=False):\n  """"""\n  Load the original CIFAR10 data\n  :param extended: include extended training data in the returned array\n  :param test_only: disables loading of both train and extra -> large speed up\n  :return: tuple of arrays which depend on the parameters\n  """"""\n  # Define files to be downloaded\n  file_urls = [\'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\']\n\n  # Maybe download data and retrieve local storage urls\n  local_urls = maybe_download(file_urls, FLAGS.data_dir)\n\n  # Extract archives and return different sets\n  dataset = extract_cifar10(local_urls[0], FLAGS.data_dir)\n\n  # Unpack tuple\n  train_data, train_labels, test_data, test_labels = dataset\n\n  # Apply whitening to input data\n  train_data = image_whitening(train_data)\n  test_data = image_whitening(test_data)\n\n  if test_only:\n    return test_data, test_labels\n  else:\n    return train_data, train_labels, test_data, test_labels\n\n\ndef ld_mnist(test_only=False):\n  """"""\n  Load the MNIST dataset\n  :param extended: include extended training data in the returned array\n  :param test_only: disables loading of both train and extra -> large speed up\n  :return: tuple of arrays which depend on the parameters\n  """"""\n  # Define files to be downloaded\n  # WARNING: changing the order of this list will break indices (cf. below)\n  file_urls = [\'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\',\n               \'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\',\n               \'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\',\n               \'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\',\n               ]\n\n  # Maybe download data and retrieve local storage urls\n  local_urls = maybe_download(file_urls, FLAGS.data_dir)\n\n  # Extract it into np arrays.\n  train_data = extract_mnist_data(local_urls[0], 60000, 28, 1)\n  train_labels = extract_mnist_labels(local_urls[1], 60000)\n  test_data = extract_mnist_data(local_urls[2], 10000, 28, 1)\n  test_labels = extract_mnist_labels(local_urls[3], 10000)\n\n  if test_only:\n    return test_data, test_labels\n  else:\n    return train_data, train_labels, test_data, test_labels\n\n\ndef partition_dataset(data, labels, nb_teachers, teacher_id):\n  """"""\n  Simple partitioning algorithm that returns the right portion of the data\n  needed by a given teacher out of a certain nb of teachers\n  :param data: input data to be partitioned\n  :param labels: output data to be partitioned\n  :param nb_teachers: number of teachers in the ensemble (affects size of each\n                      partition)\n  :param teacher_id: id of partition to retrieve\n  :return:\n  """"""\n\n  # Sanity check\n  assert len(data) == len(labels)\n  assert int(teacher_id) < int(nb_teachers)\n\n  # This will floor the possible number of batches\n  batch_len = int(len(data) / nb_teachers)\n\n  # Compute start, end indices of partition\n  start = teacher_id * batch_len\n  end = (teacher_id+1) * batch_len\n\n  # Slice partition off\n  partition_data = data[start:end]\n  partition_labels = labels[start:end]\n\n  return partition_data, partition_labels\n'"
model_zoo/models/differential_privacy/multiple_teachers/metrics.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\ndef accuracy(logits, labels):\n  """"""\n  Return accuracy of the array of logits (or label predictions) wrt the labels\n  :param logits: this can either be logits, probabilities, or a single label\n  :param labels: the correct labels to match against\n  :return: the accuracy as a float\n  """"""\n  assert len(logits) == len(labels)\n\n  if len(np.shape(logits)) > 1:\n    # Predicted labels are the argmax over axis 1\n    predicted_labels = np.argmax(logits, axis=1)\n  else:\n    # Input was already labels\n    assert len(np.shape(logits)) == 1\n    predicted_labels = logits\n\n  # Check against correct labels to compute correct guesses\n  correct = np.sum(predicted_labels == labels.reshape(len(labels)))\n\n  # Divide by number of labels to obtain accuracy\n  accuracy = float(correct) / len(labels)\n\n  # Return float value\n  return accuracy\n\n\n'"
model_zoo/models/differential_privacy/multiple_teachers/train_student.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom differential_privacy.multiple_teachers import aggregation\nfrom differential_privacy.multiple_teachers import deep_cnn\nfrom differential_privacy.multiple_teachers import input\nfrom differential_privacy.multiple_teachers import metrics\n\nFLAGS = tf.flags.FLAGS\n\ntf.flags.DEFINE_string(\'dataset\', \'svhn\', \'The name of the dataset to use\')\ntf.flags.DEFINE_integer(\'nb_labels\', 10, \'Number of output classes\')\n\ntf.flags.DEFINE_string(\'data_dir\',\'/tmp\',\'Temporary storage\')\ntf.flags.DEFINE_string(\'train_dir\',\'/tmp/train_dir\',\'Where model chkpt are saved\')\ntf.flags.DEFINE_string(\'teachers_dir\',\'/tmp/train_dir\',\n                       \'Directory where teachers checkpoints are stored.\')\n\ntf.flags.DEFINE_integer(\'teachers_max_steps\', 3000,\n                        \'Number of steps teachers were ran.\')\ntf.flags.DEFINE_integer(\'max_steps\', 3000, \'Number of steps to run student.\')\ntf.flags.DEFINE_integer(\'nb_teachers\', 10, \'Teachers in the ensemble.\')\ntf.flags.DEFINE_integer(\'stdnt_share\', 1000,\n                        \'Student share (last index) of the test data\')\ntf.flags.DEFINE_integer(\'lap_scale\', 10,\n                        \'Scale of the Laplacian noise added for privacy\')\ntf.flags.DEFINE_boolean(\'save_labels\', False,\n                        \'Dump numpy arrays of labels and clean teacher votes\')\ntf.flags.DEFINE_boolean(\'deeper\', False, \'Activate deeper CNN model\')\n\n\ndef ensemble_preds(dataset, nb_teachers, stdnt_data):\n  """"""\n  Given a dataset, a number of teachers, and some input data, this helper\n  function queries each teacher for predictions on the data and returns\n  all predictions in a single array. (That can then be aggregated into\n  one single prediction per input using aggregation.py (cf. function\n  prepare_student_data() below)\n  :param dataset: string corresponding to mnist, cifar10, or svhn\n  :param nb_teachers: number of teachers (in the ensemble) to learn from\n  :param stdnt_data: unlabeled student training data\n  :return: 3d array (teacher id, sample id, probability per class)\n  """"""\n\n  # Compute shape of array that will hold probabilities produced by each\n  # teacher, for each training point, and each output class\n  result_shape = (nb_teachers, len(stdnt_data), FLAGS.nb_labels)\n\n  # Create array that will hold result\n  result = np.zeros(result_shape, dtype=np.float32)\n\n  # Get predictions from each teacher\n  for teacher_id in xrange(nb_teachers):\n    # Compute path of checkpoint file for teacher model with ID teacher_id\n    if FLAGS.deeper:\n      ckpt_path = FLAGS.teachers_dir + \'/\' + str(dataset) + \'_\' + str(nb_teachers) + \'_teachers_\' + str(teacher_id) + \'_deep.ckpt-\' + str(FLAGS.teachers_max_steps - 1) #NOLINT(long-line)\n    else:\n      ckpt_path = FLAGS.teachers_dir + \'/\' + str(dataset) + \'_\' + str(nb_teachers) + \'_teachers_\' + str(teacher_id) + \'.ckpt-\' + str(FLAGS.teachers_max_steps - 1)  # NOLINT(long-line)\n\n    # Get predictions on our training data and store in result array\n    result[teacher_id] = deep_cnn.softmax_preds(stdnt_data, ckpt_path)\n\n    # This can take a while when there are a lot of teachers so output status\n    print(""Computed Teacher "" + str(teacher_id) + "" softmax predictions"")\n\n  return result\n\n\ndef prepare_student_data(dataset, nb_teachers, save=False):\n  """"""\n  Takes a dataset name and the size of the teacher ensemble and prepares\n  training data for the student model, according to parameters indicated\n  in flags above.\n  :param dataset: string corresponding to mnist, cifar10, or svhn\n  :param nb_teachers: number of teachers (in the ensemble) to learn from\n  :param save: if set to True, will dump student training labels predicted by\n               the ensemble of teachers (with Laplacian noise) as npy files.\n               It also dumps the clean votes for each class (without noise) and\n               the labels assigned by teachers\n  :return: pairs of (data, labels) to be used for student training and testing\n  """"""\n  assert input.create_dir_if_needed(FLAGS.train_dir)\n\n  # Load the dataset\n  if dataset == \'svhn\':\n    test_data, test_labels = input.ld_svhn(test_only=True)\n  elif dataset == \'cifar10\':\n    test_data, test_labels = input.ld_cifar10(test_only=True)\n  elif dataset == \'mnist\':\n    test_data, test_labels = input.ld_mnist(test_only=True)\n  else:\n    print(""Check value of dataset flag"")\n    return False\n\n  # Make sure there is data leftover to be used as a test set\n  assert FLAGS.stdnt_share < len(test_data)\n\n  # Prepare [unlabeled] student training data (subset of test set)\n  stdnt_data = test_data[:FLAGS.stdnt_share]\n\n  # Compute teacher predictions for student training data\n  teachers_preds = ensemble_preds(dataset, nb_teachers, stdnt_data)\n\n  # Aggregate teacher predictions to get student training labels\n  if not save:\n    stdnt_labels = aggregation.noisy_max(teachers_preds, FLAGS.lap_scale)\n  else:\n    # Request clean votes and clean labels as well\n    stdnt_labels, clean_votes, labels_for_dump = aggregation.noisy_max(teachers_preds, FLAGS.lap_scale, return_clean_votes=True) #NOLINT(long-line)\n\n    # Prepare filepath for numpy dump of clean votes\n    filepath = FLAGS.data_dir + ""/"" + str(dataset) + \'_\' + str(nb_teachers) + \'_student_clean_votes_lap_\' + str(FLAGS.lap_scale) + \'.npy\'  # NOLINT(long-line)\n\n    # Prepare filepath for numpy dump of clean labels\n    filepath_labels = FLAGS.data_dir + ""/"" + str(dataset) + \'_\' + str(nb_teachers) + \'_teachers_labels_lap_\' + str(FLAGS.lap_scale) + \'.npy\'  # NOLINT(long-line)\n\n    # Dump clean_votes array\n    with tf.gfile.Open(filepath, mode=\'w\') as file_obj:\n      np.save(file_obj, clean_votes)\n\n    # Dump labels_for_dump array\n    with tf.gfile.Open(filepath_labels, mode=\'w\') as file_obj:\n      np.save(file_obj, labels_for_dump)\n\n  # Print accuracy of aggregated labels\n  ac_ag_labels = metrics.accuracy(stdnt_labels, test_labels[:FLAGS.stdnt_share])\n  print(""Accuracy of the aggregated labels: "" + str(ac_ag_labels))\n\n  # Store unused part of test set for use as a test set after student training\n  stdnt_test_data = test_data[FLAGS.stdnt_share:]\n  stdnt_test_labels = test_labels[FLAGS.stdnt_share:]\n\n  if save:\n    # Prepare filepath for numpy dump of labels produced by noisy aggregation\n    filepath = FLAGS.data_dir + ""/"" + str(dataset) + \'_\' + str(nb_teachers) + \'_student_labels_lap_\' + str(FLAGS.lap_scale) + \'.npy\' #NOLINT(long-line)\n\n    # Dump student noisy labels array\n    with tf.gfile.Open(filepath, mode=\'w\') as file_obj:\n      np.save(file_obj, stdnt_labels)\n\n  return stdnt_data, stdnt_labels, stdnt_test_data, stdnt_test_labels\n\n\ndef train_student(dataset, nb_teachers):\n  """"""\n  This function trains a student using predictions made by an ensemble of\n  teachers. The student and teacher models are trained using the same\n  neural network architecture.\n  :param dataset: string corresponding to mnist, cifar10, or svhn\n  :param nb_teachers: number of teachers (in the ensemble) to learn from\n  :return: True if student training went well\n  """"""\n  assert input.create_dir_if_needed(FLAGS.train_dir)\n\n  # Call helper function to prepare student data using teacher predictions\n  stdnt_dataset = prepare_student_data(dataset, nb_teachers, save=True)\n\n  # Unpack the student dataset\n  stdnt_data, stdnt_labels, stdnt_test_data, stdnt_test_labels = stdnt_dataset\n\n  # Prepare checkpoint filename and path\n  if FLAGS.deeper:\n    ckpt_path = FLAGS.train_dir + \'/\' + str(dataset) + \'_\' + str(nb_teachers) + \'_student_deeper.ckpt\' #NOLINT(long-line)\n  else:\n    ckpt_path = FLAGS.train_dir + \'/\' + str(dataset) + \'_\' + str(nb_teachers) + \'_student.ckpt\'  # NOLINT(long-line)\n\n  # Start student training\n  assert deep_cnn.train(stdnt_data, stdnt_labels, ckpt_path)\n\n  # Compute final checkpoint name for student (with max number of steps)\n  ckpt_path_final = ckpt_path + \'-\' + str(FLAGS.max_steps - 1)\n\n  # Compute student label predictions on remaining chunk of test set\n  student_preds = deep_cnn.softmax_preds(stdnt_test_data, ckpt_path_final)\n\n  # Compute teacher accuracy\n  precision = metrics.accuracy(student_preds, stdnt_test_labels)\n  print(\'Precision of student after training: \' + str(precision))\n\n  return True\n\ndef main(argv=None): # pylint: disable=unused-argument\n  # Run student training according to values specified in flags\n  assert train_student(FLAGS.dataset, FLAGS.nb_teachers)\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/differential_privacy/multiple_teachers/train_teachers.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom differential_privacy.multiple_teachers import deep_cnn\nfrom differential_privacy.multiple_teachers import input\nfrom differential_privacy.multiple_teachers import metrics\n\n\ntf.flags.DEFINE_string(\'dataset\', \'svhn\', \'The name of the dataset to use\')\ntf.flags.DEFINE_integer(\'nb_labels\', 10, \'Number of output classes\')\n\ntf.flags.DEFINE_string(\'data_dir\',\'/tmp\',\'Temporary storage\')\ntf.flags.DEFINE_string(\'train_dir\',\'/tmp/train_dir\',\n                       \'Where model ckpt are saved\')\n\ntf.flags.DEFINE_integer(\'max_steps\', 3000, \'Number of training steps to run.\')\ntf.flags.DEFINE_integer(\'nb_teachers\', 50, \'Teachers in the ensemble.\')\ntf.flags.DEFINE_integer(\'teacher_id\', 0, \'ID of teacher being trained.\')\n\ntf.flags.DEFINE_boolean(\'deeper\', False, \'Activate deeper CNN model\')\n\nFLAGS = tf.flags.FLAGS\n\n\ndef train_teacher(dataset, nb_teachers, teacher_id):\n  """"""\n  This function trains a teacher (teacher id) among an ensemble of nb_teachers\n  models for the dataset specified.\n  :param dataset: string corresponding to dataset (svhn, cifar10)\n  :param nb_teachers: total number of teachers in the ensemble\n  :param teacher_id: id of the teacher being trained\n  :return: True if everything went well\n  """"""\n  # If working directories do not exist, create them\n  assert input.create_dir_if_needed(FLAGS.data_dir)\n  assert input.create_dir_if_needed(FLAGS.train_dir)\n\n  # Load the dataset\n  if dataset == \'svhn\':\n    train_data,train_labels,test_data,test_labels = input.ld_svhn(extended=True)\n  elif dataset == \'cifar10\':\n    train_data, train_labels, test_data, test_labels = input.ld_cifar10()\n  elif dataset == \'mnist\':\n    train_data, train_labels, test_data, test_labels = input.ld_mnist()\n  else:\n    print(""Check value of dataset flag"")\n    return False\n    \n  # Retrieve subset of data for this teacher\n  data, labels = input.partition_dataset(train_data, \n                                         train_labels, \n                                         nb_teachers, \n                                         teacher_id)\n\n  print(""Length of training data: "" + str(len(labels)))\n\n  # Define teacher checkpoint filename and full path\n  if FLAGS.deeper:\n    filename = str(nb_teachers) + \'_teachers_\' + str(teacher_id) + \'_deep.ckpt\'\n  else:\n    filename = str(nb_teachers) + \'_teachers_\' + str(teacher_id) + \'.ckpt\'\n  ckpt_path = FLAGS.train_dir + \'/\' + str(dataset) + \'_\' + filename\n\n  # Perform teacher training\n  assert deep_cnn.train(data, labels, ckpt_path)\n\n  # Append final step value to checkpoint for evaluation\n  ckpt_path_final = ckpt_path + \'-\' + str(FLAGS.max_steps - 1)\n\n  # Retrieve teacher probability estimates on the test data\n  teacher_preds = deep_cnn.softmax_preds(test_data, ckpt_path_final)\n\n  # Compute teacher accuracy\n  precision = metrics.accuracy(teacher_preds, test_labels)\n  print(\'Precision of teacher after training: \' + str(precision))\n\n  return True\n\n\ndef main(argv=None):  # pylint: disable=unused-argument\n  # Make a call to train_teachers with values specified in flags\n  assert train_teacher(FLAGS.dataset, FLAGS.nb_teachers, FLAGS.teacher_id)\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/differential_privacy/multiple_teachers/utils.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\ndef batch_indices(batch_nb, data_length, batch_size):\n  """"""\n  This helper function computes a batch start and end index\n  :param batch_nb: the batch number\n  :param data_length: the total length of the data being parsed by batches\n  :param batch_size: the number of inputs in each batch\n  :return: pair of (start, end) indices\n  """"""\n  # Batch start and end index\n  start = int(batch_nb * batch_size)\n  end = int((batch_nb + 1) * batch_size)\n\n  # When there are not enough inputs left, we reuse some to complete the batch\n  if end > data_length:\n    shift = end - data_length\n    start -= shift\n    end -= shift\n\n  return start, end\n'"
model_zoo/models/im2txt/im2txt/configuration.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image-to-text model and training configurations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass ModelConfig(object):\n  """"""Wrapper class for model hyperparameters.""""""\n\n  def __init__(self):\n    """"""Sets the default model hyperparameters.""""""\n    # File pattern of sharded TFRecord file containing SequenceExample protos.\n    # Must be provided in training and evaluation modes.\n    self.input_file_pattern = None\n\n    # Image format (""jpeg"" or ""png"").\n    self.image_format = ""jpeg""\n\n    # Approximate number of values per input shard. Used to ensure sufficient\n    # mixing between shards in training.\n    self.values_per_input_shard = 2300\n    # Minimum number of shards to keep in the input queue.\n    self.input_queue_capacity_factor = 2\n    # Number of threads for prefetching SequenceExample protos.\n    self.num_input_reader_threads = 1\n\n    # Name of the SequenceExample context feature containing image data.\n    self.image_feature_name = ""image/data""\n    # Name of the SequenceExample feature list containing integer captions.\n    self.caption_feature_name = ""image/caption_ids""\n\n    # Number of unique words in the vocab (plus 1, for <UNK>).\n    # The default value is larger than the expected actual vocab size to allow\n    # for differences between tokenizer versions used in preprocessing. There is\n    # no harm in using a value greater than the actual vocab size, but using a\n    # value less than the actual vocab size will result in an error.\n    self.vocab_size = 12000\n\n    # Number of threads for image preprocessing. Should be a multiple of 2.\n    self.num_preprocess_threads = 4\n\n    # Batch size.\n    self.batch_size = 32\n\n    # File containing an Inception v3 checkpoint to initialize the variables\n    # of the Inception model. Must be provided when starting training for the\n    # first time.\n    self.inception_checkpoint_file = None\n\n    # Dimensions of Inception v3 input images.\n    self.image_height = 299\n    self.image_width = 299\n\n    # Scale used to initialize model variables.\n    self.initializer_scale = 0.08\n\n    # LSTM input and output dimensionality, respectively.\n    self.embedding_size = 512\n    self.num_lstm_units = 512\n\n    # If < 1.0, the dropout keep probability applied to LSTM variables.\n    self.lstm_dropout_keep_prob = 0.7\n\n\nclass TrainingConfig(object):\n  """"""Wrapper class for training hyperparameters.""""""\n\n  def __init__(self):\n    """"""Sets the default training hyperparameters.""""""\n    # Number of examples per epoch of training data.\n    self.num_examples_per_epoch = 586363\n\n    # Optimizer for training the model.\n    self.optimizer = ""SGD""\n\n    # Learning rate for the initial phase of training.\n    self.initial_learning_rate = 2.0\n    self.learning_rate_decay_factor = 0.5\n    self.num_epochs_per_decay = 8.0\n\n    # Learning rate when fine tuning the Inception v3 parameters.\n    self.train_inception_learning_rate = 0.0005\n\n    # If not None, clip gradients to this value.\n    self.clip_gradients = 5.0\n\n    # How many model checkpoints to keep.\n    self.max_checkpoints_to_keep = 5\n'"
model_zoo/models/im2txt/im2txt/evaluate.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Evaluate the model.\n\nThis script should be run concurrently with training so that summaries show up\nin TensorBoard.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os.path\nimport time\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom im2txt import configuration\nfrom im2txt import show_and_tell_model\n\nFLAGS = tf.flags.FLAGS\n\ntf.flags.DEFINE_string(""input_file_pattern"", """",\n                       ""File pattern of sharded TFRecord input files."")\ntf.flags.DEFINE_string(""checkpoint_dir"", """",\n                       ""Directory containing model checkpoints."")\ntf.flags.DEFINE_string(""eval_dir"", """", ""Directory to write event logs."")\n\ntf.flags.DEFINE_integer(""eval_interval_secs"", 600,\n                        ""Interval between evaluation runs."")\ntf.flags.DEFINE_integer(""num_eval_examples"", 10132,\n                        ""Number of examples for evaluation."")\n\ntf.flags.DEFINE_integer(""min_global_step"", 5000,\n                        ""Minimum global step to run evaluation."")\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef evaluate_model(sess, model, global_step, summary_writer, summary_op):\n  """"""Computes perplexity-per-word over the evaluation dataset.\n\n  Summaries and perplexity-per-word are written out to the eval directory.\n\n  Args:\n    sess: Session object.\n    model: Instance of ShowAndTellModel; the model to evaluate.\n    global_step: Integer; global step of the model checkpoint.\n    summary_writer: Instance of SummaryWriter.\n    summary_op: Op for generating model summaries.\n  """"""\n  # Log model summaries on a single batch.\n  summary_str = sess.run(summary_op)\n  summary_writer.add_summary(summary_str, global_step)\n\n  # Compute perplexity over the entire dataset.\n  num_eval_batches = int(\n      math.ceil(FLAGS.num_eval_examples / model.config.batch_size))\n\n  start_time = time.time()\n  sum_losses = 0.\n  sum_weights = 0.\n  for i in xrange(num_eval_batches):\n    cross_entropy_losses, weights = sess.run([\n        model.target_cross_entropy_losses,\n        model.target_cross_entropy_loss_weights\n    ])\n    sum_losses += np.sum(cross_entropy_losses * weights)\n    sum_weights += np.sum(weights)\n    if not i % 100:\n      tf.logging.info(""Computed losses for %d of %d batches."", i + 1,\n                      num_eval_batches)\n  eval_time = time.time() - start_time\n\n  perplexity = math.exp(sum_losses / sum_weights)\n  tf.logging.info(""Perplexity = %f (%.2g sec)"", perplexity, eval_time)\n\n  # Log perplexity to the SummaryWriter.\n  summary = tf.Summary()\n  value = summary.value.add()\n  value.simple_value = perplexity\n  value.tag = ""Perplexity""\n  summary_writer.add_summary(summary, global_step)\n\n  # Write the Events file to the eval directory.\n  summary_writer.flush()\n  tf.logging.info(""Finished processing evaluation at global step %d."",\n                  global_step)\n\n\ndef run_once(model, saver, summary_writer, summary_op):\n  """"""Evaluates the latest model checkpoint.\n\n  Args:\n    model: Instance of ShowAndTellModel; the model to evaluate.\n    saver: Instance of tf.train.Saver for restoring model Variables.\n    summary_writer: Instance of SummaryWriter.\n    summary_op: Op for generating model summaries.\n  """"""\n  model_path = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n  if not model_path:\n    tf.logging.info(""Skipping evaluation. No checkpoint found in: %s"",\n                    FLAGS.checkpoint_dir)\n    return\n\n  with tf.Session() as sess:\n    # Load model from checkpoint.\n    tf.logging.info(""Loading model from checkpoint: %s"", model_path)\n    saver.restore(sess, model_path)\n    global_step = tf.train.global_step(sess, model.global_step.name)\n    tf.logging.info(""Successfully loaded %s at global step = %d."",\n                    os.path.basename(model_path), global_step)\n    if global_step < FLAGS.min_global_step:\n      tf.logging.info(""Skipping evaluation. Global step = %d < %d"", global_step,\n                      FLAGS.min_global_step)\n      return\n\n    # Start the queue runners.\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n\n    # Run evaluation on the latest checkpoint.\n    try:\n      evaluate_model(\n          sess=sess,\n          model=model,\n          global_step=global_step,\n          summary_writer=summary_writer,\n          summary_op=summary_op)\n    except Exception, e:  # pylint: disable=broad-except\n      tf.logging.error(""Evaluation failed."")\n      coord.request_stop(e)\n\n    coord.request_stop()\n    coord.join(threads, stop_grace_period_secs=10)\n\n\ndef run():\n  """"""Runs evaluation in a loop, and logs summaries to TensorBoard.""""""\n  # Create the evaluation directory if it doesn\'t exist.\n  eval_dir = FLAGS.eval_dir\n  if not tf.gfile.IsDirectory(eval_dir):\n    tf.logging.info(""Creating eval directory: %s"", eval_dir)\n    tf.gfile.MakeDirs(eval_dir)\n\n  g = tf.Graph()\n  with g.as_default():\n    # Build the model for evaluation.\n    model_config = configuration.ModelConfig()\n    model_config.input_file_pattern = FLAGS.input_file_pattern\n    model = show_and_tell_model.ShowAndTellModel(model_config, mode=""eval"")\n    model.build()\n\n    # Create the Saver to restore model Variables.\n    saver = tf.train.Saver()\n\n    # Create the summary operation and the summary writer.\n    summary_op = tf.merge_all_summaries()\n    summary_writer = tf.train.SummaryWriter(eval_dir)\n\n    g.finalize()\n\n    # Run a new evaluation run every eval_interval_secs.\n    while True:\n      start = time.time()\n      tf.logging.info(""Starting evaluation at "" + time.strftime(\n          ""%Y-%m-%d-%H:%M:%S"", time.localtime()))\n      run_once(model, saver, summary_writer, summary_op)\n      time_to_next_eval = start + FLAGS.eval_interval_secs - time.time()\n      if time_to_next_eval > 0:\n        time.sleep(time_to_next_eval)\n\n\ndef main(unused_argv):\n  assert FLAGS.input_file_pattern, ""--input_file_pattern is required""\n  assert FLAGS.checkpoint_dir, ""--checkpoint_dir is required""\n  assert FLAGS.eval_dir, ""--eval_dir is required""\n  run()\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
model_zoo/models/im2txt/im2txt/inference_wrapper.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Model wrapper class for performing inference with a ShowAndTellModel.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom im2txt import show_and_tell_model\nfrom im2txt.inference_utils import inference_wrapper_base\n\n\nclass InferenceWrapper(inference_wrapper_base.InferenceWrapperBase):\n  """"""Model wrapper class for performing inference with a ShowAndTellModel.""""""\n\n  def __init__(self):\n    super(InferenceWrapper, self).__init__()\n\n  def build_model(self, model_config):\n    model = show_and_tell_model.ShowAndTellModel(model_config, mode=""inference"")\n    model.build()\n    return model\n\n  def feed_image(self, sess, encoded_image):\n    initial_state = sess.run(fetches=""lstm/initial_state:0"",\n                             feed_dict={""image_feed:0"": encoded_image})\n    return initial_state\n\n  def inference_step(self, sess, input_feed, state_feed):\n    softmax_output, state_output = sess.run(\n        fetches=[""softmax:0"", ""lstm/state:0""],\n        feed_dict={\n            ""input_feed:0"": input_feed,\n            ""lstm/state_feed:0"": state_feed,\n        })\n    return softmax_output, state_output, None\n'"
model_zoo/models/im2txt/im2txt/run_inference.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Generate captions for images using default beam search parameters.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\n\n\nimport tensorflow as tf\n\nfrom im2txt import configuration\nfrom im2txt import inference_wrapper\nfrom im2txt.inference_utils import caption_generator\nfrom im2txt.inference_utils import vocabulary\n\nFLAGS = tf.flags.FLAGS\n\ntf.flags.DEFINE_string(""checkpoint_path"", """",\n                       ""Model checkpoint file or directory containing a ""\n                       ""model checkpoint file."")\ntf.flags.DEFINE_string(""vocab_file"", """", ""Text file containing the vocabulary."")\ntf.flags.DEFINE_string(""input_files"", """",\n                       ""File pattern or comma-separated list of file patterns ""\n                       ""of image files."")\n\n\ndef main(_):\n  # Build the inference graph.\n  g = tf.Graph()\n  with g.as_default():\n    model = inference_wrapper.InferenceWrapper()\n    restore_fn = model.build_graph_from_config(configuration.ModelConfig(),\n                                               FLAGS.checkpoint_path)\n  g.finalize()\n\n  # Create the vocabulary.\n  vocab = vocabulary.Vocabulary(FLAGS.vocab_file)\n\n  filenames = []\n  for file_pattern in FLAGS.input_files.split("",""):\n    filenames.extend(tf.gfile.Glob(file_pattern))\n  tf.logging.info(""Running caption generation on %d files matching %s"",\n                  len(filenames), FLAGS.input_files)\n\n  with tf.Session(graph=g) as sess:\n    # Load the model from checkpoint.\n    restore_fn(sess)\n\n    # Prepare the caption generator. Here we are implicitly using the default\n    # beam search parameters. See caption_generator.py for a description of the\n    # available beam search parameters.\n    generator = caption_generator.CaptionGenerator(model, vocab)\n\n    for filename in filenames:\n      with tf.gfile.GFile(filename, ""r"") as f:\n        image = f.read()\n      captions = generator.beam_search(sess, image)\n      print(""Captions for image %s:"" % os.path.basename(filename))\n      for i, caption in enumerate(captions):\n        # Ignore begin and end words.\n        sentence = [vocab.id_to_word(w) for w in caption.sentence[1:-1]]\n        sentence = "" "".join(sentence)\n        print(""  %d) %s (p=%f)"" % (i, sentence, math.exp(caption.logprob)))\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
model_zoo/models/im2txt/im2txt/show_and_tell_model.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image-to-text implementation based on http://arxiv.org/abs/1411.4555.\n\n""Show and Tell: A Neural Image Caption Generator""\nOriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom im2txt.ops import image_embedding\nfrom im2txt.ops import image_processing\nfrom im2txt.ops import inputs as input_ops\n\n\nclass ShowAndTellModel(object):\n  """"""Image-to-text implementation based on http://arxiv.org/abs/1411.4555.\n\n  ""Show and Tell: A Neural Image Caption Generator""\n  Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan\n  """"""\n\n  def __init__(self, config, mode, train_inception=False):\n    """"""Basic setup.\n\n    Args:\n      config: Object containing configuration parameters.\n      mode: ""train"", ""eval"" or ""inference"".\n      train_inception: Whether the inception submodel variables are trainable.\n    """"""\n    assert mode in [""train"", ""eval"", ""inference""]\n    self.config = config\n    self.mode = mode\n    self.train_inception = train_inception\n\n    # Reader for the input data.\n    self.reader = tf.TFRecordReader()\n\n    # To match the ""Show and Tell"" paper we initialize all variables with a\n    # random uniform initializer.\n    self.initializer = tf.random_uniform_initializer(\n        minval=-self.config.initializer_scale,\n        maxval=self.config.initializer_scale)\n\n    # A float32 Tensor with shape [batch_size, height, width, channels].\n    self.images = None\n\n    # An int32 Tensor with shape [batch_size, padded_length].\n    self.input_seqs = None\n\n    # An int32 Tensor with shape [batch_size, padded_length].\n    self.target_seqs = None\n\n    # An int32 0/1 Tensor with shape [batch_size, padded_length].\n    self.input_mask = None\n\n    # A float32 Tensor with shape [batch_size, embedding_size].\n    self.image_embeddings = None\n\n    # A float32 Tensor with shape [batch_size, padded_length, embedding_size].\n    self.seq_embeddings = None\n\n    # A float32 scalar Tensor; the total loss for the trainer to optimize.\n    self.total_loss = None\n\n    # A float32 Tensor with shape [batch_size * padded_length].\n    self.target_cross_entropy_losses = None\n\n    # A float32 Tensor with shape [batch_size * padded_length].\n    self.target_cross_entropy_loss_weights = None\n\n    # Collection of variables from the inception submodel.\n    self.inception_variables = []\n\n    # Function to restore the inception submodel from checkpoint.\n    self.init_fn = None\n\n    # Global step Tensor.\n    self.global_step = None\n\n  def is_training(self):\n    """"""Returns true if the model is built for training mode.""""""\n    return self.mode == ""train""\n\n  def process_image(self, encoded_image, thread_id=0):\n    """"""Decodes and processes an image string.\n\n    Args:\n      encoded_image: A scalar string Tensor; the encoded image.\n      thread_id: Preprocessing thread id used to select the ordering of color\n        distortions.\n\n    Returns:\n      A float32 Tensor of shape [height, width, 3]; the processed image.\n    """"""\n    return image_processing.process_image(encoded_image,\n                                          is_training=self.is_training(),\n                                          height=self.config.image_height,\n                                          width=self.config.image_width,\n                                          thread_id=thread_id,\n                                          image_format=self.config.image_format)\n\n  def build_inputs(self):\n    """"""Input prefetching, preprocessing and batching.\n\n    Outputs:\n      self.images\n      self.input_seqs\n      self.target_seqs (training and eval only)\n      self.input_mask (training and eval only)\n    """"""\n    if self.mode == ""inference"":\n      # In inference mode, images and inputs are fed via placeholders.\n      image_feed = tf.placeholder(dtype=tf.string, shape=[], name=""image_feed"")\n      input_feed = tf.placeholder(dtype=tf.int64,\n                                  shape=[None],  # batch_size\n                                  name=""input_feed"")\n\n      # Process image and insert batch dimensions.\n      images = tf.expand_dims(self.process_image(image_feed), 0)\n      input_seqs = tf.expand_dims(input_feed, 1)\n\n      # No target sequences or input mask in inference mode.\n      target_seqs = None\n      input_mask = None\n    else:\n      # Prefetch serialized SequenceExample protos.\n      input_queue = input_ops.prefetch_input_data(\n          self.reader,\n          self.config.input_file_pattern,\n          is_training=self.is_training(),\n          batch_size=self.config.batch_size,\n          values_per_shard=self.config.values_per_input_shard,\n          input_queue_capacity_factor=self.config.input_queue_capacity_factor,\n          num_reader_threads=self.config.num_input_reader_threads)\n\n      # Image processing and random distortion. Split across multiple threads\n      # with each thread applying a slightly different distortion.\n      assert self.config.num_preprocess_threads % 2 == 0\n      images_and_captions = []\n      for thread_id in range(self.config.num_preprocess_threads):\n        serialized_sequence_example = input_queue.dequeue()\n        encoded_image, caption = input_ops.parse_sequence_example(\n            serialized_sequence_example,\n            image_feature=self.config.image_feature_name,\n            caption_feature=self.config.caption_feature_name)\n        image = self.process_image(encoded_image, thread_id=thread_id)\n        images_and_captions.append([image, caption])\n\n      # Batch inputs.\n      queue_capacity = (2 * self.config.num_preprocess_threads *\n                        self.config.batch_size)\n      images, input_seqs, target_seqs, input_mask = (\n          input_ops.batch_with_dynamic_pad(images_and_captions,\n                                           batch_size=self.config.batch_size,\n                                           queue_capacity=queue_capacity))\n\n    self.images = images\n    self.input_seqs = input_seqs\n    self.target_seqs = target_seqs\n    self.input_mask = input_mask\n\n  def build_image_embeddings(self):\n    """"""Builds the image model subgraph and generates image embeddings.\n\n    Inputs:\n      self.images\n\n    Outputs:\n      self.image_embeddings\n    """"""\n    inception_output = image_embedding.inception_v3(\n        self.images,\n        trainable=self.train_inception,\n        is_training=self.is_training())\n    self.inception_variables = tf.get_collection(\n        tf.GraphKeys.VARIABLES, scope=""InceptionV3"")\n\n    # Map inception output into embedding space.\n    with tf.variable_scope(""image_embedding"") as scope:\n      image_embeddings = tf.contrib.layers.fully_connected(\n          inputs=inception_output,\n          num_outputs=self.config.embedding_size,\n          activation_fn=None,\n          weights_initializer=self.initializer,\n          biases_initializer=None,\n          scope=scope)\n\n    # Save the embedding size in the graph.\n    tf.constant(self.config.embedding_size, name=""embedding_size"")\n\n    self.image_embeddings = image_embeddings\n\n  def build_seq_embeddings(self):\n    """"""Builds the input sequence embeddings.\n\n    Inputs:\n      self.input_seqs\n\n    Outputs:\n      self.seq_embeddings\n    """"""\n    with tf.variable_scope(""seq_embedding""), tf.device(""/cpu:0""):\n      embedding_map = tf.get_variable(\n          name=""map"",\n          shape=[self.config.vocab_size, self.config.embedding_size],\n          initializer=self.initializer)\n      seq_embeddings = tf.nn.embedding_lookup(embedding_map, self.input_seqs)\n\n    self.seq_embeddings = seq_embeddings\n\n  def build_model(self):\n    """"""Builds the model.\n\n    Inputs:\n      self.image_embeddings\n      self.seq_embeddings\n      self.target_seqs (training and eval only)\n      self.input_mask (training and eval only)\n\n    Outputs:\n      self.total_loss (training and eval only)\n      self.target_cross_entropy_losses (training and eval only)\n      self.target_cross_entropy_loss_weights (training and eval only)\n    """"""\n    # This LSTM cell has biases and outputs tanh(new_c) * sigmoid(o), but the\n    # modified LSTM in the ""Show and Tell"" paper has no biases and outputs\n    # new_c * sigmoid(o).\n    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(\n        num_units=self.config.num_lstm_units, state_is_tuple=True)\n    if self.mode == ""train"":\n      lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n          lstm_cell,\n          input_keep_prob=self.config.lstm_dropout_keep_prob,\n          output_keep_prob=self.config.lstm_dropout_keep_prob)\n\n    with tf.variable_scope(""lstm"", initializer=self.initializer) as lstm_scope:\n      # Feed the image embeddings to set the initial LSTM state.\n      zero_state = lstm_cell.zero_state(\n          batch_size=self.image_embeddings.get_shape()[0], dtype=tf.float32)\n      _, initial_state = lstm_cell(self.image_embeddings, zero_state)\n\n      # Allow the LSTM variables to be reused.\n      lstm_scope.reuse_variables()\n\n      if self.mode == ""inference"":\n        # In inference mode, use concatenated states for convenient feeding and\n        # fetching.\n        tf.concat(1, initial_state, name=""initial_state"")\n\n        # Placeholder for feeding a batch of concatenated states.\n        state_feed = tf.placeholder(dtype=tf.float32,\n                                    shape=[None, sum(lstm_cell.state_size)],\n                                    name=""state_feed"")\n        state_tuple = tf.split(1, 2, state_feed)\n\n        # Run a single LSTM step.\n        lstm_outputs, state_tuple = lstm_cell(\n            inputs=tf.squeeze(self.seq_embeddings, squeeze_dims=[1]),\n            state=state_tuple)\n\n        # Concatentate the resulting state.\n        tf.concat(1, state_tuple, name=""state"")\n      else:\n        # Run the batch of sequence embeddings through the LSTM.\n        sequence_length = tf.reduce_sum(self.input_mask, 1)\n        lstm_outputs, _ = tf.nn.dynamic_rnn(cell=lstm_cell,\n                                            inputs=self.seq_embeddings,\n                                            sequence_length=sequence_length,\n                                            initial_state=initial_state,\n                                            dtype=tf.float32,\n                                            scope=lstm_scope)\n\n    # Stack batches vertically.\n    lstm_outputs = tf.reshape(lstm_outputs, [-1, lstm_cell.output_size])\n\n    with tf.variable_scope(""logits"") as logits_scope:\n      logits = tf.contrib.layers.fully_connected(\n          inputs=lstm_outputs,\n          num_outputs=self.config.vocab_size,\n          activation_fn=None,\n          weights_initializer=self.initializer,\n          scope=logits_scope)\n\n    if self.mode == ""inference"":\n      tf.nn.softmax(logits, name=""softmax"")\n    else:\n      targets = tf.reshape(self.target_seqs, [-1])\n      weights = tf.to_float(tf.reshape(self.input_mask, [-1]))\n\n      # Compute losses.\n      losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, targets)\n      batch_loss = tf.div(tf.reduce_sum(tf.mul(losses, weights)),\n                          tf.reduce_sum(weights),\n                          name=""batch_loss"")\n      tf.contrib.losses.add_loss(batch_loss)\n      total_loss = tf.contrib.losses.get_total_loss()\n\n      # Add summaries.\n      tf.scalar_summary(""batch_loss"", batch_loss)\n      tf.scalar_summary(""total_loss"", total_loss)\n      for var in tf.trainable_variables():\n        tf.histogram_summary(var.op.name, var)\n\n      self.total_loss = total_loss\n      self.target_cross_entropy_losses = losses  # Used in evaluation.\n      self.target_cross_entropy_loss_weights = weights  # Used in evaluation.\n\n  def setup_inception_initializer(self):\n    """"""Sets up the function to restore inception variables from checkpoint.""""""\n    if self.mode != ""inference"":\n      # Restore inception variables only.\n      saver = tf.train.Saver(self.inception_variables)\n\n      def restore_fn(sess):\n        tf.logging.info(""Restoring Inception variables from checkpoint file %s"",\n                        self.config.inception_checkpoint_file)\n        saver.restore(sess, self.config.inception_checkpoint_file)\n\n      self.init_fn = restore_fn\n\n  def setup_global_step(self):\n    """"""Sets up the global step Tensor.""""""\n    global_step = tf.Variable(\n        initial_value=0,\n        name=""global_step"",\n        trainable=False,\n        collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.VARIABLES])\n\n    self.global_step = global_step\n\n  def build(self):\n    """"""Creates all ops for training and evaluation.""""""\n    self.build_inputs()\n    self.build_image_embeddings()\n    self.build_seq_embeddings()\n    self.build_model()\n    self.setup_inception_initializer()\n    self.setup_global_step()\n'"
model_zoo/models/im2txt/im2txt/show_and_tell_model_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for tensorflow_models.im2txt.show_and_tell_model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom im2txt import configuration\nfrom im2txt import show_and_tell_model\n\n\nclass ShowAndTellModel(show_and_tell_model.ShowAndTellModel):\n  """"""Subclass of ShowAndTellModel without the disk I/O.""""""\n\n  def build_inputs(self):\n    if self.mode == ""inference"":\n      # Inference mode doesn\'t read from disk, so defer to parent.\n      return super(ShowAndTellModel, self).build_inputs()\n    else:\n      # Replace disk I/O with random Tensors.\n      self.images = tf.random_uniform(\n          shape=[self.config.batch_size, self.config.image_height,\n                 self.config.image_width, 3],\n          minval=-1,\n          maxval=1)\n      self.input_seqs = tf.random_uniform(\n          [self.config.batch_size, 15],\n          minval=0,\n          maxval=self.config.vocab_size,\n          dtype=tf.int64)\n      self.target_seqs = tf.random_uniform(\n          [self.config.batch_size, 15],\n          minval=0,\n          maxval=self.config.vocab_size,\n          dtype=tf.int64)\n      self.input_mask = tf.ones_like(self.input_seqs)\n\n\nclass ShowAndTellModelTest(tf.test.TestCase):\n\n  def setUp(self):\n    super(ShowAndTellModelTest, self).setUp()\n    self._model_config = configuration.ModelConfig()\n\n  def _countModelParameters(self):\n    """"""Counts the number of parameters in the model at top level scope.""""""\n    counter = {}\n    for v in tf.all_variables():\n      name = v.op.name.split(""/"")[0]\n      num_params = v.get_shape().num_elements()\n      assert num_params\n      counter[name] = counter.get(name, 0) + num_params\n    return counter\n\n  def _checkModelParameters(self):\n    """"""Verifies the number of parameters in the model.""""""\n    param_counts = self._countModelParameters()\n    expected_param_counts = {\n        ""InceptionV3"": 21802784,\n        # inception_output_size * embedding_size\n        ""image_embedding"": 1048576,\n        # vocab_size * embedding_size\n        ""seq_embedding"": 6144000,\n        # (embedding_size + num_lstm_units + 1) * 4 * num_lstm_units\n        ""lstm"": 2099200,\n        # (num_lstm_units + 1) * vocab_size\n        ""logits"": 6156000,\n        ""global_step"": 1,\n    }\n    self.assertDictEqual(expected_param_counts, param_counts)\n\n  def _checkOutputs(self, expected_shapes, feed_dict=None):\n    """"""Verifies that the model produces expected outputs.\n\n    Args:\n      expected_shapes: A dict mapping Tensor or Tensor name to expected output\n        shape.\n      feed_dict: Values of Tensors to feed into Session.run().\n    """"""\n    fetches = expected_shapes.keys()\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      outputs = sess.run(fetches, feed_dict)\n\n    for index, output in enumerate(outputs):\n      tensor = fetches[index]\n      expected = expected_shapes[tensor]\n      actual = output.shape\n      if expected != actual:\n        self.fail(""Tensor %s has shape %s (expected %s)."" %\n                  (tensor, actual, expected))\n\n  def testBuildForTraining(self):\n    model = ShowAndTellModel(self._model_config, mode=""train"")\n    model.build()\n\n    self._checkModelParameters()\n\n    expected_shapes = {\n        # [batch_size, image_height, image_width, 3]\n        model.images: (32, 299, 299, 3),\n        # [batch_size, sequence_length]\n        model.input_seqs: (32, 15),\n        # [batch_size, sequence_length]\n        model.target_seqs: (32, 15),\n        # [batch_size, sequence_length]\n        model.input_mask: (32, 15),\n        # [batch_size, embedding_size]\n        model.image_embeddings: (32, 512),\n        # [batch_size, sequence_length, embedding_size]\n        model.seq_embeddings: (32, 15, 512),\n        # Scalar\n        model.total_loss: (),\n        # [batch_size * sequence_length]\n        model.target_cross_entropy_losses: (480,),\n        # [batch_size * sequence_length]\n        model.target_cross_entropy_loss_weights: (480,),\n    }\n    self._checkOutputs(expected_shapes)\n\n  def testBuildForEval(self):\n    model = ShowAndTellModel(self._model_config, mode=""eval"")\n    model.build()\n\n    self._checkModelParameters()\n\n    expected_shapes = {\n        # [batch_size, image_height, image_width, 3]\n        model.images: (32, 299, 299, 3),\n        # [batch_size, sequence_length]\n        model.input_seqs: (32, 15),\n        # [batch_size, sequence_length]\n        model.target_seqs: (32, 15),\n        # [batch_size, sequence_length]\n        model.input_mask: (32, 15),\n        # [batch_size, embedding_size]\n        model.image_embeddings: (32, 512),\n        # [batch_size, sequence_length, embedding_size]\n        model.seq_embeddings: (32, 15, 512),\n        # Scalar\n        model.total_loss: (),\n        # [batch_size * sequence_length]\n        model.target_cross_entropy_losses: (480,),\n        # [batch_size * sequence_length]\n        model.target_cross_entropy_loss_weights: (480,),\n    }\n    self._checkOutputs(expected_shapes)\n\n  def testBuildForInference(self):\n    model = ShowAndTellModel(self._model_config, mode=""inference"")\n    model.build()\n\n    self._checkModelParameters()\n\n    # Test feeding an image to get the initial LSTM state.\n    images_feed = np.random.rand(1, 299, 299, 3)\n    feed_dict = {model.images: images_feed}\n    expected_shapes = {\n        # [batch_size, embedding_size]\n        model.image_embeddings: (1, 512),\n        # [batch_size, 2 * num_lstm_units]\n        ""lstm/initial_state:0"": (1, 1024),\n    }\n    self._checkOutputs(expected_shapes, feed_dict)\n\n    # Test feeding a batch of inputs and LSTM states to get softmax output and\n    # LSTM states.\n    input_feed = np.random.randint(0, 10, size=3)\n    state_feed = np.random.rand(3, 1024)\n    feed_dict = {""input_feed:0"": input_feed, ""lstm/state_feed:0"": state_feed}\n    expected_shapes = {\n        # [batch_size, 2 * num_lstm_units]\n        ""lstm/state:0"": (3, 1024),\n        # [batch_size, vocab_size]\n        ""softmax:0"": (3, 12000),\n    }\n    self._checkOutputs(expected_shapes, feed_dict)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
model_zoo/models/im2txt/im2txt/train.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Train the model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom im2txt import configuration\nfrom im2txt import show_and_tell_model\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.flags.DEFINE_string(""input_file_pattern"", """",\n                       ""File pattern of sharded TFRecord input files."")\ntf.flags.DEFINE_string(""inception_checkpoint_file"", """",\n                       ""Path to a pretrained inception_v3 model."")\ntf.flags.DEFINE_string(""train_dir"", """",\n                       ""Directory for saving and loading model checkpoints."")\ntf.flags.DEFINE_boolean(""train_inception"", False,\n                        ""Whether to train inception submodel variables."")\ntf.flags.DEFINE_integer(""number_of_steps"", 1000000, ""Number of training steps."")\ntf.flags.DEFINE_integer(""log_every_n_steps"", 1,\n                        ""Frequency at which loss and global step are logged."")\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef main(unused_argv):\n  assert FLAGS.input_file_pattern, ""--input_file_pattern is required""\n  assert FLAGS.train_dir, ""--train_dir is required""\n\n  model_config = configuration.ModelConfig()\n  model_config.input_file_pattern = FLAGS.input_file_pattern\n  model_config.inception_checkpoint_file = FLAGS.inception_checkpoint_file\n  training_config = configuration.TrainingConfig()\n\n  # Create training directory.\n  train_dir = FLAGS.train_dir\n  if not tf.gfile.IsDirectory(train_dir):\n    tf.logging.info(""Creating training directory: %s"", train_dir)\n    tf.gfile.MakeDirs(train_dir)\n\n  # Build the TensorFlow graph.\n  g = tf.Graph()\n  with g.as_default():\n    # Build the model.\n    model = show_and_tell_model.ShowAndTellModel(\n        model_config, mode=""train"", train_inception=FLAGS.train_inception)\n    model.build()\n\n    # Set up the learning rate.\n    learning_rate_decay_fn = None\n    if FLAGS.train_inception:\n      learning_rate = tf.constant(training_config.train_inception_learning_rate)\n    else:\n      learning_rate = tf.constant(training_config.initial_learning_rate)\n      if training_config.learning_rate_decay_factor > 0:\n        num_batches_per_epoch = (training_config.num_examples_per_epoch /\n                                 model_config.batch_size)\n        decay_steps = int(num_batches_per_epoch *\n                          training_config.num_epochs_per_decay)\n\n        def _learning_rate_decay_fn(learning_rate, global_step):\n          return tf.train.exponential_decay(\n              learning_rate,\n              global_step,\n              decay_steps=decay_steps,\n              decay_rate=training_config.learning_rate_decay_factor,\n              staircase=True)\n\n        learning_rate_decay_fn = _learning_rate_decay_fn\n\n    # Set up the training ops.\n    train_op = tf.contrib.layers.optimize_loss(\n        loss=model.total_loss,\n        global_step=model.global_step,\n        learning_rate=learning_rate,\n        optimizer=training_config.optimizer,\n        clip_gradients=training_config.clip_gradients,\n        learning_rate_decay_fn=learning_rate_decay_fn)\n\n    # Set up the Saver for saving and restoring model checkpoints.\n    saver = tf.train.Saver(max_to_keep=training_config.max_checkpoints_to_keep)\n\n  # Run training.\n  tf.contrib.slim.learning.train(\n      train_op,\n      train_dir,\n      log_every_n_steps=FLAGS.log_every_n_steps,\n      graph=g,\n      global_step=model.global_step,\n      number_of_steps=FLAGS.number_of_steps,\n      init_fn=model.init_fn,\n      saver=saver)\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
model_zoo/models/inception/inception/dataset.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Small library that points to a data set.\n\nMethods of Data class:\n  data_files: Returns a python list of all (sharded) data set files.\n  num_examples_per_epoch: Returns the number of examples in the data set.\n  num_classes: Returns the number of classes in the data set.\n  reader: Return a reader for a single entry from the data set.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom abc import ABCMeta\nfrom abc import abstractmethod\nimport os\n\n\nimport tensorflow as tf\n\nFLAGS = tf.app.flags.FLAGS\n\n# Basic model parameters.\ntf.app.flags.DEFINE_string(\'data_dir\', \'/tmp/mydata\',\n                           """"""Path to the processed data, i.e. """"""\n                           """"""TFRecord of Example protos."""""")\n\n\nclass Dataset(object):\n  """"""A simple class for handling data sets.""""""\n  __metaclass__ = ABCMeta\n\n  def __init__(self, name, subset):\n    """"""Initialize dataset using a subset and the path to the data.""""""\n    assert subset in self.available_subsets(), self.available_subsets()\n    self.name = name\n    self.subset = subset\n\n  @abstractmethod\n  def num_classes(self):\n    """"""Returns the number of classes in the data set.""""""\n    pass\n    # return 10\n\n  @abstractmethod\n  def num_examples_per_epoch(self):\n    """"""Returns the number of examples in the data subset.""""""\n    pass\n    # if self.subset == \'train\':\n    #   return 10000\n    # if self.subset == \'validation\':\n    #   return 1000\n\n  @abstractmethod\n  def download_message(self):\n    """"""Prints a download message for the Dataset.""""""\n    pass\n\n  def available_subsets(self):\n    """"""Returns the list of available subsets.""""""\n    return [\'train\', \'validation\']\n\n  def data_files(self):\n    """"""Returns a python list of all (sharded) data subset files.\n\n    Returns:\n      python list of all (sharded) data set files.\n    Raises:\n      ValueError: if there are not data_files matching the subset.\n    """"""\n    tf_record_pattern = os.path.join(FLAGS.data_dir, \'%s-*\' % self.subset)\n    data_files = tf.gfile.Glob(tf_record_pattern)\n    if not data_files:\n      print(\'No files found for dataset %s/%s at %s\' % (self.name,\n                                                        self.subset,\n                                                        FLAGS.data_dir))\n\n      self.download_message()\n      exit(-1)\n    return data_files\n\n  def reader(self):\n    """"""Return a reader for a single entry from the data set.\n\n    See io_ops.py for details of Reader class.\n\n    Returns:\n      Reader object that reads the data set.\n    """"""\n    return tf.TFRecordReader()\n'"
model_zoo/models/inception/inception/flowers_data.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Small library that points to the flowers data set.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom inception.dataset import Dataset\n\n\nclass FlowersData(Dataset):\n  """"""Flowers data set.""""""\n\n  def __init__(self, subset):\n    super(FlowersData, self).__init__(\'Flowers\', subset)\n\n  def num_classes(self):\n    """"""Returns the number of classes in the data set.""""""\n    return 5\n\n  def num_examples_per_epoch(self):\n    """"""Returns the number of examples in the data subset.""""""\n    if self.subset == \'train\':\n      return 3170\n    if self.subset == \'validation\':\n      return 500\n\n  def download_message(self):\n    """"""Instruction to download and extract the tarball from Flowers website.""""""\n\n    print(\'Failed to find any Flowers %s files\'% self.subset)\n    print(\'\')\n    print(\'If you have already downloaded and processed the data, then make \'\n          \'sure to set --data_dir to point to the directory containing the \'\n          \'location of the sharded TFRecords.\\n\')\n    print(\'Please see README.md for instructions on how to build \'\n          \'the flowers dataset using download_and_preprocess_flowers.\\n\')\n'"
model_zoo/models/inception/inception/flowers_eval.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A binary to evaluate Inception on the flowers data set.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom inception import inception_eval\nfrom inception.flowers_data import FlowersData\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(unused_argv=None):\n  dataset = FlowersData(subset=FLAGS.subset)\n  assert dataset.data_files()\n  if tf.gfile.Exists(FLAGS.eval_dir):\n    tf.gfile.DeleteRecursively(FLAGS.eval_dir)\n  tf.gfile.MakeDirs(FLAGS.eval_dir)\n  inception_eval.evaluate(dataset)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/inception/inception/flowers_train.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A binary to train Inception on the flowers data set.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nimport tensorflow as tf\n\nfrom inception import inception_train\nfrom inception.flowers_data import FlowersData\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  dataset = FlowersData(subset=FLAGS.subset)\n  assert dataset.data_files()\n  if tf.gfile.Exists(FLAGS.train_dir):\n    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n  tf.gfile.MakeDirs(FLAGS.train_dir)\n  inception_train.train(dataset)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/inception/inception/image_processing.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Read and preprocess image data.\n\n Image processing occurs on a single image at a time. Image are read and\n preprocessed in parallel across multiple threads. The resulting images\n are concatenated together to form a single batch for training or evaluation.\n\n -- Provide processed image data for a network:\n inputs: Construct batches of evaluation examples of images.\n distorted_inputs: Construct batches of training examples of images.\n batch_inputs: Construct batches of training or evaluation examples of images.\n\n -- Data processing:\n parse_example_proto: Parses an Example proto containing a training example\n   of an image.\n\n -- Image decoding:\n decode_jpeg: Decode a JPEG encoded string into a 3-D float32 Tensor.\n\n -- Image preprocessing:\n image_preprocessing: Decode and preprocess one image for evaluation or training\n distort_image: Distort one image for training a network.\n eval_image: Prepare one image for evaluation.\n distort_color: Distort the color in one image for training.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_integer(\'batch_size\', 32,\n                            """"""Number of images to process in a batch."""""")\ntf.app.flags.DEFINE_integer(\'image_size\', 299,\n                            """"""Provide square images of this size."""""")\ntf.app.flags.DEFINE_integer(\'num_preprocess_threads\', 4,\n                            """"""Number of preprocessing threads per tower. """"""\n                            """"""Please make this a multiple of 4."""""")\ntf.app.flags.DEFINE_integer(\'num_readers\', 4,\n                            """"""Number of parallel readers during train."""""")\n\n# Images are preprocessed asynchronously using multiple threads specified by\n# --num_preprocss_threads and the resulting processed images are stored in a\n# random shuffling queue. The shuffling queue dequeues --batch_size images\n# for processing on a given Inception tower. A larger shuffling queue guarantees\n# better mixing across examples within a batch and results in slightly higher\n# predictive performance in a trained model. Empirically,\n# --input_queue_memory_factor=16 works well. A value of 16 implies a queue size\n# of 1024*16 images. Assuming RGB 299x299 images, this implies a queue size of\n# 16GB. If the machine is memory limited, then decrease this factor to\n# decrease the CPU memory footprint, accordingly.\ntf.app.flags.DEFINE_integer(\'input_queue_memory_factor\', 16,\n                            """"""Size of the queue of preprocessed images. """"""\n                            """"""Default is ideal but try smaller values, e.g. """"""\n                            """"""4, 2 or 1, if host memory is constrained. See """"""\n                            """"""comments in code for more details."""""")\n\n\ndef inputs(dataset, batch_size=None, num_preprocess_threads=None):\n  """"""Generate batches of ImageNet images for evaluation.\n\n  Use this function as the inputs for evaluating a network.\n\n  Note that some (minimal) image preprocessing occurs during evaluation\n  including central cropping and resizing of the image to fit the network.\n\n  Args:\n    dataset: instance of Dataset class specifying the dataset.\n    batch_size: integer, number of examples in batch\n    num_preprocess_threads: integer, total number of preprocessing threads but\n      None defaults to FLAGS.num_preprocess_threads.\n\n  Returns:\n    images: Images. 4D tensor of size [batch_size, FLAGS.image_size,\n                                       image_size, 3].\n    labels: 1-D integer Tensor of [FLAGS.batch_size].\n  """"""\n  if not batch_size:\n    batch_size = FLAGS.batch_size\n\n  # Force all input processing onto CPU in order to reserve the GPU for\n  # the forward inference and back-propagation.\n  with tf.device(\'/cpu:0\'):\n    images, labels = batch_inputs(\n        dataset, batch_size, train=False,\n        num_preprocess_threads=num_preprocess_threads,\n        num_readers=1)\n\n  return images, labels\n\n\ndef distorted_inputs(dataset, batch_size=None, num_preprocess_threads=None):\n  """"""Generate batches of distorted versions of ImageNet images.\n\n  Use this function as the inputs for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Args:\n    dataset: instance of Dataset class specifying the dataset.\n    batch_size: integer, number of examples in batch\n    num_preprocess_threads: integer, total number of preprocessing threads but\n      None defaults to FLAGS.num_preprocess_threads.\n\n  Returns:\n    images: Images. 4D tensor of size [batch_size, FLAGS.image_size,\n                                       FLAGS.image_size, 3].\n    labels: 1-D integer Tensor of [batch_size].\n  """"""\n  if not batch_size:\n    batch_size = FLAGS.batch_size\n\n  # Force all input processing onto CPU in order to reserve the GPU for\n  # the forward inference and back-propagation.\n  with tf.device(\'/cpu:0\'):\n    images, labels = batch_inputs(\n        dataset, batch_size, train=True,\n        num_preprocess_threads=num_preprocess_threads,\n        num_readers=FLAGS.num_readers)\n  return images, labels\n\n\ndef decode_jpeg(image_buffer, scope=None):\n  """"""Decode a JPEG string into one 3-D float image Tensor.\n\n  Args:\n    image_buffer: scalar string Tensor.\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor with values ranging from [0, 1).\n  """"""\n  with tf.op_scope([image_buffer], scope, \'decode_jpeg\'):\n    # Decode the string as an RGB JPEG.\n    # Note that the resulting image contains an unknown height and width\n    # that is set dynamically by decode_jpeg. In other words, the height\n    # and width of image is unknown at compile-time.\n    image = tf.image.decode_jpeg(image_buffer, channels=3)\n\n    # After this point, all image pixels reside in [0,1)\n    # until the very end, when they\'re rescaled to (-1, 1).  The various\n    # adjust_* ops all require this range for dtype float.\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\n\ndef distort_color(image, thread_id=0, scope=None):\n  """"""Distort the color of the image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: Tensor containing single image.\n    thread_id: preprocessing thread ID.\n    scope: Optional scope for op_scope.\n  Returns:\n    color-distorted image\n  """"""\n  with tf.op_scope([image], scope, \'distort_color\'):\n    color_ordering = thread_id % 2\n\n    if color_ordering == 0:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n    elif color_ordering == 1:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.2)\n\n    # The random_* ops do not necessarily clamp.\n    image = tf.clip_by_value(image, 0.0, 1.0)\n    return image\n\n\ndef distort_image(image, height, width, bbox, thread_id=0, scope=None):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Args:\n    image: 3-D float Tensor of image\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    thread_id: integer indicating the preprocessing thread.\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training.\n  """"""\n  with tf.op_scope([image, height, width, bbox], scope, \'distort_image\'):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # Display the bounding box in the first thread only.\n    if not thread_id:\n      image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                    bbox)\n      tf.image_summary(\'image_with_bounding_boxes\', image_with_box)\n\n  # A large fraction of image datasets contain a human-annotated bounding\n  # box delineating the region of the image containing the object of interest.\n  # We choose to create a new bounding box for the object which is a randomly\n  # distorted version of the human-annotated bounding box that obeys an allowed\n  # range of aspect ratios, sizes and overlap with the human-annotated\n  # bounding box. If no box is supplied, then we assume the bounding box is\n  # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=0.1,\n        aspect_ratio_range=[0.75, 1.33],\n        area_range=[0.05, 1.0],\n        max_attempts=100,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n    if not thread_id:\n      image_with_distorted_box = tf.image.draw_bounding_boxes(\n          tf.expand_dims(image, 0), distort_bbox)\n      tf.image_summary(\'images_with_distorted_bounding_box\',\n                       image_with_distorted_box)\n\n    # Crop the image to the specified bounding box.\n    distorted_image = tf.slice(image, bbox_begin, bbox_size)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n    resize_method = thread_id % 4\n    distorted_image = tf.image.resize_images(distorted_image, [height, width],\n                                             method=resize_method)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([height, width, 3])\n    if not thread_id:\n      tf.image_summary(\'cropped_resized_image\',\n                       tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors.\n    distorted_image = distort_color(distorted_image, thread_id)\n\n    if not thread_id:\n      tf.image_summary(\'final_distorted_image\',\n                       tf.expand_dims(distorted_image, 0))\n    return distorted_image\n\n\ndef eval_image(image, height, width, scope=None):\n  """"""Prepare one image for evaluation.\n\n  Args:\n    image: 3-D float Tensor\n    height: integer\n    width: integer\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.op_scope([image, height, width], scope, \'eval_image\'):\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    image = tf.image.central_crop(image, central_fraction=0.875)\n\n    # Resize the image to the original height and width.\n    image = tf.expand_dims(image, 0)\n    image = tf.image.resize_bilinear(image, [height, width],\n                                     align_corners=False)\n    image = tf.squeeze(image, [0])\n    return image\n\n\ndef image_preprocessing(image_buffer, bbox, train, thread_id=0):\n  """"""Decode and preprocess one image for evaluation or training.\n\n  Args:\n    image_buffer: JPEG encoded string Tensor\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    train: boolean\n    thread_id: integer indicating preprocessing thread\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n  if bbox is None:\n    raise ValueError(\'Please supply a bounding box.\')\n\n  image = decode_jpeg(image_buffer)\n  height = FLAGS.image_size\n  width = FLAGS.image_size\n\n  if train:\n    image = distort_image(image, height, width, bbox, thread_id)\n  else:\n    image = eval_image(image, height, width)\n\n  # Finally, rescale to [-1,1] instead of [0, 1)\n  image = tf.sub(image, 0.5)\n  image = tf.mul(image, 2.0)\n  return image\n\n\ndef parse_example_proto(example_serialized):\n  """"""Parses an Example proto containing a training example of an image.\n\n  The output of the build_image_data.py image preprocessing script is a dataset\n  containing serialized Example protocol buffers. Each Example proto contains\n  the following fields:\n\n    image/height: 462\n    image/width: 581\n    image/colorspace: \'RGB\'\n    image/channels: 3\n    image/class/label: 615\n    image/class/synset: \'n03623198\'\n    image/class/text: \'knee pad\'\n    image/object/bbox/xmin: 0.1\n    image/object/bbox/xmax: 0.9\n    image/object/bbox/ymin: 0.2\n    image/object/bbox/ymax: 0.6\n    image/object/bbox/label: 615\n    image/format: \'JPEG\'\n    image/filename: \'ILSVRC2012_val_00041207.JPEG\'\n    image/encoded: <JPEG encoded string>\n\n  Args:\n    example_serialized: scalar Tensor tf.string containing a serialized\n      Example protocol buffer.\n\n  Returns:\n    image_buffer: Tensor tf.string containing the contents of a JPEG file.\n    label: Tensor tf.int32 containing the label.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    text: Tensor tf.string containing the human-readable label.\n  """"""\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n      \'image/class/label\': tf.FixedLenFeature([1], dtype=tf.int64,\n                                              default_value=-1),\n      \'image/class/text\': tf.FixedLenFeature([], dtype=tf.string,\n                                             default_value=\'\'),\n  }\n  sparse_float32 = tf.VarLenFeature(dtype=tf.float32)\n  # Sparse features in Example proto.\n  feature_map.update(\n      {k: sparse_float32 for k in [\'image/object/bbox/xmin\',\n                                   \'image/object/bbox/ymin\',\n                                   \'image/object/bbox/xmax\',\n                                   \'image/object/bbox/ymax\']})\n\n  features = tf.parse_single_example(example_serialized, feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n\n  xmin = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n  ymin = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n  xmax = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n  ymax = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n\n  # Note that we impose an ordering of (y, x) just to make life difficult.\n  bbox = tf.concat(0, [ymin, xmin, ymax, xmax])\n\n  # Force the variable number of bounding boxes into the shape\n  # [1, num_boxes, coords].\n  bbox = tf.expand_dims(bbox, 0)\n  bbox = tf.transpose(bbox, [0, 2, 1])\n\n  return features[\'image/encoded\'], label, bbox, features[\'image/class/text\']\n\n\ndef batch_inputs(dataset, batch_size, train, num_preprocess_threads=None,\n                 num_readers=1):\n  """"""Contruct batches of training or evaluation examples from the image dataset.\n\n  Args:\n    dataset: instance of Dataset class specifying the dataset.\n      See dataset.py for details.\n    batch_size: integer\n    train: boolean\n    num_preprocess_threads: integer, total number of preprocessing threads\n    num_readers: integer, number of parallel readers\n\n  Returns:\n    images: 4-D float Tensor of a batch of images\n    labels: 1-D integer Tensor of [batch_size].\n\n  Raises:\n    ValueError: if data is not found\n  """"""\n  with tf.name_scope(\'batch_processing\'):\n    data_files = dataset.data_files()\n    if data_files is None:\n      raise ValueError(\'No data files found for this dataset\')\n\n    # Create filename_queue\n    if train:\n      filename_queue = tf.train.string_input_producer(data_files,\n                                                      shuffle=True,\n                                                      capacity=16)\n    else:\n      filename_queue = tf.train.string_input_producer(data_files,\n                                                      shuffle=False,\n                                                      capacity=1)\n    if num_preprocess_threads is None:\n      num_preprocess_threads = FLAGS.num_preprocess_threads\n\n    if num_preprocess_threads % 4:\n      raise ValueError(\'Please make num_preprocess_threads a multiple \'\n                       \'of 4 (%d % 4 != 0).\', num_preprocess_threads)\n\n    if num_readers is None:\n      num_readers = FLAGS.num_readers\n\n    if num_readers < 1:\n      raise ValueError(\'Please make num_readers at least 1\')\n\n    # Approximate number of examples per shard.\n    examples_per_shard = 1024\n    # Size the random shuffle queue to balance between good global\n    # mixing (more examples) and memory use (fewer examples).\n    # 1 image uses 299*299*3*4 bytes = 1MB\n    # The default input_queue_memory_factor is 16 implying a shuffling queue\n    # size: examples_per_shard * 16 * 1MB = 17.6GB\n    min_queue_examples = examples_per_shard * FLAGS.input_queue_memory_factor\n    if train:\n      examples_queue = tf.RandomShuffleQueue(\n          capacity=min_queue_examples + 3 * batch_size,\n          min_after_dequeue=min_queue_examples,\n          dtypes=[tf.string])\n    else:\n      examples_queue = tf.FIFOQueue(\n          capacity=examples_per_shard + 3 * batch_size,\n          dtypes=[tf.string])\n\n    # Create multiple readers to populate the queue of examples.\n    if num_readers > 1:\n      enqueue_ops = []\n      for _ in range(num_readers):\n        reader = dataset.reader()\n        _, value = reader.read(filename_queue)\n        enqueue_ops.append(examples_queue.enqueue([value]))\n\n      tf.train.queue_runner.add_queue_runner(\n          tf.train.queue_runner.QueueRunner(examples_queue, enqueue_ops))\n      example_serialized = examples_queue.dequeue()\n    else:\n      reader = dataset.reader()\n      _, example_serialized = reader.read(filename_queue)\n\n    images_and_labels = []\n    for thread_id in range(num_preprocess_threads):\n      # Parse a serialized Example proto to extract the image and metadata.\n      image_buffer, label_index, bbox, _ = parse_example_proto(\n          example_serialized)\n      image = image_preprocessing(image_buffer, bbox, train, thread_id)\n      images_and_labels.append([image, label_index])\n\n    images, label_index_batch = tf.train.batch_join(\n        images_and_labels,\n        batch_size=batch_size,\n        capacity=2 * num_preprocess_threads * batch_size)\n\n    # Reshape images into these desired dimensions.\n    height = FLAGS.image_size\n    width = FLAGS.image_size\n    depth = 3\n\n    images = tf.cast(images, tf.float32)\n    images = tf.reshape(images, shape=[batch_size, height, width, depth])\n\n    # Display the training images in the visualizer.\n    tf.image_summary(\'images\', images)\n\n    return images, tf.reshape(label_index_batch, [batch_size])\n'"
model_zoo/models/inception/inception/imagenet_data.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Small library that points to the ImageNet data set.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nfrom inception.dataset import Dataset\n\n\nclass ImagenetData(Dataset):\n  """"""ImageNet data set.""""""\n\n  def __init__(self, subset):\n    super(ImagenetData, self).__init__(\'ImageNet\', subset)\n\n  def num_classes(self):\n    """"""Returns the number of classes in the data set.""""""\n    return 1000\n\n  def num_examples_per_epoch(self):\n    """"""Returns the number of examples in the data set.""""""\n    # Bounding box data consists of 615299 bounding boxes for 544546 images.\n    if self.subset == \'train\':\n      return 1281167\n    if self.subset == \'validation\':\n      return 50000\n\n  def download_message(self):\n    """"""Instruction to download and extract the tarball from Flowers website.""""""\n\n    print(\'Failed to find any ImageNet %s files\'% self.subset)\n    print(\'\')\n    print(\'If you have already downloaded and processed the data, then make \'\n          \'sure to set --data_dir to point to the directory containing the \'\n          \'location of the sharded TFRecords.\\n\')\n    print(\'If you have not downloaded and prepared the ImageNet data in the \'\n          \'TFRecord format, you will need to do this at least once. This \'\n          \'process could take several hours depending on the speed of your \'\n          \'computer and network connection\\n\')\n    print(\'Please see README.md for instructions on how to build \'\n          \'the ImageNet dataset using download_and_preprocess_imagenet.\\n\')\n    print(\'Note that the raw data size is 300 GB and the processed data size \'\n          \'is 150 GB. Please ensure you have at least 500GB disk space.\')\n'"
model_zoo/models/inception/inception/imagenet_distributed_train.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# pylint: disable=line-too-long\n""""""A binary to train Inception in a distributed manner using multiple systems.\n\nPlease see accompanying README.md for details and instructions.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom inception import inception_distributed_train\nfrom inception.imagenet_data import ImagenetData\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(unused_args):\n  assert FLAGS.job_name in [\'ps\', \'worker\'], \'job_name must be ps or worker\'\n\n  # Extract all the hostnames for the ps and worker jobs to construct the\n  # cluster spec.\n  ps_hosts = FLAGS.ps_hosts.split(\',\')\n  worker_hosts = FLAGS.worker_hosts.split(\',\')\n  tf.logging.info(\'PS hosts are: %s\' % ps_hosts)\n  tf.logging.info(\'Worker hosts are: %s\' % worker_hosts)\n\n  cluster_spec = tf.train.ClusterSpec({\'ps\': ps_hosts,\n                                       \'worker\': worker_hosts})\n  server = tf.train.Server(\n      {\'ps\': ps_hosts,\n       \'worker\': worker_hosts},\n      job_name=FLAGS.job_name,\n      task_index=FLAGS.task_id)\n\n  if FLAGS.job_name == \'ps\':\n    # `ps` jobs wait for incoming connections from the workers.\n    server.join()\n  else:\n    # `worker` jobs will actually do the work.\n    dataset = ImagenetData(subset=FLAGS.subset)\n    assert dataset.data_files()\n    # Only the chief checks for or creates train_dir.\n    if FLAGS.task_id == 0:\n      if not tf.gfile.Exists(FLAGS.train_dir):\n        tf.gfile.MakeDirs(FLAGS.train_dir)\n    inception_distributed_train.train(server.target, dataset, cluster_spec)\n\nif __name__ == \'__main__\':\n  tf.logging.set_verbosity(tf.logging.INFO)\n  tf.app.run()\n'"
model_zoo/models/inception/inception/imagenet_eval.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A binary to evaluate Inception on the flowers data set.\n\nNote that using the supplied pre-trained inception checkpoint, the eval should\nachieve:\n  precision @ 1 = 0.7874 recall @ 5 = 0.9436 [50000 examples]\n\nSee the README.md for more details.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom inception import inception_eval\nfrom inception.imagenet_data import ImagenetData\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(unused_argv=None):\n  dataset = ImagenetData(subset=FLAGS.subset)\n  assert dataset.data_files()\n  if tf.gfile.Exists(FLAGS.eval_dir):\n    tf.gfile.DeleteRecursively(FLAGS.eval_dir)\n  tf.gfile.MakeDirs(FLAGS.eval_dir)\n  inception_eval.evaluate(dataset)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/inception/inception/imagenet_train.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A binary to train Inception on the ImageNet data set.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\n\nimport tensorflow as tf\n\nfrom inception import inception_train\nfrom inception.imagenet_data import ImagenetData\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(_):\n  dataset = ImagenetData(subset=FLAGS.subset)\n  assert dataset.data_files()\n  if tf.gfile.Exists(FLAGS.train_dir):\n    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n  tf.gfile.MakeDirs(FLAGS.train_dir)\n  inception_train.train(dataset)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/inception/inception/inception_distributed_train.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A library to train Inception using multiple replicas with synchronous update.\n\nPlease see accompanying README.md for details and instructions.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os.path\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom inception import image_processing\nfrom inception import inception_model as inception\nfrom inception.slim import slim\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'job_name\', \'\', \'One of ""ps"", ""worker""\')\ntf.app.flags.DEFINE_string(\'ps_hosts\', \'\',\n                           """"""Comma-separated list of hostname:port for the """"""\n                           """"""parameter server jobs. e.g. """"""\n                           """"""\'machine1:2222,machine2:1111,machine2:2222\'"""""")\ntf.app.flags.DEFINE_string(\'worker_hosts\', \'\',\n                           """"""Comma-separated list of hostname:port for the """"""\n                           """"""worker jobs. e.g. """"""\n                           """"""\'machine1:2222,machine2:1111,machine2:2222\'"""""")\n\ntf.app.flags.DEFINE_string(\'train_dir\', \'/tmp/imagenet_train\',\n                           """"""Directory where to write event logs """"""\n                           """"""and checkpoint."""""")\ntf.app.flags.DEFINE_integer(\'max_steps\', 1000000, \'Number of batches to run.\')\ntf.app.flags.DEFINE_string(\'subset\', \'train\', \'Either ""train"" or ""validation"".\')\ntf.app.flags.DEFINE_boolean(\'log_device_placement\', False,\n                            \'Whether to log device placement.\')\n\n# Task ID is used to select the chief and also to access the local_step for\n# each replica to check staleness of the gradients in sync_replicas_optimizer.\ntf.app.flags.DEFINE_integer(\n    \'task_id\', 0, \'Task ID of the worker/replica running the training.\')\n\n# More details can be found in the sync_replicas_optimizer class:\n# tensorflow/python/training/sync_replicas_optimizer.py\ntf.app.flags.DEFINE_integer(\'num_replicas_to_aggregate\', -1,\n                            """"""Number of gradients to collect before """"""\n                            """"""updating the parameters."""""")\ntf.app.flags.DEFINE_integer(\'save_interval_secs\', 10 * 60,\n                            \'Save interval seconds.\')\ntf.app.flags.DEFINE_integer(\'save_summaries_secs\', 180,\n                            \'Save summaries interval seconds.\')\n\n# **IMPORTANT**\n# Please note that this learning rate schedule is heavily dependent on the\n# hardware architecture, batch size and any changes to the model architecture\n# specification. Selecting a finely tuned learning rate schedule is an\n# empirical process that requires some experimentation. Please see README.md\n# more guidance and discussion.\n#\n# Learning rate decay factor selected from https://arxiv.org/abs/1604.00981\ntf.app.flags.DEFINE_float(\'initial_learning_rate\', 0.045,\n                          \'Initial learning rate.\')\ntf.app.flags.DEFINE_float(\'num_epochs_per_decay\', 2.0,\n                          \'Epochs after which learning rate decays.\')\ntf.app.flags.DEFINE_float(\'learning_rate_decay_factor\', 0.94,\n                          \'Learning rate decay factor.\')\n\n# Constants dictating the learning rate schedule.\nRMSPROP_DECAY = 0.9                # Decay term for RMSProp.\nRMSPROP_MOMENTUM = 0.9             # Momentum in RMSProp.\nRMSPROP_EPSILON = 1.0              # Epsilon term for RMSProp.\n\n\ndef train(target, dataset, cluster_spec):\n  """"""Train Inception on a dataset for a number of steps.""""""\n  # Number of workers and parameter servers are infered from the workers and ps\n  # hosts string.\n  num_workers = len(cluster_spec.as_dict()[\'worker\'])\n  num_parameter_servers = len(cluster_spec.as_dict()[\'ps\'])\n  # If no value is given, num_replicas_to_aggregate defaults to be the number of\n  # workers.\n  if FLAGS.num_replicas_to_aggregate == -1:\n    num_replicas_to_aggregate = num_workers\n  else:\n    num_replicas_to_aggregate = FLAGS.num_replicas_to_aggregate\n\n  # Both should be greater than 0 in a distributed training.\n  assert num_workers > 0 and num_parameter_servers > 0, (\' num_workers and \'\n                                                         \'num_parameter_servers\'\n                                                         \' must be > 0.\')\n\n  # Choose worker 0 as the chief. Note that any worker could be the chief\n  # but there should be only one chief.\n  is_chief = (FLAGS.task_id == 0)\n\n  # Ops are assigned to worker by default.\n  with tf.device(\'/job:worker/task:%d\' % FLAGS.task_id):\n    # Variables and its related init/assign ops are assigned to ps.\n    with slim.scopes.arg_scope(\n        [slim.variables.variable, slim.variables.global_step],\n        device=slim.variables.VariableDeviceChooser(num_parameter_servers)):\n      # Create a variable to count the number of train() calls. This equals the\n      # number of updates applied to the variables.\n      global_step = slim.variables.global_step()\n\n      # Calculate the learning rate schedule.\n      num_batches_per_epoch = (dataset.num_examples_per_epoch() /\n                               FLAGS.batch_size)\n      # Decay steps need to be divided by the number of replicas to aggregate.\n      decay_steps = int(num_batches_per_epoch * FLAGS.num_epochs_per_decay /\n                        num_replicas_to_aggregate)\n\n      # Decay the learning rate exponentially based on the number of steps.\n      lr = tf.train.exponential_decay(FLAGS.initial_learning_rate,\n                                      global_step,\n                                      decay_steps,\n                                      FLAGS.learning_rate_decay_factor,\n                                      staircase=True)\n      # Add a summary to track the learning rate.\n      tf.scalar_summary(\'learning_rate\', lr)\n\n      # Create an optimizer that performs gradient descent.\n      opt = tf.train.RMSPropOptimizer(lr,\n                                      RMSPROP_DECAY,\n                                      momentum=RMSPROP_MOMENTUM,\n                                      epsilon=RMSPROP_EPSILON)\n\n      images, labels = image_processing.distorted_inputs(\n          dataset,\n          batch_size=FLAGS.batch_size,\n          num_preprocess_threads=FLAGS.num_preprocess_threads)\n\n      # Number of classes in the Dataset label set plus 1.\n      # Label 0 is reserved for an (unused) background class.\n      num_classes = dataset.num_classes() + 1\n      logits = inception.inference(images, num_classes, for_training=True)\n      # Add classification loss.\n      inception.loss(logits, labels)\n\n      # Gather all of the losses including regularization losses.\n      losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)\n      losses += tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n\n      total_loss = tf.add_n(losses, name=\'total_loss\')\n\n      if is_chief:\n        # Compute the moving average of all individual losses and the\n        # total loss.\n        loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n        loss_averages_op = loss_averages.apply(losses + [total_loss])\n\n        # Attach a scalar summmary to all individual losses and the total loss;\n        # do the same for the averaged version of the losses.\n        for l in losses + [total_loss]:\n          loss_name = l.op.name\n          # Name each loss as \'(raw)\' and name the moving average version of the\n          # loss as the original loss name.\n          tf.scalar_summary(loss_name + \' (raw)\', l)\n          tf.scalar_summary(loss_name, loss_averages.average(l))\n\n        # Add dependency to compute loss_averages.\n        with tf.control_dependencies([loss_averages_op]):\n          total_loss = tf.identity(total_loss)\n\n      # Track the moving averages of all trainable variables.\n      # Note that we maintain a \'double-average\' of the BatchNormalization\n      # global statistics.\n      # This is not needed when the number of replicas are small but important\n      # for synchronous distributed training with tens of workers/replicas.\n      exp_moving_averager = tf.train.ExponentialMovingAverage(\n          inception.MOVING_AVERAGE_DECAY, global_step)\n\n      variables_to_average = (\n          tf.trainable_variables() + tf.moving_average_variables())\n\n      # Add histograms for model variables.\n      for var in variables_to_average:\n        tf.histogram_summary(var.op.name, var)\n\n      # Create synchronous replica optimizer.\n      opt = tf.train.SyncReplicasOptimizer(\n          opt,\n          replicas_to_aggregate=num_replicas_to_aggregate,\n          replica_id=FLAGS.task_id,\n          total_num_replicas=num_workers,\n          variable_averages=exp_moving_averager,\n          variables_to_average=variables_to_average)\n\n      batchnorm_updates = tf.get_collection(slim.ops.UPDATE_OPS_COLLECTION)\n      assert batchnorm_updates, \'Batchnorm updates are missing\'\n      batchnorm_updates_op = tf.group(*batchnorm_updates)\n      # Add dependency to compute batchnorm_updates.\n      with tf.control_dependencies([batchnorm_updates_op]):\n        total_loss = tf.identity(total_loss)\n\n      # Compute gradients with respect to the loss.\n      grads = opt.compute_gradients(total_loss)\n\n      # Add histograms for gradients.\n      for grad, var in grads:\n        if grad is not None:\n          tf.histogram_summary(var.op.name + \'/gradients\', grad)\n\n      apply_gradients_op = opt.apply_gradients(grads, global_step=global_step)\n\n      with tf.control_dependencies([apply_gradients_op]):\n        train_op = tf.identity(total_loss, name=\'train_op\')\n\n      # Get chief queue_runners, init_tokens and clean_up_op, which is used to\n      # synchronize replicas.\n      # More details can be found in sync_replicas_optimizer.\n      chief_queue_runners = [opt.get_chief_queue_runner()]\n      init_tokens_op = opt.get_init_tokens_op()\n      clean_up_op = opt.get_clean_up_op()\n\n      # Create a saver.\n      saver = tf.train.Saver()\n\n      # Build the summary operation based on the TF collection of Summaries.\n      summary_op = tf.merge_all_summaries()\n\n      # Build an initialization operation to run below.\n      init_op = tf.initialize_all_variables()\n\n      # We run the summaries in the same thread as the training operations by\n      # passing in None for summary_op to avoid a summary_thread being started.\n      # Running summaries and training operations in parallel could run out of\n      # GPU memory.\n      sv = tf.train.Supervisor(is_chief=is_chief,\n                               logdir=FLAGS.train_dir,\n                               init_op=init_op,\n                               summary_op=None,\n                               global_step=global_step,\n                               saver=saver,\n                               save_model_secs=FLAGS.save_interval_secs)\n\n      tf.logging.info(\'%s Supervisor\' % datetime.now())\n\n      sess_config = tf.ConfigProto(\n          allow_soft_placement=True,\n          log_device_placement=FLAGS.log_device_placement)\n\n      # Get a session.\n      sess = sv.prepare_or_wait_for_session(target, config=sess_config)\n\n      # Start the queue runners.\n      queue_runners = tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS)\n      sv.start_queue_runners(sess, queue_runners)\n      tf.logging.info(\'Started %d queues for processing input data.\',\n                      len(queue_runners))\n\n      if is_chief:\n        sv.start_queue_runners(sess, chief_queue_runners)\n        sess.run(init_tokens_op)\n\n      # Train, checking for Nans. Concurrently run the summary operation at a\n      # specified interval. Note that the summary_op and train_op never run\n      # simultaneously in order to prevent running out of GPU memory.\n      next_summary_time = time.time() + FLAGS.save_summaries_secs\n      while not sv.should_stop():\n        try:\n          start_time = time.time()\n          loss_value, step = sess.run([train_op, global_step])\n          assert not np.isnan(loss_value), \'Model diverged with loss = NaN\'\n          if step > FLAGS.max_steps:\n            break\n          duration = time.time() - start_time\n\n          if step % 30 == 0:\n            examples_per_sec = FLAGS.batch_size / float(duration)\n            format_str = (\'Worker %d: %s: step %d, loss = %.2f\'\n                          \'(%.1f examples/sec; %.3f  sec/batch)\')\n            tf.logging.info(format_str %\n                            (FLAGS.task_id, datetime.now(), step, loss_value,\n                             examples_per_sec, duration))\n\n          # Determine if the summary_op should be run on the chief worker.\n          if is_chief and next_summary_time < time.time():\n            tf.logging.info(\'Running Summary operation on the chief.\')\n            summary_str = sess.run(summary_op)\n            sv.summary_computed(sess, summary_str)\n            tf.logging.info(\'Finished running Summary operation.\')\n\n            # Determine the next time for running the summary.\n            next_summary_time += FLAGS.save_summaries_secs\n        except:\n          if is_chief:\n            tf.logging.info(\'About to execute sync_clean_up_op!\')\n            sess.run(clean_up_op)\n          raise\n\n      # Stop the supervisor.  This also waits for service threads to finish.\n      sv.stop()\n\n      # Save after the training ends.\n      if is_chief:\n        saver.save(sess,\n                   os.path.join(FLAGS.train_dir, \'model.ckpt\'),\n                   global_step=global_step)\n'"
model_zoo/models/inception/inception/inception_eval.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A library to evaluate Inception on a single GPU.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport math\nimport os.path\nimport time\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom inception import image_processing\nfrom inception import inception_model as inception\n\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'eval_dir\', \'/tmp/imagenet_eval\',\n                           """"""Directory where to write event logs."""""")\ntf.app.flags.DEFINE_string(\'checkpoint_dir\', \'/tmp/imagenet_train\',\n                           """"""Directory where to read model checkpoints."""""")\n\n# Flags governing the frequency of the eval.\ntf.app.flags.DEFINE_integer(\'eval_interval_secs\', 60 * 5,\n                            """"""How often to run the eval."""""")\ntf.app.flags.DEFINE_boolean(\'run_once\', False,\n                            """"""Whether to run eval only once."""""")\n\n# Flags governing the data used for the eval.\ntf.app.flags.DEFINE_integer(\'num_examples\', 50000,\n                            """"""Number of examples to run. Note that the eval """"""\n                            """"""ImageNet dataset contains 50000 examples."""""")\ntf.app.flags.DEFINE_string(\'subset\', \'validation\',\n                           """"""Either \'validation\' or \'train\'."""""")\n\n\ndef _eval_once(saver, summary_writer, top_1_op, top_5_op, summary_op):\n  """"""Runs Eval once.\n\n  Args:\n    saver: Saver.\n    summary_writer: Summary writer.\n    top_1_op: Top 1 op.\n    top_5_op: Top 5 op.\n    summary_op: Summary op.\n  """"""\n  with tf.Session() as sess:\n    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n    if ckpt and ckpt.model_checkpoint_path:\n      if os.path.isabs(ckpt.model_checkpoint_path):\n        # Restores from checkpoint with absolute path.\n        saver.restore(sess, ckpt.model_checkpoint_path)\n      else:\n        # Restores from checkpoint with relative path.\n        saver.restore(sess, os.path.join(FLAGS.checkpoint_dir,\n                                         ckpt.model_checkpoint_path))\n\n      # Assuming model_checkpoint_path looks something like:\n      #   /my-favorite-path/imagenet_train/model.ckpt-0,\n      # extract global_step from it.\n      global_step = ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1]\n      print(\'Succesfully loaded model from %s at step=%s.\' %\n            (ckpt.model_checkpoint_path, global_step))\n    else:\n      print(\'No checkpoint file found\')\n      return\n\n    # Start the queue runners.\n    coord = tf.train.Coordinator()\n    try:\n      threads = []\n      for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n        threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n                                         start=True))\n\n      num_iter = int(math.ceil(FLAGS.num_examples / FLAGS.batch_size))\n      # Counts the number of correct predictions.\n      count_top_1 = 0.0\n      count_top_5 = 0.0\n      total_sample_count = num_iter * FLAGS.batch_size\n      step = 0\n\n      print(\'%s: starting evaluation on (%s).\' % (datetime.now(), FLAGS.subset))\n      start_time = time.time()\n      while step < num_iter and not coord.should_stop():\n        top_1, top_5 = sess.run([top_1_op, top_5_op])\n        count_top_1 += np.sum(top_1)\n        count_top_5 += np.sum(top_5)\n        step += 1\n        if step % 20 == 0:\n          duration = time.time() - start_time\n          sec_per_batch = duration / 20.0\n          examples_per_sec = FLAGS.batch_size / sec_per_batch\n          print(\'%s: [%d batches out of %d] (%.1f examples/sec; %.3f\'\n                \'sec/batch)\' % (datetime.now(), step, num_iter,\n                                examples_per_sec, sec_per_batch))\n          start_time = time.time()\n\n      # Compute precision @ 1.\n      precision_at_1 = count_top_1 / total_sample_count\n      recall_at_5 = count_top_5 / total_sample_count\n      print(\'%s: precision @ 1 = %.4f recall @ 5 = %.4f [%d examples]\' %\n            (datetime.now(), precision_at_1, recall_at_5, total_sample_count))\n\n      summary = tf.Summary()\n      summary.ParseFromString(sess.run(summary_op))\n      summary.value.add(tag=\'Precision @ 1\', simple_value=precision_at_1)\n      summary.value.add(tag=\'Recall @ 5\', simple_value=recall_at_5)\n      summary_writer.add_summary(summary, global_step)\n\n    except Exception as e:  # pylint: disable=broad-except\n      coord.request_stop(e)\n\n    coord.request_stop()\n    coord.join(threads, stop_grace_period_secs=10)\n\n\ndef evaluate(dataset):\n  """"""Evaluate model on Dataset for a number of steps.""""""\n  with tf.Graph().as_default():\n    # Get images and labels from the dataset.\n    images, labels = image_processing.inputs(dataset)\n\n    # Number of classes in the Dataset label set plus 1.\n    # Label 0 is reserved for an (unused) background class.\n    num_classes = dataset.num_classes() + 1\n\n    # Build a Graph that computes the logits predictions from the\n    # inference model.\n    logits, _ = inception.inference(images, num_classes)\n\n    # Calculate predictions.\n    top_1_op = tf.nn.in_top_k(logits, labels, 1)\n    top_5_op = tf.nn.in_top_k(logits, labels, 5)\n\n    # Restore the moving average version of the learned variables for eval.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        inception.MOVING_AVERAGE_DECAY)\n    variables_to_restore = variable_averages.variables_to_restore()\n    saver = tf.train.Saver(variables_to_restore)\n\n    # Build the summary operation based on the TF collection of Summaries.\n    summary_op = tf.merge_all_summaries()\n\n    graph_def = tf.get_default_graph().as_graph_def()\n    summary_writer = tf.train.SummaryWriter(FLAGS.eval_dir,\n                                            graph_def=graph_def)\n\n    while True:\n      _eval_once(saver, summary_writer, top_1_op, top_5_op, summary_op)\n      if FLAGS.run_once:\n        break\n      time.sleep(FLAGS.eval_interval_secs)\n'"
model_zoo/models/inception/inception/inception_model.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Build the Inception v3 network on ImageNet data set.\n\nThe Inception v3 architecture is described in http://arxiv.org/abs/1512.00567\n\nSummary of available functions:\n inference: Compute inference on the model inputs to make a prediction\n loss: Compute the loss of the prediction with respect to the labels\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nimport tensorflow as tf\n\nfrom inception.slim import slim\n\nFLAGS = tf.app.flags.FLAGS\n\n# If a model is trained using multiple GPUs, prefix all Op names with tower_name\n# to differentiate the operations. Note that this prefix is removed from the\n# names of the summaries when visualizing a model.\nTOWER_NAME = \'tower\'\n\n# Batch normalization. Constant governing the exponential moving average of\n# the \'global\' mean and variance for all activations.\nBATCHNORM_MOVING_AVERAGE_DECAY = 0.9997\n\n# The decay to use for the moving average.\nMOVING_AVERAGE_DECAY = 0.9999\n\n\ndef inference(images, num_classes, for_training=False, restore_logits=True,\n              scope=None):\n  """"""Build Inception v3 model architecture.\n\n  See here for reference: http://arxiv.org/abs/1512.00567\n\n  Args:\n    images: Images returned from inputs() or distorted_inputs().\n    num_classes: number of classes\n    for_training: If set to `True`, build the inference model for training.\n      Kernels that operate differently for inference during training\n      e.g. dropout, are appropriately configured.\n    restore_logits: whether or not the logits layers should be restored.\n      Useful for fine-tuning a model with different num_classes.\n    scope: optional prefix string identifying the ImageNet tower.\n\n  Returns:\n    Logits. 2-D float Tensor.\n    Auxiliary Logits. 2-D float Tensor of side-head. Used for training only.\n  """"""\n  # Parameters for BatchNorm.\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': BATCHNORM_MOVING_AVERAGE_DECAY,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': 0.001,\n  }\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.ops.conv2d, slim.ops.fc], weight_decay=0.00004):\n    with slim.arg_scope([slim.ops.conv2d],\n                        stddev=0.1,\n                        activation=tf.nn.relu,\n                        batch_norm_params=batch_norm_params):\n      logits, endpoints = slim.inception.inception_v3(\n          images,\n          dropout_keep_prob=0.8,\n          num_classes=num_classes,\n          is_training=for_training,\n          restore_logits=restore_logits,\n          scope=scope)\n\n  # Add summaries for viewing model statistics on TensorBoard.\n  _activation_summaries(endpoints)\n\n  # Grab the logits associated with the side head. Employed during training.\n  auxiliary_logits = endpoints[\'aux_logits\']\n\n  return logits, auxiliary_logits\n\n\ndef loss(logits, labels, batch_size=None):\n  """"""Adds all losses for the model.\n\n  Note the final loss is not returned. Instead, the list of losses are collected\n  by slim.losses. The losses are accumulated in tower_loss() and summed to\n  calculate the total loss.\n\n  Args:\n    logits: List of logits from inference(). Each entry is a 2-D float Tensor.\n    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n            of shape [batch_size]\n    batch_size: integer\n  """"""\n  if not batch_size:\n    batch_size = FLAGS.batch_size\n\n  # Reshape the labels into a dense Tensor of\n  # shape [FLAGS.batch_size, num_classes].\n  sparse_labels = tf.reshape(labels, [batch_size, 1])\n  indices = tf.reshape(tf.range(batch_size), [batch_size, 1])\n  concated = tf.concat(1, [indices, sparse_labels])\n  num_classes = logits[0].get_shape()[-1].value\n  dense_labels = tf.sparse_to_dense(concated,\n                                    [batch_size, num_classes],\n                                    1.0, 0.0)\n\n  # Cross entropy loss for the main softmax prediction.\n  slim.losses.cross_entropy_loss(logits[0],\n                                 dense_labels,\n                                 label_smoothing=0.1,\n                                 weight=1.0)\n\n  # Cross entropy loss for the auxiliary softmax head.\n  slim.losses.cross_entropy_loss(logits[1],\n                                 dense_labels,\n                                 label_smoothing=0.1,\n                                 weight=0.4,\n                                 scope=\'aux_loss\')\n\n\ndef _activation_summary(x):\n  """"""Helper to create summaries for activations.\n\n  Creates a summary that provides a histogram of activations.\n  Creates a summary that measure the sparsity of activations.\n\n  Args:\n    x: Tensor\n  """"""\n  # Remove \'tower_[0-9]/\' from the name in case this is a multi-GPU training\n  # session. This helps the clarity of presentation on tensorboard.\n  tensor_name = re.sub(\'%s_[0-9]*/\' % TOWER_NAME, \'\', x.op.name)\n  tf.histogram_summary(tensor_name + \'/activations\', x)\n  tf.scalar_summary(tensor_name + \'/sparsity\', tf.nn.zero_fraction(x))\n\n\ndef _activation_summaries(endpoints):\n  with tf.name_scope(\'summaries\'):\n    for act in endpoints.values():\n      _activation_summary(act)\n'"
model_zoo/models/inception/inception/inception_train.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A library to train Inception using multiple GPU\'s with synchronous updates.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nfrom datetime import datetime\nimport os.path\nimport re\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom inception import image_processing\nfrom inception import inception_model as inception\nfrom inception.slim import slim\n\nFLAGS = tf.app.flags.FLAGS\n\ntf.app.flags.DEFINE_string(\'train_dir\', \'/tmp/imagenet_train\',\n                           """"""Directory where to write event logs """"""\n                           """"""and checkpoint."""""")\ntf.app.flags.DEFINE_integer(\'max_steps\', 10000000,\n                            """"""Number of batches to run."""""")\ntf.app.flags.DEFINE_string(\'subset\', \'train\',\n                           """"""Either \'train\' or \'validation\'."""""")\n\n# Flags governing the hardware employed for running TensorFlow.\ntf.app.flags.DEFINE_integer(\'num_gpus\', 1,\n                            """"""How many GPUs to use."""""")\ntf.app.flags.DEFINE_boolean(\'log_device_placement\', False,\n                            """"""Whether to log device placement."""""")\n\n# Flags governing the type of training.\ntf.app.flags.DEFINE_boolean(\'fine_tune\', False,\n                            """"""If set, randomly initialize the final layer """"""\n                            """"""of weights in order to train the network on a """"""\n                            """"""new task."""""")\ntf.app.flags.DEFINE_string(\'pretrained_model_checkpoint_path\', \'\',\n                           """"""If specified, restore this pretrained model """"""\n                           """"""before beginning any training."""""")\n\n# **IMPORTANT**\n# Please note that this learning rate schedule is heavily dependent on the\n# hardware architecture, batch size and any changes to the model architecture\n# specification. Selecting a finely tuned learning rate schedule is an\n# empirical process that requires some experimentation. Please see README.md\n# more guidance and discussion.\n#\n# With 8 Tesla K40\'s and a batch size = 256, the following setup achieves\n# precision@1 = 73.5% after 100 hours and 100K steps (20 epochs).\n# Learning rate decay factor selected from http://arxiv.org/abs/1404.5997.\ntf.app.flags.DEFINE_float(\'initial_learning_rate\', 0.1,\n                          """"""Initial learning rate."""""")\ntf.app.flags.DEFINE_float(\'num_epochs_per_decay\', 30.0,\n                          """"""Epochs after which learning rate decays."""""")\ntf.app.flags.DEFINE_float(\'learning_rate_decay_factor\', 0.16,\n                          """"""Learning rate decay factor."""""")\n\n# Constants dictating the learning rate schedule.\nRMSPROP_DECAY = 0.9                # Decay term for RMSProp.\nRMSPROP_MOMENTUM = 0.9             # Momentum in RMSProp.\nRMSPROP_EPSILON = 1.0              # Epsilon term for RMSProp.\n\n\ndef _tower_loss(images, labels, num_classes, scope):\n  """"""Calculate the total loss on a single tower running the ImageNet model.\n\n  We perform \'batch splitting\'. This means that we cut up a batch across\n  multiple GPU\'s. For instance, if the batch size = 32 and num_gpus = 2,\n  then each tower will operate on an batch of 16 images.\n\n  Args:\n    images: Images. 4D tensor of size [batch_size, FLAGS.image_size,\n                                       FLAGS.image_size, 3].\n    labels: 1-D integer Tensor of [batch_size].\n    num_classes: number of classes\n    scope: unique prefix string identifying the ImageNet tower, e.g.\n      \'tower_0\'.\n\n  Returns:\n     Tensor of shape [] containing the total loss for a batch of data\n  """"""\n  # When fine-tuning a model, we do not restore the logits but instead we\n  # randomly initialize the logits. The number of classes in the output of the\n  # logit is the number of classes in specified Dataset.\n  restore_logits = not FLAGS.fine_tune\n\n  # Build inference Graph.\n  logits = inception.inference(images, num_classes, for_training=True,\n                               restore_logits=restore_logits,\n                               scope=scope)\n\n  # Build the portion of the Graph calculating the losses. Note that we will\n  # assemble the total_loss using a custom function below.\n  split_batch_size = images.get_shape().as_list()[0]\n  inception.loss(logits, labels, batch_size=split_batch_size)\n\n  # Assemble all of the losses for the current tower only.\n  losses = tf.get_collection(slim.losses.LOSSES_COLLECTION, scope)\n\n  # Calculate the total loss for the current tower.\n  regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n  total_loss = tf.add_n(losses + regularization_losses, name=\'total_loss\')\n\n  # Compute the moving average of all individual losses and the total loss.\n  loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n  loss_averages_op = loss_averages.apply(losses + [total_loss])\n\n  # Attach a scalar summmary to all individual losses and the total loss; do the\n  # same for the averaged version of the losses.\n  for l in losses + [total_loss]:\n    # Remove \'tower_[0-9]/\' from the name in case this is a multi-GPU training\n    # session. This helps the clarity of presentation on TensorBoard.\n    loss_name = re.sub(\'%s_[0-9]*/\' % inception.TOWER_NAME, \'\', l.op.name)\n    # Name each loss as \'(raw)\' and name the moving average version of the loss\n    # as the original loss name.\n    tf.scalar_summary(loss_name +\' (raw)\', l)\n    tf.scalar_summary(loss_name, loss_averages.average(l))\n\n  with tf.control_dependencies([loss_averages_op]):\n    total_loss = tf.identity(total_loss)\n  return total_loss\n\n\ndef _average_gradients(tower_grads):\n  """"""Calculate the average gradient for each shared variable across all towers.\n\n  Note that this function provides a synchronization point across all towers.\n\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over individual gradients. The inner list is over the gradient\n      calculation for each tower.\n  Returns:\n     List of pairs of (gradient, variable) where the gradient has been averaged\n     across all towers.\n  """"""\n  average_grads = []\n  for grad_and_vars in zip(*tower_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n    grads = []\n    for g, _ in grad_and_vars:\n      # Add 0 dimension to the gradients to represent the tower.\n      expanded_g = tf.expand_dims(g, 0)\n\n      # Append on a \'tower\' dimension which we will average over below.\n      grads.append(expanded_g)\n\n    # Average over the \'tower\' dimension.\n    grad = tf.concat(0, grads)\n    grad = tf.reduce_mean(grad, 0)\n\n    # Keep in mind that the Variables are redundant because they are shared\n    # across towers. So .. we will just return the first tower\'s pointer to\n    # the Variable.\n    v = grad_and_vars[0][1]\n    grad_and_var = (grad, v)\n    average_grads.append(grad_and_var)\n  return average_grads\n\n\ndef train(dataset):\n  """"""Train on dataset for a number of steps.""""""\n  with tf.Graph().as_default(), tf.device(\'/cpu:0\'):\n    # Create a variable to count the number of train() calls. This equals the\n    # number of batches processed * FLAGS.num_gpus.\n    global_step = tf.get_variable(\n        \'global_step\', [],\n        initializer=tf.constant_initializer(0), trainable=False)\n\n    # Calculate the learning rate schedule.\n    num_batches_per_epoch = (dataset.num_examples_per_epoch() /\n                             FLAGS.batch_size)\n    decay_steps = int(num_batches_per_epoch * FLAGS.num_epochs_per_decay)\n\n    # Decay the learning rate exponentially based on the number of steps.\n    lr = tf.train.exponential_decay(FLAGS.initial_learning_rate,\n                                    global_step,\n                                    decay_steps,\n                                    FLAGS.learning_rate_decay_factor,\n                                    staircase=True)\n\n    # Create an optimizer that performs gradient descent.\n    opt = tf.train.RMSPropOptimizer(lr, RMSPROP_DECAY,\n                                    momentum=RMSPROP_MOMENTUM,\n                                    epsilon=RMSPROP_EPSILON)\n\n    # Get images and labels for ImageNet and split the batch across GPUs.\n    assert FLAGS.batch_size % FLAGS.num_gpus == 0, (\n        \'Batch size must be divisible by number of GPUs\')\n    split_batch_size = int(FLAGS.batch_size / FLAGS.num_gpus)\n\n    # Override the number of preprocessing threads to account for the increased\n    # number of GPU towers.\n    num_preprocess_threads = FLAGS.num_preprocess_threads * FLAGS.num_gpus\n    images, labels = image_processing.distorted_inputs(\n        dataset,\n        num_preprocess_threads=num_preprocess_threads)\n\n    input_summaries = copy.copy(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n    # Number of classes in the Dataset label set plus 1.\n    # Label 0 is reserved for an (unused) background class.\n    num_classes = dataset.num_classes() + 1\n    \n     # Split the batch of images and labels for towers.\n    images_splits = tf.split(0, FLAGS.num_gpus, images)\n    labels_splits = tf.split(0, FLAGS.num_gpus, labels)\n\n    # Calculate the gradients for each model tower.\n    tower_grads = []\n    for i in xrange(FLAGS.num_gpus):\n      with tf.device(\'/gpu:%d\' % i):\n        with tf.name_scope(\'%s_%d\' % (inception.TOWER_NAME, i)) as scope:\n          # Force all Variables to reside on the CPU.\n          with slim.arg_scope([slim.variables.variable], device=\'/cpu:0\'):\n            # Calculate the loss for one tower of the ImageNet model. This\n            # function constructs the entire ImageNet model but shares the\n            # variables across all towers.\n            loss = _tower_loss(images_splits[i], labels_splits[i], num_classes,\n                               scope)\n\n          # Reuse variables for the next tower.\n          tf.get_variable_scope().reuse_variables()\n\n          # Retain the summaries from the final tower.\n          summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n\n          # Retain the Batch Normalization updates operations only from the\n          # final tower. Ideally, we should grab the updates from all towers\n          # but these stats accumulate extremely fast so we can ignore the\n          # other stats from the other towers without significant detriment.\n          batchnorm_updates = tf.get_collection(slim.ops.UPDATE_OPS_COLLECTION,\n                                                scope)\n\n          # Calculate the gradients for the batch of data on this ImageNet\n          # tower.\n          grads = opt.compute_gradients(loss)\n\n          # Keep track of the gradients across all towers.\n          tower_grads.append(grads)\n\n    # We must calculate the mean of each gradient. Note that this is the\n    # synchronization point across all towers.\n    grads = _average_gradients(tower_grads)\n\n    # Add a summaries for the input processing and global_step.\n    summaries.extend(input_summaries)\n\n    # Add a summary to track the learning rate.\n    summaries.append(tf.scalar_summary(\'learning_rate\', lr))\n\n    # Add histograms for gradients.\n    for grad, var in grads:\n      if grad is not None:\n        summaries.append(\n            tf.histogram_summary(var.op.name + \'/gradients\', grad))\n\n    # Apply the gradients to adjust the shared variables.\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n    # Add histograms for trainable variables.\n    for var in tf.trainable_variables():\n      summaries.append(tf.histogram_summary(var.op.name, var))\n\n    # Track the moving averages of all trainable variables.\n    # Note that we maintain a ""double-average"" of the BatchNormalization\n    # global statistics. This is more complicated then need be but we employ\n    # this for backward-compatibility with our previous models.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        inception.MOVING_AVERAGE_DECAY, global_step)\n\n    # Another possiblility is to use tf.slim.get_variables().\n    variables_to_average = (tf.trainable_variables() +\n                            tf.moving_average_variables())\n    variables_averages_op = variable_averages.apply(variables_to_average)\n\n    # Group all updates to into a single train op.\n    batchnorm_updates_op = tf.group(*batchnorm_updates)\n    train_op = tf.group(apply_gradient_op, variables_averages_op,\n                        batchnorm_updates_op)\n\n    # Create a saver.\n    saver = tf.train.Saver(tf.all_variables())\n\n    # Build the summary operation from the last tower summaries.\n    summary_op = tf.merge_summary(summaries)\n\n    # Build an initialization operation to run below.\n    init = tf.initialize_all_variables()\n\n    # Start running operations on the Graph. allow_soft_placement must be set to\n    # True to build towers on GPU, as some of the ops do not have GPU\n    # implementations.\n    sess = tf.Session(config=tf.ConfigProto(\n        allow_soft_placement=True,\n        log_device_placement=FLAGS.log_device_placement))\n    sess.run(init)\n\n    if FLAGS.pretrained_model_checkpoint_path:\n      assert tf.gfile.Exists(FLAGS.pretrained_model_checkpoint_path)\n      variables_to_restore = tf.get_collection(\n          slim.variables.VARIABLES_TO_RESTORE)\n      restorer = tf.train.Saver(variables_to_restore)\n      restorer.restore(sess, FLAGS.pretrained_model_checkpoint_path)\n      print(\'%s: Pre-trained model restored from %s\' %\n            (datetime.now(), FLAGS.pretrained_model_checkpoint_path))\n\n    # Start the queue runners.\n    tf.train.start_queue_runners(sess=sess)\n\n    summary_writer = tf.train.SummaryWriter(\n        FLAGS.train_dir,\n        graph_def=sess.graph.as_graph_def(add_shapes=True))\n\n    for step in xrange(FLAGS.max_steps):\n      start_time = time.time()\n      _, loss_value = sess.run([train_op, loss])\n      duration = time.time() - start_time\n\n      assert not np.isnan(loss_value), \'Model diverged with loss = NaN\'\n\n      if step % 10 == 0:\n        examples_per_sec = FLAGS.batch_size / float(duration)\n        format_str = (\'%s: step %d, loss = %.2f (%.1f examples/sec; %.3f \'\n                      \'sec/batch)\')\n        print(format_str % (datetime.now(), step, loss_value,\n                            examples_per_sec, duration))\n\n      if step % 100 == 0:\n        summary_str = sess.run(summary_op)\n        summary_writer.add_summary(summary_str, step)\n\n      # Save the model checkpoint periodically.\n      if step % 5000 == 0 or (step + 1) == FLAGS.max_steps:\n        checkpoint_path = os.path.join(FLAGS.train_dir, \'model.ckpt\')\n        saver.save(sess, checkpoint_path, global_step=step)\n'"
model_zoo/models/slim/datasets/__init__.py,0,b'\n'
model_zoo/models/slim/datasets/cifar10.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Cifar10 dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/data/create_cifar10_dataset.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'cifar10_%s.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 50000, \'test\': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [32 x 32 x 3] color image.\',\n    \'label\': \'A single integer between 0 and 9\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if not reader:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(shape=[32, 32, 3]),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
model_zoo/models/slim/datasets/dataset_factory.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A factory-pattern class which returns classification image/label pairs.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets import cifar10\nfrom datasets import flowers\nfrom datasets import imagenet\nfrom datasets import mnist\n\ndatasets_map = {\n    \'cifar10\': cifar10,\n    \'flowers\': flowers,\n    \'imagenet\': imagenet,\n    \'mnist\': mnist,\n}\n\n\ndef get_dataset(name, split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Given a dataset name and a split_name returns a Dataset.\n\n  Args:\n    name: String, the name of the dataset.\n    split_name: A train/test split name.\n    dataset_dir: The directory where the dataset files are stored.\n    file_pattern: The file pattern to use for matching the dataset source files.\n    reader: The subclass of tf.ReaderBase. If left as `None`, then the default\n      reader defined by each dataset is used.\n\n  Returns:\n    A `Dataset` class.\n\n  Raises:\n    ValueError: If the dataset `name` is unknown.\n  """"""\n  if name not in datasets_map:\n    raise ValueError(\'Name of dataset unknown %s\' % name)\n  return datasets_map[name].get_split(\n      split_name,\n      dataset_dir,\n      file_pattern,\n      reader)\n'"
model_zoo/models/slim/datasets/dataset_utils.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains utilities for downloading and converting datasets.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\n\nLABELS_FILENAME = \'labels.txt\'\n\n\ndef int64_feature(values):\n  """"""Returns a TF-Feature of int64s.\n\n  Args:\n    values: A scalar or list of values.\n\n  Returns:\n    a TF-Feature.\n  """"""\n  if not isinstance(values, (tuple, list)):\n    values = [values]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n\n\ndef bytes_feature(values):\n  """"""Returns a TF-Feature of bytes.\n\n  Args:\n    values: A string.\n\n  Returns:\n    a TF-Feature.\n  """"""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n\n\ndef image_to_tfexample(image_data, image_format, height, width, class_id):\n  return tf.train.Example(features=tf.train.Features(feature={\n      \'image/encoded\': bytes_feature(image_data),\n      \'image/format\': bytes_feature(image_format),\n      \'image/class/label\': int64_feature(class_id),\n      \'image/height\': int64_feature(height),\n      \'image/width\': int64_feature(width),\n  }))\n\n\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n  """"""Downloads the `tarball_url` and uncompresses it locally.\n\n  Args:\n    tarball_url: The URL of a tarball file.\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = tarball_url.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  def _progress(count, block_size, total_size):\n    sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n        filename, float(count * block_size) / float(total_size) * 100.0))\n    sys.stdout.flush()\n  filepath, _ = urllib.request.urlretrieve(tarball_url, filepath, _progress)\n  print()\n  statinfo = os.stat(filepath)\n  print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n  tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef write_label_file(labels_to_class_names, dataset_dir,\n                     filename=LABELS_FILENAME):\n  """"""Writes a file with the list of class names.\n\n  Args:\n    labels_to_class_names: A map of (integer) labels to class names.\n    dataset_dir: The directory in which the labels file should be written.\n    filename: The filename where the class names are written.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'w\') as f:\n    for label in labels_to_class_names:\n      class_name = labels_to_class_names[label]\n      f.write(\'%d:%s\\n\' % (label, class_name))\n\n\ndef has_labels(dataset_dir, filename=LABELS_FILENAME):\n  """"""Specifies whether or not the dataset directory contains a label map file.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    `True` if the labels file exists and `False` otherwise.\n  """"""\n  return tf.gfile.Exists(os.path.join(dataset_dir, filename))\n\n\ndef read_label_file(dataset_dir, filename=LABELS_FILENAME):\n  """"""Reads the labels file and returns a mapping from ID to class name.\n\n  Args:\n    dataset_dir: The directory in which the labels file is found.\n    filename: The filename where the class names are written.\n\n  Returns:\n    A map from a label (integer) to class name.\n  """"""\n  labels_filename = os.path.join(dataset_dir, filename)\n  with tf.gfile.Open(labels_filename, \'r\') as f:\n    lines = f.read().decode()\n  lines = lines.split(\'\\n\')\n  lines = filter(None, lines)\n\n  labels_to_class_names = {}\n  for line in lines:\n    index = line.index(\':\')\n    labels_to_class_names[int(line[:index])] = line[index+1:]\n  return labels_to_class_names\n'"
model_zoo/models/slim/datasets/download_and_convert_cifar10.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts cifar10 data to TFRecords of TF-Example protos.\n\nThis module downloads the cifar10 data, uncompresses it, reads the files\nthat make up the cifar10 data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take several minutes to run.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cPickle\nimport os\nimport sys\nimport tarfile\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\n# The URL where the CIFAR data can be downloaded.\n_DATA_URL = \'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\n\n# The number of training files.\n_NUM_TRAIN_FILES = 5\n\n# The height and width of each image.\n_IMAGE_SIZE = 32\n\n# The names of the classes.\n_CLASS_NAMES = [\n    \'airplane\',\n    \'automobile\',\n    \'bird\',\n    \'cat\',\n    \'deer\',\n    \'dog\',\n    \'frog\',\n    \'horse\',\n    \'ship\',\n    \'truck\',\n]\n\n\ndef _add_to_tfrecord(filename, tfrecord_writer, offset=0):\n  """"""Loads data from the cifar10 pickle files and writes files to a TFRecord.\n\n  Args:\n    filename: The filename of the cifar10 pickle file.\n    tfrecord_writer: The TFRecord writer to use for writing.\n    offset: An offset into the absolute number of images previously written.\n\n  Returns:\n    The new offset.\n  """"""\n  with tf.gfile.Open(filename, \'r\') as f:\n    data = cPickle.load(f)\n\n  images = data[\'data\']\n  num_images = images.shape[0]\n\n  images = images.reshape((num_images, 3, 32, 32))\n  labels = data[\'labels\']\n\n  with tf.Graph().as_default():\n    image_placeholder = tf.placeholder(dtype=tf.uint8)\n    encoded_image = tf.image.encode_png(image_placeholder)\n\n    with tf.Session(\'\') as sess:\n\n      for j in range(num_images):\n        sys.stdout.write(\'\\r>> Reading file [%s] image %d/%d\' % (\n            filename, offset + j + 1, offset + num_images))\n        sys.stdout.flush()\n\n        image = np.squeeze(images[j]).transpose((1, 2, 0))\n        label = labels[j]\n\n        png_string = sess.run(encoded_image,\n                              feed_dict={image_placeholder: image})\n\n        example = dataset_utils.image_to_tfexample(\n            png_string, \'png\', _IMAGE_SIZE, _IMAGE_SIZE, label)\n        tfrecord_writer.write(example.SerializeToString())\n\n  return offset + num_images\n\n\ndef _get_output_filename(dataset_dir, split_name):\n  """"""Creates the output filename.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n    split_name: The name of the train/test split.\n\n  Returns:\n    An absolute file path.\n  """"""\n  return \'%s/cifar10_%s.tfrecord\' % (dataset_dir, split_name)\n\n\ndef _download_and_uncompress_dataset(dataset_dir):\n  """"""Downloads cifar10 and uncompresses it locally.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n\n  if not os.path.exists(filepath):\n    def _progress(count, block_size, total_size):\n      sys.stdout.write(\'\\r>> Downloading %s %.1f%%\' % (\n          filename, float(count * block_size) / float(total_size) * 100.0))\n      sys.stdout.flush()\n    filepath, _ = urllib.request.urlretrieve(_DATA_URL, filepath, _progress)\n    print()\n    statinfo = os.stat(filepath)\n    print(\'Successfully downloaded\', filename, statinfo.st_size, \'bytes.\')\n    tarfile.open(filepath, \'r:gz\').extractall(dataset_dir)\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n  tf.gfile.Remove(filepath)\n\n  tmp_dir = os.path.join(dataset_dir, \'cifar-10-batches-py\')\n  tf.gfile.DeleteRecursively(tmp_dir)\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  training_filename = _get_output_filename(dataset_dir, \'train\')\n  testing_filename = _get_output_filename(dataset_dir, \'test\')\n\n  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n\n  # First, process the training data:\n  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n    offset = 0\n    for i in range(_NUM_TRAIN_FILES):\n      filename = os.path.join(dataset_dir,\n                              \'cifar-10-batches-py\',\n                              \'data_batch_%d\' % (i + 1))  # 1-indexed.\n      offset = _add_to_tfrecord(filename, tfrecord_writer, offset)\n\n  # Next, process the testing data:\n  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n    filename = os.path.join(dataset_dir,\n                            \'cifar-10-batches-py\',\n                            \'test_batch\')\n    _add_to_tfrecord(filename, tfrecord_writer)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the Cifar10 dataset!\')\n'"
model_zoo/models/slim/datasets/download_and_convert_flowers.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts Flowers data to TFRecords of TF-Example protos.\n\nThis module downloads the Flowers data, uncompresses it, reads the files\nthat make up the Flowers data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take about a minute to run.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport os\nimport random\nimport sys\n\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\n# The URL where the Flowers data can be downloaded.\n_DATA_URL = \'http://download.tensorflow.org/example_images/flower_photos.tgz\'\n\n# The number of images in the validation set.\n_NUM_VALIDATION = 350\n\n# Seed for repeatability.\n_RANDOM_SEED = 0\n\n# The number of shards per dataset split.\n_NUM_SHARDS = 5\n\n\nclass ImageReader(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def read_image_dims(self, sess, image_data):\n    image = self.decode_jpeg(sess, image_data)\n    return image.shape[0], image.shape[1]\n\n  def decode_jpeg(self, sess, image_data):\n    image = sess.run(self._decode_jpeg,\n                     feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _get_filenames_and_classes(dataset_dir):\n  """"""Returns a list of filenames and inferred class names.\n\n  Args:\n    dataset_dir: A directory containing a set of subdirectories representing\n      class names. Each subdirectory should contain PNG or JPG encoded images.\n\n  Returns:\n    A list of image file paths, relative to `dataset_dir` and the list of\n    subdirectories, representing class names.\n  """"""\n  flower_root = os.path.join(dataset_dir, \'flower_photos\')\n  directories = []\n  class_names = []\n  for filename in os.listdir(flower_root):\n    path = os.path.join(flower_root, filename)\n    if os.path.isdir(path):\n      directories.append(path)\n      class_names.append(filename)\n\n  photo_filenames = []\n  for directory in directories:\n    for filename in os.listdir(directory):\n      path = os.path.join(directory, filename)\n      photo_filenames.append(path)\n\n  return photo_filenames, sorted(class_names)\n\n\ndef _get_dataset_filename(dataset_dir, split_name, shard_id):\n  output_filename = \'flowers_%s_%05d-of-%05d.tfrecord\' % (\n      split_name, shard_id, _NUM_SHARDS)\n  return os.path.join(dataset_dir, output_filename)\n\n\ndef _convert_dataset(split_name, filenames, class_names_to_ids, dataset_dir):\n  """"""Converts the given filenames to a TFRecord dataset.\n\n  Args:\n    split_name: The name of the dataset, either \'train\' or \'validation\'.\n    filenames: A list of absolute paths to png or jpg images.\n    class_names_to_ids: A dictionary from class names (strings) to ids\n      (integers).\n    dataset_dir: The directory where the converted datasets are stored.\n  """"""\n  assert split_name in [\'train\', \'validation\']\n\n  num_per_shard = int(math.ceil(len(filenames) / float(_NUM_SHARDS)))\n\n  with tf.Graph().as_default():\n    image_reader = ImageReader()\n\n    with tf.Session(\'\') as sess:\n\n      for shard_id in range(_NUM_SHARDS):\n        output_filename = _get_dataset_filename(\n            dataset_dir, split_name, shard_id)\n\n        with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n          start_ndx = shard_id * num_per_shard\n          end_ndx = min((shard_id+1) * num_per_shard, len(filenames))\n          for i in range(start_ndx, end_ndx):\n            sys.stdout.write(\'\\r>> Converting image %d/%d shard %d\' % (\n                i+1, len(filenames), shard_id))\n            sys.stdout.flush()\n\n            # Read the filename:\n            image_data = tf.gfile.FastGFile(filenames[i], \'r\').read()\n            height, width = image_reader.read_image_dims(sess, image_data)\n\n            class_name = os.path.basename(os.path.dirname(filenames[i]))\n            class_id = class_names_to_ids[class_name]\n\n            example = dataset_utils.image_to_tfexample(\n                image_data, \'jpg\', height, width, class_id)\n            tfrecord_writer.write(example.SerializeToString())\n\n  sys.stdout.write(\'\\n\')\n  sys.stdout.flush()\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  filename = _DATA_URL.split(\'/\')[-1]\n  filepath = os.path.join(dataset_dir, filename)\n  tf.gfile.Remove(filepath)\n\n  tmp_dir = os.path.join(dataset_dir, \'flower_photos\')\n  tf.gfile.DeleteRecursively(tmp_dir)\n\n\ndef _dataset_exists(dataset_dir):\n  for split_name in [\'train\', \'validation\']:\n    for shard_id in range(_NUM_SHARDS):\n      output_filename = _get_dataset_filename(\n          dataset_dir, split_name, shard_id)\n      if not tf.gfile.Exists(output_filename):\n        return False\n  return True\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  if _dataset_exists(dataset_dir):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)\n  photo_filenames, class_names = _get_filenames_and_classes(dataset_dir)\n  class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n\n  # Divide into train and test:\n  random.seed(_RANDOM_SEED)\n  random.shuffle(photo_filenames)\n  training_filenames = photo_filenames[_NUM_VALIDATION:]\n  validation_filenames = photo_filenames[:_NUM_VALIDATION]\n\n  # First, convert the training and validation sets.\n  _convert_dataset(\'train\', training_filenames, class_names_to_ids,\n                   dataset_dir)\n  _convert_dataset(\'validation\', validation_filenames, class_names_to_ids,\n                   dataset_dir)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the Flowers dataset!\')\n\n'"
model_zoo/models/slim/datasets/download_and_convert_mnist.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nr""""""Downloads and converts MNIST data to TFRecords of TF-Example protos.\n\nThis module downloads the MNIST data, uncompresses it, reads the files\nthat make up the MNIST data and creates two TFRecord datasets: one for train\nand one for test. Each TFRecord dataset is comprised of a set of TF-Example\nprotocol buffers, each of which contain a single image and label.\n\nThe script should take about a minute to run.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport sys\n\nimport numpy as np\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\n# The URLs where the MNIST data can be downloaded.\n_DATA_URL = \'http://yann.lecun.com/exdb/mnist/\'\n_TRAIN_DATA_FILENAME = \'train-images-idx3-ubyte.gz\'\n_TRAIN_LABELS_FILENAME = \'train-labels-idx1-ubyte.gz\'\n_TEST_DATA_FILENAME = \'t10k-images-idx3-ubyte.gz\'\n_TEST_LABELS_FILENAME = \'t10k-labels-idx1-ubyte.gz\'\n\n_IMAGE_SIZE = 28\n_NUM_CHANNELS = 1\n\n# The names of the classes.\n_CLASS_NAMES = [\n    \'zero\',\n    \'one\',\n    \'two\',\n    \'three\',\n    \'four\',\n    \'five\',\n    \'size\',\n    \'seven\',\n    \'eight\',\n    \'nine\',\n]\n\n\ndef _extract_images(filename, num_images):\n  """"""Extract the images into a numpy array.\n\n  Args:\n    filename: The path to an MNIST images file.\n    num_images: The number of images in the file.\n\n  Returns:\n    A numpy array of shape [number_of_images, height, width, channels].\n  """"""\n  print(\'Extracting images from: \', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(16)\n    buf = bytestream.read(\n        _IMAGE_SIZE * _IMAGE_SIZE * num_images * _NUM_CHANNELS)\n    data = np.frombuffer(buf, dtype=np.uint8)\n    data = data.reshape(num_images, _IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\n  return data\n\n\ndef _extract_labels(filename, num_labels):\n  """"""Extract the labels into a vector of int64 label IDs.\n\n  Args:\n    filename: The path to an MNIST labels file.\n    num_labels: The number of labels in the file.\n\n  Returns:\n    A numpy array of shape [number_of_labels]\n  """"""\n  print(\'Extracting labels from: \', filename)\n  with gzip.open(filename) as bytestream:\n    bytestream.read(8)\n    buf = bytestream.read(1 * num_labels)\n    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n  return labels\n\n\ndef _add_to_tfrecord(data_filename, labels_filename, num_images,\n                     tfrecord_writer):\n  """"""Loads data from the binary MNIST files and writes files to a TFRecord.\n\n  Args:\n    data_filename: The filename of the MNIST images.\n    labels_filename: The filename of the MNIST labels.\n    num_images: The number of images in the dataset.\n    tfrecord_writer: The TFRecord writer to use for writing.\n  """"""\n  images = _extract_images(data_filename, num_images)\n  labels = _extract_labels(labels_filename, num_images)\n\n  shape = (_IMAGE_SIZE, _IMAGE_SIZE, _NUM_CHANNELS)\n  with tf.Graph().as_default():\n    image = tf.placeholder(dtype=tf.uint8, shape=shape)\n    encoded_png = tf.image.encode_png(image)\n\n    with tf.Session(\'\') as sess:\n      for j in range(num_images):\n        sys.stdout.write(\'\\r>> Converting image %d/%d\' % (j + 1, num_images))\n        sys.stdout.flush()\n\n        png_string = sess.run(encoded_png, feed_dict={image: images[j]})\n\n        example = dataset_utils.image_to_tfexample(\n            png_string, \'png\', _IMAGE_SIZE, _IMAGE_SIZE, labels[j])\n        tfrecord_writer.write(example.SerializeToString())\n\n\ndef _get_output_filename(dataset_dir, split_name):\n  """"""Creates the output filename.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n    split_name: The name of the train/test split.\n\n  Returns:\n    An absolute file path.\n  """"""\n  return \'%s/mnist_%s.tfrecord\' % (dataset_dir, split_name)\n\n\ndef _download_dataset(dataset_dir):\n  """"""Downloads MNIST locally.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  for filename in [_TRAIN_DATA_FILENAME,\n                   _TRAIN_LABELS_FILENAME,\n                   _TEST_DATA_FILENAME,\n                   _TEST_LABELS_FILENAME]:\n    filepath = os.path.join(dataset_dir, filename)\n\n    if not os.path.exists(filepath):\n      print(\'Downloading file %s...\' % filename)\n      def _progress(count, block_size, total_size):\n        sys.stdout.write(\'\\r>> Downloading %.1f%%\' % (\n            float(count * block_size) / float(total_size) * 100.0))\n        sys.stdout.flush()\n      filepath, _ = urllib.request.urlretrieve(_DATA_URL + filename,\n                                               filepath,\n                                               _progress)\n      print()\n      with tf.gfile.GFile(filepath) as f:\n        size = f.Size()\n      print(\'Successfully downloaded\', filename, size, \'bytes.\')\n\n\ndef _clean_up_temporary_files(dataset_dir):\n  """"""Removes temporary files used to create the dataset.\n\n  Args:\n    dataset_dir: The directory where the temporary files are stored.\n  """"""\n  for filename in [_TRAIN_DATA_FILENAME,\n                   _TRAIN_LABELS_FILENAME,\n                   _TEST_DATA_FILENAME,\n                   _TEST_LABELS_FILENAME]:\n    filepath = os.path.join(dataset_dir, filename)\n    tf.gfile.Remove(filepath)\n\n\ndef run(dataset_dir):\n  """"""Runs the download and conversion operation.\n\n  Args:\n    dataset_dir: The dataset directory where the dataset is stored.\n  """"""\n  if not tf.gfile.Exists(dataset_dir):\n    tf.gfile.MakeDirs(dataset_dir)\n\n  training_filename = _get_output_filename(dataset_dir, \'train\')\n  testing_filename = _get_output_filename(dataset_dir, \'test\')\n\n  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):\n    print(\'Dataset files already exist. Exiting without re-creating them.\')\n    return\n\n  _download_dataset(dataset_dir)\n\n  # First, process the training data:\n  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:\n    data_filename = os.path.join(dataset_dir, _TRAIN_DATA_FILENAME)\n    labels_filename = os.path.join(dataset_dir, _TRAIN_LABELS_FILENAME)\n    _add_to_tfrecord(data_filename, labels_filename, 60000, tfrecord_writer)\n\n  # Next, process the testing data:\n  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:\n    data_filename = os.path.join(dataset_dir, _TEST_DATA_FILENAME)\n    labels_filename = os.path.join(dataset_dir, _TEST_LABELS_FILENAME)\n    _add_to_tfrecord(data_filename, labels_filename, 10000, tfrecord_writer)\n\n  # Finally, write the labels file:\n  labels_to_class_names = dict(zip(range(len(_CLASS_NAMES)), _CLASS_NAMES))\n  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)\n\n  _clean_up_temporary_files(dataset_dir)\n  print(\'\\nFinished converting the MNIST dataset!\')\n'"
model_zoo/models/slim/datasets/flowers.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the Cifar10 dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/data/create_cifar10_dataset.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'flowers_%s_*.tfrecord\'\n\nSPLITS_TO_SIZES = {\'train\': 3320, \'validation\': 350}\n\n_NUM_CLASSES = 5\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying size.\',\n    \'label\': \'A single integer between 0 and 4\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading cifar10.\n\n  Args:\n    split_name: A train/validation split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/validation split.\n  """"""\n  if split_name not in SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'png\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
model_zoo/models/slim/datasets/imagenet.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the ImageNet ILSVRC 2012 Dataset plus some bounding boxes.\n\nSome images have one or more bounding boxes associated with the label of the\nimage. See details here: http://image-net.org/download-bboxes\n\nImageNet is based upon WordNet 3.0. To uniquely identify a synset, we use\n""WordNet ID"" (wnid), which is a concatenation of POS ( i.e. part of speech )\nand SYNSET OFFSET of WordNet. For more information, please refer to the\nWordNet documentation[http://wordnet.princeton.edu/wordnet/documentation/].\n\n""There are bounding boxes for over 3000 popular synsets available.\nFor each synset, there are on average 150 images with bounding boxes.""\n\nWARNING: Don\'t use for object detection, in this case all the bounding boxes\nof the image belong to just one class.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom six.moves import urllib\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n# TODO(nsilberman): Add tfrecord file type once the script is updated.\n_FILE_PATTERN = \'%s-*\'\n\n_SPLITS_TO_SIZES = {\n    \'train\': 1281167,\n    \'validation\': 50000,\n}\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A color image of varying height and width.\',\n    \'label\': \'The label id of the image, integer between 0 and 999\',\n    \'label_text\': \'The text of the label.\',\n    \'object/bbox\': \'A list of bounding boxes.\',\n    \'object/label\': \'A list of labels, one per each object.\',\n}\n\n_NUM_CLASSES = 1001\n\n\ndef create_readable_names_for_imagenet_labels():\n  """"""Create a dict mapping label id to human readable string.\n\n  Returns:\n      labels_to_names: dictionary where keys are integers from to 1000\n      and values are human-readable names.\n\n  We retrieve a synset file, which contains a list of valid synset labels used\n  by ILSVRC competition. There is one synset one per line, eg.\n          #   n01440764\n          #   n01443537\n  We also retrieve a synset_to_human_file, which contains a mapping from synsets\n  to human-readable names for every synset in Imagenet. These are stored in a\n  tsv format, as follows:\n          #   n02119247    black fox\n          #   n02119359    silver fox\n  We assign each synset (in alphabetical order) an integer, starting from 1\n  (since 0 is reserved for the background class).\n\n  Code is based on\n  https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py#L463\n  """"""\n\n  # pylint: disable=g-line-too-long\n  base_url = \'https://raw.githubusercontent.com/tensorflow/models/master/inception/inception/data/\'\n  synset_url = \'{}/imagenet_lsvrc_2015_synsets.txt\'.format(base_url)\n  synset_to_human_url = \'{}/imagenet_metadata.txt\'.format(base_url)\n\n  filename, _ = urllib.request.urlretrieve(synset_url)\n  synset_list = [s.strip() for s in open(filename).readlines()]\n  num_synsets_in_ilsvrc = len(synset_list)\n  assert num_synsets_in_ilsvrc == 1000\n\n  filename, _ = urllib.request.urlretrieve(synset_to_human_url)\n  synset_to_human_list = open(filename).readlines()\n  num_synsets_in_all_imagenet = len(synset_to_human_list)\n  assert num_synsets_in_all_imagenet == 21842\n\n  synset_to_human = {}\n  for s in synset_to_human_list:\n    parts = s.strip().split(\'\\t\')\n    assert len(parts) == 2\n    synset = parts[0]\n    human = parts[1]\n    synset_to_human[synset] = human\n\n  label_index = 1\n  labels_to_names = {0: \'background\'}\n  for synset in synset_list:\n    name = synset_to_human[synset]\n    labels_to_names[label_index] = name\n    label_index += 1\n\n  return labels_to_names\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading ImageNet.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature(\n          (), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature(\n          (), tf.string, default_value=\'jpeg\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [], dtype=tf.int64, default_value=-1),\n      \'image/class/text\': tf.FixedLenFeature(\n          [], dtype=tf.string, default_value=\'\'),\n      \'image/object/bbox/xmin\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/ymin\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/xmax\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/bbox/ymax\': tf.VarLenFeature(\n          dtype=tf.float32),\n      \'image/object/class/label\': tf.VarLenFeature(\n          dtype=tf.int64),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(\'image/encoded\', \'image/format\'),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\'),\n      \'label_text\': slim.tfexample_decoder.Tensor(\'image/class/text\'),\n      \'object/bbox\': slim.tfexample_decoder.BoundingBox(\n          [\'ymin\', \'xmin\', \'ymax\', \'xmax\'], \'image/object/bbox/\'),\n      \'object/label\': slim.tfexample_decoder.Tensor(\'image/object/class/label\'),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n  else:\n    labels_to_names = create_readable_names_for_imagenet_labels()\n    dataset_utils.write_label_file(labels_to_names, dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      num_classes=_NUM_CLASSES,\n      labels_to_names=labels_to_names)\n'"
model_zoo/models/slim/datasets/mnist.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides data for the MNIST dataset.\n\nThe dataset scripts used to create the dataset can be found at:\ntensorflow/models/slim/data/create_mnist_dataset.py\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = \'mnist_%s.tfrecord\'\n\n_SPLITS_TO_SIZES = {\'train\': 60000, \'test\': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    \'image\': \'A [28 x 28 x 1] grayscale image.\',\n    \'label\': \'A single integer between 0 and 9\',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n  """"""Gets a dataset tuple with instructions for reading MNIST.\n\n  Args:\n    split_name: A train/test split name.\n    dataset_dir: The base directory of the dataset sources.\n    file_pattern: The file pattern to use when matching the dataset sources.\n      It is assumed that the pattern contains a \'%s\' string so that the split\n      name can be inserted.\n    reader: The TensorFlow reader type.\n\n  Returns:\n    A `Dataset` namedtuple.\n\n  Raises:\n    ValueError: if `split_name` is not a valid train/test split.\n  """"""\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError(\'split name %s was not recognized.\' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n  # Allowing None in the signature so that dataset_factory can use the default.\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      \'image/encoded\': tf.FixedLenFeature((), tf.string, default_value=\'\'),\n      \'image/format\': tf.FixedLenFeature((), tf.string, default_value=\'raw\'),\n      \'image/class/label\': tf.FixedLenFeature(\n          [1], tf.int64, default_value=tf.zeros([1], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      \'image\': slim.tfexample_decoder.Image(shape=[28, 28, 1], channels=1),\n      \'label\': slim.tfexample_decoder.Tensor(\'image/class/label\', shape=[]),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      num_classes=_NUM_CLASSES,\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      labels_to_names=labels_to_names)\n'"
model_zoo/models/slim/deployment/__init__.py,0,b'\n'
model_zoo/models/slim/deployment/model_deploy.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Deploy Slim models across multiple clones and replicas.\n\n# TODO(sguada) docstring paragraph by (a) motivating the need for the file and\n# (b) defining clones.\n\n# TODO(sguada) describe the high-level components of model deployment.\n# E.g. ""each model deployment is composed of several parts: a DeploymentConfig,\n# which captures A, B and C, an input_fn which loads data.. etc\n\nTo easily train a model on multiple GPUs or across multiple machines this\nmodule provides a set of helper functions: `create_clones`,\n`optimize_clones` and `deploy`.\n\nUsage:\n\n  g = tf.Graph()\n\n  # Set up DeploymentConfig\n  config = model_deploy.DeploymentConfig(num_clones=2, clone_on_cpu=True)\n\n  # Create the global step on the device storing the variables.\n  with tf.device(config.variables_device()):\n    global_step = slim.create_global_step()\n\n  # Define the inputs\n  with tf.device(config.inputs_device()):\n    images, labels = LoadData(...)\n    inputs_queue = slim.data.prefetch_queue((images, labels))\n\n  # Define the optimizer.\n  with tf.device(config.optimizer_device()):\n    optimizer = tf.train.MomentumOptimizer(FLAGS.learning_rate, FLAGS.momentum)\n\n  # Define the model including the loss.\n  def model_fn(inputs_queue):\n    images, labels = inputs_queue.dequeue()\n    predictions = CreateNetwork(images)\n    slim.losses.log_loss(predictions, labels)\n\n  model_dp = model_deploy.deploy(config, model_fn, [inputs_queue],\n                                 optimizer=optimizer)\n\n  # Run training.\n  slim.learning.train(model_dp.train_op, my_log_dir,\n                      summary_op=model_dp.summary_op)\n\nThe Clone namedtuple holds together the values associated with each call to\nmodel_fn:\n  * outputs: The return values of the calls to `model_fn()`.\n  * scope: The scope used to create the clone.\n  * device: The device used to create the clone.\n\nDeployedModel namedtuple, holds together the values needed to train multiple\nclones:\n  * train_op: An operation that run the optimizer training op and include\n    all the update ops created by `model_fn`. Present only if an optimizer\n    was specified.\n  * summary_op: An operation that run the summaries created by `model_fn`\n    and process_gradients.\n  * total_loss: A `Tensor` that contains the sum of all losses created by\n    `model_fn` plus the regularization losses.\n  * clones: List of `Clone` tuples returned by `create_clones()`.\n\nDeploymentConfig parameters:\n  * num_clones: Number of model clones to deploy in each replica.\n  * clone_on_cpu: True if clones should be placed on CPU.\n  * replica_id: Integer.  Index of the replica for which the model is\n      deployed.  Usually 0 for the chief replica.\n  * num_replicas: Number of replicas to use.\n  * num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n  * worker_job_name: A name for the worker job.\n  * ps_job_name: A name for the parameter server job.\n\nTODO(sguada):\n  - describe side effect to the graph.\n  - what happens to summaries and update_ops.\n  - which graph collections are altered.\n  - write a tutorial on how to use this.\n  - analyze the possibility of calling deploy more than once.\n\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\nslim = tf.contrib.slim\n\n\n__all__ = [\'create_clones\',\n           \'deploy\',\n           \'optimize_clones\',\n           \'DeployedModel\',\n           \'DeploymentConfig\',\n           \'Clone\',\n          ]\n\n\n# Namedtuple used to represent a clone during deployment.\nClone = collections.namedtuple(\'Clone\',\n                               [\'outputs\',  # Whatever model_fn() returned.\n                                \'scope\',  # The scope used to create it.\n                                \'device\',  # The device used to create.\n                               ])\n\n# Namedtuple used to represent a DeployedModel, returned by deploy().\nDeployedModel = collections.namedtuple(\'DeployedModel\',\n                                       [\'train_op\',  # The `train_op`\n                                        \'summary_op\',  # The `summary_op`\n                                        \'total_loss\',  # The loss `Tensor`\n                                        \'clones\',  # A list of `Clones` tuples.\n                                       ])\n\n# Default parameters for DeploymentConfig\n_deployment_params = {\'num_clones\': 1,\n                      \'clone_on_cpu\': False,\n                      \'replica_id\': 0,\n                      \'num_replicas\': 1,\n                      \'num_ps_tasks\': 0,\n                      \'worker_job_name\': \'worker\',\n                      \'ps_job_name\': \'ps\'}\n\n\ndef create_clones(config, model_fn, args=None, kwargs=None):\n  """"""Creates multiple clones according to config using a `model_fn`.\n\n  The returned values of `model_fn(*args, **kwargs)` are collected along with\n  the scope and device used to created it in a namedtuple\n  `Clone(outputs, scope, device)`\n\n  Note: it is assumed that any loss created by `model_fn` is collected at\n  the tf.GraphKeys.LOSSES collection.\n\n  To recover the losses, summaries or update_ops created by the clone use:\n  ```python\n    losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, clone.scope)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n  ```\n\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n\n  Args:\n    config: A DeploymentConfig object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n\n  Returns:\n    A list of namedtuples `Clone`.\n  """"""\n  clones = []\n  args = args or []\n  kwargs = kwargs or {}\n  with slim.arg_scope([slim.model_variable, slim.variable],\n                      device=config.variables_device()):\n    # Create clones.\n    for i in range(0, config.num_clones):\n      with tf.name_scope(config.clone_scope(i)) as clone_scope:\n        clone_device = config.clone_device(i)\n        with tf.device(clone_device):\n          with tf.variable_scope(tf.get_variable_scope(),\n                                 reuse=True if i > 0 else None):\n            outputs = model_fn(*args, **kwargs)\n          clones.append(Clone(outputs, clone_scope, clone_device))\n  return clones\n\n\ndef _gather_clone_loss(clone, num_clones, regularization_losses):\n  """"""Gather the loss for a single clone.\n\n  Args:\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n\n  Returns:\n    A tensor for the total loss for the clone.  Can be None.\n  """"""\n  # The return value.\n  sum_loss = None\n  # Individual components of the loss that will need summaries.\n  clone_loss = None\n  regularization_loss = None\n  # Compute and aggregate losses on the clone device.\n  with tf.device(clone.device):\n    all_losses = []\n    clone_losses = tf.get_collection(tf.GraphKeys.LOSSES, clone.scope)\n    if clone_losses:\n      clone_loss = tf.add_n(clone_losses, name=\'clone_loss\')\n      if num_clones > 1:\n        clone_loss = tf.div(clone_loss, 1.0 * num_clones,\n                            name=\'scaled_clone_loss\')\n      all_losses.append(clone_loss)\n    if regularization_losses:\n      regularization_loss = tf.add_n(regularization_losses,\n                                     name=\'regularization_loss\')\n      all_losses.append(regularization_loss)\n    if all_losses:\n      sum_loss = tf.add_n(all_losses)\n  # Add the summaries out of the clone device block.\n  if clone_loss is not None:\n    tf.scalar_summary(clone.scope + \'/clone_loss\', clone_loss,\n                      name=\'clone_loss\')\n  if regularization_loss is not None:\n    tf.scalar_summary(\'regularization_loss\', regularization_loss,\n                      name=\'regularization_loss\')\n  return sum_loss\n\n\ndef _optimize_clone(optimizer, clone, num_clones, regularization_losses,\n                    **kwargs):\n  """"""Compute losses and gradients for a single clone.\n\n  Args:\n    optimizer: A tf.Optimizer  object.\n    clone: A Clone namedtuple.\n    num_clones: The number of clones being deployed.\n    regularization_losses: Possibly empty list of regularization_losses\n      to add to the clone losses.\n    **kwargs: Dict of kwarg to pass to compute_gradients().\n\n  Returns:\n    A tuple (clone_loss, clone_grads_and_vars).\n      - clone_loss: A tensor for the total loss for the clone.  Can be None.\n      - clone_grads_and_vars: List of (gradient, variable) for the clone.\n        Can be empty.\n  """"""\n  sum_loss = _gather_clone_loss(clone, num_clones, regularization_losses)\n  clone_grad = None\n  if sum_loss is not None:\n    with tf.device(clone.device):\n      clone_grad = optimizer.compute_gradients(sum_loss, **kwargs)\n  return sum_loss, clone_grad\n\n\ndef optimize_clones(clones, optimizer,\n                    regularization_losses=None,\n                    **kwargs):\n  """"""Compute clone losses and gradients for the given list of `Clones`.\n\n  Note: The regularization_losses are added to the first clone losses.\n\n  Args:\n   clones: List of `Clones` created by `create_clones()`.\n   optimizer: An `Optimizer` object.\n   regularization_losses: Optional list of regularization losses. If None it\n     will gather them from tf.GraphKeys.REGULARIZATION_LOSSES. Pass `[]` to\n     exclude them.\n   **kwargs: Optional list of keyword arguments to pass to `compute_gradients`.\n\n  Returns:\n   A tuple (total_loss, grads_and_vars).\n     - total_loss: A Tensor containing the average of the clone losses including\n       the regularization loss.\n     - grads_and_vars: A List of tuples (gradient, variable) containing the sum\n       of the gradients for each variable.\n\n  """"""\n  grads_and_vars = []\n  clones_losses = []\n  num_clones = len(clones)\n  if regularization_losses is None:\n    regularization_losses = tf.get_collection(\n        tf.GraphKeys.REGULARIZATION_LOSSES)\n  for clone in clones:\n    with tf.name_scope(clone.scope):\n      clone_loss, clone_grad = _optimize_clone(\n          optimizer, clone, num_clones, regularization_losses, **kwargs)\n      if clone_loss is not None:\n        clones_losses.append(clone_loss)\n        grads_and_vars.append(clone_grad)\n      # Only use regularization_losses for the first clone\n      regularization_losses = None\n  # Compute the total_loss summing all the clones_losses.\n  total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n  # Sum the gradients accross clones.\n  grads_and_vars = _sum_clones_gradients(grads_and_vars)\n  return total_loss, grads_and_vars\n\n\ndef deploy(config,\n           model_fn,\n           args=None,\n           kwargs=None,\n           optimizer=None,\n           summarize_gradients=False):\n  """"""Deploys a Slim-constructed model across multiple clones.\n\n  The deployment options are specified by the config object and support\n  deploying one or several clones on different GPUs and one or several replicas\n  of such clones.\n\n  The argument `model_fn` is called `config.num_clones` times to create the\n  model clones as `model_fn(*args, **kwargs)`.\n\n  The optional argument `optimizer` is an `Optimizer` object.  If not `None`,\n  the deployed model is configured for training with that optimizer.\n\n  If `config` specifies deployment on multiple replicas then the default\n  tensorflow device is set appropriatly for each call to `model_fn` and for the\n  slim variable creation functions: model and global variables will be created\n  on the `ps` device, the clone operations will be on the `worker` device.\n\n  Args:\n    config: A `DeploymentConfig` object.\n    model_fn: A callable. Called as `model_fn(*args, **kwargs)`\n    args: Optional list of arguments to pass to `model_fn`.\n    kwargs: Optional list of keyword arguments to pass to `model_fn`.\n    optimizer: Optional `Optimizer` object.  If passed the model is deployed\n      for training with that optimizer.\n    summarize_gradients: Whether or not add summaries to the gradients.\n\n  Returns:\n    A `DeployedModel` namedtuple.\n\n  """"""\n  # Gather initial summaries.\n  summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n  # Create Clones.\n  clones = create_clones(config, model_fn, args, kwargs)\n  first_clone = clones[0]\n\n  # Gather update_ops from the first clone. These contain, for example,\n  # the updates for the batch_norm variables created by model_fn.\n  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone.scope)\n\n  train_op = None\n  total_loss = None\n  with tf.device(config.optimizer_device()):\n    if optimizer:\n      # Place the global step on the device storing the variables.\n      with tf.device(config.variables_device()):\n        global_step = slim.get_or_create_global_step()\n\n      # Compute the gradients for the clones.\n      total_loss, clones_gradients = optimize_clones(clones, optimizer)\n\n      if clones_gradients:\n        if summarize_gradients:\n          # Add summaries to the gradients.\n          summaries |= set(_add_gradients_summaries(clones_gradients))\n\n        # Create gradient updates.\n        grad_updates = optimizer.apply_gradients(clones_gradients,\n                                                 global_step=global_step)\n        update_ops.append(grad_updates)\n\n        update_op = tf.group(*update_ops)\n        train_op = control_flow_ops.with_dependencies([update_op], total_loss,\n                                                      name=\'train_op\')\n    else:\n      clones_losses = []\n      regularization_losses = tf.get_collection(\n          tf.GraphKeys.REGULARIZATION_LOSSES)\n      for clone in clones:\n        with tf.name_scope(clone.scope):\n          clone_loss = _gather_clone_loss(clone, len(clones),\n                                          regularization_losses)\n          if clone_loss is not None:\n            clones_losses.append(clone_loss)\n          # Only use regularization_losses for the first clone\n          regularization_losses = None\n      if clones_losses:\n        total_loss = tf.add_n(clones_losses, name=\'total_loss\')\n\n    # Add the summaries from the first clone. These contain the summaries\n    # created by model_fn and either optimize_clones() or _gather_clone_loss().\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone.scope))\n\n    if total_loss is not None:\n      # Add total_loss to summary.\n      summaries.add(tf.scalar_summary(\'total_loss\', total_loss,\n                                      name=\'total_loss\'))\n\n    if summaries:\n      # Merge all summaries together.\n      summary_op = tf.merge_summary(list(summaries), name=\'summary_op\')\n    else:\n      summary_op = None\n\n  return DeployedModel(train_op, summary_op, total_loss, clones)\n\n\ndef _sum_clones_gradients(clone_grads):\n  """"""Calculate the sum gradient for each shared variable across all clones.\n\n  This function assumes that the clone_grads has been scaled appropriately by\n  1 / num_clones.\n\n  Args:\n    clone_grads: A List of List of tuples (gradient, variable), one list per\n    `Clone`.\n\n  Returns:\n     List of tuples of (gradient, variable) where the gradient has been summed\n     across all clones.\n  """"""\n  sum_grads = []\n  for grad_and_vars in zip(*clone_grads):\n    # Note that each grad_and_vars looks like the following:\n    #   ((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))\n    grads = []\n    var = grad_and_vars[0][1]\n    for g, v in grad_and_vars:\n      assert v == var\n      if g is not None:\n        grads.append(g)\n    if grads:\n      if len(grads) > 1:\n        sum_grad = tf.add_n(grads, name=var.op.name + \'/sum_grads\')\n      else:\n        sum_grad = grads[0]\n      sum_grads.append((sum_grad, var))\n  return sum_grads\n\n\ndef _add_gradients_summaries(grads_and_vars):\n  """"""Add histogram summaries to gradients.\n\n  Note: The summaries are also added to the SUMMARIES collection.\n\n  Args:\n    grads_and_vars: A list of gradient to variable pairs (tuples).\n\n  Returns:\n    The _list_ of the added summaries for grads_and_vars.\n  """"""\n  summaries = []\n  for grad, var in grads_and_vars:\n    if grad is not None:\n      if isinstance(grad, tf.IndexedSlices):\n        grad_values = grad.values\n      else:\n        grad_values = grad\n      summaries.append(tf.histogram_summary(var.op.name + \':gradient\',\n                                            grad_values))\n      summaries.append(tf.histogram_summary(var.op.name + \':gradient_norm\',\n                                            tf.global_norm([grad_values])))\n    else:\n      tf.logging.info(\'Var %s has no gradient\', var.op.name)\n  return summaries\n\n\nclass DeploymentConfig(object):\n  """"""Configuration for deploying a model with `deploy()`.\n\n  You can pass an instance of this class to `deploy()` to specify exactly\n  how to deploy the model to build.  If you do not pass one, an instance built\n  from the default deployment_hparams will be used.\n  """"""\n\n  def __init__(self,\n               num_clones=1,\n               clone_on_cpu=False,\n               replica_id=0,\n               num_replicas=1,\n               num_ps_tasks=0,\n               worker_job_name=\'worker\',\n               ps_job_name=\'ps\'):\n    """"""Create a DeploymentConfig.\n\n    The config describes how to deploy a model across multiple clones and\n    replicas.  The model will be replicated `num_clones` times in each replica.\n    If `clone_on_cpu` is True, each clone will placed on CPU.\n\n    If `num_replicas` is 1, the model is deployed via a single process.  In that\n    case `worker_device`, `num_ps_tasks`, and `ps_device` are ignored.\n\n    If `num_replicas` is greater than 1, then `worker_device` and `ps_device`\n    must specify TensorFlow devices for the `worker` and `ps` jobs and\n    `num_ps_tasks` must be positive.\n\n    Args:\n      num_clones: Number of model clones to deploy in each replica.\n      clone_on_cpu: If True clones would be placed on CPU.\n      replica_id: Integer.  Index of the replica for which the model is\n        deployed.  Usually 0 for the chief replica.\n      num_replicas: Number of replicas to use.\n      num_ps_tasks: Number of tasks for the `ps` job. 0 to not use replicas.\n      worker_job_name: A name for the worker job.\n      ps_job_name: A name for the parameter server job.\n\n    Raises:\n      ValueError: If the arguments are invalid.\n    """"""\n    if num_replicas > 1:\n      if num_ps_tasks < 1:\n        raise ValueError(\'When using replicas num_ps_tasks must be positive\')\n    if num_replicas > 1 or num_ps_tasks > 0:\n      if not worker_job_name:\n        raise ValueError(\'Must specify worker_job_name when using replicas\')\n      if not ps_job_name:\n        raise ValueError(\'Must specify ps_job_name when using parameter server\')\n    if replica_id >= num_replicas:\n      raise ValueError(\'replica_id must be less than num_replicas\')\n    self._num_clones = num_clones\n    self._clone_on_cpu = clone_on_cpu\n    self._replica_id = replica_id\n    self._num_replicas = num_replicas\n    self._num_ps_tasks = num_ps_tasks\n    self._ps_device = \'/job:\' + ps_job_name if num_ps_tasks > 0 else \'\'\n    self._worker_device = \'/job:\' + worker_job_name if num_ps_tasks > 0 else \'\'\n\n  @property\n  def num_clones(self):\n    return self._num_clones\n\n  @property\n  def clone_on_cpu(self):\n    return self._clone_on_cpu\n\n  @property\n  def replica_id(self):\n    return self._replica_id\n\n  @property\n  def num_replicas(self):\n    return self._num_replicas\n\n  @property\n  def num_ps_tasks(self):\n    return self._num_ps_tasks\n\n  @property\n  def ps_device(self):\n    return self._ps_device\n\n  @property\n  def worker_device(self):\n    return self._worker_device\n\n  def caching_device(self):\n    """"""Returns the device to use for caching variables.\n\n    Variables are cached on the worker CPU when using replicas.\n\n    Returns:\n      A device string or None if the variables do not need to be cached.\n    """"""\n    if self._num_ps_tasks > 0:\n      return lambda op: op.device\n    else:\n      return None\n\n  def clone_device(self, clone_index):\n    """"""Device used to create the clone and all the ops inside the clone.\n\n    Args:\n      clone_index: Int, representing the clone_index.\n\n    Returns:\n      A value suitable for `tf.device()`.\n\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n    if clone_index >= self._num_clones:\n      raise ValueError(\'clone_index must be less than num_clones\')\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._worker_device\n    if self._clone_on_cpu:\n      device += \'/device:CPU:0\'\n    else:\n      if self._num_clones > 1:\n        device += \'/device:GPU:%d\' % clone_index\n    return device\n\n  def clone_scope(self, clone_index):\n    """"""Name scope to create the clone.\n\n    Args:\n      clone_index: Int, representing the clone_index.\n\n    Returns:\n      A name_scope suitable for `tf.name_scope()`.\n\n    Raises:\n      ValueError: if `clone_index` is greater or equal to the number of clones"".\n    """"""\n    if clone_index >= self._num_clones:\n      raise ValueError(\'clone_index must be less than num_clones\')\n    scope = \'\'\n    if self._num_clones > 1:\n      scope = \'clone_%d\' % clone_index\n    return scope\n\n  def optimizer_device(self):\n    """"""Device to use with the optimizer.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    if self._num_ps_tasks > 0 or self._num_clones > 0:\n      return self._worker_device + \'/device:CPU:0\'\n    else:\n      return \'\'\n\n  def inputs_device(self):\n    """"""Device to use to build the inputs.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._worker_device\n    device += \'/device:CPU:0\'\n    return device\n\n  def variables_device(self):\n    """"""Returns the device to use for variables created inside the clone.\n\n    Returns:\n      A value suitable for `tf.device()`.\n    """"""\n    device = \'\'\n    if self._num_ps_tasks > 0:\n      device += self._ps_device\n    device += \'/device:CPU:0\'\n\n    class _PSDeviceChooser(object):\n      """"""Slim device chooser for variables when using PS.""""""\n\n      def __init__(self, device, tasks):\n        self._device = device\n        self._tasks = tasks\n        self._task = 0\n\n      def choose(self, op):\n        if op.device:\n          return op.device\n        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n        if node_def.op == \'Variable\':\n          t = self._task\n          self._task = (self._task + 1) % self._tasks\n          d = \'%s/task:%d\' % (self._device, t)\n          return d\n        else:\n          return op.device\n\n    if not self._num_ps_tasks:\n      return device\n    else:\n      chooser = _PSDeviceChooser(device, self._num_ps_tasks)\n      return chooser.choose\n'"
model_zoo/models/slim/deployment/model_deploy_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for model_deploy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deployment import model_deploy\n\nslim = tf.contrib.slim\n\n\nclass DeploymentConfigTest(tf.test.TestCase):\n\n  def testDefaults(self):\n    deploy_config = model_deploy.DeploymentConfig()\n\n    self.assertEqual(slim.get_variables(), [])\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testCPUonly(self):\n    deploy_config = model_deploy.DeploymentConfig(clone_on_cpu=True)\n\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'CPU:0\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testMultiGPU(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n    self.assertEqual(deploy_config.caching_device(), None)\n    self.assertDeviceEqual(deploy_config.clone_device(0), \'GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1), \'GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(), \'CPU:0\')\n    self.assertDeviceEqual(deploy_config.variables_device(), \'CPU:0\')\n\n  def testPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=1, num_ps_tasks=1)\n\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n    with tf.device(deploy_config.variables_device()):\n      a = tf.Variable(0)\n      b = tf.Variable(0)\n      c = tf.no_op()\n      d = slim.variable(\'a\', [],\n                        caching_device=deploy_config.caching_device())\n    self.assertDeviceEqual(a.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(a.device, a.value().device)\n    self.assertDeviceEqual(b.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(b.device, b.value().device)\n    self.assertDeviceEqual(c.device, \'\')\n    self.assertDeviceEqual(d.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(d.value().device, \'\')\n\n  def testMultiGPUPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_clones=2, num_ps_tasks=1)\n\n    self.assertEqual(deploy_config.caching_device()(tf.no_op()), \'\')\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1),\n                           \'/job:worker/device:GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testReplicasPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_replicas=2,\n                                                  num_ps_tasks=2)\n\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker\')\n    self.assertEqual(deploy_config.clone_scope(0), \'\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testReplicasMultiGPUPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_replicas=2,\n                                                  num_clones=2,\n                                                  num_ps_tasks=2)\n    self.assertDeviceEqual(deploy_config.clone_device(0),\n                           \'/job:worker/device:GPU:0\')\n    self.assertDeviceEqual(deploy_config.clone_device(1),\n                           \'/job:worker/device:GPU:1\')\n    self.assertEqual(deploy_config.clone_scope(0), \'clone_0\')\n    self.assertEqual(deploy_config.clone_scope(1), \'clone_1\')\n    self.assertDeviceEqual(deploy_config.optimizer_device(),\n                           \'/job:worker/device:CPU:0\')\n    self.assertDeviceEqual(deploy_config.inputs_device(),\n                           \'/job:worker/device:CPU:0\')\n\n  def testVariablesPS(self):\n    deploy_config = model_deploy.DeploymentConfig(num_ps_tasks=2)\n\n    with tf.device(deploy_config.variables_device()):\n      a = tf.Variable(0)\n      b = tf.Variable(0)\n      c = tf.no_op()\n      d = slim.variable(\'a\', [],\n                        caching_device=deploy_config.caching_device())\n\n    self.assertDeviceEqual(a.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(a.device, a.value().device)\n    self.assertDeviceEqual(b.device, \'/job:ps/task:1/device:CPU:0\')\n    self.assertDeviceEqual(b.device, b.value().device)\n    self.assertDeviceEqual(c.device, \'\')\n    self.assertDeviceEqual(d.device, \'/job:ps/task:0/device:CPU:0\')\n    self.assertDeviceEqual(d.value().device, \'\')\n\n\ndef LogisticClassifier(inputs, labels, scope=None, reuse=None):\n  with tf.variable_scope(scope, \'LogisticClassifier\', [inputs, labels],\n                         reuse=reuse):\n    predictions = slim.fully_connected(inputs, 1, activation_fn=tf.sigmoid,\n                                       scope=\'fully_connected\')\n    slim.losses.log_loss(predictions, labels)\n    return predictions\n\n\ndef BatchNormClassifier(inputs, labels, scope=None, reuse=None):\n  with tf.variable_scope(scope, \'BatchNormClassifier\', [inputs, labels],\n                         reuse=reuse):\n    inputs = slim.batch_norm(inputs, decay=0.1)\n    predictions = slim.fully_connected(inputs, 1,\n                                       activation_fn=tf.sigmoid,\n                                       scope=\'fully_connected\')\n    slim.losses.log_loss(predictions, labels)\n    return predictions\n\n\nclass CreatecloneTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testCreateLogisticClassifier(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = LogisticClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      clone = clones[0]\n      self.assertEqual(len(slim.get_variables()), 2)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(clone.outputs.op.name,\n                       \'LogisticClassifier/fully_connected/Sigmoid\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertDeviceEqual(clone.device, \'\')\n      self.assertEqual(len(slim.losses.get_losses()), 1)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(update_ops, [])\n\n  def testCreateSingleclone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      clone = clones[0]\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(clone.outputs.op.name,\n                       \'BatchNormClassifier/fully_connected/Sigmoid\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertDeviceEqual(clone.device, \'\')\n      self.assertEqual(len(slim.losses.get_losses()), 1)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n  def testCreateMulticlone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n        self.assertDeviceEqual(v.value().device, \'CPU:0\')\n      self.assertEqual(len(clones), num_clones)\n      for i, clone in enumerate(clones):\n        self.assertEqual(\n            clone.outputs.op.name,\n            \'clone_%d/BatchNormClassifier/fully_connected/Sigmoid\' % i)\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, clone.scope)\n        self.assertEqual(len(update_ops), 2)\n        self.assertEqual(clone.scope, \'clone_%d/\' % i)\n        self.assertDeviceEqual(clone.device, \'GPU:%d\' % i)\n\n  def testCreateOnecloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1,\n                                                    num_ps_tasks=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(clones), 1)\n      clone = clones[0]\n      self.assertEqual(clone.outputs.op.name,\n                       \'BatchNormClassifier/fully_connected/Sigmoid\')\n      self.assertDeviceEqual(clone.device, \'/job:worker\')\n      self.assertEqual(clone.scope, \'\')\n      self.assertEqual(len(slim.get_variables()), 5)\n      for v in slim.get_variables():\n        self.assertDeviceEqual(v.device, \'/job:ps/task:0/CPU:0\')\n        self.assertDeviceEqual(v.device, v.value().device)\n\n  def testCreateMulticloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2,\n                                                    num_ps_tasks=2)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      for i, v in enumerate(slim.get_variables()):\n        t = i % 2\n        self.assertDeviceEqual(v.device, \'/job:ps/task:%d/device:CPU:0\' % t)\n        self.assertDeviceEqual(v.device, v.value().device)\n      self.assertEqual(len(clones), 2)\n      for i, clone in enumerate(clones):\n        self.assertEqual(\n            clone.outputs.op.name,\n            \'clone_%d/BatchNormClassifier/fully_connected/Sigmoid\' % i)\n        self.assertEqual(clone.scope, \'clone_%d/\' % i)\n        self.assertDeviceEqual(clone.device, \'/job:worker/device:GPU:%d\' % i)\n\n\nclass OptimizeclonesTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testCreateLogisticClassifier(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = LogisticClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 2)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(update_ops, [])\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateSingleclone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateMulticlone(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      clone_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, clone_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), num_clones * 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateMulticloneCPU(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      num_clones = 4\n      deploy_config = model_deploy.DeploymentConfig(num_clones=num_clones,\n                                                    clone_on_cpu=True)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, model_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), num_clones * 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'\')\n        self.assertDeviceEqual(v.device, \'CPU:0\')\n\n  def testCreateOnecloneWithPS(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=1,\n                                                    num_ps_tasks=1)\n\n      self.assertEqual(slim.get_variables(), [])\n      clones = model_deploy.create_clones(deploy_config, model_fn, model_args)\n      self.assertEqual(len(slim.get_variables()), 5)\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 2)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n      total_loss, grads_and_vars = model_deploy.optimize_clones(clones,\n                                                                optimizer)\n      self.assertEqual(len(grads_and_vars), len(tf.trainable_variables()))\n      self.assertEqual(total_loss.op.name, \'total_loss\')\n      for g, v in grads_and_vars:\n        self.assertDeviceEqual(g.device, \'/job:worker\')\n        self.assertDeviceEqual(v.device, \'/job:ps/task:0/CPU:0\')\n\n\nclass DeployTest(tf.test.TestCase):\n\n  def setUp(self):\n    # Create an easy training set:\n    np.random.seed(0)\n\n    self._inputs = np.zeros((16, 4))\n    self._labels = np.random.randint(0, 2, size=(16, 1)).astype(np.float32)\n    self._logdir = self.get_temp_dir()\n\n    for i in range(16):\n      j = int(2 * self._labels[i] + np.random.randint(0, 2))\n      self._inputs[i, j] = 1\n\n  def testLocalTrainOp(self):\n    g = tf.Graph()\n    with g.as_default():\n      tf.set_random_seed(0)\n      tf_inputs = tf.constant(self._inputs, dtype=tf.float32)\n      tf_labels = tf.constant(self._labels, dtype=tf.float32)\n\n      model_fn = BatchNormClassifier\n      model_args = (tf_inputs, tf_labels)\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2,\n                                                    clone_on_cpu=True)\n\n      optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0)\n\n      self.assertEqual(slim.get_variables(), [])\n      model = model_deploy.deploy(deploy_config, model_fn, model_args,\n                                  optimizer=optimizer)\n\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      self.assertEqual(len(update_ops), 4)\n      self.assertEqual(len(model.clones), 2)\n      self.assertEqual(model.total_loss.op.name, \'total_loss\')\n      self.assertEqual(model.summary_op.op.name, \'summary_op/summary_op\')\n      self.assertEqual(model.train_op.op.name, \'train_op\')\n\n      with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        moving_mean = tf.contrib.framework.get_variables_by_name(\n            \'moving_mean\')[0]\n        moving_variance = tf.contrib.framework.get_variables_by_name(\n            \'moving_variance\')[0]\n        initial_loss = sess.run(model.total_loss)\n        initial_mean, initial_variance = sess.run([moving_mean,\n                                                   moving_variance])\n        self.assertAllClose(initial_mean, [0.0, 0.0, 0.0, 0.0])\n        self.assertAllClose(initial_variance, [1.0, 1.0, 1.0, 1.0])\n        for _ in range(10):\n          sess.run(model.train_op)\n        final_loss = sess.run(model.total_loss)\n        self.assertLess(final_loss, initial_loss / 10.0)\n\n        final_mean, final_variance = sess.run([moving_mean,\n                                               moving_variance])\n        self.assertAllClose(final_mean, [0.125, 0.25, 0.375, 0.25])\n        self.assertAllClose(final_variance, [0.109375, 0.1875,\n                                             0.234375, 0.1875])\n\n  def testNoSummariesOnGPU(self):\n    with tf.Graph().as_default():\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n      # clone function creates a fully_connected layer with a regularizer loss.\n      def ModelFn():\n        inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)\n        reg = tf.contrib.layers.l2_regularizer(0.001)\n        tf.contrib.layers.fully_connected(inputs, 30, weights_regularizer=reg)\n\n      model = model_deploy.deploy(\n          deploy_config, ModelFn,\n          optimizer=tf.train.GradientDescentOptimizer(1.0))\n      # The model summary op should have a few summary inputs and all of them\n      # should be on the CPU.\n      self.assertTrue(model.summary_op.op.inputs)\n      for inp in  model.summary_op.op.inputs:\n        self.assertEqual(\'/device:CPU:0\', inp.device)\n\n  def testNoSummariesOnGPUForEvals(self):\n    with tf.Graph().as_default():\n      deploy_config = model_deploy.DeploymentConfig(num_clones=2)\n\n      # clone function creates a fully_connected layer with a regularizer loss.\n      def ModelFn():\n        inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)\n        reg = tf.contrib.layers.l2_regularizer(0.001)\n        tf.contrib.layers.fully_connected(inputs, 30, weights_regularizer=reg)\n\n      # No optimizer here, it\'s an eval.\n      model = model_deploy.deploy(deploy_config, ModelFn)\n      # The model summary op should have a few summary inputs and all of them\n      # should be on the CPU.\n      self.assertTrue(model.summary_op.op.inputs)\n      for inp in  model.summary_op.op.inputs:\n        self.assertEqual(\'/device:CPU:0\', inp.device)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/slim/nets/__init__.py,0,b'\n'
model_zoo/models/slim/nets/alexnet.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a model definition for AlexNet.\n\nThis work was first described in:\n  ImageNet Classification with Deep Convolutional Neural Networks\n  Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton\n\nand later refined in:\n  One weird trick for parallelizing convolutional neural networks\n  Alex Krizhevsky, 2014\n\nHere we provide the implementation proposed in ""One weird trick"" and not\n""ImageNet Classification"", as per the paper, the LRN layers have been removed.\n\nUsage:\n  with slim.arg_scope(alexnet.alexnet_v2_arg_scope()):\n    outputs, end_points = alexnet.alexnet_v2(inputs)\n\n@@alexnet_v2\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef alexnet_v2_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      biases_initializer=tf.constant_initializer(0.1),\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef alexnet_v2(inputs,\n               num_classes=1000,\n               is_training=True,\n               dropout_keep_prob=0.5,\n               spatial_squeeze=True,\n               scope=\'alexnet_v2\'):\n  """"""AlexNet version 2.\n\n  Described in: http://arxiv.org/pdf/1404.5997v2.pdf\n  Parameters from:\n  github.com/akrizhevsky/cuda-convnet2/blob/master/layers/\n  layers-imagenet-1gpu.cfg\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n        The LRN layers have been removed and change the initializers from\n        random_normal_initializer to xavier_initializer.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'alexnet_v2\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=[end_points_collection]):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool1\')\n      net = slim.conv2d(net, 192, [5, 5], scope=\'conv2\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool2\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 384, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 256, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [3, 3], 2, scope=\'pool5\')\n\n      # Use conv2d instead of fully_connected layers.\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        net = slim.conv2d(net, 4096, [5, 5], padding=\'VALID\',\n                          scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer,\n                          scope=\'fc8\')\n\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nalexnet_v2.default_image_size = 224\n'"
model_zoo/models/slim/nets/alexnet_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.alexnet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import alexnet\n\nslim = tf.contrib.slim\n\n\nclass AlexnetV2Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'alexnet_v2/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 4, 7, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1\',\n                        \'alexnet_v2/pool1\',\n                        \'alexnet_v2/conv2\',\n                        \'alexnet_v2/pool2\',\n                        \'alexnet_v2/conv3\',\n                        \'alexnet_v2/conv4\',\n                        \'alexnet_v2/conv5\',\n                        \'alexnet_v2/pool5\',\n                        \'alexnet_v2/fc6\',\n                        \'alexnet_v2/fc7\',\n                        \'alexnet_v2/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      alexnet.alexnet_v2(inputs, num_classes)\n      expected_names = [\'alexnet_v2/conv1/weights\',\n                        \'alexnet_v2/conv1/biases\',\n                        \'alexnet_v2/conv2/weights\',\n                        \'alexnet_v2/conv2/biases\',\n                        \'alexnet_v2/conv3/weights\',\n                        \'alexnet_v2/conv3/biases\',\n                        \'alexnet_v2/conv4/weights\',\n                        \'alexnet_v2/conv4/biases\',\n                        \'alexnet_v2/conv5/weights\',\n                        \'alexnet_v2/conv5/biases\',\n                        \'alexnet_v2/fc6/weights\',\n                        \'alexnet_v2/fc6/biases\',\n                        \'alexnet_v2/fc7/weights\',\n                        \'alexnet_v2/fc7/biases\',\n                        \'alexnet_v2/fc8/weights\',\n                        \'alexnet_v2/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 300, 400\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = alexnet.alexnet_v2(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False,\n                                     spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 4, 7, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = alexnet.alexnet_v2(inputs)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/slim/nets/cifarnet.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the CIFAR-10 model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)\n\n\ndef cifarnet(images, num_classes=10, is_training=False,\n             dropout_keep_prob=0.5,\n             prediction_fn=slim.softmax,\n             scope=\'CifarNet\'):\n  """"""Creates a variant of the CifarNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = cifarnet.cifarnet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'CifarNet\', [images, num_classes]):\n    net = slim.conv2d(images, 64, [5, 5], scope=\'conv1\')\n    end_points[\'conv1\'] = net\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    end_points[\'pool1\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    end_points[\'conv2\'] = net\n    net = tf.nn.lrn(net, 4, bias=1.0, alpha=0.001/9.0, beta=0.75, name=\'norm2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    end_points[\'pool2\'] = net\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n    net = slim.fully_connected(net, 384, scope=\'fc3\')\n    end_points[\'fc3\'] = net\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    net = slim.fully_connected(net, 192, scope=\'fc4\')\n    end_points[\'fc4\'] = net\n    logits = slim.fully_connected(net, num_classes,\n                                  biases_initializer=tf.zeros_initializer,\n                                  weights_initializer=trunc_normal(1/192.0),\n                                  weights_regularizer=None,\n                                  activation_fn=None,\n                                  scope=\'logits\')\n\n    end_points[\'Logits\'] = logits\n    end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\ncifarnet.default_image_size = 32\n\n\ndef cifarnet_arg_scope(weight_decay=0.004):\n  """"""Defines the default cifarnet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_initializer=tf.truncated_normal_initializer(stddev=5e-2),\n      activation_fn=tf.nn.relu):\n    with slim.arg_scope(\n        [slim.fully_connected],\n        biases_initializer=tf.constant_initializer(0.1),\n        weights_initializer=trunc_normal(0.04),\n        weights_regularizer=slim.l2_regularizer(weight_decay),\n        activation_fn=tf.nn.relu) as sc:\n      return sc\n'"
model_zoo/models/slim/nets/inception.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Brings all inception models under one namespace.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# pylint: disable=unused-import\nfrom nets.inception_resnet_v2 import inception_resnet_v2\nfrom nets.inception_resnet_v2 import inception_resnet_v2_arg_scope\nfrom nets.inception_v1 import inception_v1\nfrom nets.inception_v1 import inception_v1_arg_scope\nfrom nets.inception_v1 import inception_v1_base\nfrom nets.inception_v2 import inception_v2\nfrom nets.inception_v2 import inception_v2_arg_scope\nfrom nets.inception_v2 import inception_v2_base\nfrom nets.inception_v3 import inception_v3\nfrom nets.inception_v3 import inception_v3_arg_scope\nfrom nets.inception_v3 import inception_v3_base\nfrom nets.inception_v4 import inception_v4\nfrom nets.inception_v4 import inception_v4_arg_scope\nfrom nets.inception_v4 import inception_v4_base\n# pylint: enable=unused-import\n'"
model_zoo/models/slim/nets/inception_resnet_v2.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception Resnet V2 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 35x35 resnet block.""""""\n  with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n    with tf.variable_scope(\'Branch_2\'):\n      tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n      tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n    mixed = tf.concat(3, [tower_conv, tower_conv1_1, tower_conv2_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 17x17 resnet block.""""""\n  with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n                                  scope=\'Conv2d_0b_1x7\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n                                  scope=\'Conv2d_0c_7x1\')\n    mixed = tf.concat(3, [tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n  """"""Builds the 8x8 resnet block.""""""\n  with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n    with tf.variable_scope(\'Branch_0\'):\n      tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n    with tf.variable_scope(\'Branch_1\'):\n      tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n      tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n                                  scope=\'Conv2d_0b_1x3\')\n      tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n                                  scope=\'Conv2d_0c_3x1\')\n    mixed = tf.concat(3, [tower_conv, tower_conv1_2])\n    up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n                     activation_fn=None, scope=\'Conv2d_1x1\')\n    net += scale * up\n    if activation_fn:\n      net = activation_fn(net)\n  return net\n\n\ndef inception_resnet_v2(inputs, num_classes=1001, is_training=True,\n                        dropout_keep_prob=0.8,\n                        reuse=None,\n                        scope=\'InceptionResnetV2\'):\n  """"""Creates the Inception Resnet V2 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse):\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n\n        # 149 x 149 x 32\n        net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n                          scope=\'Conv2d_1a_3x3\')\n        end_points[\'Conv2d_1a_3x3\'] = net\n        # 147 x 147 x 32\n        net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n                          scope=\'Conv2d_2a_3x3\')\n        end_points[\'Conv2d_2a_3x3\'] = net\n        # 147 x 147 x 64\n        net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n        end_points[\'Conv2d_2b_3x3\'] = net\n        # 73 x 73 x 64\n        net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                              scope=\'MaxPool_3a_3x3\')\n        end_points[\'MaxPool_3a_3x3\'] = net\n        # 73 x 73 x 80\n        net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n                          scope=\'Conv2d_3b_1x1\')\n        end_points[\'Conv2d_3b_1x1\'] = net\n        # 71 x 71 x 192\n        net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n                          scope=\'Conv2d_4a_3x3\')\n        end_points[\'Conv2d_4a_3x3\'] = net\n        # 35 x 35 x 192\n        net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                              scope=\'MaxPool_5a_3x3\')\n        end_points[\'MaxPool_5a_3x3\'] = net\n\n        # 35 x 35 x 320\n        with tf.variable_scope(\'Mixed_5b\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n                                        scope=\'Conv2d_0b_5x5\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n                                        scope=\'Conv2d_0c_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n                                         scope=\'AvgPool_0a_3x3\')\n            tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n                                       scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(3, [tower_conv, tower_conv1_1,\n                              tower_conv2_2, tower_pool_1])\n\n        end_points[\'Mixed_5b\'] = net\n        net = slim.repeat(net, 10, block35, scale=0.17)\n\n        # 17 x 17 x 1024\n        with tf.variable_scope(\'Mixed_6a\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\'VALID\',\n                                     scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n                                        stride=2, padding=\'VALID\',\n                                        scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                         scope=\'MaxPool_1a_3x3\')\n          net = tf.concat(3, [tower_conv, tower_conv1_2, tower_pool])\n\n        end_points[\'Mixed_6a\'] = net\n        net = slim.repeat(net, 20, block17, scale=0.10)\n\n        # Auxillary tower\n        with tf.variable_scope(\'AuxLogits\'):\n          aux = slim.avg_pool2d(net, 5, stride=3, padding=\'VALID\',\n                                scope=\'Conv2d_1a_3x3\')\n          aux = slim.conv2d(aux, 128, 1, scope=\'Conv2d_1b_1x1\')\n          aux = slim.conv2d(aux, 768, aux.get_shape()[1:3],\n                            padding=\'VALID\', scope=\'Conv2d_2a_5x5\')\n          aux = slim.flatten(aux)\n          aux = slim.fully_connected(aux, num_classes, activation_fn=None,\n                                     scope=\'Logits\')\n          end_points[\'AuxLogits\'] = aux\n\n        with tf.variable_scope(\'Mixed_7a\'):\n          with tf.variable_scope(\'Branch_0\'):\n            tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n                                       padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_1\'):\n            tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n                                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n            tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n                                        scope=\'Conv2d_0b_3x3\')\n            tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n                                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n                                         scope=\'MaxPool_1a_3x3\')\n          net = tf.concat(3, [tower_conv_1, tower_conv1_1,\n                              tower_conv2_2, tower_pool])\n\n        end_points[\'Mixed_7a\'] = net\n\n        net = slim.repeat(net, 9, block8, scale=0.20)\n        net = block8(net, activation_fn=None)\n\n        net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n        end_points[\'Conv2d_7b_1x1\'] = net\n\n        with tf.variable_scope(\'Logits\'):\n          end_points[\'PrePool\'] = net\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a_8x8\')\n          net = slim.flatten(net)\n\n          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                             scope=\'Dropout\')\n\n          end_points[\'PreLogitsFlatten\'] = net\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n\n    return logits, end_points\ninception_resnet_v2.default_image_size = 299\n\n\ndef inception_resnet_v2_arg_scope(weight_decay=0.00004,\n                                  batch_norm_decay=0.9997,\n                                  batch_norm_epsilon=0.001):\n  """"""Yields the scope with the default parameters for inception_resnet_v2.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Returns:\n    a arg_scope with the parameters needed for inception_resnet_v2.\n  """"""\n  # Set weight_decay for weights in conv2d and fully_connected layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    batch_norm_params = {\n        \'decay\': batch_norm_decay,\n        \'epsilon\': batch_norm_epsilon,\n    }\n    # Set activation_fn and parameters for batch_norm.\n    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,\n                        normalizer_fn=slim.batch_norm,\n                        normalizer_params=batch_norm_params) as scope:\n      return scope\n'"
model_zoo/models/slim/nets/inception_resnet_v2_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_resnet_v2.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(\'Logits\' in end_points)\n      logits = end_points[\'Logits\']\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(\'AuxLogits\' in end_points)\n      aux_logits = end_points[\'AuxLogits\']\n      self.assertListEqual(aux_logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'PrePool\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 8, 8, 1536])\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      # Force all Variables to reside on the device.\n      with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n        inception.inception_resnet_v2(inputs, num_classes)\n      for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope=\'on_cpu\'):\n        self.assertDeviceEqual(v.device, \'/cpu:0\')\n      for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope=\'on_gpu\'):\n        self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, end_points = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'PrePool\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionResnetV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_resnet_v2(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_resnet_v2(eval_inputs,\n                                                num_classes,\n                                                is_training=False,\n                                                reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/slim/nets/inception_utils.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001):\n  """"""Defines the default arg scope for inception models.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n\n  Returns:\n    An `arg_scope` to use for the inception models.\n  """"""\n  batch_norm_params = {\n      # Decay for the moving averages.\n      \'decay\': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': batch_norm_epsilon,\n      # collection containing update_ops.\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n  if use_batch_norm:\n    normalizer_fn = slim.batch_norm\n    normalizer_params = batch_norm_params\n  else:\n    normalizer_fn = None\n    normalizer_params = {}\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=tf.nn.relu,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc\n'"
model_zoo/models/slim/nets/inception_v1.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v1 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v1_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 base architecture.\n\n  This architecture is defined in:\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n      \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n      \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\', \'Mixed_5c\']\n    scope: Optional variable_scope.\n\n  Returns:\n    A dictionary from components of the network to the corresponding activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.fully_connected],\n        weights_initializer=trunc_normal(0.01)):\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                          stride=1, padding=\'SAME\'):\n        end_point = \'Conv2d_1a_7x7\'\n        net = slim.conv2d(inputs, 64, [7, 7], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_2a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2b_1x1\'\n        net = slim.conv2d(net, 64, [1, 1], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'Conv2d_2c_3x3\'\n        net = slim.conv2d(net, 192, [3, 3], scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n        end_point = \'MaxPool_3a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_3c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_4a_3x3\'\n        net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 16, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4d\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 128, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 24, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4e\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 112, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 144, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_4f\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'MaxPool_5a_2x2\'\n        net = slim.max_pool2d(net, [2, 2], stride=2, scope=end_point)\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5b\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 160, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 32, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0a_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n\n        end_point = \'Mixed_5c\'\n        with tf.variable_scope(end_point):\n          with tf.variable_scope(\'Branch_0\'):\n            branch_0 = slim.conv2d(net, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n          with tf.variable_scope(\'Branch_1\'):\n            branch_1 = slim.conv2d(net, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_2\'):\n            branch_2 = slim.conv2d(net, 48, [1, 1], scope=\'Conv2d_0a_1x1\')\n            branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope=\'Conv2d_0b_3x3\')\n          with tf.variable_scope(\'Branch_3\'):\n            branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n            branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if final_endpoint == end_point: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v1(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV1\'):\n  """"""Defines the Inception V1 architecture.\n\n  This architecture is defined in:\n\n    Going deeper with convolutions\n    Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n    http://arxiv.org/pdf/1409.4842v1.pdf.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV1\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v1_base(inputs, scope=scope)\n      with tf.variable_scope(\'Logits\'):\n        net = slim.avg_pool2d(net, [7, 7], stride=1, scope=\'MaxPool_0a_7x7\')\n        net = slim.dropout(net,\n                           dropout_keep_prob, scope=\'Dropout_0b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_0c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n\n        end_points[\'Logits\'] = logits\n        end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v1.default_image_size = 224\n\ninception_v1_arg_scope = inception_utils.inception_arg_scope\n'"
model_zoo/models/slim/nets/inception_v1_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV1Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_6c, end_points = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_6c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_6c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                          \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\',\n                          \'Mixed_3c\', \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\',\n                          \'Mixed_4d\', \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\',\n                          \'Mixed_5b\', \'Mixed_5c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'MaxPool_4a_3x3\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\',\n                 \'Mixed_4e\', \'Mixed_4f\', \'MaxPool_5a_2x2\', \'Mixed_5b\',\n                 \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v1_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV1/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v1_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Conv2d_1a_7x7\': [5, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [5, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [5, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [5, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [5, 28, 28, 192],\n                        \'Mixed_3b\': [5, 28, 28, 256],\n                        \'Mixed_3c\': [5, 28, 28, 480],\n                        \'MaxPool_4a_3x3\': [5, 14, 14, 480],\n                        \'Mixed_4b\': [5, 14, 14, 512],\n                        \'Mixed_4c\': [5, 14, 14, 512],\n                        \'Mixed_4d\': [5, 14, 14, 512],\n                        \'Mixed_4e\': [5, 14, 14, 528],\n                        \'Mixed_4f\': [5, 14, 14, 832],\n                        \'MaxPool_5a_2x2\': [5, 7, 7, 832],\n                        \'Mixed_5b\': [5, 7, 7, 832],\n                        \'Mixed_5c\': [5, 7, 7, 1024]}\n\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v1_arg_scope()):\n      inception.inception_v1_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(5607184, total_params)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, _ = inception.inception_v1_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV1/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v1(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.initialize_all_variables().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v1(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV1/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v1(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v1(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v1(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.initialize_all_variables().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/slim/nets/inception_v2.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v2 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v2_base(inputs,\n                      final_endpoint=\'Mixed_5c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception v2 (6a2).\n\n  Constructs an Inception v2 network from inputs to the given final endpoint.\n  This method can construct the network up to the layer inception(5b) as\n  described in http://arxiv.org/abs/1502.03167.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n      \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\',\n      \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\', \'Mixed_5b\',\n      \'Mixed_5c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs]):\n    with slim.arg_scope(\n        [slim.conv2d, slim.max_pool2d, slim.avg_pool2d, slim.separable_conv2d],\n        stride=1, padding=\'SAME\'):\n\n      # Note that sizes in the comments below assume an input spatial size of\n      # 224x224, however, the inputs can be of any size greater 32x32.\n\n      # 224 x 224 x 3\n      end_point = \'Conv2d_1a_7x7\'\n      # depthwise_multiplier here is different from depth_multiplier.\n      # depthwise_multiplier determines the output channels of the initial\n      # depthwise conv (see docs for tf.nn.separable_conv2d), while\n      # depth_multiplier controls the # channels of the subsequent 1x1\n      # convolution. Must have\n      #   in_channels * depthwise_multipler <= out_channels\n      # so that the separable convolution is not overparameterized.\n      depthwise_multiplier = min(int(depth(64) / 3), 8)\n      net = slim.separable_conv2d(\n          inputs, depth(64), [7, 7], depth_multiplier=depthwise_multiplier,\n          stride=2, weights_initializer=trunc_normal(1.0),\n          scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 112 x 112 x 64\n      end_point = \'MaxPool_2a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2b_1x1\'\n      net = slim.conv2d(net, depth(64), [1, 1], scope=end_point,\n                        weights_initializer=trunc_normal(0.1))\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 64\n      end_point = \'Conv2d_2c_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 56 x 56 x 192\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], scope=end_point, stride=2)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 192\n      # Inception module.\n      end_point = \'Mixed_3b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(32), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 256\n      end_point = \'Mixed_3c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 28 x 28 x 320\n      end_point = \'Mixed_4a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(160), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], stride=2, scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(\n              net, [3, 3], stride=2, scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(224), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(64), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(\n              branch_1, depth(96), [3, 3], scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_4d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 14 x 14 x 576\n      end_point = \'Mixed_4e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(96), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(96), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 14 x 14 x 576\n      end_point = \'Mixed_5a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(\n              net, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2,\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(160), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n\n      # 7 x 7 x 1024\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(352), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(\n              net, depth(192), [1, 1],\n              weights_initializer=trunc_normal(0.09),\n              scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.max_pool2d(net, [3, 3], scope=\'MaxPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(128), [1, 1],\n              weights_initializer=trunc_normal(0.1),\n              scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n        end_points[end_point] = net\n        if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v2(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV2\'):\n  """"""Inception v2 model for classification.\n\n  Constructs an Inception v2 network for classification as described in\n  http://arxiv.org/abs/1502.03167.\n\n  The default image size used to train this network is 224x224.\n\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  # Final pooling and prediction\n  with tf.variable_scope(scope, \'InceptionV2\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v2_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 1024\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v2.default_image_size = 224\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v2_arg_scope = inception_utils.inception_arg_scope\n'"
model_zoo/models/slim/nets/inception_v2_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nets.inception_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV2Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    mixed_5c, end_points = inception.inception_v2_base(inputs)\n    self.assertTrue(mixed_5c.op.name.startswith(\'InceptionV2/Mixed_5c\'))\n    self.assertListEqual(mixed_5c.get_shape().as_list(),\n                         [batch_size, 7, 7, 1024])\n    expected_endpoints = [\'Mixed_3b\', \'Mixed_3c\', \'Mixed_4a\', \'Mixed_4b\',\n                          \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\', \'Mixed_5a\',\n                          \'Mixed_5b\', \'Mixed_5c\', \'Conv2d_1a_7x7\',\n                          \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\', \'Conv2d_2c_3x3\',\n                          \'MaxPool_3a_3x3\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 224, 224\n    endpoints = [\'Conv2d_1a_7x7\', \'MaxPool_2a_3x3\', \'Conv2d_2b_1x1\',\n                 \'Conv2d_2c_3x3\', \'MaxPool_3a_3x3\', \'Mixed_3b\', \'Mixed_3c\',\n                 \'Mixed_4a\', \'Mixed_4b\', \'Mixed_4c\', \'Mixed_4d\', \'Mixed_4e\',\n                 \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\']\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v2_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV2/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed5c(self):\n    batch_size = 5\n    height, width = 224, 224\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2_base(inputs,\n                                                final_endpoint=\'Mixed_5c\')\n    endpoints_shapes = {\'Mixed_3b\': [batch_size, 28, 28, 256],\n                        \'Mixed_3c\': [batch_size, 28, 28, 320],\n                        \'Mixed_4a\': [batch_size, 14, 14, 576],\n                        \'Mixed_4b\': [batch_size, 14, 14, 576],\n                        \'Mixed_4c\': [batch_size, 14, 14, 576],\n                        \'Mixed_4d\': [batch_size, 14, 14, 576],\n                        \'Mixed_4e\': [batch_size, 14, 14, 576],\n                        \'Mixed_5a\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5b\': [batch_size, 7, 7, 1024],\n                        \'Mixed_5c\': [batch_size, 7, 7, 1024],\n                        \'Conv2d_1a_7x7\': [batch_size, 112, 112, 64],\n                        \'MaxPool_2a_3x3\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2b_1x1\': [batch_size, 56, 56, 64],\n                        \'Conv2d_2c_3x3\': [batch_size, 56, 56, 192],\n                        \'MaxPool_3a_3x3\': [batch_size, 28, 28, 192]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 224, 224\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v2_arg_scope()):\n      inception.inception_v2_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(10173112, total_params)\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v2(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v2(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v2(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 112, 112\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_5c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 4, 4, 1024])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v2(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_5c\']\n      feed_dict = {inputs: input_np}\n      tf.initialize_all_variables().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 224, 224\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v2(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV2/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v2(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v2(eval_inputs, num_classes, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 224, 224, 3])\n    logits, _ = inception.inception_v2(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.initialize_all_variables().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/slim/nets/inception_v3.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v3 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v3_base(inputs,\n                      final_endpoint=\'Mixed_7c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  Constructs an Inception v3 network from inputs to the given final endpoint.\n  This method can construct the network up to the final inception block\n  Mixed_7c.\n\n  Note that the names of the layers in the paper do not correspond to the names\n  of the endpoints registered by this function although they build the same\n  network.\n\n  Here is a mapping from the old_names to the new names:\n  Old name          | New name\n  =======================================\n  conv0             | Conv2d_1a_3x3\n  conv1             | Conv2d_2a_3x3\n  conv2             | Conv2d_2b_3x3\n  pool1             | MaxPool_3a_3x3\n  conv3             | Conv2d_3b_1x1\n  conv4             | Conv2d_4a_3x3\n  pool2             | MaxPool_5a_3x3\n  mixed_35x35x256a  | Mixed_5b\n  mixed_35x35x288a  | Mixed_5c\n  mixed_35x35x288b  | Mixed_5d\n  mixed_17x17x768a  | Mixed_6a\n  mixed_17x17x768b  | Mixed_6b\n  mixed_17x17x768c  | Mixed_6c\n  mixed_17x17x768d  | Mixed_6d\n  mixed_17x17x768e  | Mixed_6e\n  mixed_8x8x1280a   | Mixed_7a\n  mixed_8x8x2048a   | Mixed_7b\n  mixed_8x8x2048b   | Mixed_7c\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\',\n      \'Mixed_6d\', \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'VALID\'):\n      # 299 x 299 x 3\n      end_point = \'Conv2d_1a_3x3\'\n      net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 149 x 149 x 32\n      end_point = \'Conv2d_2a_3x3\'\n      net = slim.conv2d(net, depth(32), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 32\n      end_point = \'Conv2d_2b_3x3\'\n      net = slim.conv2d(net, depth(64), [3, 3], padding=\'SAME\', scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 64\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 64\n      end_point = \'Conv2d_3b_1x1\'\n      net = slim.conv2d(net, depth(80), [1, 1], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 80.\n      end_point = \'Conv2d_4a_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 71 x 71 x 192.\n      end_point = \'MaxPool_5a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 35 x 35 x 192.\n\n    # Inception blocks\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # mixed: 35 x 35 x 256.\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_1: 35 x 35 x 288.\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0b_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv_1_0c_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1],\n                                 scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_2: 35 x 35 x 288.\n      end_point = \'Mixed_5d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_3: 17 x 17 x 768.\n      end_point = \'Mixed_6a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed4: 17 x 17 x 768.\n      end_point = \'Mixed_6b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_5: 17 x 17 x 768.\n      end_point = \'Mixed_6c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_6: 17 x 17 x 768.\n      end_point = \'Mixed_6d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_7: 17 x 17 x 768.\n      end_point = \'Mixed_6e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_8: 8 x 8 x 1280.\n      end_point = \'Mixed_7a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_9: 8 x 8 x 2048.\n      end_point = \'Mixed_7b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(3, [\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0b_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(3, [\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_10: 8 x 8 x 2048.\n      end_point = \'Mixed_7c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(3, [\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0c_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(3, [\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v3(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 scope=\'InceptionV3\'):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n\n  ""Rethinking the Inception Architecture for Computer Vision""\n\n  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna.\n\n  With the default arguments this method constructs the exact model defined in\n  the paper. However, one can experiment with variations of the inception_v3\n  network by changing arguments dropout_keep_prob, min_depth and\n  depth_multiplier.\n\n  The default image size used to train this network is 299x299.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is\n        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, num_classes]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: if \'depth_multiplier\' is less than or equal to zero.\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs, num_classes],\n                         reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v3_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n\n      # Auxiliary Head logits\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        aux_logits = end_points[\'Mixed_6e\']\n        with tf.variable_scope(\'AuxLogits\'):\n          aux_logits = slim.avg_pool2d(\n              aux_logits, [5, 5], stride=3, padding=\'VALID\',\n              scope=\'AvgPool_1a_5x5\')\n          aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1],\n                                   scope=\'Conv2d_1b_1x1\')\n\n          # Shape of feature map before the final layer.\n          kernel_size = _reduced_kernel_size_for_small_input(\n              aux_logits, [5, 5])\n          aux_logits = slim.conv2d(\n              aux_logits, depth(768), kernel_size,\n              weights_initializer=trunc_normal(0.01),\n              padding=\'VALID\', scope=\'Conv2d_2a_{}x{}\'.format(*kernel_size))\n          aux_logits = slim.conv2d(\n              aux_logits, num_classes, [1, 1], activation_fn=None,\n              normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n              scope=\'Conv2d_2b_1x1\')\n          if spatial_squeeze:\n            aux_logits = tf.squeeze(aux_logits, [1, 2], name=\'SpatialSqueeze\')\n          end_points[\'AuxLogits\'] = aux_logits\n\n      # Final pooling and prediction\n      with tf.variable_scope(\'Logits\'):\n        kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])\n        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                              scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n        # 1 x 1 x 2048\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        end_points[\'PreLogits\'] = net\n        # 2048\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n        # 1000\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v3.default_image_size = 299\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n\n  Returns:\n    a tensor with the kernel size.\n\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.pack([tf.minimum(shape[1], kernel_size[0]),\n                        tf.minimum(shape[2], kernel_size[1])])\n\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v3_arg_scope = inception_utils.inception_arg_scope\n'"
model_zoo/models/slim/nets/inception_v3_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for nets.inception_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import inception\n\nslim = tf.contrib.slim\n\n\nclass InceptionV3Test(tf.test.TestCase):\n\n  def testBuildClassificationNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Predictions\' in end_points)\n    self.assertListEqual(end_points[\'Predictions\'].get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    final_endpoint, end_points = inception.inception_v3_base(inputs)\n    self.assertTrue(final_endpoint.op.name.startswith(\n        \'InceptionV3/Mixed_7c\'))\n    self.assertListEqual(final_endpoint.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    expected_endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                          \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                          \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                          \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                          \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n\n  def testBuildOnlyUptoFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    endpoints = [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n                 \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\',\n                 \'MaxPool_5a_3x3\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n                 \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n                 \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\']\n\n    for index, endpoint in enumerate(endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v3_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV3/\' + endpoint))\n        self.assertItemsEqual(endpoints[:index+1], end_points)\n\n  def testBuildAndCheckAllEndPointsUptoMixed7c(self):\n    batch_size = 5\n    height, width = 299, 299\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3_base(\n        inputs, final_endpoint=\'Mixed_7c\')\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'MaxPool_3a_3x3\': [batch_size, 73, 73, 64],\n                        \'Conv2d_3b_1x1\': [batch_size, 73, 73, 80],\n                        \'Conv2d_4a_3x3\': [batch_size, 71, 71, 192],\n                        \'MaxPool_5a_3x3\': [batch_size, 35, 35, 192],\n                        \'Mixed_5b\': [batch_size, 35, 35, 256],\n                        \'Mixed_5c\': [batch_size, 35, 35, 288],\n                        \'Mixed_5d\': [batch_size, 35, 35, 288],\n                        \'Mixed_6a\': [batch_size, 17, 17, 768],\n                        \'Mixed_6b\': [batch_size, 17, 17, 768],\n                        \'Mixed_6c\': [batch_size, 17, 17, 768],\n                        \'Mixed_6d\': [batch_size, 17, 17, 768],\n                        \'Mixed_6e\': [batch_size, 17, 17, 768],\n                        \'Mixed_7a\': [batch_size, 8, 8, 1280],\n                        \'Mixed_7b\': [batch_size, 8, 8, 2048],\n                        \'Mixed_7c\': [batch_size, 8, 8, 2048]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testModelHasExpectedNumberOfParameters(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with slim.arg_scope(inception.inception_v3_arg_scope()):\n      inception.inception_v3_base(inputs)\n    total_params, _ = slim.model_analyzer.analyze_vars(\n        slim.get_model_variables())\n    self.assertAlmostEqual(21802784, total_params)\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(\'Logits\' in end_points)\n    logits = end_points[\'Logits\']\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'AuxLogits\' in end_points)\n    aux_logits = end_points[\'AuxLogits\']\n    self.assertListEqual(aux_logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(\'Mixed_7c\' in end_points)\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 8, 8, 2048])\n    self.assertTrue(\'PreLogits\' in end_points)\n    pre_logits = end_points[\'PreLogits\']\n    self.assertListEqual(pre_logits.get_shape().as_list(),\n                         [batch_size, 1, 1, 2048])\n\n  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=0.5)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(0.5 * original_depth, new_depth)\n\n  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v3(inputs, num_classes)\n\n    endpoint_keys = [key for key in end_points.keys()\n                     if key.startswith(\'Mixed\') or key.startswith(\'Conv\')]\n\n    _, end_points_with_multiplier = inception.inception_v3(\n        inputs, num_classes, scope=\'depth_multiplied_net\',\n        depth_multiplier=2.0)\n\n    for key in endpoint_keys:\n      original_depth = end_points[key].get_shape().as_list()[3]\n      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]\n      self.assertEqual(2.0 * original_depth, new_depth)\n\n  def testRaiseValueErrorWithInvalidDepthMultiplier(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=-0.1)\n    with self.assertRaises(ValueError):\n      _ = inception.inception_v3(inputs, num_classes, depth_multiplier=0.0)\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7c\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 2048])\n\n  def testUnknownImageShape(self):\n    tf.reset_default_graph()\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))\n      logits, end_points = inception.inception_v3(inputs, num_classes)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'Mixed_7c\']\n      feed_dict = {inputs: input_np}\n      tf.initialize_all_variables().run()\n      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)\n      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 8, 2048])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n\n    inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n    logits, _ = inception.inception_v3(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV3/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, num_classes])\n    images = tf.random_uniform((batch_size, height, width, 3))\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n\n    eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n\n    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n    inception.inception_v3(train_inputs, num_classes)\n    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n    logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                       is_training=False, reuse=True)\n    predictions = tf.argmax(logits, 1)\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n  def testLogitsNotSqueezed(self):\n    num_classes = 25\n    images = tf.random_uniform([1, 299, 299, 3])\n    logits, _ = inception.inception_v3(images,\n                                       num_classes=num_classes,\n                                       spatial_squeeze=False)\n\n    with self.test_session() as sess:\n      tf.initialize_all_variables().run()\n      logits_out = sess.run(logits)\n      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/slim/nets/inception_v4.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception_utils\n\nslim = tf.contrib.slim\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n  """"""Builds Inception-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0c_3x3\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-A block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionA\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding=\'VALID\',\n                               scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n        branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(3, [branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n  """"""Builds Inception-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\'Conv2d_0c_7x1\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\'Conv2d_0b_7x1\')\n        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\'Conv2d_0c_1x7\')\n        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\'Conv2d_0d_7x1\')\n        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\'Conv2d_0e_1x7\')\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n  """"""Builds Reduction-B block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockReductionB\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\'Conv2d_0b_1x7\')\n        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\'Conv2d_0c_7x1\')\n        branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n                               padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                   scope=\'MaxPool_1a_3x3\')\n      return tf.concat(3, [branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n  """"""Builds Inception-C block for Inception v4 network.""""""\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding=\'SAME\'):\n    with tf.variable_scope(scope, \'BlockInceptionC\', [inputs], reuse=reuse):\n      with tf.variable_scope(\'Branch_0\'):\n        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n      with tf.variable_scope(\'Branch_1\'):\n        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_1 = tf.concat(3, [\n            slim.conv2d(branch_1, 256, [1, 3], scope=\'Conv2d_0b_1x3\'),\n            slim.conv2d(branch_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')])\n      with tf.variable_scope(\'Branch_2\'):\n        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\'Conv2d_0b_3x1\')\n        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\'Conv2d_0c_1x3\')\n        branch_2 = tf.concat(3, [\n            slim.conv2d(branch_2, 256, [1, 3], scope=\'Conv2d_0d_1x3\'),\n            slim.conv2d(branch_2, 256, [3, 1], scope=\'Conv2d_0e_3x1\')])\n      with tf.variable_scope(\'Branch_3\'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\'Conv2d_0b_1x1\')\n      return tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint=\'Mixed_7d\', scope=None):\n  """"""Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'Mixed_3a\', \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n      \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\', \'Mixed_6e\',\n      \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\',\n      \'Mixed_7d\']\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  """"""\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # 299 x 299 x 3\n      net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n                        padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n      if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n      # 149 x 149 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding=\'VALID\',\n                        scope=\'Conv2d_2a_3x3\')\n      if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 64, [3, 3], scope=\'Conv2d_2b_3x3\')\n      if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n      # 147 x 147 x 64\n      with tf.variable_scope(\'Mixed_3a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_0a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_0a_3x3\')\n        net = tf.concat(3, [branch_0, branch_1])\n        if add_and_check_final(\'Mixed_3a\', net): return net, end_points\n\n      # 73 x 73 x 160\n      with tf.variable_scope(\'Mixed_4a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        net = tf.concat(3, [branch_0, branch_1])\n        if add_and_check_final(\'Mixed_4a\', net): return net, end_points\n\n      # 71 x 71 x 192\n      with tf.variable_scope(\'Mixed_5a\'):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding=\'VALID\',\n                                 scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(3, [branch_0, branch_1])\n        if add_and_check_final(\'Mixed_5a\', net): return net, end_points\n\n      # 35 x 35 x 384\n      # 4 x Inception-A blocks\n      for idx in range(4):\n        block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n        net = block_inception_a(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 35 x 35 x 384\n      # Reduction-A block\n      net = block_reduction_a(net, \'Mixed_6a\')\n      if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n      # 17 x 17 x 1024\n      # 7 x Inception-B blocks\n      for idx in range(7):\n        block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n        net = block_inception_b(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 17 x 17 x 1024\n      # Reduction-B block\n      net = block_reduction_b(net, \'Mixed_7a\')\n      if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n      # 8 x 8 x 1536\n      # 3 x Inception-C blocks\n      for idx in range(3):\n        block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n        net = block_inception_c(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n  raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v4(inputs, num_classes=1001, is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope=\'InceptionV4\',\n                 create_aux_logits=True):\n  """"""Creates the Inception V4 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n    create_aux_logits: Whether to include the auxilliary logits.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n  """"""\n  end_points = {}\n  with tf.variable_scope(scope, \'InceptionV4\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v4_base(inputs, scope=scope)\n\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding=\'SAME\'):\n        # Auxiliary Head logits\n        if create_aux_logits:\n          with tf.variable_scope(\'AuxLogits\'):\n            # 17 x 17 x 1024\n            aux_logits = end_points[\'Mixed_6h\']\n            aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n                                         padding=\'VALID\',\n                                         scope=\'AvgPool_1a_5x5\')\n            aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n                                     scope=\'Conv2d_1b_1x1\')\n            aux_logits = slim.conv2d(aux_logits, 768,\n                                     aux_logits.get_shape()[1:3],\n                                     padding=\'VALID\', scope=\'Conv2d_2a\')\n            aux_logits = slim.flatten(aux_logits)\n            aux_logits = slim.fully_connected(aux_logits, num_classes,\n                                              activation_fn=None,\n                                              scope=\'Aux_logits\')\n            end_points[\'AuxLogits\'] = aux_logits\n\n        # Final pooling and prediction\n        with tf.variable_scope(\'Logits\'):\n          # 8 x 8 x 1536\n          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n                                scope=\'AvgPool_1a\')\n          # 1 x 1 x 1536\n          net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_1b\')\n          net = slim.flatten(net, scope=\'PreLogitsFlatten\')\n          end_points[\'PreLogitsFlatten\'] = net\n          # 1536\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope=\'Logits\')\n          end_points[\'Logits\'] = logits\n          end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n    return logits, end_points\ninception_v4.default_image_size = 299\n\n\ninception_v4_arg_scope = inception_utils.inception_arg_scope\n'"
model_zoo/models/slim/nets/inception_v4_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception_v4.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    auxlogits = end_points[\'AuxLogits\']\n    predictions = end_points[\'Predictions\']\n    self.assertTrue(auxlogits.op.name.startswith(\'InceptionV4/AuxLogits\'))\n    self.assertListEqual(auxlogits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    self.assertTrue(predictions.op.name.startswith(\n        \'InceptionV4/Logits/Predictions\'))\n    self.assertListEqual(predictions.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testBuildWithoutAuxLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, endpoints = inception.inception_v4(inputs, num_classes,\n                                               create_aux_logits=False)\n    self.assertFalse(\'AuxLogits\' in endpoints)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n\n  def testAllEndPointsShapes(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    _, end_points = inception.inception_v4(inputs, num_classes)\n    endpoints_shapes = {\'Conv2d_1a_3x3\': [batch_size, 149, 149, 32],\n                        \'Conv2d_2a_3x3\': [batch_size, 147, 147, 32],\n                        \'Conv2d_2b_3x3\': [batch_size, 147, 147, 64],\n                        \'Mixed_3a\': [batch_size, 73, 73, 160],\n                        \'Mixed_4a\': [batch_size, 71, 71, 192],\n                        \'Mixed_5a\': [batch_size, 35, 35, 384],\n                        # 4 x Inception-A blocks\n                        \'Mixed_5b\': [batch_size, 35, 35, 384],\n                        \'Mixed_5c\': [batch_size, 35, 35, 384],\n                        \'Mixed_5d\': [batch_size, 35, 35, 384],\n                        \'Mixed_5e\': [batch_size, 35, 35, 384],\n                        # Reduction-A block\n                        \'Mixed_6a\': [batch_size, 17, 17, 1024],\n                        # 7 x Inception-B blocks\n                        \'Mixed_6b\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6c\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6d\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6e\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6f\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6g\': [batch_size, 17, 17, 1024],\n                        \'Mixed_6h\': [batch_size, 17, 17, 1024],\n                        # Reduction-A block\n                        \'Mixed_7a\': [batch_size, 8, 8, 1536],\n                        # 3 x Inception-C blocks\n                        \'Mixed_7b\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7c\': [batch_size, 8, 8, 1536],\n                        \'Mixed_7d\': [batch_size, 8, 8, 1536],\n                        # Logits and predictions\n                        \'AuxLogits\': [batch_size, num_classes],\n                        \'PreLogitsFlatten\': [batch_size, 1536],\n                        \'Logits\': [batch_size, num_classes],\n                        \'Predictions\': [batch_size, num_classes]}\n    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())\n    for endpoint_name in endpoints_shapes:\n      expected_shape = endpoints_shapes[endpoint_name]\n      self.assertTrue(endpoint_name in end_points)\n      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),\n                           expected_shape)\n\n  def testBuildBaseNetwork(self):\n    batch_size = 5\n    height, width = 299, 299\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    net, end_points = inception.inception_v4_base(inputs)\n    self.assertTrue(net.op.name.startswith(\n        \'InceptionV4/Mixed_7d\'))\n    self.assertListEqual(net.get_shape().as_list(), [batch_size, 8, 8, 1536])\n    expected_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    self.assertItemsEqual(end_points.keys(), expected_endpoints)\n    for name, op in end_points.iteritems():\n      self.assertTrue(op.name.startswith(\'InceptionV4/\' + name))\n\n  def testBuildOnlyUpToFinalEndpoint(self):\n    batch_size = 5\n    height, width = 299, 299\n    all_endpoints = [\n        \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\', \'Mixed_3a\',\n        \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\',\n        \'Mixed_6e\', \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\',\n        \'Mixed_7b\', \'Mixed_7c\', \'Mixed_7d\']\n    for index, endpoint in enumerate(all_endpoints):\n      with tf.Graph().as_default():\n        inputs = tf.random_uniform((batch_size, height, width, 3))\n        out_tensor, end_points = inception.inception_v4_base(\n            inputs, final_endpoint=endpoint)\n        self.assertTrue(out_tensor.op.name.startswith(\n            \'InceptionV4/\' + endpoint))\n        self.assertItemsEqual(all_endpoints[:index+1], end_points)\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    # Force all Variables to reside on the device.\n    with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n      inception.inception_v4(inputs, num_classes)\n    for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope=\'on_cpu\'):\n      self.assertDeviceEqual(v.device, \'/cpu:0\')\n    for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope=\'on_gpu\'):\n      self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    inputs = tf.random_uniform((batch_size, height, width, 3))\n    logits, end_points = inception.inception_v4(inputs, num_classes)\n    self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [batch_size, num_classes])\n    pre_pool = end_points[\'Mixed_7d\']\n    self.assertListEqual(pre_pool.get_shape().as_list(),\n                         [batch_size, 3, 3, 1536])\n\n  def testUnknownBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_v4(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'InceptionV4/Logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_v4(train_inputs, num_classes)\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_v4(eval_inputs,\n                                         num_classes,\n                                         is_training=False,\n                                         reuse=True)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/slim/nets/lenet.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a variant of the LeNet model definition.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef lenet(images, num_classes=10, is_training=False,\n          dropout_keep_prob=0.5,\n          prediction_fn=slim.softmax,\n          scope=\'LeNet\'):\n  """"""Creates a variant of the LeNet model.\n\n  Note that since the output is a set of \'logits\', the values fall in the\n  interval of (-infinity, infinity). Consequently, to convert the outputs to a\n  probability distribution over the characters, one will need to convert them\n  using the softmax function:\n\n        logits = lenet.lenet(images, is_training=False)\n        probabilities = tf.nn.softmax(logits)\n        predictions = tf.argmax(logits, 1)\n\n  Args:\n    images: A batch of `Tensors` of size [batch_size, height, width, channels].\n    num_classes: the number of classes in the dataset.\n    is_training: specifies whether or not we\'re currently training the model.\n      This variable will determine the behaviour of the dropout layer.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    prediction_fn: a function to get predictions out of logits.\n    scope: Optional variable_scope.\n\n  Returns:\n    logits: the pre-softmax activations, a tensor of size\n      [batch_size, `num_classes`]\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  """"""\n  end_points = {}\n\n  with tf.variable_scope(scope, \'LeNet\', [images, num_classes]):\n    net = slim.conv2d(images, 32, [5, 5], scope=\'conv1\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool1\')\n    net = slim.conv2d(net, 64, [5, 5], scope=\'conv2\')\n    net = slim.max_pool2d(net, [2, 2], 2, scope=\'pool2\')\n    net = slim.flatten(net)\n    end_points[\'Flatten\'] = net\n\n    net = slim.fully_connected(net, 1024, scope=\'fc3\')\n    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                       scope=\'dropout3\')\n    logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                  scope=\'fc4\')\n\n  end_points[\'Logits\'] = logits\n  end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n\n  return logits, end_points\nlenet.default_image_size = 28\n\n\ndef lenet_arg_scope(weight_decay=0.0):\n  """"""Defines the default lenet argument scope.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n\n  Returns:\n    An `arg_scope` to use for the inception v3 model.\n  """"""\n  with slim.arg_scope(\n      [slim.conv2d, slim.fully_connected],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n      activation_fn=tf.nn.relu) as sc:\n    return sc\n'"
model_zoo/models/slim/nets/nets_factory.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport functools\n\nimport tensorflow as tf\n\nfrom nets import alexnet\nfrom nets import cifarnet\nfrom nets import inception\nfrom nets import lenet\nfrom nets import overfeat\nfrom nets import resnet_v1\nfrom nets import resnet_v2\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\nnetworks_map = {\'alexnet_v2\': alexnet.alexnet_v2,\n                \'cifarnet\': cifarnet.cifarnet,\n                \'overfeat\': overfeat.overfeat,\n                \'vgg_a\': vgg.vgg_a,\n                \'vgg_16\': vgg.vgg_16,\n                \'vgg_19\': vgg.vgg_19,\n                \'inception_v1\': inception.inception_v1,\n                \'inception_v2\': inception.inception_v2,\n                \'inception_v3\': inception.inception_v3,\n                \'inception_v4\': inception.inception_v4,\n                \'inception_resnet_v2\': inception.inception_resnet_v2,\n                \'lenet\': lenet.lenet,\n                \'resnet_v1_50\': resnet_v1.resnet_v1_50,\n                \'resnet_v1_101\': resnet_v1.resnet_v1_101,\n                \'resnet_v1_152\': resnet_v1.resnet_v1_152,\n                \'resnet_v1_200\': resnet_v1.resnet_v1_200,\n                \'resnet_v2_50\': resnet_v2.resnet_v2_50,\n                \'resnet_v2_101\': resnet_v2.resnet_v2_101,\n                \'resnet_v2_152\': resnet_v2.resnet_v2_152,\n                \'resnet_v2_200\': resnet_v2.resnet_v2_200,\n               }\n\narg_scopes_map = {\'alexnet_v2\': alexnet.alexnet_v2_arg_scope,\n                  \'cifarnet\': cifarnet.cifarnet_arg_scope,\n                  \'overfeat\': overfeat.overfeat_arg_scope,\n                  \'vgg_a\': vgg.vgg_arg_scope,\n                  \'vgg_16\': vgg.vgg_arg_scope,\n                  \'vgg_19\': vgg.vgg_arg_scope,\n                  \'inception_v1\': inception.inception_v3_arg_scope,\n                  \'inception_v2\': inception.inception_v3_arg_scope,\n                  \'inception_v3\': inception.inception_v3_arg_scope,\n                  \'inception_v4\': inception.inception_v4_arg_scope,\n                  \'inception_resnet_v2\':\n                  inception.inception_resnet_v2_arg_scope,\n                  \'lenet\': lenet.lenet_arg_scope,\n                  \'resnet_v1_50\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_101\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_152\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v1_200\': resnet_v1.resnet_arg_scope,\n                  \'resnet_v2_50\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_101\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_152\': resnet_v2.resnet_arg_scope,\n                  \'resnet_v2_200\': resnet_v2.resnet_arg_scope,\n                 }\n\n\ndef get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):\n  """"""Returns a network_fn such as `logits, end_points = network_fn(images)`.\n\n  Args:\n    name: The name of the network.\n    num_classes: The number of classes to use for classification.\n    weight_decay: The l2 coefficient for the model weights.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    network_fn: A function that applies the model to a batch of images. It has\n      the following signature:\n        logits, end_points = network_fn(images)\n  Raises:\n    ValueError: If network `name` is not recognized.\n  """"""\n  if name not in networks_map:\n    raise ValueError(\'Name of network unknown %s\' % name)\n  arg_scope = arg_scopes_map[name](weight_decay=weight_decay)\n  func = networks_map[name]\n  @functools.wraps(func)\n  def network_fn(images):\n    with slim.arg_scope(arg_scope):\n      return func(images, num_classes, is_training=is_training)\n  if hasattr(func, \'default_image_size\'):\n    network_fn.default_image_size = func.default_image_size\n\n  return network_fn\n'"
model_zoo/models/slim/nets/nets_factory_test.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for slim.inception.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom nets import nets_factory\n\n\nclass NetworksTest(tf.test.TestCase):\n\n  def testGetNetworkFn(self):\n    batch_size = 5\n    num_classes = 1000\n    for net in nets_factory.networks_map:\n      with self.test_session():\n        net_fn = nets_factory.get_network_fn(net, num_classes)\n        # Most networks use 224 as their default_image_size\n        image_size = getattr(net_fn, \'default_image_size\', 224)\n        inputs = tf.random_uniform((batch_size, image_size, image_size, 3))\n        logits, end_points = net_fn(inputs)\n        self.assertTrue(isinstance(logits, tf.Tensor))\n        self.assertTrue(isinstance(end_points, dict))\n        self.assertEqual(logits.get_shape().as_list()[0], batch_size)\n        self.assertEqual(logits.get_shape().as_list()[-1], num_classes)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/slim/nets/overfeat.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the model definition for the OverFeat network.\n\nThe definition for the network was obtained from:\n  OverFeat: Integrated Recognition, Localization and Detection using\n  Convolutional Networks\n  Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n  Yann LeCun, 2014\n  http://arxiv.org/abs/1312.6229\n\nUsage:\n  with slim.arg_scope(overfeat.overfeat_arg_scope()):\n    outputs, end_points = overfeat.overfeat(inputs)\n\n@@overfeat\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef overfeat_arg_scope(weight_decay=0.0005):\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\'):\n      with slim.arg_scope([slim.max_pool2d], padding=\'VALID\') as arg_sc:\n        return arg_sc\n\n\ndef overfeat(inputs,\n             num_classes=1000,\n             is_training=True,\n             dropout_keep_prob=0.5,\n             spatial_squeeze=True,\n             scope=\'overfeat\'):\n  """"""Contains the model definition for the OverFeat network.\n\n  The definition for the network was obtained from:\n    OverFeat: Integrated Recognition, Localization and Detection using\n    Convolutional Networks\n    Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus and\n    Yann LeCun, 2014\n    http://arxiv.org/abs/1312.6229\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 231x231. To use in fully\n        convolutional mode, set spatial_squeeze to false.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n\n  """"""\n  with tf.variable_scope(scope, \'overfeat\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n                        scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.conv2d(net, 256, [5, 5], padding=\'VALID\', scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.conv2d(net, 512, [3, 3], scope=\'conv3\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv4\')\n      net = slim.conv2d(net, 1024, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      with slim.arg_scope([slim.conv2d],\n                          weights_initializer=trunc_normal(0.005),\n                          biases_initializer=tf.constant_initializer(0.1)):\n        # Use conv2d instead of fully_connected layers.\n        net = slim.conv2d(net, 3072, [6, 6], padding=\'VALID\', scope=\'fc6\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout6\')\n        net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                           scope=\'dropout7\')\n        net = slim.conv2d(net, num_classes, [1, 1],\n                          activation_fn=None,\n                          normalizer_fn=None,\n                          biases_initializer=tf.zeros_initializer,\n                          scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\noverfeat.default_image_size = 231\n'"
model_zoo/models/slim/nets/overfeat_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.overfeat.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import overfeat\n\nslim = tf.contrib.slim\n\n\nclass OverFeatTest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'overfeat/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1\',\n                        \'overfeat/pool1\',\n                        \'overfeat/conv2\',\n                        \'overfeat/pool2\',\n                        \'overfeat/conv3\',\n                        \'overfeat/conv4\',\n                        \'overfeat/conv5\',\n                        \'overfeat/pool5\',\n                        \'overfeat/fc6\',\n                        \'overfeat/fc7\',\n                        \'overfeat/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      overfeat.overfeat(inputs, num_classes)\n      expected_names = [\'overfeat/conv1/weights\',\n                        \'overfeat/conv1/biases\',\n                        \'overfeat/conv2/weights\',\n                        \'overfeat/conv2/biases\',\n                        \'overfeat/conv3/weights\',\n                        \'overfeat/conv3/biases\',\n                        \'overfeat/conv4/weights\',\n                        \'overfeat/conv4/biases\',\n                        \'overfeat/conv5/weights\',\n                        \'overfeat/conv5/biases\',\n                        \'overfeat/fc6/weights\',\n                        \'overfeat/fc6/biases\',\n                        \'overfeat/fc7/weights\',\n                        \'overfeat/fc7/biases\',\n                        \'overfeat/fc8/weights\',\n                        \'overfeat/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 231, 231\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 231, 231\n    eval_height, eval_width = 281, 281\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = overfeat.overfeat(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = overfeat.overfeat(eval_inputs, is_training=False,\n                                    spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 231, 231\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = overfeat.overfeat(inputs)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/slim/nets/resnet_utils.py,1,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains building blocks for various versions of Residual Networks.\n\nResidual networks (ResNets) were proposed in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Deep Residual Learning for Image Recognition. arXiv:1512.03385, 2015\n\nMore variants were introduced in:\n  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n  Identity Mappings in Deep Residual Networks. arXiv: 1603.05027, 2016\n\nWe can obtain different ResNet variants by changing the network depth, width,\nand form of residual unit. This module implements the infrastructure for\nbuilding them. Concrete ResNet units and full ResNet networks are implemented in\nthe accompanying resnet_v1.py and resnet_v2.py modules.\n\nCompared to https://github.com/KaimingHe/deep-residual-networks, in the current\nimplementation we subsample the output activations in the last residual unit of\neach block, instead of subsampling the input activations in the first residual\nunit of each block. The two implementations give identical results but our\nimplementation is more memory efficient.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\nclass Block(collections.namedtuple(\'Block\', [\'scope\', \'unit_fn\', \'args\'])):\n  """"""A named tuple describing a ResNet block.\n\n  Its parts are:\n    scope: The scope of the `Block`.\n    unit_fn: The ResNet unit function which takes as input a `Tensor` and\n      returns another `Tensor` with the output of the ResNet unit.\n    args: A list of length equal to the number of units in the `Block`. The list\n      contains one (depth, depth_bottleneck, stride) tuple for each unit in the\n      block to serve as argument to unit_fn.\n  """"""\n\n\ndef subsample(inputs, factor, scope=None):\n  """"""Subsamples the input along the spatial dimensions.\n\n  Args:\n    inputs: A `Tensor` of size [batch, height_in, width_in, channels].\n    factor: The subsampling factor.\n    scope: Optional variable_scope.\n\n  Returns:\n    output: A `Tensor` of size [batch, height_out, width_out, channels] with the\n      input, either intact (if factor == 1) or subsampled (if factor > 1).\n  """"""\n  if factor == 1:\n    return inputs\n  else:\n    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)\n\n\ndef conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D convolution with \'SAME\' padding.\n\n  When stride > 1, then we do explicit zero-padding, followed by conv2d with\n  \'VALID\' padding.\n\n  Note that\n\n     net = conv2d_same(inputs, num_outputs, 3, stride=stride)\n\n  is equivalent to\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding=\'SAME\')\n     net = subsample(net, factor=stride)\n\n  whereas\n\n     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding=\'SAME\')\n\n  is different when the input\'s height or width is even, which is why we add the\n  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().\n\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    num_outputs: An integer, the number of output filters.\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n  if stride == 1:\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,\n                       padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,\n                       rate=rate, padding=\'VALID\', scope=scope)\n\n\n@slim.add_arg_scope\ndef stack_blocks_dense(net, blocks, output_stride=None,\n                       outputs_collections=None):\n  """"""Stacks ResNet `Blocks` and controls output feature density.\n\n  First, this function creates scopes for the ResNet in the form of\n  \'block_name/unit_1\', \'block_name/unit_2\', etc.\n\n  Second, this function allows the user to explicitly control the ResNet\n  output_stride, which is the ratio of the input to output spatial resolution.\n  This is useful for dense prediction tasks such as semantic segmentation or\n  object detection.\n\n  Most ResNets consist of 4 ResNet blocks and subsample the activations by a\n  factor of 2 when transitioning between consecutive ResNet blocks. This results\n  to a nominal ResNet output_stride equal to 8. If we set the output_stride to\n  half the nominal network stride (e.g., output_stride=4), then we compute\n  responses twice.\n\n  Control of the output feature density is implemented by atrous convolution.\n\n  Args:\n    net: A `Tensor` of size [batch, height, width, channels].\n    blocks: A list of length equal to the number of ResNet `Blocks`. Each\n      element is a ResNet `Block` object describing the units in the `Block`.\n    output_stride: If `None`, then the output will be computed at the nominal\n      network stride. If output_stride is not `None`, it specifies the requested\n      ratio of input to output spatial resolution, which needs to be equal to\n      the product of unit strides from the start up to some level of the ResNet.\n      For example, if the ResNet employs units with strides 1, 2, 1, 3, 4, 1,\n      then valid values for the output_stride are 1, 2, 6, 24 or None (which\n      is equivalent to output_stride=24).\n    outputs_collections: Collection to add the ResNet block outputs.\n\n  Returns:\n    net: Output tensor with stride equal to the specified output_stride.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  # The current_stride variable keeps track of the effective stride of the\n  # activations. This allows us to invoke atrous convolution whenever applying\n  # the next residual unit would result in the activations having stride larger\n  # than the target output_stride.\n  current_stride = 1\n\n  # The atrous convolution rate parameter.\n  rate = 1\n\n  for block in blocks:\n    with tf.variable_scope(block.scope, \'block\', [net]) as sc:\n      for i, unit in enumerate(block.args):\n        if output_stride is not None and current_stride > output_stride:\n          raise ValueError(\'The target output_stride cannot be reached.\')\n\n        with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n          unit_depth, unit_depth_bottleneck, unit_stride = unit\n\n          # If we have reached the target output_stride, then we need to employ\n          # atrous convolution with stride=1 and multiply the atrous rate by the\n          # current unit\'s stride for use in subsequent layers.\n          if output_stride is not None and current_stride == output_stride:\n            net = block.unit_fn(net,\n                                depth=unit_depth,\n                                depth_bottleneck=unit_depth_bottleneck,\n                                stride=1,\n                                rate=rate)\n            rate *= unit_stride\n\n          else:\n            net = block.unit_fn(net,\n                                depth=unit_depth,\n                                depth_bottleneck=unit_depth_bottleneck,\n                                stride=unit_stride,\n                                rate=1)\n            current_stride *= unit_stride\n      net = slim.utils.collect_named_outputs(outputs_collections, sc.name, net)\n\n  if output_stride is not None and current_stride != output_stride:\n    raise ValueError(\'The target output_stride cannot be reached.\')\n\n  return net\n\n\ndef resnet_arg_scope(weight_decay=0.0001,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  """"""Defines the default ResNet arg scope.\n\n  TODO(gpapan): The batch-normalization related default values above are\n    appropriate for use in conjunction with the reference ResNet models\n    released at https://github.com/KaimingHe/deep-residual-networks. When\n    training ResNets from scratch, they might need to be tuned.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    batch_norm_decay: The moving average decay when estimating layer activation\n      statistics in batch normalization.\n    batch_norm_epsilon: Small constant to prevent division by zero when\n      normalizing activations by their variance in batch normalization.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the resnet models.\n  """"""\n  batch_norm_params = {\n      \'decay\': batch_norm_decay,\n      \'epsilon\': batch_norm_epsilon,\n      \'scale\': batch_norm_scale,\n      \'updates_collections\': tf.GraphKeys.UPDATE_OPS,\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(weight_decay),\n      weights_initializer=slim.variance_scaling_initializer(),\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      # The following implies padding=\'SAME\' for pool1, which makes feature\n      # alignment easier for dense prediction tasks. This is also used in\n      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n      # code of \'Deep Residual Learning for Image Recognition\' uses\n      # padding=\'VALID\' for pool1. You can switch to that choice by setting\n      # slim.arg_scope([slim.max_pool2d], padding=\'VALID\').\n      with slim.arg_scope([slim.max_pool2d], padding=\'SAME\') as arg_sc:\n        return arg_sc\n'"
model_zoo/models/slim/nets/resnet_v1.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the original form of Residual Networks.\n\nThe \'v1\' residual networks (ResNets) implemented in this module were proposed\nby:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nOther variants were introduced in:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe networks defined in this module utilize the bottleneck building block of\n[1] with projection shortcuts only for increasing depths. They employ batch\nnormalization *after* every weight layer. This is the architecture used by\nMSRA in the Imagenet and MSCOCO 2016 competition models ResNet-101 and\nResNet-152. See [2; Fig. 1a] for a comparison between the current \'v1\'\narchitecture and the alternative \'v2\' architecture of [2] which uses batch\nnormalization *before* every weight layer in the so-called full pre-activation\nunits.\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v1\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n      net, end_points = resnet_v1.resnet_v1_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\n\nresnet_arg_scope = resnet_utils.resnet_arg_scope\nslim = tf.contrib.slim\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN after convolutions.\n\n  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n  its definition. Note that we use here the bottleneck variant which has an\n  extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v1\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(inputs, depth, [1, 1], stride=stride,\n                             activation_fn=None, scope=\'shortcut\')\n\n    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           activation_fn=None, scope=\'conv3\')\n\n    output = tf.nn.relu(shortcut + residual)\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v1(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              reuse=None,\n              scope=None):\n  """"""Generator for v1 ResNet models.\n\n  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v1\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n        return net, end_points\nresnet_v1.default_image_size = 224\n\n\ndef resnet_v1_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 reuse=None,\n                 scope=\'resnet_v1_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v1_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)\n  ]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v1_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v1_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v1_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v1() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v1(inputs, blocks, num_classes, is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n'"
model_zoo/models/slim/nets/resnet_v1_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.resnet_v1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v1\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = dict(tf.get_collection(\'end_points\'))\n        return net, end_points\n\n  def testEndPointsV1(self):\n    """"""Test the end points of a tiny v1 bottleneck network.""""""\n    bottleneck = resnet_v1.bottleneck\n    blocks = [resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n              resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 1)])]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v1/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v1/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v1/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          depth, depth_bottleneck, stride = unit\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net,\n                                depth=depth,\n                                depth_bottleneck=depth_bottleneck,\n                                stride=stride,\n                                rate=1)\n    return net\n\n  def _atrousValues(self, bottleneck):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n\n    Args:\n      bottleneck: The bottleneck function.\n    """"""\n    blocks = [\n        resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n        resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 2)]),\n        resnet_utils.Block(\'block3\', bottleneck, [(16, 4, 1), (16, 4, 2)]),\n        resnet_utils.Block(\'block4\', bottleneck, [(32, 8, 1), (32, 8, 1)])\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.initialize_all_variables())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n  def testAtrousValuesBottleneck(self):\n    self._atrousValues(resnet_v1.bottleneck)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v1 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v1_small\'):\n    """"""A shallow and thin ResNet v1 for faster tests.""""""\n    bottleneck = resnet_v1.bottleneck\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(4, 1, 1)] * 2 + [(4, 1, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(8, 2, 1)] * 2 + [(8, 2, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(16, 4, 1)] * 2 + [(16, 4, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(32, 8, 1)] * 2)]\n    return resnet_v1.resnet_v1(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None, is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None, is_training=False,\n                                             global_pool=False)\n            sess.run(tf.initialize_all_variables())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None, global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/slim/nets/resnet_v2.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for the preactivation form of Residual Networks.\n\nResidual networks (ResNets) were originally proposed in:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n\nThe full preactivation \'v2\' ResNet variant implemented in this module was\nintroduced by:\n[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv: 1603.05027\n\nThe key difference of the full preactivation \'v2\' variant compared to the\n\'v1\' variant in [1] is the use of batch normalization before every weight layer.\nAnother difference is that \'v2\' ResNets do not include an activation function in\nthe main pathway. Also see [2; Fig. 4e].\n\nTypical use:\n\n   from tensorflow.contrib.slim.nets import resnet_v2\n\nResNet-101 for image classification into 1000 classes:\n\n   # inputs has shape [batch, 224, 224, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope()):\n      net, end_points = resnet_v2.resnet_v2_101(inputs, 1000, is_training=False)\n\nResNet-101 for semantic segmentation into 21 classes:\n\n   # inputs has shape [batch, 513, 513, 3]\n   with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):\n      net, end_points = resnet_v2.resnet_v2_101(inputs,\n                                                21,\n                                                is_training=False,\n                                                global_pool=False,\n                                                output_stride=16)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import resnet_utils\n\nslim = tf.contrib.slim\nresnet_arg_scope = resnet_utils.resnet_arg_scope\n\n\n@slim.add_arg_scope\ndef bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,\n               outputs_collections=None, scope=None):\n  """"""Bottleneck residual unit variant with BN before convolutions.\n\n  This is the full preactivation residual unit variant proposed in [2]. See\n  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck\n  variant which has an extra bottleneck layer.\n\n  When putting together two consecutive ResNet blocks that use this unit, one\n  should use stride = 2 in the last unit of the first block.\n\n  Args:\n    inputs: A tensor of size [batch, height, width, channels].\n    depth: The depth of the ResNet unit output.\n    depth_bottleneck: The depth of the bottleneck layers.\n    stride: The ResNet unit\'s stride. Determines the amount of downsampling of\n      the units output compared to its input.\n    rate: An integer, rate for atrous convolution.\n    outputs_collections: Collection to add the ResNet unit output.\n    scope: Optional variable_scope.\n\n  Returns:\n    The ResNet unit\'s output.\n  """"""\n  with tf.variable_scope(scope, \'bottleneck_v2\', [inputs]) as sc:\n    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope=\'preact\')\n    if depth == depth_in:\n      shortcut = resnet_utils.subsample(inputs, stride, \'shortcut\')\n    else:\n      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,\n                             normalizer_fn=None, activation_fn=None,\n                             scope=\'shortcut\')\n\n    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,\n                           scope=\'conv1\')\n    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n                                        rate=rate, scope=\'conv2\')\n    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n                           normalizer_fn=None, activation_fn=None,\n                           scope=\'conv3\')\n\n    output = shortcut + residual\n\n    return slim.utils.collect_named_outputs(outputs_collections,\n                                            sc.original_name_scope,\n                                            output)\n\n\ndef resnet_v2(inputs,\n              blocks,\n              num_classes=None,\n              is_training=True,\n              global_pool=True,\n              output_stride=None,\n              include_root_block=True,\n              reuse=None,\n              scope=None):\n  """"""Generator for v2 (preactivation) ResNet models.\n\n  This function generates a family of ResNet v2 models. See the resnet_v2_*()\n  methods for specific model instantiations, obtained by selecting different\n  block instantiations that produce ResNets of various depths.\n\n  Training for image classification on Imagenet is usually done with [224, 224]\n  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n  block for the ResNets defined in [1] that have nominal stride equal to 32.\n  However, for dense prediction tasks we advise that one uses inputs with\n  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n  this case the feature maps at the ResNet output will have spatial shape\n  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n  and corners exactly aligned with the input image corners, which greatly\n  facilitates alignment of the features to the image. Using as input [225, 225]\n  images results in [8, 8] feature maps at the output of the last ResNet block.\n\n  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n  have nominal stride equal to 32 and a good choice in FCN mode is to use\n  output_stride=16 in order to increase the density of the computed features at\n  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    blocks: A list of length equal to the number of ResNet blocks. Each element\n      is a resnet_utils.Block object describing the units in the block.\n    num_classes: Number of predicted classes for classification tasks. If None\n      we return the features before the logit layer.\n    is_training: whether is training or not.\n    global_pool: If True, we perform global average pooling before computing the\n      logits. Set to True for image classification, False for dense prediction.\n    output_stride: If None, then the output will be computed at the nominal\n      network stride. If output_stride is not None, it specifies the requested\n      ratio of input to output spatial resolution.\n    include_root_block: If True, include the initial convolution followed by\n      max-pooling, if False excludes it. If excluded, `inputs` should be the\n      results of an activation-less convolution.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    scope: Optional variable_scope.\n\n\n  Returns:\n    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n      If global_pool is False, then height_out and width_out are reduced by a\n      factor of output_stride compared to the respective height_in and width_in,\n      else both height_out and width_out equal one. If num_classes is None, then\n      net is the output of the last ResNet block, potentially after global\n      average pooling. If num_classes is not None, net contains the pre-softmax\n      activations.\n    end_points: A dictionary from components of the network to the corresponding\n      activation.\n\n  Raises:\n    ValueError: If the target output_stride is not valid.\n  """"""\n  with tf.variable_scope(scope, \'resnet_v2\', [inputs], reuse=reuse) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    with slim.arg_scope([slim.conv2d, bottleneck,\n                         resnet_utils.stack_blocks_dense],\n                        outputs_collections=end_points_collection):\n      with slim.arg_scope([slim.batch_norm], is_training=is_training):\n        net = inputs\n        if include_root_block:\n          if output_stride is not None:\n            if output_stride % 4 != 0:\n              raise ValueError(\'The output_stride needs to be a multiple of 4.\')\n            output_stride /= 4\n          # We do not include batch normalization or activation functions in\n          # conv1 because the first ResNet unit will perform these. Cf.\n          # Appendix of [2].\n          with slim.arg_scope([slim.conv2d],\n                              activation_fn=None, normalizer_fn=None):\n            net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope=\'conv1\')\n          net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'pool1\')\n        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)\n        # This is needed because the pre-activation variant does not have batch\n        # normalization or activation functions in the residual unit output. See\n        # Appendix of [2].\n        net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope=\'postnorm\')\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], name=\'pool5\', keep_dims=True)\n        if num_classes is not None:\n          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                            normalizer_fn=None, scope=\'logits\')\n        # Convert end_points_collection into a dictionary of end_points.\n        end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n        if num_classes is not None:\n          end_points[\'predictions\'] = slim.softmax(net, scope=\'predictions\')\n        return net, end_points\nresnet_v2.default_image_size = 224\n\n\ndef resnet_v2_50(inputs,\n                 num_classes=None,\n                 is_training=True,\n                 global_pool=True,\n                 output_stride=None,\n                 reuse=None,\n                 scope=\'resnet_v2_50\'):\n  """"""ResNet-50 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 5 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v2_101(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v2_101\'):\n  """"""ResNet-101 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 3 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 22 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v2_152(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v2_152\'):\n  """"""ResNet-152 model of [1]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 7 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n\n\ndef resnet_v2_200(inputs,\n                  num_classes=None,\n                  is_training=True,\n                  global_pool=True,\n                  output_stride=None,\n                  reuse=None,\n                  scope=\'resnet_v2_200\'):\n  """"""ResNet-200 model of [2]. See resnet_v2() for arg and return description.""""""\n  blocks = [\n      resnet_utils.Block(\n          \'block1\', bottleneck, [(256, 64, 1)] * 2 + [(256, 64, 2)]),\n      resnet_utils.Block(\n          \'block2\', bottleneck, [(512, 128, 1)] * 23 + [(512, 128, 2)]),\n      resnet_utils.Block(\n          \'block3\', bottleneck, [(1024, 256, 1)] * 35 + [(1024, 256, 2)]),\n      resnet_utils.Block(\n          \'block4\', bottleneck, [(2048, 512, 1)] * 3)]\n  return resnet_v2(inputs, blocks, num_classes, is_training=is_training,\n                   global_pool=global_pool, output_stride=output_stride,\n                   include_root_block=True, reuse=reuse, scope=scope)\n'"
model_zoo/models/slim/nets/resnet_v2_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.resnet_v2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom nets import resnet_utils\nfrom nets import resnet_v2\n\nslim = tf.contrib.slim\n\n\ndef create_test_input(batch_size, height, width, channels):\n  """"""Create test input tensor.\n\n  Args:\n    batch_size: The number of images per batch or `None` if unknown.\n    height: The height of each image or `None` if unknown.\n    width: The width of each image or `None` if unknown.\n    channels: The number of channels per image or `None` if unknown.\n\n  Returns:\n    Either a placeholder `Tensor` of dimension\n      [batch_size, height, width, channels] if any of the inputs are `None` or a\n    constant `Tensor` with the mesh grid values along the spatial dimensions.\n  """"""\n  if None in [batch_size, height, width, channels]:\n    return tf.placeholder(tf.float32, (batch_size, height, width, channels))\n  else:\n    return tf.to_float(\n        np.tile(\n            np.reshape(\n                np.reshape(np.arange(height), [height, 1]) +\n                np.reshape(np.arange(width), [1, width]),\n                [1, height, width, 1]),\n            [batch_size, 1, 1, channels]))\n\n\nclass ResnetUtilsTest(tf.test.TestCase):\n\n  def testSubsampleThreeByThree(self):\n    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testSubsampleFourByFour(self):\n    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])\n    x = resnet_utils.subsample(x, 2)\n    expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])\n    with self.test_session():\n      self.assertAllClose(x.eval(), expected.eval())\n\n  def testConv2DSameEven(self):\n    n, n2 = 4, 2\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 26],\n                               [28, 48, 66, 37],\n                               [43, 66, 84, 46],\n                               [26, 37, 46, 22]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43],\n                               [43, 84]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = tf.to_float([[48, 37],\n                               [37, 22]])\n    y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def testConv2DSameOdd(self):\n    n, n2 = 5, 3\n\n    # Input image.\n    x = create_test_input(1, n, n, 1)\n\n    # Convolution kernel.\n    w = create_test_input(1, 3, 3, 1)\n    w = tf.reshape(w, [3, 3, 1, 1])\n\n    tf.get_variable(\'Conv/weights\', initializer=w)\n    tf.get_variable(\'Conv/biases\', initializer=tf.zeros([1]))\n    tf.get_variable_scope().reuse_variables()\n\n    y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope=\'Conv\')\n    y1_expected = tf.to_float([[14, 28, 43, 58, 34],\n                               [28, 48, 66, 84, 46],\n                               [43, 66, 84, 102, 55],\n                               [58, 84, 102, 120, 64],\n                               [34, 46, 55, 64, 30]])\n    y1_expected = tf.reshape(y1_expected, [1, n, n, 1])\n\n    y2 = resnet_utils.subsample(y1, 2)\n    y2_expected = tf.to_float([[14, 43, 34],\n                               [43, 84, 55],\n                               [34, 55, 30]])\n    y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])\n\n    y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope=\'Conv\')\n    y3_expected = y2_expected\n\n    y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope=\'Conv\')\n    y4_expected = y2_expected\n\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      self.assertAllClose(y1.eval(), y1_expected.eval())\n      self.assertAllClose(y2.eval(), y2_expected.eval())\n      self.assertAllClose(y3.eval(), y3_expected.eval())\n      self.assertAllClose(y4.eval(), y4_expected.eval())\n\n  def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):\n    """"""A plain ResNet without extra layers before or after the ResNet blocks.""""""\n    with tf.variable_scope(scope, values=[inputs]):\n      with slim.arg_scope([slim.conv2d], outputs_collections=\'end_points\'):\n        net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)\n        end_points = dict(tf.get_collection(\'end_points\'))\n        return net, end_points\n\n  def testEndPointsV2(self):\n    """"""Test the end points of a tiny v2 bottleneck network.""""""\n    bottleneck = resnet_v2.bottleneck\n    blocks = [resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n              resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 1)])]\n    inputs = create_test_input(2, 32, 16, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_plain(inputs, blocks, scope=\'tiny\')\n    expected = [\n        \'tiny/block1/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block1/unit_2/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_1/bottleneck_v2/shortcut\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_1/bottleneck_v2/conv3\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv1\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv2\',\n        \'tiny/block2/unit_2/bottleneck_v2/conv3\']\n    self.assertItemsEqual(expected, end_points)\n\n  def _stack_blocks_nondense(self, net, blocks):\n    """"""A simplified ResNet Block stacker without output stride control.""""""\n    for block in blocks:\n      with tf.variable_scope(block.scope, \'block\', [net]):\n        for i, unit in enumerate(block.args):\n          depth, depth_bottleneck, stride = unit\n          with tf.variable_scope(\'unit_%d\' % (i + 1), values=[net]):\n            net = block.unit_fn(net,\n                                depth=depth,\n                                depth_bottleneck=depth_bottleneck,\n                                stride=stride,\n                                rate=1)\n    return net\n\n  def _atrousValues(self, bottleneck):\n    """"""Verify the values of dense feature extraction by atrous convolution.\n\n    Make sure that dense feature extraction by stack_blocks_dense() followed by\n    subsampling gives identical results to feature extraction at the nominal\n    network output stride using the simple self._stack_blocks_nondense() above.\n\n    Args:\n      bottleneck: The bottleneck function.\n    """"""\n    blocks = [\n        resnet_utils.Block(\'block1\', bottleneck, [(4, 1, 1), (4, 1, 2)]),\n        resnet_utils.Block(\'block2\', bottleneck, [(8, 2, 1), (8, 2, 2)]),\n        resnet_utils.Block(\'block3\', bottleneck, [(16, 4, 1), (16, 4, 2)]),\n        resnet_utils.Block(\'block4\', bottleneck, [(32, 8, 1), (32, 8, 1)])\n    ]\n    nominal_stride = 8\n\n    # Test both odd and even input dimensions.\n    height = 30\n    width = 31\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      with slim.arg_scope([slim.batch_norm], is_training=False):\n        for output_stride in [1, 2, 4, 8, None]:\n          with tf.Graph().as_default():\n            with self.test_session() as sess:\n              tf.set_random_seed(0)\n              inputs = create_test_input(1, height, width, 3)\n              # Dense feature extraction followed by subsampling.\n              output = resnet_utils.stack_blocks_dense(inputs,\n                                                       blocks,\n                                                       output_stride)\n              if output_stride is None:\n                factor = 1\n              else:\n                factor = nominal_stride // output_stride\n\n              output = resnet_utils.subsample(output, factor)\n              # Make the two networks use the same weights.\n              tf.get_variable_scope().reuse_variables()\n              # Feature extraction at the nominal network rate.\n              expected = self._stack_blocks_nondense(inputs, blocks)\n              sess.run(tf.initialize_all_variables())\n              output, expected = sess.run([output, expected])\n              self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)\n\n  def testAtrousValuesBottleneck(self):\n    self._atrousValues(resnet_v2.bottleneck)\n\n\nclass ResnetCompleteNetworkTest(tf.test.TestCase):\n  """"""Tests with complete small ResNet v2 networks.""""""\n\n  def _resnet_small(self,\n                    inputs,\n                    num_classes=None,\n                    is_training=True,\n                    global_pool=True,\n                    output_stride=None,\n                    include_root_block=True,\n                    reuse=None,\n                    scope=\'resnet_v2_small\'):\n    """"""A shallow and thin ResNet v2 for faster tests.""""""\n    bottleneck = resnet_v2.bottleneck\n    blocks = [\n        resnet_utils.Block(\n            \'block1\', bottleneck, [(4, 1, 1)] * 2 + [(4, 1, 2)]),\n        resnet_utils.Block(\n            \'block2\', bottleneck, [(8, 2, 1)] * 2 + [(8, 2, 2)]),\n        resnet_utils.Block(\n            \'block3\', bottleneck, [(16, 4, 1)] * 2 + [(16, 4, 2)]),\n        resnet_utils.Block(\n            \'block4\', bottleneck, [(32, 8, 1)] * 2)]\n    return resnet_v2.resnet_v2(inputs, blocks, num_classes,\n                               is_training=is_training,\n                               global_pool=global_pool,\n                               output_stride=output_stride,\n                               include_root_block=include_root_block,\n                               reuse=reuse,\n                               scope=scope)\n\n  def testClassificationEndPoints(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, end_points = self._resnet_small(inputs, num_classes,\n                                              global_pool=global_pool,\n                                              scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(), [2, 1, 1, num_classes])\n    self.assertTrue(\'predictions\' in end_points)\n    self.assertListEqual(end_points[\'predictions\'].get_shape().as_list(),\n                         [2, 1, 1, num_classes])\n\n  def testClassificationShapes(self):\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(2, 224, 224, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 28, 28, 4],\n          \'resnet/block2\': [2, 14, 14, 8],\n          \'resnet/block3\': [2, 7, 7, 16],\n          \'resnet/block4\': [2, 7, 7, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 21, 21, 8],\n          \'resnet/block3\': [2, 11, 11, 16],\n          \'resnet/block4\': [2, 11, 11, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testRootlessFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    inputs = create_test_input(2, 128, 128, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs, num_classes,\n                                         global_pool=global_pool,\n                                         include_root_block=False,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 64, 64, 4],\n          \'resnet/block2\': [2, 32, 32, 8],\n          \'resnet/block3\': [2, 16, 16, 16],\n          \'resnet/block4\': [2, 16, 16, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalEndpointShapes(self):\n    global_pool = False\n    num_classes = 10\n    output_stride = 8\n    inputs = create_test_input(2, 321, 321, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      _, end_points = self._resnet_small(inputs,\n                                         num_classes,\n                                         global_pool=global_pool,\n                                         output_stride=output_stride,\n                                         scope=\'resnet\')\n      endpoint_to_shape = {\n          \'resnet/block1\': [2, 41, 41, 4],\n          \'resnet/block2\': [2, 41, 41, 8],\n          \'resnet/block3\': [2, 41, 41, 16],\n          \'resnet/block4\': [2, 41, 41, 32]}\n      for endpoint in endpoint_to_shape:\n        shape = endpoint_to_shape[endpoint]\n        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)\n\n  def testAtrousFullyConvolutionalValues(self):\n    """"""Verify dense feature extraction with atrous convolution.""""""\n    nominal_stride = 32\n    for output_stride in [4, 8, 16, 32, None]:\n      with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n        with tf.Graph().as_default():\n          with self.test_session() as sess:\n            tf.set_random_seed(0)\n            inputs = create_test_input(2, 81, 81, 3)\n            # Dense feature extraction followed by subsampling.\n            output, _ = self._resnet_small(inputs, None,\n                                           is_training=False,\n                                           global_pool=False,\n                                           output_stride=output_stride)\n            if output_stride is None:\n              factor = 1\n            else:\n              factor = nominal_stride // output_stride\n            output = resnet_utils.subsample(output, factor)\n            # Make the two networks use the same weights.\n            tf.get_variable_scope().reuse_variables()\n            # Feature extraction at the nominal network rate.\n            expected, _ = self._resnet_small(inputs, None,\n                                             is_training=False,\n                                             global_pool=False)\n            sess.run(tf.initialize_all_variables())\n            self.assertAllClose(output.eval(), expected.eval(),\n                                atol=1e-4, rtol=1e-4)\n\n  def testUnknownBatchSize(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = True\n    num_classes = 10\n    inputs = create_test_input(None, height, width, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      logits, _ = self._resnet_small(inputs, num_classes,\n                                     global_pool=global_pool,\n                                     scope=\'resnet\')\n    self.assertTrue(logits.op.name.startswith(\'resnet/logits\'))\n    self.assertListEqual(logits.get_shape().as_list(),\n                         [None, 1, 1, num_classes])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 1, 1, num_classes))\n\n  def testFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs, None,\n                                     global_pool=global_pool)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 3, 3, 32))\n\n  def testAtrousFullyConvolutionalUnknownHeightWidth(self):\n    batch = 2\n    height, width = 65, 65\n    global_pool = False\n    output_stride = 8\n    inputs = create_test_input(batch, None, None, 3)\n    with slim.arg_scope(resnet_utils.resnet_arg_scope()):\n      output, _ = self._resnet_small(inputs,\n                                     None,\n                                     global_pool=global_pool,\n                                     output_stride=output_stride)\n    self.assertListEqual(output.get_shape().as_list(),\n                         [batch, None, None, 32])\n    images = create_test_input(batch, height, width, 3)\n    with self.test_session() as sess:\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEqual(output.shape, (batch, 9, 9, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/slim/nets/vgg.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains model definitions for versions of the Oxford VGG network.\n\nThese model definitions were introduced in the following technical report:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n\nUsage:\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_a(inputs)\n\n  with slim.arg_scope(vgg.vgg_arg_scope()):\n    outputs, end_points = vgg.vgg_16(inputs)\n\n@@vgg_a\n@@vgg_16\n@@vgg_19\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n  """"""Defines the VGG arg scope.\n\n  Args:\n    weight_decay: The l2 regularization coefficient.\n\n  Returns:\n    An arg_scope.\n  """"""\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      activation_fn=tf.nn.relu,\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_initializer=tf.zeros_initializer):\n    with slim.arg_scope([slim.conv2d], padding=\'SAME\') as arg_sc:\n      return arg_sc\n\n\ndef vgg_a(inputs,\n          num_classes=1000,\n          is_training=True,\n          dropout_keep_prob=0.5,\n          spatial_squeeze=True,\n          scope=\'vgg_a\'):\n  """"""Oxford Net VGG 11-Layers version A Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_a\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 1, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=\'VALID\', scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_a.default_image_size = 224\n\n\ndef vgg_16(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_16\'):\n  """"""Oxford Net VGG 16-Layers version D Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_16\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=\'VALID\', scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_16.default_image_size = 224\n\n\ndef vgg_19(inputs,\n           num_classes=1000,\n           is_training=True,\n           dropout_keep_prob=0.5,\n           spatial_squeeze=True,\n           scope=\'vgg_19\'):\n  """"""Oxford Net VGG 19-Layers version E Example.\n\n  Note: All the fully_connected layers have been transformed to conv2d layers.\n        To use in classification mode, resize input to 224x224.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes.\n    is_training: whether or not the model is being trained.\n    dropout_keep_prob: the probability that activations are kept in the dropout\n      layers during training.\n    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\n      outputs. Useful to remove unnecessary dimensions for classification.\n    scope: Optional scope for the variables.\n\n  Returns:\n    the last op containing the log predictions and end_points dict.\n  """"""\n  with tf.variable_scope(scope, \'vgg_19\', [inputs]) as sc:\n    end_points_collection = sc.name + \'_end_points\'\n    # Collect outputs for conv2d, fully_connected and max_pool2d.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n                        outputs_collections=end_points_collection):\n      net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool2\')\n      net = slim.repeat(net, 4, slim.conv2d, 256, [3, 3], scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool3\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool4\')\n      net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope=\'conv5\')\n      net = slim.max_pool2d(net, [2, 2], scope=\'pool5\')\n      # Use conv2d instead of fully_connected layers.\n      net = slim.conv2d(net, 4096, [7, 7], padding=\'VALID\', scope=\'fc6\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout6\')\n      net = slim.conv2d(net, 4096, [1, 1], scope=\'fc7\')\n      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n                         scope=\'dropout7\')\n      net = slim.conv2d(net, num_classes, [1, 1],\n                        activation_fn=None,\n                        normalizer_fn=None,\n                        scope=\'fc8\')\n      # Convert end_points_collection into a end_point dict.\n      end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n      if spatial_squeeze:\n        net = tf.squeeze(net, [1, 2], name=\'fc8/squeezed\')\n        end_points[sc.name + \'/fc8\'] = net\n      return net, end_points\nvgg_19.default_image_size = 224\n\n# Alias\nvgg_d = vgg_16\nvgg_e = vgg_19\n'"
model_zoo/models/slim/nets/vgg_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.nets.vgg.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom nets import vgg\n\nslim = tf.contrib.slim\n\n\nclass VGGATest(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_a/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1\',\n                        \'vgg_a/pool1\',\n                        \'vgg_a/conv2/conv2_1\',\n                        \'vgg_a/pool2\',\n                        \'vgg_a/conv3/conv3_1\',\n                        \'vgg_a/conv3/conv3_2\',\n                        \'vgg_a/pool3\',\n                        \'vgg_a/conv4/conv4_1\',\n                        \'vgg_a/conv4/conv4_2\',\n                        \'vgg_a/pool4\',\n                        \'vgg_a/conv5/conv5_1\',\n                        \'vgg_a/conv5/conv5_2\',\n                        \'vgg_a/pool5\',\n                        \'vgg_a/fc6\',\n                        \'vgg_a/fc7\',\n                        \'vgg_a/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_a(inputs, num_classes)\n      expected_names = [\'vgg_a/conv1/conv1_1/weights\',\n                        \'vgg_a/conv1/conv1_1/biases\',\n                        \'vgg_a/conv2/conv2_1/weights\',\n                        \'vgg_a/conv2/conv2_1/biases\',\n                        \'vgg_a/conv3/conv3_1/weights\',\n                        \'vgg_a/conv3/conv3_1/biases\',\n                        \'vgg_a/conv3/conv3_2/weights\',\n                        \'vgg_a/conv3/conv3_2/biases\',\n                        \'vgg_a/conv4/conv4_1/weights\',\n                        \'vgg_a/conv4/conv4_1/biases\',\n                        \'vgg_a/conv4/conv4_2/weights\',\n                        \'vgg_a/conv4/conv4_2/biases\',\n                        \'vgg_a/conv5/conv5_1/weights\',\n                        \'vgg_a/conv5/conv5_1/biases\',\n                        \'vgg_a/conv5/conv5_2/weights\',\n                        \'vgg_a/conv5/conv5_2/biases\',\n                        \'vgg_a/fc6/weights\',\n                        \'vgg_a/fc6/biases\',\n                        \'vgg_a/fc7/weights\',\n                        \'vgg_a/fc7/biases\',\n                        \'vgg_a/fc8/weights\',\n                        \'vgg_a/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_a(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_a(eval_inputs, is_training=False,\n                            spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_a(inputs)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG16Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_16/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1\',\n                        \'vgg_16/conv1/conv1_2\',\n                        \'vgg_16/pool1\',\n                        \'vgg_16/conv2/conv2_1\',\n                        \'vgg_16/conv2/conv2_2\',\n                        \'vgg_16/pool2\',\n                        \'vgg_16/conv3/conv3_1\',\n                        \'vgg_16/conv3/conv3_2\',\n                        \'vgg_16/conv3/conv3_3\',\n                        \'vgg_16/pool3\',\n                        \'vgg_16/conv4/conv4_1\',\n                        \'vgg_16/conv4/conv4_2\',\n                        \'vgg_16/conv4/conv4_3\',\n                        \'vgg_16/pool4\',\n                        \'vgg_16/conv5/conv5_1\',\n                        \'vgg_16/conv5/conv5_2\',\n                        \'vgg_16/conv5/conv5_3\',\n                        \'vgg_16/pool5\',\n                        \'vgg_16/fc6\',\n                        \'vgg_16/fc7\',\n                        \'vgg_16/fc8\'\n                       ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_16(inputs, num_classes)\n      expected_names = [\'vgg_16/conv1/conv1_1/weights\',\n                        \'vgg_16/conv1/conv1_1/biases\',\n                        \'vgg_16/conv1/conv1_2/weights\',\n                        \'vgg_16/conv1/conv1_2/biases\',\n                        \'vgg_16/conv2/conv2_1/weights\',\n                        \'vgg_16/conv2/conv2_1/biases\',\n                        \'vgg_16/conv2/conv2_2/weights\',\n                        \'vgg_16/conv2/conv2_2/biases\',\n                        \'vgg_16/conv3/conv3_1/weights\',\n                        \'vgg_16/conv3/conv3_1/biases\',\n                        \'vgg_16/conv3/conv3_2/weights\',\n                        \'vgg_16/conv3/conv3_2/biases\',\n                        \'vgg_16/conv3/conv3_3/weights\',\n                        \'vgg_16/conv3/conv3_3/biases\',\n                        \'vgg_16/conv4/conv4_1/weights\',\n                        \'vgg_16/conv4/conv4_1/biases\',\n                        \'vgg_16/conv4/conv4_2/weights\',\n                        \'vgg_16/conv4/conv4_2/biases\',\n                        \'vgg_16/conv4/conv4_3/weights\',\n                        \'vgg_16/conv4/conv4_3/biases\',\n                        \'vgg_16/conv5/conv5_1/weights\',\n                        \'vgg_16/conv5/conv5_1/biases\',\n                        \'vgg_16/conv5/conv5_2/weights\',\n                        \'vgg_16/conv5/conv5_2/biases\',\n                        \'vgg_16/conv5/conv5_3/weights\',\n                        \'vgg_16/conv5/conv5_3/biases\',\n                        \'vgg_16/fc6/weights\',\n                        \'vgg_16/fc6/biases\',\n                        \'vgg_16/fc7/weights\',\n                        \'vgg_16/fc7/biases\',\n                        \'vgg_16/fc8/weights\',\n                        \'vgg_16/fc8/biases\',\n                       ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_16(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_16(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_16(inputs)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\n\nclass VGG19Test(tf.test.TestCase):\n\n  def testBuild(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/squeezed\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testFullyConvolutional(self):\n    batch_size = 1\n    height, width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False)\n      self.assertEquals(logits.op.name, \'vgg_19/fc8/BiasAdd\')\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, 2, 2, num_classes])\n\n  def testEndPoints(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1\',\n          \'vgg_19/conv1/conv1_2\',\n          \'vgg_19/pool1\',\n          \'vgg_19/conv2/conv2_1\',\n          \'vgg_19/conv2/conv2_2\',\n          \'vgg_19/pool2\',\n          \'vgg_19/conv3/conv3_1\',\n          \'vgg_19/conv3/conv3_2\',\n          \'vgg_19/conv3/conv3_3\',\n          \'vgg_19/conv3/conv3_4\',\n          \'vgg_19/pool3\',\n          \'vgg_19/conv4/conv4_1\',\n          \'vgg_19/conv4/conv4_2\',\n          \'vgg_19/conv4/conv4_3\',\n          \'vgg_19/conv4/conv4_4\',\n          \'vgg_19/pool4\',\n          \'vgg_19/conv5/conv5_1\',\n          \'vgg_19/conv5/conv5_2\',\n          \'vgg_19/conv5/conv5_3\',\n          \'vgg_19/conv5/conv5_4\',\n          \'vgg_19/pool5\',\n          \'vgg_19/fc6\',\n          \'vgg_19/fc7\',\n          \'vgg_19/fc8\'\n      ]\n      self.assertSetEqual(set(end_points.keys()), set(expected_names))\n\n  def testModelVariables(self):\n    batch_size = 5\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      vgg.vgg_19(inputs, num_classes)\n      expected_names = [\n          \'vgg_19/conv1/conv1_1/weights\',\n          \'vgg_19/conv1/conv1_1/biases\',\n          \'vgg_19/conv1/conv1_2/weights\',\n          \'vgg_19/conv1/conv1_2/biases\',\n          \'vgg_19/conv2/conv2_1/weights\',\n          \'vgg_19/conv2/conv2_1/biases\',\n          \'vgg_19/conv2/conv2_2/weights\',\n          \'vgg_19/conv2/conv2_2/biases\',\n          \'vgg_19/conv3/conv3_1/weights\',\n          \'vgg_19/conv3/conv3_1/biases\',\n          \'vgg_19/conv3/conv3_2/weights\',\n          \'vgg_19/conv3/conv3_2/biases\',\n          \'vgg_19/conv3/conv3_3/weights\',\n          \'vgg_19/conv3/conv3_3/biases\',\n          \'vgg_19/conv3/conv3_4/weights\',\n          \'vgg_19/conv3/conv3_4/biases\',\n          \'vgg_19/conv4/conv4_1/weights\',\n          \'vgg_19/conv4/conv4_1/biases\',\n          \'vgg_19/conv4/conv4_2/weights\',\n          \'vgg_19/conv4/conv4_2/biases\',\n          \'vgg_19/conv4/conv4_3/weights\',\n          \'vgg_19/conv4/conv4_3/biases\',\n          \'vgg_19/conv4/conv4_4/weights\',\n          \'vgg_19/conv4/conv4_4/biases\',\n          \'vgg_19/conv5/conv5_1/weights\',\n          \'vgg_19/conv5/conv5_1/biases\',\n          \'vgg_19/conv5/conv5_2/weights\',\n          \'vgg_19/conv5/conv5_2/biases\',\n          \'vgg_19/conv5/conv5_3/weights\',\n          \'vgg_19/conv5/conv5_3/biases\',\n          \'vgg_19/conv5/conv5_4/weights\',\n          \'vgg_19/conv5/conv5_4/biases\',\n          \'vgg_19/fc6/weights\',\n          \'vgg_19/fc6/biases\',\n          \'vgg_19/fc7/weights\',\n          \'vgg_19/fc7/biases\',\n          \'vgg_19/fc8/weights\',\n          \'vgg_19/fc8/biases\',\n      ]\n      model_variables = [v.op.name for v in slim.get_model_variables()]\n      self.assertSetEqual(set(model_variables), set(expected_names))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 224, 224\n    num_classes = 1000\n    with self.test_session():\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      predictions = tf.argmax(logits, 1)\n      self.assertListEqual(predictions.get_shape().as_list(), [batch_size])\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 2\n    eval_batch_size = 1\n    train_height, train_width = 224, 224\n    eval_height, eval_width = 256, 256\n    num_classes = 1000\n    with self.test_session():\n      train_inputs = tf.random_uniform(\n          (train_batch_size, train_height, train_width, 3))\n      logits, _ = vgg.vgg_19(train_inputs)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [train_batch_size, num_classes])\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform(\n          (eval_batch_size, eval_height, eval_width, 3))\n      logits, _ = vgg.vgg_19(eval_inputs, is_training=False,\n                             spatial_squeeze=False)\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [eval_batch_size, 2, 2, num_classes])\n      logits = tf.reduce_mean(logits, [1, 2])\n      predictions = tf.argmax(logits, 1)\n      self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])\n\n  def testForward(self):\n    batch_size = 1\n    height, width = 224, 224\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = vgg.vgg_19(inputs)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits)\n      self.assertTrue(output.any())\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/slim/preprocessing/__init__.py,0,b'\n'
model_zoo/models/slim/preprocessing/cifarnet_preprocessing.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images in CIFAR-10.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n_PADDING = 4\n\nslim = tf.contrib.slim\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         padding=_PADDING):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    padding: The amound of padding before and after each dimension of the image.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  tf.image_summary(\'image\', tf.expand_dims(image, 0))\n\n  # Transform the image to floats.\n  image = tf.to_float(image)\n  if padding > 0:\n    image = tf.pad(image, [[padding, padding], [padding, padding], [0, 0]])\n  # Randomly crop a [height, width] section of the image.\n  distorted_image = tf.random_crop(image,\n                                   [output_height, output_width, 3])\n\n  # Randomly flip the image horizontally.\n  distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n  tf.image_summary(\'distorted_image\', tf.expand_dims(distorted_image, 0))\n\n  # Because these operations are not commutative, consider randomizing\n  # the order their operation.\n  distorted_image = tf.image.random_brightness(distorted_image,\n                                               max_delta=63)\n  distorted_image = tf.image.random_contrast(distorted_image,\n                                             lower=0.2, upper=1.8)\n  # Subtract off the mean and divide by the variance of the pixels.\n  return tf.image.per_image_whitening(distorted_image)\n\n\ndef preprocess_for_eval(image, output_height, output_width):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  tf.image_summary(\'image\', tf.expand_dims(image, 0))\n  # Transform the image to floats.\n  image = tf.to_float(image)\n\n  # Resize and crop if needed.\n  resized_image = tf.image.resize_image_with_crop_or_pad(image,\n                                                         output_width,\n                                                         output_height)\n  tf.image_summary(\'resized_image\', tf.expand_dims(resized_image, 0))\n\n  # Subtract off the mean and divide by the variance of the pixels.\n  return tf.image.per_image_whitening(resized_image)\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width)\n  else:\n    return preprocess_for_eval(image, output_height, output_width)\n'"
model_zoo/models/slim/preprocessing/inception_preprocessing.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images for the Inception networks.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\n\ndef apply_with_random_selector(x, func, num_cases):\n  """"""Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.\n  """"""\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)])[0]\n\n\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n  """"""Distort the color of a Tensor image.\n\n  Each color distortion is non-commutative and thus ordering of the color ops\n  matters. Ideally we would randomly permute the ordering of the color ops.\n  Rather then adding that level of complication, we select a distinct ordering\n  of color ops for each preprocessing thread.\n\n  Args:\n    image: 3-D Tensor containing single image in [0, 1].\n    color_ordering: Python int, a type of distortion (valid values: 0-3).\n    fast_mode: Avoids slower ops (random_hue and random_contrast)\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D Tensor color-distorted image on range [0, 1]\n  Raises:\n    ValueError: if color_ordering not in [0, 3]\n  """"""\n  with tf.name_scope(scope, \'distort_color\', [image]):\n    if fast_mode:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      else:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n    else:\n      if color_ordering == 0:\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      elif color_ordering == 1:\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n      elif color_ordering == 2:\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      elif color_ordering == 3:\n        image = tf.image.random_hue(image, max_delta=0.2)\n        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      else:\n        raise ValueError(\'color_ordering must be in [0, 3]\')\n\n    # The random_* ops do not necessarily clamp.\n    return tf.clip_by_value(image, 0.0, 1.0)\n\n\ndef distorted_bounding_box_crop(image,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n  """"""Generates cropped_image using a one of the bboxes randomly distorted.\n\n  See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n  Args:\n    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n      image.\n    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n      area of the image must contain at least this fraction of any bounding box\n      supplied.\n    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n      image must have an aspect ratio = width / height within this range.\n    area_range: An optional list of `floats`. The cropped area of the image\n      must contain a fraction of the supplied image within in this range.\n    max_attempts: An optional `int`. Number of attempts at generating a cropped\n      region of the image of the specified constraints. After `max_attempts`\n      failures, return the entire image.\n    scope: Optional scope for name_scope.\n  Returns:\n    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n  """"""\n  with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image, bbox]):\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n\n    # A large fraction of image datasets contain a human-annotated bounding\n    # box delineating the region of the image containing the object of interest.\n    # We choose to create a new bounding box for the object which is a randomly\n    # distorted version of the human-annotated bounding box that obeys an\n    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n    # bounding box. If no box is supplied, then we assume the bounding box is\n    # the entire image.\n    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n        tf.shape(image),\n        bounding_boxes=bbox,\n        min_object_covered=min_object_covered,\n        aspect_ratio_range=aspect_ratio_range,\n        area_range=area_range,\n        max_attempts=max_attempts,\n        use_image_if_no_bounding_boxes=True)\n    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n\n    # Crop the image to the specified bounding box.\n    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n    return cropped_image, distort_bbox\n\n\ndef preprocess_for_train(image, height, width, bbox,\n                         fast_mode=True,\n                         scope=None):\n  """"""Distort one image for training a network.\n\n  Distorting images provides a useful technique for augmenting the data\n  set during training in order to make the network invariant to aspects\n  of the image that do not effect the label.\n\n  Additionally it would create image_summaries to display the different\n  transformations applied to the image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n    height: integer\n    width: integer\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged\n      as [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n      bi-cubic resizing, random_hue or random_contrast).\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of distorted image used for training with range [-1, 1].\n  """"""\n  with tf.name_scope(scope, \'distort_image\', [image, height, width, bbox]):\n    if bbox is None:\n      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n                         dtype=tf.float32,\n                         shape=[1, 1, 4])\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Each bounding box has shape [1, num_boxes, box coords] and\n    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n                                                  bbox)\n    tf.image_summary(\'image_with_bounding_boxes\', image_with_box)\n\n    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n    # Restore the shape since the dynamic slice based upon the bbox_size loses\n    # the third dimension.\n    distorted_image.set_shape([None, None, 3])\n    image_with_distorted_box = tf.image.draw_bounding_boxes(\n        tf.expand_dims(image, 0), distorted_bbox)\n    tf.image_summary(\'images_with_distorted_bounding_box\',\n                     image_with_distorted_box)\n\n    # This resizing operation may distort the images because the aspect\n    # ratio is not respected. We select a resize method in a round robin\n    # fashion based on the thread number.\n    # Note that ResizeMethod contains 4 enumerated resizing methods.\n\n    # We select only 1 case for fast_mode bilinear.\n    num_resize_cases = 1 if fast_mode else 4\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, method: tf.image.resize_images(x, [height, width], method=method),\n        num_cases=num_resize_cases)\n\n    tf.image_summary(\'cropped_resized_image\',\n                     tf.expand_dims(distorted_image, 0))\n\n    # Randomly flip the image horizontally.\n    distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n    # Randomly distort the colors. There are 4 ways to do it.\n    distorted_image = apply_with_random_selector(\n        distorted_image,\n        lambda x, ordering: distort_color(x, ordering, fast_mode),\n        num_cases=4)\n\n    tf.image_summary(\'final_distorted_image\',\n                     tf.expand_dims(distorted_image, 0))\n    distorted_image = tf.sub(distorted_image, 0.5)\n    distorted_image = tf.mul(distorted_image, 2.0)\n    return distorted_image\n\n\ndef preprocess_for_eval(image, height, width,\n                        central_fraction=0.875, scope=None):\n  """"""Prepare one image for evaluation.\n\n  If height and width are specified it would output an image with that size by\n  applying resize_bilinear.\n\n  If central_fraction is specified it would cropt the central fraction of the\n  input image.\n\n  Args:\n    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n      is [0, MAX], where MAX is largest positive representable number for\n      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n    height: integer\n    width: integer\n    central_fraction: Optional Float, fraction of the image to crop.\n    scope: Optional scope for name_scope.\n  Returns:\n    3-D float Tensor of prepared image.\n  """"""\n  with tf.name_scope(scope, \'eval_image\', [image, height, width]):\n    if image.dtype != tf.float32:\n      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    # Crop the central region of the image with an area containing 87.5% of\n    # the original image.\n    if central_fraction:\n      image = tf.image.central_crop(image, central_fraction=central_fraction)\n\n    if height and width:\n      # Resize the image to the specified height and width.\n      image = tf.expand_dims(image, 0)\n      image = tf.image.resize_bilinear(image, [height, width],\n                                       align_corners=False)\n      image = tf.squeeze(image, [0])\n    image = tf.sub(image, 0.5)\n    image = tf.mul(image, 2.0)\n    return image\n\n\ndef preprocess_image(image, height, width,\n                     is_training=False,\n                     bbox=None,\n                     fast_mode=True):\n  """"""Pre-process one image for training or evaluation.\n\n  Args:\n    image: 3-D Tensor [height, width, channels] with the image.\n    height: integer, image expected height.\n    width: integer, image expected width.\n    is_training: Boolean. If true it would transform an image for train,\n      otherwise it would transform it for evaluation.\n    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n      where each coordinate is [0, 1) and the coordinates are arranged as\n      [ymin, xmin, ymax, xmax].\n    fast_mode: Optional boolean, if True avoids slower transformations.\n\n  Returns:\n    3-D float Tensor containing an appropriately scaled image\n\n  Raises:\n    ValueError: if user does not provide bounding box\n  """"""\n  if is_training:\n    return preprocess_for_train(image, height, width, bbox, fast_mode)\n  else:\n    return preprocess_for_eval(image, height, width)\n'"
model_zoo/models/slim/preprocessing/lenet_preprocessing.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities for preprocessing.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef preprocess_image(image, output_height, output_width, is_training):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = tf.to_float(image)\n  image = tf.image.resize_image_with_crop_or_pad(\n      image, output_width, output_height)\n  image = tf.sub(image, 128.0)\n  image = tf.div(image, 128.0)\n  return image\n'"
model_zoo/models/slim/preprocessing/preprocessing_factory.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains a factory for building various models.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom preprocessing import cifarnet_preprocessing\nfrom preprocessing import inception_preprocessing\nfrom preprocessing import lenet_preprocessing\nfrom preprocessing import vgg_preprocessing\n\nslim = tf.contrib.slim\n\n\ndef get_preprocessing(name, is_training=False):\n  """"""Returns preprocessing_fn(image, height, width, **kwargs).\n\n  Args:\n    name: The name of the preprocessing function.\n    is_training: `True` if the model is being used for training and `False`\n      otherwise.\n\n  Returns:\n    preprocessing_fn: A function that preprocessing a single image (pre-batch).\n      It has the following signature:\n        image = preprocessing_fn(image, output_height, output_width, ...).\n\n  Raises:\n    ValueError: If Preprocessing `name` is not recognized.\n  """"""\n  preprocessing_fn_map = {\n      \'cifarnet\': cifarnet_preprocessing,\n      \'inception\': inception_preprocessing,\n      \'inception_v1\': inception_preprocessing,\n      \'inception_v2\': inception_preprocessing,\n      \'inception_v3\': inception_preprocessing,\n      \'inception_v4\': inception_preprocessing,\n      \'inception_resnet_v2\': inception_preprocessing,\n      \'lenet\': lenet_preprocessing,\n      \'resnet_v1_50\': vgg_preprocessing,\n      \'resnet_v1_101\': vgg_preprocessing,\n      \'resnet_v1_152\': vgg_preprocessing,\n      \'vgg\': vgg_preprocessing,\n      \'vgg_a\': vgg_preprocessing,\n      \'vgg_16\': vgg_preprocessing,\n      \'vgg_19\': vgg_preprocessing,\n  }\n\n  if name not in preprocessing_fn_map:\n    raise ValueError(\'Preprocessing name [%s] was not recognized\' % name)\n\n  def preprocessing_fn(image, output_height, output_width, **kwargs):\n    return preprocessing_fn_map[name].preprocess_image(\n        image, output_height, output_width, is_training=is_training, **kwargs)\n\n  return preprocessing_fn\n'"
model_zoo/models/slim/preprocessing/vgg_preprocessing.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Provides utilities to preprocess images.\n\nThe preprocessing steps for VGG were introduced in the following technical\nreport:\n\n  Very Deep Convolutional Networks For Large-Scale Image Recognition\n  Karen Simonyan and Andrew Zisserman\n  arXiv technical report, 2015\n  PDF: http://arxiv.org/pdf/1409.1556.pdf\n  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\n  CC-BY-4.0\n\nMore information can be obtained from the VGG website:\nwww.robots.ox.ac.uk/~vgg/research/very_deep/\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\nslim = tf.contrib.slim\n\n_R_MEAN = 123.68\n_G_MEAN = 116.78\n_B_MEAN = 103.94\n\n_RESIZE_SIDE_MIN = 256\n_RESIZE_SIDE_MAX = 512\n\n\ndef _crop(image, offset_height, offset_width, crop_height, crop_width):\n  """"""Crops the given image using the provided offsets and sizes.\n\n  Note that the method doesn\'t assume we know the input image size but it does\n  assume we know the input image rank.\n\n  Args:\n    image: an image of shape [height, width, channels].\n    offset_height: a scalar tensor indicating the height offset.\n    offset_width: a scalar tensor indicating the width offset.\n    crop_height: the height of the cropped image.\n    crop_width: the width of the cropped image.\n\n  Returns:\n    the cropped (and resized) image.\n\n  Raises:\n    InvalidArgumentError: if the rank is not 3 or if the image dimensions are\n      less than the crop size.\n  """"""\n  original_shape = tf.shape(image)\n\n  rank_assertion = tf.Assert(\n      tf.equal(tf.rank(image), 3),\n      [\'Rank of image must be equal to 3.\'])\n  cropped_shape = control_flow_ops.with_dependencies(\n      [rank_assertion],\n      tf.pack([crop_height, crop_width, original_shape[2]]))\n\n  size_assertion = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(original_shape[0], crop_height),\n          tf.greater_equal(original_shape[1], crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  offsets = tf.to_int32(tf.pack([offset_height, offset_width, 0]))\n\n  # Use tf.slice instead of crop_to_bounding box as it accepts tensors to\n  # define the crop size.\n  image = control_flow_ops.with_dependencies(\n      [size_assertion],\n      tf.slice(image, offsets, cropped_shape))\n  return tf.reshape(image, cropped_shape)\n\n\ndef _random_crop(image_list, crop_height, crop_width):\n  """"""Crops the given list of images.\n\n  The function applies the same crop to each image in the list. This can be\n  effectively applied when there are multiple image inputs of the same\n  dimension such as:\n\n    image, depths, normals = _random_crop([image, depths, normals], 120, 150)\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the new height.\n    crop_width: the new width.\n\n  Returns:\n    the image_list with cropped images.\n\n  Raises:\n    ValueError: if there are multiple image inputs provided with different size\n      or the images are smaller than the crop dimensions.\n  """"""\n  if not image_list:\n    raise ValueError(\'Empty image_list.\')\n\n  # Compute the rank assertions.\n  rank_assertions = []\n  for i in range(len(image_list)):\n    image_rank = tf.rank(image_list[i])\n    rank_assert = tf.Assert(\n        tf.equal(image_rank, 3),\n        [\'Wrong rank for tensor  %s [expected] [actual]\',\n         image_list[i].name, 3, image_rank])\n    rank_assertions.append(rank_assert)\n\n  image_shape = control_flow_ops.with_dependencies(\n      [rank_assertions[0]],\n      tf.shape(image_list[0]))\n  image_height = image_shape[0]\n  image_width = image_shape[1]\n  crop_size_assert = tf.Assert(\n      tf.logical_and(\n          tf.greater_equal(image_height, crop_height),\n          tf.greater_equal(image_width, crop_width)),\n      [\'Crop size greater than the image size.\'])\n\n  asserts = [rank_assertions[0], crop_size_assert]\n\n  for i in range(1, len(image_list)):\n    image = image_list[i]\n    asserts.append(rank_assertions[i])\n    shape = control_flow_ops.with_dependencies([rank_assertions[i]],\n                                               tf.shape(image))\n    height = shape[0]\n    width = shape[1]\n\n    height_assert = tf.Assert(\n        tf.equal(height, image_height),\n        [\'Wrong height for tensor %s [expected][actual]\',\n         image.name, height, image_height])\n    width_assert = tf.Assert(\n        tf.equal(width, image_width),\n        [\'Wrong width for tensor %s [expected][actual]\',\n         image.name, width, image_width])\n    asserts.extend([height_assert, width_assert])\n\n  # Create a random bounding box.\n  #\n  # Use tf.random_uniform and not numpy.random.rand as doing the former would\n  # generate random numbers at graph eval time, unlike the latter which\n  # generates random numbers at graph definition time.\n  max_offset_height = control_flow_ops.with_dependencies(\n      asserts, tf.reshape(image_height - crop_height + 1, []))\n  max_offset_width = control_flow_ops.with_dependencies(\n      asserts, tf.reshape(image_width - crop_width + 1, []))\n  offset_height = tf.random_uniform(\n      [], maxval=max_offset_height, dtype=tf.int32)\n  offset_width = tf.random_uniform(\n      [], maxval=max_offset_width, dtype=tf.int32)\n\n  return [_crop(image, offset_height, offset_width,\n                crop_height, crop_width) for image in image_list]\n\n\ndef _central_crop(image_list, crop_height, crop_width):\n  """"""Performs central crops of the given image list.\n\n  Args:\n    image_list: a list of image tensors of the same dimension but possibly\n      varying channel.\n    crop_height: the height of the image following the crop.\n    crop_width: the width of the image following the crop.\n\n  Returns:\n    the list of cropped images.\n  """"""\n  outputs = []\n  for image in image_list:\n    image_height = tf.shape(image)[0]\n    image_width = tf.shape(image)[1]\n\n    offset_height = (image_height - crop_height) / 2\n    offset_width = (image_width - crop_width) / 2\n\n    outputs.append(_crop(image, offset_height, offset_width,\n                         crop_height, crop_width))\n  return outputs\n\n\ndef _mean_image_subtraction(image, means):\n  """"""Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn\'t match the\n      number of values in `means`.\n  """"""\n  if image.get_shape().ndims != 3:\n    raise ValueError(\'Input must be of size [height, width, C>0]\')\n  num_channels = image.get_shape().as_list()[-1]\n  if len(means) != num_channels:\n    raise ValueError(\'len(means) must match the number of channels\')\n\n  channels = tf.split(2, num_channels, image)\n  for i in range(num_channels):\n    channels[i] -= means[i]\n  return tf.concat(2, channels)\n\n\ndef _smallest_size_at_least(height, width, smallest_side):\n  """"""Computes new shape with the smallest side equal to `smallest_side`.\n\n  Computes new shape with the smallest side equal to `smallest_side` while\n  preserving the original aspect ratio.\n\n  Args:\n    height: an int32 scalar tensor indicating the current height.\n    width: an int32 scalar tensor indicating the current width.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    new_height: an int32 scalar tensor indicating the new height.\n    new_width: and int32 scalar tensor indicating the new width.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  height = tf.to_float(height)\n  width = tf.to_float(width)\n  smallest_side = tf.to_float(smallest_side)\n\n  scale = tf.cond(tf.greater(height, width),\n                  lambda: smallest_side / width,\n                  lambda: smallest_side / height)\n  new_height = tf.to_int32(height * scale)\n  new_width = tf.to_int32(width * scale)\n  return new_height, new_width\n\n\ndef _aspect_preserving_resize(image, smallest_side):\n  """"""Resize images preserving the original aspect ratio.\n\n  Args:\n    image: A 3-D image `Tensor`.\n    smallest_side: A python integer or scalar `Tensor` indicating the size of\n      the smallest side after resize.\n\n  Returns:\n    resized_image: A 3-D tensor containing the resized image.\n  """"""\n  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)\n\n  shape = tf.shape(image)\n  height = shape[0]\n  width = shape[1]\n  new_height, new_width = _smallest_size_at_least(height, width, smallest_side)\n  image = tf.expand_dims(image, 0)\n  resized_image = tf.image.resize_bilinear(image, [new_height, new_width],\n                                           align_corners=False)\n  resized_image = tf.squeeze(resized_image)\n  resized_image.set_shape([None, None, 3])\n  return resized_image\n\n\ndef preprocess_for_train(image,\n                         output_height,\n                         output_width,\n                         resize_side_min=_RESIZE_SIDE_MIN,\n                         resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image for training.\n\n  Note that the actual resizing scale is sampled from\n    [`resize_size_min`, `resize_size_max`].\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  resize_side = tf.random_uniform(\n      [], minval=resize_side_min, maxval=resize_side_max+1, dtype=tf.int32)\n\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _random_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  image = tf.image.random_flip_left_right(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_for_eval(image, output_height, output_width, resize_side):\n  """"""Preprocesses the given image for evaluation.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    resize_side: The smallest side of the image for aspect-preserving resizing.\n\n  Returns:\n    A preprocessed image.\n  """"""\n  image = _aspect_preserving_resize(image, resize_side)\n  image = _central_crop([image], output_height, output_width)[0]\n  image.set_shape([output_height, output_width, 3])\n  image = tf.to_float(image)\n  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n\n\ndef preprocess_image(image, output_height, output_width, is_training=False,\n                     resize_side_min=_RESIZE_SIDE_MIN,\n                     resize_side_max=_RESIZE_SIDE_MAX):\n  """"""Preprocesses the given image.\n\n  Args:\n    image: A `Tensor` representing an image of arbitrary size.\n    output_height: The height of the image after preprocessing.\n    output_width: The width of the image after preprocessing.\n    is_training: `True` if we\'re preprocessing the image for training and\n      `False` otherwise.\n    resize_side_min: The lower bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, then this value\n      is used for rescaling.\n    resize_side_max: The upper bound for the smallest side of the image for\n      aspect-preserving resizing. If `is_training` is `False`, this value is\n      ignored. Otherwise, the resize side is sampled from\n        [resize_size_min, resize_size_max].\n\n  Returns:\n    A preprocessed image.\n  """"""\n  if is_training:\n    return preprocess_for_train(image, output_height, output_width,\n                                resize_side_min, resize_side_max)\n  else:\n    return preprocess_for_eval(image, output_height, output_width,\n                               resize_side_min)\n'"
model_zoo/models/street/python/decoder.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Basic CTC+recoder decoder.\n\nDecodes a sequence of class-ids into UTF-8 text.\nFor basic information on CTC See:\nAlex Graves et al. Connectionist Temporal Classification: Labelling Unsegmented\nSequence Data with Recurrent Neural Networks.\nhttp://www.cs.toronto.edu/~graves/icml_2006.pdf\n""""""\nimport collections\nimport re\n\nimport errorcounter as ec\nimport tensorflow as tf\n\n# Named tuple Part describes a part of a multi (1 or more) part code that\n# represents a utf-8 string. For example, Chinese character \'x\' might be\n# represented by 3 codes of which (utf8=\'x\', index=1, num_codes3) would be the\n# middle part. (The actual code is not stored in the tuple).\nPart = collections.namedtuple(\'Part\', \'utf8 index, num_codes\')\n\n\n# Class that decodes a sequence of class-ids into UTF-8 text.\nclass Decoder(object):\n  """"""Basic CTC+recoder decoder.""""""\n\n  def __init__(self, filename):\n    r""""""Constructs a Decoder.\n\n    Reads the text file describing the encoding and build the encoder.\n    The text file contains lines of the form:\n    <code>[,<code>]*\\t<string>\n    Each line defines a mapping from a sequence of one or more integer codes to\n    a corresponding utf-8 string.\n    Args:\n      filename:   Name of file defining the decoding sequences.\n    """"""\n    # self.decoder is a list of lists of Part(utf8, index, num_codes).\n    # The index to the top-level list is a code. The list given by the code\n    # index is a list of the parts represented by that code, Eg if the code 42\n    # represents the 2nd (index 1) out of 3 part of Chinese character \'x\', then\n    # self.decoder[42] = [..., (utf8=\'x\', index=1, num_codes3), ...] where ...\n    # means all other uses of the code 42.\n    self.decoder = []\n    if filename:\n      self._InitializeDecoder(filename)\n\n  def SoftmaxEval(self, sess, model, num_steps):\n    """"""Evaluate a model in softmax mode.\n\n    Adds char, word recall and sequence error rate events to the sw summary\n    writer, and returns them as well\n    TODO(rays) Add LogisticEval.\n    Args:\n      sess:  A tensor flow Session.\n      model: The model to run in the session. Requires a VGSLImageModel or any\n        other class that has a using_ctc attribute and a RunAStep(sess) method\n        that reurns a softmax result with corresponding labels.\n      num_steps: Number of steps to evaluate for.\n    Returns:\n      ErrorRates named tuple.\n    Raises:\n      ValueError: If an unsupported number of dimensions is used.\n    """"""\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n    # Run the requested number of evaluation steps, gathering the outputs of the\n    # softmax and the true labels of the evaluation examples.\n    total_label_counts = ec.ErrorCounts(0, 0, 0, 0)\n    total_word_counts = ec.ErrorCounts(0, 0, 0, 0)\n    sequence_errors = 0\n    for _ in xrange(num_steps):\n      softmax_result, labels = model.RunAStep(sess)\n      # Collapse softmax to same shape as labels.\n      predictions = softmax_result.argmax(axis=-1)\n      # Exclude batch from num_dims.\n      num_dims = len(predictions.shape) - 1\n      batch_size = predictions.shape[0]\n      null_label = softmax_result.shape[-1] - 1\n      for b in xrange(batch_size):\n        if num_dims == 2:\n          # TODO(rays) Support 2-d data.\n          raise ValueError(\'2-d label data not supported yet!\')\n        else:\n          if num_dims == 1:\n            pred_batch = predictions[b, :]\n            labels_batch = labels[b, :]\n          else:\n            pred_batch = [predictions[b]]\n            labels_batch = [labels[b]]\n          text = self.StringFromCTC(pred_batch, model.using_ctc, null_label)\n          truth = self.StringFromCTC(labels_batch, False, null_label)\n          # Note that recall_errs is false negatives (fn) aka drops/deletions.\n          # Actual recall would be 1-fn/truth_words.\n          # Likewise precision_errs is false positives (fp) aka adds/insertions.\n          # Actual precision would be 1-fp/ocr_words.\n          total_word_counts = ec.AddErrors(total_word_counts,\n                                           ec.CountWordErrors(text, truth))\n          total_label_counts = ec.AddErrors(total_label_counts,\n                                            ec.CountErrors(text, truth))\n          if text != truth:\n            sequence_errors += 1\n\n    coord.request_stop()\n    coord.join(threads)\n    return ec.ComputeErrorRates(total_label_counts, total_word_counts,\n                                sequence_errors, num_steps * batch_size)\n\n  def StringFromCTC(self, ctc_labels, merge_dups, null_label):\n    """"""Decodes CTC output to a string.\n\n    Extracts only sequences of codes that are allowed by self.decoder.\n    Labels that make illegal code sequences are dropped.\n    Note that, by its nature of taking only top choices, this is much weaker\n    than a full-blown beam search that considers all the softmax outputs.\n    For languages without many multi-code sequences, this doesn\'t make much\n    difference, but for complex scripts the accuracy will be much lower.\n    Args:\n      ctc_labels: List of class labels including null characters to remove.\n      merge_dups: If True, Duplicate labels will be merged\n      null_label: Label value to ignore.\n\n    Returns:\n      Labels decoded to a string.\n    """"""\n    # Run regular ctc on the labels, extracting a list of codes.\n    codes = self._CodesFromCTC(ctc_labels, merge_dups, null_label)\n    length = len(codes)\n    if length == 0:\n      return \'\'\n    # strings and partials are both indexed by the same index as codes.\n    # strings[i] is the best completed string upto position i, and\n    # partials[i] is a list of partial code sequences at position i.\n    # Warning: memory is squared-order in length.\n    strings = []\n    partials = []\n    for pos in xrange(length):\n      code = codes[pos]\n      parts = self.decoder[code]\n      partials.append([])\n      strings.append(\'\')\n      # Iterate over the parts that this code can represent.\n      for utf8, index, num_codes in parts:\n        if index > pos:\n          continue\n        # We can use code if it is an initial code (index==0) or continues a\n        # sequence in the partials list at the previous position.\n        if index == 0 or partials[pos - 1].count(\n            Part(utf8, index - 1, num_codes)) > 0:\n          if index < num_codes - 1:\n            # Save the partial sequence.\n            partials[-1].append(Part(utf8, index, num_codes))\n          elif not strings[-1]:\n            # A code sequence is completed. Append to the best string that we\n            # had where it started.\n            if pos >= num_codes:\n              strings[-1] = strings[pos - num_codes] + utf8\n            else:\n              strings[-1] = utf8\n      if not strings[-1] and pos > 0:\n        # We didn\'t get anything here so copy the previous best string, skipping\n        # the current code, but it may just be a partial anyway.\n        strings[-1] = strings[-2]\n    return strings[-1]\n\n  def _InitializeDecoder(self, filename):\n    """"""Reads the decoder file and initializes self.decoder from it.\n\n    Args:\n      filename: Name of text file mapping codes to utf8 strings.\n    Raises:\n      ValueError: if the input file is not parsed correctly.\n    """"""\n    line_re = re.compile(r\'(?P<codes>\\d+(,\\d+)*)\\t(?P<utf8>.+)\')\n    with tf.gfile.GFile(filename) as f:\n      for line in f:\n        m = line_re.match(line)\n        if m is None:\n          raise ValueError(\'Unmatched line:\', line)\n        # codes is the sequence that maps to the string.\n        str_codes = m.groupdict()[\'codes\'].split(\',\')\n        codes = []\n        for code in str_codes:\n          codes.append(int(code))\n        utf8 = m.groupdict()[\'utf8\']\n        num_codes = len(codes)\n        for index, code in enumerate(codes):\n          while code >= len(self.decoder):\n            self.decoder.append([])\n          self.decoder[code].append(Part(utf8, index, num_codes))\n\n  def _CodesFromCTC(self, ctc_labels, merge_dups, null_label):\n    """"""Collapses CTC output to regular output.\n\n    Args:\n      ctc_labels: List of class labels including null characters to remove.\n      merge_dups: If True, Duplicate labels will be merged.\n      null_label: Label value to ignore.\n\n    All trailing zeros are removed!!\n    TODO(rays) This may become a problem with non-CTC models.\n    If using charset, this should not be a problem as zero is always space.\n    tf.pad can only append zero, so we have to be able to drop them, as a\n    non-ctc will have learned to output trailing zeros instead of trailing\n    nulls. This is awkward, as the stock ctc loss function requires that the\n    null character be num_classes-1.\n    Returns:\n      (List of) Labels with null characters removed.\n    """"""\n    out_labels = []\n    prev_label = -1\n    zeros_needed = 0\n    for label in ctc_labels:\n      if label == null_label:\n        prev_label = -1\n      elif label != prev_label or not merge_dups:\n        if label == 0:\n          # Count zeros and only emit them when it is clear there is a non-zero\n          # after, so as to truncate away all trailing zeros.\n          zeros_needed += 1\n        else:\n          if merge_dups and zeros_needed > 0:\n            out_labels.append(0)\n          else:\n            out_labels += [0] * zeros_needed\n          zeros_needed = 0\n          out_labels.append(label)\n        prev_label = label\n    return out_labels\n'"
model_zoo/models/street/python/decoder_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for decoder.""""""\nimport os\n\nimport tensorflow as tf\nimport decoder\n\n\ndef _testdata(filename):\n  return os.path.join(\'../testdata/\', filename)\n\n\nclass DecoderTest(tf.test.TestCase):\n\n  def testCodesFromCTC(self):\n    """"""Tests that the simple CTC decoder drops nulls and duplicates.\n    """"""\n    ctc_labels = [9, 9, 9, 1, 9, 2, 2, 3, 9, 9, 0, 0, 1, 9, 1, 9, 9, 9]\n    decode = decoder.Decoder(filename=None)\n    non_null_labels = decode._CodesFromCTC(\n        ctc_labels, merge_dups=False, null_label=9)\n    self.assertEqual(non_null_labels, [1, 2, 2, 3, 0, 0, 1, 1])\n    idempotent_labels = decode._CodesFromCTC(\n        non_null_labels, merge_dups=False, null_label=9)\n    self.assertEqual(idempotent_labels, non_null_labels)\n    collapsed_labels = decode._CodesFromCTC(\n        ctc_labels, merge_dups=True, null_label=9)\n    self.assertEqual(collapsed_labels, [1, 2, 3, 0, 1, 1])\n    non_idempotent_labels = decode._CodesFromCTC(\n        collapsed_labels, merge_dups=True, null_label=9)\n    self.assertEqual(non_idempotent_labels, [1, 2, 3, 0, 1])\n\n  def testStringFromCTC(self):\n    """"""Tests that the decoder can decode sequences including multi-codes.\n    """"""\n    #             -  f  -  a  r  -  m(1/2)m     -junk sp b  a  r  -  n  -\n    ctc_labels = [9, 6, 9, 1, 3, 9, 4, 9, 5, 5, 9, 5, 0, 2, 1, 3, 9, 4, 9]\n    decode = decoder.Decoder(filename=_testdata(\'charset_size_10.txt\'))\n    text = decode.StringFromCTC(ctc_labels, merge_dups=True, null_label=9)\n    self.assertEqual(text, \'farm barn\')\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/street/python/errorcounter.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Some simple tools for error counting.\n\n""""""\nimport collections\n\n# Named tuple Error counts describes the counts needed to accumulate errors\n# over multiple trials:\n#   false negatives (aka drops or deletions),\n#   false positives: (aka adds or insertions),\n#   truth_count: number of elements in ground truth = denominator for fn,\n#   test_count:  number of elements in test string = denominator for fp,\n# Note that recall = 1 - fn/truth_count, precision = 1 - fp/test_count,\n# accuracy = 1 - (fn + fp) / (truth_count + test_count).\nErrorCounts = collections.namedtuple(\'ErrorCounts\', [\'fn\', \'fp\', \'truth_count\',\n                                                     \'test_count\'])\n\n# Named tuple for error rates, as a percentage. Accuracies are just 100-error.\nErrorRates = collections.namedtuple(\'ErrorRates\',\n                                    [\'label_error\', \'word_recall_error\',\n                                     \'word_precision_error\', \'sequence_error\'])\n\n\ndef CountWordErrors(ocr_text, truth_text):\n  """"""Counts the word drop and add errors as a bag of words.\n\n  Args:\n    ocr_text:    OCR text string.\n    truth_text:  Truth text string.\n\n  Returns:\n    ErrorCounts named tuple.\n  """"""\n  # Convert to lists of words.\n  return CountErrors(ocr_text.split(), truth_text.split())\n\n\ndef CountErrors(ocr_text, truth_text):\n  """"""Counts the drops and adds between 2 bags of iterables.\n\n  Simple bag of objects count returns the number of dropped and added\n  elements, regardless of order, from anything that is iterable, eg\n  a pair of strings gives character errors, and a pair of word lists give\n  word errors.\n  Args:\n    ocr_text:    OCR text iterable (eg string for chars, word list for words).\n    truth_text:  Truth text iterable.\n\n  Returns:\n    ErrorCounts named tuple.\n  """"""\n  counts = collections.Counter(truth_text)\n  counts.subtract(ocr_text)\n  drops = sum(c for c in counts.values() if c > 0)\n  adds = sum(-c for c in counts.values() if c < 0)\n  return ErrorCounts(drops, adds, len(truth_text), len(ocr_text))\n\n\ndef AddErrors(counts1, counts2):\n  """"""Adds the counts and returns a new sum tuple.\n\n  Args:\n    counts1: ErrorCounts named tuples to sum.\n    counts2: ErrorCounts named tuples to sum.\n  Returns:\n    Sum of counts1, counts2.\n  """"""\n  return ErrorCounts(counts1.fn + counts2.fn, counts1.fp + counts2.fp,\n                     counts1.truth_count + counts2.truth_count,\n                     counts1.test_count + counts2.test_count)\n\n\ndef ComputeErrorRates(label_counts, word_counts, seq_errors, num_seqs):\n  """"""Returns an ErrorRates corresponding to the given counts.\n\n  Args:\n    label_counts: ErrorCounts for the character labels\n    word_counts:  ErrorCounts for the words\n    seq_errors:   Number of sequence errors\n    num_seqs:     Total sequences\n  Returns:\n    ErrorRates corresponding to the given counts.\n  """"""\n  label_errors = label_counts.fn + label_counts.fp\n  num_labels = label_counts.truth_count + label_counts.test_count\n  return ErrorRates(\n      ComputeErrorRate(label_errors, num_labels),\n      ComputeErrorRate(word_counts.fn, word_counts.truth_count),\n      ComputeErrorRate(word_counts.fp, word_counts.test_count),\n      ComputeErrorRate(seq_errors, num_seqs))\n\n\ndef ComputeErrorRate(error_count, truth_count):\n  """"""Returns a sanitized percent error rate from the raw counts.\n\n  Prevents div by 0 and clips return to 100%.\n  Args:\n    error_count: Number of errors.\n    truth_count: Number to divide by.\n\n  Returns:\n    100.0 * error_count / truth_count clipped to 100.\n  """"""\n  if truth_count == 0:\n    truth_count = 1\n    error_count = 1\n  elif error_count > truth_count:\n    error_count = truth_count\n  return error_count * 100.0 / truth_count\n'"
model_zoo/models/street/python/errorcounter_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for errorcounter.""""""\nimport tensorflow as tf\nimport errorcounter as ec\n\n\nclass ErrorcounterTest(tf.test.TestCase):\n\n  def testComputeErrorRate(self):\n    """"""Tests that the percent calculation works as expected.\n    """"""\n    rate = ec.ComputeErrorRate(error_count=0, truth_count=0)\n    self.assertEqual(rate, 100.0)\n    rate = ec.ComputeErrorRate(error_count=1, truth_count=0)\n    self.assertEqual(rate, 100.0)\n    rate = ec.ComputeErrorRate(error_count=10, truth_count=1)\n    self.assertEqual(rate, 100.0)\n    rate = ec.ComputeErrorRate(error_count=0, truth_count=1)\n    self.assertEqual(rate, 0.0)\n    rate = ec.ComputeErrorRate(error_count=3, truth_count=12)\n    self.assertEqual(rate, 25.0)\n\n  def testCountErrors(self):\n    """"""Tests that the error counter works as expected.\n    """"""\n    truth_str = \'farm barn\'\n    counts = ec.CountErrors(ocr_text=truth_str, truth_text=truth_str)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=0, fp=0, truth_count=9, test_count=9))\n    # With a period on the end, we get a char error.\n    dot_str = \'farm barn.\'\n    counts = ec.CountErrors(ocr_text=dot_str, truth_text=truth_str)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=0, fp=1, truth_count=9, test_count=10))\n    counts = ec.CountErrors(ocr_text=truth_str, truth_text=dot_str)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=1, fp=0, truth_count=10, test_count=9))\n    # Space is just another char.\n    no_space = \'farmbarn\'\n    counts = ec.CountErrors(ocr_text=no_space, truth_text=truth_str)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=1, fp=0, truth_count=9, test_count=8))\n    counts = ec.CountErrors(ocr_text=truth_str, truth_text=no_space)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=0, fp=1, truth_count=8, test_count=9))\n    # Lose them all.\n    counts = ec.CountErrors(ocr_text=\'\', truth_text=truth_str)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=9, fp=0, truth_count=9, test_count=0))\n    counts = ec.CountErrors(ocr_text=truth_str, truth_text=\'\')\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=0, fp=9, truth_count=0, test_count=9))\n\n  def testCountWordErrors(self):\n    """"""Tests that the error counter works as expected.\n    """"""\n    truth_str = \'farm barn\'\n    counts = ec.CountWordErrors(ocr_text=truth_str, truth_text=truth_str)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=0, fp=0, truth_count=2, test_count=2))\n    # With a period on the end, we get a word error.\n    dot_str = \'farm barn.\'\n    counts = ec.CountWordErrors(ocr_text=dot_str, truth_text=truth_str)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=1, fp=1, truth_count=2, test_count=2))\n    counts = ec.CountWordErrors(ocr_text=truth_str, truth_text=dot_str)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=1, fp=1, truth_count=2, test_count=2))\n    # Space is special.\n    no_space = \'farmbarn\'\n    counts = ec.CountWordErrors(ocr_text=no_space, truth_text=truth_str)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=2, fp=1, truth_count=2, test_count=1))\n    counts = ec.CountWordErrors(ocr_text=truth_str, truth_text=no_space)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=1, fp=2, truth_count=1, test_count=2))\n    # Lose them all.\n    counts = ec.CountWordErrors(ocr_text=\'\', truth_text=truth_str)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=2, fp=0, truth_count=2, test_count=0))\n    counts = ec.CountWordErrors(ocr_text=truth_str, truth_text=\'\')\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=0, fp=2, truth_count=0, test_count=2))\n    # With a space in ba rn, there is an extra add.\n    sp_str = \'farm ba rn\'\n    counts = ec.CountWordErrors(ocr_text=sp_str, truth_text=truth_str)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=1, fp=2, truth_count=2, test_count=3))\n    counts = ec.CountWordErrors(ocr_text=truth_str, truth_text=sp_str)\n    self.assertEqual(\n        counts, ec.ErrorCounts(\n            fn=2, fp=1, truth_count=3, test_count=2))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/street/python/nn_ops.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Ops and utilities for neural networks.\n\nFor now, just an LSTM layer.\n""""""\nimport shapes\nimport tensorflow as tf\nrnn = tf.load_op_library(""../cc/rnn_ops.so"")\n\n\ndef rnn_helper(inp,\n               length,\n               cell_type=None,\n               direction=""forward"",\n               name=None,\n               *args,\n               **kwargs):\n  """"""Adds ops for a recurrent neural network layer.\n\n  This function calls an actual implementation of a recurrent neural network\n  based on `cell_type`.\n\n  There are three modes depending on the value of `direction`:\n\n    forward: Adds a forward RNN.\n    backward: Adds a backward RNN.\n    bidirectional: Adds both forward and backward RNNs and creates a\n                   bidirectional RNN.\n\n  Args:\n    inp: A 3-D tensor of shape [`batch_size`, `max_length`, `feature_dim`].\n    length: A 1-D tensor of shape [`batch_size`] and type int64. Each element\n            represents the length of the corresponding sequence in `inp`.\n    cell_type: Cell type of RNN. Currently can only be ""lstm"".\n    direction: One of ""forward"", ""backward"", ""bidirectional"".\n    name: Name of the op.\n    *args: Other arguments to the layer.\n    **kwargs: Keyword arugments to the layer.\n\n  Returns:\n    A 3-D tensor of shape [`batch_size`, `max_length`, `num_nodes`].\n  """"""\n\n  assert cell_type is not None\n  rnn_func = None\n  if cell_type == ""lstm"":\n    rnn_func = lstm_layer\n  assert rnn_func is not None\n  assert direction in [""forward"", ""backward"", ""bidirectional""]\n\n  with tf.variable_scope(name):\n    if direction in [""forward"", ""bidirectional""]:\n      forward = rnn_func(\n          inp=inp,\n          length=length,\n          backward=False,\n          name=""forward"",\n          *args,\n          **kwargs)\n      if isinstance(forward, tuple):\n        # lstm_layer returns a tuple (output, memory). We only need the first\n        # element.\n        forward = forward[0]\n    if direction in [""backward"", ""bidirectional""]:\n      backward = rnn_func(\n          inp=inp,\n          length=length,\n          backward=True,\n          name=""backward"",\n          *args,\n          **kwargs)\n      if isinstance(backward, tuple):\n        # lstm_layer returns a tuple (output, memory). We only need the first\n        # element.\n        backward = backward[0]\n    if direction == ""forward"":\n      out = forward\n    elif direction == ""backward"":\n      out = backward\n    else:\n      out = tf.concat(2, [forward, backward])\n  return out\n\n\n@tf.RegisterShape(""VariableLSTM"")\ndef _variable_lstm_shape(op):\n  """"""Shape function for the VariableLSTM op.""""""\n  input_shape = op.inputs[0].get_shape().with_rank(4)\n  state_shape = op.inputs[1].get_shape().with_rank(2)\n  memory_shape = op.inputs[2].get_shape().with_rank(2)\n  w_m_m_shape = op.inputs[3].get_shape().with_rank(3)\n  batch_size = input_shape[0].merge_with(state_shape[0])\n  batch_size = input_shape[0].merge_with(memory_shape[0])\n  seq_len = input_shape[1]\n  gate_num = input_shape[2].merge_with(w_m_m_shape[1])\n  output_dim = input_shape[3].merge_with(state_shape[1])\n  output_dim = output_dim.merge_with(memory_shape[1])\n  output_dim = output_dim.merge_with(w_m_m_shape[0])\n  output_dim = output_dim.merge_with(w_m_m_shape[2])\n  return [[batch_size, seq_len, output_dim],\n          [batch_size, seq_len, gate_num, output_dim],\n          [batch_size, seq_len, output_dim]]\n\n\n@tf.RegisterGradient(""VariableLSTM"")\ndef _variable_lstm_grad(op, act_grad, gate_grad, mem_grad):\n  """"""Gradient function for the VariableLSTM op.""""""\n  initial_state = op.inputs[1]\n  initial_memory = op.inputs[2]\n  w_m_m = op.inputs[3]\n  act = op.outputs[0]\n  gate_raw_act = op.outputs[1]\n  memory = op.outputs[2]\n  return rnn.variable_lstm_grad(initial_state, initial_memory, w_m_m, act,\n                                gate_raw_act, memory, act_grad, gate_grad,\n                                mem_grad)\n\n\ndef lstm_layer(inp,\n               length=None,\n               state=None,\n               memory=None,\n               num_nodes=None,\n               backward=False,\n               clip=50.0,\n               reg_func=tf.nn.l2_loss,\n               weight_reg=False,\n               weight_collection=""LSTMWeights"",\n               bias_reg=False,\n               stddev=None,\n               seed=None,\n               decode=False,\n               use_native_weights=False,\n               name=None):\n  """"""Adds ops for an LSTM layer.\n\n  This adds ops for the following operations:\n\n    input => (forward-LSTM|backward-LSTM) => output\n\n  The direction of the LSTM is determined by `backward`. If it is false, the\n  forward LSTM is used, the backward one otherwise.\n\n  Args:\n    inp: A 3-D tensor of shape [`batch_size`, `max_length`, `feature_dim`].\n    length: A 1-D tensor of shape [`batch_size`] and type int64. Each element\n            represents the length of the corresponding sequence in `inp`.\n    state: If specified, uses it as the initial state.\n    memory: If specified, uses it as the initial memory.\n    num_nodes: The number of LSTM cells.\n    backward: If true, reverses the `inp` before adding the ops. The output is\n              also reversed so that the direction is the same as `inp`.\n    clip: Value used to clip the cell values.\n    reg_func: Function used for the weight regularization such as\n              `tf.nn.l2_loss`.\n    weight_reg: If true, regularize the filter weights with `reg_func`.\n    weight_collection: Collection to add the weights to for regularization.\n    bias_reg: If true, regularize the bias vector with `reg_func`.\n    stddev: Standard deviation used to initialize the variables.\n    seed: Seed used to initialize the variables.\n    decode: If true, does not add ops which are not used for inference.\n    use_native_weights: If true, uses weights in the same format as the native\n                        implementations.\n    name: Name of the op.\n\n  Returns:\n    A 3-D tensor of shape [`batch_size`, `max_length`, `num_nodes`].\n  """"""\n  with tf.variable_scope(name):\n    if backward:\n      if length is None:\n        inp = tf.reverse(inp, [False, True, False])\n      else:\n        inp = tf.reverse_sequence(inp, length, 1, 0)\n\n    num_prev = inp.get_shape()[2]\n    if stddev:\n      initializer = tf.truncated_normal_initializer(stddev=stddev, seed=seed)\n    else:\n      initializer = tf.uniform_unit_scaling_initializer(seed=seed)\n\n    if use_native_weights:\n      with tf.variable_scope(""LSTMCell""):\n        w = tf.get_variable(\n            ""W_0"",\n            shape=[num_prev + num_nodes, 4 * num_nodes],\n            initializer=initializer,\n            dtype=tf.float32)\n        w_i_m = tf.slice(w, [0, 0], [num_prev, 4 * num_nodes], name=""w_i_m"")\n        w_m_m = tf.reshape(\n            tf.slice(w, [num_prev, 0], [num_nodes, 4 * num_nodes]),\n            [num_nodes, 4, num_nodes],\n            name=""w_m_m"")\n    else:\n      w_i_m = tf.get_variable(""w_i_m"", [num_prev, 4 * num_nodes],\n                              initializer=initializer)\n      w_m_m = tf.get_variable(""w_m_m"", [num_nodes, 4, num_nodes],\n                              initializer=initializer)\n\n    if not decode and weight_reg:\n      tf.add_to_collection(weight_collection, reg_func(w_i_m, name=""w_i_m_reg""))\n      tf.add_to_collection(weight_collection, reg_func(w_m_m, name=""w_m_m_reg""))\n\n    batch_size = shapes.tensor_dim(inp, dim=0)\n    num_frames = shapes.tensor_dim(inp, dim=1)\n    prev = tf.reshape(inp, tf.pack([batch_size * num_frames, num_prev]))\n\n    if use_native_weights:\n      with tf.variable_scope(""LSTMCell""):\n        b = tf.get_variable(\n            ""B"",\n            shape=[4 * num_nodes],\n            initializer=tf.zeros_initializer,\n            dtype=tf.float32)\n      biases = tf.identity(b, name=""biases"")\n    else:\n      biases = tf.get_variable(\n          ""biases"", [4 * num_nodes], initializer=tf.constant_initializer(0.0))\n    if not decode and bias_reg:\n      tf.add_to_collection(\n          weight_collection, reg_func(\n              biases, name=""biases_reg""))\n    prev = tf.nn.xw_plus_b(prev, w_i_m, biases)\n\n    prev = tf.reshape(prev, tf.pack([batch_size, num_frames, 4, num_nodes]))\n    if state is None:\n      state = tf.fill(tf.pack([batch_size, num_nodes]), 0.0)\n    if memory is None:\n      memory = tf.fill(tf.pack([batch_size, num_nodes]), 0.0)\n\n    out, _, mem = rnn.variable_lstm(prev, state, memory, w_m_m, clip=clip)\n\n    if backward:\n      if length is None:\n        out = tf.reverse(out, [False, True, False])\n      else:\n        out = tf.reverse_sequence(out, length, 1, 0)\n\n  return out, mem\n'"
model_zoo/models/street/python/shapes.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Shape manipulation functions.\n\nrotate_dimensions: prepares for a rotating transpose by returning a rotated\n  list of dimension indices.\ntransposing_reshape: allows a dimension to be factorized, with one of the pieces\n  transferred to another dimension, or to transpose factors within a single\n  dimension.\ntensor_dim: gets a shape dimension as a constant integer if known otherwise a\n  runtime usable tensor value.\ntensor_shape: returns the full shape of a tensor as the tensor_dim.\n""""""\nimport tensorflow as tf\n\n\ndef rotate_dimensions(num_dims, src_dim, dest_dim):\n  """"""Returns a list of dimension indices that will rotate src_dim to dest_dim.\n\n  src_dim is moved to dest_dim, with all intervening dimensions shifted towards\n  the hole left by src_dim. Eg:\n  num_dims = 4, src_dim=3, dest_dim=1\n  Returned list=[0, 3, 1, 2]\n  For a tensor with dims=[5, 4, 3, 2] a transpose would yield [5, 2, 4, 3].\n  Args:\n    num_dims: The number of dimensions to handle.\n    src_dim:  The dimension to move.\n    dest_dim: The dimension to move src_dim to.\n\n  Returns:\n    A list of rotated dimension indices.\n  """"""\n  # List of dimensions for transpose.\n  dim_list = range(num_dims)\n  # Shuffle src_dim to dest_dim by swapping to shuffle up the other dims.\n  step = 1 if dest_dim > src_dim else -1\n  for x in xrange(src_dim, dest_dim, step):\n    dim_list[x], dim_list[x + step] = dim_list[x + step], dim_list[x]\n  return dim_list\n\n\ndef transposing_reshape(tensor,\n                        src_dim,\n                        part_a,\n                        part_b,\n                        dest_dim_a,\n                        dest_dim_b,\n                        name=None):\n  """"""Splits src_dim and sends one of the pieces to another dim.\n\n  Terminology:\n  A matrix is often described as \'row-major\' or \'column-major\', which doesn\'t\n  help if you can\'t remember which is the row index and which is the column,\n  even if you know what \'major\' means, so here is a simpler explanation of it:\n  When TF stores a tensor of size [d0, d1, d2, d3] indexed by [i0, i1, i2, i3],\n  the memory address of an element is calculated using:\n  ((i0 * d1 + i1) * d2 + i2) * d3 + i3, so, d0 is the MOST SIGNIFICANT dimension\n  and d3 the LEAST SIGNIFICANT, just like in the decimal number 1234, 1 is the\n  most significant digit and 4 the least significant. In both cases the most\n  significant is multiplied by the largest number to determine its \'value\'.\n  Furthermore, if we reshape the tensor to [d0\'=d0, d1\'=d1 x d2, d2\'=d3], then\n  the MOST SIGNIFICANT part of d1\' is d1 and the LEAST SIGNIFICANT part of d1\'\n  is d2.\n\n  Action:\n  transposing_reshape splits src_dim into factors [part_a, part_b], and sends\n  the most significant part (of size  part_a) to be the most significant part of\n  dest_dim_a*(Exception: see NOTE 2), and the least significant part (of size\n  part_b) to be the most significant part of dest_dim_b.\n  This is basically a combination of reshape, rotating transpose, reshape.\n  NOTE1: At least one of dest_dim_a and dest_dim_b must equal src_dim, ie one of\n  the parts always stays put, so src_dim is never totally destroyed and the\n  output number of dimensions is always the same as the input.\n  NOTE2: If dest_dim_a == dest_dim_b == src_dim, then parts a and b are simply\n  transposed within src_dim to become part_b x part_a, so the most significant\n  part becomes the least significant part and vice versa. Thus if you really\n  wanted to make one of the parts the least significant side of the destiantion,\n  the destination dimension can be internally transposed with a second call to\n  transposing_reshape.\n  NOTE3: One of part_a and part_b may be -1 to allow src_dim to be of unknown\n  size with one known-size factor. Otherwise part_a * part_b must equal the size\n  of src_dim.\n  NOTE4: The reshape preserves as many known-at-graph-build-time dimension sizes\n  as are available.\n\n  Example:\n  Input dims=[5, 2, 6, 2]\n  tensor=[[[[0, 1][2, 3][4, 5][6, 7][8, 9][10, 11]]\n           [[12, 13][14, 15][16, 17][18, 19][20, 21][22, 23]]\n          [[[24, 25]...\n  src_dim=2, part_a=2, part_b=3, dest_dim_a=3, dest_dim_b=2\n  output dims =[5, 2, 3, 4]\n  output tensor=[[[[0, 1, 6, 7][2, 3, 8, 9][4, 5, 10, 11]]\n                  [[12, 13, 18, 19][14, 15, 20, 21][16, 17, 22, 23]]]\n                 [[[24, 26, 28]...\n  Example2:\n  Input dims=[phrases, words, letters]=[2, 6, x]\n  tensor=[[[the][cat][sat][on][the][mat]]\n         [[a][stitch][in][time][saves][nine]]]\n  We can factorize the 6 words into 3x2 = [[the][cat]][[sat][on]][[the][mat]]\n  or 2x3=[[the][cat][sat]][[on][the][mat]] and\n  src_dim=1, part_a=3, part_b=2, dest_dim_a=1, dest_dim_b=1\n  would yield:\n  [[[the][sat][the][cat][on][mat]]\n   [[a][in][saves][stitch][time][nine]]], but\n  src_dim=1, part_a=2, part_b=3, dest_dim_a=1, dest_dim_b=1\n  would yield:\n  [[[the][on][cat][the][sat][mat]]\n   [[a][time][stitch][saves][in][nine]]], and\n  src_dim=1, part_a=2, part_b=3, dest_dim_a=0, dest_dim_b=1\n  would yield:\n  [[[the][cat][sat]]\n   [[a][stitch][in]]\n   [[on][the][mat]]\n   [[time][saves][nine]]]\n  Now remember that the words above represent any least-significant subset of\n  the input dimensions.\n\n  Args:\n    tensor:     A tensor to reshape.\n    src_dim:    The dimension to split.\n    part_a:     The first factor of the split.\n    part_b:     The second factor of the split.\n    dest_dim_a: The dimension to move part_a of src_dim to.\n    dest_dim_b: The dimension to move part_b of src_dim to.\n    name:       Optional base name for all the ops.\n\n  Returns:\n    Reshaped tensor.\n\n  Raises:\n    ValueError: If the args are invalid.\n  """"""\n  if dest_dim_a != src_dim and dest_dim_b != src_dim:\n    raise ValueError(\n        \'At least one of dest_dim_a, dest_dim_b must equal src_dim!\')\n  if part_a == 0 or part_b == 0:\n    raise ValueError(\'Zero not allowed for part_a or part_b!\')\n  if part_a < 0 and part_b < 0:\n    raise ValueError(\'At least one of part_a and part_b must be positive!\')\n  if not name:\n    name = \'transposing_reshape\'\n  prev_shape = tensor_shape(tensor)\n  expanded = tf.reshape(\n      tensor,\n      prev_shape[:src_dim] + [part_a, part_b] + prev_shape[src_dim + 1:],\n      name=name + \'_reshape_in\')\n  dest = dest_dim_b\n  if dest_dim_a != src_dim:\n    # We are just moving part_a to dest_dim_a.\n    dest = dest_dim_a\n  else:\n    # We are moving part_b to dest_dim_b.\n    src_dim += 1\n  dim_list = rotate_dimensions(len(expanded.get_shape()), src_dim, dest)\n  expanded = tf.transpose(expanded, dim_list, name=name + \'_rot_transpose\')\n  # Reshape identity except dest,dest+1, which get merged.\n  ex_shape = tensor_shape(expanded)\n  combined = ex_shape[dest] * ex_shape[dest + 1]\n  return tf.reshape(\n      expanded,\n      ex_shape[:dest] + [combined] + ex_shape[dest + 2:],\n      name=name + \'_reshape_out\')\n\n\ndef tensor_dim(tensor, dim):\n  """"""Returns int dimension if known at a graph build time else a tensor.\n\n  If the size of the dim of tensor is known at graph building time, then that\n  known value is returned, otherwise (instead of None), a Tensor that will give\n  the size of the dimension when the graph is run. The return value will be\n  accepted by tf.reshape in multiple (or even all) dimensions, even when the\n  sizes are not known at graph building time, unlike -1, which can only be used\n  in one dimension. It is a bad idea to use tf.shape all the time, as some ops\n  demand a known (at graph build time) size. This function therefore returns\n  the best available, most useful dimension size.\n  Args:\n    tensor: Input tensor.\n    dim:    Dimension to find the size of.\n\n  Returns:\n    An integer if shape is known at build time, otherwise a tensor of int32.\n  """"""\n  result = tensor.get_shape().as_list()[dim]\n  if result is None:\n    result = tf.shape(tensor)[dim]\n  return result\n\n\ndef tensor_shape(tensor):\n  """"""Returns a heterogeneous list of tensor_dim for the tensor.\n\n  See tensor_dim for a more detailed explanation.\n  Args:\n    tensor: Input tensor.\n\n  Returns:\n    A heterogeneous list of integers and int32 tensors.\n  """"""\n  result = []\n  for d in xrange(len(tensor.get_shape())):\n    result.append(tensor_dim(tensor, d))\n  return result\n'"
model_zoo/models/street/python/shapes_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for shapes.""""""\n\nimport numpy as np\nimport tensorflow as tf\nimport shapes\n\n\ndef _rand(*size):\n  return np.random.uniform(size=size).astype(\'f\')\n\n\nclass ShapesTest(tf.test.TestCase):\n  """"""Tests just the shapes from a call to transposing_reshape.""""""\n\n  def __init__(self, other):\n    super(ShapesTest, self).__init__(other)\n    self.batch_size = 4\n    self.im_height = 24\n    self.im_width = 36\n    self.depth = 20\n\n  def testReshapeTile(self):\n    """"""Tests that a tiled input can be reshaped to the batch dimension.""""""\n    fake = tf.placeholder(\n        tf.float32, shape=(None, None, None, self.depth), name=\'inputs\')\n    real = _rand(self.batch_size, self.im_height, self.im_width, self.depth)\n    with self.test_session() as sess:\n      outputs = shapes.transposing_reshape(\n          fake, src_dim=2, part_a=3, part_b=-1, dest_dim_a=0, dest_dim_b=2)\n      res_image = sess.run([outputs], feed_dict={fake: real})\n      self.assertEqual(\n          tuple(res_image[0].shape),\n          (self.batch_size * 3, self.im_height, self.im_width / 3, self.depth))\n\n  def testReshapeDepth(self):\n    """"""Tests that depth can be reshaped to the x dimension.""""""\n    fake = tf.placeholder(\n        tf.float32, shape=(None, None, None, self.depth), name=\'inputs\')\n    real = _rand(self.batch_size, self.im_height, self.im_width, self.depth)\n    with self.test_session() as sess:\n      outputs = shapes.transposing_reshape(\n          fake, src_dim=3, part_a=4, part_b=-1, dest_dim_a=2, dest_dim_b=3)\n      res_image = sess.run([outputs], feed_dict={fake: real})\n      self.assertEqual(\n          tuple(res_image[0].shape),\n          (self.batch_size, self.im_height, self.im_width * 4, self.depth / 4))\n\n\nclass DataTest(tf.test.TestCase):\n  """"""Tests that the data is moved correctly in a call to transposing_reshape.\n\n  """"""\n\n  def testTransposingReshape_2_2_3_2_1(self):\n    """"""Case: dest_a == src, dest_b < src: Split with Least sig part going left.\n    """"""\n    with self.test_session() as sess:\n      fake = tf.placeholder(\n          tf.float32, shape=(None, None, None, 2), name=\'inputs\')\n      outputs = shapes.transposing_reshape(\n          fake, src_dim=2, part_a=2, part_b=3, dest_dim_a=2, dest_dim_b=1)\n      # Make real inputs. The tensor looks like this:\n      # tensor=[[[[0, 1][2, 3][4, 5][6, 7][8, 9][10, 11]]\n      #          [[12, 13][14, 15][16, 17][18, 19][20, 21][22, 23]]\n      #         [[[24, 25]...\n      real = np.arange(120).reshape((5, 2, 6, 2))\n      np_array = sess.run([outputs], feed_dict={fake: real})[0]\n      self.assertEqual(tuple(np_array.shape), (5, 6, 2, 2))\n      self.assertAllEqual(np_array[0, :, :, :],\n                          [[[0, 1], [6, 7]], [[12, 13], [18, 19]],\n                           [[2, 3], [8, 9]], [[14, 15], [20, 21]],\n                           [[4, 5], [10, 11]], [[16, 17], [22, 23]]])\n\n  def testTransposingReshape_2_2_3_2_3(self):\n    """"""Case: dest_a == src, dest_b > src: Split with Least sig part going right.\n    """"""\n    with self.test_session() as sess:\n      fake = tf.placeholder(\n          tf.float32, shape=(None, None, None, 2), name=\'inputs\')\n      outputs = shapes.transposing_reshape(\n          fake, src_dim=2, part_a=2, part_b=3, dest_dim_a=2, dest_dim_b=3)\n      # Make real inputs. The tensor looks like this:\n      # tensor=[[[[0, 1][2, 3][4, 5][6, 7][8, 9][10, 11]]\n      #          [[12, 13][14, 15][16, 17][18, 19][20, 21][22, 23]]\n      #         [[[24, 25]...\n      real = np.arange(120).reshape((5, 2, 6, 2))\n      np_array = sess.run([outputs], feed_dict={fake: real})[0]\n      self.assertEqual(tuple(np_array.shape), (5, 2, 2, 6))\n      self.assertAllEqual(\n          np_array[0, :, :, :],\n          [[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11]],\n           [[12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]]])\n\n  def testTransposingReshape_2_2_3_2_2(self):\n    """"""Case: dest_a == src, dest_b == src. Transpose within dimension 2.\n    """"""\n    with self.test_session() as sess:\n      fake = tf.placeholder(\n          tf.float32, shape=(None, None, None, 2), name=\'inputs\')\n      outputs = shapes.transposing_reshape(\n          fake, src_dim=2, part_a=2, part_b=3, dest_dim_a=2, dest_dim_b=2)\n      # Make real inputs. The tensor looks like this:\n      # tensor=[[[[0, 1][2, 3][4, 5][6, 7][8, 9][10, 11]]\n      #          [[12, 13][14, 15][16, 17][18, 19][20, 21][22, 23]]\n      #         [[[24, 25]...\n      real = np.arange(120).reshape((5, 2, 6, 2))\n      np_array = sess.run([outputs], feed_dict={fake: real})[0]\n      self.assertEqual(tuple(np_array.shape), (5, 2, 6, 2))\n      self.assertAllEqual(\n          np_array[0, :, :, :],\n          [[[0, 1], [6, 7], [2, 3], [8, 9], [4, 5], [10, 11]],\n           [[12, 13], [18, 19], [14, 15], [20, 21], [16, 17], [22, 23]]])\n\n  def testTransposingReshape_2_2_3_1_2(self):\n    """"""Case: dest_a < src, dest_b == src. Split with Most sig part going left.\n    """"""\n    with self.test_session() as sess:\n      fake = tf.placeholder(\n          tf.float32, shape=(None, None, None, 2), name=\'inputs\')\n      outputs = shapes.transposing_reshape(\n          fake, src_dim=2, part_a=2, part_b=3, dest_dim_a=1, dest_dim_b=2)\n      # Make real inputs. The tensor looks like this:\n      # tensor=[[[[0, 1][2, 3][4, 5][6, 7][8, 9][10, 11]]\n      #          [[12, 13][14, 15][16, 17][18, 19][20, 21][22, 23]]\n      #         [[[24, 25]...\n      real = np.arange(120).reshape((5, 2, 6, 2))\n      np_array = sess.run([outputs], feed_dict={fake: real})[0]\n      self.assertEqual(tuple(np_array.shape), (5, 4, 3, 2))\n      self.assertAllEqual(np_array[0, :, :, :],\n                          [[[0, 1], [2, 3], [4, 5]],\n                           [[12, 13], [14, 15], [16, 17]],\n                           [[6, 7], [8, 9], [10, 11]],\n                           [[18, 19], [20, 21], [22, 23]]])\n\n  def testTransposingReshape_2_2_3_3_2(self):\n    """"""Case: dest_a < src, dest_b == src. Split with Most sig part going right.\n    """"""\n    with self.test_session() as sess:\n      fake = tf.placeholder(\n          tf.float32, shape=(None, None, None, 2), name=\'inputs\')\n      outputs = shapes.transposing_reshape(\n          fake, src_dim=2, part_a=2, part_b=3, dest_dim_a=3, dest_dim_b=2)\n      # Make real inputs. The tensor looks like this:\n      # tensor=[[[[0, 1][2, 3][4, 5][6, 7][8, 9][10, 11]]\n      #          [[12, 13][14, 15][16, 17][18, 19][20, 21][22, 23]]\n      #         [[[24, 25]...\n      real = np.arange(120).reshape((5, 2, 6, 2))\n      np_array = sess.run([outputs], feed_dict={fake: real})[0]\n      self.assertEqual(tuple(np_array.shape), (5, 2, 3, 4))\n      self.assertAllEqual(\n          np_array[0, :, :, :],\n          [[[0, 1, 6, 7], [2, 3, 8, 9], [4, 5, 10, 11]],\n           [[12, 13, 18, 19], [14, 15, 20, 21], [16, 17, 22, 23]]])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/street/python/vgsl_eval.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Model eval separate from training.""""""\nfrom tensorflow import app\nfrom tensorflow.python.platform import flags\n\nimport vgsl_model\n\nflags.DEFINE_string(\'eval_dir\', \'/tmp/mdir/eval\',\n                    \'Directory where to write event logs.\')\nflags.DEFINE_string(\'graph_def_file\', None,\n                    \'Output eval graph definition file.\')\nflags.DEFINE_string(\'train_dir\', \'/tmp/mdir\',\n                    \'Directory where to find training checkpoints.\')\nflags.DEFINE_string(\'model_str\',\n                    \'1,150,600,3[S2(4x150)0,2 Ct5,5,16 Mp2,2 Ct5,5,64 Mp3,3\'\n                    \'([Lrys64 Lbx128][Lbys64 Lbx128][Lfys64 Lbx128])S3(3x0)2,3\'\n                    \'Lfx128 Lrx128 S0(1x4)0,3 Do Lfx256]O1c134\',\n                    \'Network description.\')\nflags.DEFINE_integer(\'num_steps\', 1000, \'Number of steps to run evaluation.\')\nflags.DEFINE_integer(\'eval_interval_secs\', 60,\n                     \'Time interval between eval runs.\')\nflags.DEFINE_string(\'eval_data\', None, \'Evaluation data filepattern\')\nflags.DEFINE_string(\'decoder\', None, \'Charset decoder\')\n\nFLAGS = flags.FLAGS\n\n\ndef main(argv):\n  del argv\n  vgsl_model.Eval(FLAGS.train_dir, FLAGS.eval_dir, FLAGS.model_str,\n                  FLAGS.eval_data, FLAGS.decoder, FLAGS.num_steps,\n                  FLAGS.graph_def_file, FLAGS.eval_interval_secs)\n\n\nif __name__ == \'__main__\':\n  app.run()\n'"
model_zoo/models/street/python/vgsl_input.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""String network description language to define network layouts.""""""\nimport collections\nimport tensorflow as tf\nfrom tensorflow.python.ops import parsing_ops\n\n# Named tuple for the standard tf image tensor Shape.\n# batch_size:     Number of images to batch-up for training.\n# height:         Fixed height of image or None for variable.\n# width:          Fixed width of image or None for variable.\n# depth:          Desired depth in bytes per pixel of input images.\nImageShape = collections.namedtuple(\'ImageTensorDims\',\n                                    [\'batch_size\', \'height\', \'width\', \'depth\'])\n\n\ndef ImageInput(input_pattern, num_threads, shape, using_ctc, reader=None):\n  """"""Creates an input image tensor from the input_pattern filenames.\n\n  TODO(rays) Expand for 2-d labels, 0-d labels, and logistic targets.\n  Args:\n    input_pattern:  Filenames of the dataset(s) to read.\n    num_threads:    Number of preprocessing threads.\n    shape:          ImageShape with the desired shape of the input.\n    using_ctc:      Take the unpadded_class labels instead of padded.\n    reader:         Function that returns an actual reader to read Examples from\n      input files. If None, uses tf.TFRecordReader().\n  Returns:\n    images:   Float Tensor containing the input image scaled to [-1.28, 1.27].\n    heights:  Tensor int64 containing the heights of the images.\n    widths:   Tensor int64 containing the widths of the images.\n    labels:   Serialized SparseTensor containing the int64 labels.\n    sparse_labels:   Serialized SparseTensor containing the int64 labels.\n    truths:   Tensor string of the utf8 truth texts.\n  Raises:\n    ValueError: if the optimizer type is unrecognized.\n  """"""\n  data_files = tf.gfile.Glob(input_pattern)\n  assert data_files, \'no files found for dataset \' + input_pattern\n  queue_capacity = shape.batch_size * num_threads * 2\n  filename_queue = tf.train.string_input_producer(\n      data_files, capacity=queue_capacity)\n\n  # Create a subgraph with its own reader (but sharing the\n  # filename_queue) for each preprocessing thread.\n  images_and_label_lists = []\n  for _ in range(num_threads):\n    image, height, width, labels, text = _ReadExamples(filename_queue, shape,\n                                                       using_ctc, reader)\n    images_and_label_lists.append([image, height, width, labels, text])\n  # Create a queue that produces the examples in batches.\n  images, heights, widths, labels, truths = tf.train.batch_join(\n      images_and_label_lists,\n      batch_size=shape.batch_size,\n      capacity=16 * shape.batch_size,\n      dynamic_pad=True)\n  # Deserialize back to sparse, because the batcher doesn\'t do sparse.\n  labels = tf.deserialize_many_sparse(labels, tf.int64)\n  sparse_labels = tf.cast(labels, tf.int32)\n  labels = tf.sparse_tensor_to_dense(labels)\n  labels = tf.reshape(labels, [shape.batch_size, -1], name=\'Labels\')\n  # Crush the other shapes to just the batch dimension.\n  heights = tf.reshape(heights, [-1], name=\'Heights\')\n  widths = tf.reshape(widths, [-1], name=\'Widths\')\n  truths = tf.reshape(truths, [-1], name=\'Truths\')\n  # Give the images a nice name as well.\n  images = tf.identity(images, name=\'Images\')\n\n  tf.image_summary(\'Images\', images)\n  return images, heights, widths, labels, sparse_labels, truths\n\n\ndef _ReadExamples(filename_queue, shape, using_ctc, reader=None):\n  """"""Builds network input tensor ops for TF Example.\n\n  Args:\n    filename_queue: Queue of filenames, from tf.train.string_input_producer\n    shape:          ImageShape with the desired shape of the input.\n    using_ctc:      Take the unpadded_class labels instead of padded.\n    reader:         Function that returns an actual reader to read Examples from\n      input files. If None, uses tf.TFRecordReader().\n  Returns:\n    image:   Float Tensor containing the input image scaled to [-1.28, 1.27].\n    height:  Tensor int64 containing the height of the image.\n    width:   Tensor int64 containing the width of the image.\n    labels:  Serialized SparseTensor containing the int64 labels.\n    text:    Tensor string of the utf8 truth text.\n  """"""\n  if reader:\n    reader = reader()\n  else:\n    reader = tf.TFRecordReader()\n  _, example_serialized = reader.read(filename_queue)\n  example_serialized = tf.reshape(example_serialized, shape=[])\n  features = tf.parse_single_example(\n      example_serialized,\n      {\'image/encoded\': parsing_ops.FixedLenFeature(\n          [1], dtype=tf.string, default_value=\'\'),\n       \'image/text\': parsing_ops.FixedLenFeature(\n           [1], dtype=tf.string, default_value=\'\'),\n       \'image/class\': parsing_ops.VarLenFeature(dtype=tf.int64),\n       \'image/unpadded_class\': parsing_ops.VarLenFeature(dtype=tf.int64),\n       \'image/height\': parsing_ops.FixedLenFeature(\n           [1], dtype=tf.int64, default_value=1),\n       \'image/width\': parsing_ops.FixedLenFeature(\n           [1], dtype=tf.int64, default_value=1)})\n  if using_ctc:\n    labels = features[\'image/unpadded_class\']\n  else:\n    labels = features[\'image/class\']\n  labels = tf.serialize_sparse(labels)\n  image = tf.reshape(features[\'image/encoded\'], shape=[], name=\'encoded\')\n  image = _ImageProcessing(image, shape)\n  height = tf.reshape(features[\'image/height\'], [-1])\n  width = tf.reshape(features[\'image/width\'], [-1])\n  text = tf.reshape(features[\'image/text\'], shape=[])\n\n  return image, height, width, labels, text\n\n\ndef _ImageProcessing(image_buffer, shape):\n  """"""Convert a PNG string into an input tensor.\n\n  We allow for fixed and variable sizes.\n  Does fixed conversion to floats in the range [-1.28, 1.27].\n  Args:\n    image_buffer: Tensor containing a PNG encoded image.\n    shape:          ImageShape with the desired shape of the input.\n  Returns:\n    image:        Decoded, normalized image in the range [-1.28, 1.27].\n  """"""\n  image = tf.image.decode_png(image_buffer, channels=shape.depth)\n  image.set_shape([shape.height, shape.width, shape.depth])\n  image = tf.cast(image, tf.float32)\n  image = tf.sub(image, 128.0)\n  image = tf.mul(image, 1 / 100.0)\n  return image\n'"
model_zoo/models/street/python/vgsl_model.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""String network description language to define network layouts.""""""\nimport re\nimport time\n\nimport decoder\nimport errorcounter as ec\nimport shapes\nimport tensorflow as tf\nimport vgsl_input\nimport vgslspecs\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.core.framework import summary_pb2\nfrom tensorflow.python.platform import tf_logging as logging\n\n\n# Parameters for rate decay.\n# We divide the learning_rate_halflife by DECAY_STEPS_FACTOR and use DECAY_RATE\n# as the decay factor for the learning rate, ie we use the DECAY_STEPS_FACTORth\n# root of 2 as the decay rate every halflife/DECAY_STEPS_FACTOR to achieve the\n# desired halflife.\nDECAY_STEPS_FACTOR = 16\nDECAY_RATE = pow(0.5, 1.0 / DECAY_STEPS_FACTOR)\n\n\ndef Train(train_dir,\n          model_str,\n          train_data,\n          max_steps,\n          master=\'\',\n          task=0,\n          ps_tasks=0,\n          initial_learning_rate=0.001,\n          final_learning_rate=0.001,\n          learning_rate_halflife=160000,\n          optimizer_type=\'Adam\',\n          num_preprocess_threads=1,\n          reader=None):\n  """"""Testable trainer with no dependence on FLAGS.\n\n  Args:\n    train_dir: Directory to write checkpoints.\n    model_str: Network specification string.\n    train_data: Training data file pattern.\n    max_steps: Number of training steps to run.\n    master: Name of the TensorFlow master to use.\n    task: Task id of this replica running the training. (0 will be master).\n    ps_tasks: Number of tasks in ps job, or 0 if no ps job.\n    initial_learning_rate: Learing rate at start of training.\n    final_learning_rate: Asymptotic minimum learning rate.\n    learning_rate_halflife: Number of steps over which to halve the difference\n      between initial and final learning rate.\n    optimizer_type: One of \'GradientDescent\', \'AdaGrad\', \'Momentum\', \'Adam\'.\n    num_preprocess_threads: Number of input threads.\n    reader: Function that returns an actual reader to read Examples from input\n      files. If None, uses tf.TFRecordReader().\n  """"""\n  if master.startswith(\'local\'):\n    device = tf.ReplicaDeviceSetter(ps_tasks)\n  else:\n    device = \'/cpu:0\'\n  with tf.Graph().as_default():\n    with tf.device(device):\n      model = InitNetwork(train_data, model_str, \'train\', initial_learning_rate,\n                          final_learning_rate, learning_rate_halflife,\n                          optimizer_type, num_preprocess_threads, reader)\n\n      # Create a Supervisor.  It will take care of initialization, summaries,\n      # checkpoints, and recovery.\n      #\n      # When multiple replicas of this program are running, the first one,\n      # identified by --task=0 is the \'chief\' supervisor.  It is the only one\n      # that takes case of initialization, etc.\n      sv = tf.train.Supervisor(\n          logdir=train_dir,\n          is_chief=(task == 0),\n          saver=model.saver,\n          save_summaries_secs=10,\n          save_model_secs=30,\n          recovery_wait_secs=5)\n\n      step = 0\n      while step < max_steps:\n        try:\n          # Get an initialized, and possibly recovered session.  Launch the\n          # services: Checkpointing, Summaries, step counting.\n          with sv.managed_session(master) as sess:\n            while step < max_steps:\n              _, step = model.TrainAStep(sess)\n              if sv.coord.should_stop():\n                break\n        except tf.errors.AbortedError as e:\n          logging.error(\'Received error:%s\', e)\n          continue\n\n\ndef Eval(train_dir,\n         eval_dir,\n         model_str,\n         eval_data,\n         decoder_file,\n         num_steps,\n         graph_def_file=None,\n         eval_interval_secs=0,\n         reader=None):\n  """"""Restores a model from a checkpoint and evaluates it.\n\n  Args:\n    train_dir: Directory to find checkpoints.\n    eval_dir: Directory to write summary events.\n    model_str: Network specification string.\n    eval_data: Evaluation data file pattern.\n    decoder_file: File to read to decode the labels.\n    num_steps: Number of eval steps to run.\n    graph_def_file: File to write graph definition to for freezing.\n    eval_interval_secs: How often to run evaluations, or once if 0.\n    reader: Function that returns an actual reader to read Examples from input\n      files. If None, uses tf.TFRecordReader().\n  Returns:\n    (char error rate, word recall error rate, sequence error rate) as percent.\n  Raises:\n    ValueError: If unimplemented feature is used.\n  """"""\n  decode = None\n  if decoder_file:\n    decode = decoder.Decoder(decoder_file)\n\n  # Run eval.\n  rates = ec.ErrorRates(\n      label_error=None,\n      word_recall_error=None,\n      word_precision_error=None,\n      sequence_error=None)\n  with tf.Graph().as_default():\n    model = InitNetwork(eval_data, model_str, \'eval\', reader=reader)\n    sw = tf.train.SummaryWriter(eval_dir)\n\n    while True:\n      sess = tf.Session(\'\')\n      if graph_def_file is not None:\n        # Write the eval version of the graph to a file for freezing.\n        if not tf.gfile.Exists(graph_def_file):\n          with tf.gfile.FastGFile(graph_def_file, \'w\') as f:\n            f.write(\n                sess.graph.as_graph_def(add_shapes=True).SerializeToString())\n      ckpt = tf.train.get_checkpoint_state(train_dir)\n      if ckpt and ckpt.model_checkpoint_path:\n        step = model.Restore(ckpt.model_checkpoint_path, sess)\n        if decode:\n          rates = decode.SoftmaxEval(sess, model, num_steps)\n          _AddRateToSummary(\'Label error rate\', rates.label_error, step, sw)\n          _AddRateToSummary(\'Word recall error rate\', rates.word_recall_error,\n                            step, sw)\n          _AddRateToSummary(\'Word precision error rate\',\n                            rates.word_precision_error, step, sw)\n          _AddRateToSummary(\'Sequence error rate\', rates.sequence_error, step,\n                            sw)\n          sw.flush()\n          print \'Error rates=\', rates\n        else:\n          raise ValueError(\'Non-softmax decoder evaluation not implemented!\')\n      if eval_interval_secs:\n        time.sleep(eval_interval_secs)\n      else:\n        break\n  return rates\n\n\ndef InitNetwork(input_pattern,\n                model_spec,\n                mode=\'eval\',\n                initial_learning_rate=0.00005,\n                final_learning_rate=0.00005,\n                halflife=1600000,\n                optimizer_type=\'Adam\',\n                num_preprocess_threads=1,\n                reader=None):\n  """"""Constructs a python tensor flow model defined by model_spec.\n\n  Args:\n    input_pattern: File pattern of the data in tfrecords of Example.\n    model_spec: Concatenation of input spec, model spec and output spec.\n      See Build below for input/output spec. For model spec, see vgslspecs.py\n    mode: One of \'train\', \'eval\'\n    initial_learning_rate: Initial learning rate for the network.\n    final_learning_rate: Final learning rate for the network.\n    halflife: Number of steps over which to halve the difference between\n              initial and final learning rate for the network.\n    optimizer_type: One of \'GradientDescent\', \'AdaGrad\', \'Momentum\', \'Adam\'.\n    num_preprocess_threads: Number of threads to use for image processing.\n    reader: Function that returns an actual reader to read Examples from input\n      files. If None, uses tf.TFRecordReader().\n    Eval tasks need only specify input_pattern and model_spec.\n\n  Returns:\n    A VGSLImageModel class.\n\n  Raises:\n    ValueError: if the model spec syntax is incorrect.\n  """"""\n  model = VGSLImageModel(mode, model_spec, initial_learning_rate,\n                         final_learning_rate, halflife)\n  left_bracket = model_spec.find(\'[\')\n  right_bracket = model_spec.rfind(\']\')\n  if left_bracket < 0 or right_bracket < 0:\n    raise ValueError(\'Failed to find [] in model spec! \', model_spec)\n  input_spec = model_spec[:left_bracket]\n  layer_spec = model_spec[left_bracket:right_bracket + 1]\n  output_spec = model_spec[right_bracket + 1:]\n  model.Build(input_pattern, input_spec, layer_spec, output_spec,\n              optimizer_type, num_preprocess_threads, reader)\n  return model\n\n\nclass VGSLImageModel(object):\n  """"""Class that builds a tensor flow model for training or evaluation.\n  """"""\n\n  def __init__(self, mode, model_spec, initial_learning_rate,\n               final_learning_rate, halflife):\n    """"""Constructs a VGSLImageModel.\n\n    Args:\n      mode:        One of ""train"", ""eval""\n      model_spec:  Full model specification string, for reference only.\n      initial_learning_rate: Initial learning rate for the network.\n      final_learning_rate: Final learning rate for the network.\n      halflife: Number of steps over which to halve the difference between\n                initial and final learning rate for the network.\n    """"""\n    # The string that was used to build this model.\n    self.model_spec = model_spec\n    # The layers between input and output.\n    self.layers = None\n    # The train/eval mode.\n    self.mode = mode\n    # The initial learning rate.\n    self.initial_learning_rate = initial_learning_rate\n    self.final_learning_rate = final_learning_rate\n    self.decay_steps = halflife / DECAY_STEPS_FACTOR\n    self.decay_rate = DECAY_RATE\n    # Tensor for the labels.\n    self.labels = None\n    self.sparse_labels = None\n    # Debug data containing the truth text.\n    self.truths = None\n    # Tensor for loss\n    self.loss = None\n    # Train operation\n    self.train_op = None\n    # Tensor for the global step counter\n    self.global_step = None\n    # Tensor for the output predictions (usually softmax)\n    self.output = None\n    # True if we are using CTC training mode.\n    self.using_ctc = False\n    # Saver object to load or restore the variables.\n    self.saver = None\n\n  def Build(self, input_pattern, input_spec, model_spec, output_spec,\n            optimizer_type, num_preprocess_threads, reader):\n    """"""Builds the model from the separate input/layers/output spec strings.\n\n    Args:\n      input_pattern: File pattern of the data in tfrecords of TF Example format.\n      input_spec: Specification of the input layer:\n        batchsize,height,width,depth (4 comma-separated integers)\n          Training will run with batches of batchsize images, but runtime can\n          use any batch size.\n          height and/or width can be 0 or -1, indicating variable size,\n          otherwise all images must be the given size.\n          depth must be 1 or 3 to indicate greyscale or color.\n          NOTE 1-d image input, treating the y image dimension as depth, can\n          be achieved using S1(1x0)1,3 as the first op in the model_spec, but\n          the y-size of the input must then be fixed.\n      model_spec: Model definition. See vgslspecs.py\n      output_spec: Output layer definition:\n        O(2|1|0)(l|s|c)n output layer with n classes.\n          2 (heatmap) Output is a 2-d vector map of the input (possibly at\n            different scale).\n          1 (sequence) Output is a 1-d sequence of vector values.\n          0 (value) Output is a 0-d single vector value.\n          l uses a logistic non-linearity on the output, allowing multiple\n            hot elements in any output vector value.\n          s uses a softmax non-linearity, with one-hot output in each value.\n          c uses a softmax with CTC. Can only be used with s (sequence).\n          NOTE Only O1s and O1c are currently supported.\n      optimizer_type: One of \'GradientDescent\', \'AdaGrad\', \'Momentum\', \'Adam\'.\n      num_preprocess_threads: Number of threads to use for image processing.\n      reader: Function that returns an actual reader to read Examples from input\n        files. If None, uses tf.TFRecordReader().\n    """"""\n    self.global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n    shape = _ParseInputSpec(input_spec)\n    out_dims, out_func, num_classes = _ParseOutputSpec(output_spec)\n    self.using_ctc = out_func == \'c\'\n    images, heights, widths, labels, sparse, _ = vgsl_input.ImageInput(\n        input_pattern, num_preprocess_threads, shape, self.using_ctc, reader)\n    self.labels = labels\n    self.sparse_labels = sparse\n    self.layers = vgslspecs.VGSLSpecs(widths, heights, self.mode == \'train\')\n    last_layer = self.layers.Build(images, model_spec)\n    self._AddOutputs(last_layer, out_dims, out_func, num_classes)\n    if self.mode == \'train\':\n      self._AddOptimizer(optimizer_type)\n\n    # For saving the model across training and evaluation\n    self.saver = tf.train.Saver()\n\n  def TrainAStep(self, sess):\n    """"""Runs a training step in the session.\n\n    Args:\n      sess: Session in which to train the model.\n    Returns:\n      loss, global_step.\n    """"""\n    _, loss, step = sess.run([self.train_op, self.loss, self.global_step])\n    return loss, step\n\n  def Restore(self, checkpoint_path, sess):\n    """"""Restores the model from the given checkpoint path into the session.\n\n    Args:\n      checkpoint_path: File pathname of the checkpoint.\n      sess:            Session in which to restore the model.\n    Returns:\n      global_step of the model.\n    """"""\n    self.saver.restore(sess, checkpoint_path)\n    return tf.train.global_step(sess, self.global_step)\n\n  def RunAStep(self, sess):\n    """"""Runs a step for eval in the session.\n\n    Args:\n      sess:            Session in which to run the model.\n    Returns:\n      output tensor result, labels tensor result.\n    """"""\n    return sess.run([self.output, self.labels])\n\n  def _AddOutputs(self, prev_layer, out_dims, out_func, num_classes):\n    """"""Adds the output layer and loss function.\n\n    Args:\n      prev_layer:  Output of last layer of main network.\n      out_dims:    Number of output dimensions, 0, 1 or 2.\n      out_func:    Output non-linearity. \'s\' or \'c\'=softmax, \'l\'=logistic.\n      num_classes: Number of outputs/size of last output dimension.\n    """"""\n    height_in = shapes.tensor_dim(prev_layer, dim=1)\n    logits, outputs = self._AddOutputLayer(prev_layer, out_dims, out_func,\n                                           num_classes)\n    if self.mode == \'train\':\n      # Setup loss for training.\n      self.loss = self._AddLossFunction(logits, height_in, out_dims, out_func)\n      tf.scalar_summary(\'loss\', self.loss, name=\'loss\')\n    elif out_dims == 0:\n      # Be sure the labels match the output, even in eval mode.\n      self.labels = tf.slice(self.labels, [0, 0], [-1, 1])\n      self.labels = tf.reshape(self.labels, [-1])\n\n    logging.info(\'Final output=%s\', outputs)\n    logging.info(\'Labels tensor=%s\', self.labels)\n    self.output = outputs\n\n  def _AddOutputLayer(self, prev_layer, out_dims, out_func, num_classes):\n    """"""Add the fully-connected logits and SoftMax/Logistic output Layer.\n\n    Args:\n      prev_layer:  Output of last layer of main network.\n      out_dims:    Number of output dimensions, 0, 1 or 2.\n      out_func:    Output non-linearity. \'s\' or \'c\'=softmax, \'l\'=logistic.\n      num_classes: Number of outputs/size of last output dimension.\n\n    Returns:\n      logits:  Pre-softmax/logistic fully-connected output shaped to out_dims.\n      outputs: Post-softmax/logistic shaped to out_dims.\n\n    Raises:\n      ValueError: if syntax is incorrect.\n    """"""\n    # Reduce dimensionality appropriate to the output dimensions.\n    batch_in = shapes.tensor_dim(prev_layer, dim=0)\n    height_in = shapes.tensor_dim(prev_layer, dim=1)\n    width_in = shapes.tensor_dim(prev_layer, dim=2)\n    depth_in = shapes.tensor_dim(prev_layer, dim=3)\n    if out_dims:\n      # Combine any remaining height and width with batch and unpack after.\n      shaped = tf.reshape(prev_layer, [-1, depth_in])\n    else:\n      # Everything except batch goes to depth, and therefore has to be known.\n      shaped = tf.reshape(prev_layer, [-1, height_in * width_in * depth_in])\n    logits = slim.fully_connected(shaped, num_classes, activation_fn=None)\n    if out_func == \'l\':\n      raise ValueError(\'Logistic not yet supported!\')\n    else:\n      output = tf.nn.softmax(logits)\n    # Reshape to the dessired output.\n    if out_dims == 2:\n      output_shape = [batch_in, height_in, width_in, num_classes]\n    elif out_dims == 1:\n      output_shape = [batch_in, height_in * width_in, num_classes]\n    else:\n      output_shape = [batch_in, num_classes]\n    output = tf.reshape(output, output_shape, name=\'Output\')\n    logits = tf.reshape(logits, output_shape)\n    return logits, output\n\n  def _AddLossFunction(self, logits, height_in, out_dims, out_func):\n    """"""Add the appropriate loss function.\n\n    Args:\n      logits:  Pre-softmax/logistic fully-connected output shaped to out_dims.\n      height_in:  Height of logits before going into the softmax layer.\n      out_dims:   Number of output dimensions, 0, 1 or 2.\n      out_func:   Output non-linearity. \'s\' or \'c\'=softmax, \'l\'=logistic.\n\n    Returns:\n      loss: That which is to be minimized.\n\n    Raises:\n      ValueError: if logistic is used.\n    """"""\n    if out_func == \'c\':\n      # Transpose batch to the middle.\n      ctc_input = tf.transpose(logits, [1, 0, 2])\n      # Compute the widths of each batch element from the input widths.\n      widths = self.layers.GetLengths(dim=2, factor=height_in)\n      cross_entropy = tf.nn.ctc_loss(ctc_input, self.sparse_labels, widths)\n    elif out_func == \'s\':\n      if out_dims == 2:\n        self.labels = _PadLabels3d(logits, self.labels)\n      elif out_dims == 1:\n        self.labels = _PadLabels2d(\n            shapes.tensor_dim(\n                logits, dim=1), self.labels)\n      else:\n        self.labels = tf.slice(self.labels, [0, 0], [-1, 1])\n        self.labels = tf.reshape(self.labels, [-1])\n      cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits, self.labels, name=\'xent\')\n    else:\n      # TODO(rays) Labels need an extra dimension for logistic, so different\n      # padding functions are needed, as well as a different loss function.\n      raise ValueError(\'Logistic not yet supported!\')\n    return tf.reduce_sum(cross_entropy)\n\n  def _AddOptimizer(self, optimizer_type):\n    """"""Adds an optimizer with learning rate decay to minimize self.loss.\n\n    Args:\n      optimizer_type: One of \'GradientDescent\', \'AdaGrad\', \'Momentum\', \'Adam\'.\n    Raises:\n      ValueError: if the optimizer type is unrecognized.\n    """"""\n    learn_rate_delta = self.initial_learning_rate - self.final_learning_rate\n    learn_rate_dec = tf.add(\n        tf.train.exponential_decay(learn_rate_delta, self.global_step,\n                                   self.decay_steps, self.decay_rate),\n        self.final_learning_rate)\n    if optimizer_type == \'GradientDescent\':\n      opt = tf.train.GradientDescentOptimizer(learn_rate_dec)\n    elif optimizer_type == \'AdaGrad\':\n      opt = tf.train.AdagradOptimizer(learn_rate_dec)\n    elif optimizer_type == \'Momentum\':\n      opt = tf.train.MomentumOptimizer(learn_rate_dec, momentum=0.9)\n    elif optimizer_type == \'Adam\':\n      opt = tf.train.AdamOptimizer(learning_rate=learn_rate_dec)\n    else:\n      raise ValueError(\'Invalid optimizer type: \' + optimizer_type)\n    tf.scalar_summary(\'learn_rate\', learn_rate_dec, name=\'lr_summ\')\n\n    self.train_op = opt.minimize(\n        self.loss, global_step=self.global_step, name=\'train\')\n\n\ndef _PadLabels3d(logits, labels):\n  """"""Pads or slices 3-d labels to match logits.\n\n  Covers the case of 2-d softmax output, when labels is [batch, height, width]\n  and logits is [batch, height, width, onehot]\n  Args:\n    logits: 4-d Pre-softmax fully-connected output.\n    labels: 3-d, but not necessarily matching in size.\n\n  Returns:\n    labels: Resized by padding or clipping to match logits.\n  """"""\n  logits_shape = shapes.tensor_shape(logits)\n  labels_shape = shapes.tensor_shape(labels)\n  labels = tf.reshape(labels, [-1, labels_shape[2]])\n  labels = _PadLabels2d(logits_shape[2], labels)\n  labels = tf.reshape(labels, [labels_shape[0], -1])\n  labels = _PadLabels2d(logits_shape[1] * logits_shape[2], labels)\n  return tf.reshape(labels, [labels_shape[0], logits_shape[1], logits_shape[2]])\n\n\ndef _PadLabels2d(logits_size, labels):\n  """"""Pads or slices the 2nd dimension of 2-d labels to match logits_size.\n\n  Covers the case of 1-d softmax output, when labels is [batch, seq] and\n  logits is [batch, seq, onehot]\n  Args:\n    logits_size: Tensor returned from tf.shape giving the target size.\n    labels:      2-d, but not necessarily matching in size.\n\n  Returns:\n    labels: Resized by padding or clipping the last dimension to logits_size.\n  """"""\n  pad = logits_size - tf.shape(labels)[1]\n\n  def _PadFn():\n    return tf.pad(labels, [[0, 0], [0, pad]])\n\n  def _SliceFn():\n    return tf.slice(labels, [0, 0], [-1, logits_size])\n\n  return tf.cond(tf.greater(pad, 0), _PadFn, _SliceFn)\n\n\ndef _ParseInputSpec(input_spec):\n  """"""Parses input_spec and returns the numbers obtained therefrom.\n\n  Args:\n    input_spec:  Specification of the input layer. See Build.\n\n  Returns:\n    shape:      ImageShape with the desired shape of the input.\n\n  Raises:\n    ValueError: if syntax is incorrect.\n  """"""\n  pattern = re.compile(R\'(\\d+),(\\d+),(\\d+),(\\d+)\')\n  m = pattern.match(input_spec)\n  if m is None:\n    raise ValueError(\'Failed to parse input spec:\' + input_spec)\n  batch_size = int(m.group(1))\n  y_size = int(m.group(2)) if int(m.group(2)) > 0 else None\n  x_size = int(m.group(3)) if int(m.group(3)) > 0 else None\n  depth = int(m.group(4))\n  if depth not in [1, 3]:\n    raise ValueError(\'Depth must be 1 or 3, had:\', depth)\n  return vgsl_input.ImageShape(batch_size, y_size, x_size, depth)\n\n\ndef _ParseOutputSpec(output_spec):\n  """"""Parses the output spec.\n\n  Args:\n    output_spec: Output layer definition. See Build.\n\n  Returns:\n    out_dims:     2|1|0 for 2-d, 1-d, 0-d.\n    out_func:     l|s|c for logistic, softmax, softmax+CTC\n    num_classes:  Number of classes in output.\n\n  Raises:\n    ValueError: if syntax is incorrect.\n  """"""\n  pattern = re.compile(R\'(O)(0|1|2)(l|s|c)(\\d+)\')\n  m = pattern.match(output_spec)\n  if m is None:\n    raise ValueError(\'Failed to parse output spec:\' + output_spec)\n  out_dims = int(m.group(2))\n  out_func = m.group(3)\n  if out_func == \'c\' and out_dims != 1:\n    raise ValueError(\'CTC can only be used with a 1-D sequence!\')\n  num_classes = int(m.group(4))\n  return out_dims, out_func, num_classes\n\n\ndef _AddRateToSummary(tag, rate, step, sw):\n  """"""Adds the given rate to the summary with the given tag.\n\n  Args:\n    tag:   Name for this value.\n    rate:  Value to add to the summary. Perhaps an error rate.\n    step:  Global step of the graph for the x-coordinate of the summary.\n    sw:    Summary writer to which to write the rate value.\n  """"""\n  sw.add_summary(\n      summary_pb2.Summary(value=[summary_pb2.Summary.Value(\n          tag=tag, simple_value=rate)]), step)\n'"
model_zoo/models/street/python/vgsl_model_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for vgsl_model.""""""\nimport os\n\nimport numpy as np\nimport tensorflow as tf\nimport vgsl_input\nimport vgsl_model\n\n\ndef _testdata(filename):\n  return os.path.join(\'../testdata/\', filename)\n\n\ndef _rand(*size):\n  return np.random.uniform(size=size).astype(\'f\')\n\n\nclass VgslModelTest(tf.test.TestCase):\n\n  def testParseInputSpec(self):\n    """"""The parser must return the numbers in the correct order.\n    """"""\n    shape = vgsl_model._ParseInputSpec(input_spec=\'32,42,256,3\')\n    self.assertEqual(\n        shape,\n        vgsl_input.ImageShape(\n            batch_size=32, height=42, width=256, depth=3))\n    # Nones must be inserted for zero sizes.\n    shape = vgsl_model._ParseInputSpec(input_spec=\'1,0,0,3\')\n    self.assertEqual(\n        shape,\n        vgsl_input.ImageShape(\n            batch_size=1, height=None, width=None, depth=3))\n\n  def testParseOutputSpec(self):\n    """"""The parser must return the correct args in the correct order.\n    """"""\n    out_dims, out_func, num_classes = vgsl_model._ParseOutputSpec(\n        output_spec=\'O1c142\')\n    self.assertEqual(out_dims, 1)\n    self.assertEqual(out_func, \'c\')\n    self.assertEqual(num_classes, 142)\n    out_dims, out_func, num_classes = vgsl_model._ParseOutputSpec(\n        output_spec=\'O2s99\')\n    self.assertEqual(out_dims, 2)\n    self.assertEqual(out_func, \'s\')\n    self.assertEqual(num_classes, 99)\n    out_dims, out_func, num_classes = vgsl_model._ParseOutputSpec(\n        output_spec=\'O0l12\')\n    self.assertEqual(out_dims, 0)\n    self.assertEqual(out_func, \'l\')\n    self.assertEqual(num_classes, 12)\n\n  def testPadLabels2d(self):\n    """"""Must pad timesteps in labels to match logits.\n    """"""\n    with self.test_session() as sess:\n      # Make placeholders for logits and labels.\n      ph_logits = tf.placeholder(tf.float32, shape=(None, None, 42))\n      ph_labels = tf.placeholder(tf.int64, shape=(None, None))\n      padded_labels = vgsl_model._PadLabels2d(tf.shape(ph_logits)[1], ph_labels)\n      # Make actual inputs.\n      real_logits = _rand(4, 97, 42)\n      real_labels = _rand(4, 85)\n      np_array = sess.run([padded_labels],\n                          feed_dict={ph_logits: real_logits,\n                                     ph_labels: real_labels})[0]\n      self.assertEqual(tuple(np_array.shape), (4, 97))\n      real_labels = _rand(4, 97)\n      np_array = sess.run([padded_labels],\n                          feed_dict={ph_logits: real_logits,\n                                     ph_labels: real_labels})[0]\n      self.assertEqual(tuple(np_array.shape), (4, 97))\n      real_labels = _rand(4, 100)\n      np_array = sess.run([padded_labels],\n                          feed_dict={ph_logits: real_logits,\n                                     ph_labels: real_labels})[0]\n      self.assertEqual(tuple(np_array.shape), (4, 97))\n\n  def testPadLabels3d(self):\n    """"""Must pad height and width in labels to match logits.\n\n    The tricky thing with 3-d is that the rows and columns need to remain\n    intact, so we\'ll test it with small known data.\n    """"""\n    with self.test_session() as sess:\n      # Make placeholders for logits and labels.\n      ph_logits = tf.placeholder(tf.float32, shape=(None, None, None, 42))\n      ph_labels = tf.placeholder(tf.int64, shape=(None, None, None))\n      padded_labels = vgsl_model._PadLabels3d(ph_logits, ph_labels)\n      # Make actual inputs.\n      real_logits = _rand(1, 3, 4, 42)\n      # Test all 9 combinations of height x width in [small, ok, big]\n      real_labels = np.arange(6).reshape((1, 2, 3))  # Height small, width small\n      np_array = sess.run([padded_labels],\n                          feed_dict={ph_logits: real_logits,\n                                     ph_labels: real_labels})[0]\n      self.assertEqual(tuple(np_array.shape), (1, 3, 4))\n      self.assertAllEqual(np_array[0, :, :],\n                          [[0, 1, 2, 0], [3, 4, 5, 0], [0, 0, 0, 0]])\n      real_labels = np.arange(8).reshape((1, 2, 4))  # Height small, width ok\n      np_array = sess.run([padded_labels],\n                          feed_dict={ph_logits: real_logits,\n                                     ph_labels: real_labels})[0]\n      self.assertEqual(tuple(np_array.shape), (1, 3, 4))\n      self.assertAllEqual(np_array[0, :, :],\n                          [[0, 1, 2, 3], [4, 5, 6, 7], [0, 0, 0, 0]])\n      real_labels = np.arange(10).reshape((1, 2, 5))  # Height small, width big\n      np_array = sess.run([padded_labels],\n                          feed_dict={ph_logits: real_logits,\n                                     ph_labels: real_labels})[0]\n      self.assertEqual(tuple(np_array.shape), (1, 3, 4))\n      self.assertAllEqual(np_array[0, :, :],\n                          [[0, 1, 2, 3], [5, 6, 7, 8], [0, 0, 0, 0]])\n      real_labels = np.arange(9).reshape((1, 3, 3))  # Height ok, width small\n      np_array = sess.run([padded_labels],\n                          feed_dict={ph_logits: real_logits,\n                                     ph_labels: real_labels})[0]\n      self.assertEqual(tuple(np_array.shape), (1, 3, 4))\n      self.assertAllEqual(np_array[0, :, :],\n                          [[0, 1, 2, 0], [3, 4, 5, 0], [6, 7, 8, 0]])\n      real_labels = np.arange(12).reshape((1, 3, 4))  # Height ok, width ok\n      np_array = sess.run([padded_labels],\n                          feed_dict={ph_logits: real_logits,\n                                     ph_labels: real_labels})[0]\n      self.assertEqual(tuple(np_array.shape), (1, 3, 4))\n      self.assertAllEqual(np_array[0, :, :],\n                          [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]])\n      real_labels = np.arange(15).reshape((1, 3, 5))  # Height ok, width big\n      np_array = sess.run([padded_labels],\n                          feed_dict={ph_logits: real_logits,\n                                     ph_labels: real_labels})[0]\n      self.assertEqual(tuple(np_array.shape), (1, 3, 4))\n      self.assertAllEqual(np_array[0, :, :],\n                          [[0, 1, 2, 3], [5, 6, 7, 8], [10, 11, 12, 13]])\n      real_labels = np.arange(12).reshape((1, 4, 3))  # Height big, width small\n      np_array = sess.run([padded_labels],\n                          feed_dict={ph_logits: real_logits,\n                                     ph_labels: real_labels})[0]\n      self.assertEqual(tuple(np_array.shape), (1, 3, 4))\n      self.assertAllEqual(np_array[0, :, :],\n                          [[0, 1, 2, 0], [3, 4, 5, 0], [6, 7, 8, 0]])\n      real_labels = np.arange(16).reshape((1, 4, 4))  # Height big, width ok\n      np_array = sess.run([padded_labels],\n                          feed_dict={ph_logits: real_logits,\n                                     ph_labels: real_labels})[0]\n      self.assertEqual(tuple(np_array.shape), (1, 3, 4))\n      self.assertAllEqual(np_array[0, :, :],\n                          [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]])\n      real_labels = np.arange(20).reshape((1, 4, 5))  # Height big, width big\n      np_array = sess.run([padded_labels],\n                          feed_dict={ph_logits: real_logits,\n                                     ph_labels: real_labels})[0]\n      self.assertEqual(tuple(np_array.shape), (1, 3, 4))\n      self.assertAllEqual(np_array[0, :, :],\n                          [[0, 1, 2, 3], [5, 6, 7, 8], [10, 11, 12, 13]])\n\n  def testEndToEndSizes0d(self):\n    """"""Tests that the output sizes match when training/running real 0d data.\n\n    Uses mnist with dual summarizing LSTMs to reduce to a single value.\n    """"""\n    filename = _testdata(\'mnist-tiny\')\n    with self.test_session() as sess:\n      model = vgsl_model.InitNetwork(\n          filename,\n          model_spec=\'4,0,0,1[Cr5,5,16 Mp3,3 Lfys16 Lfxs16]O0s12\',\n          mode=\'train\')\n      tf.initialize_all_variables().run(session=sess)\n      coord = tf.train.Coordinator()\n      tf.train.start_queue_runners(sess=sess, coord=coord)\n      _, step = model.TrainAStep(sess)\n      self.assertEqual(step, 1)\n      output, labels = model.RunAStep(sess)\n      self.assertEqual(len(output.shape), 2)\n      self.assertEqual(len(labels.shape), 1)\n      self.assertEqual(output.shape[0], labels.shape[0])\n      self.assertEqual(output.shape[1], 12)\n\n  # TODO(rays) Support logistic and test with Imagenet (as 0d, multi-object.)\n\n  def testEndToEndSizes1dCTC(self):\n    """"""Tests that the output sizes match when training with CTC.\n\n    Basic bidi LSTM on top of convolution and summarizing LSTM with CTC.\n    """"""\n    filename = _testdata(\'arial-32-tiny\')\n    with self.test_session() as sess:\n      model = vgsl_model.InitNetwork(\n          filename,\n          model_spec=\'2,0,0,1[Cr5,5,16 Mp3,3 Lfys16 Lbx100]O1c105\',\n          mode=\'train\')\n      tf.initialize_all_variables().run(session=sess)\n      coord = tf.train.Coordinator()\n      tf.train.start_queue_runners(sess=sess, coord=coord)\n      _, step = model.TrainAStep(sess)\n      self.assertEqual(step, 1)\n      output, labels = model.RunAStep(sess)\n      self.assertEqual(len(output.shape), 3)\n      self.assertEqual(len(labels.shape), 2)\n      self.assertEqual(output.shape[0], labels.shape[0])\n      # This is ctc - the only cast-iron guarantee is labels <= output.\n      self.assertLessEqual(labels.shape[1], output.shape[1])\n      self.assertEqual(output.shape[2], 105)\n\n  def testEndToEndSizes1dFixed(self):\n    """"""Tests that the output sizes match when training/running 1 data.\n\n    Convolution, summarizing LSTM with fwd rev fwd to allow no CTC.\n    """"""\n    filename = _testdata(\'numbers-16-tiny\')\n    with self.test_session() as sess:\n      model = vgsl_model.InitNetwork(\n          filename,\n          model_spec=\'8,0,0,1[Cr5,5,16 Mp3,3 Lfys16 Lfx64 Lrx64 Lfx64]O1s12\',\n          mode=\'train\')\n      tf.initialize_all_variables().run(session=sess)\n      coord = tf.train.Coordinator()\n      tf.train.start_queue_runners(sess=sess, coord=coord)\n      _, step = model.TrainAStep(sess)\n      self.assertEqual(step, 1)\n      output, labels = model.RunAStep(sess)\n      self.assertEqual(len(output.shape), 3)\n      self.assertEqual(len(labels.shape), 2)\n      self.assertEqual(output.shape[0], labels.shape[0])\n      # Not CTC, output lengths match.\n      self.assertEqual(output.shape[1], labels.shape[1])\n      self.assertEqual(output.shape[2], 12)\n\n  # TODO(rays) Get a 2-d dataset and support 2d (heat map) outputs.\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/street/python/vgsl_train.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Model trainer for single or multi-replica training.""""""\nfrom tensorflow import app\nfrom tensorflow.python.platform import flags\n\nimport vgsl_model\n\nflags.DEFINE_string(\'master\', \'\', \'Name of the TensorFlow master to use.\')\nflags.DEFINE_string(\'train_dir\', \'/tmp/mdir\',\n                    \'Directory where to write event logs.\')\nflags.DEFINE_string(\'model_str\',\n                    \'1,150,600,3[S2(4x150)0,2 Ct5,5,16 Mp2,2 Ct5,5,64 Mp3,3\'\n                    \'([Lrys64 Lbx128][Lbys64 Lbx128][Lfys64 Lbx128])S3(3x0)2,3\'\n                    \'Lfx128 Lrx128 S0(1x4)0,3 Do Lfx256]O1c134\',\n                    \'Network description.\')\nflags.DEFINE_integer(\'max_steps\', 10000, \'Number of steps to train for.\')\nflags.DEFINE_integer(\'task\', 0, \'Task id of the replica running the training.\')\nflags.DEFINE_integer(\'ps_tasks\', 0, \'Number of tasks in the ps job.\'\n                     \'If 0 no ps job is used.\')\nflags.DEFINE_string(\'train_data\', None, \'Training data filepattern\')\nflags.DEFINE_float(\'initial_learning_rate\', 0.00002, \'Initial learning rate\')\nflags.DEFINE_float(\'final_learning_rate\', 0.00002, \'Final learning rate\')\nflags.DEFINE_integer(\'learning_rate_halflife\', 1600000,\n                     \'Halflife of learning rate\')\nflags.DEFINE_string(\'optimizer_type\', \'Adam\',\n                    \'Optimizer from:GradientDescent, AdaGrad, Momentum, Adam\')\nflags.DEFINE_integer(\'num_preprocess_threads\', 4, \'Number of input threads\')\n\nFLAGS = flags.FLAGS\n\n\ndef main(argv):\n  del argv\n  vgsl_model.Train(FLAGS.train_dir, FLAGS.model_str, FLAGS.train_data,\n                   FLAGS.max_steps, FLAGS.master, FLAGS.task, FLAGS.ps_tasks,\n                   FLAGS.initial_learning_rate, FLAGS.final_learning_rate,\n                   FLAGS.learning_rate_halflife, FLAGS.optimizer_type,\n                   FLAGS.num_preprocess_threads)\n\n\nif __name__ == \'__main__\':\n  app.run()\n'"
model_zoo/models/street/python/vgslspecs.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""String network description language mapping to TF-Slim calls where possible.\n\nSee vglspecs.md for detailed description.\n""""""\n\nimport re\nfrom string import maketrans\n\nimport nn_ops\nimport shapes\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\n# Class that builds a set of ops to manipulate variable-sized images.\nclass VGSLSpecs(object):\n  """"""Layers that can be built from a string definition.""""""\n\n  def __init__(self, widths, heights, is_training):\n    """"""Constructs a VGSLSpecs.\n\n    Args:\n      widths:  Tensor of size batch_size of the widths of the inputs.\n      heights: Tensor of size batch_size of the heights of the inputs.\n      is_training: True if the graph should be build for training.\n    """"""\n    # The string that was used to build this model.\n    self.model_str = None\n    # True if we are training\n    self.is_training = is_training\n    # Tensor for the size of the images, of size batch_size.\n    self.widths = widths\n    self.heights = heights\n    # Overall reduction factors of this model so far for each dimension.\n    # TODO(rays) consider building a graph from widths and heights instead of\n    # computing a scale factor.\n    self.reduction_factors = [1.0, 1.0, 1.0, 1.0]\n    # List of Op parsers.\n    # TODO(rays) add more Op types as needed.\n    self.valid_ops = [self.AddSeries, self.AddParallel, self.AddConvLayer,\n                      self.AddMaxPool, self.AddDropout, self.AddReShape,\n                      self.AddFCLayer, self.AddLSTMLayer]\n    # Translation table to convert unacceptable characters that may occur\n    # in op strings that cannot be used as names.\n    self.transtab = maketrans(\'(,)\', \'___\')\n\n  def Build(self, prev_layer, model_str):\n    """"""Builds a network with input prev_layer from a VGSLSpecs description.\n\n    Args:\n      prev_layer: The input tensor.\n      model_str:  Model definition similar to Tesseract as follows:\n        ============ FUNCTIONAL OPS ============\n        C(s|t|r|l|m)[{name}]<y>,<x>,<d> Convolves using a y,x window, with no\n          shrinkage, SAME infill, d outputs, with s|t|r|l|m non-linear layer.\n          (s|t|r|l|m) specifies the type of non-linearity:\n          s = sigmoid\n          t = tanh\n          r = relu\n          l = linear (i.e., None)\n          m = softmax\n        F(s|t|r|l|m)[{name}]<d> Fully-connected with s|t|r|l|m non-linearity and\n          d outputs. Reduces height, width to 1. Input height and width must be\n          constant.\n        L(f|r|b)(x|y)[s][{name}]<n> LSTM cell with n outputs.\n          f runs the LSTM forward only.\n          r runs the LSTM reversed only.\n          b runs the LSTM bidirectionally.\n          x runs the LSTM in the x-dimension (on data with or without the\n             y-dimension).\n          y runs the LSTM in the y-dimension (data must have a y dimension).\n          s (optional) summarizes the output in the requested dimension,\n             outputting only the final step, collapsing the dimension to a\n             single element.\n          Examples:\n          Lfx128 runs a forward-only LSTM in the x-dimension with 128\n                 outputs, treating any y dimension independently.\n          Lfys64 runs a forward-only LSTM in the y-dimension with 64 outputs\n                 and collapses the y-dimension to 1 element.\n          NOTE that Lbxsn is implemented as (LfxsnLrxsn) since the summaries\n          need to be taken from opposite ends of the output\n        Do[{name}] Insert a dropout layer.\n        ============ PLUMBING OPS ============\n        [...] Execute ... networks in series (layers).\n        (...) Execute ... networks in parallel, with their output concatenated\n          in depth.\n        S[{name}]<d>(<a>x<b>)<e>,<f> Splits one dimension, moves one part to\n          another dimension.\n          Splits input dimension d into a x b, sending the high part (a) to the\n          high side of dimension e, and the low part (b) to the high side of\n          dimension f. Exception: if d=e=f, then then dimension d is internally\n          transposed to bxa.\n          Either a or b can be zero, meaning whatever is left after taking out\n          the other, allowing dimensions to be of variable size.\n          Eg. S3(3x50)2,3 will split the 150-element depth into 3x50, with the 3\n          going to the most significant part of the width, and the 50 part\n          staying in depth.\n          This will rearrange a 3x50 output parallel operation to spread the 3\n          output sets over width.\n        Mp[{name}]<y>,<x> Maxpool the input, reducing the (y,x) rectangle to a\n          single vector value.\n\n    Returns:\n      Output tensor\n    """"""\n    self.model_str = model_str\n    final_layer, _ = self.BuildFromString(prev_layer, 0)\n    return final_layer\n\n  def GetLengths(self, dim=2, factor=1):\n    """"""Returns the lengths of the batch of elements in the given dimension.\n\n    WARNING: The returned sizes may not exactly match TF\'s calculation.\n    Args:\n      dim: dimension to get the sizes of, in [1,2]. batch, depth not allowed.\n      factor: A scalar value to multiply by.\n\n    Returns:\n      The original heights/widths scaled by the current scaling of the model and\n      the given factor.\n\n    Raises:\n      ValueError: If the args are invalid.\n    """"""\n    if dim == 1:\n      lengths = self.heights\n    elif dim == 2:\n      lengths = self.widths\n    else:\n      raise ValueError(\'Invalid dimension given to GetLengths\')\n    lengths = tf.cast(lengths, tf.float32)\n    if self.reduction_factors[dim] is not None:\n      lengths = tf.div(lengths, self.reduction_factors[dim])\n    else:\n      lengths = tf.ones_like(lengths)\n    if factor != 1:\n      lengths = tf.mul(lengths, tf.cast(factor, tf.float32))\n    return tf.cast(lengths, tf.int32)\n\n  def BuildFromString(self, prev_layer, index):\n    """"""Adds the layers defined by model_str[index:] to the model.\n\n    Args:\n      prev_layer: Input tensor.\n      index:      Position in model_str to start parsing\n\n    Returns:\n      Output tensor, next model_str index.\n\n    Raises:\n      ValueError: If the model string is unrecognized.\n    """"""\n    index = self._SkipWhitespace(index)\n    for op in self.valid_ops:\n      output_layer, next_index = op(prev_layer, index)\n      if output_layer is not None:\n        return output_layer, next_index\n    if output_layer is not None:\n      return output_layer, next_index\n    raise ValueError(\'Unrecognized model string:\' + self.model_str[index:])\n\n  def AddSeries(self, prev_layer, index):\n    """"""Builds a sequence of layers for a VGSLSpecs model.\n\n    Args:\n      prev_layer: Input tensor.\n      index:      Position in model_str to start parsing\n\n    Returns:\n      Output tensor of the series, end index in model_str.\n\n    Raises:\n      ValueError: If [] are unbalanced.\n    """"""\n    if self.model_str[index] != \'[\':\n      return None, None\n    index += 1\n    while index < len(self.model_str) and self.model_str[index] != \']\':\n      prev_layer, index = self.BuildFromString(prev_layer, index)\n    if index == len(self.model_str):\n      raise ValueError(\'Missing ] at end of series!\' + self.model_str)\n    return prev_layer, index + 1\n\n  def AddParallel(self, prev_layer, index):\n    """"""tf.concats outputs of layers that run on the same inputs.\n\n    Args:\n      prev_layer: Input tensor.\n      index:      Position in model_str to start parsing\n\n    Returns:\n      Output tensor of the parallel,  end index in model_str.\n\n    Raises:\n      ValueError: If () are unbalanced or the elements don\'t match.\n    """"""\n    if self.model_str[index] != \'(\':\n      return None, None\n    index += 1\n    layers = []\n    num_dims = 0\n    # Each parallel must output the same, including any reduction factor, in\n    # all dimensions except depth.\n    # We have to save the starting factors, so they don\'t get reduced by all\n    # the elements of the parallel, only once.\n    original_factors = self.reduction_factors\n    final_factors = None\n    while index < len(self.model_str) and self.model_str[index] != \')\':\n      self.reduction_factors = original_factors\n      layer, index = self.BuildFromString(prev_layer, index)\n      if num_dims == 0:\n        num_dims = len(layer.get_shape())\n      elif num_dims != len(layer.get_shape()):\n        raise ValueError(\'All elements of parallel must return same num dims\')\n      layers.append(layer)\n      if final_factors:\n        if final_factors != self.reduction_factors:\n          raise ValueError(\'All elements of parallel must scale the same\')\n      else:\n        final_factors = self.reduction_factors\n    if index == len(self.model_str):\n      raise ValueError(\'Missing ) at end of parallel!\' + self.model_str)\n    return tf.concat(num_dims - 1, layers), index + 1\n\n  def AddConvLayer(self, prev_layer, index):\n    """"""Add a single standard convolutional layer.\n\n    Args:\n      prev_layer: Input tensor.\n      index:      Position in model_str to start parsing\n\n    Returns:\n      Output tensor, end index in model_str.\n    """"""\n    pattern = re.compile(R\'(C)(s|t|r|l|m)({\\w+})?(\\d+),(\\d+),(\\d+)\')\n    m = pattern.match(self.model_str, index)\n    if m is None:\n      return None, None\n    name = self._GetLayerName(m.group(0), index, m.group(3))\n    width = int(m.group(4))\n    height = int(m.group(5))\n    depth = int(m.group(6))\n    fn = self._NonLinearity(m.group(2))\n    return slim.conv2d(\n        prev_layer, depth, [height, width], activation_fn=fn,\n        scope=name), m.end()\n\n  def AddMaxPool(self, prev_layer, index):\n    """"""Add a maxpool layer.\n\n    Args:\n      prev_layer: Input tensor.\n      index:      Position in model_str to start parsing\n\n    Returns:\n      Output tensor, end index in model_str.\n    """"""\n    pattern = re.compile(R\'(Mp)({\\w+})?(\\d+),(\\d+)(?:,(\\d+),(\\d+))?\')\n    m = pattern.match(self.model_str, index)\n    if m is None:\n      return None, None\n    name = self._GetLayerName(m.group(0), index, m.group(2))\n    height = int(m.group(3))\n    width = int(m.group(4))\n    y_stride = height if m.group(5) is None else m.group(5)\n    x_stride = width if m.group(6) is None else m.group(6)\n    self.reduction_factors[1] *= y_stride\n    self.reduction_factors[2] *= x_stride\n    return slim.max_pool2d(\n        prev_layer, [height, width], [y_stride, x_stride],\n        padding=\'SAME\',\n        scope=name), m.end()\n\n  def AddDropout(self, prev_layer, index):\n    """"""Adds a dropout layer.\n\n    Args:\n      prev_layer: Input tensor.\n      index:      Position in model_str to start parsing\n\n    Returns:\n      Output tensor, end index in model_str.\n    """"""\n    pattern = re.compile(R\'(Do)({\\w+})?\')\n    m = pattern.match(self.model_str, index)\n    if m is None:\n      return None, None\n    name = self._GetLayerName(m.group(0), index, m.group(2))\n    layer = slim.dropout(\n        prev_layer, 0.5, is_training=self.is_training, scope=name)\n    return layer, m.end()\n\n  def AddReShape(self, prev_layer, index):\n    """"""Reshapes the input tensor by moving each (x_scale,y_scale) rectangle to.\n\n       the depth dimension. NOTE that the TF convention is that inputs are\n       [batch, y, x, depth].\n\n    Args:\n      prev_layer: Input tensor.\n      index:      Position in model_str to start parsing\n\n    Returns:\n      Output tensor, end index in model_str.\n    """"""\n    pattern = re.compile(R\'(S)(?:{(\\w)})?(\\d+)\\((\\d+)x(\\d+)\\)(\\d+),(\\d+)\')\n    m = pattern.match(self.model_str, index)\n    if m is None:\n      return None, None\n    name = self._GetLayerName(m.group(0), index, m.group(2))\n    src_dim = int(m.group(3))\n    part_a = int(m.group(4))\n    part_b = int(m.group(5))\n    dest_dim_a = int(m.group(6))\n    dest_dim_b = int(m.group(7))\n    if part_a == 0:\n      part_a = -1\n    if part_b == 0:\n      part_b = -1\n    prev_shape = tf.shape(prev_layer)\n    layer = shapes.transposing_reshape(\n        prev_layer, src_dim, part_a, part_b, dest_dim_a, dest_dim_b, name=name)\n    # Compute scale factors.\n    result_shape = tf.shape(layer)\n    for i in xrange(len(self.reduction_factors)):\n      if self.reduction_factors[i] is not None:\n        factor1 = tf.cast(self.reduction_factors[i], tf.float32)\n        factor2 = tf.cast(prev_shape[i], tf.float32)\n        divisor = tf.cast(result_shape[i], tf.float32)\n        self.reduction_factors[i] = tf.div(tf.mul(factor1, factor2), divisor)\n    return layer, m.end()\n\n  def AddFCLayer(self, prev_layer, index):\n    """"""Parse expression and add Fully Connected Layer.\n\n    Args:\n      prev_layer: Input tensor.\n      index:      Position in model_str to start parsing\n\n    Returns:\n      Output tensor, end index in model_str.\n    """"""\n    pattern = re.compile(R\'(F)(s|t|r|l|m)({\\w+})?(\\d+)\')\n    m = pattern.match(self.model_str, index)\n    if m is None:\n      return None, None\n    fn = self._NonLinearity(m.group(2))\n    name = self._GetLayerName(m.group(0), index, m.group(3))\n    depth = int(m.group(4))\n    input_depth = shapes.tensor_dim(prev_layer, 1) * shapes.tensor_dim(\n        prev_layer, 2) * shapes.tensor_dim(prev_layer, 3)\n    # The slim fully connected is actually a 1x1 conv, so we have to crush the\n    # dimensions on input.\n    # Everything except batch goes to depth, and therefore has to be known.\n    shaped = tf.reshape(\n        prev_layer, [-1, input_depth], name=name + \'_reshape_in\')\n    output = slim.fully_connected(shaped, depth, activation_fn=fn, scope=name)\n    # Width and height are collapsed to 1.\n    self.reduction_factors[1] = None\n    self.reduction_factors[2] = None\n    return tf.reshape(\n        output, [shapes.tensor_dim(prev_layer, 0), 1, 1, depth],\n        name=name + \'_reshape_out\'), m.end()\n\n  def AddLSTMLayer(self, prev_layer, index):\n    """"""Parse expression and add LSTM Layer.\n\n    Args:\n      prev_layer: Input tensor.\n      index:      Position in model_str to start parsing\n\n    Returns:\n      Output tensor, end index in model_str.\n    """"""\n    pattern = re.compile(R\'(L)(f|r|b)(x|y)(s)?({\\w+})?(\\d+)\')\n    m = pattern.match(self.model_str, index)\n    if m is None:\n      return None, None\n    direction = m.group(2)\n    dim = m.group(3)\n    summarize = m.group(4) == \'s\'\n    name = self._GetLayerName(m.group(0), index, m.group(5))\n    depth = int(m.group(6))\n    if direction == \'b\' and summarize:\n      fwd = self._LSTMLayer(prev_layer, \'forward\', dim, True, depth,\n                            name + \'_forward\')\n      back = self._LSTMLayer(prev_layer, \'backward\', dim, True, depth,\n                             name + \'_reverse\')\n      return tf.concat(3, [fwd, back], name=name + \'_concat\'), m.end()\n    if direction == \'f\':\n      direction = \'forward\'\n    elif direction == \'r\':\n      direction = \'backward\'\n    else:\n      direction = \'bidirectional\'\n    outputs = self._LSTMLayer(prev_layer, direction, dim, summarize, depth,\n                              name)\n    if summarize:\n      # The x or y dimension is getting collapsed.\n      if dim == \'x\':\n        self.reduction_factors[2] = None\n      else:\n        self.reduction_factors[1] = None\n    return outputs, m.end()\n\n  def _LSTMLayer(self, prev_layer, direction, dim, summarize, depth, name):\n    """"""Adds an LSTM layer with the given pre-parsed attributes.\n\n    Always maps 4-D to 4-D regardless of summarize.\n    Args:\n      prev_layer: Input tensor.\n      direction:  \'forward\' \'backward\' or \'bidirectional\'\n      dim:        \'x\' or \'y\', dimension to consider as time.\n      summarize:  True if we are to return only the last timestep.\n      depth:      Output depth.\n      name:       Some string naming the op.\n\n    Returns:\n      Output tensor.\n    """"""\n    # If the target dimension is y, we need to transpose.\n    if dim == \'x\':\n      lengths = self.GetLengths(2, 1)\n      inputs = prev_layer\n    else:\n      lengths = self.GetLengths(1, 1)\n      inputs = tf.transpose(prev_layer, [0, 2, 1, 3], name=name + \'_ytrans_in\')\n    input_batch = shapes.tensor_dim(inputs, 0)\n    num_slices = shapes.tensor_dim(inputs, 1)\n    num_steps = shapes.tensor_dim(inputs, 2)\n    input_depth = shapes.tensor_dim(inputs, 3)\n    # Reshape away the other dimension.\n    inputs = tf.reshape(\n        inputs, [-1, num_steps, input_depth], name=name + \'_reshape_in\')\n    # We need to replicate the lengths by the size of the other dimension, and\n    # any changes that have been made to the batch dimension.\n    tile_factor = tf.to_float(input_batch *\n                              num_slices) / tf.to_float(tf.shape(lengths)[0])\n    lengths = tf.tile(lengths, [tf.cast(tile_factor, tf.int32)])\n    lengths = tf.cast(lengths, tf.int64)\n    outputs = nn_ops.rnn_helper(\n        inputs,\n        lengths,\n        cell_type=\'lstm\',\n        num_nodes=depth,\n        direction=direction,\n        name=name,\n        stddev=0.1)\n    # Output depth is doubled if bi-directional.\n    if direction == \'bidirectional\':\n      output_depth = depth * 2\n    else:\n      output_depth = depth\n    # Restore the other dimension.\n    if summarize:\n      outputs = tf.slice(\n          outputs, [0, num_steps - 1, 0], [-1, 1, -1], name=name + \'_sum_slice\')\n      outputs = tf.reshape(\n          outputs, [input_batch, num_slices, 1, output_depth],\n          name=name + \'_reshape_out\')\n    else:\n      outputs = tf.reshape(\n          outputs, [input_batch, num_slices, num_steps, output_depth],\n          name=name + \'_reshape_out\')\n    if dim == \'y\':\n      outputs = tf.transpose(outputs, [0, 2, 1, 3], name=name + \'_ytrans_out\')\n    return outputs\n\n  def _NonLinearity(self, code):\n    """"""Returns the non-linearity function pointer for the given string code.\n\n    For forwards compatibility, allows the full names for stand-alone\n    non-linearities, as well as the single-letter names used in ops like C,F.\n    Args:\n      code: String code representing a non-linearity function.\n    Returns:\n      non-linearity function represented by the code.\n    """"""\n    if code in [\'s\', \'Sig\']:\n      return tf.sigmoid\n    elif code in [\'t\', \'Tanh\']:\n      return tf.tanh\n    elif code in [\'r\', \'Relu\']:\n      return tf.nn.relu\n    elif code in [\'m\', \'Smax\']:\n      return tf.nn.softmax\n    return None\n\n  def _GetLayerName(self, op_str, index, name_str):\n    """"""Generates a name for the op, using a user-supplied name if possible.\n\n    Args:\n      op_str:     String representing the parsed op.\n      index:      Position in model_str of the start of the op.\n      name_str:   User-supplied {name} with {} that need removing or None.\n\n    Returns:\n      Selected name.\n    """"""\n    if name_str:\n      return name_str[1:-1]\n    else:\n      return op_str.translate(self.transtab) + \'_\' + str(index)\n\n  def _SkipWhitespace(self, index):\n    """"""Skips any leading whitespace in the model description.\n\n    Args:\n      index:      Position in model_str to start parsing\n\n    Returns:\n      end index in model_str of whitespace.\n    """"""\n    pattern = re.compile(R\'([ \\t\\n]+)\')\n    m = pattern.match(self.model_str, index)\n    if m is None:\n      return index\n    return m.end()\n'"
model_zoo/models/street/python/vgslspecs_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for vgslspecs.""""""\n\nimport numpy as np\nimport tensorflow as tf\nimport vgslspecs\n\n\ndef _rand(*size):\n  return np.random.uniform(size=size).astype(\'f\')\n\n\nclass VgslspecsTest(tf.test.TestCase):\n\n  def __init__(self, other):\n    super(VgslspecsTest, self).__init__(other)\n    self.max_width = 36\n    self.max_height = 24\n    self.batch_size = 4\n\n  def SetupInputs(self):\n    # Make placeholders for standard inputs.\n    # Everything is variable in the input, except the depth.\n    self.ph_image = tf.placeholder(\n        tf.float32, shape=(None, None, None, 3), name=\'inputs\')\n    self.ph_widths = tf.placeholder(tf.int64, shape=(None,), name=\'w\')\n    self.ph_heights = tf.placeholder(tf.int64, shape=(None,), name=\'h\')\n    # Make actual inputs.\n    self.in_image = _rand(self.batch_size, self.max_height, self.max_width, 3)\n    self.in_widths = [24, 12, self.max_width, 30]\n    self.in_heights = [self.max_height, 18, 12, 6]\n\n  def ExpectScaledSize(self, spec, target_shape, factor=1):\n    """"""Tests that the output of the graph of the given spec has target_shape.""""""\n    with tf.Graph().as_default():\n      with self.test_session() as sess:\n        self.SetupInputs()\n        # Only the placeholders are given at construction time.\n        vgsl = vgslspecs.VGSLSpecs(self.ph_widths, self.ph_heights, True)\n        outputs = vgsl.Build(self.ph_image, spec)\n        # Compute the expected output widths from the given scale factor.\n        target_widths = tf.div(self.in_widths, factor).eval()\n        target_heights = tf.div(self.in_heights, factor).eval()\n        # Run with the \'real\' data.\n        tf.initialize_all_variables().run()\n        res_image, res_widths, res_heights = sess.run(\n            [outputs, vgsl.GetLengths(2), vgsl.GetLengths(1)],\n            feed_dict={self.ph_image: self.in_image,\n                       self.ph_widths: self.in_widths,\n                       self.ph_heights: self.in_heights})\n        self.assertEqual(tuple(res_image.shape), target_shape)\n        if target_shape[1] > 1:\n          self.assertEqual(tuple(res_heights), tuple(target_heights))\n        if target_shape[2] > 1:\n          self.assertEqual(tuple(res_widths), tuple(target_widths))\n\n  def testSameSizeConv(self):\n    """"""Test all types of Conv. There is no scaling.""""""\n    self.ExpectScaledSize(\n        \'[Cs{MyConv}5,5,16 Ct3,3,12 Cr4,4,24 Cl5,5,64]\',\n        (self.batch_size, self.max_height, self.max_width, 64))\n\n  def testSameSizeLSTM(self):\n    """"""Test all non-reducing LSTMs. Output depth is doubled with BiDi.""""""\n    self.ExpectScaledSize(\'[Lfx16 Lrx8 Do Lbx24 Lfy12 Do{MyDo} Lry7 Lby32]\',\n                          (self.batch_size, self.max_height, self.max_width,\n                           64))\n\n  def testSameSizeParallel(self):\n    """"""Parallel affects depth, but not scale.""""""\n    self.ExpectScaledSize(\'[Cs5,5,16 (Lfx{MyLSTM}32 Lrx32 Lbx16)]\',\n                          (self.batch_size, self.max_height, self.max_width,\n                           96))\n\n  def testScalingOps(self):\n    """"""Test a heterogeneous series with scaling.""""""\n    self.ExpectScaledSize(\'[Cs5,5,16 Mp{MyPool}2,2 Ct3,3,32 Mp3,3 Lfx32 Lry64]\',\n                          (self.batch_size, self.max_height / 6,\n                           self.max_width / 6, 64), 6)\n\n  def testXReduction(self):\n    """"""Test a heterogeneous series with reduction of x-dimension.""""""\n    self.ExpectScaledSize(\'[Cr5,5,16 Mp2,2 Ct3,3,32 Mp3,3 Lfxs32 Lry64]\',\n                          (self.batch_size, self.max_height / 6, 1, 64), 6)\n\n  def testYReduction(self):\n    """"""Test a heterogeneous series with reduction of y-dimension.""""""\n    self.ExpectScaledSize(\'[Cl5,5,16 Mp2,2 Ct3,3,32 Mp3,3 Lfys32 Lfx64]\',\n                          (self.batch_size, 1, self.max_width / 6, 64), 6)\n\n  def testXYReduction(self):\n    """"""Test a heterogeneous series with reduction to 0-d.""""""\n    self.ExpectScaledSize(\n        \'[Cr5,5,16 Lfys32 Lfxs64 Fr{MyFC}16 Ft20 Fl12 Fs32 Fm40]\',\n        (self.batch_size, 1, 1, 40))\n\n  def testReshapeTile(self):\n    """"""Tests that a tiled input can be reshaped to the batch dimension.""""""\n    self.ExpectScaledSize(\'[S2(3x0)0,2 Cr5,5,16 Lfys16]\',\n                          (self.batch_size * 3, 1, self.max_width / 3, 16), 3)\n\n  def testReshapeDepth(self):\n    """"""Tests that depth can be reshaped to the x dimension.""""""\n    self.ExpectScaledSize(\'[Cl5,5,16 Mp3,3 (Lrys32 Lbys16 Lfys32) S3(3x0)2,3]\',\n                          (self.batch_size, 1, self.max_width, 32))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/syntaxnet/syntaxnet/beam_reader_ops_test.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for beam_reader_ops.""""""\n\n\nimport os.path\nimport time\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.platform import tf_logging as logging\n\nfrom syntaxnet import structured_graph_builder\nfrom syntaxnet.ops import gen_parser_ops\n\nFLAGS = tf.app.flags.FLAGS\nif not hasattr(FLAGS, \'test_srcdir\'):\n  FLAGS.test_srcdir = \'\'\nif not hasattr(FLAGS, \'test_tmpdir\'):\n  FLAGS.test_tmpdir = tf.test.get_temp_dir()\n\n\nclass ParsingReaderOpsTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    # Creates a task context with the correct testing paths.\n    initial_task_context = os.path.join(\n        FLAGS.test_srcdir,\n        \'syntaxnet/\'\n        \'testdata/context.pbtxt\')\n    self._task_context = os.path.join(FLAGS.test_tmpdir, \'context.pbtxt\')\n    with open(initial_task_context, \'r\') as fin:\n      with open(self._task_context, \'w\') as fout:\n        fout.write(fin.read().replace(\'SRCDIR\', FLAGS.test_srcdir)\n                   .replace(\'OUTPATH\', FLAGS.test_tmpdir))\n\n    # Creates necessary term maps.\n    with self.test_session() as sess:\n      gen_parser_ops.lexicon_builder(task_context=self._task_context,\n                                     corpus_name=\'training-corpus\').run()\n      self._num_features, self._num_feature_ids, _, self._num_actions = (\n          sess.run(gen_parser_ops.feature_size(task_context=self._task_context,\n                                               arg_prefix=\'brain_parser\')))\n\n  def MakeGraph(self,\n                max_steps=10,\n                beam_size=2,\n                batch_size=1,\n                **kwargs):\n    """"""Constructs a structured learning graph.""""""\n    assert max_steps > 0, \'Empty network not supported.\'\n\n    logging.info(\'MakeGraph + %s\', kwargs)\n\n    with self.test_session(graph=tf.Graph()) as sess:\n      feature_sizes, domain_sizes, embedding_dims, num_actions = sess.run(\n          gen_parser_ops.feature_size(task_context=self._task_context))\n    embedding_dims = [8, 8, 8]\n    hidden_layer_sizes = []\n    learning_rate = 0.01\n    builder = structured_graph_builder.StructuredGraphBuilder(\n        num_actions,\n        feature_sizes,\n        domain_sizes,\n        embedding_dims,\n        hidden_layer_sizes,\n        seed=1,\n        max_steps=max_steps,\n        beam_size=beam_size,\n        gate_gradients=True,\n        use_locking=True,\n        use_averaging=False,\n        check_parameters=False,\n        **kwargs)\n    builder.AddTraining(self._task_context,\n                        batch_size,\n                        learning_rate=learning_rate,\n                        decay_steps=1000,\n                        momentum=0.9,\n                        corpus_name=\'training-corpus\')\n    builder.AddEvaluation(self._task_context,\n                          batch_size,\n                          evaluation_max_steps=25,\n                          corpus_name=None)\n    builder.training[\'inits\'] = tf.group(*builder.inits.values(), name=\'inits\')\n    return builder\n\n  def Train(self, **kwargs):\n    with self.test_session(graph=tf.Graph()) as sess:\n      max_steps = 3\n      batch_size = 3\n      beam_size = 3\n      builder = (\n          self.MakeGraph(\n              max_steps=max_steps, beam_size=beam_size,\n              batch_size=batch_size, **kwargs))\n      logging.info(\'params: %s\', builder.params.keys())\n      logging.info(\'variables: %s\', builder.variables.keys())\n\n      t = builder.training\n      sess.run(t[\'inits\'])\n      costs = []\n      gold_slots = []\n      alive_steps_vector = []\n      every_n = 5\n      walltime = time.time()\n      for step in range(10):\n        if step > 0 and step % every_n == 0:\n          new_walltime = time.time()\n          logging.info(\n              \'Step: %d <cost>: %f <gold_slot>: %f <alive_steps>: %f <iter \'\n              \'time>: %f ms\',\n              step, sum(costs[-every_n:]) / float(every_n),\n              sum(gold_slots[-every_n:]) / float(every_n),\n              sum(alive_steps_vector[-every_n:]) / float(every_n),\n              1000 * (new_walltime - walltime) / float(every_n))\n          walltime = new_walltime\n\n        cost, gold_slot, alive_steps, _ = sess.run(\n            [t[\'cost\'], t[\'gold_slot\'], t[\'alive_steps\'], t[\'train_op\']])\n        costs.append(cost)\n        gold_slots.append(gold_slot.mean())\n        alive_steps_vector.append(alive_steps.mean())\n\n      if builder._only_train:\n        trainable_param_names = [\n            k for k in builder.params if k in builder._only_train]\n      else:\n        trainable_param_names = builder.params.keys()\n      if builder._use_averaging:\n        for v in trainable_param_names:\n          avg = builder.variables[\'%s_avg_var\' % v].eval()\n          tf.assign(builder.params[v], avg).eval()\n\n      # Reset for pseudo eval.\n      costs = []\n      gold_slots = []\n      alive_stepss = []\n      for step in range(10):\n        cost, gold_slot, alive_steps = sess.run(\n            [t[\'cost\'], t[\'gold_slot\'], t[\'alive_steps\']])\n        costs.append(cost)\n        gold_slots.append(gold_slot.mean())\n        alive_stepss.append(alive_steps.mean())\n\n      logging.info(\n          \'Pseudo eval: <cost>: %f <gold_slot>: %f <alive_steps>: %f\',\n          sum(costs[-every_n:]) / float(every_n),\n          sum(gold_slots[-every_n:]) / float(every_n),\n          sum(alive_stepss[-every_n:]) / float(every_n))\n\n  def PathScores(self, iterations, beam_size, max_steps, batch_size):\n    with self.test_session(graph=tf.Graph()) as sess:\n      t = self.MakeGraph(beam_size=beam_size, max_steps=max_steps,\n                         batch_size=batch_size).training\n      sess.run(t[\'inits\'])\n      all_path_scores = []\n      beam_path_scores = []\n      for i in range(iterations):\n        logging.info(\'run %d\', i)\n        tensors = (\n            sess.run(\n                [t[\'alive_steps\'], t[\'concat_scores\'],\n                 t[\'all_path_scores\'], t[\'beam_path_scores\'],\n                 t[\'indices\'], t[\'path_ids\']]))\n\n        logging.info(\'alive for %s, all_path_scores and beam_path_scores, \'\n                     \'indices and path_ids:\'\n                     \'\\n%s\\n%s\\n%s\\n%s\',\n                     tensors[0], tensors[2], tensors[3], tensors[4], tensors[5])\n        logging.info(\'diff:\\n%s\', tensors[2] - tensors[3])\n\n        all_path_scores.append(tensors[2])\n        beam_path_scores.append(tensors[3])\n      return all_path_scores, beam_path_scores\n\n  def testParseUntilNotAlive(self):\n    """"""Ensures that the \'alive\' condition works in the Cond ops.""""""\n    with self.test_session(graph=tf.Graph()) as sess:\n      t = self.MakeGraph(batch_size=3, beam_size=2, max_steps=5).training\n      sess.run(t[\'inits\'])\n      for i in range(5):\n        logging.info(\'run %d\', i)\n        tf_alive = t[\'alive\'].eval()\n        self.assertFalse(any(tf_alive))\n\n  def testParseMomentum(self):\n    """"""Ensures that Momentum training can be done using the gradients.""""""\n    self.Train()\n    self.Train(model_cost=\'perceptron_loss\')\n    self.Train(model_cost=\'perceptron_loss\',\n               only_train=\'softmax_weight,softmax_bias\', softmax_init=0)\n    self.Train(only_train=\'softmax_weight,softmax_bias\', softmax_init=0)\n\n  def testPathScoresAgree(self):\n    """"""Ensures that path scores computed in the beam are same in the net.""""""\n    all_path_scores, beam_path_scores = self.PathScores(\n        iterations=1, beam_size=130, max_steps=5, batch_size=1)\n    self.assertArrayNear(all_path_scores[0], beam_path_scores[0], 1e-6)\n\n  def testBatchPathScoresAgree(self):\n    """"""Ensures that path scores computed in the beam are same in the net.""""""\n    all_path_scores, beam_path_scores = self.PathScores(\n        iterations=1, beam_size=130, max_steps=5, batch_size=22)\n    self.assertArrayNear(all_path_scores[0], beam_path_scores[0], 1e-6)\n\n  def testBatchOneStepPathScoresAgree(self):\n    """"""Ensures that path scores computed in the beam are same in the net.""""""\n    all_path_scores, beam_path_scores = self.PathScores(\n        iterations=1, beam_size=130, max_steps=1, batch_size=22)\n    self.assertArrayNear(all_path_scores[0], beam_path_scores[0], 1e-6)\n\n\nif __name__ == \'__main__\':\n  googletest.main()\n'"
model_zoo/models/syntaxnet/syntaxnet/conll2tree.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A program to generate ASCII trees from conll files.""""""\n\nimport collections\nimport re\n\nimport asciitree\nimport tensorflow as tf\n\nimport syntaxnet.load_parser_ops\n\nfrom tensorflow.python.platform import tf_logging as logging\nfrom syntaxnet import sentence_pb2\nfrom syntaxnet.ops import gen_parser_ops\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'task_context\',\n                    \'syntaxnet/models/parsey_mcparseface/context.pbtxt\',\n                    \'Path to a task context with inputs and parameters for \'\n                    \'feature extractors.\')\nflags.DEFINE_string(\'corpus_name\', \'stdin-conll\',\n                    \'Path to a task context with inputs and parameters for \'\n                    \'feature extractors.\')\n\n\ndef to_dict(sentence):\n  """"""Builds a dictionary representing the parse tree of a sentence.\n\n     Note that the suffix ""@id"" (where \'id\' is a number) is appended to each\n     element to handle the sentence that has multiple elements with identical\n     representation. Those suffix needs to be removed after the asciitree is\n     rendered.\n\n  Args:\n    sentence: Sentence protocol buffer to represent.\n  Returns:\n    Dictionary mapping tokens to children.\n  """"""\n  token_str = list()\n  children = [[] for token in sentence.token]\n  root = -1\n  for i in range(0, len(sentence.token)):\n    token = sentence.token[i]\n    token_str.append(\'%s %s %s @%d\' %\n                     (token.word, token.tag, token.label, (i+1)))\n    if token.head == -1:\n      root = i\n    else:\n      children[token.head].append(i)\n\n  def _get_dict(i):\n    d = collections.OrderedDict()\n    for c in children[i]:\n      d[token_str[c]] = _get_dict(c)\n    return d\n\n  tree = collections.OrderedDict()\n  tree[token_str[root]] = _get_dict(root)\n  return tree\n\n\ndef main(unused_argv):\n  logging.set_verbosity(logging.INFO)\n  with tf.Session() as sess:\n    src = gen_parser_ops.document_source(batch_size=32,\n                                         corpus_name=FLAGS.corpus_name,\n                                         task_context=FLAGS.task_context)\n    sentence = sentence_pb2.Sentence()\n    while True:\n      documents, finished = sess.run(src)\n      logging.info(\'Read %d documents\', len(documents))\n      for d in documents:\n        sentence.ParseFromString(d)\n        tr = asciitree.LeftAligned()\n        d = to_dict(sentence)\n        print \'Input: %s\' % sentence.text\n        print \'Parse:\'\n        tr_str = tr(d)\n        pat = re.compile(r\'\\s*@\\d+$\')\n        for tr_ln in tr_str.splitlines():\n          print pat.sub(\'\', tr_ln)\n\n      if finished:\n        break\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/syntaxnet/syntaxnet/graph_builder.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Builds parser models.""""""\n\nimport tensorflow as tf\n\nimport syntaxnet.load_parser_ops\n\nfrom tensorflow.python.ops import control_flow_ops as cf\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.platform import tf_logging as logging\n\nfrom syntaxnet.ops import gen_parser_ops\n\n\ndef BatchedSparseToDense(sparse_indices, output_size):\n  """"""Batch compatible sparse to dense conversion.\n\n  This is useful for one-hot coded target labels.\n\n  Args:\n    sparse_indices: [batch_size] tensor containing one index per batch\n    output_size: needed in order to generate the correct dense output\n\n  Returns:\n    A [batch_size, output_size] dense tensor.\n  """"""\n  eye = tf.diag(tf.fill([output_size], tf.constant(1, tf.float32)))\n  return tf.nn.embedding_lookup(eye, sparse_indices)\n\n\ndef EmbeddingLookupFeatures(params, sparse_features, allow_weights):\n  """"""Computes embeddings for each entry of sparse features sparse_features.\n\n  Args:\n    params: list of 2D tensors containing vector embeddings\n    sparse_features: 1D tensor of strings. Each entry is a string encoding of\n      dist_belief.SparseFeatures, and represents a variable length list of\n      feature ids, and optionally, corresponding weights values.\n    allow_weights: boolean to control whether the weights returned from the\n      SparseFeatures are used to multiply the embeddings.\n\n  Returns:\n    A tensor representing the combined embeddings for the sparse features.\n    For each entry s in sparse_features, the function looks up the embeddings\n    for each id and sums them into a single tensor weighing them by the\n    weight of each id. It returns a tensor with each entry of sparse_features\n    replaced by this combined embedding.\n  """"""\n  if not isinstance(params, list):\n    params = [params]\n  # Lookup embeddings.\n  sparse_features = tf.convert_to_tensor(sparse_features)\n  indices, ids, weights = gen_parser_ops.unpack_sparse_features(sparse_features)\n  embeddings = tf.nn.embedding_lookup(params, ids)\n\n  if allow_weights:\n    # Multiply by weights, reshaping to allow broadcast.\n    broadcast_weights_shape = tf.concat(0, [tf.shape(weights), [1]])\n    embeddings *= tf.reshape(weights, broadcast_weights_shape)\n\n  # Sum embeddings by index.\n  return tf.unsorted_segment_sum(embeddings, indices, tf.size(sparse_features))\n\n\nclass GreedyParser(object):\n  """"""Builds a Chen & Manning style greedy neural net parser.\n\n  Builds a graph with an optional reader op connected at one end and\n  operations needed to train the network on the other. Supports multiple\n  network instantiations sharing the same parameters and network topology.\n\n  The following named nodes are added to the training and eval networks:\n    epochs: a tensor containing the current epoch number\n    cost: a tensor containing the current training step cost\n    gold_actions: a tensor containing actions from gold decoding\n    feature_endpoints: a list of sparse feature vectors\n    logits: output of the final layer before computing softmax\n  The training network also contains:\n    train_op: an op that executes a single training step\n\n  Typical usage:\n\n  parser = graph_builder.GreedyParser(num_actions, num_features,\n                                      num_feature_ids, embedding_sizes,\n                                      hidden_layer_sizes)\n  parser.AddTraining(task_context, batch_size=5)\n  with tf.Session(\'local\') as sess:\n    # This works because the session uses the same default graph as the\n    # GraphBuilder did.\n    sess.run(parser.inits.values())\n    while True:\n      tf_epoch, _ = sess.run([parser.training[\'epoch\'],\n                              parser.training[\'train_op\']])\n      if tf_epoch[0] > 0:\n        break\n  """"""\n\n  def __init__(self,\n               num_actions,\n               num_features,\n               num_feature_ids,\n               embedding_sizes,\n               hidden_layer_sizes,\n               seed=None,\n               gate_gradients=False,\n               use_locking=False,\n               embedding_init=1.0,\n               relu_init=1e-4,\n               bias_init=0.2,\n               softmax_init=1e-4,\n               averaging_decay=0.9999,\n               use_averaging=True,\n               check_parameters=True,\n               check_every=1,\n               allow_feature_weights=False,\n               only_train=\'\',\n               arg_prefix=None,\n               **unused_kwargs):\n    """"""Initialize the graph builder with parameters defining the network.\n\n    Args:\n      num_actions: int size of the set of parser actions\n      num_features: int list of dimensions of the feature vectors\n      num_feature_ids: int list of same length as num_features corresponding to\n        the sizes of the input feature spaces\n      embedding_sizes: int list of same length as num_features of the desired\n        embedding layer sizes\n      hidden_layer_sizes: int list of desired relu layer sizes; may be empty\n      seed: optional random initializer seed to enable reproducibility\n      gate_gradients: if True, gradient updates are computed synchronously,\n        ensuring consistency and reproducibility\n      use_locking: if True, use locking to avoid read-write contention when\n        updating Variables\n      embedding_init: sets the std dev of normal initializer of embeddings to\n        embedding_init / embedding_size ** .5\n      relu_init: sets the std dev of normal initializer of relu weights\n        to relu_init\n      bias_init: sets constant initializer of relu bias to bias_init\n      softmax_init: sets the std dev of normal initializer of softmax init\n        to softmax_init\n      averaging_decay: decay for exponential moving average when computing\n        averaged parameters, set to 1 to do vanilla averaging\n      use_averaging: whether to use moving averages of parameters during evals\n      check_parameters: whether to check for NaN/Inf parameters during\n        training\n      check_every: checks numerics every check_every steps.\n      allow_feature_weights: whether feature weights are allowed.\n      only_train: the comma separated set of parameter names to train. If empty,\n        all model parameters will be trained.\n      arg_prefix: prefix for context parameters.\n    """"""\n    self._num_actions = num_actions\n    self._num_features = num_features\n    self._num_feature_ids = num_feature_ids\n    self._embedding_sizes = embedding_sizes\n    self._hidden_layer_sizes = hidden_layer_sizes\n    self._seed = seed\n    self._gate_gradients = gate_gradients\n    self._use_locking = use_locking\n    self._use_averaging = use_averaging\n    self._check_parameters = check_parameters\n    self._check_every = check_every\n    self._allow_feature_weights = allow_feature_weights\n    self._only_train = set(only_train.split(\',\')) if only_train else None\n    self._feature_size = len(embedding_sizes)\n    self._embedding_init = embedding_init\n    self._relu_init = relu_init\n    self._softmax_init = softmax_init\n    self._arg_prefix = arg_prefix\n    # Parameters of the network with respect to which training is done.\n    self.params = {}\n    # Other variables, with respect to which no training is done, but which we\n    # nonetheless need to save in order to capture the state of the graph.\n    self.variables = {}\n    # Operations to initialize any nodes that require initialization.\n    self.inits = {}\n    # Training- and eval-related nodes.\n    self.training = {}\n    self.evaluation = {}\n    self.saver = None\n    # Nodes to compute moving averages of parameters, called every train step.\n    self._averaging = {}\n    self._averaging_decay = averaging_decay\n    # Pretrained embeddings that can be used instead of constant initializers.\n    self._pretrained_embeddings = {}\n    # After the following \'with\' statement, we\'ll be able to re-enter the\n    # \'params\' scope by re-using the self._param_scope member variable. See for\n    # instance _AddParam.\n    with tf.name_scope(\'params\') as self._param_scope:\n      self._relu_bias_init = tf.constant_initializer(bias_init)\n\n  @property\n  def embedding_size(self):\n    size = 0\n    for i in range(self._feature_size):\n      size += self._num_features[i] * self._embedding_sizes[i]\n    return size\n\n  def _AddParam(self,\n                shape,\n                dtype,\n                name,\n                initializer=None,\n                return_average=False):\n    """"""Add a model parameter w.r.t. we expect to compute gradients.\n\n    _AddParam creates both regular parameters (usually for training) and\n    averaged nodes (usually for inference). It returns one or the other based\n    on the \'return_average\' arg.\n\n    Args:\n      shape: int list, tensor shape of the parameter to create\n      dtype: tf.DataType, data type of the parameter\n      name: string, name of the parameter in the TF graph\n      initializer: optional initializer for the paramter\n      return_average: if False, return parameter otherwise return moving average\n\n    Returns:\n      parameter or averaged parameter\n    """"""\n    if name not in self.params:\n      step = tf.cast(self.GetStep(), tf.float32)\n      # Put all parameters and their initializing ops in their own scope\n      # irrespective of the current scope (training or eval).\n      with tf.name_scope(self._param_scope):\n        self.params[name] = tf.get_variable(name, shape, dtype, initializer)\n        param = self.params[name]\n        if initializer is not None:\n          self.inits[name] = state_ops.init_variable(param, initializer)\n        if self._averaging_decay == 1:\n          logging.info(\'Using vanilla averaging of parameters.\')\n          ema = tf.train.ExponentialMovingAverage(decay=(step / (step + 1.0)),\n                                                  num_updates=None)\n        else:\n          ema = tf.train.ExponentialMovingAverage(decay=self._averaging_decay,\n                                                  num_updates=step)\n        self._averaging[name + \'_avg_update\'] = ema.apply([param])\n        self.variables[name + \'_avg_var\'] = ema.average(param)\n        self.inits[name + \'_avg_init\'] = state_ops.init_variable(\n            ema.average(param), tf.zeros_initializer)\n    return (self.variables[name + \'_avg_var\'] if return_average else\n            self.params[name])\n\n  def GetStep(self):\n    def OnesInitializer(shape, dtype=tf.float32, partition_info=None):\n      return tf.ones(shape, dtype)\n    return self._AddVariable([], tf.int32, \'step\', OnesInitializer)\n\n  def _AddVariable(self, shape, dtype, name, initializer=None):\n    if name in self.variables:\n      return self.variables[name]\n    self.variables[name] = tf.get_variable(name, shape, dtype, initializer)\n    if initializer is not None:\n      self.inits[name] = state_ops.init_variable(self.variables[name],\n                                                 initializer)\n    return self.variables[name]\n\n  def _ReluWeightInitializer(self):\n    with tf.name_scope(self._param_scope):\n      return tf.random_normal_initializer(stddev=self._relu_init,\n                                          seed=self._seed)\n\n  def _EmbeddingMatrixInitializer(self, index, embedding_size):\n    if index in self._pretrained_embeddings:\n      return self._pretrained_embeddings[index]\n    else:\n      return tf.random_normal_initializer(\n          stddev=self._embedding_init / embedding_size**.5,\n          seed=self._seed)\n\n  def _AddEmbedding(self,\n                    features,\n                    num_features,\n                    num_ids,\n                    embedding_size,\n                    index,\n                    return_average=False):\n    """"""Adds an embedding matrix and passes the `features` vector through it.""""""\n    embedding_matrix = self._AddParam(\n        [num_ids, embedding_size],\n        tf.float32,\n        \'embedding_matrix_%d\' % index,\n        self._EmbeddingMatrixInitializer(index, embedding_size),\n        return_average=return_average)\n    embedding = EmbeddingLookupFeatures(embedding_matrix,\n                                        tf.reshape(features,\n                                                   [-1],\n                                                   name=\'feature_%d\' % index),\n                                        self._allow_feature_weights)\n    return tf.reshape(embedding, [-1, num_features * embedding_size])\n\n  def _BuildNetwork(self, feature_endpoints, return_average=False):\n    """"""Builds a feed-forward part of the net given features as input.\n\n    The network topology is already defined in the constructor, so multiple\n    calls to BuildForward build multiple networks whose parameters are all\n    shared. It is the source of the input features and the use of the output\n    that distinguishes each network.\n\n    Args:\n      feature_endpoints: tensors with input features to the network\n      return_average: whether to use moving averages as model parameters\n\n    Returns:\n      logits: output of the final layer before computing softmax\n    """"""\n    assert len(feature_endpoints) == self._feature_size\n\n    # Create embedding layer.\n    embeddings = []\n    for i in range(self._feature_size):\n      embeddings.append(self._AddEmbedding(feature_endpoints[i],\n                                           self._num_features[i],\n                                           self._num_feature_ids[i],\n                                           self._embedding_sizes[i],\n                                           i,\n                                           return_average=return_average))\n\n    last_layer = tf.concat(1, embeddings)\n    last_layer_size = self.embedding_size\n\n    # Create ReLU layers.\n    for i, hidden_layer_size in enumerate(self._hidden_layer_sizes):\n      weights = self._AddParam(\n          [last_layer_size, hidden_layer_size],\n          tf.float32,\n          \'weights_%d\' % i,\n          self._ReluWeightInitializer(),\n          return_average=return_average)\n      bias = self._AddParam([hidden_layer_size],\n                            tf.float32,\n                            \'bias_%d\' % i,\n                            self._relu_bias_init,\n                            return_average=return_average)\n      last_layer = tf.nn.relu_layer(last_layer,\n                                    weights,\n                                    bias,\n                                    name=\'layer_%d\' % i)\n      last_layer_size = hidden_layer_size\n\n    # Create softmax layer.\n    softmax_weight = self._AddParam(\n        [last_layer_size, self._num_actions],\n        tf.float32,\n        \'softmax_weight\',\n        tf.random_normal_initializer(stddev=self._softmax_init,\n                                     seed=self._seed),\n        return_average=return_average)\n    softmax_bias = self._AddParam(\n        [self._num_actions],\n        tf.float32,\n        \'softmax_bias\',\n        tf.zeros_initializer,\n        return_average=return_average)\n    logits = tf.nn.xw_plus_b(last_layer,\n                             softmax_weight,\n                             softmax_bias,\n                             name=\'logits\')\n    return {\'logits\': logits}\n\n  def _AddGoldReader(self, task_context, batch_size, corpus_name):\n    features, epochs, gold_actions = (\n        gen_parser_ops.gold_parse_reader(task_context,\n                                         self._feature_size,\n                                         batch_size,\n                                         corpus_name=corpus_name,\n                                         arg_prefix=self._arg_prefix))\n    return {\'gold_actions\': tf.identity(gold_actions,\n                                        name=\'gold_actions\'),\n            \'epochs\': tf.identity(epochs,\n                                  name=\'epochs\'),\n            \'feature_endpoints\': features}\n\n  def _AddDecodedReader(self, task_context, batch_size, transition_scores,\n                        corpus_name):\n    features, epochs, eval_metrics, documents = (\n        gen_parser_ops.decoded_parse_reader(transition_scores,\n                                            task_context,\n                                            self._feature_size,\n                                            batch_size,\n                                            corpus_name=corpus_name,\n                                            arg_prefix=self._arg_prefix))\n    return {\'eval_metrics\': eval_metrics,\n            \'epochs\': tf.identity(epochs,\n                                  name=\'epochs\'),\n            \'feature_endpoints\': features,\n            \'documents\': documents}\n\n  def _AddCostFunction(self, batch_size, gold_actions, logits):\n    """"""Cross entropy plus L2 loss on weights and biases of the hidden layers.""""""\n    dense_golden = BatchedSparseToDense(gold_actions, self._num_actions)\n    cross_entropy = tf.div(\n        tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(\n            logits, dense_golden)), batch_size)\n    regularized_params = [tf.nn.l2_loss(p)\n                          for k, p in self.params.items()\n                          if k.startswith(\'weights\') or k.startswith(\'bias\')]\n    l2_loss = 1e-4 * tf.add_n(regularized_params) if regularized_params else 0\n    return {\'cost\': tf.add(cross_entropy, l2_loss, name=\'cost\')}\n\n  def AddEvaluation(self,\n                    task_context,\n                    batch_size,\n                    evaluation_max_steps=300,\n                    corpus_name=\'documents\'):\n    """"""Builds the forward network only without the training operation.\n\n    Args:\n      task_context: file path from which to read the task context.\n      batch_size: batch size to request from reader op.\n      evaluation_max_steps: max number of parsing actions during evaluation,\n          only used in beam parsing.\n      corpus_name: name of the task input to read parses from.\n\n    Returns:\n      Dictionary of named eval nodes.\n    """"""\n    def _AssignTransitionScores():\n      return tf.assign(nodes[\'transition_scores\'],\n                       nodes[\'logits\'], validate_shape=False)\n    def _Pass():\n      return tf.constant(-1.0)\n    unused_evaluation_max_steps = evaluation_max_steps\n    with tf.name_scope(\'evaluation\'):\n      nodes = self.evaluation\n      nodes[\'transition_scores\'] = self._AddVariable(\n          [batch_size, self._num_actions], tf.float32, \'transition_scores\',\n          tf.constant_initializer(-1.0))\n      nodes.update(self._AddDecodedReader(task_context, batch_size, nodes[\n          \'transition_scores\'], corpus_name))\n      nodes.update(self._BuildNetwork(nodes[\'feature_endpoints\'],\n                                      return_average=self._use_averaging))\n      nodes[\'eval_metrics\'] = cf.with_dependencies(\n          [tf.cond(tf.greater(tf.size(nodes[\'logits\']), 0),\n                   _AssignTransitionScores, _Pass)],\n          nodes[\'eval_metrics\'], name=\'eval_metrics\')\n    return nodes\n\n  def _IncrementCounter(self, counter):\n    return state_ops.assign_add(counter, 1, use_locking=True)\n\n  def _AddLearningRate(self, initial_learning_rate, decay_steps):\n    """"""Returns a learning rate that decays by 0.96 every decay_steps.\n\n    Args:\n      initial_learning_rate: initial value of the learning rate\n      decay_steps: decay by 0.96 every this many steps\n\n    Returns:\n      learning rate variable.\n    """"""\n    step = self.GetStep()\n    return cf.with_dependencies(\n        [self._IncrementCounter(step)],\n        tf.train.exponential_decay(initial_learning_rate,\n                                   step,\n                                   decay_steps,\n                                   0.96,\n                                   staircase=True))\n\n  def AddPretrainedEmbeddings(self, index, embeddings_path, task_context):\n    """"""Embeddings at the given index will be set to pretrained values.""""""\n\n    def _Initializer(shape, dtype=tf.float32, partition_info=None):\n      unused_dtype = dtype\n      t = gen_parser_ops.word_embedding_initializer(\n          vectors=embeddings_path,\n          task_context=task_context,\n          embedding_init=self._embedding_init)\n\n      t.set_shape(shape)\n      return t\n\n    self._pretrained_embeddings[index] = _Initializer\n\n  def AddTraining(self,\n                  task_context,\n                  batch_size,\n                  learning_rate=0.1,\n                  decay_steps=4000,\n                  momentum=0.9,\n                  corpus_name=\'documents\'):\n    """"""Builds a trainer to minimize the cross entropy cost function.\n\n    Args:\n      task_context: file path from which to read the task context\n      batch_size: batch size to request from reader op\n      learning_rate: initial value of the learning rate\n      decay_steps: decay learning rate by 0.96 every this many steps\n      momentum: momentum parameter used when training with momentum\n      corpus_name: name of the task input to read parses from\n\n    Returns:\n      Dictionary of named training nodes.\n    """"""\n    with tf.name_scope(\'training\'):\n      nodes = self.training\n      nodes.update(self._AddGoldReader(task_context, batch_size, corpus_name))\n      nodes.update(self._BuildNetwork(nodes[\'feature_endpoints\'],\n                                      return_average=False))\n      nodes.update(self._AddCostFunction(batch_size, nodes[\'gold_actions\'],\n                                         nodes[\'logits\']))\n      # Add the optimizer\n      if self._only_train:\n        trainable_params = [v\n                            for k, v in self.params.iteritems()\n                            if k in self._only_train]\n      else:\n        trainable_params = self.params.values()\n      lr = self._AddLearningRate(learning_rate, decay_steps)\n      optimizer = tf.train.MomentumOptimizer(lr,\n                                             momentum,\n                                             use_locking=self._use_locking)\n      train_op = optimizer.minimize(nodes[\'cost\'], var_list=trainable_params)\n      for param in trainable_params:\n        slot = optimizer.get_slot(param, \'momentum\')\n        self.inits[slot.name] = state_ops.init_variable(slot,\n                                                        tf.zeros_initializer)\n        self.variables[slot.name] = slot\n      numerical_checks = [\n          tf.check_numerics(param,\n                            message=\'Parameter is not finite.\')\n          for param in trainable_params\n          if param.dtype.base_dtype in [tf.float32, tf.float64]\n      ]\n      check_op = tf.group(*numerical_checks)\n      avg_update_op = tf.group(*self._averaging.values())\n      train_ops = [train_op]\n      if self._check_parameters:\n        train_ops.append(check_op)\n      if self._use_averaging:\n        train_ops.append(avg_update_op)\n      nodes[\'train_op\'] = tf.group(*train_ops, name=\'train_op\')\n    return nodes\n\n  def AddSaver(self, slim_model=False):\n    """"""Adds ops to save and restore model parameters.\n\n    Args:\n      slim_model: whether only averaged variables are saved.\n\n    Returns:\n      the saver object.\n    """"""\n    # We have to put the save op in the root scope otherwise running\n    # ""save/restore_all"" won\'t find the ""save/Const"" node it expects.\n    with tf.name_scope(None):\n      variables_to_save = self.params.copy()\n      variables_to_save.update(self.variables)\n      if slim_model:\n        for key in variables_to_save.keys():\n          if not key.endswith(\'avg_var\'):\n            del variables_to_save[key]\n      self.saver = tf.train.Saver(variables_to_save)\n    return self.saver\n'"
model_zoo/models/syntaxnet/syntaxnet/graph_builder_test.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for graph_builder.""""""\n\n\n# disable=no-name-in-module,unused-import,g-bad-import-order,maybe-no-member\nimport os.path\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.platform import googletest\n\nfrom syntaxnet import graph_builder\nfrom syntaxnet import sparse_pb2\nfrom syntaxnet.ops import gen_parser_ops\n\nFLAGS = tf.app.flags.FLAGS\nif not hasattr(FLAGS, \'test_srcdir\'):\n  FLAGS.test_srcdir = \'\'\nif not hasattr(FLAGS, \'test_tmpdir\'):\n  FLAGS.test_tmpdir = tf.test.get_temp_dir()\n\n\nclass GraphBuilderTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    # Creates a task context with the correct testing paths.\n    initial_task_context = os.path.join(\n        FLAGS.test_srcdir,\n        \'syntaxnet/\'\n        \'testdata/context.pbtxt\')\n    self._task_context = os.path.join(FLAGS.test_tmpdir, \'context.pbtxt\')\n    with open(initial_task_context, \'r\') as fin:\n      with open(self._task_context, \'w\') as fout:\n        fout.write(fin.read().replace(\'SRCDIR\', FLAGS.test_srcdir)\n                   .replace(\'OUTPATH\', FLAGS.test_tmpdir))\n\n    # Creates necessary term maps.\n    with self.test_session() as sess:\n      gen_parser_ops.lexicon_builder(task_context=self._task_context,\n                                     corpus_name=\'training-corpus\').run()\n      self._num_features, self._num_feature_ids, _, self._num_actions = (\n          sess.run(gen_parser_ops.feature_size(task_context=self._task_context,\n                                               arg_prefix=\'brain_parser\')))\n\n  def MakeBuilder(self, use_averaging=True, **kw_args):\n    # Set the seed and gate_gradients to ensure reproducibility.\n    return graph_builder.GreedyParser(\n        self._num_actions, self._num_features, self._num_feature_ids,\n        embedding_sizes=[8, 8, 8], hidden_layer_sizes=[32, 32], seed=42,\n        gate_gradients=True, use_averaging=use_averaging, **kw_args)\n\n  def FindNode(self, name):\n    for node in tf.get_default_graph().as_graph_def().node:\n      if node.name == name:\n        return node\n    return None\n\n  def NodeFound(self, name):\n    return self.FindNode(name) is not None\n\n  def testScope(self):\n    # Set up the network topology\n    graph = tf.Graph()\n    with graph.as_default():\n      parser = self.MakeBuilder()\n      parser.AddTraining(self._task_context,\n                         batch_size=10,\n                         corpus_name=\'training-corpus\')\n      parser.AddEvaluation(self._task_context,\n                           batch_size=2,\n                           corpus_name=\'tuning-corpus\')\n      parser.AddSaver()\n\n      # Check that the node ids we may rely on are there with the expected\n      # names.\n      self.assertEqual(parser.training[\'logits\'].name, \'training/logits:0\')\n      self.assertTrue(self.NodeFound(\'training/logits\'))\n      self.assertTrue(self.NodeFound(\'training/feature_0\'))\n      self.assertTrue(self.NodeFound(\'training/feature_1\'))\n      self.assertTrue(self.NodeFound(\'training/feature_2\'))\n      self.assertFalse(self.NodeFound(\'training/feature_3\'))\n\n      self.assertEqual(parser.evaluation[\'logits\'].name, \'evaluation/logits:0\')\n      self.assertTrue(self.NodeFound(\'evaluation/logits\'))\n\n      # The saver node is expected to be in the root scope.\n      self.assertTrue(self.NodeFound(\'save/restore_all\'))\n\n      # Also check that the parameters have the scope we expect.\n      self.assertTrue(self.NodeFound(\'embedding_matrix_0\'))\n      self.assertTrue(self.NodeFound(\'embedding_matrix_1\'))\n      self.assertTrue(self.NodeFound(\'embedding_matrix_2\'))\n      self.assertFalse(self.NodeFound(\'embedding_matrix_3\'))\n\n  def testNestedScope(self):\n    # It\'s OK to put the whole graph in a scope of its own.\n    graph = tf.Graph()\n    with graph.as_default():\n      with graph.name_scope(\'top\'):\n        parser = self.MakeBuilder()\n        parser.AddTraining(self._task_context,\n                           batch_size=10,\n                           corpus_name=\'training-corpus\')\n        parser.AddSaver()\n\n      self.assertTrue(self.NodeFound(\'top/training/logits\'))\n      self.assertTrue(self.NodeFound(\'top/training/feature_0\'))\n\n      # The saver node is expected to be in the root scope no matter what.\n      self.assertFalse(self.NodeFound(\'top/save/restore_all\'))\n      self.assertTrue(self.NodeFound(\'save/restore_all\'))\n\n  def testUseCustomGraphs(self):\n    batch_size = 10\n\n    # Use separate custom graphs.\n    custom_train_graph = tf.Graph()\n    with custom_train_graph.as_default():\n      train_parser = self.MakeBuilder()\n      train_parser.AddTraining(self._task_context,\n                               batch_size,\n                               corpus_name=\'training-corpus\')\n\n    custom_eval_graph = tf.Graph()\n    with custom_eval_graph.as_default():\n      eval_parser = self.MakeBuilder()\n      eval_parser.AddEvaluation(self._task_context,\n                                batch_size,\n                                corpus_name=\'tuning-corpus\')\n\n    # The following session runs should not fail.\n    with self.test_session(graph=custom_train_graph) as sess:\n      self.assertTrue(self.NodeFound(\'training/logits\'))\n      sess.run(train_parser.inits.values())\n      sess.run([\'training/logits:0\'])\n\n    with self.test_session(graph=custom_eval_graph) as sess:\n      self.assertFalse(self.NodeFound(\'training/logits\'))\n      self.assertTrue(self.NodeFound(\'evaluation/logits\'))\n      sess.run(eval_parser.inits.values())\n      sess.run([\'evaluation/logits:0\'])\n\n  def testTrainingAndEvalAreIndependent(self):\n    batch_size = 10\n    graph = tf.Graph()\n    with graph.as_default():\n      parser = self.MakeBuilder(use_averaging=False)\n      parser.AddTraining(self._task_context,\n                         batch_size,\n                         corpus_name=\'training-corpus\')\n      parser.AddEvaluation(self._task_context,\n                           batch_size,\n                           corpus_name=\'tuning-corpus\')\n    with self.test_session(graph=graph) as sess:\n      sess.run(parser.inits.values())\n      # Before any training updates are performed, both training and eval nets\n      # should return the same computations.\n      eval_logits, = sess.run([parser.evaluation[\'logits\']])\n      training_logits, = sess.run([parser.training[\'logits\']])\n      self.assertNear(abs((eval_logits - training_logits).sum()), 0, 1e-6)\n\n      # After training, activations should differ.\n      for _ in range(5):\n        eval_logits = parser.evaluation[\'logits\'].eval()\n      for _ in range(5):\n        training_logits, _ = sess.run([parser.training[\'logits\'],\n                                       parser.training[\'train_op\']])\n      self.assertGreater(abs((eval_logits - training_logits).sum()), 0, 1e-3)\n\n  def testReproducibility(self):\n    batch_size = 10\n\n    def ComputeACost(graph):\n      with graph.as_default():\n        parser = self.MakeBuilder(use_averaging=False)\n        parser.AddTraining(self._task_context,\n                           batch_size,\n                           corpus_name=\'training-corpus\')\n        parser.AddEvaluation(self._task_context,\n                             batch_size,\n                             corpus_name=\'tuning-corpus\')\n      with self.test_session(graph=graph) as sess:\n        sess.run(parser.inits.values())\n        for _ in range(5):\n          cost, _ = sess.run([parser.training[\'cost\'],\n                              parser.training[\'train_op\']])\n      return cost\n\n    cost1 = ComputeACost(tf.Graph())\n    cost2 = ComputeACost(tf.Graph())\n    self.assertNear(cost1, cost2, 1e-8)\n\n  def testAddTrainingAndEvalOrderIndependent(self):\n    batch_size = 10\n\n    graph1 = tf.Graph()\n    with graph1.as_default():\n      parser = self.MakeBuilder(use_averaging=False)\n      parser.AddTraining(self._task_context,\n                         batch_size,\n                         corpus_name=\'training-corpus\')\n      parser.AddEvaluation(self._task_context,\n                           batch_size,\n                           corpus_name=\'tuning-corpus\')\n    with self.test_session(graph=graph1) as sess:\n      sess.run(parser.inits.values())\n      metrics1 = None\n      for _ in range(50):\n        cost1, _ = sess.run([parser.training[\'cost\'],\n                             parser.training[\'train_op\']])\n        em1 = parser.evaluation[\'eval_metrics\'].eval()\n        metrics1 = metrics1 + em1 if metrics1 is not None else em1\n\n    # Reverse the order in which Training and Eval stacks are added.\n    graph2 = tf.Graph()\n    with graph2.as_default():\n      parser = self.MakeBuilder(use_averaging=False)\n      parser.AddEvaluation(self._task_context,\n                           batch_size,\n                           corpus_name=\'tuning-corpus\')\n      parser.AddTraining(self._task_context,\n                         batch_size,\n                         corpus_name=\'training-corpus\')\n    with self.test_session(graph=graph2) as sess:\n      sess.run(parser.inits.values())\n      metrics2 = None\n      for _ in range(50):\n        cost2, _ = sess.run([parser.training[\'cost\'],\n                             parser.training[\'train_op\']])\n        em2 = parser.evaluation[\'eval_metrics\'].eval()\n        metrics2 = metrics2 + em2 if metrics2 is not None else em2\n\n    self.assertNear(cost1, cost2, 1e-8)\n    self.assertEqual(abs(metrics1 - metrics2).sum(), 0)\n\n  def testEvalMetrics(self):\n    batch_size = 10\n    graph = tf.Graph()\n    with graph.as_default():\n      parser = self.MakeBuilder()\n      parser.AddEvaluation(self._task_context,\n                           batch_size,\n                           corpus_name=\'tuning-corpus\')\n    with self.test_session(graph=graph) as sess:\n      sess.run(parser.inits.values())\n      tokens = 0\n      correct_heads = 0\n      for _ in range(100):\n        eval_metrics = sess.run(parser.evaluation[\'eval_metrics\'])\n        tokens += eval_metrics[0]\n        correct_heads += eval_metrics[1]\n      self.assertGreater(tokens, 0)\n      self.assertGreaterEqual(tokens, correct_heads)\n      self.assertGreaterEqual(correct_heads, 0)\n\n  def MakeSparseFeatures(self, ids, weights):\n    f = sparse_pb2.SparseFeatures()\n    for i, w in zip(ids, weights):\n      f.id.append(i)\n      f.weight.append(w)\n    return f.SerializeToString()\n\n  def testEmbeddingOp(self):\n    graph = tf.Graph()\n    with self.test_session(graph=graph):\n      params = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]],\n                           tf.float32)\n\n      var = variables.Variable([self.MakeSparseFeatures([1, 2], [1.0, 1.0]),\n                                self.MakeSparseFeatures([], [])])\n      var.initializer.run()\n      embeddings = graph_builder.EmbeddingLookupFeatures(params, var,\n                                                         True).eval()\n      self.assertAllClose([[8.0, 10.0], [0.0, 0.0]], embeddings)\n\n      var = variables.Variable([self.MakeSparseFeatures([], []),\n                                self.MakeSparseFeatures([0, 2],\n                                                        [0.5, 2.0])])\n      var.initializer.run()\n      embeddings = graph_builder.EmbeddingLookupFeatures(params, var,\n                                                         True).eval()\n      self.assertAllClose([[0.0, 0.0], [10.5, 13.0]], embeddings)\n\n  def testOnlyTrainSomeParameters(self):\n    batch_size = 10\n    graph = tf.Graph()\n    with graph.as_default():\n      parser = self.MakeBuilder(use_averaging=False, only_train=\'softmax_bias\')\n      parser.AddTraining(self._task_context,\n                         batch_size,\n                         corpus_name=\'training-corpus\')\n    with self.test_session(graph=graph) as sess:\n      sess.run(parser.inits.values())\n      # Before training, save the state of two of the parameters.\n      bias0, weight0 = sess.run([parser.params[\'softmax_bias\'],\n                                 parser.params[\'softmax_weight\']])\n\n      for _ in range(5):\n        bias, weight, _ = sess.run([parser.params[\'softmax_bias\'],\n                                    parser.params[\'softmax_weight\'],\n                                    parser.training[\'train_op\']])\n\n      # After training, only one of the parameters should have changed.\n      self.assertAllEqual(weight, weight0)\n      self.assertGreater(abs(bias - bias0).sum(), 0, 1e-5)\n\n\nif __name__ == \'__main__\':\n  googletest.main()\n'"
model_zoo/models/syntaxnet/syntaxnet/lexicon_builder_test.py,0,"b'# coding=utf-8\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for lexicon_builder.""""""\n\n\n# disable=no-name-in-module,unused-import,g-bad-import-order,maybe-no-member\nimport os.path\nimport tensorflow as tf\n\nimport syntaxnet.load_parser_ops\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.platform import tf_logging as logging\n\nfrom syntaxnet import sentence_pb2\nfrom syntaxnet import task_spec_pb2\nfrom syntaxnet.ops import gen_parser_ops\n\nFLAGS = tf.app.flags.FLAGS\n\nCONLL_DOC1 = u\'\'\'1 \xe0\xa4\xac\xe0\xa4\xbe\xe0\xa4\xa4 _ n NN _ _ _ _ _\n2 \xe0\xa4\x97\xe0\xa4\xb2\xe0\xa4\xa4 _ adj JJ _ _ _ _ _\n3 \xe0\xa4\xb9\xe0\xa5\x8b _ v VM _ _ _ _ _\n4 \xe0\xa4\xa4\xe0\xa5\x8b _ avy CC _ _ _ _ _\n5 \xe0\xa4\x97\xe0\xa5\x81\xe0\xa4\xb8\xe0\xa5\x8d\xe0\xa4\xb8\xe0\xa4\xbe _ n NN _ _ _ _ _\n6 \xe0\xa4\xb8\xe0\xa5\x87\xe0\xa4\xb2\xe0\xa5\x87\xe0\xa4\xac\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xbf\xe0\xa4\x9f\xe0\xa4\xbf\xe0\xa4\x9c _ n NN _ _ _ _ _\n7 \xe0\xa4\x95\xe0\xa5\x8b _ psp PSP _ _ _ _ _\n8 \xe0\xa4\xad\xe0\xa5\x80 _ avy RP _ _ _ _ _\n9 \xe0\xa4\x86\xe0\xa4\xa8\xe0\xa4\xbe _ v VM _ _ _ _ _\n10 \xe0\xa4\xb2\xe0\xa4\xbe\xe0\xa4\x9c\xe0\xa4\xae\xe0\xa5\x80 _ adj JJ _ _ _ _ _\n11 \xe0\xa4\xb9\xe0\xa5\x88 _ v VM _ _ _ _ _\n12 \xe0\xa5\xa4 _ punc SYM _ _ _ _ _\'\'\'\n\nCONLL_DOC2 = u\'\'\'1 \xe0\xa4\xb2\xe0\xa5\x87\xe0\xa4\x95\xe0\xa4\xbf\xe0\xa4\xa8 _ avy CC _ _ _ _ _\n2 \xe0\xa4\x85\xe0\xa4\xad\xe0\xa4\xbf\xe0\xa4\xa8\xe0\xa5\x87\xe0\xa4\xa4\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa5\x80 _ n NN _ _ _ _ _\n3 \xe0\xa4\x95\xe0\xa5\x87 _ psp PSP _ _ _ _ _\n4 \xe0\xa4\x87\xe0\xa4\xb8 _ pn DEM _ _ _ _ _\n5 \xe0\xa4\x95\xe0\xa4\xa6\xe0\xa4\xae _ n NN _ _ _ _ _\n6 \xe0\xa4\xb8\xe0\xa5\x87 _ psp PSP _ _ _ _ _\n7 \xe0\xa4\xb5\xe0\xa4\xb9\xe0\xa4\xbe\xe0\xa4\x82 _ pn PRP _ _ _ _ _\n8 \xe0\xa4\xb0\xe0\xa4\x82\xe0\xa4\x97 _ n NN _ _ _ _ _\n9 \xe0\xa4\xae\xe0\xa5\x87\xe0\xa4\x82 _ psp PSP _ _ _ _ _\n10 \xe0\xa4\xad\xe0\xa4\x82\xe0\xa4\x97 _ adj JJ _ _ _ _ _\n11 \xe0\xa4\xaa\xe0\xa5\x9c _ v VM _ _ _ _ _\n12 \xe0\xa4\x97\xe0\xa4\xaf\xe0\xa4\xbe _ v VAUX _ _ _ _ _\n13 \xe0\xa5\xa4 _ punc SYM _ _ _ _ _\'\'\'\n\nTAGS = [\'NN\', \'JJ\', \'VM\', \'CC\', \'PSP\', \'RP\', \'JJ\', \'SYM\', \'DEM\', \'PRP\', \'VAUX\']\n\nCATEGORIES = [\'n\', \'adj\', \'v\', \'avy\', \'n\', \'psp\', \'punc\', \'pn\']\n\nTOKENIZED_DOCS = u\'\'\'\xe0\xa4\xac\xe0\xa4\xbe\xe0\xa4\xa4 \xe0\xa4\x97\xe0\xa4\xb2\xe0\xa4\xa4 \xe0\xa4\xb9\xe0\xa5\x8b \xe0\xa4\xa4\xe0\xa5\x8b \xe0\xa4\x97\xe0\xa5\x81\xe0\xa4\xb8\xe0\xa5\x8d\xe0\xa4\xb8\xe0\xa4\xbe \xe0\xa4\xb8\xe0\xa5\x87\xe0\xa4\xb2\xe0\xa5\x87\xe0\xa4\xac\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xbf\xe0\xa4\x9f\xe0\xa4\xbf\xe0\xa4\x9c \xe0\xa4\x95\xe0\xa5\x8b \xe0\xa4\xad\xe0\xa5\x80 \xe0\xa4\x86\xe0\xa4\xa8\xe0\xa4\xbe \xe0\xa4\xb2\xe0\xa4\xbe\xe0\xa4\x9c\xe0\xa4\xae\xe0\xa5\x80 \xe0\xa4\xb9\xe0\xa5\x88 \xe0\xa5\xa4\n\xe0\xa4\xb2\xe0\xa5\x87\xe0\xa4\x95\xe0\xa4\xbf\xe0\xa4\xa8 \xe0\xa4\x85\xe0\xa4\xad\xe0\xa4\xbf\xe0\xa4\xa8\xe0\xa5\x87\xe0\xa4\xa4\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa5\x80 \xe0\xa4\x95\xe0\xa5\x87 \xe0\xa4\x87\xe0\xa4\xb8 \xe0\xa4\x95\xe0\xa4\xa6\xe0\xa4\xae \xe0\xa4\xb8\xe0\xa5\x87 \xe0\xa4\xb5\xe0\xa4\xb9\xe0\xa4\xbe\xe0\xa4\x82 \xe0\xa4\xb0\xe0\xa4\x82\xe0\xa4\x97 \xe0\xa4\xae\xe0\xa5\x87\xe0\xa4\x82 \xe0\xa4\xad\xe0\xa4\x82\xe0\xa4\x97 \xe0\xa4\xaa\xe0\xa5\x9c \xe0\xa4\x97\xe0\xa4\xaf\xe0\xa4\xbe \xe0\xa5\xa4\n\'\'\'\n\nCHARS = u\'\'\'\xe0\xa4\x85 \xe0\xa4\x87 \xe0\xa4\x86 \xe0\xa4\x95 \xe0\xa4\x97 \xe0\xa4\x9c \xe0\xa4\x9f \xe0\xa4\xa4 \xe0\xa4\xa6 \xe0\xa4\xa8 \xe0\xa4\xaa \xe0\xa4\xad \xe0\xa4\xac \xe0\xa4\xaf \xe0\xa4\xae \xe0\xa4\xb0 \xe0\xa4\xb2 \xe0\xa4\xb5 \xe0\xa4\xb9 \xe0\xa4\xb8 \xe0\xa4\xbf \xe0\xa4\xbe \xe0\xa5\x81 \xe0\xa5\x80 \xe0\xa5\x87 \xe0\xa5\x88 \xe0\xa5\x8b \xe0\xa5\x8d \xe0\xa5\x9c \xe0\xa5\xa4 \xe0\xa4\x82\'\'\'\n\nCOMMENTS = u\'# Line with fake comments.\'\n\n\nclass LexiconBuilderTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    if not hasattr(FLAGS, \'test_srcdir\'):\n      FLAGS.test_srcdir = \'\'\n    if not hasattr(FLAGS, \'test_tmpdir\'):\n      FLAGS.test_tmpdir = tf.test.get_temp_dir()\n    self.corpus_file = os.path.join(FLAGS.test_tmpdir, \'documents.conll\')\n    self.context_file = os.path.join(FLAGS.test_tmpdir, \'context.pbtxt\')\n\n  def AddInput(self, name, file_pattern, record_format, context):\n    inp = context.input.add()\n    inp.name = name\n    inp.record_format.append(record_format)\n    inp.part.add().file_pattern = file_pattern\n\n  def WriteContext(self, corpus_format):\n    context = task_spec_pb2.TaskSpec()\n    self.AddInput(\'documents\', self.corpus_file, corpus_format, context)\n    for name in (\'word-map\', \'lcword-map\', \'tag-map\',\n                 \'category-map\', \'label-map\', \'prefix-table\',\n                 \'suffix-table\', \'tag-to-category\', \'char-map\'):\n      self.AddInput(name, os.path.join(FLAGS.test_tmpdir, name), \'\', context)\n    logging.info(\'Writing context to: %s\', self.context_file)\n    with open(self.context_file, \'w\') as f:\n      f.write(str(context))\n\n  def ReadNextDocument(self, sess, doc_source):\n    doc_str, last = sess.run(doc_source)\n    if doc_str:\n      doc = sentence_pb2.Sentence()\n      doc.ParseFromString(doc_str[0])\n    else:\n      doc = None\n    return doc, last\n\n  def ValidateDocuments(self):\n    doc_source = gen_parser_ops.document_source(self.context_file, batch_size=1)\n    with self.test_session() as sess:\n      logging.info(\'Reading document1\')\n      doc, last = self.ReadNextDocument(sess, doc_source)\n      self.assertEqual(len(doc.token), 12)\n      self.assertEqual(u\'\xe0\xa4\xb2\xe0\xa4\xbe\xe0\xa4\x9c\xe0\xa4\xae\xe0\xa5\x80\', doc.token[9].word)\n      self.assertFalse(last)\n      logging.info(\'Reading document2\')\n      doc, last = self.ReadNextDocument(sess, doc_source)\n      self.assertEqual(len(doc.token), 13)\n      self.assertEqual(u\'\xe0\xa4\xad\xe0\xa4\x82\xe0\xa4\x97\', doc.token[9].word)\n      self.assertFalse(last)\n      logging.info(\'Hitting end of the dataset\')\n      doc, last = self.ReadNextDocument(sess, doc_source)\n      self.assertTrue(doc is None)\n      self.assertTrue(last)\n\n  def ValidateTagToCategoryMap(self):\n    with file(os.path.join(FLAGS.test_tmpdir, \'tag-to-category\'), \'r\') as f:\n      entries = [line.strip().split(\'\\t\') for line in f.readlines()]\n    for tag, category in entries:\n      self.assertIn(tag, TAGS)\n      self.assertIn(category, CATEGORIES)\n\n  def LoadMap(self, map_name):\n    loaded_map = {}\n    with file(os.path.join(FLAGS.test_tmpdir, map_name), \'r\') as f:\n      for line in f:\n        entries = line.strip().split(\' \')\n        if len(entries) == 2:\n          loaded_map[entries[0]] = entries[1]\n    return loaded_map\n\n  def ValidateCharMap(self):\n    char_map = self.LoadMap(\'char-map\')\n    self.assertEqual(len(char_map), len(CHARS.split(\' \')))\n    for char in CHARS.split(\' \'):\n      self.assertIn(char.encode(\'utf-8\'), char_map)\n\n  def ValidateWordMap(self):\n    word_map = self.LoadMap(\'word-map\')\n    for word in filter(None, TOKENIZED_DOCS.replace(\'\\n\', \' \').split(\' \')):\n      self.assertIn(word.encode(\'utf-8\'), word_map)\n\n  def BuildLexicon(self):\n    with self.test_session():\n      gen_parser_ops.lexicon_builder(task_context=self.context_file).run()\n\n  def testCoNLLFormat(self):\n    self.WriteContext(\'conll-sentence\')\n    logging.info(\'Writing conll file to: %s\', self.corpus_file)\n    with open(self.corpus_file, \'w\') as f:\n      f.write((CONLL_DOC1 + u\'\\n\\n\' + CONLL_DOC2 + u\'\\n\')\n              .replace(\' \', \'\\t\').encode(\'utf-8\'))\n    self.ValidateDocuments()\n    self.BuildLexicon()\n    self.ValidateTagToCategoryMap()\n    self.ValidateCharMap()\n    self.ValidateWordMap()\n\n  def testCoNLLFormatExtraNewlinesAndComments(self):\n    self.WriteContext(\'conll-sentence\')\n    with open(self.corpus_file, \'w\') as f:\n      f.write((u\'\\n\\n\\n\' + CONLL_DOC1 + u\'\\n\\n\\n\' + COMMENTS +\n               u\'\\n\\n\' + CONLL_DOC2).replace(\' \', \'\\t\').encode(\'utf-8\'))\n    self.ValidateDocuments()\n    self.BuildLexicon()\n    self.ValidateTagToCategoryMap()\n\n  def testTokenizedTextFormat(self):\n    self.WriteContext(\'tokenized-text\')\n    with open(self.corpus_file, \'w\') as f:\n      f.write(TOKENIZED_DOCS.encode(\'utf-8\'))\n    self.ValidateDocuments()\n    self.BuildLexicon()\n\n  def testTokenizedTextFormatExtraNewlines(self):\n    self.WriteContext(\'tokenized-text\')\n    with open(self.corpus_file, \'w\') as f:\n      f.write((u\'\\n\\n\\n\' + TOKENIZED_DOCS + u\'\\n\\n\\n\').encode(\'utf-8\'))\n    self.ValidateDocuments()\n    self.BuildLexicon()\n\nif __name__ == \'__main__\':\n  googletest.main()\n'"
model_zoo/models/syntaxnet/syntaxnet/load_parser_ops.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Loads parser_ops shared library.""""""\n\nimport os.path\nimport tensorflow as tf\n\ntf.load_op_library(\n    os.path.join(tf.resource_loader.get_data_files_path(),\n                 \'parser_ops.so\'))\n'"
model_zoo/models/syntaxnet/syntaxnet/parser_eval.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A program to annotate a conll file with a tensorflow neural net parser.""""""\n\n\nimport os\nimport os.path\nimport time\nimport tempfile\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.platform import tf_logging as logging\n\nfrom google.protobuf import text_format\n\nfrom syntaxnet import sentence_pb2\nfrom syntaxnet import graph_builder\nfrom syntaxnet import structured_graph_builder\nfrom syntaxnet.ops import gen_parser_ops\nfrom syntaxnet import task_spec_pb2\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n\nflags.DEFINE_string(\'task_context\', \'\',\n                    \'Path to a task context with inputs and parameters for \'\n                    \'feature extractors.\')\nflags.DEFINE_string(\'resource_dir\', \'\',\n                    \'Optional base directory for task context resources.\')\nflags.DEFINE_string(\'model_path\', \'\', \'Path to model parameters.\')\nflags.DEFINE_string(\'arg_prefix\', None, \'Prefix for context parameters.\')\nflags.DEFINE_string(\'graph_builder\', \'greedy\',\n                    \'Which graph builder to use, either greedy or structured.\')\nflags.DEFINE_string(\'input\', \'stdin\',\n                    \'Name of the context input to read data from.\')\nflags.DEFINE_string(\'output\', \'stdout\',\n                    \'Name of the context input to write data to.\')\nflags.DEFINE_string(\'hidden_layer_sizes\', \'200,200\',\n                    \'Comma separated list of hidden layer sizes.\')\nflags.DEFINE_integer(\'batch_size\', 32,\n                     \'Number of sentences to process in parallel.\')\nflags.DEFINE_integer(\'beam_size\', 8, \'Number of slots for beam parsing.\')\nflags.DEFINE_integer(\'max_steps\', 1000, \'Max number of steps to take.\')\nflags.DEFINE_bool(\'slim_model\', False,\n                  \'Whether to expect only averaged variables.\')\n\n\ndef RewriteContext(task_context):\n  context = task_spec_pb2.TaskSpec()\n  with gfile.FastGFile(task_context) as fin:\n    text_format.Merge(fin.read(), context)\n  for resource in context.input:\n    for part in resource.part:\n      if part.file_pattern != \'-\':\n        part.file_pattern = os.path.join(FLAGS.resource_dir, part.file_pattern)\n  with tempfile.NamedTemporaryFile(delete=False) as fout:\n    fout.write(str(context))\n    return fout.name\n\n\ndef Eval(sess):\n  """"""Builds and evaluates a network.""""""\n  task_context = FLAGS.task_context\n  if FLAGS.resource_dir:\n    task_context = RewriteContext(task_context)\n  feature_sizes, domain_sizes, embedding_dims, num_actions = sess.run(\n      gen_parser_ops.feature_size(task_context=task_context,\n                                  arg_prefix=FLAGS.arg_prefix))\n\n  t = time.time()\n  hidden_layer_sizes = map(int, FLAGS.hidden_layer_sizes.split(\',\'))\n  logging.info(\'Building training network with parameters: feature_sizes: %s \'\n               \'domain_sizes: %s\', feature_sizes, domain_sizes)\n  if FLAGS.graph_builder == \'greedy\':\n    parser = graph_builder.GreedyParser(num_actions,\n                                        feature_sizes,\n                                        domain_sizes,\n                                        embedding_dims,\n                                        hidden_layer_sizes,\n                                        gate_gradients=True,\n                                        arg_prefix=FLAGS.arg_prefix)\n  else:\n    parser = structured_graph_builder.StructuredGraphBuilder(\n        num_actions,\n        feature_sizes,\n        domain_sizes,\n        embedding_dims,\n        hidden_layer_sizes,\n        gate_gradients=True,\n        arg_prefix=FLAGS.arg_prefix,\n        beam_size=FLAGS.beam_size,\n        max_steps=FLAGS.max_steps)\n  parser.AddEvaluation(task_context,\n                       FLAGS.batch_size,\n                       corpus_name=FLAGS.input,\n                       evaluation_max_steps=FLAGS.max_steps)\n\n  parser.AddSaver(FLAGS.slim_model)\n  sess.run(parser.inits.values())\n  parser.saver.restore(sess, FLAGS.model_path)\n\n  sink_documents = tf.placeholder(tf.string)\n  sink = gen_parser_ops.document_sink(sink_documents,\n                                      task_context=task_context,\n                                      corpus_name=FLAGS.output)\n  t = time.time()\n  num_epochs = None\n  num_tokens = 0\n  num_correct = 0\n  num_documents = 0\n  while True:\n    tf_eval_epochs, tf_eval_metrics, tf_documents = sess.run([\n        parser.evaluation[\'epochs\'],\n        parser.evaluation[\'eval_metrics\'],\n        parser.evaluation[\'documents\'],\n    ])\n\n    if len(tf_documents):\n      logging.info(\'Processed %d documents\', len(tf_documents))\n      num_documents += len(tf_documents)\n      sess.run(sink, feed_dict={sink_documents: tf_documents})\n\n    num_tokens += tf_eval_metrics[0]\n    num_correct += tf_eval_metrics[1]\n    if num_epochs is None:\n      num_epochs = tf_eval_epochs\n    elif num_epochs < tf_eval_epochs:\n      break\n\n  logging.info(\'Total processed documents: %d\', num_documents)\n  if num_tokens > 0:\n    eval_metric = 100.0 * num_correct / num_tokens\n    logging.info(\'num correct tokens: %d\', num_correct)\n    logging.info(\'total tokens: %d\', num_tokens)\n    logging.info(\'Seconds elapsed in evaluation: %.2f, \'\n                 \'eval metric: %.2f%%\', time.time() - t, eval_metric)\n\n\ndef main(unused_argv):\n  logging.set_verbosity(logging.INFO)\n  with tf.Session() as sess:\n    Eval(sess)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/syntaxnet/syntaxnet/parser_trainer.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A program to train a tensorflow neural net parser from a a conll file.""""""\n\n\n\nimport os\nimport os.path\nimport time\nimport tensorflow as tf\n\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.python.platform import tf_logging as logging\n\nfrom google.protobuf import text_format\n\nfrom syntaxnet import graph_builder\nfrom syntaxnet import structured_graph_builder\nfrom syntaxnet.ops import gen_parser_ops\nfrom syntaxnet import task_spec_pb2\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(\'tf_master\', \'\',\n                    \'TensorFlow execution engine to connect to.\')\nflags.DEFINE_string(\'output_path\', \'\', \'Top level for output.\')\nflags.DEFINE_string(\'task_context\', \'\',\n                    \'Path to a task context with resource locations and \'\n                    \'parameters.\')\nflags.DEFINE_string(\'arg_prefix\', None, \'Prefix for context parameters.\')\nflags.DEFINE_string(\'params\', \'0\', \'Unique identifier of parameter grid point.\')\nflags.DEFINE_string(\'training_corpus\', \'training-corpus\',\n                    \'Name of the context input to read training data from.\')\nflags.DEFINE_string(\'tuning_corpus\', \'tuning-corpus\',\n                    \'Name of the context input to read tuning data from.\')\nflags.DEFINE_string(\'word_embeddings\', None,\n                    \'Recordio containing pretrained word embeddings, will be \'\n                    \'loaded as the first embedding matrix.\')\nflags.DEFINE_bool(\'compute_lexicon\', False, \'\')\nflags.DEFINE_bool(\'projectivize_training_set\', False, \'\')\nflags.DEFINE_string(\'hidden_layer_sizes\', \'200,200\',\n                    \'Comma separated list of hidden layer sizes.\')\nflags.DEFINE_string(\'graph_builder\', \'greedy\',\n                    \'Graph builder to use, either ""greedy"" or ""structured"".\')\nflags.DEFINE_integer(\'batch_size\', 32,\n                     \'Number of sentences to process in parallel.\')\nflags.DEFINE_integer(\'beam_size\', 10, \'Number of slots for beam parsing.\')\nflags.DEFINE_integer(\'num_epochs\', 10, \'Number of epochs to train for.\')\nflags.DEFINE_integer(\'max_steps\', 50,\n                     \'Max number of parser steps during a training step.\')\nflags.DEFINE_integer(\'report_every\', 100,\n                     \'Report cost and training accuracy every this many steps.\')\nflags.DEFINE_integer(\'checkpoint_every\', 5000,\n                     \'Measure tuning UAS and checkpoint every this many steps.\')\nflags.DEFINE_bool(\'slim_model\', False,\n                  \'Whether to remove non-averaged variables, for compactness.\')\nflags.DEFINE_float(\'learning_rate\', 0.1, \'Initial learning rate parameter.\')\nflags.DEFINE_integer(\'decay_steps\', 4000,\n                     \'Decay learning rate by 0.96 every this many steps.\')\nflags.DEFINE_float(\'momentum\', 0.9,\n                   \'Momentum parameter for momentum optimizer.\')\nflags.DEFINE_string(\'seed\', \'0\', \'Initialization seed for TF variables.\')\nflags.DEFINE_string(\'pretrained_params\', None,\n                    \'Path to model from which to load params.\')\nflags.DEFINE_string(\'pretrained_params_names\', None,\n                    \'List of names of tensors to load from pretrained model.\')\nflags.DEFINE_float(\'averaging_decay\', 0.9999,\n                   \'Decay for exponential moving average when computing\'\n                   \'averaged parameters, set to 1 to do vanilla averaging.\')\n\n\ndef StageName():\n  return os.path.join(FLAGS.arg_prefix, FLAGS.graph_builder)\n\n\ndef OutputPath(path):\n  return os.path.join(FLAGS.output_path, StageName(), FLAGS.params, path)\n\n\ndef RewriteContext():\n  context = task_spec_pb2.TaskSpec()\n  with gfile.FastGFile(FLAGS.task_context) as fin:\n    text_format.Merge(fin.read(), context)\n  for resource in context.input:\n    if resource.creator == StageName():\n      del resource.part[:]\n      part = resource.part.add()\n      part.file_pattern = os.path.join(OutputPath(resource.name))\n  with gfile.FastGFile(OutputPath(\'context\'), \'w\') as fout:\n    fout.write(str(context))\n\n\ndef WriteStatus(num_steps, eval_metric, best_eval_metric):\n  status = os.path.join(os.getenv(\'GOOGLE_STATUS_DIR\') or \'/tmp\', \'STATUS\')\n  message = (\'Parameters: %s | Steps: %d | Tuning score: %.2f%% | \'\n             \'Best tuning score: %.2f%%\' % (FLAGS.params, num_steps,\n                                            eval_metric, best_eval_metric))\n  with gfile.FastGFile(status, \'w\') as fout:\n    fout.write(message)\n  with gfile.FastGFile(OutputPath(\'status\'), \'a\') as fout:\n    fout.write(message + \'\\n\')\n\n\ndef Eval(sess, parser, num_steps, best_eval_metric):\n  """"""Evaluates a network and checkpoints it to disk.\n\n  Args:\n    sess: tensorflow session to use\n    parser: graph builder containing all ops references\n    num_steps: number of training steps taken, for logging\n    best_eval_metric: current best eval metric, to decide whether this model is\n        the best so far\n\n  Returns:\n    new best eval metric\n  """"""\n  logging.info(\'Evaluating training network.\')\n  t = time.time()\n  num_epochs = None\n  num_tokens = 0\n  num_correct = 0\n  while True:\n    tf_eval_epochs, tf_eval_metrics = sess.run([\n        parser.evaluation[\'epochs\'], parser.evaluation[\'eval_metrics\']\n    ])\n    num_tokens += tf_eval_metrics[0]\n    num_correct += tf_eval_metrics[1]\n    if num_epochs is None:\n      num_epochs = tf_eval_epochs\n    elif num_epochs < tf_eval_epochs:\n      break\n  eval_metric = 0 if num_tokens == 0 else (100.0 * num_correct / num_tokens)\n  logging.info(\'Seconds elapsed in evaluation: %.2f, \'\n               \'eval metric: %.2f%%\', time.time() - t, eval_metric)\n  WriteStatus(num_steps, eval_metric, max(eval_metric, best_eval_metric))\n\n  # Save parameters.\n  if FLAGS.output_path:\n    logging.info(\'Writing out trained parameters.\')\n    parser.saver.save(sess, OutputPath(\'latest-model\'))\n    if eval_metric > best_eval_metric:\n      parser.saver.save(sess, OutputPath(\'model\'))\n\n  return max(eval_metric, best_eval_metric)\n\n\ndef Train(sess, num_actions, feature_sizes, domain_sizes, embedding_dims):\n  """"""Builds and trains the network.\n\n  Args:\n    sess: tensorflow session to use.\n    num_actions: number of possible golden actions.\n    feature_sizes: size of each feature vector.\n    domain_sizes: number of possible feature ids in each feature vector.\n    embedding_dims: embedding dimension to use for each feature group.\n  """"""\n  t = time.time()\n  hidden_layer_sizes = map(int, FLAGS.hidden_layer_sizes.split(\',\'))\n  logging.info(\'Building training network with parameters: feature_sizes: %s \'\n               \'domain_sizes: %s\', feature_sizes, domain_sizes)\n\n  if FLAGS.graph_builder == \'greedy\':\n    parser = graph_builder.GreedyParser(num_actions,\n                                        feature_sizes,\n                                        domain_sizes,\n                                        embedding_dims,\n                                        hidden_layer_sizes,\n                                        seed=int(FLAGS.seed),\n                                        gate_gradients=True,\n                                        averaging_decay=FLAGS.averaging_decay,\n                                        arg_prefix=FLAGS.arg_prefix)\n  else:\n    parser = structured_graph_builder.StructuredGraphBuilder(\n        num_actions,\n        feature_sizes,\n        domain_sizes,\n        embedding_dims,\n        hidden_layer_sizes,\n        seed=int(FLAGS.seed),\n        gate_gradients=True,\n        averaging_decay=FLAGS.averaging_decay,\n        arg_prefix=FLAGS.arg_prefix,\n        beam_size=FLAGS.beam_size,\n        max_steps=FLAGS.max_steps)\n\n  task_context = OutputPath(\'context\')\n  if FLAGS.word_embeddings is not None:\n    parser.AddPretrainedEmbeddings(0, FLAGS.word_embeddings, task_context)\n\n  corpus_name = (\'projectivized-training-corpus\' if\n                 FLAGS.projectivize_training_set else FLAGS.training_corpus)\n  parser.AddTraining(task_context,\n                     FLAGS.batch_size,\n                     learning_rate=FLAGS.learning_rate,\n                     momentum=FLAGS.momentum,\n                     decay_steps=FLAGS.decay_steps,\n                     corpus_name=corpus_name)\n  parser.AddEvaluation(task_context,\n                       FLAGS.batch_size,\n                       corpus_name=FLAGS.tuning_corpus)\n  parser.AddSaver(FLAGS.slim_model)\n\n  # Save graph.\n  if FLAGS.output_path:\n    with gfile.FastGFile(OutputPath(\'graph\'), \'w\') as f:\n      f.write(sess.graph_def.SerializeToString())\n\n  logging.info(\'Initializing...\')\n  num_epochs = 0\n  cost_sum = 0.0\n  num_steps = 0\n  best_eval_metric = 0.0\n  sess.run(parser.inits.values())\n\n  if FLAGS.pretrained_params is not None:\n    logging.info(\'Loading pretrained params from %s\', FLAGS.pretrained_params)\n    feed_dict = {\'save/Const:0\': FLAGS.pretrained_params}\n    targets = []\n    for node in sess.graph_def.node:\n      if (node.name.startswith(\'save/Assign\') and\n          node.input[0] in FLAGS.pretrained_params_names.split(\',\')):\n        logging.info(\'Loading %s with op %s\', node.input[0], node.name)\n        targets.append(node.name)\n    sess.run(targets, feed_dict=feed_dict)\n\n  logging.info(\'Training...\')\n  while num_epochs < FLAGS.num_epochs:\n    tf_epochs, tf_cost, _ = sess.run([parser.training[\n        \'epochs\'], parser.training[\'cost\'], parser.training[\'train_op\']])\n    num_epochs = tf_epochs\n    num_steps += 1\n    cost_sum += tf_cost\n    if num_steps % FLAGS.report_every == 0:\n      logging.info(\'Epochs: %d, num steps: %d, \'\n                   \'seconds elapsed: %.2f, avg cost: %.2f, \', num_epochs,\n                   num_steps, time.time() - t, cost_sum / FLAGS.report_every)\n      cost_sum = 0.0\n    if num_steps % FLAGS.checkpoint_every == 0:\n      best_eval_metric = Eval(sess, parser, num_steps, best_eval_metric)\n\n\ndef main(unused_argv):\n  logging.set_verbosity(logging.INFO)\n  if not gfile.IsDirectory(OutputPath(\'\')):\n    gfile.MakeDirs(OutputPath(\'\'))\n\n  # Rewrite context.\n  RewriteContext()\n\n  # Creates necessary term maps.\n  if FLAGS.compute_lexicon:\n    logging.info(\'Computing lexicon...\')\n    with tf.Session(FLAGS.tf_master) as sess:\n      gen_parser_ops.lexicon_builder(task_context=OutputPath(\'context\'),\n                                     corpus_name=FLAGS.training_corpus).run()\n  with tf.Session(FLAGS.tf_master) as sess:\n    feature_sizes, domain_sizes, embedding_dims, num_actions = sess.run(\n        gen_parser_ops.feature_size(task_context=OutputPath(\'context\'),\n                                    arg_prefix=FLAGS.arg_prefix))\n\n  # Well formed and projectivize.\n  if FLAGS.projectivize_training_set:\n    logging.info(\'Preprocessing...\')\n    with tf.Session(FLAGS.tf_master) as sess:\n      source, last = gen_parser_ops.document_source(\n          task_context=OutputPath(\'context\'),\n          batch_size=FLAGS.batch_size,\n          corpus_name=FLAGS.training_corpus)\n      sink = gen_parser_ops.document_sink(\n          task_context=OutputPath(\'context\'),\n          corpus_name=\'projectivized-training-corpus\',\n          documents=gen_parser_ops.projectivize_filter(\n              gen_parser_ops.well_formed_filter(source,\n                                                task_context=OutputPath(\n                                                    \'context\')),\n              task_context=OutputPath(\'context\')))\n      while True:\n        tf_last, _ = sess.run([last, sink])\n        if tf_last:\n          break\n\n  logging.info(\'Training...\')\n  with tf.Session(FLAGS.tf_master) as sess:\n    Train(sess, num_actions, feature_sizes, domain_sizes, embedding_dims)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/syntaxnet/syntaxnet/reader_ops_test.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for reader_ops.""""""\n\n\nimport os.path\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.platform import tf_logging as logging\n\nfrom syntaxnet import dictionary_pb2\nfrom syntaxnet import graph_builder\nfrom syntaxnet import sparse_pb2\nfrom syntaxnet.ops import gen_parser_ops\n\n\nFLAGS = tf.app.flags.FLAGS\nif not hasattr(FLAGS, \'test_srcdir\'):\n  FLAGS.test_srcdir = \'\'\nif not hasattr(FLAGS, \'test_tmpdir\'):\n  FLAGS.test_tmpdir = tf.test.get_temp_dir()\n\n\nclass ParsingReaderOpsTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    # Creates a task context with the correct testing paths.\n    initial_task_context = os.path.join(\n        FLAGS.test_srcdir,\n        \'syntaxnet/\'\n        \'testdata/context.pbtxt\')\n    self._task_context = os.path.join(FLAGS.test_tmpdir, \'context.pbtxt\')\n    with open(initial_task_context, \'r\') as fin:\n      with open(self._task_context, \'w\') as fout:\n        fout.write(fin.read().replace(\'SRCDIR\', FLAGS.test_srcdir)\n                   .replace(\'OUTPATH\', FLAGS.test_tmpdir))\n\n    # Creates necessary term maps.\n    with self.test_session() as sess:\n      gen_parser_ops.lexicon_builder(task_context=self._task_context,\n                                     corpus_name=\'training-corpus\').run()\n      self._num_features, self._num_feature_ids, _, self._num_actions = (\n          sess.run(gen_parser_ops.feature_size(task_context=self._task_context,\n                                               arg_prefix=\'brain_parser\')))\n\n  def GetMaxId(self, sparse_features):\n    max_id = 0\n    for x in sparse_features:\n      for y in x:\n        f = sparse_pb2.SparseFeatures()\n        f.ParseFromString(y)\n        for i in f.id:\n          max_id = max(i, max_id)\n    return max_id\n\n  def testParsingReaderOp(self):\n    # Runs the reader over the test input for two epochs.\n    num_steps_a = 0\n    num_actions = 0\n    num_word_ids = 0\n    num_tag_ids = 0\n    num_label_ids = 0\n    batch_size = 10\n    with self.test_session() as sess:\n      (words, tags, labels), epochs, gold_actions = (\n          gen_parser_ops.gold_parse_reader(self._task_context,\n                                           3,\n                                           batch_size,\n                                           corpus_name=\'training-corpus\'))\n      while True:\n        tf_gold_actions, tf_epochs, tf_words, tf_tags, tf_labels = (\n            sess.run([gold_actions, epochs, words, tags, labels]))\n        num_steps_a += 1\n        num_actions = max(num_actions, max(tf_gold_actions) + 1)\n        num_word_ids = max(num_word_ids, self.GetMaxId(tf_words) + 1)\n        num_tag_ids = max(num_tag_ids, self.GetMaxId(tf_tags) + 1)\n        num_label_ids = max(num_label_ids, self.GetMaxId(tf_labels) + 1)\n        self.assertIn(tf_epochs, [0, 1, 2])\n        if tf_epochs > 1:\n          break\n\n    # Runs the reader again, this time with a lot of added graph nodes.\n    num_steps_b = 0\n    with self.test_session() as sess:\n      num_features = [6, 6, 4]\n      num_feature_ids = [num_word_ids, num_tag_ids, num_label_ids]\n      embedding_sizes = [8, 8, 8]\n      hidden_layer_sizes = [32, 32]\n      # Here we aim to test the iteration of the reader op in a complex network,\n      # not the GraphBuilder.\n      parser = graph_builder.GreedyParser(\n          num_actions, num_features, num_feature_ids, embedding_sizes,\n          hidden_layer_sizes)\n      parser.AddTraining(self._task_context,\n                         batch_size,\n                         corpus_name=\'training-corpus\')\n      sess.run(parser.inits.values())\n      while True:\n        tf_epochs, tf_cost, _ = sess.run(\n            [parser.training[\'epochs\'], parser.training[\'cost\'],\n             parser.training[\'train_op\']])\n        num_steps_b += 1\n        self.assertGreaterEqual(tf_cost, 0)\n        self.assertIn(tf_epochs, [0, 1, 2])\n        if tf_epochs > 1:\n          break\n\n    # Assert that the two runs made the exact same number of steps.\n    logging.info(\'Number of steps in the two runs: %d, %d\',\n                 num_steps_a, num_steps_b)\n    self.assertEqual(num_steps_a, num_steps_b)\n\n  def testParsingReaderOpWhileLoop(self):\n    feature_size = 3\n    batch_size = 5\n\n    def ParserEndpoints():\n      return gen_parser_ops.gold_parse_reader(self._task_context,\n                                              feature_size,\n                                              batch_size,\n                                              corpus_name=\'training-corpus\')\n\n    with self.test_session() as sess:\n      # The \'condition\' and \'body\' functions expect as many arguments as there\n      # are loop variables. \'condition\' depends on the \'epoch\' loop variable\n      # only, so we disregard the remaining unused function arguments. \'body\'\n      # returns a list of updated loop variables.\n      def Condition(epoch, *unused_args):\n        return tf.less(epoch, 2)\n\n      def Body(epoch, num_actions, *feature_args):\n        # By adding one of the outputs of the reader op (\'epoch\') as a control\n        # dependency to the reader op we force the repeated evaluation of the\n        # reader op.\n        with epoch.graph.control_dependencies([epoch]):\n          features, epoch, gold_actions = ParserEndpoints()\n        num_actions = tf.maximum(num_actions,\n                                 tf.reduce_max(gold_actions, [0], False) + 1)\n        feature_ids = []\n        for i in range(len(feature_args)):\n          feature_ids.append(features[i])\n        return [epoch, num_actions] + feature_ids\n\n      epoch = ParserEndpoints()[-2]\n      num_actions = tf.constant(0)\n      loop_vars = [epoch, num_actions]\n\n      res = sess.run(\n          tf.while_loop(Condition, Body, loop_vars,\n                        shape_invariants=[tf.TensorShape(None)] * 2,\n                        parallel_iterations=1))\n      logging.info(\'Result: %s\', res)\n      self.assertEqual(res[0], 2)\n\n  def testWordEmbeddingInitializer(self):\n    def _TokenEmbedding(token, embedding):\n      e = dictionary_pb2.TokenEmbedding()\n      e.token = token\n      e.vector.values.extend(embedding)\n      return e.SerializeToString()\n\n    # Provide embeddings for the first three words in the word map.\n    records_path = os.path.join(FLAGS.test_tmpdir, \'sstable-00000-of-00001\')\n    writer = tf.python_io.TFRecordWriter(records_path)\n    writer.write(_TokenEmbedding(\'.\', [1, 2]))\n    writer.write(_TokenEmbedding(\',\', [3, 4]))\n    writer.write(_TokenEmbedding(\'the\', [5, 6]))\n    del writer\n\n    with self.test_session():\n      embeddings = gen_parser_ops.word_embedding_initializer(\n          vectors=records_path,\n          task_context=self._task_context).eval()\n    self.assertAllClose(\n        np.array([[1. / (1 + 4) ** .5, 2. / (1 + 4) ** .5],\n                  [3. / (9 + 16) ** .5, 4. / (9 + 16) ** .5],\n                  [5. / (25 + 36) ** .5, 6. / (25 + 36) ** .5]]),\n        embeddings[:3,])\n\n\nif __name__ == \'__main__\':\n  googletest.main()\n'"
model_zoo/models/syntaxnet/syntaxnet/structured_graph_builder.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Build structured parser models.""""""\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops as cf\nfrom tensorflow.python.ops import state_ops\nfrom tensorflow.python.ops import tensor_array_ops\n\nfrom syntaxnet import graph_builder\nfrom syntaxnet.ops import gen_parser_ops\n\ntf.NotDifferentiable(\'BeamParseReader\')\ntf.NotDifferentiable(\'BeamParser\')\ntf.NotDifferentiable(\'BeamParserOutput\')\n\n\ndef AddCrossEntropy(batch_size, n):\n  """"""Adds a cross entropy cost function.""""""\n  cross_entropies = []\n  def _Pass():\n    return tf.constant(0, dtype=tf.float32, shape=[1])\n\n  for beam_id in range(batch_size):\n    beam_gold_slot = tf.reshape(tf.slice(n[\'gold_slot\'], [beam_id], [1]), [1])\n    def _ComputeCrossEntropy():\n      """"""Adds ops to compute cross entropy of the gold path in a beam.""""""\n      # Requires a cast so that UnsortedSegmentSum, in the gradient,\n      # is happy with the type of its input \'segment_ids\', which\n      # must be int32.\n      idx = tf.cast(\n          tf.reshape(\n              tf.where(tf.equal(n[\'beam_ids\'], beam_id)), [-1]), tf.int32)\n      beam_scores = tf.reshape(tf.gather(n[\'all_path_scores\'], idx), [1, -1])\n      num = tf.shape(idx)\n      return tf.nn.softmax_cross_entropy_with_logits(\n          beam_scores, tf.expand_dims(\n              tf.sparse_to_dense(beam_gold_slot, num, [1.], 0.), 0))\n    # The conditional here is needed to deal with the last few batches of the\n    # corpus which can contain -1 in beam_gold_slot for empty batch slots.\n    cross_entropies.append(cf.cond(\n        beam_gold_slot[0] >= 0, _ComputeCrossEntropy, _Pass))\n  return {\'cross_entropy\': tf.div(tf.add_n(cross_entropies), batch_size)}\n\n\nclass StructuredGraphBuilder(graph_builder.GreedyParser):\n  """"""Extends the standard GreedyParser with a CRF objective using a beam.\n\n  The constructor takes two additional keyword arguments.\n  beam_size: the maximum size the beam can grow to.\n  max_steps: the maximum number of steps in any particular beam.\n\n  The model supports batch training with the batch_size argument to the\n  AddTraining method.\n  """"""\n\n  def __init__(self, *args, **kwargs):\n    self._beam_size = kwargs.pop(\'beam_size\', 10)\n    self._max_steps = kwargs.pop(\'max_steps\', 25)\n    super(StructuredGraphBuilder, self).__init__(*args, **kwargs)\n\n  def _AddBeamReader(self,\n                     task_context,\n                     batch_size,\n                     corpus_name,\n                     until_all_final=False,\n                     always_start_new_sentences=False):\n    """"""Adds an op capable of reading sentences and parsing them with a beam.""""""\n    features, state, epochs = gen_parser_ops.beam_parse_reader(\n        task_context=task_context,\n        feature_size=self._feature_size,\n        beam_size=self._beam_size,\n        batch_size=batch_size,\n        corpus_name=corpus_name,\n        allow_feature_weights=self._allow_feature_weights,\n        arg_prefix=self._arg_prefix,\n        continue_until_all_final=until_all_final,\n        always_start_new_sentences=always_start_new_sentences)\n    return {\'state\': state, \'features\': features, \'epochs\': epochs}\n\n  def _BuildSequence(self,\n                     batch_size,\n                     max_steps,\n                     features,\n                     state,\n                     use_average=False):\n    """"""Adds a sequence of beam parsing steps.""""""\n    def Advance(state, step, scores_array, alive, alive_steps, *features):\n      scores = self._BuildNetwork(features,\n                                  return_average=use_average)[\'logits\']\n      scores_array = scores_array.write(step, scores)\n      features, state, alive = (\n          gen_parser_ops.beam_parser(state, scores, self._feature_size))\n      return [state, step + 1, scores_array, alive, alive_steps + tf.cast(\n          alive, tf.int32)] + list(features)\n\n    # args: (state, step, scores_array, alive, alive_steps, *features)\n    def KeepGoing(*args):\n      return tf.logical_and(args[1] < max_steps, tf.reduce_any(args[3]))\n\n    step = tf.constant(0, tf.int32, [])\n    scores_array = tensor_array_ops.TensorArray(dtype=tf.float32,\n                                                size=0,\n                                                dynamic_size=True)\n    alive = tf.constant(True, tf.bool, [batch_size])\n    alive_steps = tf.constant(0, tf.int32, [batch_size])\n    t = tf.while_loop(\n        KeepGoing,\n        Advance,\n        [state, step, scores_array, alive, alive_steps] + list(features),\n        shape_invariants=[tf.TensorShape(None)] * (len(features) + 5),\n        parallel_iterations=100)\n\n    # Link to the final nodes/values of ops that have passed through While:\n    return {\'state\': t[0],\n            \'concat_scores\': t[2].concat(),\n            \'alive\': t[3],\n            \'alive_steps\': t[4]}\n\n  def AddTraining(self,\n                  task_context,\n                  batch_size,\n                  learning_rate=0.1,\n                  decay_steps=4000,\n                  momentum=None,\n                  corpus_name=\'documents\'):\n    with tf.name_scope(\'training\'):\n      n = self.training\n      n[\'accumulated_alive_steps\'] = self._AddVariable(\n          [batch_size], tf.int32, \'accumulated_alive_steps\',\n          tf.zeros_initializer)\n      n.update(self._AddBeamReader(task_context, batch_size, corpus_name))\n      # This adds a required \'step\' node too:\n      learning_rate = tf.constant(learning_rate, dtype=tf.float32)\n      n[\'learning_rate\'] = self._AddLearningRate(learning_rate, decay_steps)\n      # Call BuildNetwork *only* to set up the params outside of the main loop.\n      self._BuildNetwork(list(n[\'features\']))\n\n      n.update(self._BuildSequence(batch_size, self._max_steps, n[\'features\'],\n                                   n[\'state\']))\n\n      flat_concat_scores = tf.reshape(n[\'concat_scores\'], [-1])\n      (indices_and_paths, beams_and_slots, n[\'gold_slot\'], n[\n          \'beam_path_scores\']) = gen_parser_ops.beam_parser_output(n[\n              \'state\'])\n      n[\'indices\'] = tf.reshape(tf.gather(indices_and_paths, [0]), [-1])\n      n[\'path_ids\'] = tf.reshape(tf.gather(indices_and_paths, [1]), [-1])\n      n[\'all_path_scores\'] = tf.sparse_segment_sum(\n          flat_concat_scores, n[\'indices\'], n[\'path_ids\'])\n      n[\'beam_ids\'] = tf.reshape(tf.gather(beams_and_slots, [0]), [-1])\n      n.update(AddCrossEntropy(batch_size, n))\n\n      if self._only_train:\n        trainable_params = {k: v for k, v in self.params.iteritems()\n                            if k in self._only_train}\n      else:\n        trainable_params = self.params\n      for p in trainable_params:\n        tf.logging.info(\'trainable_param: %s\', p)\n\n      regularized_params = [\n          tf.nn.l2_loss(p) for k, p in trainable_params.iteritems()\n          if k.startswith(\'weights\') or k.startswith(\'bias\')]\n      l2_loss = 1e-4 * tf.add_n(regularized_params) if regularized_params else 0\n\n      n[\'cost\'] = tf.add(n[\'cross_entropy\'], l2_loss, name=\'cost\')\n\n      n[\'gradients\'] = tf.gradients(n[\'cost\'], trainable_params.values())\n\n      with tf.control_dependencies([n[\'alive_steps\']]):\n        update_accumulators = tf.group(\n            tf.assign_add(n[\'accumulated_alive_steps\'], n[\'alive_steps\']))\n\n      def ResetAccumulators():\n        return tf.assign(\n            n[\'accumulated_alive_steps\'], tf.zeros([batch_size], tf.int32))\n      n[\'reset_accumulators_func\'] = ResetAccumulators\n\n      optimizer = tf.train.MomentumOptimizer(n[\'learning_rate\'],\n                                             momentum,\n                                             use_locking=self._use_locking)\n      train_op = optimizer.minimize(n[\'cost\'],\n                                    var_list=trainable_params.values())\n      for param in trainable_params.values():\n        slot = optimizer.get_slot(param, \'momentum\')\n        self.inits[slot.name] = state_ops.init_variable(slot,\n                                                        tf.zeros_initializer)\n        self.variables[slot.name] = slot\n\n      def NumericalChecks():\n        return tf.group(*[\n            tf.check_numerics(param, message=\'Parameter is not finite.\')\n            for param in trainable_params.values()\n            if param.dtype.base_dtype in [tf.float32, tf.float64]])\n      check_op = cf.cond(tf.equal(tf.mod(self.GetStep(), self._check_every), 0),\n                         NumericalChecks, tf.no_op)\n      avg_update_op = tf.group(*self._averaging.values())\n      train_ops = [train_op]\n      if self._check_parameters:\n        train_ops.append(check_op)\n      if self._use_averaging:\n        train_ops.append(avg_update_op)\n      with tf.control_dependencies([update_accumulators]):\n        n[\'train_op\'] = tf.group(*train_ops, name=\'train_op\')\n      n[\'alive_steps\'] = tf.identity(n[\'alive_steps\'], name=\'alive_steps\')\n    return n\n\n  def AddEvaluation(self,\n                    task_context,\n                    batch_size,\n                    evaluation_max_steps=300,\n                    corpus_name=None):\n    with tf.name_scope(\'evaluation\'):\n      n = self.evaluation\n      n.update(self._AddBeamReader(task_context,\n                                   batch_size,\n                                   corpus_name,\n                                   until_all_final=True,\n                                   always_start_new_sentences=True))\n      self._BuildNetwork(\n          list(n[\'features\']),\n          return_average=self._use_averaging)\n      n.update(self._BuildSequence(batch_size, evaluation_max_steps, n[\n          \'features\'], n[\'state\'], use_average=self._use_averaging))\n      n[\'eval_metrics\'], n[\'documents\'] = (\n          gen_parser_ops.beam_eval_output(n[\'state\']))\n    return n\n'"
model_zoo/models/syntaxnet/syntaxnet/text_formats_test.py,0,"b'# coding=utf-8\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for english_tokenizer.""""""\n\n\n# disable=no-name-in-module,unused-import,g-bad-import-order,maybe-no-member\nimport os.path\nimport tensorflow as tf\n\nimport syntaxnet.load_parser_ops\n\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.platform import googletest\nfrom tensorflow.python.platform import tf_logging as logging\n\nfrom syntaxnet import sentence_pb2\nfrom syntaxnet import task_spec_pb2\nfrom syntaxnet.ops import gen_parser_ops\n\nFLAGS = tf.app.flags.FLAGS\n\n\nclass TextFormatsTest(test_util.TensorFlowTestCase):\n\n  def setUp(self):\n    if not hasattr(FLAGS, \'test_srcdir\'):\n      FLAGS.test_srcdir = \'\'\n    if not hasattr(FLAGS, \'test_tmpdir\'):\n      FLAGS.test_tmpdir = tf.test.get_temp_dir()\n    self.corpus_file = os.path.join(FLAGS.test_tmpdir, \'documents.conll\')\n    self.context_file = os.path.join(FLAGS.test_tmpdir, \'context.pbtxt\')\n\n  def AddInput(self, name, file_pattern, record_format, context):\n    inp = context.input.add()\n    inp.name = name\n    inp.record_format.append(record_format)\n    inp.part.add().file_pattern = file_pattern\n\n  def AddParameter(self, name, value, context):\n    param = context.parameter.add()\n    param.name = name\n    param.value = value\n\n  def WriteContext(self, corpus_format):\n    context = task_spec_pb2.TaskSpec()\n    self.AddInput(\'documents\', self.corpus_file, corpus_format, context)\n    for name in (\'word-map\', \'lcword-map\', \'tag-map\',\n                 \'category-map\', \'label-map\', \'prefix-table\',\n                 \'suffix-table\', \'tag-to-category\'):\n      self.AddInput(name, os.path.join(FLAGS.test_tmpdir, name), \'\', context)\n    logging.info(\'Writing context to: %s\', self.context_file)\n    with open(self.context_file, \'w\') as f:\n      f.write(str(context))\n\n  def ReadNextDocument(self, sess, sentence):\n    sentence_str, = sess.run([sentence])\n    if sentence_str:\n      sentence_doc = sentence_pb2.Sentence()\n      sentence_doc.ParseFromString(sentence_str[0])\n    else:\n      sentence_doc = None\n    return sentence_doc\n\n  def CheckTokenization(self, sentence, tokenization):\n    self.WriteContext(\'english-text\')\n    logging.info(\'Writing text file to: %s\', self.corpus_file)\n    with open(self.corpus_file, \'w\') as f:\n      f.write(sentence)\n    sentence, _ = gen_parser_ops.document_source(\n        self.context_file, batch_size=1)\n    with self.test_session() as sess:\n      sentence_doc = self.ReadNextDocument(sess, sentence)\n      self.assertEqual(\' \'.join([t.word for t in sentence_doc.token]),\n                       tokenization)\n\n  def CheckUntokenizedDoc(self, sentence, words, starts, ends):\n    self.WriteContext(\'untokenized-text\')\n    logging.info(\'Writing text file to: %s\', self.corpus_file)\n    with open(self.corpus_file, \'w\') as f:\n      f.write(sentence)\n    sentence, _ = gen_parser_ops.document_source(\n        self.context_file, batch_size=1)\n    with self.test_session() as sess:\n      sentence_doc = self.ReadNextDocument(sess, sentence)\n      self.assertEqual(len(sentence_doc.token), len(words))\n      self.assertEqual(len(sentence_doc.token), len(starts))\n      self.assertEqual(len(sentence_doc.token), len(ends))\n      for i, token in enumerate(sentence_doc.token):\n        self.assertEqual(token.word.encode(\'utf-8\'), words[i])\n        self.assertEqual(token.start, starts[i])\n        self.assertEqual(token.end, ends[i])\n\n  def testUntokenized(self):\n    self.CheckUntokenizedDoc(\'\xe4\xb8\x80\xe4\xb8\xaa\xe6\xb5\x8b\xe8\xaf\x95\', [\'\xe4\xb8\x80\', \'\xe4\xb8\xaa\', \'\xe6\xb5\x8b\', \'\xe8\xaf\x95\'],\n                             [0, 3, 6, 9], [2, 5, 8, 11])\n    self.CheckUntokenizedDoc(\'Hello \', [\'H\', \'e\', \'l\', \'l\', \'o\', \' \'],\n                             [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5])\n\n  def testSegmentationTrainingData(self):\n    doc1_lines = [\'\xe6\xb5\x8b\xe8\xaf\x95\tNO_SPACE\\n\',\n                  \'\xe7\x9a\x84\tNO_SPACE\\n\',\n                  \'\xe5\x8f\xa5\xe5\xad\x90\tNO_SPACE\']\n    doc1_text = \'\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\'\n    doc1_tokens = [\'\xe6\xb5\x8b\', \'\xe8\xaf\x95\', \'\xe7\x9a\x84\', \'\xe5\x8f\xa5\', \'\xe5\xad\x90\']\n    doc1_break_levles = [1, 0, 1, 1, 0]\n    doc2_lines = [\'That\tNO_SPACE\\n\',\n                  \'\\\'s\tSPACE\\n\',\n                  \'a\tSPACE\\n\',\n                  \'good\tSPACE\\n\',\n                  \'point\tNO_SPACE\\n\',\n                  \'.\tNO_SPACE\']\n    doc2_text = \'That\\\'s a good point.\'\n    doc2_tokens = [\'T\', \'h\', \'a\', \'t\', \'\\\'\', \'s\', \' \', \'a\', \' \', \'g\', \'o\', \'o\',\n                   \'d\', \' \', \'p\', \'o\', \'i\', \'n\', \'t\', \'.\']\n    doc2_break_levles = [1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n                         0, 1]\n    self.CheckSegmentationTrainingData(doc1_lines, doc1_text, doc1_tokens,\n                                       doc1_break_levles)\n    self.CheckSegmentationTrainingData(doc2_lines, doc2_text, doc2_tokens,\n                                       doc2_break_levles)\n\n  def CheckSegmentationTrainingData(self, doc_lines, doc_text, doc_words,\n                                    break_levels):\n    # Prepare context.\n    self.WriteContext(\'segment-train-data\')\n\n    # Prepare test sentence.\n    with open(self.corpus_file, \'w\') as f:\n      f.write(\'\'.join(doc_lines))\n\n    # Test converted sentence.\n    sentence, _ = gen_parser_ops.document_source(\n        self.context_file, batch_size=1)\n    with self.test_session() as sess:\n      sentence_doc = self.ReadNextDocument(sess, sentence)\n      self.assertEqual(doc_text.decode(\'utf-8\'), sentence_doc.text)\n      self.assertEqual([t.decode(\'utf-8\') for t in doc_words],\n                       [t.word for t in sentence_doc.token])\n      self.assertEqual(break_levels,\n                       [t.break_level for t in sentence_doc.token])\n\n  def testSimple(self):\n    self.CheckTokenization(\'Hello, world!\', \'Hello , world !\')\n    self.CheckTokenization(\'""Hello""\', ""`` Hello \'\'"")\n    self.CheckTokenization(\'{""Hello@#$\', \'-LRB- `` Hello @ # $\')\n    self.CheckTokenization(\'""Hello...""\', ""`` Hello ... \'\'"")\n    self.CheckTokenization(\'()[]{}<>\',\n                           \'-LRB- -RRB- -LRB- -RRB- -LRB- -RRB- < >\')\n    self.CheckTokenization(\'Hello--world\', \'Hello -- world\')\n    self.CheckTokenization(""Isn\'t"", ""Is n\'t"")\n    self.CheckTokenization(""n\'t"", ""n\'t"")\n    self.CheckTokenization(\'Hello Mr. Smith.\', \'Hello Mr. Smith .\')\n    self.CheckTokenization(""It\'s Mr. Smith\'s."", ""It \'s Mr. Smith \'s ."")\n    self.CheckTokenization(""It\'s the Smiths\'."", ""It \'s the Smiths \' ."")\n    self.CheckTokenization(\'Gotta go\', \'Got ta go\')\n    self.CheckTokenization(\'50-year-old\', \'50-year-old\')\n\n  def testUrl(self):\n    self.CheckTokenization(\'http://www.google.com/news is down\',\n                           \'http : //www.google.com/news is down\')\n\n\nif __name__ == \'__main__\':\n  googletest.main()\n'"
model_zoo/models/differential_privacy/dp_sgd/dp_mnist/dp_mnist.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Example differentially private trainer and evaluator for MNIST.\n""""""\nfrom __future__ import division\n\nimport json\nimport os\nimport sys\nimport time\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom differential_privacy.dp_sgd.dp_optimizer import dp_optimizer\nfrom differential_privacy.dp_sgd.dp_optimizer import dp_pca\nfrom differential_privacy.dp_sgd.dp_optimizer import sanitizer\nfrom differential_privacy.dp_sgd.dp_optimizer import utils\nfrom differential_privacy.privacy_accountant.tf import accountant\n\n# parameters for the training\ntf.flags.DEFINE_integer(""batch_size"", 600,\n                        ""The training batch size."")\ntf.flags.DEFINE_integer(""batches_per_lot"", 1,\n                        ""Number of batches per lot."")\n# Together, batch_size and batches_per_lot determine lot_size.\ntf.flags.DEFINE_integer(""num_training_steps"", 50000,\n                        ""The number of training steps.""\n                        ""This counts number of lots."")\n\ntf.flags.DEFINE_bool(""randomize"", True,\n                     ""If true, randomize the input data; otherwise use a fixed ""\n                     ""seed and non-randomized input."")\ntf.flags.DEFINE_bool(""freeze_bottom_layers"", False,\n                     ""If true, only train on the logit layer."")\ntf.flags.DEFINE_bool(""save_mistakes"", False,\n                     ""If true, save the mistakes made during testing."")\ntf.flags.DEFINE_float(""lr"", 0.05, ""start learning rate"")\ntf.flags.DEFINE_float(""end_lr"", 0.05, ""end learning rate"")\ntf.flags.DEFINE_float(""lr_saturate_epochs"", 0,\n                      ""learning rate saturate epochs; set to 0 for a constant ""\n                      ""learning rate of --lr."")\n\n# For searching parameters\ntf.flags.DEFINE_integer(""projection_dimensions"", 60,\n                        ""PCA projection dimensions, or 0 for no projection."")\ntf.flags.DEFINE_integer(""num_hidden_layers"", 1,\n                        ""Number of hidden layers in the network"")\ntf.flags.DEFINE_integer(""hidden_layer_num_units"", 1000,\n                        ""Number of units per hidden layer"")\ntf.flags.DEFINE_float(""default_gradient_l2norm_bound"", 4.0, ""norm clipping"")\ntf.flags.DEFINE_integer(""num_conv_layers"", 0,\n                        ""Number of convolutional layers to use."")\n\ntf.flags.DEFINE_string(""training_data_path"",\n                       ""/tmp/mnist/mnist_train.tfrecord"",\n                       ""Location of the training data."")\ntf.flags.DEFINE_string(""eval_data_path"",\n                       ""/tmp/mnist/mnist_test.tfrecord"",\n                       ""Location of the eval data."")\ntf.flags.DEFINE_integer(""eval_steps"", 10,\n                        ""Evaluate the model every eval_steps"")\n\n# Parameters for privacy spending. We allow linearly varying eps during\n# training.\ntf.flags.DEFINE_string(""accountant_type"", ""Moments"", ""Moments, Amortized."")\n\n# Flags that control privacy spending during training.\ntf.flags.DEFINE_float(""eps"", 1.0,\n                      ""Start privacy spending for one epoch of training, ""\n                      ""used if accountant_type is Amortized."")\ntf.flags.DEFINE_float(""end_eps"", 1.0,\n                      ""End privacy spending for one epoch of training, ""\n                      ""used if accountant_type is Amortized."")\ntf.flags.DEFINE_float(""eps_saturate_epochs"", 0,\n                      ""Stop varying epsilon after eps_saturate_epochs. Set to ""\n                      ""0 for constant eps of --eps. ""\n                      ""Used if accountant_type is Amortized."")\ntf.flags.DEFINE_float(""delta"", 1e-5,\n                      ""Privacy spending for training. Constant through ""\n                      ""training, used if accountant_type is Amortized."")\ntf.flags.DEFINE_float(""sigma"", 4.0,\n                      ""Noise sigma, used only if accountant_type is Moments"")\n\n\n# Flags that control privacy spending for the pca projection\n# (only used if --projection_dimensions > 0).\ntf.flags.DEFINE_float(""pca_eps"", 0.5,\n                      ""Privacy spending for PCA, used if accountant_type is ""\n                      ""Amortized."")\ntf.flags.DEFINE_float(""pca_delta"", 0.005,\n                      ""Privacy spending for PCA, used if accountant_type is ""\n                      ""Amortized."")\n\ntf.flags.DEFINE_float(""pca_sigma"", 7.0,\n                      ""Noise sigma for PCA, used if accountant_type is Moments"")\n\ntf.flags.DEFINE_string(""target_eps"", ""0.125,0.25,0.5,1,2,4,8"",\n                       ""Log the privacy loss for the target epsilon\'s. Only ""\n                       ""used when accountant_type is Moments."")\ntf.flags.DEFINE_float(""target_delta"", 1e-5,\n                      ""Maximum delta for --terminate_based_on_privacy."")\ntf.flags.DEFINE_bool(""terminate_based_on_privacy"", False,\n                     ""Stop training if privacy spent exceeds ""\n                     ""(max(--target_eps), --target_delta), even ""\n                     ""if --num_training_steps have not yet been completed."")\n\ntf.flags.DEFINE_string(""save_path"", ""/tmp/mnist_dir"",\n                       ""Directory for saving model outputs."")\n\nFLAGS = tf.flags.FLAGS\nNUM_TRAINING_IMAGES = 60000\nNUM_TESTING_IMAGES = 10000\nIMAGE_SIZE = 28\n\n\ndef MnistInput(mnist_data_file, batch_size, randomize):\n  """"""Create operations to read the MNIST input file.\n\n  Args:\n    mnist_data_file: Path of a file containing the MNIST images to process.\n    batch_size: size of the mini batches to generate.\n    randomize: If true, randomize the dataset.\n\n  Returns:\n    images: A tensor with the formatted image data. shape [batch_size, 28*28]\n    labels: A tensor with the labels for each image.  shape [batch_size]\n  """"""\n  file_queue = tf.train.string_input_producer([mnist_data_file])\n  reader = tf.TFRecordReader()\n  _, value = reader.read(file_queue)\n  example = tf.parse_single_example(\n      value,\n      features={""image/encoded"": tf.FixedLenFeature(shape=(), dtype=tf.string),\n                ""image/class/label"": tf.FixedLenFeature([1], tf.int64)})\n\n  image = tf.cast(tf.image.decode_png(example[""image/encoded""], channels=1),\n                  tf.float32)\n  image = tf.reshape(image, [IMAGE_SIZE * IMAGE_SIZE])\n  image /= 255\n  label = tf.cast(example[""image/class/label""], dtype=tf.int32)\n  label = tf.reshape(label, [])\n\n  if randomize:\n    images, labels = tf.train.shuffle_batch(\n        [image, label], batch_size=batch_size,\n        capacity=(batch_size * 100),\n        min_after_dequeue=(batch_size * 10))\n  else:\n    images, labels = tf.train.batch([image, label], batch_size=batch_size)\n\n  return images, labels\n\n\ndef Eval(mnist_data_file, network_parameters, num_testing_images,\n         randomize, load_path, save_mistakes=False):\n  """"""Evaluate MNIST for a number of steps.\n\n  Args:\n    mnist_data_file: Path of a file containing the MNIST images to process.\n    network_parameters: parameters for defining and training the network.\n    num_testing_images: the number of images we will evaluate on.\n    randomize: if false, randomize; otherwise, read the testing images\n      sequentially.\n    load_path: path where to load trained parameters from.\n    save_mistakes: save the mistakes if True.\n\n  Returns:\n    The evaluation accuracy as a float.\n  """"""\n  batch_size = 100\n  # Like for training, we need a session for executing the TensorFlow graph.\n  with tf.Graph().as_default(), tf.Session() as sess:\n    # Create the basic Mnist model.\n    images, labels = MnistInput(mnist_data_file, batch_size, randomize)\n    logits, _, _ = utils.BuildNetwork(images, network_parameters)\n    softmax = tf.nn.softmax(logits)\n\n    # Load the variables.\n    ckpt_state = tf.train.get_checkpoint_state(load_path)\n    if not (ckpt_state and ckpt_state.model_checkpoint_path):\n      raise ValueError(""No model checkpoint to eval at %s\\n"" % load_path)\n\n    saver = tf.train.Saver()\n    saver.restore(sess, ckpt_state.model_checkpoint_path)\n    coord = tf.train.Coordinator()\n    _ = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    total_examples = 0\n    correct_predictions = 0\n    image_index = 0\n    mistakes = []\n    for _ in xrange((num_testing_images + batch_size - 1) // batch_size):\n      predictions, label_values = sess.run([softmax, labels])\n\n      # Count how many were predicted correctly.\n      for prediction, label_value in zip(predictions, label_values):\n        total_examples += 1\n        if np.argmax(prediction) == label_value:\n          correct_predictions += 1\n        elif save_mistakes:\n          mistakes.append({""index"": image_index,\n                           ""label"": label_value,\n                           ""pred"": np.argmax(prediction)})\n        image_index += 1\n\n  return (correct_predictions / total_examples,\n          mistakes if save_mistakes else None)\n\n\ndef Train(mnist_train_file, mnist_test_file, network_parameters, num_steps,\n          save_path, eval_steps=0):\n  """"""Train MNIST for a number of steps.\n\n  Args:\n    mnist_train_file: path of MNIST train data file.\n    mnist_test_file: path of MNIST test data file.\n    network_parameters: parameters for defining and training the network.\n    num_steps: number of steps to run. Here steps = lots\n    save_path: path where to save trained parameters.\n    eval_steps: evaluate the model every eval_steps.\n\n  Returns:\n    the result after the final training step.\n\n  Raises:\n    ValueError: if the accountant_type is not supported.\n  """"""\n  batch_size = FLAGS.batch_size\n\n  params = {""accountant_type"": FLAGS.accountant_type,\n            ""task_id"": 0,\n            ""batch_size"": FLAGS.batch_size,\n            ""projection_dimensions"": FLAGS.projection_dimensions,\n            ""default_gradient_l2norm_bound"":\n            network_parameters.default_gradient_l2norm_bound,\n            ""num_hidden_layers"": FLAGS.num_hidden_layers,\n            ""hidden_layer_num_units"": FLAGS.hidden_layer_num_units,\n            ""num_examples"": NUM_TRAINING_IMAGES,\n            ""learning_rate"": FLAGS.lr,\n            ""end_learning_rate"": FLAGS.end_lr,\n            ""learning_rate_saturate_epochs"": FLAGS.lr_saturate_epochs\n           }\n  # Log different privacy parameters dependent on the accountant type.\n  if FLAGS.accountant_type == ""Amortized"":\n    params.update({""flag_eps"": FLAGS.eps,\n                   ""flag_delta"": FLAGS.delta,\n                   ""flag_pca_eps"": FLAGS.pca_eps,\n                   ""flag_pca_delta"": FLAGS.pca_delta,\n                  })\n  elif FLAGS.accountant_type == ""Moments"":\n    params.update({""sigma"": FLAGS.sigma,\n                   ""pca_sigma"": FLAGS.pca_sigma,\n                  })\n\n  with tf.Graph().as_default(), tf.Session() as sess, tf.device(\'/cpu:0\'):\n    # Create the basic Mnist model.\n    images, labels = MnistInput(mnist_train_file, batch_size, FLAGS.randomize)\n\n    logits, projection, training_params = utils.BuildNetwork(\n        images, network_parameters)\n\n    cost = tf.nn.softmax_cross_entropy_with_logits(\n        logits, tf.one_hot(labels, 10))\n\n    # The actual cost is the average across the examples.\n    cost = tf.reduce_sum(cost, [0]) / batch_size\n\n    if FLAGS.accountant_type == ""Amortized"":\n      priv_accountant = accountant.AmortizedAccountant(NUM_TRAINING_IMAGES)\n      sigma = None\n      pca_sigma = None\n      with_privacy = FLAGS.eps > 0\n    elif FLAGS.accountant_type == ""Moments"":\n      priv_accountant = accountant.GaussianMomentsAccountant(\n          NUM_TRAINING_IMAGES)\n      sigma = FLAGS.sigma\n      pca_sigma = FLAGS.pca_sigma\n      with_privacy = FLAGS.sigma > 0\n    else:\n      raise ValueError(""Undefined accountant type, needs to be ""\n                       ""Amortized or Moments, but got %s"" % FLAGS.accountant)\n    # Note: Here and below, we scale down the l2norm_bound by\n    # batch_size. This is because per_example_gradients computes the\n    # gradient of the minibatch loss with respect to each individual\n    # example, and the minibatch loss (for our model) is the *average*\n    # loss over examples in the minibatch. Hence, the scale of the\n    # per-example gradients goes like 1 / batch_size.\n    gaussian_sanitizer = sanitizer.AmortizedGaussianSanitizer(\n        priv_accountant,\n        [network_parameters.default_gradient_l2norm_bound / batch_size, True])\n\n    for var in training_params:\n      if ""gradient_l2norm_bound"" in training_params[var]:\n        l2bound = training_params[var][""gradient_l2norm_bound""] / batch_size\n        gaussian_sanitizer.set_option(var,\n                                      sanitizer.ClipOption(l2bound, True))\n    lr = tf.placeholder(tf.float32)\n    eps = tf.placeholder(tf.float32)\n    delta = tf.placeholder(tf.float32)\n\n    init_ops = []\n    if network_parameters.projection_type == ""PCA"":\n      with tf.variable_scope(""pca""):\n        # Compute differentially private PCA.\n        all_data, _ = MnistInput(mnist_train_file, NUM_TRAINING_IMAGES, False)\n        pca_projection = dp_pca.ComputeDPPrincipalProjection(\n            all_data, network_parameters.projection_dimensions,\n            gaussian_sanitizer, [FLAGS.pca_eps, FLAGS.pca_delta], pca_sigma)\n        assign_pca_proj = tf.assign(projection, pca_projection)\n        init_ops.append(assign_pca_proj)\n\n    # Add global_step\n    global_step = tf.Variable(0, dtype=tf.int32, trainable=False,\n                              name=""global_step"")\n\n    if with_privacy:\n      gd_op = dp_optimizer.DPGradientDescentOptimizer(\n          lr,\n          [eps, delta],\n          gaussian_sanitizer,\n          sigma=sigma,\n          batches_per_lot=FLAGS.batches_per_lot).minimize(\n              cost, global_step=global_step)\n    else:\n      gd_op = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n\n    saver = tf.train.Saver()\n    coord = tf.train.Coordinator()\n    _ = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    # We need to maintain the intialization sequence.\n    for v in tf.trainable_variables():\n      sess.run(tf.initialize_variables([v]))\n    sess.run(tf.initialize_all_variables())\n    sess.run(init_ops)\n\n    results = []\n    start_time = time.time()\n    prev_time = start_time\n    filename = ""results-0.json""\n    log_path = os.path.join(save_path, filename)\n\n    target_eps = [float(s) for s in FLAGS.target_eps.split("","")]\n    if FLAGS.accountant_type == ""Amortized"":\n      # Only matters if --terminate_based_on_privacy is true.\n      target_eps = [max(target_eps)]\n    max_target_eps = max(target_eps)\n\n    lot_size = FLAGS.batches_per_lot * FLAGS.batch_size\n    lots_per_epoch = NUM_TRAINING_IMAGES / lot_size\n    for step in xrange(num_steps):\n      epoch = step / lots_per_epoch\n      curr_lr = utils.VaryRate(FLAGS.lr, FLAGS.end_lr,\n                               FLAGS.lr_saturate_epochs, epoch)\n      curr_eps = utils.VaryRate(FLAGS.eps, FLAGS.end_eps,\n                                FLAGS.eps_saturate_epochs, epoch)\n      for _ in xrange(FLAGS.batches_per_lot):\n        _ = sess.run(\n            [gd_op], feed_dict={lr: curr_lr, eps: curr_eps, delta: FLAGS.delta})\n      sys.stderr.write(""step: %d\\n"" % step)\n\n      # See if we should stop training due to exceeded privacy budget:\n      should_terminate = False\n      terminate_spent_eps_delta = None\n      if with_privacy and FLAGS.terminate_based_on_privacy:\n        terminate_spent_eps_delta = priv_accountant.get_privacy_spent(\n            sess, target_eps=[max_target_eps])[0]\n        # For the Moments accountant, we should always have\n        # spent_eps == max_target_eps.\n        if (terminate_spent_eps_delta.spent_delta > FLAGS.target_delta or\n            terminate_spent_eps_delta.spent_eps > max_target_eps):\n          should_terminate = True\n\n      if (eval_steps > 0 and (step + 1) % eval_steps == 0) or should_terminate:\n        if with_privacy:\n          spent_eps_deltas = priv_accountant.get_privacy_spent(\n              sess, target_eps=target_eps)\n        else:\n          spent_eps_deltas = [accountant.EpsDelta(0, 0)]\n        for spent_eps, spent_delta in spent_eps_deltas:\n          sys.stderr.write(""spent privacy: eps %.4f delta %.5g\\n"" % (\n              spent_eps, spent_delta))\n\n        saver.save(sess, save_path=save_path + ""/ckpt"")\n        train_accuracy, _ = Eval(mnist_train_file, network_parameters,\n                                 num_testing_images=NUM_TESTING_IMAGES,\n                                 randomize=True, load_path=save_path)\n        sys.stderr.write(""train_accuracy: %.2f\\n"" % train_accuracy)\n        test_accuracy, mistakes = Eval(mnist_test_file, network_parameters,\n                                       num_testing_images=NUM_TESTING_IMAGES,\n                                       randomize=False, load_path=save_path,\n                                       save_mistakes=FLAGS.save_mistakes)\n        sys.stderr.write(""eval_accuracy: %.2f\\n"" % test_accuracy)\n\n        curr_time = time.time()\n        elapsed_time = curr_time - prev_time\n        prev_time = curr_time\n\n        results.append({""step"": step+1,  # Number of lots trained so far.\n                        ""elapsed_secs"": elapsed_time,\n                        ""spent_eps_deltas"": spent_eps_deltas,\n                        ""train_accuracy"": train_accuracy,\n                        ""test_accuracy"": test_accuracy,\n                        ""mistakes"": mistakes})\n        loginfo = {""elapsed_secs"": curr_time-start_time,\n                   ""spent_eps_deltas"": spent_eps_deltas,\n                   ""train_accuracy"": train_accuracy,\n                   ""test_accuracy"": test_accuracy,\n                   ""num_training_steps"": step+1,  # Steps so far.\n                   ""mistakes"": mistakes,\n                   ""result_series"": results}\n        loginfo.update(params)\n        if log_path:\n          with tf.gfile.Open(log_path, ""w"") as f:\n            json.dump(loginfo, f, indent=2)\n            f.write(""\\n"")\n            f.close()\n\n      if should_terminate:\n        break\n\n\ndef main(_):\n  network_parameters = utils.NetworkParameters()\n\n  # If the ASCII proto isn\'t specified, then construct a config protobuf based\n  # on 3 flags.\n  network_parameters.input_size = IMAGE_SIZE ** 2\n  network_parameters.default_gradient_l2norm_bound = (\n      FLAGS.default_gradient_l2norm_bound)\n  if FLAGS.projection_dimensions > 0 and FLAGS.num_conv_layers > 0:\n    raise ValueError(""Currently you can\'t do PCA and have convolutions""\n                     ""at the same time. Pick one"")\n\n    # could add support for PCA after convolutions.\n    # Currently BuildNetwork can build the network with conv followed by\n    # projection, but the PCA training works on data, rather than data run\n    # through a few layers. Will need to init the convs before running the\n    # PCA, and need to change the PCA subroutine to take a network and perhaps\n    # allow for batched inputs, to handle larger datasets.\n  if FLAGS.num_conv_layers > 0:\n    conv = utils.ConvParameters()\n    conv.name = ""conv1""\n    conv.in_channels = 1\n    conv.out_channels = 128\n    conv.num_outputs = 128 * 14 * 14\n    network_parameters.conv_parameters.append(conv)\n    # defaults for the rest: 5x5,stride 1, relu, maxpool 2x2,stride 2.\n    # insize 28x28, bias, stddev 0.1, non-trainable.\n  if FLAGS.num_conv_layers > 1:\n    conv = network_parameters.ConvParameters()\n    conv.name = ""conv2""\n    conv.in_channels = 128\n    conv.out_channels = 128\n    conv.num_outputs = 128 * 7 * 7\n    conv.in_size = 14\n    # defaults for the rest: 5x5,stride 1, relu, maxpool 2x2,stride 2.\n    # bias, stddev 0.1, non-trainable.\n    network_parameters.conv_parameters.append(conv)\n\n  if FLAGS.num_conv_layers > 2:\n    raise ValueError(""Currently --num_conv_layers must be 0,1 or 2.""\n                     ""Manually create a network_parameters proto for more."")\n\n  if FLAGS.projection_dimensions > 0:\n    network_parameters.projection_type = ""PCA""\n    network_parameters.projection_dimensions = FLAGS.projection_dimensions\n  for i in xrange(FLAGS.num_hidden_layers):\n    hidden = utils.LayerParameters()\n    hidden.name = ""hidden%d"" % i\n    hidden.num_units = FLAGS.hidden_layer_num_units\n    hidden.relu = True\n    hidden.with_bias = False\n    hidden.trainable = not FLAGS.freeze_bottom_layers\n    network_parameters.layer_parameters.append(hidden)\n\n  logits = utils.LayerParameters()\n  logits.name = ""logits""\n  logits.num_units = 10\n  logits.relu = False\n  logits.with_bias = False\n  network_parameters.layer_parameters.append(logits)\n\n  Train(FLAGS.training_data_path,\n        FLAGS.eval_data_path,\n        network_parameters,\n        FLAGS.num_training_steps,\n        FLAGS.save_path,\n        eval_steps=FLAGS.eval_steps)\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
model_zoo/models/differential_privacy/dp_sgd/dp_optimizer/dp_optimizer.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Differentially private optimizers.\n""""""\nfrom __future__ import division\n\nimport tensorflow as tf\n\nfrom differential_privacy.dp_sgd.dp_optimizer import utils\nfrom differential_privacy.dp_sgd.per_example_gradients import per_example_gradients\n\n\nclass DPGradientDescentOptimizer(tf.train.GradientDescentOptimizer):\n  """"""Differentially private gradient descent optimizer.\n  """"""\n\n  def __init__(self, learning_rate, eps_delta, sanitizer,\n               sigma=None, use_locking=False, name=""DPGradientDescent"",\n               batches_per_lot=1):\n    """"""Construct a differentially private gradient descent optimizer.\n\n    The optimizer uses fixed privacy budget for each batch of training.\n\n    Args:\n      learning_rate: for GradientDescentOptimizer.\n      eps_delta: EpsDelta pair for each epoch.\n      sanitizer: for sanitizing the graident.\n      sigma: noise sigma. If None, use eps_delta pair to compute sigma;\n        otherwise use supplied sigma directly.\n      use_locking: use locking.\n      name: name for the object.\n      batches_per_lot: Number of batches in a lot.\n    """"""\n\n    super(DPGradientDescentOptimizer, self).__init__(learning_rate,\n                                                     use_locking, name)\n\n    # Also, if needed, define the gradient accumulators\n    self._batches_per_lot = batches_per_lot\n    self._grad_accum_dict = {}\n    if batches_per_lot > 1:\n      self._batch_count = tf.Variable(1, dtype=tf.int32, trainable=False,\n                                      name=""batch_count"")\n      var_list = tf.trainable_variables()\n      with tf.variable_scope(""grad_acc_for""):\n        for var in var_list:\n          v_grad_accum = tf.Variable(tf.zeros_like(var),\n                                     trainable=False,\n                                     name=utils.GetTensorOpName(var))\n          self._grad_accum_dict[var.name] = v_grad_accum\n\n    self._eps_delta = eps_delta\n    self._sanitizer = sanitizer\n    self._sigma = sigma\n\n  def compute_sanitized_gradients(self, loss, var_list=None,\n                                  add_noise=True):\n    """"""Compute the sanitized gradients.\n\n    Args:\n      loss: the loss tensor.\n      var_list: the optional variables.\n      add_noise: if true, then add noise. Always clip.\n    Returns:\n      a pair of (list of sanitized gradients) and privacy spending accumulation\n      operations.\n    Raises:\n      TypeError: if var_list contains non-variable.\n    """"""\n\n    self._assert_valid_dtypes([loss])\n\n    xs = [tf.convert_to_tensor(x) for x in var_list]\n    px_grads = per_example_gradients.PerExampleGradients(loss, xs)\n    sanitized_grads = []\n    for px_grad, v in zip(px_grads, var_list):\n      tensor_name = utils.GetTensorOpName(v)\n      sanitized_grad = self._sanitizer.sanitize(\n          px_grad, self._eps_delta, sigma=self._sigma,\n          tensor_name=tensor_name, add_noise=add_noise,\n          num_examples=self._batches_per_lot * tf.slice(\n              tf.shape(px_grad), [0], [1]))\n      sanitized_grads.append(sanitized_grad)\n\n    return sanitized_grads\n\n  def minimize(self, loss, global_step=None, var_list=None,\n               name=None):\n    """"""Minimize using sanitized gradients.\n\n    This gets a var_list which is the list of trainable variables.\n    For each var in var_list, we defined a grad_accumulator variable\n    during init. When batches_per_lot > 1, we accumulate the gradient\n    update in those. At the end of each lot, we apply the update back to\n    the variable. This has the effect that for each lot we compute\n    gradients at the point at the beginning of the lot, and then apply one\n    update at the end of the lot. In other words, semantically, we are doing\n    SGD with one lot being the equivalent of one usual batch of size\n    batch_size * batches_per_lot.\n    This allows us to simulate larger batches than our memory size would permit.\n\n    The lr and the num_steps are in the lot world.\n\n    Args:\n      loss: the loss tensor.\n      global_step: the optional global step.\n      var_list: the optional variables.\n      name: the optional name.\n    Returns:\n      the operation that runs one step of DP gradient descent.\n    """"""\n\n    # First validate the var_list\n\n    if var_list is None:\n      var_list = tf.trainable_variables()\n    for var in var_list:\n      if not isinstance(var, tf.Variable):\n        raise TypeError(""Argument is not a variable.Variable: %s"" % var)\n\n    # Modification: apply gradient once every batches_per_lot many steps.\n    # This may lead to smaller error\n\n    if self._batches_per_lot == 1:\n      sanitized_grads = self.compute_sanitized_gradients(\n          loss, var_list=var_list)\n\n      grads_and_vars = zip(sanitized_grads, var_list)\n      self._assert_valid_dtypes([v for g, v in grads_and_vars if g is not None])\n\n      apply_grads = self.apply_gradients(grads_and_vars,\n                                         global_step=global_step, name=name)\n      return apply_grads\n\n    # Condition for deciding whether to accumulate the gradient\n    # or actually apply it.\n    # we use a private self_batch_count to keep track of number of batches.\n    # global step will count number of lots processed.\n\n    update_cond = tf.equal(tf.constant(0),\n                           tf.mod(self._batch_count,\n                                  tf.constant(self._batches_per_lot)))\n\n    # Things to do for batches other than last of the lot.\n    # Add non-noisy clipped grads to shadow variables.\n\n    def non_last_in_lot_op(loss, var_list):\n      """"""Ops to do for a typical batch.\n\n      For a batch that is not the last one in the lot, we simply compute the\n      sanitized gradients and apply them to the grad_acc variables.\n\n      Args:\n        loss: loss function tensor\n        var_list: list of variables\n      Returns:\n        A tensorflow op to do the updates to the gradient accumulators\n      """"""\n      sanitized_grads = self.compute_sanitized_gradients(\n          loss, var_list=var_list, add_noise=False)\n\n      update_ops_list = []\n      for var, grad in zip(var_list, sanitized_grads):\n        grad_acc_v = self._grad_accum_dict[var.name]\n        update_ops_list.append(grad_acc_v.assign_add(grad))\n      update_ops_list.append(self._batch_count.assign_add(1))\n      return tf.group(*update_ops_list)\n\n    # Things to do for last batch of a lot.\n    # Add noisy clipped grads to accumulator.\n    # Apply accumulated grads to vars.\n\n    def last_in_lot_op(loss, var_list, global_step):\n      """"""Ops to do for last batch in a lot.\n\n      For the last batch in the lot, we first add the sanitized gradients to\n      the gradient acc variables, and then apply these\n      values over to the original variables (via an apply gradient)\n\n      Args:\n        loss: loss function tensor\n        var_list: list of variables\n        global_step: optional global step to be passed to apply_gradients\n      Returns:\n        A tensorflow op to push updates from shadow vars to real vars.\n      """"""\n\n      # We add noise in the last lot. This is why we need this code snippet\n      # that looks almost identical to the non_last_op case here.\n      sanitized_grads = self.compute_sanitized_gradients(\n          loss, var_list=var_list, add_noise=True)\n\n      normalized_grads = []\n      for var, grad in zip(var_list, sanitized_grads):\n        grad_acc_v = self._grad_accum_dict[var.name]\n        # To handle the lr difference per lot vs per batch, we divide the\n        # update by number of batches per lot.\n        normalized_grad = tf.div(grad_acc_v.assign_add(grad),\n                                 tf.to_float(self._batches_per_lot))\n\n        normalized_grads.append(normalized_grad)\n\n      with tf.control_dependencies(normalized_grads):\n        grads_and_vars = zip(normalized_grads, var_list)\n        self._assert_valid_dtypes(\n            [v for g, v in grads_and_vars if g is not None])\n        apply_san_grads = self.apply_gradients(grads_and_vars,\n                                               global_step=global_step,\n                                               name=""apply_grads"")\n\n      # Now reset the accumulators to zero\n      resets_list = []\n      with tf.control_dependencies([apply_san_grads]):\n        for _, acc in self._grad_accum_dict.items():\n          reset = tf.assign(acc, tf.zeros_like(acc))\n          resets_list.append(reset)\n      resets_list.append(self._batch_count.assign_add(1))\n\n      last_step_update = tf.group(*([apply_san_grads] + resets_list))\n      return last_step_update\n    # pylint: disable=g-long-lambda\n    update_op = tf.cond(update_cond,\n                        lambda: last_in_lot_op(\n                            loss, var_list,\n                            global_step),\n                        lambda: non_last_in_lot_op(\n                            loss, var_list))\n    return tf.group(update_op)\n'"
model_zoo/models/differential_privacy/dp_sgd/dp_optimizer/dp_pca.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Differentially private optimizers.\n""""""\nimport tensorflow as tf\n\nfrom differential_privacy.dp_sgd.dp_optimizer import sanitizer as san\n\n\ndef ComputeDPPrincipalProjection(data, projection_dims,\n                                 sanitizer, eps_delta, sigma):\n  """"""Compute differentially private projection.\n\n  Args:\n    data: the input data, each row is a data vector.\n    projection_dims: the projection dimension.\n    sanitizer: the sanitizer used for acheiving privacy.\n    eps_delta: (eps, delta) pair.\n    sigma: if not None, use noise sigma; otherwise compute it using\n      eps_delta pair.\n  Returns:\n    A projection matrix with projection_dims columns.\n  """"""\n\n  eps, delta = eps_delta\n  # Normalize each row.\n  normalized_data = tf.nn.l2_normalize(data, 1)\n  covar = tf.matmul(tf.transpose(normalized_data), normalized_data)\n  saved_shape = tf.shape(covar)\n  num_examples = tf.slice(tf.shape(data), [0], [1])\n  if eps > 0:\n    # Since the data is already normalized, there is no need to clip\n    # the covariance matrix.\n    assert delta > 0\n    saned_covar = sanitizer.sanitize(\n        tf.reshape(covar, [1, -1]), eps_delta, sigma=sigma,\n        option=san.ClipOption(1.0, False), num_examples=num_examples)\n    saned_covar = tf.reshape(saned_covar, saved_shape)\n    # Symmetrize saned_covar. This also reduces the noise variance.\n    saned_covar = 0.5 * (saned_covar + tf.transpose(saned_covar))\n  else:\n    saned_covar = covar\n\n  # Compute the eigen decomposition of the covariance matrix, and\n  # return the top projection_dims eigen vectors, represented as columns of\n  # the projection matrix.\n  eigvals, eigvecs = tf.self_adjoint_eig(saned_covar)\n  _, topk_indices = tf.nn.top_k(eigvals, projection_dims)\n  topk_indices = tf.reshape(topk_indices, [projection_dims])\n  # Gather and return the corresponding eigenvectors.\n  return tf.transpose(tf.gather(tf.transpose(eigvecs), topk_indices))\n'"
model_zoo/models/differential_privacy/dp_sgd/dp_optimizer/sanitizer.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Defines Sanitizer class for sanitizing tensors.\n\nA sanitizer first limits the sensitivity of a tensor and then adds noise\nto the tensor. The parameters are determined by the privacy_spending and the\nother parameters. It also uses an accountant to keep track of the privacy\nspending.\n""""""\nfrom __future__ import division\n\nimport collections\n\nimport tensorflow as tf\n\nfrom differential_privacy.dp_sgd.dp_optimizer import utils\n\n\nClipOption = collections.namedtuple(""ClipOption"",\n                                    [""l2norm_bound"", ""clip""])\n\n\nclass AmortizedGaussianSanitizer(object):\n  """"""Sanitizer with Gaussian noise and amoritzed privacy spending accounting.\n\n  This sanitizes a tensor by first clipping the tensor, summing the tensor\n  and then adding appropriate amount of noise. It also uses an amortized\n  accountant to keep track of privacy spending.\n  """"""\n\n  def __init__(self, accountant, default_option):\n    """"""Construct an AmortizedGaussianSanitizer.\n\n    Args:\n      accountant: the privacy accountant. Expect an amortized one.\n      default_option: the default ClipOptoin.\n    """"""\n\n    self._accountant = accountant\n    self._default_option = default_option\n    self._options = {}\n\n  def set_option(self, tensor_name, option):\n    """"""Set options for an individual tensor.\n\n    Args:\n      tensor_name: the name of the tensor.\n      option: clip option.\n    """"""\n\n    self._options[tensor_name] = option\n\n  def sanitize(self, x, eps_delta, sigma=None,\n               option=ClipOption(None, None), tensor_name=None,\n               num_examples=None, add_noise=True):\n    """"""Sanitize the given tensor.\n\n    This santize a given tensor by first applying l2 norm clipping and then\n    adding Gaussian noise. It calls the privacy accountant for updating the\n    privacy spending.\n\n    Args:\n      x: the tensor to sanitize.\n      eps_delta: a pair of eps, delta for (eps,delta)-DP. Use it to\n        compute sigma if sigma is None.\n      sigma: if sigma is not None, use sigma.\n      option: a ClipOption which, if supplied, used for\n        clipping and adding noise.\n      tensor_name: the name of the tensor.\n      num_examples: if None, use the number of ""rows"" of x.\n      add_noise: if True, then add noise, else just clip.\n    Returns:\n      a pair of sanitized tensor and the operation to accumulate privacy\n      spending.\n    """"""\n\n    if sigma is None:\n      # pylint: disable=unpacking-non-sequence\n      eps, delta = eps_delta\n      with tf.control_dependencies(\n          [tf.Assert(tf.greater(eps, 0),\n                     [""eps needs to be greater than 0""]),\n           tf.Assert(tf.greater(delta, 0),\n                     [""delta needs to be greater than 0""])]):\n        # The following formula is taken from\n        #   Dwork and Roth, The Algorithmic Foundations of Differential\n        #   Privacy, Appendix A.\n        #   http://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf\n        sigma = tf.sqrt(2.0 * tf.log(1.25 / delta)) / eps\n\n    l2norm_bound, clip = option\n    if l2norm_bound is None:\n      l2norm_bound, clip = self._default_option\n      if ((tensor_name is not None) and\n          (tensor_name in self._options)):\n        l2norm_bound, clip = self._options[tensor_name]\n    if clip:\n      x = utils.BatchClipByL2norm(x, l2norm_bound)\n\n    if add_noise:\n      if num_examples is None:\n        num_examples = tf.slice(tf.shape(x), [0], [1])\n      privacy_accum_op = self._accountant.accumulate_privacy_spending(\n          eps_delta, sigma, num_examples)\n      with tf.control_dependencies([privacy_accum_op]):\n        saned_x = utils.AddGaussianNoise(tf.reduce_sum(x, 0),\n                                         sigma * l2norm_bound)\n    else:\n      saned_x = tf.reduce_sum(x, 0)\n    return saned_x\n'"
model_zoo/models/differential_privacy/dp_sgd/dp_optimizer/utils.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Utils for building and training NN models.\n""""""\nfrom __future__ import division\n\nimport math\n\nimport numpy\nimport tensorflow as tf\n\n\nclass LayerParameters(object):\n  """"""class that defines a non-conv layer.""""""\n  def __init__(self):\n    self.name = """"\n    self.num_units = 0\n    self._with_bias = False\n    self.relu = False\n    self.gradient_l2norm_bound = 0.0\n    self.bias_gradient_l2norm_bound = 0.0\n    self.trainable = True\n    self.weight_decay = 0.0\n\n\nclass ConvParameters(object):\n  """"""class that defines a conv layer.""""""\n  def __init__(self):\n    self.patch_size = 5\n    self.stride = 1\n    self.in_channels = 1\n    self.out_channels = 0\n    self.with_bias = True\n    self.relu = True\n    self.max_pool = True\n    self.max_pool_size = 2\n    self.max_pool_stride = 2\n    self.trainable = False\n    self.in_size = 28\n    self.name = """"\n    self.num_outputs = 0\n    self.bias_stddev = 0.1\n\n\n# Parameters for a layered neural network.\nclass NetworkParameters(object):\n  """"""class that define the overall model structure.""""""\n  def __init__(self):\n    self.input_size = 0\n    self.projection_type = \'NONE\'  # NONE, RANDOM, PCA\n    self.projection_dimensions = 0\n    self.default_gradient_l2norm_bound = 0.0\n    self.layer_parameters = []  # List of LayerParameters\n    self.conv_parameters = []  # List of ConvParameters\n\n\ndef GetTensorOpName(x):\n  """"""Get the name of the op that created a tensor.\n\n  Useful for naming related tensors, as \':\' in name field of op is not permitted\n\n  Args:\n    x: the input tensor.\n  Returns:\n    the name of the op.\n  """"""\n\n  t = x.name.rsplit("":"", 1)\n  if len(t) == 1:\n    return x.name\n  else:\n    return t[0]\n\n\ndef BuildNetwork(inputs, network_parameters):\n  """"""Build a network using the given parameters.\n\n  Args:\n    inputs: a Tensor of floats containing the input data.\n    network_parameters: NetworkParameters object\n      that describes the parameters for the network.\n  Returns:\n    output, training_parameters: where the outputs (a tensor) is the output\n      of the network, and training_parameters (a dictionary that maps the\n      name of each variable to a dictionary of parameters) is the parameters\n      used during training.\n  """"""\n\n  training_parameters = {}\n  num_inputs = network_parameters.input_size\n  outputs = inputs\n  projection = None\n\n  # First apply convolutions, if needed\n  for conv_param in network_parameters.conv_parameters:\n    outputs = tf.reshape(\n        outputs,\n        [-1, conv_param.in_size, conv_param.in_size,\n         conv_param.in_channels])\n    conv_weights_name = ""%s_conv_weight"" % (conv_param.name)\n    conv_bias_name = ""%s_conv_bias"" % (conv_param.name)\n    conv_std_dev = 1.0 / (conv_param.patch_size\n                          * math.sqrt(conv_param.in_channels))\n    conv_weights = tf.Variable(\n        tf.truncated_normal([conv_param.patch_size,\n                             conv_param.patch_size,\n                             conv_param.in_channels,\n                             conv_param.out_channels],\n                            stddev=conv_std_dev),\n        trainable=conv_param.trainable,\n        name=conv_weights_name)\n    conv_bias = tf.Variable(\n        tf.truncated_normal([conv_param.out_channels],\n                            stddev=conv_param.bias_stddev),\n        trainable=conv_param.trainable,\n        name=conv_bias_name)\n    training_parameters[conv_weights_name] = {}\n    training_parameters[conv_bias_name] = {}\n    conv = tf.nn.conv2d(outputs, conv_weights,\n                        strides=[1, conv_param.stride,\n                                 conv_param.stride, 1],\n                        padding=""SAME"")\n    relud = tf.nn.relu(conv + conv_bias)\n    mpd = tf.nn.max_pool(relud, ksize=[1,\n                                       conv_param.max_pool_size,\n                                       conv_param.max_pool_size, 1],\n                         strides=[1, conv_param.max_pool_stride,\n                                  conv_param.max_pool_stride, 1],\n                         padding=""SAME"")\n    outputs = mpd\n    num_inputs = conv_param.num_outputs\n    # this should equal\n    # in_size * in_size * out_channels / (stride * max_pool_stride)\n\n  # once all the convs are done, reshape to make it flat\n  outputs = tf.reshape(outputs, [-1, num_inputs])\n\n  # Now project, if needed\n  if network_parameters.projection_type is not ""NONE"":\n    projection = tf.Variable(tf.truncated_normal(\n        [num_inputs, network_parameters.projection_dimensions],\n        stddev=1.0 / math.sqrt(num_inputs)), trainable=False, name=""projection"")\n    num_inputs = network_parameters.projection_dimensions\n    outputs = tf.matmul(outputs, projection)\n\n  # Now apply any other layers\n\n  for layer_parameters in network_parameters.layer_parameters:\n    num_units = layer_parameters.num_units\n    hidden_weights_name = ""%s_weight"" % (layer_parameters.name)\n    hidden_weights = tf.Variable(\n        tf.truncated_normal([num_inputs, num_units],\n                            stddev=1.0 / math.sqrt(num_inputs)),\n        name=hidden_weights_name, trainable=layer_parameters.trainable)\n    training_parameters[hidden_weights_name] = {}\n    if layer_parameters.gradient_l2norm_bound:\n      training_parameters[hidden_weights_name][""gradient_l2norm_bound""] = (\n          layer_parameters.gradient_l2norm_bound)\n    if layer_parameters.weight_decay:\n      training_parameters[hidden_weights_name][""weight_decay""] = (\n          layer_parameters.weight_decay)\n\n    outputs = tf.matmul(outputs, hidden_weights)\n    if layer_parameters.with_bias:\n      hidden_biases_name = ""%s_bias"" % (layer_parameters.name)\n      hidden_biases = tf.Variable(tf.zeros([num_units]),\n                                  name=hidden_biases_name)\n      training_parameters[hidden_biases_name] = {}\n      if layer_parameters.bias_gradient_l2norm_bound:\n        training_parameters[hidden_biases_name][\n            ""bias_gradient_l2norm_bound""] = (\n                layer_parameters.bias_gradient_l2norm_bound)\n\n      outputs += hidden_biases\n    if layer_parameters.relu:\n      outputs = tf.nn.relu(outputs)\n    # num_inputs for the next layer is num_units in the current layer.\n    num_inputs = num_units\n\n  return outputs, projection, training_parameters\n\n\ndef VaryRate(start, end, saturate_epochs, epoch):\n  """"""Compute a linearly varying number.\n\n  Decrease linearly from start to end until epoch saturate_epochs.\n\n  Args:\n    start: the initial number.\n    end: the end number.\n    saturate_epochs: after this we do not reduce the number; if less than\n      or equal to zero, just return start.\n    epoch: the current learning epoch.\n  Returns:\n    the caculated number.\n  """"""\n  if saturate_epochs <= 0:\n    return start\n\n  step = (start - end) / (saturate_epochs - 1)\n  if epoch < saturate_epochs:\n    return start - step * epoch\n  else:\n    return end\n\n\ndef BatchClipByL2norm(t, upper_bound, name=None):\n  """"""Clip an array of tensors by L2 norm.\n\n  Shrink each dimension-0 slice of tensor (for matrix it is each row) such\n  that the l2 norm is at most upper_bound. Here we clip each row as it\n  corresponds to each example in the batch.\n\n  Args:\n    t: the input tensor.\n    upper_bound: the upperbound of the L2 norm.\n    name: optional name.\n  Returns:\n    the clipped tensor.\n  """"""\n\n  assert upper_bound > 0\n  with tf.op_scope([t, upper_bound], name, ""batch_clip_by_l2norm"") as name:\n    saved_shape = tf.shape(t)\n    batch_size = tf.slice(saved_shape, [0], [1])\n    t2 = tf.reshape(t, tf.concat(0, [batch_size, [-1]]))\n    upper_bound_inv = tf.fill(tf.slice(saved_shape, [0], [1]),\n                              tf.constant(1.0/upper_bound))\n    # Add a small number to avoid divide by 0\n    l2norm_inv = tf.rsqrt(tf.reduce_sum(t2 * t2, [1]) + 0.000001)\n    scale = tf.minimum(l2norm_inv, upper_bound_inv) * upper_bound\n    clipped_t = tf.matmul(tf.diag(scale), t2)\n    clipped_t = tf.reshape(clipped_t, saved_shape, name=name)\n  return clipped_t\n\n\ndef SoftThreshold(t, threshold_ratio, name=None):\n  """"""Soft-threshold a tensor by the mean value.\n\n  Softthreshold each dimension-0 vector (for matrix it is each column) by\n  the mean of absolute value multiplied by the threshold_ratio factor. Here\n  we soft threshold each column as it corresponds to each unit in a layer.\n\n  Args:\n    t: the input tensor.\n    threshold_ratio: the threshold ratio.\n    name: the optional name for the returned tensor.\n  Returns:\n    the thresholded tensor, where each entry is soft-thresholded by\n    threshold_ratio times the mean of the aboslute value of each column.\n  """"""\n\n  assert threshold_ratio >= 0\n  with tf.op_scope([t, threshold_ratio], name, ""soft_thresholding"") as name:\n    saved_shape = tf.shape(t)\n    t2 = tf.reshape(t, tf.concat(0, [tf.slice(saved_shape, [0], [1]), -1]))\n    t_abs = tf.abs(t2)\n    t_x = tf.sign(t2) * tf.nn.relu(t_abs -\n                                   (tf.reduce_mean(t_abs, [0],\n                                                   keep_dims=True) *\n                                    threshold_ratio))\n    return tf.reshape(t_x, saved_shape, name=name)\n\n\ndef AddGaussianNoise(t, sigma, name=None):\n  """"""Add i.i.d. Gaussian noise (0, sigma^2) to every entry of t.\n\n  Args:\n    t: the input tensor.\n    sigma: the stddev of the Gaussian noise.\n    name: optional name.\n  Returns:\n    the noisy tensor.\n  """"""\n\n  with tf.op_scope([t, sigma], name, ""add_gaussian_noise"") as name:\n    noisy_t = t + tf.random_normal(tf.shape(t), stddev=sigma)\n  return noisy_t\n\n\ndef GenerateBinomialTable(m):\n  """"""Generate binomial table.\n\n  Args:\n    m: the size of the table.\n  Returns:\n    A two dimensional array T where T[i][j] = (i choose j),\n    for 0<= i, j <=m.\n  """"""\n\n  table = numpy.zeros((m + 1, m + 1), dtype=numpy.float64)\n  for i in range(m + 1):\n    table[i, 0] = 1\n  for i in range(1, m + 1):\n    for j in range(1, m + 1):\n      v = table[i - 1, j] + table[i - 1, j -1]\n      assert not math.isnan(v) and not math.isinf(v)\n      table[i, j] = v\n  return tf.convert_to_tensor(table)\n'"
model_zoo/models/differential_privacy/dp_sgd/per_example_gradients/per_example_gradients.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Per-example gradients for selected ops.""""""\n\nimport collections\n\nimport tensorflow as tf\n\nOrderedDict = collections.OrderedDict\n\n\ndef _ListUnion(list_1, list_2):\n  """"""Returns the union of two lists.\n\n  Python sets can have a non-deterministic iteration order. In some\n  contexts, this could lead to TensorFlow producing two different\n  programs when the same Python script is run twice. In these contexts\n  we use lists instead of sets.\n\n  This function is not designed to be especially fast and should only\n  be used with small lists.\n\n  Args:\n    list_1: A list\n    list_2: Another list\n\n  Returns:\n    A new list containing one copy of each unique element of list_1 and\n    list_2. Uniqueness is determined by ""x in union"" logic; e.g. two\n    string of that value appearing in the union.\n\n  Raises:\n    TypeError: The arguments are not lists.\n  """"""\n\n  if not (isinstance(list_1, list) and isinstance(list_2, list)):\n    raise TypeError(""Arguments must be lists."")\n\n  union = []\n  for x in list_1 + list_2:\n    if x not in union:\n      union.append(x)\n\n  return union\n\n\ndef Interface(ys, xs):\n  """"""Maps xs to consumers.\n\n    Returns a dict mapping each element of xs to any of its consumers that are\n    indirectly consumed by ys.\n\n  Args:\n    ys: The outputs\n    xs: The inputs\n  Returns:\n    out: Dict mapping each member x of `xs` to a list of all Tensors that are\n         direct consumers of x and are eventually consumed by a member of\n         `ys`.\n  """"""\n\n  if isinstance(ys, (list, tuple)):\n    queue = list(ys)\n  else:\n    queue = [ys]\n\n  out = OrderedDict()\n  if isinstance(xs, (list, tuple)):\n    for x in xs:\n      out[x] = []\n  else:\n    out[xs] = []\n\n  done = set()\n\n  while queue:\n    y = queue.pop()\n    if y in done:\n      continue\n    done = done.union(set([y]))\n    for x in y.op.inputs:\n      if x in out:\n        out[x].append(y)\n      else:\n        assert id(x) not in [id(foo) for foo in out]\n    queue.extend(y.op.inputs)\n\n  return out\n\n\nclass PXGRegistry(object):\n  """"""Per-Example Gradient registry.\n\n  Maps names of ops to per-example gradient rules for those ops.\n  These rules are only needed for ops that directly touch values that\n  are shared between examples. For most machine learning applications,\n  this means only ops that directly operate on the parameters.\n\n\n  See http://arxiv.org/abs/1510.01799 for more information, and please\n  consider citing that tech report if you use this function in published\n  research.\n  """"""\n\n  def __init__(self):\n    self.d = OrderedDict()\n\n  def __call__(self, op,\n               colocate_gradients_with_ops=False,\n               gate_gradients=False):\n    if op.node_def.op not in self.d:\n      raise NotImplementedError(""No per-example gradient rule registered ""\n                                ""for "" + op.node_def.op + "" in pxg_registry."")\n    return self.d[op.node_def.op](op,\n                                  colocate_gradients_with_ops,\n                                  gate_gradients)\n\n  def Register(self, op_name, pxg_class):\n    """"""Associates `op_name` key with `pxg_class` value.\n\n    Registers `pxg_class` as the class that will be called to perform\n    per-example differentiation through ops with `op_name`.\n\n    Args:\n      op_name: String op name.\n      pxg_class: An instance of any class with the same signature as MatMulPXG.\n    """"""\n    self.d[op_name] = pxg_class\n\n\npxg_registry = PXGRegistry()\n\n\nclass MatMulPXG(object):\n  """"""Per-example gradient rule for MatMul op.\n  """"""\n\n  def __init__(self, op,\n               colocate_gradients_with_ops=False,\n               gate_gradients=False):\n    """"""Construct an instance of the rule for `op`.\n\n    Args:\n      op: The Operation to differentiate through.\n      colocate_gradients_with_ops: currently unsupported\n      gate_gradients: currently unsupported\n    """"""\n    assert op.node_def.op == ""MatMul""\n    self.op = op\n    self.colocate_gradients_with_ops = colocate_gradients_with_ops\n    self.gate_gradients = gate_gradients\n\n  def __call__(self, x, z_grads):\n    """"""Build the graph for the per-example gradient through the op.\n\n    Assumes that the MatMul was called with a design matrix with examples\n    in rows as the first argument and parameters as the second argument.\n\n    Args:\n      x: The Tensor to differentiate with respect to. This tensor must\n         represent the weights.\n      z_grads: The list of gradients on the output of the op.\n\n    Returns:\n      x_grads: A Tensor containing the gradient with respect to `x` for\n       each example. This is a 3-D tensor, with the first axis corresponding\n       to examples and the remaining axes matching the shape of x.\n    """"""\n    idx = list(self.op.inputs).index(x)\n    assert idx != -1\n    assert len(z_grads) == len(self.op.outputs)\n    assert idx == 1  # We expect weights to be arg 1\n    # We don\'t expect anyone to per-example differentiate with repsect\n    # to anything other than the weights.\n    x, _ = self.op.inputs\n    z_grads, = z_grads\n    x_expanded = tf.expand_dims(x, 2)\n    z_grads_expanded = tf.expand_dims(z_grads, 1)\n    return tf.mul(x_expanded, z_grads_expanded)\n\n\npxg_registry.Register(""MatMul"", MatMulPXG)\n\n\nclass Conv2DPXG(object):\n  """"""Per-example gradient rule of Conv2d op.\n\n  Same interface as MatMulPXG.\n  """"""\n\n  def __init__(self, op,\n               colocate_gradients_with_ops=False,\n               gate_gradients=False):\n\n    assert op.node_def.op == ""Conv2D""\n    self.op = op\n    self.colocate_gradients_with_ops = colocate_gradients_with_ops\n    self.gate_gradients = gate_gradients\n\n  def _PxConv2DBuilder(self, input_, w, strides, padding):\n    """"""conv2d run separately per example, to help compute per-example gradients.\n\n    Args:\n      input_: tensor containing a minibatch of images / feature maps.\n              Shape [batch_size, rows, columns, channels]\n      w: convolution kernels. Shape\n        [kernel rows, kernel columns, input channels, output channels]\n      strides: passed through to regular conv_2d\n      padding: passed through to regular conv_2d\n\n    Returns:\n      conv: the output of the convolution.\n         single tensor, same as what regular conv_2d does\n      w_px: a list of batch_size copies of w. each copy was used\n          for the corresponding example in the minibatch.\n           calling tf.gradients on the copy gives the gradient for just\n                  that example.\n    """"""\n    input_shape = [int(e) for e in input_.get_shape()]\n    batch_size = input_shape[0]\n    input_px = [tf.slice(\n        input_, [example] + [0] * 3, [1] + input_shape[1:]) for example\n                in xrange(batch_size)]\n    for input_x in input_px:\n      assert int(input_x.get_shape()[0]) == 1\n    w_px = [tf.identity(w) for example in xrange(batch_size)]\n    conv_px = [tf.nn.conv2d(input_x, w_x,\n                            strides=strides,\n                            padding=padding)\n               for input_x, w_x in zip(input_px, w_px)]\n    for conv_x in conv_px:\n      num_x = int(conv_x.get_shape()[0])\n      assert num_x == 1, num_x\n    assert len(conv_px) == batch_size\n    conv = tf.concat(0, conv_px)\n    assert int(conv.get_shape()[0]) == batch_size\n    return conv, w_px\n\n  def __call__(self, w, z_grads):\n    idx = list(self.op.inputs).index(w)\n    # Make sure that `op` was actually applied to `w`\n    assert idx != -1\n    assert len(z_grads) == len(self.op.outputs)\n    # The following assert may be removed when we are ready to use this\n    # for general purpose code.\n    # This assert is only expected to hold in the contex of our preliminary\n    # MNIST experiments.\n    assert idx == 1  # We expect convolution weights to be arg 1\n\n    images, filters = self.op.inputs\n    strides = self.op.get_attr(""strides"")\n    padding = self.op.get_attr(""padding"")\n    # Currently assuming that one specifies at most these four arguments and\n    # that all other arguments to conv2d are set to default.\n\n    conv, w_px = self._PxConv2DBuilder(images, filters, strides, padding)\n    z_grads, = z_grads\n\n    gradients_list = tf.gradients(conv, w_px, z_grads,\n                                  colocate_gradients_with_ops=\n                                  self.colocate_gradients_with_ops,\n                                  gate_gradients=self.gate_gradients)\n\n    return tf.pack(gradients_list)\n\npxg_registry.Register(""Conv2D"", Conv2DPXG)\n\n\nclass AddPXG(object):\n  """"""Per-example gradient rule for Add op.\n\n  Same interface as MatMulPXG.\n  """"""\n\n  def __init__(self, op,\n               colocate_gradients_with_ops=False,\n               gate_gradients=False):\n\n    assert op.node_def.op == ""Add""\n    self.op = op\n    self.colocate_gradients_with_ops = colocate_gradients_with_ops\n    self.gate_gradients = gate_gradients\n\n  def __call__(self, x, z_grads):\n    idx = list(self.op.inputs).index(x)\n    # Make sure that `op` was actually applied to `x`\n    assert idx != -1\n    assert len(z_grads) == len(self.op.outputs)\n    # The following assert may be removed when we are ready to use this\n    # for general purpose code.\n    # This assert is only expected to hold in the contex of our preliminary\n    # MNIST experiments.\n    assert idx == 1 # We expect biases to be arg 1\n    # We don\'t expect anyone to per-example differentiate with respect\n    # to anything other than the biases.\n    x, _ = self.op.inputs\n    z_grads, = z_grads\n    return z_grads\n\n\npxg_registry.Register(""Add"", AddPXG)\n\n\ndef PerExampleGradients(ys, xs, grad_ys=None, name=""gradients"",\n                        colocate_gradients_with_ops=False,\n                        gate_gradients=False):\n  """"""Symbolic differentiation, separately for each example.\n\n  Matches the interface of tf.gradients, but the return values each have an\n  additional axis corresponding to the examples.\n\n  Assumes that the cost in `ys` is additive across examples.\n  e.g., no batch normalization.\n  Individual rules for each op specify their own assumptions about how\n  examples are put into tensors.\n  """"""\n\n  # Find the interface between the xs and the cost\n  for x in xs:\n    assert isinstance(x, tf.Tensor), type(x)\n  interface = Interface(ys, xs)\n  merged_interface = []\n  for x in xs:\n    merged_interface = _ListUnion(merged_interface, interface[x])\n  # Differentiate with respect to the interface\n  interface_gradients = tf.gradients(ys, merged_interface, grad_ys=grad_ys,\n                                     name=name,\n                                     colocate_gradients_with_ops=\n                                     colocate_gradients_with_ops,\n                                     gate_gradients=gate_gradients)\n  grad_dict = OrderedDict(zip(merged_interface, interface_gradients))\n  # Build the per-example gradients with respect to the xs\n  if colocate_gradients_with_ops:\n    raise NotImplementedError(""The per-example gradients are not yet ""\n                              ""colocated with ops."")\n  if gate_gradients:\n    raise NotImplementedError(""The per-example gradients are not yet ""\n                              ""gated."")\n  out = []\n  for x in xs:\n    zs = interface[x]\n    ops = []\n    for z in zs:\n      ops = _ListUnion(ops, [z.op])\n    if len(ops) != 1:\n      raise NotImplementedError(""Currently we only support the case ""\n                                ""where each x is consumed by exactly ""\n                                ""one op. but %s is consumed by %d ops.""\n                                % (x.name, len(ops)))\n    op = ops[0]\n    pxg_rule = pxg_registry(op, colocate_gradients_with_ops, gate_gradients)\n    x_grad = pxg_rule(x, [grad_dict[z] for z in zs])\n    out.append(x_grad)\n  return out\n'"
model_zoo/models/differential_privacy/privacy_accountant/python/gaussian_moments.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A standalone utility for computing the log moments.\n\nThe utility for computing the log moments. It consists of two methods.\ncompute_log_moment(q, sigma, T, lmbd) computes the log moment with sampling\nprobability q, noise sigma, order lmbd, and T steps. get_privacy_spent computes\ndelta (or eps) given log moments and eps (or delta).\n\nExample use:\n\nSuppose that we have run an algorithm with parameters, an array of\n(q1, sigma1, T1) ... (qk, sigmak, Tk), and we wish to compute eps for a given\ndelta. The example code would be:\n\n  max_lmbd = 32\n  lmbds = xrange(1, max_lmbd + 1)\n  log_moments = []\n  for lmbd in lmbds:\n    log_moment = 0\n    for q, sigma, T in parameters:\n      log_moment += compute_log_moment(q, sigma, T, lmbd)\n    log_moments.append((lmbd, log_moment))\n  eps, delta = get_privacy_spent(log_moments, target_delta=delta)\n\nTo verify that the I1 >= I2 (see comments in GaussianMomentsAccountant in\naccountant.py for the context), run the same loop above with verify=True\npassed to compute_log_moment.\n""""""\nimport math\nimport sys\n\nimport numpy as np\nimport scipy.integrate as integrate\nimport scipy.stats\nfrom sympy.mpmath import mp\n\n\ndef _to_np_float64(v):\n  if math.isnan(v) or math.isinf(v):\n    return np.inf\n  return np.float64(v)\n\n\n######################\n# FLOAT64 ARITHMETIC #\n######################\n\n\ndef pdf_gauss(x, sigma, mean=0):\n  return scipy.stats.norm.pdf(x, loc=mean, scale=sigma)\n\n\ndef cropped_ratio(a, b):\n  if a < 1E-50 and b < 1E-50:\n    return 1.\n  else:\n    return a / b\n\n\ndef integral_inf(fn):\n  integral, _ = integrate.quad(fn, -np.inf, np.inf)\n  return integral\n\n\ndef integral_bounded(fn, lb, ub):\n  integral, _ = integrate.quad(fn, lb, ub)\n  return integral\n\n\ndef distributions(sigma, q):\n  mu0 = lambda y: pdf_gauss(y, sigma=sigma, mean=0.0)\n  mu1 = lambda y: pdf_gauss(y, sigma=sigma, mean=1.0)\n  mu = lambda y: (1 - q) * mu0(y) + q * mu1(y)\n  return mu0, mu1, mu\n\n\ndef compute_a(sigma, q, lmbd, verbose=False):\n  lmbd_int = int(math.ceil(lmbd))\n  if lmbd_int == 0:\n    return 1.0\n\n  a_lambda_first_term_exact = 0\n  a_lambda_second_term_exact = 0\n  for i in xrange(lmbd_int + 1):\n    coef_i = scipy.special.binom(lmbd_int, i) * (q ** i)\n    s1, s2 = 0, 0\n    for j in xrange(i + 1):\n      coef_j = scipy.special.binom(i, j) * (-1) ** (i - j)\n      s1 += coef_j * np.exp((j * j - j) / (2.0 * (sigma ** 2)))\n      s2 += coef_j * np.exp((j * j + j) / (2.0 * (sigma ** 2)))\n    a_lambda_first_term_exact += coef_i * s1\n    a_lambda_second_term_exact += coef_i * s2\n\n  a_lambda_exact = ((1.0 - q) * a_lambda_first_term_exact +\n                    q * a_lambda_second_term_exact)\n  if verbose:\n    print ""A: by binomial expansion    {} = {} + {}"".format(\n        a_lambda_exact,\n        (1.0 - q) * a_lambda_first_term_exact,\n        q * a_lambda_second_term_exact)\n  return _to_np_float64(a_lambda_exact)\n\n\ndef compute_b(sigma, q, lmbd, verbose=False):\n  mu0, _, mu = distributions(sigma, q)\n\n  b_lambda_fn = lambda z: mu0(z) * np.power(cropped_ratio(mu0(z), mu(z)), lmbd)\n  b_lambda = integral_inf(b_lambda_fn)\n  m = sigma ** 2 * (np.log((2. - q) / (1. - q)) + 1. / (2 * sigma ** 2))\n\n  b_fn = lambda z: (np.power(mu0(z) / mu(z), lmbd) -\n                    np.power(mu(-z) / mu0(z), lmbd))\n  if verbose:\n    print ""M ="", m\n    print ""f(-M) = {} f(M) = {}"".format(b_fn(-m), b_fn(m))\n    assert b_fn(-m) < 0 and b_fn(m) < 0\n\n  b_lambda_int1_fn = lambda z: (mu0(z) *\n                                np.power(cropped_ratio(mu0(z), mu(z)), lmbd))\n  b_lambda_int2_fn = lambda z: (mu0(z) *\n                                np.power(cropped_ratio(mu(z), mu0(z)), lmbd))\n  b_int1 = integral_bounded(b_lambda_int1_fn, -m, m)\n  b_int2 = integral_bounded(b_lambda_int2_fn, -m, m)\n\n  a_lambda_m1 = compute_a(sigma, q, lmbd - 1)\n  b_bound = a_lambda_m1 + b_int1 - b_int2\n\n  if verbose:\n    print ""B: by numerical integration"", b_lambda\n    print ""B must be no more than     "", b_bound\n  print b_lambda, b_bound\n  return _to_np_float64(b_lambda)\n\n\n###########################\n# MULTIPRECISION ROUTINES #\n###########################\n\n\ndef pdf_gauss_mp(x, sigma, mean):\n  return mp.mpf(1.) / mp.sqrt(mp.mpf(""2."") * sigma ** 2 * mp.pi) * mp.exp(\n      - (x - mean) ** 2 / (mp.mpf(""2."") * sigma ** 2))\n\n\ndef integral_inf_mp(fn):\n  integral, _ = mp.quad(fn, [-mp.inf, mp.inf], error=True)\n  return integral\n\n\ndef integral_bounded_mp(fn, lb, ub):\n  integral, _ = mp.quad(fn, [lb, ub], error=True)\n  return integral\n\n\ndef distributions_mp(sigma, q):\n  mu0 = lambda y: pdf_gauss_mp(y, sigma=sigma, mean=mp.mpf(0))\n  mu1 = lambda y: pdf_gauss_mp(y, sigma=sigma, mean=mp.mpf(1))\n  mu = lambda y: (1 - q) * mu0(y) + q * mu1(y)\n  return mu0, mu1, mu\n\n\ndef compute_a_mp(sigma, q, lmbd, verbose=False):\n  lmbd_int = int(math.ceil(lmbd))\n  if lmbd_int == 0:\n    return 1.0\n\n  mu0, mu1, mu = distributions_mp(sigma, q)\n  a_lambda_fn = lambda z: mu(z) * (mu(z) / mu0(z)) ** lmbd_int\n  a_lambda_first_term_fn = lambda z: mu0(z) * (mu(z) / mu0(z)) ** lmbd_int\n  a_lambda_second_term_fn = lambda z: mu1(z) * (mu(z) / mu0(z)) ** lmbd_int\n\n  a_lambda = integral_inf_mp(a_lambda_fn)\n  a_lambda_first_term = integral_inf_mp(a_lambda_first_term_fn)\n  a_lambda_second_term = integral_inf_mp(a_lambda_second_term_fn)\n\n  if verbose:\n    print ""A: by numerical integration {} = {} + {}"".format(\n        a_lambda,\n        (1 - q) * a_lambda_first_term,\n        q * a_lambda_second_term)\n\n  return _to_np_float64(a_lambda)\n\n\ndef compute_b_mp(sigma, q, lmbd, verbose=False):\n  lmbd_int = int(math.ceil(lmbd))\n  if lmbd_int == 0:\n    return 1.0\n\n  mu0, _, mu = distributions_mp(sigma, q)\n\n  b_lambda_fn = lambda z: mu0(z) * (mu0(z) / mu(z)) ** lmbd_int\n  b_lambda = integral_inf_mp(b_lambda_fn)\n\n  m = sigma ** 2 * (mp.log((2 - q) / (1 - q)) + 1 / (2 * (sigma ** 2)))\n  b_fn = lambda z: ((mu0(z) / mu(z)) ** lmbd_int -\n                    (mu(-z) / mu0(z)) ** lmbd_int)\n  if verbose:\n    print ""M ="", m\n    print ""f(-M) = {} f(M) = {}"".format(b_fn(-m), b_fn(m))\n    assert b_fn(-m) < 0 and b_fn(m) < 0\n\n  b_lambda_int1_fn = lambda z: mu0(z) * (mu0(z) / mu(z)) ** lmbd_int\n  b_lambda_int2_fn = lambda z: mu0(z) * (mu(z) / mu0(z)) ** lmbd_int\n  b_int1 = integral_bounded_mp(b_lambda_int1_fn, -m, m)\n  b_int2 = integral_bounded_mp(b_lambda_int2_fn, -m, m)\n\n  a_lambda_m1 = compute_a_mp(sigma, q, lmbd - 1)\n  b_bound = a_lambda_m1 + b_int1 - b_int2\n\n  if verbose:\n    print ""B by numerical integration"", b_lambda\n    print ""B must be no more than    "", b_bound\n  assert b_lambda < b_bound + 1e-5\n  return _to_np_float64(b_lambda)\n\n\ndef _compute_delta(log_moments, eps):\n  """"""Compute delta for given log_moments and eps.\n\n  Args:\n    log_moments: the log moments of privacy loss, in the form of pairs\n      of (moment_order, log_moment)\n    eps: the target epsilon.\n  Returns:\n    delta\n  """"""\n  min_delta = 1.0\n  for moment_order, log_moment in log_moments:\n    if moment_order == 0:\n      continue\n    if math.isinf(log_moment) or math.isnan(log_moment):\n      sys.stderr.write(""The %d-th order is inf or Nan\\n"" % moment_order)\n      continue\n    if log_moment < moment_order * eps:\n      min_delta = min(min_delta,\n                      math.exp(log_moment - moment_order * eps))\n  return min_delta\n\n\ndef _compute_eps(log_moments, delta):\n  """"""Compute epsilon for given log_moments and delta.\n\n  Args:\n    log_moments: the log moments of privacy loss, in the form of pairs\n      of (moment_order, log_moment)\n    delta: the target delta.\n  Returns:\n    epsilon\n  """"""\n  min_eps = float(""inf"")\n  for moment_order, log_moment in log_moments:\n    if moment_order == 0:\n      continue\n    if math.isinf(log_moment) or math.isnan(log_moment):\n      sys.stderr.write(""The %d-th order is inf or Nan\\n"" % moment_order)\n      continue\n    min_eps = min(min_eps, (log_moment - math.log(delta)) / moment_order)\n  return min_eps\n\n\ndef compute_log_moment(q, sigma, steps, lmbd, verify=False, verbose=False):\n  """"""Compute the log moment of Gaussian mechanism for given parameters.\n\n  Args:\n    q: the sampling ratio.\n    sigma: the noise sigma.\n    steps: the number of steps.\n    lmbd: the moment order.\n    verify: if False, only compute the symbolic version. If True, computes\n      both symbolic and numerical solutions and verifies the results match.\n    verbose: if True, print out debug information.\n  Returns:\n    the log moment with type np.float64, could be np.inf.\n  """"""\n  moment = compute_a(sigma, q, lmbd, verbose=verbose)\n  if verify:\n    mp.dps = 50\n    moment_a_mp = compute_a_mp(sigma, q, lmbd, verbose=verbose)\n    moment_b_mp = compute_b_mp(sigma, q, lmbd, verbose=verbose)\n    np.testing.assert_allclose(moment, moment_a_mp, rtol=1e-10)\n    if not np.isinf(moment_a_mp):\n      # The following test fails for (1, np.inf)!\n      np.testing.assert_array_less(moment_b_mp, moment_a_mp)\n  if np.isinf(moment):\n    return np.inf\n  else:\n    return np.log(moment) * steps\n\n\ndef get_privacy_spent(log_moments, target_eps=None, target_delta=None):\n  """"""Compute delta (or eps) for given eps (or delta) from log moments.\n\n  Args:\n    log_moments: array of (moment_order, log_moment) pairs.\n    target_eps: if not None, the epsilon for which we would like to compute\n      corresponding delta value.\n    target_delta: if not None, the delta for which we would like to compute\n      corresponding epsilon value. Exactly one of target_eps and target_delta\n      is None.\n  Returns:\n    eps, delta pair\n  """"""\n  assert (target_eps is None) ^ (target_delta is None)\n  assert not ((target_eps is None) and (target_delta is None))\n  if target_eps is not None:\n    return (target_eps, _compute_delta(log_moments, target_eps))\n  else:\n    return (_compute_eps(log_moments, target_delta), target_delta)\n'"
model_zoo/models/differential_privacy/privacy_accountant/tf/accountant.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Defines Accountant class for keeping track of privacy spending.\n\nA privacy accountant keeps track of privacy spendings. It has methods\naccumulate_privacy_spending and get_privacy_spent. Here we only define\nAmortizedAccountant which tracks the privacy spending in the amortized\nway. It uses privacy amplication via sampling to compute the privacy\nspending for each batch and strong composition (specialized for Gaussian\nnoise) for accumulate the privacy spending.\n""""""\nfrom __future__ import division\n\nimport abc\nimport collections\nimport math\nimport sys\n\nimport numpy\nimport tensorflow as tf\n\nfrom differential_privacy.dp_sgd.dp_optimizer import utils\n\nEpsDelta = collections.namedtuple(""EpsDelta"", [""spent_eps"", ""spent_delta""])\n\n\n# TODO(liqzhang) To ensure the same API for AmortizedAccountant and\n# MomentsAccountant, we pass the union of arguments to both, so we\n# have unused_sigma for AmortizedAccountant and unused_eps_delta for\n# MomentsAccountant. Consider to revise the API to avoid the unused\n# arguments.  It would be good to use @abc.abstractmethod, etc, to\n# define the common interface as a base class.\nclass AmortizedAccountant(object):\n  """"""Keep track of privacy spending in an amortized way.\n\n  AmortizedAccountant accumulates the privacy spending by assuming\n  all the examples are processed uniformly at random so the spending is\n  amortized among all the examples. And we assume that we use Gaussian noise\n  so the accumulation is on eps^2 and delta, using advanced composition.\n  """"""\n\n  def __init__(self, total_examples):\n    """"""Initialization. Currently only support amortized tracking.\n\n    Args:\n      total_examples: total number of examples.\n    """"""\n\n    assert total_examples > 0\n    self._total_examples = total_examples\n    self._eps_squared_sum = tf.Variable(tf.zeros([1]), trainable=False,\n                                        name=""eps_squared_sum"")\n    self._delta_sum = tf.Variable(tf.zeros([1]), trainable=False,\n                                  name=""delta_sum"")\n\n  def accumulate_privacy_spending(self, eps_delta, unused_sigma,\n                                  num_examples):\n    """"""Accumulate the privacy spending.\n\n    Currently only support approximate privacy. Here we assume we use Gaussian\n    noise on randomly sampled batch so we get better composition: 1. the per\n    batch privacy is computed using privacy amplication via sampling bound;\n    2. the composition is done using the composition with Gaussian noise.\n    TODO(liqzhang) Add a link to a document that describes the bounds used.\n\n    Args:\n      eps_delta: EpsDelta pair which can be tensors.\n      unused_sigma: the noise sigma. Unused for this accountant.\n      num_examples: the number of examples involved.\n    Returns:\n      a TensorFlow operation for updating the privacy spending.\n    """"""\n\n    eps, delta = eps_delta\n    with tf.control_dependencies(\n        [tf.Assert(tf.greater(delta, 0),\n                   [""delta needs to be greater than 0""])]):\n      amortize_ratio = (tf.cast(num_examples, tf.float32) * 1.0 /\n                        self._total_examples)\n      # Use privacy amplification via sampling bound.\n      # See Lemma 2.2 in http://arxiv.org/pdf/1405.7085v2.pdf\n      # TODO(liqzhang) Add a link to a document with formal statement\n      # and proof.\n      amortize_eps = tf.reshape(tf.log(1.0 + amortize_ratio * (\n          tf.exp(eps) - 1.0)), [1])\n      amortize_delta = tf.reshape(amortize_ratio * delta, [1])\n      return tf.group(*[tf.assign_add(self._eps_squared_sum,\n                                      tf.square(amortize_eps)),\n                        tf.assign_add(self._delta_sum, amortize_delta)])\n\n  def get_privacy_spent(self, sess, target_eps=None):\n    """"""Report the spending so far.\n\n    Args:\n      sess: the session to run the tensor.\n      target_eps: the target epsilon. Unused.\n    Returns:\n      the list containing a single EpsDelta, with values as Python floats (as\n      opposed to numpy.float64). This is to be consistent with\n      MomentAccountant which can return a list of (eps, delta) pair.\n    """"""\n\n    # pylint: disable=unused-argument\n    unused_target_eps = target_eps\n    eps_squared_sum, delta_sum = sess.run([self._eps_squared_sum,\n                                           self._delta_sum])\n    return [EpsDelta(math.sqrt(eps_squared_sum), float(delta_sum))]\n\n\nclass MomentsAccountant(object):\n  """"""Privacy accountant which keeps track of moments of privacy loss.\n\n  Note: The constructor of this class creates tf.Variables that must\n  be initialized with tf.initialize_all_variables() or similar calls.\n\n  MomentsAccountant accumulates the high moments of the privacy loss. It\n  requires a method for computing differenital moments of the noise (See\n  below for the definition). So every specific accountant should subclass\n  this class by implementing _differential_moments method.\n\n  Denote by X_i the random variable of privacy loss at the i-th step.\n  Consider two databases D, D\' which differ by one item. X_i takes value\n  log Pr[M(D\')==x]/Pr[M(D)==x] with probability Pr[M(D)==x].\n  In MomentsAccountant, we keep track of y_i(L) = log E[exp(L X_i)] for some\n  large enough L. To compute the final privacy spending,  we apply Chernoff\n  bound (assuming the random noise added at each step is independent) to\n  bound the total privacy loss Z = sum X_i as follows:\n    Pr[Z > e] = Pr[exp(L Z) > exp(L e)]\n              < E[exp(L Z)] / exp(L e)\n              = Prod_i E[exp(L X_i)] / exp(L e)\n              = exp(sum_i log E[exp(L X_i)]) / exp(L e)\n              = exp(sum_i y_i(L) - L e)\n  Hence the mechanism is (e, d)-differentially private for\n    d =  exp(sum_i y_i(L) - L e).\n  We require d < 1, i.e. e > sum_i y_i(L) / L. We maintain y_i(L) for several\n  L to compute the best d for any give e (normally should be the lowest L\n  such that 2 * sum_i y_i(L) / L < e.\n\n  We further assume that at each step, the mechanism operates on a random\n  sample with sampling probability q = batch_size / total_examples. Then\n    E[exp(L X)] = E[(Pr[M(D)==x / Pr[M(D\')==x])^L]\n  By distinguishign two cases of wether D < D\' or D\' < D, we have\n  that\n    E[exp(L X)] <= max (I1, I2)\n  where\n    I1 = (1-q) E ((1-q) + q P(X+1) / P(X))^L + q E ((1-q) + q P(X) / P(X-1))^L\n    I2 = E (P(X) / ((1-q) + q P(X+1)))^L\n\n  In order to compute I1 and I2, one can consider to\n    1. use an asymptotic bound, which recovers the advance composition theorem;\n    2. use the closed formula (like GaussianMomentsAccountant);\n    3. use numerical integration or random sample estimation.\n\n  Dependent on the distribution, we can often obtain a tigher estimation on\n  the moments and hence a more accurate estimation of the privacy loss than\n  obtained using generic composition theorems.\n\n  """"""\n\n  __metaclass__ = abc.ABCMeta\n\n  def __init__(self, total_examples, moment_orders=32):\n    """"""Initialize a MomentsAccountant.\n\n    Args:\n      total_examples: total number of examples.\n      moment_orders: the order of moments to keep.\n    """"""\n\n    assert total_examples > 0\n    self._total_examples = total_examples\n    self._moment_orders = (moment_orders\n                           if isinstance(moment_orders, (list, tuple))\n                           else range(1, moment_orders + 1))\n    self._max_moment_order = max(self._moment_orders)\n    assert self._max_moment_order < 100, ""The moment order is too large.""\n    self._log_moments = [tf.Variable(numpy.float64(0.0),\n                                     trainable=False,\n                                     name=(""log_moments-%d"" % moment_order))\n                         for moment_order in self._moment_orders]\n\n  @abc.abstractmethod\n  def _compute_log_moment(self, sigma, q, moment_order):\n    """"""Compute high moment of privacy loss.\n\n    Args:\n      sigma: the noise sigma, in the multiples of the sensitivity.\n      q: the sampling ratio.\n      moment_order: the order of moment.\n    Returns:\n      log E[exp(moment_order * X)]\n    """"""\n    pass\n\n  def accumulate_privacy_spending(self, unused_eps_delta,\n                                  sigma, num_examples):\n    """"""Accumulate privacy spending.\n\n    In particular, accounts for privacy spending when we assume there\n    are num_examples, and we are releasing the vector\n    (sum_{i=1}^{num_examples} x_i) + Normal(0, stddev=l2norm_bound*sigma)\n    where l2norm_bound is the maximum l2_norm of each example x_i, and\n    the num_examples have been randomly selected out of a pool of\n    self.total_examples.\n\n    Args:\n      unused_eps_delta: EpsDelta pair which can be tensors. Unused\n        in this accountant.\n      sigma: the noise sigma, in the multiples of the sensitivity (that is,\n        if the l2norm sensitivity is k, then the caller must have added\n        Gaussian noise with stddev=k*sigma to the result of the query).\n      num_examples: the number of examples involved.\n    Returns:\n      a TensorFlow operation for updating the privacy spending.\n    """"""\n    q = tf.cast(num_examples, tf.float64) * 1.0 / self._total_examples\n\n    moments_accum_ops = []\n    for i in range(len(self._log_moments)):\n      moment = self._compute_log_moment(sigma, q, self._moment_orders[i])\n      moments_accum_ops.append(tf.assign_add(self._log_moments[i], moment))\n    return tf.group(*moments_accum_ops)\n\n  def _compute_delta(self, log_moments, eps):\n    """"""Compute delta for given log_moments and eps.\n\n    Args:\n      log_moments: the log moments of privacy loss, in the form of pairs\n        of (moment_order, log_moment)\n      eps: the target epsilon.\n    Returns:\n      delta\n    """"""\n    min_delta = 1.0\n    for moment_order, log_moment in log_moments:\n      if math.isinf(log_moment) or math.isnan(log_moment):\n        sys.stderr.write(""The %d-th order is inf or Nan\\n"" % moment_order)\n        continue\n      if log_moment < moment_order * eps:\n        min_delta = min(min_delta,\n                        math.exp(log_moment - moment_order * eps))\n    return min_delta\n\n  def _compute_eps(self, log_moments, delta):\n    min_eps = float(""inf"")\n    for moment_order, log_moment in log_moments:\n      if math.isinf(log_moment) or math.isnan(log_moment):\n        sys.stderr.write(""The %d-th order is inf or Nan\\n"" % moment_order)\n        continue\n      min_eps = min(min_eps, (log_moment - math.log(delta)) / moment_order)\n    return min_eps\n\n  def get_privacy_spent(self, sess, target_eps=None, target_deltas=None):\n    """"""Compute privacy spending in (e, d)-DP form for a single or list of eps.\n\n    Args:\n      sess: the session to run the tensor.\n      target_eps: a list of target epsilon\'s for which we would like to\n        compute corresponding delta value.\n      target_deltas: a list of target deltas for which we would like to\n        compute the corresponding eps value. Caller must specify\n        either target_eps or target_delta.\n    Returns:\n      A list of EpsDelta pairs.\n    """"""\n    assert (target_eps is None) ^ (target_deltas is None)\n    eps_deltas = []\n    log_moments = sess.run(self._log_moments)\n    log_moments_with_order = zip(self._moment_orders, log_moments)\n    if target_eps is not None:\n      for eps in target_eps:\n        eps_deltas.append(\n            EpsDelta(eps, self._compute_delta(log_moments_with_order, eps)))\n    else:\n      assert target_deltas\n      for delta in target_deltas:\n        eps_deltas.append(\n            EpsDelta(self._compute_eps(log_moments_with_order, delta), delta))\n    return eps_deltas\n\n\nclass GaussianMomentsAccountant(MomentsAccountant):\n  """"""MomentsAccountant which assumes Gaussian noise.\n\n  GaussianMomentsAccountant assumes the noise added is centered Gaussian\n  noise N(0, sigma^2 I). In this case, we can compute the differential moments\n  accurately using a formula.\n\n  For asymptotic bound, for Gaussian noise with variance sigma^2, we can show\n  for L < sigma^2,  q L < sigma,\n    log E[exp(L X)] = O(q^2 L^2 / sigma^2).\n  Using this we derive that for training T epoches, with batch ratio q,\n  the Gaussian mechanism with variance sigma^2 (with q < 1/sigma) is (e, d)\n  private for d = exp(T/q q^2 L^2 / sigma^2 - L e). Setting L = sigma^2,\n  Tq = e/2, the mechanism is (e, exp(-e sigma^2/2))-DP. Equivalently, the\n  mechanism is (e, d)-DP if sigma = sqrt{2 log(1/d)}/e, q < 1/sigma,\n  and T < e/(2q). This bound is better than the bound obtained using general\n  composition theorems, by an Omega(sqrt{log k}) factor on epsilon, if we run\n  k steps. Since we use direct estimate, the obtained privacy bound has tight\n  constant.\n\n  For GaussianMomentAccountant, it suffices to compute I1, as I1 >= I2,\n  which reduce to computing E(P(x+s)/P(x+s-1) - 1)^i for s = 0 and 1. In the\n  companion gaussian_moments.py file, we supply procedure for computing both\n  I1 and I2 (the computation of I2 is through multi-precision integration\n  package). It can be verified that indeed I1 >= I2 for wide range of parameters\n  we have tried, though at the moment we are unable to prove this claim.\n\n  We recommend that when using this accountant, users independently verify\n  using gaussian_moments.py that for their parameters, I1 is indeed larger\n  than I2. This can be done by following the instructions in\n  gaussian_moments.py.\n  """"""\n\n  def __init__(self, total_examples, moment_orders=32):\n    """"""Initialization.\n\n    Args:\n      total_examples: total number of examples.\n      moment_orders: the order of moments to keep.\n    """"""\n    super(self.__class__, self).__init__(total_examples, moment_orders)\n    self._binomial_table = utils.GenerateBinomialTable(self._max_moment_order)\n\n  def _differential_moments(self, sigma, s, t):\n    """"""Compute 0 to t-th differential moments for Gaussian variable.\n\n        E[(P(x+s)/P(x+s-1)-1)^t]\n      = sum_{i=0}^t (t choose i) (-1)^{t-i} E[(P(x+s)/P(x+s-1))^i]\n      = sum_{i=0}^t (t choose i) (-1)^{t-i} E[exp(-i*(2*x+2*s-1)/(2*sigma^2))]\n      = sum_{i=0}^t (t choose i) (-1)^{t-i} exp(i(i+1-2*s)/(2 sigma^2))\n    Args:\n      sigma: the noise sigma, in the multiples of the sensitivity.\n      s: the shift.\n      t: 0 to t-th moment.\n    Returns:\n      0 to t-th moment as a tensor of shape [t+1].\n    """"""\n    assert t <= self._max_moment_order, (""The order of %d is out ""\n                                         ""of the upper bound %d.""\n                                         % (t, self._max_moment_order))\n    binomial = tf.slice(self._binomial_table, [0, 0],\n                        [t + 1, t + 1])\n    signs = numpy.zeros((t + 1, t + 1), dtype=numpy.float64)\n    for i in range(t + 1):\n      for j in range(t + 1):\n        signs[i, j] = 1.0 - 2 * ((i - j) % 2)\n    exponents = tf.constant([j * (j + 1.0 - 2.0 * s) / (2.0 * sigma * sigma)\n                             for j in range(t + 1)], dtype=tf.float64)\n    # x[i, j] = binomial[i, j] * signs[i, j] = (i choose j) * (-1)^{i-j}\n    x = tf.mul(binomial, signs)\n    # y[i, j] = x[i, j] * exp(exponents[j])\n    #         = (i choose j) * (-1)^{i-j} * exp(j(j-1)/(2 sigma^2))\n    # Note: this computation is done by broadcasting pointwise multiplication\n    # between [t+1, t+1] tensor and [t+1] tensor.\n    y = tf.mul(x, tf.exp(exponents))\n    # z[i] = sum_j y[i, j]\n    #      = sum_j (i choose j) * (-1)^{i-j} * exp(j(j-1)/(2 sigma^2))\n    z = tf.reduce_sum(y, 1)\n    return z\n\n  def _compute_log_moment(self, sigma, q, moment_order):\n    """"""Compute high moment of privacy loss.\n\n    Args:\n      sigma: the noise sigma, in the multiples of the sensitivity.\n      q: the sampling ratio.\n      moment_order: the order of moment.\n    Returns:\n      log E[exp(moment_order * X)]\n    """"""\n    assert moment_order <= self._max_moment_order, (""The order of %d is out ""\n                                                    ""of the upper bound %d.""\n                                                    % (moment_order,\n                                                       self._max_moment_order))\n    binomial_table = tf.slice(self._binomial_table, [moment_order, 0],\n                              [1, moment_order + 1])\n    # qs = [1 q q^2 ... q^L] = exp([0 1 2 ... L] * log(q))\n    qs = tf.exp(tf.constant([i * 1.0 for i in range(moment_order + 1)],\n                            dtype=tf.float64) * tf.cast(\n                                tf.log(q), dtype=tf.float64))\n    moments0 = self._differential_moments(sigma, 0.0, moment_order)\n    term0 = tf.reduce_sum(binomial_table * qs * moments0)\n    moments1 = self._differential_moments(sigma, 1.0, moment_order)\n    term1 = tf.reduce_sum(binomial_table * qs * moments1)\n    return tf.squeeze(tf.log(tf.cast(q * term0 + (1.0 - q) * term1,\n                                     tf.float64)))\n\n\nclass DummyAccountant(object):\n  """"""An accountant that does no accounting.""""""\n\n  def accumulate_privacy_spending(self, *unused_args):\n    return tf.no_op()\n\n  def get_privacy_spent(self, unused_sess, **unused_kwargs):\n    return [EpsDelta(numpy.inf, 1.0)]\n'"
model_zoo/models/im2txt/im2txt/data/build_mscoco_data.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts MSCOCO data to TFRecord file format with SequenceExample protos.\n\nThe MSCOCO images are expected to reside in JPEG files located in the following\ndirectory structure:\n\n  train_image_dir/COCO_train2014_000000000151.jpg\n  train_image_dir/COCO_train2014_000000000260.jpg\n  ...\n\nand\n\n  val_image_dir/COCO_val2014_000000000042.jpg\n  val_image_dir/COCO_val2014_000000000073.jpg\n  ...\n\nThe MSCOCO annotations JSON files are expected to reside in train_captions_file\nand val_captions_file respectively.\n\nThis script converts the combined MSCOCO data into sharded data files consisting\nof 256, 4 and 8 TFRecord files, respectively:\n\n  output_dir/train-00000-of-00256\n  output_dir/train-00001-of-00256\n  ...\n  output_dir/train-00255-of-00256\n\nand\n\n  output_dir/val-00000-of-00004\n  ...\n  output_dir/val-00003-of-00004\n\nand\n\n  output_dir/test-00000-of-00008\n  ...\n  output_dir/test-00007-of-00008\n\nEach TFRecord file contains ~2300 records. Each record within the TFRecord file\nis a serialized SequenceExample proto consisting of precisely one image-caption\npair. Note that each image has multiple captions (usually 5) and therefore each\nimage is replicated multiple times in the TFRecord files.\n\nThe SequenceExample proto contains the following fields:\n\n  context:\n    image/image_id: integer MSCOCO image identifier\n    image/data: string containing JPEG encoded image in RGB colorspace\n\n  feature_lists:\n    image/caption: list of strings containing the (tokenized) caption words\n    image/caption_ids: list of integer ids corresponding to the caption words\n\nThe captions are tokenized using the NLTK (http://www.nltk.org/) word tokenizer.\nThe vocabulary of word identifiers is constructed from the sorted list (by\ndescending frequency) of word tokens in the training set. Only tokens appearing\nat least 4 times are considered; all other words get the ""unknown"" word id.\n\nNOTE: This script will consume around 100GB of disk space because each image\nin the MSCOCO dataset is replicated ~5 times (once per caption) in the output.\nThis is done for two reasons:\n  1. In order to better shuffle the training data.\n  2. It makes it easier to perform asynchronous preprocessing of each image in\n     TensorFlow.\n\nRunning this script using 16 threads may take around 1 hour on a HP Z420.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import Counter\nfrom collections import namedtuple\nfrom datetime import datetime\nimport json\nimport os.path\nimport random\nimport sys\nimport threading\n\n\n\nimport nltk.tokenize\nimport numpy as np\nimport tensorflow as tf\n\ntf.flags.DEFINE_string(""train_image_dir"", ""/tmp/train2014/"",\n                       ""Training image directory."")\ntf.flags.DEFINE_string(""val_image_dir"", ""/tmp/val2014"",\n                       ""Validation image directory."")\n\ntf.flags.DEFINE_string(""train_captions_file"", ""/tmp/captions_train2014.json"",\n                       ""Training captions JSON file."")\ntf.flags.DEFINE_string(""val_captions_file"", ""/tmp/captions_val2014.json"",\n                       ""Validation captions JSON file."")\n\ntf.flags.DEFINE_string(""output_dir"", ""/tmp/"", ""Output data directory."")\n\ntf.flags.DEFINE_integer(""train_shards"", 256,\n                        ""Number of shards in training TFRecord files."")\ntf.flags.DEFINE_integer(""val_shards"", 4,\n                        ""Number of shards in validation TFRecord files."")\ntf.flags.DEFINE_integer(""test_shards"", 8,\n                        ""Number of shards in testing TFRecord files."")\n\ntf.flags.DEFINE_string(""start_word"", ""<S>"",\n                       ""Special word added to the beginning of each sentence."")\ntf.flags.DEFINE_string(""end_word"", ""</S>"",\n                       ""Special word added to the end of each sentence."")\ntf.flags.DEFINE_string(""unknown_word"", ""<UNK>"",\n                       ""Special word meaning \'unknown\'."")\ntf.flags.DEFINE_integer(""min_word_count"", 4,\n                        ""The minimum number of occurrences of each word in the ""\n                        ""training set for inclusion in the vocabulary."")\ntf.flags.DEFINE_string(""word_counts_output_file"", ""/tmp/word_counts.txt"",\n                       ""Output vocabulary file of word counts."")\n\ntf.flags.DEFINE_integer(""num_threads"", 8,\n                        ""Number of threads to preprocess the images."")\n\nFLAGS = tf.flags.FLAGS\n\nImageMetadata = namedtuple(""ImageMetadata"",\n                           [""image_id"", ""filename"", ""captions""])\n\n\nclass Vocabulary(object):\n  """"""Simple vocabulary wrapper.""""""\n\n  def __init__(self, vocab, unk_id):\n    """"""Initializes the vocabulary.\n\n    Args:\n      vocab: A dictionary of word to word_id.\n      unk_id: Id of the special \'unknown\' word.\n    """"""\n    self._vocab = vocab\n    self._unk_id = unk_id\n\n  def word_to_id(self, word):\n    """"""Returns the integer id of a word string.""""""\n    if word in self._vocab:\n      return self._vocab[word]\n    else:\n      return self._unk_id\n\n\nclass ImageDecoder(object):\n  """"""Helper class for decoding images in TensorFlow.""""""\n\n  def __init__(self):\n    # Create a single TensorFlow Session for all image decoding calls.\n    self._sess = tf.Session()\n\n    # TensorFlow ops for JPEG decoding.\n    self._encoded_jpeg = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._encoded_jpeg, channels=3)\n\n  def decode_jpeg(self, encoded_jpeg):\n    image = self._sess.run(self._decode_jpeg,\n                           feed_dict={self._encoded_jpeg: encoded_jpeg})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _int64_feature(value):\n  """"""Wrapper for inserting an int64 Feature into a SequenceExample proto.""""""\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n  """"""Wrapper for inserting a bytes Feature into a SequenceExample proto.""""""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))\n\n\ndef _int64_feature_list(values):\n  """"""Wrapper for inserting an int64 FeatureList into a SequenceExample proto.""""""\n  return tf.train.FeatureList(feature=[_int64_feature(v) for v in values])\n\n\ndef _bytes_feature_list(values):\n  """"""Wrapper for inserting a bytes FeatureList into a SequenceExample proto.""""""\n  return tf.train.FeatureList(feature=[_bytes_feature(v) for v in values])\n\n\ndef _to_sequence_example(image, decoder, vocab):\n  """"""Builds a SequenceExample proto for an image-caption pair.\n\n  Args:\n    image: An ImageMetadata object.\n    decoder: An ImageDecoder object.\n    vocab: A Vocabulary object.\n\n  Returns:\n    A SequenceExample proto.\n  """"""\n  with tf.gfile.FastGFile(image.filename, ""r"") as f:\n    encoded_image = f.read()\n\n  try:\n    decoder.decode_jpeg(encoded_image)\n  except (tf.errors.InvalidArgumentError, AssertionError):\n    print(""Skipping file with invalid JPEG data: %s"" % image.filename)\n    return\n\n  context = tf.train.Features(feature={\n      ""image/image_id"": _int64_feature(image.image_id),\n      ""image/data"": _bytes_feature(encoded_image),\n  })\n\n  assert len(image.captions) == 1\n  caption = image.captions[0]\n  caption_ids = [vocab.word_to_id(word) for word in caption]\n  feature_lists = tf.train.FeatureLists(feature_list={\n      ""image/caption"": _bytes_feature_list(caption),\n      ""image/caption_ids"": _int64_feature_list(caption_ids)\n  })\n  sequence_example = tf.train.SequenceExample(\n      context=context, feature_lists=feature_lists)\n\n  return sequence_example\n\n\ndef _process_image_files(thread_index, ranges, name, images, decoder, vocab,\n                         num_shards):\n  """"""Processes and saves a subset of images as TFRecord files in one thread.\n\n  Args:\n    thread_index: Integer thread identifier within [0, len(ranges)].\n    ranges: A list of pairs of integers specifying the ranges of the dataset to\n      process in parallel.\n    name: Unique identifier specifying the dataset.\n    images: List of ImageMetadata.\n    decoder: An ImageDecoder object.\n    vocab: A Vocabulary object.\n    num_shards: Integer number of shards for the output files.\n  """"""\n  # Each thread produces N shards where N = num_shards / num_threads. For\n  # instance, if num_shards = 128, and num_threads = 2, then the first thread\n  # would produce shards [0, 64).\n  num_threads = len(ranges)\n  assert not num_shards % num_threads\n  num_shards_per_batch = int(num_shards / num_threads)\n\n  shard_ranges = np.linspace(ranges[thread_index][0], ranges[thread_index][1],\n                             num_shards_per_batch + 1).astype(int)\n  num_images_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n\n  counter = 0\n  for s in xrange(num_shards_per_batch):\n    # Generate a sharded version of the file name, e.g. \'train-00002-of-00010\'\n    shard = thread_index * num_shards_per_batch + s\n    output_filename = ""%s-%.5d-of-%.5d"" % (name, shard, num_shards)\n    output_file = os.path.join(FLAGS.output_dir, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    shard_counter = 0\n    images_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n    for i in images_in_shard:\n      image = images[i]\n\n      sequence_example = _to_sequence_example(image, decoder, vocab)\n      if sequence_example is not None:\n        writer.write(sequence_example.SerializeToString())\n        shard_counter += 1\n        counter += 1\n\n      if not counter % 1000:\n        print(""%s [thread %d]: Processed %d of %d items in thread batch."" %\n              (datetime.now(), thread_index, counter, num_images_in_thread))\n        sys.stdout.flush()\n\n    writer.close()\n    print(""%s [thread %d]: Wrote %d image-caption pairs to %s"" %\n          (datetime.now(), thread_index, shard_counter, output_file))\n    sys.stdout.flush()\n    shard_counter = 0\n  print(""%s [thread %d]: Wrote %d image-caption pairs to %d shards."" %\n        (datetime.now(), thread_index, counter, num_shards_per_batch))\n  sys.stdout.flush()\n\n\ndef _process_dataset(name, images, vocab, num_shards):\n  """"""Processes a complete data set and saves it as a TFRecord.\n\n  Args:\n    name: Unique identifier specifying the dataset.\n    images: List of ImageMetadata.\n    vocab: A Vocabulary object.\n    num_shards: Integer number of shards for the output files.\n  """"""\n  # Break up each image into a separate entity for each caption.\n  images = [ImageMetadata(image.image_id, image.filename, [caption])\n            for image in images for caption in image.captions]\n\n  # Shuffle the ordering of images. Make the randomization repeatable.\n  random.seed(12345)\n  random.shuffle(images)\n\n  # Break the images into num_threads batches. Batch i is defined as\n  # images[ranges[i][0]:ranges[i][1]].\n  num_threads = min(num_shards, FLAGS.num_threads)\n  spacing = np.linspace(0, len(images), num_threads + 1).astype(np.int)\n  ranges = []\n  threads = []\n  for i in xrange(len(spacing) - 1):\n    ranges.append([spacing[i], spacing[i + 1]])\n\n  # Create a mechanism for monitoring when all threads are finished.\n  coord = tf.train.Coordinator()\n\n  # Create a utility for decoding JPEG images to run sanity checks.\n  decoder = ImageDecoder()\n\n  # Launch a thread for each batch.\n  print(""Launching %d threads for spacings: %s"" % (num_threads, ranges))\n  for thread_index in xrange(len(ranges)):\n    args = (thread_index, ranges, name, images, decoder, vocab, num_shards)\n    t = threading.Thread(target=_process_image_files, args=args)\n    t.start()\n    threads.append(t)\n\n  # Wait for all the threads to terminate.\n  coord.join(threads)\n  print(""%s: Finished processing all %d image-caption pairs in data set \'%s\'."" %\n        (datetime.now(), len(images), name))\n\n\ndef _create_vocab(captions):\n  """"""Creates the vocabulary of word to word_id.\n\n  The vocabulary is saved to disk in a text file of word counts. The id of each\n  word in the file is its corresponding 0-based line number.\n\n  Args:\n    captions: A list of lists of strings.\n\n  Returns:\n    A Vocabulary object.\n  """"""\n  print(""Creating vocabulary."")\n  counter = Counter()\n  for c in captions:\n    counter.update(c)\n  print(""Total words:"", len(counter))\n\n  # Filter uncommon words and sort by descending count.\n  word_counts = [x for x in counter.items() if x[1] >= FLAGS.min_word_count]\n  word_counts.sort(key=lambda x: x[1], reverse=True)\n  print(""Words in vocabulary:"", len(word_counts))\n\n  # Write out the word counts file.\n  with tf.gfile.FastGFile(FLAGS.word_counts_output_file, ""w"") as f:\n    f.write(""\\n"".join([""%s %d"" % (w, c) for w, c in word_counts]))\n  print(""Wrote vocabulary file:"", FLAGS.word_counts_output_file)\n\n  # Create the vocabulary dictionary.\n  reverse_vocab = [x[0] for x in word_counts]\n  unk_id = len(reverse_vocab)\n  vocab_dict = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])\n  vocab = Vocabulary(vocab_dict, unk_id)\n\n  return vocab\n\n\ndef _process_caption(caption):\n  """"""Processes a caption string into a list of tonenized words.\n\n  Args:\n    caption: A string caption.\n\n  Returns:\n    A list of strings; the tokenized caption.\n  """"""\n  tokenized_caption = [FLAGS.start_word]\n  tokenized_caption.extend(nltk.tokenize.word_tokenize(caption.lower()))\n  tokenized_caption.append(FLAGS.end_word)\n  return tokenized_caption\n\n\ndef _load_and_process_metadata(captions_file, image_dir):\n  """"""Loads image metadata from a JSON file and processes the captions.\n\n  Args:\n    captions_file: JSON file containing caption annotations.\n    image_dir: Directory containing the image files.\n\n  Returns:\n    A list of ImageMetadata.\n  """"""\n  with tf.gfile.FastGFile(captions_file, ""r"") as f:\n    caption_data = json.load(f)\n\n  # Extract the filenames.\n  id_to_filename = [(x[""id""], x[""file_name""]) for x in caption_data[""images""]]\n\n  # Extract the captions. Each image_id is associated with multiple captions.\n  id_to_captions = {}\n  for annotation in caption_data[""annotations""]:\n    image_id = annotation[""image_id""]\n    caption = annotation[""caption""]\n    id_to_captions.setdefault(image_id, [])\n    id_to_captions[image_id].append(caption)\n\n  assert len(id_to_filename) == len(id_to_captions)\n  assert set([x[0] for x in id_to_filename]) == set(id_to_captions.keys())\n  print(""Loaded caption metadata for %d images from %s"" %\n        (len(id_to_filename), captions_file))\n\n  # Process the captions and combine the data into a list of ImageMetadata.\n  print(""Proccessing captions."")\n  image_metadata = []\n  num_captions = 0\n  for image_id, base_filename in id_to_filename:\n    filename = os.path.join(image_dir, base_filename)\n    captions = [_process_caption(c) for c in id_to_captions[image_id]]\n    image_metadata.append(ImageMetadata(image_id, filename, captions))\n    num_captions += len(captions)\n  print(""Finished processing %d captions for %d images in %s"" %\n        (num_captions, len(id_to_filename), captions_file))\n\n  return image_metadata\n\n\ndef main(unused_argv):\n  def _is_valid_num_shards(num_shards):\n    """"""Returns True if num_shards is compatible with FLAGS.num_threads.""""""\n    return num_shards < FLAGS.num_threads or not num_shards % FLAGS.num_threads\n\n  assert _is_valid_num_shards(FLAGS.train_shards), (\n      ""Please make the FLAGS.num_threads commensurate with FLAGS.train_shards"")\n  assert _is_valid_num_shards(FLAGS.val_shards), (\n      ""Please make the FLAGS.num_threads commensurate with FLAGS.val_shards"")\n  assert _is_valid_num_shards(FLAGS.test_shards), (\n      ""Please make the FLAGS.num_threads commensurate with FLAGS.test_shards"")\n\n  if not tf.gfile.IsDirectory(FLAGS.output_dir):\n    tf.gfile.MakeDirs(FLAGS.output_dir)\n\n  # Load image metadata from caption files.\n  mscoco_train_dataset = _load_and_process_metadata(FLAGS.train_captions_file,\n                                                    FLAGS.train_image_dir)\n  mscoco_val_dataset = _load_and_process_metadata(FLAGS.val_captions_file,\n                                                  FLAGS.val_image_dir)\n\n  # Redistribute the MSCOCO data as follows:\n  #   train_dataset = 100% of mscoco_train_dataset + 85% of mscoco_val_dataset.\n  #   val_dataset = 5% of mscoco_val_dataset (for validation during training).\n  #   test_dataset = 10% of mscoco_val_dataset (for final evaluation).\n  train_cutoff = int(0.85 * len(mscoco_val_dataset))\n  val_cutoff = int(0.90 * len(mscoco_val_dataset))\n  train_dataset = mscoco_train_dataset + mscoco_val_dataset[0:train_cutoff]\n  val_dataset = mscoco_val_dataset[train_cutoff:val_cutoff]\n  test_dataset = mscoco_val_dataset[val_cutoff:]\n\n  # Create vocabulary from the training captions.\n  train_captions = [c for image in train_dataset for c in image.captions]\n  vocab = _create_vocab(train_captions)\n\n  _process_dataset(""train"", train_dataset, vocab, FLAGS.train_shards)\n  _process_dataset(""val"", val_dataset, vocab, FLAGS.val_shards)\n  _process_dataset(""test"", test_dataset, vocab, FLAGS.test_shards)\n\n\nif __name__ == ""__main__"":\n  tf.app.run()\n'"
model_zoo/models/im2txt/im2txt/inference_utils/caption_generator.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Class for generating captions from an image-to-text model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport heapq\nimport math\n\n\nimport numpy as np\n\n\nclass Caption(object):\n  """"""Represents a complete or partial caption.""""""\n\n  def __init__(self, sentence, state, logprob, score, metadata=None):\n    """"""Initializes the Caption.\n\n    Args:\n      sentence: List of word ids in the caption.\n      state: Model state after generating the previous word.\n      logprob: Log-probability of the caption.\n      score: Score of the caption.\n      metadata: Optional metadata associated with the partial sentence. If not\n        None, a list of strings with the same length as \'sentence\'.\n    """"""\n    self.sentence = sentence\n    self.state = state\n    self.logprob = logprob\n    self.score = score\n    self.metadata = metadata\n\n  def __cmp__(self, other):\n    """"""Compares Captions by score.""""""\n    assert isinstance(other, Caption)\n    if self.score == other.score:\n      return 0\n    elif self.score < other.score:\n      return -1\n    else:\n      return 1\n\n\nclass TopN(object):\n  """"""Maintains the top n elements of an incrementally provided set.""""""\n\n  def __init__(self, n):\n    self._n = n\n    self._data = []\n\n  def size(self):\n    assert self._data is not None\n    return len(self._data)\n\n  def push(self, x):\n    """"""Pushes a new element.""""""\n    assert self._data is not None\n    if len(self._data) < self._n:\n      heapq.heappush(self._data, x)\n    else:\n      heapq.heappushpop(self._data, x)\n\n  def extract(self, sort=False):\n    """"""Extracts all elements from the TopN. This is a destructive operation.\n\n    The only method that can be called immediately after extract() is reset().\n\n    Args:\n      sort: Whether to return the elements in descending sorted order.\n\n    Returns:\n      A list of data; the top n elements provided to the set.\n    """"""\n    assert self._data is not None\n    data = self._data\n    self._data = None\n    if sort:\n      data.sort(reverse=True)\n    return data\n\n  def reset(self):\n    """"""Returns the TopN to an empty state.""""""\n    self._data = []\n\n\nclass CaptionGenerator(object):\n  """"""Class to generate captions from an image-to-text model.""""""\n\n  def __init__(self,\n               model,\n               vocab,\n               beam_size=3,\n               max_caption_length=20,\n               length_normalization_factor=0.0):\n    """"""Initializes the generator.\n\n    Args:\n      model: Object encapsulating a trained image-to-text model. Must have\n        methods feed_image() and inference_step(). For example, an instance of\n        InferenceWrapperBase.\n      vocab: A Vocabulary object.\n      beam_size: Beam size to use when generating captions.\n      max_caption_length: The maximum caption length before stopping the search.\n      length_normalization_factor: If != 0, a number x such that captions are\n        scored by logprob/length^x, rather than logprob. This changes the\n        relative scores of captions depending on their lengths. For example, if\n        x > 0 then longer captions will be favored.\n    """"""\n    self.vocab = vocab\n    self.model = model\n\n    self.beam_size = beam_size\n    self.max_caption_length = max_caption_length\n    self.length_normalization_factor = length_normalization_factor\n\n  def beam_search(self, sess, encoded_image):\n    """"""Runs beam search caption generation on a single image.\n\n    Args:\n      sess: TensorFlow Session object.\n      encoded_image: An encoded image string.\n\n    Returns:\n      A list of Caption sorted by descending score.\n    """"""\n    # Feed in the image to get the initial state.\n    initial_state = self.model.feed_image(sess, encoded_image)\n\n    initial_beam = Caption(\n        sentence=[self.vocab.start_id],\n        state=initial_state[0],\n        logprob=0.0,\n        score=0.0,\n        metadata=[""""])\n    partial_captions = TopN(self.beam_size)\n    partial_captions.push(initial_beam)\n    complete_captions = TopN(self.beam_size)\n\n    # Run beam search.\n    for _ in range(self.max_caption_length - 1):\n      partial_captions_list = partial_captions.extract()\n      partial_captions.reset()\n      input_feed = np.array([c.sentence[-1] for c in partial_captions_list])\n      state_feed = np.array([c.state for c in partial_captions_list])\n\n      softmax, new_states, metadata = self.model.inference_step(sess,\n                                                                input_feed,\n                                                                state_feed)\n\n      for i, partial_caption in enumerate(partial_captions_list):\n        word_probabilities = softmax[i]\n        state = new_states[i]\n        # For this partial caption, get the beam_size most probable next words.\n        words_and_probs = list(enumerate(word_probabilities))\n        words_and_probs.sort(key=lambda x: -x[1])\n        words_and_probs = words_and_probs[0:self.beam_size]\n        # Each next word gives a new partial caption.\n        for w, p in words_and_probs:\n          if p < 1e-12:\n            continue  # Avoid log(0).\n          sentence = partial_caption.sentence + [w]\n          logprob = partial_caption.logprob + math.log(p)\n          score = logprob\n          if metadata:\n            metadata_list = partial_caption.metadata + [metadata[i]]\n          else:\n            metadata_list = None\n          if w == self.vocab.end_id:\n            if self.length_normalization_factor > 0:\n              score /= len(sentence)**self.length_normalization_factor\n            beam = Caption(sentence, state, logprob, score, metadata_list)\n            complete_captions.push(beam)\n          else:\n            beam = Caption(sentence, state, logprob, score, metadata_list)\n            partial_captions.push(beam)\n      if partial_captions.size() == 0:\n        # We have run out of partial candidates; happens when beam_size = 1.\n        break\n\n    # If we have no complete captions then fall back to the partial captions.\n    # But never output a mixture of complete and partial captions because a\n    # partial caption could have a higher score than all the complete captions.\n    if not complete_captions.size():\n      complete_captions = partial_captions\n\n    return complete_captions.extract(sort=True)\n'"
model_zoo/models/im2txt/im2txt/inference_utils/caption_generator_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Unit tests for CaptionGenerator.""""""\n\nimport math\n\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom im2txt.inference_utils import caption_generator\n\n\nclass FakeVocab(object):\n  """"""Fake Vocabulary for testing purposes.""""""\n\n  def __init__(self):\n    self.start_id = 0  # Word id denoting sentence start.\n    self.end_id = 1  # Word id denoting sentence end.\n\n\nclass FakeModel(object):\n  """"""Fake model for testing purposes.""""""\n\n  def __init__(self):\n    # Number of words in the vocab.\n    self._vocab_size = 12\n\n    # Dimensionality of the nominal model state.\n    self._state_size = 1\n\n    # Map of previous word to the probability distribution of the next word.\n    self._probabilities = {\n        0: {1: 0.1,\n            2: 0.2,\n            3: 0.3,\n            4: 0.4},\n        2: {5: 0.1,\n            6: 0.9},\n        3: {1: 0.1,\n            7: 0.4,\n            8: 0.5},\n        4: {1: 0.3,\n            9: 0.3,\n            10: 0.4},\n        5: {1: 1.0},\n        6: {1: 1.0},\n        7: {1: 1.0},\n        8: {1: 1.0},\n        9: {1: 0.5,\n            11: 0.5},\n        10: {1: 1.0},\n        11: {1: 1.0},\n    }\n\n  # pylint: disable=unused-argument\n\n  def feed_image(self, sess, encoded_image):\n    # Return a nominal model state.\n    return np.zeros([1, self._state_size])\n\n  def inference_step(self, sess, input_feed, state_feed):\n    # Compute the matrix of softmax distributions for the next batch of words.\n    batch_size = input_feed.shape[0]\n    softmax_output = np.zeros([batch_size, self._vocab_size])\n    for batch_index, word_id in enumerate(input_feed):\n      for next_word, probability in self._probabilities[word_id].items():\n        softmax_output[batch_index, next_word] = probability\n\n    # Nominal state and metadata.\n    new_state = np.zeros([batch_size, self._state_size])\n    metadata = None\n\n    return softmax_output, new_state, metadata\n\n  # pylint: enable=unused-argument\n\n\nclass CaptionGeneratorTest(tf.test.TestCase):\n\n  def _assertExpectedCaptions(self,\n                              expected_captions,\n                              beam_size=3,\n                              max_caption_length=20,\n                              length_normalization_factor=0):\n    """"""Tests that beam search generates the expected captions.\n\n    Args:\n      expected_captions: A sequence of pairs (sentence, probability), where\n        sentence is a list of integer ids and probability is a float in [0, 1].\n      beam_size: Parameter passed to beam_search().\n      max_caption_length: Parameter passed to beam_search().\n      length_normalization_factor: Parameter passed to beam_search().\n    """"""\n    expected_sentences = [c[0] for c in expected_captions]\n    expected_probabilities = [c[1] for c in expected_captions]\n\n    # Generate captions.\n    generator = caption_generator.CaptionGenerator(\n        model=FakeModel(),\n        vocab=FakeVocab(),\n        beam_size=beam_size,\n        max_caption_length=max_caption_length,\n        length_normalization_factor=length_normalization_factor)\n    actual_captions = generator.beam_search(sess=None, encoded_image=None)\n\n    actual_sentences = [c.sentence for c in actual_captions]\n    actual_probabilities = [math.exp(c.logprob) for c in actual_captions]\n\n    self.assertEqual(expected_sentences, actual_sentences)\n    self.assertAllClose(expected_probabilities, actual_probabilities)\n\n  def testBeamSize(self):\n    # Beam size = 1.\n    expected = [([0, 4, 10, 1], 0.16)]\n    self._assertExpectedCaptions(expected, beam_size=1)\n\n    # Beam size = 2.\n    expected = [([0, 4, 10, 1], 0.16), ([0, 3, 8, 1], 0.15)]\n    self._assertExpectedCaptions(expected, beam_size=2)\n\n    # Beam size = 3.\n    expected = [\n        ([0, 2, 6, 1], 0.18), ([0, 4, 10, 1], 0.16), ([0, 3, 8, 1], 0.15)\n    ]\n    self._assertExpectedCaptions(expected, beam_size=3)\n\n  def testMaxLength(self):\n    # Max length = 1.\n    expected = [([0], 1.0)]\n    self._assertExpectedCaptions(expected, max_caption_length=1)\n\n    # Max length = 2.\n    # There are no complete sentences, so partial sentences are returned.\n    expected = [([0, 4], 0.4), ([0, 3], 0.3), ([0, 2], 0.2)]\n    self._assertExpectedCaptions(expected, max_caption_length=2)\n\n    # Max length = 3.\n    # There is at least one complete sentence, so only complete sentences are\n    # returned.\n    expected = [([0, 4, 1], 0.12), ([0, 3, 1], 0.03)]\n    self._assertExpectedCaptions(expected, max_caption_length=3)\n\n    # Max length = 4.\n    expected = [\n        ([0, 2, 6, 1], 0.18), ([0, 4, 10, 1], 0.16), ([0, 3, 8, 1], 0.15)\n    ]\n    self._assertExpectedCaptions(expected, max_caption_length=4)\n\n  def testLengthNormalization(self):\n    # Length normalization factor = 3.\n    # The longest caption is returned first, despite having low probability,\n    # because it has the highest log(probability)/length**3.\n    expected = [\n        ([0, 4, 9, 11, 1], 0.06),\n        ([0, 2, 6, 1], 0.18),\n        ([0, 4, 10, 1], 0.16),\n        ([0, 3, 8, 1], 0.15),\n    ]\n    self._assertExpectedCaptions(\n        expected, beam_size=4, length_normalization_factor=3)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/im2txt/im2txt/inference_utils/inference_wrapper_base.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Base wrapper class for performing inference with an image-to-text model.\n\nSubclasses must implement the following methods:\n\n  build_model():\n    Builds the model for inference and returns the model object.\n\n  feed_image():\n    Takes an encoded image and returns the initial model state, where ""state""\n    is a numpy array whose specifics are defined by the subclass, e.g.\n    concatenated LSTM state. It\'s assumed that feed_image() will be called\n    precisely once at the start of inference for each image. Subclasses may\n    compute and/or save per-image internal context in this method.\n\n  inference_step():\n    Takes a batch of inputs and states at a single time-step. Returns the\n    softmax output corresponding to the inputs, and the new states of the batch.\n    Optionally also returns metadata about the current inference step, e.g. a\n    serialized numpy array containing activations from a particular model layer.\n\nClient usage:\n  1. Build the model inference graph via build_graph_from_config() or\n     build_graph_from_proto().\n  2. Call the resulting restore_fn to load the model checkpoint.\n  3. For each image in a batch of images:\n     a) Call feed_image() once to get the initial state.\n     b) For each step of caption generation, call inference_step().\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path\n\n\nimport tensorflow as tf\n\n# pylint: disable=unused-argument\n\n\nclass InferenceWrapperBase(object):\n  """"""Base wrapper class for performing inference with an image-to-text model.""""""\n\n  def __init__(self):\n    pass\n\n  def build_model(self, model_config):\n    """"""Builds the model for inference.\n\n    Args:\n      model_config: Object containing configuration for building the model.\n\n    Returns:\n      model: The model object.\n    """"""\n    tf.logging.fatal(""Please implement build_model in subclass"")\n\n  def _create_restore_fn(self, checkpoint_path, saver):\n    """"""Creates a function that restores a model from checkpoint.\n\n    Args:\n      checkpoint_path: Checkpoint file or a directory containing a checkpoint\n        file.\n      saver: Saver for restoring variables from the checkpoint file.\n\n    Returns:\n      restore_fn: A function such that restore_fn(sess) loads model variables\n        from the checkpoint file.\n\n    Raises:\n      ValueError: If checkpoint_path does not refer to a checkpoint file or a\n        directory containing a checkpoint file.\n    """"""\n    if tf.gfile.IsDirectory(checkpoint_path):\n      checkpoint_path = tf.train.latest_checkpoint(checkpoint_path)\n      if not checkpoint_path:\n        raise ValueError(""No checkpoint file found in: %s"" % checkpoint_path)\n\n    def _restore_fn(sess):\n      tf.logging.info(""Loading model from checkpoint: %s"", checkpoint_path)\n      saver.restore(sess, checkpoint_path)\n      tf.logging.info(""Successfully loaded checkpoint: %s"",\n                      os.path.basename(checkpoint_path))\n\n    return _restore_fn\n\n  def build_graph_from_config(self, model_config, checkpoint_path):\n    """"""Builds the inference graph from a configuration object.\n\n    Args:\n      model_config: Object containing configuration for building the model.\n      checkpoint_path: Checkpoint file or a directory containing a checkpoint\n        file.\n\n    Returns:\n      restore_fn: A function such that restore_fn(sess) loads model variables\n        from the checkpoint file.\n    """"""\n    tf.logging.info(""Building model."")\n    self.build_model(model_config)\n    saver = tf.train.Saver()\n\n    return self._create_restore_fn(checkpoint_path, saver)\n\n  def build_graph_from_proto(self, graph_def_file, saver_def_file,\n                             checkpoint_path):\n    """"""Builds the inference graph from serialized GraphDef and SaverDef protos.\n\n    Args:\n      graph_def_file: File containing a serialized GraphDef proto.\n      saver_def_file: File containing a serialized SaverDef proto.\n      checkpoint_path: Checkpoint file or a directory containing a checkpoint\n        file.\n\n    Returns:\n      restore_fn: A function such that restore_fn(sess) loads model variables\n        from the checkpoint file.\n    """"""\n    # Load the Graph.\n    tf.logging.info(""Loading GraphDef from file: %s"", graph_def_file)\n    graph_def = tf.GraphDef()\n    with tf.gfile.FastGFile(graph_def_file, ""rb"") as f:\n      graph_def.ParseFromString(f.read())\n    tf.import_graph_def(graph_def, name="""")\n\n    # Load the Saver.\n    tf.logging.info(""Loading SaverDef from file: %s"", saver_def_file)\n    saver_def = tf.train.SaverDef()\n    with tf.gfile.FastGFile(saver_def_file, ""rb"") as f:\n      saver_def.ParseFromString(f.read())\n    saver = tf.train.Saver(saver_def=saver_def)\n\n    return self._create_restore_fn(checkpoint_path, saver)\n\n  def feed_image(self, sess, encoded_image):\n    """"""Feeds an image and returns the initial model state.\n\n    See comments at the top of file.\n\n    Args:\n      sess: TensorFlow Session object.\n      encoded_image: An encoded image string.\n\n    Returns:\n      state: A numpy array of shape [1, state_size].\n    """"""\n    tf.logging.fatal(""Please implement feed_image in subclass"")\n\n  def inference_step(self, sess, input_feed, state_feed):\n    """"""Runs one step of inference.\n\n    Args:\n      sess: TensorFlow Session object.\n      input_feed: A numpy array of shape [batch_size].\n      state_feed: A numpy array of shape [batch_size, state_size].\n\n    Returns:\n      softmax_output: A numpy array of shape [batch_size, vocab_size].\n      new_state: A numpy array of shape [batch_size, state_size].\n      metadata: Optional. If not None, a string containing metadata about the\n        current inference step (e.g. serialized numpy array containing\n        activations from a particular model layer.).\n    """"""\n    tf.logging.fatal(""Please implement inference_step in subclass"")\n\n# pylint: enable=unused-argument\n'"
model_zoo/models/im2txt/im2txt/inference_utils/vocabulary.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Vocabulary class for an image-to-text model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\n\nclass Vocabulary(object):\n  """"""Vocabulary class for an image-to-text model.""""""\n\n  def __init__(self,\n               vocab_file,\n               start_word=""<S>"",\n               end_word=""</S>"",\n               unk_word=""<UNK>""):\n    """"""Initializes the vocabulary.\n\n    Args:\n      vocab_file: File containing the vocabulary, where the words are the first\n        whitespace-separated token on each line (other tokens are ignored) and\n        the word ids are the corresponding line numbers.\n      start_word: Special word denoting sentence start.\n      end_word: Special word denoting sentence end.\n      unk_word: Special word denoting unknown words.\n    """"""\n    if not tf.gfile.Exists(vocab_file):\n      tf.logging.fatal(""Vocab file %s not found."", vocab_file)\n    tf.logging.info(""Initializing vocabulary from file: %s"", vocab_file)\n\n    with tf.gfile.GFile(vocab_file, mode=""r"") as f:\n      reverse_vocab = list(f.readlines())\n    reverse_vocab = [line.split()[0] for line in reverse_vocab]\n    assert start_word in reverse_vocab\n    assert end_word in reverse_vocab\n    if unk_word not in reverse_vocab:\n      reverse_vocab.append(unk_word)\n    vocab = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])\n\n    tf.logging.info(""Created vocabulary with %d words"" % len(vocab))\n\n    self.vocab = vocab  # vocab[word] = id\n    self.reverse_vocab = reverse_vocab  # reverse_vocab[id] = word\n\n    # Save special word ids.\n    self.start_id = vocab[start_word]\n    self.end_id = vocab[end_word]\n    self.unk_id = vocab[unk_word]\n\n  def word_to_id(self, word):\n    """"""Returns the integer word id of a word string.""""""\n    if word in self.vocab:\n      return self.vocab[word]\n    else:\n      return self.unk_id\n\n  def id_to_word(self, word_id):\n    """"""Returns the word string of an integer word id.""""""\n    if word_id >= len(self.reverse_vocab):\n      return self.reverse_vocab[self.unk_id]\n    else:\n      return self.reverse_vocab[word_id]\n'"
model_zoo/models/im2txt/im2txt/ops/image_embedding.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Image embedding ops.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_base\n\nslim = tf.contrib.slim\n\n\ndef inception_v3(images,\n                 trainable=True,\n                 is_training=True,\n                 weight_decay=0.00004,\n                 stddev=0.1,\n                 dropout_keep_prob=0.8,\n                 use_batch_norm=True,\n                 batch_norm_params=None,\n                 add_summaries=True,\n                 scope=""InceptionV3""):\n  """"""Builds an Inception V3 subgraph for image embeddings.\n\n  Args:\n    images: A float32 Tensor of shape [batch, height, width, channels].\n    trainable: Whether the inception submodel should be trainable or not.\n    is_training: Boolean indicating training mode or not.\n    weight_decay: Coefficient for weight regularization.\n    stddev: The standard deviation of the trunctated normal weight initializer.\n    dropout_keep_prob: Dropout keep probability.\n    use_batch_norm: Whether to use batch normalization.\n    batch_norm_params: Parameters for batch normalization. See\n      tf.contrib.layers.batch_norm for details.\n    add_summaries: Whether to add activation summaries.\n    scope: Optional Variable scope.\n\n  Returns:\n    end_points: A dictionary of activations from inception_v3 layers.\n  """"""\n  # Only consider the inception model to be in training mode if it\'s trainable.\n  is_inception_model_training = trainable and is_training\n\n  if use_batch_norm:\n    # Default parameters for batch normalization.\n    if not batch_norm_params:\n      batch_norm_params = {\n          ""is_training"": is_inception_model_training,\n          ""trainable"": trainable,\n          # Decay for the moving averages.\n          ""decay"": 0.9997,\n          # Epsilon to prevent 0s in variance.\n          ""epsilon"": 0.001,\n          # Collection containing the moving mean and moving variance.\n          ""variables_collections"": {\n              ""beta"": None,\n              ""gamma"": None,\n              ""moving_mean"": [""moving_vars""],\n              ""moving_variance"": [""moving_vars""],\n          }\n      }\n  else:\n    batch_norm_params = None\n\n  if trainable:\n    weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n  else:\n    weights_regularizer = None\n\n  with tf.variable_scope(scope, ""InceptionV3"", [images]) as scope:\n    with slim.arg_scope(\n        [slim.conv2d, slim.fully_connected],\n        weights_regularizer=weights_regularizer,\n        trainable=trainable):\n      with slim.arg_scope(\n          [slim.conv2d],\n          weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n          activation_fn=tf.nn.relu,\n          normalizer_fn=slim.batch_norm,\n          normalizer_params=batch_norm_params):\n        net, end_points = inception_v3_base(images, scope=scope)\n        with tf.variable_scope(""logits""):\n          shape = net.get_shape()\n          net = slim.avg_pool2d(net, shape[1:3], padding=""VALID"", scope=""pool"")\n          net = slim.dropout(\n              net,\n              keep_prob=dropout_keep_prob,\n              is_training=is_inception_model_training,\n              scope=""dropout"")\n          net = slim.flatten(net, scope=""flatten"")\n\n  # Add summaries.\n  if add_summaries:\n    for v in end_points.values():\n      tf.contrib.layers.summaries.summarize_activation(v)\n\n  return net\n'"
model_zoo/models/im2txt/im2txt/ops/image_embedding_test.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Tests for tensorflow_models.im2txt.ops.image_embedding.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom im2txt.ops import image_embedding\n\n\nclass InceptionV3Test(tf.test.TestCase):\n\n  def setUp(self):\n    super(InceptionV3Test, self).setUp()\n\n    batch_size = 4\n    height = 299\n    width = 299\n    num_channels = 3\n    self._images = tf.placeholder(tf.float32,\n                                  [batch_size, height, width, num_channels])\n    self._batch_size = batch_size\n\n  def _countInceptionParameters(self):\n    """"""Counts the number of parameters in the inception model at top scope.""""""\n    counter = {}\n    for v in tf.all_variables():\n      name_tokens = v.op.name.split(""/"")\n      if name_tokens[0] == ""InceptionV3"":\n        name = ""InceptionV3/"" + name_tokens[1]\n        num_params = v.get_shape().num_elements()\n        assert num_params\n        counter[name] = counter.get(name, 0) + num_params\n    return counter\n\n  def _verifyParameterCounts(self):\n    """"""Verifies the number of parameters in the inception model.""""""\n    param_counts = self._countInceptionParameters()\n    expected_param_counts = {\n        ""InceptionV3/Conv2d_1a_3x3"": 960,\n        ""InceptionV3/Conv2d_2a_3x3"": 9312,\n        ""InceptionV3/Conv2d_2b_3x3"": 18624,\n        ""InceptionV3/Conv2d_3b_1x1"": 5360,\n        ""InceptionV3/Conv2d_4a_3x3"": 138816,\n        ""InceptionV3/Mixed_5b"": 256368,\n        ""InceptionV3/Mixed_5c"": 277968,\n        ""InceptionV3/Mixed_5d"": 285648,\n        ""InceptionV3/Mixed_6a"": 1153920,\n        ""InceptionV3/Mixed_6b"": 1298944,\n        ""InceptionV3/Mixed_6c"": 1692736,\n        ""InceptionV3/Mixed_6d"": 1692736,\n        ""InceptionV3/Mixed_6e"": 2143872,\n        ""InceptionV3/Mixed_7a"": 1699584,\n        ""InceptionV3/Mixed_7b"": 5047872,\n        ""InceptionV3/Mixed_7c"": 6080064,\n    }\n    self.assertDictEqual(expected_param_counts, param_counts)\n\n  def _assertCollectionSize(self, expected_size, collection):\n    actual_size = len(tf.get_collection(collection))\n    if expected_size != actual_size:\n      self.fail(""Found %d items in collection %s (expected %d)."" %\n                (actual_size, collection, expected_size))\n\n  def testTrainableTrueIsTrainingTrue(self):\n    embeddings = image_embedding.inception_v3(\n        self._images, trainable=True, is_training=True)\n    self.assertEqual([self._batch_size, 2048], embeddings.get_shape().as_list())\n\n    self._verifyParameterCounts()\n    self._assertCollectionSize(376, tf.GraphKeys.VARIABLES)\n    self._assertCollectionSize(188, tf.GraphKeys.TRAINABLE_VARIABLES)\n    self._assertCollectionSize(188, tf.GraphKeys.UPDATE_OPS)\n    self._assertCollectionSize(94, tf.GraphKeys.REGULARIZATION_LOSSES)\n    self._assertCollectionSize(0, tf.GraphKeys.LOSSES)\n    self._assertCollectionSize(23, tf.GraphKeys.SUMMARIES)\n\n  def testTrainableTrueIsTrainingFalse(self):\n    embeddings = image_embedding.inception_v3(\n        self._images, trainable=True, is_training=False)\n    self.assertEqual([self._batch_size, 2048], embeddings.get_shape().as_list())\n\n    self._verifyParameterCounts()\n    self._assertCollectionSize(376, tf.GraphKeys.VARIABLES)\n    self._assertCollectionSize(188, tf.GraphKeys.TRAINABLE_VARIABLES)\n    self._assertCollectionSize(0, tf.GraphKeys.UPDATE_OPS)\n    self._assertCollectionSize(94, tf.GraphKeys.REGULARIZATION_LOSSES)\n    self._assertCollectionSize(0, tf.GraphKeys.LOSSES)\n    self._assertCollectionSize(23, tf.GraphKeys.SUMMARIES)\n\n  def testTrainableFalseIsTrainingTrue(self):\n    embeddings = image_embedding.inception_v3(\n        self._images, trainable=False, is_training=True)\n    self.assertEqual([self._batch_size, 2048], embeddings.get_shape().as_list())\n\n    self._verifyParameterCounts()\n    self._assertCollectionSize(376, tf.GraphKeys.VARIABLES)\n    self._assertCollectionSize(0, tf.GraphKeys.TRAINABLE_VARIABLES)\n    self._assertCollectionSize(0, tf.GraphKeys.UPDATE_OPS)\n    self._assertCollectionSize(0, tf.GraphKeys.REGULARIZATION_LOSSES)\n    self._assertCollectionSize(0, tf.GraphKeys.LOSSES)\n    self._assertCollectionSize(23, tf.GraphKeys.SUMMARIES)\n\n  def testTrainableFalseIsTrainingFalse(self):\n    embeddings = image_embedding.inception_v3(\n        self._images, trainable=False, is_training=False)\n    self.assertEqual([self._batch_size, 2048], embeddings.get_shape().as_list())\n\n    self._verifyParameterCounts()\n    self._assertCollectionSize(376, tf.GraphKeys.VARIABLES)\n    self._assertCollectionSize(0, tf.GraphKeys.TRAINABLE_VARIABLES)\n    self._assertCollectionSize(0, tf.GraphKeys.UPDATE_OPS)\n    self._assertCollectionSize(0, tf.GraphKeys.REGULARIZATION_LOSSES)\n    self._assertCollectionSize(0, tf.GraphKeys.LOSSES)\n    self._assertCollectionSize(23, tf.GraphKeys.SUMMARIES)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
model_zoo/models/im2txt/im2txt/ops/image_processing.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Helper functions for image preprocessing.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\n\ndef distort_image(image, thread_id):\n  """"""Perform random distortions on an image.\n\n  Args:\n    image: A float32 Tensor of shape [height, width, 3] with values in [0, 1).\n    thread_id: Preprocessing thread id used to select the ordering of color\n      distortions. There should be a multiple of 2 preprocessing threads.\n\n  Returns:\n    distorted_image: A float32 Tensor of shape [height, width, 3] with values in\n      [0, 1].\n  """"""\n  # Randomly flip horizontally.\n  with tf.name_scope(""flip_horizontal"", values=[image]):\n    image = tf.image.random_flip_left_right(image)\n\n  # Randomly distort the colors based on thread id.\n  color_ordering = thread_id % 2\n  with tf.name_scope(""distort_color"", values=[image]):\n    if color_ordering == 0:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.032)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n    elif color_ordering == 1:\n      image = tf.image.random_brightness(image, max_delta=32. / 255.)\n      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n      image = tf.image.random_hue(image, max_delta=0.032)\n\n    # The random_* ops do not necessarily clamp.\n    image = tf.clip_by_value(image, 0.0, 1.0)\n\n  return image\n\n\ndef process_image(encoded_image,\n                  is_training,\n                  height,\n                  width,\n                  resize_height=346,\n                  resize_width=346,\n                  thread_id=0,\n                  image_format=""jpeg""):\n  """"""Decode an image, resize and apply random distortions.\n\n  In training, images are distorted slightly differently depending on thread_id.\n\n  Args:\n    encoded_image: String Tensor containing the image.\n    is_training: Boolean; whether preprocessing for training or eval.\n    height: Height of the output image.\n    width: Width of the output image.\n    resize_height: If > 0, resize height before crop to final dimensions.\n    resize_width: If > 0, resize width before crop to final dimensions.\n    thread_id: Preprocessing thread id used to select the ordering of color\n      distortions. There should be a multiple of 2 preprocessing threads.\n    image_format: ""jpeg"" or ""png"".\n\n  Returns:\n    A float32 Tensor of shape [height, width, 3] with values in [-1, 1].\n\n  Raises:\n    ValueError: If image_format is invalid.\n  """"""\n  # Helper function to log an image summary to the visualizer. Summaries are\n  # only logged in thread 0.\n  def image_summary(name, image):\n    if not thread_id:\n      tf.image_summary(name, tf.expand_dims(image, 0))\n\n  # Decode image into a float32 Tensor of shape [?, ?, 3] with values in [0, 1).\n  with tf.name_scope(""decode"", values=[encoded_image]):\n    if image_format == ""jpeg"":\n      image = tf.image.decode_jpeg(encoded_image, channels=3)\n    elif image_format == ""png"":\n      image = tf.image.decode_png(encoded_image, channels=3)\n    else:\n      raise ValueError(""Invalid image format: %s"" % image_format)\n  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n  image_summary(""original_image"", image)\n\n  # Resize image.\n  assert (resize_height > 0) == (resize_width > 0)\n  if resize_height:\n    image = tf.image.resize_images(image,\n                                   size=[resize_height, resize_width],\n                                   method=tf.image.ResizeMethod.BILINEAR)\n\n  # Crop to final dimensions.\n  if is_training:\n    image = tf.random_crop(image, [height, width, 3])\n  else:\n    # Central crop, assuming resize_height > height, resize_width > width.\n    image = tf.image.resize_image_with_crop_or_pad(image, height, width)\n\n  image_summary(""resized_image"", image)\n\n  # Randomly distort the image.\n  if is_training:\n    image = distort_image(image, thread_id)\n\n  image_summary(""final_image"", image)\n\n  # Rescale to [-1,1] instead of [0, 1]\n  image = tf.sub(image, 0.5)\n  image = tf.mul(image, 2.0)\n  return image\n'"
model_zoo/models/im2txt/im2txt/ops/inputs.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Input ops.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\n\ndef parse_sequence_example(serialized, image_feature, caption_feature):\n  """"""Parses a tensorflow.SequenceExample into an image and caption.\n\n  Args:\n    serialized: A scalar string Tensor; a single serialized SequenceExample.\n    image_feature: Name of SequenceExample context feature containing image\n      data.\n    caption_feature: Name of SequenceExample feature list containing integer\n      captions.\n\n  Returns:\n    encoded_image: A scalar string Tensor containing a JPEG encoded image.\n    caption: A 1-D uint64 Tensor with dynamically specified length.\n  """"""\n  context, sequence = tf.parse_single_sequence_example(\n      serialized,\n      context_features={\n          image_feature: tf.FixedLenFeature([], dtype=tf.string)\n      },\n      sequence_features={\n          caption_feature: tf.FixedLenSequenceFeature([], dtype=tf.int64),\n      })\n\n  encoded_image = context[image_feature]\n  caption = sequence[caption_feature]\n  return encoded_image, caption\n\n\ndef prefetch_input_data(reader,\n                        file_pattern,\n                        is_training,\n                        batch_size,\n                        values_per_shard,\n                        input_queue_capacity_factor=16,\n                        num_reader_threads=1,\n                        shard_queue_name=""filename_queue"",\n                        value_queue_name=""input_queue""):\n  """"""Prefetches string values from disk into an input queue.\n\n  In training the capacity of the queue is important because a larger queue\n  means better mixing of training examples between shards. The minimum number of\n  values kept in the queue is values_per_shard * input_queue_capacity_factor,\n  where input_queue_memory factor should be chosen to trade-off better mixing\n  with memory usage.\n\n  Args:\n    reader: Instance of tf.ReaderBase.\n    file_pattern: Comma-separated list of file patterns (e.g.\n        /tmp/train_data-?????-of-00100).\n    is_training: Boolean; whether prefetching for training or eval.\n    batch_size: Model batch size used to determine queue capacity.\n    values_per_shard: Approximate number of values per shard.\n    input_queue_capacity_factor: Minimum number of values to keep in the queue\n      in multiples of values_per_shard. See comments above.\n    num_reader_threads: Number of reader threads to fill the queue.\n    shard_queue_name: Name for the shards filename queue.\n    value_queue_name: Name for the values input queue.\n\n  Returns:\n    A Queue containing prefetched string values.\n  """"""\n  data_files = []\n  for pattern in file_pattern.split("",""):\n    data_files.extend(tf.gfile.Glob(pattern))\n  if not data_files:\n    tf.logging.fatal(""Found no input files matching %s"", file_pattern)\n  else:\n    tf.logging.info(""Prefetching values from %d files matching %s"",\n                    len(data_files), file_pattern)\n\n  if is_training:\n    filename_queue = tf.train.string_input_producer(\n        data_files, shuffle=True, capacity=16, name=shard_queue_name)\n    min_queue_examples = values_per_shard * input_queue_capacity_factor\n    capacity = min_queue_examples + 100 * batch_size\n    values_queue = tf.RandomShuffleQueue(\n        capacity=capacity,\n        min_after_dequeue=min_queue_examples,\n        dtypes=[tf.string],\n        name=""random_"" + value_queue_name)\n  else:\n    filename_queue = tf.train.string_input_producer(\n        data_files, shuffle=False, capacity=1, name=shard_queue_name)\n    capacity = values_per_shard + 3 * batch_size\n    values_queue = tf.FIFOQueue(\n        capacity=capacity, dtypes=[tf.string], name=""fifo_"" + value_queue_name)\n\n  enqueue_ops = []\n  for _ in range(num_reader_threads):\n    _, value = reader.read(filename_queue)\n    enqueue_ops.append(values_queue.enqueue([value]))\n  tf.train.queue_runner.add_queue_runner(tf.train.queue_runner.QueueRunner(\n      values_queue, enqueue_ops))\n  tf.scalar_summary(\n      ""queue/%s/fraction_of_%d_full"" % (values_queue.name, capacity),\n      tf.cast(values_queue.size(), tf.float32) * (1. / capacity))\n\n  return values_queue\n\n\ndef batch_with_dynamic_pad(images_and_captions,\n                           batch_size,\n                           queue_capacity,\n                           add_summaries=True):\n  """"""Batches input images and captions.\n\n  This function splits the caption into an input sequence and a target sequence,\n  where the target sequence is the input sequence right-shifted by 1. Input and\n  target sequences are batched and padded up to the maximum length of sequences\n  in the batch. A mask is created to distinguish real words from padding words.\n\n  Example:\n    Actual captions in the batch (\'-\' denotes padded character):\n      [\n        [ 1 2 5 4 5 ],\n        [ 1 2 3 4 - ],\n        [ 1 2 3 - - ],\n      ]\n\n    input_seqs:\n      [\n        [ 1 2 3 4 ],\n        [ 1 2 3 - ],\n        [ 1 2 - - ],\n      ]\n\n    target_seqs:\n      [\n        [ 2 3 4 5 ],\n        [ 2 3 4 - ],\n        [ 2 3 - - ],\n      ]\n\n    mask:\n      [\n        [ 1 1 1 1 ],\n        [ 1 1 1 0 ],\n        [ 1 1 0 0 ],\n      ]\n\n  Args:\n    images_and_captions: A list of pairs [image, caption], where image is a\n      Tensor of shape [height, width, channels] and caption is a 1-D Tensor of\n      any length. Each pair will be processed and added to the queue in a\n      separate thread.\n    batch_size: Batch size.\n    queue_capacity: Queue capacity.\n    add_summaries: If true, add caption length summaries.\n\n  Returns:\n    images: A Tensor of shape [batch_size, height, width, channels].\n    input_seqs: An int32 Tensor of shape [batch_size, padded_length].\n    target_seqs: An int32 Tensor of shape [batch_size, padded_length].\n    mask: An int32 0/1 Tensor of shape [batch_size, padded_length].\n  """"""\n  enqueue_list = []\n  for image, caption in images_and_captions:\n    caption_length = tf.shape(caption)[0]\n    input_length = tf.expand_dims(tf.sub(caption_length, 1), 0)\n\n    input_seq = tf.slice(caption, [0], input_length)\n    target_seq = tf.slice(caption, [1], input_length)\n    indicator = tf.ones(input_length, dtype=tf.int32)\n    enqueue_list.append([image, input_seq, target_seq, indicator])\n\n  images, input_seqs, target_seqs, mask = tf.train.batch_join(\n      enqueue_list,\n      batch_size=batch_size,\n      capacity=queue_capacity,\n      dynamic_pad=True,\n      name=""batch_and_pad"")\n\n  if add_summaries:\n    lengths = tf.add(tf.reduce_sum(mask, 1), 1)\n    tf.scalar_summary(""caption_length/batch_min"", tf.reduce_min(lengths))\n    tf.scalar_summary(""caption_length/batch_max"", tf.reduce_max(lengths))\n    tf.scalar_summary(""caption_length/batch_mean"", tf.reduce_mean(lengths))\n\n  return images, input_seqs, target_seqs, mask\n'"
model_zoo/models/inception/inception/data/build_image_data.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts image data to TFRecords file format with Example protos.\n\nThe image data set is expected to reside in JPEG files located in the\nfollowing directory structure.\n\n  data_dir/label_0/image0.jpeg\n  data_dir/label_0/image1.jpg\n  ...\n  data_dir/label_1/weird-image.jpeg\n  data_dir/label_1/my-image.jpeg\n  ...\n\nwhere the sub-directory is the unique label associated with these images.\n\nThis TensorFlow script converts the training and evaluation data into\na sharded data set consisting of TFRecord files\n\n  train_directory/train-00000-of-01024\n  train_directory/train-00001-of-01024\n  ...\n  train_directory/train-00127-of-01024\n\nand\n\n  validation_directory/validation-00000-of-00128\n  validation_directory/validation-00001-of-00128\n  ...\n  validation_directory/validation-00127-of-00128\n\nwhere we have selected 1024 and 128 shards for each data set. Each record\nwithin the TFRecord file is a serialized Example proto. The Example proto\ncontains the following fields:\n\n  image/encoded: string containing JPEG encoded image in RGB colorspace\n  image/height: integer, image height in pixels\n  image/width: integer, image width in pixels\n  image/colorspace: string, specifying the colorspace, always \'RGB\'\n  image/channels: integer, specifying the number of channels, always 3\n  image/format: string, specifying the format, always\'JPEG\'\n\n  image/filename: string containing the basename of the image file\n            e.g. \'n01440764_10026.JPEG\' or \'ILSVRC2012_val_00000293.JPEG\'\n  image/class/label: integer specifying the index in a classification layer.\n    The label ranges from [0, num_labels] where 0 is unused and left as\n    the background class.\n  image/class/text: string specifying the human-readable version of the label\n    e.g. \'dog\'\n\nIf you data set involves bounding boxes, please look at build_imagenet_data.py.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os\nimport random\nimport sys\nimport threading\n\n\nimport numpy as np\nimport tensorflow as tf\n\ntf.app.flags.DEFINE_string(\'train_directory\', \'/tmp/\',\n                           \'Training data directory\')\ntf.app.flags.DEFINE_string(\'validation_directory\', \'/tmp/\',\n                           \'Validation data directory\')\ntf.app.flags.DEFINE_string(\'output_directory\', \'/tmp/\',\n                           \'Output data directory\')\n\ntf.app.flags.DEFINE_integer(\'train_shards\', 2,\n                            \'Number of shards in training TFRecord files.\')\ntf.app.flags.DEFINE_integer(\'validation_shards\', 2,\n                            \'Number of shards in validation TFRecord files.\')\n\ntf.app.flags.DEFINE_integer(\'num_threads\', 2,\n                            \'Number of threads to preprocess the images.\')\n\n# The labels file contains a list of valid labels are held in this file.\n# Assumes that the file contains entries as such:\n#   dog\n#   cat\n#   flower\n# where each line corresponds to a label. We map each label contained in\n# the file to an integer corresponding to the line number starting from 0.\ntf.app.flags.DEFINE_string(\'labels_file\', \'\', \'Labels file\')\n\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n  """"""Wrapper for inserting int64 features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _bytes_feature(value):\n  """"""Wrapper for inserting bytes features into Example proto.""""""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _convert_to_example(filename, image_buffer, label, text, height, width):\n  """"""Build an Example proto for an example.\n\n  Args:\n    filename: string, path to an image file, e.g., \'/path/to/example.JPG\'\n    image_buffer: string, JPEG encoding of RGB image\n    label: integer, identifier for the ground truth for the network\n    text: string, unique human-readable, e.g. \'dog\'\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n  Returns:\n    Example proto\n  """"""\n\n  colorspace = \'RGB\'\n  channels = 3\n  image_format = \'JPEG\'\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      \'image/height\': _int64_feature(height),\n      \'image/width\': _int64_feature(width),\n      \'image/colorspace\': _bytes_feature(colorspace),\n      \'image/channels\': _int64_feature(channels),\n      \'image/class/label\': _int64_feature(label),\n      \'image/class/text\': _bytes_feature(text),\n      \'image/format\': _bytes_feature(image_format),\n      \'image/filename\': _bytes_feature(os.path.basename(filename)),\n      \'image/encoded\': _bytes_feature(image_buffer)}))\n  return example\n\n\nclass ImageCoder(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Create a single Session to run all image coding calls.\n    self._sess = tf.Session()\n\n    # Initializes function that converts PNG to JPEG data.\n    self._png_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_png(self._png_data, channels=3)\n    self._png_to_jpeg = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def png_to_jpeg(self, image_data):\n    return self._sess.run(self._png_to_jpeg,\n                          feed_dict={self._png_data: image_data})\n\n  def decode_jpeg(self, image_data):\n    image = self._sess.run(self._decode_jpeg,\n                           feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _is_png(filename):\n  """"""Determine if a file contains a PNG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a PNG.\n  """"""\n  return \'.png\' in filename\n\n\ndef _process_image(filename, coder):\n  """"""Process a single image file.\n\n  Args:\n    filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n  Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    height: integer, image height in pixels.\n    width: integer, image width in pixels.\n  """"""\n  # Read the image file.\n  with tf.gfile.FastGFile(filename, \'r\') as f:\n    image_data = f.read()\n\n  # Convert any PNG to JPEG\'s for consistency.\n  if _is_png(filename):\n    print(\'Converting PNG to JPEG for %s\' % filename)\n    image_data = coder.png_to_jpeg(image_data)\n\n  # Decode the RGB JPEG.\n  image = coder.decode_jpeg(image_data)\n\n  # Check that image converted to RGB\n  assert len(image.shape) == 3\n  height = image.shape[0]\n  width = image.shape[1]\n  assert image.shape[2] == 3\n\n  return image_data, height, width\n\n\ndef _process_image_files_batch(coder, thread_index, ranges, name, filenames,\n                               texts, labels, num_shards):\n  """"""Processes and saves list of images as TFRecord in 1 thread.\n\n  Args:\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    thread_index: integer, unique batch to run index is within [0, len(ranges)).\n    ranges: list of pairs of integers specifying ranges of each batches to\n      analyze in parallel.\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    texts: list of strings; each string is human readable, e.g. \'dog\'\n    labels: list of integer; each integer identifies the ground truth\n    num_shards: integer number of shards for this data set.\n  """"""\n  # Each thread produces N shards where N = int(num_shards / num_threads).\n  # For instance, if num_shards = 128, and the num_threads = 2, then the first\n  # thread would produce shards [0, 64).\n  num_threads = len(ranges)\n  assert not num_shards % num_threads\n  num_shards_per_batch = int(num_shards / num_threads)\n\n  shard_ranges = np.linspace(ranges[thread_index][0],\n                             ranges[thread_index][1],\n                             num_shards_per_batch + 1).astype(int)\n  num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n\n  counter = 0\n  for s in xrange(num_shards_per_batch):\n    # Generate a sharded version of the file name, e.g. \'train-00002-of-00010\'\n    shard = thread_index * num_shards_per_batch + s\n    output_filename = \'%s-%.5d-of-%.5d\' % (name, shard, num_shards)\n    output_file = os.path.join(FLAGS.output_directory, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    shard_counter = 0\n    files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n      text = texts[i]\n\n      image_buffer, height, width = _process_image(filename, coder)\n\n      example = _convert_to_example(filename, image_buffer, label,\n                                    text, height, width)\n      writer.write(example.SerializeToString())\n      shard_counter += 1\n      counter += 1\n\n      if not counter % 1000:\n        print(\'%s [thread %d]: Processed %d of %d images in thread batch.\' %\n              (datetime.now(), thread_index, counter, num_files_in_thread))\n        sys.stdout.flush()\n\n    writer.close()\n    print(\'%s [thread %d]: Wrote %d images to %s\' %\n          (datetime.now(), thread_index, shard_counter, output_file))\n    sys.stdout.flush()\n    shard_counter = 0\n  print(\'%s [thread %d]: Wrote %d images to %d shards.\' %\n        (datetime.now(), thread_index, counter, num_files_in_thread))\n  sys.stdout.flush()\n\n\ndef _process_image_files(name, filenames, texts, labels, num_shards):\n  """"""Process and save list of images as TFRecord of Example protos.\n\n  Args:\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    texts: list of strings; each string is human readable, e.g. \'dog\'\n    labels: list of integer; each integer identifies the ground truth\n    num_shards: integer number of shards for this data set.\n  """"""\n  assert len(filenames) == len(texts)\n  assert len(filenames) == len(labels)\n\n  # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n  spacing = np.linspace(0, len(filenames), FLAGS.num_threads + 1).astype(np.int)\n  ranges = []\n  for i in xrange(len(spacing) - 1):\n    ranges.append([spacing[i], spacing[i+1]])\n\n  # Launch a thread for each batch.\n  print(\'Launching %d threads for spacings: %s\' % (FLAGS.num_threads, ranges))\n  sys.stdout.flush()\n\n  # Create a mechanism for monitoring when all threads are finished.\n  coord = tf.train.Coordinator()\n\n  # Create a generic TensorFlow-based utility for converting all image codings.\n  coder = ImageCoder()\n\n  threads = []\n  for thread_index in xrange(len(ranges)):\n    args = (coder, thread_index, ranges, name, filenames,\n            texts, labels, num_shards)\n    t = threading.Thread(target=_process_image_files_batch, args=args)\n    t.start()\n    threads.append(t)\n\n  # Wait for all the threads to terminate.\n  coord.join(threads)\n  print(\'%s: Finished writing all %d images in data set.\' %\n        (datetime.now(), len(filenames)))\n  sys.stdout.flush()\n\n\ndef _find_image_files(data_dir, labels_file):\n  """"""Build a list of all images files and labels in the data set.\n\n  Args:\n    data_dir: string, path to the root directory of images.\n\n      Assumes that the image data set resides in JPEG files located in\n      the following directory structure.\n\n        data_dir/dog/another-image.JPEG\n        data_dir/dog/my-image.jpg\n\n      where \'dog\' is the label associated with these images.\n\n    labels_file: string, path to the labels file.\n\n      The list of valid labels are held in this file. Assumes that the file\n      contains entries as such:\n        dog\n        cat\n        flower\n      where each line corresponds to a label. We map each label contained in\n      the file to an integer starting with the integer 0 corresponding to the\n      label contained in the first line.\n\n  Returns:\n    filenames: list of strings; each string is a path to an image file.\n    texts: list of strings; each string is the class, e.g. \'dog\'\n    labels: list of integer; each integer identifies the ground truth.\n  """"""\n  print(\'Determining list of input files and labels from %s.\' % data_dir)\n  unique_labels = [l.strip() for l in tf.gfile.FastGFile(\n      labels_file, \'r\').readlines()]\n\n  labels = []\n  filenames = []\n  texts = []\n\n  # Leave label index 0 empty as a background class.\n  label_index = 1\n\n  # Construct the list of JPEG files and labels.\n  for text in unique_labels:\n    jpeg_file_path = \'%s/%s/*\' % (data_dir, text)\n    matching_files = tf.gfile.Glob(jpeg_file_path)\n\n    labels.extend([label_index] * len(matching_files))\n    texts.extend([text] * len(matching_files))\n    filenames.extend(matching_files)\n\n    if not label_index % 100:\n      print(\'Finished finding files in %d of %d classes.\' % (\n          label_index, len(labels)))\n    label_index += 1\n\n  # Shuffle the ordering of all image files in order to guarantee\n  # random ordering of the images with respect to label in the\n  # saved TFRecord files. Make the randomization repeatable.\n  shuffled_index = range(len(filenames))\n  random.seed(12345)\n  random.shuffle(shuffled_index)\n\n  filenames = [filenames[i] for i in shuffled_index]\n  texts = [texts[i] for i in shuffled_index]\n  labels = [labels[i] for i in shuffled_index]\n\n  print(\'Found %d JPEG files across %d labels inside %s.\' %\n        (len(filenames), len(unique_labels), data_dir))\n  return filenames, texts, labels\n\n\ndef _process_dataset(name, directory, num_shards, labels_file):\n  """"""Process a complete data set and save it as a TFRecord.\n\n  Args:\n    name: string, unique identifier specifying the data set.\n    directory: string, root path to the data set.\n    num_shards: integer number of shards for this data set.\n    labels_file: string, path to the labels file.\n  """"""\n  filenames, texts, labels = _find_image_files(directory, labels_file)\n  _process_image_files(name, filenames, texts, labels, num_shards)\n\n\ndef main(unused_argv):\n  assert not FLAGS.train_shards % FLAGS.num_threads, (\n      \'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards\')\n  assert not FLAGS.validation_shards % FLAGS.num_threads, (\n      \'Please make the FLAGS.num_threads commensurate with \'\n      \'FLAGS.validation_shards\')\n  print(\'Saving results to %s\' % FLAGS.output_directory)\n\n  # Run it!\n  _process_dataset(\'validation\', FLAGS.validation_directory,\n                   FLAGS.validation_shards, FLAGS.labels_file)\n  _process_dataset(\'train\', FLAGS.train_directory,\n                   FLAGS.train_shards, FLAGS.labels_file)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/inception/inception/data/build_imagenet_data.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Converts ImageNet data to TFRecords file format with Example protos.\n\nThe raw ImageNet data set is expected to reside in JPEG files located in the\nfollowing directory structure.\n\n  data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n  data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n  ...\n\nwhere \'n01440764\' is the unique synset label associated with\nthese images.\n\nThe training data set consists of 1000 sub-directories (i.e. labels)\neach containing 1200 JPEG images for a total of 1.2M JPEG images.\n\nThe evaluation data set consists of 1000 sub-directories (i.e. labels)\neach containing 50 JPEG images for a total of 50K JPEG images.\n\nThis TensorFlow script converts the training and evaluation data into\na sharded data set consisting of 1024 and 128 TFRecord files, respectively.\n\n  train_directory/train-00000-of-01024\n  train_directory/train-00001-of-01024\n  ...\n  train_directory/train-00127-of-01024\n\nand\n\n  validation_directory/validation-00000-of-00128\n  validation_directory/validation-00001-of-00128\n  ...\n  validation_directory/validation-00127-of-00128\n\nEach validation TFRecord file contains ~390 records. Each training TFREcord\nfile contains ~1250 records. Each record within the TFRecord file is a\nserialized Example proto. The Example proto contains the following fields:\n\n  image/encoded: string containing JPEG encoded image in RGB colorspace\n  image/height: integer, image height in pixels\n  image/width: integer, image width in pixels\n  image/colorspace: string, specifying the colorspace, always \'RGB\'\n  image/channels: integer, specifying the number of channels, always 3\n  image/format: string, specifying the format, always\'JPEG\'\n\n  image/filename: string containing the basename of the image file\n            e.g. \'n01440764_10026.JPEG\' or \'ILSVRC2012_val_00000293.JPEG\'\n  image/class/label: integer specifying the index in a classification layer.\n    The label ranges from [1, 1000] where 0 is not used.\n  image/class/synset: string specifying the unique ID of the label,\n    e.g. \'n01440764\'\n  image/class/text: string specifying the human-readable version of the label\n    e.g. \'red fox, Vulpes vulpes\'\n\n  image/object/bbox/xmin: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/xmax: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/ymin: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/ymax: list of integers specifying the 0+ human annotated\n    bounding boxes\n  image/object/bbox/label: integer specifying the index in a classification\n    layer. The label ranges from [1, 1000] where 0 is not used. Note this is\n    always identical to the image label.\n\nNote that the length of xmin is identical to the length of xmax, ymin and ymax\nfor each example.\n\nRunning this script using 16 threads may take around ~2.5 hours on a HP Z420.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os\nimport random\nimport sys\nimport threading\n\n\nimport numpy as np\nimport tensorflow as tf\n\ntf.app.flags.DEFINE_string(\'train_directory\', \'/tmp/\',\n                           \'Training data directory\')\ntf.app.flags.DEFINE_string(\'validation_directory\', \'/tmp/\',\n                           \'Validation data directory\')\ntf.app.flags.DEFINE_string(\'output_directory\', \'/tmp/\',\n                           \'Output data directory\')\n\ntf.app.flags.DEFINE_integer(\'train_shards\', 1024,\n                            \'Number of shards in training TFRecord files.\')\ntf.app.flags.DEFINE_integer(\'validation_shards\', 128,\n                            \'Number of shards in validation TFRecord files.\')\n\ntf.app.flags.DEFINE_integer(\'num_threads\', 8,\n                            \'Number of threads to preprocess the images.\')\n\n# The labels file contains a list of valid labels are held in this file.\n# Assumes that the file contains entries as such:\n#   n01440764\n#   n01443537\n#   n01484850\n# where each line corresponds to a label expressed as a synset. We map\n# each synset contained in the file to an integer (based on the alphabetical\n# ordering). See below for details.\ntf.app.flags.DEFINE_string(\'labels_file\',\n                           \'imagenet_lsvrc_2015_synsets.txt\',\n                           \'Labels file\')\n\n# This file containing mapping from synset to human-readable label.\n# Assumes each line of the file looks like:\n#\n#   n02119247    black fox\n#   n02119359    silver fox\n#   n02119477    red fox, Vulpes fulva\n#\n# where each line corresponds to a unique mapping. Note that each line is\n# formatted as <synset>\\t<human readable label>.\ntf.app.flags.DEFINE_string(\'imagenet_metadata_file\',\n                           \'imagenet_metadata.txt\',\n                           \'ImageNet metadata file\')\n\n# This file is the output of process_bounding_box.py\n# Assumes each line of the file looks like:\n#\n#   n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n#\n# where each line corresponds to one bounding box annotation associated\n# with an image. Each line can be parsed as:\n#\n#   <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n#\n# Note that there might exist mulitple bounding box annotations associated\n# with an image file.\ntf.app.flags.DEFINE_string(\'bounding_box_file\',\n                           \'./imagenet_2012_bounding_boxes.csv\',\n                           \'Bounding box file\')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _int64_feature(value):\n  """"""Wrapper for inserting int64 features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef _float_feature(value):\n  """"""Wrapper for inserting float features into Example proto.""""""\n  if not isinstance(value, list):\n    value = [value]\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef _bytes_feature(value):\n  """"""Wrapper for inserting bytes features into Example proto.""""""\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _convert_to_example(filename, image_buffer, label, synset, human, bbox,\n                        height, width):\n  """"""Build an Example proto for an example.\n\n  Args:\n    filename: string, path to an image file, e.g., \'/path/to/example.JPG\'\n    image_buffer: string, JPEG encoding of RGB image\n    label: integer, identifier for the ground truth for the network\n    synset: string, unique WordNet ID specifying the label, e.g., \'n02323233\'\n    human: string, human-readable label, e.g., \'red fox, Vulpes vulpes\'\n    bbox: list of bounding boxes; each box is a list of integers\n      specifying [xmin, ymin, xmax, ymax]. All boxes are assumed to belong to\n      the same label as the image label.\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n  Returns:\n    Example proto\n  """"""\n  xmin = []\n  ymin = []\n  xmax = []\n  ymax = []\n  for b in bbox:\n    assert len(b) == 4\n    # pylint: disable=expression-not-assigned\n    [l.append(point) for l, point in zip([xmin, ymin, xmax, ymax], b)]\n    # pylint: enable=expression-not-assigned\n\n  colorspace = \'RGB\'\n  channels = 3\n  image_format = \'JPEG\'\n\n  example = tf.train.Example(features=tf.train.Features(feature={\n      \'image/height\': _int64_feature(height),\n      \'image/width\': _int64_feature(width),\n      \'image/colorspace\': _bytes_feature(colorspace),\n      \'image/channels\': _int64_feature(channels),\n      \'image/class/label\': _int64_feature(label),\n      \'image/class/synset\': _bytes_feature(synset),\n      \'image/class/text\': _bytes_feature(human),\n      \'image/object/bbox/xmin\': _float_feature(xmin),\n      \'image/object/bbox/xmax\': _float_feature(xmax),\n      \'image/object/bbox/ymin\': _float_feature(ymin),\n      \'image/object/bbox/ymax\': _float_feature(ymax),\n      \'image/object/bbox/label\': _int64_feature([label] * len(xmin)),\n      \'image/format\': _bytes_feature(image_format),\n      \'image/filename\': _bytes_feature(os.path.basename(filename)),\n      \'image/encoded\': _bytes_feature(image_buffer)}))\n  return example\n\n\nclass ImageCoder(object):\n  """"""Helper class that provides TensorFlow image coding utilities.""""""\n\n  def __init__(self):\n    # Create a single Session to run all image coding calls.\n    self._sess = tf.Session()\n\n    # Initializes function that converts PNG to JPEG data.\n    self._png_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_png(self._png_data, channels=3)\n    self._png_to_jpeg = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that converts CMYK JPEG data to RGB JPEG data.\n    self._cmyk_data = tf.placeholder(dtype=tf.string)\n    image = tf.image.decode_jpeg(self._cmyk_data, channels=0)\n    self._cmyk_to_rgb = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n\n    # Initializes function that decodes RGB JPEG data.\n    self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n    self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n  def png_to_jpeg(self, image_data):\n    return self._sess.run(self._png_to_jpeg,\n                          feed_dict={self._png_data: image_data})\n\n  def cmyk_to_rgb(self, image_data):\n    return self._sess.run(self._cmyk_to_rgb,\n                          feed_dict={self._cmyk_data: image_data})\n\n  def decode_jpeg(self, image_data):\n    image = self._sess.run(self._decode_jpeg,\n                           feed_dict={self._decode_jpeg_data: image_data})\n    assert len(image.shape) == 3\n    assert image.shape[2] == 3\n    return image\n\n\ndef _is_png(filename):\n  """"""Determine if a file contains a PNG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a PNG.\n  """"""\n  # File list from:\n  # https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU\n  return \'n02105855_2933.JPEG\' in filename\n\n\ndef _is_cmyk(filename):\n  """"""Determine if file contains a CMYK JPEG format image.\n\n  Args:\n    filename: string, path of the image file.\n\n  Returns:\n    boolean indicating if the image is a JPEG encoded with CMYK color space.\n  """"""\n  # File list from:\n  # https://github.com/cytsai/ilsvrc-cmyk-image-list\n  blacklist = [\'n01739381_1309.JPEG\', \'n02077923_14822.JPEG\',\n               \'n02447366_23489.JPEG\', \'n02492035_15739.JPEG\',\n               \'n02747177_10752.JPEG\', \'n03018349_4028.JPEG\',\n               \'n03062245_4620.JPEG\', \'n03347037_9675.JPEG\',\n               \'n03467068_12171.JPEG\', \'n03529860_11437.JPEG\',\n               \'n03544143_17228.JPEG\', \'n03633091_5218.JPEG\',\n               \'n03710637_5125.JPEG\', \'n03961711_5286.JPEG\',\n               \'n04033995_2932.JPEG\', \'n04258138_17003.JPEG\',\n               \'n04264628_27969.JPEG\', \'n04336792_7448.JPEG\',\n               \'n04371774_5854.JPEG\', \'n04596742_4225.JPEG\',\n               \'n07583066_647.JPEG\', \'n13037406_4650.JPEG\']\n  return filename.split(\'/\')[-1] in blacklist\n\n\ndef _process_image(filename, coder):\n  """"""Process a single image file.\n\n  Args:\n    filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n  Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    height: integer, image height in pixels.\n    width: integer, image width in pixels.\n  """"""\n  # Read the image file.\n  with tf.gfile.FastGFile(filename, \'r\') as f:\n    image_data = f.read()\n\n  # Clean the dirty data.\n  if _is_png(filename):\n    # 1 image is a PNG.\n    print(\'Converting PNG to JPEG for %s\' % filename)\n    image_data = coder.png_to_jpeg(image_data)\n  elif _is_cmyk(filename):\n    # 22 JPEG images are in CMYK colorspace.\n    print(\'Converting CMYK to RGB for %s\' % filename)\n    image_data = coder.cmyk_to_rgb(image_data)\n\n  # Decode the RGB JPEG.\n  image = coder.decode_jpeg(image_data)\n\n  # Check that image converted to RGB\n  assert len(image.shape) == 3\n  height = image.shape[0]\n  width = image.shape[1]\n  assert image.shape[2] == 3\n\n  return image_data, height, width\n\n\ndef _process_image_files_batch(coder, thread_index, ranges, name, filenames,\n                               synsets, labels, humans, bboxes, num_shards):\n  """"""Processes and saves list of images as TFRecord in 1 thread.\n\n  Args:\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    thread_index: integer, unique batch to run index is within [0, len(ranges)).\n    ranges: list of pairs of integers specifying ranges of each batches to\n      analyze in parallel.\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    synsets: list of strings; each string is a unique WordNet ID\n    labels: list of integer; each integer identifies the ground truth\n    humans: list of strings; each string is a human-readable label\n    bboxes: list of bounding boxes for each image. Note that each entry in this\n      list might contain from 0+ entries corresponding to the number of bounding\n      box annotations for the image.\n    num_shards: integer number of shards for this data set.\n  """"""\n  # Each thread produces N shards where N = int(num_shards / num_threads).\n  # For instance, if num_shards = 128, and the num_threads = 2, then the first\n  # thread would produce shards [0, 64).\n  num_threads = len(ranges)\n  assert not num_shards % num_threads\n  num_shards_per_batch = int(num_shards / num_threads)\n\n  shard_ranges = np.linspace(ranges[thread_index][0],\n                             ranges[thread_index][1],\n                             num_shards_per_batch + 1).astype(int)\n  num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n\n  counter = 0\n  for s in xrange(num_shards_per_batch):\n    # Generate a sharded version of the file name, e.g. \'train-00002-of-00010\'\n    shard = thread_index * num_shards_per_batch + s\n    output_filename = \'%s-%.5d-of-%.5d\' % (name, shard, num_shards)\n    output_file = os.path.join(FLAGS.output_directory, output_filename)\n    writer = tf.python_io.TFRecordWriter(output_file)\n\n    shard_counter = 0\n    files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n    for i in files_in_shard:\n      filename = filenames[i]\n      label = labels[i]\n      synset = synsets[i]\n      human = humans[i]\n      bbox = bboxes[i]\n\n      image_buffer, height, width = _process_image(filename, coder)\n\n      example = _convert_to_example(filename, image_buffer, label,\n                                    synset, human, bbox,\n                                    height, width)\n      writer.write(example.SerializeToString())\n      shard_counter += 1\n      counter += 1\n\n      if not counter % 1000:\n        print(\'%s [thread %d]: Processed %d of %d images in thread batch.\' %\n              (datetime.now(), thread_index, counter, num_files_in_thread))\n        sys.stdout.flush()\n\n    writer.close()\n    print(\'%s [thread %d]: Wrote %d images to %s\' %\n          (datetime.now(), thread_index, shard_counter, output_file))\n    sys.stdout.flush()\n    shard_counter = 0\n  print(\'%s [thread %d]: Wrote %d images to %d shards.\' %\n        (datetime.now(), thread_index, counter, num_files_in_thread))\n  sys.stdout.flush()\n\n\ndef _process_image_files(name, filenames, synsets, labels, humans,\n                         bboxes, num_shards):\n  """"""Process and save list of images as TFRecord of Example protos.\n\n  Args:\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    synsets: list of strings; each string is a unique WordNet ID\n    labels: list of integer; each integer identifies the ground truth\n    humans: list of strings; each string is a human-readable label\n    bboxes: list of bounding boxes for each image. Note that each entry in this\n      list might contain from 0+ entries corresponding to the number of bounding\n      box annotations for the image.\n    num_shards: integer number of shards for this data set.\n  """"""\n  assert len(filenames) == len(synsets)\n  assert len(filenames) == len(labels)\n  assert len(filenames) == len(humans)\n  assert len(filenames) == len(bboxes)\n\n  # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n  spacing = np.linspace(0, len(filenames), FLAGS.num_threads + 1).astype(np.int)\n  ranges = []\n  threads = []\n  for i in xrange(len(spacing) - 1):\n    ranges.append([spacing[i], spacing[i+1]])\n\n  # Launch a thread for each batch.\n  print(\'Launching %d threads for spacings: %s\' % (FLAGS.num_threads, ranges))\n  sys.stdout.flush()\n\n  # Create a mechanism for monitoring when all threads are finished.\n  coord = tf.train.Coordinator()\n\n  # Create a generic TensorFlow-based utility for converting all image codings.\n  coder = ImageCoder()\n\n  threads = []\n  for thread_index in xrange(len(ranges)):\n    args = (coder, thread_index, ranges, name, filenames,\n            synsets, labels, humans, bboxes, num_shards)\n    t = threading.Thread(target=_process_image_files_batch, args=args)\n    t.start()\n    threads.append(t)\n\n  # Wait for all the threads to terminate.\n  coord.join(threads)\n  print(\'%s: Finished writing all %d images in data set.\' %\n        (datetime.now(), len(filenames)))\n  sys.stdout.flush()\n\n\ndef _find_image_files(data_dir, labels_file):\n  """"""Build a list of all images files and labels in the data set.\n\n  Args:\n    data_dir: string, path to the root directory of images.\n\n      Assumes that the ImageNet data set resides in JPEG files located in\n      the following directory structure.\n\n        data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n        data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n\n      where \'n01440764\' is the unique synset label associated with these images.\n\n    labels_file: string, path to the labels file.\n\n      The list of valid labels are held in this file. Assumes that the file\n      contains entries as such:\n        n01440764\n        n01443537\n        n01484850\n      where each line corresponds to a label expressed as a synset. We map\n      each synset contained in the file to an integer (based on the alphabetical\n      ordering) starting with the integer 1 corresponding to the synset\n      contained in the first line.\n\n      The reason we start the integer labels at 1 is to reserve label 0 as an\n      unused background class.\n\n  Returns:\n    filenames: list of strings; each string is a path to an image file.\n    synsets: list of strings; each string is a unique WordNet ID.\n    labels: list of integer; each integer identifies the ground truth.\n  """"""\n  print(\'Determining list of input files and labels from %s.\' % data_dir)\n  challenge_synsets = [l.strip() for l in\n                       tf.gfile.FastGFile(labels_file, \'r\').readlines()]\n\n  labels = []\n  filenames = []\n  synsets = []\n\n  # Leave label index 0 empty as a background class.\n  label_index = 1\n\n  # Construct the list of JPEG files and labels.\n  for synset in challenge_synsets:\n    jpeg_file_path = \'%s/%s/*.JPEG\' % (data_dir, synset)\n    matching_files = tf.gfile.Glob(jpeg_file_path)\n\n    labels.extend([label_index] * len(matching_files))\n    synsets.extend([synset] * len(matching_files))\n    filenames.extend(matching_files)\n\n    if not label_index % 100:\n      print(\'Finished finding files in %d of %d classes.\' % (\n          label_index, len(challenge_synsets)))\n    label_index += 1\n\n  # Shuffle the ordering of all image files in order to guarantee\n  # random ordering of the images with respect to label in the\n  # saved TFRecord files. Make the randomization repeatable.\n  shuffled_index = range(len(filenames))\n  random.seed(12345)\n  random.shuffle(shuffled_index)\n\n  filenames = [filenames[i] for i in shuffled_index]\n  synsets = [synsets[i] for i in shuffled_index]\n  labels = [labels[i] for i in shuffled_index]\n\n  print(\'Found %d JPEG files across %d labels inside %s.\' %\n        (len(filenames), len(challenge_synsets), data_dir))\n  return filenames, synsets, labels\n\n\ndef _find_human_readable_labels(synsets, synset_to_human):\n  """"""Build a list of human-readable labels.\n\n  Args:\n    synsets: list of strings; each string is a unique WordNet ID.\n    synset_to_human: dict of synset to human labels, e.g.,\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n\n  Returns:\n    List of human-readable strings corresponding to each synset.\n  """"""\n  humans = []\n  for s in synsets:\n    assert s in synset_to_human, (\'Failed to find: %s\' % s)\n    humans.append(synset_to_human[s])\n  return humans\n\n\ndef _find_image_bounding_boxes(filenames, image_to_bboxes):\n  """"""Find the bounding boxes for a given image file.\n\n  Args:\n    filenames: list of strings; each string is a path to an image file.\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  Returns:\n    List of bounding boxes for each image. Note that each entry in this\n    list might contain from 0+ entries corresponding to the number of bounding\n    box annotations for the image.\n  """"""\n  num_image_bbox = 0\n  bboxes = []\n  for f in filenames:\n    basename = os.path.basename(f)\n    if basename in image_to_bboxes:\n      bboxes.append(image_to_bboxes[basename])\n      num_image_bbox += 1\n    else:\n      bboxes.append([])\n  print(\'Found %d images with bboxes out of %d images\' % (\n      num_image_bbox, len(filenames)))\n  return bboxes\n\n\ndef _process_dataset(name, directory, num_shards, synset_to_human,\n                     image_to_bboxes):\n  """"""Process a complete data set and save it as a TFRecord.\n\n  Args:\n    name: string, unique identifier specifying the data set.\n    directory: string, root path to the data set.\n    num_shards: integer number of shards for this data set.\n    synset_to_human: dict of synset to human labels, e.g.,\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n    image_to_bboxes: dictionary mapping image file names to a list of\n      bounding boxes. This list contains 0+ bounding boxes.\n  """"""\n  filenames, synsets, labels = _find_image_files(directory, FLAGS.labels_file)\n  humans = _find_human_readable_labels(synsets, synset_to_human)\n  bboxes = _find_image_bounding_boxes(filenames, image_to_bboxes)\n  _process_image_files(name, filenames, synsets, labels,\n                       humans, bboxes, num_shards)\n\n\ndef _build_synset_lookup(imagenet_metadata_file):\n  """"""Build lookup for synset to human-readable label.\n\n  Args:\n    imagenet_metadata_file: string, path to file containing mapping from\n      synset to human-readable label.\n\n      Assumes each line of the file looks like:\n\n        n02119247    black fox\n        n02119359    silver fox\n        n02119477    red fox, Vulpes fulva\n\n      where each line corresponds to a unique mapping. Note that each line is\n      formatted as <synset>\\t<human readable label>.\n\n  Returns:\n    Dictionary of synset to human labels, such as:\n      \'n02119022\' --> \'red fox, Vulpes vulpes\'\n  """"""\n  lines = tf.gfile.FastGFile(imagenet_metadata_file, \'r\').readlines()\n  synset_to_human = {}\n  for l in lines:\n    if l:\n      parts = l.strip().split(\'\\t\')\n      assert len(parts) == 2\n      synset = parts[0]\n      human = parts[1]\n      synset_to_human[synset] = human\n  return synset_to_human\n\n\ndef _build_bounding_box_lookup(bounding_box_file):\n  """"""Build a lookup from image file to bounding boxes.\n\n  Args:\n    bounding_box_file: string, path to file with bounding boxes annotations.\n\n      Assumes each line of the file looks like:\n\n        n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n\n      where each line corresponds to one bounding box annotation associated\n      with an image. Each line can be parsed as:\n\n        <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n\n      Note that there might exist mulitple bounding box annotations associated\n      with an image file. This file is the output of process_bounding_boxes.py.\n\n  Returns:\n    Dictionary mapping image file names to a list of bounding boxes. This list\n    contains 0+ bounding boxes.\n  """"""\n  lines = tf.gfile.FastGFile(bounding_box_file, \'r\').readlines()\n  images_to_bboxes = {}\n  num_bbox = 0\n  num_image = 0\n  for l in lines:\n    if l:\n      parts = l.split(\',\')\n      assert len(parts) == 5, (\'Failed to parse: %s\' % l)\n      filename = parts[0]\n      xmin = float(parts[1])\n      ymin = float(parts[2])\n      xmax = float(parts[3])\n      ymax = float(parts[4])\n      box = [xmin, ymin, xmax, ymax]\n\n      if filename not in images_to_bboxes:\n        images_to_bboxes[filename] = []\n        num_image += 1\n      images_to_bboxes[filename].append(box)\n      num_bbox += 1\n\n  print(\'Successfully read %d bounding boxes \'\n        \'across %d images.\' % (num_bbox, num_image))\n  return images_to_bboxes\n\n\ndef main(unused_argv):\n  assert not FLAGS.train_shards % FLAGS.num_threads, (\n      \'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards\')\n  assert not FLAGS.validation_shards % FLAGS.num_threads, (\n      \'Please make the FLAGS.num_threads commensurate with \'\n      \'FLAGS.validation_shards\')\n  print(\'Saving results to %s\' % FLAGS.output_directory)\n\n  # Build a map from synset to human-readable label.\n  synset_to_human = _build_synset_lookup(FLAGS.imagenet_metadata_file)\n  image_to_bboxes = _build_bounding_box_lookup(FLAGS.bounding_box_file)\n\n  # Run it!\n  _process_dataset(\'validation\', FLAGS.validation_directory,\n                   FLAGS.validation_shards, synset_to_human, image_to_bboxes)\n  _process_dataset(\'train\', FLAGS.train_directory, FLAGS.train_shards,\n                   synset_to_human, image_to_bboxes)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
model_zoo/models/inception/inception/data/preprocess_imagenet_validation_data.py,0,"b'#!/usr/bin/python\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Process the ImageNet Challenge bounding boxes for TensorFlow model training.\n\nAssociate the ImageNet 2012 Challenge validation data set with labels.\n\nThe raw ImageNet validation data set is expected to reside in JPEG files\nlocated in the following directory structure.\n\n data_dir/ILSVRC2012_val_00000001.JPEG\n data_dir/ILSVRC2012_val_00000002.JPEG\n ...\n data_dir/ILSVRC2012_val_00050000.JPEG\n\nThis script moves the files into a directory structure like such:\n data_dir/n01440764/ILSVRC2012_val_00000293.JPEG\n data_dir/n01440764/ILSVRC2012_val_00000543.JPEG\n ...\nwhere \'n01440764\' is the unique synset label associated with\nthese images.\n\nThis directory reorganization requires a mapping from validation image\nnumber (i.e. suffix of the original file) to the associated label. This\nis provided in the ImageNet development kit via a Matlab file.\n\nIn order to make life easier and divorce ourselves from Matlab, we instead\nsupply a custom text file that provides this mapping for us.\n\nSample usage:\n  ./preprocess_imagenet_validation_data.py ILSVRC2012_img_val \\\n  imagenet_2012_validation_synset_labels.txt\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport os.path\nimport sys\n\n\nif __name__ == \'__main__\':\n  if len(sys.argv) < 3:\n    print(\'Invalid usage\\n\'\n          \'usage: preprocess_imagenet_validation_data.py \'\n          \'<validation data dir> <validation labels file>\')\n    sys.exit(-1)\n  data_dir = sys.argv[1]\n  validation_labels_file = sys.argv[2]\n\n  # Read in the 50000 synsets associated with the validation data set.\n  labels = [l.strip() for l in open(validation_labels_file).readlines()]\n  unique_labels = set(labels)\n\n  # Make all sub-directories in the validation data dir.\n  for label in unique_labels:\n    labeled_data_dir = os.path.join(data_dir, label)\n    os.makedirs(labeled_data_dir)\n\n  # Move all of the image to the appropriate sub-directory.\n  for i in xrange(len(labels)):\n    basename = \'ILSVRC2012_val_000%.5d.JPEG\' % (i + 1)\n    original_filename = os.path.join(data_dir, basename)\n    if not os.path.exists(original_filename):\n      print(\'Failed to find: \' % original_filename)\n      sys.exit(-1)\n    new_filename = os.path.join(data_dir, labels[i], basename)\n    os.rename(original_filename, new_filename)\n'"
model_zoo/models/inception/inception/data/process_bounding_boxes.py,0,"b'#!/usr/bin/python\n# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Process the ImageNet Challenge bounding boxes for TensorFlow model training.\n\nThis script is called as\n\nprocess_bounding_boxes.py <dir> [synsets-file]\n\nWhere <dir> is a directory containing the downloaded and unpacked bounding box\ndata. If [synsets-file] is supplied, then only the bounding boxes whose\nsynstes are contained within this file are returned. Note that the\n[synsets-file] file contains synset ids, one per line.\n\nThe script dumps out a CSV text file in which each line contains an entry.\n  n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940\n\nThe entry can be read as:\n  <JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>\n\nThe bounding box for <JPEG file name> contains two points (xmin, ymin) and\n(xmax, ymax) specifying the lower-left corner and upper-right corner of a\nbounding box in *relative* coordinates.\n\nThe user supplies a directory where the XML files reside. The directory\nstructure in the directory <dir> is assumed to look like this:\n\n<dir>/nXXXXXXXX/nXXXXXXXX_YYYY.xml\n\nEach XML file contains a bounding box annotation. The script:\n\n (1) Parses the XML file and extracts the filename, label and bounding box info.\n\n (2) The bounding box is specified in the XML files as integer (xmin, ymin) and\n    (xmax, ymax) *relative* to image size displayed to the human annotator. The\n    size of the image displayed to the human annotator is stored in the XML file\n    as integer (height, width).\n\n    Note that the displayed size will differ from the actual size of the image\n    downloaded from image-net.org. To make the bounding box annotation useable,\n    we convert bounding box to floating point numbers relative to displayed\n    height and width of the image.\n\n    Note that each XML file might contain N bounding box annotations.\n\n    Note that the points are all clamped at a range of [0.0, 1.0] because some\n    human annotations extend outside the range of the supplied image.\n\n    See details here: http://image-net.org/download-bboxes\n\n(3) By default, the script outputs all valid bounding boxes. If a\n    [synsets-file] is supplied, only the subset of bounding boxes associated\n    with those synsets are outputted. Importantly, one can supply a list of\n    synsets in the ImageNet Challenge and output the list of bounding boxes\n    associated with the training images of the ILSVRC.\n\n    We use these bounding boxes to inform the random distortion of images\n    supplied to the network.\n\nIf you run this script successfully, you will see the following output\nto stderr:\n> Finished processing 544546 XML files.\n> Skipped 0 XML files not in ImageNet Challenge.\n> Skipped 0 bounding boxes not in ImageNet Challenge.\n> Wrote 615299 bounding boxes from 544546 annotated images.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os.path\nimport sys\nimport xml.etree.ElementTree as ET\n\n\nclass BoundingBox(object):\n  pass\n\n\ndef GetItem(name, root, index=0):\n  count = 0\n  for item in root.iter(name):\n    if count == index:\n      return item.text\n    count += 1\n  # Failed to find ""index"" occurrence of item.\n  return -1\n\n\ndef GetInt(name, root, index=0):\n  return int(GetItem(name, root, index))\n\n\ndef FindNumberBoundingBoxes(root):\n  index = 0\n  while True:\n    if GetInt(\'xmin\', root, index) == -1:\n      break\n    index += 1\n  return index\n\n\ndef ProcessXMLAnnotation(xml_file):\n  """"""Process a single XML file containing a bounding box.""""""\n  # pylint: disable=broad-except\n  try:\n    tree = ET.parse(xml_file)\n  except Exception:\n    print(\'Failed to parse: \' + xml_file, file=sys.stderr)\n    return None\n  # pylint: enable=broad-except\n  root = tree.getroot()\n\n  num_boxes = FindNumberBoundingBoxes(root)\n  boxes = []\n\n  for index in xrange(num_boxes):\n    box = BoundingBox()\n    # Grab the \'index\' annotation.\n    box.xmin = GetInt(\'xmin\', root, index)\n    box.ymin = GetInt(\'ymin\', root, index)\n    box.xmax = GetInt(\'xmax\', root, index)\n    box.ymax = GetInt(\'ymax\', root, index)\n\n    box.width = GetInt(\'width\', root)\n    box.height = GetInt(\'height\', root)\n    box.filename = GetItem(\'filename\', root) + \'.JPEG\'\n    box.label = GetItem(\'name\', root)\n\n    xmin = float(box.xmin) / float(box.width)\n    xmax = float(box.xmax) / float(box.width)\n    ymin = float(box.ymin) / float(box.height)\n    ymax = float(box.ymax) / float(box.height)\n\n    # Some images contain bounding box annotations that\n    # extend outside of the supplied image. See, e.g.\n    # n03127925/n03127925_147.xml\n    # Additionally, for some bounding boxes, the min > max\n    # or the box is entirely outside of the image.\n    min_x = min(xmin, xmax)\n    max_x = max(xmin, xmax)\n    box.xmin_scaled = min(max(min_x, 0.0), 1.0)\n    box.xmax_scaled = min(max(max_x, 0.0), 1.0)\n\n    min_y = min(ymin, ymax)\n    max_y = max(ymin, ymax)\n    box.ymin_scaled = min(max(min_y, 0.0), 1.0)\n    box.ymax_scaled = min(max(max_y, 0.0), 1.0)\n\n    boxes.append(box)\n\n  return boxes\n\nif __name__ == \'__main__\':\n  if len(sys.argv) < 2 or len(sys.argv) > 3:\n    print(\'Invalid usage\\n\'\n          \'usage: process_bounding_boxes.py <dir> [synsets-file]\',\n          file=sys.stderr)\n    sys.exit(-1)\n\n  xml_files = glob.glob(sys.argv[1] + \'/*/*.xml\')\n  print(\'Identified %d XML files in %s\' % (len(xml_files), sys.argv[1]),\n        file=sys.stderr)\n\n  if len(sys.argv) == 3:\n    labels = set([l.strip() for l in open(sys.argv[2]).readlines()])\n    print(\'Identified %d synset IDs in %s\' % (len(labels), sys.argv[2]),\n          file=sys.stderr)\n  else:\n    labels = None\n\n  skipped_boxes = 0\n  skipped_files = 0\n  saved_boxes = 0\n  saved_files = 0\n  for file_index, one_file in enumerate(xml_files):\n    # Example: <...>/n06470073/n00141669_6790.xml\n    label = os.path.basename(os.path.dirname(one_file))\n\n    # Determine if the annotation is from an ImageNet Challenge label.\n    if labels is not None and label not in labels:\n      skipped_files += 1\n      continue\n\n    bboxes = ProcessXMLAnnotation(one_file)\n    assert bboxes is not None, \'No bounding boxes found in \' + one_file\n\n    found_box = False\n    for bbox in bboxes:\n      if labels is not None:\n        if bbox.label != label:\n          # Note: There is a slight bug in the bounding box annotation data.\n          # Many of the dog labels have the human label \'Scottish_deerhound\'\n          # instead of the synset ID \'n02092002\' in the bbox.label field. As a\n          # simple hack to overcome this issue, we only exclude bbox labels\n          # *which are synset ID\'s* that do not match original synset label for\n          # the XML file.\n          if bbox.label in labels:\n            skipped_boxes += 1\n            continue\n\n      # Guard against improperly specified boxes.\n      if (bbox.xmin_scaled >= bbox.xmax_scaled or\n          bbox.ymin_scaled >= bbox.ymax_scaled):\n        skipped_boxes += 1\n        continue\n\n      # Note bbox.filename occasionally contains \'%s\' in the name. This is\n      # data set noise that is fixed by just using the basename of the XML file.\n      image_filename = os.path.splitext(os.path.basename(one_file))[0]\n      print(\'%s.JPEG,%.4f,%.4f,%.4f,%.4f\' %\n            (image_filename,\n             bbox.xmin_scaled, bbox.ymin_scaled,\n             bbox.xmax_scaled, bbox.ymax_scaled))\n\n      saved_boxes += 1\n      found_box = True\n    if found_box:\n      saved_files += 1\n    else:\n      skipped_files += 1\n\n    if not file_index % 5000:\n      print(\'--> processed %d of %d XML files.\' %\n            (file_index + 1, len(xml_files)),\n            file=sys.stderr)\n      print(\'--> skipped %d boxes and %d XML files.\' %\n            (skipped_boxes, skipped_files), file=sys.stderr)\n\n  print(\'Finished processing %d XML files.\' % len(xml_files), file=sys.stderr)\n  print(\'Skipped %d XML files not in ImageNet Challenge.\' % skipped_files,\n        file=sys.stderr)\n  print(\'Skipped %d bounding boxes not in ImageNet Challenge.\' % skipped_boxes,\n        file=sys.stderr)\n  print(\'Wrote %d bounding boxes from %d annotated images.\' %\n        (saved_boxes, saved_files),\n        file=sys.stderr)\n  print(\'Finished.\', file=sys.stderr)\n'"
model_zoo/models/inception/inception/slim/collections_test.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for inception.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom inception.slim import slim\n\n\ndef get_variables(scope=None):\n  return slim.variables.get_variables(scope)\n\n\ndef get_variables_by_name(name):\n  return slim.variables.get_variables_by_name(name)\n\n\nclass CollectionsTest(tf.test.TestCase):\n\n  def testVariables(self):\n    batch_size = 5\n    height, width = 299, 299\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      with slim.arg_scope([slim.ops.conv2d],\n                          batch_norm_params={\'decay\': 0.9997}):\n        slim.inception.inception_v3(inputs)\n      self.assertEqual(len(get_variables()), 388)\n      self.assertEqual(len(get_variables_by_name(\'weights\')), 98)\n      self.assertEqual(len(get_variables_by_name(\'biases\')), 2)\n      self.assertEqual(len(get_variables_by_name(\'beta\')), 96)\n      self.assertEqual(len(get_variables_by_name(\'gamma\')), 0)\n      self.assertEqual(len(get_variables_by_name(\'moving_mean\')), 96)\n      self.assertEqual(len(get_variables_by_name(\'moving_variance\')), 96)\n\n  def testVariablesWithoutBatchNorm(self):\n    batch_size = 5\n    height, width = 299, 299\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      with slim.arg_scope([slim.ops.conv2d],\n                          batch_norm_params=None):\n        slim.inception.inception_v3(inputs)\n      self.assertEqual(len(get_variables()), 196)\n      self.assertEqual(len(get_variables_by_name(\'weights\')), 98)\n      self.assertEqual(len(get_variables_by_name(\'biases\')), 98)\n      self.assertEqual(len(get_variables_by_name(\'beta\')), 0)\n      self.assertEqual(len(get_variables_by_name(\'gamma\')), 0)\n      self.assertEqual(len(get_variables_by_name(\'moving_mean\')), 0)\n      self.assertEqual(len(get_variables_by_name(\'moving_variance\')), 0)\n\n  def testVariablesByLayer(self):\n    batch_size = 5\n    height, width = 299, 299\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      with slim.arg_scope([slim.ops.conv2d],\n                          batch_norm_params={\'decay\': 0.9997}):\n        slim.inception.inception_v3(inputs)\n      self.assertEqual(len(get_variables()), 388)\n      self.assertEqual(len(get_variables(\'conv0\')), 4)\n      self.assertEqual(len(get_variables(\'conv1\')), 4)\n      self.assertEqual(len(get_variables(\'conv2\')), 4)\n      self.assertEqual(len(get_variables(\'conv3\')), 4)\n      self.assertEqual(len(get_variables(\'conv4\')), 4)\n      self.assertEqual(len(get_variables(\'mixed_35x35x256a\')), 28)\n      self.assertEqual(len(get_variables(\'mixed_35x35x288a\')), 28)\n      self.assertEqual(len(get_variables(\'mixed_35x35x288b\')), 28)\n      self.assertEqual(len(get_variables(\'mixed_17x17x768a\')), 16)\n      self.assertEqual(len(get_variables(\'mixed_17x17x768b\')), 40)\n      self.assertEqual(len(get_variables(\'mixed_17x17x768c\')), 40)\n      self.assertEqual(len(get_variables(\'mixed_17x17x768d\')), 40)\n      self.assertEqual(len(get_variables(\'mixed_17x17x768e\')), 40)\n      self.assertEqual(len(get_variables(\'mixed_8x8x2048a\')), 36)\n      self.assertEqual(len(get_variables(\'mixed_8x8x2048b\')), 36)\n      self.assertEqual(len(get_variables(\'logits\')), 2)\n      self.assertEqual(len(get_variables(\'aux_logits\')), 10)\n\n  def testVariablesToRestore(self):\n    batch_size = 5\n    height, width = 299, 299\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      with slim.arg_scope([slim.ops.conv2d],\n                          batch_norm_params={\'decay\': 0.9997}):\n        slim.inception.inception_v3(inputs)\n      variables_to_restore = tf.get_collection(\n          slim.variables.VARIABLES_TO_RESTORE)\n      self.assertEqual(len(variables_to_restore), 388)\n      self.assertListEqual(variables_to_restore, get_variables())\n\n  def testVariablesToRestoreWithoutLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      with slim.arg_scope([slim.ops.conv2d],\n                          batch_norm_params={\'decay\': 0.9997}):\n        slim.inception.inception_v3(inputs, restore_logits=False)\n      variables_to_restore = tf.get_collection(\n          slim.variables.VARIABLES_TO_RESTORE)\n      self.assertEqual(len(variables_to_restore), 384)\n\n  def testRegularizationLosses(self):\n    batch_size = 5\n    height, width = 299, 299\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      with slim.arg_scope([slim.ops.conv2d, slim.ops.fc], weight_decay=0.00004):\n        slim.inception.inception_v3(inputs)\n      losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n      self.assertEqual(len(losses), len(get_variables_by_name(\'weights\')))\n\n  def testTotalLossWithoutRegularization(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1001\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      dense_labels = tf.random_uniform((batch_size, num_classes))\n      with slim.arg_scope([slim.ops.conv2d, slim.ops.fc], weight_decay=0):\n        logits, end_points = slim.inception.inception_v3(\n            inputs,\n            num_classes=num_classes)\n        # Cross entropy loss for the main softmax prediction.\n        slim.losses.cross_entropy_loss(logits,\n                                       dense_labels,\n                                       label_smoothing=0.1,\n                                       weight=1.0)\n        # Cross entropy loss for the auxiliary softmax head.\n        slim.losses.cross_entropy_loss(end_points[\'aux_logits\'],\n                                       dense_labels,\n                                       label_smoothing=0.1,\n                                       weight=0.4,\n                                       scope=\'aux_loss\')\n      losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)\n      self.assertEqual(len(losses), 2)\n\n  def testTotalLossWithRegularization(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      dense_labels = tf.random_uniform((batch_size, num_classes))\n      with slim.arg_scope([slim.ops.conv2d, slim.ops.fc], weight_decay=0.00004):\n        logits, end_points = slim.inception.inception_v3(inputs, num_classes)\n        # Cross entropy loss for the main softmax prediction.\n        slim.losses.cross_entropy_loss(logits,\n                                       dense_labels,\n                                       label_smoothing=0.1,\n                                       weight=1.0)\n        # Cross entropy loss for the auxiliary softmax head.\n        slim.losses.cross_entropy_loss(end_points[\'aux_logits\'],\n                                       dense_labels,\n                                       label_smoothing=0.1,\n                                       weight=0.4,\n                                       scope=\'aux_loss\')\n      losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)\n      self.assertEqual(len(losses), 2)\n      reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n      self.assertEqual(len(reg_losses), 98)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/inception/inception/slim/inception_model.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Inception-v3 expressed in TensorFlow-Slim.\n\n  Usage:\n\n  # Parameters for BatchNorm.\n  batch_norm_params = {\n      # Decay for the batch_norm moving averages.\n      \'decay\': BATCHNORM_MOVING_AVERAGE_DECAY,\n      # epsilon to prevent 0s in variance.\n      \'epsilon\': 0.001,\n  }\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.ops.conv2d, slim.ops.fc], weight_decay=0.00004):\n    with slim.arg_scope([slim.ops.conv2d],\n                        stddev=0.1,\n                        activation=tf.nn.relu,\n                        batch_norm_params=batch_norm_params):\n      # Force all Variables to reside on the CPU.\n      with slim.arg_scope([slim.variables.variable], device=\'/cpu:0\'):\n        logits, endpoints = slim.inception.inception_v3(\n            images,\n            dropout_keep_prob=0.8,\n            num_classes=num_classes,\n            is_training=for_training,\n            restore_logits=restore_logits,\n            scope=scope)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom inception.slim import ops\nfrom inception.slim import scopes\n\n\ndef inception_v3(inputs,\n                 dropout_keep_prob=0.8,\n                 num_classes=1000,\n                 is_training=True,\n                 restore_logits=True,\n                 scope=\'\'):\n  """"""Latest Inception from http://arxiv.org/abs/1512.00567.\n\n    ""Rethinking the Inception Architecture for Computer Vision""\n\n    Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n    Zbigniew Wojna\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    dropout_keep_prob: dropout keep_prob.\n    num_classes: number of predicted classes.\n    is_training: whether is training or not.\n    restore_logits: whether or not the logits layers should be restored.\n      Useful for fine-tuning a model with different num_classes.\n    scope: Optional scope for op_scope.\n\n  Returns:\n    a list containing \'logits\', \'aux_logits\' Tensors.\n  """"""\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n  with tf.op_scope([inputs], scope, \'inception_v3\'):\n    with scopes.arg_scope([ops.conv2d, ops.fc, ops.batch_norm, ops.dropout],\n                          is_training=is_training):\n      with scopes.arg_scope([ops.conv2d, ops.max_pool, ops.avg_pool],\n                            stride=1, padding=\'VALID\'):\n        # 299 x 299 x 3\n        end_points[\'conv0\'] = ops.conv2d(inputs, 32, [3, 3], stride=2,\n                                         scope=\'conv0\')\n        # 149 x 149 x 32\n        end_points[\'conv1\'] = ops.conv2d(end_points[\'conv0\'], 32, [3, 3],\n                                         scope=\'conv1\')\n        # 147 x 147 x 32\n        end_points[\'conv2\'] = ops.conv2d(end_points[\'conv1\'], 64, [3, 3],\n                                         padding=\'SAME\', scope=\'conv2\')\n        # 147 x 147 x 64\n        end_points[\'pool1\'] = ops.max_pool(end_points[\'conv2\'], [3, 3],\n                                           stride=2, scope=\'pool1\')\n        # 73 x 73 x 64\n        end_points[\'conv3\'] = ops.conv2d(end_points[\'pool1\'], 80, [1, 1],\n                                         scope=\'conv3\')\n        # 73 x 73 x 80.\n        end_points[\'conv4\'] = ops.conv2d(end_points[\'conv3\'], 192, [3, 3],\n                                         scope=\'conv4\')\n        # 71 x 71 x 192.\n        end_points[\'pool2\'] = ops.max_pool(end_points[\'conv4\'], [3, 3],\n                                           stride=2, scope=\'pool2\')\n        # 35 x 35 x 192.\n        net = end_points[\'pool2\']\n      # Inception blocks\n      with scopes.arg_scope([ops.conv2d, ops.max_pool, ops.avg_pool],\n                            stride=1, padding=\'SAME\'):\n        # mixed: 35 x 35 x 256.\n        with tf.variable_scope(\'mixed_35x35x256a\'):\n          with tf.variable_scope(\'branch1x1\'):\n            branch1x1 = ops.conv2d(net, 64, [1, 1])\n          with tf.variable_scope(\'branch5x5\'):\n            branch5x5 = ops.conv2d(net, 48, [1, 1])\n            branch5x5 = ops.conv2d(branch5x5, 64, [5, 5])\n          with tf.variable_scope(\'branch3x3dbl\'):\n            branch3x3dbl = ops.conv2d(net, 64, [1, 1])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n          with tf.variable_scope(\'branch_pool\'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 32, [1, 1])\n          net = tf.concat(3, [branch1x1, branch5x5, branch3x3dbl, branch_pool])\n          end_points[\'mixed_35x35x256a\'] = net\n        # mixed_1: 35 x 35 x 288.\n        with tf.variable_scope(\'mixed_35x35x288a\'):\n          with tf.variable_scope(\'branch1x1\'):\n            branch1x1 = ops.conv2d(net, 64, [1, 1])\n          with tf.variable_scope(\'branch5x5\'):\n            branch5x5 = ops.conv2d(net, 48, [1, 1])\n            branch5x5 = ops.conv2d(branch5x5, 64, [5, 5])\n          with tf.variable_scope(\'branch3x3dbl\'):\n            branch3x3dbl = ops.conv2d(net, 64, [1, 1])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n          with tf.variable_scope(\'branch_pool\'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 64, [1, 1])\n          net = tf.concat(3, [branch1x1, branch5x5, branch3x3dbl, branch_pool])\n          end_points[\'mixed_35x35x288a\'] = net\n        # mixed_2: 35 x 35 x 288.\n        with tf.variable_scope(\'mixed_35x35x288b\'):\n          with tf.variable_scope(\'branch1x1\'):\n            branch1x1 = ops.conv2d(net, 64, [1, 1])\n          with tf.variable_scope(\'branch5x5\'):\n            branch5x5 = ops.conv2d(net, 48, [1, 1])\n            branch5x5 = ops.conv2d(branch5x5, 64, [5, 5])\n          with tf.variable_scope(\'branch3x3dbl\'):\n            branch3x3dbl = ops.conv2d(net, 64, [1, 1])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n          with tf.variable_scope(\'branch_pool\'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 64, [1, 1])\n          net = tf.concat(3, [branch1x1, branch5x5, branch3x3dbl, branch_pool])\n          end_points[\'mixed_35x35x288b\'] = net\n        # mixed_3: 17 x 17 x 768.\n        with tf.variable_scope(\'mixed_17x17x768a\'):\n          with tf.variable_scope(\'branch3x3\'):\n            branch3x3 = ops.conv2d(net, 384, [3, 3], stride=2, padding=\'VALID\')\n          with tf.variable_scope(\'branch3x3dbl\'):\n            branch3x3dbl = ops.conv2d(net, 64, [1, 1])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 96, [3, 3],\n                                      stride=2, padding=\'VALID\')\n          with tf.variable_scope(\'branch_pool\'):\n            branch_pool = ops.max_pool(net, [3, 3], stride=2, padding=\'VALID\')\n          net = tf.concat(3, [branch3x3, branch3x3dbl, branch_pool])\n          end_points[\'mixed_17x17x768a\'] = net\n        # mixed4: 17 x 17 x 768.\n        with tf.variable_scope(\'mixed_17x17x768b\'):\n          with tf.variable_scope(\'branch1x1\'):\n            branch1x1 = ops.conv2d(net, 192, [1, 1])\n          with tf.variable_scope(\'branch7x7\'):\n            branch7x7 = ops.conv2d(net, 128, [1, 1])\n            branch7x7 = ops.conv2d(branch7x7, 128, [1, 7])\n            branch7x7 = ops.conv2d(branch7x7, 192, [7, 1])\n          with tf.variable_scope(\'branch7x7dbl\'):\n            branch7x7dbl = ops.conv2d(net, 128, [1, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 128, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 128, [1, 7])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 128, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [1, 7])\n          with tf.variable_scope(\'branch_pool\'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 192, [1, 1])\n          net = tf.concat(3, [branch1x1, branch7x7, branch7x7dbl, branch_pool])\n          end_points[\'mixed_17x17x768b\'] = net\n        # mixed_5: 17 x 17 x 768.\n        with tf.variable_scope(\'mixed_17x17x768c\'):\n          with tf.variable_scope(\'branch1x1\'):\n            branch1x1 = ops.conv2d(net, 192, [1, 1])\n          with tf.variable_scope(\'branch7x7\'):\n            branch7x7 = ops.conv2d(net, 160, [1, 1])\n            branch7x7 = ops.conv2d(branch7x7, 160, [1, 7])\n            branch7x7 = ops.conv2d(branch7x7, 192, [7, 1])\n          with tf.variable_scope(\'branch7x7dbl\'):\n            branch7x7dbl = ops.conv2d(net, 160, [1, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 160, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 160, [1, 7])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 160, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [1, 7])\n          with tf.variable_scope(\'branch_pool\'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 192, [1, 1])\n          net = tf.concat(3, [branch1x1, branch7x7, branch7x7dbl, branch_pool])\n          end_points[\'mixed_17x17x768c\'] = net\n        # mixed_6: 17 x 17 x 768.\n        with tf.variable_scope(\'mixed_17x17x768d\'):\n          with tf.variable_scope(\'branch1x1\'):\n            branch1x1 = ops.conv2d(net, 192, [1, 1])\n          with tf.variable_scope(\'branch7x7\'):\n            branch7x7 = ops.conv2d(net, 160, [1, 1])\n            branch7x7 = ops.conv2d(branch7x7, 160, [1, 7])\n            branch7x7 = ops.conv2d(branch7x7, 192, [7, 1])\n          with tf.variable_scope(\'branch7x7dbl\'):\n            branch7x7dbl = ops.conv2d(net, 160, [1, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 160, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 160, [1, 7])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 160, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [1, 7])\n          with tf.variable_scope(\'branch_pool\'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 192, [1, 1])\n          net = tf.concat(3, [branch1x1, branch7x7, branch7x7dbl, branch_pool])\n          end_points[\'mixed_17x17x768d\'] = net\n        # mixed_7: 17 x 17 x 768.\n        with tf.variable_scope(\'mixed_17x17x768e\'):\n          with tf.variable_scope(\'branch1x1\'):\n            branch1x1 = ops.conv2d(net, 192, [1, 1])\n          with tf.variable_scope(\'branch7x7\'):\n            branch7x7 = ops.conv2d(net, 192, [1, 1])\n            branch7x7 = ops.conv2d(branch7x7, 192, [1, 7])\n            branch7x7 = ops.conv2d(branch7x7, 192, [7, 1])\n          with tf.variable_scope(\'branch7x7dbl\'):\n            branch7x7dbl = ops.conv2d(net, 192, [1, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [1, 7])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [7, 1])\n            branch7x7dbl = ops.conv2d(branch7x7dbl, 192, [1, 7])\n          with tf.variable_scope(\'branch_pool\'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 192, [1, 1])\n          net = tf.concat(3, [branch1x1, branch7x7, branch7x7dbl, branch_pool])\n          end_points[\'mixed_17x17x768e\'] = net\n        # Auxiliary Head logits\n        aux_logits = tf.identity(end_points[\'mixed_17x17x768e\'])\n        with tf.variable_scope(\'aux_logits\'):\n          aux_logits = ops.avg_pool(aux_logits, [5, 5], stride=3,\n                                    padding=\'VALID\')\n          aux_logits = ops.conv2d(aux_logits, 128, [1, 1], scope=\'proj\')\n          # Shape of feature map before the final layer.\n          shape = aux_logits.get_shape()\n          aux_logits = ops.conv2d(aux_logits, 768, shape[1:3], stddev=0.01,\n                                  padding=\'VALID\')\n          aux_logits = ops.flatten(aux_logits)\n          aux_logits = ops.fc(aux_logits, num_classes, activation=None,\n                              stddev=0.001, restore=restore_logits)\n          end_points[\'aux_logits\'] = aux_logits\n        # mixed_8: 8 x 8 x 1280.\n        # Note that the scope below is not changed to not void previous\n        # checkpoints.\n        # (TODO) Fix the scope when appropriate.\n        with tf.variable_scope(\'mixed_17x17x1280a\'):\n          with tf.variable_scope(\'branch3x3\'):\n            branch3x3 = ops.conv2d(net, 192, [1, 1])\n            branch3x3 = ops.conv2d(branch3x3, 320, [3, 3], stride=2,\n                                   padding=\'VALID\')\n          with tf.variable_scope(\'branch7x7x3\'):\n            branch7x7x3 = ops.conv2d(net, 192, [1, 1])\n            branch7x7x3 = ops.conv2d(branch7x7x3, 192, [1, 7])\n            branch7x7x3 = ops.conv2d(branch7x7x3, 192, [7, 1])\n            branch7x7x3 = ops.conv2d(branch7x7x3, 192, [3, 3],\n                                     stride=2, padding=\'VALID\')\n          with tf.variable_scope(\'branch_pool\'):\n            branch_pool = ops.max_pool(net, [3, 3], stride=2, padding=\'VALID\')\n          net = tf.concat(3, [branch3x3, branch7x7x3, branch_pool])\n          end_points[\'mixed_17x17x1280a\'] = net\n        # mixed_9: 8 x 8 x 2048.\n        with tf.variable_scope(\'mixed_8x8x2048a\'):\n          with tf.variable_scope(\'branch1x1\'):\n            branch1x1 = ops.conv2d(net, 320, [1, 1])\n          with tf.variable_scope(\'branch3x3\'):\n            branch3x3 = ops.conv2d(net, 384, [1, 1])\n            branch3x3 = tf.concat(3, [ops.conv2d(branch3x3, 384, [1, 3]),\n                                      ops.conv2d(branch3x3, 384, [3, 1])])\n          with tf.variable_scope(\'branch3x3dbl\'):\n            branch3x3dbl = ops.conv2d(net, 448, [1, 1])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 384, [3, 3])\n            branch3x3dbl = tf.concat(3, [ops.conv2d(branch3x3dbl, 384, [1, 3]),\n                                         ops.conv2d(branch3x3dbl, 384, [3, 1])])\n          with tf.variable_scope(\'branch_pool\'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 192, [1, 1])\n          net = tf.concat(3, [branch1x1, branch3x3, branch3x3dbl, branch_pool])\n          end_points[\'mixed_8x8x2048a\'] = net\n        # mixed_10: 8 x 8 x 2048.\n        with tf.variable_scope(\'mixed_8x8x2048b\'):\n          with tf.variable_scope(\'branch1x1\'):\n            branch1x1 = ops.conv2d(net, 320, [1, 1])\n          with tf.variable_scope(\'branch3x3\'):\n            branch3x3 = ops.conv2d(net, 384, [1, 1])\n            branch3x3 = tf.concat(3, [ops.conv2d(branch3x3, 384, [1, 3]),\n                                      ops.conv2d(branch3x3, 384, [3, 1])])\n          with tf.variable_scope(\'branch3x3dbl\'):\n            branch3x3dbl = ops.conv2d(net, 448, [1, 1])\n            branch3x3dbl = ops.conv2d(branch3x3dbl, 384, [3, 3])\n            branch3x3dbl = tf.concat(3, [ops.conv2d(branch3x3dbl, 384, [1, 3]),\n                                         ops.conv2d(branch3x3dbl, 384, [3, 1])])\n          with tf.variable_scope(\'branch_pool\'):\n            branch_pool = ops.avg_pool(net, [3, 3])\n            branch_pool = ops.conv2d(branch_pool, 192, [1, 1])\n          net = tf.concat(3, [branch1x1, branch3x3, branch3x3dbl, branch_pool])\n          end_points[\'mixed_8x8x2048b\'] = net\n        # Final pooling and prediction\n        with tf.variable_scope(\'logits\'):\n          shape = net.get_shape()\n          net = ops.avg_pool(net, shape[1:3], padding=\'VALID\', scope=\'pool\')\n          # 1 x 1 x 2048\n          net = ops.dropout(net, dropout_keep_prob, scope=\'dropout\')\n          net = ops.flatten(net, scope=\'flatten\')\n          # 2048\n          logits = ops.fc(net, num_classes, activation=None, scope=\'logits\',\n                          restore=restore_logits)\n          # 1000\n          end_points[\'logits\'] = logits\n          end_points[\'predictions\'] = tf.nn.softmax(logits, name=\'predictions\')\n      return logits, end_points\n\n\ndef inception_v3_parameters(weight_decay=0.00004, stddev=0.1,\n                            batch_norm_decay=0.9997, batch_norm_epsilon=0.001):\n  """"""Yields the scope with the default parameters for inception_v3.\n\n  Args:\n    weight_decay: the weight decay for weights variables.\n    stddev: standard deviation of the truncated guassian weight distribution.\n    batch_norm_decay: decay for the moving average of batch_norm momentums.\n    batch_norm_epsilon: small float added to variance to avoid dividing by zero.\n\n  Yields:\n    a arg_scope with the parameters needed for inception_v3.\n  """"""\n  # Set weight_decay for weights in Conv and FC layers.\n  with scopes.arg_scope([ops.conv2d, ops.fc],\n                        weight_decay=weight_decay):\n    # Set stddev, activation and parameters for batch_norm.\n    with scopes.arg_scope([ops.conv2d],\n                          stddev=stddev,\n                          activation=tf.nn.relu,\n                          batch_norm_params={\n                              \'decay\': batch_norm_decay,\n                              \'epsilon\': batch_norm_epsilon}) as arg_scope:\n      yield arg_scope\n'"
model_zoo/models/inception/inception/slim/inception_test.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.inception.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom inception.slim import inception_model as inception\n\n\nclass InceptionTest(tf.test.TestCase):\n\n  def testBuildLogits(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_v3(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n\n  def testBuildEndPoints(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      _, end_points = inception.inception_v3(inputs, num_classes)\n      self.assertTrue(\'logits\' in end_points)\n      logits = end_points[\'logits\']\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      self.assertTrue(\'aux_logits\' in end_points)\n      aux_logits = end_points[\'aux_logits\']\n      self.assertListEqual(aux_logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'mixed_8x8x2048b\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 8, 8, 2048])\n\n  def testVariablesSetDevice(self):\n    batch_size = 5\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      # Force all Variables to reside on the device.\n      with tf.variable_scope(\'on_cpu\'), tf.device(\'/cpu:0\'):\n        inception.inception_v3(inputs, num_classes)\n      with tf.variable_scope(\'on_gpu\'), tf.device(\'/gpu:0\'):\n        inception.inception_v3(inputs, num_classes)\n      for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope=\'on_cpu\'):\n        self.assertDeviceEqual(v.device, \'/cpu:0\')\n      for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope=\'on_gpu\'):\n        self.assertDeviceEqual(v.device, \'/gpu:0\')\n\n  def testHalfSizeImages(self):\n    batch_size = 5\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session():\n      inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, end_points = inception.inception_v3(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [batch_size, num_classes])\n      pre_pool = end_points[\'mixed_8x8x2048b\']\n      self.assertListEqual(pre_pool.get_shape().as_list(),\n                           [batch_size, 3, 3, 2048])\n\n  def testUnknowBatchSize(self):\n    batch_size = 1\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      inputs = tf.placeholder(tf.float32, (None, height, width, 3))\n      logits, _ = inception.inception_v3(inputs, num_classes)\n      self.assertTrue(logits.op.name.startswith(\'logits\'))\n      self.assertListEqual(logits.get_shape().as_list(),\n                           [None, num_classes])\n      images = tf.random_uniform((batch_size, height, width, 3))\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(logits, {inputs: images.eval()})\n      self.assertEquals(output.shape, (batch_size, num_classes))\n\n  def testEvaluation(self):\n    batch_size = 2\n    height, width = 299, 299\n    num_classes = 1000\n    with self.test_session() as sess:\n      eval_inputs = tf.random_uniform((batch_size, height, width, 3))\n      logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                         is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (batch_size,))\n\n  def testTrainEvalWithReuse(self):\n    train_batch_size = 5\n    eval_batch_size = 2\n    height, width = 150, 150\n    num_classes = 1000\n    with self.test_session() as sess:\n      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))\n      inception.inception_v3(train_inputs, num_classes)\n      tf.get_variable_scope().reuse_variables()\n      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))\n      logits, _ = inception.inception_v3(eval_inputs, num_classes,\n                                         is_training=False)\n      predictions = tf.argmax(logits, 1)\n      sess.run(tf.initialize_all_variables())\n      output = sess.run(predictions)\n      self.assertEquals(output.shape, (eval_batch_size,))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/inception/inception/slim/losses.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains convenience wrappers for various Neural Network TensorFlow losses.\n\n  All the losses defined here add themselves to the LOSSES_COLLECTION\n  collection.\n\n  l1_loss: Define a L1 Loss, useful for regularization, i.e. lasso.\n  l2_loss: Define a L2 Loss, useful for regularization, i.e. weight decay.\n  cross_entropy_loss: Define a cross entropy loss using\n    softmax_cross_entropy_with_logits. Useful for classification.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# In order to gather all losses in a network, the user should use this\n# key for get_collection, i.e:\n#   losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)\nLOSSES_COLLECTION = \'_losses\'\n\n\ndef l1_regularizer(weight=1.0, scope=None):\n  """"""Define a L1 regularizer.\n\n  Args:\n    weight: scale the loss by this factor.\n    scope: Optional scope for op_scope.\n\n  Returns:\n    a regularizer function.\n  """"""\n  def regularizer(tensor):\n    with tf.op_scope([tensor], scope, \'L1Regularizer\'):\n      l1_weight = tf.convert_to_tensor(weight,\n                                       dtype=tensor.dtype.base_dtype,\n                                       name=\'weight\')\n      return tf.mul(l1_weight, tf.reduce_sum(tf.abs(tensor)), name=\'value\')\n  return regularizer\n\n\ndef l2_regularizer(weight=1.0, scope=None):\n  """"""Define a L2 regularizer.\n\n  Args:\n    weight: scale the loss by this factor.\n    scope: Optional scope for op_scope.\n\n  Returns:\n    a regularizer function.\n  """"""\n  def regularizer(tensor):\n    with tf.op_scope([tensor], scope, \'L2Regularizer\'):\n      l2_weight = tf.convert_to_tensor(weight,\n                                       dtype=tensor.dtype.base_dtype,\n                                       name=\'weight\')\n      return tf.mul(l2_weight, tf.nn.l2_loss(tensor), name=\'value\')\n  return regularizer\n\n\ndef l1_l2_regularizer(weight_l1=1.0, weight_l2=1.0, scope=None):\n  """"""Define a L1L2 regularizer.\n\n  Args:\n    weight_l1: scale the L1 loss by this factor.\n    weight_l2: scale the L2 loss by this factor.\n    scope: Optional scope for op_scope.\n\n  Returns:\n    a regularizer function.\n  """"""\n  def regularizer(tensor):\n    with tf.op_scope([tensor], scope, \'L1L2Regularizer\'):\n      weight_l1_t = tf.convert_to_tensor(weight_l1,\n                                         dtype=tensor.dtype.base_dtype,\n                                         name=\'weight_l1\')\n      weight_l2_t = tf.convert_to_tensor(weight_l2,\n                                         dtype=tensor.dtype.base_dtype,\n                                         name=\'weight_l2\')\n      reg_l1 = tf.mul(weight_l1_t, tf.reduce_sum(tf.abs(tensor)),\n                      name=\'value_l1\')\n      reg_l2 = tf.mul(weight_l2_t, tf.nn.l2_loss(tensor),\n                      name=\'value_l2\')\n      return tf.add(reg_l1, reg_l2, name=\'value\')\n  return regularizer\n\n\ndef l1_loss(tensor, weight=1.0, scope=None):\n  """"""Define a L1Loss, useful for regularize, i.e. lasso.\n\n  Args:\n    tensor: tensor to regularize.\n    weight: scale the loss by this factor.\n    scope: Optional scope for op_scope.\n\n  Returns:\n    the L1 loss op.\n  """"""\n  with tf.op_scope([tensor], scope, \'L1Loss\'):\n    weight = tf.convert_to_tensor(weight,\n                                  dtype=tensor.dtype.base_dtype,\n                                  name=\'loss_weight\')\n    loss = tf.mul(weight, tf.reduce_sum(tf.abs(tensor)), name=\'value\')\n    tf.add_to_collection(LOSSES_COLLECTION, loss)\n    return loss\n\n\ndef l2_loss(tensor, weight=1.0, scope=None):\n  """"""Define a L2Loss, useful for regularize, i.e. weight decay.\n\n  Args:\n    tensor: tensor to regularize.\n    weight: an optional weight to modulate the loss.\n    scope: Optional scope for op_scope.\n\n  Returns:\n    the L2 loss op.\n  """"""\n  with tf.op_scope([tensor], scope, \'L2Loss\'):\n    weight = tf.convert_to_tensor(weight,\n                                  dtype=tensor.dtype.base_dtype,\n                                  name=\'loss_weight\')\n    loss = tf.mul(weight, tf.nn.l2_loss(tensor), name=\'value\')\n    tf.add_to_collection(LOSSES_COLLECTION, loss)\n    return loss\n\n\ndef cross_entropy_loss(logits, one_hot_labels, label_smoothing=0,\n                       weight=1.0, scope=None):\n  """"""Define a Cross Entropy loss using softmax_cross_entropy_with_logits.\n\n  It can scale the loss by weight factor, and smooth the labels.\n\n  Args:\n    logits: [batch_size, num_classes] logits outputs of the network .\n    one_hot_labels: [batch_size, num_classes] target one_hot_encoded labels.\n    label_smoothing: if greater than 0 then smooth the labels.\n    weight: scale the loss by this factor.\n    scope: Optional scope for op_scope.\n\n  Returns:\n    A tensor with the softmax_cross_entropy loss.\n  """"""\n  logits.get_shape().assert_is_compatible_with(one_hot_labels.get_shape())\n  with tf.op_scope([logits, one_hot_labels], scope, \'CrossEntropyLoss\'):\n    num_classes = one_hot_labels.get_shape()[-1].value\n    one_hot_labels = tf.cast(one_hot_labels, logits.dtype)\n    if label_smoothing > 0:\n      smooth_positives = 1.0 - label_smoothing\n      smooth_negatives = label_smoothing / num_classes\n      one_hot_labels = one_hot_labels * smooth_positives + smooth_negatives\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,\n                                                            one_hot_labels,\n                                                            name=\'xentropy\')\n    weight = tf.convert_to_tensor(weight,\n                                  dtype=logits.dtype.base_dtype,\n                                  name=\'loss_weight\')\n    loss = tf.mul(weight, tf.reduce_mean(cross_entropy), name=\'value\')\n    tf.add_to_collection(LOSSES_COLLECTION, loss)\n    return loss\n'"
model_zoo/models/inception/inception/slim/losses_test.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.losses.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom inception.slim import losses\n\n\nclass LossesTest(tf.test.TestCase):\n\n  def testL1Loss(self):\n    with self.test_session():\n      shape = [5, 5, 5]\n      num_elem = 5 * 5 * 5\n      weights = tf.constant(1.0, shape=shape)\n      wd = 0.01\n      loss = losses.l1_loss(weights, wd)\n      self.assertEquals(loss.op.name, \'L1Loss/value\')\n      self.assertAlmostEqual(loss.eval(), num_elem * wd, 5)\n\n  def testL2Loss(self):\n    with self.test_session():\n      shape = [5, 5, 5]\n      num_elem = 5 * 5 * 5\n      weights = tf.constant(1.0, shape=shape)\n      wd = 0.01\n      loss = losses.l2_loss(weights, wd)\n      self.assertEquals(loss.op.name, \'L2Loss/value\')\n      self.assertAlmostEqual(loss.eval(), num_elem * wd / 2, 5)\n\n\nclass RegularizersTest(tf.test.TestCase):\n\n  def testL1Regularizer(self):\n    with self.test_session():\n      shape = [5, 5, 5]\n      num_elem = 5 * 5 * 5\n      tensor = tf.constant(1.0, shape=shape)\n      loss = losses.l1_regularizer()(tensor)\n      self.assertEquals(loss.op.name, \'L1Regularizer/value\')\n      self.assertAlmostEqual(loss.eval(), num_elem, 5)\n\n  def testL1RegularizerWithScope(self):\n    with self.test_session():\n      shape = [5, 5, 5]\n      num_elem = 5 * 5 * 5\n      tensor = tf.constant(1.0, shape=shape)\n      loss = losses.l1_regularizer(scope=\'L1\')(tensor)\n      self.assertEquals(loss.op.name, \'L1/value\')\n      self.assertAlmostEqual(loss.eval(), num_elem, 5)\n\n  def testL1RegularizerWithWeight(self):\n    with self.test_session():\n      shape = [5, 5, 5]\n      num_elem = 5 * 5 * 5\n      tensor = tf.constant(1.0, shape=shape)\n      weight = 0.01\n      loss = losses.l1_regularizer(weight)(tensor)\n      self.assertEquals(loss.op.name, \'L1Regularizer/value\')\n      self.assertAlmostEqual(loss.eval(), num_elem * weight, 5)\n\n  def testL2Regularizer(self):\n    with self.test_session():\n      shape = [5, 5, 5]\n      num_elem = 5 * 5 * 5\n      tensor = tf.constant(1.0, shape=shape)\n      loss = losses.l2_regularizer()(tensor)\n      self.assertEquals(loss.op.name, \'L2Regularizer/value\')\n      self.assertAlmostEqual(loss.eval(), num_elem / 2, 5)\n\n  def testL2RegularizerWithScope(self):\n    with self.test_session():\n      shape = [5, 5, 5]\n      num_elem = 5 * 5 * 5\n      tensor = tf.constant(1.0, shape=shape)\n      loss = losses.l2_regularizer(scope=\'L2\')(tensor)\n      self.assertEquals(loss.op.name, \'L2/value\')\n      self.assertAlmostEqual(loss.eval(), num_elem / 2, 5)\n\n  def testL2RegularizerWithWeight(self):\n    with self.test_session():\n      shape = [5, 5, 5]\n      num_elem = 5 * 5 * 5\n      tensor = tf.constant(1.0, shape=shape)\n      weight = 0.01\n      loss = losses.l2_regularizer(weight)(tensor)\n      self.assertEquals(loss.op.name, \'L2Regularizer/value\')\n      self.assertAlmostEqual(loss.eval(), num_elem * weight / 2, 5)\n\n  def testL1L2Regularizer(self):\n    with self.test_session():\n      shape = [5, 5, 5]\n      num_elem = 5 * 5 * 5\n      tensor = tf.constant(1.0, shape=shape)\n      loss = losses.l1_l2_regularizer()(tensor)\n      self.assertEquals(loss.op.name, \'L1L2Regularizer/value\')\n      self.assertAlmostEqual(loss.eval(), num_elem + num_elem / 2, 5)\n\n  def testL1L2RegularizerWithScope(self):\n    with self.test_session():\n      shape = [5, 5, 5]\n      num_elem = 5 * 5 * 5\n      tensor = tf.constant(1.0, shape=shape)\n      loss = losses.l1_l2_regularizer(scope=\'L1L2\')(tensor)\n      self.assertEquals(loss.op.name, \'L1L2/value\')\n      self.assertAlmostEqual(loss.eval(), num_elem + num_elem / 2, 5)\n\n  def testL1L2RegularizerWithWeights(self):\n    with self.test_session():\n      shape = [5, 5, 5]\n      num_elem = 5 * 5 * 5\n      tensor = tf.constant(1.0, shape=shape)\n      weight_l1 = 0.01\n      weight_l2 = 0.05\n      loss = losses.l1_l2_regularizer(weight_l1, weight_l2)(tensor)\n      self.assertEquals(loss.op.name, \'L1L2Regularizer/value\')\n      self.assertAlmostEqual(loss.eval(),\n                             num_elem * weight_l1 + num_elem * weight_l2 / 2, 5)\n\n\nclass CrossEntropyLossTest(tf.test.TestCase):\n\n  def testCrossEntropyLossAllCorrect(self):\n    with self.test_session():\n      logits = tf.constant([[10.0, 0.0, 0.0],\n                            [0.0, 10.0, 0.0],\n                            [0.0, 0.0, 10.0]])\n      labels = tf.constant([[1, 0, 0],\n                            [0, 1, 0],\n                            [0, 0, 1]])\n      loss = losses.cross_entropy_loss(logits, labels)\n      self.assertEquals(loss.op.name, \'CrossEntropyLoss/value\')\n      self.assertAlmostEqual(loss.eval(), 0.0, 3)\n\n  def testCrossEntropyLossAllWrong(self):\n    with self.test_session():\n      logits = tf.constant([[10.0, 0.0, 0.0],\n                            [0.0, 10.0, 0.0],\n                            [0.0, 0.0, 10.0]])\n      labels = tf.constant([[0, 0, 1],\n                            [1, 0, 0],\n                            [0, 1, 0]])\n      loss = losses.cross_entropy_loss(logits, labels)\n      self.assertEquals(loss.op.name, \'CrossEntropyLoss/value\')\n      self.assertAlmostEqual(loss.eval(), 10.0, 3)\n\n  def testCrossEntropyLossAllWrongWithWeight(self):\n    with self.test_session():\n      logits = tf.constant([[10.0, 0.0, 0.0],\n                            [0.0, 10.0, 0.0],\n                            [0.0, 0.0, 10.0]])\n      labels = tf.constant([[0, 0, 1],\n                            [1, 0, 0],\n                            [0, 1, 0]])\n      loss = losses.cross_entropy_loss(logits, labels, weight=0.5)\n      self.assertEquals(loss.op.name, \'CrossEntropyLoss/value\')\n      self.assertAlmostEqual(loss.eval(), 5.0, 3)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/inception/inception/slim/ops.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains convenience wrappers for typical Neural Network TensorFlow layers.\n\n   Additionally it maintains a collection with update_ops that need to be\n   updated after the ops have been computed, for exmaple to update moving means\n   and moving variances of batch_norm.\n\n   Ops that have different behavior during training or eval have an is_training\n   parameter. Additionally Ops that contain variables.variable have a trainable\n   parameter, which control if the ops variables are trainable or not.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\n\nfrom tensorflow.python.training import moving_averages\n\nfrom inception.slim import losses\nfrom inception.slim import scopes\nfrom inception.slim import variables\n\n# Used to keep the update ops done by batch_norm.\nUPDATE_OPS_COLLECTION = \'_update_ops_\'\n\n\n@scopes.add_arg_scope\ndef batch_norm(inputs,\n               decay=0.999,\n               center=True,\n               scale=False,\n               epsilon=0.001,\n               moving_vars=\'moving_vars\',\n               activation=None,\n               is_training=True,\n               trainable=True,\n               restore=True,\n               scope=None,\n               reuse=None):\n  """"""Adds a Batch Normalization layer.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels]\n            or [batch_size, channels].\n    decay: decay for the moving average.\n    center: If True, subtract beta. If False, beta is not created and ignored.\n    scale: If True, multiply by gamma. If False, gamma is\n      not used. When the next layer is linear (also e.g. ReLU), this can be\n      disabled since the scaling can be done by the next layer.\n    epsilon: small float added to variance to avoid dividing by zero.\n    moving_vars: collection to store the moving_mean and moving_variance.\n    activation: activation function.\n    is_training: whether or not the model is in training mode.\n    trainable: whether or not the variables should be trainable or not.\n    restore: whether or not the variables should be marked for restore.\n    scope: Optional scope for variable_op_scope.\n    reuse: whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n\n  Returns:\n    a tensor representing the output of the operation.\n\n  """"""\n  inputs_shape = inputs.get_shape()\n  with tf.variable_op_scope([inputs], scope, \'BatchNorm\', reuse=reuse):\n    axis = list(range(len(inputs_shape) - 1))\n    params_shape = inputs_shape[-1:]\n    # Allocate parameters for the beta and gamma of the normalization.\n    beta, gamma = None, None\n    if center:\n      beta = variables.variable(\'beta\',\n                                params_shape,\n                                initializer=tf.zeros_initializer,\n                                trainable=trainable,\n                                restore=restore)\n    if scale:\n      gamma = variables.variable(\'gamma\',\n                                 params_shape,\n                                 initializer=tf.ones_initializer,\n                                 trainable=trainable,\n                                 restore=restore)\n    # Create moving_mean and moving_variance add them to\n    # GraphKeys.MOVING_AVERAGE_VARIABLES collections.\n    moving_collections = [moving_vars, tf.GraphKeys.MOVING_AVERAGE_VARIABLES]\n    moving_mean = variables.variable(\'moving_mean\',\n                                     params_shape,\n                                     initializer=tf.zeros_initializer,\n                                     trainable=False,\n                                     restore=restore,\n                                     collections=moving_collections)\n    moving_variance = variables.variable(\'moving_variance\',\n                                         params_shape,\n                                         initializer=tf.ones_initializer,\n                                         trainable=False,\n                                         restore=restore,\n                                         collections=moving_collections)\n    if is_training:\n      # Calculate the moments based on the individual batch.\n      mean, variance = tf.nn.moments(inputs, axis)\n\n      update_moving_mean = moving_averages.assign_moving_average(\n          moving_mean, mean, decay)\n      tf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_mean)\n      update_moving_variance = moving_averages.assign_moving_average(\n          moving_variance, variance, decay)\n      tf.add_to_collection(UPDATE_OPS_COLLECTION, update_moving_variance)\n    else:\n      # Just use the moving_mean and moving_variance.\n      mean = moving_mean\n      variance = moving_variance\n    # Normalize the activations.\n    outputs = tf.nn.batch_normalization(\n        inputs, mean, variance, beta, gamma, epsilon)\n    outputs.set_shape(inputs.get_shape())\n    if activation:\n      outputs = activation(outputs)\n    return outputs\n\n\ndef _two_element_tuple(int_or_tuple):\n  """"""Converts `int_or_tuple` to height, width.\n\n  Several of the functions that follow accept arguments as either\n  a tuple of 2 integers or a single integer.  A single integer\n  indicates that the 2 values of the tuple are the same.\n\n  This functions normalizes the input value by always returning a tuple.\n\n  Args:\n    int_or_tuple: A list of 2 ints, a single int or a tf.TensorShape.\n\n  Returns:\n    A tuple with 2 values.\n\n  Raises:\n    ValueError: If `int_or_tuple` it not well formed.\n  """"""\n  if isinstance(int_or_tuple, (list, tuple)):\n    if len(int_or_tuple) != 2:\n      raise ValueError(\'Must be a list with 2 elements: %s\' % int_or_tuple)\n    return int(int_or_tuple[0]), int(int_or_tuple[1])\n  if isinstance(int_or_tuple, int):\n    return int(int_or_tuple), int(int_or_tuple)\n  if isinstance(int_or_tuple, tf.TensorShape):\n    if len(int_or_tuple) == 2:\n      return int_or_tuple[0], int_or_tuple[1]\n  raise ValueError(\'Must be an int, a list with 2 elements or a TensorShape of \'\n                   \'length 2\')\n\n\n@scopes.add_arg_scope\ndef conv2d(inputs,\n           num_filters_out,\n           kernel_size,\n           stride=1,\n           padding=\'SAME\',\n           activation=tf.nn.relu,\n           stddev=0.01,\n           bias=0.0,\n           weight_decay=0,\n           batch_norm_params=None,\n           is_training=True,\n           trainable=True,\n           restore=True,\n           scope=None,\n           reuse=None):\n  """"""Adds a 2D convolution followed by an optional batch_norm layer.\n\n  conv2d creates a variable called \'weights\', representing the convolutional\n  kernel, that is convolved with the input. If `batch_norm_params` is None, a\n  second variable called \'biases\' is added to the result of the convolution\n  operation.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_filters_out: the number of output filters.\n    kernel_size: a list of length 2: [kernel_height, kernel_width] of\n      of the filters. Can be an int if both values are the same.\n    stride: a list of length 2: [stride_height, stride_width].\n      Can be an int if both strides are the same.  Note that presently\n      both strides must have the same value.\n    padding: one of \'VALID\' or \'SAME\'.\n    activation: activation function.\n    stddev: standard deviation of the truncated guassian weight distribution.\n    bias: the initial value of the biases.\n    weight_decay: the weight decay.\n    batch_norm_params: parameters for the batch_norm. If is None don\'t use it.\n    is_training: whether or not the model is in training mode.\n    trainable: whether or not the variables should be trainable or not.\n    restore: whether or not the variables should be marked for restore.\n    scope: Optional scope for variable_op_scope.\n    reuse: whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n  Returns:\n    a tensor representing the output of the operation.\n\n  """"""\n  with tf.variable_op_scope([inputs], scope, \'Conv\', reuse=reuse):\n    kernel_h, kernel_w = _two_element_tuple(kernel_size)\n    stride_h, stride_w = _two_element_tuple(stride)\n    num_filters_in = inputs.get_shape()[-1]\n    weights_shape = [kernel_h, kernel_w,\n                     num_filters_in, num_filters_out]\n    weights_initializer = tf.truncated_normal_initializer(stddev=stddev)\n    l2_regularizer = None\n    if weight_decay and weight_decay > 0:\n      l2_regularizer = losses.l2_regularizer(weight_decay)\n    weights = variables.variable(\'weights\',\n                                 shape=weights_shape,\n                                 initializer=weights_initializer,\n                                 regularizer=l2_regularizer,\n                                 trainable=trainable,\n                                 restore=restore)\n    conv = tf.nn.conv2d(inputs, weights, [1, stride_h, stride_w, 1],\n                        padding=padding)\n    if batch_norm_params is not None:\n      with scopes.arg_scope([batch_norm], is_training=is_training,\n                            trainable=trainable, restore=restore):\n        outputs = batch_norm(conv, **batch_norm_params)\n    else:\n      bias_shape = [num_filters_out,]\n      bias_initializer = tf.constant_initializer(bias)\n      biases = variables.variable(\'biases\',\n                                  shape=bias_shape,\n                                  initializer=bias_initializer,\n                                  trainable=trainable,\n                                  restore=restore)\n      outputs = tf.nn.bias_add(conv, biases)\n    if activation:\n      outputs = activation(outputs)\n    return outputs\n\n\n@scopes.add_arg_scope\ndef fc(inputs,\n       num_units_out,\n       activation=tf.nn.relu,\n       stddev=0.01,\n       bias=0.0,\n       weight_decay=0,\n       batch_norm_params=None,\n       is_training=True,\n       trainable=True,\n       restore=True,\n       scope=None,\n       reuse=None):\n  """"""Adds a fully connected layer followed by an optional batch_norm layer.\n\n  FC creates a variable called \'weights\', representing the fully connected\n  weight matrix, that is multiplied by the input. If `batch_norm` is None, a\n  second variable called \'biases\' is added to the result of the initial\n  vector-matrix multiplication.\n\n  Args:\n    inputs: a [B x N] tensor where B is the batch size and N is the number of\n            input units in the layer.\n    num_units_out: the number of output units in the layer.\n    activation: activation function.\n    stddev: the standard deviation for the weights.\n    bias: the initial value of the biases.\n    weight_decay: the weight decay.\n    batch_norm_params: parameters for the batch_norm. If is None don\'t use it.\n    is_training: whether or not the model is in training mode.\n    trainable: whether or not the variables should be trainable or not.\n    restore: whether or not the variables should be marked for restore.\n    scope: Optional scope for variable_op_scope.\n    reuse: whether or not the layer and its variables should be reused. To be\n      able to reuse the layer scope must be given.\n\n  Returns:\n     the tensor variable representing the result of the series of operations.\n  """"""\n  with tf.variable_op_scope([inputs], scope, \'FC\', reuse=reuse):\n    num_units_in = inputs.get_shape()[1]\n    weights_shape = [num_units_in, num_units_out]\n    weights_initializer = tf.truncated_normal_initializer(stddev=stddev)\n    l2_regularizer = None\n    if weight_decay and weight_decay > 0:\n      l2_regularizer = losses.l2_regularizer(weight_decay)\n    weights = variables.variable(\'weights\',\n                                 shape=weights_shape,\n                                 initializer=weights_initializer,\n                                 regularizer=l2_regularizer,\n                                 trainable=trainable,\n                                 restore=restore)\n    if batch_norm_params is not None:\n      outputs = tf.matmul(inputs, weights)\n      with scopes.arg_scope([batch_norm], is_training=is_training,\n                            trainable=trainable, restore=restore):\n        outputs = batch_norm(outputs, **batch_norm_params)\n    else:\n      bias_shape = [num_units_out,]\n      bias_initializer = tf.constant_initializer(bias)\n      biases = variables.variable(\'biases\',\n                                  shape=bias_shape,\n                                  initializer=bias_initializer,\n                                  trainable=trainable,\n                                  restore=restore)\n      outputs = tf.nn.xw_plus_b(inputs, weights, biases)\n    if activation:\n      outputs = activation(outputs)\n    return outputs\n\n\ndef one_hot_encoding(labels, num_classes, scope=None):\n  """"""Transform numeric labels into onehot_labels.\n\n  Args:\n    labels: [batch_size] target labels.\n    num_classes: total number of classes.\n    scope: Optional scope for op_scope.\n  Returns:\n    one hot encoding of the labels.\n  """"""\n  with tf.op_scope([labels], scope, \'OneHotEncoding\'):\n    batch_size = labels.get_shape()[0]\n    indices = tf.expand_dims(tf.range(0, batch_size), 1)\n    labels = tf.cast(tf.expand_dims(labels, 1), indices.dtype)\n    concated = tf.concat(1, [indices, labels])\n    onehot_labels = tf.sparse_to_dense(\n        concated, tf.pack([batch_size, num_classes]), 1.0, 0.0)\n    onehot_labels.set_shape([batch_size, num_classes])\n    return onehot_labels\n\n\n@scopes.add_arg_scope\ndef max_pool(inputs, kernel_size, stride=2, padding=\'VALID\', scope=None):\n  """"""Adds a Max Pooling layer.\n\n  It is assumed by the wrapper that the pooling is only done per image and not\n  in depth or batch.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, depth].\n    kernel_size: a list of length 2: [kernel_height, kernel_width] of the\n      pooling kernel over which the op is computed. Can be an int if both\n      values are the same.\n    stride: a list of length 2: [stride_height, stride_width].\n      Can be an int if both strides are the same.  Note that presently\n      both strides must have the same value.\n    padding: the padding method, either \'VALID\' or \'SAME\'.\n    scope: Optional scope for op_scope.\n\n  Returns:\n    a tensor representing the results of the pooling operation.\n  Raises:\n    ValueError: if \'kernel_size\' is not a 2-D list\n  """"""\n  with tf.op_scope([inputs], scope, \'MaxPool\'):\n    kernel_h, kernel_w = _two_element_tuple(kernel_size)\n    stride_h, stride_w = _two_element_tuple(stride)\n    return tf.nn.max_pool(inputs,\n                          ksize=[1, kernel_h, kernel_w, 1],\n                          strides=[1, stride_h, stride_w, 1],\n                          padding=padding)\n\n\n@scopes.add_arg_scope\ndef avg_pool(inputs, kernel_size, stride=2, padding=\'VALID\', scope=None):\n  """"""Adds a Avg Pooling layer.\n\n  It is assumed by the wrapper that the pooling is only done per image and not\n  in depth or batch.\n\n  Args:\n    inputs: a tensor of size [batch_size, height, width, depth].\n    kernel_size: a list of length 2: [kernel_height, kernel_width] of the\n      pooling kernel over which the op is computed. Can be an int if both\n      values are the same.\n    stride: a list of length 2: [stride_height, stride_width].\n      Can be an int if both strides are the same.  Note that presently\n      both strides must have the same value.\n    padding: the padding method, either \'VALID\' or \'SAME\'.\n    scope: Optional scope for op_scope.\n\n  Returns:\n    a tensor representing the results of the pooling operation.\n  """"""\n  with tf.op_scope([inputs], scope, \'AvgPool\'):\n    kernel_h, kernel_w = _two_element_tuple(kernel_size)\n    stride_h, stride_w = _two_element_tuple(stride)\n    return tf.nn.avg_pool(inputs,\n                          ksize=[1, kernel_h, kernel_w, 1],\n                          strides=[1, stride_h, stride_w, 1],\n                          padding=padding)\n\n\n@scopes.add_arg_scope\ndef dropout(inputs, keep_prob=0.5, is_training=True, scope=None):\n  """"""Returns a dropout layer applied to the input.\n\n  Args:\n    inputs: the tensor to pass to the Dropout layer.\n    keep_prob: the probability of keeping each input unit.\n    is_training: whether or not the model is in training mode. If so, dropout is\n    applied and values scaled. Otherwise, inputs is returned.\n    scope: Optional scope for op_scope.\n\n  Returns:\n    a tensor representing the output of the operation.\n  """"""\n  if is_training and keep_prob > 0:\n    with tf.op_scope([inputs], scope, \'Dropout\'):\n      return tf.nn.dropout(inputs, keep_prob)\n  else:\n    return inputs\n\n\ndef flatten(inputs, scope=None):\n  """"""Flattens the input while maintaining the batch_size.\n\n    Assumes that the first dimension represents the batch.\n\n  Args:\n    inputs: a tensor of size [batch_size, ...].\n    scope: Optional scope for op_scope.\n\n  Returns:\n    a flattened tensor with shape [batch_size, k].\n  Raises:\n    ValueError: if inputs.shape is wrong.\n  """"""\n  if len(inputs.get_shape()) < 2:\n    raise ValueError(\'Inputs must be have a least 2 dimensions\')\n  dims = inputs.get_shape()[1:]\n  k = dims.num_elements()\n  with tf.op_scope([inputs], scope, \'Flatten\'):\n    return tf.reshape(inputs, [-1, k])\n\n\ndef repeat_op(repetitions, inputs, op, *args, **kwargs):\n  """"""Build a sequential Tower starting from inputs by using an op repeatedly.\n\n  It creates new scopes for each operation by increasing the counter.\n  Example: given repeat_op(3, _, ops.conv2d, 64, [3, 3], scope=\'conv1\')\n    it will repeat the given op under the following variable_scopes:\n      conv1/Conv\n      conv1/Conv_1\n      conv1/Conv_2\n\n  Args:\n    repetitions: number or repetitions.\n    inputs: a tensor of size [batch_size, height, width, channels].\n    op: an operation.\n    *args: args for the op.\n    **kwargs: kwargs for the op.\n\n  Returns:\n    a tensor result of applying the operation op, num times.\n  Raises:\n    ValueError: if the op is unknown or wrong.\n  """"""\n  scope = kwargs.pop(\'scope\', None)\n  with tf.variable_op_scope([inputs], scope, \'RepeatOp\'):\n    tower = inputs\n    for _ in range(repetitions):\n      tower = op(tower, *args, **kwargs)\n    return tower\n'"
model_zoo/models/inception/inception/slim/ops_test.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.ops.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\n\nfrom inception.slim import ops\nfrom inception.slim import scopes\nfrom inception.slim import variables\n\n\nclass ConvTest(tf.test.TestCase):\n\n  def testCreateConv(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.conv2d(images, 32, [3, 3])\n      self.assertEquals(output.op.name, \'Conv/Relu\')\n      self.assertListEqual(output.get_shape().as_list(), [5, height, width, 32])\n\n  def testCreateSquareConv(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.conv2d(images, 32, 3)\n      self.assertEquals(output.op.name, \'Conv/Relu\')\n      self.assertListEqual(output.get_shape().as_list(), [5, height, width, 32])\n\n  def testCreateConvWithTensorShape(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.conv2d(images, 32, images.get_shape()[1:3])\n      self.assertEquals(output.op.name, \'Conv/Relu\')\n      self.assertListEqual(output.get_shape().as_list(), [5, height, width, 32])\n\n  def testCreateFullyConv(self):\n    height, width = 6, 6\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 32), seed=1)\n      output = ops.conv2d(images, 64, images.get_shape()[1:3], padding=\'VALID\')\n      self.assertEquals(output.op.name, \'Conv/Relu\')\n      self.assertListEqual(output.get_shape().as_list(), [5, 1, 1, 64])\n\n  def testCreateVerticalConv(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.conv2d(images, 32, [3, 1])\n      self.assertEquals(output.op.name, \'Conv/Relu\')\n      self.assertListEqual(output.get_shape().as_list(),\n                           [5, height, width, 32])\n\n  def testCreateHorizontalConv(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.conv2d(images, 32, [1, 3])\n      self.assertEquals(output.op.name, \'Conv/Relu\')\n      self.assertListEqual(output.get_shape().as_list(),\n                           [5, height, width, 32])\n\n  def testCreateConvWithStride(self):\n    height, width = 6, 6\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.conv2d(images, 32, [3, 3], stride=2)\n      self.assertEquals(output.op.name, \'Conv/Relu\')\n      self.assertListEqual(output.get_shape().as_list(),\n                           [5, height/2, width/2, 32])\n\n  def testCreateConvCreatesWeightsAndBiasesVars(self):\n    height, width = 3, 3\n    images = tf.random_uniform((5, height, width, 3), seed=1)\n    with self.test_session():\n      self.assertFalse(variables.get_variables(\'conv1/weights\'))\n      self.assertFalse(variables.get_variables(\'conv1/biases\'))\n      ops.conv2d(images, 32, [3, 3], scope=\'conv1\')\n      self.assertTrue(variables.get_variables(\'conv1/weights\'))\n      self.assertTrue(variables.get_variables(\'conv1/biases\'))\n\n  def testCreateConvWithScope(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.conv2d(images, 32, [3, 3], scope=\'conv1\')\n      self.assertEquals(output.op.name, \'conv1/Relu\')\n\n  def testCreateConvWithoutActivation(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.conv2d(images, 32, [3, 3], activation=None)\n      self.assertEquals(output.op.name, \'Conv/BiasAdd\')\n\n  def testCreateConvValid(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.conv2d(images, 32, [3, 3], padding=\'VALID\')\n      self.assertListEqual(output.get_shape().as_list(), [5, 1, 1, 32])\n\n  def testCreateConvWithWD(self):\n    height, width = 3, 3\n    with self.test_session() as sess:\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.conv2d(images, 32, [3, 3], weight_decay=0.01)\n      wd = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)[0]\n      self.assertEquals(wd.op.name,\n                        \'Conv/weights/Regularizer/L2Regularizer/value\')\n      sess.run(tf.initialize_all_variables())\n      self.assertTrue(sess.run(wd) <= 0.01)\n\n  def testCreateConvWithoutWD(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.conv2d(images, 32, [3, 3], weight_decay=0)\n      self.assertEquals(\n          tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES), [])\n\n  def testReuseVars(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.conv2d(images, 32, [3, 3], scope=\'conv1\')\n      self.assertEquals(len(variables.get_variables()), 2)\n      ops.conv2d(images, 32, [3, 3], scope=\'conv1\', reuse=True)\n      self.assertEquals(len(variables.get_variables()), 2)\n\n  def testNonReuseVars(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.conv2d(images, 32, [3, 3])\n      self.assertEquals(len(variables.get_variables()), 2)\n      ops.conv2d(images, 32, [3, 3])\n      self.assertEquals(len(variables.get_variables()), 4)\n\n  def testReuseConvWithWD(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.conv2d(images, 32, [3, 3], weight_decay=0.01, scope=\'conv1\')\n      self.assertEquals(len(variables.get_variables()), 2)\n      self.assertEquals(\n          len(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)), 1)\n      ops.conv2d(images, 32, [3, 3], weight_decay=0.01, scope=\'conv1\',\n                 reuse=True)\n      self.assertEquals(len(variables.get_variables()), 2)\n      self.assertEquals(\n          len(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)), 1)\n\n  def testConvWithBatchNorm(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 32), seed=1)\n      with scopes.arg_scope([ops.conv2d], batch_norm_params={\'decay\': 0.9}):\n        net = ops.conv2d(images, 32, [3, 3])\n        net = ops.conv2d(net, 32, [3, 3])\n      self.assertEquals(len(variables.get_variables()), 8)\n      self.assertEquals(len(variables.get_variables(\'Conv/BatchNorm\')), 3)\n      self.assertEquals(len(variables.get_variables(\'Conv_1/BatchNorm\')), 3)\n\n  def testReuseConvWithBatchNorm(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 32), seed=1)\n      with scopes.arg_scope([ops.conv2d], batch_norm_params={\'decay\': 0.9}):\n        net = ops.conv2d(images, 32, [3, 3], scope=\'Conv\')\n        net = ops.conv2d(net, 32, [3, 3], scope=\'Conv\', reuse=True)\n      self.assertEquals(len(variables.get_variables()), 4)\n      self.assertEquals(len(variables.get_variables(\'Conv/BatchNorm\')), 3)\n      self.assertEquals(len(variables.get_variables(\'Conv_1/BatchNorm\')), 0)\n\n\nclass FCTest(tf.test.TestCase):\n\n  def testCreateFC(self):\n    height, width = 3, 3\n    with self.test_session():\n      inputs = tf.random_uniform((5, height * width * 3), seed=1)\n      output = ops.fc(inputs, 32)\n      self.assertEquals(output.op.name, \'FC/Relu\')\n      self.assertListEqual(output.get_shape().as_list(), [5, 32])\n\n  def testCreateFCWithScope(self):\n    height, width = 3, 3\n    with self.test_session():\n      inputs = tf.random_uniform((5, height * width * 3), seed=1)\n      output = ops.fc(inputs, 32, scope=\'fc1\')\n      self.assertEquals(output.op.name, \'fc1/Relu\')\n\n  def testCreateFcCreatesWeightsAndBiasesVars(self):\n    height, width = 3, 3\n    inputs = tf.random_uniform((5, height * width * 3), seed=1)\n    with self.test_session():\n      self.assertFalse(variables.get_variables(\'fc1/weights\'))\n      self.assertFalse(variables.get_variables(\'fc1/biases\'))\n      ops.fc(inputs, 32, scope=\'fc1\')\n      self.assertTrue(variables.get_variables(\'fc1/weights\'))\n      self.assertTrue(variables.get_variables(\'fc1/biases\'))\n\n  def testReuseVars(self):\n    height, width = 3, 3\n    inputs = tf.random_uniform((5, height * width * 3), seed=1)\n    with self.test_session():\n      ops.fc(inputs, 32, scope=\'fc1\')\n      self.assertEquals(len(variables.get_variables(\'fc1\')), 2)\n      ops.fc(inputs, 32, scope=\'fc1\', reuse=True)\n      self.assertEquals(len(variables.get_variables(\'fc1\')), 2)\n\n  def testNonReuseVars(self):\n    height, width = 3, 3\n    inputs = tf.random_uniform((5, height * width * 3), seed=1)\n    with self.test_session():\n      ops.fc(inputs, 32)\n      self.assertEquals(len(variables.get_variables(\'FC\')), 2)\n      ops.fc(inputs, 32)\n      self.assertEquals(len(variables.get_variables(\'FC\')), 4)\n\n  def testCreateFCWithoutActivation(self):\n    height, width = 3, 3\n    with self.test_session():\n      inputs = tf.random_uniform((5, height * width * 3), seed=1)\n      output = ops.fc(inputs, 32, activation=None)\n      self.assertEquals(output.op.name, \'FC/xw_plus_b\')\n\n  def testCreateFCWithWD(self):\n    height, width = 3, 3\n    with self.test_session() as sess:\n      inputs = tf.random_uniform((5, height * width * 3), seed=1)\n      ops.fc(inputs, 32, weight_decay=0.01)\n      wd = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)[0]\n      self.assertEquals(wd.op.name,\n                        \'FC/weights/Regularizer/L2Regularizer/value\')\n      sess.run(tf.initialize_all_variables())\n      self.assertTrue(sess.run(wd) <= 0.01)\n\n  def testCreateFCWithoutWD(self):\n    height, width = 3, 3\n    with self.test_session():\n      inputs = tf.random_uniform((5, height * width * 3), seed=1)\n      ops.fc(inputs, 32, weight_decay=0)\n      self.assertEquals(\n          tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES), [])\n\n  def testReuseFCWithWD(self):\n    height, width = 3, 3\n    with self.test_session():\n      inputs = tf.random_uniform((5, height * width * 3), seed=1)\n      ops.fc(inputs, 32, weight_decay=0.01, scope=\'fc\')\n      self.assertEquals(len(variables.get_variables()), 2)\n      self.assertEquals(\n          len(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)), 1)\n      ops.fc(inputs, 32, weight_decay=0.01, scope=\'fc\', reuse=True)\n      self.assertEquals(len(variables.get_variables()), 2)\n      self.assertEquals(\n          len(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)), 1)\n\n  def testFCWithBatchNorm(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height * width * 3), seed=1)\n      with scopes.arg_scope([ops.fc], batch_norm_params={}):\n        net = ops.fc(images, 27)\n        net = ops.fc(net, 27)\n      self.assertEquals(len(variables.get_variables()), 8)\n      self.assertEquals(len(variables.get_variables(\'FC/BatchNorm\')), 3)\n      self.assertEquals(len(variables.get_variables(\'FC_1/BatchNorm\')), 3)\n\n  def testReuseFCWithBatchNorm(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height * width * 3), seed=1)\n      with scopes.arg_scope([ops.fc], batch_norm_params={\'decay\': 0.9}):\n        net = ops.fc(images, 27, scope=\'fc1\')\n        net = ops.fc(net, 27, scope=\'fc1\', reuse=True)\n      self.assertEquals(len(variables.get_variables()), 4)\n      self.assertEquals(len(variables.get_variables(\'fc1/BatchNorm\')), 3)\n\n\nclass MaxPoolTest(tf.test.TestCase):\n\n  def testCreateMaxPool(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.max_pool(images, [3, 3])\n      self.assertEquals(output.op.name, \'MaxPool/MaxPool\')\n      self.assertListEqual(output.get_shape().as_list(), [5, 1, 1, 3])\n\n  def testCreateSquareMaxPool(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.max_pool(images, 3)\n      self.assertEquals(output.op.name, \'MaxPool/MaxPool\')\n      self.assertListEqual(output.get_shape().as_list(), [5, 1, 1, 3])\n\n  def testCreateMaxPoolWithScope(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.max_pool(images, [3, 3], scope=\'pool1\')\n      self.assertEquals(output.op.name, \'pool1/MaxPool\')\n\n  def testCreateMaxPoolSAME(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.max_pool(images, [3, 3], padding=\'SAME\')\n      self.assertListEqual(output.get_shape().as_list(), [5, 2, 2, 3])\n\n  def testCreateMaxPoolStrideSAME(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.max_pool(images, [3, 3], stride=1, padding=\'SAME\')\n      self.assertListEqual(output.get_shape().as_list(), [5, height, width, 3])\n\n  def testGlobalMaxPool(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.max_pool(images, images.get_shape()[1:3], stride=1)\n      self.assertListEqual(output.get_shape().as_list(), [5, 1, 1, 3])\n\n\nclass AvgPoolTest(tf.test.TestCase):\n\n  def testCreateAvgPool(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.avg_pool(images, [3, 3])\n      self.assertEquals(output.op.name, \'AvgPool/AvgPool\')\n      self.assertListEqual(output.get_shape().as_list(), [5, 1, 1, 3])\n\n  def testCreateSquareAvgPool(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.avg_pool(images, 3)\n      self.assertEquals(output.op.name, \'AvgPool/AvgPool\')\n      self.assertListEqual(output.get_shape().as_list(), [5, 1, 1, 3])\n\n  def testCreateAvgPoolWithScope(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.avg_pool(images, [3, 3], scope=\'pool1\')\n      self.assertEquals(output.op.name, \'pool1/AvgPool\')\n\n  def testCreateAvgPoolSAME(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.avg_pool(images, [3, 3], padding=\'SAME\')\n      self.assertListEqual(output.get_shape().as_list(), [5, 2, 2, 3])\n\n  def testCreateAvgPoolStrideSAME(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.avg_pool(images, [3, 3], stride=1, padding=\'SAME\')\n      self.assertListEqual(output.get_shape().as_list(), [5, height, width, 3])\n\n  def testGlobalAvgPool(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.avg_pool(images, images.get_shape()[1:3], stride=1)\n      self.assertListEqual(output.get_shape().as_list(), [5, 1, 1, 3])\n\n\nclass OneHotEncodingTest(tf.test.TestCase):\n\n  def testOneHotEncodingCreate(self):\n    with self.test_session():\n      labels = tf.constant([0, 1, 2])\n      output = ops.one_hot_encoding(labels, num_classes=3)\n      self.assertEquals(output.op.name, \'OneHotEncoding/SparseToDense\')\n      self.assertListEqual(output.get_shape().as_list(), [3, 3])\n\n  def testOneHotEncoding(self):\n    with self.test_session():\n      labels = tf.constant([0, 1, 2])\n      one_hot_labels = tf.constant([[1, 0, 0],\n                                    [0, 1, 0],\n                                    [0, 0, 1]])\n      output = ops.one_hot_encoding(labels, num_classes=3)\n      self.assertAllClose(output.eval(), one_hot_labels.eval())\n\n\nclass DropoutTest(tf.test.TestCase):\n\n  def testCreateDropout(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.dropout(images)\n      self.assertEquals(output.op.name, \'Dropout/dropout/mul_1\')\n      output.get_shape().assert_is_compatible_with(images.get_shape())\n\n  def testCreateDropoutNoTraining(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1, name=\'images\')\n      output = ops.dropout(images, is_training=False)\n      self.assertEquals(output, images)\n\n\nclass FlattenTest(tf.test.TestCase):\n\n  def testFlatten4D(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1, name=\'images\')\n      output = ops.flatten(images)\n      self.assertEquals(output.get_shape().num_elements(),\n                        images.get_shape().num_elements())\n      self.assertEqual(output.get_shape()[0], images.get_shape()[0])\n\n  def testFlatten3D(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width), seed=1, name=\'images\')\n      output = ops.flatten(images)\n      self.assertEquals(output.get_shape().num_elements(),\n                        images.get_shape().num_elements())\n      self.assertEqual(output.get_shape()[0], images.get_shape()[0])\n\n  def testFlattenBatchSize(self):\n    height, width = 3, 3\n    with self.test_session() as sess:\n      images = tf.random_uniform((5, height, width, 3), seed=1, name=\'images\')\n      inputs = tf.placeholder(tf.int32, (None, height, width, 3))\n      output = ops.flatten(inputs)\n      self.assertEquals(output.get_shape().as_list(),\n                        [None, height * width * 3])\n      output = sess.run(output, {inputs: images.eval()})\n      self.assertEquals(output.size,\n                        images.get_shape().num_elements())\n      self.assertEqual(output.shape[0], images.get_shape()[0])\n\n\nclass BatchNormTest(tf.test.TestCase):\n\n  def testCreateOp(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      output = ops.batch_norm(images)\n      self.assertTrue(output.op.name.startswith(\'BatchNorm/batchnorm\'))\n      self.assertListEqual(output.get_shape().as_list(), [5, height, width, 3])\n\n  def testCreateVariables(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.batch_norm(images)\n      beta = variables.get_variables_by_name(\'beta\')[0]\n      self.assertEquals(beta.op.name, \'BatchNorm/beta\')\n      gamma = variables.get_variables_by_name(\'gamma\')\n      self.assertEquals(gamma, [])\n      moving_mean = tf.moving_average_variables()[0]\n      moving_variance = tf.moving_average_variables()[1]\n      self.assertEquals(moving_mean.op.name, \'BatchNorm/moving_mean\')\n      self.assertEquals(moving_variance.op.name, \'BatchNorm/moving_variance\')\n\n  def testCreateVariablesWithScale(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.batch_norm(images, scale=True)\n      beta = variables.get_variables_by_name(\'beta\')[0]\n      gamma = variables.get_variables_by_name(\'gamma\')[0]\n      self.assertEquals(beta.op.name, \'BatchNorm/beta\')\n      self.assertEquals(gamma.op.name, \'BatchNorm/gamma\')\n      moving_mean = tf.moving_average_variables()[0]\n      moving_variance = tf.moving_average_variables()[1]\n      self.assertEquals(moving_mean.op.name, \'BatchNorm/moving_mean\')\n      self.assertEquals(moving_variance.op.name, \'BatchNorm/moving_variance\')\n\n  def testCreateVariablesWithoutCenterWithScale(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.batch_norm(images, center=False, scale=True)\n      beta = variables.get_variables_by_name(\'beta\')\n      self.assertEquals(beta, [])\n      gamma = variables.get_variables_by_name(\'gamma\')[0]\n      self.assertEquals(gamma.op.name, \'BatchNorm/gamma\')\n      moving_mean = tf.moving_average_variables()[0]\n      moving_variance = tf.moving_average_variables()[1]\n      self.assertEquals(moving_mean.op.name, \'BatchNorm/moving_mean\')\n      self.assertEquals(moving_variance.op.name, \'BatchNorm/moving_variance\')\n\n  def testCreateVariablesWithoutCenterWithoutScale(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.batch_norm(images, center=False, scale=False)\n      beta = variables.get_variables_by_name(\'beta\')\n      self.assertEquals(beta, [])\n      gamma = variables.get_variables_by_name(\'gamma\')\n      self.assertEquals(gamma, [])\n      moving_mean = tf.moving_average_variables()[0]\n      moving_variance = tf.moving_average_variables()[1]\n      self.assertEquals(moving_mean.op.name, \'BatchNorm/moving_mean\')\n      self.assertEquals(moving_variance.op.name, \'BatchNorm/moving_variance\')\n\n  def testMovingAverageVariables(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.batch_norm(images, scale=True)\n      moving_mean = tf.moving_average_variables()[0]\n      moving_variance = tf.moving_average_variables()[1]\n      self.assertEquals(moving_mean.op.name, \'BatchNorm/moving_mean\')\n      self.assertEquals(moving_variance.op.name, \'BatchNorm/moving_variance\')\n\n  def testUpdateOps(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.batch_norm(images)\n      update_ops = tf.get_collection(ops.UPDATE_OPS_COLLECTION)\n      update_moving_mean = update_ops[0]\n      update_moving_variance = update_ops[1]\n      self.assertEquals(update_moving_mean.op.name,\n                        \'BatchNorm/AssignMovingAvg\')\n      self.assertEquals(update_moving_variance.op.name,\n                        \'BatchNorm/AssignMovingAvg_1\')\n\n  def testReuseVariables(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.batch_norm(images, scale=True, scope=\'bn\')\n      ops.batch_norm(images, scale=True, scope=\'bn\', reuse=True)\n      beta = variables.get_variables_by_name(\'beta\')\n      gamma = variables.get_variables_by_name(\'gamma\')\n      self.assertEquals(len(beta), 1)\n      self.assertEquals(len(gamma), 1)\n      moving_vars = tf.get_collection(\'moving_vars\')\n      self.assertEquals(len(moving_vars), 2)\n\n  def testReuseUpdateOps(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      ops.batch_norm(images, scope=\'bn\')\n      self.assertEquals(len(tf.get_collection(ops.UPDATE_OPS_COLLECTION)), 2)\n      ops.batch_norm(images, scope=\'bn\', reuse=True)\n      self.assertEquals(len(tf.get_collection(ops.UPDATE_OPS_COLLECTION)), 4)\n\n  def testCreateMovingVars(self):\n    height, width = 3, 3\n    with self.test_session():\n      images = tf.random_uniform((5, height, width, 3), seed=1)\n      _ = ops.batch_norm(images, moving_vars=\'moving_vars\')\n      moving_mean = tf.get_collection(\'moving_vars\',\n                                      \'BatchNorm/moving_mean\')\n      self.assertEquals(len(moving_mean), 1)\n      self.assertEquals(moving_mean[0].op.name, \'BatchNorm/moving_mean\')\n      moving_variance = tf.get_collection(\'moving_vars\',\n                                          \'BatchNorm/moving_variance\')\n      self.assertEquals(len(moving_variance), 1)\n      self.assertEquals(moving_variance[0].op.name, \'BatchNorm/moving_variance\')\n\n  def testComputeMovingVars(self):\n    height, width = 3, 3\n    with self.test_session() as sess:\n      image_shape = (10, height, width, 3)\n      image_values = np.random.rand(*image_shape)\n      expected_mean = np.mean(image_values, axis=(0, 1, 2))\n      expected_var = np.var(image_values, axis=(0, 1, 2))\n      images = tf.constant(image_values, shape=image_shape, dtype=tf.float32)\n      output = ops.batch_norm(images, decay=0.1)\n      update_ops = tf.get_collection(ops.UPDATE_OPS_COLLECTION)\n      with tf.control_dependencies(update_ops):\n        barrier = tf.no_op(name=\'gradient_barrier\')\n        output = control_flow_ops.with_dependencies([barrier], output)\n      # Initialize all variables\n      sess.run(tf.initialize_all_variables())\n      moving_mean = variables.get_variables(\'BatchNorm/moving_mean\')[0]\n      moving_variance = variables.get_variables(\'BatchNorm/moving_variance\')[0]\n      mean, variance = sess.run([moving_mean, moving_variance])\n      # After initialization moving_mean == 0 and moving_variance == 1.\n      self.assertAllClose(mean, [0] * 3)\n      self.assertAllClose(variance, [1] * 3)\n      for _ in range(10):\n        sess.run([output])\n      mean = moving_mean.eval()\n      variance = moving_variance.eval()\n      # After 10 updates with decay 0.1 moving_mean == expected_mean and\n      # moving_variance == expected_var.\n      self.assertAllClose(mean, expected_mean)\n      self.assertAllClose(variance, expected_var)\n\n  def testEvalMovingVars(self):\n    height, width = 3, 3\n    with self.test_session() as sess:\n      image_shape = (10, height, width, 3)\n      image_values = np.random.rand(*image_shape)\n      expected_mean = np.mean(image_values, axis=(0, 1, 2))\n      expected_var = np.var(image_values, axis=(0, 1, 2))\n      images = tf.constant(image_values, shape=image_shape, dtype=tf.float32)\n      output = ops.batch_norm(images, decay=0.1, is_training=False)\n      update_ops = tf.get_collection(ops.UPDATE_OPS_COLLECTION)\n      with tf.control_dependencies(update_ops):\n        barrier = tf.no_op(name=\'gradient_barrier\')\n        output = control_flow_ops.with_dependencies([barrier], output)\n      # Initialize all variables\n      sess.run(tf.initialize_all_variables())\n      moving_mean = variables.get_variables(\'BatchNorm/moving_mean\')[0]\n      moving_variance = variables.get_variables(\'BatchNorm/moving_variance\')[0]\n      mean, variance = sess.run([moving_mean, moving_variance])\n      # After initialization moving_mean == 0 and moving_variance == 1.\n      self.assertAllClose(mean, [0] * 3)\n      self.assertAllClose(variance, [1] * 3)\n      # Simulate assigment from saver restore.\n      init_assigns = [tf.assign(moving_mean, expected_mean),\n                      tf.assign(moving_variance, expected_var)]\n      sess.run(init_assigns)\n      for _ in range(10):\n        sess.run([output], {images: np.random.rand(*image_shape)})\n      mean = moving_mean.eval()\n      variance = moving_variance.eval()\n      # Although we feed different images, the moving_mean and moving_variance\n      # shouldn\'t change.\n      self.assertAllClose(mean, expected_mean)\n      self.assertAllClose(variance, expected_var)\n\n  def testReuseVars(self):\n    height, width = 3, 3\n    with self.test_session() as sess:\n      image_shape = (10, height, width, 3)\n      image_values = np.random.rand(*image_shape)\n      expected_mean = np.mean(image_values, axis=(0, 1, 2))\n      expected_var = np.var(image_values, axis=(0, 1, 2))\n      images = tf.constant(image_values, shape=image_shape, dtype=tf.float32)\n      output = ops.batch_norm(images, decay=0.1, is_training=False)\n      update_ops = tf.get_collection(ops.UPDATE_OPS_COLLECTION)\n      with tf.control_dependencies(update_ops):\n        barrier = tf.no_op(name=\'gradient_barrier\')\n        output = control_flow_ops.with_dependencies([barrier], output)\n      # Initialize all variables\n      sess.run(tf.initialize_all_variables())\n      moving_mean = variables.get_variables(\'BatchNorm/moving_mean\')[0]\n      moving_variance = variables.get_variables(\'BatchNorm/moving_variance\')[0]\n      mean, variance = sess.run([moving_mean, moving_variance])\n      # After initialization moving_mean == 0 and moving_variance == 1.\n      self.assertAllClose(mean, [0] * 3)\n      self.assertAllClose(variance, [1] * 3)\n      # Simulate assigment from saver restore.\n      init_assigns = [tf.assign(moving_mean, expected_mean),\n                      tf.assign(moving_variance, expected_var)]\n      sess.run(init_assigns)\n      for _ in range(10):\n        sess.run([output], {images: np.random.rand(*image_shape)})\n      mean = moving_mean.eval()\n      variance = moving_variance.eval()\n      # Although we feed different images, the moving_mean and moving_variance\n      # shouldn\'t change.\n      self.assertAllClose(mean, expected_mean)\n      self.assertAllClose(variance, expected_var)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/inception/inception/slim/scopes.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the new arg_scope used for TF-Slim ops.\n\n  Allows one to define models much more compactly by eliminating boilerplate\n  code. This is accomplished through the use of argument scoping (arg_scope).\n\n  Example of how to use scopes.arg_scope:\n\n  with scopes.arg_scope(ops.conv2d, padding=\'SAME\',\n                      stddev=0.01, weight_decay=0.0005):\n    net = ops.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\', scope=\'conv1\')\n    net = ops.conv2d(net, 256, [5, 5], scope=\'conv2\')\n\n  The first call to conv2d will overwrite padding:\n    ops.conv2d(inputs, 64, [11, 11], 4, padding=\'VALID\',\n              stddev=0.01, weight_decay=0.0005, scope=\'conv1\')\n\n  The second call to Conv will use predefined args:\n    ops.conv2d(inputs, 256, [5, 5], padding=\'SAME\',\n               stddev=0.01, weight_decay=0.0005, scope=\'conv2\')\n\n  Example of how to reuse an arg_scope:\n  with scopes.arg_scope(ops.conv2d, padding=\'SAME\',\n                      stddev=0.01, weight_decay=0.0005) as conv2d_arg_scope:\n    net = ops.conv2d(net, 256, [5, 5], scope=\'conv1\')\n    ....\n\n  with scopes.arg_scope(conv2d_arg_scope):\n    net = ops.conv2d(net, 256, [5, 5], scope=\'conv2\')\n\n  Example of how to use scopes.add_arg_scope:\n\n  @scopes.add_arg_scope\n  def conv2d(*args, **kwargs)\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport functools\n\nfrom tensorflow.python.framework import ops\n\n_ARGSTACK_KEY = (""__arg_stack"",)\n\n_DECORATED_OPS = set()\n\n\ndef _get_arg_stack():\n  stack = ops.get_collection(_ARGSTACK_KEY)\n  if stack:\n    return stack[0]\n  else:\n    stack = [{}]\n    ops.add_to_collection(_ARGSTACK_KEY, stack)\n    return stack\n\n\ndef _current_arg_scope():\n  stack = _get_arg_stack()\n  return stack[-1]\n\n\ndef _add_op(op):\n  key_op = (op.__module__, op.__name__)\n  if key_op not in _DECORATED_OPS:\n    _DECORATED_OPS.add(key_op)\n\n\n@contextlib.contextmanager\ndef arg_scope(list_ops_or_scope, **kwargs):\n  """"""Stores the default arguments for the given set of list_ops.\n\n  For usage, please see examples at top of the file.\n\n  Args:\n    list_ops_or_scope: List or tuple of operations to set argument scope for or\n      a dictionary containg the current scope. When list_ops_or_scope is a dict,\n      kwargs must be empty. When list_ops_or_scope is a list or tuple, then\n      every op in it need to be decorated with @add_arg_scope to work.\n    **kwargs: keyword=value that will define the defaults for each op in\n              list_ops. All the ops need to accept the given set of arguments.\n\n  Yields:\n    the current_scope, which is a dictionary of {op: {arg: value}}\n  Raises:\n    TypeError: if list_ops is not a list or a tuple.\n    ValueError: if any op in list_ops has not be decorated with @add_arg_scope.\n  """"""\n  if isinstance(list_ops_or_scope, dict):\n    # Assumes that list_ops_or_scope is a scope that is being reused.\n    if kwargs:\n      raise ValueError(""When attempting to re-use a scope by suppling a""\n                       ""dictionary, kwargs must be empty."")\n    current_scope = list_ops_or_scope.copy()\n    try:\n      _get_arg_stack().append(current_scope)\n      yield current_scope\n    finally:\n      _get_arg_stack().pop()\n  else:\n    # Assumes that list_ops_or_scope is a list/tuple of ops with kwargs.\n    if not isinstance(list_ops_or_scope, (list, tuple)):\n      raise TypeError(""list_ops_or_scope must either be a list/tuple or reused""\n                      ""scope (i.e. dict)"")\n    try:\n      current_scope = _current_arg_scope().copy()\n      for op in list_ops_or_scope:\n        key_op = (op.__module__, op.__name__)\n        if not has_arg_scope(op):\n          raise ValueError(""%s is not decorated with @add_arg_scope"", key_op)\n        if key_op in current_scope:\n          current_kwargs = current_scope[key_op].copy()\n          current_kwargs.update(kwargs)\n          current_scope[key_op] = current_kwargs\n        else:\n          current_scope[key_op] = kwargs.copy()\n      _get_arg_stack().append(current_scope)\n      yield current_scope\n    finally:\n      _get_arg_stack().pop()\n\n\ndef add_arg_scope(func):\n  """"""Decorates a function with args so it can be used within an arg_scope.\n\n  Args:\n    func: function to decorate.\n\n  Returns:\n    A tuple with the decorated function func_with_args().\n  """"""\n  @functools.wraps(func)\n  def func_with_args(*args, **kwargs):\n    current_scope = _current_arg_scope()\n    current_args = kwargs\n    key_func = (func.__module__, func.__name__)\n    if key_func in current_scope:\n      current_args = current_scope[key_func].copy()\n      current_args.update(kwargs)\n    return func(*args, **current_args)\n  _add_op(func)\n  return func_with_args\n\n\ndef has_arg_scope(func):\n  """"""Checks whether a func has been decorated with @add_arg_scope or not.\n\n  Args:\n    func: function to check.\n\n  Returns:\n    a boolean.\n  """"""\n  key_op = (func.__module__, func.__name__)\n  return key_op in _DECORATED_OPS\n'"
model_zoo/models/inception/inception/slim/scopes_test.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests slim.scopes.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport tensorflow as tf\nfrom inception.slim import scopes\n\n\n@scopes.add_arg_scope\ndef func1(*args, **kwargs):\n  return (args, kwargs)\n\n\n@scopes.add_arg_scope\ndef func2(*args, **kwargs):\n  return (args, kwargs)\n\n\nclass ArgScopeTest(tf.test.TestCase):\n\n  def testEmptyArgScope(self):\n    with self.test_session():\n      self.assertEqual(scopes._current_arg_scope(), {})\n\n  def testCurrentArgScope(self):\n    func1_kwargs = {\'a\': 1, \'b\': None, \'c\': [1]}\n    key_op = (func1.__module__, func1.__name__)\n    current_scope = {key_op: func1_kwargs.copy()}\n    with self.test_session():\n      with scopes.arg_scope([func1], a=1, b=None, c=[1]) as scope:\n        self.assertDictEqual(scope, current_scope)\n\n  def testCurrentArgScopeNested(self):\n    func1_kwargs = {\'a\': 1, \'b\': None, \'c\': [1]}\n    func2_kwargs = {\'b\': 2, \'d\': [2]}\n    key = lambda f: (f.__module__, f.__name__)\n    current_scope = {key(func1): func1_kwargs.copy(),\n                     key(func2): func2_kwargs.copy()}\n    with self.test_session():\n      with scopes.arg_scope([func1], a=1, b=None, c=[1]):\n        with scopes.arg_scope([func2], b=2, d=[2]) as scope:\n          self.assertDictEqual(scope, current_scope)\n\n  def testReuseArgScope(self):\n    func1_kwargs = {\'a\': 1, \'b\': None, \'c\': [1]}\n    key_op = (func1.__module__, func1.__name__)\n    current_scope = {key_op: func1_kwargs.copy()}\n    with self.test_session():\n      with scopes.arg_scope([func1], a=1, b=None, c=[1]) as scope1:\n        pass\n      with scopes.arg_scope(scope1) as scope:\n        self.assertDictEqual(scope, current_scope)\n\n  def testReuseArgScopeNested(self):\n    func1_kwargs = {\'a\': 1, \'b\': None, \'c\': [1]}\n    func2_kwargs = {\'b\': 2, \'d\': [2]}\n    key = lambda f: (f.__module__, f.__name__)\n    current_scope1 = {key(func1): func1_kwargs.copy()}\n    current_scope2 = {key(func1): func1_kwargs.copy(),\n                      key(func2): func2_kwargs.copy()}\n    with self.test_session():\n      with scopes.arg_scope([func1], a=1, b=None, c=[1]) as scope1:\n        with scopes.arg_scope([func2], b=2, d=[2]) as scope2:\n          pass\n      with scopes.arg_scope(scope1):\n        self.assertDictEqual(scopes._current_arg_scope(), current_scope1)\n      with scopes.arg_scope(scope2):\n        self.assertDictEqual(scopes._current_arg_scope(), current_scope2)\n\n  def testSimpleArgScope(self):\n    func1_args = (0,)\n    func1_kwargs = {\'a\': 1, \'b\': None, \'c\': [1]}\n    with self.test_session():\n      with scopes.arg_scope([func1], a=1, b=None, c=[1]):\n        args, kwargs = func1(0)\n        self.assertTupleEqual(args, func1_args)\n        self.assertDictEqual(kwargs, func1_kwargs)\n\n  def testSimpleArgScopeWithTuple(self):\n    func1_args = (0,)\n    func1_kwargs = {\'a\': 1, \'b\': None, \'c\': [1]}\n    with self.test_session():\n      with scopes.arg_scope((func1,), a=1, b=None, c=[1]):\n        args, kwargs = func1(0)\n        self.assertTupleEqual(args, func1_args)\n        self.assertDictEqual(kwargs, func1_kwargs)\n\n  def testOverwriteArgScope(self):\n    func1_args = (0,)\n    func1_kwargs = {\'a\': 1, \'b\': 2, \'c\': [1]}\n    with scopes.arg_scope([func1], a=1, b=None, c=[1]):\n      args, kwargs = func1(0, b=2)\n      self.assertTupleEqual(args, func1_args)\n      self.assertDictEqual(kwargs, func1_kwargs)\n\n  def testNestedArgScope(self):\n    func1_args = (0,)\n    func1_kwargs = {\'a\': 1, \'b\': None, \'c\': [1]}\n    with scopes.arg_scope([func1], a=1, b=None, c=[1]):\n      args, kwargs = func1(0)\n      self.assertTupleEqual(args, func1_args)\n      self.assertDictEqual(kwargs, func1_kwargs)\n      func1_kwargs[\'b\'] = 2\n      with scopes.arg_scope([func1], b=2):\n        args, kwargs = func1(0)\n        self.assertTupleEqual(args, func1_args)\n        self.assertDictEqual(kwargs, func1_kwargs)\n\n  def testSharedArgScope(self):\n    func1_args = (0,)\n    func1_kwargs = {\'a\': 1, \'b\': None, \'c\': [1]}\n    with scopes.arg_scope([func1, func2], a=1, b=None, c=[1]):\n      args, kwargs = func1(0)\n      self.assertTupleEqual(args, func1_args)\n      self.assertDictEqual(kwargs, func1_kwargs)\n      args, kwargs = func2(0)\n      self.assertTupleEqual(args, func1_args)\n      self.assertDictEqual(kwargs, func1_kwargs)\n\n  def testSharedArgScopeTuple(self):\n    func1_args = (0,)\n    func1_kwargs = {\'a\': 1, \'b\': None, \'c\': [1]}\n    with scopes.arg_scope((func1, func2), a=1, b=None, c=[1]):\n      args, kwargs = func1(0)\n      self.assertTupleEqual(args, func1_args)\n      self.assertDictEqual(kwargs, func1_kwargs)\n      args, kwargs = func2(0)\n      self.assertTupleEqual(args, func1_args)\n      self.assertDictEqual(kwargs, func1_kwargs)\n\n  def testPartiallySharedArgScope(self):\n    func1_args = (0,)\n    func1_kwargs = {\'a\': 1, \'b\': None, \'c\': [1]}\n    func2_args = (1,)\n    func2_kwargs = {\'a\': 1, \'b\': None, \'d\': [2]}\n    with scopes.arg_scope([func1, func2], a=1, b=None):\n      with scopes.arg_scope([func1], c=[1]), scopes.arg_scope([func2], d=[2]):\n        args, kwargs = func1(0)\n        self.assertTupleEqual(args, func1_args)\n        self.assertDictEqual(kwargs, func1_kwargs)\n        args, kwargs = func2(1)\n        self.assertTupleEqual(args, func2_args)\n        self.assertDictEqual(kwargs, func2_kwargs)\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
model_zoo/models/inception/inception/slim/slim.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""TF-Slim grouped API. Please see README.md for details and usage.""""""\n# pylint: disable=unused-import\n\n# Collapse tf-slim into a single namespace.\nfrom inception.slim import inception_model as inception\nfrom inception.slim import losses\nfrom inception.slim import ops\nfrom inception.slim import scopes\nfrom inception.slim import variables\nfrom inception.slim.scopes import arg_scope\n'"
model_zoo/models/inception/inception/slim/variables.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains convenience wrappers for creating variables in TF-Slim.\n\nThe variables module is typically used for defining model variables from the\nops routines (see slim.ops). Such variables are used for training, evaluation\nand inference of models.\n\nAll the variables created through this module would be added to the\nMODEL_VARIABLES collection, if you create a model variable outside slim, it can\nbe added with slim.variables.add_variable(external_variable, reuse).\n\nUsage:\n  weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n  l2_regularizer = lambda t: losses.l2_loss(t, weight=0.0005)\n  weights = variables.variable(\'weights\',\n                               shape=[100, 100],\n                               initializer=weights_initializer,\n                               regularizer=l2_regularizer,\n                               device=\'/cpu:0\')\n\n  biases = variables.variable(\'biases\',\n                              shape=[100],\n                              initializer=tf.zeros_initializer,\n                              device=\'/cpu:0\')\n\n  # More complex example.\n\n  net = slim.ops.conv2d(input, 32, [3, 3], scope=\'conv1\')\n  net = slim.ops.conv2d(net, 64, [3, 3], scope=\'conv2\')\n  with slim.arg_scope([variables.variable], restore=False):\n    net = slim.ops.conv2d(net, 64, [3, 3], scope=\'conv3\')\n\n  # Get all model variables from all the layers.\n  model_variables = slim.variables.get_variables()\n\n  # Get all model variables from a specific the layer, i.e \'conv1\'.\n  conv1_variables = slim.variables.get_variables(\'conv1\')\n\n  # Get all weights from all the layers.\n  weights = slim.variables.get_variables_by_name(\'weights\')\n\n  # Get all bias from all the layers.\n  biases = slim.variables.get_variables_by_name(\'biases\')\n\n  # Get all variables to restore.\n  # (i.e. only those created by \'conv1\' and \'conv2\')\n  variables_to_restore = slim.variables.get_variables_to_restore()\n\n************************************************\n* Initializing model variables from a checkpoint\n************************************************\n\n# Create some variables.\nv1 = slim.variables.variable(name=""v1"", ..., restore=False)\nv2 = slim.variables.variable(name=""v2"", ...) # By default restore=True\n...\n# The list of variables to restore should only contain \'v2\'.\nvariables_to_restore = slim.variables.get_variables_to_restore()\nrestorer = tf.train.Saver(variables_to_restore)\nwith tf.Session() as sess:\n  # Restore variables from disk.\n  restorer.restore(sess, ""/tmp/model.ckpt"")\n  print(""Model restored."")\n  # Do some work with the model\n  ...\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom inception.slim import scopes\n\n# Collection containing all the variables created using slim.variables\nMODEL_VARIABLES = \'_model_variables_\'\n\n# Collection containing the slim.variables that are created with restore=True.\nVARIABLES_TO_RESTORE = \'_variables_to_restore_\'\n\n\ndef add_variable(var, restore=True):\n  """"""Adds a variable to the MODEL_VARIABLES collection.\n\n    Optionally it will add the variable to  the VARIABLES_TO_RESTORE collection.\n  Args:\n    var: a variable.\n    restore: whether the variable should be added to the\n      VARIABLES_TO_RESTORE collection.\n\n  """"""\n  collections = [MODEL_VARIABLES]\n  if restore:\n    collections.append(VARIABLES_TO_RESTORE)\n  for collection in collections:\n    if var not in tf.get_collection(collection):\n      tf.add_to_collection(collection, var)\n\n\ndef get_variables(scope=None, suffix=None):\n  """"""Gets the list of variables, filtered by scope and/or suffix.\n\n  Args:\n    scope: an optional scope for filtering the variables to return.\n    suffix: an optional suffix for filtering the variables to return.\n\n  Returns:\n    a copied list of variables with scope and suffix.\n  """"""\n  candidates = tf.get_collection(MODEL_VARIABLES, scope)[:]\n  if suffix is not None:\n    candidates = [var for var in candidates if var.op.name.endswith(suffix)]\n  return candidates\n\n\ndef get_variables_to_restore():\n  """"""Gets the list of variables to restore.\n\n  Returns:\n    a copied list of variables.\n  """"""\n  return tf.get_collection(VARIABLES_TO_RESTORE)[:]\n\n\ndef get_variables_by_name(given_name, scope=None):\n  """"""Gets the list of variables that were given that name.\n\n  Args:\n    given_name: name given to the variable without scope.\n    scope: an optional scope for filtering the variables to return.\n\n  Returns:\n    a copied list of variables with the given name and prefix.\n  """"""\n  return get_variables(scope=scope, suffix=given_name)\n\n\ndef get_unique_variable(name):\n  """"""Gets the variable uniquely identified by that name.\n\n  Args:\n    name: a name that uniquely identifies the variable.\n\n  Returns:\n    a tensorflow variable.\n\n  Raises:\n    ValueError: if no variable uniquely identified by the name exists.\n  """"""\n  candidates = tf.get_collection(tf.GraphKeys.VARIABLES, name)\n  if not candidates:\n    raise ValueError(\'Couldnt find variable %s\' % name)\n\n  for candidate in candidates:\n    if candidate.op.name == name:\n      return candidate\n  raise ValueError(\'Variable %s does not uniquely identify a variable\', name)\n\n\nclass VariableDeviceChooser(object):\n  """"""Slim device chooser for variables.\n\n  When using a parameter server it will assign them in a round-robin fashion.\n  When not using a parameter server it allows GPU:0 placement otherwise CPU:0.\n  """"""\n\n  def __init__(self,\n               num_parameter_servers=0,\n               ps_device=\'/job:ps\',\n               placement=\'CPU:0\'):\n    """"""Initialize VariableDeviceChooser.\n\n    Args:\n      num_parameter_servers: number of parameter servers.\n      ps_device: string representing the parameter server device.\n      placement: string representing the placement of the variable either CPU:0\n        or GPU:0. When using parameter servers forced to CPU:0.\n    """"""\n    self._num_ps = num_parameter_servers\n    self._ps_device = ps_device\n    self._placement = placement if num_parameter_servers == 0 else \'CPU:0\'\n    self._next_task_id = 0\n\n  def __call__(self, op):\n    device_string = \'\'\n    if self._num_ps > 0:\n      task_id = self._next_task_id\n      self._next_task_id = (self._next_task_id + 1) % self._num_ps\n      device_string = \'%s/task:%d\' % (self._ps_device, task_id)\n    device_string += \'/%s\' % self._placement\n    return device_string\n\n\n# TODO(sguada) Remove once get_variable is able to colocate op.devices.\ndef variable_device(device, name):\n  """"""Fix the variable device to colocate its ops.""""""\n  if callable(device):\n    var_name = tf.get_variable_scope().name + \'/\' + name\n    var_def = tf.NodeDef(name=var_name, op=\'Variable\')\n    device = device(var_def)\n  if device is None:\n    device = \'\'\n  return device\n\n\n@scopes.add_arg_scope\ndef global_step(device=\'\'):\n  """"""Returns the global step variable.\n\n  Args:\n    device: Optional device to place the variable. It can be an string or a\n      function that is called to get the device for the variable.\n\n  Returns:\n    the tensor representing the global step variable.\n  """"""\n  global_step_ref = tf.get_collection(tf.GraphKeys.GLOBAL_STEP)\n  if global_step_ref:\n    return global_step_ref[0]\n  else:\n    collections = [\n        VARIABLES_TO_RESTORE,\n        tf.GraphKeys.VARIABLES,\n        tf.GraphKeys.GLOBAL_STEP,\n    ]\n    # Get the device for the variable.\n    with tf.device(variable_device(device, \'global_step\')):\n      return tf.get_variable(\'global_step\', shape=[], dtype=tf.int64,\n                             initializer=tf.zeros_initializer,\n                             trainable=False, collections=collections)\n\n\n@scopes.add_arg_scope\ndef variable(name, shape=None, dtype=tf.float32, initializer=None,\n             regularizer=None, trainable=True, collections=None, device=\'\',\n             restore=True):\n  """"""Gets an existing variable with these parameters or creates a new one.\n\n    It also add itself to a group with its name.\n\n  Args:\n    name: the name of the new or existing variable.\n    shape: shape of the new or existing variable.\n    dtype: type of the new or existing variable (defaults to `DT_FLOAT`).\n    initializer: initializer for the variable if one is created.\n    regularizer: a (Tensor -> Tensor or None) function; the result of\n        applying it on a newly created variable will be added to the collection\n        GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.\n    trainable: If `True` also add the variable to the graph collection\n      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n    collections: A list of collection names to which the Variable will be added.\n      Note that the variable is always also added to the tf.GraphKeys.VARIABLES\n      and MODEL_VARIABLES collections.\n    device: Optional device to place the variable. It can be an string or a\n      function that is called to get the device for the variable.\n    restore: whether the variable should be added to the\n      VARIABLES_TO_RESTORE collection.\n\n  Returns:\n    The created or existing variable.\n  """"""\n  collections = list(collections or [])\n\n  # Make sure variables are added to tf.GraphKeys.VARIABLES and MODEL_VARIABLES\n  collections += [tf.GraphKeys.VARIABLES, MODEL_VARIABLES]\n  # Add to VARIABLES_TO_RESTORE if necessary\n  if restore:\n    collections.append(VARIABLES_TO_RESTORE)\n  # Remove duplicates\n  collections = set(collections)\n  # Get the device for the variable.\n  with tf.device(variable_device(device, name)):\n    return tf.get_variable(name, shape=shape, dtype=dtype,\n                           initializer=initializer, regularizer=regularizer,\n                           trainable=trainable, collections=collections)\n'"
model_zoo/models/inception/inception/slim/variables_test.py,0,"b'# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for slim.variables.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom inception.slim import scopes\nfrom inception.slim import variables\n\n\nclass VariablesTest(tf.test.TestCase):\n\n  def testCreateVariable(self):\n    with self.test_session():\n      with tf.variable_scope(\'A\'):\n        a = variables.variable(\'a\', [5])\n        self.assertEquals(a.op.name, \'A/a\')\n        self.assertListEqual(a.get_shape().as_list(), [5])\n\n  def testGetVariables(self):\n    with self.test_session():\n      with tf.variable_scope(\'A\'):\n        a = variables.variable(\'a\', [5])\n      with tf.variable_scope(\'B\'):\n        b = variables.variable(\'a\', [5])\n      self.assertEquals([a, b], variables.get_variables())\n      self.assertEquals([a], variables.get_variables(\'A\'))\n      self.assertEquals([b], variables.get_variables(\'B\'))\n\n  def testGetVariablesSuffix(self):\n    with self.test_session():\n      with tf.variable_scope(\'A\'):\n        a = variables.variable(\'a\', [5])\n      with tf.variable_scope(\'A\'):\n        b = variables.variable(\'b\', [5])\n      self.assertEquals([a], variables.get_variables(suffix=\'a\'))\n      self.assertEquals([b], variables.get_variables(suffix=\'b\'))\n\n  def testGetVariableWithSingleVar(self):\n    with self.test_session():\n      with tf.variable_scope(\'parent\'):\n        a = variables.variable(\'child\', [5])\n      self.assertEquals(a, variables.get_unique_variable(\'parent/child\'))\n\n  def testGetVariableWithDistractors(self):\n    with self.test_session():\n      with tf.variable_scope(\'parent\'):\n        a = variables.variable(\'child\', [5])\n        with tf.variable_scope(\'child\'):\n          variables.variable(\'grandchild1\', [7])\n          variables.variable(\'grandchild2\', [9])\n      self.assertEquals(a, variables.get_unique_variable(\'parent/child\'))\n\n  def testGetVariableThrowsExceptionWithNoMatch(self):\n    var_name = \'cant_find_me\'\n    with self.test_session():\n      with self.assertRaises(ValueError):\n        variables.get_unique_variable(var_name)\n\n  def testGetThrowsExceptionWithChildrenButNoMatch(self):\n    var_name = \'parent/child\'\n    with self.test_session():\n      with tf.variable_scope(var_name):\n        variables.variable(\'grandchild1\', [7])\n        variables.variable(\'grandchild2\', [9])\n      with self.assertRaises(ValueError):\n        variables.get_unique_variable(var_name)\n\n  def testGetVariablesToRestore(self):\n    with self.test_session():\n      with tf.variable_scope(\'A\'):\n        a = variables.variable(\'a\', [5])\n      with tf.variable_scope(\'B\'):\n        b = variables.variable(\'a\', [5])\n      self.assertEquals([a, b], variables.get_variables_to_restore())\n\n  def testNoneGetVariablesToRestore(self):\n    with self.test_session():\n      with tf.variable_scope(\'A\'):\n        a = variables.variable(\'a\', [5], restore=False)\n      with tf.variable_scope(\'B\'):\n        b = variables.variable(\'a\', [5], restore=False)\n      self.assertEquals([], variables.get_variables_to_restore())\n      self.assertEquals([a, b], variables.get_variables())\n\n  def testGetMixedVariablesToRestore(self):\n    with self.test_session():\n      with tf.variable_scope(\'A\'):\n        a = variables.variable(\'a\', [5])\n        b = variables.variable(\'b\', [5], restore=False)\n      with tf.variable_scope(\'B\'):\n        c = variables.variable(\'c\', [5])\n        d = variables.variable(\'d\', [5], restore=False)\n      self.assertEquals([a, b, c, d], variables.get_variables())\n      self.assertEquals([a, c], variables.get_variables_to_restore())\n\n  def testReuseVariable(self):\n    with self.test_session():\n      with tf.variable_scope(\'A\'):\n        a = variables.variable(\'a\', [])\n      with tf.variable_scope(\'A\', reuse=True):\n        b = variables.variable(\'a\', [])\n      self.assertEquals(a, b)\n      self.assertListEqual([a], variables.get_variables())\n\n  def testVariableWithDevice(self):\n    with self.test_session():\n      with tf.variable_scope(\'A\'):\n        a = variables.variable(\'a\', [], device=\'cpu:0\')\n        b = variables.variable(\'b\', [], device=\'cpu:1\')\n      self.assertDeviceEqual(a.device, \'cpu:0\')\n      self.assertDeviceEqual(b.device, \'cpu:1\')\n\n  def testVariableWithDeviceFromScope(self):\n    with self.test_session():\n      with tf.device(\'/cpu:0\'):\n        a = variables.variable(\'a\', [])\n        b = variables.variable(\'b\', [], device=\'cpu:1\')\n      self.assertDeviceEqual(a.device, \'cpu:0\')\n      self.assertDeviceEqual(b.device, \'cpu:1\')\n\n  def testVariableWithDeviceFunction(self):\n    class DevFn(object):\n\n      def __init__(self):\n        self.counter = -1\n\n      def __call__(self, op):\n        self.counter += 1\n        return \'cpu:%d\' % self.counter\n\n    with self.test_session():\n      with scopes.arg_scope([variables.variable], device=DevFn()):\n        a = variables.variable(\'a\', [])\n        b = variables.variable(\'b\', [])\n        c = variables.variable(\'c\', [], device=\'cpu:12\')\n        d = variables.variable(\'d\', [])\n        with tf.device(\'cpu:99\'):\n          e_init = tf.constant(12)\n        e = variables.variable(\'e\', initializer=e_init)\n      self.assertDeviceEqual(a.device, \'cpu:0\')\n      self.assertDeviceEqual(a.initial_value.device, \'cpu:0\')\n      self.assertDeviceEqual(b.device, \'cpu:1\')\n      self.assertDeviceEqual(b.initial_value.device, \'cpu:1\')\n      self.assertDeviceEqual(c.device, \'cpu:12\')\n      self.assertDeviceEqual(c.initial_value.device, \'cpu:12\')\n      self.assertDeviceEqual(d.device, \'cpu:2\')\n      self.assertDeviceEqual(d.initial_value.device, \'cpu:2\')\n      self.assertDeviceEqual(e.device, \'cpu:3\')\n      self.assertDeviceEqual(e.initial_value.device, \'cpu:99\')\n\n  def testVariableWithReplicaDeviceSetter(self):\n    with self.test_session():\n      with tf.device(tf.train.replica_device_setter(ps_tasks=2)):\n        a = variables.variable(\'a\', [])\n        b = variables.variable(\'b\', [])\n        c = variables.variable(\'c\', [], device=\'cpu:12\')\n        d = variables.variable(\'d\', [])\n        with tf.device(\'cpu:99\'):\n          e_init = tf.constant(12)\n        e = variables.variable(\'e\', initializer=e_init)\n      # The values below highlight how the replica_device_setter puts initial\n      # values on the worker job, and how it merges explicit devices.\n      self.assertDeviceEqual(a.device, \'/job:ps/task:0/cpu:0\')\n      self.assertDeviceEqual(a.initial_value.device, \'/job:worker/cpu:0\')\n      self.assertDeviceEqual(b.device, \'/job:ps/task:1/cpu:0\')\n      self.assertDeviceEqual(b.initial_value.device, \'/job:worker/cpu:0\')\n      self.assertDeviceEqual(c.device, \'/job:ps/task:0/cpu:12\')\n      self.assertDeviceEqual(c.initial_value.device, \'/job:worker/cpu:12\')\n      self.assertDeviceEqual(d.device, \'/job:ps/task:1/cpu:0\')\n      self.assertDeviceEqual(d.initial_value.device, \'/job:worker/cpu:0\')\n      self.assertDeviceEqual(e.device, \'/job:ps/task:0/cpu:0\')\n      self.assertDeviceEqual(e.initial_value.device, \'/job:worker/cpu:99\')\n\n  def testVariableWithVariableDeviceChooser(self):\n\n    with tf.Graph().as_default():\n      device_fn = variables.VariableDeviceChooser(num_parameter_servers=2)\n      with scopes.arg_scope([variables.variable], device=device_fn):\n        a = variables.variable(\'a\', [])\n        b = variables.variable(\'b\', [])\n        c = variables.variable(\'c\', [], device=\'cpu:12\')\n        d = variables.variable(\'d\', [])\n        with tf.device(\'cpu:99\'):\n          e_init = tf.constant(12)\n        e = variables.variable(\'e\', initializer=e_init)\n      # The values below highlight how the VariableDeviceChooser puts initial\n      # values on the same device as the variable job.\n      self.assertDeviceEqual(a.device, \'/job:ps/task:0/cpu:0\')\n      self.assertDeviceEqual(a.initial_value.device, a.device)\n      self.assertDeviceEqual(b.device, \'/job:ps/task:1/cpu:0\')\n      self.assertDeviceEqual(b.initial_value.device, b.device)\n      self.assertDeviceEqual(c.device, \'/cpu:12\')\n      self.assertDeviceEqual(c.initial_value.device, c.device)\n      self.assertDeviceEqual(d.device, \'/job:ps/task:0/cpu:0\')\n      self.assertDeviceEqual(d.initial_value.device, d.device)\n      self.assertDeviceEqual(e.device, \'/job:ps/task:1/cpu:0\')\n      self.assertDeviceEqual(e.initial_value.device, \'/cpu:99\')\n\n  def testVariableGPUPlacement(self):\n\n    with tf.Graph().as_default():\n      device_fn = variables.VariableDeviceChooser(placement=\'gpu:0\')\n      with scopes.arg_scope([variables.variable], device=device_fn):\n        a = variables.variable(\'a\', [])\n        b = variables.variable(\'b\', [])\n        c = variables.variable(\'c\', [], device=\'cpu:12\')\n        d = variables.variable(\'d\', [])\n        with tf.device(\'cpu:99\'):\n          e_init = tf.constant(12)\n        e = variables.variable(\'e\', initializer=e_init)\n      # The values below highlight how the VariableDeviceChooser puts initial\n      # values on the same device as the variable job.\n      self.assertDeviceEqual(a.device, \'/gpu:0\')\n      self.assertDeviceEqual(a.initial_value.device, a.device)\n      self.assertDeviceEqual(b.device, \'/gpu:0\')\n      self.assertDeviceEqual(b.initial_value.device, b.device)\n      self.assertDeviceEqual(c.device, \'/cpu:12\')\n      self.assertDeviceEqual(c.initial_value.device, c.device)\n      self.assertDeviceEqual(d.device, \'/gpu:0\')\n      self.assertDeviceEqual(d.initial_value.device, d.device)\n      self.assertDeviceEqual(e.device, \'/gpu:0\')\n      self.assertDeviceEqual(e.initial_value.device, \'/cpu:99\')\n\n  def testVariableCollection(self):\n    with self.test_session():\n      a = variables.variable(\'a\', [], collections=\'A\')\n      b = variables.variable(\'b\', [], collections=\'B\')\n      self.assertEquals(a, tf.get_collection(\'A\')[0])\n      self.assertEquals(b, tf.get_collection(\'B\')[0])\n\n  def testVariableCollections(self):\n    with self.test_session():\n      a = variables.variable(\'a\', [], collections=[\'A\', \'C\'])\n      b = variables.variable(\'b\', [], collections=[\'B\', \'C\'])\n      self.assertEquals(a, tf.get_collection(\'A\')[0])\n      self.assertEquals(b, tf.get_collection(\'B\')[0])\n\n  def testVariableCollectionsWithArgScope(self):\n    with self.test_session():\n      with scopes.arg_scope([variables.variable], collections=\'A\'):\n        a = variables.variable(\'a\', [])\n        b = variables.variable(\'b\', [])\n      self.assertListEqual([a, b], tf.get_collection(\'A\'))\n\n  def testVariableCollectionsWithArgScopeNested(self):\n    with self.test_session():\n      with scopes.arg_scope([variables.variable], collections=\'A\'):\n        a = variables.variable(\'a\', [])\n        with scopes.arg_scope([variables.variable], collections=\'B\'):\n          b = variables.variable(\'b\', [])\n      self.assertEquals(a, tf.get_collection(\'A\')[0])\n      self.assertEquals(b, tf.get_collection(\'B\')[0])\n\n  def testVariableCollectionsWithArgScopeNonNested(self):\n    with self.test_session():\n      with scopes.arg_scope([variables.variable], collections=\'A\'):\n        a = variables.variable(\'a\', [])\n      with scopes.arg_scope([variables.variable], collections=\'B\'):\n        b = variables.variable(\'b\', [])\n      variables.variable(\'c\', [])\n      self.assertListEqual([a], tf.get_collection(\'A\'))\n      self.assertListEqual([b], tf.get_collection(\'B\'))\n\n  def testVariableRestoreWithArgScopeNested(self):\n    with self.test_session():\n      with scopes.arg_scope([variables.variable], restore=True):\n        a = variables.variable(\'a\', [])\n        with scopes.arg_scope([variables.variable],\n                              trainable=False,\n                              collections=[\'A\', \'B\']):\n          b = variables.variable(\'b\', [])\n        c = variables.variable(\'c\', [])\n      self.assertListEqual([a, b, c], variables.get_variables_to_restore())\n      self.assertListEqual([a, c], tf.trainable_variables())\n      self.assertListEqual([b], tf.get_collection(\'A\'))\n      self.assertListEqual([b], tf.get_collection(\'B\'))\n\n\nclass GetVariablesByNameTest(tf.test.TestCase):\n\n  def testGetVariableGivenNameScoped(self):\n    with self.test_session():\n      with tf.variable_scope(\'A\'):\n        a = variables.variable(\'a\', [5])\n        b = variables.variable(\'b\', [5])\n        self.assertEquals([a], variables.get_variables_by_name(\'a\'))\n        self.assertEquals([b], variables.get_variables_by_name(\'b\'))\n\n  def testGetVariablesByNameReturnsByValueWithScope(self):\n    with self.test_session():\n      with tf.variable_scope(\'A\'):\n        a = variables.variable(\'a\', [5])\n        matched_variables = variables.get_variables_by_name(\'a\')\n\n        # If variables.get_variables_by_name returns the list by reference, the\n        # following append should persist, and be returned, in subsequent calls\n        # to variables.get_variables_by_name(\'a\').\n        matched_variables.append(4)\n\n        matched_variables = variables.get_variables_by_name(\'a\')\n        self.assertEquals([a], matched_variables)\n\n  def testGetVariablesByNameReturnsByValueWithoutScope(self):\n    with self.test_session():\n      a = variables.variable(\'a\', [5])\n      matched_variables = variables.get_variables_by_name(\'a\')\n\n      # If variables.get_variables_by_name returns the list by reference, the\n      # following append should persist, and be returned, in subsequent calls\n      # to variables.get_variables_by_name(\'a\').\n      matched_variables.append(4)\n\n      matched_variables = variables.get_variables_by_name(\'a\')\n      self.assertEquals([a], matched_variables)\n\n\nclass GlobalStepTest(tf.test.TestCase):\n\n  def testStable(self):\n    with tf.Graph().as_default():\n      gs = variables.global_step()\n      gs2 = variables.global_step()\n      self.assertTrue(gs is gs2)\n\n  def testDevice(self):\n    with tf.Graph().as_default():\n      with scopes.arg_scope([variables.global_step], device=\'/gpu:0\'):\n        gs = variables.global_step()\n      self.assertDeviceEqual(gs.device, \'/gpu:0\')\n\n  def testDeviceFn(self):\n    class DevFn(object):\n\n      def __init__(self):\n        self.counter = -1\n\n      def __call__(self, op):\n        self.counter += 1\n        return \'/cpu:%d\' % self.counter\n\n    with tf.Graph().as_default():\n      with scopes.arg_scope([variables.global_step], device=DevFn()):\n        gs = variables.global_step()\n        gs2 = variables.global_step()\n      self.assertDeviceEqual(gs.device, \'/cpu:0\')\n      self.assertEquals(gs, gs2)\n      self.assertDeviceEqual(gs2.device, \'/cpu:0\')\n\n  def testReplicaDeviceSetter(self):\n    device_fn = tf.train.replica_device_setter(2)\n    with tf.Graph().as_default():\n      with scopes.arg_scope([variables.global_step], device=device_fn):\n        gs = variables.global_step()\n        gs2 = variables.global_step()\n        self.assertEquals(gs, gs2)\n        self.assertDeviceEqual(gs.device, \'/job:ps/task:0\')\n        self.assertDeviceEqual(gs.initial_value.device, \'/job:ps/task:0\')\n        self.assertDeviceEqual(gs2.device, \'/job:ps/task:0\')\n        self.assertDeviceEqual(gs2.initial_value.device, \'/job:ps/task:0\')\n\n  def testVariableWithVariableDeviceChooser(self):\n\n    with tf.Graph().as_default():\n      device_fn = variables.VariableDeviceChooser()\n      with scopes.arg_scope([variables.global_step], device=device_fn):\n        gs = variables.global_step()\n        gs2 = variables.global_step()\n        self.assertEquals(gs, gs2)\n        self.assertDeviceEqual(gs.device, \'cpu:0\')\n        self.assertDeviceEqual(gs.initial_value.device, gs.device)\n        self.assertDeviceEqual(gs2.device, \'cpu:0\')\n        self.assertDeviceEqual(gs2.initial_value.device, gs2.device)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
