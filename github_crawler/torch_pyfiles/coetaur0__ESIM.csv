file_path,api_count,code
setup.py,0,"b""from setuptools import setup\n\n\nsetup(name='ESIM',\n      version='1.0.1',\n      url='https://github.com/coetaur0/ESIM',\n      license='Apache 2',\n      author='Aurelien Coet',\n      author_email='aurelien.coet19@gmail.com',\n      description='Implementation of the ESIM model for NLI with PyTorch',\n      packages=[\n        'esim'\n      ],\n      install_requires=[\n        'wget',\n        'numpy',\n        'nltk',\n        'matplotlib',\n        'tqdm',\n        'torch'\n      ])\n"""
esim/__init__.py,0,b''
esim/data.py,8,"b'""""""\nPreprocessor and dataset definition for NLI.\n""""""\n# Aurelien Coet, 2018.\n\nimport string\nimport torch\nimport numpy as np\n\nfrom collections import Counter\nfrom torch.utils.data import Dataset\n\n\nclass Preprocessor(object):\n    """"""\n    Preprocessor class for Natural Language Inference datasets.\n\n    The class can be used to read NLI datasets, build worddicts for them\n    and transform their premises, hypotheses and labels into lists of\n    integer indices.\n    """"""\n\n    def __init__(self,\n                 lowercase=False,\n                 ignore_punctuation=False,\n                 num_words=None,\n                 stopwords=[],\n                 labeldict={},\n                 bos=None,\n                 eos=None):\n        """"""\n        Args:\n            lowercase: A boolean indicating whether the words in the datasets\n                being preprocessed must be lowercased or not. Defaults to\n                False.\n            ignore_punctuation: A boolean indicating whether punctuation must\n                be ignored or not in the datasets preprocessed by the object.\n            num_words: An integer indicating the number of words to use in the\n                worddict of the object. If set to None, all the words in the\n                data are kept. Defaults to None.\n            stopwords: A list of words that must be ignored when building the\n                worddict for a dataset. Defaults to an empty list.\n            bos: A string indicating the symbol to use for the \'beginning of\n                sentence\' token in the data. If set to None, the token isn\'t\n                used. Defaults to None.\n            eos: A string indicating the symbol to use for the \'end of\n                sentence\' token in the data. If set to None, the token isn\'t\n                used. Defaults to None.\n        """"""\n        self.lowercase = lowercase\n        self.ignore_punctuation = ignore_punctuation\n        self.num_words = num_words\n        self.stopwords = stopwords\n        self.labeldict = labeldict\n        self.bos = bos\n        self.eos = eos\n\n    def read_data(self, filepath):\n        """"""\n        Read the premises, hypotheses and labels from some NLI dataset\'s\n        file and return them in a dictionary. The file should be in the same\n        form as SNLI\'s .txt files.\n\n        Args:\n            filepath: The path to a file containing some premises, hypotheses\n                and labels that must be read. The file should be formatted in\n                the same way as the SNLI (and MultiNLI) dataset.\n\n        Returns:\n            A dictionary containing three lists, one for the premises, one for\n            the hypotheses, and one for the labels in the input data.\n        """"""\n        with open(filepath, ""r"", encoding=""utf8"") as input_data:\n            ids, premises, hypotheses, labels = [], [], [], []\n\n            # Translation tables to remove parentheses and punctuation from\n            # strings.\n            parentheses_table = str.maketrans({""("": None, "")"": None})\n            punct_table = str.maketrans({key: "" ""\n                                         for key in string.punctuation})\n\n            # Ignore the headers on the first line of the file.\n            next(input_data)\n\n            for line in input_data:\n                line = line.strip().split(""\\t"")\n\n                # Ignore sentences that have no gold label.\n                if line[0] == ""-"":\n                    continue\n\n                pair_id = line[7]\n                premise = line[1]\n                hypothesis = line[2]\n\n                # Remove \'(\' and \')\' from the premises and hypotheses.\n                premise = premise.translate(parentheses_table)\n                hypothesis = hypothesis.translate(parentheses_table)\n\n                if self.lowercase:\n                    premise = premise.lower()\n                    hypothesis = hypothesis.lower()\n\n                if self.ignore_punctuation:\n                    premise = premise.translate(punct_table)\n                    hypothesis = hypothesis.translate(punct_table)\n\n                # Each premise and hypothesis is split into a list of words.\n                premises.append([w for w in premise.rstrip().split()\n                                 if w not in self.stopwords])\n                hypotheses.append([w for w in hypothesis.rstrip().split()\n                                   if w not in self.stopwords])\n                labels.append(line[0])\n                ids.append(pair_id)\n\n            return {""ids"": ids,\n                    ""premises"": premises,\n                    ""hypotheses"": hypotheses,\n                    ""labels"": labels}\n\n    def build_worddict(self, data):\n        """"""\n        Build a dictionary associating words to unique integer indices for\n        some dataset. The worddict can then be used to transform the words\n        in datasets to their indices.\n\n        Args:\n            data: A dictionary containing the premises, hypotheses and\n                labels of some NLI dataset, in the format returned by the\n                \'read_data\' method of the Preprocessor class.\n        """"""\n        words = []\n        [words.extend(sentence) for sentence in data[""premises""]]\n        [words.extend(sentence) for sentence in data[""hypotheses""]]\n\n        counts = Counter(words)\n        num_words = self.num_words\n        if self.num_words is None:\n            num_words = len(counts)\n\n        self.worddict = {}\n\n        # Special indices are used for padding, out-of-vocabulary words, and\n        # beginning and end of sentence tokens.\n        self.worddict[""_PAD_""] = 0\n        self.worddict[""_OOV_""] = 1\n\n        offset = 2\n        if self.bos:\n            self.worddict[""_BOS_""] = 2\n            offset += 1\n        if self.eos:\n            self.worddict[""_EOS_""] = 3\n            offset += 1\n\n        for i, word in enumerate(counts.most_common(num_words)):\n            self.worddict[word[0]] = i + offset\n\n        if self.labeldict == {}:\n            label_names = set(data[""labels""])\n            self.labeldict = {label_name: i\n                              for i, label_name in enumerate(label_names)}\n\n    def words_to_indices(self, sentence):\n        """"""\n        Transform the words in a sentence to their corresponding integer\n        indices.\n\n        Args:\n            sentence: A list of words that must be transformed to indices.\n\n        Returns:\n            A list of indices.\n        """"""\n        indices = []\n        # Include the beggining of sentence token at the start of the sentence\n        # if one is defined.\n        if self.bos:\n            indices.append(self.worddict[""_BOS_""])\n\n        for word in sentence:\n            if word in self.worddict:\n                index = self.worddict[word]\n            else:\n                # Words absent from \'worddict\' are treated as a special\n                # out-of-vocabulary word (OOV).\n                index = self.worddict[""_OOV_""]\n            indices.append(index)\n        # Add the end of sentence token at the end of the sentence if one\n        # is defined.\n        if self.eos:\n            indices.append(self.worddict[""_EOS_""])\n\n        return indices\n\n    def indices_to_words(self, indices):\n        """"""\n        Transform the indices in a list to their corresponding words in\n        the object\'s worddict.\n\n        Args:\n            indices: A list of integer indices corresponding to words in\n                the Preprocessor\'s worddict.\n\n        Returns:\n            A list of words.\n        """"""\n        return [list(self.worddict.keys())[list(self.worddict.values())\n                                           .index(i)]\n                for i in indices]\n\n    def transform_to_indices(self, data):\n        """"""\n        Transform the words in the premises and hypotheses of a dataset, as\n        well as their associated labels, to integer indices.\n\n        Args:\n            data: A dictionary containing lists of premises, hypotheses\n                and labels, in the format returned by the \'read_data\'\n                method of the Preprocessor class.\n\n        Returns:\n            A dictionary containing the transformed premises, hypotheses and\n            labels.\n        """"""\n        transformed_data = {""ids"": [],\n                            ""premises"": [],\n                            ""hypotheses"": [],\n                            ""labels"": []}\n\n        for i, premise in enumerate(data[""premises""]):\n            # Ignore sentences that have a label for which no index was\n            # defined in \'labeldict\'.\n            label = data[""labels""][i]\n            if label not in self.labeldict and label != ""hidden"":\n                continue\n\n            transformed_data[""ids""].append(data[""ids""][i])\n\n            if label == ""hidden"":\n                transformed_data[""labels""].append(-1)\n            else:\n                transformed_data[""labels""].append(self.labeldict[label])\n\n            indices = self.words_to_indices(premise)\n            transformed_data[""premises""].append(indices)\n\n            indices = self.words_to_indices(data[""hypotheses""][i])\n            transformed_data[""hypotheses""].append(indices)\n\n        return transformed_data\n\n    def build_embedding_matrix(self, embeddings_file):\n        """"""\n        Build an embedding matrix with pretrained weights for object\'s\n        worddict.\n\n        Args:\n            embeddings_file: A file containing pretrained word embeddings.\n\n        Returns:\n            A numpy matrix of size (num_words+n_special_tokens, embedding_dim)\n            containing pretrained word embeddings (the +n_special_tokens is for\n            the padding and out-of-vocabulary tokens, as well as BOS and EOS if\n            they\'re used).\n        """"""\n        # Load the word embeddings in a dictionnary.\n        embeddings = {}\n        with open(embeddings_file, ""r"", encoding=""utf8"") as input_data:\n            for line in input_data:\n                line = line.split()\n\n                try:\n                    # Check that the second element on the line is the start\n                    # of the embedding and not another word. Necessary to\n                    # ignore multiple word lines.\n                    float(line[1])\n                    word = line[0]\n                    if word in self.worddict:\n                        embeddings[word] = line[1:]\n\n                # Ignore lines corresponding to multiple words separated\n                # by spaces.\n                except ValueError:\n                    continue\n\n        num_words = len(self.worddict)\n        embedding_dim = len(list(embeddings.values())[0])\n        embedding_matrix = np.zeros((num_words, embedding_dim))\n\n        # Actual building of the embedding matrix.\n        missed = 0\n        for word, i in self.worddict.items():\n            if word in embeddings:\n                embedding_matrix[i] = np.array(embeddings[word], dtype=float)\n            else:\n                if word == ""_PAD_"":\n                    continue\n                missed += 1\n                # Out of vocabulary words are initialised with random gaussian\n                # samples.\n                embedding_matrix[i] = np.random.normal(size=(embedding_dim))\n        print(""Missed words: "", missed)\n\n        return embedding_matrix\n\n\nclass NLIDataset(Dataset):\n    """"""\n    Dataset class for Natural Language Inference datasets.\n\n    The class can be used to read preprocessed datasets where the premises,\n    hypotheses and labels have been transformed to unique integer indices\n    (this can be done with the \'preprocess_data\' script in the \'scripts\'\n    folder of this repository).\n    """"""\n\n    def __init__(self,\n                 data,\n                 padding_idx=0,\n                 max_premise_length=None,\n                 max_hypothesis_length=None):\n        """"""\n        Args:\n            data: A dictionary containing the preprocessed premises,\n                hypotheses and labels of some dataset.\n            padding_idx: An integer indicating the index being used for the\n                padding token in the preprocessed data. Defaults to 0.\n            max_premise_length: An integer indicating the maximum length\n                accepted for the sequences in the premises. If set to None,\n                the length of the longest premise in \'data\' is used.\n                Defaults to None.\n            max_hypothesis_length: An integer indicating the maximum length\n                accepted for the sequences in the hypotheses. If set to None,\n                the length of the longest hypothesis in \'data\' is used.\n                Defaults to None.\n        """"""\n        self.premises_lengths = [len(seq) for seq in data[""premises""]]\n        self.max_premise_length = max_premise_length\n        if self.max_premise_length is None:\n            self.max_premise_length = max(self.premises_lengths)\n\n        self.hypotheses_lengths = [len(seq) for seq in data[""hypotheses""]]\n        self.max_hypothesis_length = max_hypothesis_length\n        if self.max_hypothesis_length is None:\n            self.max_hypothesis_length = max(self.hypotheses_lengths)\n\n        self.num_sequences = len(data[""premises""])\n\n        self.data = {""ids"": [],\n                     ""premises"": torch.ones((self.num_sequences,\n                                             self.max_premise_length),\n                                            dtype=torch.long) * padding_idx,\n                     ""hypotheses"": torch.ones((self.num_sequences,\n                                               self.max_hypothesis_length),\n                                              dtype=torch.long) * padding_idx,\n                     ""labels"": torch.tensor(data[""labels""], dtype=torch.long)}\n\n        for i, premise in enumerate(data[""premises""]):\n            self.data[""ids""].append(data[""ids""][i])\n            end = min(len(premise), self.max_premise_length)\n            self.data[""premises""][i][:end] = torch.tensor(premise[:end])\n\n            hypothesis = data[""hypotheses""][i]\n            end = min(len(hypothesis), self.max_hypothesis_length)\n            self.data[""hypotheses""][i][:end] = torch.tensor(hypothesis[:end])\n\n    def __len__(self):\n        return self.num_sequences\n\n    def __getitem__(self, index):\n        return {""id"": self.data[""ids""][index],\n                ""premise"": self.data[""premises""][index],\n                ""premise_length"": min(self.premises_lengths[index],\n                                      self.max_premise_length),\n                ""hypothesis"": self.data[""hypotheses""][index],\n                ""hypothesis_length"": min(self.hypotheses_lengths[index],\n                                         self.max_hypothesis_length),\n                ""label"": self.data[""labels""][index]}\n'"
esim/layers.py,4,"b'""""""\nDefinition of custom layers for the ESIM model.\n""""""\n# Aurelien Coet, 2018.\n\nimport torch.nn as nn\n\nfrom .utils import sort_by_seq_lens, masked_softmax, weighted_sum\n\n\n# Class widely inspired from:\n# https://github.com/allenai/allennlp/blob/master/allennlp/modules/input_variational_dropout.py\nclass RNNDropout(nn.Dropout):\n    """"""\n    Dropout layer for the inputs of RNNs.\n\n    Apply the same dropout mask to all the elements of the same sequence in\n    a batch of sequences of size (batch, sequences_length, embedding_dim).\n    """"""\n\n    def forward(self, sequences_batch):\n        """"""\n        Apply dropout to the input batch of sequences.\n\n        Args:\n            sequences_batch: A batch of sequences of vectors that will serve\n                as input to an RNN.\n                Tensor of size (batch, sequences_length, emebdding_dim).\n\n        Returns:\n            A new tensor on which dropout has been applied.\n        """"""\n        ones = sequences_batch.data.new_ones(sequences_batch.shape[0],\n                                             sequences_batch.shape[-1])\n        dropout_mask = nn.functional.dropout(ones, self.p, self.training,\n                                             inplace=False)\n        return dropout_mask.unsqueeze(1) * sequences_batch\n\n\nclass Seq2SeqEncoder(nn.Module):\n    """"""\n    RNN taking variable length padded sequences of vectors as input and\n    encoding them into padded sequences of vectors of the same length.\n\n    This module is useful to handle batches of padded sequences of vectors\n    that have different lengths and that need to be passed through a RNN.\n    The sequences are sorted in descending order of their lengths, packed,\n    passed through the RNN, and the resulting sequences are then padded and\n    permuted back to the original order of the input sequences.\n    """"""\n\n    def __init__(self,\n                 rnn_type,\n                 input_size,\n                 hidden_size,\n                 num_layers=1,\n                 bias=True,\n                 dropout=0.0,\n                 bidirectional=False):\n        """"""\n        Args:\n            rnn_type: The type of RNN to use as encoder in the module.\n                Must be a class inheriting from torch.nn.RNNBase\n                (such as torch.nn.LSTM for example).\n            input_size: The number of expected features in the input of the\n                module.\n            hidden_size: The number of features in the hidden state of the RNN\n                used as encoder by the module.\n            num_layers: The number of recurrent layers in the encoder of the\n                module. Defaults to 1.\n            bias: If False, the encoder does not use bias weights b_ih and\n                b_hh. Defaults to True.\n            dropout: If non-zero, introduces a dropout layer on the outputs\n                of each layer of the encoder except the last one, with dropout\n                probability equal to \'dropout\'. Defaults to 0.0.\n            bidirectional: If True, the encoder of the module is bidirectional.\n                Defaults to False.\n        """"""\n        assert issubclass(rnn_type, nn.RNNBase),\\\n            ""rnn_type must be a class inheriting from torch.nn.RNNBase""\n\n        super(Seq2SeqEncoder, self).__init__()\n\n        self.rnn_type = rnn_type\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n\n        self._encoder = rnn_type(input_size,\n                                 hidden_size,\n                                 num_layers=num_layers,\n                                 bias=bias,\n                                 batch_first=True,\n                                 dropout=dropout,\n                                 bidirectional=bidirectional)\n\n    def forward(self, sequences_batch, sequences_lengths):\n        """"""\n        Args:\n            sequences_batch: A batch of variable length sequences of vectors.\n                The batch is assumed to be of size\n                (batch, sequence, vector_dim).\n            sequences_lengths: A 1D tensor containing the sizes of the\n                sequences in the input batch.\n\n        Returns:\n            reordered_outputs: The outputs (hidden states) of the encoder for\n                the sequences in the input batch, in the same order.\n        """"""\n        sorted_batch, sorted_lengths, _, restoration_idx =\\\n            sort_by_seq_lens(sequences_batch, sequences_lengths)\n        packed_batch = nn.utils.rnn.pack_padded_sequence(sorted_batch,\n                                                         sorted_lengths,\n                                                         batch_first=True)\n\n        outputs, _ = self._encoder(packed_batch, None)\n\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs,\n                                                      batch_first=True)\n        reordered_outputs = outputs.index_select(0, restoration_idx)\n\n        return reordered_outputs\n\n\nclass SoftmaxAttention(nn.Module):\n    """"""\n    Attention layer taking premises and hypotheses encoded by an RNN as input\n    and computing the soft attention between their elements.\n\n    The dot product of the encoded vectors in the premises and hypotheses is\n    first computed. The softmax of the result is then used in a weighted sum\n    of the vectors of the premises for each element of the hypotheses, and\n    conversely for the elements of the premises.\n    """"""\n\n    def forward(self,\n                premise_batch,\n                premise_mask,\n                hypothesis_batch,\n                hypothesis_mask):\n        """"""\n        Args:\n            premise_batch: A batch of sequences of vectors representing the\n                premises in some NLI task. The batch is assumed to have the\n                size (batch, sequences, vector_dim).\n            premise_mask: A mask for the sequences in the premise batch, to\n                ignore padding data in the sequences during the computation of\n                the attention.\n            hypothesis_batch: A batch of sequences of vectors representing the\n                hypotheses in some NLI task. The batch is assumed to have the\n                size (batch, sequences, vector_dim).\n            hypothesis_mask: A mask for the sequences in the hypotheses batch,\n                to ignore padding data in the sequences during the computation\n                of the attention.\n\n        Returns:\n            attended_premises: The sequences of attention vectors for the\n                premises in the input batch.\n            attended_hypotheses: The sequences of attention vectors for the\n                hypotheses in the input batch.\n        """"""\n        # Dot product between premises and hypotheses in each sequence of\n        # the batch.\n        similarity_matrix = premise_batch.bmm(hypothesis_batch.transpose(2, 1)\n                                                              .contiguous())\n\n        # Softmax attention weights.\n        prem_hyp_attn = masked_softmax(similarity_matrix, hypothesis_mask)\n        hyp_prem_attn = masked_softmax(similarity_matrix.transpose(1, 2)\n                                                        .contiguous(),\n                                       premise_mask)\n\n        # Weighted sums of the hypotheses for the the premises attention,\n        # and vice-versa for the attention of the hypotheses.\n        attended_premises = weighted_sum(hypothesis_batch,\n                                         prem_hyp_attn,\n                                         premise_mask)\n        attended_hypotheses = weighted_sum(premise_batch,\n                                           hyp_prem_attn,\n                                           hypothesis_mask)\n\n        return attended_premises, attended_hypotheses\n'"
esim/model.py,8,"b'""""""\nDefinition of the ESIM model.\n""""""\n# Aurelien Coet, 2018.\n\nimport torch\nimport torch.nn as nn\n\nfrom .layers import RNNDropout, Seq2SeqEncoder, SoftmaxAttention\nfrom .utils import get_mask, replace_masked\n\n\nclass ESIM(nn.Module):\n    """"""\n    Implementation of the ESIM model presented in the paper ""Enhanced LSTM for\n    Natural Language Inference"" by Chen et al.\n    """"""\n\n    def __init__(self,\n                 vocab_size,\n                 embedding_dim,\n                 hidden_size,\n                 embeddings=None,\n                 padding_idx=0,\n                 dropout=0.5,\n                 num_classes=3,\n                 device=""cpu""):\n        """"""\n        Args:\n            vocab_size: The size of the vocabulary of embeddings in the model.\n            embedding_dim: The dimension of the word embeddings.\n            hidden_size: The size of all the hidden layers in the network.\n            embeddings: A tensor of size (vocab_size, embedding_dim) containing\n                pretrained word embeddings. If None, word embeddings are\n                initialised randomly. Defaults to None.\n            padding_idx: The index of the padding token in the premises and\n                hypotheses passed as input to the model. Defaults to 0.\n            dropout: The dropout rate to use between the layers of the network.\n                A dropout rate of 0 corresponds to using no dropout at all.\n                Defaults to 0.5.\n            num_classes: The number of classes in the output of the network.\n                Defaults to 3.\n            device: The name of the device on which the model is being\n                executed. Defaults to \'cpu\'.\n        """"""\n        super(ESIM, self).__init__()\n\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.dropout = dropout\n        self.device = device\n\n        self._word_embedding = nn.Embedding(self.vocab_size,\n                                            self.embedding_dim,\n                                            padding_idx=padding_idx,\n                                            _weight=embeddings)\n\n        if self.dropout:\n            self._rnn_dropout = RNNDropout(p=self.dropout)\n            # self._rnn_dropout = nn.Dropout(p=self.dropout)\n\n        self._encoding = Seq2SeqEncoder(nn.LSTM,\n                                        self.embedding_dim,\n                                        self.hidden_size,\n                                        bidirectional=True)\n\n        self._attention = SoftmaxAttention()\n\n        self._projection = nn.Sequential(nn.Linear(4*2*self.hidden_size,\n                                                   self.hidden_size),\n                                         nn.ReLU())\n\n        self._composition = Seq2SeqEncoder(nn.LSTM,\n                                           self.hidden_size,\n                                           self.hidden_size,\n                                           bidirectional=True)\n\n        self._classification = nn.Sequential(nn.Dropout(p=self.dropout),\n                                             nn.Linear(2*4*self.hidden_size,\n                                                       self.hidden_size),\n                                             nn.Tanh(),\n                                             nn.Dropout(p=self.dropout),\n                                             nn.Linear(self.hidden_size,\n                                                       self.num_classes))\n\n        # Initialize all weights and biases in the model.\n        self.apply(_init_esim_weights)\n\n    def forward(self,\n                premises,\n                premises_lengths,\n                hypotheses,\n                hypotheses_lengths):\n        """"""\n        Args:\n            premises: A batch of varaible length sequences of word indices\n                representing premises. The batch is assumed to be of size\n                (batch, premises_length).\n            premises_lengths: A 1D tensor containing the lengths of the\n                premises in \'premises\'.\n            hypothesis: A batch of varaible length sequences of word indices\n                representing hypotheses. The batch is assumed to be of size\n                (batch, hypotheses_length).\n            hypotheses_lengths: A 1D tensor containing the lengths of the\n                hypotheses in \'hypotheses\'.\n\n        Returns:\n            logits: A tensor of size (batch, num_classes) containing the\n                logits for each output class of the model.\n            probabilities: A tensor of size (batch, num_classes) containing\n                the probabilities of each output class in the model.\n        """"""\n        premises_mask = get_mask(premises, premises_lengths).to(self.device)\n        hypotheses_mask = get_mask(hypotheses, hypotheses_lengths)\\\n            .to(self.device)\n\n        embedded_premises = self._word_embedding(premises)\n        embedded_hypotheses = self._word_embedding(hypotheses)\n\n        if self.dropout:\n            embedded_premises = self._rnn_dropout(embedded_premises)\n            embedded_hypotheses = self._rnn_dropout(embedded_hypotheses)\n\n        encoded_premises = self._encoding(embedded_premises,\n                                          premises_lengths)\n        encoded_hypotheses = self._encoding(embedded_hypotheses,\n                                            hypotheses_lengths)\n\n        attended_premises, attended_hypotheses =\\\n            self._attention(encoded_premises, premises_mask,\n                            encoded_hypotheses, hypotheses_mask)\n\n        enhanced_premises = torch.cat([encoded_premises,\n                                       attended_premises,\n                                       encoded_premises - attended_premises,\n                                       encoded_premises * attended_premises],\n                                      dim=-1)\n        enhanced_hypotheses = torch.cat([encoded_hypotheses,\n                                         attended_hypotheses,\n                                         encoded_hypotheses -\n                                         attended_hypotheses,\n                                         encoded_hypotheses *\n                                         attended_hypotheses],\n                                        dim=-1)\n\n        projected_premises = self._projection(enhanced_premises)\n        projected_hypotheses = self._projection(enhanced_hypotheses)\n\n        if self.dropout:\n            projected_premises = self._rnn_dropout(projected_premises)\n            projected_hypotheses = self._rnn_dropout(projected_hypotheses)\n\n        v_ai = self._composition(projected_premises, premises_lengths)\n        v_bj = self._composition(projected_hypotheses, hypotheses_lengths)\n\n        v_a_avg = torch.sum(v_ai * premises_mask.unsqueeze(1)\n                                                .transpose(2, 1), dim=1)\\\n            / torch.sum(premises_mask, dim=1, keepdim=True)\n        v_b_avg = torch.sum(v_bj * hypotheses_mask.unsqueeze(1)\n                                                  .transpose(2, 1), dim=1)\\\n            / torch.sum(hypotheses_mask, dim=1, keepdim=True)\n\n        v_a_max, _ = replace_masked(v_ai, premises_mask, -1e7).max(dim=1)\n        v_b_max, _ = replace_masked(v_bj, hypotheses_mask, -1e7).max(dim=1)\n\n        v = torch.cat([v_a_avg, v_a_max, v_b_avg, v_b_max], dim=1)\n\n        logits = self._classification(v)\n        probabilities = nn.functional.softmax(logits, dim=-1)\n\n        return logits, probabilities\n\n\ndef _init_esim_weights(module):\n    """"""\n    Initialise the weights of the ESIM model.\n    """"""\n    if isinstance(module, nn.Linear):\n        nn.init.xavier_uniform_(module.weight.data)\n        nn.init.constant_(module.bias.data, 0.0)\n\n    elif isinstance(module, nn.LSTM):\n        nn.init.xavier_uniform_(module.weight_ih_l0.data)\n        nn.init.orthogonal_(module.weight_hh_l0.data)\n        nn.init.constant_(module.bias_ih_l0.data, 0.0)\n        nn.init.constant_(module.bias_hh_l0.data, 0.0)\n        hidden_size = module.bias_hh_l0.data.shape[0] // 4\n        module.bias_hh_l0.data[hidden_size:(2*hidden_size)] = 1.0\n\n        if (module.bidirectional):\n            nn.init.xavier_uniform_(module.weight_ih_l0_reverse.data)\n            nn.init.orthogonal_(module.weight_hh_l0_reverse.data)\n            nn.init.constant_(module.bias_ih_l0_reverse.data, 0.0)\n            nn.init.constant_(module.bias_hh_l0_reverse.data, 0.0)\n            module.bias_hh_l0_reverse.data[hidden_size:(2*hidden_size)] = 1.0\n'"
esim/utils.py,4,"b'""""""\nUtility functions for the ESIM model.\n""""""\n# Aurelien Coet, 2018.\n\nimport torch\nimport torch.nn as nn\n\n\n# Code widely inspired from:\n# https://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py.\ndef sort_by_seq_lens(batch, sequences_lengths, descending=True):\n    """"""\n    Sort a batch of padded variable length sequences by their length.\n\n    Args:\n        batch: A batch of padded variable length sequences. The batch should\n            have the dimensions (batch_size x max_sequence_length x *).\n        sequences_lengths: A tensor containing the lengths of the sequences in the\n            input batch. The tensor should be of size (batch_size).\n        descending: A boolean value indicating whether to sort the sequences\n            by their lengths in descending order. Defaults to True.\n\n    Returns:\n        sorted_batch: A tensor containing the input batch reordered by\n            sequences lengths.\n        sorted_seq_lens: A tensor containing the sorted lengths of the\n            sequences in the input batch.\n        sorting_idx: A tensor containing the indices used to permute the input\n            batch in order to get \'sorted_batch\'.\n        restoration_idx: A tensor containing the indices that can be used to\n            restore the order of the sequences in \'sorted_batch\' so that it\n            matches the input batch.\n    """"""\n    sorted_seq_lens, sorting_index =\\\n        sequences_lengths.sort(0, descending=descending)\n\n    sorted_batch = batch.index_select(0, sorting_index)\n\n    idx_range =\\\n        sequences_lengths.new_tensor(torch.arange(0, len(sequences_lengths)))\n    _, reverse_mapping = sorting_index.sort(0, descending=False)\n    restoration_index = idx_range.index_select(0, reverse_mapping)\n\n    return sorted_batch, sorted_seq_lens, sorting_index, restoration_index\n\n\ndef get_mask(sequences_batch, sequences_lengths):\n    """"""\n    Get the mask for a batch of padded variable length sequences.\n\n    Args:\n        sequences_batch: A batch of padded variable length sequences\n            containing word indices. Must be a 2-dimensional tensor of size\n            (batch, sequence).\n        sequences_lengths: A tensor containing the lengths of the sequences in\n            \'sequences_batch\'. Must be of size (batch).\n\n    Returns:\n        A mask of size (batch, max_sequence_length), where max_sequence_length\n        is the length of the longest sequence in the batch.\n    """"""\n    batch_size = sequences_batch.size()[0]\n    max_length = torch.max(sequences_lengths)\n    mask = torch.ones(batch_size, max_length, dtype=torch.float)\n    mask[sequences_batch[:, :max_length] == 0] = 0.0\n    return mask\n\n\n# Code widely inspired from:\n# https://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py.\ndef masked_softmax(tensor, mask):\n    """"""\n    Apply a masked softmax on the last dimension of a tensor.\n    The input tensor and mask should be of size (batch, *, sequence_length).\n\n    Args:\n        tensor: The tensor on which the softmax function must be applied along\n            the last dimension.\n        mask: A mask of the same size as the tensor with 0s in the positions of\n            the values that must be masked and 1s everywhere else.\n\n    Returns:\n        A tensor of the same size as the inputs containing the result of the\n        softmax.\n    """"""\n    tensor_shape = tensor.size()\n    reshaped_tensor = tensor.view(-1, tensor_shape[-1])\n\n    # Reshape the mask so it matches the size of the input tensor.\n    while mask.dim() < tensor.dim():\n        mask = mask.unsqueeze(1)\n    mask = mask.expand_as(tensor).contiguous().float()\n    reshaped_mask = mask.view(-1, mask.size()[-1])\n\n    result = nn.functional.softmax(reshaped_tensor * reshaped_mask, dim=-1)\n    result = result * reshaped_mask\n    # 1e-13 is added to avoid divisions by zero.\n    result = result / (result.sum(dim=-1, keepdim=True) + 1e-13)\n\n    return result.view(*tensor_shape)\n\n\n# Code widely inspired from:\n# https://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py.\ndef weighted_sum(tensor, weights, mask):\n    """"""\n    Apply a weighted sum on the vectors along the last dimension of \'tensor\',\n    and mask the vectors in the result with \'mask\'.\n\n    Args:\n        tensor: A tensor of vectors on which a weighted sum must be applied.\n        weights: The weights to use in the weighted sum.\n        mask: A mask to apply on the result of the weighted sum.\n\n    Returns:\n        A new tensor containing the result of the weighted sum after the mask\n        has been applied on it.\n    """"""\n    weighted_sum = weights.bmm(tensor)\n\n    while mask.dim() < weighted_sum.dim():\n        mask = mask.unsqueeze(1)\n    mask = mask.transpose(-1, -2)\n    mask = mask.expand_as(weighted_sum).contiguous().float()\n\n    return weighted_sum * mask\n\n\n# Code inspired from:\n# https://github.com/allenai/allennlp/blob/master/allennlp/nn/util.py.\ndef replace_masked(tensor, mask, value):\n    """"""\n    Replace the all the values of vectors in \'tensor\' that are masked in\n    \'masked\' by \'value\'.\n\n    Args:\n        tensor: The tensor in which the masked vectors must have their values\n            replaced.\n        mask: A mask indicating the vectors which must have their values\n            replaced.\n        value: The value to place in the masked vectors of \'tensor\'.\n\n    Returns:\n        A new tensor of the same size as \'tensor\' where the values of the\n        vectors masked in \'mask\' were replaced by \'value\'.\n    """"""\n    mask = mask.unsqueeze(1).transpose(2, 1)\n    reverse_mask = 1.0 - mask\n    values_to_add = value * reverse_mask\n    return tensor * mask + values_to_add\n\n\ndef correct_predictions(output_probabilities, targets):\n    """"""\n    Compute the number of predictions that match some target classes in the\n    output of a model.\n\n    Args:\n        output_probabilities: A tensor of probabilities for different output\n            classes.\n        targets: The indices of the actual target classes.\n\n    Returns:\n        The number of correct predictions in \'output_probabilities\'.\n    """"""\n    _, out_classes = output_probabilities.max(dim=1)\n    correct = (out_classes == targets).sum()\n    return correct.item()\n'"
scripts/fetch_data.py,0,"b'""""""\nFetch datasets and pretrained word embeddings for the ESIM model.\n\nBy default, the script downloads the following.\n    - The SNLI corpus;\n    - GloVe word embeddings (840B - 300d).\n""""""\n# Aurelien Coet, 2018.\n\nimport os\nimport argparse\nimport zipfile\nimport wget\n\n\ndef download(url, targetdir):\n    """"""\n    Download a file and save it in some target directory.\n\n    Args:\n        url: The url from which the file must be downloaded.\n        targetdir: The path to the directory where the file must be saved.\n\n    Returns:\n        The path to the downloaded file.\n    """"""\n    print(""* Downloading data from {}..."".format(url))\n    filepath = os.path.join(targetdir, url.split(\'/\')[-1])\n    wget.download(url, filepath)\n    return filepath\n\n\ndef unzip(filepath):\n    """"""\n    Extract the data from a zipped file and delete the archive.\n\n    Args:\n        filepath: The path to the zipped file.\n    """"""\n    print(""\\n* Extracting: {}..."".format(filepath))\n    dirpath = os.path.dirname(filepath)\n    with zipfile.ZipFile(filepath) as zf:\n        for name in zf.namelist():\n            # Ignore useless files in archives.\n            if ""__MACOSX"" in name or\\\n               "".DS_Store"" in name or\\\n               ""Icon"" in name:\n                continue\n            zf.extract(name, dirpath)\n    # Delete the archive once the data has been extracted.\n    os.remove(filepath)\n\n\ndef download_unzip(url, targetdir):\n    """"""\n    Download and unzip data from some url and save it in a target directory.\n\n    Args:\n        url: The url to download the data from.\n        targetdir: The target directory in which to download and unzip the\n                   data.\n    """"""\n    filepath = os.path.join(targetdir, url.split(\'/\')[-1])\n    target = os.path.join(targetdir,\n                          ""."".join((url.split(\'/\')[-1]).split(\'.\')[:-1]))\n\n    if not os.path.exists(targetdir):\n        print(""* Creating target directory {}..."".format(targetdir))\n        os.makedirs(targetdir)\n\n    # Skip download and unzipping if the unzipped data is already available.\n    if os.path.exists(target) or os.path.exists(target + "".txt""):\n        print(""* Found unzipped data in {}, skipping download and unzip...""\n              .format(targetdir))\n    # Skip downloading if the zipped data is already available.\n    elif os.path.exists(filepath):\n        print(""* Found zipped data in {} - skipping download...""\n              .format(targetdir))\n        unzip(filepath)\n    # Download and unzip otherwise.\n    else:\n        unzip(download(url, targetdir))\n\n\nif __name__ == ""__main__"":\n    # Default data.\n    snli_url = ""https://nlp.stanford.edu/projects/snli/snli_1.0.zip""\n    glove_url = ""http://www-nlp.stanford.edu/data/glove.840B.300d.zip""\n\n    parser = argparse.ArgumentParser(description=\'Download the SNLI dataset\')\n    parser.add_argument(""--dataset_url"",\n                        default=snli_url,\n                        help=""URL of the dataset to download"")\n    parser.add_argument(""--embeddings_url"",\n                        default=glove_url,\n                        help=""URL of the pretrained embeddings to download"")\n    parser.add_argument(""--target_dir"",\n                        default=os.path.join("".."", ""data""),\n                        help=""Path to a directory where data must be saved"")\n    args = parser.parse_args()\n\n    if not os.path.exists(args.target_dir):\n        os.makedirs(args.target_dir)\n\n    print(20*""="", ""Fetching the dataset:"", 20*\'=\')\n    download_unzip(args.dataset_url, os.path.join(args.target_dir, ""dataset""))\n\n    print(20*""="", ""Fetching the word embeddings:"", 20*""="")\n    download_unzip(args.embeddings_url,\n                   os.path.join(args.target_dir, ""embeddings""))\n'"
scripts/preprocessing/preprocess_bnli.py,0,"b'""""""\nPreprocess the Breaking NLI data set.\n""""""\n\nimport os\nimport json\nimport pickle\nimport argparse\n\nfrom nltk import word_tokenize\nfrom esim.data import Preprocessor\n\n\ndef jsonl_to_txt(input_file, output_file):\n    """"""\n    Transform the Breaking NLI data from a jsonl file to .txt for\n    further processing.\n\n    Args:\n        input_file: The path to the Breaking NLI data set in jsonl format.\n        output_file: The path to the .txt file where the tranformed data must\n            be saved.\n    """"""\n    with open(input_file, \'r\') as input_f, open(output_file, \'w\') as output_f:\n        output_f.write(""label\\tsentence1\\tsentence2\\t\\t\\t\\t\\t\\tpairID\\n"")\n\n        for line in input_f:\n            data = json.loads(line)\n\n            # Sentences in the Breaking NLI data set aren\'t distributed in the\n            # form of binary parses, so we must tokenise them with nltk.\n            sentence1 = word_tokenize(data[\'sentence1\'])\n            sentence1 = "" "".join(sentence1)\n            sentence2 = word_tokenize(data[\'sentence2\'])\n            sentence2 = "" "".join(sentence2)\n\n            # The 5 tabs between sentence 2 and the pairID are added to\n            # follow the same structure as the txt files in SNLI and MNLI.\n            output_f.write(data[\'gold_label\'] + ""\\t"" + sentence1 + ""\\t"" +\n                           sentence2 + ""\\t\\t\\t\\t\\t"" + str(data[\'pairID\']) +\n                           ""\\n"")\n\n\ndef preprocess_BNLI_data(input_file,\n                         targetdir,\n                         worddict,\n                         labeldict):\n    """"""\n    Preprocess the BNLI data set so it can be used to test a model trained\n    on SNLI.\n\n    Args:\n        inputdir: The path to the file containing the Breaking NLI (BNLI) data.\n        target_dir: The path to the directory where the preprocessed Breaking\n            NLI data must be saved.\n        worddict: The path to the pickled worddict used for preprocessing the\n            training data on which models were trained before being tested on\n            BNLI.\n        labeldict: The dict of labels used for the training data on which\n            models were trained before being tested on BNLI.\n    """"""\n    if not os.path.exists(targetdir):\n        os.makedirs(targetdir)\n\n    output_file = os.path.join(targetdir, ""bnli.txt"")\n\n    print(20*""="", "" Preprocessing Breaking NLI data set "", 20*""="")\n    print(""\\t* Tranforming jsonl data to txt..."")\n    jsonl_to_txt(input_file, output_file)\n\n    preprocessor = Preprocessor(labeldict=labeldict)\n\n    with open(worddict, \'rb\') as pkl:\n        wdict = pickle.load(pkl)\n    preprocessor.worddict = wdict\n\n    print(""\\t* Reading txt data..."")\n    data = preprocessor.read_data(output_file)\n\n    print(""\\t* Transforming words in premises and hypotheses to indices..."")\n    transformed_data = preprocessor.transform_to_indices(data)\n\n    print(""\\t* Saving result..."")\n    with open(os.path.join(targetdir, ""bnli_data.pkl""), \'wb\') as pkl_file:\n        pickle.dump(transformed_data, pkl_file)\n\n\nif __name__ == ""__main__"":\n    default_config = ""../../config/preprocessing/bnli_preprocessing.json""\n\n    parser = argparse.ArgumentParser(description=""Preprocess the Breaking\\\n NLI (BNLI) dataset"")\n    parser.add_argument(""--config"",\n                        default=default_config,\n                        help=""Path to a configuration file for preprocessing BNLI"")\n    args = parser.parse_args()\n\n    script_dir = os.path.dirname(os.path.realpath(__file__))\n\n    if args.config == default_config:\n        config_path = os.path.join(script_dir, args.config)\n    else:\n        config_path = args.config\n\n    with open(os.path.normpath(config_path), ""r"") as cfg_file:\n        config = json.load(cfg_file)\n\n    preprocess_BNLI_data(\n        os.path.normpath(os.path.join(script_dir, config[""data_file""])),\n        os.path.normpath(os.path.join(script_dir, config[""target_dir""])),\n        os.path.normpath(os.path.join(script_dir, config[""worddict""])),\n        config[""labeldict""]\n    )\n'"
scripts/preprocessing/preprocess_mnli.py,0,"b'""""""\nPreprocess the MultiNLI dataset and word embeddings to be used by the\nESIM model.\n""""""\n# Aurelien Coet, 2019.\n\nimport os\nimport pickle\nimport argparse\nimport fnmatch\nimport json\n\nfrom esim.data import Preprocessor\n\n\ndef preprocess_MNLI_data(inputdir,\n                         embeddings_file,\n                         targetdir,\n                         lowercase=False,\n                         ignore_punctuation=False,\n                         num_words=None,\n                         stopwords=[],\n                         labeldict={},\n                         bos=None,\n                         eos=None):\n    """"""\n    Preprocess the data from the MultiNLI corpus so it can be used by the\n    ESIM model.\n    Compute a worddict from the train set, and transform the words in\n    the sentences of the corpus to their indices, as well as the labels.\n    Build an embedding matrix from pretrained word vectors.\n    The preprocessed data is saved in pickled form in some target directory.\n\n    Args:\n        inputdir: The path to the directory containing the NLI corpus.\n        embeddings_file: The path to the file containing the pretrained\n            word vectors that must be used to build the embedding matrix.\n        targetdir: The path to the directory where the preprocessed data\n            must be saved.\n        lowercase: Boolean value indicating whether to lowercase the premises\n            and hypotheseses in the input data. Defautls to False.\n        ignore_punctuation: Boolean value indicating whether to remove\n            punctuation from the input data. Defaults to False.\n        num_words: Integer value indicating the size of the vocabulary to use\n            for the word embeddings. If set to None, all words are kept.\n            Defaults to None.\n        stopwords: A list of words that must be ignored when preprocessing\n            the data. Defaults to an empty list.\n        bos: A string indicating the symbol to use for beginning of sentence\n            tokens. If set to None, bos tokens aren\'t used. Defaults to None.\n        eos: A string indicating the symbol to use for end of sentence tokens.\n            If set to None, eos tokens aren\'t used. Defaults to None.\n    """"""\n    if not os.path.exists(targetdir):\n        os.makedirs(targetdir)\n\n    # Retrieve the train, dev and test data files from the dataset directory.\n    train_file = """"\n    matched_dev_file = """"\n    mismatched_dev_file = """"\n    matched_test_file = """"\n    mismatched_test_file = """"\n    for file in os.listdir(inputdir):\n        if fnmatch.fnmatch(file, ""*_train.txt""):\n            train_file = file\n        elif fnmatch.fnmatch(file, ""*_dev_matched.txt""):\n            matched_dev_file = file\n        elif fnmatch.fnmatch(file, ""*_dev_mismatched.txt""):\n            mismatched_dev_file = file\n        elif fnmatch.fnmatch(file, ""*_test_matched_unlabeled.txt""):\n            matched_test_file = file\n        elif fnmatch.fnmatch(file, ""*_test_mismatched_unlabeled.txt""):\n            mismatched_test_file = file\n\n    # -------------------- Train data preprocessing -------------------- #\n    preprocessor = Preprocessor(lowercase=lowercase,\n                                ignore_punctuation=ignore_punctuation,\n                                num_words=num_words,\n                                stopwords=stopwords,\n                                labeldict=labeldict,\n                                bos=bos,\n                                eos=eos)\n\n    print(20*""="", "" Preprocessing train set "", 20*""="")\n    print(""\\t* Reading data..."")\n    data = preprocessor.read_data(os.path.join(inputdir, train_file))\n\n    print(""\\t* Computing worddict and saving it..."")\n    preprocessor.build_worddict(data)\n    with open(os.path.join(targetdir, ""worddict.pkl""), ""wb"") as pkl_file:\n        pickle.dump(preprocessor.worddict, pkl_file)\n\n    print(""\\t* Transforming words in premises and hypotheses to indices..."")\n    transformed_data = preprocessor.transform_to_indices(data)\n    print(""\\t* Saving result..."")\n    with open(os.path.join(targetdir, ""train_data.pkl""), ""wb"") as pkl_file:\n        pickle.dump(transformed_data, pkl_file)\n\n    # -------------------- Validation data preprocessing -------------------- #\n    print(20*""="", "" Preprocessing dev sets "", 20*""="")\n    print(""\\t* Reading matched dev data..."")\n    data = preprocessor.read_data(os.path.join(inputdir, matched_dev_file))\n\n    print(""\\t* Transforming words in premises and hypotheses to indices..."")\n    transformed_data = preprocessor.transform_to_indices(data)\n    print(""\\t* Saving result..."")\n    with open(os.path.join(targetdir, ""matched_dev_data.pkl""), ""wb"") as pkl_file:\n        pickle.dump(transformed_data, pkl_file)\n\n    print(""\\t* Reading mismatched dev data..."")\n    data = preprocessor.read_data(os.path.join(inputdir, mismatched_dev_file))\n\n    print(""\\t* Transforming words in premises and hypotheses to indices..."")\n    transformed_data = preprocessor.transform_to_indices(data)\n    print(""\\t* Saving result..."")\n    with open(os.path.join(targetdir, ""mismatched_dev_data.pkl""), ""wb"") as pkl_file:\n        pickle.dump(transformed_data, pkl_file)\n\n    # -------------------- Test data preprocessing -------------------- #\n    print(20*""="", "" Preprocessing test sets "", 20*""="")\n    print(""\\t* Reading matched test data..."")\n    data = preprocessor.read_data(os.path.join(inputdir, matched_test_file))\n\n    print(""\\t* Transforming words in premises and hypotheses to indices..."")\n    transformed_data = preprocessor.transform_to_indices(data)\n    print(""\\t* Saving result..."")\n    with open(os.path.join(targetdir, ""matched_test_data.pkl""), ""wb"") as pkl_file:\n        pickle.dump(transformed_data, pkl_file)\n\n    print(""\\t* Reading mismatched test data..."")\n    data = preprocessor.read_data(os.path.join(inputdir, mismatched_test_file))\n\n    print(""\\t* Transforming words in premises and hypotheses to indices..."")\n    transformed_data = preprocessor.transform_to_indices(data)\n    print(""\\t* Saving result..."")\n    with open(os.path.join(targetdir, ""mismatched_test_data.pkl""), ""wb"") as pkl_file:\n        pickle.dump(transformed_data, pkl_file)\n\n    # -------------------- Embeddings preprocessing -------------------- #\n    print(20*""="", "" Preprocessing embeddings "", 20*""="")\n    print(""\\t* Building embedding matrix and saving it..."")\n    embed_matrix = preprocessor.build_embedding_matrix(embeddings_file)\n    with open(os.path.join(targetdir, ""embeddings.pkl""), ""wb"") as pkl_file:\n        pickle.dump(embed_matrix, pkl_file)\n\n\nif __name__ == ""__main__"":\n    default_config = ""../../config/preprocessing/mnli_preprocessing.json""\n\n    parser = argparse.ArgumentParser(description=""Preprocess the MultiNLI dataset"")\n    parser.add_argument(""--config"",\n                        default=default_config,\n                        help=""Path to a configuration file for preprocessing MultiNLI"")\n    args = parser.parse_args()\n\n    script_dir = os.path.dirname(os.path.realpath(__file__))\n\n    if args.config == default_config:\n        config_path = os.path.join(script_dir, args.config)\n    else:\n        config_path = args.config\n\n    with open(os.path.normpath(config_path), \'r\') as cfg_file:\n        config = json.load(cfg_file)\n\n    preprocess_MNLI_data(\n        os.path.normpath(os.path.join(script_dir, config[""data_dir""])),\n        os.path.normpath(os.path.join(script_dir, config[""embeddings_file""])),\n        os.path.normpath(os.path.join(script_dir, config[""target_dir""])),\n        lowercase=config[""lowercase""],\n        ignore_punctuation=config[""ignore_punctuation""],\n        num_words=config[""num_words""],\n        stopwords=config[""stopwords""],\n        labeldict=config[""labeldict""],\n        bos=config[""bos""],\n        eos=config[""eos""]\n    )\n'"
scripts/preprocessing/preprocess_snli.py,0,"b'""""""\nPreprocess the SNLI dataset and word embeddings to be used by the ESIM model.\n""""""\n# Aurelien Coet, 2018.\n\nimport os\nimport pickle\nimport argparse\nimport fnmatch\nimport json\n\nfrom esim.data import Preprocessor\n\n\ndef preprocess_SNLI_data(inputdir,\n                         embeddings_file,\n                         targetdir,\n                         lowercase=False,\n                         ignore_punctuation=False,\n                         num_words=None,\n                         stopwords=[],\n                         labeldict={},\n                         bos=None,\n                         eos=None):\n    """"""\n    Preprocess the data from the SNLI corpus so it can be used by the\n    ESIM model.\n    Compute a worddict from the train set, and transform the words in\n    the sentences of the corpus to their indices, as well as the labels.\n    Build an embedding matrix from pretrained word vectors.\n    The preprocessed data is saved in pickled form in some target directory.\n\n    Args:\n        inputdir: The path to the directory containing the NLI corpus.\n        embeddings_file: The path to the file containing the pretrained\n            word vectors that must be used to build the embedding matrix.\n        targetdir: The path to the directory where the preprocessed data\n            must be saved.\n        lowercase: Boolean value indicating whether to lowercase the premises\n            and hypotheseses in the input data. Defautls to False.\n        ignore_punctuation: Boolean value indicating whether to remove\n            punctuation from the input data. Defaults to False.\n        num_words: Integer value indicating the size of the vocabulary to use\n            for the word embeddings. If set to None, all words are kept.\n            Defaults to None.\n        stopwords: A list of words that must be ignored when preprocessing\n            the data. Defaults to an empty list.\n        bos: A string indicating the symbol to use for beginning of sentence\n            tokens. If set to None, bos tokens aren\'t used. Defaults to None.\n        eos: A string indicating the symbol to use for end of sentence tokens.\n            If set to None, eos tokens aren\'t used. Defaults to None.\n    """"""\n    if not os.path.exists(targetdir):\n        os.makedirs(targetdir)\n\n    # Retrieve the train, dev and test data files from the dataset directory.\n    train_file = """"\n    dev_file = """"\n    test_file = """"\n    for file in os.listdir(inputdir):\n        if fnmatch.fnmatch(file, ""*_train.txt""):\n            train_file = file\n        elif fnmatch.fnmatch(file, ""*_dev.txt""):\n            dev_file = file\n        elif fnmatch.fnmatch(file, ""*_test.txt""):\n            test_file = file\n\n    # -------------------- Train data preprocessing -------------------- #\n    preprocessor = Preprocessor(lowercase=lowercase,\n                                ignore_punctuation=ignore_punctuation,\n                                num_words=num_words,\n                                stopwords=stopwords,\n                                labeldict=labeldict,\n                                bos=bos,\n                                eos=eos)\n\n    print(20*""="", "" Preprocessing train set "", 20*""="")\n    print(""\\t* Reading data..."")\n    data = preprocessor.read_data(os.path.join(inputdir, train_file))\n\n    print(""\\t* Computing worddict and saving it..."")\n    preprocessor.build_worddict(data)\n    with open(os.path.join(targetdir, ""worddict.pkl""), ""wb"") as pkl_file:\n        pickle.dump(preprocessor.worddict, pkl_file)\n\n    print(""\\t* Transforming words in premises and hypotheses to indices..."")\n    transformed_data = preprocessor.transform_to_indices(data)\n    print(""\\t* Saving result..."")\n    with open(os.path.join(targetdir, ""train_data.pkl""), ""wb"") as pkl_file:\n        pickle.dump(transformed_data, pkl_file)\n\n    # -------------------- Validation data preprocessing -------------------- #\n    print(20*""="", "" Preprocessing dev set "", 20*""="")\n    print(""\\t* Reading data..."")\n    data = preprocessor.read_data(os.path.join(inputdir, dev_file))\n\n    print(""\\t* Transforming words in premises and hypotheses to indices..."")\n    transformed_data = preprocessor.transform_to_indices(data)\n    print(""\\t* Saving result..."")\n    with open(os.path.join(targetdir, ""dev_data.pkl""), ""wb"") as pkl_file:\n        pickle.dump(transformed_data, pkl_file)\n\n    # -------------------- Test data preprocessing -------------------- #\n    print(20*""="", "" Preprocessing test set "", 20*""="")\n    print(""\\t* Reading data..."")\n    data = preprocessor.read_data(os.path.join(inputdir, test_file))\n\n    print(""\\t* Transforming words in premises and hypotheses to indices..."")\n    transformed_data = preprocessor.transform_to_indices(data)\n    print(""\\t* Saving result..."")\n    with open(os.path.join(targetdir, ""test_data.pkl""), ""wb"") as pkl_file:\n        pickle.dump(transformed_data, pkl_file)\n\n    # -------------------- Embeddings preprocessing -------------------- #\n    print(20*""="", "" Preprocessing embeddings "", 20*""="")\n    print(""\\t* Building embedding matrix and saving it..."")\n    embed_matrix = preprocessor.build_embedding_matrix(embeddings_file)\n    with open(os.path.join(targetdir, ""embeddings.pkl""), ""wb"") as pkl_file:\n        pickle.dump(embed_matrix, pkl_file)\n\n\nif __name__ == ""__main__"":\n    default_config = ""../../config/preprocessing/snli_preprocessing.json""\n\n    parser = argparse.ArgumentParser(description=""Preprocess the SNLI dataset"")\n    parser.add_argument(\n        ""--config"",\n        default=default_config,\n        help=""Path to a configuration file for preprocessing SNLI""\n    )\n    args = parser.parse_args()\n\n    script_dir = os.path.dirname(os.path.realpath(__file__))\n\n    if args.config == default_config:\n        config_path = os.path.join(script_dir, args.config)\n    else:\n        config_path = args.config\n\n    with open(os.path.normpath(config_path), ""r"") as cfg_file:\n        config = json.load(cfg_file)\n\n    preprocess_SNLI_data(\n        os.path.normpath(os.path.join(script_dir, config[""data_dir""])),\n        os.path.normpath(os.path.join(script_dir, config[""embeddings_file""])),\n        os.path.normpath(os.path.join(script_dir, config[""target_dir""])),\n        lowercase=config[""lowercase""],\n        ignore_punctuation=config[""ignore_punctuation""],\n        num_words=config[""num_words""],\n        stopwords=config[""stopwords""],\n        labeldict=config[""labeldict""],\n        bos=config[""bos""],\n        eos=config[""eos""]\n    )\n'"
scripts/testing/test_mnli.py,4,"b'""""""\nTest the ESIM model on the preprocessed MultiNLI dataset.\n""""""\n# Aurelien Coet, 2019.\n\nimport os\nimport pickle\nimport argparse\nimport torch\nimport json\n\nfrom torch.utils.data import DataLoader\nfrom esim.data import NLIDataset\nfrom esim.model import ESIM\n\n\ndef predict(model, dataloader, labeldict):\n    """"""\n    Predict the labels of an unlabelled test set with a pretrained model.\n\n    Args:\n        model: The torch module which must be used to make predictions.\n        dataloader: A DataLoader object to iterate over some dataset.\n        labeldict: A dictionary associating labels to integer values.\n\n    Returns:\n        A dictionary associating pair ids to predicted labels.\n    """"""\n    # Switch the model to eval mode.\n    model.eval()\n    device = model.device\n\n    # Revert the labeldict to associate integers to labels.\n    labels = {index: label for label, index in labeldict.items()}\n    predictions = {}\n\n    # Deactivate autograd for evaluation.\n    with torch.no_grad():\n        for batch in dataloader:\n\n            # Move input and output data to the GPU if one is used.\n            ids = batch[""id""]\n            premises = batch[\'premise\'].to(device)\n            premises_lengths = batch[\'premise_length\'].to(device)\n            hypotheses = batch[\'hypothesis\'].to(device)\n            hypotheses_lengths = batch[\'hypothesis_length\'].to(device)\n\n            _, probs = model(premises,\n                             premises_lengths,\n                             hypotheses,\n                             hypotheses_lengths)\n\n            _, preds = probs.max(dim=1)\n\n            for i, pair_id in enumerate(ids):\n                predictions[pair_id] = labels[int(preds[i])]\n\n    return predictions\n\n\ndef main(test_files, pretrained_file, labeldict, output_dir, batch_size=32):\n    """"""\n    Test the ESIM model with pretrained weights on the MultiNLI dataset.\n\n    Args:\n        test_files: The paths to the preprocessed matched and mismatched MNLI\n            test sets.\n        pretrained_file: The path to a checkpoint produced by the\n            \'train_mnli\' script.\n        labeldict: A dictionary associating labels (classes) to integer values.\n        output_dir: The path to a directory where the predictions of the model\n            must be saved.\n        batch_size: The size of the batches used for testing. Defaults to 32.\n    """"""\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n    print(20 * ""="", "" Preparing for testing "", 20 * ""="")\n\n    output_dir = os.path.normpath(output_dir)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    checkpoint = torch.load(pretrained_file)\n\n    # Retrieve model parameters from the checkpoint.\n    vocab_size = checkpoint[\'model\'][\'_word_embedding.weight\'].size(0)\n    embedding_dim = checkpoint[\'model\'][\'_word_embedding.weight\'].size(1)\n    hidden_size = checkpoint[\'model\'][\'_projection.0.weight\'].size(0)\n    num_classes = checkpoint[\'model\'][\'_classification.4.weight\'].size(0)\n\n    print(""\\t* Loading test data..."")\n    with open(os.path.normpath(test_files[""matched""]), \'rb\') as pkl:\n        matched_test_data = NLIDataset(pickle.load(pkl))\n    with open(os.path.normpath(test_files[""mismatched""]), \'rb\') as pkl:\n        mismatched_test_data = NLIDataset(pickle.load(pkl))\n\n    matched_test_loader = DataLoader(matched_test_data,\n                                     shuffle=False,\n                                     batch_size=batch_size)\n    mismatched_test_loader = DataLoader(mismatched_test_data,\n                                        shuffle=False,\n                                        batch_size=batch_size)\n\n    print(""\\t* Building model..."")\n    model = ESIM(vocab_size,\n                 embedding_dim,\n                 hidden_size,\n                 num_classes=num_classes,\n                 device=device).to(device)\n\n    model.load_state_dict(checkpoint[\'model\'])\n\n    print(20 * ""="",\n          "" Prediction on MNLI with ESIM model on device: {} "".format(device),\n          20 * ""="")\n\n    print(""\\t* Prediction for matched test set..."")\n    predictions = predict(model, matched_test_loader, labeldict)\n\n    with open(os.path.join(output_dir, ""matched_predictions.csv""), \'w\') as output_f:\n        output_f.write(""pairID,gold_label\\n"")\n        for pair_id in predictions:\n            output_f.write(pair_id+"",""+predictions[pair_id]+""\\n"")\n\n    print(""\\t* Prediction for mismatched test set..."")\n    predictions = predict(model, mismatched_test_loader, labeldict)\n\n    with open(os.path.join(output_dir, ""mismatched_predictions.csv""), \'w\') as output_f:\n        output_f.write(""pairID,gold_label\\n"")\n        for pair_id in predictions:\n            output_f.write(pair_id+"",""+predictions[pair_id]+""\\n"")\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Test the ESIM model on\\\n the MNLI matched and mismatched test sets\')\n    parser.add_argument(\'checkpoint\',\n                        help=""Path to a checkpoint with a pretrained model"")\n    parser.add_argument(\'--config\', default=\'../config/testing/mnli_testing.json\',\n                        help=\'Path to a configuration file\')\n    args = parser.parse_args()\n\n    with open(os.path.normpath(args.config), \'r\') as config_file:\n        config = json.load(config_file)\n\n    main(config[\'test_files\'],\n         args.checkpoint,\n         config[\'labeldict\'],\n         config[\'output_dir\'],\n         config[\'batch_size\'])\n'"
scripts/testing/test_snli.py,4,"b'""""""\nTest the ESIM model on some preprocessed dataset.\n""""""\n# Aurelien Coet, 2018.\n\nimport time\nimport pickle\nimport argparse\nimport torch\n\nfrom torch.utils.data import DataLoader\nfrom esim.data import NLIDataset\nfrom esim.model import ESIM\nfrom esim.utils import correct_predictions\n\n\ndef test(model, dataloader):\n    """"""\n    Test the accuracy of a model on some labelled test dataset.\n\n    Args:\n        model: The torch module on which testing must be performed.\n        dataloader: A DataLoader object to iterate over some dataset.\n\n    Returns:\n        batch_time: The average time to predict the classes of a batch.\n        total_time: The total time to process the whole dataset.\n        accuracy: The accuracy of the model on the input data.\n    """"""\n    # Switch the model to eval mode.\n    model.eval()\n    device = model.device\n\n    time_start = time.time()\n    batch_time = 0.0\n    accuracy = 0.0\n\n    # Deactivate autograd for evaluation.\n    with torch.no_grad():\n        for batch in dataloader:\n            batch_start = time.time()\n\n            # Move input and output data to the GPU if one is used.\n            premises = batch[""premise""].to(device)\n            premises_lengths = batch[""premise_length""].to(device)\n            hypotheses = batch[""hypothesis""].to(device)\n            hypotheses_lengths = batch[""hypothesis_length""].to(device)\n            labels = batch[""label""].to(device)\n\n            _, probs = model(premises,\n                             premises_lengths,\n                             hypotheses,\n                             hypotheses_lengths)\n\n            accuracy += correct_predictions(probs, labels)\n            batch_time += time.time() - batch_start\n\n    batch_time /= len(dataloader)\n    total_time = time.time() - time_start\n    accuracy /= (len(dataloader.dataset))\n\n    return batch_time, total_time, accuracy\n\n\ndef main(test_file, pretrained_file, batch_size=32):\n    """"""\n    Test the ESIM model with pretrained weights on some dataset.\n\n    Args:\n        test_file: The path to a file containing preprocessed NLI data.\n        pretrained_file: The path to a checkpoint produced by the\n            \'train_model\' script.\n        vocab_size: The number of words in the vocabulary of the model\n            being tested.\n        embedding_dim: The size of the embeddings in the model.\n        hidden_size: The size of the hidden layers in the model. Must match\n            the size used during training. Defaults to 300.\n        num_classes: The number of classes in the output of the model. Must\n            match the value used during training. Defaults to 3.\n        batch_size: The size of the batches used for testing. Defaults to 32.\n    """"""\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n    print(20 * ""="", "" Preparing for testing "", 20 * ""="")\n\n    checkpoint = torch.load(pretrained_file)\n\n    # Retrieving model parameters from checkpoint.\n    vocab_size = checkpoint[""model""][""_word_embedding.weight""].size(0)\n    embedding_dim = checkpoint[""model""][\'_word_embedding.weight\'].size(1)\n    hidden_size = checkpoint[""model""][""_projection.0.weight""].size(0)\n    num_classes = checkpoint[""model""][""_classification.4.weight""].size(0)\n\n    print(""\\t* Loading test data..."")\n    with open(test_file, ""rb"") as pkl:\n        test_data = NLIDataset(pickle.load(pkl))\n\n    test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n\n    print(""\\t* Building model..."")\n    model = ESIM(vocab_size,\n                 embedding_dim,\n                 hidden_size,\n                 num_classes=num_classes,\n                 device=device).to(device)\n\n    model.load_state_dict(checkpoint[""model""])\n\n    print(20 * ""="",\n          "" Testing ESIM model on device: {} "".format(device),\n          20 * ""="")\n    batch_time, total_time, accuracy = test(model, test_loader)\n\n    print(""-> Average batch processing time: {:.4f}s, total test time:\\\n {:.4f}s, accuracy: {:.4f}%"".format(batch_time, total_time, (accuracy*100)))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Test the ESIM model on\\\n some dataset"")\n    parser.add_argument(""test_data"",\n                        help=""Path to a file containing preprocessed test data"")\n    parser.add_argument(""checkpoint"",\n                        help=""Path to a checkpoint with a pretrained model"")\n    parser.add_argument(""--batch_size"", type=int, default=32,\n                        help=""Batch size to use during testing"")\n    args = parser.parse_args()\n\n    main(args.test_data,\n         args.checkpoint,\n         args.batch_size)\n'"
scripts/training/train_mnli.py,9,"b'""""""\nTrain the ESIM model on the preprocessed MultiNLI dataset.\n""""""\n# Aurelien Coet, 2019.\n\nimport os\nimport argparse\nimport pickle\nimport torch\nimport json\n\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\n\nfrom torch.utils.data import DataLoader\nfrom esim.data import NLIDataset\nfrom esim.model import ESIM\nfrom utils import train, validate\n\n\ndef main(train_file,\n         valid_files,\n         embeddings_file,\n         target_dir,\n         hidden_size=300,\n         dropout=0.5,\n         num_classes=3,\n         epochs=64,\n         batch_size=32,\n         lr=0.0004,\n         patience=5,\n         max_grad_norm=10.0,\n         checkpoint=None):\n    """"""\n    Train the ESIM model on the SNLI dataset.\n\n    Args:\n        train_file: A path to some preprocessed data that must be used\n            to train the model.\n        valid_files: A dict containing the paths to the preprocessed matched\n            and mismatched datasets that must be used to validate the model.\n        embeddings_file: A path to some preprocessed word embeddings that\n            must be used to initialise the model.\n        target_dir: The path to a directory where the trained model must\n            be saved.\n        hidden_size: The size of the hidden layers in the model. Defaults\n            to 300.\n        dropout: The dropout rate to use in the model. Defaults to 0.5.\n        num_classes: The number of classes in the output of the model.\n            Defaults to 3.\n        epochs: The maximum number of epochs for training. Defaults to 64.\n        batch_size: The size of the batches for training. Defaults to 32.\n        lr: The learning rate for the optimizer. Defaults to 0.0004.\n        patience: The patience to use for early stopping. Defaults to 5.\n        checkpoint: A checkpoint from which to continue training. If None,\n            training starts from scratch. Defaults to None.\n    """"""\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n    print(20 * ""="", "" Preparing for training "", 20 * ""="")\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # -------------------- Data loading ------------------- #\n    print(""\\t* Loading training data..."")\n    with open(train_file, ""rb"") as pkl:\n        train_data = NLIDataset(pickle.load(pkl))\n\n    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n\n    print(""\\t* Loading validation data..."")\n    with open(os.path.normpath(valid_files[""matched""]), ""rb"") as pkl:\n        matched_valid_data = NLIDataset(pickle.load(pkl))\n\n    with open(os.path.normpath(valid_files[""mismatched""]), ""rb"") as pkl:\n        mismatched_valid_data = NLIDataset(pickle.load(pkl))\n\n    matched_valid_loader = DataLoader(matched_valid_data,\n                                      shuffle=False,\n                                      batch_size=batch_size)\n    mismatched_valid_loader = DataLoader(mismatched_valid_data,\n                                         shuffle=False,\n                                         batch_size=batch_size)\n\n    # -------------------- Model definition ------------------- #\n    print(\'\\t* Building model...\')\n    with open(embeddings_file, ""rb"") as pkl:\n        embeddings = torch.tensor(pickle.load(pkl), dtype=torch.float)\\\n                     .to(device)\n\n    model = ESIM(embeddings.shape[0],\n                 embeddings.shape[1],\n                 hidden_size,\n                 embeddings=embeddings,\n                 dropout=dropout,\n                 num_classes=num_classes,\n                 device=device).to(device)\n\n    # -------------------- Preparation for training  ------------------- #\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                           mode=""max"",\n                                                           factor=0.5,\n                                                           patience=0)\n\n    best_score = 0.0\n    start_epoch = 1\n\n    # Data for loss curves plot.\n    epochs_count = []\n    train_losses = []\n    matched_valid_losses = []\n    mismatched_valid_losses = []\n\n    # Continuing training from a checkpoint if one was given as argument.\n    if checkpoint:\n        checkpoint = torch.load(checkpoint)\n        start_epoch = checkpoint[""epoch""] + 1\n        best_score = checkpoint[""best_score""]\n\n        print(""\\t* Training will continue on existing model from epoch {}...""\n              .format(start_epoch))\n\n        model.load_state_dict(checkpoint[""model""])\n        optimizer.load_state_dict(checkpoint[""optimizer""])\n        epochs_count = checkpoint[""epochs_count""]\n        train_losses = checkpoint[""train_losses""]\n        matched_valid_losses = checkpoint[""match_valid_losses""]\n        mismatched_valid_losses = checkpoint[""mismatch_valid_losses""]\n\n    # Compute loss and accuracy before starting (or resuming) training.\n    _, valid_loss, valid_accuracy = validate(model,\n                                             matched_valid_loader,\n                                             criterion)\n    print(""\\t* Validation loss before training on matched data: {:.4f}, accuracy: {:.4f}%""\n          .format(valid_loss, (valid_accuracy*100)))\n\n    _, valid_loss, valid_accuracy = validate(model,\n                                             mismatched_valid_loader,\n                                             criterion)\n    print(""\\t* Validation loss before training on mismatched data: {:.4f}, accuracy: {:.4f}%""\n          .format(valid_loss, (valid_accuracy*100)))\n\n    # -------------------- Training epochs ------------------- #\n    print(""\\n"",\n          20 * ""="",\n          ""Training ESIM model on device: {}"".format(device),\n          20 * ""="")\n\n    patience_counter = 0\n    for epoch in range(start_epoch, epochs+1):\n        epochs_count.append(epoch)\n\n        print(""* Training epoch {}:"".format(epoch))\n        epoch_time, epoch_loss, epoch_accuracy = train(model,\n                                                       train_loader,\n                                                       optimizer,\n                                                       criterion,\n                                                       epoch,\n                                                       max_grad_norm)\n\n        train_losses.append(epoch_loss)\n        print(""-> Training time: {:.4f}s, loss = {:.4f}, accuracy: {:.4f}%\\n""\n              .format(epoch_time, epoch_loss, (epoch_accuracy*100)))\n\n        print(""* Validation for epoch {} on matched data:"".format(epoch))\n        epoch_time, epoch_loss, epoch_accuracy = validate(model,\n                                                          matched_valid_loader,\n                                                          criterion)\n        matched_valid_losses.append(epoch_loss)\n        print(""-> Valid. time: {:.4f}s, loss: {:.4f}, accuracy: {:.4f}%""\n              .format(epoch_time, epoch_loss, (epoch_accuracy*100)))\n\n        print(""* Validation for epoch {} on mismatched data:"".format(epoch))\n        epoch_time, epoch_loss, mis_epoch_accuracy = validate(model,\n                                                              mismatched_valid_loader,\n                                                              criterion)\n        mismatched_valid_losses.append(epoch_loss)\n        print(""-> Valid. time: {:.4f}s, loss: {:.4f}, accuracy: {:.4f}%\\n""\n              .format(epoch_time, epoch_loss, (mis_epoch_accuracy*100)))\n\n        # Update the optimizer\'s learning rate with the scheduler.\n        scheduler.step(epoch_accuracy)\n\n        # Early stopping on validation accuracy.\n        if epoch_accuracy < best_score:\n            patience_counter += 1\n        else:\n            best_score = epoch_accuracy\n            patience_counter = 0\n            # Save the best model. The optimizer is not saved to avoid having\n            # a checkpoint file that is too heavy to be shared. To resume\n            # training from the best model, use the \'esim_*.pth.tar\'\n            # checkpoints instead.\n            torch.save({""epoch"": epoch,\n                        ""model"": model.state_dict(),\n                        ""best_score"": best_score,\n                        ""epochs_count"": epochs_count,\n                        ""train_losses"": train_losses,\n                        ""match_valid_losses"": matched_valid_losses,\n                        ""mismatch_valid_losses"": mismatched_valid_losses},\n                       os.path.join(target_dir, ""best.pth.tar""))\n\n        # Save the model at each epoch.\n        torch.save({""epoch"": epoch,\n                    ""model"": model.state_dict(),\n                    ""best_score"": best_score,\n                    ""optimizer"": optimizer.state_dict(),\n                    ""epochs_count"": epochs_count,\n                    ""train_losses"": train_losses,\n                    ""match_valid_losses"": matched_valid_losses,\n                    ""mismatch_valid_losses"": mismatched_valid_losses},\n                   os.path.join(target_dir, ""esim_{}.pth.tar"".format(epoch)))\n\n        if patience_counter >= patience:\n            print(""-> Early stopping: patience limit reached, stopping..."")\n            break\n\n    # Plotting of the loss curves for the train and validation sets.\n    plt.figure()\n    plt.plot(epochs_count, train_losses, ""-r"")\n    plt.plot(epochs_count, matched_valid_losses, ""-b"")\n    plt.plot(epochs_count, mismatched_valid_losses, ""-g"")\n    plt.xlabel(""epoch"")\n    plt.ylabel(""loss"")\n    plt.legend([""Training loss"",\n                ""Validation loss (matched set)"",\n                ""Validation loss (mismatched set)""])\n    plt.title(""Cross entropy loss"")\n    plt.show()\n\n\nif __name__ == ""__main__"":\n    default_config = ""../../config/training/mnli_training.json""\n\n    parser = argparse.ArgumentParser(description=""Train the ESIM model on MultiNLI"")\n    parser.add_argument(""--config"",\n                        default=default_config,\n                        help=""Path to a json configuration file"")\n    parser.add_argument(""--checkpoint"",\n                        default=None,\n                        help=""Path to a checkpoint file to resume training"")\n    args = parser.parse_args()\n\n    script_dir = os.path.dirname(os.path.realpath(__file__))\n\n    if args.config == default_config:\n        config_path = os.path.join(script_dir, args.config)\n    else:\n        config_path = args.config\n\n    with open(os.path.normpath(config_path), ""r"") as config_file:\n        config = json.load(config_file)\n\n    main(os.path.normpath(os.path.join(script_dir, config[""train_data""])),\n         config[""valid_data""],\n         os.path.normpath(os.path.join(script_dir, config[""embeddings""])),\n         os.path.normpath(os.path.join(script_dir, config[""target_dir""])),\n         config[""hidden_size""],\n         config[""dropout""],\n         config[""num_classes""],\n         config[""epochs""],\n         config[""batch_size""],\n         config[""lr""],\n         config[""patience""],\n         config[""max_gradient_norm""],\n         args.checkpoint)\n'"
scripts/training/train_snli.py,9,"b'""""""\nTrain the ESIM model on the preprocessed SNLI dataset.\n""""""\n# Aurelien Coet, 2018.\n\nimport os\nimport argparse\nimport pickle\nimport torch\nimport json\n\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\n\nfrom torch.utils.data import DataLoader\nfrom esim.data import NLIDataset\nfrom esim.model import ESIM\nfrom utils import train, validate\n\n\ndef main(train_file,\n         valid_file,\n         embeddings_file,\n         target_dir,\n         hidden_size=300,\n         dropout=0.5,\n         num_classes=3,\n         epochs=64,\n         batch_size=32,\n         lr=0.0004,\n         patience=5,\n         max_grad_norm=10.0,\n         checkpoint=None):\n    """"""\n    Train the ESIM model on the SNLI dataset.\n\n    Args:\n        train_file: A path to some preprocessed data that must be used\n            to train the model.\n        valid_file: A path to some preprocessed data that must be used\n            to validate the model.\n        embeddings_file: A path to some preprocessed word embeddings that\n            must be used to initialise the model.\n        target_dir: The path to a directory where the trained model must\n            be saved.\n        hidden_size: The size of the hidden layers in the model. Defaults\n            to 300.\n        dropout: The dropout rate to use in the model. Defaults to 0.5.\n        num_classes: The number of classes in the output of the model.\n            Defaults to 3.\n        epochs: The maximum number of epochs for training. Defaults to 64.\n        batch_size: The size of the batches for training. Defaults to 32.\n        lr: The learning rate for the optimizer. Defaults to 0.0004.\n        patience: The patience to use for early stopping. Defaults to 5.\n        checkpoint: A checkpoint from which to continue training. If None,\n            training starts from scratch. Defaults to None.\n    """"""\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n    print(20 * ""="", "" Preparing for training "", 20 * ""="")\n\n    if not os.path.exists(target_dir):\n        os.makedirs(target_dir)\n\n    # -------------------- Data loading ------------------- #\n    print(""\\t* Loading training data..."")\n    with open(train_file, ""rb"") as pkl:\n        train_data = NLIDataset(pickle.load(pkl))\n\n    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n\n    print(""\\t* Loading validation data..."")\n    with open(valid_file, ""rb"") as pkl:\n        valid_data = NLIDataset(pickle.load(pkl))\n\n    valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n\n    # -------------------- Model definition ------------------- #\n    print(""\\t* Building model..."")\n    with open(embeddings_file, ""rb"") as pkl:\n        embeddings = torch.tensor(pickle.load(pkl), dtype=torch.float)\\\n                     .to(device)\n\n    model = ESIM(embeddings.shape[0],\n                 embeddings.shape[1],\n                 hidden_size,\n                 embeddings=embeddings,\n                 dropout=dropout,\n                 num_classes=num_classes,\n                 device=device).to(device)\n\n    # -------------------- Preparation for training  ------------------- #\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                           mode=""max"",\n                                                           factor=0.5,\n                                                           patience=0)\n\n    best_score = 0.0\n    start_epoch = 1\n\n    # Data for loss curves plot.\n    epochs_count = []\n    train_losses = []\n    valid_losses = []\n\n    # Continuing training from a checkpoint if one was given as argument.\n    if checkpoint:\n        checkpoint = torch.load(checkpoint)\n        start_epoch = checkpoint[""epoch""] + 1\n        best_score = checkpoint[""best_score""]\n\n        print(""\\t* Training will continue on existing model from epoch {}...""\n              .format(start_epoch))\n\n        model.load_state_dict(checkpoint[""model""])\n        optimizer.load_state_dict(checkpoint[""optimizer""])\n        epochs_count = checkpoint[""epochs_count""]\n        train_losses = checkpoint[""train_losses""]\n        valid_losses = checkpoint[""valid_losses""]\n\n    # Compute loss and accuracy before starting (or resuming) training.\n    _, valid_loss, valid_accuracy = validate(model,\n                                             valid_loader,\n                                             criterion)\n    print(""\\t* Validation loss before training: {:.4f}, accuracy: {:.4f}%""\n          .format(valid_loss, (valid_accuracy*100)))\n\n    # -------------------- Training epochs ------------------- #\n    print(""\\n"",\n          20 * ""="",\n          ""Training ESIM model on device: {}"".format(device),\n          20 * ""="")\n\n    patience_counter = 0\n    for epoch in range(start_epoch, epochs+1):\n        epochs_count.append(epoch)\n\n        print(""* Training epoch {}:"".format(epoch))\n        epoch_time, epoch_loss, epoch_accuracy = train(model,\n                                                       train_loader,\n                                                       optimizer,\n                                                       criterion,\n                                                       epoch,\n                                                       max_grad_norm)\n\n        train_losses.append(epoch_loss)\n        print(""-> Training time: {:.4f}s, loss = {:.4f}, accuracy: {:.4f}%""\n              .format(epoch_time, epoch_loss, (epoch_accuracy*100)))\n\n        print(""* Validation for epoch {}:"".format(epoch))\n        epoch_time, epoch_loss, epoch_accuracy = validate(model,\n                                                          valid_loader,\n                                                          criterion)\n\n        valid_losses.append(epoch_loss)\n        print(""-> Valid. time: {:.4f}s, loss: {:.4f}, accuracy: {:.4f}%\\n""\n              .format(epoch_time, epoch_loss, (epoch_accuracy*100)))\n\n        # Update the optimizer\'s learning rate with the scheduler.\n        scheduler.step(epoch_accuracy)\n\n        # Early stopping on validation accuracy.\n        if epoch_accuracy < best_score:\n            patience_counter += 1\n        else:\n            best_score = epoch_accuracy\n            patience_counter = 0\n            # Save the best model. The optimizer is not saved to avoid having\n            # a checkpoint file that is too heavy to be shared. To resume\n            # training from the best model, use the \'esim_*.pth.tar\'\n            # checkpoints instead.\n            torch.save({""epoch"": epoch,\n                        ""model"": model.state_dict(),\n                        ""best_score"": best_score,\n                        ""epochs_count"": epochs_count,\n                        ""train_losses"": train_losses,\n                        ""valid_losses"": valid_losses},\n                       os.path.join(target_dir, ""best.pth.tar""))\n\n        # Save the model at each epoch.\n        torch.save({""epoch"": epoch,\n                    ""model"": model.state_dict(),\n                    ""best_score"": best_score,\n                    ""optimizer"": optimizer.state_dict(),\n                    ""epochs_count"": epochs_count,\n                    ""train_losses"": train_losses,\n                    ""valid_losses"": valid_losses},\n                   os.path.join(target_dir, ""esim_{}.pth.tar"".format(epoch)))\n\n        if patience_counter >= patience:\n            print(""-> Early stopping: patience limit reached, stopping..."")\n            break\n\n    # Plotting of the loss curves for the train and validation sets.\n    plt.figure()\n    plt.plot(epochs_count, train_losses, ""-r"")\n    plt.plot(epochs_count, valid_losses, ""-b"")\n    plt.xlabel(""epoch"")\n    plt.ylabel(""loss"")\n    plt.legend([""Training loss"", ""Validation loss""])\n    plt.title(""Cross entropy loss"")\n    plt.show()\n\n\nif __name__ == ""__main__"":\n    default_config = ""../../config/training/snli_training.json""\n\n    parser = argparse.ArgumentParser(description=""Train the ESIM model on SNLI"")\n    parser.add_argument(""--config"",\n                        default=default_config,\n                        help=""Path to a json configuration file"")\n    parser.add_argument(""--checkpoint"",\n                        default=None,\n                        help=""Path to a checkpoint file to resume training"")\n    args = parser.parse_args()\n\n    script_dir = os.path.dirname(os.path.realpath(__file__))\n\n    if args.config == default_config:\n        config_path = os.path.join(script_dir, args.config)\n    else:\n        config_path = args.config\n\n    with open(os.path.normpath(config_path), \'r\') as config_file:\n        config = json.load(config_file)\n\n    main(os.path.normpath(os.path.join(script_dir, config[""train_data""])),\n         os.path.normpath(os.path.join(script_dir, config[""valid_data""])),\n         os.path.normpath(os.path.join(script_dir, config[""embeddings""])),\n         os.path.normpath(os.path.join(script_dir, config[""target_dir""])),\n         config[""hidden_size""],\n         config[""dropout""],\n         config[""num_classes""],\n         config[""epochs""],\n         config[""batch_size""],\n         config[""lr""],\n         config[""patience""],\n         config[""max_gradient_norm""],\n         args.checkpoint)\n'"
scripts/training/utils.py,2,"b'""""""\nUtility functions for training and validating models.\n""""""\n\nimport time\nimport torch\n\nimport torch.nn as nn\n\nfrom tqdm import tqdm\nfrom esim.utils import correct_predictions\n\n\ndef train(model,\n          dataloader,\n          optimizer,\n          criterion,\n          epoch_number,\n          max_gradient_norm):\n    """"""\n    Train a model for one epoch on some input data with a given optimizer and\n    criterion.\n\n    Args:\n        model: A torch module that must be trained on some input data.\n        dataloader: A DataLoader object to iterate over the training data.\n        optimizer: A torch optimizer to use for training on the input model.\n        criterion: A loss criterion to use for training.\n        epoch_number: The number of the epoch for which training is performed.\n        max_gradient_norm: Max. norm for gradient norm clipping.\n\n    Returns:\n        epoch_time: The total time necessary to train the epoch.\n        epoch_loss: The training loss computed for the epoch.\n        epoch_accuracy: The accuracy computed for the epoch.\n    """"""\n    # Switch the model to train mode.\n    model.train()\n    device = model.device\n\n    epoch_start = time.time()\n    batch_time_avg = 0.0\n    running_loss = 0.0\n    correct_preds = 0\n\n    tqdm_batch_iterator = tqdm(dataloader)\n    for batch_index, batch in enumerate(tqdm_batch_iterator):\n        batch_start = time.time()\n\n        # Move input and output data to the GPU if it is used.\n        premises = batch[""premise""].to(device)\n        premises_lengths = batch[""premise_length""].to(device)\n        hypotheses = batch[""hypothesis""].to(device)\n        hypotheses_lengths = batch[""hypothesis_length""].to(device)\n        labels = batch[""label""].to(device)\n\n        optimizer.zero_grad()\n\n        logits, probs = model(premises,\n                              premises_lengths,\n                              hypotheses,\n                              hypotheses_lengths)\n        loss = criterion(logits, labels)\n        loss.backward()\n\n        nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)\n        optimizer.step()\n\n        batch_time_avg += time.time() - batch_start\n        running_loss += loss.item()\n        correct_preds += correct_predictions(probs, labels)\n\n        description = ""Avg. batch proc. time: {:.4f}s, loss: {:.4f}""\\\n                      .format(batch_time_avg/(batch_index+1),\n                              running_loss/(batch_index+1))\n        tqdm_batch_iterator.set_description(description)\n\n    epoch_time = time.time() - epoch_start\n    epoch_loss = running_loss / len(dataloader)\n    epoch_accuracy = correct_preds / len(dataloader.dataset)\n\n    return epoch_time, epoch_loss, epoch_accuracy\n\n\ndef validate(model, dataloader, criterion):\n    """"""\n    Compute the loss and accuracy of a model on some validation dataset.\n\n    Args:\n        model: A torch module for which the loss and accuracy must be\n            computed.\n        dataloader: A DataLoader object to iterate over the validation data.\n        criterion: A loss criterion to use for computing the loss.\n        epoch: The number of the epoch for which validation is performed.\n        device: The device on which the model is located.\n\n    Returns:\n        epoch_time: The total time to compute the loss and accuracy on the\n            entire validation set.\n        epoch_loss: The loss computed on the entire validation set.\n        epoch_accuracy: The accuracy computed on the entire validation set.\n    """"""\n    # Switch to evaluate mode.\n    model.eval()\n    device = model.device\n\n    epoch_start = time.time()\n    running_loss = 0.0\n    running_accuracy = 0.0\n\n    # Deactivate autograd for evaluation.\n    with torch.no_grad():\n        for batch in dataloader:\n            # Move input and output data to the GPU if one is used.\n            premises = batch[""premise""].to(device)\n            premises_lengths = batch[""premise_length""].to(device)\n            hypotheses = batch[""hypothesis""].to(device)\n            hypotheses_lengths = batch[""hypothesis_length""].to(device)\n            labels = batch[""label""].to(device)\n\n            logits, probs = model(premises,\n                                  premises_lengths,\n                                  hypotheses,\n                                  hypotheses_lengths)\n            loss = criterion(logits, labels)\n\n            running_loss += loss.item()\n            running_accuracy += correct_predictions(probs, labels)\n\n    epoch_time = time.time() - epoch_start\n    epoch_loss = running_loss / len(dataloader)\n    epoch_accuracy = running_accuracy / (len(dataloader.dataset))\n\n    return epoch_time, epoch_loss, epoch_accuracy\n'"
