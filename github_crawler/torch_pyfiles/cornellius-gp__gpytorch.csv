file_path,api_count,code
setup.py,3,"b'#!/usr/bin/env python3\n\nimport io\nimport os\nimport re\n\nfrom setuptools import find_packages, setup\n\n\n# Get version\ndef read(*names, **kwargs):\n    with io.open(os.path.join(os.path.dirname(__file__), *names), encoding=kwargs.get(""encoding"", ""utf8"")) as fp:\n        return fp.read()\n\n\ndef find_version(*file_paths):\n    version_file = read(*file_paths)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"", version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(""Unable to find version string."")\n\n\nreadme = open(""README.md"").read()\nversion = find_version(""gpytorch"", ""__init__.py"")\n\n\ntorch_min = ""1.5""\ninstall_requires = ["">="".join([""torch"", torch_min])]\n# if recent dev version of PyTorch is installed, no need to install stable\ntry:\n    import torch\n\n    if torch.__version__ >= torch_min:\n        install_requires = []\nexcept ImportError:\n    pass\n\n\n# Run the setup\nsetup(\n    name=""gpytorch"",\n    version=version,\n    description=""An implementation of Gaussian Processes in Pytorch"",\n    long_description=readme,\n    long_description_content_type=""text/markdown"",\n    author=""Jake Gardner, Geoff Pleiss"",\n    url=""https://gpytorch.ai"",\n    author_email=""jrg365@cornell.edu, gpleiss@gmail.com"",\n    project_urls={\n        ""Documentation"": ""https://gpytorch.readthedocs.io"",\n        ""Source"": ""https://github.com/cornellius-gp/gpytorch/"",\n    },\n    license=""MIT"",\n    classifiers=[""Development Status :: 4 - Beta"", ""Programming Language :: Python :: 3""],\n    packages=find_packages(),\n    python_requires="">=3.6"",\n    install_requires=install_requires,\n    extras_require={\n        ""dev"": [""black"", ""twine"", ""pre-commit""],\n        ""docs"": [""ipython"", ""ipykernel"", ""sphinx<3.0.0"", ""sphinx_rtd_theme"", ""nbsphinx"", ""m2r""],\n        ""examples"": [""ipython"", ""jupyter"", ""matplotlib"", ""scipy"", ""torchvision"", ""tqdm""],\n        ""pyro"": [""pyro-ppl>=1.0.0""],\n        ""keops"": [""pykeops>=1.1.1""],\n        ""test"": [""flake8"", ""flake8-print"", ""pytest"", ""nbval""],\n    },\n)\n'"
examples/LBFGS.py,19,"b'import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import reduce\nfrom copy import deepcopy\nfrom torch.optim import Optimizer\n\n\ndef is_legal(v):\n    """"""\n    Checks that tensor is not NaN or Inf.\n\n    Inputs:\n        v (tensor): tensor to be checked\n\n    """"""\n    legal = not torch.isnan(v).any() and not torch.isinf(v)\n\n    return legal\n\n\ndef polyinterp(points, x_min_bound=None, x_max_bound=None, plot=False):\n    """"""\n    Gives the minimizer and minimum of the interpolating polynomial over given points\n    based on function and derivative information. Defaults to bisection if no critical\n    points are valid.\n\n    Based on polyinterp.m Matlab function in minFunc by Mark Schmidt with some slight\n    modifications.\n\n    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n    Last edited 12/6/18.\n\n    Inputs:\n        points (nparray): two-dimensional array with each point of form [x f g]\n        x_min_bound (float): minimum value that brackets minimum (default: minimum of points)\n        x_max_bound (float): maximum value that brackets minimum (default: maximum of points)\n        plot (bool): plot interpolating polynomial\n\n    Outputs:\n        x_sol (float): minimizer of interpolating polynomial\n        F_min (float): minimum of interpolating polynomial\n\n    Note:\n      . Set f or g to np.nan if they are unknown\n\n    """"""\n    no_points = points.shape[0]\n    order = np.sum(1 - np.isnan(points[:, 1:3]).astype(""int"")) - 1\n\n    x_min = np.min(points[:, 0])\n    x_max = np.max(points[:, 0])\n\n    # compute bounds of interpolation area\n    if x_min_bound is None:\n        x_min_bound = x_min\n    if x_max_bound is None:\n        x_max_bound = x_max\n\n    # explicit formula for quadratic interpolation\n    if no_points == 2 and order == 2 and plot is False:\n        # Solution to quadratic interpolation is given by:\n        # a = -(f1 - f2 - g1(x1 - x2))/(x1 - x2)^2\n        # x_min = x1 - g1/(2a)\n        # if x1 = 0, then is given by:\n        # x_min = - (g1*x2^2)/(2(f2 - f1 - g1*x2))\n\n        if points[0, 0] == 0:\n            x_sol = (\n                -points[0, 2] * points[1, 0] ** 2 / (2 * (points[1, 1] - points[0, 1] - points[0, 2] * points[1, 0]))\n            )\n        else:\n            a = (\n                -(points[0, 1] - points[1, 1] - points[0, 2] * (points[0, 0] - points[1, 0]))\n                / (points[0, 0] - points[1, 0]) ** 2\n            )\n            x_sol = points[0, 0] - points[0, 2] / (2 * a)\n\n        x_sol = np.minimum(np.maximum(x_min_bound, x_sol), x_max_bound)\n\n    # explicit formula for cubic interpolation\n    elif no_points == 2 and order == 3 and plot is False:\n        # Solution to cubic interpolation is given by:\n        # d1 = g1 + g2 - 3((f1 - f2)/(x1 - x2))\n        # d2 = sqrt(d1^2 - g1*g2)\n        # x_min = x2 - (x2 - x1)*((g2 + d2 - d1)/(g2 - g1 + 2*d2))\n        d1 = points[0, 2] + points[1, 2] - 3 * ((points[0, 1] - points[1, 1]) / (points[0, 0] - points[1, 0]))\n        d2 = np.sqrt(d1 ** 2 - points[0, 2] * points[1, 2])\n        if np.isreal(d2):\n            x_sol = points[1, 0] - (points[1, 0] - points[0, 0]) * (\n                (points[1, 2] + d2 - d1) / (points[1, 2] - points[0, 2] + 2 * d2)\n            )\n            x_sol = np.minimum(np.maximum(x_min_bound, x_sol), x_max_bound)\n        else:\n            x_sol = (x_max_bound + x_min_bound) / 2\n\n    # solve linear system\n    else:\n        # define linear constraints\n        A = np.zeros((0, order + 1))\n        b = np.zeros((0, 1))\n\n        # add linear constraints on function values\n        for i in range(no_points):\n            if not np.isnan(points[i, 1]):\n                constraint = np.zeros((1, order + 1))\n                for j in range(order, -1, -1):\n                    constraint[0, order - j] = points[i, 0] ** j\n                A = np.append(A, constraint, 0)\n                b = np.append(b, points[i, 1])\n\n        # add linear constraints on gradient values\n        for i in range(no_points):\n            if not np.isnan(points[i, 2]):\n                constraint = np.zeros((1, order + 1))\n                for j in range(order):\n                    constraint[0, j] = (order - j) * points[i, 0] ** (order - j - 1)\n                A = np.append(A, constraint, 0)\n                b = np.append(b, points[i, 2])\n\n        # check if system is solvable\n        if A.shape[0] != A.shape[1] or np.linalg.matrix_rank(A) != A.shape[0]:\n            x_sol = (x_min_bound + x_max_bound) / 2\n            f_min = np.Inf\n        else:\n            # solve linear system for interpolating polynomial\n            coeff = np.linalg.solve(A, b)\n\n            # compute critical points\n            dcoeff = np.zeros(order)\n            for i in range(len(coeff) - 1):\n                dcoeff[i] = coeff[i] * (order - i)\n\n            crit_pts = np.array([x_min_bound, x_max_bound])\n            crit_pts = np.append(crit_pts, points[:, 0])\n\n            if not np.isinf(dcoeff).any():\n                roots = np.roots(dcoeff)\n                crit_pts = np.append(crit_pts, roots)\n\n            # test critical points\n            f_min = np.Inf\n            x_sol = (x_min_bound + x_max_bound) / 2  # defaults to bisection\n            for crit_pt in crit_pts:\n                if np.isreal(crit_pt) and crit_pt >= x_min_bound and crit_pt <= x_max_bound:\n                    F_cp = np.polyval(coeff, crit_pt)\n                    if np.isreal(F_cp) and F_cp < f_min:\n                        x_sol = np.real(crit_pt)\n                        f_min = np.real(F_cp)\n\n            if plot:\n                plt.figure()\n                x = np.arange(x_min_bound, x_max_bound, (x_max_bound - x_min_bound) / 10000)\n                f = np.polyval(coeff, x)\n                plt.plot(x, f)\n                plt.plot(x_sol, f_min, ""x"")\n\n    return x_sol\n\n\nclass LBFGS(Optimizer):\n    """"""\n    Implements the L-BFGS algorithm. Compatible with multi-batch and full-overlap\n    L-BFGS implementations and (stochastic) Powell damping. Partly based on the\n    original L-BFGS implementation in PyTorch, Mark Schmidt\'s minFunc MATLAB code,\n    and Michael Overton\'s weak Wolfe line search MATLAB code.\n\n    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n    Last edited 12/6/18.\n\n    Warnings:\n      . Does not support per-parameter options and parameter groups.\n      . All parameters have to be on a single device.\n\n    Inputs:\n        lr (float): steplength or learning rate (default: 1)\n        history_size (int): update history size (default: 10)\n        line_search (str): designates line search to use (default: \'Wolfe\')\n            Options:\n                \'None\': uses steplength designated in algorithm\n                \'Armijo\': uses Armijo backtracking line search\n                \'Wolfe\': uses Armijo-Wolfe bracketing line search\n        dtype: data type (default: torch.float)\n        debug (bool): debugging mode\n\n    References:\n    [1] Berahas, Albert S., Jorge Nocedal, and Martin Tak\xc3\xa1c. ""A Multi-Batch L-BFGS\n        Method for Machine Learning."" Advances in Neural Information Processing\n        Systems. 2016.\n    [2] Bollapragada, Raghu, et al. ""A Progressive Batching L-BFGS Method for Machine\n        Learning."" International Conference on Machine Learning. 2018.\n    [3] Lewis, Adrian S., and Michael L. Overton. ""Nonsmooth Optimization via Quasi-Newton\n        Methods."" Mathematical Programming 141.1-2 (2013): 135-163.\n    [4] Liu, Dong C., and Jorge Nocedal. ""On the Limited Memory BFGS Method for\n        Large Scale Optimization."" Mathematical Programming 45.1-3 (1989): 503-528.\n    [5] Nocedal, Jorge. ""Updating Quasi-Newton Matrices With Limited Storage.""\n        Mathematics of Computation 35.151 (1980): 773-782.\n    [6] Nocedal, Jorge, and Stephen J. Wright. ""Numerical Optimization."" Springer New York,\n        2006.\n    [7] Schmidt, Mark. ""minFunc: Unconstrained Differentiable Multivariate Optimization\n        in Matlab."" Software available at http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html\n        (2005).\n    [8] Schraudolph, Nicol N., Jin Yu, and Simon G\xc3\xbcnter. ""A Stochastic Quasi-Newton\n        Method for Online Convex Optimization."" Artificial Intelligence and Statistics.\n        2007.\n    [9] Wang, Xiao, et al. ""Stochastic Quasi-Newton Methods for Nonconvex Stochastic\n        Optimization."" SIAM Journal on Optimization 27.2 (2017): 927-956.\n\n    """"""\n\n    def __init__(self, params, lr=1, history_size=10, line_search=""Wolfe"", dtype=torch.float, debug=False):\n\n        # ensure inputs are valid\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0 <= history_size:\n            raise ValueError(""Invalid history size: {}"".format(history_size))\n        if line_search not in [""Armijo"", ""Wolfe"", ""None""]:\n            raise ValueError(""Invalid line search: {}"".format(line_search))\n\n        defaults = dict(lr=lr, history_size=history_size, line_search=line_search, dtype=dtype, debug=debug)\n        super(LBFGS, self).__init__(params, defaults)\n\n        if len(self.param_groups) != 1:\n            raise ValueError(""L-BFGS doesn\'t support per-parameter options "" ""(parameter groups)"")\n\n        self._params = self.param_groups[0][""params""]\n        self._numel_cache = None\n\n        state = self.state[""global_state""]\n        state.setdefault(""n_iter"", 0)\n        state.setdefault(""curv_skips"", 0)\n        state.setdefault(""fail_skips"", 0)\n        state.setdefault(""H_diag"", 1)\n        state.setdefault(""fail"", True)\n\n        state[""old_dirs""] = []\n        state[""old_stps""] = []\n\n    def _numel(self):\n        if self._numel_cache is None:\n            self._numel_cache = reduce(lambda total, p: total + p.numel(), self._params, 0)\n        return self._numel_cache\n\n    def _gather_flat_grad(self):\n        views = []\n        for p in self._params:\n            if p.grad is None:\n                view = p.data.new(p.data.numel()).zero_()\n            elif p.grad.data.is_sparse:\n                view = p.grad.data.to_dense().view(-1)\n            else:\n                view = p.grad.data.view(-1)\n            views.append(view)\n        return torch.cat(views, 0)\n\n    def _add_update(self, step_size, update):\n        offset = 0\n        for p in self._params:\n            numel = p.numel()\n            # view as to avoid deprecated pointwise semantics\n            p.data.add_(update[offset : offset + numel].view_as(p.data), alpha=step_size)\n            offset += numel\n        assert offset == self._numel()\n\n    def _copy_params(self):\n        current_params = []\n        for param in self._params:\n            current_params.append(deepcopy(param.data))\n        return current_params\n\n    def _load_params(self, current_params):\n        i = 0\n        for param in self._params:\n            param.data[:] = current_params[i]\n            i += 1\n\n    def line_search(self, line_search):\n        """"""\n        Switches line search option.\n\n        Inputs:\n            line_search (str): designates line search to use\n                Options:\n                    \'None\': uses steplength designated in algorithm\n                    \'Armijo\': uses Armijo backtracking line search\n                    \'Wolfe\': uses Armijo-Wolfe bracketing line search\n\n        """"""\n\n        group = self.param_groups[0]\n        group[""line_search""] = line_search\n\n        return\n\n    def two_loop_recursion(self, vec):\n        """"""\n        Performs two-loop recursion on given vector to obtain Hv.\n\n        Inputs:\n            vec (tensor): 1-D tensor to apply two-loop recursion to\n\n        Output:\n            r (tensor): matrix-vector product Hv\n\n        """"""\n\n        group = self.param_groups[0]\n        history_size = group[""history_size""]\n\n        state = self.state[""global_state""]\n        old_dirs = state.get(""old_dirs"")  # change in gradients\n        old_stps = state.get(""old_stps"")  # change in iterates\n        H_diag = state.get(""H_diag"")\n\n        # compute the product of the inverse Hessian approximation and the gradient\n        num_old = len(old_dirs)\n\n        if ""rho"" not in state:\n            state[""rho""] = [None] * history_size\n            state[""alpha""] = [None] * history_size\n        rho = state[""rho""]\n        alpha = state[""alpha""]\n\n        for i in range(num_old):\n            rho[i] = 1.0 / old_stps[i].dot(old_dirs[i])\n\n        q = vec\n        for i in range(num_old - 1, -1, -1):\n            alpha[i] = old_dirs[i].dot(q) * rho[i]\n            q.add_(old_stps[i], alpha=-alpha[i])\n\n        # multiply by initial Hessian\n        # r/d is the final direction\n        r = torch.mul(q, H_diag)\n        for i in range(num_old):\n            beta = old_stps[i].dot(r) * rho[i]\n            r.add_(old_dirs[i], alpha=(alpha[i] - beta))\n\n        return r\n\n    def curvature_update(self, flat_grad, eps=1e-2, damping=False):\n        """"""\n        Performs curvature update.\n\n        Inputs:\n            flat_grad (tensor): 1-D tensor of flattened gradient for computing\n                gradient difference with previously stored gradient\n            eps (float): constant for curvature pair rejection or damping (default: 1e-2)\n            damping (bool): flag for using Powell damping (default: False)\n        """"""\n\n        assert len(self.param_groups) == 1\n\n        # load parameters\n        if eps <= 0:\n            raise (ValueError(""Invalid eps; must be positive.""))\n\n        group = self.param_groups[0]\n        history_size = group[""history_size""]\n        debug = group[""debug""]\n\n        # variables cached in state (for tracing)\n        state = self.state[""global_state""]\n        fail = state.get(""fail"")\n\n        # check if line search failed\n        if not fail:\n\n            d = state.get(""d"")\n            t = state.get(""t"")\n            old_dirs = state.get(""old_dirs"")\n            old_stps = state.get(""old_stps"")\n            H_diag = state.get(""H_diag"")\n            prev_flat_grad = state.get(""prev_flat_grad"")\n            Bs = state.get(""Bs"")\n\n            # compute y\'s\n            y = flat_grad.sub(prev_flat_grad)\n            s = d.mul(t)\n            sBs = s.dot(Bs)\n            ys = y.dot(s)  # y*s\n\n            # update L-BFGS matrix\n            if ys > eps * sBs or damping == True:\n\n                # perform Powell damping\n                if damping == True and ys < eps * sBs:\n                    if debug:\n                        print(""Applying Powell damping..."")\n                    theta = ((1 - eps) * sBs) / (sBs - ys)\n                    y = theta * y + (1 - theta) * Bs\n\n                # updating memory\n                if len(old_dirs) == history_size:\n                    # shift history by one (limited-memory)\n                    old_dirs.pop(0)\n                    old_stps.pop(0)\n\n                # store new direction/step\n                old_dirs.append(s)\n                old_stps.append(y)\n\n                # update scale of initial Hessian approximation\n                H_diag = ys / y.dot(y)  # (y*y)\n\n                state[""old_dirs""] = old_dirs\n                state[""old_stps""] = old_stps\n                state[""H_diag""] = H_diag\n\n            else:\n                # save skip\n                state[""curv_skips""] += 1\n                if debug:\n                    print(""Curvature pair skipped due to failed criterion"")\n\n        else:\n            # save skip\n            state[""fail_skips""] += 1\n            if debug:\n                print(""Line search failed; curvature pair update skipped"")\n\n        return\n\n    def _step(self, p_k, g_Ok, g_Sk=None, options={}):\n        """"""\n        Performs a single optimization step.\n\n        Inputs:\n            p_k (tensor): 1-D tensor specifying search direction\n            g_Ok (tensor): 1-D tensor of flattened gradient over overlap O_k used\n                            for gradient differencing in curvature pair update\n            g_Sk (tensor): 1-D tensor of flattened gradient over full sample S_k\n                            used for curvature pair damping or rejection criterion,\n                            if None, will use g_Ok (default: None)\n            options (dict): contains options for performing line search\n\n        Options for Armijo backtracking line search:\n            \'closure\' (callable): reevaluates model and returns function value\n            \'current_loss\' (tensor): objective value at current iterate (default: F(x_k))\n            \'gtd\' (tensor): inner product g_Ok\'d in line search (default: g_Ok\'d)\n            \'eta\' (tensor): factor for decreasing steplength > 0 (default: 2)\n            \'c1\' (tensor): sufficient decrease constant in (0, 1) (default: 1e-4)\n            \'max_ls\' (int): maximum number of line search steps permitted (default: 10)\n            \'interpolate\' (bool): flag for using interpolation (default: True)\n            \'inplace\' (bool): flag for inplace operations (default: True)\n            \'ls_debug\' (bool): debugging mode for line search\n\n        Options for Wolfe line search:\n            \'closure\' (callable): reevaluates model and returns function value\n            \'current_loss\' (tensor): objective value at current iterate (default: F(x_k))\n            \'gtd\' (tensor): inner product g_Ok\'d in line search (default: g_Ok\'d)\n            \'eta\' (float): factor for extrapolation (default: 2)\n            \'c1\' (float): sufficient decrease constant in (0, 1) (default: 1e-4)\n            \'c2\' (float): curvature condition constant in (0, 1) (default: 0.9)\n            \'max_ls\' (int): maximum number of line search steps permitted (default: 10)\n            \'interpolate\' (bool): flag for using interpolation (default: True)\n            \'inplace\' (bool): flag for inplace operations (default: True)\n            \'ls_debug\' (bool): debugging mode for line search\n\n        Outputs (depends on line search):\n          . No line search:\n                t (float): steplength\n          . Armijo backtracking line search:\n                F_new (tensor): loss function at new iterate\n                t (tensor): final steplength\n                ls_step (int): number of backtracks\n                closure_eval (int): number of closure evaluations\n                desc_dir (bool): descent direction flag\n                    True: p_k is descent direction with respect to the line search\n                    function\n                    False: p_k is not a descent direction with respect to the line\n                    search function\n                fail (bool): failure flag\n                    True: line search reached maximum number of iterations, failed\n                    False: line search succeeded\n          . Wolfe line search:\n                F_new (tensor): loss function at new iterate\n                g_new (tensor): gradient at new iterate\n                t (float): final steplength\n                ls_step (int): number of backtracks\n                closure_eval (int): number of closure evaluations\n                grad_eval (int): number of gradient evaluations\n                desc_dir (bool): descent direction flag\n                    True: p_k is descent direction with respect to the line search\n                    function\n                    False: p_k is not a descent direction with respect to the line\n                    search function\n                fail (bool): failure flag\n                    True: line search reached maximum number of iterations, failed\n                    False: line search succeeded\n\n        Notes:\n          . If encountering line search failure in the deterministic setting, one\n            should try increasing the maximum number of line search steps max_ls.\n\n        """"""\n\n        assert len(self.param_groups) == 1\n\n        # load parameter options\n        group = self.param_groups[0]\n        lr = group[""lr""]\n        line_search = group[""line_search""]\n        dtype = group[""dtype""]\n        debug = group[""debug""]\n\n        # variables cached in state (for tracing)\n        state = self.state[""global_state""]\n        d = state.get(""d"")\n        t = state.get(""t"")\n        prev_flat_grad = state.get(""prev_flat_grad"")\n        Bs = state.get(""Bs"")\n\n        # keep track of nb of iterations\n        state[""n_iter""] += 1\n\n        # set search direction\n        d = p_k\n\n        # modify previous gradient\n        if prev_flat_grad is None:\n            prev_flat_grad = g_Ok.clone()\n        else:\n            prev_flat_grad.copy_(g_Ok)\n\n        # set initial step size\n        t = lr\n\n        # closure evaluation counter\n        closure_eval = 0\n\n        if g_Sk is None:\n            g_Sk = g_Ok.clone()\n\n        # perform Armijo backtracking line search\n        if line_search == ""Armijo"":\n\n            # load options\n            if options:\n                if ""closure"" not in options.keys():\n                    raise (ValueError(""closure option not specified.""))\n                else:\n                    closure = options[""closure""]\n\n                if ""gtd"" not in options.keys():\n                    gtd = g_Ok.dot(d)\n                else:\n                    gtd = options[""gtd""]\n\n                if ""current_loss"" not in options.keys():\n                    F_k = closure()\n                    closure_eval += 1\n                else:\n                    F_k = options[""current_loss""]\n\n                if ""eta"" not in options.keys():\n                    eta = 2\n                elif options[""eta""] <= 0:\n                    raise (ValueError(""Invalid eta; must be positive.""))\n                else:\n                    eta = options[""eta""]\n\n                if ""c1"" not in options.keys():\n                    c1 = 1e-4\n                elif options[""c1""] >= 1 or options[""c1""] <= 0:\n                    raise (ValueError(""Invalid c1; must be strictly between 0 and 1.""))\n                else:\n                    c1 = options[""c1""]\n\n                if ""max_ls"" not in options.keys():\n                    max_ls = 10\n                elif options[""max_ls""] <= 0:\n                    raise (ValueError(""Invalid max_ls; must be positive.""))\n                else:\n                    max_ls = options[""max_ls""]\n\n                if ""interpolate"" not in options.keys():\n                    interpolate = True\n                else:\n                    interpolate = options[""interpolate""]\n\n                if ""inplace"" not in options.keys():\n                    inplace = True\n                else:\n                    inplace = options[""inplace""]\n\n                if ""ls_debug"" not in options.keys():\n                    ls_debug = False\n                else:\n                    ls_debug = options[""ls_debug""]\n\n            else:\n                raise (ValueError(""Options are not specified; need closure evaluating function.""))\n\n            # initialize values\n            if interpolate:\n                if torch.cuda.is_available():\n                    F_prev = torch.tensor(np.nan, dtype=dtype).cuda()\n                else:\n                    F_prev = torch.tensor(np.nan, dtype=dtype)\n\n            ls_step = 0\n            t_prev = 0  # old steplength\n            fail = False  # failure flag\n\n            # begin print for debug mode\n            if ls_debug:\n                print(\n                    ""==================================== Begin Armijo line search ===================================""\n                )\n                print(""F(x): %.8e  g*d: %.8e"" % (F_k, gtd))\n\n            # check if search direction is descent direction\n            if gtd >= 0:\n                desc_dir = False\n                if debug:\n                    print(""Not a descent direction!"")\n            else:\n                desc_dir = True\n\n            # store values if not in-place\n            if not inplace:\n                current_params = self._copy_params()\n\n            # update and evaluate at new point\n            self._add_update(t, d)\n            F_new = closure()\n            closure_eval += 1\n\n            # print info if debugging\n            if ls_debug:\n                print(\n                    ""LS Step: %d  t: %.8e  F(x+td): %.8e  F-c1*t*g*d: %.8e  F(x): %.8e""\n                    % (ls_step, t, F_new, F_k + c1 * t * gtd, F_k)\n                )\n\n            # check Armijo condition\n            while F_new > F_k + c1 * t * gtd or not is_legal(F_new):\n\n                # check if maximum number of iterations reached\n                if ls_step >= max_ls:\n                    if inplace:\n                        self._add_update(-t, d)\n                    else:\n                        self._load_params(current_params)\n\n                    t = 0\n                    F_new = closure()\n                    closure_eval += 1\n                    fail = True\n                    break\n\n                else:\n                    # store current steplength\n                    t_new = t\n\n                    # compute new steplength\n\n                    # if first step or not interpolating, then multiply by factor\n                    if ls_step == 0 or not interpolate or not is_legal(F_new):\n                        t = t / eta\n\n                    # if second step, use function value at new point along with\n                    # gradient and function at current iterate\n                    elif ls_step == 1 or not is_legal(F_prev):\n                        t = polyinterp(np.array([[0, F_k.item(), gtd.item()], [t_new, F_new.item(), np.nan]]))\n\n                    # otherwise, use function values at new point, previous point,\n                    # and gradient and function at current iterate\n                    else:\n                        t = polyinterp(\n                            np.array(\n                                [\n                                    [0, F_k.item(), gtd.item()],\n                                    [t_new, F_new.item(), np.nan],\n                                    [t_prev, F_prev.item(), np.nan],\n                                ]\n                            )\n                        )\n\n                    # if values are too extreme, adjust t\n                    if interpolate:\n                        if t < 1e-3 * t_new:\n                            t = 1e-3 * t_new\n                        elif t > 0.6 * t_new:\n                            t = 0.6 * t_new\n\n                        # store old point\n                        F_prev = F_new\n                        t_prev = t_new\n\n                    # update iterate and reevaluate\n                    if inplace:\n                        self._add_update(t - t_new, d)\n                    else:\n                        self._load_params(current_params)\n                        self._add_update(t, d)\n\n                    F_new = closure()\n                    closure_eval += 1\n                    ls_step += 1  # iterate\n\n                    # print info if debugging\n                    if ls_debug:\n                        print(\n                            ""LS Step: %d  t: %.8e  F(x+td):   %.8e  F-c1*t*g*d: %.8e  F(x): %.8e""\n                            % (ls_step, t, F_new, F_k + c1 * t * gtd, F_k)\n                        )\n\n            # store Bs\n            if Bs is None:\n                Bs = (g_Sk.mul(-t)).clone()\n            else:\n                Bs.copy_(g_Sk.mul(-t))\n\n            # print final steplength\n            if ls_debug:\n                print(""Final Steplength:"", t)\n                print(\n                    ""===================================== End Armijo line search ====================================""\n                )\n\n            state[""d""] = d\n            state[""prev_flat_grad""] = prev_flat_grad\n            state[""t""] = t\n            state[""Bs""] = Bs\n            state[""fail""] = fail\n\n            return F_new, t, ls_step, closure_eval, desc_dir, fail\n\n        # perform weak Wolfe line search\n        elif line_search == ""Wolfe"":\n\n            # load options\n            if options:\n                if ""closure"" not in options.keys():\n                    raise (ValueError(""closure option not specified.""))\n                else:\n                    closure = options[""closure""]\n\n                if ""current_loss"" not in options.keys():\n                    F_k = closure()\n                    closure_eval += 1\n                else:\n                    F_k = options[""current_loss""]\n\n                if ""gtd"" not in options.keys():\n                    gtd = g_Ok.dot(d)\n                else:\n                    gtd = options[""gtd""]\n\n                if ""eta"" not in options.keys():\n                    eta = 2\n                elif options[""eta""] <= 1:\n                    raise (ValueError(""Invalid eta; must be greater than 1.""))\n                else:\n                    eta = options[""eta""]\n\n                if ""c1"" not in options.keys():\n                    c1 = 1e-4\n                elif options[""c1""] >= 1 or options[""c1""] <= 0:\n                    raise (ValueError(""Invalid c1; must be strictly between 0 and 1.""))\n                else:\n                    c1 = options[""c1""]\n\n                if ""c2"" not in options.keys():\n                    c2 = 0.9\n                elif options[""c2""] >= 1 or options[""c2""] <= 0:\n                    raise (ValueError(""Invalid c2; must be strictly between 0 and 1.""))\n                elif options[""c2""] <= c1:\n                    raise (ValueError(""Invalid c2; must be strictly larger than c1.""))\n                else:\n                    c2 = options[""c2""]\n\n                if ""max_ls"" not in options.keys():\n                    max_ls = 10\n                elif options[""max_ls""] <= 0:\n                    raise (ValueError(""Invalid max_ls; must be positive.""))\n                else:\n                    max_ls = options[""max_ls""]\n\n                if ""interpolate"" not in options.keys():\n                    interpolate = True\n                else:\n                    interpolate = options[""interpolate""]\n\n                if ""inplace"" not in options.keys():\n                    inplace = True\n                else:\n                    inplace = options[""inplace""]\n\n                if ""ls_debug"" not in options.keys():\n                    ls_debug = False\n                else:\n                    ls_debug = options[""ls_debug""]\n\n            else:\n                raise (ValueError(""Options are not specified; need closure evaluating function.""))\n\n            # initialize counters\n            ls_step = 0\n            grad_eval = 0  # tracks gradient evaluations\n            t_prev = 0  # old steplength\n\n            # initialize bracketing variables and flag\n            alpha = 0\n            beta = float(""Inf"")\n            fail = False\n\n            # initialize values for line search\n            if interpolate:\n                F_a = F_k\n                g_a = gtd\n\n                if torch.cuda.is_available():\n                    F_b = torch.tensor(np.nan, dtype=dtype).cuda()\n                    g_b = torch.tensor(np.nan, dtype=dtype).cuda()\n                else:\n                    F_b = torch.tensor(np.nan, dtype=dtype)\n                    g_b = torch.tensor(np.nan, dtype=dtype)\n\n            # begin print for debug mode\n            if ls_debug:\n                print(\n                    ""==================================== Begin Wolfe line search ====================================""\n                )\n                print(""F(x): %.8e  g*d: %.8e"" % (F_k, gtd))\n\n            # check if search direction is descent direction\n            if gtd >= 0:\n                desc_dir = False\n                if debug:\n                    print(""Not a descent direction!"")\n            else:\n                desc_dir = True\n\n            # store values if not in-place\n            if not inplace:\n                current_params = self._copy_params()\n\n            # update and evaluate at new point\n            self._add_update(t, d)\n            F_new = closure()\n            closure_eval += 1\n\n            # main loop\n            while True:\n\n                # check if maximum number of line search steps have been reached\n                if ls_step >= max_ls:\n                    if inplace:\n                        self._add_update(-t, d)\n                    else:\n                        self._load_params(current_params)\n\n                    t = 0\n                    F_new = closure()\n                    F_new.backward()\n                    g_new = self._gather_flat_grad()\n                    closure_eval += 1\n                    grad_eval += 1\n                    fail = True\n                    break\n\n                # print info if debugging\n                if ls_debug:\n                    print(""LS Step: %d  t: %.8e  alpha: %.8e  beta: %.8e"" % (ls_step, t, alpha, beta))\n                    print(""Armijo:  F(x+td): %.8e  F-c1*t*g*d: %.8e  F(x): %.8e"" % (F_new, F_k + c1 * t * gtd, F_k))\n\n                # check Armijo condition\n                if F_new > F_k + c1 * t * gtd:\n\n                    # set upper bound\n                    beta = t\n                    t_prev = t\n\n                    # update interpolation quantities\n                    if interpolate:\n                        F_b = F_new\n                        if torch.cuda.is_available():\n                            g_b = torch.tensor(np.nan, dtype=dtype).cuda()\n                        else:\n                            g_b = torch.tensor(np.nan, dtype=dtype)\n\n                else:\n\n                    # compute gradient\n                    F_new.backward()\n                    g_new = self._gather_flat_grad()\n                    grad_eval += 1\n                    gtd_new = g_new.dot(d)\n\n                    # print info if debugging\n                    if ls_debug:\n                        print(""Wolfe: g(x+td)*d: %.8e  c2*g*d: %.8e  gtd: %.8e"" % (gtd_new, c2 * gtd, gtd))\n\n                    # check curvature condition\n                    if gtd_new < c2 * gtd:\n\n                        # set lower bound\n                        alpha = t\n                        t_prev = t\n\n                        # update interpolation quantities\n                        if interpolate:\n                            F_a = F_new\n                            g_a = gtd_new\n\n                    else:\n                        break\n\n                # compute new steplength\n\n                # if first step or not interpolating, then bisect or multiply by factor\n                if not interpolate or not is_legal(F_b):\n                    if beta == float(""Inf""):\n                        t = eta * t\n                    else:\n                        t = (alpha + beta) / 2.0\n\n                # otherwise interpolate between a and b\n                else:\n                    t = polyinterp(np.array([[alpha, F_a.item(), g_a.item()], [beta, F_b.item(), g_b.item()]]))\n\n                    # if values are too extreme, adjust t\n                    if beta == float(""Inf""):\n                        if t > 2 * eta * t_prev:\n                            t = 2 * eta * t_prev\n                        elif t < eta * t_prev:\n                            t = eta * t_prev\n                    else:\n                        if t < alpha + 0.2 * (beta - alpha):\n                            t = alpha + 0.2 * (beta - alpha)\n                        elif t > (beta - alpha) / 2.0:\n                            t = (beta - alpha) / 2.0\n\n                    # if we obtain nonsensical value from interpolation\n                    if t <= 0:\n                        t = (beta - alpha) / 2.0\n\n                # update parameters\n                if inplace:\n                    self._add_update(t - t_prev, d)\n                else:\n                    self._load_params(current_params)\n                    self._add_update(t, d)\n\n                # evaluate closure\n                F_new = closure()\n                closure_eval += 1\n                ls_step += 1\n\n            # store Bs\n            if Bs is None:\n                Bs = (g_Sk.mul(-t)).clone()\n            else:\n                Bs.copy_(g_Sk.mul(-t))\n\n            # print final steplength\n            if ls_debug:\n                print(""Final Steplength:"", t)\n                print(\n                    ""===================================== End Wolfe line search =====================================""\n                )\n\n            state[""d""] = d\n            state[""prev_flat_grad""] = prev_flat_grad\n            state[""t""] = t\n            state[""Bs""] = Bs\n            state[""fail""] = fail\n\n            return F_new, g_new, t, ls_step, closure_eval, grad_eval, desc_dir, fail\n\n        else:\n\n            # perform update\n            self._add_update(t, d)\n\n            # store Bs\n            if Bs is None:\n                Bs = (g_Sk.mul(-t)).clone()\n            else:\n                Bs.copy_(g_Sk.mul(-t))\n\n            state[""d""] = d\n            state[""prev_flat_grad""] = prev_flat_grad\n            state[""t""] = t\n            state[""Bs""] = Bs\n            state[""fail""] = False\n\n            return t\n\n    def step(self, p_k, g_Ok, g_Sk=None, options={}):\n        return self._step(p_k, g_Ok, g_Sk, options)\n\n\n#%% Full-Batch (Deterministic) L-BFGS Optimizer (Wrapper)\n\n\nclass FullBatchLBFGS(LBFGS):\n    """"""\n    Implements full-batch or deterministic L-BFGS algorithm. Compatible with\n    Powell damping. Can be used when evaluating a deterministic function and\n    gradient. Wraps the LBFGS optimizer. Performs the two-loop recursion,\n    updating, and curvature updating in a single step.\n\n    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n    Last edited 11/15/18.\n\n    Warnings:\n      . Does not support per-parameter options and parameter groups.\n      . All parameters have to be on a single device.\n\n    Inputs:\n        lr (float): steplength or learning rate (default: 1)\n        history_size (int): update history size (default: 10)\n        line_search (str): designates line search to use (default: \'Wolfe\')\n            Options:\n                \'None\': uses steplength designated in algorithm\n                \'Armijo\': uses Armijo backtracking line search\n                \'Wolfe\': uses Armijo-Wolfe bracketing line search\n        dtype: data type (default: torch.float)\n        debug (bool): debugging mode\n\n    """"""\n\n    def __init__(self, params, lr=1, history_size=10, line_search=""Wolfe"", dtype=torch.float, debug=False):\n        super(FullBatchLBFGS, self).__init__(params, lr, history_size, line_search, dtype, debug)\n\n    def step(self, options={}):\n        """"""\n        Performs a single optimization step.\n\n        Inputs:\n            options (dict): contains options for performing line search\n\n        General Options:\n            \'eps\' (float): constant for curvature pair rejection or damping (default: 1e-2)\n            \'damping\' (bool): flag for using Powell damping (default: False)\n\n        Options for Armijo backtracking line search:\n            \'closure\' (callable): reevaluates model and returns function value\n            \'current_loss\' (tensor): objective value at current iterate (default: F(x_k))\n            \'gtd\' (tensor): inner product g_Ok\'d in line search (default: g_Ok\'d)\n            \'eta\' (tensor): factor for decreasing steplength > 0 (default: 2)\n            \'c1\' (tensor): sufficient decrease constant in (0, 1) (default: 1e-4)\n            \'max_ls\' (int): maximum number of line search steps permitted (default: 10)\n            \'interpolate\' (bool): flag for using interpolation (default: True)\n            \'inplace\' (bool): flag for inplace operations (default: True)\n            \'ls_debug\' (bool): debugging mode for line search\n\n        Options for Wolfe line search:\n            \'closure\' (callable): reevaluates model and returns function value\n            \'current_loss\' (tensor): objective value at current iterate (default: F(x_k))\n            \'gtd\' (tensor): inner product g_Ok\'d in line search (default: g_Ok\'d)\n            \'eta\' (float): factor for extrapolation (default: 2)\n            \'c1\' (float): sufficient decrease constant in (0, 1) (default: 1e-4)\n            \'c2\' (float): curvature condition constant in (0, 1) (default: 0.9)\n            \'max_ls\' (int): maximum number of line search steps permitted (default: 10)\n            \'interpolate\' (bool): flag for using interpolation (default: True)\n            \'inplace\' (bool): flag for inplace operations (default: True)\n            \'ls_debug\' (bool): debugging mode for line search\n\n        Outputs (depends on line search):\n          . No line search:\n                t (float): steplength\n          . Armijo backtracking line search:\n                F_new (tensor): loss function at new iterate\n                t (tensor): final steplength\n                ls_step (int): number of backtracks\n                closure_eval (int): number of closure evaluations\n                desc_dir (bool): descent direction flag\n                    True: p_k is descent direction with respect to the line search\n                    function\n                    False: p_k is not a descent direction with respect to the line\n                    search function\n                fail (bool): failure flag\n                    True: line search reached maximum number of iterations, failed\n                    False: line search succeeded\n          . Wolfe line search:\n                F_new (tensor): loss function at new iterate\n                g_new (tensor): gradient at new iterate\n                t (float): final steplength\n                ls_step (int): number of backtracks\n                closure_eval (int): number of closure evaluations\n                grad_eval (int): number of gradient evaluations\n                desc_dir (bool): descent direction flag\n                    True: p_k is descent direction with respect to the line search\n                    function\n                    False: p_k is not a descent direction with respect to the line\n                    search function\n                fail (bool): failure flag\n                    True: line search reached maximum number of iterations, failed\n                    False: line search succeeded\n\n        Notes:\n          . If encountering line search failure in the deterministic setting, one\n            should try increasing the maximum number of line search steps max_ls.\n\n        """"""\n\n        # load options for damping and eps\n        if ""damping"" not in options.keys():\n            damping = False\n        else:\n            damping = options[""damping""]\n\n        if ""eps"" not in options.keys():\n            eps = 1e-2\n        else:\n            eps = options[""eps""]\n\n        # gather gradient\n        grad = self._gather_flat_grad()\n\n        # update curvature if after 1st iteration\n        state = self.state[""global_state""]\n        if state[""n_iter""] > 0:\n            self.curvature_update(grad, eps, damping)\n\n        # compute search direction\n        p = self.two_loop_recursion(-grad)\n\n        # take step\n        return self._step(p, grad, options=options)\n'"
gpytorch/__init__.py,0,"b'#!/usr/bin/env python3\nfrom . import (\n    beta_features,\n    distributions,\n    kernels,\n    lazy,\n    likelihoods,\n    means,\n    mlls,\n    models,\n    priors,\n    settings,\n    utils,\n    variational,\n)\nfrom .functions import (  # Deprecated\n    add_diag,\n    add_jitter,\n    dsmm,\n    inv_matmul,\n    inv_quad,\n    inv_quad_logdet,\n    log_normal_cdf,\n    logdet,\n    matmul,\n    root_decomposition,\n    root_inv_decomposition,\n)\nfrom .lazy import cat, delazify, lazify\nfrom .mlls import ExactMarginalLogLikelihood\nfrom .module import Module\n\n__version__ = ""1.1.1""\n\n__all__ = [\n    # Submodules\n    ""distributions"",\n    ""kernels"",\n    ""lazy"",\n    ""likelihoods"",\n    ""means"",\n    ""mlls"",\n    ""models"",\n    ""priors"",\n    ""utils"",\n    ""variational"",\n    # Classes\n    ""Module"",\n    ""ExactMarginalLogLikelihood"",\n    # Functions\n    ""add_diag"",\n    ""add_jitter"",\n    ""cat"",\n    ""delazify"",\n    ""dsmm"",\n    ""inv_matmul"",\n    ""inv_quad"",\n    ""inv_quad_logdet"",\n    ""lazify"",\n    ""logdet"",\n    ""log_normal_cdf"",\n    ""matmul"",\n    ""root_decomposition"",\n    ""root_inv_decomposition"",\n    # Context managers\n    ""beta_features"",\n    ""settings"",\n    # Other\n    ""__version__"",\n]\n'"
gpytorch/beta_features.py,2,"b'#!/usr/bin/env python3\n\nimport warnings\n\nfrom .settings import _feature_flag, _value_context\n\n\nclass _moved_beta_feature(object):\n    def __init__(self, new_cls, orig_name=None):\n        self.new_cls = new_cls\n        self.orig_name = orig_name if orig_name is not None else ""gpytorch.settings.{}"".format(new_cls.__name__)\n\n    def __call__(self, *args, **kwargs):\n        warnings.warn(\n            ""`{}` has moved to `gpytorch.settings.{}`."".format(self.orig_name, self.new_cls.__name__),\n            DeprecationWarning,\n        )\n        return self.new_cls(*args, **kwargs)\n\n    def __getattr__(self, name):\n        return getattr(self.new_cls, name)\n\n\nclass checkpoint_kernel(_value_context):\n    """"""\n    Should the kernel be computed in chunks with checkpointing or not? (Default, no)\n\n    If `split_size = 0`:\n        The kernel is computed explicitly. During training, the kernel matrix is\n        kept in memory for the backward pass. This is the fastest option but the\n        most memory intensive.\n    If `split_size > 0`:\n        The kernel is never fully computed or stored. Instead, the kernel is only\n        accessed through matrix multiplication. The matrix multiplication is\n        computed in `segments` chunks. This is slower, but requires significantly less memory.\n\n    Default: 0\n    """"""\n\n    _global_value = 0\n\n\nclass default_preconditioner(_feature_flag):\n    """"""\n    Add a diagonal correction to scalable inducing point methods\n    """"""\n\n    pass\n\n\n__all__ = [""checkpoint_kernel"", ""default_preconditioner""]\n'"
gpytorch/module.py,10,"b'#!/usr/bin/env python3\n\nimport itertools\nfrom collections import OrderedDict\n\nimport torch\nfrom torch import nn\nfrom torch.distributions import Distribution\n\nfrom .constraints import Interval\nfrom .lazy import LazyTensor\nfrom .utils.deprecation import DeprecationError\n\n\nclass Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self._added_loss_terms = OrderedDict()\n        self._priors = OrderedDict()\n        self._constraints = OrderedDict()\n\n        self._strict_init = True\n        self._load_strict_shapes = True\n\n        self._register_load_state_dict_pre_hook(self._load_state_hook_ignore_shapes)\n\n    def __call__(self, *inputs, **kwargs):\n        outputs = self.forward(*inputs, **kwargs)\n        if isinstance(outputs, list):\n            return [_validate_module_outputs(output) for output in outputs]\n        return _validate_module_outputs(outputs)\n\n    def _get_module_and_name(self, parameter_name):\n        """"""Get module and name from full parameter name.""""""\n        module, name = parameter_name.split(""."", 1)\n        if module in self._modules:\n            return self.__getattr__(module), name\n        else:\n            raise AttributeError(\n                ""Invalid parameter name {}. {} has no module {}"".format(parameter_name, type(self).__name__, module)\n            )\n\n    def _strict(self, value):\n        _set_strict(self, value)\n\n    def added_loss_terms(self):\n        for _, strategy in self.named_added_loss_terms():\n            yield strategy\n\n    def forward(self, *inputs, **kwargs):\n        raise NotImplementedError\n\n    def constraints(self):\n        for _, constraint in self.named_constraints():\n            yield constraint\n\n    def hyperparameters(self):\n        for _, param in self.named_hyperparameters():\n            yield param\n\n    def initialize(self, **kwargs):\n        """"""\n        Set a value for a parameter\n\n        kwargs: (param_name, value) - parameter to initialize.\n        Can also initialize recursively by passing in the full name of a\n        parameter. For example if model has attribute model.likelihood,\n        we can initialize the noise with either\n        `model.initialize(**{\'likelihood.noise\': 0.1})`\n        or\n        `model.likelihood.initialize(noise=0.1)`.\n        The former method would allow users to more easily store the\n        initialization values as one object.\n\n        Value can take the form of a tensor, a float, or an int\n        """"""\n\n        for name, val in kwargs.items():\n            if isinstance(val, int):\n                val = float(val)\n            if ""."" in name:\n                module, name = self._get_module_and_name(name)\n                module.initialize(**{name: val})\n            elif not hasattr(self, name):\n                raise AttributeError(""Unknown parameter {p} for {c}"".format(p=name, c=self.__class__.__name__))\n            elif name not in self._parameters and name not in self._buffers:\n                setattr(self, name, val)\n            elif torch.is_tensor(val):\n                constraint = self.constraint_for_parameter_name(name)\n                if constraint is not None and constraint.enforced and not constraint.check_raw(val):\n                    raise RuntimeError(\n                        ""Attempting to manually set a parameter value that is out of bounds of ""\n                        f""its current constraints, {constraint}. ""\n                        ""Most likely, you want to do the following:\\n likelihood = GaussianLikelihood""\n                        ""(noise_constraint=gpytorch.constraints.GreaterThan(better_lower_bound))""\n                    )\n                try:\n                    self.__getattr__(name).data.copy_(val.expand_as(self.__getattr__(name)))\n                except RuntimeError:\n                    if not self._strict_init:\n                        self.__getattr__(name).data = val\n                    else:\n                        self.__getattr__(name).data.copy_(val.view_as(self.__getattr__(name)))\n\n            elif isinstance(val, float):\n                constraint = self.constraint_for_parameter_name(name)\n                if constraint is not None and not constraint.check_raw(val):\n                    raise RuntimeError(\n                        ""Attempting to manually set a parameter value that is out of bounds of ""\n                        f""its current constraints, {constraint}. ""\n                        ""Most likely, you want to do the following:\\n likelihood = GaussianLikelihood""\n                        ""(noise_constraint=gpytorch.constraints.GreaterThan(better_lower_bound))""\n                    )\n                self.__getattr__(name).data.fill_(val)\n            else:\n                raise AttributeError(""Type {t} not valid for initializing parameter {p}"".format(t=type(val), p=name))\n\n            # Ensure value is contained in support of prior (if present)\n            prior_name = ""_"".join([name, ""prior""])\n            if prior_name in self._priors:\n                prior, closure, _ = self._priors[prior_name]\n                try:\n                    prior._validate_sample(closure())\n                except ValueError as e:\n                    raise ValueError(""Invalid input value for prior {}. Error:\\n{}"".format(prior_name, e))\n\n        return self\n\n    def named_added_loss_terms(self):\n        """"""Returns an iterator over module variational strategies, yielding both\n        the name of the variational strategy as well as the strategy itself.\n\n        Yields:\n            (string, VariationalStrategy): Tuple containing the name of the\n                strategy and the strategy\n\n        """"""\n        return _extract_named_added_loss_terms(module=self, memo=None, prefix="""")\n\n    def named_hyperparameters(self):\n        for name, param in self.named_parameters():\n            if ""variational_"" not in name:\n                yield name, param\n\n    def named_priors(self, memo=None, prefix=""""):\n        """"""Returns an iterator over the module\'s priors, yielding the name of the prior,\n        the prior, the associated parameter names, and the transformation callable.\n\n        Yields:\n            (string, Prior, tuple((Parameter, callable)), callable): Tuple containing:\n                - the name of the prior\n                - the prior\n                - a tuple of tuples (param, transform), one for each of the parameters associated with the prior\n                - the prior\'s transform to be called on the parameters\n        """"""\n        return _extract_named_priors(module=self, memo=None, prefix="""")\n\n    def named_constraints(self, memo=None, prefix=""""):\n        return _extract_named_constraints(module=self, memo=None, prefix="""")\n\n    def named_variational_parameters(self):\n        for name, param in self.named_parameters():\n            if ""variational_"" in name:\n                yield name, param\n\n    def register_added_loss_term(self, name):\n        self._added_loss_terms[name] = None\n\n    def register_parameter(self, name, parameter, prior=None):\n        r""""""\n        Adds a parameter to the module. The parameter can be accessed as an attribute using the given name.\n\n        Args:\n            :attr:`name` (str):\n                The name of the parameter\n            :attr:`parameter` (torch.nn.Parameter):\n                The parameter\n        """"""\n        if prior is not None:\n            raise DeprecationError(\n                ""Setting a prior upon registering a parameter is deprecated. Please use ""\n                "".register_prior(\'{name}_prior\', prior, \'{name}\') instead."".format(name=name)\n            )\n        if ""_parameters"" not in self.__dict__:\n            raise AttributeError(""Cannot assign parameter before Module.__init__() call"")\n        super().register_parameter(name, parameter)\n\n    def register_prior(self, name, prior, param_or_closure, setting_closure=None):\n        """"""\n        Adds a prior to the module. The prior can be accessed as an attribute using the given name.\n\n        Args:\n            :attr:`name` (str):\n                The name of the prior\n            :attr:`prior` (Prior):\n                The prior to be registered`\n            :attr:`param_or_closure` (string or callable):\n                Either the name of the parameter, or a closure (which upon calling evalutes a function on\n                one or more parameters):\n                single parameter without a transform: `.register_prior(""foo_prior"", foo_prior, ""foo_param"")`\n                transform a single parameter (e.g. put a log-Normal prior on it):\n                `.register_prior(""foo_prior"", NormalPrior(0, 1), lambda: torch.log(self.foo_param))`\n                function of multiple parameters:\n                `.register_prior(""foo2_prior"", foo2_prior, lambda: f(self.param1, self.param2)))`\n            :attr:`setting_closure` (callable, optional):\n                A function taking in a tensor in (transformed) parameter space and initializing the\n                internal parameter representation to the proper value by applying the inverse transform.\n                Enables setting parametres directly in the transformed space, as well as sampling\n                parameter values from priors (see `sample_from_prior`)\n\n        """"""\n        if isinstance(param_or_closure, str):\n            if param_or_closure not in self._parameters and not hasattr(self, param_or_closure):\n                raise AttributeError(\n                    ""Unknown parameter {name} for {module}"".format(\n                        name=param_or_closure, module=self.__class__.__name__\n                    )\n                    + "" Make sure the parameter is registered before registering a prior.""\n                )\n\n            def closure():\n                return getattr(self, param_or_closure)\n\n            if setting_closure is not None:\n                raise RuntimeError(""Must specify a closure instead of a parameter name when providing setting_closure"")\n\n            def setting_closure(val):\n                return self.initialize(**{param_or_closure: val})\n\n        else:\n            closure = param_or_closure\n        self.add_module(name, prior)\n        self._priors[name] = (prior, closure, setting_closure)\n\n    def register_constraint(self, param_name, constraint, replace=True):\n        if param_name not in self._parameters:\n            raise RuntimeError(""Attempting to register constraint for nonexistent parameter."")\n\n        constraint_name = param_name + ""_constraint""\n        if constraint_name in self._constraints:\n            current_constraint = self._constraints[constraint_name]\n        else:\n            current_constraint = None\n\n        if isinstance(current_constraint, Interval) and not replace:\n            new_constraint = constraint.intersect(current_constraint)\n        else:\n            new_constraint = constraint\n\n        self.add_module(constraint_name, new_constraint)\n        self._constraints[constraint_name] = new_constraint\n\n        # re-initialize the parameter if the constraint specifies an initial value\n        if new_constraint.initial_value is not None:\n            self.initialize(**{param_name: new_constraint.initial_value})\n\n    def constraint_for_parameter_name(self, param_name):\n        base_module = self\n        base_name = param_name\n\n        while ""."" in base_name:\n            components = base_name.split(""."")\n            submodule_name = components[0]\n            submodule = getattr(base_module, submodule_name)\n\n            base_module = submodule\n            base_name = ""."".join(components[1:])\n\n        try:\n            constraint_name = base_name + ""_constraint""\n            return base_module._constraints.get(constraint_name)\n        except AttributeError:  # submodule may not always be a gpytorch module\n            return None\n\n    def _load_state_hook_ignore_shapes(\n        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n    ):\n        if not self._load_strict_shapes:\n            local_name_params = itertools.chain(self._parameters.items(), self._buffers.items())\n            local_state = {k: v for k, v in local_name_params if v is not None}\n\n            for name, param in local_state.items():\n                key = prefix + name\n                if key in state_dict:\n                    param.data = state_dict[key].data\n\n    def load_strict_shapes(self, value):\n        def apply_fn(module):\n            module._load_strict_shapes = value\n\n        self.apply(apply_fn)\n\n    def named_parameters_and_constraints(self):\n        for name, param in self.named_parameters():\n            yield name, param, self.constraint_for_parameter_name(name)\n\n    def sample_from_prior(self, prior_name):\n        """"""Sample parameter values from prior. Modifies the module\'s parameters in-place.""""""\n        if prior_name not in self._priors:\n            raise RuntimeError(""Unknown prior name \'{}\'"".format(prior_name))\n        prior, _, setting_closure = self._priors[prior_name]\n        if setting_closure is None:\n            raise RuntimeError(""Must provide inverse transform to be able to sample from prior."")\n        setting_closure(prior.sample())\n\n    def pyro_sample_from_prior(self):\n        """"""\n        For each parameter in this Module and submodule that have defined priors, sample a value for that parameter\n        from its corresponding prior with a pyro.sample primitive and load the resulting value in to the parameter.\n\n        This method can be used in a Pyro model to conveniently define pyro sample sites for all\n        parameters of the model that have GPyTorch priors registered to them.\n        """"""\n        return _pyro_sample_from_prior(module=self, memo=None, prefix="""")\n\n    def local_load_samples(self, samples_dict, memo, prefix):\n        """"""\n        Defines local behavior of this Module when loading parameters from a samples_dict generated by a Pyro\n        sampling mechanism.\n\n        The default behavior here should almost always be called from any overriding class. However, a class may\n        want to add additional functionality, such as reshaping things to account for the fact that parameters will\n        acquire an extra batch dimension corresponding to the number of samples drawn.\n        """"""\n        self._strict(False)\n        for name, (prior, closure, setting_closure) in self._priors.items():\n            if prior is not None and prior not in memo:\n                memo.add(prior)\n                setting_closure(samples_dict[prefix + (""."" if prefix else """") + name])\n        self._strict(True)\n\n    def pyro_load_from_samples(self, samples_dict):\n        """"""\n        Convert this Module in to a batch Module by loading parameters from the given `samples_dict`. `samples_dict`\n        is typically produced by a Pyro sampling mechanism.\n\n        Note that the keys of the samples_dict should correspond to prior names (covar_module.outputscale_prior) rather\n        than parameter names (covar_module.raw_outputscale), because we will use the setting_closure associated with\n        the prior to properly set the unconstrained parameter.\n\n        Args:\n            :attr:`samples_dict` (dict): Dictionary mapping *prior names* to sample values.\n        """"""\n        return _pyro_load_from_samples(module=self, samples_dict=samples_dict, memo=None, prefix="""")\n\n    def update_added_loss_term(self, name, added_loss_term):\n        from .mlls import AddedLossTerm\n\n        if not isinstance(added_loss_term, AddedLossTerm):\n            raise RuntimeError(""added_loss_term must be a AddedLossTerm"")\n        if name not in self._added_loss_terms.keys():\n            raise RuntimeError(""added_loss_term {} not registered"".format(name))\n        self._added_loss_terms[name] = added_loss_term\n\n    def variational_parameters(self):\n        for _, param in self.named_variational_parameters():\n            yield param\n\n    def __getattr__(self, name):\n        try:\n            return super().__getattr__(name)\n        except AttributeError as e:\n            try:\n                return super().__getattribute__(name)\n            except AttributeError:\n                raise e\n\n\ndef _validate_module_outputs(outputs):\n    if isinstance(outputs, tuple):\n        if not all(\n            torch.is_tensor(output) or isinstance(output, Distribution) or isinstance(output, LazyTensor)\n            for output in outputs\n        ):\n            raise RuntimeError(\n                ""All outputs must be a Distribution, torch.Tensor, or LazyTensor. ""\n                ""Got {}"".format([output.__class__.__name__ for output in outputs])\n            )\n        if len(outputs) == 1:\n            outputs = outputs[0]\n        return outputs\n    elif torch.is_tensor(outputs) or isinstance(outputs, Distribution) or isinstance(outputs, LazyTensor):\n        return outputs\n    else:\n        raise RuntimeError(\n            ""Output must be a Distribution, torch.Tensor, or LazyTensor. Got {}"".format(outputs.__class__.__name__)\n        )\n\n\ndef _set_strict(module, value, memo=None):\n    if memo is None:\n        memo = set()\n\n    if hasattr(module, ""_strict_init""):\n        module._strict_init = value\n\n    for mname, module_ in module.named_children():\n        _set_strict(module_, value)\n\n\ndef _pyro_sample_from_prior(module, memo=None, prefix=""""):\n    try:\n        import pyro\n    except ImportError:\n        raise RuntimeError(""Cannot call pyro_sample_from_prior without pyro installed!"")\n    if memo is None:\n        memo = set()\n    if hasattr(module, ""_priors""):\n        for prior_name, (prior, closure, setting_closure) in module._priors.items():\n            if prior is not None and prior not in memo:\n                if setting_closure is None:\n                    raise RuntimeError(\n                        ""Cannot use Pyro for sampling without a setting_closure for each prior,""\n                        f"" but the following prior had none: {prior_name}, {prior}.""\n                    )\n                memo.add(prior)\n                prior = prior.expand(closure().shape)\n                value = pyro.sample(prefix + (""."" if prefix else """") + prior_name, prior)\n                setting_closure(value)\n\n    for mname, module_ in module.named_children():\n        submodule_prefix = prefix + (""."" if prefix else """") + mname\n        _pyro_sample_from_prior(module=module_, memo=memo, prefix=submodule_prefix)\n\n\ndef _pyro_load_from_samples(module, samples_dict, memo=None, prefix=""""):\n    if memo is None:\n        memo = set()\n    if hasattr(module, ""_priors""):\n        module.local_load_samples(samples_dict, memo, prefix)\n\n    for mname, module_ in module.named_children():\n        submodule_prefix = prefix + (""."" if prefix else """") + mname\n        _pyro_load_from_samples(module_, samples_dict, memo=memo, prefix=submodule_prefix)\n\n\ndef _extract_named_added_loss_terms(module, memo=None, prefix=""""):\n    if memo is None:\n        memo = set()\n    if hasattr(module, ""_added_loss_terms""):\n        for name, strategy in module._added_loss_terms.items():\n            if strategy is not None and strategy not in memo:\n                memo.add(strategy)\n                yield prefix + (""."" if prefix else """") + name, strategy\n    for mname, module_ in module.named_children():\n        submodule_prefix = prefix + (""."" if prefix else """") + mname\n        for name, strategy in _extract_named_added_loss_terms(module=module_, memo=memo, prefix=submodule_prefix):\n            yield name, strategy\n\n\ndef _extract_named_priors(module, memo=None, prefix=""""):\n    if memo is None:\n        memo = set()\n    if hasattr(module, ""_priors""):\n        for name, (prior, closure, inv_closure) in module._priors.items():\n            if prior is not None and prior not in memo:\n                memo.add(prior)\n                full_name = (""."" if prefix else """").join([prefix, name])\n                yield full_name, prior, closure, inv_closure\n    for mname, module_ in module.named_children():\n        submodule_prefix = prefix + (""."" if prefix else """") + mname\n        for name, prior, closure, inv_closure in _extract_named_priors(module_, memo=memo, prefix=submodule_prefix):\n            yield name, prior, closure, inv_closure\n\n\ndef _extract_named_constraints(module, memo=None, prefix=""""):\n    if memo is None:\n        memo = set()\n    if hasattr(module, ""_constraints""):\n        for name, constraint in module._constraints.items():\n            if constraint is not None and constraint not in memo:\n                memo.add(constraint)\n                full_name = (""."" if prefix else """").join([prefix, name])\n                yield full_name, constraint\n    for mname, module_ in module.named_children():\n        submodule_prefix = prefix + (""."" if prefix else """") + mname\n        for name, constraint in _extract_named_constraints(module_, memo=memo, prefix=submodule_prefix):\n            yield name, constraint\n'"
gpytorch/settings.py,11,"b'#!/usr/bin/env python3\n\n\nclass _feature_flag(object):\n    _state = False\n\n    @classmethod\n    def on(cls):\n        return cls._state\n\n    @classmethod\n    def off(cls):\n        return not cls._state\n\n    @classmethod\n    def _set_state(cls, state):\n        cls._state = state\n\n    def __init__(self, state=True):\n        self.prev = self.__class__.on()\n        self.state = state\n\n    def __enter__(self):\n        self.__class__._set_state(self.state)\n\n    def __exit__(self, *args):\n        self.__class__._set_state(self.prev)\n        return False\n\n\nclass _value_context(object):\n    _global_value = None\n\n    @classmethod\n    def value(cls):\n        return cls._global_value\n\n    @classmethod\n    def _set_value(cls, value):\n        cls._global_value = value\n\n    def __init__(self, value):\n        self._orig_value = self.__class__.value()\n        self._instance_value = value\n\n    def __enter__(self,):\n        self.__class__._set_value(self._instance_value)\n\n    def __exit__(self, *args):\n        self.__class__._set_value(self._orig_value)\n        return False\n\n\nclass _fast_covar_root_decomposition(_feature_flag):\n    r""""""\n    This feature flag controls how matrix root decompositions (:math:`K = L L^\\top`) are computed\n    (e.g. for sampling, computing caches, etc.).\n\n    If set to True, covariance matrices :math:`K` are decomposed with low-rank approximations :math:`L L^\\top`,\n    (:math:`L \\in \\mathbb R^{n \\times k}`) using the Lanczos algorithm.\n    This is faster for large matrices and exploits structure in the covariance matrix if applicable.\n\n    If set to False, covariance matrices :math:`K` are decomposed using the Cholesky decomposition.\n\n    .. warning ::\n\n        Setting this to False will compute a complete Cholesky decomposition of covariance matrices.\n        This may be infeasible for GPs with structure covariance matrices.\n\n    See also: :class:`gpytorch.settings.max_root_decomposition_size` (to control the\n    size of the low rank decomposition used).\n    """"""\n\n    _state = True\n\n\nclass _fast_log_prob(_feature_flag):\n    r""""""\n    This feature flag controls how to compute the marginal log likelihood of exact GPs\n    and the log probability of multivariate normal distributions.\n\n    If set to True, log_prob is computed using a modified conjugate gradients algorithm (as\n    described in `GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration`_.\n    This is a stochastic computation, but it is much faster for large matrices\n    and exploits structure in the covariance matrix if applicable.\n\n    If set to False, `log_prob` is computed using the Cholesky decomposition.\n\n    .. warning ::\n\n        Setting this to False will compute a complete Cholesky decomposition of covariance matrices.\n        This may be infeasible for GPs with structure covariance matrices.\n\n    See also: :class:`gpytorch.settings.num_trace_samples` (to control the\n    stochasticity of the fast `log_prob` estimates).\n\n    .. _GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration:\n        https://arxiv.org/pdf/1809.11165.pdf\n    """"""\n\n    _state = True\n\n\nclass _fast_solves(_feature_flag):\n    r""""""\n    This feature flag controls how to compute solves with positive definite matrices.\n    If set to True, solves are computed using preconditioned conjugate gradients.\n    If set to False, `log_prob` is computed using the Cholesky decomposition.\n\n    .. warning ::\n\n        Setting this to False will compute a complete Cholesky decomposition of covariance matrices.\n        This may be infeasible for GPs with structure covariance matrices.\n    """"""\n\n    _state = True\n\n\nclass skip_posterior_variances(_feature_flag):\n    """"""\n    Whether or not to skip the posterior covariance matrix when doing an ExactGP\n    forward pass. If this is on, the returned gpytorch MultivariateNormal will have a\n    ZeroLazyTensor as its covariance matrix. This allows gpytorch to not compute\n    the covariance matrix when it is not needed, speeding up computations.\n    """"""\n\n    _state = False\n\n\nclass detach_test_caches(_feature_flag):\n    """"""\n    Whether or not to detach caches computed for making predictions. In most cases, you will want this,\n    as this will speed up derivative computations of the predictions with respect to test inputs. However,\n    if you also need derivatives with respect to training inputs (e.g., because you have fantasy observations),\n    then you must disable this.\n    """"""\n\n    _state = True\n\n\nclass deterministic_probes(_feature_flag):\n    """"""\n    Whether or not to resample probe vectors every iteration of training. If True, we use the same set of probe vectors\n    for computing log determinants each iteration. This introduces small amounts of bias in to the MLL, but allows us\n    to compute a deterministic estimate of it which makes optimizers like L-BFGS more viable choices.\n\n    NOTE: Currently, probe vectors are cached in a global scope. Therefore, this setting cannot be used\n    if multiple independent GP models are being trained in the same context (i.e., it works fine with a single GP model)\n    """"""\n\n    _state = False\n    probe_vectors = None\n\n    @classmethod\n    def _set_state(cls, state):\n        cls._state = state\n        cls.probe_vectors = None\n\n\nclass debug(_feature_flag):\n    """"""\n    Whether or not to perform ""safety"" checks on the supplied data.\n    (For example, that the correct training data is supplied in Exact GP training mode)\n    Pros: fewer data checks, fewer warning messages\n    Cons: possibility of supplying incorrect data, model accidentially in wrong mode\n    """"""\n\n    _state = True\n\n\nclass fast_pred_var(_feature_flag):\n    """"""\n    Fast predictive variances using Lanczos Variance Estimates (LOVE)\n    Use this for improved performance when computing predictive variances.\n\n    As described in the paper:\n\n    `Constant-Time Predictive Distributions for Gaussian Processes`_.\n\n    See also: :class:`gpytorch.settings.max_root_decomposition_size` (to control the\n    size of the low rank decomposition used for variance estimates).\n\n    .. _`Constant-Time Predictive Distributions for Gaussian Processes`:\n        https://arxiv.org/pdf/1803.06058.pdf\n    """"""\n\n    _num_probe_vectors = 1\n\n    @classmethod\n    def num_probe_vectors(cls):\n        return cls._num_probe_vectors\n\n    @classmethod\n    def _set_num_probe_vectors(cls, value):\n        cls._num_probe_vectors = value\n\n    def __init__(self, state=True, num_probe_vectors=1):\n        self.orig_value = self.__class__.num_probe_vectors()\n        self.value = num_probe_vectors\n        super(fast_pred_var, self).__init__(state)\n\n    def __enter__(self):\n        self.__class__._set_num_probe_vectors(self.value)\n        super(fast_pred_var, self).__enter__()\n\n    def __exit__(self, *args):\n        self.__class__._set_num_probe_vectors(self.orig_value)\n        return super(fast_pred_var, self).__exit__()\n\n\nclass fast_pred_samples(_feature_flag):\n    """"""\n    Fast predictive samples using Lanczos Variance Estimates (LOVE).\n    Use this for improved performance when sampling from a predictive posterior matrix.\n\n    As described in the paper:\n\n    `Constant-Time Predictive Distributions for Gaussian Processes`_.\n\n    See also: :class:`gpytorch.settings.max_root_decomposition_size` (to control the\n    size of the low rank decomposition used for samples).\n\n    .. _`Constant-Time Predictive Distributions for Gaussian Processes`:\n        https://arxiv.org/pdf/1803.06058.pdf\n    """"""\n\n    pass\n\n\nclass fast_computations(object):\n    r""""""\n    This feature flag controls whether or not to use fast approximations to various mathematical\n    functions used in GP inference.\n    The functions that can be controlled are:\n\n    * :attr:`covar_root_decomposition`\n        This feature flag controls how matrix root decompositions\n        (:math:`K = L L^\\top`) are computed (e.g. for sampling, computing caches, etc.).\n\n        * If set to True,\n            covariance matrices :math:`K` are decomposed with low-rank approximations :math:`L L^\\top`,\n            (:math:`L \\in \\mathbb R^{n \\times k}`) using the Lanczos algorithm.\n            This is faster for large matrices and exploits structure in the covariance matrix if applicable.\n\n        * If set to False,\n            covariance matrices :math:`K` are decomposed using the Cholesky decomposition.\n\n    * :attr:`log_prob`\n        This feature flag controls how GPyTorch computes the marginal log likelihood for exact GPs\n        and `log_prob` for multivariate normal distributions\n\n        * If set to True,\n            `log_prob` is computed using a modified conjugate gradients algorithm (as\n            described in `GPyTorch Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration`_.\n            This is a stochastic computation, but it is much faster for large matrices\n            and exploits structure in the covariance matrix if applicable.\n\n        * If set to False,\n            `log_prob` is computed using the Cholesky decomposition.\n\n    * :attr:`fast_solves`\n        This feature flag controls how GPyTorch computes the solves of positive-definite matrices.\n\n        * If set to True,\n            Solves are computed with preconditioned conjugate gradients.\n\n        * If set to False,\n            Solves are computed using the Cholesky decomposition.\n\n    .. warning ::\n\n        Setting this to False will compute a complete Cholesky decomposition of covariance matrices.\n        This may be infeasible for GPs with structure covariance matrices.\n\n    By default, approximations are used for all of these functions (except for solves).\n    Setting any of them to False will use exact computations instead.\n\n    See also:\n        * :class:`gpytorch.settings.max_root_decomposition_size`\n            (to control the size of the low rank decomposition used)\n        * :class:`gpytorch.settings.num_trace_samples`\n            (to control the stochasticity of the fast `log_prob` estimates)\n\n    .. _GPyTorch Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration:\n        https://arxiv.org/pdf/1809.11165.pdf\n    """"""\n    covar_root_decomposition = _fast_covar_root_decomposition\n    log_prob = _fast_log_prob\n    solves = _fast_solves\n\n    def __init__(self, covar_root_decomposition=True, log_prob=True, solves=True):\n        self.covar_root_decomposition = _fast_covar_root_decomposition(covar_root_decomposition)\n        self.log_prob = _fast_log_prob(log_prob)\n        self.solves = _fast_solves(solves)\n\n    def __enter__(self):\n        self.covar_root_decomposition.__enter__()\n        self.log_prob.__enter__()\n        self.solves.__enter__()\n\n    def __exit__(self, *args):\n        self.covar_root_decomposition.__exit__()\n        self.log_prob.__exit__()\n        self.solves.__exit__()\n        return False\n\n\nclass lazily_evaluate_kernels(_feature_flag):\n    """"""\n    Lazily compute the entries of covariance matrices (set to True by default).\n    This can result in memory and speed savings - if say cross covariance terms are not needed\n    or if you only need to compute variances (not covariances).\n\n    If set to False, gpytorch will always compute the entire covariance matrix between\n    training and test data.\n    """"""\n\n    _state = True\n\n\nclass max_eager_kernel_size(_value_context):\n    """"""\n    If the joint train/test covariance matrix is less than this size, then we will avoid as\n    much lazy evaluation of the kernel as possible.\n    Default: 512\n    """"""\n\n    _global_value = 512\n\n\nclass max_cg_iterations(_value_context):\n    """"""\n    The maximum number of conjugate gradient iterations to perform (when computing\n    matrix solves). A higher value rarely results in more accurate solves -- instead, lower the CG tolerance.\n    Default: 1000\n    """"""\n\n    _global_value = 1000\n\n\nclass cholesky_jitter(_value_context):\n    """"""\n    The jitter value passed to `psd_safe_cholesky` when using cholesky solves.\n    Default: None\n    """"""\n\n    _global_value = None\n\n\nclass cg_tolerance(_value_context):\n    """"""\n    Relative residual tolerance to use for terminating CG.\n\n    Default: 1\n    """"""\n\n    _global_value = 1\n\n\nclass preconditioner_tolerance(_value_context):\n    """"""\n    Diagonal trace tolerance to use for checking preconditioner convergence.\n\n    Default: 1e-3\n    """"""\n\n    _global_value = 1e-3\n\n\nclass eval_cg_tolerance(_value_context):\n    """"""\n    Relative residual tolerance to use for terminating CG when making predictions.\n\n    Default: 0.01\n    """"""\n\n    _global_value = 0.01\n\n\nclass _use_eval_tolerance(_feature_flag):\n    _state = False\n\n\nclass max_cholesky_size(_value_context):\n    """"""\n    If the size of of a LazyTensor is less than `max_cholesky_size`,\n    then `root_decomposition` and `inv_matmul` of LazyTensor will use Cholesky rather than Lanczos/CG.\n    Default: 800\n    """"""\n\n    _global_value = 800\n\n\nclass max_root_decomposition_size(_value_context):\n    """"""\n    The maximum number of Lanczos iterations to perform\n    This is used when 1) computing variance estiamtes 2) when drawing from MVNs,\n    or 3) for kernel multiplication\n    More values results in higher accuracy\n    Default: 100\n    """"""\n\n    _global_value = 100\n\n\nclass max_preconditioner_size(_value_context):\n    """"""\n    The maximum size of preconditioner to use. 0 corresponds to turning\n    preconditioning off. When enabled, usually a value of around ~10 works fairly well.\n    Default: 0\n    """"""\n\n    _global_value = 15\n\n\nclass max_lanczos_quadrature_iterations(_value_context):\n    r""""""\n    The maximum number of Lanczos iterations to perform when doing stochastic\n    Lanczos quadrature. This is ONLY used for log determinant calculations and\n    computing Tr(K^{-1}dK/d\\theta)\n    """"""\n\n    _global_value = 20\n\n\nclass memory_efficient(_feature_flag):\n    """"""\n    Whether or not to use Toeplitz math with gridded data, grid inducing point modules\n    Pros: memory efficient, faster on CPU\n    Cons: slower on GPUs with < 10000 inducing points\n    """"""\n\n    _state = False\n\n\nclass min_preconditioning_size(_value_context):\n    """"""\n    If the size of of a LazyTensor is less than `min_preconditioning_size`,\n    then we won\'t use pivoted Cholesky based preconditioning.\n\n    Default: 2000\n    """"""\n\n    _global_value = 2000\n\n\nclass num_likelihood_samples(_value_context):\n    """"""\n    The number of samples to draw from a latent GP when computing a likelihood\n    This is used in variational inference and training\n    Default: 10\n    """"""\n\n    _global_value = 10\n\n\nclass num_gauss_hermite_locs(_value_context):\n    """"""\n    The number of samples to draw from a latent GP when computing a likelihood\n    This is used in variational inference and training\n    Default: 10\n    """"""\n\n    _global_value = 20\n\n\nclass num_trace_samples(_value_context):\n    """"""\n    The number of samples to draw when stochastically computing the trace of a matrix\n    More values results in more accurate trace estimations\n    If the value is set to 0, then the trace will be deterministically computed\n    Default: 10\n    """"""\n\n    _global_value = 10\n\n\nclass prior_mode(_feature_flag):\n    """"""\n    If set to true, GP models will be evaluated in prior mode.\n    This allows evaluating any Exact GP model in prior mode, even it if has training data / targets.\n    """"""\n\n    _state = False\n\n\nclass skip_logdet_forward(_feature_flag):\n    """"""\n    .. warning:\n\n        ADVANCED FEATURE. Use this feature ONLY IF you\'re using\n        `gpytorch.mlls.MarginalLogLikelihood` as loss functions for optimizing\n        hyperparameters/variational parameters.  DO NOT use this feature if you\n        need accurate estimates of the MLL (i.e. for model selection, MCMC,\n        second order optimizaiton methods, etc.)\n\n    This feature does not affect the gradients returned by\n    :meth:`gpytorch.distributions.MultivariateNormal.log_prob`\n    (used by `gpytorch.mlls.MarginalLogLikelihood`).\n    The gradients remain unbiased estimates, and therefore can be used with SGD.\n    However, the actual likelihood value returned by the forward\n    pass will skip certain computations (i.e. the logdet computation), and will therefore\n    be improper estimates.\n\n    If you\'re using SGD (or a varient) to optimize parameters, you probably\n    don\'t need an accurate MLL estimate; you only need accurate gradients. So\n    this setting may give your model a performance boost.\n    """"""\n\n    _state = False\n\n\nclass terminate_cg_by_size(_feature_flag):\n    """"""\n    If set to true, cg will terminate after n iterations for an n x n matrix.\n    """"""\n\n    _state = False\n\n\nclass trace_mode(_feature_flag):\n    """"""\n    If set to True, we will generally try to avoid calling our built in PyTorch functions, because these cannot\n    be run through torch.jit.trace.\n\n    Note that this will sometimes involve explicitly evaluating lazy tensors and various other slowdowns and\n    inefficiencies. As a result, you really shouldn\'t use this feature context unless you are calling torch.jit.trace\n    on a GPyTorch model.\n\n    Our hope is that this flag will not be necessary long term, once https://github.com/pytorch/pytorch/issues/22329\n    is fixed.\n    """"""\n\n    _state = False\n\n\nclass tridiagonal_jitter(_value_context):\n    """"""\n    The (relative) amount of noise to add to the diagonal of tridiagonal matrices before\n    eigendecomposing. root_decomposition becomes slightly more stable with this, as we need\n    to take the square root of the eigenvalues. Any eigenvalues still negative after adding jitter\n    will be zeroed out.\n    """"""\n\n    _global_value = 1e-6\n\n\nclass use_toeplitz(_feature_flag):\n    """"""\n    Whether or not to use Toeplitz math with gridded data, grid inducing point modules\n    Pros: memory efficient, faster on CPU\n    Cons: slower on GPUs with < 10000 inducing points\n    """"""\n\n    _state = True\n'"
test/__init__.py,0,b'#! /usr/bin/env python3\n'
docs/source/conf.py,12,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport io\nimport re\nimport shutil\nimport sys\n\n# Mock - so RTD doesn\'t have to import torch\nfrom unittest.mock import MagicMock  # noqa\n\nimport sphinx_rtd_theme  # noqa\n\n\ndef read(*names, **kwargs):\n    with io.open(\n        os.path.join(os.path.dirname(__file__), "".."", "".."", *names), encoding=kwargs.get(""encoding"", ""utf8"")\n    ) as fp:\n        return fp.read()\n\n\ndef find_version(*file_paths):\n    version_file = read(*file_paths)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"", version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(""Unable to find version string."")\n\n\nsys.path.append(os.path.abspath(os.path.join(__file__, "".."", "".."", "".."")))\n\n\n# Mechanism to mock out modules\nclass ModuleMock(object):\n    def __init__(self, *args, **kwargs):\n        pass\n\n\n# We need some dirty hackary to fix the distributions mocking\nclass _Distribution(object):\n    pass\n\n\n# More dirty hackary\nclass _SubDistribution(object):\n    pass\n\n\nclass _Kernel(object):\n    pass\n\n\n# Putting all of our dirty hacks together\nclass Mock(MagicMock):\n    __metaclass__ = type\n\n    @classmethod\n    def __getattr__(cls, name):\n        if ""Module"" == name:\n            return ModuleMock\n        elif ""Distribution"" in name:\n            return _Distribution\n        elif ""Normal"" in name or ""Gamma"" in name or ""Wishart"" in name or ""Uniform"" in name:\n            return _SubDistribution\n        elif ""Kernel"" in name or ""Parallel"" in name:\n            return _Kernel\n        else:\n            res = MagicMock()\n            res.Module = ModuleMock\n            res.__metaclass__ = type\n            return res\n\n\nMOCK_MODULES = [\n    ""pyro"",\n    ""pyro.distributions"",\n    ""pyro.distributions.torch_distribution"",\n    ""torch"",\n    ""torch.autograd"",\n    ""torch.nn"",\n    ""torch.nn.functional"",\n    ""torch.nn.parallel"",\n    ""torch.optim"",\n    ""torch.utils"",\n    ""torch.utils.data"",\n    ""torch.distributions.kl"",\n    ""torch.distributions.multivariate_normal"",\n    ""torch.distributions.utils"",\n    ""torch.distributions"",\n    ""torch.optim.lr_scheduler"",\n    ""numpy"",\n]\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\n# - Copy over examples folder to docs/source\n# This makes it so that nbsphinx properly loads the notebook images\n\nexamples_source = os.path.abspath(os.path.join(os.path.dirname(__file__), "".."", "".."", ""examples""))\nexamples_dest = os.path.abspath(os.path.join(os.path.dirname(__file__), ""examples""))\n\nif os.path.exists(examples_dest):\n    shutil.rmtree(examples_dest)\nos.mkdir(examples_dest)\n\nfor root, dirs, files in os.walk(examples_source):\n    for dr in dirs:\n        os.mkdir(os.path.join(root.replace(examples_source, examples_dest), dr))\n    for fil in files:\n        if os.path.splitext(fil)[1] in ["".ipynb"", "".md"", "".rst""]:\n            source_filename = os.path.join(root, fil)\n            dest_filename = source_filename.replace(examples_source, examples_dest)\n            shutil.copyfile(source_filename, dest_filename)\n\n# -- Project information -----------------------------------------------------\n\nproject = ""GPyTorch""\ncopyright = ""2019, Cornellius GP""\nauthor = ""Cornellius GP""\n\n# The short X.Y version\nversion = find_version(""gpytorch"", ""__init__.py"")\n# The full version, including alpha/beta/rc tags\nrelease = version\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""sphinx.ext.coverage"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.githubpages"",\n    ""sphinx.ext.autodoc"",\n    ""nbsphinx"",\n    ""m2r"",\n]\n\n# Disable docstring inheritance\nautodoc_inherit_docstrings = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = ["".rst"", "".md""]\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\n    ""_build"", ""**.ipynb_checkpoints"", ""examples/**/README.rst"",\n    ""examples/README.rst"", ""examples/index.rst""\n]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = ""sphinx""\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\nhtml_theme_options = {\n    ""collapse_navigation"": False,\n    ""display_version"": True,\n    # \'logo_only\': False,\n}\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""_static""]\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = ""GPyTorchdoc""\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [(master_doc, ""GPyTorch.tex"", ""GPyTorch Documentation"", ""Cornellius GP"", ""manual"")]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, ""gpytorch"", ""GPyTorch Documentation"", [author], 1)]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        ""GPyTorch"",\n        ""GPyTorch Documentation"",\n        author,\n        ""GPyTorch"",\n        ""One line description of project."",\n        ""Miscellaneous"",\n    )\n]\n\n\n# -- Extension configuration -------------------------------------------------\n'"
examples/06_PyTorch_NN_Integration_DKL/densenet.py,3,"b'#!/usr/bin/env python3\n\n# This implementation is based on the DenseNet-BC implementation in torchvision\n# https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(""norm1"", nn.BatchNorm2d(num_input_features)),\n        self.add_module(""relu1"", nn.ReLU(inplace=True)),\n        self.add_module(\n            ""conv1"", nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n        ),\n        self.add_module(""norm2"", nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(""relu2"", nn.ReLU(inplace=True)),\n        self.add_module(\n            ""conv2"", nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n        ),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(""norm"", nn.BatchNorm2d(num_input_features))\n        self.add_module(""relu"", nn.ReLU(inplace=True))\n        self.add_module(""conv"", nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False))\n        self.add_module(""pool"", nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module(""denselayer%d"" % (i + 1), layer)\n\n\nclass DenseNet(nn.Module):\n    r""""""Densenet-BC model class, based on\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 3 or 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n            (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    """"""\n\n    def __init__(\n        self,\n        growth_rate=12,\n        block_config=(16, 16, 16),\n        compression=0.5,\n        num_init_features=24,\n        bn_size=4,\n        drop_rate=0,\n        avgpool_size=8,\n        num_classes=10,\n    ):\n\n        super(DenseNet, self).__init__()\n        assert 0 < compression <= 1, ""compression of densenet should be between 0 and 1""\n        self.avgpool_size = avgpool_size\n\n        # First convolution\n        self.features = nn.Sequential(\n            OrderedDict([(""conv0"", nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False))])\n        )\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n            )\n            self.features.add_module(""denseblock%d"" % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(\n                    num_input_features=num_features, num_output_features=int(num_features * compression)\n                )\n                self.features.add_module(""transition%d"" % (i + 1), trans)\n                num_features = int(num_features * compression)\n\n        # Final batch norm\n        self.features.add_module(""norm_final"", nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(features.size(0), -1)\n        out = self.classifier(out)\n        return out\n'"
gpytorch/constraints/__init__.py,0,"b'from .constraints import GreaterThan, Interval, LessThan, Positive\n\n__all__ = [""GreaterThan"", ""Interval"", ""LessThan"", ""Positive""]\n'"
gpytorch/constraints/constraints.py,19,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\nfrom torch import sigmoid\nfrom torch.nn import Module\nfrom torch.nn.functional import softplus\n\nfrom .. import settings\nfrom ..utils.transforms import _get_inv_param_transform, inv_sigmoid, inv_softplus\n\n\nclass Interval(Module):\n    def __init__(self, lower_bound, upper_bound, transform=sigmoid, inv_transform=inv_sigmoid, initial_value=None):\n        """"""\n        Defines an interval constraint for GP model parameters, specified by a lower bound and upper bound. For usage\n        details, see the documentation for :meth:`~gpytorch.module.Module.register_constraint`.\n\n        Args:\n            lower_bound (float or torch.Tensor): The lower bound on the parameter.\n            upper_bound (float or torch.Tensor): The upper bound on the parameter.\n        """"""\n        lower_bound = torch.as_tensor(lower_bound)\n        upper_bound = torch.as_tensor(upper_bound)\n\n        if torch.any(torch.ge(lower_bound, upper_bound)):\n            raise RuntimeError(""Got parameter bounds with empty intervals."")\n\n        super().__init__()\n\n        self.lower_bound = lower_bound\n        self.upper_bound = upper_bound\n\n        self._transform = transform\n        self._inv_transform = inv_transform\n        self._initial_value = initial_value\n\n        if transform is not None and inv_transform is None:\n            self._inv_transform = _get_inv_param_transform(transform)\n\n    def _apply(self, fn):\n        self.lower_bound = fn(self.lower_bound)\n        self.upper_bound = fn(self.upper_bound)\n        return super()._apply(fn)\n\n    @property\n    def enforced(self):\n        return self._transform is not None\n\n    def check(self, tensor):\n        return bool(torch.all(tensor <= self.upper_bound) and torch.all(tensor >= self.lower_bound))\n\n    def check_raw(self, tensor):\n        return bool(\n            torch.all((self.transform(tensor) <= self.upper_bound))\n            and torch.all(self.transform(tensor) >= self.lower_bound)\n        )\n\n    def intersect(self, other):\n        """"""\n        Returns a new Interval constraint that is the intersection of this one and another specified one.\n\n        Args:\n            other (Interval): Interval constraint to intersect with\n\n        Returns:\n            Interval: intersection if this interval with the other one.\n        """"""\n        if self.transform != other.transform:\n            raise RuntimeError(""Cant intersect Interval constraints with conflicting transforms!"")\n\n        lower_bound = torch.max(self.lower_bound, other.lower_bound)\n        upper_bound = torch.min(self.upper_bound, other.upper_bound)\n        return Interval(lower_bound, upper_bound)\n\n    def transform(self, tensor):\n        """"""\n        Transforms a tensor to satisfy the specified bounds.\n\n        If upper_bound is finite, we assume that `self.transform` saturates at 1 as tensor -> infinity. Similarly,\n        if lower_bound is finite, we assume that `self.transform` saturates at 0 as tensor -> -infinity.\n\n        Example transforms for one of the bounds being finite include torch.exp and torch.nn.functional.softplus.\n        An example transform for the case where both are finite is torch.nn.functional.sigmoid.\n        """"""\n        if not self.enforced:\n            return tensor\n\n        if settings.debug.on():\n            max_bound = torch.max(self.upper_bound)\n            min_bound = torch.min(self.lower_bound)\n\n            if max_bound == math.inf or min_bound == -math.inf:\n                raise RuntimeError(\n                    ""Cannot make an Interval directly with non-finite bounds. Use a derived class like ""\n                    ""GreaterThan or LessThan instead.""\n                )\n\n        transformed_tensor = (self._transform(tensor) * (self.upper_bound - self.lower_bound)) + self.lower_bound\n\n        return transformed_tensor\n\n    def inverse_transform(self, transformed_tensor):\n        """"""\n        Applies the inverse transformation.\n        """"""\n        if not self.enforced:\n            return transformed_tensor\n\n        if settings.debug.on():\n            max_bound = torch.max(self.upper_bound)\n            min_bound = torch.min(self.lower_bound)\n\n            if max_bound == math.inf or min_bound == -math.inf:\n                raise RuntimeError(\n                    ""Cannot make an Interval directly with non-finite bounds. Use a derived class like ""\n                    ""GreaterThan or LessThan instead.""\n                )\n\n        tensor = self._inv_transform((transformed_tensor - self.lower_bound) / (self.upper_bound - self.lower_bound))\n\n        return tensor\n\n    @property\n    def initial_value(self):\n        """"""\n        The initial parameter value (if specified, None otherwise)\n        """"""\n        return self._initial_value\n\n    def __repr__(self):\n        if self.lower_bound.numel() == 1 and self.upper_bound.numel() == 1:\n            return self._get_name() + f""({self.lower_bound:.3E}, {self.upper_bound:.3E})""\n        else:\n            return super().__repr__()\n\n    def __iter__(self):\n        yield self.lower_bound\n        yield self.upper_bound\n\n\nclass GreaterThan(Interval):\n    def __init__(self, lower_bound, transform=softplus, inv_transform=inv_softplus, initial_value=None):\n        super().__init__(\n            lower_bound=lower_bound,\n            upper_bound=math.inf,\n            transform=transform,\n            inv_transform=inv_transform,\n            initial_value=initial_value,\n        )\n\n    def __repr__(self):\n        if self.lower_bound.numel() == 1:\n            return self._get_name() + f""({self.lower_bound:.3E})""\n        else:\n            return super().__repr__()\n\n    def transform(self, tensor):\n        transformed_tensor = self._transform(tensor) + self.lower_bound if self.enforced else tensor\n        return transformed_tensor\n\n    def inverse_transform(self, transformed_tensor):\n        tensor = self._inv_transform(transformed_tensor - self.lower_bound) if self.enforced else transformed_tensor\n        return tensor\n\n\nclass Positive(GreaterThan):\n    def __init__(self, transform=softplus, inv_transform=inv_softplus, initial_value=None):\n        super().__init__(lower_bound=0.0, transform=transform, inv_transform=inv_transform, initial_value=initial_value)\n\n    def __repr__(self):\n        return self._get_name() + ""()""\n\n    def transform(self, tensor):\n        transformed_tensor = self._transform(tensor) if self.enforced else tensor\n        return transformed_tensor\n\n    def inverse_transform(self, transformed_tensor):\n        tensor = self._inv_transform(transformed_tensor) if self.enforced else transformed_tensor\n        return tensor\n\n\nclass LessThan(Interval):\n    def __init__(self, upper_bound, transform=softplus, inv_transform=inv_softplus):\n        super().__init__(\n            lower_bound=-math.inf, upper_bound=upper_bound, transform=transform, inv_transform=inv_transform\n        )\n\n    def transform(self, tensor):\n        transformed_tensor = -self._transform(-tensor) + self.upper_bound if self.enforced else tensor\n        return transformed_tensor\n\n    def inverse_transform(self, transformed_tensor):\n        tensor = -self._inv_transform(-(transformed_tensor - self.upper_bound)) if self.enforced else transformed_tensor\n        return tensor\n\n    def __repr__(self):\n        return self._get_name() + f""({self.upper_bound:.3E})""\n'"
gpytorch/distributions/__init__.py,1,"b'#!/usr/bin/env python3\n\nfrom .delta import Delta\nfrom .distribution import Distribution\nfrom .multitask_multivariate_normal import MultitaskMultivariateNormal\nfrom .multivariate_normal import MultivariateNormal\n\n# Get the set of distributions from either PyTorch or Pyro\ntry:\n    # If pyro is installed, use that set of base distributions\n    import pyro.distributions as base_distributions\nexcept ImportError:\n    # Otherwise, use PyTorch\n    import torch.distributions as base_distributions\n\n\n__all__ = [""Delta"", ""Distribution"", ""MultivariateNormal"", ""MultitaskMultivariateNormal"", ""base_distributions""]\n'"
gpytorch/distributions/delta.py,7,"b'#!/usr/bin/env python3\n\nimport numbers\n\nimport torch\nfrom torch.distributions.kl import register_kl\n\nfrom .distribution import Distribution\nfrom .multivariate_normal import MultivariateNormal\n\ntry:\n    from pyro.distributions import Delta\n\nexcept ImportError:\n    # Mostly copied from https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/delta.py\n    class Delta(Distribution):\n        """"""\n        Degenerate discrete distribution (a single point).\n\n        Discrete distribution that assigns probability one to the single element in\n        its support. Delta distribution parameterized by a random choice should not\n        be used with MCMC based inference, as doing so produces incorrect results.\n\n        :param torch.Tensor v: The single support element.\n        :param torch.Tensor log_density: An optional density for this Delta. This\n            is useful to keep the class of :class:`Delta` distributions closed\n            under differentiable transformation.\n        :param int event_dim: Optional event dimension, defaults to zero.\n        """"""\n\n        has_rsample = True\n\n        def __init__(self, v, log_density=0.0, event_dim=0, validate_args=None):\n            if event_dim > v.dim():\n                raise ValueError(""Expected event_dim <= v.dim(), actual {} vs {}"".format(event_dim, v.dim()))\n            batch_dim = v.dim() - event_dim\n            batch_shape = v.shape[:batch_dim]\n            event_shape = v.shape[batch_dim:]\n            if isinstance(log_density, numbers.Number):\n                log_density = torch.full(batch_shape, log_density, dtype=v.dtype, device=v.device)\n            elif validate_args and log_density.shape != batch_shape:\n                raise ValueError(""Expected log_density.shape = {}, actual {}"".format(log_density.shape, batch_shape))\n            self.v = v\n            self.log_density = log_density\n            super().__init__(batch_shape, event_shape, validate_args=validate_args)\n\n        def expand(self, batch_shape, _instance=None):\n            new = self._get_checked_instance(Delta, _instance)\n            batch_shape = torch.Size(batch_shape)\n            new.v = self.v.expand(batch_shape + self.event_shape)\n            new.log_density = self.log_density.expand(batch_shape)\n            super().__init__(batch_shape, self.event_shape, validate_args=False)\n            new._validate_args = self._validate_args\n            return new\n\n        def rsample(self, sample_shape=torch.Size()):\n            shape = sample_shape + self.v.shape\n            return self.v.expand(shape)\n\n        def log_prob(self, x):\n            v = self.v.expand(self.batch_shape + self.event_shape)\n            log_prob = (x == v).type(x.dtype).log()\n            if len(self.event_shape):\n                log_prob = log_prob.sum(list(range(-1, -len(self.event_shape) - 1, -1)))\n            return log_prob + self.log_density\n\n        @property\n        def mean(self):\n            return self.v\n\n        @property\n        def variance(self):\n            return torch.zeros_like(self.v)\n\n\n@register_kl(Delta, MultivariateNormal)\ndef kl_mvn_mvn(p_dist, q_dist):\n    return -q_dist.log_prob(p_dist.mean)\n'"
gpytorch/distributions/distribution.py,2,"b'#!/usr/bin/env python3\n\nfrom torch.distributions import Distribution as TDistribution\n\n\nclass _DistributionBase(TDistribution):\n    """"""\n    The base class of Distributions. (Same as torch.distribution.Distribution\n    or pyro.distribution.Distribution).\n    """"""\n\n    @property\n    def islazy(self):\n        return self._islazy\n\n    def __add__(self, other):\n        raise NotImplementedError()\n\n    def __div__(self, other):\n        raise NotImplementedError()\n\n    def __mul__(self, other):\n        raise NotImplementedError()\n\n\ntry:\n    # If pyro is installed, add the TorchDistributionMixin\n    from pyro.distributions.torch_distribution import TorchDistributionMixin\n\n    class Distribution(_DistributionBase, TorchDistributionMixin):\n        pass\n\n\nexcept ImportError:\n\n    class Distribution(_DistributionBase):\n        pass\n'"
gpytorch/distributions/multitask_multivariate_normal.py,41,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..lazy import BlockDiagLazyTensor, BlockInterleavedLazyTensor, CatLazyTensor, LazyTensor\nfrom .multivariate_normal import MultivariateNormal\n\n\nclass MultitaskMultivariateNormal(MultivariateNormal):\n    """"""\n    Constructs a multi-output multivariate Normal random variable, based on mean and covariance\n    Can be multi-output multivariate, or a batch of multi-output multivariate Normal\n\n    Passing a matrix mean corresponds to a multi-output multivariate Normal\n    Passing a matrix mean corresponds to a batch of multivariate Normals\n\n    :param torch.Tensor mean:  An `n x t` or batch `b x n x t` matrix of means for the MVN distribution.\n    :param ~gpytorch.lazy.LazyTensor covar: An `nt x nt` or batch `b x nt x nt`\n        covariance matrix of MVN distribution.\n    :param bool validate_args: (default=False) If True, validate `mean` anad `covariance_matrix` arguments.\n    :param bool interleaved: (default=True) If True, covariance matrix is interpreted as block-diagonal w.r.t.\n        inter-task covariances for each observation. If False, it is interpreted as block-diagonal\n        w.r.t. inter-observation covariance for each task.\n    """"""\n\n    def __init__(self, mean, covariance_matrix, validate_args=False, interleaved=True):\n        if not torch.is_tensor(mean) and not isinstance(mean, LazyTensor):\n            raise RuntimeError(""The mean of a MultitaskMultivariateNormal must be a Tensor or LazyTensor"")\n\n        if not torch.is_tensor(covariance_matrix) and not isinstance(covariance_matrix, LazyTensor):\n            raise RuntimeError(""The covariance of a MultitaskMultivariateNormal must be a Tensor or LazyTensor"")\n\n        if mean.dim() < 2:\n            raise RuntimeError(""mean should be a matrix or a batch matrix (batch mode)"")\n\n        self._output_shape = mean.shape\n        # TODO: Instead of transpose / view operations, use a PermutationLazyTensor (see #539) to handle interleaving\n        self._interleaved = interleaved\n        if self._interleaved:\n            mean_mvn = mean.reshape(*mean.shape[:-2], -1)\n        else:\n            mean_mvn = mean.transpose(-1, -2).reshape(*mean.shape[:-2], -1)\n        super().__init__(mean=mean_mvn, covariance_matrix=covariance_matrix, validate_args=validate_args)\n\n    @property\n    def event_shape(self):\n        return self._output_shape[-2:]\n\n    @classmethod\n    def from_batch_mvn(cls, batch_mvn, task_dim=-1):\n        """"""\n        Reinterprate a batch of multivariate normal distributions as an (independent) multitask multivariate normal\n        distribution.\n\n        :param ~gpytorch.distributions.MultivariateNormal batch_mvn: The base MVN distribution.\n            (This distribution should have at least one batch dimension).\n        :param int task_dim: Which batch dimension should be interpreted as the dimension for the independent tasks.\n        :returns: the independent multitask distribution\n        :rtype: gpytorch.distributions.MultitaskMultivariateNormal\n\n        Example:\n            >>> # model is a gpytorch.models.VariationalGP\n            >>> # likelihood is a gpytorch.likelihoods.Likelihood\n            >>> mean = torch.randn(4, 2, 3)\n            >>> covar_factor = torch.randn(4, 2, 3, 3)\n            >>> covar = covar_factor @ covar_factor.transpose(-1, -2)\n            >>> mvn = gpytorch.distributions.MultivariateNormal(mean, covar)\n            >>> print(mvn.event_shape, mvn.batch_shape)\n            >>> # torch.Size([3]), torch.Size([4, 2])\n            >>>\n            >>> mmvn = MultitaskMultivariateNormal.from_batch_mvn(mvn, task_dim=-1)\n            >>> print(mmvn.event_shape, mmvn.batch_shape)\n            >>> # torch.Size([3, 2]), torch.Size([4])\n        """"""\n        orig_task_dim = task_dim\n        task_dim = task_dim if task_dim >= 0 else (len(batch_mvn.batch_shape) + task_dim)\n        if task_dim < 0 or task_dim > len(batch_mvn.batch_shape):\n            raise ValueError(\n                f""task_dim of {orig_task_dim} is incompatible with MVN batch shape of {batch_mvn.batch_shape}""\n            )\n\n        num_dim = batch_mvn.mean.dim()\n        res = cls(\n            mean=batch_mvn.mean.permute(*range(0, task_dim), *range(task_dim + 1, num_dim), task_dim),\n            covariance_matrix=BlockInterleavedLazyTensor(batch_mvn.lazy_covariance_matrix, block_dim=task_dim),\n        )\n        return res\n\n    @classmethod\n    def from_independent_mvns(cls, mvns):\n        """"""\n        Convert an iterable of MVNs into a :obj:`~gpytorch.distributions.MultitaskMultivariateNormal`.\n        The resulting distribution will have :attr:`len(mvns)` tasks, and the tasks will be independent.\n\n        :param ~gpytorch.distributions.MultitaskNormal mvn: The base MVN distributions.\n        :returns: the independent multitask distribution\n        :rtype: gpytorch.distributions.MultitaskMultivariateNormal\n\n        Example:\n            >>> # model is a gpytorch.models.VariationalGP\n            >>> # likelihood is a gpytorch.likelihoods.Likelihood\n            >>> mean = torch.randn(4, 3)\n            >>> covar_factor = torch.randn(4, 3, 3)\n            >>> covar = covar_factor @ covar_factor.transpose(-1, -2)\n            >>> mvn1 = gpytorch.distributions.MultivariateNormal(mean, covar)\n            >>>\n            >>> mean = torch.randn(4, 3)\n            >>> covar_factor = torch.randn(4, 3, 3)\n            >>> covar = covar_factor @ covar_factor.transpose(-1, -2)\n            >>> mvn2 = gpytorch.distributions.MultivariateNormal(mean, covar)\n            >>>\n            >>> mmvn = MultitaskMultivariateNormal.from_independent_mvns([mvn1, mvn2])\n            >>> print(mmvn.event_shape, mmvn.batch_shape)\n            >>> # torch.Size([3, 2]), torch.Size([4])\n        """"""\n        if len(mvns) < 2:\n            raise ValueError(""Must provide at least 2 MVNs to form a MultitaskMultivariateNormal"")\n        if any(isinstance(mvn, MultitaskMultivariateNormal) for mvn in mvns):\n            raise ValueError(""Cannot accept MultitaskMultivariateNormals"")\n        if not all(m.batch_shape == mvns[0].batch_shape for m in mvns[1:]):\n            raise ValueError(""All MultivariateNormals must have the same batch shape"")\n        if not all(m.event_shape == mvns[0].event_shape for m in mvns[1:]):\n            raise ValueError(""All MultivariateNormals must have the same event shape"")\n        mean = torch.stack([mvn.mean for mvn in mvns], -1)\n        # TODO: To do the following efficiently, we don\'t want to evaluate the\n        # covariance matrices. Instead, we want to use the lazies directly in the\n        # BlockDiagLazyTensor. This will require implementing a new BatchLazyTensor:\n\n        # https://github.com/cornellius-gp/gpytorch/issues/468\n        covar_blocks_lazy = CatLazyTensor(\n            *[mvn.lazy_covariance_matrix.unsqueeze(0) for mvn in mvns], dim=0, output_device=mean.device\n        )\n        covar_lazy = BlockDiagLazyTensor(covar_blocks_lazy, block_dim=0)\n        return cls(mean=mean, covariance_matrix=covar_lazy, interleaved=False)\n\n    @classmethod\n    def from_repeated_mvn(cls, mvn, num_tasks):\n        """"""\n        Convert a single MVN into a :obj:`~gpytorch.distributions.MultitaskMultivariateNormal`,\n        where each task shares the same mean and covariance.\n\n        :param ~gpytorch.distributions.MultitaskNormal mvn: The base MVN distribution.\n        :param int num_tasks: How many tasks to create.\n        :returns: the independent multitask distribution\n        :rtype: gpytorch.distributions.MultitaskMultivariateNormal\n\n        Example:\n            >>> # model is a gpytorch.models.VariationalGP\n            >>> # likelihood is a gpytorch.likelihoods.Likelihood\n            >>> mean = torch.randn(4, 3)\n            >>> covar_factor = torch.randn(4, 3, 3)\n            >>> covar = covar_factor @ covar_factor.transpose(-1, -2)\n            >>> mvn = gpytorch.distributions.MultivariateNormal(mean, covar)\n            >>> print(mvn.event_shape, mvn.batch_shape)\n            >>> # torch.Size([3]), torch.Size([4])\n            >>>\n            >>> mmvn = MultitaskMultivariateNormal.from_repeated_mvn(mvn, num_tasks=2)\n            >>> print(mmvn.event_shape, mmvn.batch_shape)\n            >>> # torch.Size([3, 2]), torch.Size([4])\n        """"""\n        return cls.from_batch_mvn(mvn.expand(torch.Size([num_tasks]) + mvn.batch_shape), task_dim=0)\n\n    def expand(self, batch_size):\n        new_mean = self.mean.expand(torch.Size(batch_size) + self.mean.shape[-2:])\n        new_covar = self._covar.expand(torch.Size(batch_size) + self._covar.shape[-2:])\n        res = self.__class__(new_mean, new_covar, interleaved=self._interleaved)\n        return res\n\n    def get_base_samples(self, sample_shape=torch.Size()):\n        base_samples = super().get_base_samples(sample_shape)\n        if not self._interleaved:\n            # flip shape of last two dimensions\n            new_shape = sample_shape + self._output_shape[:-2] + self._output_shape[:-3:-1]\n            return base_samples.view(new_shape).transpose(-1, -2).contiguous()\n        return base_samples.view(*sample_shape, *self._output_shape)\n\n    def log_prob(self, value):\n        if not self._interleaved:\n            # flip shape of last two dimensions\n            new_shape = value.shape[:-2] + value.shape[:-3:-1]\n            value = value.view(new_shape).transpose(-1, -2).contiguous()\n        return super().log_prob(value.view(*value.shape[:-2], -1))\n\n    @property\n    def mean(self):\n        mean = super().mean\n        if not self._interleaved:\n            # flip shape of last two dimensions\n            new_shape = self._output_shape[:-2] + self._output_shape[:-3:-1]\n            return mean.view(new_shape).transpose(-1, -2).contiguous()\n        return mean.view(self._output_shape)\n\n    @property\n    def num_tasks(self):\n        return self._output_shape[-1]\n\n    def rsample(self, sample_shape=torch.Size(), base_samples=None):\n        if base_samples is not None:\n            # Make sure that the base samples agree with the distribution\n            mean_shape = self.mean.shape\n            base_sample_shape = base_samples.shape[-self.mean.ndimension() :]\n            if mean_shape != base_sample_shape:\n                raise RuntimeError(\n                    ""The shape of base_samples (minus sample shape dimensions) should agree with the shape ""\n                    ""of self.mean. Expected ...{} but got {}"".format(mean_shape, base_sample_shape)\n                )\n            sample_shape = base_samples.shape[: -self.mean.ndimension()]\n            base_samples = base_samples.view(*sample_shape, *self.loc.shape)\n\n        samples = super().rsample(sample_shape=sample_shape, base_samples=base_samples)\n        if not self._interleaved:\n            # flip shape of last two dimensions\n            new_shape = sample_shape + self._output_shape[:-2] + self._output_shape[:-3:-1]\n            return samples.view(new_shape).transpose(-1, -2).contiguous()\n        return samples.view(sample_shape + self._output_shape)\n\n    @property\n    def variance(self):\n        var = super().variance\n        if not self._interleaved:\n            # flip shape of last two dimensions\n            new_shape = self._output_shape[:-2] + self._output_shape[:-3:-1]\n            return var.view(new_shape).transpose(-1, -2).contiguous()\n        return var.view(self._output_shape)\n'"
gpytorch/distributions/multivariate_normal.py,15,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\nfrom torch.distributions import MultivariateNormal as TMultivariateNormal\nfrom torch.distributions.kl import register_kl\nfrom torch.distributions.utils import _standard_normal, lazy_property\n\nfrom .. import settings\nfrom ..lazy import DiagLazyTensor, LazyTensor, delazify, lazify\nfrom ..utils.broadcasting import _mul_broadcast_shape\nfrom .distribution import Distribution\n\n\nclass MultivariateNormal(TMultivariateNormal, Distribution):\n    """"""\n    Constructs a multivariate normal random variable, based on mean and covariance.\n    Can be multivariate, or a batch of multivariate normals\n\n    Passing a vector mean corresponds to a multivariate normal.\n    Passing a matrix mean corresponds to a batch of multivariate normals.\n\n    :param torch.tensor mean: Vector n or matrix b x n mean of mvn distribution.\n    :param ~gpytorch.lazy.LazyTensor covar: Matrix n x n or batch matrix b x n x n covariance of\n        mvn distribution.\n    """"""\n\n    def __init__(self, mean, covariance_matrix, validate_args=False):\n        self._islazy = isinstance(mean, LazyTensor) or isinstance(covariance_matrix, LazyTensor)\n        if self._islazy:\n            if validate_args:\n                ms = mean.size(-1)\n                cs1 = covariance_matrix.size(-1)\n                cs2 = covariance_matrix.size(-2)\n                if not (ms == cs1 and ms == cs2):\n                    raise ValueError(f""Wrong shapes in {self._repr_sizes(mean, covariance_matrix)}"")\n            self.loc = mean\n            self._covar = covariance_matrix\n            self.__unbroadcasted_scale_tril = None\n            self._validate_args = validate_args\n            batch_shape = _mul_broadcast_shape(self.loc.shape[:-1], covariance_matrix.shape[:-2])\n            event_shape = self.loc.shape[-1:]\n            # TODO: Integrate argument validation for LazyTensors into torch.distribution validation logic\n            super(TMultivariateNormal, self).__init__(batch_shape, event_shape, validate_args=False)\n        else:\n            super().__init__(loc=mean, covariance_matrix=covariance_matrix, validate_args=validate_args)\n\n    @property\n    def _unbroadcasted_scale_tril(self):\n        if self.islazy and self.__unbroadcasted_scale_tril is None:\n            # cache root decoposition\n            ust = delazify(self.lazy_covariance_matrix.cholesky())\n            self.__unbroadcasted_scale_tril = ust\n        return self.__unbroadcasted_scale_tril\n\n    @_unbroadcasted_scale_tril.setter\n    def _unbroadcasted_scale_tril(self, ust):\n        if self.islazy:\n            raise NotImplementedError(""Cannot set _unbroadcasted_scale_tril for lazy MVN distributions"")\n        else:\n            self.__unbroadcasted_scale_tril = ust\n\n    def expand(self, batch_size):\n        new_loc = self.loc.expand(torch.Size(batch_size) + self.loc.shape[-1:])\n        new_covar = self._covar.expand(torch.Size(batch_size) + self._covar.shape[-2:])\n        res = self.__class__(new_loc, new_covar)\n        return res\n\n    def confidence_region(self):\n        """"""\n        Returns 2 standard deviations above and below the mean.\n\n        :rtype: (torch.Tensor, torch.Tensor)\n        :return: pair of tensors of size (b x d) or (d), where\n            b is the batch size and d is the dimensionality of the random\n            variable. The first (second) Tensor is the lower (upper) end of\n            the confidence region.\n        """"""\n        std2 = self.stddev.mul_(2)\n        mean = self.mean\n        return mean.sub(std2), mean.add(std2)\n\n    @staticmethod\n    def _repr_sizes(mean, covariance_matrix):\n        return f""MultivariateNormal(loc: {mean.size()}, scale: {covariance_matrix.size()})""\n\n    @lazy_property\n    def covariance_matrix(self):\n        if self.islazy:\n            return self._covar.evaluate()\n        else:\n            return super().covariance_matrix\n\n    def get_base_samples(self, sample_shape=torch.Size()):\n        """"""Get i.i.d. standard Normal samples (to be used with rsample(base_samples=base_samples))""""""\n        with torch.no_grad():\n            shape = self._extended_shape(sample_shape)\n            base_samples = _standard_normal(shape, dtype=self.loc.dtype, device=self.loc.device)\n        return base_samples\n\n    @lazy_property\n    def lazy_covariance_matrix(self):\n        """"""\n        The covariance_matrix, represented as a LazyTensor\n        """"""\n        if self.islazy:\n            return self._covar\n        else:\n            return lazify(super().covariance_matrix)\n\n    def log_prob(self, value):\n        if settings.fast_computations.log_prob.off():\n            return super().log_prob(value)\n\n        if self._validate_args:\n            self._validate_sample(value)\n\n        mean, covar = self.loc, self.lazy_covariance_matrix\n        diff = value - mean\n\n        # Repeat the covar to match the batch shape of diff\n        if diff.shape[:-1] != covar.batch_shape:\n            if len(diff.shape[:-1]) < len(covar.batch_shape):\n                diff = diff.expand(covar.shape[:-1])\n            else:\n                padded_batch_shape = (*(1 for _ in range(diff.dim() + 1 - covar.dim())), *covar.batch_shape)\n                covar = covar.repeat(\n                    *(diff_size // covar_size for diff_size, covar_size in zip(diff.shape[:-1], padded_batch_shape)),\n                    1,\n                    1,\n                )\n\n        # Get log determininat and first part of quadratic form\n        inv_quad, logdet = covar.inv_quad_logdet(inv_quad_rhs=diff.unsqueeze(-1), logdet=True)\n\n        res = -0.5 * sum([inv_quad, logdet, diff.size(-1) * math.log(2 * math.pi)])\n        return res\n\n    def rsample(self, sample_shape=torch.Size(), base_samples=None):\n        covar = self.lazy_covariance_matrix\n        if base_samples is None:\n            # Create some samples\n            num_samples = sample_shape.numel() or 1\n\n            # Get samples\n            res = covar.zero_mean_mvn_samples(num_samples) + self.loc.unsqueeze(0)\n            res = res.view(sample_shape + self.loc.shape)\n\n        else:\n            # Make sure that the base samples agree with the distribution\n            if self.loc.shape != base_samples.shape[-self.loc.dim() :]:\n                raise RuntimeError(\n                    ""The size of base_samples (minus sample shape dimensions) should agree with the size ""\n                    ""of self.loc. Expected ...{} but got {}"".format(self.loc.shape, base_samples.shape)\n                )\n\n            # Determine what the appropriate sample_shape parameter is\n            sample_shape = base_samples.shape[: base_samples.dim() - self.loc.dim()]\n\n            # Reshape samples to be batch_size x num_dim x num_samples\n            # or num_bim x num_samples\n            base_samples = base_samples.view(-1, *self.loc.shape)\n            base_samples = base_samples.permute(*range(1, self.loc.dim() + 1), 0)\n\n            # Now reparameterize those base samples\n            covar_root = covar.root_decomposition().root\n            # If necessary, adjust base_samples for rank of root decomposition\n            if covar_root.shape[-1] < base_samples.shape[-2]:\n                base_samples = base_samples[..., : covar_root.shape[-1], :]\n            elif covar_root.shape[-1] > base_samples.shape[-2]:\n                raise RuntimeError(""Incompatible dimension of `base_samples`"")\n            res = covar_root.matmul(base_samples) + self.loc.unsqueeze(-1)\n\n            # Permute and reshape new samples to be original size\n            res = res.permute(-1, *range(self.loc.dim())).contiguous()\n            res = res.view(sample_shape + self.loc.shape)\n\n        return res\n\n    def sample(self, sample_shape=torch.Size(), base_samples=None):\n        with torch.no_grad():\n            return self.rsample(sample_shape=sample_shape, base_samples=base_samples)\n\n    @property\n    def variance(self):\n        if self.islazy:\n            # overwrite this since torch MVN uses unbroadcasted_scale_tril for this\n            diag = self.lazy_covariance_matrix.diag()\n            diag = diag.view(diag.shape[:-1] + self._event_shape)\n            return diag.expand(self._batch_shape + self._event_shape)\n        else:\n            return super().variance\n\n    def __add__(self, other):\n        if isinstance(other, MultivariateNormal):\n            return self.__class__(\n                mean=self.mean + other.mean,\n                covariance_matrix=(self.lazy_covariance_matrix + other.lazy_covariance_matrix),\n            )\n        elif isinstance(other, int) or isinstance(other, float):\n            return self.__class__(self.mean + other, self.lazy_covariance_matrix)\n        else:\n            raise RuntimeError(""Unsupported type {} for addition w/ MultivariateNormal"".format(type(other)))\n\n    def __radd__(self, other):\n        if other == 0:\n            return self\n        return self.__add__(other)\n\n    def __mul__(self, other):\n        if not (isinstance(other, int) or isinstance(other, float)):\n            raise RuntimeError(""Can only multiply by scalars"")\n        if other == 1:\n            return self\n        return self.__class__(mean=self.mean * other, covariance_matrix=self.lazy_covariance_matrix * (other ** 2))\n\n    def __truediv__(self, other):\n        return self.__mul__(1.0 / other)\n\n    def __getitem__(self, idx):\n        if not isinstance(idx, tuple):\n            idx = (idx,)\n        rest_idx = idx[:-1]\n        last_idx = idx[-1]\n        new_mean = self.mean[idx]\n\n        if len(idx) <= self.mean.dim() - 1 and (Ellipsis not in rest_idx):\n            new_cov = self.lazy_covariance_matrix[idx]\n        elif len(idx) > self.mean.dim():\n            raise IndexError(f""Index {idx} has too many dimensions"")\n        else:\n            # In this case we know last_idx corresponds to the last dimension\n            # of mean and the last two dimensions of lazy_covariance_matrix\n            if isinstance(last_idx, int):\n                new_cov = DiagLazyTensor(self.lazy_covariance_matrix.diag()[(*rest_idx, last_idx)])\n            elif isinstance(last_idx, slice):\n                new_cov = self.lazy_covariance_matrix[(*rest_idx, last_idx, last_idx)]\n            elif last_idx is (...):\n                new_cov = self.lazy_covariance_matrix[rest_idx]\n            else:\n                new_cov = self.lazy_covariance_matrix[(*rest_idx, last_idx, slice(None, None, None))][..., last_idx]\n        return self.__class__(mean=new_mean, covariance_matrix=new_cov)\n\n\n@register_kl(MultivariateNormal, MultivariateNormal)\ndef kl_mvn_mvn(p_dist, q_dist):\n    output_shape = _mul_broadcast_shape(p_dist.batch_shape, q_dist.batch_shape)\n    if output_shape != p_dist.batch_shape:\n        p_dist = p_dist.expand(output_shape)\n    if output_shape != q_dist.batch_shape:\n        q_dist = q_dist.expand(output_shape)\n\n    q_mean = q_dist.loc\n    q_covar = q_dist.lazy_covariance_matrix\n\n    p_mean = p_dist.loc\n    p_covar = p_dist.lazy_covariance_matrix\n    root_p_covar = p_covar.root_decomposition().root.evaluate()\n\n    mean_diffs = p_mean - q_mean\n    if isinstance(root_p_covar, LazyTensor):\n        # right now this just catches if root_p_covar is a DiagLazyTensor,\n        # but we may want to be smarter about this in the future\n        root_p_covar = root_p_covar.evaluate()\n    inv_quad_rhs = torch.cat([mean_diffs.unsqueeze(-1), root_p_covar], -1)\n    logdet_p_covar = p_covar.logdet()\n    trace_plus_inv_quad_form, logdet_q_covar = q_covar.inv_quad_logdet(inv_quad_rhs=inv_quad_rhs, logdet=True)\n\n    # Compute the KL Divergence.\n    res = 0.5 * sum([logdet_q_covar, logdet_p_covar.mul(-1), trace_plus_inv_quad_form, -float(mean_diffs.size(-1))])\n    return res\n'"
gpytorch/functions/__init__.py,5,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ._dsmm import DSMM\nfrom ._log_normal_cdf import LogNormalCDF\nfrom .matern_covariance import MaternCovariance\nfrom .rbf_covariance import RBFCovariance\n\n\ndef add_diag(input, diag):\n    """"""\n    Adds a diagonal matrix s*I to the input matrix input.\n\n    Args:\n        :attr:`input` (Tensor (nxn) or (bxnxn)):\n            Tensor or LazyTensor wrapping matrix to add diagonal component to.\n        :attr:`diag` (scalar or Tensor (n) or Tensor (bxn) or Tensor (bx1)):\n            Diagonal component to add to tensor\n\n    Returns:\n        :obj:`Tensor` (bxnxn or nxn)\n    """"""\n    from ..lazy import lazify\n\n    return lazify(input).add_diag(diag)\n\n\ndef add_jitter(mat, jitter_val=1e-3):\n    """"""\n    Adds ""jitter"" to the diagonal of a matrix.\n    This ensures that a matrix that *should* be positive definite *is* positive definate.\n\n    Args:\n        - mat (matrix nxn) - Positive definite matrxi\n\n    Returns: (matrix nxn)\n    """"""\n    if hasattr(mat, ""add_jitter""):\n        return mat.add_jitter(jitter_val)\n    else:\n        diag = torch.eye(mat.size(-1), dtype=mat.dtype, device=mat.device).mul_(jitter_val)\n        if mat.ndimension() == 3:\n            return mat + diag.unsqueeze(0).expand(mat.size(0), mat.size(1), mat.size(2))\n        else:\n            return mat + diag\n\n\ndef dsmm(sparse_mat, dense_mat):\n    """"""\n    Performs the (batch) matrix multiplication S x D\n    where S is a sparse matrix and D is a dense matrix\n\n    Args:\n        - sparse_mat (matrix (b x)mxn) - Tensor wrapping sparse matrix\n        - dense_mat (matrix (b x)nxo) - Tensor wrapping dense matrix\n\n    Returns:\n        - matrix (b x)mxo - Result\n    """"""\n    return DSMM().apply(sparse_mat, dense_mat)\n\n\ndef log_normal_cdf(x):\n    """"""\n    Computes the element-wise log standard normal CDF of an input tensor x.\n\n    This function should always be preferred over calling normal_cdf and taking the log\n    manually, as it is more numerically stable.\n    """"""\n    return LogNormalCDF().apply(x)\n\n\ndef matmul(mat, rhs):\n    """"""\n    Computes a matrix multiplication between a matrix (mat) and a right hand side (rhs).\n    If mat is a tensor, then this is the same as torch.matmul.\n    This function can work on lazy tensors though\n\n    Args:\n        - mat (matrix nxn) - left hand size matrix\n        - rhs (matrix nxk) - rhs matrix or vector\n\n    Returns:\n        - matrix nxk\n    """"""\n    return mat.matmul(rhs)\n\n\ndef inv_matmul(mat, right_tensor, left_tensor=None):\n    r""""""\n    Computes a linear solve (w.r.t :attr:`mat` = :math:`A`) with several right hand sides :math:`R`.\n    I.e. computes\n\n    ... math::\n\n        \\begin{equation}\n            A^{-1} R,\n        \\end{equation}\n\n    where :math:`R` is :attr:`right_tensor` and :math:`A` is :attr:`mat`.\n\n    If :attr:`left_tensor` is supplied, computes\n\n    ... math::\n\n        \\begin{equation}\n            L A^{-1} R,\n        \\end{equation}\n\n    where :math:`L` is :attr:`left_tensor`. Supplying this can reduce the number of\n    CG calls required.\n\n    Args:\n        - :obj:`torch.tensor` (n x k) - Matrix :math:`R` right hand sides\n        - :obj:`torch.tensor` (m x n) - Optional matrix :math:`L` to perform left multiplication with\n\n    Returns:\n        - :obj:`torch.tensor` - :math:`A^{-1}R` or :math:`LA^{-1}R`.\n    """"""\n    from ..lazy import lazify\n\n    return lazify(mat).inv_matmul(right_tensor, left_tensor)\n\n\ndef inv_quad(mat, tensor):\n    """"""\n    Computes an inverse quadratic form (w.r.t mat) with several right hand sides.\n    I.e. computes tr( tensor^T mat^{-1} tensor )\n\n    Args:\n        - tensor (tensor nxk) - Vector (or matrix) for inverse quad\n\n    Returns:\n        - tensor - tr( tensor^T (mat)^{-1} tensor )\n    """"""\n    res, _ = inv_quad_logdet(mat, inv_quad_rhs=tensor, logdet=False)\n    return res\n\n\ndef inv_quad_logdet(mat, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):\n    """"""\n    Computes an inverse quadratic form (w.r.t mat) with several right hand sides.\n    I.e. computes tr( tensor^T mat^{-1} tensor )\n    In addition, computes an (approximate) log determinant of the the matrix\n\n    Args:\n        - tensor (tensor nxk) - Vector (or matrix) for inverse quad\n\n    Returns:\n        - scalar - tr( tensor^T (mat)^{-1} tensor )\n        - scalar - log determinant\n    """"""\n    from ..lazy import lazify\n\n    return lazify(mat).inv_quad_logdet(inv_quad_rhs, logdet, reduce_inv_quad=reduce_inv_quad)\n\n\ndef logdet(mat):\n    """"""\n    Computes an (approximate) log determinant of the matrix\n\n    Returns:\n        - scalar - log determinant\n    """"""\n    _, res = inv_quad_logdet(mat, inv_quad_rhs=None, logdet=True)\n    return res\n\n\ndef root_decomposition(mat):\n    """"""\n    Returns a (usually low-rank) root decomposotion lazy tensor of a PSD matrix.\n    This can be used for sampling from a Gaussian distribution, or for obtaining a\n    low-rank version of a matrix\n    """"""\n    from ..lazy import lazify\n\n    return lazify(mat).root_decomposition()\n\n\ndef root_inv_decomposition(mat, initial_vectors=None, test_vectors=None):\n    """"""\n    Returns a (usually low-rank) root decomposotion lazy tensor of a PSD matrix.\n    This can be used for sampling from a Gaussian distribution, or for obtaining a\n    low-rank version of a matrix\n    """"""\n    from ..lazy import lazify\n\n    return lazify(mat).root_inv_decomposition(initial_vectors, test_vectors)\n\n\n__all__ = [\n    ""MaternCovariance"",\n    ""RBFCovariance"",\n    ""add_diag"",\n    ""dsmm"",\n    ""inv_matmul"",\n    ""inv_quad"",\n    ""inv_quad_logdet"",\n    ""logdet"",\n    ""log_normal_cdf"",\n    ""matmul"",\n    ""normal_cdf"",\n    ""root_decomposition"",\n    ""root_inv_decomposition"",\n    # Deprecated\n    ""inv_quad_log_det"",\n    ""log_det"",\n]\n'"
gpytorch/functions/_dsmm.py,1,"b'#!/usr/bin/env python3\n\nfrom torch.autograd import Function\n\nfrom ..utils.sparse import bdsmm\n\n\nclass DSMM(Function):\n    @staticmethod\n    def forward(ctx, sparse, dense):\n        ctx.sparse = sparse\n        return bdsmm(ctx.sparse, dense)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return None, bdsmm(ctx.sparse.transpose(-1, -2), grad_output)\n'"
gpytorch/functions/_inv_matmul.py,5,"b'#!/usr/bin/env python3\n\nimport torch\nfrom torch.autograd import Function\n\nfrom .. import settings\n\n\ndef _solve(lazy_tsr, rhs):\n    if settings.fast_computations.solves.off() or lazy_tsr.size(-1) <= settings.max_cholesky_size.value():\n        return lazy_tsr._cholesky()._cholesky_solve(rhs)\n    else:\n        with torch.no_grad():\n            preconditioner = lazy_tsr.detach()._inv_matmul_preconditioner()\n        return lazy_tsr._solve(rhs, preconditioner)\n\n\nclass InvMatmul(Function):\n    @staticmethod\n    def forward(ctx, representation_tree, has_left, *args):\n        left_tensor = None\n        right_tensor = None\n        matrix_args = None\n\n        ctx.representation_tree = representation_tree\n        ctx.has_left = has_left\n\n        if ctx.has_left:\n            left_tensor, right_tensor, *matrix_args = args\n        else:\n            right_tensor, *matrix_args = args\n        orig_right_tensor = right_tensor\n        lazy_tsr = ctx.representation_tree(*matrix_args)\n\n        ctx.is_vector = False\n        if right_tensor.ndimension() == 1:\n            right_tensor = right_tensor.unsqueeze(-1)\n            ctx.is_vector = True\n\n        # Perform solves (for inv_quad) and tridiagonalization (for estimating logdet)\n        if ctx.has_left:\n            rhs = torch.cat([left_tensor.transpose(-1, -2), right_tensor], -1)\n            solves = _solve(lazy_tsr, rhs)\n            res = solves[..., left_tensor.size(-2) :]\n            res = left_tensor @ res\n        else:\n            solves = _solve(lazy_tsr, right_tensor)\n            res = solves\n\n        if ctx.is_vector:\n            res = res.squeeze(-1)\n\n        if ctx.has_left:\n            args = [solves, left_tensor, orig_right_tensor] + list(matrix_args)\n        else:\n            args = [solves, orig_right_tensor] + list(matrix_args)\n        ctx.save_for_backward(*args)\n        if settings.memory_efficient.off():\n            ctx._lazy_tsr = lazy_tsr\n\n        return res\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Extract items that were saved\n        if ctx.has_left:\n            solves, left_tensor, right_tensor, *matrix_args = ctx.saved_tensors\n            left_solves = solves[..., : left_tensor.size(-2)]\n            right_solves = solves[..., left_tensor.size(-2) :]\n        else:\n            right_solves, right_tensor, *matrix_args = ctx.saved_tensors\n\n        # Get matrix functions\n        if hasattr(ctx, ""_lazy_tsr""):\n            lazy_tsr = ctx._lazy_tsr\n        else:\n            lazy_tsr = ctx.representation_tree(*matrix_args)\n\n        # Define gradient placeholders\n        arg_grads = [None] * len(matrix_args)\n        left_grad = None\n        right_grad = None\n        if any(ctx.needs_input_grad):\n            # De-vectorize objects\n            if ctx.is_vector:\n                right_tensor = right_tensor.unsqueeze(-1)\n                grad_output = grad_output.unsqueeze(-1)\n\n            if not ctx.has_left:\n                # Compute self^{-1} grad_output\n                left_solves = InvMatmul.apply(ctx.representation_tree, False, grad_output, *matrix_args)\n\n                if any(ctx.needs_input_grad[3:]):\n                    # We call _quad_form_derivative to compute dl/dK\n                    # To ensure that this term is symmetric, we concatenate the left and right solves together,\n                    # and divide the result by 1/2\n                    arg_grads = lazy_tsr._quad_form_derivative(\n                        torch.cat([left_solves, right_solves], -1), torch.cat([right_solves, left_solves], -1).mul(-0.5)\n                    )\n                if ctx.needs_input_grad[2]:\n                    right_grad = left_solves\n                    if ctx.is_vector:\n                        right_grad.squeeze_(-1)\n\n                return tuple([None, None] + [right_grad] + list(arg_grads))\n\n            else:\n                left_solves = left_solves @ grad_output\n\n                if ctx.needs_input_grad[3]:\n                    left_grad = grad_output @ right_solves.transpose(-1, -2)\n                if any(ctx.needs_input_grad[4:]):\n                    # We do this concatenation to ensure that the gradient of lazy_tsr is symmetric\n                    arg_grads = lazy_tsr._quad_form_derivative(\n                        torch.cat([left_solves, right_solves], -1), torch.cat([right_solves, left_solves], -1).mul(-0.5)\n                    )\n                if ctx.needs_input_grad[2]:\n                    right_grad = left_solves\n                    if ctx.is_vector:\n                        right_grad.squeeze_(-1)\n\n                return tuple([None, None] + [left_grad, right_grad] + list(arg_grads))\n'"
gpytorch/functions/_inv_quad.py,3,"b'#!/usr/bin/env python3\n\nimport torch\nfrom torch.autograd import Function\n\nfrom .. import settings\n\n\ndef _solve(lazy_tsr, rhs):\n    if (\n        settings.fast_computations.solves.off()\n        or settings.fast_computations.log_prob.off()\n        or lazy_tsr.size(-1) <= settings.max_cholesky_size.value()\n    ):\n        return lazy_tsr._cholesky()._cholesky_solve(rhs)\n    else:\n        with torch.no_grad():\n            preconditioner = lazy_tsr.detach()._inv_matmul_preconditioner()\n        return lazy_tsr._solve(rhs, preconditioner)\n\n\nclass InvQuad(Function):\n    """"""\n    Given a PSD matrix A (or a batch of PSD matrices A), this function computes b A^{-1} b\n    where b is a vector or batch of vectors\n    """"""\n\n    @staticmethod\n    def forward(ctx, representation_tree, *args):\n        """"""\n        *args - The arguments representing the PSD matrix A (or batch of PSD matrices A)\n        If inv_quad is true, the first entry in *args is inv_quad_rhs (Tensor)\n        - the RHS of the matrix solves.\n\n        Returns:\n        - (Scalar) The inverse quadratic form (or None, if inv_quad is False)\n        - (Scalar) The log determinant (or None, if logdet is False)\n        """"""\n        inv_quad_rhs, *matrix_args = args\n        ctx.representation_tree = representation_tree\n        # Get closure for matmul\n        lazy_tsr = ctx.representation_tree(*matrix_args)\n\n        # RHS for inv_quad\n        ctx.is_vector = False\n        if inv_quad_rhs.ndimension() == 1:\n            inv_quad_rhs = inv_quad_rhs.unsqueeze(-1)\n            ctx.is_vector = True\n\n        # Perform solves (for inv_quad) and tridiagonalization (for estimating logdet)\n        inv_quad_solves = _solve(lazy_tsr, inv_quad_rhs)\n        inv_quad_term = (inv_quad_solves * inv_quad_rhs).sum(-2)\n\n        to_save = matrix_args + [inv_quad_solves]\n        ctx.save_for_backward(*to_save)\n\n        if settings.memory_efficient.off():\n            ctx._lazy_tsr = lazy_tsr\n\n        return inv_quad_term\n\n    @staticmethod\n    def backward(ctx, inv_quad_grad_output):\n        *matrix_args, inv_quad_solves = ctx.saved_tensors\n\n        if hasattr(ctx, ""_lazy_tsr""):\n            lazy_tsr = ctx._lazy_tsr\n        else:\n            lazy_tsr = ctx.representation_tree(*matrix_args)\n\n        # Fix grad_output sizes\n        inv_quad_grad_output = inv_quad_grad_output.unsqueeze(-2)\n        neg_inv_quad_solves_times_grad_out = inv_quad_solves.mul(inv_quad_grad_output).mul(-1)\n\n        matrix_arg_grads = [None] * len(matrix_args)\n\n        # input_1 gradient\n        if any(ctx.needs_input_grad[2:]):\n            left_factors = neg_inv_quad_solves_times_grad_out\n            right_factors = inv_quad_solves\n            matrix_arg_grads = lazy_tsr._quad_form_derivative(left_factors, right_factors)\n\n        # input_2 gradients\n        if ctx.needs_input_grad[1]:\n            inv_quad_rhs_grad = neg_inv_quad_solves_times_grad_out.mul(-2)\n        else:\n            inv_quad_rhs_grad = torch.zeros_like(inv_quad_solves)\n        if ctx.is_vector:\n            inv_quad_rhs_grad.squeeze_(-1)\n\n        res = tuple([None] + [inv_quad_rhs_grad] + list(matrix_arg_grads))\n        return tuple(res)\n'"
gpytorch/functions/_inv_quad_log_det.py,18,"b'#!/usr/bin/env python3\n\nimport warnings\n\nimport torch\nfrom torch.autograd import Function\n\nfrom .. import settings\nfrom ..utils.lanczos import lanczos_tridiag_to_diag\nfrom ..utils.stochastic_lq import StochasticLQ\n\n\nclass InvQuadLogDet(Function):\n    """"""\n    Given a PSD matrix A (or a batch of PSD matrices A), this function computes one or both\n    of the following\n    - The matrix solves A^{-1} b\n    - logdet(A)\n    """"""\n\n    @staticmethod\n    def forward(\n        ctx,\n        representation_tree,\n        dtype,\n        device,\n        matrix_shape,\n        batch_shape=torch.Size(),\n        inv_quad=False,\n        logdet=False,\n        probe_vectors=None,\n        probe_vector_norms=None,\n        *args,\n    ):\n        """"""\n        *args - The arguments representing the PSD matrix A (or batch of PSD matrices A)\n        If self.inv_quad is true, the first entry in *args is inv_quad_rhs (Tensor)\n        - the RHS of the matrix solves.\n\n        Returns:\n        - (Scalar) The inverse quadratic form (or None, if self.inv_quad is False)\n        - (Scalar) The log determinant (or None, self.if logdet is False)\n        """"""\n\n        if not (inv_quad or logdet):\n            raise RuntimeError(""Either inv_quad or logdet must be true (or both)"")\n\n        ctx.representation_tree = representation_tree\n        ctx.dtype = dtype\n        ctx.device = device\n        ctx.matrix_shape = matrix_shape\n        ctx.batch_shape = batch_shape\n        ctx.inv_quad = inv_quad\n        ctx.logdet = logdet\n\n        matrix_args = None\n        inv_quad_rhs = None\n        if ctx.inv_quad:\n            matrix_args = args[1:]\n            inv_quad_rhs = args[0]\n        else:\n            matrix_args = args\n\n        # Get closure for matmul\n        lazy_tsr = ctx.representation_tree(*matrix_args)\n        with torch.no_grad():\n            preconditioner, precond_lt, logdet_correction = lazy_tsr._preconditioner()\n\n        ctx.preconditioner = preconditioner\n\n        if (probe_vectors is None or probe_vector_norms is None) and logdet:\n            num_random_probes = settings.num_trace_samples.value()\n            if preconditioner is None:\n                if settings.deterministic_probes.on():\n                    warnings.warn(\n                        ""Deterministic probes will currently work only if you aren\'t training multiple independent""\n                        "" models simultaneously."",\n                        UserWarning,\n                    )\n                    if settings.deterministic_probes.probe_vectors is None:\n                        probe_vectors = torch.empty(matrix_shape[-1], num_random_probes, dtype=dtype, device=device)\n                        probe_vectors.bernoulli_().mul_(2).add_(-1)\n                        settings.deterministic_probes.probe_vectors = probe_vectors\n                    else:\n                        probe_vectors = settings.deterministic_probes.probe_vectors\n                else:\n                    probe_vectors = torch.empty(matrix_shape[-1], num_random_probes, dtype=dtype, device=device)\n                    probe_vectors.bernoulli_().mul_(2).add_(-1)\n\n                probe_vector_norms = torch.norm(probe_vectors, 2, dim=-2, keepdim=True)\n                if batch_shape is not None:\n                    probe_vectors = probe_vectors.expand(*batch_shape, matrix_shape[-1], num_random_probes)\n                    probe_vector_norms = probe_vector_norms.expand(*batch_shape, 1, num_random_probes)\n            else:  # When preconditioning, probe vectors must be drawn from N(0, P)\n                if precond_lt.size()[-2:] == torch.Size([1, 1]):\n                    covar_root = precond_lt.evaluate().sqrt()\n                else:\n                    covar_root = precond_lt.root_decomposition().root\n\n                if settings.deterministic_probes.on():\n                    warnings.warn(\n                        ""Deterministic probes will currently work only if you aren\'t training multiple independent""\n                        "" models simultaneously."",\n                        UserWarning,\n                    )\n                    base_samples = settings.deterministic_probes.probe_vectors\n                    if base_samples is None or covar_root.size(-1) != base_samples.size(-2):\n                        base_samples = torch.randn(\n                            *precond_lt.batch_shape,\n                            covar_root.size(-1),\n                            num_random_probes,\n                            dtype=precond_lt.dtype,\n                            device=precond_lt.device,\n                        )\n                        settings.deterministic_probes.probe_vectors = base_samples\n\n                    probe_vectors = covar_root.matmul(base_samples).permute(-1, *range(precond_lt.dim() - 1))\n                else:\n                    base_samples = torch.randn(\n                        *precond_lt.batch_shape,\n                        covar_root.size(-1),\n                        num_random_probes,\n                        dtype=precond_lt.dtype,\n                        device=precond_lt.device,\n                    )\n                    probe_vectors = precond_lt.zero_mean_mvn_samples(num_random_probes)\n                probe_vectors = probe_vectors.unsqueeze(-2).transpose(0, -2).squeeze(0).transpose(-2, -1).contiguous()\n                probe_vector_norms = torch.norm(probe_vectors, p=2, dim=-2, keepdim=True)\n            probe_vectors = probe_vectors.div(probe_vector_norms)\n\n        ctx.probe_vectors = probe_vectors\n        ctx.probe_vector_norms = probe_vector_norms\n\n        if ctx.logdet and not ctx.probe_vectors.numel():\n            raise RuntimeError(""Probe vectors were not supplied for logdet computation"")\n\n        # Collect terms for LinearCG\n        # We use LinearCG for both matrix solves and for stochastically estimating the log det\n        rhs_list = []\n        num_random_probes = 0\n        num_inv_quad_solves = 0\n\n        # RHS for logdet\n        if ctx.logdet:\n            rhs_list.append(ctx.probe_vectors)\n            num_random_probes = ctx.probe_vectors.size(-1)\n\n        # RHS for inv_quad\n        ctx.is_vector = False\n        if ctx.inv_quad:\n            if inv_quad_rhs.ndimension() == 1:\n                inv_quad_rhs = inv_quad_rhs.unsqueeze(-1)\n                ctx.is_vector = True\n            rhs_list.append(inv_quad_rhs)\n            num_inv_quad_solves = inv_quad_rhs.size(-1)\n\n        # Perform solves (for inv_quad) and tridiagonalization (for estimating logdet)\n        rhs = torch.cat(rhs_list, -1)\n        t_mat = None\n        if ctx.logdet and settings.skip_logdet_forward.off():\n            solves, t_mat = lazy_tsr._solve(rhs, preconditioner, num_tridiag=num_random_probes)\n\n        else:\n            solves = lazy_tsr._solve(rhs, preconditioner, num_tridiag=0)\n\n        # Final values to return\n        logdet_term = torch.zeros(lazy_tsr.batch_shape, dtype=ctx.dtype, device=ctx.device)\n        inv_quad_term = torch.zeros(lazy_tsr.batch_shape, dtype=ctx.dtype, device=ctx.device)\n\n        # Compute logdet from tridiagonalization\n        if ctx.logdet and settings.skip_logdet_forward.off():\n            if torch.any(torch.isnan(t_mat)).item():\n                logdet_term = torch.tensor(float(""nan""), dtype=ctx.dtype, device=ctx.device)\n            else:\n                if ctx.batch_shape is None:\n                    t_mat = t_mat.unsqueeze(1)\n                eigenvalues, eigenvectors = lanczos_tridiag_to_diag(t_mat)\n                slq = StochasticLQ()\n                (logdet_term,) = slq.evaluate(ctx.matrix_shape, eigenvalues, eigenvectors, [lambda x: x.log()])\n\n                # Add correction\n                if logdet_correction is not None:\n                    logdet_term = logdet_term + logdet_correction\n\n        # Extract inv_quad solves from all the solves\n        if ctx.inv_quad:\n            inv_quad_solves = solves.narrow(-1, num_random_probes, num_inv_quad_solves)\n            inv_quad_term = (inv_quad_solves * inv_quad_rhs).sum(-2)\n\n        ctx.num_random_probes = num_random_probes\n        ctx.num_inv_quad_solves = num_inv_quad_solves\n\n        to_save = list(matrix_args) + [solves]\n        ctx.save_for_backward(*to_save)\n\n        if settings.memory_efficient.off():\n            ctx._lazy_tsr = lazy_tsr\n\n        return inv_quad_term, logdet_term\n\n    @staticmethod\n    def backward(ctx, inv_quad_grad_output, logdet_grad_output):\n        matrix_arg_grads = None\n        inv_quad_rhs_grad = None\n\n        # Which backward passes should we compute?\n        compute_inv_quad_grad = inv_quad_grad_output.abs().sum() and ctx.inv_quad\n        compute_logdet_grad = logdet_grad_output.abs().sum() and ctx.logdet\n\n        # Get input arguments, and get gradients in the proper form\n        matrix_args = ctx.saved_tensors[:-1]\n        solves = ctx.saved_tensors[-1]\n\n        if hasattr(ctx, ""_lazy_tsr""):\n            lazy_tsr = ctx._lazy_tsr\n        else:\n            lazy_tsr = ctx.representation_tree(*matrix_args)\n\n        # Fix grad_output sizes\n        if ctx.inv_quad:\n            inv_quad_grad_output = inv_quad_grad_output.unsqueeze(-2)\n        if compute_logdet_grad:\n            logdet_grad_output = logdet_grad_output.unsqueeze(-1)\n            logdet_grad_output.unsqueeze_(-1)\n\n        # Divide up the solves\n        probe_vector_solves = None\n        inv_quad_solves = None\n        neg_inv_quad_solves_times_grad_out = None\n        if compute_logdet_grad:\n            coef = 1.0 / ctx.probe_vectors.size(-1)\n            probe_vector_solves = solves.narrow(-1, 0, ctx.num_random_probes).mul(coef)\n            probe_vector_solves.mul_(ctx.probe_vector_norms).mul_(logdet_grad_output)\n            probe_vectors = ctx.probe_vectors.mul(ctx.probe_vector_norms)\n        if ctx.inv_quad:\n            inv_quad_solves = solves.narrow(-1, ctx.num_random_probes, ctx.num_inv_quad_solves)\n            neg_inv_quad_solves_times_grad_out = inv_quad_solves.mul(inv_quad_grad_output).mul_(-1)\n\n        # input_1 gradient\n        if any(ctx.needs_input_grad):\n            # Collect terms for arg grads\n            left_factors_list = []\n            right_factors_list = []\n\n            if compute_logdet_grad:\n                left_factors_list.append(probe_vector_solves)\n                if ctx.preconditioner is not None:\n                    probe_vectors = ctx.preconditioner(probe_vectors)\n                right_factors_list.append(probe_vectors)\n\n            if compute_inv_quad_grad:\n                left_factors_list.append(neg_inv_quad_solves_times_grad_out)\n                right_factors_list.append(inv_quad_solves)\n\n            left_factors = torch.cat(left_factors_list, -1)\n            right_factors = torch.cat(right_factors_list, -1)\n            matrix_arg_grads = lazy_tsr._quad_form_derivative(left_factors, right_factors)\n\n        # input_2 gradients\n        if compute_inv_quad_grad and ctx.needs_input_grad[9]:\n            inv_quad_rhs_grad = neg_inv_quad_solves_times_grad_out.mul_(-2)\n        elif ctx.inv_quad:\n            inv_quad_rhs_grad = torch.zeros_like(inv_quad_solves)\n        if ctx.is_vector:\n            inv_quad_rhs_grad.squeeze_(-1)\n\n        if ctx.inv_quad:\n            res = [inv_quad_rhs_grad] + list(matrix_arg_grads)\n        else:\n            res = list(matrix_arg_grads)\n\n        return tuple([None] * 9 + res)\n'"
gpytorch/functions/_log_normal_cdf.py,13,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\nfrom torch.autograd import Function\nfrom torch.distributions import Normal\n\n\nclass LogNormalCDF(Function):\n    @staticmethod\n    def forward(ctx, z):\n        c = torch.tensor(\n            [\n                0.00048204,\n                -0.00142906,\n                0.0013200243174,\n                0.0009461589032,\n                -0.0045563339802,\n                0.00556964649138,\n                0.00125993961762116,\n                -0.01621575378835404,\n                0.02629651521057465,\n                -0.001829764677455021,\n                2 * (1 - math.pi / 3),\n                (4 - math.pi) / 3,\n                1,\n                1,\n            ],\n            dtype=z.dtype,\n            device=z.device,\n        )\n\n        r = torch.tensor(\n            [\n                1.2753666447299659525,\n                5.019049726784267463450,\n                6.1602098531096305441,\n                7.409740605964741794425,\n                2.9788656263939928886,\n            ],\n            dtype=z.dtype,\n            device=z.device,\n        )\n\n        q = torch.tensor(\n            [\n                2.260528520767326969592,\n                9.3960340162350541504,\n                12.048951927855129036034,\n                17.081440747466004316,\n                9.608965327192787870698,\n                3.3690752069827527677,\n            ],\n            dtype=z.dtype,\n            device=z.device,\n        )\n\n        log_phi_z = torch.zeros_like(z)\n\n        # Three cases to handle: An entry of z is near zero, an entry of z is small, or an entry of z neither of these.\n        z_near_zero = z.pow(2).lt(0.04)\n        z_is_small = z.lt(-1)\n        z_is_ordinary = ~(z_near_zero | z_is_small)\n\n        # Case 1: Entries of z that are near zero\n        if z_near_zero.sum() > 0:\n            log_phi_first = -z.masked_select(z_near_zero).div_(math.sqrt(2 * math.pi))\n            f = 0\n            for c_i in c.tolist():\n                f = log_phi_first.mul(c_i + f)\n\n            log_phi_z.masked_scatter_(z_near_zero, f.mul_(-2).sub_(math.log(2)))\n\n        # Case 2: Entries of z that are very small\n        if z_is_small.sum() > 0:\n            z_where_z_is_small = z.masked_select(z_is_small)\n            numerator = torch.tensor(0.5641895835477550741, dtype=z.dtype, device=z.device)\n            numerator = numerator.expand_as(z_where_z_is_small)\n            denominator = torch.tensor(1.0, dtype=z.dtype, device=z.device)\n            denominator = denominator.expand_as(z_where_z_is_small)\n\n            for r_i in r:\n                numerator = -z_where_z_is_small.mul(numerator.div(math.sqrt(2))) + r_i\n\n            for q_i in q:\n                denominator = -z_where_z_is_small.mul(denominator.div(math.sqrt(2))) + q_i\n\n            e = numerator.div(denominator)\n            log_phi_z.masked_scatter_(z_is_small, torch.log(e / 2) - z_where_z_is_small.pow(2).div_(2))\n\n            ctx.denominator = denominator\n            ctx.numerator = numerator\n\n        log_phi_z.masked_scatter_(z_is_ordinary, torch.log(Normal(0.0, 1.0).cdf(z.masked_select(z_is_ordinary))))\n\n        ctx.save_for_backward(z, log_phi_z)\n        return log_phi_z\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        z, log_phi_z = ctx.saved_tensors\n        log_phi_z_grad = torch.zeros_like(z)\n\n        z_is_small = z.lt(-1)\n        z_is_not_small = ~z_is_small\n\n        if z_is_small.sum() > 0:\n            log_phi_z_grad[z_is_small] = torch.abs(ctx.denominator.div(ctx.numerator)).mul(math.sqrt(2 / math.pi))\n\n        exp = z[z_is_not_small].pow(2).div(-2).sub(log_phi_z[z_is_not_small]).add(math.log(0.5))\n\n        log_phi_z_grad[z_is_not_small] = torch.exp(exp).mul(math.sqrt(2 / math.pi))\n\n        return log_phi_z_grad.mul(grad_output)\n'"
gpytorch/functions/_matmul.py,1,"b'#!/usr/bin/env python3\n\nfrom torch.autograd import Function\n\nfrom .. import settings\n\n\nclass Matmul(Function):\n    @staticmethod\n    def forward(ctx, representation_tree, rhs, *matrix_args):\n        ctx.representation_tree = representation_tree\n        orig_rhs = rhs\n\n        if rhs.ndimension() == 1:\n            is_vector = True\n            rhs = rhs.unsqueeze(-1)\n        else:\n            is_vector = False\n\n        lazy_tsr = ctx.representation_tree(*matrix_args)\n        res = lazy_tsr._matmul(rhs)\n\n        to_save = [orig_rhs] + list(matrix_args)\n        ctx.save_for_backward(*to_save)\n        if settings.memory_efficient.off():\n            ctx._lazy_tsr = lazy_tsr\n\n        # Squeeze if necessary\n        if is_vector:\n            res = res.squeeze(-1)\n        return res\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        rhs = ctx.saved_tensors[0]\n        matrix_args = ctx.saved_tensors[1:]\n        rhs_shape = rhs.shape\n\n        rhs_grad = None\n        arg_grads = [None] * len(matrix_args)\n\n        # input_1 gradient\n        if any(ctx.needs_input_grad[2:]):\n            rhs = rhs.unsqueeze(-1) if (rhs.ndimension() == 1) else rhs\n            grad_output_matrix = grad_output.unsqueeze(-1) if grad_output.ndimension() == 1 else grad_output\n            arg_grads = ctx.representation_tree(*matrix_args)._quad_form_derivative(grad_output_matrix, rhs)\n\n        # input_2 gradient\n        if ctx.needs_input_grad[1]:\n            if hasattr(ctx, ""_lazy_tsr""):\n                lazy_tsr = ctx._lazy_tsr\n            else:\n                lazy_tsr = ctx.representation_tree(*matrix_args)\n\n            if grad_output.dim() == 1:\n                # Confusing Cublas_Sgemv bug when grad_output is single dimensional on GPU.\n                rhs_grad = lazy_tsr._t_matmul(grad_output.unsqueeze(-1)).squeeze(-1)\n            else:\n                rhs_grad = lazy_tsr._t_matmul(grad_output)\n\n            # For broadcasting\n            if rhs_grad.dim() > len(rhs_shape):\n                rhs_grad = rhs_grad.reshape(-1, *rhs_shape).sum(0)\n\n        return tuple([None] + [rhs_grad] + list(arg_grads))\n'"
gpytorch/functions/_root_decomposition.py,7,"b'#!/usr/bin/env python3\n\nimport torch\nfrom torch.autograd import Function\n\nfrom .. import settings\nfrom ..utils import lanczos\n\n\nclass RootDecomposition(Function):\n    @staticmethod\n    def forward(\n        ctx,\n        representation_tree,\n        max_iter,\n        dtype,\n        device,\n        batch_shape,\n        matrix_shape,\n        root,\n        inverse,\n        initial_vectors,\n        *matrix_args,\n    ):\n        r""""""\n        :param list matrix_args: The arguments representing the symmetric matrix A (or batch of PSD matrices A)\n\n        :rtype: (torch.Tensor, torch.Tensor)\n        :return: :attr:`R`, such that :math:`R R^T \\approx A`, and :attr:`R_inv`, such that\n            :math:`R_{inv} R_{inv}^T \\approx A^{-1}` (will only be populated if self.inverse = True)\n        """"""\n        from ..lazy import lazify\n\n        ctx.representation_tree = representation_tree\n        ctx.device = device\n        ctx.dtype = dtype\n        ctx.matrix_shape = matrix_shape\n        ctx.max_iter = max_iter\n        ctx.batch_shape = batch_shape\n        ctx.root = root\n        ctx.inverse = inverse\n        ctx.initial_vectors = initial_vectors\n\n        # Get closure for matmul\n        lazy_tsr = ctx.representation_tree(*matrix_args)\n        matmul_closure = lazy_tsr._matmul\n        # Do lanczos\n        q_mat, t_mat = lanczos.lanczos_tridiag(\n            matmul_closure,\n            ctx.max_iter,\n            dtype=ctx.dtype,\n            device=ctx.device,\n            matrix_shape=ctx.matrix_shape,\n            batch_shape=ctx.batch_shape,\n            init_vecs=ctx.initial_vectors,\n        )\n\n        if ctx.batch_shape is None:\n            q_mat = q_mat.unsqueeze(-3)\n            t_mat = t_mat.unsqueeze(-3)\n        if t_mat.ndimension() == 3:  # If we only used one probe vector\n            q_mat = q_mat.unsqueeze(0)\n            t_mat = t_mat.unsqueeze(0)\n        n_probes = t_mat.size(0)\n\n        mins = lazify(t_mat).diag().min(dim=-1, keepdim=True)[0].unsqueeze(-1)\n        jitter_mat = (settings.tridiagonal_jitter.value() * mins) * torch.eye(\n            t_mat.size(-1), device=t_mat.device, dtype=t_mat.dtype\n        ).expand_as(t_mat)\n        eigenvalues, eigenvectors = lanczos.lanczos_tridiag_to_diag(t_mat + jitter_mat)\n\n        # Get orthogonal matrix and eigenvalue roots\n        q_mat = q_mat.matmul(eigenvectors)\n        root_evals = eigenvalues.sqrt()\n\n        # Store q_mat * t_mat_chol\n        # Decide if we\'re computing the inverse, or the regular root\n        root = torch.empty(0, dtype=q_mat.dtype, device=q_mat.device)\n        inverse = torch.empty(0, dtype=q_mat.dtype, device=q_mat.device)\n        if ctx.inverse:\n            inverse = q_mat / root_evals.unsqueeze(-2)\n        if ctx.root:\n            root = q_mat * root_evals.unsqueeze(-2)\n\n        if settings.memory_efficient.off():\n            ctx._lazy_tsr = lazy_tsr\n\n        if ctx.batch_shape is None:\n            root = root.squeeze(1) if root.numel() else root\n            q_mat = q_mat.squeeze(1)\n            t_mat = t_mat.squeeze(1)\n            root_evals = root_evals.squeeze(1)\n            inverse = inverse.squeeze(1) if inverse.numel() else inverse\n        if n_probes == 1:\n            root = root.squeeze(0) if root.numel() else root\n            q_mat = q_mat.squeeze(0)\n            t_mat = t_mat.squeeze(0)\n            root_evals = root_evals.squeeze(0)\n            inverse = inverse.squeeze(0) if inverse.numel() else inverse\n\n        to_save = list(matrix_args) + [q_mat, root_evals, inverse]\n        ctx.save_for_backward(*to_save)\n        return root, inverse\n\n    @staticmethod\n    def backward(ctx, root_grad_output, inverse_grad_output):\n        # Taken from http://homepages.inf.ed.ac.uk/imurray2/pub/16choldiff/choldiff.pdf\n        if any(ctx.needs_input_grad):\n\n            def is_empty(tensor):\n                return tensor.numel() == 0 or (tensor.numel() == 1 and tensor[0] == 0)\n\n            # Fix outputs and gradients\n            if is_empty(root_grad_output):\n                root_grad_output = None\n            if is_empty(inverse_grad_output):\n                inverse_grad_output = None\n\n            # Get saved tensors\n            matrix_args = ctx.saved_tensors[:-3]\n            q_mat = ctx.saved_tensors[-3]\n            root_evals = ctx.saved_tensors[-2]\n            inverse = ctx.saved_tensors[-1]\n            is_batch = False\n\n            if root_grad_output is not None:\n                if root_grad_output.ndimension() == 2 and q_mat.ndimension() > 2:\n                    root_grad_output = root_grad_output.unsqueeze(0)\n                    is_batch = True\n                if root_grad_output.ndimension() == 3 and q_mat.ndimension() > 3:\n                    root_grad_output = root_grad_output.unsqueeze(0)\n                    is_batch = True\n            if inverse_grad_output is not None:\n                if inverse_grad_output.ndimension() == 2 and q_mat.ndimension() > 2:\n                    inverse_grad_output = inverse_grad_output.unsqueeze(0)\n                    is_batch = True\n                if inverse_grad_output.ndimension() == 3 and q_mat.ndimension() > 3:\n                    inverse_grad_output = inverse_grad_output.unsqueeze(0)\n                    is_batch = True\n\n            # Get closure for matmul\n            if hasattr(ctx, ""_lazy_tsr""):\n                lazy_tsr = ctx._lazy_tsr\n            else:\n                lazy_tsr = ctx.representation_tree(*matrix_args)\n\n            # Get root inverse\n            if not ctx.inverse:\n                inverse = q_mat / root_evals.unsqueeze(-2)\n            # Left factor:\n            left_factor = torch.zeros_like(inverse)\n            if root_grad_output is not None:\n                left_factor.add_(root_grad_output)\n            if inverse_grad_output is not None:\n                # -root^-T grad_output.T root^-T\n                left_factor.sub_(torch.matmul(inverse, inverse_grad_output.transpose(-1, -2)).matmul(inverse))\n\n            # Right factor\n            right_factor = inverse.div(2.0)\n\n            # Fix batches\n            if is_batch:\n                left_factor = left_factor.permute(1, 0, 2, 3).contiguous()\n                left_factor = left_factor.view(inverse.size(1), -1, left_factor.size(-1))\n                right_factor = right_factor.permute(1, 0, 2, 3).contiguous()\n                right_factor = right_factor.view(inverse.size(1), -1, right_factor.size(-1))\n            else:\n                left_factor = left_factor.contiguous()\n                right_factor = right_factor.contiguous()\n            res = lazy_tsr._quad_form_derivative(left_factor, right_factor)\n\n            return tuple([None] * 9 + list(res))\n        else:\n            pass\n'"
gpytorch/functions/matern_covariance.py,1,"b'import math\n\nimport torch\n\n\nclass MaternCovariance(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x1, x2, lengthscale, nu, dist_func):\n        if any(ctx.needs_input_grad[:2]):\n            raise RuntimeError(""MaternCovariance cannot compute gradients with "" ""respect to x1 and x2"")\n        if lengthscale.size(-1) > 1:\n            raise ValueError(""MaternCovariance cannot handle multiple lengthscales"")\n        # Subtract mean for numerical stability. Won\'t affect computations\n        # because covariance matrix is stationary.\n        needs_grad = any(ctx.needs_input_grad)\n        mean = x1.reshape(-1, x1.size(-1)).mean(0)[(None,) * (x1.dim() - 1)]\n        x1_ = (x1 - mean).div(lengthscale)\n        x2_ = (x2 - mean).div(lengthscale)\n        scaled_unitless_dist = dist_func(x1_, x2_).mul_(math.sqrt(2 * nu))\n        if nu == 0.5:\n            # 1 kernel sized Tensor if no grad else 2\n            scaled_unitless_dist_ = scaled_unitless_dist.clone() if needs_grad else scaled_unitless_dist\n            exp_component = scaled_unitless_dist_.neg_().exp_()\n            covar_mat = exp_component\n            if needs_grad:\n                d_output_d_input = scaled_unitless_dist.div_(lengthscale).mul_(exp_component)\n        elif nu == 1.5:\n            # 2 kernel sized Tensors if no grad else 3\n            if needs_grad:\n                scaled_unitless_dist_ = scaled_unitless_dist.clone()\n            linear_term = scaled_unitless_dist.clone().add_(1)\n            exp_component = scaled_unitless_dist.neg_().exp_()\n            covar_mat = linear_term.mul_(exp_component)\n            if needs_grad:\n                d_output_d_input = scaled_unitless_dist_.pow_(2).div_(lengthscale).mul_(exp_component)\n        elif nu == 2.5:\n            # 3 kernel sized Tensors if no grad else 4\n            linear_term = scaled_unitless_dist.clone().add_(1)\n            quadratic_term = scaled_unitless_dist.clone().pow_(2).div_(3)\n            exp_component = scaled_unitless_dist.neg_().exp_()\n            if needs_grad:\n                covar_mat = (linear_term + quadratic_term).mul_(exp_component)\n                d_output_d_input = linear_term.mul_(quadratic_term).mul_(exp_component).div_(lengthscale)\n            else:\n                covar_mat = exp_component.mul_(linear_term.add_(quadratic_term))\n        if needs_grad:\n            ctx.save_for_backward(d_output_d_input)\n        return covar_mat\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        d_output_d_input = ctx.saved_tensors[0]\n        lengthscale_grad = grad_output * d_output_d_input\n        return None, None, lengthscale_grad, None, None\n'"
gpytorch/functions/rbf_covariance.py,1,"b'import torch\n\n\nclass RBFCovariance(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x1, x2, lengthscale, sq_dist_func):\n        if any(ctx.needs_input_grad[:2]):\n            raise RuntimeError(""RBFCovariance cannot compute gradients with "" ""respect to x1 and x2"")\n        if lengthscale.size(-1) > 1:\n            raise ValueError(""RBFCovariance cannot handle multiple lengthscales"")\n        needs_grad = any(ctx.needs_input_grad)\n        x1_ = x1.div(lengthscale)\n        x2_ = x2.div(lengthscale)\n        unitless_sq_dist = sq_dist_func(x1_, x2_)\n        # clone because inplace operations will mess with what\'s saved for backward\n        unitless_sq_dist_ = unitless_sq_dist.clone() if needs_grad else unitless_sq_dist\n        covar_mat = unitless_sq_dist_.div_(-2.0).exp_()\n        if needs_grad:\n            d_output_d_input = unitless_sq_dist.mul_(covar_mat).div_(lengthscale)\n            ctx.save_for_backward(d_output_d_input)\n        return covar_mat\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        d_output_d_input = ctx.saved_tensors[0]\n        lengthscale_grad = grad_output * d_output_d_input\n        return None, None, lengthscale_grad, None\n'"
gpytorch/kernels/__init__.py,0,"b'#!/usr/bin/env python3\nfrom . import keops\nfrom .additive_structure_kernel import AdditiveStructureKernel\nfrom .arc_kernel import ArcKernel\nfrom .cosine_kernel import CosineKernel\nfrom .cylindrical_kernel import CylindricalKernel\nfrom .grid_interpolation_kernel import GridInterpolationKernel\nfrom .grid_kernel import GridKernel\nfrom .index_kernel import IndexKernel\nfrom .inducing_point_kernel import InducingPointKernel\nfrom .kernel import AdditiveKernel, Kernel, ProductKernel\nfrom .lcm_kernel import LCMKernel\nfrom .linear_kernel import LinearKernel\nfrom .matern_kernel import MaternKernel\nfrom .multi_device_kernel import MultiDeviceKernel\nfrom .multitask_kernel import MultitaskKernel\nfrom .newton_girard_additive_kernel import NewtonGirardAdditiveKernel\nfrom .periodic_kernel import PeriodicKernel\nfrom .polynomial_kernel import PolynomialKernel\nfrom .polynomial_kernel_grad import PolynomialKernelGrad\nfrom .product_structure_kernel import ProductStructureKernel\nfrom .rbf_kernel import RBFKernel\nfrom .rbf_kernel_grad import RBFKernelGrad\nfrom .rq_kernel import RQKernel\nfrom .scale_kernel import ScaleKernel\nfrom .spectral_mixture_kernel import SpectralMixtureKernel\n\n__all__ = [\n    ""keops"",\n    ""Kernel"",\n    ""ArcKernel"",\n    ""AdditiveKernel"",\n    ""AdditiveStructureKernel"",\n    ""CylindricalKernel"",\n    ""MultiDeviceKernel"",\n    ""CosineKernel"",\n    ""GridKernel"",\n    ""GridInterpolationKernel"",\n    ""IndexKernel"",\n    ""InducingPointKernel"",\n    ""LCMKernel"",\n    ""LinearKernel"",\n    ""MaternKernel"",\n    ""MultitaskKernel"",\n    ""NewtonGirardAdditiveKernel"",\n    ""PeriodicKernel"",\n    ""PolynomialKernel"",\n    ""PolynomialKernelGrad"",\n    ""ProductKernel"",\n    ""ProductStructureKernel"",\n    ""RBFKernel"",\n    ""RBFKernelGrad"",\n    ""RQKernel"",\n    ""ScaleKernel"",\n    ""SpectralMixtureKernel"",\n]\n'"
gpytorch/kernels/additive_structure_kernel.py,1,"b'#!/usr/bin/env python3\n\nfrom .kernel import Kernel\n\n\nclass AdditiveStructureKernel(Kernel):\n    r""""""\n    A Kernel decorator for kernels with additive structure. If a kernel decomposes\n    additively, then this module will be much more computationally efficient.\n\n    A kernel function `k` decomposes additively if it can be written as\n\n    .. math::\n\n       \\begin{equation*}\n          k(\\mathbf{x_1}, \\mathbf{x_2}) = k\'(x_1^{(1)}, x_2^{(1)}) + \\ldots + k\'(x_1^{(d)}, x_2^{(d)})\n       \\end{equation*}\n\n    for some kernel :math:`k\'` that operates on a subset of dimensions.\n\n    Given a `b x n x d` input, `AdditiveStructureKernel` computes `d` one-dimensional kernels\n    (using the supplied base_kernel), and then adds the component kernels together.\n    Unlike :class:`~gpytorch.kernels.AdditiveKernel`, `AdditiveStructureKernel` computes each\n    of the additive terms in batch, making it very fast.\n\n    Args:\n        :attr:`base_kernel` (Kernel):\n            The kernel to approximate with KISS-GP\n        :attr:`num_dims` (int):\n            The dimension of the input data.\n        :attr:`active_dims` (tuple of ints, optional):\n            Passed down to the `base_kernel`.\n    """"""\n\n    @property\n    def is_stationary(self) -> bool:\n        """"""\n        Kernel is stationary if the base kernel is stationary.\n        """"""\n        return self.base_kernel.is_stationary\n\n    def __init__(self, base_kernel, num_dims, active_dims=None):\n        super(AdditiveStructureKernel, self).__init__(active_dims=active_dims)\n        self.base_kernel = base_kernel\n        self.num_dims = num_dims\n\n    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n        if last_dim_is_batch:\n            raise RuntimeError(""AdditiveStructureKernel does not accept the last_dim_is_batch argument."")\n\n        res = self.base_kernel(x1, x2, diag=diag, last_dim_is_batch=True, **params)\n        res = res.sum(-2 if diag else -3)\n        return res\n\n    def prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood):\n        return self.base_kernel.prediction_strategy(train_inputs, train_prior_dist, train_labels, likelihood)\n\n    def num_outputs_per_input(self, x1, x2):\n        return self.base_kernel.num_outputs_per_input(x1, x2)\n'"
gpytorch/kernels/arc_kernel.py,28,"b'#!/usr/bin/env python3\n\nfrom math import pi\nfrom typing import Optional\n\nimport torch\n\nfrom ..constraints import Interval, Positive\nfrom ..priors import Prior\nfrom .kernel import Kernel\n\n\nclass ArcKernel(Kernel):\n    r"""""" Computes a covariance matrix based on the Arc Kernel\n    (https://arxiv.org/abs/1409.4011) between inputs :math:`\\mathbf{x_1}`\n    and :math:`\\mathbf{x_2}`. First it applies a cylindrical embedding:\n\n    .. math::\n        g_{i}(\\mathbf{x}) = \\begin{cases}\n        [0, 0]^{T} & \\delta_{i}(\\mathbf{x}) = \\text{false}\\\\\n        \\omega_{i} \\left[ \\sin{\\pi\\rho_{i}\\frac{x_{i}}{u_{i}-l_{i}}},\n        \\cos{\\pi\\rho_{i}\\frac{x_{i}}{u_{i}-l_{i}}} \\right] & \\text{otherwise}\n        \\end{cases}\n\n    where\n    * :math:`\\rho` is the angle parameter.\n    * :math:`\\omega` is a radius parameter.\n\n    then the kernel is built with the particular covariance function, e.g.\n\n    .. math::\n        \\begin{equation}\n        k_{i}(\\mathbf{x}, \\mathbf{x\'}) =\n        \\sigma^{2}\\exp \\left(-\\frac{1}{2}d_{i}(\\mathbf{x}, \\mathbf{x^{\'}}) \\right)^{2}\n        \\end{equation}\n\n    and the produt between dimensions\n\n    .. math::\n        \\begin{equation}\n        k_{i}(\\mathbf{x}, \\mathbf{x\'}) =\n        \\sigma^{2}\\exp \\left(-\\frac{1}{2}d_{i}(\\mathbf{x}, \\mathbf{x^{\'}}) \\right)^{2}\n        \\end{equation}\n\n    .. note::\n        This kernel does not have an `outputscale` parameter. To add a scaling\n        parameter, decorate this kernel with a\n        :class:`gpytorch.kernels.ScaleKernel`.\n        When using with an input of `b x n x d` dimensions, decorate this\n        kernel with :class:`gpytorch.kernel.ProductStructuredKernel , setting\n        the number of dims, `num_dims to d.`\n\n    .. note::\n        This kernel does not have an ARD lengthscale option.\n\n    :param base_kernel: (Default :obj:`gpytorch.kernels.MaternKernel(nu=2.5)`.)\n        The euclidean covariance of choice.\n    :type base_kernel: :obj:`~gpytorch.kernels.Kernel`\n    :param ard_num_dims: (Default `None`.) The number of dimensions to compute the kernel for.\n        The kernel has two parameters which are individually defined for each\n        dimension, defaults to None\n    :type ard_num_dims: int, optional\n    :param angle_prior: Set this if you want to apply a prior to the period angle parameter.\n    :type angle_prior: :obj:`~gpytorch.priors.Prior`, optional\n    :param radius_prior: Set this if you want to apply a prior to the lengthscale parameter.\n    :type radius_prior: :obj:`~gpytorch.priors.Prior`, optional\n\n    :var torch.Tensor radius: The radius parameter. Size = `*batch_shape  x 1`.\n    :var torch.Tensor angle: The period angle parameter. Size = `*batch_shape  x 1`.\n\n    Example:\n        >>> x = torch.randn(10, 5)\n        >>> # Non-batch: Simple option\n        ... base_kernel = gpytorch.kernels.MaternKernel(nu=2.5)\n        >>> base_kernel.raw_lengthscale.requires_grad_(False)\n        >>> covar_module = gpytorch.kernels.ProductStructureKernel(\n                gpytorch.kernels.ScaleKernel(\n                    ArcKernel(base_kernel,\n                              angle_prior=gpytorch.priors.GammaPrior(0.5,1),\n                              radius_prior=gpytorch.priors.GammaPrior(3,2),\n                              ard_num_dims=x.shape[-1])),\n                num_dims=x.shape[-1])\n        >>> covar = covar_module(x)\n        >>> print(covar.shape)\n        >>> # Now with batch\n        >>> covar_module = gpytorch.kernels.ProductStructureKernel(\n                gpytorch.kernels.ScaleKernel(\n                    ArcKernel(base_kernel,\n                              angle_prior=gpytorch.priors.GammaPrior(0.5,1),\n                              radius_prior=gpytorch.priors.GammaPrior(3,2),\n                              ard_num_dims=x.shape[-1])),\n                num_dims=x.shape[-1])\n        >>> covar = covar_module(x\n        >>> print(covar.shape)\n    """"""\n\n    has_lengthscale = True\n\n    def __init__(\n        self,\n        base_kernel,\n        delta_func: Optional = None,\n        angle_prior: Optional[Prior] = None,\n        radius_prior: Optional[Prior] = None,\n        **kwargs,\n    ):\n        super(ArcKernel, self).__init__(has_lengthscale=True, **kwargs)\n\n        if self.ard_num_dims is None:\n            self.last_dim = 1\n        else:\n            self.last_dim = self.ard_num_dims\n\n        if delta_func is None:\n            self.delta_func = self.default_delta_func\n        else:\n            self.delta_func = delta_func\n\n        # TODO: check the errors given by interval\n        angle_constraint = Interval(0.1, 0.9)\n        self.register_parameter(\n            name=""raw_angle"", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, self.last_dim)),\n        )\n        if angle_prior is not None:\n            self.register_prior(\n                ""angle_prior"", angle_prior, lambda: self.angle, lambda v: self._set_angle(v),\n            )\n\n        self.register_constraint(""raw_angle"", angle_constraint)\n\n        self.register_parameter(\n            name=""raw_radius"", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, self.last_dim)),\n        )\n\n        if radius_prior is not None:\n            self.register_prior(\n                ""radius_prior"", radius_prior, lambda: self.radius, lambda v: self._set_radius(v),\n            )\n\n        radius_constraint = Positive()\n        self.register_constraint(""raw_radius"", radius_constraint)\n\n        self.base_kernel = base_kernel\n        if self.base_kernel.has_lengthscale:\n            self.base_kernel.lengthscale = 1\n            self.base_kernel.raw_lengthscale.requires_grad_(False)\n\n    @property\n    def angle(self):\n        return self.raw_angle_constraint.transform(self.raw_angle)\n\n    @angle.setter\n    def angle(self, value):\n        self._set_angle(value)\n\n    def _set_angle(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_angle)\n        self.initialize(raw_angle=self.raw_angle_constraint.inverse_transform(value))\n\n    @property\n    def radius(self):\n        return self.raw_radius_constraint.transform(self.raw_radius)\n\n    @radius.setter\n    def radius(self, value):\n        self._set_radius(value)\n\n    def _set_radius(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_radius)\n        self.initialize(raw_radius=self.raw_radius_constraint.inverse_transform(value))\n\n    def embedding(self, x):\n        mask = self.delta_func(x)\n        x_ = x.div(self.lengthscale)\n        x_s = self.radius * torch.sin(pi * self.angle * x_) * mask\n        x_c = self.radius * torch.cos(pi * self.angle * x_) * mask\n        x_ = torch.cat((x_s, x_c), dim=-1)\n        return x_\n\n    def default_delta_func(self, x):\n        return torch.ones_like(x)\n\n    def forward(self, x1, x2, diag=False, **params):\n        x1_, x2_ = self.embedding(x1), self.embedding(x2)\n        return self.base_kernel(x1_, x2_, diag=diag)\n'"
gpytorch/kernels/cosine_kernel.py,11,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\n\nfrom ..constraints import Positive\nfrom .kernel import Kernel\n\n\nclass CosineKernel(Kernel):\n    r""""""\n    Computes a covariance matrix based on the cosine kernel\n    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n\n    .. math::\n\n       \\begin{equation*}\n          k_{\\text{Cosine}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\cos \\left(\n            \\pi \\Vert \\mathbf{x_1} - \\mathbf{x_2} \\Vert_2 / p \\right)\n       \\end{equation*}\n\n    where :math:`p` is the period length parameter.\n\n    Args:\n        :attr:`batch_shape` (torch.Size, optional):\n            Set this if you want a separate lengthscale for each\n            batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`\n        :attr:`active_dims` (tuple of ints, optional):\n            Set this if you want to compute the covariance of only a few input dimensions. The ints\n            corresponds to the indices of the dimensions. Default: `None`.\n        :attr:`period_length_prior` (Prior, optional):\n            Set this if you want to apply a prior to the period length parameter.  Default: `None`\n        :attr:`period_length_constraint` (Constraint, optional):\n            Set this if you want to apply a constraint to the period length parameter. Default: `Positive`.\n        :attr:`eps` (float):\n            The minimum value that the lengthscale/period length can take\n            (prevents divide by zero errors). Default: `1e-6`.\n\n    Attributes:\n        :attr:`period_length` (Tensor):\n            The period length parameter. Size = `*batch_shape x 1 x 1`.\n\n    Example:\n        >>> x = torch.randn(10, 5)\n        >>> # Non-batch: Simple option\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.CosineKernel())\n        >>>\n        >>> batch_x = torch.randn(2, 10, 5)\n        >>> # Batch: Simple option\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.CosineKernel())\n        >>> # Batch: different lengthscale for each batch\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.CosineKernel(batch_shape=torch.Size([2])))\n        >>> covar = covar_module(x)  # Output: LazyVariable of size (2 x 10 x 10)\n    """"""\n\n    is_stationary = True\n\n    def __init__(self, period_length_prior=None, period_length_constraint=None, **kwargs):\n        super(CosineKernel, self).__init__(**kwargs)\n\n        self.register_parameter(\n            name=""raw_period_length"", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1))\n        )\n\n        if period_length_constraint is None:\n            period_length_constraint = Positive()\n\n        if period_length_prior is not None:\n            self.register_prior(\n                ""period_length_prior"",\n                period_length_prior,\n                lambda: self.period_length,\n                lambda v: self._set_period_length(v),\n            )\n\n        self.register_constraint(""raw_period_length"", period_length_constraint)\n\n    @property\n    def period_length(self):\n        return self.raw_period_length_constraint.transform(self.raw_period_length)\n\n    @period_length.setter\n    def period_length(self, value):\n        return self._set_period_length(value)\n\n    def _set_period_length(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_period_length)\n\n        self.initialize(raw_period_length=self.raw_period_length_constraint.inverse_transform(value))\n\n    def forward(self, x1, x2, **params):\n        x1_ = x1.div(self.period_length)\n        x2_ = x2.div(self.period_length)\n        diff = self.covar_dist(x1_, x2_, **params)\n        res = torch.cos(diff.mul(math.pi))\n        return res\n'"
gpytorch/kernels/cylindrical_kernel.py,21,"b'#!/usr/bin/env python3\n\nfrom typing import Optional\n\nimport torch\n\nfrom .. import settings\nfrom ..constraints import Interval, Positive\nfrom ..priors import Prior\nfrom .kernel import Kernel\n\n\nclass CylindricalKernel(Kernel):\n    r""""""\n    Computes a covariance matrix based on the Cylindrical Kernel between\n    inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`.\n    It was proposed in `BOCK: Bayesian Optimization with Cylindrical Kernels`.\n    See http://proceedings.mlr.press/v80/oh18a.html for more details\n\n    .. note::\n        The data must lie completely within the unit ball.\n\n    Args:\n        :attr:`num_angular_weights` (int):\n            The number of components in the angular kernel\n        :attr:`radial_base_kernel` (gpytorch.kernel):\n            The base kernel for computing the radial kernel\n        :attr:`batch_size` (int, optional):\n            Set this if the data is batch of input data.\n            It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `1`\n        :attr:`eps` (float):\n            Small floating point number used to improve numerical stability\n            in kernel computations. Default: `1e-6`\n        :attr:`param_transform` (function, optional):\n            Set this if you want to use something other than softplus to ensure positiveness of parameters.\n        :attr:`inv_param_transform` (function, optional):\n            Set this to allow setting parameters directly in transformed space and sampling from priors.\n            Automatically inferred for common transformations such as torch.exp or torch.nn.functional.softplus.\n    """"""\n\n    def __init__(\n        self,\n        num_angular_weights: int,\n        radial_base_kernel: Kernel,\n        eps: Optional[int] = 1e-6,\n        angular_weights_prior: Optional[Prior] = None,\n        angular_weights_constraint: Optional[Interval] = None,\n        alpha_prior: Optional[Prior] = None,\n        alpha_constraint: Optional[Interval] = None,\n        beta_prior: Optional[Prior] = None,\n        beta_constraint: Optional[Interval] = None,\n        **kwargs,\n    ):\n        if angular_weights_constraint is None:\n            angular_weights_constraint = Positive()\n\n        if alpha_constraint is None:\n            alpha_constraint = Positive()\n\n        if beta_constraint is None:\n            beta_constraint = Positive()\n\n        super().__init__(**kwargs)\n        self.num_angular_weights = num_angular_weights\n        self.radial_base_kernel = radial_base_kernel\n        self.eps = eps\n\n        self.register_parameter(\n            name=""raw_angular_weights"",\n            parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, num_angular_weights)),\n        )\n        self.register_constraint(""raw_angular_weights"", angular_weights_constraint)\n        self.register_parameter(name=""raw_alpha"", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1)))\n        self.register_constraint(""raw_alpha"", alpha_constraint)\n        self.register_parameter(name=""raw_beta"", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1)))\n        self.register_constraint(""raw_beta"", beta_constraint)\n\n        if angular_weights_prior is not None:\n            self.register_prior(\n                ""angular_weights_prior"",\n                angular_weights_prior,\n                lambda: self.angular_weights,\n                lambda v: self._set_angular_weights(v),\n            )\n        if alpha_prior is not None:\n            self.register_prior(""alpha_prior"", alpha_prior, lambda: self.alpha, lambda v: self._set_alpha(v))\n        if beta_prior is not None:\n            self.register_prior(""beta_prior"", beta_prior, lambda: self.beta, lambda v: self._set_beta(v))\n\n    @property\n    def angular_weights(self) -> torch.Tensor:\n        return self.raw_angular_weights_constraint.transform(self.raw_angular_weights)\n\n    @angular_weights.setter\n    def angular_weights(self, value: torch.Tensor) -> None:\n        if not torch.is_tensor(value):\n            value = torch.tensor(value)\n\n        self.initialize(raw_angular_weights=self.raw_angular_weights_constraint.inverse_transform(value))\n\n    @property\n    def alpha(self) -> torch.Tensor:\n        return self.raw_alpha_constraint.transform(self.raw_alpha)\n\n    @alpha.setter\n    def alpha(self, value: torch.Tensor) -> None:\n        if not torch.is_tensor(value):\n            value = torch.tensor(value)\n\n        self.initialize(raw_alpha=self.raw_alpha_constraint.inverse_transform(value))\n\n    @property\n    def beta(self) -> torch.Tensor:\n        return self.raw_beta_constraint.transform(self.raw_beta)\n\n    @beta.setter\n    def beta(self, value: torch.Tensor) -> None:\n        if not torch.is_tensor(value):\n            value = torch.tensor(value)\n\n        self.initialize(raw_beta=self.raw_beta_constraint.inverse_transform(value))\n\n    def forward(self, x1: torch.Tensor, x2: torch.Tensor, diag: Optional[bool] = False, **params) -> torch.Tensor:\n\n        x1_, x2_ = x1.clone(), x2.clone()\n        # Jitter datapoints that are exactly 0\n        x1_[x1_ == 0], x2_[x2_ == 0] = x1_[x1_ == 0] + self.eps, x2_[x2_ == 0] + self.eps\n        r1, r2 = x1_.norm(dim=-1, keepdim=True), x2_.norm(dim=-1, keepdim=True)\n\n        if torch.any(r1 > 1.0) or torch.any(r2 > 1.0):\n            raise RuntimeError(""Cylindrical kernel not defined for data points with radius > 1. Scale your data!"")\n\n        a1, a2 = x1.div(r1), x2.div(r2)\n        if not diag:\n            gram_mat = a1.matmul(a2.transpose(-2, -1))\n            for p in range(self.num_angular_weights):\n                if p == 0:\n                    angular_kernel = self.angular_weights[..., 0, None, None]\n                else:\n                    angular_kernel = angular_kernel + self.angular_weights[..., p, None, None].mul(gram_mat.pow(p))\n        else:\n            gram_mat = a1.mul(a2).sum(-1)\n            for p in range(self.num_angular_weights):\n                if p == 0:\n                    angular_kernel = self.angular_weights[..., 0, None]\n                else:\n                    angular_kernel = angular_kernel + self.angular_weights[..., p, None].mul(gram_mat.pow(p))\n\n        with settings.lazily_evaluate_kernels(False):\n            radial_kernel = self.radial_base_kernel(self.kuma(r1), self.kuma(r2), diag=diag, **params)\n        return radial_kernel.mul(angular_kernel)\n\n    def kuma(self, x: torch.Tensor) -> torch.Tensor:\n        alpha = self.alpha.view(*self.batch_shape, 1, 1)\n        beta = self.beta.view(*self.batch_shape, 1, 1)\n\n        res = 1 - (1 - x.pow(alpha) + self.eps).pow(beta)\n        return res\n\n    def num_outputs_per_input(self, x1: torch.Tensor, x2: torch.Tensor) -> int:\n        return self.radial_base_kernel.num_outputs_per_input(x1, x2)\n'"
gpytorch/kernels/grid_interpolation_kernel.py,5,"b'#!/usr/bin/env python3\n\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\n\nfrom ..lazy import InterpolatedLazyTensor, lazify\nfrom ..models.exact_prediction_strategies import InterpolatedPredictionStrategy\nfrom ..utils.broadcasting import _mul_broadcast_shape\nfrom ..utils.grid import create_grid\nfrom ..utils.interpolation import Interpolation\nfrom .grid_kernel import GridKernel\nfrom .kernel import Kernel\n\n\nclass GridInterpolationKernel(GridKernel):\n    r""""""\n    Implements the KISS-GP (or SKI) approximation for a given kernel.\n    It was proposed in `Kernel Interpolation for Scalable Structured Gaussian Processes`_,\n    and offers extremely fast and accurate Kernel approximations for large datasets.\n\n    Given a base kernel `k`, the covariance :math:`k(\\mathbf{x_1}, \\mathbf{x_2})` is approximated by\n    using a grid of regularly spaced *inducing points*:\n\n    .. math::\n\n       \\begin{equation*}\n          k(\\mathbf{x_1}, \\mathbf{x_2}) = \\mathbf{w_{x_1}}^\\top K_{U,U} \\mathbf{w_{x_2}}\n       \\end{equation*}\n\n    where\n\n    * :math:`U` is the set of gridded inducing points\n\n    * :math:`K_{U,U}` is the kernel matrix between the inducing points\n\n    * :math:`\\mathbf{w_{x_1}}` and :math:`\\mathbf{w_{x_2}}` are sparse vectors based on\n      :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}` that apply cubic interpolation.\n\n    The user should supply the size of the grid (using the :attr:`grid_size` attribute).\n    To choose a reasonable grid value, we highly recommend using the\n    :func:`gpytorch.utils.grid.choose_grid_size` helper function.\n    The bounds of the grid will automatically be determined by data.\n\n    (Alternatively, you can hard-code bounds using the :attr:`grid_bounds`, which\n    will speed up this kernel\'s computations.)\n\n    .. note::\n\n        `GridInterpolationKernel` can only wrap **stationary kernels** (such as RBF, Matern,\n        Periodic, Spectral Mixture, etc.)\n\n    Args:\n        - :attr:`base_kernel` (Kernel):\n            The kernel to approximate with KISS-GP\n        - :attr:`grid_size` (Union[int, List[int]]):\n            The size of the grid in each dimension.\n            If a single int is provided, then every dimension will have the same grid size.\n        - :attr:`num_dims` (int):\n            The dimension of the input data. Required if `grid_bounds=None`\n        - :attr:`grid_bounds` (tuple(float, float), optional):\n            The bounds of the grid, if known (high performance mode).\n            The length of the tuple must match the number of dimensions.\n            The entries represent the min/max values for each dimension.\n        - :attr:`active_dims` (tuple of ints, optional):\n            Passed down to the `base_kernel`.\n\n    .. _Kernel Interpolation for Scalable Structured Gaussian Processes:\n        http://proceedings.mlr.press/v37/wilson15.pdf\n    """"""\n\n    def __init__(\n        self,\n        base_kernel: Kernel,\n        grid_size: Union[int, List[int]],\n        num_dims: int = None,\n        grid_bounds: Optional[Tuple[float, float]] = None,\n        active_dims: Tuple[int, ...] = None,\n    ):\n        has_initialized_grid = 0\n        grid_is_dynamic = True\n\n        # Make some temporary grid bounds, if none exist\n        if grid_bounds is None:\n            if num_dims is None:\n                raise RuntimeError(""num_dims must be supplied if grid_bounds is None"")\n            else:\n                # Create some temporary grid bounds - they\'ll be changed soon\n                grid_bounds = tuple((-1.0, 1.0) for _ in range(num_dims))\n        else:\n            has_initialized_grid = 1\n            grid_is_dynamic = False\n            if num_dims is None:\n                num_dims = len(grid_bounds)\n            elif num_dims != len(grid_bounds):\n                raise RuntimeError(\n                    ""num_dims ({}) disagrees with the number of supplied ""\n                    ""grid_bounds ({})"".format(num_dims, len(grid_bounds))\n                )\n\n        if isinstance(grid_size, int):\n            grid_sizes = [grid_size for _ in range(num_dims)]\n        else:\n            grid_sizes = list(grid_size)\n\n        if len(grid_sizes) != num_dims:\n            raise RuntimeError(""The number of grid sizes provided through grid_size do not match num_dims."")\n\n        # Initialize values and the grid\n        self.grid_is_dynamic = grid_is_dynamic\n        self.num_dims = num_dims\n        self.grid_sizes = grid_sizes\n        self.grid_bounds = grid_bounds\n        grid = create_grid(self.grid_sizes, self.grid_bounds)\n\n        super(GridInterpolationKernel, self).__init__(\n            base_kernel=base_kernel, grid=grid, interpolation_mode=True, active_dims=active_dims,\n        )\n        self.register_buffer(""has_initialized_grid"", torch.tensor(has_initialized_grid, dtype=torch.bool))\n\n    @property\n    def _tight_grid_bounds(self):\n        grid_spacings = tuple((bound[1] - bound[0]) / self.grid_sizes[i] for i, bound in enumerate(self.grid_bounds))\n        return tuple(\n            (bound[0] + 2.01 * spacing, bound[1] - 2.01 * spacing)\n            for bound, spacing in zip(self.grid_bounds, grid_spacings)\n        )\n\n    def _compute_grid(self, inputs, last_dim_is_batch=False):\n        n_data, n_dimensions = inputs.size(-2), inputs.size(-1)\n        if last_dim_is_batch:\n            inputs = inputs.transpose(-1, -2).unsqueeze(-1)\n            n_dimensions = 1\n        batch_shape = inputs.shape[:-2]\n\n        inputs = inputs.reshape(-1, n_dimensions)\n        interp_indices, interp_values = Interpolation().interpolate(self.grid, inputs)\n        interp_indices = interp_indices.view(*batch_shape, n_data, -1)\n        interp_values = interp_values.view(*batch_shape, n_data, -1)\n        return interp_indices, interp_values\n\n    def _inducing_forward(self, last_dim_is_batch, **params):\n        return super().forward(self.grid, self.grid, last_dim_is_batch=last_dim_is_batch, **params)\n\n    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n        # See if we need to update the grid or not\n        if self.grid_is_dynamic:  # This is true if a grid_bounds wasn\'t passed in\n            if torch.equal(x1, x2):\n                x = x1.reshape(-1, self.num_dims)\n            else:\n                x = torch.cat([x1.reshape(-1, self.num_dims), x2.reshape(-1, self.num_dims)])\n            x_maxs = x.max(0)[0].tolist()\n            x_mins = x.min(0)[0].tolist()\n\n            # We need to update the grid if\n            # 1) it hasn\'t ever been initialized, or\n            # 2) if any of the grid points are ""out of bounds""\n            update_grid = (not self.has_initialized_grid.item()) or any(\n                x_min < bound[0] or x_max > bound[1]\n                for x_min, x_max, bound in zip(x_mins, x_maxs, self._tight_grid_bounds)\n            )\n\n            # Update the grid if needed\n            if update_grid:\n                grid_spacings = tuple(\n                    (x_max - x_min) / (gs - 4.02) for gs, x_min, x_max in zip(self.grid_sizes, x_mins, x_maxs)\n                )\n                self.grid_bounds = tuple(\n                    (x_min - 2.01 * spacing, x_max + 2.01 * spacing)\n                    for x_min, x_max, spacing in zip(x_mins, x_maxs, grid_spacings)\n                )\n                grid = create_grid(\n                    self.grid_sizes, self.grid_bounds, dtype=self.grid[0].dtype, device=self.grid[0].device,\n                )\n                self.update_grid(grid)\n\n        base_lazy_tsr = lazify(self._inducing_forward(last_dim_is_batch=last_dim_is_batch, **params))\n        if last_dim_is_batch:\n            base_lazy_tsr = base_lazy_tsr.repeat(*x1.shape[:-2], x1.size(-1), 1, 1)\n\n        left_interp_indices, left_interp_values = self._compute_grid(x1, last_dim_is_batch)\n        if torch.equal(x1, x2):\n            right_interp_indices = left_interp_indices\n            right_interp_values = left_interp_values\n        else:\n            right_interp_indices, right_interp_values = self._compute_grid(x2, last_dim_is_batch)\n\n        batch_shape = _mul_broadcast_shape(\n            base_lazy_tsr.batch_shape, left_interp_indices.shape[:-2], right_interp_indices.shape[:-2],\n        )\n        res = InterpolatedLazyTensor(\n            base_lazy_tsr.expand(*batch_shape, *base_lazy_tsr.matrix_shape),\n            left_interp_indices.detach().expand(*batch_shape, *left_interp_indices.shape[-2:]),\n            left_interp_values.expand(*batch_shape, *left_interp_values.shape[-2:]),\n            right_interp_indices.detach().expand(*batch_shape, *right_interp_indices.shape[-2:]),\n            right_interp_values.expand(*batch_shape, *right_interp_values.shape[-2:]),\n        )\n\n        if diag:\n            return res.diag()\n        else:\n            return res\n\n    def prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood):\n        return InterpolatedPredictionStrategy(train_inputs, train_prior_dist, train_labels, likelihood)\n'"
gpytorch/kernels/grid_kernel.py,4,"b'#!/usr/bin/env python3\n\nfrom typing import List\n\nimport torch\nfrom torch import Tensor\n\nfrom .. import settings\nfrom ..lazy import KroneckerProductLazyTensor, ToeplitzLazyTensor, cat, delazify\nfrom ..utils.grid import convert_legacy_grid, create_data_from_grid\nfrom .kernel import Kernel\n\n\nclass GridKernel(Kernel):\n    r""""""\n    If the input data :math:`X` are regularly spaced on a grid, then\n    `GridKernel` can dramatically speed up computatations for stationary kernel.\n\n    GridKernel exploits Toeplitz and Kronecker structure within the covariance matrix.\n    See `Fast kernel learning for multidimensional pattern extrapolation`_ for more info.\n\n    .. note::\n\n        `GridKernel` can only wrap **stationary kernels** (such as RBF, Matern,\n        Periodic, Spectral Mixture, etc.)\n\n    Args:\n        :attr:`base_kernel` (Kernel):\n            The kernel to speed up with grid methods.\n        :attr:`grid` (Tensor):\n            A g x d tensor where column i consists of the projections of the\n            grid in dimension i.\n        :attr:`active_dims` (tuple of ints, optional):\n            Passed down to the `base_kernel`.\n        :attr:`interpolation_mode` (bool):\n            Used for GridInterpolationKernel where we want the covariance\n            between points in the projections of the grid of each dimension.\n            We do this by treating `grid` as d batches of g x 1 tensors by\n            calling base_kernel(grid, grid) with last_dim_is_batch to get a d x g x g Tensor\n            which we Kronecker product to get a g x g KroneckerProductLazyTensor.\n\n    .. _Fast kernel learning for multidimensional pattern extrapolation:\n        http://www.cs.cmu.edu/~andrewgw/manet.pdf\n    """"""\n\n    is_stationary = True\n\n    def __init__(\n        self, base_kernel: Kernel, grid: List[Tensor], interpolation_mode: bool = False, active_dims: bool = None\n    ):\n        if not base_kernel.is_stationary:\n            raise RuntimeError(""The base_kernel for GridKernel must be stationary."")\n\n        super().__init__(active_dims=active_dims)\n        if torch.is_tensor(grid):\n            grid = convert_legacy_grid(grid)\n        self.interpolation_mode = interpolation_mode\n        self.base_kernel = base_kernel\n        self.num_dims = len(grid)\n        self.register_buffer_list(""grid"", grid)\n        if not self.interpolation_mode:\n            self.register_buffer(""full_grid"", create_data_from_grid(grid))\n\n    def register_buffer_list(self, base_name, tensors):\n        """"""Helper to register several buffers at once under a single base name""""""\n        for i, tensor in enumerate(tensors):\n            self.register_buffer(base_name + ""_"" + str(i), tensor)\n\n    def train(self, mode=True):\n        if hasattr(self, ""_cached_kernel_mat""):\n            del self._cached_kernel_mat\n        return super(GridKernel, self).train(mode)\n\n    @property\n    def grid(self):\n        return [getattr(self, f""grid_{i}"") for i in range(self.num_dims)]\n\n    def update_grid(self, grid):\n        """"""\n        Supply a new `grid` if it ever changes.\n        """"""\n        if torch.is_tensor(grid):\n            grid = convert_legacy_grid(grid)\n\n        if len(grid) != self.num_dims:\n            raise RuntimeError(""New grid should have the same number of dimensions as before."")\n\n        for i in range(self.num_dims):\n            setattr(self, f""grid_{i}"", grid[i])\n\n        if not self.interpolation_mode:\n            self.full_grid = create_data_from_grid(self.grid)\n\n        if hasattr(self, ""_cached_kernel_mat""):\n            del self._cached_kernel_mat\n        return self\n\n    @property\n    def is_ragged(self):\n        return not all(self.grid[0].size() == proj.size() for proj in self.grid)\n\n    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n        grid = self.grid\n        if last_dim_is_batch and self.is_ragged:\n            raise ValueError(""last_dim_is_batch requires all dimensions to have same number of grid points"")\n\n        if not self.interpolation_mode:\n            if len(x1.shape[:-2]):\n                full_grid = self.full_grid.expand(*x1.shape[:-2], *self.full_grid.shape[-2:])\n            else:\n                full_grid = self.full_grid\n\n        if self.interpolation_mode or (torch.equal(x1, full_grid) and torch.equal(x2, full_grid)):\n            if not self.training and hasattr(self, ""_cached_kernel_mat""):\n                return self._cached_kernel_mat\n            # Can exploit Toeplitz structure if grid points in each dimension are equally\n            # spaced and using a translation-invariant kernel\n            if settings.use_toeplitz.on():\n                first_grid_point = [proj[0].unsqueeze(0) for proj in grid]\n                covars = [\n                    self.base_kernel(first, proj, last_dim_is_batch=False, **params)\n                    for first, proj in zip(first_grid_point, grid)\n                ]  # Each entry i contains a 1 x grid_size[i] covariance matrix\n                covars = [delazify(c) for c in covars]\n\n                if last_dim_is_batch:\n                    # Toeplitz expects batches of columns so we concatenate the\n                    # 1 x grid_size[i] tensors together\n                    # Note that this requires all the dimensions to have the same number of grid points\n                    covar = ToeplitzLazyTensor(torch.cat(covars, dim=-2))\n                else:\n                    # Non-batched ToeplitzLazyTensor expects a 1D tensor, so we squeeze out the row dimension\n                    covars = [ToeplitzLazyTensor(c.squeeze(-2)) for c in covars]\n                    # Due to legacy reasons, KroneckerProductLazyTensor(A, B, C) is actually (C Kron B Kron A)\n                    covar = KroneckerProductLazyTensor(*covars[::-1])\n            else:\n                covars = [\n                    self.base_kernel(proj, proj, last_dim_is_batch=False, **params) for proj in grid\n                ]  # Each entry i contains a grid_size[i] x grid_size[i] covariance matrix\n                if last_dim_is_batch:\n                    # Note that this requires all the dimensions to have the same number of grid points\n                    covar = cat([c.unsqueeze(-3) for c in covars], dim=-3)\n                else:\n                    covar = KroneckerProductLazyTensor(*covars[::-1])\n\n            if not self.training:\n                self._cached_kernel_mat = covar\n\n            return covar\n        else:\n            return self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n\n    def num_outputs_per_input(self, x1, x2):\n        return self.base_kernel.num_outputs_per_input(x1, x2)\n'"
gpytorch/kernels/index_kernel.py,5,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..constraints import Positive\nfrom ..lazy import DiagLazyTensor, InterpolatedLazyTensor, PsdSumLazyTensor, RootLazyTensor\nfrom ..utils.broadcasting import _mul_broadcast_shape\nfrom .kernel import Kernel\n\n\nclass IndexKernel(Kernel):\n    r""""""\n    A kernel for discrete indices. Kernel is defined by a lookup table.\n\n    .. math::\n\n        \\begin{equation}\n            k(i, j) = \\left(BB^\\top + \\text{diag}(\\mathbf v) \\right)_{i, j}\n        \\end{equation}\n\n    where :math:`B` is a low-rank matrix, and :math:`\\mathbf v` is a  non-negative vector.\n    These parameters are learned.\n\n    Args:\n        :attr:`num_tasks` (int):\n            Total number of indices.\n        :attr:`batch_shape` (torch.Size, optional):\n            Set if the MultitaskKernel is operating on batches of data (and you want different\n            parameters for each batch)\n        :attr:`rank` (int):\n            Rank of :math:`B` matrix.\n        :attr:`prior` (:obj:`gpytorch.priors.Prior`):\n            Prior for :math:`B` matrix.\n        :attr:`var_constraint` (Constraint, optional):\n            Constraint for added diagonal component. Default: `Positive`.\n\n    Attributes:\n        covar_factor:\n            The :math:`B` matrix.\n        raw_var:\n            The element-wise log of the :math:`\\mathbf v` vector.\n    """"""\n\n    def __init__(self, num_tasks, rank=1, prior=None, var_constraint=None, **kwargs):\n        if rank > num_tasks:\n            raise RuntimeError(""Cannot create a task covariance matrix larger than the number of tasks"")\n        super().__init__(**kwargs)\n\n        if var_constraint is None:\n            var_constraint = Positive()\n\n        self.register_parameter(\n            name=""covar_factor"", parameter=torch.nn.Parameter(torch.randn(*self.batch_shape, num_tasks, rank))\n        )\n        self.register_parameter(name=""raw_var"", parameter=torch.nn.Parameter(torch.randn(*self.batch_shape, num_tasks)))\n        if prior is not None:\n            self.register_prior(""IndexKernelPrior"", prior, self._eval_covar_matrix)\n\n        self.register_constraint(""raw_var"", var_constraint)\n\n    @property\n    def var(self):\n        return self.raw_var_constraint.transform(self.raw_var)\n\n    @var.setter\n    def var(self, value):\n        self._set_var(value)\n\n    def _set_var(self, value):\n        self.initialize(raw_var=self.raw_var_constraint.inverse_transform(value))\n\n    def _eval_covar_matrix(self):\n        cf = self.covar_factor\n        return cf @ cf.transpose(-1, -2) + torch.diag_embed(self.var)\n\n    @property\n    def covar_matrix(self):\n        var = self.var\n        res = PsdSumLazyTensor(RootLazyTensor(self.covar_factor), DiagLazyTensor(var))\n        return res\n\n    def forward(self, i1, i2, **params):\n        covar_matrix = self._eval_covar_matrix()\n        batch_shape = _mul_broadcast_shape(i1.shape[:-2], self.batch_shape)\n        index_shape = batch_shape + i1.shape[-2:]\n\n        res = InterpolatedLazyTensor(\n            base_lazy_tensor=covar_matrix,\n            left_interp_indices=i1.expand(index_shape),\n            right_interp_indices=i2.expand(index_shape),\n        )\n        return res\n'"
gpytorch/kernels/inducing_point_kernel.py,6,"b'#!/usr/bin/env python3\n\nimport copy\nimport math\n\nimport torch\n\nfrom .. import settings\nfrom ..distributions import MultivariateNormal\nfrom ..lazy import DiagLazyTensor, MatmulLazyTensor, PsdSumLazyTensor, RootLazyTensor, delazify\nfrom ..mlls import InducingPointKernelAddedLossTerm\nfrom ..utils.cholesky import psd_safe_cholesky\nfrom .kernel import Kernel\n\n\nclass InducingPointKernel(Kernel):\n    def __init__(self, base_kernel, inducing_points, likelihood, active_dims=None):\n        super(InducingPointKernel, self).__init__(active_dims=active_dims)\n        self.base_kernel = base_kernel\n        self.likelihood = likelihood\n\n        if inducing_points.ndimension() == 1:\n            inducing_points = inducing_points.unsqueeze(-1)\n\n        self.register_parameter(name=""inducing_points"", parameter=torch.nn.Parameter(inducing_points))\n        self.register_added_loss_term(""inducing_point_loss_term"")\n\n    def train(self, mode=True):\n        if hasattr(self, ""_cached_kernel_mat""):\n            del self._cached_kernel_mat\n        return super(InducingPointKernel, self).train(mode)\n\n    @property\n    def _inducing_mat(self):\n        if not self.training and hasattr(self, ""_cached_kernel_mat""):\n            return self._cached_kernel_mat\n        else:\n            res = delazify(self.base_kernel(self.inducing_points, self.inducing_points))\n            if not self.training:\n                self._cached_kernel_mat = res\n            return res\n\n    @property\n    def _inducing_inv_root(self):\n        if not self.training and hasattr(self, ""_cached_kernel_inv_root""):\n            return self._cached_kernel_inv_root\n        else:\n            chol = psd_safe_cholesky(self._inducing_mat, upper=True, jitter=settings.cholesky_jitter.value())\n            eye = torch.eye(chol.size(-1), device=chol.device, dtype=chol.dtype)\n            inv_root = torch.triangular_solve(eye, chol)[0]\n\n            res = inv_root\n            if not self.training:\n                self._cached_kernel_inv_root = res\n            return res\n\n    def _get_covariance(self, x1, x2):\n        k_ux1 = delazify(self.base_kernel(x1, self.inducing_points))\n        if torch.equal(x1, x2):\n            covar = RootLazyTensor(k_ux1.matmul(self._inducing_inv_root))\n\n            # Diagonal correction for predictive posterior\n            correction = (self.base_kernel(x1, x2, diag=True) - covar.diag()).clamp(0, math.inf)\n            covar = PsdSumLazyTensor(covar, DiagLazyTensor(correction))\n        else:\n            k_ux2 = delazify(self.base_kernel(x2, self.inducing_points))\n            covar = MatmulLazyTensor(\n                k_ux1.matmul(self._inducing_inv_root), k_ux2.matmul(self._inducing_inv_root).transpose(-1, -2)\n            )\n\n        return covar\n\n    def _covar_diag(self, inputs):\n        if inputs.ndimension() == 1:\n            inputs = inputs.unsqueeze(1)\n\n        # Get diagonal of covar\n        covar_diag = delazify(self.base_kernel(inputs, diag=True))\n        return DiagLazyTensor(covar_diag)\n\n    def forward(self, x1, x2, diag=False, **kwargs):\n        covar = self._get_covariance(x1, x2)\n\n        if self.training:\n            if not torch.equal(x1, x2):\n                raise RuntimeError(""x1 should equal x2 in training mode"")\n            zero_mean = torch.zeros_like(x1.select(-1, 0))\n            new_added_loss_term = InducingPointKernelAddedLossTerm(\n                MultivariateNormal(zero_mean, self._covar_diag(x1)),\n                MultivariateNormal(zero_mean, covar),\n                self.likelihood,\n            )\n            self.update_added_loss_term(""inducing_point_loss_term"", new_added_loss_term)\n\n        if diag:\n            return covar.diag()\n        else:\n            return covar\n\n    def num_outputs_per_input(self, x1, x2):\n        return self.base_kernel.num_outputs_per_input(x1, x2)\n\n    def __deepcopy__(self, memo):\n        replace_inv_root = False\n        replace_kernel_mat = False\n\n        if hasattr(self, ""_cached_kernel_inv_root""):\n            replace_inv_root = True\n            kernel_inv_root = self._cached_kernel_inv_root\n        if hasattr(self, ""_cached_kernel_mat""):\n            replace_kernel_mat = True\n            kernel_mat = self._cached_kernel_mat\n\n        cp = self.__class__(\n            base_kernel=copy.deepcopy(self.base_kernel),\n            inducing_points=copy.deepcopy(self.inducing_points),\n            likelihood=self.likelihood,\n            active_dims=self.active_dims,\n        )\n\n        if replace_inv_root:\n            cp._cached_kernel_inv_root = kernel_inv_root\n\n        if replace_kernel_mat:\n            cp._cached_kernel_mat = kernel_mat\n\n        return cp\n'"
gpytorch/kernels/kernel.py,35,"b'#!/usr/bin/env python3\n\nimport warnings\nfrom abc import abstractmethod\nfrom copy import deepcopy\n\nimport torch\nfrom torch.nn import ModuleList\n\nfrom .. import settings\nfrom ..constraints import Positive\nfrom ..lazy import LazyEvaluatedKernelTensor, ZeroLazyTensor, delazify, lazify\nfrom ..models.exact_prediction_strategies import DefaultPredictionStrategy, SumPredictionStrategy\nfrom ..module import Module\nfrom ..utils.broadcasting import _mul_broadcast_shape\n\n\ndef default_postprocess_script(x):\n    return x\n\n\nclass Distance(torch.nn.Module):\n    def __init__(self, postprocess_script=default_postprocess_script):\n        super().__init__()\n        self._postprocess = postprocess_script\n\n    def _sq_dist(self, x1, x2, postprocess, x1_eq_x2=False):\n        # TODO: use torch squared cdist once implemented: https://github.com/pytorch/pytorch/pull/25799\n        adjustment = x1.mean(-2, keepdim=True)\n        x1 = x1 - adjustment\n        x2 = x2 - adjustment  # x1 and x2 should be identical in all dims except -2 at this point\n\n        # Compute squared distance matrix using quadratic expansion\n        x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)\n        x1_pad = torch.ones_like(x1_norm)\n        if x1_eq_x2 and not x1.requires_grad and not x2.requires_grad:\n            x2_norm, x2_pad = x1_norm, x1_pad\n        else:\n            x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)\n            x2_pad = torch.ones_like(x2_norm)\n        x1_ = torch.cat([-2.0 * x1, x1_norm, x1_pad], dim=-1)\n        x2_ = torch.cat([x2, x2_pad, x2_norm], dim=-1)\n        res = x1_.matmul(x2_.transpose(-2, -1))\n\n        if x1_eq_x2 and not x1.requires_grad and not x2.requires_grad:\n            res.diagonal(dim1=-2, dim2=-1).fill_(0)\n\n        # Zero out negative values\n        res.clamp_min_(0)\n        return self._postprocess(res) if postprocess else res\n\n    def _dist(self, x1, x2, postprocess, x1_eq_x2=False):\n        # TODO: use torch cdist once implementation is improved: https://github.com/pytorch/pytorch/pull/25799\n        res = self._sq_dist(x1, x2, postprocess=False, x1_eq_x2=x1_eq_x2)\n        res = res.clamp_min_(1e-30).sqrt_()\n        return self._postprocess(res) if postprocess else res\n\n\nclass Kernel(Module):\n    r""""""\n    Kernels in GPyTorch are implemented as a :class:`gpytorch.Module` that, when called on two :obj:`torch.tensor`\n    objects `x1` and `x2` returns either a :obj:`torch.tensor` or a :obj:`gpytorch.lazy.LazyTensor` that represents\n    the covariance matrix between `x1` and `x2`.\n\n    In the typical use case, to extend this class means to implement the :func:`~gpytorch.kernels.Kernel.forward`\n    method.\n\n    .. note::\n        The :func:`~gpytorch.kernels.Kernel.__call__` does some additional internal work. In particular,\n        all kernels are lazily evaluated so that, in some cases, we can index in to the kernel matrix before actually\n        computing it. Furthermore, many built in kernel modules return LazyTensors that allow for more efficient\n        inference than if we explicitly computed the kernel matrix itself.\n\n        As a result, if you want to use a :obj:`gpytorch.kernels.Kernel` object just to get an actual\n        :obj:`torch.tensor` representing the covariance matrix, you may need to call the\n        :func:`gpytorch.lazy.LazyTensor.evaluate` method on the output.\n\n    This base :class:`Kernel` class includes a lengthscale parameter\n    :math:`\\Theta`, which is used by many common kernel functions.\n    There are a few options for the lengthscale:\n\n    * Default: No lengthscale (i.e. :math:`\\Theta` is the identity matrix).\n\n    * Single lengthscale: One lengthscale can be applied to all input dimensions/batches\n      (i.e. :math:`\\Theta` is a constant diagonal matrix).\n      This is controlled by setting the attribute `has_lengthscale=True`.\n\n    * ARD: Each input dimension gets its own separate lengthscale\n      (i.e. :math:`\\Theta` is a non-constant diagonal matrix).\n      This is controlled by the `ard_num_dims` keyword argument (as well as `has_lengthscale=True`).\n\n    In batch-mode (i.e. when :math:`x_1` and :math:`x_2` are batches of input matrices), each\n    batch of data can have its own lengthscale parameter by setting the `batch_shape`\n    keyword argument to the appropriate number of batches.\n\n    .. note::\n\n        The :attr:`lengthscale` parameter is parameterized on a log scale to constrain it to be positive.\n        You can set a prior on this parameter using the :attr:`lengthscale_prior` argument.\n\n    Base Args:\n        :attr:`ard_num_dims` (int, optional):\n            Set this if you want a separate lengthscale for each input\n            dimension. It should be `d` if :attr:`x1` is a `n x d` matrix.  Default: `None`\n        :attr:`batch_shape` (torch.Size, optional):\n            Set this if you want a separate lengthscale for each batch of input\n            data. It should be `b1 x ... x bk` if :attr:`x1` is a `b1 x ... x bk x n x d` tensor.\n        :attr:`active_dims` (tuple of ints, optional):\n            Set this if you want to compute the covariance of only a few input dimensions. The ints\n            corresponds to the indices of the dimensions. Default: `None`.\n        :attr:`lengthscale_prior` (Prior, optional):\n            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`\n        :attr:`lengthscale_constraint` (Constraint, optional):\n            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n        :attr:`eps` (float):\n            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n\n    Base Attributes:\n        :attr:`lengthscale` (Tensor):\n            The lengthscale parameter. Size/shape of parameter depends on the\n            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n\n    Example:\n        >>> covar_module = gpytorch.kernels.LinearKernel()\n        >>> x1 = torch.randn(50, 3)\n        >>> lazy_covar_matrix = covar_module(x1) # Returns a RootLazyTensor\n        >>> tensor_covar_matrix = lazy_covar_matrix.evaluate() # Gets the actual tensor for this kernel matrix\n    """"""\n\n    has_lengthscale = False\n\n    def __init__(\n        self,\n        ard_num_dims=None,\n        batch_shape=torch.Size([]),\n        active_dims=None,\n        lengthscale_prior=None,\n        lengthscale_constraint=None,\n        eps=1e-6,\n        **kwargs,\n    ):\n        super(Kernel, self).__init__()\n        self._batch_shape = batch_shape\n        if active_dims is not None and not torch.is_tensor(active_dims):\n            active_dims = torch.tensor(active_dims, dtype=torch.long)\n        self.register_buffer(""active_dims"", active_dims)\n        self.ard_num_dims = ard_num_dims\n\n        self.eps = eps\n\n        param_transform = kwargs.get(""param_transform"")\n\n        if lengthscale_constraint is None:\n            lengthscale_constraint = Positive()\n\n        if param_transform is not None:\n            warnings.warn(\n                ""The \'param_transform\' argument is now deprecated. If you want to use a different ""\n                ""transformation, specify a different \'lengthscale_constraint\' instead."",\n                DeprecationWarning,\n            )\n\n        if self.has_lengthscale:\n            lengthscale_num_dims = 1 if ard_num_dims is None else ard_num_dims\n            self.register_parameter(\n                name=""raw_lengthscale"",\n                parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, lengthscale_num_dims)),\n            )\n            if lengthscale_prior is not None:\n                self.register_prior(\n                    ""lengthscale_prior"", lengthscale_prior, lambda: self.lengthscale, lambda v: self._set_lengthscale(v)\n                )\n\n            self.register_constraint(""raw_lengthscale"", lengthscale_constraint)\n\n        self.distance_module = None\n        # TODO: Remove this on next official PyTorch release.\n        self.__pdist_supports_batch = True\n\n    @abstractmethod\n    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n        r""""""\n        Computes the covariance between x1 and x2.\n        This method should be imlemented by all Kernel subclasses.\n\n        Args:\n            :attr:`x1` (Tensor `n x d` or `b x n x d`):\n                First set of data\n            :attr:`x2` (Tensor `m x d` or `b x m x d`):\n                Second set of data\n            :attr:`diag` (bool):\n                Should the Kernel compute the whole kernel, or just the diag?\n            :attr:`last_dim_is_batch` (tuple, optional):\n                If this is true, it treats the last dimension of the data as another batch dimension.\n                (Useful for additive structure over the dimensions). Default: False\n\n        Returns:\n            :class:`Tensor` or :class:`gpytorch.lazy.LazyTensor`.\n                The exact size depends on the kernel\'s evaluation mode:\n\n                * `full_covar`: `n x m` or `b x n x m`\n                * `full_covar` with `last_dim_is_batch=True`: `k x n x m` or `b x k x n x m`\n                * `diag`: `n` or `b x n`\n                * `diag` with `last_dim_is_batch=True`: `k x n` or `b x k x n`\n        """"""\n        raise NotImplementedError()\n\n    @property\n    def batch_shape(self):\n        kernels = list(self.sub_kernels())\n        if len(kernels):\n            return _mul_broadcast_shape(self._batch_shape, *[k.batch_shape for k in kernels])\n        else:\n            return self._batch_shape\n\n    @batch_shape.setter\n    def batch_shape(self, val):\n        self._batch_shape = val\n\n    @property\n    def dtype(self):\n        if self.has_lengthscale:\n            return self.lengthscale.dtype\n        else:\n            for param in self.parameters():\n                return param.dtype\n            return torch.get_default_dtype()\n\n    @property\n    def is_stationary(self) -> bool:\n        """"""\n        Property to indicate whether kernel is stationary or not.\n        """"""\n        return self.has_lengthscale\n\n    @property\n    def lengthscale(self):\n        if self.has_lengthscale:\n            return self.raw_lengthscale_constraint.transform(self.raw_lengthscale)\n        else:\n            return None\n\n    @lengthscale.setter\n    def lengthscale(self, value):\n        self._set_lengthscale(value)\n\n    def _set_lengthscale(self, value):\n        if not self.has_lengthscale:\n            raise RuntimeError(""Kernel has no lengthscale."")\n\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_lengthscale)\n\n        self.initialize(raw_lengthscale=self.raw_lengthscale_constraint.inverse_transform(value))\n\n    def local_load_samples(self, samples_dict, memo, prefix):\n        num_samples = next(iter(samples_dict.values())).size(0)\n        self.batch_shape = torch.Size([num_samples]) + self.batch_shape\n        super().local_load_samples(samples_dict, memo, prefix)\n\n    def covar_dist(\n        self,\n        x1,\n        x2,\n        diag=False,\n        last_dim_is_batch=False,\n        square_dist=False,\n        dist_postprocess_func=default_postprocess_script,\n        postprocess=True,\n        **params,\n    ):\n        r""""""\n        This is a helper method for computing the Euclidean distance between\n        all pairs of points in x1 and x2.\n\n        Args:\n            :attr:`x1` (Tensor `n x d` or `b1 x ... x bk x n x d`):\n                First set of data.\n            :attr:`x2` (Tensor `m x d` or `b1 x ... x bk x m x d`):\n                Second set of data.\n            :attr:`diag` (bool):\n                Should we return the whole distance matrix, or just the diagonal? If True, we must have `x1 == x2`.\n            :attr:`last_dim_is_batch` (tuple, optional):\n                Is the last dimension of the data a batch dimension or not?\n            :attr:`square_dist` (bool):\n                Should we square the distance matrix before returning?\n\n        Returns:\n            (:class:`Tensor`, :class:`Tensor) corresponding to the distance matrix between `x1` and `x2`.\n            The shape depends on the kernel\'s mode\n            * `diag=False`\n            * `diag=False` and `last_dim_is_batch=True`: (`b x d x n x n`)\n            * `diag=True`\n            * `diag=True` and `last_dim_is_batch=True`: (`b x d x n`)\n        """"""\n        if last_dim_is_batch:\n            x1 = x1.transpose(-1, -2).unsqueeze(-1)\n            x2 = x2.transpose(-1, -2).unsqueeze(-1)\n\n        x1_eq_x2 = torch.equal(x1, x2)\n\n        # torch scripts expect tensors\n        postprocess = torch.tensor(postprocess)\n\n        res = None\n\n        # Cache the Distance object or else JIT will recompile every time\n        if not self.distance_module or self.distance_module._postprocess != dist_postprocess_func:\n            self.distance_module = Distance(dist_postprocess_func)\n\n        if diag:\n            # Special case the diagonal because we can return all zeros most of the time.\n            if x1_eq_x2:\n                res = torch.zeros(*x1.shape[:-2], x1.shape[-2], dtype=x1.dtype, device=x1.device)\n                if postprocess:\n                    res = dist_postprocess_func(res)\n                return res\n            else:\n                res = torch.norm(x1 - x2, p=2, dim=-1)\n                if square_dist:\n                    res = res.pow(2)\n            if postprocess:\n                res = dist_postprocess_func(res)\n            return res\n\n        elif square_dist:\n            res = self.distance_module._sq_dist(x1, x2, postprocess, x1_eq_x2)\n        else:\n            res = self.distance_module._dist(x1, x2, postprocess, x1_eq_x2)\n\n        return res\n\n    def named_sub_kernels(self):\n        for name, module in self._modules.items():\n            if isinstance(module, Kernel):\n                yield name, module\n\n    def num_outputs_per_input(self, x1, x2):\n        """"""\n        How many outputs are produced per input (default 1)\n        if x1 is size `n x d` and x2 is size `m x d`, then the size of the kernel\n        will be `(n * num_outputs_per_input) x (m * num_outputs_per_input)`\n        Default: 1\n        """"""\n        return 1\n\n    def prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood):\n        return DefaultPredictionStrategy(train_inputs, train_prior_dist, train_labels, likelihood)\n\n    def sub_kernels(self):\n        for _, kernel in self.named_sub_kernels():\n            yield kernel\n\n    def __call__(self, x1, x2=None, diag=False, last_dim_is_batch=False, **params):\n        x1_, x2_ = x1, x2\n\n        # Select the active dimensions\n        if self.active_dims is not None:\n            x1_ = x1_.index_select(-1, self.active_dims)\n            if x2_ is not None:\n                x2_ = x2_.index_select(-1, self.active_dims)\n\n        # Give x1_ and x2_ a last dimension, if necessary\n        if x1_.ndimension() == 1:\n            x1_ = x1_.unsqueeze(1)\n        if x2_ is not None:\n            if x2_.ndimension() == 1:\n                x2_ = x2_.unsqueeze(1)\n            if not x1_.size(-1) == x2_.size(-1):\n                raise RuntimeError(""x1_ and x2_ must have the same number of dimensions!"")\n\n        if x2_ is None:\n            x2_ = x1_\n\n        # Check that ard_num_dims matches the supplied number of dimensions\n        if settings.debug.on():\n            if self.ard_num_dims is not None and self.ard_num_dims != x1_.size(-1):\n                raise RuntimeError(\n                    ""Expected the input to have {} dimensionality ""\n                    ""(based on the ard_num_dims argument). Got {}."".format(self.ard_num_dims, x1_.size(-1))\n                )\n\n        if diag:\n            res = super(Kernel, self).__call__(x1_, x2_, diag=True, last_dim_is_batch=last_dim_is_batch, **params)\n            # Did this Kernel eat the diag option?\n            # If it does not return a LazyEvaluatedKernelTensor, we can call diag on the output\n            if not isinstance(res, LazyEvaluatedKernelTensor):\n                if res.dim() == x1_.dim() and res.shape[-2:] == torch.Size((x1_.size(-2), x2_.size(-2))):\n                    res = res.diag()\n            return res\n\n        else:\n            if settings.lazily_evaluate_kernels.on():\n                res = LazyEvaluatedKernelTensor(x1_, x2_, kernel=self, last_dim_is_batch=last_dim_is_batch, **params)\n            else:\n                res = lazify(super(Kernel, self).__call__(x1_, x2_, last_dim_is_batch=last_dim_is_batch, **params))\n            return res\n\n    def __getstate__(self):\n        # JIT ScriptModules cannot be pickled\n        self.distance_module = None\n        return self.__dict__\n\n    def __add__(self, other):\n        return AdditiveKernel(self, other)\n\n    def __mul__(self, other):\n        return ProductKernel(self, other)\n\n    def __setstate__(self, d):\n        self.__dict__ = d\n\n    def __getitem__(self, index):\n        if len(self.batch_shape) == 0:\n            return self\n\n        new_kernel = deepcopy(self)\n        # Process the index\n        index = index if isinstance(index, tuple) else (index,)\n\n        for param_name, param in self._parameters.items():\n            new_kernel._parameters[param_name].data = param.__getitem__(index)\n            ndim_removed = len(param.shape) - len(new_kernel._parameters[param_name].shape)\n            new_batch_shape_len = len(self.batch_shape) - ndim_removed\n            new_kernel.batch_shape = new_kernel._parameters[param_name].shape[:new_batch_shape_len]\n\n        for sub_module_name, sub_module in self.named_sub_kernels():\n            self._modules[sub_module_name] = sub_module.__getitem__(index)\n\n        return new_kernel\n\n\nclass AdditiveKernel(Kernel):\n    """"""\n    A Kernel that supports summing over multiple component kernels.\n\n    Example:\n        >>> covar_module = RBFKernel(active_dims=torch.tensor([1])) + RBFKernel(active_dims=torch.tensor([2]))\n        >>> x1 = torch.randn(50, 2)\n        >>> additive_kernel_matrix = covar_module(x1)\n    """"""\n\n    @property\n    def is_stationary(self) -> bool:\n        """"""\n        Kernel is stationary if all components are stationary.\n        """"""\n        return all(k.is_stationary for k in self.kernels)\n\n    def __init__(self, *kernels):\n        super(AdditiveKernel, self).__init__()\n        self.kernels = ModuleList(kernels)\n\n    def forward(self, x1, x2, diag=False, **params):\n        res = ZeroLazyTensor() if not diag else 0\n        for kern in self.kernels:\n            next_term = kern(x1, x2, diag=diag, **params)\n            if not diag:\n                res = res + lazify(next_term)\n            else:\n                res = res + next_term\n\n        return res\n\n    def prediction_strategy(self, train_inputs, train_prior_dist, train_labels, likelihood):\n        return SumPredictionStrategy(train_inputs, train_prior_dist, train_labels, likelihood)\n\n    def num_outputs_per_input(self, x1, x2):\n        return self.kernels[0].num_outputs_per_input(x1, x2)\n\n    def __getitem__(self, index):\n        new_kernel = deepcopy(self)\n        for i, kernel in enumerate(self.kernels):\n            new_kernel.kernels[i] = self.kernels[i].__getitem__(index)\n\n        return new_kernel\n\n\nclass ProductKernel(Kernel):\n    """"""\n    A Kernel that supports elementwise multiplying multiple component kernels together.\n\n    Example:\n        >>> covar_module = RBFKernel(active_dims=torch.tensor([1])) * RBFKernel(active_dims=torch.tensor([2]))\n        >>> x1 = torch.randn(50, 2)\n        >>> kernel_matrix = covar_module(x1) # The RBF Kernel already decomposes multiplicatively, so this is foolish!\n    """"""\n\n    @property\n    def is_stationary(self) -> bool:\n        """"""\n        Kernel is stationary if all components are stationary.\n        """"""\n        return all(k.is_stationary for k in self.kernels)\n\n    def __init__(self, *kernels):\n        super(ProductKernel, self).__init__()\n        self.kernels = ModuleList(kernels)\n\n    def forward(self, x1, x2, diag=False, **params):\n        x1_eq_x2 = torch.equal(x1, x2)\n\n        if not x1_eq_x2:\n            # If x1 != x2, then we can\'t make a MulLazyTensor because the kernel won\'t necessarily be square/symmetric\n            res = delazify(self.kernels[0](x1, x2, diag=diag, **params))\n        else:\n            res = self.kernels[0](x1, x2, diag=diag, **params)\n\n            if not diag:\n                res = lazify(res)\n\n        for kern in self.kernels[1:]:\n            next_term = kern(x1, x2, diag=diag, **params)\n            if not x1_eq_x2:\n                # Again delazify if x1 != x2\n                res = res * delazify(next_term)\n            else:\n                if not diag:\n                    res = res * lazify(next_term)\n                else:\n                    res = res * next_term\n\n        return res\n\n    def num_outputs_per_input(self, x1, x2):\n        return self.kernels[0].num_outputs_per_input(x1, x2)\n\n    def __getitem__(self, index):\n        new_kernel = deepcopy(self)\n        for i, kernel in enumerate(self.kernels):\n            new_kernel.kernels[i] = self.kernels[i].__getitem__(index)\n\n        return new_kernel\n'"
gpytorch/kernels/lcm_kernel.py,4,"b'#!/usr/bin/env python3\n\nfrom copy import deepcopy\n\nfrom torch.nn import ModuleList\n\nfrom .kernel import Kernel\nfrom .multitask_kernel import MultitaskKernel\n\n\nclass LCMKernel(Kernel):\n    """"""\n    This kernel supports the LCM kernel. It allows the user to specify a list of\n    base kernels to use, and individual `MultitaskKernel` objects are fit to each\n    of them. The final kernel is the linear sum of the Kronecker product of all\n    these base kernels with their respective `MultitaskKernel` objects.\n\n    The returned object is of type :obj:`gpytorch.lazy.KroneckerProductLazyTensor`.\n    """"""\n\n    def __init__(self, base_kernels, num_tasks, rank=1, task_covar_prior=None):\n        """"""\n        Args:\n            base_kernels (:type: list of `Kernel` objects): A list of base kernels.\n            num_tasks (int): The number of output tasks to fit.\n            rank (int): Rank of index kernel to use for task covariance matrix for each\n                        of the base kernels.\n            task_covar_prior (:obj:`gpytorch.priors.Prior`): Prior to use for each\n                task kernel. See :class:`gpytorch.kernels.IndexKernel` for details.\n        """"""\n        if len(base_kernels) < 1:\n            raise ValueError(""At least one base kernel must be provided."")\n        for k in base_kernels:\n            if not isinstance(k, Kernel):\n                raise ValueError(""base_kernels must only contain Kernel objects"")\n        super(LCMKernel, self).__init__()\n        self.covar_module_list = ModuleList(\n            [\n                MultitaskKernel(base_kernel, num_tasks=num_tasks, rank=rank, task_covar_prior=task_covar_prior)\n                for base_kernel in base_kernels\n            ]\n        )\n\n    def forward(self, x1, x2, **params):\n        res = self.covar_module_list[0].forward(x1, x2, **params)\n        for m in self.covar_module_list[1:]:\n            res += m.forward(x1, x2, **params)\n        return res\n\n    def num_outputs_per_input(self, x1, x2):\n        """"""\n        Given `n` data points `x1` and `m` datapoints `x2`, this multitask kernel\n        returns an `(n*num_tasks) x (m*num_tasks)` covariance matrix.\n        """"""\n        return self.covar_module_list[0].num_outputs_per_input(x1, x2)\n\n    def __getitem__(self, index):\n        new_kernel = deepcopy(self)\n        new_kernel.covar_module_list = ModuleList(\n            [base_kernel.__getitem__(index) for base_kernel in self.covar_module_list]\n        )\n        return new_kernel\n'"
gpytorch/kernels/linear_kernel.py,8,"b'#!/usr/bin/env python3\n\nimport warnings\n\nimport torch\n\nfrom ..constraints import Positive\nfrom ..lazy import MatmulLazyTensor, RootLazyTensor\nfrom .kernel import Kernel\n\n\nclass LinearKernel(Kernel):\n    r""""""\n    Computes a covariance matrix based on the Linear kernel\n    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n\n    .. math::\n        \\begin{equation*}\n            k_\\text{Linear}(\\mathbf{x_1}, \\mathbf{x_2}) = v\\mathbf{x_1}^\\top\n            \\mathbf{x_2}.\n        \\end{equation*}\n\n    where\n\n    * :math:`v` is a :attr:`variance` parameter.\n\n\n    .. note::\n\n        To implement this efficiently, we use a :obj:`gpytorch.lazy.RootLazyTensor` during training and a\n        :class:`gpytorch.lazy.MatmulLazyTensor` during test. These lazy tensors represent matrices of the form\n        :math:`K = XX^{\\top}` and :math:`K = XZ^{\\top}`. This makes inference\n        efficient because a matrix-vector product :math:`Kv` can be computed as\n        :math:`Kv=X(X^{\\top}v)`, where the base multiply :math:`Xv` takes only\n        :math:`O(nd)` time and space.\n\n    Args:\n        :attr:`variance_prior` (:class:`gpytorch.priors.Prior`):\n            Prior over the variance parameter (default `None`).\n        :attr:`variance_constraint` (Constraint, optional):\n            Constraint to place on variance parameter. Default: `Positive`.\n        :attr:`active_dims` (list):\n            List of data dimensions to operate on.\n            `len(active_dims)` should equal `num_dimensions`.\n    """"""\n\n    def __init__(self, num_dimensions=None, offset_prior=None, variance_prior=None, variance_constraint=None, **kwargs):\n        super(LinearKernel, self).__init__(**kwargs)\n        if variance_constraint is None:\n            variance_constraint = Positive()\n\n        if num_dimensions is not None:\n            # Remove after 1.0\n            warnings.warn(""The `num_dimensions` argument is deprecated and no longer used."", DeprecationWarning)\n            self.register_parameter(name=""offset"", parameter=torch.nn.Parameter(torch.zeros(1, 1, num_dimensions)))\n        if offset_prior is not None:\n            # Remove after 1.0\n            warnings.warn(""The `offset_prior` argument is deprecated and no longer used."", DeprecationWarning)\n        self.register_parameter(name=""raw_variance"", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1)))\n        if variance_prior is not None:\n            self.register_prior(\n                ""variance_prior"", variance_prior, lambda: self.variance, lambda v: self._set_variance(v)\n            )\n\n        self.register_constraint(""raw_variance"", variance_constraint)\n\n    @property\n    def variance(self):\n        return self.raw_variance_constraint.transform(self.raw_variance)\n\n    @variance.setter\n    def variance(self, value):\n        self._set_variance(value)\n\n    def _set_variance(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_variance)\n        self.initialize(raw_variance=self.raw_variance_constraint.inverse_transform(value))\n\n    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n        x1_ = x1 * self.variance.sqrt()\n        if last_dim_is_batch:\n            x1_ = x1_.transpose(-1, -2).unsqueeze(-1)\n\n        if x1.size() == x2.size() and torch.equal(x1, x2):\n            # Use RootLazyTensor when x1 == x2 for efficiency when composing\n            # with other kernels\n            prod = RootLazyTensor(x1_)\n\n        else:\n            x2_ = x2 * self.variance.sqrt()\n            if last_dim_is_batch:\n                x2_ = x2_.transpose(-1, -2).unsqueeze(-1)\n\n            prod = MatmulLazyTensor(x1_, x2_.transpose(-2, -1))\n\n        if diag:\n            return prod.diag()\n        else:\n            return prod\n'"
gpytorch/kernels/matern_kernel.py,11,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\n\nfrom ..functions import MaternCovariance\nfrom ..settings import trace_mode\nfrom .kernel import Kernel\n\n\nclass MaternKernel(Kernel):\n    r""""""\n    Computes a covariance matrix based on the Matern kernel\n    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n\n    .. math::\n\n       \\begin{equation*}\n          k_{\\text{Matern}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\frac{2^{1 - \\nu}}{\\Gamma(\\nu)}\n          \\left( \\sqrt{2 \\nu} d \\right) K_\\nu \\left( \\sqrt{2 \\nu} d \\right)\n       \\end{equation*}\n\n    where\n\n    * :math:`d = (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-1} (\\mathbf{x_1} - \\mathbf{x_2})`\n      is the distance between\n      :math:`x_1` and :math:`x_2` scaled by the :attr:`lengthscale` parameter :math:`\\Theta`.\n    * :math:`\\nu` is a smoothness parameter (takes values 1/2, 3/2, or 5/2). Smaller values are less smooth.\n    * :math:`K_\\nu` is a modified Bessel function.\n\n    There are a few options for the lengthscale parameter :math:`\\Theta`:\n    See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.\n\n    .. note::\n\n        This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n\n    Args:\n        :attr:`nu` (float):\n            The smoothness parameter: either 1/2, 3/2, or 5/2.\n        :attr:`ard_num_dims` (int, optional):\n            Set this if you want a separate lengthscale for each\n            input dimension. It should be `d` if :attr:`x1` is a `n x d` matrix. Default: `None`\n        :attr:`batch_shape` (torch.Size, optional):\n            Set this if you want a separate lengthscale for each\n             batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`\n        :attr:`active_dims` (tuple of ints, optional):\n            Set this if you want to\n            compute the covariance of only a few input dimensions. The ints\n            corresponds to the indices of the dimensions. Default: `None`.\n        :attr:`lengthscale_prior` (Prior, optional):\n            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`\n        :attr:`lengthscale_constraint` (Constraint, optional):\n            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n        :attr:`eps` (float):\n            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n\n    Attributes:\n        :attr:`lengthscale` (Tensor):\n            The lengthscale parameter. Size/shape of parameter depends on the\n            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n\n    Example:\n        >>> x = torch.randn(10, 5)\n        >>> # Non-batch: Simple option\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))\n        >>> # Non-batch: ARD (different lengthscale for each input dimension)\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=5))\n        >>> covar = covar_module(x)  # Output: LazyVariable of size (10 x 10)\n        >>>\n        >>> batch_x = torch.randn(2, 10, 5)\n        >>> # Batch: Simple option\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))\n        >>> # Batch: different lengthscale for each batch\n        >>> covar_module = gpytorch.kernels.MaternKernel(nu=0.5, batch_shape=torch.Size([2])\n        >>> covar = covar_module(x)  # Output: LazyVariable of size (2 x 10 x 10)\n    """"""\n\n    has_lengthscale = True\n\n    def __init__(self, nu=2.5, **kwargs):\n        if nu not in {0.5, 1.5, 2.5}:\n            raise RuntimeError(""nu expected to be 0.5, 1.5, or 2.5"")\n        super(MaternKernel, self).__init__(**kwargs)\n        self.nu = nu\n\n    def forward(self, x1, x2, diag=False, **params):\n        if (\n            x1.requires_grad\n            or x2.requires_grad\n            or (self.ard_num_dims is not None and self.ard_num_dims > 1)\n            or diag\n            or trace_mode.on()\n        ):\n            mean = x1.reshape(-1, x1.size(-1)).mean(0)[(None,) * (x1.dim() - 1)]\n\n            x1_ = (x1 - mean).div(self.lengthscale)\n            x2_ = (x2 - mean).div(self.lengthscale)\n            distance = self.covar_dist(x1_, x2_, diag=diag, **params)\n            exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n\n            if self.nu == 0.5:\n                constant_component = 1\n            elif self.nu == 1.5:\n                constant_component = (math.sqrt(3) * distance).add(1)\n            elif self.nu == 2.5:\n                constant_component = (math.sqrt(5) * distance).add(1).add(5.0 / 3.0 * distance ** 2)\n            return constant_component * exp_component\n        return MaternCovariance().apply(\n            x1, x2, self.lengthscale, self.nu, lambda x1, x2: self.covar_dist(x1, x2, **params)\n        )\n'"
gpytorch/kernels/multi_device_kernel.py,7,"b'#!/usr/bin/env python3\n\nimport torch\nfrom torch.nn.parallel import DataParallel\n\nfrom .. import settings\nfrom ..lazy import CatLazyTensor, lazify\nfrom .kernel import Kernel\n\n\nclass MultiDeviceKernel(DataParallel, Kernel):\n    r""""""\n    Allocates the covariance matrix on distributed devices, e.g. multiple GPUs.\n\n    Args:\n        - :attr:`base_kernel`: Base kernel to distribute\n        - :attr:`device_ids`: list of `torch.device` objects to place kernel chunks on\n        - :attr:`output_device`: Device where outputs will be placed\n    """"""\n\n    def __init__(self, base_kernel, device_ids, output_device=None, create_cuda_context=True, **kwargs):\n        # Need to warm up each GPU otherwise scattering in forward will be\n        # EXTREMELY slow. This memory will be available as soon as we leave __init__\n        if create_cuda_context:\n            for d in device_ids:\n                _ = torch.tensor([], device=d)\n\n        DataParallel.__init__(self, module=base_kernel, device_ids=device_ids, output_device=output_device, dim=-2)\n\n        self.output_device = output_device if output_device else device_ids[0]\n\n        self.__cached_x1 = torch.empty(1)\n        self.__cached_x2 = torch.empty(1)\n\n    @property\n    def base_kernel(self):\n        return self.module\n\n    def forward(self, x1, x2, diag=False, **kwargs):\n        if diag:\n            return self.module.forward(x1, x2, diag=True, **kwargs).to(self.output_device)\n\n        if x1.size(-2) < len(self.device_ids) + 1:\n            return self.module.forward(x1, x2, diag=diag, **kwargs).to(self.output_device)\n\n        if not x1.device == self.__cached_x1.device or not torch.equal(x1, self.__cached_x1):\n            self._x1_scattered, self._kwargs = self.scatter((x1,), kwargs, self.device_ids)\n            self.__cached_x1 = x1\n\n        if not x2.device == self.__cached_x2.device or not torch.equal(x2, self.__cached_x2):\n            self._x2_subs = [x2.to(x1_[0].device) for x1_ in self._x1_scattered]\n            self.__cached_x2 = x2\n\n        inputs = tuple((x1_[0], x2_) for x1_, x2_ in zip(self._x1_scattered, self._x2_subs))\n\n        if not self.device_ids:\n            return self.module.forward(*inputs, **self._kwargs)\n\n        if len(self.device_ids) == 1:\n            return self.module.forward(*inputs[0], **self._kwargs[0])\n\n        # JIT modules can\'t be pickled and replicated yet\n        # But reinitializing the distance_module every forward pass\n        # is slow and should be removed once JIT modules can be pickled\n        def set_distance_module_to_none(module):\n            if hasattr(module, ""distance_module""):\n                module.distance_module = None\n\n        self.module.apply(set_distance_module_to_none)\n        # Can\'t cache the replication because the base kernel module can change every time (e.g. param updates)\n        replicas = self.replicate(self.module, self.device_ids[: len(inputs)])\n\n        # TODO: parallel_apply might be too heavyweight in some cases?\n        with settings.lazily_evaluate_kernels(False):\n            outputs = self.parallel_apply(replicas, inputs, self._kwargs)\n\n        return self.gather(outputs, self.output_device)\n\n    def gather(self, outputs, output_device):\n        return CatLazyTensor(*[lazify(o) for o in outputs], dim=self.dim, output_device=self.output_device)\n\n    def num_outputs_per_input(self, x1, x2):\n        return self.base_kernel.num_outputs_per_input(x1, x2)\n'"
gpytorch/kernels/multitask_kernel.py,5,"b'#!/usr/bin/env python3\n\nfrom ..lazy import KroneckerProductLazyTensor, lazify\nfrom .index_kernel import IndexKernel\nfrom .kernel import Kernel\n\n\nclass MultitaskKernel(Kernel):\n    r""""""\n    Kernel supporting Kronecker style multitask Gaussian processes (where every data point is evaluated at every\n    task) using :class:`gpytorch.kernels.IndexKernel` as a basic multitask kernel.\n\n    Given a base covariance module to be used for the data, :math:`K_{XX}`, this kernel computes a task kernel of\n    specified size :math:`K_{TT}` and returns :math:`K = K_{TT} \\otimes K_{XX}`. as an\n    :obj:`gpytorch.lazy.KroneckerProductLazyTensor`.\n\n    :param ~gpytorch.kernels.Kernel data_covar_module: Kernel to use as the data kernel.\n    :param int num_tasks: Number of tasks\n    :param int rank: (default 1) Rank of index kernel to use for task covariance matrix.\n    :param ~gpytorch.priors.Prior task_covar_prior: (default None) Prior to use for task kernel.\n        See :class:`gpytorch.kernels.IndexKernel` for details.\n    :param dict kwargs: Additional arguments to pass to the kernel.\n    """"""\n\n    def __init__(self, data_covar_module, num_tasks, rank=1, task_covar_prior=None, **kwargs):\n        """"""\n        """"""\n        super(MultitaskKernel, self).__init__(**kwargs)\n        self.task_covar_module = IndexKernel(\n            num_tasks=num_tasks, batch_shape=self.batch_shape, rank=rank, prior=task_covar_prior\n        )\n        self.data_covar_module = data_covar_module\n        self.num_tasks = num_tasks\n\n    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n        if last_dim_is_batch:\n            raise RuntimeError(""MultitaskKernel does not accept the last_dim_is_batch argument."")\n        covar_i = self.task_covar_module.covar_matrix\n        if len(x1.shape[:-2]):\n            covar_i = covar_i.repeat(*x1.shape[:-2], 1, 1)\n        covar_x = lazify(self.data_covar_module.forward(x1, x2, **params))\n        res = KroneckerProductLazyTensor(covar_x, covar_i)\n        return res.diag() if diag else res\n\n    def num_outputs_per_input(self, x1, x2):\n        """"""\n        Given `n` data points `x1` and `m` datapoints `x2`, this multitask\n        kernel returns an `(n*num_tasks) x (m*num_tasks)` covariance matrix.\n        """"""\n        return self.num_tasks\n'"
gpytorch/kernels/newton_girard_additive_kernel.py,9,"b'import torch\n\nfrom ..constraints import Positive\nfrom ..lazy import delazify\nfrom .kernel import Kernel\n\n\nclass NewtonGirardAdditiveKernel(Kernel):\n    def __init__(self, base_kernel, num_dims, max_degree=None, active_dims=None, **kwargs):\n        """"""Create an Additive Kernel a la https://arxiv.org/abs/1112.4394 using Newton-Girard Formulae\n\n        :param base_kernel: a base 1-dimensional kernel. NOTE: put ard_num_dims=d in the base kernel...\n        :param max_degree: the maximum numbers of kernel degrees to compute\n        :param active_dims:\n        :param kwargs:\n        """"""\n        super(NewtonGirardAdditiveKernel, self).__init__(active_dims=active_dims, **kwargs)\n\n        self.base_kernel = base_kernel\n        self.num_dims = num_dims\n        if max_degree is None:\n            self.max_degree = self.num_dims\n        elif max_degree > self.num_dims:  # force cap on max_degree (silently)\n            self.max_degree = self.num_dims\n        else:\n            self.max_degree = max_degree\n\n        self.register_parameter(\n            name=""raw_outputscale"", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, self.max_degree))\n        )\n        outputscale_constraint = Positive()\n        self.register_constraint(""raw_outputscale"", outputscale_constraint)\n        self.outputscale_constraint = outputscale_constraint\n        self.outputscale = [1 / self.max_degree for _ in range(self.max_degree)]\n\n    @property\n    def outputscale(self):\n        return self.raw_outputscale_constraint.transform(self.raw_outputscale)\n\n    @outputscale.setter\n    def outputscale(self, value):\n        self._set_outputscale(value)\n\n    def _set_outputscale(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_outputscale)\n\n        self.initialize(raw_outputscale=self.outputscale_constraint.inverse_transform(value))\n\n    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n        """"""Forward proceeds by Newton-Girard formulae""""""\n        if last_dim_is_batch:\n            raise RuntimeError(""NewtonGirardAdditiveKernel does not accept the last_dim_is_batch argument."")\n\n        # NOTE: comments about shape are only correct for the single-batch cases.\n        # kern_values is just the order-1 terms\n        # kern_values = D x n x n unless diag=True\n        kern_values = delazify(self.base_kernel(x1, x2, diag=diag, last_dim_is_batch=True, **params))\n        # last dim is batch, which gets moved up to pos. 1\n\n        kernel_dim = -3 if not diag else -2\n\n        shape = [1 for _ in range(len(kern_values.shape) + 1)]\n        shape[kernel_dim - 1] = -1\n        kvals = torch.arange(1, self.max_degree + 1, device=kern_values.device).reshape(*shape)\n        # kvals = R x 1 x 1 x 1 (these are indexes only)\n\n        # e_n = torch.ones(self.max_degree+1, *kern_values.shape[1:], device=kern_values.device)  # includes 0\n        # e_n: elementary symmetric polynomial of degree n (e.g. z1 z2 + z1 z3 + z2 z3)\n        # e_n is R x n x n, and the array is properly 0 indexed.\n        shape = [d_ for d_ in kern_values.shape]\n        shape[kernel_dim] = self.max_degree + 1\n        e_n = torch.empty(*shape, device=kern_values.device)\n        if kernel_dim == -3:\n            e_n[..., 0, :, :] = 1.0\n        else:\n            e_n[..., 0, :] = 1.0\n\n        # power sums s_k (e.g. sum_i^num_dims z_i^k\n        # s_k is R x n x n\n        s_k = kern_values.unsqueeze(kernel_dim - 1).pow(kvals).sum(dim=kernel_dim)\n\n        # just the constant -1\n        m1 = torch.tensor([-1], dtype=torch.float, device=kern_values.device)\n\n        shape = [1 for _ in range(len(kern_values.shape))]\n        shape[kernel_dim] = -1\n        for deg in range(1, self.max_degree + 1):  # deg goes from 1 to R (it\'s 1-indexed!)\n            # we avg over k [1, ..., deg] (-1)^(k-1)e_{deg-k} s_{k}\n\n            ks = torch.arange(1, deg + 1, device=kern_values.device, dtype=torch.float).reshape(*shape)  # use for pow\n            kslong = torch.arange(1, deg + 1, device=kern_values.device, dtype=torch.long)  # use for indexing\n\n            # note that s_k is 0-indexed, so we must subtract 1 from kslong\n            sum_ = (\n                m1.pow(ks - 1) * e_n.index_select(kernel_dim, deg - kslong) * s_k.index_select(kernel_dim, kslong - 1)\n            ).sum(dim=kernel_dim) / deg\n            if kernel_dim == -3:\n                e_n[..., deg, :, :] = sum_\n            else:\n                e_n[..., deg, :] = sum_\n\n        if kernel_dim == -3:\n            return (self.outputscale.unsqueeze(-1).unsqueeze(-1) * e_n.narrow(kernel_dim, 1, self.max_degree)).sum(\n                dim=kernel_dim\n            )\n        else:\n            return (self.outputscale.unsqueeze(-1) * e_n.narrow(kernel_dim, 1, self.max_degree)).sum(dim=kernel_dim)\n'"
gpytorch/kernels/periodic_kernel.py,12,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\n\nfrom ..constraints import Positive\nfrom .kernel import Kernel\n\n\nclass PeriodicKernel(Kernel):\n    r"""""" Computes a covariance matrix based on the periodic kernel\n    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n\n    .. math::\n\n       \\begin{equation*}\n          k_{\\text{Periodic}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\exp \\left(\n            \\frac{2 \\sin^2 \\left( \\pi \\Vert \\mathbf{x_1} - \\mathbf{x_2} \\Vert_1 / p \\right) }\n            { \\ell^2 } \\right)\n       \\end{equation*}\n\n    where\n\n    * :math:`p` is the periord length parameter.\n    * :math:`\\ell` is a lengthscale parameter.\n\n    .. note::\n\n        This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n\n    .. note::\n\n        This kernel does not have an ARD lengthscale option.\n\n    Args:\n        :attr:`batch_shape` (torch.Size, optional):\n            Set this if you want a separate lengthscale for each\n             batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.\n        :attr:`active_dims` (tuple of ints, optional):\n            Set this if you want to compute the covariance of only a few input dimensions. The ints\n            corresponds to the indices of the dimensions. Default: `None`.\n        :attr:`period_length_prior` (Prior, optional):\n            Set this if you want to apply a prior to the period length parameter.  Default: `None`.\n        :attr:`lengthscale_prior` (Prior, optional):\n            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.\n        :attr:`lengthscale_constraint` (Constraint, optional):\n            Set this if you want to apply a constraint to the value of the lengthscale. Default: `Positive`.\n        :attr:`period_length_constraint` (Constraint, optional):\n            Set this if you want to apply a constraint to the value of the period length. Default: `Positive`.\n        :attr:`eps` (float):\n            The minimum value that the lengthscale/period length can take\n            (prevents divide by zero errors). Default: `1e-6`.\n\n    Attributes:\n        :attr:`lengthscale` (Tensor):\n            The lengthscale parameter. Size = `*batch_shape x 1 x 1`.\n        :attr:`period_length` (Tensor):\n            The period length parameter. Size = `*batch_shape x 1 x 1`.\n\n    Example:\n        >>> x = torch.randn(10, 5)\n        >>> # Non-batch: Simple option\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())\n        >>>\n        >>> batch_x = torch.randn(2, 10, 5)\n        >>> # Batch: Simple option\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel())\n        >>> # Batch: different lengthscale for each batch\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.PeriodicKernel(batch_size=2))\n        >>> covar = covar_module(x)  # Output: LazyVariable of size (2 x 10 x 10)\n    """"""\n\n    has_lengthscale = True\n\n    def __init__(self, period_length_prior=None, period_length_constraint=None, **kwargs):\n        super(PeriodicKernel, self).__init__(**kwargs)\n        if period_length_constraint is None:\n            period_length_constraint = Positive()\n\n        self.register_parameter(\n            name=""raw_period_length"", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1, 1))\n        )\n\n        if period_length_prior is not None:\n            self.register_prior(\n                ""period_length_prior"",\n                period_length_prior,\n                lambda: self.period_length,\n                lambda v: self._set_period_length(v),\n            )\n\n        self.register_constraint(""raw_period_length"", period_length_constraint)\n\n    @property\n    def period_length(self):\n        return self.raw_period_length_constraint.transform(self.raw_period_length)\n\n    @period_length.setter\n    def period_length(self, value):\n        self._set_period_length(value)\n\n    def _set_period_length(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_period_length)\n        self.initialize(raw_period_length=self.raw_period_length_constraint.inverse_transform(value))\n\n    def forward(self, x1, x2, diag=False, **params):\n        x1_ = x1.div(self.period_length)\n        x2_ = x2.div(self.period_length)\n        diff = self.covar_dist(x1_, x2_, diag=diag, **params)\n        res = torch.sin(diff.mul(math.pi)).pow(2).mul(-2 / self.lengthscale).exp_()\n        if diag:\n            res = res.squeeze(0)\n        return res\n'"
gpytorch/kernels/polynomial_kernel.py,13,"b'#!/usr/bin/env python3\n\nfrom typing import Optional\n\nimport torch\n\nfrom ..constraints import Interval, Positive\nfrom ..priors import Prior\nfrom .kernel import Kernel\n\n\nclass PolynomialKernel(Kernel):\n    r""""""\n    Computes a covariance matrix based on the Polynomial kernel\n    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n\n    .. math::\n        \\begin{equation*}\n            k_\\text{Poly}(\\mathbf{x_1}, \\mathbf{x_2}) = (\\mathbf{x_1}^\\top\n            \\mathbf{x_2} + c)^{d}.\n        \\end{equation*}\n\n    where\n\n    * :math:`c` is an :attr:`offset` parameter.\n\n    Args:\n        :attr:`offset_prior` (:class:`gpytorch.priors.Prior`):\n            Prior over the offset parameter (default `None`).\n        :attr:`offset_constraint` (Constraint, optional):\n            Constraint to place on offset parameter. Default: `Positive`.\n        :attr:`active_dims` (list):\n            List of data dimensions to operate on.\n            `len(active_dims)` should equal `num_dimensions`.\n    """"""\n\n    def __init__(\n        self, power: int, offset_prior: Optional[Prior] = None, offset_constraint: Optional[Interval] = None, **kwargs\n    ):\n        super().__init__(**kwargs)\n        if offset_constraint is None:\n            offset_constraint = Positive()\n\n        self.register_parameter(name=""raw_offset"", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1)))\n\n        # We want the power to be a float so we dont have to worry about its device / dtype.\n        if torch.is_tensor(power):\n            if power.numel() > 1:\n                raise RuntimeError(""Cant create a Polynomial kernel with more than one power"")\n            else:\n                power = power.item()\n\n        self.power = power\n\n        if offset_prior is not None:\n            self.register_prior(""offset_prior"", offset_prior, lambda: self.offset, lambda v: self._set_offset(v))\n\n        self.register_constraint(""raw_offset"", offset_constraint)\n\n    @property\n    def offset(self) -> torch.Tensor:\n        return self.raw_offset_constraint.transform(self.raw_offset)\n\n    @offset.setter\n    def offset(self, value: torch.Tensor) -> None:\n        self._set_offset(value)\n\n    def _set_offset(self, value: torch.Tensor) -> None:\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_offset)\n        self.initialize(raw_offset=self.raw_offset_constraint.inverse_transform(value))\n\n    def forward(\n        self,\n        x1: torch.Tensor,\n        x2: torch.Tensor,\n        diag: Optional[bool] = False,\n        last_dim_is_batch: Optional[bool] = False,\n        **params,\n    ) -> torch.Tensor:\n        offset = self.offset.view(*self.batch_shape, 1, 1)\n\n        if last_dim_is_batch:\n            x1 = x1.transpose(-1, -2).unsqueeze(-1)\n            x2 = x2.transpose(-1, -2).unsqueeze(-1)\n\n        if diag:\n            return ((x1 * x2).sum(dim=-1) + self.offset).pow(self.power)\n\n        if x1.dim() == 2 and x2.dim() == 2:\n            return torch.addmm(offset, x1, x2.transpose(-2, -1)).pow(self.power)\n        else:\n            return (torch.matmul(x1, x2.transpose(-2, -1)) + offset).pow(self.power)\n'"
gpytorch/kernels/polynomial_kernel_grad.py,14,"b""#!/usr/bin/env python3\n\nfrom typing import Optional\n\nimport torch\n\nfrom .polynomial_kernel import PolynomialKernel\n\n\nclass PolynomialKernelGrad(PolynomialKernel):\n    def forward(\n        self,\n        x1: torch.Tensor,\n        x2: torch.Tensor,\n        diag: Optional[bool] = False,\n        last_dim_is_batch: Optional[bool] = False,\n        **params,\n    ) -> torch.Tensor:\n        offset = self.offset.view(*self.batch_shape, 1, 1)\n\n        batch_shape = x1.shape[:-2]\n        n1, d = x1.shape[-2:]\n        n2 = x2.shape[-2]\n\n        if diag:\n            base_diag = (x1 * x2).sum(dim=-1) + self.offset\n            K11_diag = base_diag.pow(self.power)\n\n            all_outers_diag = (x1 * x2).transpose(-2, -1).reshape(*batch_shape, -1)\n            K22_base_diag = self.power * (self.power - 1) * base_diag.pow(self.power - 2)\n            K12_base_diag = self.power * base_diag.pow(self.power - 1)\n\n            K22_diag = all_outers_diag * K22_base_diag.repeat(d) + K12_base_diag.repeat(d)\n\n            return torch.cat([K11_diag, K22_diag], dim=-1)\n        else:\n            base_inner_prod = torch.matmul(x1, x2.transpose(-2, -1)) + offset\n            K11 = base_inner_prod.pow(self.power)\n\n            K12_base = self.power * base_inner_prod.pow(self.power - 1)\n            K12 = torch.zeros(*batch_shape, n1, n2 * d, dtype=x1.dtype, device=x1.device)\n\n            ones_ = torch.ones(*batch_shape, d, 1, n2, dtype=x1.dtype, device=x1.device)\n            K12_outer_prods = torch.matmul(x1.transpose(-2, -1).unsqueeze(-1), ones_)\n            K12 = (K12_base.unsqueeze(-3) * K12_outer_prods).transpose(-3, -2).reshape(*batch_shape, n1, d * n2)\n\n            ones_ = torch.ones(*batch_shape, d, n1, 1, dtype=x1.dtype, device=x1.device)\n            K21_outer_prods = torch.matmul(ones_, x2.transpose(-2, -1).unsqueeze(-2))\n            K21 = (K12_base.unsqueeze(-3) * K21_outer_prods).view(*batch_shape, d * n1, n2)\n\n            K22_base = self.power * (self.power - 1) * base_inner_prod.pow(self.power - 2)\n            K22 = torch.zeros(*batch_shape, n1 * d, n2 * d, dtype=x1.dtype, device=x1.device)\n            all_outers = x1.unsqueeze(-2).unsqueeze(-2).transpose(-2, -1).matmul(x2.unsqueeze(-3).unsqueeze(-2))\n            all_outers = all_outers.transpose(-4, -2).transpose(-3, -1)\n            K22 = K22_base.unsqueeze(-3).unsqueeze(-3) * all_outers  # d x d x n1 x n2\n\n            # Can't avoid this for loop without unnecessary memory duplication, which is worse.\n            for i in range(d):\n                K22[..., i, i, :, :] = K22[..., i, i, :, :] + K12_base\n\n            K22 = K22.transpose(-4, -3).transpose(-3, -2).reshape(*batch_shape, n1 * d, n2 * d)\n\n            K = torch.cat([torch.cat([K11, K12], dim=-1), torch.cat([K21, K22], dim=-1)], dim=-2)\n\n            # Apply perfect shuffle\n            pi1 = torch.arange(n1 * (d + 1)).view(d + 1, n1).t().reshape((n1 * (d + 1)))\n            pi2 = torch.arange(n2 * (d + 1)).view(d + 1, n2).t().reshape((n2 * (d + 1)))\n            K = K[..., pi1, :][..., :, pi2]\n\n            return K\n\n    def num_outputs_per_input(self, x1, x2):\n        return x1.size(-1) + 1\n"""
gpytorch/kernels/product_structure_kernel.py,1,"b'#!/usr/bin/env python3\n\nfrom ..lazy import lazify\nfrom .kernel import Kernel\n\n\nclass ProductStructureKernel(Kernel):\n    r""""""\n    A Kernel decorator for kernels with product structure. If a kernel decomposes\n    multiplicatively, then this module will be much more computationally efficient.\n\n    A kernel function `k` has product structure if it can be written as\n\n    .. math::\n\n       \\begin{equation*}\n          k(\\mathbf{x_1}, \\mathbf{x_2}) = k\'(x_1^{(1)}, x_2^{(1)}) * \\ldots * k\'(x_1^{(d)}, x_2^{(d)})\n       \\end{equation*}\n\n    for some kernel :math:`k\'` that operates on each dimension.\n\n    Given a `b x n x d` input, `ProductStructureKernel` computes `d` one-dimensional kernels\n    (using the supplied base_kernel), and then multiplies the component kernels together.\n    Unlike :class:`~gpytorch.kernels.ProductKernel`, `ProductStructureKernel` computes each\n    of the product terms in batch, making it very fast.\n\n    See `Product Kernel Interpolation for Scalable Gaussian Processes`_ for more detail.\n\n    Args:\n        - :attr:`base_kernel` (Kernel):\n            The kernel to approximate with KISS-GP\n        - :attr:`num_dims` (int):\n            The dimension of the input data.\n        - :attr:`active_dims` (tuple of ints, optional):\n            Passed down to the `base_kernel`.\n\n    .. _Product Kernel Interpolation for Scalable Gaussian Processes:\n        https://arxiv.org/pdf/1802.08903\n    """"""\n\n    @property\n    def is_stationary(self) -> bool:\n        """"""\n        Kernel is stationary if the base kernel is stationary.\n        """"""\n        return self.base_kernel.is_stationary\n\n    def __init__(self, base_kernel, num_dims, active_dims=None):\n        super(ProductStructureKernel, self).__init__(active_dims=active_dims)\n        self.base_kernel = base_kernel\n        self.num_dims = num_dims\n\n    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n        if last_dim_is_batch:\n            raise RuntimeError(""ProductStructureKernel does not accept the last_dim_is_batch argument."")\n\n        res = self.base_kernel(x1, x2, diag=diag, last_dim_is_batch=True, **params)\n        res = res.prod(-2 if diag else -3)\n        return res\n\n    def num_outputs_per_input(self, x1, x2):\n        return self.base_kernel.num_outputs_per_input(x1, x2)\n\n    def __call__(self, x1_, x2_=None, diag=False, last_dim_is_batch=False, **params):\n        """"""\n        We cannot lazily evaluate actual kernel calls when using SKIP, because we\n        cannot root decompose rectangular matrices.\n\n        Because we slice in to the kernel during prediction to get the test x train\n        covar before calling evaluate_kernel, the order of operations would mean we\n        would get a MulLazyTensor representing a rectangular matrix, which we\n        cannot matmul with because we cannot root decompose it. Thus, SKIP actually\n        *requires* that we work with the full (train + test) x (train + test)\n        kernel matrix.\n        """"""\n        res = super().__call__(x1_, x2_, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n        res = lazify(res).evaluate_kernel()\n        return res\n'"
gpytorch/kernels/rbf_kernel.py,10,"b'#!/usr/bin/env python3\n\nfrom ..functions import RBFCovariance\nfrom ..settings import trace_mode\nfrom .kernel import Kernel\n\n\ndef postprocess_rbf(dist_mat):\n    return dist_mat.div_(-2).exp_()\n\n\nclass RBFKernel(Kernel):\n    r""""""\n    Computes a covariance matrix based on the RBF (squared exponential) kernel\n    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n\n    .. math::\n\n       \\begin{equation*}\n          k_{\\text{RBF}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\exp \\left( -\\frac{1}{2}\n          (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-2} (\\mathbf{x_1} - \\mathbf{x_2}) \\right)\n       \\end{equation*}\n\n    where :math:`\\Theta` is a :attr:`lengthscale` parameter.\n    See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.\n\n    .. note::\n\n        This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n\n    Args:\n        :attr:`ard_num_dims` (int, optional):\n            Set this if you want a separate lengthscale for each\n            input dimension. It should be `d` if :attr:`x1` is a `n x d` matrix. Default: `None`\n        :attr:`batch_shape` (torch.Size, optional):\n            Set this if you want a separate lengthscale for each\n            batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.\n        :attr:`active_dims` (tuple of ints, optional):\n            Set this if you want to compute the covariance of only a few input dimensions. The ints\n            corresponds to the indices of the dimensions. Default: `None`.\n        :attr:`lengthscale_prior` (Prior, optional):\n            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.\n        :attr:`lengthscale_constraint` (Constraint, optional):\n            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n        :attr:`eps` (float):\n            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n\n    Attributes:\n        :attr:`lengthscale` (Tensor):\n            The lengthscale parameter. Size/shape of parameter depends on the\n            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n\n    Example:\n        >>> x = torch.randn(10, 5)\n        >>> # Non-batch: Simple option\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n        >>> # Non-batch: ARD (different lengthscale for each input dimension)\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=5))\n        >>> covar = covar_module(x)  # Output: LazyTensor of size (10 x 10)\n        >>>\n        >>> batch_x = torch.randn(2, 10, 5)\n        >>> # Batch: Simple option\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n        >>> # Batch: different lengthscale for each batch\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([2])))\n        >>> covar = covar_module(x)  # Output: LazyTensor of size (2 x 10 x 10)\n    """"""\n\n    has_lengthscale = True\n\n    def forward(self, x1, x2, diag=False, **params):\n        if (\n            x1.requires_grad\n            or x2.requires_grad\n            or (self.ard_num_dims is not None and self.ard_num_dims > 1)\n            or diag\n            or trace_mode.on()\n        ):\n            x1_ = x1.div(self.lengthscale)\n            x2_ = x2.div(self.lengthscale)\n            return self.covar_dist(\n                x1_, x2_, square_dist=True, diag=diag, dist_postprocess_func=postprocess_rbf, postprocess=True, **params\n            )\n        return RBFCovariance().apply(\n            x1,\n            x2,\n            self.lengthscale,\n            lambda x1, x2: self.covar_dist(\n                x1, x2, square_dist=True, diag=False, dist_postprocess_func=postprocess_rbf, postprocess=False, **params\n            ),\n        )\n'"
gpytorch/kernels/rbf_kernel_grad.py,20,"b'#!/usr/bin/env python3\nimport torch\n\nfrom ..lazy.kronecker_product_lazy_tensor import KroneckerProductLazyTensor\nfrom .rbf_kernel import RBFKernel, postprocess_rbf\n\n\nclass RBFKernelGrad(RBFKernel):\n    r""""""\n    Computes a covariance matrix of the RBF kernel that models the covariance\n    between the values and partial derivatives for inputs :math:`\\mathbf{x_1}`\n    and :math:`\\mathbf{x_2}`.\n\n    See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.\n\n    .. note::\n\n        This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n\n    Args:\n        :attr:`batch_shape` (torch.Size, optional):\n            Set this if you want a separate lengthscale for each\n             batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.\n        :attr:`active_dims` (tuple of ints, optional):\n            Set this if you want to compute the covariance of only a few input dimensions. The ints\n            corresponds to the indices of the dimensions. Default: `None`.\n        :attr:`lengthscale_prior` (Prior, optional):\n            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.\n        :attr:`lengthscale_constraint` (Constraint, optional):\n            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n        :attr:`eps` (float):\n            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n\n    Attributes:\n        :attr:`lengthscale` (Tensor):\n            The lengthscale parameter. Size/shape of parameter depends on the\n            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n\n    Example:\n        >>> x = torch.randn(10, 5)\n        >>> # Non-batch: Simple option\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernelGrad())\n        >>> covar = covar_module(x)  # Output: LazyTensor of size (60 x 60), where 60 = n * (d + 1)\n        >>>\n        >>> batch_x = torch.randn(2, 10, 5)\n        >>> # Batch: Simple option\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernelGrad())\n        >>> # Batch: different lengthscale for each batch\n        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernelGrad(batch_shape=torch.Size([2])))\n        >>> covar = covar_module(x)  # Output: LazyTensor of size (2 x 60 x 60)\n    """"""\n\n    def forward(self, x1, x2, diag=False, **params):\n        batch_shape = x1.shape[:-2]\n        n_batch_dims = len(batch_shape)\n        n1, d = x1.shape[-2:]\n        n2 = x2.shape[-2]\n\n        K = torch.zeros(*batch_shape, n1 * (d + 1), n2 * (d + 1), device=x1.device, dtype=x1.dtype)\n\n        if not diag:\n            # Scale the inputs by the lengthscale (for stability)\n            x1_ = x1.div(self.lengthscale)\n            x2_ = x2.div(self.lengthscale)\n\n            # Form all possible rank-1 products for the gradient and Hessian blocks\n            outer = x1_.view(*batch_shape, n1, 1, d) - x2_.view(*batch_shape, 1, n2, d)\n            outer = outer / self.lengthscale.unsqueeze(-2)\n            outer = torch.transpose(outer, -1, -2).contiguous()\n\n            # 1) Kernel block\n            diff = self.covar_dist(x1_, x2_, square_dist=True, dist_postprocess_func=postprocess_rbf, **params)\n            K_11 = diff\n            K[..., :n1, :n2] = K_11\n\n            # 2) First gradient block\n            outer1 = outer.view(*batch_shape, n1, n2 * d)\n            K[..., :n1, n2:] = outer1 * K_11.repeat([*([1] * (n_batch_dims + 1)), d])\n\n            # 3) Second gradient block\n            outer2 = outer.transpose(-1, -3).reshape(*batch_shape, n2, n1 * d)\n            outer2 = outer2.transpose(-1, -2)\n            K[..., n1:, :n2] = -outer2 * K_11.repeat([*([1] * n_batch_dims), d, 1])\n\n            # 4) Hessian block\n            outer3 = outer1.repeat([*([1] * n_batch_dims), d, 1]) * outer2.repeat([*([1] * (n_batch_dims + 1)), d])\n            kp = KroneckerProductLazyTensor(\n                torch.eye(d, d, device=x1.device, dtype=x1.dtype).repeat(*batch_shape, 1, 1) / self.lengthscale.pow(2),\n                torch.ones(n1, n2, device=x1.device, dtype=x1.dtype).repeat(*batch_shape, 1, 1),\n            )\n            chain_rule = kp.evaluate() - outer3\n            K[..., n1:, n2:] = chain_rule * K_11.repeat([*([1] * n_batch_dims), d, d])\n\n            # Symmetrize for stability\n            if n1 == n2 and torch.eq(x1, x2).all():\n                K = 0.5 * (K.transpose(-1, -2) + K)\n\n            # Apply a perfect shuffle permutation to match the MutiTask ordering\n            pi1 = torch.arange(n1 * (d + 1)).view(d + 1, n1).t().reshape((n1 * (d + 1)))\n            pi2 = torch.arange(n2 * (d + 1)).view(d + 1, n2).t().reshape((n2 * (d + 1)))\n            K = K[..., pi1, :][..., :, pi2]\n\n            return K\n\n        else:\n            if not (n1 == n2 and torch.eq(x1, x2).all()):\n                raise RuntimeError(""diag=True only works when x1 == x2"")\n\n            kernel_diag = super(RBFKernelGrad, self).forward(x1, x2, diag=True)\n            grad_diag = torch.ones(*batch_shape, n2, d, device=x1.device, dtype=x1.dtype) / self.lengthscale.pow_(2)\n            grad_diag = grad_diag.transpose(-1, -2).reshape(*batch_shape, n2 * d)\n            k_diag = torch.cat((kernel_diag, grad_diag), dim=-1)\n            pi = torch.arange(n2 * (d + 1)).view(d + 1, n2).t().reshape((n2 * (d + 1)))\n            return k_diag[..., pi]\n\n    def num_outputs_per_input(self, x1, x2):\n        return x1.size(-1) + 1\n'"
gpytorch/kernels/rq_kernel.py,7,"b'from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport torch\n\nfrom ..constraints import Positive\nfrom .kernel import Kernel\n\n\nclass RQKernel(Kernel):\n    r""""""\n    Computes a covariance matrix based on the rational quadratic kernel\n    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n\n    .. math::\n\n       \\begin{equation*}\n          k_{\\text{RQ}}(\\mathbf{x_1}, \\mathbf{x_2}) =  \\left(1 + \\frac{1}{2\\alpha}\n          (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-2} (\\mathbf{x_1} - \\mathbf{x_2}) \\right)^{-\\alpha}\n       \\end{equation*}\n\n    where :math:`\\Theta` is a :attr:`lengthscale` parameter, and :math:`\\alpha` is the\n    rational quadratic relative weighting parameter.\n    See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.\n\n    .. note::\n\n        This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n\n    Args:\n        :attr:`ard_num_dims` (int, optional):\n            Set this if you want a separate lengthscale for each\n            input dimension. It should be `d` if :attr:`x1` is a `n x d` matrix. Default: `None`\n        :attr:`batch_shape` (torch.Size, optional):\n            Set this if you want a separate lengthscale for each\n            batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`.\n        :attr:`active_dims` (tuple of ints, optional):\n            Set this if you want to compute the covariance of only a few input dimensions. The ints\n            corresponds to the indices of the dimensions. Default: `None`.\n        :attr:`lengthscale_prior` (Prior, optional):\n            Set this if you want to apply a prior to the lengthscale parameter.  Default: `None`.\n        :attr:`lengthscale_constraint` (Constraint, optional):\n            Set this if you want to apply a constraint to the lengthscale parameter. Default: `Positive`.\n        :attr:`alpha_constraint` (Constraint, optional):\n            Set this if you want to apply a constraint to the alpha parameter. Default: `Positive`.\n        :attr:`eps` (float):\n            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n\n    Attributes:\n        :attr:`lengthscale` (Tensor):\n            The lengthscale parameter. Size/shape of parameter depends on the\n            :attr:`ard_num_dims` and :attr:`batch_shape` arguments.\n        :attr:`alpha` (Tensor):\n            The rational quadratic relative weighting parameter. Size/shape of parameter depends\n            on the :attr:`batch_shape` argument\n    """"""\n\n    has_lengthscale = True\n\n    def __init__(self, alpha_constraint=None, **kwargs):\n        super(RQKernel, self).__init__(**kwargs)\n        self.register_parameter(name=""raw_alpha"", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, 1)))\n        if alpha_constraint is None:\n            alpha_constraint = Positive()\n\n        self.register_constraint(""raw_alpha"", alpha_constraint)\n\n    def forward(self, x1, x2, diag=False, **params):\n        def postprocess_rq(dist):\n            alpha = self.alpha\n            for _ in range(1, len(dist.shape) - len(self.batch_shape)):\n                alpha = alpha.unsqueeze(-1)\n            return (1 + dist.div(2 * alpha)).pow(-alpha)\n\n        x1_ = x1.div(self.lengthscale)\n        x2_ = x2.div(self.lengthscale)\n        return self.covar_dist(\n            x1_, x2_, square_dist=True, diag=diag, dist_postprocess_func=postprocess_rq, postprocess=True, **params\n        )\n\n    @property\n    def alpha(self):\n        return self.raw_alpha_constraint.transform(self.raw_alpha)\n\n    @alpha.setter\n    def alpha(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_lengthscale)\n        self.initialize(raw_alpha=self.raw_alpha_constraint.inverse_transform(value))\n'"
gpytorch/kernels/scale_kernel.py,8,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..constraints import Positive\nfrom ..lazy import delazify\nfrom .kernel import Kernel\n\n\nclass ScaleKernel(Kernel):\n    r""""""\n    Decorates an existing kernel object with an output scale, i.e.\n\n    .. math::\n\n       \\begin{equation*}\n          K_{\\text{scaled}} = \\theta_\\text{scale} K_{\\text{orig}}\n       \\end{equation*}\n\n    where :math:`\\theta_\\text{scale}` is the `outputscale` parameter.\n\n    In batch-mode (i.e. when :math:`x_1` and :math:`x_2` are batches of input matrices), each\n    batch of data can have its own `outputscale` parameter by setting the `batch_shape`\n    keyword argument to the appropriate number of batches.\n\n    .. note::\n        The :attr:`outputscale` parameter is parameterized on a log scale to constrain it to be positive.\n        You can set a prior on this parameter using the :attr:`outputscale_prior` argument.\n\n    Args:\n        :attr:`base_kernel` (Kernel):\n            The base kernel to be scaled.\n        :attr:`batch_shape` (int, optional):\n            Set this if you want a separate outputscale for each batch of input data. It should be `b`\n            if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([])`\n        :attr:`outputscale_prior` (Prior, optional): Set this if you want to apply a prior to the outputscale\n            parameter.  Default: `None`\n        :attr:`outputscale_constraint` (Constraint, optional): Set this if you want to apply a constraint to the\n            outputscale parameter. Default: `Positive`.\n\n    Attributes:\n        :attr:`base_kernel` (Kernel):\n            The kernel module to be scaled.\n        :attr:`outputscale` (Tensor):\n            The outputscale parameter. Size/shape of parameter depends on the :attr:`batch_shape` arguments.\n\n    Example:\n        >>> x = torch.randn(10, 5)\n        >>> base_covar_module = gpytorch.kernels.RBFKernel()\n        >>> scaled_covar_module = gpytorch.kernels.ScaleKernel(base_covar_module)\n        >>> covar = scaled_covar_module(x)  # Output: LazyTensor of size (10 x 10)\n    """"""\n\n    @property\n    def is_stationary(self) -> bool:\n        """"""\n        Kernel is stationary if base kernel is stationary.\n        """"""\n        return self.base_kernel.is_stationary\n\n    def __init__(self, base_kernel, outputscale_prior=None, outputscale_constraint=None, **kwargs):\n        if base_kernel.active_dims is not None:\n            kwargs[""active_dims""] = base_kernel.active_dims\n        super(ScaleKernel, self).__init__(**kwargs)\n        if outputscale_constraint is None:\n            outputscale_constraint = Positive()\n\n        self.base_kernel = base_kernel\n        outputscale = torch.zeros(*self.batch_shape) if len(self.batch_shape) else torch.tensor(0.0)\n        self.register_parameter(name=""raw_outputscale"", parameter=torch.nn.Parameter(outputscale))\n        if outputscale_prior is not None:\n            self.register_prior(\n                ""outputscale_prior"", outputscale_prior, lambda: self.outputscale, lambda v: self._set_outputscale(v)\n            )\n\n        self.register_constraint(""raw_outputscale"", outputscale_constraint)\n\n    @property\n    def outputscale(self):\n        return self.raw_outputscale_constraint.transform(self.raw_outputscale)\n\n    @outputscale.setter\n    def outputscale(self, value):\n        self._set_outputscale(value)\n\n    def _set_outputscale(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_outputscale)\n        self.initialize(raw_outputscale=self.raw_outputscale_constraint.inverse_transform(value))\n\n    def forward(self, x1, x2, last_dim_is_batch=False, diag=False, **params):\n        orig_output = self.base_kernel.forward(x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n        outputscales = self.outputscale\n        if last_dim_is_batch:\n            outputscales = outputscales.unsqueeze(-1)\n        if diag:\n            outputscales = outputscales.unsqueeze(-1)\n            return delazify(orig_output) * outputscales\n        else:\n            outputscales = outputscales.view(*outputscales.shape, 1, 1)\n            return orig_output.mul(outputscales)\n\n    def num_outputs_per_input(self, x1, x2):\n        return self.base_kernel.num_outputs_per_input(x1, x2)\n'"
gpytorch/kernels/spectral_mixture_kernel.py,22,"b'#!/usr/bin/env python3\n\nimport logging\nimport math\n\nimport torch\n\nfrom ..constraints import Positive\nfrom .kernel import Kernel\n\nlogger = logging.getLogger()\n\n\nclass SpectralMixtureKernel(Kernel):\n    r""""""\n    Computes a covariance matrix based on the Spectral Mixture Kernel\n    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n    It was proposed in `Gaussian Process Kernels for Pattern Discovery and Extrapolation`_.\n\n    .. note::\n\n        Unlike other kernels,\n        * :attr:`ard_num_dims` **must equal** the number of dimensions of the data\n        * :attr:`batch_shape` **must equal** the batch size of the data (torch.Size([1]) if the data is not batched)\n        * :attr:`batch_shape` **cannot** contain more than one batch dimension.\n        * This kernel should not be combined with a :class:`gpytorch.kernels.ScaleKernel`.\n\n    Args:\n        :attr:`num_mixtures` (int, optional):\n            The number of components in the mixture.\n        :attr:`ard_num_dims` (int, optional):\n            Set this to match the dimensionality of the input.\n            It should be `d` if :attr:`x1` is a `n x d` matrix. Default: `1`\n        :attr:`batch_shape` (torch.Size, optional):\n            Set this if the data is\n             batch of input data. It should be `b` if :attr:`x1` is a `b x n x d` tensor. Default: `torch.Size([1])`\n        :attr:`active_dims` (tuple of ints, optional):\n            Set this if you want to compute the covariance of only a few input dimensions. The ints\n            corresponds to the indices of the dimensions. Default: `None`.\n        :attr:`eps` (float):\n            The minimum value that the lengthscale can take (prevents divide by zero errors). Default: `1e-6`.\n\n    Attributes:\n        :attr:`mixture_lengthscale` (Tensor):\n            The lengthscale parameter. Given `k` mixture components, and `b x n x d` data, this will be of\n            size `b x k x 1 x d`.\n        :attr:`mixture_means` (Tensor):\n            The mixture mean parameters (`b x k x 1 x d`).\n        :attr:`mixture_weights` (Tensor):\n            The mixture weight parameters (`b x k`).\n\n    Example:\n        >>> # Non-batch\n        >>> x = torch.randn(10, 5)\n        >>> covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, ard_num_dims=5)\n        >>> covar = covar_module(x)  # Output: LazyVariable of size (10 x 10)\n        >>>\n        >>> # Batch\n        >>> batch_x = torch.randn(2, 10, 5)\n        >>> covar_module = gpytorch.kernels.SpectralMixtureKernel(num_mixtures=4, batch_size=2, ard_num_dims=5)\n        >>> covar = covar_module(x)  # Output: LazyVariable of size (10 x 10)\n\n\n    .. _Gaussian Process Kernels for Pattern Discovery and Extrapolation:\n        https://arxiv.org/pdf/1302.4245.pdf\n    """"""\n\n    is_stationary = True  # kernel is stationary even though it does not have a lengthscale\n\n    def __init__(\n        self,\n        num_mixtures=None,\n        ard_num_dims=1,\n        batch_shape=torch.Size([]),\n        mixture_scales_prior=None,\n        mixture_scales_constraint=None,\n        mixture_means_prior=None,\n        mixture_means_constraint=None,\n        mixture_weights_prior=None,\n        mixture_weights_constraint=None,\n        **kwargs,\n    ):\n        if num_mixtures is None:\n            raise RuntimeError(""num_mixtures is a required argument"")\n        if mixture_means_prior is not None or mixture_scales_prior is not None or mixture_weights_prior is not None:\n            logger.warning(""Priors not implemented for SpectralMixtureKernel"")\n\n        # This kernel does not use the default lengthscale\n        super(SpectralMixtureKernel, self).__init__(ard_num_dims=ard_num_dims, batch_shape=batch_shape, **kwargs)\n        self.num_mixtures = num_mixtures\n\n        if mixture_scales_constraint is None:\n            mixture_scales_constraint = Positive()\n\n        if mixture_means_constraint is None:\n            mixture_means_constraint = Positive()\n\n        if mixture_weights_constraint is None:\n            mixture_weights_constraint = Positive()\n\n        self.register_parameter(\n            name=""raw_mixture_weights"", parameter=torch.nn.Parameter(torch.zeros(*self.batch_shape, self.num_mixtures))\n        )\n        ms_shape = torch.Size([*self.batch_shape, self.num_mixtures, 1, self.ard_num_dims])\n        self.register_parameter(name=""raw_mixture_means"", parameter=torch.nn.Parameter(torch.zeros(ms_shape)))\n        self.register_parameter(name=""raw_mixture_scales"", parameter=torch.nn.Parameter(torch.zeros(ms_shape)))\n\n        self.register_constraint(""raw_mixture_scales"", mixture_scales_constraint)\n        self.register_constraint(""raw_mixture_means"", mixture_means_constraint)\n        self.register_constraint(""raw_mixture_weights"", mixture_weights_constraint)\n\n    @property\n    def mixture_scales(self):\n        return self.raw_mixture_scales_constraint.transform(self.raw_mixture_scales)\n\n    @mixture_scales.setter\n    def mixture_scales(self, value):\n        self._set_mixture_scales(value)\n\n    def _set_mixture_scales(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_mixture_scales)\n        self.initialize(raw_mixture_scales=self.raw_mixture_scales_constraint.inverse_transform(value))\n\n    @property\n    def mixture_means(self):\n        return self.raw_mixture_means_constraint.transform(self.raw_mixture_means)\n\n    @mixture_means.setter\n    def mixture_means(self, value):\n        self._set_mixture_means(value)\n\n    def _set_mixture_means(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_mixture_means)\n        self.initialize(raw_mixture_means=self.raw_mixture_means_constraint.inverse_transform(value))\n\n    @property\n    def mixture_weights(self):\n        return self.raw_mixture_weights_constraint.transform(self.raw_mixture_weights)\n\n    @mixture_weights.setter\n    def mixture_weights(self, value):\n        self._set_mixture_weights(value)\n\n    def _set_mixture_weights(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_mixture_weights)\n        self.initialize(raw_mixture_weights=self.raw_mixture_weights_constraint.inverse_transform(value))\n\n    def initialize_from_data_empspect(self, train_x, train_y):\n        """"""\n        Initialize mixture components based on the empirical spectrum of the data.\n\n        This will often be better than the standard initialize_from_data method.\n        """"""\n        import numpy as np\n        from scipy.fftpack import fft\n        from scipy.integrate import cumtrapz\n\n        N = train_x.size(-2)\n        emp_spect = np.abs(fft(train_y.cpu().detach().numpy())) ** 2 / N\n        M = math.floor(N / 2)\n\n        freq1 = np.arange(M + 1)\n        freq2 = np.arange(-M + 1, 0)\n        freq = np.hstack((freq1, freq2)) / N\n        freq = freq[: M + 1]\n        emp_spect = emp_spect[: M + 1]\n\n        total_area = np.trapz(emp_spect, freq)\n        spec_cdf = np.hstack((np.zeros(1), cumtrapz(emp_spect, freq)))\n        spec_cdf = spec_cdf / total_area\n\n        a = np.random.rand(1000, self.ard_num_dims)\n        p, q = np.histogram(a, spec_cdf)\n        bins = np.digitize(a, q)\n        slopes = (spec_cdf[bins] - spec_cdf[bins - 1]) / (freq[bins] - freq[bins - 1])\n        intercepts = spec_cdf[bins - 1] - slopes * freq[bins - 1]\n        inv_spec = (a - intercepts) / slopes\n\n        from sklearn.mixture import GaussianMixture\n\n        GMM = GaussianMixture(n_components=self.num_mixtures, covariance_type=""diag"").fit(inv_spec)\n        means = GMM.means_\n        varz = GMM.covariances_\n        weights = GMM.weights_\n\n        self.mixture_means = means\n        self.mixture_scales = varz\n        self.mixture_weights = weights\n\n    def initialize_from_data(self, train_x, train_y, **kwargs):\n        if not torch.is_tensor(train_x) or not torch.is_tensor(train_y):\n            raise RuntimeError(""train_x and train_y should be tensors"")\n        if train_x.ndimension() == 1:\n            train_x = train_x.unsqueeze(-1)\n        if train_x.ndimension() == 2:\n            train_x = train_x.unsqueeze(0)\n\n        train_x_sort = train_x.sort(1)[0]\n        max_dist = train_x_sort[:, -1, :] - train_x_sort[:, 0, :]\n        min_dist_sort = (train_x_sort[:, 1:, :] - train_x_sort[:, :-1, :]).squeeze(0)\n        min_dist = torch.zeros(1, self.ard_num_dims, dtype=train_x.dtype, device=train_x.device)\n        for ind in range(self.ard_num_dims):\n            min_dist[:, ind] = min_dist_sort[((min_dist_sort[:, ind]).nonzero())[0], ind]\n\n        # Inverse of lengthscales should be drawn from truncated Gaussian | N(0, max_dist^2) |\n        self.raw_mixture_scales.data.normal_().mul_(max_dist).abs_().pow_(-1)\n        self.raw_mixture_scales.data = self.raw_mixture_scales_constraint.inverse_transform(\n            self.raw_mixture_scales.data\n        )\n        # Draw means from Unif(0, 0.5 / minimum distance between two points)\n        self.raw_mixture_means.data.uniform_().mul_(0.5).div_(min_dist)\n        self.raw_mixture_means.data = self.raw_mixture_means_constraint.inverse_transform(self.raw_mixture_means.data)\n        # Mixture weights should be roughly the stdv of the y values divided by the number of mixtures\n        self.raw_mixture_weights.data.fill_(train_y.std() / self.num_mixtures)\n        self.raw_mixture_weights.data = self.raw_mixture_weights_constraint.inverse_transform(\n            self.raw_mixture_weights.data\n        )\n\n    def _create_input_grid(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n        """"""\n        This is a helper method for creating a grid of the kernel\'s inputs.\n        Use this helper rather than maually creating a meshgrid.\n\n        The grid dimensions depend on the kernel\'s evaluation mode.\n\n        Args:\n            :attr:`x1` (Tensor `n x d` or `b x n x d`)\n            :attr:`x2` (Tensor `m x d` or `b x m x d`) - for diag mode, these must be the same inputs\n\n        Returns:\n            (:class:`Tensor`, :class:`Tensor) corresponding to the gridded `x1` and `x2`.\n            The shape depends on the kernel\'s mode\n\n            * `full_covar`: (`b x n x 1 x d` and `b x 1 x m x d`)\n            * `full_covar` with `last_dim_is_batch=True`: (`b x k x n x 1 x 1` and `b x k x 1 x m x 1`)\n            * `diag`: (`b x n x d` and `b x n x d`)\n            * `diag` with `last_dim_is_batch=True`: (`b x k x n x 1` and `b x k x n x 1`)\n        """"""\n        x1_, x2_ = x1, x2\n        if last_dim_is_batch:\n            x1_ = x1_.transpose(-1, -2).unsqueeze(-1)\n            if torch.equal(x1, x2):\n                x2_ = x1_\n            else:\n                x2_ = x2_.transpose(-1, -2).unsqueeze(-1)\n\n        if diag:\n            return x1_, x2_\n        else:\n            return x1_.unsqueeze(-2), x2_.unsqueeze(-3)\n\n    def forward(self, x1, x2, last_dim_is_batch=False, **params):\n        batch_shape = x1.shape[:-2]\n        n, num_dims = x1.shape[-2:]\n\n        if not num_dims == self.ard_num_dims:\n            raise RuntimeError(\n                ""The SpectralMixtureKernel expected the input to have {} dimensionality ""\n                ""(based on the ard_num_dims argument). Got {}."".format(self.ard_num_dims, num_dims)\n            )\n        if not batch_shape == self.batch_shape:\n            raise RuntimeError(\n                ""The SpectralMixtureKernel expected the input to have a batch_size of {} ""\n                ""(based on the batch_size argument). Got {}."".format(self.batch_shape, batch_shape)\n            )\n\n        # Expand x1 and x2 to account for the number of mixtures\n        # Should make x1/x2 (b x k x n x d) for k mixtures\n        x1_ = x1.unsqueeze(len(batch_shape))\n        x2_ = x2.unsqueeze(len(batch_shape))\n\n        # Compute distances - scaled by appropriate parameters\n        x1_exp = x1_ * self.mixture_scales\n        x2_exp = x2_ * self.mixture_scales\n        x1_cos = x1_ * self.mixture_means\n        x2_cos = x2_ * self.mixture_means\n\n        # Create grids\n        x1_exp_, x2_exp_ = self._create_input_grid(x1_exp, x2_exp, last_dim_is_batch=last_dim_is_batch, **params)\n        x1_cos_, x2_cos_ = self._create_input_grid(x1_cos, x2_cos, last_dim_is_batch=last_dim_is_batch, **params)\n\n        # Compute the exponential and cosine terms\n        exp_term = (x1_exp_ - x2_exp_).pow_(2).mul_(-2 * math.pi ** 2)\n        cos_term = (x1_cos_ - x2_cos_).mul_(2 * math.pi)\n        res = exp_term.exp_() * cos_term.cos_()\n\n        # Product over dimensions\n        if last_dim_is_batch:\n            res = res.squeeze(-1)\n        else:\n            res = res.prod(-1)\n\n        # Sum over mixtures\n        mixture_weights = self.mixture_weights\n\n        if last_dim_is_batch:\n            mixture_weights = mixture_weights.unsqueeze(-1)\n        while mixture_weights.dim() < res.dim():\n            mixture_weights = mixture_weights.unsqueeze(-1)\n\n        res = (res * mixture_weights).sum(len(batch_shape))\n        return res\n'"
gpytorch/lazy/__init__.py,0,"b'#!/usr/bin/env python3\n\nfrom .added_diag_lazy_tensor import AddedDiagLazyTensor\nfrom .batch_repeat_lazy_tensor import BatchRepeatLazyTensor\nfrom .block_diag_lazy_tensor import BlockDiagLazyTensor\nfrom .block_interleaved_lazy_tensor import BlockInterleavedLazyTensor\nfrom .block_lazy_tensor import BlockLazyTensor\nfrom .cached_cg_lazy_tensor import CachedCGLazyTensor\nfrom .cat_lazy_tensor import CatLazyTensor, cat\nfrom .chol_lazy_tensor import CholLazyTensor\nfrom .constant_mul_lazy_tensor import ConstantMulLazyTensor\nfrom .diag_lazy_tensor import DiagLazyTensor\nfrom .interpolated_lazy_tensor import InterpolatedLazyTensor\nfrom .keops_lazy_tensor import KeOpsLazyTensor\nfrom .kronecker_product_lazy_tensor import KroneckerProductLazyTensor\nfrom .lazy_evaluated_kernel_tensor import LazyEvaluatedKernelTensor\nfrom .lazy_tensor import LazyTensor, delazify\nfrom .matmul_lazy_tensor import MatmulLazyTensor\nfrom .mul_lazy_tensor import MulLazyTensor\nfrom .non_lazy_tensor import NonLazyTensor, lazify\nfrom .psd_sum_lazy_tensor import PsdSumLazyTensor\nfrom .root_lazy_tensor import RootLazyTensor\nfrom .sum_batch_lazy_tensor import SumBatchLazyTensor\nfrom .sum_lazy_tensor import SumLazyTensor\nfrom .toeplitz_lazy_tensor import ToeplitzLazyTensor\nfrom .zero_lazy_tensor import ZeroLazyTensor\n\n__all__ = [\n    ""delazify"",\n    ""lazify"",\n    ""cat"",\n    ""LazyTensor"",\n    ""LazyEvaluatedKernelTensor"",\n    ""AddedDiagLazyTensor"",\n    ""BatchRepeatLazyTensor"",\n    ""BlockLazyTensor"",\n    ""BlockDiagLazyTensor"",\n    ""BlockInterleavedLazyTensor"",\n    ""CachedCGLazyTensor"",\n    ""CatLazyTensor"",\n    ""CholLazyTensor"",\n    ""ConstantMulLazyTensor"",\n    ""DiagLazyTensor"",\n    ""InterpolatedLazyTensor"",\n    ""KeOpsLazyTensor"",\n    ""KroneckerProductLazyTensor"",\n    ""MatmulLazyTensor"",\n    ""MulLazyTensor"",\n    ""NonLazyTensor"",\n    ""PsdSumLazyTensor"",\n    ""RootLazyTensor"",\n    ""SumLazyTensor"",\n    ""SumBatchLazyTensor"",\n    ""ToeplitzLazyTensor"",\n    ""ZeroLazyTensor"",\n]\n'"
gpytorch/lazy/added_diag_lazy_tensor.py,6,"b'#!/usr/bin/env python3\n\nimport warnings\n\nimport torch\n\nfrom .. import settings\nfrom ..utils import broadcasting, pivoted_cholesky\nfrom ..utils.warnings import NumericalWarning\nfrom .diag_lazy_tensor import DiagLazyTensor\nfrom .psd_sum_lazy_tensor import PsdSumLazyTensor\nfrom .root_lazy_tensor import RootLazyTensor\nfrom .sum_lazy_tensor import SumLazyTensor\n\n\nclass AddedDiagLazyTensor(SumLazyTensor):\n    """"""\n    A SumLazyTensor, but of only two lazy tensors, the second of which must be\n    a DiagLazyTensor.\n    """"""\n\n    def __init__(self, *lazy_tensors, preconditioner_override=None):\n        lazy_tensors = list(lazy_tensors)\n        super(AddedDiagLazyTensor, self).__init__(*lazy_tensors, preconditioner_override=preconditioner_override)\n        if len(lazy_tensors) > 2:\n            raise RuntimeError(""An AddedDiagLazyTensor can only have two components"")\n\n        broadcasting._mul_broadcast_shape(lazy_tensors[0].shape, lazy_tensors[1].shape)\n\n        if isinstance(lazy_tensors[0], DiagLazyTensor) and isinstance(lazy_tensors[1], DiagLazyTensor):\n            raise RuntimeError(""Trying to lazily add two DiagLazyTensors. Create a single DiagLazyTensor instead."")\n        elif isinstance(lazy_tensors[0], DiagLazyTensor):\n            self._diag_tensor = lazy_tensors[0]\n            self._lazy_tensor = lazy_tensors[1]\n        elif isinstance(lazy_tensors[1], DiagLazyTensor):\n            self._diag_tensor = lazy_tensors[1]\n            self._lazy_tensor = lazy_tensors[0]\n        else:\n            raise RuntimeError(""One of the LazyTensors input to AddedDiagLazyTensor must be a DiagLazyTensor!"")\n\n        self.preconditioner_override = preconditioner_override\n\n        # Placeholders\n        self._constant_diag = None\n        self._noise = None\n        self._piv_chol_self = None  # <- Doesn\'t need to be an attribute, but used for testing purposes\n        self._precond_lt = None\n        self._precond_logdet_cache = None\n        self._q_cache = None\n        self._r_cache = None\n\n    def _matmul(self, rhs):\n        return torch.addcmul(self._lazy_tensor._matmul(rhs), self._diag_tensor._diag.unsqueeze(-1), rhs)\n\n    def add_diag(self, added_diag):\n        return AddedDiagLazyTensor(self._lazy_tensor, self._diag_tensor.add_diag(added_diag))\n\n    def __add__(self, other):\n        from .diag_lazy_tensor import DiagLazyTensor\n\n        if isinstance(other, DiagLazyTensor):\n            return AddedDiagLazyTensor(self._lazy_tensor, self._diag_tensor + other)\n        else:\n            return AddedDiagLazyTensor(self._lazy_tensor + other, self._diag_tensor)\n\n    def _preconditioner(self):\n        if self.preconditioner_override is not None:\n            return self.preconditioner_override(self)\n\n        if settings.max_preconditioner_size.value() == 0 or self.size(-1) < settings.min_preconditioning_size.value():\n            return None, None, None\n\n        if self._q_cache is None:\n            max_iter = settings.max_preconditioner_size.value()\n            self._piv_chol_self = pivoted_cholesky.pivoted_cholesky(self._lazy_tensor, max_iter)\n            if torch.any(torch.isnan(self._piv_chol_self)).item():\n                warnings.warn(\n                    ""NaNs encountered in preconditioner computation. Attempting to continue without preconditioning."",\n                    NumericalWarning,\n                )\n                return None, None, None\n            self._init_cache()\n\n        # NOTE: We cannot memoize this precondition closure as it causes a memory leak\n        def precondition_closure(tensor):\n            qqt = self._q_cache.matmul(self._q_cache.transpose(-2, -1).matmul(tensor))\n            if self._constant_diag:\n                return (1 / self._noise) * (tensor - qqt)\n            return (tensor / self._noise) - qqt\n\n        return (precondition_closure, self._precond_lt, self._precond_logdet_cache)\n\n    def _init_cache(self):\n        *batch_shape, n, k = self._piv_chol_self.shape\n        self._noise = self._diag_tensor.diag().unsqueeze(-1)\n        self._constant_diag = torch.equal(self._noise, self._noise[0] * torch.ones_like(self._noise))\n        eye = torch.eye(k, dtype=self._piv_chol_self.dtype, device=self._piv_chol_self.device)\n\n        if self._constant_diag:\n            self._init_cache_for_constant_diag(eye, batch_shape, n, k)\n        else:\n            self._init_cache_for_non_constant_diag(eye, batch_shape, n)\n\n        self._precond_lt = PsdSumLazyTensor(RootLazyTensor(self._piv_chol_self), self._diag_tensor)\n\n    def _init_cache_for_constant_diag(self, eye, batch_shape, n, k):\n        # We can factor out the noise for for both QR and solves.\n        self._noise = self._noise.narrow(-2, 0, 1)\n        self._q_cache, self._r_cache = torch.qr(torch.cat((self._piv_chol_self, self._noise.sqrt() * eye), dim=-2))\n        self._q_cache = self._q_cache[..., :n, :]\n\n        # Use the matrix determinant lemma for the logdet, using the fact that R\'R = L_k\'L_k + s*I\n        logdet = self._r_cache.diagonal(dim1=-1, dim2=-2).abs().log().sum(-1).mul(2)\n        logdet = logdet + (n - k) * self._noise.squeeze(-2).squeeze(-1).log()\n        self._precond_logdet_cache = logdet.view(*batch_shape) if len(batch_shape) else logdet.squeeze()\n\n    def _init_cache_for_non_constant_diag(self, eye, batch_shape, n):\n        # With non-constant diagonals, we cant factor out the noise as easily\n        self._q_cache, self._r_cache = torch.qr(torch.cat((self._piv_chol_self / self._noise.sqrt(), eye)))\n        self._q_cache = self._q_cache[..., :n, :] / self._noise.sqrt()\n\n        logdet = self._r_cache.diagonal(dim1=-1, dim2=-2).abs().log().sum(-1).mul(2)\n        logdet -= (1.0 / self._noise).log().sum([-1, -2])\n        self._precond_logdet_cache = logdet.view(*batch_shape) if len(batch_shape) else logdet.squeeze()\n'"
gpytorch/lazy/batch_repeat_lazy_tensor.py,14,"b'#!/usr/bin/env python3\n\nimport itertools\n\nimport torch\n\nfrom .. import settings\nfrom ..utils.broadcasting import _matmul_broadcast_shape\nfrom ..utils.memoize import cached\nfrom .lazy_tensor import LazyTensor\n\n\nclass BatchRepeatLazyTensor(LazyTensor):\n    def __init__(self, base_lazy_tensor, batch_repeat=torch.Size((1,))):\n        if settings.debug.on():\n            if not isinstance(batch_repeat, torch.Size):\n                raise RuntimeError(\n                    ""batch_repeat must be a torch.Size, got a {} instead"".format(batch_repeat.__class__.__name__)\n                )\n            if isinstance(base_lazy_tensor, BatchRepeatLazyTensor):\n                raise RuntimeError(\n                    ""BatchRepeatLazyTensor recieved the following args:\\n""\n                    ""base_lazy_tensor: {} (size: {}), batch_repeat: {}."".format(\n                        base_lazy_tensor, base_lazy_tensor.shape, batch_repeat\n                    )\n                )\n\n        # Are we adding batch dimensions to the lazy tensor?\n        # If so, we\'ll unsqueeze the base_lazy_tensor so it has the same number of dimensions\n        for _ in range(len(batch_repeat) + 2 - base_lazy_tensor.dim()):\n            base_lazy_tensor = base_lazy_tensor.unsqueeze(0)\n\n        super().__init__(base_lazy_tensor, batch_repeat=batch_repeat)\n        self.base_lazy_tensor = base_lazy_tensor\n        self.batch_repeat = batch_repeat\n\n    @cached(name=""cholesky"")\n    def _cholesky(self):\n        res = self.base_lazy_tensor._cholesky()\n        res = res.repeat(*self.batch_repeat, 1, 1)\n        return res\n\n    def _cholesky_solve(self, rhs):\n        output_shape = _matmul_broadcast_shape(self.shape, rhs.shape)\n        if rhs.shape != output_shape:\n            rhs = rhs.expand(*output_shape)\n\n        rhs = self._move_repeat_batches_to_columns(rhs, output_shape)\n        res = self.base_lazy_tensor._cholesky_solve(rhs)\n        res = self._move_repeat_batches_back(res, output_shape)\n        return res\n\n    def _compute_batch_repeat_size(self, current_batch_shape, desired_batch_shape):\n        batch_repeat = torch.Size(\n            desired_batch_size // current_batch_size\n            for desired_batch_size, current_batch_size in zip(desired_batch_shape, current_batch_shape)\n        )\n        return batch_repeat\n\n    def _expand_batch(self, batch_shape):\n        padding_dims = torch.Size(tuple(1 for _ in range(max(len(batch_shape) + 2 - self.base_lazy_tensor.dim(), 0))))\n        current_batch_shape = padding_dims + self.base_lazy_tensor.batch_shape\n        return self.__class__(\n            self.base_lazy_tensor, batch_repeat=self._compute_batch_repeat_size(current_batch_shape, batch_shape)\n        )\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        # First remove any new batch indices that were added - they aren\'t necessary\n        num_true_batch_indices = self.base_lazy_tensor.dim() - 2\n        batch_indices = batch_indices[len(batch_indices) - num_true_batch_indices :]\n\n        # Now adjust the indices batch_indices that were repeated\n        batch_indices = [\n            batch_index.fmod(size) for batch_index, size in zip(batch_indices, self.base_lazy_tensor.batch_shape)\n        ]\n\n        # Now call the sub _get_indices method\n        res = self.base_lazy_tensor._get_indices(row_index, col_index, *batch_indices)\n        return res\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        args = []\n        kwargs = self.base_lazy_tensor._kwargs\n        num_base_batch_dims = len(self.base_lazy_tensor.batch_shape)\n\n        for arg in self.base_lazy_tensor._args:\n            if torch.is_tensor(arg) or isinstance(arg, LazyTensor):\n                arg_base_shape_len = max(arg.dim() - num_base_batch_dims, 0)\n                args.append(arg.repeat(*self.batch_repeat, *[1 for _ in range(arg_base_shape_len)]))\n            else:\n                args.append(arg)\n\n        new_lazy_tensor = self.base_lazy_tensor.__class__(*args, **kwargs)\n        return new_lazy_tensor._getitem(row_index, col_index, *batch_indices)\n\n    def _matmul(self, rhs):\n        output_shape = _matmul_broadcast_shape(self.shape, rhs.shape)\n\n        # only attempt broadcasting if the non-batch dimensions are the same\n        if self.is_square:\n            if rhs.shape != output_shape:\n                rhs = rhs.expand(*output_shape)\n\n            rhs = self._move_repeat_batches_to_columns(rhs, output_shape)\n            res = self.base_lazy_tensor._matmul(rhs)\n            res = self._move_repeat_batches_back(res, output_shape)\n            return res\n        else:\n            # otherwise, we will rely on base tensor broadcasting\n            res = self.base_lazy_tensor._matmul(rhs)\n            if res.shape != output_shape:\n                res = res.expand(*output_shape)\n\n            return res\n\n    def _move_repeat_batches_back(self, batch_matrix, output_shape):\n        """"""\n        The opposite of _move_repeat_batches_to_columns\n\n        Takes a b x m x nr tensor, and moves the batches associated with repeating\n        So that the tensor is now rb x m x n.\n        """"""\n        if hasattr(self, ""_batch_move_memo""):\n            padded_base_batch_shape, batch_repeat = self.__batch_move_memo\n            del self.__batch_move_memo\n        else:\n            padding_dims = torch.Size(tuple(1 for _ in range(max(len(output_shape) - self.base_lazy_tensor.dim(), 0))))\n            padded_base_batch_shape = padding_dims + self.base_lazy_tensor.batch_shape\n            batch_repeat = self._compute_batch_repeat_size(padded_base_batch_shape, output_shape[:-2])\n\n        # Now we have to move the columns back to their original repeat dimensions\n        batch_matrix = batch_matrix.view(*padded_base_batch_shape, output_shape[-2], -1, *batch_repeat)\n        output_dims = len(output_shape)\n        dims = tuple(\n            itertools.chain.from_iterable([i + output_dims, i] for i in range(len(padded_base_batch_shape)))\n        ) + (output_dims - 2, output_dims - 1)\n        batch_matrix = batch_matrix.permute(*dims).contiguous()\n\n        # Combine the repeat and the batch dimensions, and return the batch_matrix\n        batch_matrix = batch_matrix.view(*output_shape)\n        return batch_matrix\n\n    def _move_repeat_batches_to_columns(self, batch_matrix, output_shape):\n        """"""\n        Takes a rb x m x n tensor, and moves the batches associated with repeating\n        So that the tensor is now b x m x nr.\n        This allows us to use the base_lazy_tensor routines.\n        """"""\n        padding_dims = torch.Size(tuple(1 for _ in range(max(len(output_shape) - self.base_lazy_tensor.dim(), 0))))\n        padded_base_batch_shape = padding_dims + self.base_lazy_tensor.batch_shape\n        batch_repeat = self._compute_batch_repeat_size(padded_base_batch_shape, output_shape[:-2])\n\n        # Reshape batch_matrix so that each batch dimension is split in two:\n        # The repeated part, and the actual part\n        split_shape = torch.Size(\n            tuple(\n                itertools.chain.from_iterable(\n                    [repeat, size] for repeat, size in zip(batch_repeat, padded_base_batch_shape)\n                )\n            )\n            + output_shape[-2:]\n        )\n        batch_matrix = batch_matrix.view(*split_shape)\n\n        # Now chuck the repeat parts of the batch dimensions into the last dimension of batch_matrix\n        # These will act like extra columns of the batch matrix that we are multiplying against\n        # The repeated part, and the actual part\n        repeat_dims = range(0, len(batch_repeat) * 2, 2)\n        batch_dims = range(1, len(batch_repeat) * 2, 2)\n        batch_matrix = batch_matrix.permute(*batch_dims, -2, -1, *repeat_dims).contiguous()\n        batch_matrix = batch_matrix.view(*self.base_lazy_tensor.batch_shape, output_shape[-2], -1)\n\n        self.__batch_move_memo = output_shape, padded_base_batch_shape, batch_repeat\n        return batch_matrix\n\n    def _permute_batch(self, *dims):\n        new_batch_repeat = torch.Size(tuple(self.batch_repeat[dim] for dim in dims))\n        res = self.__class__(self.base_lazy_tensor._permute_batch(*dims), batch_repeat=new_batch_repeat)\n        return res\n\n    def _quad_form_derivative(self, left_vectors, right_vectors):\n        if self.is_square:\n            left_output_shape = _matmul_broadcast_shape(self.shape, left_vectors.shape)\n            if left_output_shape != left_vectors.shape:\n                left_vectors = left_vectors.expand(left_output_shape)\n\n            right_output_shape = _matmul_broadcast_shape(self.shape, right_vectors.shape)\n            if right_output_shape != right_vectors.shape:\n                right_vectors = right_vectors.expand(right_output_shape)\n\n            left_vectors = self._move_repeat_batches_to_columns(left_vectors, left_output_shape)\n            right_vectors = self._move_repeat_batches_to_columns(right_vectors, right_output_shape)\n\n            return self.base_lazy_tensor._quad_form_derivative(left_vectors, right_vectors)\n        else:\n            return super()._quad_form_derivative(left_vectors, right_vectors)\n\n    def _root_decomposition(self):\n        return self.base_lazy_tensor._root_decomposition().repeat(*self.batch_repeat, 1, 1)\n\n    def _root_inv_decomposition(self, initial_vectors=None):\n        return self.base_lazy_tensor._root_inv_decomposition().repeat(*self.batch_repeat, 1, 1)\n\n    def _size(self):\n        repeated_batch_shape = torch.Size(\n            size * repeat for size, repeat in zip(self.base_lazy_tensor.batch_shape, self.batch_repeat)\n        )\n        res = torch.Size(repeated_batch_shape + self.base_lazy_tensor.matrix_shape)\n        return res\n\n    def _transpose_nonbatch(self):\n        return self.__class__(self.base_lazy_tensor._transpose_nonbatch(), batch_repeat=self.batch_repeat)\n\n    def _unsqueeze_batch(self, dim):\n        base_lazy_tensor = self.base_lazy_tensor\n        batch_repeat = list(self.batch_repeat)\n        batch_repeat.insert(dim, 1)\n        batch_repeat = torch.Size(batch_repeat)\n        # If the dim only adds a new padded dimension, then we\'re done\n        # Otherwise we have to also unsqueeze the base_lazy_tensor\n        base_unsqueeze_dim = dim - (len(self.base_lazy_tensor.batch_shape) - len(self.base_lazy_tensor.batch_shape))\n        if base_unsqueeze_dim > 0:\n            base_lazy_tensor = base_lazy_tensor._unsqueeze_batch(base_unsqueeze_dim)\n        return self.__class__(base_lazy_tensor, batch_repeat=batch_repeat)\n\n    def add_jitter(self, jitter_val=1e-3):\n        return self.__class__(self.base_lazy_tensor.add_jitter(jitter_val=jitter_val), batch_repeat=self.batch_repeat)\n\n    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):\n        if not self.is_square:\n            raise RuntimeError(\n                ""inv_quad_logdet only operates on (batches of) square (positive semi-definite) LazyTensors. ""\n                ""Got a {} of size {}."".format(self.__class__.__name__, self.size())\n            )\n\n        if inv_quad_rhs is not None:\n            if self.dim() != inv_quad_rhs.dim():\n                raise RuntimeError(\n                    ""LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number ""\n                    ""of dimensions."".format(self.shape, inv_quad_rhs.shape)\n                )\n            elif self.batch_shape != inv_quad_rhs.shape[:-2] or self.shape[-1] != inv_quad_rhs.shape[-2]:\n                raise RuntimeError(\n                    ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(\n                        self.shape, inv_quad_rhs.shape\n                    )\n                )\n\n        if inv_quad_rhs is not None:\n            output_shape = _matmul_broadcast_shape(self.shape, inv_quad_rhs.shape)\n            inv_quad_rhs = self._move_repeat_batches_to_columns(inv_quad_rhs, output_shape)\n\n        inv_quad_term, logdet_term = self.base_lazy_tensor.inv_quad_logdet(inv_quad_rhs, logdet, reduce_inv_quad=False)\n\n        if inv_quad_term is not None and inv_quad_term.numel():\n            inv_quad_term = inv_quad_term.view(*inv_quad_term.shape[:-1], -1, 1, self.batch_repeat.numel())\n            output_shape = list(output_shape)\n            output_shape[-2] = 1\n            inv_quad_term = self._move_repeat_batches_back(inv_quad_term, output_shape).squeeze(-2)\n            if reduce_inv_quad:\n                inv_quad_term = inv_quad_term.sum(-1)\n\n        if logdet_term is not None and logdet_term.numel():\n            logdet_term = logdet_term.repeat(*self.batch_repeat)\n\n        return inv_quad_term, logdet_term\n\n    def repeat(self, *sizes):\n        if len(sizes) < 3 or tuple(sizes[-2:]) != (1, 1):\n            raise RuntimeError(\n                ""Invalid repeat arguments {}. Currently, repeat only works to create repeated ""\n                ""batches of a 2D LazyTensor."".format(tuple(sizes))\n            )\n\n        padded_batch_repeat = tuple(1 for _ in range(len(sizes) - 2 - len(self.batch_repeat))) + self.batch_repeat\n        return self.__class__(\n            self.base_lazy_tensor,\n            batch_repeat=torch.Size(\n                orig_repeat_size * new_repeat_size\n                for orig_repeat_size, new_repeat_size in zip(padded_batch_repeat, sizes[:-2])\n            ),\n        )\n'"
gpytorch/lazy/block_diag_lazy_tensor.py,2,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils.memoize import cached\nfrom .block_lazy_tensor import BlockLazyTensor\n\n\nclass BlockDiagLazyTensor(BlockLazyTensor):\n    """"""\n    Represents a lazy tensor that is the block diagonal of square matrices.\n    The :attr:`block_dim` attribute specifies which dimension of the base LazyTensor\n    specifies the blocks.\n    For example, (with `block_dim=-3` a `k x n x n` tensor represents `k` `n x n` blocks (a `kn x kn` matrix).\n    A `b x k x n x n` tensor represents `k` `b x n x n` blocks (a `b x kn x kn` batch matrix).\n\n    Args:\n        :attr:`base_lazy_tensor` (LazyTensor or Tensor):\n            Must be at least 3 dimensional.\n        :attr:`block_dim` (int):\n            The dimension that specifies the blocks.\n    """"""\n\n    @property\n    def num_blocks(self):\n        return self.base_lazy_tensor.size(-3)\n\n    def _add_batch_dim(self, other):\n        *batch_shape, num_rows, num_cols = other.shape\n        batch_shape = list(batch_shape)\n\n        batch_shape.append(self.num_blocks)\n        other = other.view(*batch_shape, num_rows // self.num_blocks, num_cols)\n        return other\n\n    @cached(name=""cholesky"")\n    def _cholesky(self):\n        return self.__class__(self.base_lazy_tensor._cholesky())\n\n    def _cholesky_solve(self, rhs):\n        rhs = self._add_batch_dim(rhs)\n        res = self.base_lazy_tensor._cholesky_solve(rhs)\n        res = self._remove_batch_dim(res)\n        return res\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        # Figure out what block the row/column indices belong to\n        row_index_block = row_index // self.base_lazy_tensor.size(-2)\n        col_index_block = col_index // self.base_lazy_tensor.size(-1)\n\n        # Find the row/col index within each block\n        row_index = row_index.fmod(self.base_lazy_tensor.size(-2))\n        col_index = col_index.fmod(self.base_lazy_tensor.size(-1))\n\n        # If the row/column blocks do not agree, then we have off diagonal elements\n        # These elements should be zeroed out\n        res = self.base_lazy_tensor._get_indices(row_index, col_index, *batch_indices, row_index_block)\n        res = res * torch.eq(row_index_block, col_index_block).type_as(res)\n        return res\n\n    def _remove_batch_dim(self, other):\n        shape = list(other.shape)\n        del shape[-3]\n        shape[-2] *= self.num_blocks\n        other = other.reshape(*shape)\n        return other\n\n    def _root_decomposition(self):\n        return self.__class__(self.base_lazy_tensor._root_decomposition())\n\n    def _root_inv_decomposition(self, initial_vectors=None):\n        return self.__class__(self.base_lazy_tensor._root_inv_decomposition(initial_vectors))\n\n    def _size(self):\n        shape = list(self.base_lazy_tensor.shape)\n        shape[-2] *= shape[-3]\n        shape[-1] *= shape[-3]\n        del shape[-3]\n        return torch.Size(shape)\n\n    def _solve(self, rhs, preconditioner, num_tridiag=0):\n        if num_tridiag:\n            return super()._solve(rhs, preconditioner, num_tridiag=num_tridiag)\n        else:\n            rhs = self._add_batch_dim(rhs)\n            res = self.base_lazy_tensor._solve(rhs, preconditioner, num_tridiag=None)\n            res = self._remove_batch_dim(res)\n            return res\n\n    def diag(self):\n        res = self.base_lazy_tensor.diag().contiguous()\n        return res.view(*self.batch_shape, self.size(-1))\n\n    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):\n        if inv_quad_rhs is not None:\n            inv_quad_rhs = self._add_batch_dim(inv_quad_rhs)\n        inv_quad_res, logdet_res = self.base_lazy_tensor.inv_quad_logdet(\n            inv_quad_rhs, logdet, reduce_inv_quad=reduce_inv_quad\n        )\n        if inv_quad_res is not None and inv_quad_res.numel():\n            if reduce_inv_quad:\n                inv_quad_res = inv_quad_res.view(*self.base_lazy_tensor.batch_shape)\n                inv_quad_res = inv_quad_res.sum(-1)\n            else:\n                inv_quad_res = inv_quad_res.view(*self.base_lazy_tensor.batch_shape, inv_quad_res.size(-1))\n                inv_quad_res = inv_quad_res.sum(-2)\n        if logdet_res is not None and logdet_res.numel():\n            logdet_res = logdet_res.view(*logdet_res.shape).sum(-1)\n        return inv_quad_res, logdet_res\n'"
gpytorch/lazy/block_interleaved_lazy_tensor.py,2,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils.memoize import cached\nfrom .block_lazy_tensor import BlockLazyTensor\n\n\nclass BlockInterleavedLazyTensor(BlockLazyTensor):\n    """"""\n    Represents a lazy tensor that is the block diagonal of square matrices.\n    The :attr:`block_dim` attribute specifies which dimension of the base LazyTensor\n    specifies the blocks.\n    For example, (with `block_dim=-3` a `k x n x n` tensor represents `k` `n x n` blocks (a `kn x kn` matrix).\n    A `b x k x n x n` tensor represents `k` `b x n x n` blocks (a `b x kn x kn` batch matrix).\n\n    Args:\n        :attr:`base_lazy_tensor` (LazyTensor or Tensor):\n            Must be at least 3 dimensional.\n        :attr:`block_dim` (int):\n            The dimension that specifies the blocks.\n    """"""\n\n    @property\n    def num_blocks(self):\n        return self.base_lazy_tensor.size(-3)\n\n    def _add_batch_dim(self, other):\n        *batch_shape, num_rows, num_cols = other.shape\n        batch_shape = list(batch_shape)\n\n        batch_shape.append(num_rows // self.num_blocks)\n        other = other.view(*batch_shape, self.num_blocks, num_cols)\n        other = other.transpose(-2, -3).contiguous()\n        return other\n\n    @cached(name=""cholesky"")\n    def _cholesky(self):\n        return self.__class__(self.base_lazy_tensor._cholesky())\n\n    def _cholesky_solve(self, rhs):\n        rhs = self._add_batch_dim(rhs)\n        res = self.base_lazy_tensor._cholesky_solve(rhs)\n        res = self._remove_batch_dim(res)\n        return res\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        # Figure out what block the row/column indices belong to\n        row_index_block = row_index.fmod(self.base_lazy_tensor.size(-3))\n        col_index_block = col_index.fmod(self.base_lazy_tensor.size(-3))\n\n        # Find the row/col index within each block\n        row_index = row_index // self.base_lazy_tensor.size(-3)\n        col_index = col_index // self.base_lazy_tensor.size(-3)\n\n        # If the row/column blocks do not agree, then we have off diagonal elements\n        # These elements should be zeroed out\n        res = self.base_lazy_tensor._get_indices(row_index, col_index, *batch_indices, row_index_block)\n        res = res * torch.eq(row_index_block, col_index_block).type_as(res)\n        return res\n\n    def _remove_batch_dim(self, other):\n        other = other.transpose(-2, -3).contiguous()\n        shape = list(other.shape)\n        del shape[-2]\n        shape[-2] *= self.num_blocks\n        other = other.reshape(*shape)\n        return other\n\n    def _root_decomposition(self):\n        return self.__class__(self.base_lazy_tensor._root_decomposition())\n\n    def _root_inv_decomposition(self, initial_vectors=None):\n        return self.__class__(self.base_lazy_tensor._root_inv_decomposition(initial_vectors))\n\n    def _size(self):\n        shape = list(self.base_lazy_tensor.shape)\n        shape[-2] *= shape[-3]\n        shape[-1] *= shape[-3]\n        del shape[-3]\n        return torch.Size(shape)\n\n    def _solve(self, rhs, preconditioner, num_tridiag=0):\n        if num_tridiag:\n            return super()._solve(rhs, preconditioner, num_tridiag=num_tridiag)\n        else:\n            rhs = self._add_batch_dim(rhs)\n            res = self.base_lazy_tensor._solve(rhs, preconditioner, num_tridiag=None)\n            res = self._remove_batch_dim(res)\n            return res\n\n    def diag(self):\n        block_diag = self.base_lazy_tensor.diag()\n        return block_diag.transpose(-1, -2).contiguous().view(*block_diag.shape[:-2], -1)\n\n    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):\n        if inv_quad_rhs is not None:\n            inv_quad_rhs = self._add_batch_dim(inv_quad_rhs)\n        inv_quad_res, logdet_res = self.base_lazy_tensor.inv_quad_logdet(\n            inv_quad_rhs, logdet, reduce_inv_quad=reduce_inv_quad\n        )\n        if inv_quad_res is not None and inv_quad_res.numel():\n            if reduce_inv_quad:\n                inv_quad_res = inv_quad_res.view(*self.base_lazy_tensor.batch_shape)\n                inv_quad_res = inv_quad_res.sum(-1)\n            else:\n                inv_quad_res = inv_quad_res.view(*self.base_lazy_tensor.batch_shape, inv_quad_res.size(-1))\n                inv_quad_res = inv_quad_res.sum(-2)\n        if logdet_res is not None and logdet_res.numel():\n            logdet_res = logdet_res.view(*logdet_res.shape).sum(-1)\n        return inv_quad_res, logdet_res\n'"
gpytorch/lazy/block_lazy_tensor.py,3,"b'#!/usr/bin/env python3\n\nfrom abc import abstractmethod\n\nimport torch\n\nfrom ..utils.getitem import _is_noop_index, _noop_index\nfrom .lazy_tensor import LazyTensor\nfrom .non_lazy_tensor import lazify\n\n\nclass BlockLazyTensor(LazyTensor):\n    """"""\n    An abstract LazyTensor class for block tensors.\n    Super classes will determine how the different blocks are layed out\n    (e.g. block diagonal, sum over blocks, etc.)\n\n    BlockLazyTensors represent the groups of blocks as a batched Tensor.\n    The :attr:block_dim` attribute specifies which dimension of the base LazyTensor\n    specifies the blocks.\n    For example, (with `block_dim=-3` a `k x n x n` tensor represents `k` `n x n` blocks.\n    A `b x k x n x n` tensor represents `k` `b x n x n` blocks.\n\n    Args:\n        - :attr:`base_lazy_tensor` (LazyTensor or Tensor):\n            Must be at least 3 dimenional.\n        - :attr:`block_dim` (int):\n            The dimension that specifies blocks.\n    """"""\n\n    def __init__(self, base_lazy_tensor, block_dim=-3):\n        if base_lazy_tensor.dim() < 3:\n            raise RuntimeError(\n                ""base_lazy_tensor must be a batch matrix (i.e. at least 3 dimensions - got ""\n                ""{}"".format(base_lazy_tensor.dim())\n            )\n\n        # Make sure block_dim is negative\n        block_dim = block_dim if block_dim < 0 else (block_dim - base_lazy_tensor.dim())\n\n        # Everything is MUCH easier to write if the last batch dimension is the block dimension\n        # I.e. blopck_dim = -3\n        # We\'ll permute the dimensions if this is not the case\n        if block_dim != -3:\n            positive_block_dim = base_lazy_tensor.dim() + block_dim\n            base_lazy_tensor = base_lazy_tensor._permute_batch(\n                *range(positive_block_dim),\n                *range(positive_block_dim + 1, base_lazy_tensor.dim() - 2),\n                positive_block_dim,\n            )\n\n        super(BlockLazyTensor, self).__init__(lazify(base_lazy_tensor))\n        self.base_lazy_tensor = base_lazy_tensor\n\n    @abstractmethod\n    def _add_batch_dim(self, other):\n        raise NotImplementedError\n\n    def _expand_batch(self, batch_shape):\n        batch_shape = torch.Size((*batch_shape, self.base_lazy_tensor.size(-3)))\n        res = self.__class__(self.base_lazy_tensor._expand_batch(batch_shape))\n        return res\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        # First the easy case: just batch indexing\n        if _is_noop_index(row_index) and _is_noop_index(col_index):\n            return self.__class__(self.base_lazy_tensor._getitem(row_index, col_index, *batch_indices, _noop_index))\n\n        # If either of the dimensions are indices, it\'s too complicated - go with the base case\n        if not isinstance(row_index, slice) or not isinstance(col_index, slice):\n            # It\'s too complicated to deal with tensor indices in this case - we\'ll use the super method\n            return super()._getitem(row_index, col_index, *batch_indices)\n\n        # Now we know that row_index and col_index\n        num_blocks = self.num_blocks\n        num_rows, num_cols = self.matrix_shape\n        row_start, row_end, row_step = row_index.start or 0, row_index.stop or num_rows, row_index.step\n        col_start, col_end, col_step = col_index.start or 0, col_index.stop or num_cols, col_index.step\n\n        # If we have a step, it\'s too complicated - go with the base case\n        if row_step is not None or col_step is not None:\n            return super()._getitem(row_index, col_index, *batch_indices)\n\n        # Let\'s make sure that the slice dimensions perfectly correspond with the number of\n        # outputs per input that we have\n        # Otherwise - its too complicated. We\'ll go with the base case\n        if (row_start % num_blocks) or (col_start % num_blocks) or (row_end % num_blocks) or (col_end % num_blocks):\n            return super()._getitem(row_index, col_index, *batch_indices)\n\n        # Otherwise - let\'s divide the slices by the number of outputs per input\n        row_index = slice(row_start // num_blocks, row_end // num_blocks, None)\n        col_index = slice(col_start // num_blocks, col_end // num_blocks, None)\n\n        # Now we can try the super call!\n        new_base_lazy_tensor = self.base_lazy_tensor._getitem(row_index, col_index, *batch_indices)\n\n        # Now construct a kernel with those indices\n        return self.__class__(new_base_lazy_tensor, block_dim=-3)\n\n    def _matmul(self, rhs):\n        isvector = rhs.ndimension() == 1\n        if isvector:\n            rhs = rhs.unsqueeze(1)\n\n        rhs = self._add_batch_dim(rhs)\n        res = self.base_lazy_tensor._matmul(rhs)\n        res = self._remove_batch_dim(res)\n\n        if isvector:\n            res = res.squeeze(-1)\n        return res\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        if left_vecs.ndim == 1:\n            left_vecs = left_vecs.unsqueeze(-1)\n            right_vecs = right_vecs.unsqueeze(-1)\n        # deal with left_vecs having batch dimensions\n        elif left_vecs.size(-1) != right_vecs.size(-1):\n            left_vecs = left_vecs.unsqueeze(-1)\n        left_vecs = self._add_batch_dim(left_vecs)\n        right_vecs = self._add_batch_dim(right_vecs)\n        res = self.base_lazy_tensor._quad_form_derivative(left_vecs, right_vecs)\n        return res\n\n    def _permute_batch(self, *dims):\n        if torch.is_tensor(self.base_lazy_tensor):\n            base_lazy_tensor = self.base_lazy_tensor.permute(*dims, -3, -2, -1)\n        else:\n            base_lazy_tensor = self.base_lazy_tensor._permute_batch(*dims, self.base_lazy_tensor.dim() - 3)\n        res = self.__class__(base_lazy_tensor)\n        return res\n\n    def _unsqueeze_batch(self, dim):\n        if torch.is_tensor(self.base_lazy_tensor):\n            base_lazy_tensor = self.base_lazy_tensor.unsqueeze(dim)\n        else:\n            base_lazy_tensor = self.base_lazy_tensor._unsqueeze_batch(dim)\n        res = self.__class__(base_lazy_tensor)\n        return res\n\n    @abstractmethod\n    def _remove_batch_dim(self, other):\n        raise NotImplementedError\n\n    def _mul_constant(self, other):\n        # We\'re using a custom method here - the constant mul is applied to the base_lazy tensor\n        # This preserves the block structure\n        from .constant_mul_lazy_tensor import ConstantMulLazyTensor\n\n        return self.__class__(ConstantMulLazyTensor(self.base_lazy_tensor, other))\n\n    def _transpose_nonbatch(self):\n        return self.__class__(self.base_lazy_tensor._transpose_nonbatch())\n\n    def zero_mean_mvn_samples(self, num_samples):\n        res = self.base_lazy_tensor.zero_mean_mvn_samples(num_samples)\n        res = self._remove_batch_dim(res.unsqueeze(-1)).squeeze(-1)\n        return res\n'"
gpytorch/lazy/cached_cg_lazy_tensor.py,25,"b'#!/usr/bin/env python3\n\nimport warnings\n\nimport torch\n\nfrom .. import settings\nfrom ..utils.warnings import ExtraComputationWarning\nfrom .chol_lazy_tensor import CholLazyTensor\nfrom .lazy_tensor import LazyTensor\n\n\nclass CachedCGLazyTensor(LazyTensor):\n    """"""\n    A LazyTensor wrapper that eagerly computes many CG calls in batch.\n    This maximizes CG parallelism for fast inference.\n    Used primarily for variational inference with GPs.\n\n    Args:\n        :attr:`base_lazy_tensor` (:class:`gpytorch.lazy.LazyTensor`):\n            the LazyTensor to wrap\n        :attr:`eager_rhss` (list of :class:`gpytorch.lazy.LazyTensor`):\n            list of right-hand sides with eagerly-computed solves\n        :attr:`solves` (list of :class:`gpytorch.lazy.LazyTensor`):\n            list of solves associated with :attr:`eager_rhss`\n        :attr:`probe_vectors` (:class:`gpytorch.lazy.LazyTensor`, optional):\n            normalized probe vectors (for computing logdet with SLQ)\n        :attr:`probe_vector_norms` (:class:`gpytorch.lazy.LazyTensor`, optional):\n            norms associated with :attr:`probe_vectors` that will return :attr:`probe_vectors`\n            to having identity covariance (for computing logdet with SLQ)\n        :attr:`probe_vector_solves` (:class:`gpytorch.lazy.LazyTensor`, optional):\n            solves associated with :attr:`probe_vectors` (for computing logdet with SLQ)\n        :attr:`probe_vector_tmats` (:class:`gpytorch.lazy.LazyTensor`, optional):\n            Lanczos tridiagonal matrices associated with :attr:`probe_vectors`\n            (for computing logdet with SLQ)\n    """"""\n\n    @classmethod\n    def precompute_terms(cls, base_lazy_tensor, eager_rhs, logdet_terms=True, include_tmats=True):\n        """"""\n        Computes the solves, probe vectors, probe_vector norms, probe vector solves, and probe vector\n        tridiagonal matrices to construct a CachedCGLazyTensor\n\n        Set logdet_terms to False if you are not going to compute the logdet of the LazyTensor\n        """"""\n        with torch.no_grad():\n            if logdet_terms:\n                # Generate probe vectors\n                num_random_probes = settings.num_trace_samples.value()\n                probe_vectors = torch.empty(\n                    base_lazy_tensor.matrix_shape[-1],\n                    num_random_probes,\n                    dtype=base_lazy_tensor.dtype,\n                    device=base_lazy_tensor.device,\n                )\n                probe_vectors.bernoulli_().mul_(2).add_(-1)\n                probe_vectors = probe_vectors.expand(\n                    *base_lazy_tensor.batch_shape, base_lazy_tensor.matrix_shape[-1], num_random_probes\n                )\n                probe_vector_norms = torch.norm(probe_vectors, 2, dim=-2, keepdim=True)\n                probe_vectors = probe_vectors.div(probe_vector_norms)\n\n                # Compute solves\n                if include_tmats:\n                    all_solves, probe_vector_tmats, = base_lazy_tensor._solve(\n                        torch.cat([probe_vectors, eager_rhs], -1),\n                        preconditioner=base_lazy_tensor._preconditioner()[0],\n                        num_tridiag=probe_vectors.size(-1),\n                    )\n                else:\n                    all_solves = base_lazy_tensor._solve(\n                        torch.cat([probe_vectors, eager_rhs], -1), preconditioner=base_lazy_tensor._preconditioner()[0]\n                    )\n                    probe_vector_tmats = torch.tensor([])\n                probe_vector_solves = all_solves[..., : probe_vectors.size(-1)].detach()\n                solves = all_solves[..., probe_vectors.size(-1) :]\n\n                return (\n                    solves.detach(),\n                    probe_vectors.detach(),\n                    probe_vector_norms.detach(),\n                    probe_vector_solves.detach(),\n                    probe_vector_tmats.detach(),\n                )\n\n            else:\n                # Compute solves\n                if settings.fast_computations.log_prob.on():\n                    solves = base_lazy_tensor._solve(eager_rhs, preconditioner=base_lazy_tensor._preconditioner()[0])\n                else:\n                    solves = base_lazy_tensor._cholesky()._cholesky_solve(eager_rhs)\n                dtype = solves.dtype\n                device = solves.device\n                return (\n                    solves.detach(),\n                    torch.tensor([], dtype=dtype, device=device),\n                    torch.tensor([], dtype=dtype, device=device),\n                    torch.tensor([], dtype=dtype, device=device),\n                    torch.tensor([], dtype=dtype, device=device),\n                )\n\n    def __init__(\n        self,\n        base_lazy_tensor,\n        eager_rhss=[],\n        solves=[],\n        probe_vectors=torch.tensor([]),\n        probe_vector_norms=torch.tensor([]),\n        probe_vector_solves=torch.tensor([]),\n        probe_vector_tmats=torch.tensor([]),\n    ):\n        super(CachedCGLazyTensor, self).__init__(\n            base_lazy_tensor,\n            eager_rhss=eager_rhss,\n            solves=solves,\n            probe_vectors=probe_vectors,\n            probe_vector_norms=probe_vector_norms,\n            probe_vector_solves=probe_vector_solves,\n            probe_vector_tmats=probe_vector_tmats,\n        )\n        self.base_lazy_tensor = base_lazy_tensor\n        self.eager_rhss = [eager_rhs.detach() for eager_rhs in eager_rhss]\n        self.solves = [solve.detach() for solve in solves]\n        self.probe_vectors = probe_vectors.detach()\n        self.probe_vector_norms = probe_vector_norms.detach()\n        self.probe_vector_solves = probe_vector_solves.detach()\n        self.probe_vector_tmats = probe_vector_tmats.detach()\n\n    @property\n    def requires_grad(self):\n        return self.base_lazy_tensor.requires_grad\n\n    @requires_grad.setter\n    def requires_grad(self, val):\n        self.base_lazy_tensor.requires_grad = val\n\n    def _cholesky(self):\n        res = self.__class__(\n            self.base_lazy_tensor._cholesky(),\n            eager_rhss=self.eager_rhss,\n            solves=self.solves,\n            probe_vectors=self.probe_vectors,\n            probe_vector_norms=self.probe_vector_norms,\n            probe_vector_solves=self.probe_vector_solves,\n            probe_vector_tmats=self.probe_vector_tmats,\n        )\n        return res\n\n    def _cholesky_solve(self, rhs):\n        # Here we check to see what solves we\'ve already performed\n        for eager_rhs, solve in zip(self.eager_rhss, self.solves):\n            if torch.equal(rhs, eager_rhs):\n                return solve\n\n        if settings.debug.on():\n            warnings.warn(\n                ""CachedCGLazyTensor had to run CG on a tensor of size {}. For best performance, this ""\n                ""LazyTensor should pre-register all vectors to run CG against."".format(rhs.shape),\n                ExtraComputationWarning,\n            )\n        return super(CachedCGLazyTensor, self)._cholesky_solve(rhs)\n\n    def _expand_batch(self, batch_shape):\n        return self.base_lazy_tensor._expand_batch(batch_shape)\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        return self.base_lazy_tensor._get_indices(row_index, col_index, *batch_indices)\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        return self.base_lazy_tensor._getitem(row_index, col_index, *batch_indices)\n\n    def _matmul(self, tensor):\n        return self.base_lazy_tensor._matmul(tensor)\n\n    def _probe_vectors_and_norms(self):\n        return self.probe_vectors, self.probe_vector_norms\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        return self.base_lazy_tensor._quad_form_derivative(left_vecs, right_vecs)\n\n    def _solve(self, rhs, preconditioner, num_tridiag=0):\n        if num_tridiag:\n            probe_vectors = rhs[..., :num_tridiag].detach()\n            if torch.equal(probe_vectors, self.probe_vectors):\n                probe_vector_solves = self.probe_vector_solves\n                tmats = self.probe_vector_tmats\n            else:\n                if settings.debug.on():\n                    warnings.warn(\n                        ""CachedCGLazyTensor did not recognize the supplied probe vectors for tridiagonalization."",\n                        ExtraComputationWarning,\n                    )\n                return super(CachedCGLazyTensor, self)._solve(rhs, preconditioner, num_tridiag=num_tridiag)\n\n        # Here we check to see what solves we\'ve already performed\n        truncated_rhs = rhs[..., (num_tridiag or 0) :]\n        for eager_rhs, solve in zip(self.eager_rhss, self.solves):\n            if torch.equal(truncated_rhs, eager_rhs):\n                if num_tridiag:\n                    return torch.cat([probe_vector_solves, solve], -1), tmats\n                else:\n                    return solve\n\n        if settings.debug.on():\n            warnings.warn(\n                ""CachedCGLazyTensor had to run CG on a tensor of size {}. For best performance, this ""\n                ""LazyTensor should pre-register all vectors to run CG against."".format(rhs.shape),\n                ExtraComputationWarning,\n            )\n        return super(CachedCGLazyTensor, self)._solve(rhs, preconditioner, num_tridiag=num_tridiag)\n\n    def _size(self):\n        return self.base_lazy_tensor._size()\n\n    def _t_matmul(self, tensor):\n        return self.base_lazy_tensor._t_matmul(tensor)\n\n    def _transpose_nonbatch(self):\n        return self.base_lazy_tensor._transpose_nonbatch()\n\n    def detach_(self):\n        self.base_lazy_tensor.detach_()\n        return self\n\n    def inv_matmul(self, right_tensor, left_tensor=None):\n        if not isinstance(self.base_lazy_tensor, CholLazyTensor):\n            return super().inv_matmul(right_tensor, left_tensor=left_tensor)\n\n        with settings.fast_computations(solves=False):\n            return super().inv_matmul(right_tensor, left_tensor=left_tensor)\n\n    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):\n        if not isinstance(self.base_lazy_tensor, CholLazyTensor):\n            return super().inv_quad_logdet(inv_quad_rhs=inv_quad_rhs, logdet=logdet, reduce_inv_quad=reduce_inv_quad)\n\n        if not self.is_square:\n            raise RuntimeError(\n                ""inv_quad_logdet only operates on (batches of) square (positive semi-definite) LazyTensors. ""\n                ""Got a {} of size {}."".format(self.__class__.__name__, self.size())\n            )\n\n        if inv_quad_rhs is not None:\n            if self.dim() == 2 and inv_quad_rhs.dim() == 1:\n                if self.shape[-1] != inv_quad_rhs.numel():\n                    raise RuntimeError(\n                        ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(\n                            self.shape, inv_quad_rhs.shape\n                        )\n                    )\n            elif self.dim() != inv_quad_rhs.dim():\n                raise RuntimeError(\n                    ""LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number ""\n                    ""of dimensions."".format(self.shape, inv_quad_rhs.shape)\n                )\n            elif self.shape[-1] != inv_quad_rhs.shape[-2]:\n                raise RuntimeError(\n                    ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(\n                        self.shape, inv_quad_rhs.shape\n                    )\n                )\n\n        inv_quad_term = None\n        logdet_term = None\n\n        if inv_quad_rhs is not None:\n            inv_quad_term = self.inv_quad(inv_quad_rhs, reduce_inv_quad=reduce_inv_quad)\n\n        if logdet:\n            logdet_term = self.base_lazy_tensor._chol_diag.pow(2).log().sum(-1)\n\n        return inv_quad_term, logdet_term\n\n\n__all__ = [""ExtraComputationWarning"", ""CachedCGLazyTensor""]\n'"
gpytorch/lazy/cat_lazy_tensor.py,27,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom .. import settings\nfrom ..utils.broadcasting import _matmul_broadcast_shape, _mul_broadcast_shape\nfrom ..utils.deprecation import bool_compat\nfrom ..utils.getitem import _noop_index\nfrom .lazy_tensor import LazyTensor, delazify\nfrom .non_lazy_tensor import NonLazyTensor, lazify\n\n\ndef cat(inputs, dim=0, output_device=None):\n    if all(torch.is_tensor(i) for i in inputs):\n        return torch.cat(inputs, dim=dim)\n\n    inputs = [lazify(i) for i in inputs]\n\n    if all(isinstance(i, NonLazyTensor) for i in inputs):\n        # Dont form a CatLazyTensor if all tensors are NonLazyTensor\n        return lazify(torch.cat([delazify(i) for i in inputs], dim=dim))\n\n    if output_device is None and all(i.device == inputs[0].device for i in inputs):\n        output_device = inputs[0].device\n    elif output_device is None:\n        raise RuntimeError(""Trying to concat lazy tensors on different devices without specifying an output device."")\n\n    return CatLazyTensor(*inputs, dim=dim, output_device=output_device)\n\n\nclass CatLazyTensor(LazyTensor):\n    r""""""\n    A `LazyTensor` that represents the concatenation of other lazy tensors.\n    Each LazyTensor must have the same shape except in the concatenating\n    dimension.\n\n    Args:\n        - :attr:`lazy_tensors` (list of LazyTensors):\n            A list of LazyTensors whose sizes are the same except in\n            concatenating dimension :attr:`dim`\n        - :attr:`dim` (int):\n            The concatenating dimension which can be a batch dimension.\n        - :attr:`output_device` (torch.device):\n            The CatLazyTensor will appear to appear on :attr:`output_device`\n            and place any output `torch.Tensors` on :attr:`output_device`\n    """"""\n\n    def _check_args(self, *lazy_tensors, dim=0, output_device=None):\n        if len(lazy_tensors) == 0:\n            raise RuntimeError(""List of LazyTensors must be non-empty"")\n        elif len(lazy_tensors) == 1:\n            raise RuntimeError(""Why are we trying to concatenate a single LazyTensor?"")\n        if not all([isinstance(t, LazyTensor) for t in lazy_tensors]):\n            raise RuntimeError(""CatLazyTensor requires a list of all LazyTensors"")\n\n        rep_tensor = lazy_tensors[0]\n        rep_tensor_noncat_shape = list(rep_tensor.shape)\n        del rep_tensor_noncat_shape[dim]\n\n        for t in lazy_tensors:\n            if t.dim() != rep_tensor.dim():\n                raise RuntimeError(""All tensors must have the same number of dimensions"")\n\n            t_noncat_shape = list(t.shape)\n            del t_noncat_shape[dim]\n            if t_noncat_shape != rep_tensor_noncat_shape:\n                raise RuntimeError(""All LazyTensors must have the same size in "" ""the non-concatenation dimension"")\n\n    def __init__(self, *lazy_tensors, dim=0, output_device=None):\n        # Make sure index is negative index\n        rep_tensor = lazy_tensors[0]\n        ndims = rep_tensor.ndimension()\n        if dim >= 0:\n            positive_dim = dim\n            dim = dim - ndims\n        else:\n            positive_dim = ndims + dim\n\n        # Standard initialization\n        super().__init__(*lazy_tensors, dim=dim, output_device=output_device)\n        self.lazy_tensors = lazy_tensors\n        self.cat_dim = dim\n        self.output_device = output_device\n\n        # Helpers for _getitem\n        cat_dim_sizes = torch.tensor([t.size(dim) for t in lazy_tensors], device=output_device)\n        cat_dim_cum_sizes = torch.zeros(len(lazy_tensors) + 1, dtype=torch.long, device=output_device)\n        torch.cumsum(cat_dim_sizes, dim=-1, out=cat_dim_cum_sizes[1:])\n        idx_to_tensor_idx = torch.empty(cat_dim_cum_sizes[-1].item(), dtype=torch.long, device=output_device)\n        for tsr_idx, (start_idx, end_idx) in enumerate(zip(cat_dim_cum_sizes[:-1], cat_dim_cum_sizes[1:])):\n            idx_to_tensor_idx[start_idx.item() : end_idx.item()].fill_(tsr_idx)\n\n        self.cat_dim_sizes = cat_dim_sizes\n        self.cat_dim_cum_sizes = cat_dim_cum_sizes\n        self.idx_to_tensor_idx = idx_to_tensor_idx\n        self._shape = torch.Size(\n            (*rep_tensor.shape[:positive_dim], cat_dim_cum_sizes[-1].item(), *rep_tensor.shape[positive_dim + 1 :])\n        )\n\n    def _split_slice(self, slice_idx):\n        """"""\n        Splits a slice(a, b, None) in to a list of slices [slice(a1, b1, None), slice(a2, b2, None), ...]\n        so that each slice in the list slices in to a single tensor that we have concatenated with this LazyTensor.\n        """"""\n        if slice_idx.step is not None:\n            # TODO: Add support for this eventually.\n            raise RuntimeError(""Slicing a CatLazyTensor with a step is not currently supported!"")\n        start_idx = slice_idx.start if slice_idx.start is not None else 0\n        stop_idx = slice_idx.stop if slice_idx.stop is not None else self.size(self.cat_dim)\n\n        first_tensor_idx = self.idx_to_tensor_idx[start_idx].item()\n        last_tensor_idx = self.idx_to_tensor_idx[stop_idx - 1].item()\n\n        first_tensor_start_index = start_idx - self.cat_dim_cum_sizes[first_tensor_idx].item()\n        last_tensor_stop_index = stop_idx - self.cat_dim_cum_sizes[last_tensor_idx].item()\n\n        if first_tensor_idx == last_tensor_idx:\n            return [first_tensor_idx], [slice(first_tensor_start_index, last_tensor_stop_index, None)]\n        else:\n            num_middle_tensors = last_tensor_idx - first_tensor_idx - 1\n            first_slice = slice(first_tensor_start_index, None, None)\n            last_slice = slice(None, last_tensor_stop_index, None)\n            return (\n                list(range(first_tensor_idx, last_tensor_idx + 1)),\n                [first_slice] + [_noop_index] * num_middle_tensors + [last_slice],\n            )\n\n    def _expand_batch(self, batch_shape):\n        lazy_tensors = [lazy_tensor._expand_batch(batch_shape) for lazy_tensor in self.lazy_tensors]\n        res = self.__class__(*lazy_tensors, dim=self.cat_dim, output_device=self.output_device)\n        return res\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        indices = [*batch_indices, row_index, col_index]\n        target_shape = _mul_broadcast_shape(*[index.shape for index in indices])\n        indices = [index.expand(target_shape).reshape(-1) for index in indices]\n        cat_dim_indices = indices[self.cat_dim]\n\n        # Find out for which indices we switch to different tensors\n        target_tensors = self.idx_to_tensor_idx[cat_dim_indices]\n        does_switch_tensor = torch.ones(target_tensors.numel() + 1, dtype=bool_compat, device=self.device)\n        torch.ne(target_tensors[:-1], target_tensors[1:], out=does_switch_tensor[1:-1])\n\n        # Get the LazyTensors that will comprise the new LazyTensor\n        lazy_tensor_indices = target_tensors[does_switch_tensor[:-1]].tolist()\n        lazy_tensors = [self.lazy_tensors[idx] for idx in lazy_tensor_indices]\n\n        # Get the new set of indices for each of the LazyTensors\n        switch_tensor = does_switch_tensor.nonzero().squeeze(-1)\n        split_sizes = (switch_tensor[1:] - switch_tensor[:-1]).tolist()\n        sub_indices = zip(\n            *[\n                list(index.split(split_sizes)) if torch.is_tensor(index) else [index] * len(split_sizes)\n                for index in indices\n            ]\n        )\n        # Make everything a list\n        sub_indices = [list(sub_index) for sub_index in sub_indices]\n\n        # Make sure that we have adjusted the start and ends of the indices that correspond to the cat dim\n        for lazy_tensor_idx, sub_index in zip(lazy_tensor_indices, sub_indices):\n            sub_index[self.cat_dim] = sub_index[self.cat_dim] - self.cat_dim_cum_sizes[lazy_tensor_idx]\n\n        res_list = [\n            lazy_tensor._get_indices(sub_index[-2], sub_index[-1], *sub_index[:-2])\n            for lazy_tensor, sub_index in zip(lazy_tensors, sub_indices)\n        ]\n        if len(res_list) == 1:\n            return res_list[0].view(target_shape).to(self.device)\n        else:\n            return torch.cat(res_list).view(target_shape).to(self.device)\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        indices = [*batch_indices, row_index, col_index]\n        cat_dim_indices = indices[self.cat_dim]\n\n        if isinstance(cat_dim_indices, slice):\n            if cat_dim_indices == _noop_index:\n                res_list = [\n                    lazy_tensor._getitem(row_index, col_index, *batch_indices) for lazy_tensor in self.lazy_tensors\n                ]\n\n            else:\n                res_list = []\n                tensor_idxs, target_slices = self._split_slice(cat_dim_indices)\n                for tensor_idx, target_slice in zip(tensor_idxs, target_slices):\n                    indices[self.cat_dim] = target_slice\n                    res = self.lazy_tensors[tensor_idx]._getitem(indices[-2], indices[-1], *indices[:-2])\n                    res_list.append(res)\n\n        elif torch.is_tensor(cat_dim_indices):\n            # Find out for which indices we switch to different tensors\n            target_tensors = self.idx_to_tensor_idx[cat_dim_indices]\n            does_switch_tensor = torch.ones(target_tensors.numel() + 1, dtype=bool_compat, device=self.device)\n            torch.ne(target_tensors[:-1], target_tensors[1:], out=does_switch_tensor[1:-1])\n\n            # Get the LazyTensors that will comprise the new LazyTensor\n            lazy_tensor_indices = target_tensors[does_switch_tensor[:-1]].tolist()\n            lazy_tensors = [self.lazy_tensors[idx] for idx in lazy_tensor_indices]\n\n            # Get the new set of indices for each of the LazyTensors\n            switch_tensor = does_switch_tensor.nonzero().squeeze(-1)\n            split_sizes = (switch_tensor[1:] - switch_tensor[:-1]).tolist()\n            sub_indices = zip(\n                *[\n                    list(index.split(split_sizes)) if torch.is_tensor(index) else [index] * len(split_sizes)\n                    for index in indices\n                ]\n            )\n            # Make everything a list\n            sub_indices = [list(sub_index) for sub_index in sub_indices]\n\n            # Make sure that we have adjusted the start and ends of the indices that correspond to the cat dim\n            for lazy_tensor_idx, sub_index in zip(lazy_tensor_indices, sub_indices):\n                sub_index[self.cat_dim] = sub_index[self.cat_dim] - self.cat_dim_cum_sizes[lazy_tensor_idx]\n\n            res_list = [\n                lazy_tensor._getitem(sub_index[-2], sub_index[-1], *sub_index[:-2])\n                for lazy_tensor, sub_index in zip(lazy_tensors, sub_indices)\n            ]\n\n        elif isinstance(cat_dim_indices, int):  # Should only happen for cat on batch dim\n            target_tensor = self.idx_to_tensor_idx[cat_dim_indices].item()\n            cat_dim_indices = cat_dim_indices - self.cat_dim_cum_sizes[target_tensor]\n            indices[self.cat_dim] = cat_dim_indices\n            res_list = [self.lazy_tensors[target_tensor]._getitem(indices[-2], indices[-1], *indices[:-2])]\n\n        # Process the list\n        if len(res_list) == 1:\n            return res_list[0].to(self.output_device)\n        else:\n            res = self.__class__(*res_list, dim=self.cat_dim, output_device=self.output_device)\n            return res\n\n    def _matmul(self, rhs):\n        output_device = self.device if self.device is not None else rhs.device\n        # make a copy of `rhs` on each device\n        rhs_ = []\n        for d in self.devices:\n            if d != rhs.device:\n                rhs_.append(rhs.to(d))\n            else:\n                rhs_.append(rhs)\n\n        if self.cat_dim == -2:\n            res_list = [t._matmul(rhs) for t, rhs in zip(self.lazy_tensors, rhs_)]\n            # copy result back to output device\n            res_list = [x.to(output_device) for x in res_list]\n            res = torch.cat(res_list, dim=-2)\n        elif self.cat_dim == -1:\n            curr_idx = 0\n            res_list = []\n            index = [slice(None, None, None) for _ in range(rhs.ndimension())]\n            for t, size, rhs in zip(self.lazy_tensors, self.cat_dim_sizes, rhs_):\n                index[-2] = slice(curr_idx, curr_idx + size, None)\n                res_list.append(t._matmul(rhs[index]))\n                curr_idx += size\n            # copy result back to output device and sum\n            res_list = [x.to(output_device) for x in res_list]\n            res = 0.0\n            for x in res_list:\n                res = res + x\n        else:\n            output_shape = _matmul_broadcast_shape(self.shape, rhs.shape)\n            rhs = rhs.expand(*output_shape[:-2], *rhs.shape[-2:])\n            curr_idx = 0\n            res_list = []\n            for t, size in zip(self.lazy_tensors, self.cat_dim_sizes):\n                sub_rhs = rhs.narrow(self.cat_dim, curr_idx, size)\n                res_list.append(t._matmul(sub_rhs))\n                curr_idx += size\n            # copy result back to output device\n            res_list = [x.to(output_device) for x in res_list]\n            res = torch.cat(res_list, dim=self.cat_dim)\n\n        return res\n\n    def _permute_batch(self, *dims):\n        lazy_tensors = [lazy_tensor._permute_batch(*dims) for lazy_tensor in self.lazy_tensors]\n        if self.cat_dim < -2:\n            positive_cat_dim = self.dim() + self.cat_dim\n            new_cat_dim = dims.index(positive_cat_dim)\n        else:\n            new_cat_dim = self.cat_dim\n        return self.__class__(*lazy_tensors, dim=new_cat_dim, output_device=self.output_device)\n\n    def _size(self):\n        return self._shape\n\n    def _transpose_nonbatch(self):\n        if self.cat_dim == -2:\n            new_dim = -1\n        elif self.cat_dim == -1:\n            new_dim = -2\n        else:\n            new_dim = self.cat_dim\n        return self.__class__(\n            *[t._transpose_nonbatch() for t in self.lazy_tensors], dim=new_dim, output_device=self.output_device\n        )\n\n    def _unsqueeze_batch(self, dim):\n        cat_dim = self.dim() + self.cat_dim\n        lazy_tensors = [lazy_tensor._unsqueeze_batch(dim) for lazy_tensor in self.lazy_tensors]\n        res = self.__class__(\n            *lazy_tensors, dim=(cat_dim + 1 if dim <= cat_dim else cat_dim), output_device=self.output_device\n        )\n        return res\n\n    def diag(self):\n        if settings.debug.on():\n            if not self.is_square:\n                raise RuntimeError(""Diag works on square matrices (or batches)"")\n\n        if self.cat_dim == -2:\n            res = []\n            curr_col = 0\n            for t in self.lazy_tensors:\n                n_rows, n_cols = t.shape[-2:]\n                rows = torch.arange(0, n_rows, dtype=torch.long, device=t.device)\n                cols = torch.arange(curr_col, curr_col + n_rows, dtype=torch.long, device=t.device)\n                res.append(t[..., rows, cols].to(self.device))\n                curr_col += n_rows\n            res = torch.cat(res, dim=-1)\n        elif self.cat_dim == -1:\n            res = []\n            curr_row = 0\n            for t in self.lazy_tensors:\n                n_rows, n_cols = t.shape[-2:]\n                rows = torch.arange(curr_row, curr_row + n_cols, dtype=torch.long, device=t.device)\n                cols = torch.arange(0, n_cols, dtype=torch.long, device=t.device)\n                curr_row += n_cols\n                res.append(t[..., rows, cols].to(self.device))\n            res = torch.cat(res, dim=-1)\n        else:\n            res = torch.cat([t.diag().to(self.device) for t in self.lazy_tensors], dim=self.cat_dim + 1)\n        return res\n\n    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):\n        res = super().inv_quad_logdet(inv_quad_rhs, logdet, reduce_inv_quad)\n        return tuple(r.to(self.device) for r in res)\n\n    @property\n    def device(self):\n        return self.output_device\n\n    @property\n    def devices(self):\n        return [t.device for t in self.lazy_tensors]\n\n    @property\n    def device_count(self):\n        return len(set(self.devices))\n\n    def to(self, device_id):\n        """"""\n        returns a new CatLazyTensor with device_id as the output_device\n        Warning: this does not move the LazyTensors in this CatLazyTensor to\n        device_id\n        """"""\n        new_kwargs = dict(self._kwargs)\n        new_kwargs[""output_device""] = device_id\n        return self.__class__(*self._args, **new_kwargs)\n\n    def all_to(self, device_id):\n        """"""\n        Create a new CatLazyTensor with all LazyTensors in CatLazyTensor moved\n        to one device device. The new CatLazyTensor also has device_id as the\n        output_device.\n        """"""\n        new_args = []\n        new_kwargs = {}\n        for arg in self._args:\n            if hasattr(arg, ""to""):\n                new_args.append(arg.to(device_id))\n            else:\n                new_args.append(arg)\n        for name, val in self._kwargs.items():\n            if hasattr(val, ""to""):\n                new_kwargs[name] = val.to(device_id)\n            else:\n                new_kwargs[name] = val\n        new_kwargs[""output_device""] = device_id\n        return self.__class__(*new_args, **new_kwargs)\n'"
gpytorch/lazy/chol_lazy_tensor.py,2,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom .. import settings\nfrom .batch_repeat_lazy_tensor import BatchRepeatLazyTensor\nfrom .lazy_tensor import delazify\nfrom .root_lazy_tensor import RootLazyTensor\n\n\nclass CholLazyTensor(RootLazyTensor):\n    def __init__(self, chol):\n        # Check that we have a lower triangular matrix\n        if settings.debug.on():\n            delazy_chol = (\n                delazify(chol) if not isinstance(chol, BatchRepeatLazyTensor) else delazify(chol.base_lazy_tensor)\n            )\n            mask = torch.ones(delazy_chol.shape[-2:], dtype=delazy_chol.dtype, device=delazy_chol.device).triu_(1)\n            if torch.max(delazy_chol.mul(mask)).item() > 1e-3 and torch.equal(delazy_chol, delazy_chol):\n                raise RuntimeError(""CholLazyVariable should take a lower-triangular matrix in the constructor."")\n\n        # Run super constructor\n        super(CholLazyTensor, self).__init__(chol)\n\n    @property\n    def _chol(self):\n        if not hasattr(self, ""_chol_memo""):\n            self._chol_memo = self.root.evaluate()\n        return self._chol_memo\n\n    @property\n    def _chol_diag(self):\n        if not hasattr(self, ""_chol_diag_memo""):\n            self._chol_diag_memo = self._chol.diagonal(dim1=-2, dim2=-1).clone()\n        return self._chol_diag_memo\n\n    def _cholesky(self):\n        return self.root\n\n    def _solve(self, rhs, preconditioner, num_tridiag=0):\n        if num_tridiag:\n            return super()._solve(rhs, preconditioner, num_tridiag=num_tridiag)\n        else:\n            return self.root._cholesky_solve(rhs)\n\n    def inv_matmul(self, right_tensor, left_tensor=None):\n        with settings.fast_computations(solves=False):\n            return super().inv_matmul(right_tensor, left_tensor=left_tensor)\n\n    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):\n        if not self.is_square:\n            raise RuntimeError(\n                ""inv_quad_logdet only operates on (batches of) square (positive semi-definite) LazyTensors. ""\n                ""Got a {} of size {}."".format(self.__class__.__name__, self.size())\n            )\n\n        if inv_quad_rhs is not None:\n            if self.dim() == 2 and inv_quad_rhs.dim() == 1:\n                if self.shape[-1] != inv_quad_rhs.numel():\n                    raise RuntimeError(\n                        ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(\n                            self.shape, inv_quad_rhs.shape\n                        )\n                    )\n            elif self.dim() != inv_quad_rhs.dim():\n                raise RuntimeError(\n                    ""LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number ""\n                    ""of dimensions."".format(self.shape, inv_quad_rhs.shape)\n                )\n            elif self.shape[-1] != inv_quad_rhs.shape[-2]:\n                raise RuntimeError(\n                    ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(\n                        self.shape, inv_quad_rhs.shape\n                    )\n                )\n\n        inv_quad_term = None\n        logdet_term = None\n\n        if inv_quad_rhs is not None:\n            inv_quad_term = self.inv_quad(inv_quad_rhs, reduce_inv_quad=reduce_inv_quad)\n\n        if logdet:\n            logdet_term = self._chol_diag.pow(2).log().sum(-1)\n\n        return inv_quad_term, logdet_term\n'"
gpytorch/lazy/constant_mul_lazy_tensor.py,9,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils.memoize import cached\nfrom .lazy_tensor import LazyTensor\n\n\nclass ConstantMulLazyTensor(LazyTensor):\n    """"""\n    A LazyTensor that multiplies a base LazyTensor by a scalar constant:\n\n    ```\n    constant_mul_lazy_tensor = constant * base_lazy_tensor\n    ```\n\n    .. note::\n\n        To element-wise multiply two lazy tensors, see :class:`gpytorch.lazy.MulLazyTensor`\n\n    Args:\n        base_lazy_tensor (LazyTensor) or (b x n x m)): The base_lazy tensor\n        constant (Tensor): The constant\n\n    If `base_lazy_tensor` represents a matrix (non-batch), then `constant` must be a\n    0D tensor, or a 1D tensor with one element.\n\n    If `base_lazy_tensor` represents a batch of matrices (b x m x n), then `constant` can be\n    either:\n    - A 0D tensor - the same constant is applied to all matrices in the batch\n    - A 1D tensor with one element - the same constant is applied to all matrices\n    - A 1D tensor with `b` elements - a different constant is applied to each matrix\n\n    Example::\n\n        >>> base_base_lazy_tensor = gpytorch.lazy.ToeplitzLazyTensor([1, 2, 3])\n        >>> constant = torch.tensor(1.2)\n        >>> new_base_lazy_tensor = gpytorch.lazy.ConstantMulLazyTensor(base_base_lazy_tensor, constant)\n        >>> new_base_lazy_tensor.evaluate()\n        >>> # Returns:\n        >>> # [[ 1.2, 2.4, 3.6 ]\n        >>> #  [ 2.4, 1.2, 2.4 ]\n        >>> #  [ 3.6, 2.4, 1.2 ]]\n        >>>\n        >>> base_base_lazy_tensor = gpytorch.lazy.ToeplitzLazyTensor([[1, 2, 3], [2, 3, 4]])\n        >>> constant = torch.tensor([1.2, 0.5])\n        >>> new_base_lazy_tensor = gpytorch.lazy.ConstantMulLazyTensor(base_base_lazy_tensor, constant)\n        >>> new_base_lazy_tensor.evaluate()\n        >>> # Returns:\n        >>> # [[[ 1.2, 2.4, 3.6 ]\n        >>> #   [ 2.4, 1.2, 2.4 ]\n        >>> #   [ 3.6, 2.4, 1.2 ]]\n        >>> #  [[ 1, 1.5, 2 ]\n        >>> #   [ 1.5, 1, 1.5 ]\n        >>> #   [ 2, 1.5, 1 ]]]\n    """"""\n\n    def __init__(self, base_lazy_tensor, constant):\n        if not torch.is_tensor(constant):\n            constant = torch.tensor(constant, device=base_lazy_tensor.device, dtype=base_lazy_tensor.dtype)\n\n        super(ConstantMulLazyTensor, self).__init__(base_lazy_tensor, constant)\n        self.base_lazy_tensor = base_lazy_tensor\n        self._constant = constant\n\n    def _approx_diag(self):\n        res = self.base_lazy_tensor._approx_diag()\n        return res * self._constant.unsqueeze(-1)\n\n    def _expand_batch(self, batch_shape):\n        return self.__class__(self.base_lazy_tensor._expand_batch(batch_shape), self._constant.expand(*batch_shape))\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        # NOTE TO FUTURE SELF:\n        # This custom __getitem__ is actually very important!\n        # It prevents constructing an InterpolatedLazyTensor when one isn\'t needed\n        # This affects runntimes by up to 5x on simple exact GPs\n        # Run __getitem__ on the base_lazy_tensor and the constant\n        base_lazy_tensor = self.base_lazy_tensor._get_indices(row_index, col_index, *batch_indices)\n        constant = self._constant.expand(self.batch_shape)[batch_indices]\n        return base_lazy_tensor * constant\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        # NOTE TO FUTURE SELF:\n        # This custom __getitem__ is actually very important!\n        # It prevents constructing an InterpolatedLazyTensor when one isn\'t needed\n        # This affects runntimes by up to 5x on simple exact GPs\n        # Run __getitem__ on the base_lazy_tensor and the constant\n        base_lazy_tensor = self.base_lazy_tensor._getitem(row_index, col_index, *batch_indices)\n        constant = self._constant.expand(self.batch_shape)[batch_indices]\n        constant = constant.view(*constant.shape, 1, 1)\n        return base_lazy_tensor * constant\n\n    def _matmul(self, rhs):\n        res = self.base_lazy_tensor._matmul(rhs)\n        res = res * self.expanded_constant\n        return res\n\n    def _permute_batch(self, *dims):\n        return self.__class__(\n            self.base_lazy_tensor._permute_batch(*dims), self._constant.expand(self.batch_shape).permute(*dims)\n        )\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        # Gradient with respect to the constant\n        constant_deriv = left_vecs * self.base_lazy_tensor._matmul(right_vecs)\n        constant_deriv = constant_deriv.sum(-2).sum(-1)\n        while constant_deriv.dim() > self._constant.dim():\n            constant_deriv = constant_deriv.sum(0)\n        for i in range(self._constant.dim()):\n            if self._constant.size(i) == 1:\n                constant_deriv = constant_deriv.sum(i, keepdim=True)\n\n        # Get derivaties of everything else\n        left_vecs = left_vecs * self.expanded_constant\n        res = self.base_lazy_tensor._quad_form_derivative(left_vecs, right_vecs)\n\n        return tuple(res) + (constant_deriv,)\n\n    def _size(self):\n        return self.base_lazy_tensor.size()\n\n    def _t_matmul(self, rhs):\n        res = self.base_lazy_tensor._t_matmul(rhs)\n        res = res * self.expanded_constant\n        return res\n\n    def _transpose_nonbatch(self):\n        return ConstantMulLazyTensor(self.base_lazy_tensor._transpose_nonbatch(), self._constant)\n\n    @property\n    def expanded_constant(self):\n        # Make sure that the constant can be expanded to the appropriate size\n        try:\n            constant = self._constant.view(*self._constant.shape, 1, 1)\n        except RuntimeError:\n            raise RuntimeError(\n                ""ConstantMulLazyTensor of size {} received an invalid constant of size {}."".format(\n                    self.base_lazy_tensor.shape, self._constant.shape\n                )\n            )\n\n        return constant\n\n    def diag(self):\n        res = self.base_lazy_tensor.diag()\n        return res * self._constant.unsqueeze(-1)\n\n    @cached\n    def evaluate(self):\n        res = self.base_lazy_tensor.evaluate()\n        return res * self.expanded_constant\n'"
gpytorch/lazy/diag_lazy_tensor.py,6,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils.broadcasting import _mul_broadcast_shape\nfrom ..utils.memoize import cached\nfrom .lazy_tensor import LazyTensor\nfrom .non_lazy_tensor import NonLazyTensor\n\n\nclass DiagLazyTensor(LazyTensor):\n    def __init__(self, diag):\n        """"""\n        Diagonal lazy tensor. Supports arbitrary batch sizes.\n\n        Args:\n            :attr:`diag` (Tensor):\n                A `b1 x ... x bk x n` Tensor, representing a `b1 x ... x bk`-sized batch\n                of `n x n` diagonal matrices\n        """"""\n        super().__init__(diag)\n        self._diag = diag\n\n    def __add__(self, other):\n        if isinstance(other, DiagLazyTensor):\n            return self.add_diag(other._diag)\n        from .added_diag_lazy_tensor import AddedDiagLazyTensor\n\n        return AddedDiagLazyTensor(other, self)\n\n    @cached(name=""cholesky"")\n    def _cholesky(self):\n        return self.sqrt()\n\n    def _cholesky_solve(self, rhs):\n        return rhs / self._diag.pow(2)\n\n    def _expand_batch(self, batch_shape):\n        return self.__class__(self._diag.expand(*batch_shape, self._diag.size(-1)))\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        res = self._diag[(*batch_indices, row_index)]\n        # If row and col index don\'t agree, then we have off diagonal elements\n        # Those should be zero\'d out\n        res = res * torch.eq(row_index, col_index).to(device=res.device, dtype=res.dtype)\n        return res\n\n    def _matmul(self, rhs):\n        # to perform matrix multiplication with diagonal matrices we can just\n        # multiply element-wise with the diagonal (using proper broadcasting)\n        if rhs.ndimension() == 1:\n            return self._diag * rhs\n        # special case if we have a NonLazyTensor\n        if isinstance(rhs, NonLazyTensor):\n            return NonLazyTensor(self._diag.unsqueeze(-1) * rhs.tensor)\n        return self._diag.unsqueeze(-1) * rhs\n\n    def _mul_constant(self, constant):\n        return self.__class__(self._diag * constant.unsqueeze(-1))\n\n    def _mul_matrix(self, other):\n        if isinstance(other, DiagLazyTensor):\n            return self.__class__(self._diag * other._diag)\n        else:\n            return self.__class__(self._diag * other.diag())\n\n    def _prod_batch(self, dim):\n        return self.__class__(self._diag.prod(dim))\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        # TODO: Use proper batching for input vectors (prepand to shape rathern than append)\n        res = left_vecs * right_vecs\n        if res.ndimension() > self._diag.ndimension():\n            res = res.sum(-1)\n        return (res,)\n\n    def _root_decomposition(self):\n        return self.sqrt()\n\n    def _root_inv_decomposition(self, initial_vectors=None):\n        return DiagLazyTensor(self._diag.reciprocal()).sqrt()\n\n    def _size(self):\n        return self._diag.shape + self._diag.shape[-1:]\n\n    def _sum_batch(self, dim):\n        return self.__class__(self._diag.sum(dim))\n\n    def _t_matmul(self, rhs):\n        # Diagonal matrices always commute\n        return self._matmul(rhs)\n\n    def _transpose_nonbatch(self):\n        return self\n\n    def abs(self):\n        return DiagLazyTensor(self._diag.abs())\n\n    def add_diag(self, added_diag):\n        shape = _mul_broadcast_shape(self._diag.shape, added_diag.shape)\n        return DiagLazyTensor(self._diag.expand(shape) + added_diag.expand(shape))\n\n    def diag(self):\n        return self._diag\n\n    @cached\n    def evaluate(self):\n        if self._diag.dim() == 0:\n            return self._diag\n        return torch.diag_embed(self._diag)\n\n    def exp(self):\n        return DiagLazyTensor(self._diag.exp())\n\n    def inverse(self):\n        return DiagLazyTensor(self._diag.reciprocal())\n\n    def inv_matmul(self, right_tensor, left_tensor=None):\n        res = self.inverse()._matmul(right_tensor)\n        if left_tensor is not None:\n            res = left_tensor @ res\n        return res\n\n    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):\n\n        # TODO: Use proper batching for inv_quad_rhs (prepand to shape rathern than append)\n        if inv_quad_rhs is None:\n            rhs_batch_shape = torch.Size()\n        else:\n            rhs_batch_shape = inv_quad_rhs.shape[1 + self.batch_dim :]\n\n        if inv_quad_rhs is None:\n            inv_quad_term = torch.empty(0, dtype=self.dtype, device=self.device)\n        else:\n            diag = self._diag\n            for _ in rhs_batch_shape:\n                diag = diag.unsqueeze(-1)\n            inv_quad_term = inv_quad_rhs.div(diag).mul(inv_quad_rhs).sum(-(1 + len(rhs_batch_shape)))\n            if reduce_inv_quad:\n                inv_quad_term = inv_quad_term.sum(-1)\n\n        if not logdet:\n            logdet_term = torch.empty(0, dtype=self.dtype, device=self.device)\n        else:\n            logdet_term = self._diag.log().sum(-1)\n\n        return inv_quad_term, logdet_term\n\n    def log(self):\n        return DiagLazyTensor(self._diag.log())\n\n    def matmul(self, other):\n        # this is trivial if we multiply two DiagLazyTensors\n        if isinstance(other, DiagLazyTensor):\n            return DiagLazyTensor(self._diag * other._diag)\n        # special case if we have a NonLazyTensor\n        if isinstance(other, NonLazyTensor):\n            return NonLazyTensor(self._diag.unsqueeze(-1) * other.tensor)\n        return super().matmul(other)\n\n    def sqrt(self):\n        return DiagLazyTensor(self._diag.sqrt())\n\n    def zero_mean_mvn_samples(self, num_samples):\n        base_samples = torch.randn(num_samples, *self._diag.shape, dtype=self.dtype, device=self.device)\n        return base_samples * self._diag.sqrt()\n'"
gpytorch/lazy/interpolated_lazy_tensor.py,14,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils import sparse\nfrom ..utils.broadcasting import _pad_with_singletons\nfrom ..utils.getitem import _noop_index\nfrom ..utils.interpolation import left_interp, left_t_interp\nfrom .lazy_tensor import LazyTensor\nfrom .non_lazy_tensor import NonLazyTensor, lazify\nfrom .root_lazy_tensor import RootLazyTensor\n\n\nclass InterpolatedLazyTensor(LazyTensor):\n    def _check_args(\n        self, base_lazy_tensor, left_interp_indices, left_interp_values, right_interp_indices, right_interp_values\n    ):\n        if left_interp_indices.size() != left_interp_values.size():\n            return ""Expected left_interp_indices ({}) to have the same size as left_interp_values ({})"".format(\n                left_interp_indices.size(), left_interp_values.size()\n            )\n        if right_interp_indices.size() != right_interp_values.size():\n            return ""Expected right_interp_indices ({}) to have the same size as right_interp_values ({})"".format(\n                right_interp_indices.size(), right_interp_values.size()\n            )\n        if left_interp_indices.shape[:-2] != right_interp_indices.shape[:-2]:\n            return (\n                ""left interp size ({}) is incompatible with right interp size ({}). Make sure the two have the ""\n                ""same number of batch dimensions"".format(left_interp_indices.size(), right_interp_indices.size())\n            )\n        if left_interp_indices.shape[:-2] != base_lazy_tensor.shape[:-2]:\n            return (\n                ""left interp size ({}) is incompatible with base lazy tensor size ({}). Make sure the two have the ""\n                ""same number of batch dimensions"".format(left_interp_indices.size(), base_lazy_tensor.size())\n            )\n\n    def __init__(\n        self,\n        base_lazy_tensor,\n        left_interp_indices=None,\n        left_interp_values=None,\n        right_interp_indices=None,\n        right_interp_values=None,\n    ):\n        base_lazy_tensor = lazify(base_lazy_tensor)\n\n        if left_interp_indices is None:\n            num_rows = base_lazy_tensor.size(-2)\n            left_interp_indices = torch.arange(0, num_rows, dtype=torch.long, device=base_lazy_tensor.device)\n            left_interp_indices.unsqueeze_(-1)\n            left_interp_indices = left_interp_indices.expand(*base_lazy_tensor.batch_shape, num_rows, 1)\n\n        if left_interp_values is None:\n            left_interp_values = torch.ones(\n                left_interp_indices.size(), dtype=base_lazy_tensor.dtype, device=base_lazy_tensor.device\n            )\n\n        if right_interp_indices is None:\n            num_cols = base_lazy_tensor.size(-1)\n            right_interp_indices = torch.arange(0, num_cols, dtype=torch.long, device=base_lazy_tensor.device)\n            right_interp_indices.unsqueeze_(-1)\n            right_interp_indices = right_interp_indices.expand(*base_lazy_tensor.batch_shape, num_cols, 1)\n\n        if right_interp_values is None:\n            right_interp_values = torch.ones(\n                right_interp_indices.size(), dtype=base_lazy_tensor.dtype, device=base_lazy_tensor.device\n            )\n\n        if left_interp_indices.shape[:-2] != base_lazy_tensor.batch_shape:\n            try:\n                base_lazy_tensor = base_lazy_tensor._expand_batch(left_interp_indices.shape[:-2])\n            except RuntimeError:\n                raise RuntimeError(\n                    ""interp size ({}) is incompatible with base_lazy_tensor size ({}). "".format(\n                        right_interp_indices.size(), base_lazy_tensor.size()\n                    )\n                )\n\n        super(InterpolatedLazyTensor, self).__init__(\n            base_lazy_tensor, left_interp_indices, left_interp_values, right_interp_indices, right_interp_values\n        )\n        self.base_lazy_tensor = base_lazy_tensor\n        self.left_interp_indices = left_interp_indices\n        self.left_interp_values = left_interp_values\n        self.right_interp_indices = right_interp_indices\n        self.right_interp_values = right_interp_values\n\n    def _approx_diag(self):\n        base_diag_root = self.base_lazy_tensor.diag().sqrt()\n        left_res = left_interp(self.left_interp_indices, self.left_interp_values, base_diag_root.unsqueeze(-1))\n        right_res = left_interp(self.right_interp_indices, self.right_interp_values, base_diag_root.unsqueeze(-1))\n        res = left_res * right_res\n        return res.squeeze(-1)\n\n    def _expand_batch(self, batch_shape):\n        return self.__class__(\n            self.base_lazy_tensor._expand_batch(batch_shape),\n            self.left_interp_indices.expand(*batch_shape, *self.left_interp_indices.shape[-2:]),\n            self.left_interp_values.expand(*batch_shape, *self.left_interp_values.shape[-2:]),\n            self.right_interp_indices.expand(*batch_shape, *self.right_interp_indices.shape[-2:]),\n            self.right_interp_values.expand(*batch_shape, *self.right_interp_values.shape[-2:]),\n        )\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        left_interp_indices = self.left_interp_indices.__getitem__((*batch_indices, row_index)).unsqueeze(-2)\n        right_interp_indices = self.right_interp_indices.__getitem__((*batch_indices, col_index)).unsqueeze(-1)\n        base_vals = self.base_lazy_tensor._get_indices(\n            left_interp_indices,\n            right_interp_indices,\n            *[batch_index.view(*batch_index.shape, 1, 1) for batch_index in batch_indices],\n        )\n\n        left_interp_values = self.left_interp_values.__getitem__((*batch_indices, row_index)).unsqueeze(-2)\n        right_interp_values = self.right_interp_values.__getitem__((*batch_indices, col_index)).unsqueeze(-1)\n        interp_values = left_interp_values * right_interp_values\n\n        res = (base_vals * interp_values).sum([-2, -1])\n        return res\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        # Handle batch dimensions\n        # Construt a new LazyTensor\n        base_lazy_tensor = self.base_lazy_tensor\n        left_interp_indices = self.left_interp_indices\n        left_interp_values = self.left_interp_values\n        right_interp_indices = self.right_interp_indices\n        right_interp_values = self.right_interp_values\n\n        if len(batch_indices):\n            base_lazy_tensor = base_lazy_tensor._getitem(_noop_index, _noop_index, *batch_indices)\n\n        # Special case: if both row and col are not indexed, then we are done\n        if row_index is _noop_index and col_index is _noop_index:\n            left_interp_indices = left_interp_indices[batch_indices]\n            left_interp_values = left_interp_values[batch_indices]\n            right_interp_indices = right_interp_indices[batch_indices]\n            right_interp_values = right_interp_values[batch_indices]\n\n            return self.__class__(\n                base_lazy_tensor,\n                left_interp_indices,\n                left_interp_values,\n                right_interp_indices,\n                right_interp_values,\n                **self._kwargs,\n            )\n\n        # Normal case: we have to do some processing on either the rows or columns\n        # We will handle this through ""interpolation""\n        left_interp_indices = left_interp_indices[(*batch_indices, row_index, _noop_index)]\n        left_interp_values = left_interp_values[(*batch_indices, row_index, _noop_index)]\n        right_interp_indices = right_interp_indices[(*batch_indices, col_index, _noop_index)]\n        right_interp_values = right_interp_values[(*batch_indices, col_index, _noop_index)]\n\n        # Construct interpolated LazyTensor\n        res = self.__class__(\n            base_lazy_tensor,\n            left_interp_indices,\n            left_interp_values,\n            right_interp_indices,\n            right_interp_values,\n            **self._kwargs,\n        )\n        return res\n\n    def _matmul(self, rhs):\n        # Get sparse tensor representations of left/right interp matrices\n        left_interp_t = self._sparse_left_interp_t(self.left_interp_indices, self.left_interp_values)\n        right_interp_t = self._sparse_right_interp_t(self.right_interp_indices, self.right_interp_values)\n\n        if rhs.ndimension() == 1:\n            is_vector = True\n            rhs = rhs.unsqueeze(-1)\n        else:\n            is_vector = False\n\n        # right_interp^T * rhs\n        right_interp_res = sparse.bdsmm(right_interp_t, rhs)\n\n        # base_lazy_tensor * right_interp^T * rhs\n        base_res = self.base_lazy_tensor._matmul(right_interp_res)\n\n        # left_interp * base_lazy_tensor * right_interp^T * rhs\n        left_interp_mat = left_interp_t.transpose(-1, -2)\n        res = sparse.bdsmm(left_interp_mat, base_res)\n\n        # Squeeze if necessary\n        if is_vector:\n            res = res.squeeze(-1)\n        return res\n\n    def _mul_constant(self, other):\n        # We\'re using a custom method here - the constant mul is applied to the base_lazy tensor\n        # This preserves the interpolated structure\n        return self.__class__(\n            self.base_lazy_tensor._mul_constant(other),\n            self.left_interp_indices,\n            self.left_interp_values,\n            self.right_interp_indices,\n            self.right_interp_values,\n        )\n\n    def _t_matmul(self, rhs):\n        # Get sparse tensor representations of left/right interp matrices\n        left_interp_t = self._sparse_left_interp_t(self.left_interp_indices, self.left_interp_values)\n        right_interp_t = self._sparse_right_interp_t(self.right_interp_indices, self.right_interp_values)\n\n        if rhs.ndimension() == 1:\n            is_vector = True\n            rhs = rhs.unsqueeze(-1)\n        else:\n            is_vector = False\n\n        # right_interp^T * rhs\n        left_interp_res = sparse.bdsmm(left_interp_t, rhs)\n\n        # base_lazy_tensor * right_interp^T * rhs\n        base_res = self.base_lazy_tensor._t_matmul(left_interp_res)\n\n        # left_interp * base_lazy_tensor * right_interp^T * rhs\n        right_interp_mat = right_interp_t.transpose(-1, -2)\n        res = sparse.bdsmm(right_interp_mat, base_res)\n\n        # Squeeze if necessary\n        if is_vector:\n            res = res.squeeze(-1)\n        return res\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        # Get sparse tensor representations of left/right interp matrices\n        left_interp_t = self._sparse_left_interp_t(self.left_interp_indices, self.left_interp_values)\n        right_interp_t = self._sparse_right_interp_t(self.right_interp_indices, self.right_interp_values)\n\n        if left_vecs.ndimension() == 1:\n            left_vecs = left_vecs.unsqueeze(1)\n            right_vecs = right_vecs.unsqueeze(1)\n\n        # base_lazy_tensor grad\n        left_res = sparse.bdsmm(left_interp_t, left_vecs)\n        right_res = sparse.bdsmm(right_interp_t, right_vecs)\n        base_lv_grad = list(self.base_lazy_tensor._quad_form_derivative(left_res, right_res))\n\n        # left_interp_values grad\n        n_vecs = right_res.size(-1)\n        n_left_rows = self.left_interp_indices.size(-2)\n        n_right_rows = self.right_interp_indices.size(-2)\n        n_left_interp = self.left_interp_indices.size(-1)\n        n_right_interp = self.right_interp_indices.size(-1)\n        n_inducing = right_res.size(-2)\n\n        # left_interp_values grad\n        right_interp_right_res = self.base_lazy_tensor._matmul(right_res).contiguous()\n        batch_shape = torch.Size(right_interp_right_res.shape[:-2])\n        batch_size = batch_shape.numel()\n        if len(batch_shape):\n            batch_offset = torch.arange(0, batch_size, dtype=torch.long, device=self.device).view(*batch_shape)\n            batch_offset.unsqueeze_(-1).unsqueeze_(-1).mul_(n_inducing)\n            batched_right_interp_indices = self.right_interp_indices\n            batched_left_interp_indices = (self.left_interp_indices + batch_offset).view(-1)\n        else:\n            batched_left_interp_indices = self.left_interp_indices.view(-1)\n\n        flattened_right_interp_right_res = right_interp_right_res.view(batch_size * n_inducing, n_vecs)\n        selected_right_vals = flattened_right_interp_right_res.index_select(0, batched_left_interp_indices)\n        selected_right_vals = selected_right_vals.view(*batch_shape, n_left_rows, n_left_interp, n_vecs)\n        left_values_grad = (selected_right_vals * left_vecs.unsqueeze(-2)).sum(-1)\n\n        # right_interp_values_grad\n        left_interp_left_res = self.base_lazy_tensor._t_matmul(left_res).contiguous()\n        batch_shape = left_interp_left_res.shape[:-2]\n        batch_size = batch_shape.numel()\n        if len(batch_shape):\n            batch_offset = torch.arange(0, batch_size, dtype=torch.long, device=self.device).view(*batch_shape)\n            batch_offset.unsqueeze_(-1).unsqueeze_(-1).mul_(n_inducing)\n            batched_right_interp_indices = (self.right_interp_indices + batch_offset).view(-1)\n        else:\n            batched_right_interp_indices = self.right_interp_indices.view(-1)\n\n        flattened_left_interp_left_res = left_interp_left_res.view(batch_size * n_inducing, n_vecs)\n        selected_left_vals = flattened_left_interp_left_res.index_select(0, batched_right_interp_indices)\n        selected_left_vals = selected_left_vals.view(*batch_shape, n_right_rows, n_right_interp, n_vecs)\n        right_values_grad = (selected_left_vals * right_vecs.unsqueeze(-2)).sum(-1)\n\n        # Return zero grad for interp indices\n        res = tuple(\n            base_lv_grad\n            + [\n                torch.zeros_like(self.left_interp_indices),\n                left_values_grad,\n                torch.zeros_like(self.right_interp_indices),\n                right_values_grad,\n            ]\n        )\n        return res\n\n    def _size(self):\n        return torch.Size(\n            self.base_lazy_tensor.batch_shape + (self.left_interp_indices.size(-2), self.right_interp_indices.size(-2))\n        )\n\n    def _transpose_nonbatch(self):\n        res = self.__class__(\n            self.base_lazy_tensor.transpose(-1, -2),\n            self.right_interp_indices,\n            self.right_interp_values,\n            self.left_interp_indices,\n            self.left_interp_values,\n            **self._kwargs,\n        )\n        return res\n\n    def _sparse_left_interp_t(self, left_interp_indices_tensor, left_interp_values_tensor):\n        if hasattr(self, ""_sparse_left_interp_t_memo""):\n            if torch.equal(self._left_interp_indices_memo, left_interp_indices_tensor) and torch.equal(\n                self._left_interp_values_memo, left_interp_values_tensor\n            ):\n                return self._sparse_left_interp_t_memo\n\n        left_interp_t = sparse.make_sparse_from_indices_and_values(\n            left_interp_indices_tensor, left_interp_values_tensor, self.base_lazy_tensor.size()[-2]\n        )\n        self._left_interp_indices_memo = left_interp_indices_tensor\n        self._left_interp_values_memo = left_interp_values_tensor\n        self._sparse_left_interp_t_memo = left_interp_t\n        return self._sparse_left_interp_t_memo\n\n    def _sparse_right_interp_t(self, right_interp_indices_tensor, right_interp_values_tensor):\n        if hasattr(self, ""_sparse_right_interp_t_memo""):\n            if torch.equal(self._right_interp_indices_memo, right_interp_indices_tensor) and torch.equal(\n                self._right_interp_values_memo, right_interp_values_tensor\n            ):\n                return self._sparse_right_interp_t_memo\n\n        right_interp_t = sparse.make_sparse_from_indices_and_values(\n            right_interp_indices_tensor, right_interp_values_tensor, self.base_lazy_tensor.size()[-1]\n        )\n        self._right_interp_indices_memo = right_interp_indices_tensor\n        self._right_interp_values_memo = right_interp_values_tensor\n        self._sparse_right_interp_t_memo = right_interp_t\n        return self._sparse_right_interp_t_memo\n\n    def _sum_batch(self, dim):\n        left_interp_indices = self.left_interp_indices\n        left_interp_values = self.left_interp_values\n        right_interp_indices = self.right_interp_indices\n        right_interp_values = self.right_interp_values\n\n        # Increase interpolation indices appropriately\n        left_factor = torch.arange(0, left_interp_indices.size(dim), dtype=torch.long, device=self.device)\n        left_factor = _pad_with_singletons(left_factor, 0, self.dim() - dim - 1)\n        left_factor = left_factor * self.base_lazy_tensor.size(-2)\n        left_interp_indices = left_interp_indices.add(left_factor)\n        right_factor = torch.arange(0, right_interp_indices.size(dim), dtype=torch.long, device=self.device)\n        right_factor = _pad_with_singletons(right_factor, 0, self.dim() - dim - 1)\n        right_factor = right_factor * self.base_lazy_tensor.size(-1)\n        right_interp_indices = right_interp_indices.add(right_factor)\n\n        # Rearrange the indices and values\n        permute_order = (*range(0, dim), *range(dim + 1, self.dim()), dim)\n        left_shape = (*left_interp_indices.shape[:dim], *left_interp_indices.shape[dim + 1 : -1], -1)\n        right_shape = (*right_interp_indices.shape[:dim], *right_interp_indices.shape[dim + 1 : -1], -1)\n        left_interp_indices = left_interp_indices.permute(permute_order).reshape(left_shape)\n        left_interp_values = left_interp_values.permute(permute_order).reshape(left_shape)\n        right_interp_indices = right_interp_indices.permute(permute_order).reshape(right_shape)\n        right_interp_values = right_interp_values.permute(permute_order).reshape(right_shape)\n\n        # Make the base_lazy tensor block diagonal\n        from .block_diag_lazy_tensor import BlockDiagLazyTensor\n\n        block_diag = BlockDiagLazyTensor(self.base_lazy_tensor, block_dim=dim)\n\n        # Finally! We have an interpolated lazy tensor again\n        return InterpolatedLazyTensor(\n            block_diag, left_interp_indices, left_interp_values, right_interp_indices, right_interp_values\n        )\n\n    def diag(self):\n        if isinstance(self.base_lazy_tensor, RootLazyTensor) and isinstance(self.base_lazy_tensor.root, NonLazyTensor):\n            left_interp_vals = left_interp(\n                self.left_interp_indices, self.left_interp_values, self.base_lazy_tensor.root.evaluate()\n            )\n            right_interp_vals = left_interp(\n                self.right_interp_indices, self.right_interp_values, self.base_lazy_tensor.root.evaluate()\n            )\n            return (left_interp_vals * right_interp_vals).sum(-1)\n        else:\n            return super(InterpolatedLazyTensor, self).diag()\n\n    def matmul(self, tensor):\n        # We\'re using a custom matmul here, because it is significantly faster than\n        # what we get from the function factory.\n        # The _matmul_closure is optimized for repeated calls, such as for inv_matmul\n\n        if tensor.ndimension() == 1:\n            is_vector = True\n            tensor = tensor.unsqueeze(-1)\n        else:\n            is_vector = False\n\n        # right_interp^T * tensor\n        base_size = self.base_lazy_tensor.size(-1)\n        right_interp_res = left_t_interp(self.right_interp_indices, self.right_interp_values, tensor, base_size)\n\n        # base_lazy_tensor * right_interp^T * tensor\n        base_res = self.base_lazy_tensor.matmul(right_interp_res)\n\n        # left_interp * base_lazy_tensor * right_interp^T * tensor\n        res = left_interp(self.left_interp_indices, self.left_interp_values, base_res)\n\n        # Squeeze if necessary\n        if is_vector:\n            res = res.squeeze(-1)\n        return res\n\n    def zero_mean_mvn_samples(self, num_samples):\n        base_samples = self.base_lazy_tensor.zero_mean_mvn_samples(num_samples)\n        batch_iter = tuple(range(1, base_samples.dim()))\n        base_samples = base_samples.permute(*batch_iter, 0)\n        res = left_interp(self.left_interp_indices, self.left_interp_values, base_samples).contiguous()\n        batch_iter = tuple(range(res.dim() - 1))\n        return res.permute(-1, *batch_iter).contiguous()\n'"
gpytorch/lazy/keops_lazy_tensor.py,2,"b'import torch\n\nfrom ..utils.getitem import _noop_index\nfrom ..utils.memoize import cached\nfrom .lazy_tensor import LazyTensor\n\n\nclass KeOpsLazyTensor(LazyTensor):\n    def __init__(self, x1, x2, covar_func, **params):\n        super().__init__(x1, x2, covar_func=covar_func, **params)\n\n        self.x1 = x1.contiguous()\n        self.x2 = x2.contiguous()\n        self.covar_func = covar_func\n        self.params = params\n\n    @cached(name=""kernel_diag"")\n    def diag(self):\n        """"""\n        Explicitly compute kernel diag via covar_func when it is needed rather than relying on lazy tensor ops.\n        """"""\n        return self.covar_func(self.x1, self.x2, diag=True)\n\n    @property\n    @cached(name=""covar_mat"")\n    def covar_mat(self):\n        return self.covar_func(self.x1, self.x2, **self.params)\n\n    def _matmul(self, rhs):\n        return self.covar_mat @ rhs.contiguous()\n\n    def _size(self):\n        return torch.Size(self.covar_mat.shape)\n\n    def _transpose_nonbatch(self):\n        return KeOpsLazyTensor(self.x2, self.x1, self.covar_func)\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        x1_ = self.x1[(*batch_indices, row_index)]\n        x2_ = self.x2[(*batch_indices, col_index)]\n        return self.covar_func(x1_, x2_, diag=True, **self.params)\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        x1 = self.x1\n        x2 = self.x2\n        dim_index = _noop_index\n\n        # Get the indices of x1 and x2 that matter for the kernel\n        # Call x1[*batch_indices, row_index, :]\n        try:\n            x1 = x1[(*batch_indices, row_index, dim_index)]\n        # We\'re going to handle multi-batch indexing with a try-catch loop\n        # This way - in the default case, we can avoid doing expansions of x1 which can be timely\n        except IndexError:\n            if isinstance(batch_indices, slice):\n                x1 = x1.expand(1, *self.x1.shape[-2:])[(*batch_indices, row_index, dim_index)]\n            elif isinstance(batch_indices, tuple):\n                if any(not isinstance(bi, slice) for bi in batch_indices):\n                    raise RuntimeError(\n                        f""Attempting to tensor index a non-batch matrix\'s batch dimensions. ""\n                        ""Got batch index {batch_indices} but my shape was {self.shape}""\n                    )\n                x1 = x1.expand(*([1] * len(batch_indices)), *self.x1.shape[-2:])\n                x1 = x1[(*batch_indices, row_index, dim_index)]\n\n        # Call x2[*batch_indices, row_index, :]\n        try:\n            x2 = x2[(*batch_indices, col_index, dim_index)]\n        # We\'re going to handle multi-batch indexing with a try-catch loop\n        # This way - in the default case, we can avoid doing expansions of x1 which can be timely\n        except IndexError:\n            if isinstance(batch_indices, slice):\n                x2 = x2.expand(1, *self.x2.shape[-2:])[(*batch_indices, row_index, dim_index)]\n            elif isinstance(batch_indices, tuple):\n                if any([not isinstance(bi, slice) for bi in batch_indices]):\n                    raise RuntimeError(\n                        f""Attempting to tensor index a non-batch matrix\'s batch dimensions. ""\n                        ""Got batch index {batch_indices} but my shape was {self.shape}""\n                    )\n                x2 = x2.expand(*([1] * len(batch_indices)), *self.x2.shape[-2:])\n                x2 = x2[(*batch_indices, row_index, dim_index)]\n\n        # Now construct a kernel with those indices\n        return self.__class__(x1, x2, covar_func=self.covar_func, **self.params)\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        """"""\n        Use default behavior, but KeOps does not automatically make args contiguous like torch.matmul.\n\n        This is necessary for variational GP models.\n        """"""\n        return super()._quad_form_derivative(left_vecs.contiguous(), right_vecs.contiguous())\n'"
gpytorch/lazy/kronecker_product_lazy_tensor.py,2,"b'#!/usr/bin/env python3\n\nimport operator\nfrom functools import reduce\n\nimport torch\n\nfrom ..utils.broadcasting import _matmul_broadcast_shape\nfrom ..utils.memoize import cached\nfrom .lazy_tensor import LazyTensor\nfrom .non_lazy_tensor import lazify\n\n\ndef _prod(iterable):\n    return reduce(operator.mul, iterable, 1)\n\n\ndef _matmul(lazy_tensors, kp_shape, rhs):\n    output_shape = _matmul_broadcast_shape(kp_shape, rhs.shape)\n    output_batch_shape = output_shape[:-2]\n\n    res = rhs.contiguous().expand(*output_batch_shape, *rhs.shape[-2:])\n    num_cols = rhs.size(-1)\n    for lazy_tensor in lazy_tensors:\n        res = res.view(*output_batch_shape, lazy_tensor.size(-1), -1)\n        factor = lazy_tensor._matmul(res)\n        factor = factor.view(*output_batch_shape, lazy_tensor.size(-2), -1, num_cols).transpose(-3, -2)\n        res = factor.reshape(*output_batch_shape, -1, num_cols)\n    return res\n\n\ndef _t_matmul(lazy_tensors, kp_shape, rhs):\n    kp_t_shape = (*kp_shape[:-2], kp_shape[-1], kp_shape[-2])\n    output_shape = _matmul_broadcast_shape(kp_t_shape, rhs.shape)\n    output_batch_shape = torch.Size(output_shape[:-2])\n\n    res = rhs.contiguous().expand(*output_batch_shape, *rhs.shape[-2:])\n    num_cols = rhs.size(-1)\n    for lazy_tensor in lazy_tensors:\n        res = res.view(*output_batch_shape, lazy_tensor.size(-2), -1)\n        factor = lazy_tensor._t_matmul(res)\n        factor = factor.view(*output_batch_shape, lazy_tensor.size(-1), -1, num_cols).transpose(-3, -2)\n        res = factor.reshape(*output_batch_shape, -1, num_cols)\n    return res\n\n\nclass KroneckerProductLazyTensor(LazyTensor):\n    def __init__(self, *lazy_tensors):\n        try:\n            lazy_tensors = tuple(lazify(lazy_tensor) for lazy_tensor in lazy_tensors)\n        except TypeError:\n            raise RuntimeError(""KroneckerProductLazyTensor is intended to wrap lazy tensors."")\n        for prev_lazy_tensor, curr_lazy_tensor in zip(lazy_tensors[:-1], lazy_tensors[1:]):\n            if prev_lazy_tensor.batch_shape != curr_lazy_tensor.batch_shape:\n                raise RuntimeError(\n                    ""KroneckerProductLazyTensor expects lazy tensors with the ""\n                    ""same batch shapes. Got {}."".format([lv.batch_shape for lv in lazy_tensors])\n                )\n        super(KroneckerProductLazyTensor, self).__init__(*lazy_tensors)\n        self.lazy_tensors = lazy_tensors\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        row_factor = self.size(-2)\n        col_factor = self.size(-1)\n\n        res = None\n        for lazy_tensor in self.lazy_tensors:\n            sub_row_size = lazy_tensor.size(-2)\n            sub_col_size = lazy_tensor.size(-1)\n\n            row_factor //= sub_row_size\n            col_factor //= sub_col_size\n            sub_res = lazy_tensor._get_indices(\n                (row_index // row_factor).fmod(sub_row_size),\n                (col_index // col_factor).fmod(sub_col_size),\n                *batch_indices,\n            )\n            res = sub_res if res is None else (sub_res * res)\n\n        return res\n\n    def _matmul(self, rhs):\n        is_vec = rhs.ndimension() == 1\n        if is_vec:\n            rhs = rhs.unsqueeze(-1)\n\n        res = _matmul(self.lazy_tensors, self.shape, rhs.contiguous())\n\n        if is_vec:\n            res = res.squeeze(-1)\n        return res\n\n    def _t_matmul(self, rhs):\n        is_vec = rhs.ndimension() == 1\n        if is_vec:\n            rhs = rhs.unsqueeze(-1)\n\n        res = _t_matmul(self.lazy_tensors, self.shape, rhs.contiguous())\n\n        if is_vec:\n            res = res.squeeze(-1)\n        return res\n\n    def _expand_batch(self, batch_shape):\n        return self.__class__(*[lazy_tensor._expand_batch(batch_shape) for lazy_tensor in self.lazy_tensors])\n\n    @cached(name=""size"")\n    def _size(self):\n        left_size = _prod(lazy_tensor.size(-2) for lazy_tensor in self.lazy_tensors)\n        right_size = _prod(lazy_tensor.size(-1) for lazy_tensor in self.lazy_tensors)\n        return torch.Size((*self.lazy_tensors[0].batch_shape, left_size, right_size))\n\n    def _transpose_nonbatch(self):\n        return self.__class__(*(lazy_tensor._transpose_nonbatch() for lazy_tensor in self.lazy_tensors), **self._kwargs)\n'"
gpytorch/lazy/lazy_evaluated_kernel_tensor.py,14,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom .. import beta_features, settings\nfrom ..utils import broadcasting\nfrom ..utils.getitem import _noop_index\nfrom ..utils.memoize import cached\nfrom .lazy_tensor import LazyTensor\nfrom .non_lazy_tensor import lazify\n\n\nclass LazyEvaluatedKernelTensor(LazyTensor):\n    _check_size = False\n\n    def _check_args(self, x1, x2, kernel, last_dim_is_batch=False, **params):\n        if not torch.is_tensor(x1):\n            return ""x1 must be a tensor. Got {}"".format(x1.__class__.__name__)\n        if not torch.is_tensor(x2):\n            return ""x1 must be a tensor. Got {}"".format(x1.__class__.__name__)\n\n    def __init__(self, x1, x2, kernel, last_dim_is_batch=False, **params):\n        super(LazyEvaluatedKernelTensor, self).__init__(\n            x1, x2, kernel=kernel, last_dim_is_batch=last_dim_is_batch, **params\n        )\n        self.kernel = kernel\n        self.x1 = x1\n        self.x2 = x2\n        self.last_dim_is_batch = last_dim_is_batch\n        self.params = params\n\n    @property\n    def dtype(self):\n        return self.kernel.dtype\n\n    @property\n    def device(self):\n        return self.x1.device\n\n    def _expand_batch(self, batch_shape):\n        return self.evaluate_kernel()._expand_batch(batch_shape)\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        x1 = self.x1\n        x2 = self.x2\n        num_outs_per_in = self.kernel.num_outputs_per_input(x1, x2)\n\n        # The row index and col index should exactly correspond to which entries of x1 and x2 we need\n        # So we\'ll basically call x1[*batch_indices, row_index, :], x2[*batch_indices, col_index, :]\n\n        # However - if we have multiple outputs per input, then the indices won\'t directly\n        # correspond to the entries of row/col. We\'ll have to do a little pre-processing\n        if num_outs_per_in != 1:\n            if not isinstance(x1, slice) or not isinstance(x2, slice):\n                # It\'s too complicated to deal with tensor indices in this case - we\'ll use the super method\n                return self.evaluate_kernel()._getitem(row_index, col_index, *batch_indices)\n\n            # Now we know that x1 and x2 are slices\n            # Let\'s make sure that the slice dimensions perfectly correspond with the number of\n            # outputs per input that we have\n            row_start, row_end, row_step = row_index.start, row_index.stop, row_index.step\n            col_start, col_end, col_step = col_index.start, col_index.stop, col_index.step\n            if row_step is not None or col_step is not None:\n                return self.evaluate_kernel()._getitem(row_index, col_index, *batch_indices)\n            if (\n                (row_start % num_outs_per_in)\n                or (col_start % num_outs_per_in)\n                or (row_end % num_outs_per_in)\n                or (col_end % num_outs_per_in)\n            ):\n                return self.evaluate_kernel()._getitem(row_index, col_index, *batch_indices)\n\n            # Otherwise - let\'s divide the slices by the number of outputs per input\n            row_index = slice(row_start // num_outs_per_in, row_end // num_outs_per_in, None)\n            col_index = slice(col_start // num_outs_per_in, col_end // num_outs_per_in, None)\n\n        # Define the index we\'re using for the last index\n        # If the last index corresponds to a batch, then we\'ll use the appropriate batch_index\n        # Otherwise, we\'ll use the _noop_index\n        if self.last_dim_is_batch:\n            *batch_indices, dim_index = batch_indices\n        else:\n            dim_index = _noop_index\n\n        # Get the indices of x1 and x2 that matter for the kernel\n        # Call x1[*batch_indices, row_index, :]\n        try:\n            x1 = x1[(*batch_indices, row_index, dim_index)]\n        # We\'re going to handle multi-batch indexing with a try-catch loop\n        # This way - in the default case, we can avoid doing expansions of x1 which can be timely\n        except IndexError:\n            if isinstance(batch_indices, slice):\n                x1 = x1.expand(1, *self.x1.shape[-2:])[(*batch_indices, row_index, dim_index)]\n            elif isinstance(batch_indices, tuple):\n                if any(not isinstance(bi, slice) for bi in batch_indices):\n                    raise RuntimeError(\n                        f""Attempting to tensor index a non-batch matrix\'s batch dimensions. ""\n                        ""Got batch index {batch_indices} but my shape was {self.shape}""\n                    )\n                x1 = x1.expand(*([1] * len(batch_indices)), *self.x1.shape[-2:])\n                x1 = x1[(*batch_indices, row_index, dim_index)]\n\n        # Call x2[*batch_indices, col_index, :]\n        try:\n            x2 = x2[(*batch_indices, col_index, dim_index)]\n        # We\'re going to handle multi-batch indexing with a try-catch loop\n        # This way - in the default case, we can avoid doing expansions of x1 which can be timely\n        except IndexError:\n            if isinstance(batch_indices, slice):\n                x2 = x2.expand(1, *self.x2.shape[-2:])[(*batch_indices, col_index, dim_index)]\n            elif isinstance(batch_indices, tuple):\n                if any([not isinstance(bi, slice) for bi in batch_indices]):\n                    raise RuntimeError(\n                        f""Attempting to tensor index a non-batch matrix\'s batch dimensions. ""\n                        ""Got batch index {batch_indices} but my shape was {self.shape}""\n                    )\n                x2 = x2.expand(*([1] * len(batch_indices)), *self.x2.shape[-2:])\n                x2 = x2[(*batch_indices, col_index, dim_index)]\n\n        if len(batch_indices) == 0 or all(ind == slice(None, None, None) for ind in batch_indices):\n            new_kernel = self.kernel  # Avoid unnecessary copying when we aren\'t explicitly indexing batch dims\n        else:\n            new_kernel = self.kernel.__getitem__(batch_indices)\n\n        # Now construct a kernel with those indices\n        return self.__class__(x1, x2, kernel=new_kernel, last_dim_is_batch=self.last_dim_is_batch, **self.params)\n\n    def _matmul(self, rhs):\n        # This _matmul is defined computes the kernel in chunks\n        # It is only used when we are using kernel checkpointing\n        # It won\'t be called if checkpointing is off\n        x1 = self.x1\n        x2 = self.x2\n\n        split_size = beta_features.checkpoint_kernel.value()\n        if not split_size:\n            raise RuntimeError(\n                ""Should not have ended up in LazyEvaluatedKernelTensor._matmul without kernel checkpointing. ""\n                ""This is probably a bug in GPyTorch.""\n            )\n\n        with torch.no_grad(), settings.lazily_evaluate_kernels(False):\n            sub_x1s = torch.split(x1, split_size, dim=-2)\n            res = []\n            for sub_x1 in sub_x1s:\n                sub_kernel_matrix = lazify(\n                    self.kernel(sub_x1, x2, diag=False, last_dim_is_batch=self.last_dim_is_batch, **self.params)\n                )\n                res.append(sub_kernel_matrix._matmul(rhs))\n\n            res = torch.cat(res, dim=-2)\n            return res\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        # This _quad_form_derivative computes the kernel in chunks\n        # It is only used when we are using kernel checkpointing\n        # It won\'t be called if checkpointing is off\n        split_size = beta_features.checkpoint_kernel.value()\n        if not split_size:\n            raise RuntimeError(\n                ""Should not have ended up in LazyEvaluatedKernelTensor._quad_form_derivative without kernel ""\n                ""checkpointing. This is probably a bug in GPyTorch.""\n            )\n\n        x1 = self.x1.detach().requires_grad_(True)\n        x2 = self.x2.detach().requires_grad_(True)\n\n        # Break objects into chunks\n        sub_x1s = torch.split(x1, split_size, dim=-2)\n        sub_left_vecss = torch.split(left_vecs, split_size, dim=-2)\n        # Compute the gradient in chunks\n        for sub_x1, sub_left_vecs in zip(sub_x1s, sub_left_vecss):\n            sub_x1.detach_().requires_grad_(True)\n            with torch.enable_grad(), settings.lazily_evaluate_kernels(False):\n                sub_kernel_matrix = lazify(\n                    self.kernel(sub_x1, x2, diag=False, last_dim_is_batch=self.last_dim_is_batch, **self.params)\n                )\n            sub_grad_outputs = tuple(sub_kernel_matrix._quad_form_derivative(sub_left_vecs, right_vecs))\n            sub_kernel_outputs = tuple(sub_kernel_matrix.representation())\n            torch.autograd.backward(sub_kernel_outputs, sub_grad_outputs)\n\n        x1.grad = torch.cat([sub_x1.grad.data for sub_x1 in sub_x1s], dim=-2)\n        return x1.grad, x2.grad\n\n    @cached(name=""size"")\n    def _size(self):\n        if settings.debug.on():\n            if hasattr(self.kernel, ""size""):\n                raise RuntimeError(""Kernels must define `num_outputs_per_input` and should not define `size`"")\n\n        x1 = self.x1\n        x2 = self.x2\n        num_outputs_per_input = self.kernel.num_outputs_per_input(x1, x2)\n        num_rows = x1.size(-2) * num_outputs_per_input\n        num_cols = x2.size(-2) * num_outputs_per_input\n\n        # Default case - when we\'re not using broadcasting\n        # We write this case special for efficiency\n        if x1.shape[:-2] == x2.shape[:-2] and x1.shape[:-2] == self.kernel.batch_shape:\n            expected_size = self.kernel.batch_shape + torch.Size((num_rows, num_cols))\n\n        # When we\'re using broadcasting\n        else:\n            expected_size = broadcasting._matmul_broadcast_shape(\n                torch.Size([*x1.shape[:-2], num_rows, x1.size(-1)]),\n                torch.Size([*x2.shape[:-2], x2.size(-1), num_cols]),\n                error_msg=""x1 and x2 were not broadcastable to a proper kernel shape. ""\n                ""Got x1.shape = {} and x2.shape = {}"".format(str(x1.shape), str(x2.shape)),\n            )\n            expected_size = (\n                broadcasting._mul_broadcast_shape(\n                    expected_size[:-2],\n                    self.kernel.batch_shape,\n                    error_msg=(\n                        f""x1 and x2 were not broadcastable with kernel of batch_shape {self.kernel.batch_shape}. ""\n                        f""Got x1.shape = {x1.shape} and x2.shape = {x2.shape}""\n                    ),\n                )\n                + expected_size[-2:]\n            )\n\n        # Handle when the last dim is batch\n        if self.last_dim_is_batch:\n            expected_size = expected_size[:-2] + x1.shape[-1:] + expected_size[-2:]\n        return expected_size\n\n    def _transpose_nonbatch(self):\n        return self.__class__(\n            self.x2, self.x1, kernel=self.kernel, last_dim_is_batch=self.last_dim_is_batch, **self.params\n        )\n\n    def add_jitter(self, jitter_val=1e-3):\n        return self.evaluate_kernel().add_jitter(jitter_val)\n\n    def _unsqueeze_batch(self, dim):\n        return self.evaluate_kernel()._unsqueeze_batch(dim)\n\n    @cached(name=""kernel_diag"")\n    def diag(self):\n        """"""\n        Getting the diagonal of a kernel can be handled more efficiently by\n        transposing the batch and data dimension before calling the kernel.\n        Implementing it this way allows us to compute predictions more efficiently\n        in cases where only the variances are required.\n        """"""\n        from ..kernels import Kernel\n\n        x1 = self.x1\n        x2 = self.x2\n\n        res = super(Kernel, self.kernel).__call__(\n            x1, x2, diag=True, last_dim_is_batch=self.last_dim_is_batch, **self.params\n        )\n\n        # Now we\'ll make sure that the shape we\'re getting from diag makes sense\n        if settings.debug.on():\n            expected_shape = self.shape[:-1]\n            if res.shape != expected_shape:\n                raise RuntimeError(\n                    ""The kernel {} is not equipped to handle and diag. Expected size {}. ""\n                    ""Got size {}"".format(self.__class__.__name__, expected_shape, res.shape)\n                )\n\n        if isinstance(res, LazyTensor):\n            res = res.evaluate()\n        return res.view(self.shape[:-1]).contiguous()\n\n    @cached(name=""kernel_eval"")\n    def evaluate_kernel(self):\n        """"""\n        NB: This is a meta LazyTensor, in the sense that evaluate can return\n        a LazyTensor if the kernel being evaluated does so.\n        """"""\n        x1 = self.x1\n        x2 = self.x2\n\n        with settings.lazily_evaluate_kernels(False):\n            temp_active_dims = self.kernel.active_dims\n            self.kernel.active_dims = None\n            res = self.kernel(x1, x2, diag=False, last_dim_is_batch=self.last_dim_is_batch, **self.params)\n            self.kernel.active_dims = temp_active_dims\n\n        # Check the size of the output\n        if settings.debug.on():\n            if res.shape != self.shape:\n                raise RuntimeError(\n                    f""The expected shape of the kernel was {self.shape}, but got {res.shape}. ""\n                    ""This is likely a bug in GPyTorch.""\n                )\n\n        return lazify(res)\n\n    @cached\n    def evaluate(self):\n        return self.evaluate_kernel().evaluate()\n\n    def repeat(self, *repeats):\n        if len(repeats) == 1 and hasattr(repeats[0], ""__iter__""):\n            repeats = repeats[0]\n        *batch_repeat, row_repeat, col_repeat = repeats\n\n        x1 = self.x1.repeat(*batch_repeat, row_repeat, 1)\n        x2 = self.x2.repeat(*batch_repeat, col_repeat, 1)\n        return self.__class__(x1, x2, kernel=self.kernel, last_dim_is_batch=self.last_dim_is_batch, **self.params)\n\n    def representation(self):\n        # If we\'re checkpointing the kernel, we\'ll use chunked _matmuls defined in LazyEvaluatedKernelTensor\n        if beta_features.checkpoint_kernel.value():\n            return super().representation()\n        # Otherwise, we\'ll evaluate the kernel (or at least its LazyTensor representation) and use its\n        # representation\n        else:\n            return self.evaluate_kernel().representation()\n\n    def representation_tree(self):\n        # If we\'re checkpointing the kernel, we\'ll use chunked _matmuls defined in LazyEvaluatedKernelTensor\n        if beta_features.checkpoint_kernel.value():\n            return super().representation_tree()\n        # Otherwise, we\'ll evaluate the kernel (or at least its LazyTensor representation) and use its\n        # representation\n        else:\n            return self.evaluate_kernel().representation_tree()\n\n    def __getitem__(self, index):\n        """"""\n        Supports subindexing of the matrix this LazyTensor represents. This may return either another\n        :obj:`gpytorch.lazy.LazyTensor` or a :obj:`torch.tensor` depending on the exact implementation.\n        """"""\n        # Process the index\n        index = index if isinstance(index, tuple) else (index,)\n        # Special case for the most common case: [..., slice, slice]\n        if len(index) == 3 and index[0] is Ellipsis and isinstance(index[1], slice) and isinstance(index[2], slice):\n            _, row_index, col_index = index\n            batch_indices = [slice(None, None, None)] * (self.dim() - 2)\n            return self._getitem(row_index, col_index, *batch_indices)\n        else:\n            return super().__getitem__(index)\n'"
gpytorch/lazy/lazy_tensor.py,124,"b'#!/usr/bin/env python3\n\nimport math\nimport warnings\nfrom abc import ABC, abstractmethod\n\nimport torch\n\nimport gpytorch\n\nfrom .. import settings, utils\nfrom ..functions._inv_matmul import InvMatmul\nfrom ..functions._inv_quad import InvQuad\nfrom ..functions._inv_quad_log_det import InvQuadLogDet\nfrom ..functions._matmul import Matmul\nfrom ..functions._root_decomposition import RootDecomposition\nfrom ..utils.broadcasting import _matmul_broadcast_shape, _mul_broadcast_shape\nfrom ..utils.cholesky import psd_safe_cholesky\nfrom ..utils.deprecation import _deprecate_renamed_methods\nfrom ..utils.getitem import _compute_getitem_size, _convert_indices_to_tensors, _is_noop_index, _noop_index\nfrom ..utils.memoize import add_to_cache, cached\nfrom ..utils.warnings import NumericalWarning\nfrom .lazy_tensor_representation_tree import LazyTensorRepresentationTree\n\n\nclass LazyTensor(ABC):\n    r""""""\n    Base class for LazyTensors in GPyTorch.\n\n    In GPyTorch, nearly all covariance matrices for Gaussian processes are handled internally as some variety of\n    LazyTensor. A LazyTensor is an object that represents a tensor object, similar to :class:`torch.tensor`, but\n    typically differs in two ways:\n\n    #. A tensor represented by a LazyTensor can typically be represented more efficiently than storing a full matrix.\n       For example, a LazyTensor representing :math:`K=XX^{\\top}` where :math:`K` is :math:`n \\times n` but\n       :math:`X` is :math:`n \\times d` might store :math:`X` instead of :math:`K` directly.\n    #. A LazyTensor typically defines a matmul routine that performs :math:`KM` that is more efficient than storing\n       the full matrix. Using the above example, performing :math:`KM=X(X^{\\top}M)` requires only :math:`O(nd)` time,\n       rather than the :math:`O(n^2)` time required if we were storing :math:`K` directly.\n\n    In order to define a new LazyTensor class that can be used as a covariance matrix in GPyTorch, a user must define\n    at a minimum the following methods (in each example, :math:`K` denotes the matrix that the LazyTensor represents)\n\n    * :func:`~gpytorch.lazy.LazyTensor._matmul`, which performs a matrix multiplication :math:`KM`\n    * :func:`~gpytorch.lazy.LazyTensor._size`, which returns a :class:`torch.Size` containing the dimensions of\n      :math:`K`.\n    * :func:`~gpytorch.lazy.LazyTensor._transpose_nonbatch`, which returns a transposed version of the LazyTensor\n\n    In addition to these, the following methods should be implemented for maximum efficiency\n\n    * :func:`~gpytorch.lazy.LazyTensor._quad_form_derivative`, which computes the derivative of a quadratic form\n      with the LazyTensor (e.g. :math:`d (a^T X b) / dX`).\n    * :func:`~gpytorch.lazy.LazyTensor._get_indices`, which returns a :class:`torch.Tensor` containing elements that\n      are given by various tensor indices.\n    * :func:`~gpytorch.lazy.LazyTensor._expand_batch`, which expands the batch dimensions of LazyTensors.\n    * :func:`~gpytorch.lazy.LazyTensor._check_args`, which performs error checking on the arguments supplied to the\n      LazyTensor constructor.\n\n    In addition to these, a LazyTensor *may* need to define the following functions if it does anything interesting\n    with the batch dimensions (e.g. sums along them, adds additional ones, etc):\n    :func:`~gpytorch.lazy.LazyTensor._unsqueeze_batch`, :func:`~gpytorch.lazy.LazyTensor._getitem`, and\n    :func:`~gpytorch.lazy.LazyTensor._permute_batch`.\n    See the documentation for these methods for details.\n\n    .. note::\n        The base LazyTensor class provides default implementations of many other operations in order to mimic the\n        behavior of a standard tensor as closely as possible. For example, we provide default implementations of\n        :func:`~gpytorch.lazy.LazyTensor.__getitem__`, :func:`~gpytorch.lazy.LazyTensor.__add__`, etc that either\n        make use of other lazy tensors or exploit the functions that **must** be defined above.\n\n        Rather than overriding the public methods, we recommend that you override the private versions associated\n        with these methods (e.g. - write a custom `_getitem` verses a custom `__getitem__`). This is because the\n        public methods do quite a bit of error checking and casing that doesn\'t need to be repeated.\n\n    .. note::\n        LazyTensors are designed by default to optionally represent batches of matrices. Thus, the size of a\n        LazyTensor may be (for example) :math:`b \\times n \\times n`. Many of the methods are designed to efficiently\n        operate on these batches if present.\n    """"""\n\n    def _check_args(self, *args, **kwargs):\n        """"""\n        (Optional) run checks to see that input arguments and kwargs are valid\n\n        Return:\n            None (if all checks pass) or str (error message to raise)\n        """"""\n        return None\n\n    def __init__(self, *args, **kwargs):\n        if settings.debug.on():\n            err = self._check_args(*args, **kwargs)\n            if err is not None:\n                raise ValueError(err)\n\n        self._args = args\n        self._kwargs = kwargs\n\n    ####\n    # The following methods need to be defined by the LazyTensor\n    ####\n    @abstractmethod\n    def _matmul(self, rhs):\n        """"""\n        Performs a matrix multiplication :math:`KM` with the matrix :math:`K` that this LazyTensor represents. Should\n        behave as :func:`torch.matmul`. If the LazyTensor represents a batch of matrices, this method should therefore\n        operate in batch mode as well.\n\n        ..note::\n            This method is intended to be used only internally by various Functions that support backpropagation\n            (e.g., :class:`gpytorch.functions.Matmul`). Once this method is defined, it is strongly recommended that\n            one use :func:`~gpytorch.lazy.LazyTensor.matmul` instead, which makes use of this method properly.\n\n        Args:\n            rhs (:obj:`torch.tensor`): the matrix :math:`M` to multiply with.\n\n        Returns:\n            :obj:`torch.tensor`: matrix * rhs\n        """"""\n        raise NotImplementedError(""The class {} requires a _matmul function!"".format(self.__class__.__name__))\n\n    @abstractmethod\n    def _size(self):\n        """"""\n        Returns the size of the resulting Tensor that the lazy tensor represents.\n\n        ..note::\n            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.size`,\n            which does some additional work. Calling this method directly is discouraged.\n\n        Returns:\n            :obj:`torch.Size`: The size of the matrix :math:`K` represented by this LazyTensor\n        """"""\n        raise NotImplementedError(""The class {} requires a _size function!"".format(self.__class__.__name__))\n\n    @abstractmethod\n    def _transpose_nonbatch(self):\n        """"""\n        Transposes non-batch dimensions (e.g. last two)\n        Implement this method, rather than transpose() or t().\n\n        ..note::\n            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.transpose`, which\n            does some additional work. Calling this method directly is discouraged.\n        """"""\n        raise NotImplementedError(\n            ""The class {} requires a _transpose_nonbatch function!"".format(self.__class__.__name__)\n        )\n\n    ####\n    # The following methods MIGHT have be over-written by LazyTensor subclasses\n    # if the LazyTensor does weird things with the batch dimensions\n    ####\n    def _permute_batch(self, *dims):\n        """"""\n        Permute the batch dimensions.\n        This probably won\'t have to be overwritten by LazyTensors, unless they use batch dimensions\n        in a special way (e.g. BlockDiagLazyTensor, SumBatchLazyTensor)\n\n        ..note::\n            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.unsqueeze`,\n            which does some additional work. Calling this method directly is discouraged.\n\n        Args:\n            dims (tuple of ints):\n                The new order for the `self.dim() - 2` dimensions.\n                It WILL contain each of the positive batch dimensions exactly once.\n        """"""\n        components = []\n        for component in self._args:\n            if torch.is_tensor(component):\n                extra_dims = range(len(dims), component.dim())\n                components.append(component.permute(*dims, *extra_dims))\n            elif isinstance(component, LazyTensor):\n                components.append(component._permute_batch(*dims))\n            else:\n                components.append(component)\n\n        res = self.__class__(*components, **self._kwargs)\n        return res\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        """"""\n        Supports subindexing of the matrix this LazyTensor represents.\n\n        The indices passed into this method will either be:\n            Tensor indices\n            Slices\n\n        ..note::\n            LazyTensor.__getitem__ uses this as a helper method. If you are writing your own custom LazyTensor,\n            override this method rather than __getitem__ (so that you don\'t have to repeat the extra work)\n\n        ..note::\n            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.__getitem__`,\n            which does some additional work. Calling this method directly is discouraged.\n\n        This method has a number of restrictions on the type of arguments that are passed in to reduce\n        the complexity of __getitem__ calls in PyTorch. In particular:\n            - This method only accepts slices and tensors for the row/column indices (no ints)\n            - The row and column dimensions don\'t dissapear (e.g. from Tensor indexing). These cases are\n              handled by the `_getindices` method\n\n        Args:\n            :attr:`row_index` (slice, Tensor):\n                Index for the row of the LazyTensor\n            :attr:`col_index` (slice, Tensor):\n                Index for the col of the LazyTensor\n            :attr:`batch_indices` (tuple of slice, int, Tensor):\n                Indices for the batch dimensions\n\n        Returns:\n            `LazyTensor`\n        """"""\n        # Special case: if both row and col are not indexed, then we are done\n        if _is_noop_index(row_index) and _is_noop_index(col_index):\n            if len(batch_indices):\n                components = [component[batch_indices] for component in self._args]\n                res = self.__class__(*components, **self._kwargs)\n                return res\n            else:\n                return self\n\n        # Normal case: we have to do some processing on either the rows or columns\n        # We will handle this through ""interpolation""\n        row_interp_indices = torch.arange(0, self.size(-2), dtype=torch.long, device=self.device).view(-1, 1)\n        row_interp_indices = row_interp_indices.expand(*self.batch_shape, -1, 1)\n        row_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self.device).expand_as(row_interp_indices)\n\n        col_interp_indices = torch.arange(0, self.size(-1), dtype=torch.long, device=self.device).view(-1, 1)\n        col_interp_indices = col_interp_indices.expand(*self.batch_shape, -1, 1)\n        col_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self.device).expand_as(col_interp_indices)\n\n        # Construct interpolated LazyTensor\n        from . import InterpolatedLazyTensor\n\n        res = InterpolatedLazyTensor(self, row_interp_indices, row_interp_values, col_interp_indices, col_interp_values)\n        return res._getitem(row_index, col_index, *batch_indices)\n\n    def _unsqueeze_batch(self, dim):\n        """"""\n        Unsqueezes a batch dimension (positive-indexed only)\n        This probably won\'t have to be overwritten by LazyTensors, unless they use batch dimensions\n        in a special way (e.g. BlockDiagLazyTensor, SumBatchLazyTensor)\n\n        ..note::\n            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.unsqueeze`,\n            which does some additional work. Calling this method directly is discouraged.\n        """"""\n        components = [component.unsqueeze(dim) for component in self._args]\n        res = self.__class__(*components, **self._kwargs)\n        return res\n\n    ####\n    # The following methods PROBABLY should be over-written by LazyTensor subclasses for efficiency\n    ####\n    def _expand_batch(self, batch_shape):\n        """"""\n        Expands along batch dimensions.\n\n        ..note::\n            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.expand`,\n            which does some additional work. Calling this method directly is discouraged.\n        """"""\n        current_shape = torch.Size([1 for _ in range(len(batch_shape) - self.dim() - 2)] + list(self.batch_shape))\n        batch_repeat = torch.Size(\n            [expand_size // current_size for expand_size, current_size in zip(batch_shape, current_shape)]\n        )\n        return self.repeat(*batch_repeat, 1, 1)\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        """"""\n        This method selects elements from the LazyTensor based on tensor indices for each dimension.\n        All indices are tensor indices that are broadcastable.\n        There will be exactly one index per dimension of the LazyTensor\n\n        ..note::\n            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.__getitem__`,\n            which does some additional work. Calling this method directly is discouraged.\n\n        Args:\n            row_index (LongTensor): indices to select from row of LazyTensor\n            row_index (LongTensor): indices to select from col of LazyTensor\n            batch_indices (tuple LongTensor): indices to select from batch dimensions.\n\n        Returns:\n            Tensor (size determined by broadcasted shape of indices) of selected values\n        """"""\n        final_shape = _mul_broadcast_shape(*(index.shape for index in batch_indices), row_index.shape, col_index.shape)\n        row_index = row_index.expand(final_shape)\n        col_index = col_index.expand(final_shape)\n        batch_indices = tuple(index.expand(final_shape) for index in batch_indices)\n\n        base_lazy_tensor = self._getitem(_noop_index, _noop_index, *batch_indices)._expand_batch(final_shape)\n\n        # Create some interoplation indices and values\n        row_interp_indices = torch.arange(0, self.size(-2), dtype=torch.long, device=self.device)\n        row_interp_indices = row_interp_indices[row_index].unsqueeze_(-1).unsqueeze_(-1)\n        row_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self.device).expand_as(row_interp_indices)\n\n        col_interp_indices = torch.arange(0, self.size(-1), dtype=torch.long, device=self.device)\n        col_interp_indices = col_interp_indices[col_index].unsqueeze_(-1).unsqueeze_(-1)\n        col_interp_values = torch.tensor(1.0, dtype=self.dtype, device=self.device).expand_as(col_interp_indices)\n\n        # Construct interpolated LazyTensor\n        from . import InterpolatedLazyTensor\n\n        res = (\n            InterpolatedLazyTensor(\n                base_lazy_tensor, row_interp_indices, row_interp_values, col_interp_indices, col_interp_values\n            )\n            .evaluate()\n            .squeeze(-2)\n            .squeeze(-1)\n        )\n        return res\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        """"""\n        Given u (left_vecs) and v (right_vecs),\n        Computes the derivatives of (u^t K v) w.r.t. K\n\n        ..note::\n            This method is intended to be used only internally by various Functions that support backpropagation.\n            For example, this method is used internally by :func:`~gpytorch.lazy.LazyTensor.inv_quad_logdet`. It is\n            not likely that users will need to call this method directly.\n\n        Returns:\n            :obj:`torch.tensor`: derivative with respect to the arguments that are actually used to represent this\n                                   this LazyTensor.\n        """"""\n        from collections import deque\n\n        args = tuple(self.representation())\n        args_with_grads = tuple(arg for arg in args if arg.requires_grad)\n\n        # Easy case: if we don\'t require any gradients, then just return!\n        if not len(args_with_grads):\n            return tuple(None for _ in args)\n\n        # Normal case: we\'ll use the autograd to get us a derivative\n        with torch.autograd.enable_grad():\n            loss = (left_vecs * self._matmul(right_vecs)).sum()\n            loss.requires_grad_(True)\n            actual_grads = deque(torch.autograd.grad(loss, args_with_grads, allow_unused=True))\n\n        # Now make sure that the object we return has one entry for every item in args\n        grads = []\n        for arg in args:\n            if arg.requires_grad:\n                grads.append(actual_grads.popleft())\n            else:\n                grads.append(None)\n\n        return tuple(grads)\n\n    ####\n    # Class definitions\n    ####\n    _check_size = True\n\n    ####\n    # Standard LazyTensor methods\n    ####\n    @property\n    def _args(self):\n        return self._args_memo\n\n    @_args.setter\n    def _args(self, args):\n        self._args_memo = args\n\n    def _approx_diag(self):\n        """"""\n        (Optional) returns an (approximate) diagonal of the matrix\n\n        Sometimes computing an exact diagonal is a bit computationally slow\n        When we don\'t need an exact diagonal (e.g. for the pivoted cholesky\n        decomposition, this function is called\n\n        Defaults to calling the exact diagonal function\n\n        Returns:\n            tensor: - the diagonal (or batch of diagonals)\n        """"""\n        return self.diag()\n\n    @cached(name=""cholesky"")\n    def _cholesky(self):\n        """"""\n        (Optional) Cholesky-factorizes the LazyTensor\n\n        ..note::\n            This method is used as an internal helper. Calling this method directly is discouraged.\n\n        Returns:\n            (LazyTensor) Cholesky factor\n        """"""\n        from .non_lazy_tensor import NonLazyTensor\n        from .keops_lazy_tensor import KeOpsLazyTensor\n\n        evaluated_kern_mat = self.evaluate_kernel()\n\n        if any(isinstance(sub_mat, KeOpsLazyTensor) for sub_mat in evaluated_kern_mat._args):\n            raise RuntimeError(""Cannot run Cholesky with KeOps: it will either be really slow or not work."")\n\n        evaluated_mat = evaluated_kern_mat.evaluate()\n\n        # if the tensor is a scalar, we can just take the square root\n        if evaluated_mat.size(-1) == 1:\n            return NonLazyTensor(evaluated_mat.clamp_min(0.0).sqrt())\n\n        # contiguous call is necessary here\n        cholesky = psd_safe_cholesky(evaluated_mat, jitter=settings.cholesky_jitter.value()).contiguous()\n        return NonLazyTensor(cholesky)\n\n    def _cholesky_solve(self, rhs):\n        """"""\n        (Optional) Assuming that `self` is a Cholesky factor, computes the cholesky solve\n\n        ..note::\n            This method is used as an internal helper. Calling this method directly is discouraged.\n\n        Returns:\n            (LazyTensor) Cholesky factor\n        """"""\n        return torch.cholesky_solve(rhs, self.evaluate())\n\n    def _inv_matmul_preconditioner(self):\n        """"""\n        (Optional) define a preconditioner that can be used for linear systems, but not necessarily\n        for log determinants. By default, this can call :meth:`~gpytorch.lazy.LazyTensor._preconditioner`.\n\n        Returns:\n            function: a function on x which performs P^{-1}(x)\n        """"""\n        base_precond, _, _ = self._preconditioner()\n\n        if base_precond is not None:\n            return base_precond\n        elif gpytorch.beta_features.default_preconditioner.on():\n            if hasattr(self, ""_default_preconditioner_cache""):\n                U, S, V = self._default_preconditioner_cache\n            else:\n                precond_basis_size = min(gpytorch.settings.max_preconditioner_size.value(), self.size(-1))\n                random_basis = torch.randn(\n                    self.batch_shape + torch.Size((self.size(-2), precond_basis_size)),\n                    device=self.device,\n                    dtype=self.dtype,\n                )\n                projected_mat = self._matmul(random_basis)\n                proj_q = torch.qr(projected_mat)\n                orthog_projected_mat = self._matmul(proj_q).transpose(-2, -1)\n                U, S, V = torch.svd(orthog_projected_mat)\n                U = proj_q.matmul(U)\n\n                self._default_preconditioner_cache = (U, S, V)\n\n            def preconditioner(v):\n                res = V.transpose(-2, -1).matmul(v)\n                res = (1 / S).unsqueeze(-1) * res\n                res = U.matmul(res)\n                return res\n\n            return preconditioner\n        else:\n            return None\n\n    def _mul_constant(self, other):\n        """"""\n        Multiplies the LazyTensor by a costant.\n\n        ..note::\n            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.mul`,\n            which does some additional work. Calling this method directly is discouraged.\n\n        Returns:\n            :obj:`gpytorch.lazy.LazyTensor`\n        """"""\n        from .constant_mul_lazy_tensor import ConstantMulLazyTensor\n\n        return ConstantMulLazyTensor(self, other)\n\n    def _mul_matrix(self, other):\n        """"""\n        Multiplies the LazyTensor by a (batch of) matrices.\n\n        ..note::\n            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.mul`,\n            which does some additional work. Calling this method directly is discouraged.\n\n        Returns:\n            :obj:`gpytorch.lazy.LazyTensor`\n        """"""\n        from .non_lazy_tensor import NonLazyTensor\n        from .mul_lazy_tensor import MulLazyTensor\n\n        self = self.evaluate_kernel()\n        other = other.evaluate_kernel()\n        if isinstance(self, NonLazyTensor) or isinstance(other, NonLazyTensor):\n            return NonLazyTensor(self.evaluate() * other.evaluate())\n        else:\n            left_lazy_tensor = self if self._root_decomposition_size() < other._root_decomposition_size() else other\n            right_lazy_tensor = other if left_lazy_tensor is self else self\n            return MulLazyTensor(left_lazy_tensor.root_decomposition(), right_lazy_tensor.root_decomposition())\n\n    def _preconditioner(self):\n        """"""\n        (Optional) define a preconditioner (P) for linear conjugate gradients\n\n        Returns:\n            function: a function on x which performs P^{-1}(x)\n            scalar: the log determinant of P\n        """"""\n        return None, None, None\n\n    def _probe_vectors_and_norms(self):\n        return None, None\n\n    def _prod_batch(self, dim):\n        """"""\n        Multiply the LazyTensor across a batch dimension (supplied as a positive number).\n\n        ..note::\n            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.prod`,\n            which does some additional work. Calling this method directly is discouraged.\n\n        Returns:\n            :obj:`gpytorch.lazy.LazyTensor`\n        """"""\n        from .mul_lazy_tensor import MulLazyTensor\n        from .root_lazy_tensor import RootLazyTensor\n\n        if self.size(dim) == 1:\n            return self.squeeze(dim)\n\n        roots = self.root_decomposition().root.evaluate()\n        num_batch = roots.size(dim)\n\n        while True:\n            # Take care of extra roots (odd roots), if they exist\n            if num_batch % 2:\n                shape = list(roots.shape)\n                shape[dim] = 1\n                extra_root = torch.full(\n                    shape, dtype=self.dtype, device=self.device, fill_value=(1.0 / math.sqrt(self.size(-2)))\n                )\n                roots = torch.cat([roots, extra_root], dim)\n                num_batch += 1\n\n            # Divide and conqour\n            # Assumes that there\'s an even number of roots\n            part1_index = [_noop_index] * roots.dim()\n            part1_index[dim] = slice(None, num_batch // 2, None)\n            part1 = roots[tuple(part1_index)].contiguous()\n            part2_index = [_noop_index] * roots.dim()\n            part2_index[dim] = slice(num_batch // 2, None, None)\n            part2 = roots[tuple(part2_index)].contiguous()\n\n            if num_batch // 2 == 1:\n                part1 = part1.squeeze(dim)\n                part2 = part2.squeeze(dim)\n                res = MulLazyTensor(RootLazyTensor(part1), RootLazyTensor(part2))\n                break\n            else:\n                res = MulLazyTensor(RootLazyTensor(part1), RootLazyTensor(part2))\n                roots = res.root_decomposition().root.evaluate()\n                num_batch = num_batch // 2\n\n        return res\n\n    def _root_decomposition(self):\n        """"""\n        Returns the (usually low-rank) root of a lazy tensor of a PSD matrix.\n\n        ..note::\n            This method is used internally by the related function\n            :func:`~gpytorch.lazy.LazyTensor.root_decomposition`, which does some additional work.\n            Calling this method directly is discouraged.\n\n        Returns:\n            (Tensor or LazyTensor): The root of the root decomposition\n        """"""\n        func = RootDecomposition()\n        res, _ = func.apply(\n            self.representation_tree(),\n            self._root_decomposition_size(),\n            self.dtype,\n            self.device,\n            self.batch_shape,\n            self.matrix_shape,\n            True,\n            False,\n            None,\n            *self.representation(),\n        )\n\n        return res\n\n    def _root_decomposition_size(self):\n        """"""\n        This is the inner size of the root decomposition.\n        This is primarily used to determine if it will be cheaper to compute a\n        different root or not\n        """"""\n        return settings.max_root_decomposition_size.value()\n\n    def _root_inv_decomposition(self, initial_vectors=None):\n        """"""\n        Returns the (usually low-rank) inverse root of a lazy tensor of a PSD matrix.\n\n        ..note::\n            This method is used internally by the related function\n            :func:`~gpytorch.lazy.LazyTensor.root_inv_decomposition`, which does some additional work.\n            Calling this method directly is discouraged.\n\n        Returns:\n            (Tensor or LazyTensor): The root of the inverse root decomposition\n        """"""\n        from .root_lazy_tensor import RootLazyTensor\n\n        func = RootDecomposition()\n        roots, inv_roots = func.apply(\n            self.representation_tree(),\n            self._root_decomposition_size(),\n            self.dtype,\n            self.device,\n            self.batch_shape,\n            self.matrix_shape,\n            True,\n            True,\n            initial_vectors,\n            *self.representation(),\n        )\n\n        if initial_vectors is not None and initial_vectors.size(-1) > 1:\n            add_to_cache(self, ""root_decomposition"", RootLazyTensor(roots[0]))\n        else:\n            add_to_cache(self, ""root_decomposition"", RootLazyTensor(roots))\n\n        return inv_roots\n\n    def _solve(self, rhs, preconditioner, num_tridiag=0):\n        return utils.linear_cg(\n            self._matmul,\n            rhs,\n            n_tridiag=num_tridiag,\n            max_iter=settings.max_cg_iterations.value(),\n            max_tridiag_iter=settings.max_lanczos_quadrature_iterations.value(),\n            preconditioner=preconditioner,\n        )\n\n    def _sum_batch(self, dim):\n        """"""\n        Sum the LazyTensor across a batch dimension (supplied as a positive number).\n\n        ..note::\n            This method is used internally by the related function :func:`~gpytorch.lazy.LazyTensor.sum`,\n            which does some additional work. Calling this method directly is discouraged.\n\n        Returns:\n            :obj:`gpytorch.lazy.LazyTensor`\n        """"""\n        from .sum_batch_lazy_tensor import SumBatchLazyTensor\n\n        return SumBatchLazyTensor(self, block_dim=dim)\n\n    def _t_matmul(self, rhs):\n        r""""""\n        Performs a transpose matrix multiplication :math:`K^{\\top}M` with the matrix :math:`K` that this\n        LazyTensor represents.\n\n        Args:\n            rhs (:obj:`torch.tensor`): the matrix :math:`M` to multiply with.\n\n        Returns:\n            :obj:`torch.tensor`: matrix * rhs\n        """"""\n        return self.transpose(-1, -2)._matmul(rhs)\n\n    def add_diag(self, diag):\n        """"""\n        Adds an element to the diagonal of the matrix.\n\n        Args:\n            - diag (Scalar Tensor)\n        """"""\n        from .diag_lazy_tensor import DiagLazyTensor\n        from .added_diag_lazy_tensor import AddedDiagLazyTensor\n\n        if not self.is_square:\n            raise RuntimeError(""add_diag only defined for square matrices"")\n\n        try:\n            expanded_diag = diag.expand(self.shape[:-1])\n        except RuntimeError:\n            raise RuntimeError(\n                ""add_diag for LazyTensor of size {} received invalid diagonal of size {}."".format(\n                    self.shape, diag.shape\n                )\n            )\n\n        return AddedDiagLazyTensor(self, DiagLazyTensor(expanded_diag))\n\n    def add_jitter(self, jitter_val=1e-3):\n        """"""\n        Adds jitter (i.e., a small diagonal component) to the matrix this\n        LazyTensor represents. This could potentially be implemented as a no-op,\n        however this could lead to numerical instabilities, so this should only\n        be done at the user\'s risk.\n        """"""\n        diag = torch.tensor(jitter_val, dtype=self.dtype, device=self.device)\n        return self.add_diag(diag)\n\n    @property\n    def batch_dim(self):\n        """"""\n        Returns the dimension of the shape over which the tensor is batched.\n        """"""\n        return len(self.batch_shape)\n\n    @property\n    def batch_shape(self):\n        """"""\n        Returns the shape over which the tensor is batched.\n        """"""\n        return self.shape[:-2]\n\n    def cholesky(self, upper=False):\n        """"""\n        Cholesky-factorizes the LazyTensor\n\n        Parameters:\n            upper (bool) - upper triangular or lower triangular factor (default: False)\n\n        Returns:\n            (LazyTensor) Cholesky factor (lower triangular)\n        """"""\n        res = self._cholesky()\n        if upper:\n            res = res.transpose(-1, -2)\n        return res\n\n    def clone(self):\n        """"""\n        Clones the LazyTensor (creates clones of all underlying tensors)\n        """"""\n        args = [arg.clone() if hasattr(arg, ""clone"") else arg for arg in self._args]\n        kwargs = {key: val.clone() if hasattr(val, ""clone"") else val for key, val in self._kwargs.items()}\n        return self.__class__(*args, **kwargs)\n\n    def cpu(self):\n        """"""\n        Returns:\n            :obj:`~gpytorch.lazy.LazyTensor`: a new LazyTensor identical to ``self``, but on the CPU.\n        """"""\n        new_args = []\n        new_kwargs = {}\n        for arg in self._args:\n            if hasattr(arg, ""cpu""):\n                new_args.append(arg.cpu())\n            else:\n                new_args.append(arg)\n        for name, val in self._kwargs.items():\n            if hasattr(val, ""cpu""):\n                new_kwargs[name] = val.cpu()\n            else:\n                new_kwargs[name] = val\n        return self.__class__(*new_args, **new_kwargs)\n\n    def cuda(self, device_id=None):\n        """"""\n        This method operates identically to :func:`torch.nn.Module.cuda`.\n\n        Args:\n            device_id (:obj:`str`, optional):\n                Device ID of GPU to use.\n        Returns:\n            :obj:`~gpytorch.lazy.LazyTensor`:\n                a new LazyTensor identical to ``self``, but on the GPU.\n        """"""\n        new_args = []\n        new_kwargs = {}\n        for arg in self._args:\n            if hasattr(arg, ""cuda""):\n                new_args.append(arg.cuda(device_id))\n            else:\n                new_args.append(arg)\n        for name, val in self._kwargs.items():\n            if hasattr(val, ""cuda""):\n                new_kwargs[name] = val.cuda(device_id)\n            else:\n                new_kwargs[name] = val\n        return self.__class__(*new_args, **new_kwargs)\n\n    @property\n    def device(self):\n        return self._args[0].device\n\n    def detach(self):\n        """"""\n        Removes the LazyTensor from the current computation graph.\n        (In practice, this function removes all Tensors that make up the\n        LazyTensor from the computation graph.)\n        """"""\n        return self.clone().detach_()\n\n    def detach_(self):\n        """"""\n        An in-place version of `detach`.\n        """"""\n        for arg in self._args:\n            if hasattr(arg, ""detach""):\n                arg.detach_()\n        for val in self._kwargs.values():\n            if hasattr(val, ""detach""):\n                val.detach_()\n        return self\n\n    def diag(self):\n        r""""""\n        As :func:`torch.diag`, returns the diagonal of the matrix :math:`K` this LazyTensor represents as a vector.\n\n        :rtype: torch.tensor\n        :return: The diagonal of :math:`K`. If :math:`K` is :math:`n \\times n`, this will be a length\n            n vector. If this LazyTensor represents a batch (e.g., is :math:`b \\times n \\times n`), this will be a\n            :math:`b \\times n` matrix of diagonals, one for each matrix in the batch.\n        """"""\n        if settings.debug.on():\n            if not self.is_square:\n                raise RuntimeError(""Diag works on square matrices (or batches)"")\n\n        row_col_iter = torch.arange(0, self.matrix_shape[-1], dtype=torch.long, device=self.device)\n        return self[..., row_col_iter, row_col_iter]\n\n    def dim(self):\n        """"""\n        Alias of :meth:`~gpytorch.lazy.LazyTensor.ndimension`\n        """"""\n        return self.ndimension()\n\n    @property\n    def dtype(self):\n        return self._args[0].dtype\n\n    def expand(self, *sizes):\n        if len(sizes) == 1 and hasattr(sizes, ""__iter__""):\n            sizes = sizes[0]\n        if len(sizes) < 2 or tuple(sizes[-2:]) != self.matrix_shape:\n            raise RuntimeError(\n                ""Invalid expand arguments {}. Currently, repeat only works to create repeated ""\n                ""batches of a 2D LazyTensor."".format(tuple(sizes))\n            )\n        elif all(isinstance(size, int) for size in sizes):\n            shape = torch.Size(sizes)\n        else:\n            raise RuntimeError(""Invalid arguments {} to expand."".format(sizes))\n\n        res = self._expand_batch(batch_shape=shape[:-2])\n        return res\n\n    @cached\n    def evaluate(self):\n        """"""\n        Explicitly evaluates the matrix this LazyTensor represents. This function\n        should return a Tensor storing an exact representation of this LazyTensor.\n        """"""\n        num_rows, num_cols = self.matrix_shape\n\n        if num_rows < num_cols:\n            eye = torch.eye(num_rows, dtype=self.dtype, device=self.device)\n            eye = eye.expand(*self.batch_shape, num_rows, num_rows)\n            res = self.transpose(-1, -2).matmul(eye).transpose(-1, -2).contiguous()\n        else:\n            eye = torch.eye(num_cols, dtype=self.dtype, device=self.device)\n            eye = eye.expand(*self.batch_shape, num_cols, num_cols)\n            res = self.matmul(eye)\n        return res\n\n    def evaluate_kernel(self):\n        """"""\n        Return a new LazyTensor representing the same one as this one, but with\n        all lazily evaluated kernels actually evaluated.\n        """"""\n        return self.representation_tree()(*self.representation())\n\n    def inv_matmul(self, right_tensor, left_tensor=None):\n        r""""""\n        Computes a linear solve (w.r.t self = :math:`A`) with several right hand sides :math:`R`.\n        I.e. computes\n\n        ... math::\n\n            \\begin{equation}\n                A^{-1} R,\n            \\end{equation}\n\n        where :math:`R` is :attr:`right_tensor` and :math:`A` is the LazyTensor.\n\n        If :attr:`left_tensor` is supplied, computes\n\n        ... math::\n\n            \\begin{equation}\n                L A^{-1} R,\n            \\end{equation}\n\n        where :math:`L` is :attr:`left_tensor`. Supplying this can reduce the number of\n        CG calls required.\n\n        Args:\n            - :obj:`torch.tensor` (n x k) - Matrix :math:`R` right hand sides\n            - :obj:`torch.tensor` (m x n) - Optional matrix :math:`L` to perform left multiplication with\n\n        Returns:\n            - :obj:`torch.tensor` - :math:`A^{-1}R` or :math:`LA^{-1}R`.\n        """"""\n        if not self.is_square:\n            raise RuntimeError(\n                ""inv_matmul only operates on (batches of) square (positive semi-definite) LazyTensors. ""\n                ""Got a {} of size {}."".format(self.__class__.__name__, self.size())\n            )\n\n        if self.dim() == 2 and right_tensor.dim() == 1:\n            if self.shape[-1] != right_tensor.numel():\n                raise RuntimeError(\n                    ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(\n                        self.shape, right_tensor.shape\n                    )\n                )\n\n        func = InvMatmul\n        if left_tensor is None:\n            return func.apply(self.representation_tree(), False, right_tensor, *self.representation())\n        else:\n            return func.apply(self.representation_tree(), True, left_tensor, right_tensor, *self.representation())\n\n    def inv_quad(self, tensor, reduce_inv_quad=True):\n        """"""\n        Computes an inverse quadratic form (w.r.t self) with several right hand sides.\n        I.e. computes tr( tensor^T self^{-1} tensor )\n\n        NOTE: Don\'t overwrite this function!\n        Instead, overwrite inv_quad_logdet\n\n        Args:\n            - tensor (tensor nxk) - Vector (or matrix) for inverse quad\n\n        Returns:\n            - tensor - tr( tensor^T (self)^{-1} tensor )\n        """"""\n        if not self.is_square:\n            raise RuntimeError(\n                ""inv_quad only operates on (batches of) square (positive semi-definite) LazyTensors. ""\n                ""Got a {} of size {}."".format(self.__class__.__name__, self.size())\n            )\n\n        if self.dim() == 2 and tensor.dim() == 1:\n            if self.shape[-1] != tensor.numel():\n                raise RuntimeError(\n                    ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(\n                        self.shape, tensor.shape\n                    )\n                )\n        elif self.dim() != tensor.dim():\n            raise RuntimeError(\n                ""LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number ""\n                ""of dimensions."".format(self.shape, tensor.shape)\n            )\n        elif self.shape[-1] != tensor.shape[-2]:\n            raise RuntimeError(\n                ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(\n                    self.shape, tensor.shape\n                )\n            )\n\n        args = (tensor,) + self.representation()\n        func = InvQuad.apply\n        inv_quad_term = func(self.representation_tree(), *args)\n\n        if reduce_inv_quad:\n            inv_quad_term = inv_quad_term.sum(-1)\n        return inv_quad_term\n\n    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):\n        """"""\n        Computes an inverse quadratic form (w.r.t self) with several right hand sides.\n        I.e. computes tr( tensor^T self^{-1} tensor )\n        In addition, computes an (approximate) log determinant of the the matrix\n\n        Args:\n            - tensor (tensor nxk) - Vector (or matrix) for inverse quad\n\n        Returns:\n            - scalar - tr( tensor^T (self)^{-1} tensor )\n            - scalar - log determinant\n        """"""\n        # Special case: use Cholesky to compute these terms\n        if settings.fast_computations.log_prob.off() or (self.size(-1) <= settings.max_cholesky_size.value()):\n            from .chol_lazy_tensor import CholLazyTensor\n\n            cholesky = CholLazyTensor(self.cholesky())\n            return cholesky.inv_quad_logdet(inv_quad_rhs=inv_quad_rhs, logdet=logdet, reduce_inv_quad=reduce_inv_quad)\n\n        # Default: use modified batch conjugate gradients to compute these terms\n        # See NeurIPS 2018 paper: https://arxiv.org/abs/1809.11165\n        if not self.is_square:\n            raise RuntimeError(\n                ""inv_quad_logdet only operates on (batches of) square (positive semi-definite) LazyTensors. ""\n                ""Got a {} of size {}."".format(self.__class__.__name__, self.size())\n            )\n\n        if inv_quad_rhs is not None:\n            if self.dim() == 2 and inv_quad_rhs.dim() == 1:\n                if self.shape[-1] != inv_quad_rhs.numel():\n                    raise RuntimeError(\n                        ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(\n                            self.shape, inv_quad_rhs.shape\n                        )\n                    )\n            elif self.dim() != inv_quad_rhs.dim():\n                raise RuntimeError(\n                    ""LazyTensor (size={}) and right-hand-side Tensor (size={}) should have the same number ""\n                    ""of dimensions."".format(self.shape, inv_quad_rhs.shape)\n                )\n            elif self.batch_shape != inv_quad_rhs.shape[:-2] or self.shape[-1] != inv_quad_rhs.shape[-2]:\n                raise RuntimeError(\n                    ""LazyTensor (size={}) cannot be multiplied with right-hand-side Tensor (size={})."".format(\n                        self.shape, inv_quad_rhs.shape\n                    )\n                )\n\n        args = self.representation()\n        if inv_quad_rhs is not None:\n            args = [inv_quad_rhs] + list(args)\n\n        probe_vectors, probe_vector_norms = self._probe_vectors_and_norms()\n\n        func = InvQuadLogDet.apply\n\n        inv_quad_term, logdet_term = func(\n            self.representation_tree(),\n            self.dtype,\n            self.device,\n            self.matrix_shape,\n            self.batch_shape,\n            (inv_quad_rhs is not None),\n            logdet,\n            probe_vectors,\n            probe_vector_norms,\n            *args,\n        )\n\n        if inv_quad_term.numel() and reduce_inv_quad:\n            inv_quad_term = inv_quad_term.sum(-1)\n        return inv_quad_term, logdet_term\n\n    @property\n    def is_square(self):\n        return self.matrix_shape[0] == self.matrix_shape[1]\n\n    def logdet(self):\n        """"""\n        Computes an (approximate) log determinant of the matrix\n\n        NOTE: Don\'t overwrite this function!\n        Instead, overwrite inv_quad_logdet\n\n        Returns:\n            - scalar: log determinant\n        """"""\n        _, res = self.inv_quad_logdet(inv_quad_rhs=None, logdet=True)\n        return res\n\n    def matmul(self, other):\n        """"""\n        Multiplies self by a matrix\n\n        Args:\n            other (:obj:`torch.tensor`): Matrix or vector to multiply with. Can be either a :obj:`torch.tensor`\n                or a :obj:`gpytorch.lazy.LazyTensor`.\n\n        Returns:\n            :obj:`torch.tensor`: Tensor or LazyTensor containing the result of the matrix multiplication :math:`KM`,\n            where :math:`K` is the (batched) matrix that this :obj:`gpytorch.lazy.LazyTensor` represents, and :math:`M`\n            is the (batched) matrix input to this method.\n        """"""\n        # TODO: Move this check to MatmulLazyTensor and Matmul (so we can pass the shapes through from there)\n        _matmul_broadcast_shape(self.shape, other.shape)\n\n        if isinstance(other, LazyTensor):\n            from .matmul_lazy_tensor import MatmulLazyTensor\n\n            return MatmulLazyTensor(self, other)\n\n        func = Matmul()\n        return func.apply(self.representation_tree(), other, *self.representation())\n\n    @property\n    def matrix_shape(self):\n        """"""\n        Returns the shape of the matrix being represented (without batching).\n        """"""\n        return torch.Size(self.shape[-2:])\n\n    def mul(self, other):\n        """"""\n        Multiplies the matrix by a constant, or elementwise the matrix by another matrix\n\n        Args:\n            other (:obj:`torch.tensor` or :obj:`~gpytorch.lazy.LazyTensor`): constant or matrix to elementwise\n            multiply by.\n\n        Returns:\n            :obj:`gpytorch.lazy.LazyTensor`: Another lazy tensor representing the result of the multiplication. if\n            other was a constant (or batch of constants), this will likely be a\n            :obj:`gpytorch.lazy.ConstantMulLazyTensor`. If other was\n            another matrix, this will likely be a :obj:`gpytorch.lazy.MulLazyTensor`.\n        """"""\n        from .zero_lazy_tensor import ZeroLazyTensor\n        from .non_lazy_tensor import lazify\n\n        if isinstance(other, ZeroLazyTensor):\n            return other\n\n        if not (torch.is_tensor(other) or isinstance(other, LazyTensor)):\n            other = torch.tensor(other, dtype=self.dtype, device=self.device)\n\n        try:\n            _mul_broadcast_shape(self.shape, other.shape)\n        except RuntimeError:\n            raise RuntimeError(\n                ""Cannot multiply LazyTensor of size {} by an object of size {}"".format(self.shape, other.shape)\n            )\n\n        if torch.is_tensor(other):\n            if other.numel() == 1:\n                return self._mul_constant(other.squeeze())\n            elif other.shape[-2:] == torch.Size((1, 1)):\n                return self._mul_constant(other.view(*other.shape[:-2]))\n\n        return self._mul_matrix(lazify(other))\n\n    def ndimension(self):\n        """"""\n        Returns the number of dimensions\n        """"""\n        return len(self.size())\n\n    def numel(self):\n        """"""\n        Returns the number of elements\n        """"""\n        return self.shape.numel()\n\n    def numpy(self):\n        """"""\n        Return self as an evaluated numpy array\n        """"""\n        return self.evaluate().detach().cpu().numpy()\n\n    def permute(self, *dims):\n        num_dims = self.dim()\n        orig_dims = dims\n        dims = tuple(dim if dim >= 0 else dim + num_dims for dim in dims)\n\n        if settings.debug.on():\n            if len(dims) != num_dims:\n                raise RuntimeError(""number of dims don\'t match in permute"")\n            if sorted(set(dims)) != sorted(dims):\n                raise RuntimeError(""repeated dim in permute"")\n\n            for dim, orig_dim in zip(dims, orig_dims):\n                if dim >= num_dims:\n                    raise RuntimeError(\n                        ""Dimension out of range (expected to be in range of [{}, {}], but got ""\n                        ""{}."".format(-num_dims, num_dims - 1, orig_dim)\n                    )\n\n        if dims[-2:] != (num_dims - 2, num_dims - 1):\n            raise ValueError(""At the moment, cannot permute the non-batch dimensions of LazyTensors."")\n\n        return self._permute_batch(*dims[:-2])\n\n    def prod(self, dim=None):\n        """"""\n        For a `b x n x m` LazyTensor, compute the product over the batch dimension.\n\n        The `mul_batch_size` controls whether or not the batch dimension is grouped when multiplying.\n            * `mul_batch_size=None` (default): The entire batch dimension is multiplied. Returns a `n x n` LazyTensor.\n            * `mul_batch_size=k`: Creates `b/k` groups, and muls the `k` entries of this group.\n                (The LazyTensor is reshaped as a `b/k x k x n x m` LazyTensor and the `k` dimension is multiplied over.\n                Returns a `b/k x n x m` LazyTensor.\n\n        Args:\n            :attr:`mul_batch_size` (int or None):\n                Controls the number of groups that are multiplied over (default: None).\n\n        Returns:\n            :obj:`~gpytorch.lazy.LazyTensor`\n\n        Example:\n            >>> lazy_tensor = gpytorch.lazy.NonLazyTensor(torch.tensor([\n                    [[2, 4], [1, 2]],\n                    [[1, 1], [0, -1]],\n                    [[2, 1], [1, 0]],\n                    [[3, 2], [2, -1]],\n                ]))\n            >>> lazy_tensor.mul_batch().evaluate()\n            >>> # Returns: torch.Tensor([[12, 8], [0, 0]])\n            >>> lazy_tensor.mul_batch(mul_batch_size=2)\n            >>> # Returns: torch.Tensor([[[2, 4], [0, -2]], [[6, 2], [2, 0]]])\n        """"""\n        if dim is None:\n            raise ValueError(""At the moment, LazyTensor.prod requires a dim argument (got None)"")\n\n        orig_dim = dim\n        if dim < 0:\n            dim = self.dim() + dim\n        if dim >= len(self.batch_shape):\n            raise ValueError(\n                ""At the moment, LazyTensor.prod only works on batch dimensions. ""\n                ""Got dim={} for LazyTensor of shape {}"".format(orig_dim, self.shape)\n            )\n\n        return self._prod_batch(dim)\n\n    def repeat(self, *sizes):\n        """"""\n        Repeats this tensor along the specified dimensions.\n\n        Currently, this only works to create repeated batches of a 2D LazyTensor.\n        I.e. all calls should be `lazy_tensor.repeat(<size>, 1, 1)`.\n\n        Example:\n            >>> lazy_tensor = gpytorch.lazy.ToeplitzLazyTensor(torch.tensor([4. 1., 0.5]))\n            >>> lazy_tensor.repeat(2, 1, 1).evaluate()\n            tensor([[[4.0000, 1.0000, 0.5000],\n                     [1.0000, 4.0000, 1.0000],\n                     [0.5000, 1.0000, 4.0000]],\n                    [[4.0000, 1.0000, 0.5000],\n                     [1.0000, 4.0000, 1.0000],\n                     [0.5000, 1.0000, 4.0000]]])\n        """"""\n        from .batch_repeat_lazy_tensor import BatchRepeatLazyTensor\n\n        if len(sizes) < 3 or tuple(sizes[-2:]) != (1, 1):\n            raise RuntimeError(\n                ""Invalid repeat arguments {}. Currently, repeat only works to create repeated ""\n                ""batches of a 2D LazyTensor."".format(tuple(sizes))\n            )\n        return BatchRepeatLazyTensor(self, batch_repeat=torch.Size(sizes[:-2]))\n\n    def representation(self):\n        """"""\n        Returns the Tensors that are used to define the LazyTensor\n        """"""\n        representation = []\n        for arg in self._args:\n            if torch.is_tensor(arg):\n                representation.append(arg)\n            elif hasattr(arg, ""representation"") and callable(arg.representation):  # Is it a LazyTensor?\n                representation += list(arg.representation())\n            else:\n                raise RuntimeError(""Representation of a LazyTensor should consist only of Tensors"")\n        return tuple(representation)\n\n    def representation_tree(self):\n        """"""\n        Returns a :obj:`gpytorch.lazy.LazyTensorRepresentationTree` tree object that recursively encodes the\n        representation of this lazy tensor. In particular, if the definition of this lazy tensor depends on other\n        lazy tensors, the tree is an object that can be used to reconstruct the full structure of this lazy tensor,\n        including all subobjects. This is used internally.\n        """"""\n        return LazyTensorRepresentationTree(self)\n\n    @property\n    def requires_grad(self):\n        return any(\n            arg.requires_grad\n            for arg in tuple(self._args) + tuple(self._kwargs.values())\n            if hasattr(arg, ""requires_grad"")\n        )\n\n    @requires_grad.setter\n    def requires_grad(self, val):\n        for arg in self._args:\n            if hasattr(arg, ""requires_grad""):\n                if arg.dtype in (torch.float, torch.double, torch.half):\n                    arg.requires_grad = val\n        for arg in self._kwargs.values():\n            if hasattr(arg, ""requires_grad""):\n                arg.requires_grad = val\n\n    def requires_grad_(self, val):\n        """"""\n        Sets `requires_grad=val` on all the Tensors that make up the LazyTensor\n        This is an inplace operation.\n        """"""\n        self.requires_grad = val\n        return self\n\n    @cached(name=""root_decomposition"")\n    def root_decomposition(self):\n        """"""\n        Returns a (usually low-rank) root decomposition lazy tensor of a PSD matrix.\n        This can be used for sampling from a Gaussian distribution, or for obtaining a\n        low-rank version of a matrix\n        """"""\n        from .chol_lazy_tensor import CholLazyTensor\n        from .root_lazy_tensor import RootLazyTensor\n\n        if not self.is_square:\n            raise RuntimeError(\n                ""root_decomposition only operates on (batches of) square (symmetric) LazyTensors. ""\n                ""Got a {} of size {}."".format(self.__class__.__name__, self.size())\n            )\n\n        if (\n            self.size(-1) <= settings.max_cholesky_size.value()\n            or settings.fast_computations.covar_root_decomposition.off()\n        ):\n            try:\n                res = self.cholesky()\n                return CholLazyTensor(res)\n\n            except RuntimeError as e:\n                warnings.warn(\n                    ""Runtime Error when computing Cholesky decomposition: {}. Using RootDecomposition."".format(e),\n                    NumericalWarning,\n                )\n\n        res = self._root_decomposition()\n        return RootLazyTensor(res)\n\n    @cached(name=""root_inv_decomposition"")\n    def root_inv_decomposition(self, initial_vectors=None, test_vectors=None):\n        """"""\n        Returns a (usually low-rank) root decomposotion lazy tensor of a PSD matrix.\n        This can be used for sampling from a Gaussian distribution, or for obtaining a\n        low-rank version of a matrix\n        """"""\n        from .root_lazy_tensor import RootLazyTensor\n        from .non_lazy_tensor import lazify\n\n        if self.shape[-2:].numel() == 1:\n            return RootLazyTensor(1 / self.evaluate().sqrt())\n\n        if (\n            self.size(-1) <= settings.max_cholesky_size.value()\n            or settings.fast_computations.covar_root_decomposition.off()\n        ):\n            try:\n                L = delazify(self.cholesky())\n                # we know L is triangular, so inverting is a simple triangular solve agaist the identity\n                # we don\'t need the batch shape here, thanks to broadcasting\n                Eye = torch.eye(L.shape[-2], device=L.device, dtype=L.dtype)\n                Linv = torch.triangular_solve(Eye, L, upper=False)[0]\n                res = lazify(Linv.transpose(-1, -2))\n                return RootLazyTensor(res)\n            except RuntimeError as e:\n                warnings.warn(\n                    ""Runtime Error when computing Cholesky decomposition: {}. Using RootDecomposition."".format(e),\n                    NumericalWarning,\n                )\n\n        if not self.is_square:\n            raise RuntimeError(\n                ""root_inv_decomposition only operates on (batches of) square (symmetric) LazyTensors. ""\n                ""Got a {} of size {}."".format(self.__class__.__name__, self.size())\n            )\n\n        if initial_vectors is not None:\n            if self.dim() == 2 and initial_vectors.dim() == 1:\n                if self.shape[-1] != initial_vectors.numel():\n                    raise RuntimeError(\n                        ""LazyTensor (size={}) cannot be multiplied with initial_vectors (size={})."".format(\n                            self.shape, initial_vectors.shape\n                        )\n                    )\n            elif self.dim() != initial_vectors.dim():\n                raise RuntimeError(\n                    ""LazyTensor (size={}) and initial_vectors (size={}) should have the same number ""\n                    ""of dimensions."".format(self.shape, initial_vectors.shape)\n                )\n            elif self.batch_shape != initial_vectors.shape[:-2] or self.shape[-1] != initial_vectors.shape[-2]:\n                raise RuntimeError(\n                    ""LazyTensor (size={}) cannot be multiplied with initial_vectors (size={})."".format(\n                        self.shape, initial_vectors.shape\n                    )\n                )\n\n        inv_roots = self._root_inv_decomposition(initial_vectors)\n\n        # Choose the best of the inv_roots, if there were more than one initial vectors\n        if initial_vectors is not None and initial_vectors.size(-1) > 1:\n            num_probes = initial_vectors.size(-1)\n            test_vectors = test_vectors.unsqueeze(0)\n\n            # Compute solves\n            solves = inv_roots.matmul(inv_roots.transpose(-1, -2).matmul(test_vectors))\n\n            # Compute self * solves\n            solves = (\n                solves.permute(*range(1, self.dim() + 1), 0)\n                .contiguous()\n                .view(*self.batch_shape, self.matrix_shape[-1], -1)\n            )\n            mat_times_solves = self.matmul(solves)\n            mat_times_solves = mat_times_solves.view(*self.batch_shape, self.matrix_shape[-1], -1, num_probes).permute(\n                -1, *range(0, self.dim())\n            )\n\n            # Compute residuals\n            residuals = (mat_times_solves - test_vectors).norm(2, dim=-2)\n            residuals = residuals.view(residuals.size(0), -1).sum(-1)\n\n            # Choose solve that best fits\n            _, best_solve_index = residuals.min(0)\n            inv_root = inv_roots[best_solve_index].squeeze(0)\n\n        else:\n            inv_root = inv_roots\n\n        return RootLazyTensor(inv_root)\n\n    def size(self, val=None):\n        """"""\n        Returns the size of the resulting Tensor that the lazy tensor represents\n        """"""\n        size = self._size()\n        if val is not None:\n            return size[val]\n        return size\n\n    def squeeze(self, dim):\n        if self.size(dim) != 1:\n            return self\n        else:\n            index = [_noop_index] * self.dim()\n            index[dim] = 0\n            index = tuple(index)\n            return self[index]\n\n    @property\n    def shape(self):\n        return self.size()\n\n    def sum(self, dim=None):\n        """"""\n        Sum the LazyTensor across a dimension.\n        The `dim` controls which batch dimension is summed over.\n        If set to None, then sums all dimensions\n\n        Args:\n            :attr:`dim` (int):\n                Which dimension is being summed over (default=None)\n\n        Returns:\n            :obj:`~gpytorch.lazy.LazyTensor` or Tensor.\n\n        Example:\n            >>> lazy_tensor = gpytorch.lazy.NonLazyTensor(torch.tensor([\n                    [[2, 4], [1, 2]],\n                    [[1, 1], [0, -1]],\n                    [[2, 1], [1, 0]],\n                    [[3, 2], [2, -1]],\n                ]))\n            >>> lazy_tensor.sum(0).evaluate()\n        """"""\n        # Case: summing everything\n        if dim is None:\n            ones = torch.ones(self.size(-2), 1, dtype=self.dtype, device=self.device)\n            return (self @ ones).sum()\n\n        # Otherwise: make dim positive\n        orig_dim = dim\n        if dim < 0:\n            dim = self.dim() + dim\n\n        # Case: summing across columns\n        if dim == (self.dim() - 1):\n            ones = torch.ones(self.size(-1), 1, dtype=self.dtype, device=self.device)\n            return (self @ ones).squeeze(-1)\n        # Case: summing across rows\n        elif dim == (self.dim() - 2):\n            ones = torch.ones(self.size(-2), 1, dtype=self.dtype, device=self.device)\n            return (self.transpose(-1, -2) @ ones).squeeze(-1)\n        # Otherwise: it\'s a batch dimension\n        elif dim < self.dim():\n            return self._sum_batch(dim)\n        else:\n            raise ValueError(""Invalid dim ({}) for LazyTensor of size {}"".format(orig_dim, self.shape))\n\n    def to(self, device_id):\n        """"""\n        A device-agnostic method of moving the lazy_tensor to the specified device.\n\n        Args:\n            device_id (:obj: `torch.device`): Which device to use (GPU or CPU).\n        Returns:\n            :obj:`~gpytorch.lazy.LazyTensor`: New LazyTensor identical to self on specified device\n        """"""\n        new_args = []\n        new_kwargs = {}\n        for arg in self._args:\n            if hasattr(arg, ""to""):\n                new_args.append(arg.to(device_id))\n            else:\n                new_args.append(arg)\n        for name, val in self._kwargs.items():\n            if hasattr(val, ""to""):\n                new_kwargs[name] = val.to(device_id)\n            else:\n                new_kwargs[name] = val\n        return self.__class__(*new_args, **new_kwargs)\n\n    def t(self):\n        """"""\n        Alias of :meth:`~gpytorch.lazy.LazyTensor.transpose` for 2D LazyTensor.\n        (Tranposes the two dimensions.)\n        """"""\n        if self.ndimension() != 2:\n            raise RuntimeError(""Cannot call t for more than 2 dimensions"")\n        return self.transpose(0, 1)\n\n    def transpose(self, dim1, dim2):\n        """"""\n        Transpose the dimensions `dim1` and `dim2` of the LazyTensor.\n\n        Example:\n            >>> lazy_tensor = gpytorch.lazy.NonLazyTensor(torch.randn(3, 5))\n            >>> lazy_tensor.transpose(0, 1)\n        """"""\n        ndimension = self.ndimension()\n        if dim1 < 0:\n            dim1 = ndimension + dim1\n        if dim2 < 0:\n            dim2 = ndimension + dim2\n        if dim1 >= ndimension or dim2 >= ndimension or not isinstance(dim1, int) or not isinstance(dim2, int):\n            raise RuntimeError(""Invalid dimension"")\n\n        # Batch case\n        if dim1 < ndimension - 2 and dim2 < ndimension - 2:\n            small_dim = dim1 if dim1 < dim2 else dim2\n            large_dim = dim2 if dim1 < dim2 else dim1\n            res = self._permute_batch(\n                *range(small_dim),\n                large_dim,\n                *range(small_dim + 1, large_dim),\n                small_dim,\n                *range(large_dim + 1, ndimension - 2),\n            )\n\n        elif dim1 >= ndimension - 2 and dim2 >= ndimension - 2:\n            res = self._transpose_nonbatch()\n\n        else:\n            raise RuntimeError(""Cannot transpose batch dimension with non-batch dimension"")\n\n        return res\n\n    def unsqueeze(self, dim):\n        positive_dim = (self.dim() + dim + 1) if dim < 0 else dim\n        if positive_dim > len(self.batch_shape):\n            raise ValueError(\n                ""Can only unsqueeze batch dimensions of {} (size {}). Got ""\n                ""dim={}."".format(self.__class__.__name__, self.shape, dim)\n            )\n        res = self._unsqueeze_batch(positive_dim)\n        return res\n\n    def zero_mean_mvn_samples(self, num_samples):\n        """"""\n        Assumes that self is a covariance matrix, or a batch of covariance matrices.\n        Returns samples from a zero-mean MVN, defined by self (as covariance matrix)\n\n        Self should be symmetric, either (batch_size x num_dim x num_dim) or (num_dim x num_dim)\n\n        Args:\n            :attr:`num_samples` (int):\n                Number of samples to draw.\n\n        Returns:\n            :obj:`torch.tensor`:\n                Samples from MVN (num_samples x batch_size x num_dim) or (num_samples x num_dim)\n        """"""\n        if self.size()[-2:] == torch.Size([1, 1]):\n            covar_root = self.evaluate().sqrt()\n        else:\n            covar_root = self.root_decomposition().root\n\n        base_samples = torch.randn(\n            *self.batch_shape, covar_root.size(-1), num_samples, dtype=self.dtype, device=self.device\n        )\n        samples = covar_root.matmul(base_samples).permute(-1, *range(self.dim() - 1)).contiguous()\n\n        return samples\n\n    def __add__(self, other):\n        """"""\n        Return a :obj:`gpytorch.lazy.LazyTensor` that represents the sum of this lazy tensor and another matrix\n        or lazy tensor.\n\n        Args:\n            :attr:`other` (:obj:`torch.tensor` or :obj:`gpytorch.lazy.LazyTensor`):\n                Matrix to add to this one.\n\n        Returns:\n            :obj:`gpytorch.lazy.SumLazyTensor`:\n                A sum lazy tensor representing the sum of this lazy tensor and other.\n        """"""\n        from .sum_lazy_tensor import SumLazyTensor\n        from .zero_lazy_tensor import ZeroLazyTensor\n        from .diag_lazy_tensor import DiagLazyTensor\n        from .added_diag_lazy_tensor import AddedDiagLazyTensor\n        from .non_lazy_tensor import lazify\n        from torch import Tensor\n\n        if isinstance(other, ZeroLazyTensor):\n            return self\n        elif isinstance(other, DiagLazyTensor):\n            return AddedDiagLazyTensor(self, other)\n        elif isinstance(other, Tensor):\n            other = lazify(other)\n            shape = _mul_broadcast_shape(self.shape, other.shape)\n            return SumLazyTensor(self.expand(shape), other.expand(shape))\n        else:\n            return SumLazyTensor(self, other)\n\n    def __div__(self, other):\n        """"""\n        Return a :obj:`gpytorch.lazy.LazyTensor` that represents the product of this lazy tensor and\n        the elementwise reciprocal of another matrix or lazy tensor.\n\n        Args:\n            :attr:`other` (:obj:`torch.tensor` or :obj:`gpytorch.lazy.LazyTensor`):\n                Matrix to divide this one by.\n\n        Returns:\n            :obj:`gpytorch.lazy.MulLazyTensor`:\n                Result of division.\n        """"""\n        from .zero_lazy_tensor import ZeroLazyTensor\n\n        if isinstance(other, ZeroLazyTensor):\n            raise RuntimeError(""Attempted to divide by a ZeroLazyTensor (divison by zero)"")\n\n        return self.mul(1.0 / other)\n\n    def __getitem__(self, index):\n        """"""\n        Supports subindexing of the matrix this LazyTensor represents. This may return either another\n        :obj:`gpytorch.lazy.LazyTensor` or a :obj:`torch.tensor` depending on the exact implementation.\n        """"""\n        ndimension = self.ndimension()\n\n        # Process the index\n        index = index if isinstance(index, tuple) else (index,)\n        index = tuple(torch.tensor(idx) if isinstance(idx, list) else idx for idx in index)\n        index = tuple(idx.item() if torch.is_tensor(idx) and not len(idx.shape) else idx for idx in index)\n\n        # Handle the ellipsis\n        # Find the index of the ellipsis\n        ellipsis_locs = tuple(index for index, item in enumerate(index) if item is Ellipsis)\n        if settings.debug.on():\n            if len(ellipsis_locs) > 1:\n                raise RuntimeError(\n                    ""Cannot have multiple ellipsis in a __getitem__ call. LazyTensor {} ""\n                    "" received index {}."".format(self, index)\n                )\n        if len(ellipsis_locs) == 1:\n            ellipsis_loc = ellipsis_locs[0]\n            num_to_fill_in = ndimension - (len(index) - 1)\n            index = index[:ellipsis_loc] + tuple(_noop_index for _ in range(num_to_fill_in)) + index[ellipsis_loc + 1 :]\n\n        # Pad the index with empty indices\n        index = index + tuple(_noop_index for _ in range(ndimension - len(index)))\n\n        # Make the index a tuple again\n        *batch_indices, row_index, col_index = index\n\n        # Helpers to determine what the final shape will be if we\'re tensor indexed\n        batch_has_tensor_index = bool(len(batch_indices)) and any(torch.is_tensor(index) for index in batch_indices)\n        row_has_tensor_index = torch.is_tensor(row_index)\n        col_has_tensor_index = torch.is_tensor(col_index)\n        # These are the cases where the row and/or column indices will be ""absorbed"" into other indices\n        row_col_are_absorbed = any(\n            (\n                batch_has_tensor_index and (row_has_tensor_index or col_has_tensor_index),\n                not batch_has_tensor_index and (row_has_tensor_index and col_has_tensor_index),\n            )\n        )\n\n        # If we\'re indexing the LT with ints or slices\n        # Replace the ints with slices, and we\'ll just squeeze the dimensions later\n        squeeze_row = False\n        squeeze_col = False\n        if isinstance(row_index, int):\n            row_index = slice(row_index, row_index + 1, None)\n            squeeze_row = True\n        if isinstance(col_index, int):\n            col_index = slice(col_index, col_index + 1, None)\n            squeeze_col = True\n\n        # Call self._getitem - now that the index has been processed\n        # Alternatively, if we\'re using tensor indices and losing dimensions, use self._get_indices\n        if row_col_are_absorbed:\n            # Convert all indices into tensor indices\n            *batch_indices, row_index, col_index, = _convert_indices_to_tensors(\n                self, (*batch_indices, row_index, col_index)\n            )\n            res = self._get_indices(row_index, col_index, *batch_indices)\n        else:\n            res = self._getitem(row_index, col_index, *batch_indices)\n\n        # If we selected a single row and/or column (or did tensor indexing), we\'ll be retuning a tensor\n        # with the appropriate shape\n        if squeeze_row or squeeze_col or row_col_are_absorbed:\n            res = delazify(res)\n        if squeeze_row:\n            res = res.squeeze(-2)\n        if squeeze_col:\n            res = res.squeeze(-1)\n\n        # Make sure we\'re getting the expected shape\n        if settings.debug.on() and self.__class__._check_size:\n            expected_shape = _compute_getitem_size(self, index)\n            if expected_shape != res.shape:\n                raise RuntimeError(\n                    ""{}.__getitem__ failed! Expected a final shape of size {}, got {}. This is a bug with GPyTorch, ""\n                    ""or your custom LazyTensor."".format(self.__class__.__name__, expected_shape, res.shape)\n                )\n\n        # We\'re done!\n        return res\n\n    def __matmul__(self, other):\n        return self.matmul(other)\n\n    def __mul__(self, other):\n        return self.mul(other)\n\n    def __radd__(self, other):\n        return self + other\n\n    def __rmul__(self, other):\n        return self.mul(other)\n\n    def __sub__(self, other):\n        return self + other.mul(-1)\n\n\ndef _import_dotted_name(name):\n    components = name.split(""."")\n    obj = __import__(components[0])\n    for component in components[1:]:\n        obj = getattr(obj, component)\n    return obj\n\n\ndef delazify(obj):\n    """"""\n    A function which ensures that `obj` is a (normal) Tensor.\n\n    If `obj` is a Tensor, this function does nothing.\n    If `obj` is a LazyTensor, this function evaluates it.\n    """"""\n\n    if torch.is_tensor(obj):\n        return obj\n    elif isinstance(obj, LazyTensor):\n        return obj.evaluate()\n    else:\n        raise TypeError(""object of class {} cannot be made into a Tensor"".format(obj.__class__.__name__))\n\n\n_deprecate_renamed_methods(LazyTensor, inv_quad_log_det=""inv_quad_logdet"", log_det=""logdet"")\n\n__all__ = [""LazyTensor"", ""delazify""]\n'"
gpytorch/lazy/lazy_tensor_representation_tree.py,0,"b'#!/usr/bin/env python3\n\n\nclass LazyTensorRepresentationTree(object):\n    def __init__(self, lazy_tsr):\n        self._cls = lazy_tsr.__class__\n        self._kwargs = lazy_tsr._kwargs\n\n        counter = 0\n        self.children = []\n        for arg in lazy_tsr._args:\n            if hasattr(arg, ""representation"") and callable(arg.representation):  # Is it a lazy tensor?\n                representation_size = len(arg.representation())\n                self.children.append((slice(counter, counter + representation_size, None), arg.representation_tree()))\n                counter += representation_size\n            else:\n                self.children.append((counter, None))\n                counter += 1\n\n    def __call__(self, *flattened_representation):\n        unflattened_representation = []\n\n        for index, subtree in self.children:\n            if subtree is None:\n                unflattened_representation.append(flattened_representation[index])\n            else:\n                sub_representation = flattened_representation[index]\n                unflattened_representation.append(subtree(*sub_representation))\n\n        return self._cls(*unflattened_representation, **self._kwargs)\n'"
gpytorch/lazy/matmul_lazy_tensor.py,3,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils.broadcasting import _matmul_broadcast_shape, _pad_with_singletons\nfrom ..utils.getitem import _noop_index\nfrom ..utils.memoize import cached\nfrom .diag_lazy_tensor import DiagLazyTensor\nfrom .lazy_tensor import LazyTensor\nfrom .non_lazy_tensor import NonLazyTensor, lazify\n\n\ndef _inner_repeat(tensor, amt):\n    return tensor.unsqueeze(-1).repeat(amt, 1).squeeze(-1)\n\n\ndef _outer_repeat(tensor, amt):\n    return tensor.unsqueeze(-1).repeat(1, amt).view(-1)\n\n\nclass MatmulLazyTensor(LazyTensor):\n    def __init__(self, left_lazy_tensor, right_lazy_tensor):\n        left_lazy_tensor = lazify(left_lazy_tensor)\n        right_lazy_tensor = lazify(right_lazy_tensor)\n\n        super().__init__(left_lazy_tensor, right_lazy_tensor)\n        self.left_lazy_tensor = left_lazy_tensor\n        self.right_lazy_tensor = right_lazy_tensor\n\n    def _expand_batch(self, batch_shape):\n        return self.__class__(\n            self.left_lazy_tensor._expand_batch(batch_shape), self.right_lazy_tensor._expand_batch(batch_shape)\n        )\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        row_index = row_index.unsqueeze(-1)\n        col_index = col_index.unsqueeze(-1)\n        batch_indices = tuple(batch_index.unsqueeze(-1) for batch_index in batch_indices)\n        inner_index = torch.arange(0, self.left_lazy_tensor.size(-1), device=self.device)\n        inner_index = _pad_with_singletons(inner_index, row_index.dim() - 1, 0)\n\n        left_tensor = self.left_lazy_tensor._get_indices(row_index, inner_index, *batch_indices)\n        right_tensor = self.right_lazy_tensor._get_indices(inner_index, col_index, *batch_indices)\n        res = (left_tensor * right_tensor).sum(-1)\n        return res\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        # Make sure we\'re not generating more memory with our ""efficient"" method\n        if torch.is_tensor(row_index) and torch.is_tensor(col_index):\n            num_indices = row_index.numel()\n            if num_indices > self.matrix_shape.numel():\n                return lazify(self.evaluate())._getitem(row_index, col_index, *batch_indices)\n\n        left_tensor = self.left_lazy_tensor._getitem(row_index, _noop_index, *batch_indices)\n        right_tensor = self.right_lazy_tensor._getitem(_noop_index, col_index, *batch_indices)\n\n        res = MatmulLazyTensor(left_tensor, right_tensor)\n        return res\n\n    def _matmul(self, right_lazy_tensor):\n        return self.left_lazy_tensor._matmul(self.right_lazy_tensor._matmul(right_lazy_tensor))\n\n    def _t_matmul(self, right_lazy_tensor):\n        return self.right_lazy_tensor._t_matmul(self.left_lazy_tensor._t_matmul(right_lazy_tensor))\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        if left_vecs.ndimension() == 1:\n            left_vecs = left_vecs.unsqueeze(1)\n            right_vecs = right_vecs.unsqueeze(1)\n        right_vecs_times_right_lazy_tensor = self.right_lazy_tensor._matmul(right_vecs)\n        left_vecs_times_left_lazy_tensor_t = self.left_lazy_tensor._t_matmul(left_vecs)\n        left_grad = self.left_lazy_tensor._quad_form_derivative(left_vecs, right_vecs_times_right_lazy_tensor)\n        right_grad = self.right_lazy_tensor._quad_form_derivative(left_vecs_times_left_lazy_tensor_t, right_vecs)\n\n        left_grad = (left_grad,) if not isinstance(left_grad, tuple) else left_grad\n        right_grad = (right_grad,) if not isinstance(right_grad, tuple) else right_grad\n        return left_grad + right_grad\n\n    def _size(self):\n        return _matmul_broadcast_shape(self.left_lazy_tensor.shape, self.right_lazy_tensor.shape)\n\n    def _transpose_nonbatch(self, *args):\n        return self.__class__(self.right_lazy_tensor._transpose_nonbatch(), self.left_lazy_tensor._transpose_nonbatch())\n\n    def diag(self):\n        if isinstance(self.left_lazy_tensor, NonLazyTensor) and isinstance(self.right_lazy_tensor, NonLazyTensor):\n            return (self.left_lazy_tensor.tensor * self.right_lazy_tensor.tensor.transpose(-1, -2)).sum(-1)\n        elif isinstance(self.left_lazy_tensor, DiagLazyTensor) or isinstance(self.right_lazy_tensor, DiagLazyTensor):\n            return self.left_lazy_tensor.diag() * self.right_lazy_tensor.diag()\n        else:\n            return super().diag()\n\n    @cached\n    def evaluate(self):\n        return torch.matmul(self.left_lazy_tensor.evaluate(), self.right_lazy_tensor.evaluate())\n'"
gpytorch/lazy/mul_lazy_tensor.py,2,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils.broadcasting import _matmul_broadcast_shape\nfrom ..utils.memoize import cached\nfrom .lazy_tensor import LazyTensor\nfrom .root_lazy_tensor import RootLazyTensor\n\n\nclass MulLazyTensor(LazyTensor):\n    def _check_args(self, left_lazy_tensor, right_lazy_tensor):\n        if not isinstance(left_lazy_tensor, LazyTensor) or not isinstance(right_lazy_tensor, LazyTensor):\n            return ""MulLazyTensor expects two LazyTensors.""\n        if left_lazy_tensor.shape != right_lazy_tensor.shape:\n            return ""MulLazyTensor expects two LazyTensors of the same size: got {} and {}."".format(\n                left_lazy_tensor, right_lazy_tensor\n            )\n\n    def __init__(self, left_lazy_tensor, right_lazy_tensor):\n        """"""\n        Args:\n            - lazy_tensors (A list of LazyTensor) - A list of LazyTensor to multiplicate with.\n        """"""\n        if not isinstance(left_lazy_tensor, RootLazyTensor):\n            left_lazy_tensor = left_lazy_tensor.root_decomposition()\n        if not isinstance(right_lazy_tensor, RootLazyTensor):\n            right_lazy_tensor = right_lazy_tensor.root_decomposition()\n        super(MulLazyTensor, self).__init__(left_lazy_tensor, right_lazy_tensor)\n        self.left_lazy_tensor = left_lazy_tensor\n        self.right_lazy_tensor = right_lazy_tensor\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        left_res = self.left_lazy_tensor._get_indices(row_index, col_index, *batch_indices)\n        right_res = self.right_lazy_tensor._get_indices(row_index, col_index, *batch_indices)\n        return left_res * right_res\n\n    def _matmul(self, rhs):\n        output_shape = _matmul_broadcast_shape(self.shape, rhs.shape)\n        output_batch_shape = output_shape[:-2]\n\n        is_vector = False\n        if rhs.ndimension() == 1:\n            rhs = rhs.unsqueeze(1)\n            is_vector = True\n\n        # Here we have a root decomposition\n        if isinstance(self.left_lazy_tensor, RootLazyTensor):\n            left_root = self.left_lazy_tensor.root.evaluate()\n            left_res = rhs.unsqueeze(-2) * left_root.unsqueeze(-1)\n\n            rank = left_root.size(-1)\n            n = self.size(-1)\n            m = rhs.size(-1)\n            # Now implement the formula (A . B) v = diag(A D_v B)\n            left_res = left_res.view(*output_batch_shape, n, rank * m)\n            left_res = self.right_lazy_tensor._matmul(left_res)\n            left_res = left_res.view(*output_batch_shape, n, rank, m)\n            res = left_res.mul_(left_root.unsqueeze(-1)).sum(-2)\n        # This is the case where we\'re not doing a root decomposition, because the matrix is too small\n        else:\n            res = (self.left_lazy_tensor.evaluate() * self.right_lazy_tensor.evaluate()).matmul(rhs)\n        res = res.squeeze(-1) if is_vector else res\n        return res\n\n    def _mul_constant(self, other):\n        return self.__class__(self.left_lazy_tensor._mul_constant(other), self.right_lazy_tensor)\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        if left_vecs.ndimension() == 1:\n            left_vecs = left_vecs.unsqueeze(1)\n            right_vecs = right_vecs.unsqueeze(1)\n\n        *batch_shape, n, num_vecs = left_vecs.size()\n\n        if isinstance(self.right_lazy_tensor, RootLazyTensor):\n            right_root = self.right_lazy_tensor.root.evaluate()\n            left_factor = left_vecs.unsqueeze(-2) * right_root.unsqueeze(-1)\n            right_factor = right_vecs.unsqueeze(-2) * right_root.unsqueeze(-1)\n            right_rank = right_root.size(-1)\n        else:\n            right_rank = n\n            eye = torch.eye(n, dtype=self.right_lazy_tensor.dtype, device=self.right_lazy_tensor.device)\n            left_factor = left_vecs.unsqueeze(-2) * self.right_lazy_tensor.evaluate().unsqueeze(-1)\n            right_factor = right_vecs.unsqueeze(-2) * eye.unsqueeze(-1)\n\n        left_factor = left_factor.view(*batch_shape, n, num_vecs * right_rank)\n        right_factor = right_factor.view(*batch_shape, n, num_vecs * right_rank)\n        left_deriv_args = self.left_lazy_tensor._quad_form_derivative(left_factor, right_factor)\n\n        if isinstance(self.left_lazy_tensor, RootLazyTensor):\n            left_root = self.left_lazy_tensor.root.evaluate()\n            left_factor = left_vecs.unsqueeze(-2) * left_root.unsqueeze(-1)\n            right_factor = right_vecs.unsqueeze(-2) * left_root.unsqueeze(-1)\n            left_rank = left_root.size(-1)\n        else:\n            left_rank = n\n            eye = torch.eye(n, dtype=self.left_lazy_tensor.dtype, device=self.left_lazy_tensor.device)\n            left_factor = left_vecs.unsqueeze(-2) * self.left_lazy_tensor.evaluate().unsqueeze(-1)\n            right_factor = right_vecs.unsqueeze(-2) * eye.unsqueeze(-1)\n\n        left_factor = left_factor.view(*batch_shape, n, num_vecs * left_rank)\n        right_factor = right_factor.view(*batch_shape, n, num_vecs * left_rank)\n        right_deriv_args = self.right_lazy_tensor._quad_form_derivative(left_factor, right_factor)\n\n        return tuple(list(left_deriv_args) + list(right_deriv_args))\n\n    def _expand_batch(self, batch_shape):\n        return self.__class__(\n            self.left_lazy_tensor._expand_batch(batch_shape), self.right_lazy_tensor._expand_batch(batch_shape)\n        )\n\n    def diag(self):\n        res = self.left_lazy_tensor.diag() * self.right_lazy_tensor.diag()\n        return res\n\n    @cached\n    def evaluate(self):\n        return self.left_lazy_tensor.evaluate() * self.right_lazy_tensor.evaluate()\n\n    def _size(self):\n        return self.left_lazy_tensor.size()\n\n    def _transpose_nonbatch(self):\n        # mul.lazy_tensor only works with symmetric matrices\n        return self\n\n    def representation(self):\n        """"""\n        Returns the Tensors that are used to define the LazyTensor\n        """"""\n        res = super(MulLazyTensor, self).representation()\n        return res\n\n    def representation_tree(self):\n        return super(MulLazyTensor, self).representation_tree()\n'"
gpytorch/lazy/non_lazy_tensor.py,7,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom .lazy_tensor import LazyTensor\n\n\nclass NonLazyTensor(LazyTensor):\n    def _check_args(self, tsr):\n        if not torch.is_tensor(tsr):\n            return ""NonLazyTensor must take a torch.Tensor; got {}"".format(tsr.__class__.__name__)\n        if tsr.dim() < 2:\n            return ""NonLazyTensor expects a matrix (or batches of matrices) - got a Tensor of size {}."".format(\n                tsr.shape\n            )\n\n    def __init__(self, tsr):\n        """"""\n        Not a lazy tensor\n\n        Args:\n        - tsr (Tensor: matrix) a Tensor\n        """"""\n        super(NonLazyTensor, self).__init__(tsr)\n        self.tensor = tsr\n\n    def _expand_batch(self, batch_shape):\n        return self.__class__(self.tensor.expand(*batch_shape, *self.matrix_shape))\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        # Perform the __getitem__\n        res = self.tensor[(*batch_indices, row_index, col_index)]\n        return res\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        # Perform the __getitem__\n        res = self.tensor[(*batch_indices, row_index, col_index)]\n        return self.__class__(res)\n\n    def _matmul(self, rhs):\n        return torch.matmul(self.tensor, rhs)\n\n    def _prod_batch(self, dim):\n        return self.__class__(self.tensor.prod(dim))\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        res = left_vecs.matmul(right_vecs.transpose(-1, -2))\n        return (res,)\n\n    def _size(self):\n        return self.tensor.size()\n\n    def _sum_batch(self, dim):\n        return self.__class__(self.tensor.sum(dim))\n\n    def _transpose_nonbatch(self):\n        return NonLazyTensor(self.tensor.transpose(-1, -2))\n\n    def _t_matmul(self, rhs):\n        return torch.matmul(self.tensor.transpose(-1, -2), rhs)\n\n    def diag(self):\n        if self.tensor.ndimension() < 3:\n            return self.tensor.diag()\n        else:\n            row_col_iter = torch.arange(0, self.matrix_shape[-1], dtype=torch.long, device=self.device)\n            return self.tensor[..., row_col_iter, row_col_iter].view(*self.batch_shape, -1)\n\n    def evaluate(self):\n        return self.tensor\n\n    def __add__(self, other):\n        if isinstance(other, NonLazyTensor):\n            return NonLazyTensor(self.tensor + other.tensor)\n        elif isinstance(other, torch.Tensor):\n            return NonLazyTensor(self.tensor + other)\n        else:\n            return super(NonLazyTensor, self).__add__(other)\n\n    def mul(self, other):\n        if isinstance(other, NonLazyTensor):\n            return NonLazyTensor(self.tensor * other.tensor)\n        else:\n            return super(NonLazyTensor, self).mul(other)\n\n\ndef lazify(obj):\n    """"""\n    A function which ensures that `obj` is a LazyTensor.\n\n    If `obj` is a LazyTensor, this function does nothing.\n    If `obj` is a (normal) Tensor, this function wraps it with a `NonLazyTensor`.\n    """"""\n\n    if torch.is_tensor(obj):\n        return NonLazyTensor(obj)\n    elif isinstance(obj, LazyTensor):\n        return obj\n    else:\n        raise TypeError(""object of class {} cannot be made into a LazyTensor"".format(obj.__class__.__name__))\n\n\n__all__ = [""NonLazyTensor"", ""lazify""]\n'"
gpytorch/lazy/psd_sum_lazy_tensor.py,0,"b'#!/usr/bin/env python3\n\nfrom .sum_lazy_tensor import SumLazyTensor\n\n\nclass PsdSumLazyTensor(SumLazyTensor):\n    """"""\n    A SumLazyTensor, but where every component of the sum is positive semi-definite\n    """"""\n\n    def zero_mean_mvn_samples(self, num_samples):\n        return sum(lazy_tensor.zero_mean_mvn_samples(num_samples) for lazy_tensor in self.lazy_tensors)\n'"
gpytorch/lazy/root_lazy_tensor.py,5,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils.broadcasting import _pad_with_singletons\nfrom ..utils.getitem import _equal_indices, _noop_index\nfrom ..utils.memoize import cached\nfrom .lazy_tensor import LazyTensor\nfrom .matmul_lazy_tensor import MatmulLazyTensor\nfrom .non_lazy_tensor import NonLazyTensor, lazify\n\n\nclass RootLazyTensor(LazyTensor):\n    def __init__(self, root):\n        root = lazify(root)\n        super(RootLazyTensor, self).__init__(root)\n        self.root = root\n\n    def _expand_batch(self, batch_shape):\n        return self.__class__(self.root._expand_batch(batch_shape))\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        row_index = row_index.unsqueeze(-1)\n        col_index = col_index.unsqueeze(-1)\n        batch_indices = tuple(batch_index.unsqueeze(-1) for batch_index in batch_indices)\n        inner_index = torch.arange(0, self.root.size(-1), device=self.device)\n        inner_index = _pad_with_singletons(inner_index, row_index.dim() - 1, 0)\n\n        left_tensor = self.root._get_indices(row_index, inner_index, *batch_indices)\n        if torch.equal(row_index, col_index):\n            res = left_tensor.pow(2).sum(-1)\n        else:\n            right_tensor = self.root._get_indices(col_index, inner_index, *batch_indices)\n            res = (left_tensor * right_tensor).sum(-1)\n        return res\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        # Make sure we\'re not generating more memory with our ""efficient"" method\n        if torch.is_tensor(row_index) and torch.is_tensor(col_index):\n            num_indices = row_index.numel()\n            if num_indices > self.matrix_shape.numel():\n                return lazify(self.evaluate())._getitem(row_index, col_index, *batch_indices)\n\n        left_tensor = self.root._getitem(row_index, _noop_index, *batch_indices)\n        if _equal_indices(row_index, col_index):\n            res = self.__class__(left_tensor)\n        else:\n            right_tensor = self.root._getitem(col_index, _noop_index, *batch_indices)\n            res = MatmulLazyTensor(left_tensor, right_tensor.transpose(-1, -2))\n\n        return res\n\n    def _matmul(self, rhs):\n        return self.root._matmul(self.root._t_matmul(rhs))\n\n    def _t_matmul(self, rhs):\n        # Matrix is symmetric\n        return self._matmul(rhs)\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        right_vecs_times_rhs = self.root._t_matmul(right_vecs)\n        left_vecs_times_lhs_t = self.root._t_matmul(left_vecs)\n\n        deriv_part_1 = self.root._quad_form_derivative(left_vecs, right_vecs_times_rhs)\n        deriv_part_2 = self.root._quad_form_derivative(right_vecs, left_vecs_times_lhs_t)\n\n        deriv = []\n        for item_part_1, item_part_2 in zip(deriv_part_1, deriv_part_2):\n            deriv.append(item_part_1 + item_part_2)\n        return tuple(deriv)\n\n    def _root_decomposition(self):\n        return self.root\n\n    def _root_decomposition_size(self):\n        return self.root.size(-1)\n\n    def _size(self):\n        return torch.Size((*self.root.batch_shape, self.root.size(-2), self.root.size(-2)))\n\n    def _transpose_nonbatch(self):\n        return self\n\n    def diag(self):\n        if isinstance(self.root, NonLazyTensor):\n            return (self.root.tensor ** 2).sum(-1)\n        else:\n            return super(RootLazyTensor, self).diag()\n\n    @cached\n    def evaluate(self):\n        eval_root = self.root.evaluate()\n        return torch.matmul(eval_root, eval_root.transpose(-1, -2))\n'"
gpytorch/lazy/sum_batch_lazy_tensor.py,2,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils.broadcasting import _pad_with_singletons\nfrom ..utils.getitem import _noop_index\nfrom .block_lazy_tensor import BlockLazyTensor\n\n\nclass SumBatchLazyTensor(BlockLazyTensor):\n    """"""\n    Represents a lazy tensor that is actually the sum of several lazy tensors blocks.\n    The :attr:`block_dim` attribute specifies which dimension of the base LazyTensor\n    specifies the blocks.\n    For example, (with `block_dim=-3` a `k x n x n` tensor represents `k` `n x n` blocks (a `n x n` matrix).\n    A `b x k x n x n` tensor represents `k` `b x n x n` blocks (a `b x n x n` batch matrix).\n\n    Args:\n        :attr:`base_lazy_tensor` (LazyTensor):\n            A `k x n x n` LazyTensor, or a `b x k x n x n` LazyTensor.\n        :attr:`block_dim` (int):\n            The dimension that specifies the blocks.\n    """"""\n\n    def _add_batch_dim(self, other):\n        shape = list(other.shape)\n        expand_shape = list(other.shape)\n        shape.insert(-2, 1)\n        expand_shape.insert(-2, self.base_lazy_tensor.size(-3))\n        other = other.reshape(*shape).expand(*expand_shape)\n        return other\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        # Create an extra index for the summed dimension\n        sum_index = torch.arange(0, self.base_lazy_tensor.size(-3), device=self.device)\n        sum_index = _pad_with_singletons(sum_index, row_index.dim(), 0)\n        row_index = row_index.unsqueeze(-1)\n        col_index = col_index.unsqueeze(-1)\n        batch_indices = [index.unsqueeze(-1) for index in batch_indices]\n\n        res = self.base_lazy_tensor._get_indices(row_index, col_index, *batch_indices, sum_index)\n        return res.sum(-1)\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        res = self.base_lazy_tensor._getitem(row_index, col_index, *batch_indices, _noop_index)\n        return self.__class__(res, **self._kwargs)\n\n    def _remove_batch_dim(self, other):\n        return other.sum(-3)\n\n    def _size(self):\n        shape = list(self.base_lazy_tensor.shape)\n        del shape[-3]\n        return torch.Size(shape)\n\n    def diag(self):\n        diag = self.base_lazy_tensor.diag().sum(-2)\n        return diag\n\n    def evaluate(self):\n        return self.base_lazy_tensor.evaluate().sum(dim=-3)  # BlockLazyTensors always use dim3 for the block_dim\n'"
gpytorch/lazy/sum_lazy_tensor.py,0,"b'#!/usr/bin/env python3\nfrom torch import Tensor\n\nfrom ..utils.broadcasting import _mul_broadcast_shape\nfrom ..utils.memoize import cached\nfrom .lazy_tensor import LazyTensor\nfrom .non_lazy_tensor import lazify\nfrom .zero_lazy_tensor import ZeroLazyTensor\n\n# from .broadcasted_lazy_tensor import BroadcastedLazyTensor\n\n\nclass SumLazyTensor(LazyTensor):\n    def __init__(self, *lazy_tensors, **kwargs):\n        try:\n            lazy_tensors = tuple(lazify(lt) for lt in lazy_tensors)\n        except TypeError:\n            raise TypeError(""All arguments of a SumLazyTensor should be LazyTensors or Tensors"")\n        batch_shape = _mul_broadcast_shape(*[lt.batch_shape for lt in lazy_tensors])\n        lazy_tensors = tuple(\n            lt._expand_batch(batch_shape) if lt.batch_shape != batch_shape else lt for lt in lazy_tensors\n        )\n        super(SumLazyTensor, self).__init__(*lazy_tensors, **kwargs)\n\n        self.lazy_tensors = lazy_tensors\n\n    def _expand_batch(self, batch_shape):\n        expanded_tensors = [lazy_tensor._expand_batch(batch_shape) for lazy_tensor in self.lazy_tensors]\n        return self.__class__(*expanded_tensors)\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        results = [lazy_tensor._get_indices(row_index, col_index, *batch_indices) for lazy_tensor in self.lazy_tensors]\n        return sum(results)\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        results = [lazy_tensor._getitem(row_index, col_index, *batch_indices) for lazy_tensor in self.lazy_tensors]\n        return SumLazyTensor(*results)\n\n    def _matmul(self, rhs):\n        return sum(lazy_tensor._matmul(rhs) for lazy_tensor in self.lazy_tensors)\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        return tuple(\n            var for lazy_tensor in self.lazy_tensors for var in lazy_tensor._quad_form_derivative(left_vecs, right_vecs)\n        )\n\n    def _size(self):\n        return _mul_broadcast_shape(*[lt.shape for lt in self.lazy_tensors])\n\n    def _sum_batch(self, dim):\n        return self.__class__(*(lazy_tensor._sum_batch(dim) for lazy_tensor in self.lazy_tensors))\n\n    def _t_matmul(self, rhs):\n        return sum(lazy_tensor._t_matmul(rhs) for lazy_tensor in self.lazy_tensors)\n\n    def _transpose_nonbatch(self):\n        lazy_tensors_t = [lazy_tensor.transpose(-1, -2) for lazy_tensor in self.lazy_tensors]\n        return self.__class__(*lazy_tensors_t)\n\n    @cached\n    def evaluate(self):\n        return sum(lazy_tensor.evaluate() for lazy_tensor in self.lazy_tensors)\n\n    def __add__(self, other):\n        from .diag_lazy_tensor import DiagLazyTensor\n        from .added_diag_lazy_tensor import AddedDiagLazyTensor\n\n        if isinstance(other, ZeroLazyTensor):\n            return self\n        elif isinstance(other, DiagLazyTensor):\n            return AddedDiagLazyTensor(self, other)\n        elif isinstance(other, SumLazyTensor):\n            return SumLazyTensor(*(list(self.lazy_tensors) + list(other.lazy_tensors)))\n        elif isinstance(other, LazyTensor):\n            return SumLazyTensor(*(list(self.lazy_tensors) + [other]))\n        elif isinstance(other, Tensor):\n            # get broadcast shape, assuming mul broadcasting the same as add broadcasting\n            broadcasted_shape = _mul_broadcast_shape(self.shape, other.shape)\n\n            # lazify + broadcast other\n            broadcasted_other = lazify(other.expand(broadcasted_shape))\n\n            # update the lazy tensors\' shape as well\n            if broadcasted_shape != self.shape:\n                broadcasted_lts = [\n                    lt.expand(*broadcasted_shape, 1).squeeze(-1).transpose(-1, -2) for lt in self.lazy_tensors\n                ]\n            else:\n                broadcasted_lts = list(self.lazy_tensors)\n\n            return SumLazyTensor(*(broadcasted_lts + [broadcasted_other]))\n        else:\n            raise AttributeError(""other must be a LazyTensor"")\n\n    def diag(self):\n        return sum(lazy_tensor.diag().contiguous() for lazy_tensor in self.lazy_tensors)\n'"
gpytorch/lazy/toeplitz_lazy_tensor.py,2,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils.toeplitz import sym_toeplitz_derivative_quadratic_form, sym_toeplitz_matmul\nfrom .lazy_tensor import LazyTensor\n\n\nclass ToeplitzLazyTensor(LazyTensor):\n    def __init__(self, column):\n        """"""\n        Args:\n            :attr: `column` (Tensor)\n                If `column` is a 1D Tensor of length `n`, this represents a\n                Toeplitz matrix with `column` as its first column.\n                If `column` is `b_1 x b_2 x ... x b_k x n`, then this represents a batch\n                `b_1 x b_2 x ... x b_k` of Toeplitz matrices.\n        """"""\n        super(ToeplitzLazyTensor, self).__init__(column)\n        self.column = column\n\n    def _expand_batch(self, batch_shape):\n        return self.__class__(self.column.expand(*batch_shape, self.column.size(-1)))\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        toeplitz_indices = (row_index - col_index).fmod(self.size(-1)).abs().long()\n        return self.column[(*batch_indices, toeplitz_indices)]\n\n    def _matmul(self, rhs):\n        return sym_toeplitz_matmul(self.column, rhs)\n\n    def _t_matmul(self, rhs):\n        # Matrix is symmetric\n        return self._matmul(rhs)\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        if left_vecs.ndimension() == 1:\n            left_vecs = left_vecs.unsqueeze(1)\n            right_vecs = right_vecs.unsqueeze(1)\n\n        res = sym_toeplitz_derivative_quadratic_form(left_vecs, right_vecs)\n\n        # Collapse any expanded broadcast dimensions\n        if res.dim() > self.column.dim():\n            res = res.view(-1, *self.column.shape).sum(0)\n\n        return (res,)\n\n    def _size(self):\n        return torch.Size((*self.column.shape, self.column.size(-1)))\n\n    def _transpose_nonbatch(self):\n        return ToeplitzLazyTensor(self.column)\n\n    def add_jitter(self, jitter_val=1e-3):\n        jitter = torch.zeros_like(self.column)\n        jitter.narrow(-1, 0, 1).fill_(jitter_val)\n        return ToeplitzLazyTensor(self.column.add(jitter))\n\n    def diag(self):\n        """"""\n        Gets the diagonal of the Toeplitz matrix wrapped by this object.\n        """"""\n        diag_term = self.column[..., 0]\n        if self.column.ndimension() > 1:\n            diag_term = diag_term.unsqueeze(-1)\n        return diag_term.expand(*self.column.size())\n'"
gpytorch/lazy/zero_lazy_tensor.py,6,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils.broadcasting import _mul_broadcast_shape\nfrom ..utils.getitem import _compute_getitem_size\nfrom ..utils.memoize import cached\nfrom .lazy_tensor import LazyTensor\n\n\nclass ZeroLazyTensor(LazyTensor):\n    """"""\n    Special LazyTensor representing zero.\n    """"""\n\n    def __init__(self, *sizes, dtype=None, device=None):\n        super(ZeroLazyTensor, self).__init__(*sizes)\n        self.sizes = list(sizes)\n\n        self._dtype = dtype or torch.get_default_dtype()\n        self._device = device or torch.device(""cpu"")\n\n    @property\n    def dtype(self):\n        return self._dtype\n\n    @property\n    def device(self):\n        return self._device\n\n    def _expand_batch(self, batch_shape):\n        return self.__class__(*batch_shape, *self.sizes[-2:], dtype=self._dtype, device=self._device)\n\n    def _get_indices(self, row_index, col_index, *batch_indices):\n        new_size = _compute_getitem_size(self, batch_indices + (row_index, col_index))\n        return ZeroLazyTensor(*new_size)\n\n    def _getitem(self, row_index, col_index, *batch_indices):\n        new_size = _compute_getitem_size(self, batch_indices + (row_index, col_index))\n        return ZeroLazyTensor(*new_size)\n\n    def _matmul(self, rhs):\n        rhs_size_ind = -2 if rhs.ndimension() > 1 else -1\n        if self.size(-1) != rhs.size(rhs_size_ind):\n            raise RuntimeError(""Size mismatch, self: {}, rhs: {}"".format(self.size(), rhs.size()))\n        return rhs * 0\n\n    def _prod_batch(self, dim):\n        sizes = list(self.sizes)\n        del sizes[dim]\n        return self.__class__(*sizes, dtype=self._dtype, device=self._device)\n\n    def _quad_form_derivative(self, left_vecs, right_vecs):\n        raise RuntimeError(""Backwards through a ZeroLazyTensor is not possible"")\n\n    def _root_decomposition(self):\n        raise RuntimeError(""ZeroLazyTensors are not positive definite!"")\n\n    def _root_inv_decomposition(self, initial_vectors=None):\n        raise RuntimeError(""ZeroLazyTensors are not positive definite!"")\n\n    def _root_decomposition_size(self):\n        raise RuntimeError(""ZeroLazyTensors are not positive definite!"")\n\n    def _size(self):\n        return torch.Size(self.sizes)\n\n    def _sum_batch(self, dim):\n        sizes = list(self.sizes)\n        del sizes[dim]\n        return self.__class__(*sizes, dtype=self._dtype, device=self._device)\n\n    def _t_matmul(self, rhs):\n        rhs_size_ind = -2 if rhs.ndimension() > 1 else -1\n        if self.size(-1) != rhs.size(rhs_size_ind):\n            raise RuntimeError(""Size mismatch, self: {}, rhs: {}"".format(self.size(), rhs.size()))\n        return rhs * 0\n\n    def _transpose_nonbatch(self):\n        return self.transpose(-2, -1)\n\n    def _unsqueeze_batch(self, dim):\n        sizes = self.sizes.copy()\n        sizes.insert(dim, 1)\n        return self.__class__(*sizes, dtype=self._dtype, device=self._device)\n\n    def add_diag(self, diag):\n        from .diag_lazy_tensor import DiagLazyTensor\n\n        if self.size(-1) != self.size(-2):\n            raise RuntimeError(""add_diag only defined for square matrices"")\n\n        if self.ndimension() == 3:\n            if diag.ndimension() == 0:\n                diag = diag.view(1, 1).expand(self.size(0), self.size(1))\n            elif diag.ndimension() == 1:\n                diag = diag.unsqueeze(0).expand(self.size(0), self.size(1))\n            elif diag.ndimension() == 2:\n                diag = diag.expand(self.size(0), self.size(1))\n            else:\n                raise RuntimeError(\n                    ""For a 3D tensor ({}), add_diag expects a 1D or 2D diag. ""\n                    ""Got size ({})"".format(self.size(), diag.size())\n                )\n        else:\n            if diag.ndimension() == 0:\n                diag = diag.view(1).expand(self.size(0))\n            elif diag.ndimension() == 1:\n                diag = diag.expand(self.size(0))\n            else:\n                raise RuntimeError(\n                    ""For a 3D tensor ({}), add_diag expects a 1D or 2D diag. ""\n                    ""Got size ({})"".format(self.size(), diag.size())\n                )\n\n        res = DiagLazyTensor(diag)\n        if res.size() != self.size():\n            raise RuntimeError(\n                ""Diag dimensions are incompatible with the base LazyTensor dimensions. ""\n                ""Diag size corresponds to a {} Tensor - expected {}"".format(res.size(), self.size())\n            )\n        return res\n\n    def diag(self):\n        shape = self.shape\n        if shape[-1] != shape[-2]:\n            raise RuntimeError(""diag works on square matrices (or batches)"")\n        return torch.zeros(shape[:-1], dtype=self.dtype, device=self.device)\n\n    @cached\n    def evaluate(self):\n        return torch.zeros(*self.sizes)\n\n    def inv_matmul(self, right_tensor, left_tensor=None):\n        raise RuntimeError(""ZeroLazyTensors are not invertible!"")\n\n    def inv_quad(self, tensor):\n        raise RuntimeError(""ZeroLazyTensors are not invertible!"")\n\n    def inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, reduce_inv_quad=True):\n        raise RuntimeError(""ZeroLazyTensors are not invertible!"")\n\n    def logdet(self):\n        return torch.log(torch.tensor(0.0))\n\n    def matmul(self, tensor):\n        tensor_size_ind = -2 if tensor.ndimension() > 1 else -1\n        if self.size(-1) != tensor.size(tensor_size_ind):\n            raise RuntimeError(""Size mismatch, self: {}, tensor: {}"".format(self.size(), tensor.size()))\n        return tensor * 0\n\n    def mul(self, other):\n        shape = _mul_broadcast_shape(self.shape, other.shape)\n        return self.__class__(*shape, dtype=self._dtype, device=self._device)\n\n    def transpose(self, dim1, dim2):\n        sizes = self.sizes.copy()\n        tmp = sizes[dim1]\n        sizes[dim1] = sizes[dim2]\n        sizes[dim2] = tmp\n\n        return ZeroLazyTensor(*sizes)\n\n    def __add__(self, other):\n        return other\n\n    def __div__(self, other):\n        return self\n\n    def __mul__(self, other):\n        return self\n'"
gpytorch/likelihoods/__init__.py,0,"b'#!/usr/bin/env python3\n\nfrom .bernoulli_likelihood import BernoulliLikelihood\nfrom .gaussian_likelihood import FixedNoiseGaussianLikelihood, GaussianLikelihood, _GaussianLikelihoodBase\nfrom .likelihood import Likelihood, _OneDimensionalLikelihood\nfrom .likelihood_list import LikelihoodList\nfrom .multitask_gaussian_likelihood import (\n    MultitaskGaussianLikelihood,\n    MultitaskGaussianLikelihoodKronecker,\n    _MultitaskGaussianLikelihoodBase,\n)\nfrom .noise_models import HeteroskedasticNoise\nfrom .softmax_likelihood import SoftmaxLikelihood\n\n__all__ = [\n    ""_GaussianLikelihoodBase"",\n    ""_OneDimensionalLikelihood"",\n    ""_MultitaskGaussianLikelihoodBase"",\n    ""BernoulliLikelihood"",\n    ""FixedNoiseGaussianLikelihood"",\n    ""GaussianLikelihood"",\n    ""HeteroskedasticNoise"",\n    ""Likelihood"",\n    ""LikelihoodList"",\n    ""MultitaskGaussianLikelihood"",\n    ""MultitaskGaussianLikelihoodKronecker"",\n    ""SoftmaxLikelihood"",\n]\n'"
gpytorch/likelihoods/bernoulli_likelihood.py,2,"b'#!/usr/bin/env python3\n\nimport warnings\n\nimport torch\n\nfrom ..distributions import base_distributions\nfrom ..functions import log_normal_cdf\nfrom .likelihood import _OneDimensionalLikelihood\n\n\nclass BernoulliLikelihood(_OneDimensionalLikelihood):\n    r""""""\n    Implements the Bernoulli likelihood used for GP classification, using\n    Probit regression (i.e., the latent function is warped to be in [0,1]\n    using the standard Normal CDF \\Phi(x)). Given the identity \\Phi(-x) =\n    1-\\Phi(x), we can write the likelihood compactly as:\n\n    .. math::\n        \\begin{equation*}\n            p(Y=y|f)=\\Phi(yf)\n        \\end{equation*}\n    """"""\n\n    def forward(self, function_samples, **kwargs):\n        output_probs = base_distributions.Normal(0, 1).cdf(function_samples)\n        return base_distributions.Bernoulli(probs=output_probs)\n\n    def log_marginal(self, observations, function_dist, *args, **kwargs):\n        marginal = self.marginal(function_dist, *args, **kwargs)\n        return marginal.log_prob(observations)\n\n    def marginal(self, function_dist, **kwargs):\n        mean = function_dist.mean\n        var = function_dist.variance\n        link = mean.div(torch.sqrt(1 + var))\n        output_probs = base_distributions.Normal(0, 1).cdf(link)\n        return base_distributions.Bernoulli(probs=output_probs)\n\n    def expected_log_prob(self, observations, function_dist, *params, **kwargs):\n        if torch.any(observations.eq(-1)):\n            # Remove after 1.0\n            warnings.warn(\n                ""BernoulliLikelihood.expected_log_prob expects observations with labels in {0, 1}. ""\n                ""Observations with labels in {-1, 1} are deprecated."",\n                DeprecationWarning,\n            )\n        else:\n            observations = observations.mul(2).sub(1)\n        # Custom function here so we can use log_normal_cdf rather than Normal.cdf\n        # This is going to be less prone to overflow errors\n        log_prob_lambda = lambda function_samples: log_normal_cdf(function_samples.mul(observations))\n        log_prob = self.quadrature(log_prob_lambda, function_dist)\n        return log_prob\n'"
gpytorch/likelihoods/gaussian_likelihood.py,9,"b'#!/usr/bin/env python3\n\nimport math\nimport warnings\nfrom copy import deepcopy\nfrom typing import Any, Optional\n\nimport torch\nfrom torch import Tensor\n\nfrom ..distributions import MultivariateNormal, base_distributions\nfrom ..lazy import ZeroLazyTensor\nfrom ..utils.warnings import GPInputWarning\nfrom .likelihood import Likelihood\nfrom .noise_models import FixedGaussianNoise, HomoskedasticNoise, Noise\n\n\nclass _GaussianLikelihoodBase(Likelihood):\n    """"""Base class for Gaussian Likelihoods, supporting general heteroskedastic noise models.""""""\n\n    def __init__(self, noise_covar: Noise, **kwargs: Any) -> None:\n\n        super().__init__()\n        param_transform = kwargs.get(""param_transform"")\n        if param_transform is not None:\n            warnings.warn(\n                ""The \'param_transform\' argument is now deprecated. If you want to use a different ""\n                ""transformaton, specify a different \'noise_constraint\' instead."",\n                DeprecationWarning,\n            )\n\n        self.noise_covar = noise_covar\n\n    def _shaped_noise_covar(self, base_shape: torch.Size, *params: Any, **kwargs: Any):\n        return self.noise_covar(*params, shape=base_shape, **kwargs)\n\n    def expected_log_prob(self, target: Tensor, input: MultivariateNormal, *params: Any, **kwargs: Any) -> Tensor:\n        mean, variance = input.mean, input.variance\n        num_event_dim = len(input.event_shape)\n\n        noise = self._shaped_noise_covar(mean.shape, *params, **kwargs).diag()\n        # Potentially reshape the noise to deal with the multitask case\n        noise = noise.view(*noise.shape[:-1], *input.event_shape)\n\n        res = ((target - mean) ** 2 + variance) / noise + noise.log() + math.log(2 * math.pi)\n        res = res.mul(-0.5)\n        if num_event_dim > 1:  # Do appropriate summation for multitask Gaussian likelihoods\n            res = res.sum(list(range(-1, -num_event_dim, -1)))\n        return res\n\n    def forward(self, function_samples: Tensor, *params: Any, **kwargs: Any) -> base_distributions.Normal:\n        noise = self._shaped_noise_covar(function_samples.shape, *params, **kwargs).diag()\n        return base_distributions.Normal(function_samples, noise.sqrt())\n\n    def log_marginal(\n        self, observations: Tensor, function_dist: MultivariateNormal, *params: Any, **kwargs: Any\n    ) -> Tensor:\n        marginal = self.marginal(function_dist, *params, **kwargs)\n        # We\'re making everything conditionally independent\n        indep_dist = base_distributions.Normal(marginal.mean, marginal.variance.clamp_min(1e-8).sqrt())\n        res = indep_dist.log_prob(observations)\n\n        # Do appropriate summation for multitask Gaussian likelihoods\n        num_event_dim = len(function_dist.event_shape)\n        if num_event_dim > 1:\n            res = res.sum(list(range(-1, -num_event_dim, -1)))\n        return res\n\n    def marginal(self, function_dist: MultivariateNormal, *params: Any, **kwargs: Any) -> MultivariateNormal:\n        mean, covar = function_dist.mean, function_dist.lazy_covariance_matrix\n        noise_covar = self._shaped_noise_covar(mean.shape, *params, **kwargs)\n        full_covar = covar + noise_covar\n        return function_dist.__class__(mean, full_covar)\n\n\nclass GaussianLikelihood(_GaussianLikelihoodBase):\n    def __init__(self, noise_prior=None, noise_constraint=None, batch_shape=torch.Size(), **kwargs):\n        noise_covar = HomoskedasticNoise(\n            noise_prior=noise_prior, noise_constraint=noise_constraint, batch_shape=batch_shape\n        )\n        super().__init__(noise_covar=noise_covar)\n\n    @property\n    def noise(self) -> Tensor:\n        return self.noise_covar.noise\n\n    @noise.setter\n    def noise(self, value: Tensor) -> None:\n        self.noise_covar.initialize(noise=value)\n\n    @property\n    def raw_noise(self) -> Tensor:\n        return self.noise_covar.raw_noise\n\n    @raw_noise.setter\n    def raw_noise(self, value: Tensor) -> None:\n        self.noise_covar.initialize(raw_noise=value)\n\n\nclass FixedNoiseGaussianLikelihood(_GaussianLikelihoodBase):\n    """"""\n    A Likelihood that assumes fixed heteroscedastic noise. This is useful when you have fixed, known observation\n    noise for each training example.\n\n    Args:\n        :attr:`noise` (Tensor):\n            Known observation noise (variance) for each training example.\n        :attr:`learn_additional_noise` (bool, optional):\n            Set to true if you additionally want to learn added diagonal noise, similar to GaussianLikelihood.\n\n    Note that this likelihood takes an additional argument when you call it, `noise`, that adds a specified amount\n    of noise to the passed MultivariateNormal. This allows for adding known observational noise to test data.\n\n    Example:\n        >>> train_x = torch.randn(55, 2)\n        >>> noises = torch.ones(55) * 0.01\n        >>> likelihood = FixedNoiseGaussianLikelihood(noise=noises, learn_additional_noise=True)\n        >>> pred_y = likelihood(gp_model(train_x))\n        >>>\n        >>> test_x = torch.randn(21, 2)\n        >>> test_noises = torch.ones(21) * 0.02\n        >>> pred_y = likelihood(gp_model(test_x), noise=test_noises)\n    """"""\n\n    def __init__(\n        self,\n        noise: Tensor,\n        learn_additional_noise: Optional[bool] = False,\n        batch_shape: Optional[torch.Size] = torch.Size(),\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(noise_covar=FixedGaussianNoise(noise=noise))\n\n        if learn_additional_noise:\n            noise_prior = kwargs.get(""noise_prior"", None)\n            noise_constraint = kwargs.get(""noise_constraint"", None)\n            self.second_noise_covar = HomoskedasticNoise(\n                noise_prior=noise_prior, noise_constraint=noise_constraint, batch_shape=batch_shape\n            )\n        else:\n            self.second_noise_covar = None\n\n    @property\n    def noise(self) -> Tensor:\n        return self.noise_covar.noise + self.second_noise\n\n    @noise.setter\n    def noise(self, value: Tensor) -> None:\n        self.noise_covar.initialize(noise=value)\n\n    @property\n    def second_noise(self) -> Tensor:\n        if self.second_noise_covar is None:\n            return 0\n        else:\n            return self.second_noise_covar.noise\n\n    @second_noise.setter\n    def second_noise(self, value: Tensor) -> None:\n        if self.second_noise_covar is None:\n            raise RuntimeError(\n                ""Attempting to set secondary learned noise for FixedNoiseGaussianLikelihood, ""\n                ""but learn_additional_noise must have been False!""\n            )\n        self.second_noise_covar.initialize(noise=value)\n\n    def get_fantasy_likelihood(self, **kwargs):\n        if ""noise"" not in kwargs:\n            raise RuntimeError(""FixedNoiseGaussianLikelihood.fantasize requires a `noise` kwarg"")\n        old_noise_covar = self.noise_covar\n        self.noise_covar = None\n        fantasy_liklihood = deepcopy(self)\n        self.noise_covar = old_noise_covar\n\n        old_noise = old_noise_covar.noise\n        new_noise = kwargs.get(""noise"")\n        if old_noise.dim() != new_noise.dim():\n            old_noise = old_noise.expand(*new_noise.shape[:-1], old_noise.shape[-1])\n        fantasy_liklihood.noise_covar = FixedGaussianNoise(noise=torch.cat([old_noise, new_noise], -1))\n        return fantasy_liklihood\n\n    def _shaped_noise_covar(self, base_shape: torch.Size, *params: Any, **kwargs: Any):\n        if len(params) > 0:\n            # we can infer the shape from the params\n            shape = None\n        else:\n            # here shape[:-1] is the batch shape requested, and shape[-1] is `n`, the number of points\n            shape = base_shape\n\n        res = self.noise_covar(*params, shape=shape, **kwargs)\n\n        if self.second_noise_covar is not None:\n            res = res + self.second_noise_covar(*params, shape=shape, **kwargs)\n        elif isinstance(res, ZeroLazyTensor):\n            warnings.warn(\n                ""You have passed data through a FixedNoiseGaussianLikelihood that did not match the size ""\n                ""of the fixed noise, *and* you did not specify noise. This is treated as a no-op."",\n                GPInputWarning,\n            )\n\n        return res\n'"
gpytorch/likelihoods/likelihood.py,23,"b'#!/usr/bin/env python3\n\nimport math\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom copy import deepcopy\n\nimport torch\n\nfrom .. import settings\nfrom ..distributions import MultivariateNormal, base_distributions\nfrom ..module import Module\nfrom ..utils.quadrature import GaussHermiteQuadrature1D\nfrom ..utils.warnings import GPInputWarning\n\n\nclass _Likelihood(Module, ABC):\n    def __init__(self, max_plate_nesting=1):\n        super().__init__()\n        self.max_plate_nesting = max_plate_nesting\n\n    def _draw_likelihood_samples(self, function_dist, *args, sample_shape=None, **kwargs):\n        if sample_shape is None:\n            sample_shape = torch.Size(\n                [settings.num_likelihood_samples.value()]\n                + [1] * (self.max_plate_nesting - len(function_dist.batch_shape) - 1)\n            )\n        else:\n            sample_shape = sample_shape[: -len(function_dist.batch_shape) - 1]\n        if self.training:\n            num_event_dims = len(function_dist.event_shape)\n            function_dist = base_distributions.Normal(function_dist.mean, function_dist.variance.sqrt())\n            function_dist = base_distributions.Independent(function_dist, num_event_dims - 1)\n        function_samples = function_dist.rsample(sample_shape)\n        return self.forward(function_samples, *args, **kwargs)\n\n    def expected_log_prob(self, observations, function_dist, *args, **kwargs):\n        likelihood_samples = self._draw_likelihood_samples(function_dist, *args, **kwargs)\n        res = likelihood_samples.log_prob(observations).mean(dim=0)\n        return res\n\n    @abstractmethod\n    def forward(self, function_samples, *args, **kwargs):\n        raise NotImplementedError\n\n    def get_fantasy_likelihood(self, **kwargs):\n        return deepcopy(self)\n\n    def log_marginal(self, observations, function_dist, *args, **kwargs):\n        likelihood_samples = self._draw_likelihood_samples(function_dist, *args, **kwargs)\n        log_probs = likelihood_samples.log_prob(observations)\n        res = log_probs.sub(math.log(log_probs.size(0))).logsumexp(dim=0)\n        return res\n\n    def marginal(self, function_dist, *args, **kwargs):\n        res = self._draw_likelihood_samples(function_dist, *args, **kwargs)\n        return res\n\n    def __call__(self, input, *args, **kwargs):\n        # Conditional\n        if torch.is_tensor(input):\n            return super().__call__(input, *args, **kwargs)\n        # Marginal\n        elif isinstance(input, MultivariateNormal):\n            return self.marginal(input, *args, **kwargs)\n        # Error\n        else:\n            raise RuntimeError(\n                ""Likelihoods expects a MultivariateNormal input to make marginal predictions, or a ""\n                ""torch.Tensor for conditional predictions. Got a {}"".format(input.__class__.__name__)\n            )\n\n\ntry:\n    import pyro\n\n    class Likelihood(_Likelihood):\n        r""""""\n        A Likelihood in GPyTorch specifies the mapping from latent function values\n        :math:`f(\\mathbf X)` to observed labels :math:`y`.\n\n        For example, in the case of regression this might be a Gaussian\n        distribution, as :math:`y(\\mathbf x)` is equal to :math:`f(\\mathbf x)` plus Gaussian noise:\n\n        .. math::\n            y(\\mathbf x) = f(\\mathbf x) + \\epsilon, \\:\\:\\:\\: \\epsilon ~ N(0,\\sigma^{2}_{n} \\mathbf I)\n\n        In the case of classification, this might be a Bernoulli distribution,\n        where the probability that :math:`y=1` is given by the latent function\n        passed through some sigmoid or probit function:\n\n        .. math::\n            y(\\mathbf x) = \\begin{cases}\n                1 & \\text{w/ probability} \\:\\: \\sigma(f(\\mathbf x)) \\\\\n                0 & \\text{w/ probability} \\:\\: 1-\\sigma(f(\\mathbf x))\n            \\end{cases}\n\n        In either case, to implement a likelihood function, GPyTorch only\n        requires a :attr:`forward` method that computes the conditional distribution\n        :math:`p(y \\mid f(\\mathbf x))`.\n\n        Calling this object does one of two things:\n\n            - If likelihood is called with a :class:`torch.Tensor` object, then it is\n              assumed that the input is samples from :math:`f(\\mathbf x)`. This\n              returns the *conditional* distribution `p(y|f(\\mathbf x))`.\n            - If likelihood is called with a :class:`~gpytorch.distribution.MultivariateNormal` object,\n              then it is assumed that the input is the distribution :math:`f(\\mathbf x)`.\n              This returns the *marginal* distribution `p(y|\\mathbf x)`.\n\n        Args:\n            :attr:`max_plate_nesting` (int, default=1)\n                (For Pyro integration only). How many batch dimensions are in the function.\n                This should be modified if the likelihood uses plated random variables.\n        """"""\n\n        @property\n        def num_data(self):\n            if hasattr(self, ""_num_data""):\n                return self._num_data\n            else:\n                warnings.warn(\n                    ""likelihood.num_data isn\'t set. This might result in incorrect ELBO scaling."", GPInputWarning\n                )\n                return """"\n\n        @num_data.setter\n        def num_data(self, val):\n            self._num_data = val\n\n        @property\n        def name_prefix(self):\n            if hasattr(self, ""_name_prefix""):\n                return self._name_prefix\n            else:\n                return """"\n\n        @name_prefix.setter\n        def name_prefix(self, val):\n            self._name_prefix = val\n\n        def _draw_likelihood_samples(self, function_dist, *args, sample_shape=None, **kwargs):\n            if self.training:\n                num_event_dims = len(function_dist.event_shape)\n                function_dist = base_distributions.Normal(function_dist.mean, function_dist.variance.sqrt())\n                function_dist = base_distributions.Independent(function_dist, num_event_dims - 1)\n\n            plate_name = self.name_prefix + "".num_particles_vectorized""\n            num_samples = settings.num_likelihood_samples.value()\n            max_plate_nesting = max(self.max_plate_nesting, len(function_dist.batch_shape))\n            with pyro.plate(plate_name, size=num_samples, dim=(-max_plate_nesting - 1)):\n                if sample_shape is None:\n                    function_samples = pyro.sample(self.name_prefix, function_dist.mask(False))\n                    # Deal with the fact that we\'re not assuming conditional indendence over data points here\n                    function_samples = function_samples.squeeze(-len(function_dist.event_shape) - 1)\n                else:\n                    sample_shape = sample_shape[: -len(function_dist.batch_shape)]\n                    function_samples = function_dist(sample_shape)\n\n                if not self.training:\n                    function_samples = function_samples.squeeze(-len(function_dist.event_shape) - 1)\n                return self.forward(function_samples, *args, **kwargs)\n\n        def expected_log_prob(self, observations, function_dist, *args, **kwargs):\n            r""""""\n            (Used by :obj:`~gpytorch.mlls.VariationalELBO` for variational inference.)\n\n            Computes the expected log likelihood, where the expectation is over the GP variational distribution.\n\n            .. math::\n                \\sum_{\\mathbf x, y} \\mathbb{E}_{q\\left( f(\\mathbf x) \\right)}\n                \\left[ \\log p \\left( y \\mid f(\\mathbf x) \\right) \\right]\n\n            Args:\n                :attr:`observations` (:class:`torch.Tensor`)\n                    Values of :math:`y`.\n                :attr:`function_dist` (:class:`~gpytorch.distributions.MultivariateNormal`)\n                    Distribution for :math:`f(x)`.\n                :attr:`args`, :attr:`kwargs`\n                    Passed to the `forward` function\n\n            Returns\n                `torch.Tensor` (log probability)\n            """"""\n            return super().expected_log_prob(observations, function_dist, *args, **kwargs)\n\n        @abstractmethod\n        def forward(self, function_samples, *args, data={}, **kwargs):\n            r""""""\n            Computes the conditional distribution :math:`p(\\mathbf y \\mid\n            \\mathbf f, \\ldots)` that defines the likelihood.\n\n            :param torch.Tensor function_samples: Samples from the function (:math:`\\mathbf f`)\n            :param dict data: (Optional, Pyro integration only) Additional\n                variables (:math:`\\ldots`) that the likelihood needs to condition\n                on. The keys of the dictionary will correspond to Pyro sample sites\n                in the likelihood\'s model/guide.\n            :param args: Additional args\n            :param kwargs: Additional kwargs\n            :return: Distribution object (with same shape as :attr:`function_samples`)\n            :rtype: :obj:`Distribution`\n            """"""\n            raise NotImplementedError\n\n        def get_fantasy_likelihood(self, **kwargs):\n            """"""\n            """"""\n            return super().get_fantasy_likelihood(**kwargs)\n\n        def log_marginal(self, observations, function_dist, *args, **kwargs):\n            r""""""\n            (Used by :obj:`~gpytorch.mlls.PredictiveLogLikelihood` for approximate inference.)\n\n            Computes the log marginal likelihood of the approximate predictive distribution\n\n            .. math::\n                \\sum_{\\mathbf x, y} \\log \\mathbb{E}_{q\\left( f(\\mathbf x) \\right)}\n                \\left[ p \\left( y \\mid f(\\mathbf x) \\right) \\right]\n\n            Note that this differs from :meth:`expected_log_prob` because the :math:`log` is on the outside\n            of the expectation.\n\n            Args:\n                :attr:`observations` (:class:`torch.Tensor`)\n                    Values of :math:`y`.\n                :attr:`function_dist` (:class:`~gpytorch.distributions.MultivariateNormal`)\n                    Distribution for :math:`f(x)`.\n                :attr:`args`, :attr:`kwargs`\n                    Passed to the `forward` function\n\n            Returns\n                `torch.Tensor` (log probability)\n            """"""\n            return super().log_marginal(observations, function_dist, *args, **kwargs)\n\n        def marginal(self, function_dist, *args, **kwargs):\n            r""""""\n            Computes a predictive distribution :math:`p(y^* | \\mathbf x^*)` given either a posterior\n            distribution :math:`p(\\mathbf f | \\mathcal D, \\mathbf x)` or a\n            prior distribution :math:`p(\\mathbf f|\\mathbf x)` as input.\n\n            With both exact inference and variational inference, the form of\n            :math:`p(\\mathbf f|\\mathcal D, \\mathbf x)` or :math:`p(\\mathbf f|\n            \\mathbf x)` should usually be Gaussian. As a result, :attr:`function_dist`\n            should usually be a :obj:`~gpytorch.distributions.MultivariateNormal` specified by the mean and\n            (co)variance of :math:`p(\\mathbf f|...)`.\n\n            Args:\n                :attr:`function_dist` (:class:`~gpytorch.distributions.MultivariateNormal`)\n                    Distribution for :math:`f(x)`.\n                :attr:`args`, :attr:`kwargs`\n                    Passed to the `forward` function\n\n            Returns:\n                Distribution object (the marginal distribution, or samples from it)\n            """"""\n            return super().marginal(function_dist, *args, **kwargs)\n\n        def pyro_guide(self, function_dist, target, *args, **kwargs):\n            r""""""\n            (For Pyro integration only).\n\n            Part of the guide function for the likelihood.\n            This should be re-defined if the likelihood contains any latent variables that need to be infered.\n\n            :param ~gpytorch.distributions.MultivariateNormal function_dist: Distribution of latent function\n                :math:`q(\\mathbf f)`.\n            :param torch.Tensor target: Observed :math:`\\mathbf y`.\n            :param args: Additional args (for :meth:`~forward`).\n            :param kwargs: Additional kwargs (for :meth:`~forward`).\n            """"""\n            with pyro.plate(self.name_prefix + "".data_plate"", dim=-1):\n                pyro.sample(self.name_prefix + "".f"", function_dist)\n\n        def pyro_model(self, function_dist, target, *args, **kwargs):\n            r""""""\n            (For Pyro integration only).\n\n            Part of the model function for the likelihood.\n            It should return the\n            This should be re-defined if the likelihood contains any latent variables that need to be infered.\n\n            :param ~gpytorch.distributions.MultivariateNormal function_dist: Distribution of latent function\n                :math:`p(\\mathbf f)`.\n            :param torch.Tensor target: Observed :math:`\\mathbf y`.\n            :param args: Additional args (for :meth:`~forward`).\n            :param kwargs: Additional kwargs (for :meth:`~forward`).\n            """"""\n            with pyro.plate(self.name_prefix + "".data_plate"", dim=-1):\n                function_samples = pyro.sample(self.name_prefix + "".f"", function_dist)\n                output_dist = self(function_samples, *args, **kwargs)\n                return self.sample_target(output_dist, target)\n\n        def sample_target(self, output_dist, target):\n            scale = (self.num_data or output_dist.batch_shape[-1]) / output_dist.batch_shape[-1]\n            with pyro.poutine.scale(scale=scale):\n                return pyro.sample(self.name_prefix + "".y"", output_dist, obs=target)\n\n        def __call__(self, input, *args, **kwargs):\n            # Conditional\n            if torch.is_tensor(input):\n                return super().__call__(input, *args, **kwargs)\n            # Marginal\n            elif any(\n                [\n                    isinstance(input, MultivariateNormal),\n                    isinstance(input, pyro.distributions.Normal),\n                    (\n                        isinstance(input, pyro.distributions.Independent)\n                        and isinstance(input.base_dist, pyro.distributions.Normal)\n                    ),\n                ]\n            ):\n                return self.marginal(input, *args, **kwargs)\n            # Error\n            else:\n                raise RuntimeError(\n                    ""Likelihoods expects a MultivariateNormal or Normal input to make marginal predictions, or a ""\n                    ""torch.Tensor for conditional predictions. Got a {}"".format(input.__class__.__name__)\n                )\n\n\nexcept ImportError:\n\n    class Likelihood(_Likelihood):\n        @property\n        def num_data(self):\n            warnings.warn(""num_data is only used for likehoods that are integrated with Pyro."", RuntimeWarning)\n            return 0\n\n        @num_data.setter\n        def num_data(self, val):\n            warnings.warn(""num_data is only used for likehoods that are integrated with Pyro."", RuntimeWarning)\n\n        @property\n        def name_prefix(self):\n            warnings.warn(""name_prefix is only used for likehoods that are integrated with Pyro."", RuntimeWarning)\n            return """"\n\n        @name_prefix.setter\n        def name_prefix(self, val):\n            warnings.warn(""name_prefix is only used for likehoods that are integrated with Pyro."", RuntimeWarning)\n\n\nclass _OneDimensionalLikelihood(Likelihood, ABC):\n    r""""""\n    A specific case of :obj:`~gpytorch.likelihoods.Likelihood` when the GP represents a one-dimensional\n    output. (I.e. for a specific :math:`\\mathbf x`, :math:`f(\\mathbf x) \\in \\mathbb{R}`.)\n\n    Inheriting from this likelihood reduces the variance when computing approximate GP objective functions\n    by using 1D Gauss-Hermite quadrature.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.quadrature = GaussHermiteQuadrature1D()\n\n    def expected_log_prob(self, observations, function_dist, *args, **kwargs):\n        log_prob_lambda = lambda function_samples: self.forward(function_samples).log_prob(observations)\n        log_prob = self.quadrature(log_prob_lambda, function_dist)\n        return log_prob\n\n    def log_marginal(self, observations, function_dist, *args, **kwargs):\n        prob_lambda = lambda function_samples: self.forward(function_samples).log_prob(observations).exp()\n        prob = self.quadrature(prob_lambda, function_dist)\n        return prob.log()\n'"
gpytorch/likelihoods/likelihood_list.py,2,"b'#! /usr/bin/env python3\n\nfrom torch.nn import ModuleList\n\nfrom gpytorch.likelihoods import Likelihood\n\n\ndef _get_tuple_args_(*args):\n    for arg in args:\n        if isinstance(arg, tuple):\n            yield arg\n        else:\n            yield (arg,)\n\n\nclass LikelihoodList(Likelihood):\n    def __init__(self, *likelihoods):\n        super().__init__()\n        self.likelihoods = ModuleList(likelihoods)\n\n    def expected_log_prob(self, *args, **kwargs):\n        return [\n            likelihood.expected_log_prob(*args_, **kwargs)\n            for likelihood, args_ in zip(self.likelihoods, _get_tuple_args_(*args))\n        ]\n\n    def forward(self, *args, **kwargs):\n        if ""noise"" in kwargs:\n            noise = kwargs.pop(""noise"")\n            # if noise kwarg is passed, assume it\'s an iterable of noise tensors\n            return [\n                likelihood.forward(*args_, {**kwargs, ""noise"": noise_})\n                for likelihood, args_, noise_ in zip(self.likelihoods, _get_tuple_args_(*args), noise)\n            ]\n        else:\n            return [\n                likelihood.forward(*args_, **kwargs)\n                for likelihood, args_ in zip(self.likelihoods, _get_tuple_args_(*args))\n            ]\n\n    def pyro_sample_output(self, *args, **kwargs):\n        return [\n            likelihood.pyro_sample_output(*args_, **kwargs)\n            for likelihood, args_ in zip(self.likelihoods, _get_tuple_args_(*args))\n        ]\n\n    def __call__(self, *args, **kwargs):\n        if ""noise"" in kwargs:\n            noise = kwargs.pop(""noise"")\n            # if noise kwarg is passed, assume it\'s an iterable of noise tensors\n            return [\n                likelihood(*args_, {**kwargs, ""noise"": noise_})\n                for likelihood, args_, noise_ in zip(self.likelihoods, _get_tuple_args_(*args), noise)\n            ]\n        else:\n            return [\n                likelihood(*args_, **kwargs) for likelihood, args_ in zip(self.likelihoods, _get_tuple_args_(*args))\n            ]\n'"
gpytorch/likelihoods/multitask_gaussian_likelihood.py,35,"b'#!/usr/bin/env python3\n\nimport warnings\nfrom typing import Any\n\nimport torch\nfrom torch import Tensor\n\nfrom ..constraints import GreaterThan\nfrom ..distributions import base_distributions\nfrom ..functions import add_diag\nfrom ..lazy import (\n    BlockDiagLazyTensor,\n    DiagLazyTensor,\n    KroneckerProductLazyTensor,\n    MatmulLazyTensor,\n    RootLazyTensor,\n    lazify,\n)\nfrom ..likelihoods import Likelihood, _GaussianLikelihoodBase\nfrom ..utils.warnings import OldVersionWarning\nfrom .noise_models import MultitaskHomoskedasticNoise\n\n\nclass _MultitaskGaussianLikelihoodBase(_GaussianLikelihoodBase):\n    """"""Base class for multi-task Gaussian Likelihoods, supporting general heteroskedastic noise models. """"""\n\n    def __init__(self, num_tasks, noise_covar, rank=0, task_correlation_prior=None, batch_shape=torch.Size()):\n        """"""\n        Args:\n            num_tasks (int):\n                Number of tasks.\n            noise_covar (:obj:`gpytorch.module.Module`):\n                A model for the noise covariance. This can be a simple homoskedastic noise model, or a GP\n                that is to be fitted on the observed measurement errors.\n            rank (int):\n                The rank of the task noise covariance matrix to fit. If `rank` is set to 0, then a diagonal covariance\n                matrix is fit.\n            task_correlation_prior (:obj:`gpytorch.priors.Prior`):\n                Prior to use over the task noise correlation matrix. Only used when `rank` > 0.\n            batch_shape (torch.Size):\n                Number of batches.\n        """"""\n        super().__init__(noise_covar=noise_covar)\n        if rank != 0:\n            if rank > num_tasks:\n                raise ValueError(f""Cannot have rank ({rank}) greater than num_tasks ({num_tasks})"")\n            tidcs = torch.tril_indices(num_tasks, rank, dtype=torch.long)\n            self.tidcs = tidcs[:, 1:]  # (1, 1) must be 1.0, no need to parameterize this\n            task_noise_corr = torch.randn(*batch_shape, self.tidcs.size(-1))\n            self.register_parameter(""task_noise_corr"", torch.nn.Parameter(task_noise_corr))\n            if task_correlation_prior is not None:\n                self.register_prior(\n                    ""MultitaskErrorCorrelationPrior"", task_correlation_prior, lambda: self._eval_corr_matrix\n                )\n        elif task_correlation_prior is not None:\n            raise ValueError(""Can only specify task_correlation_prior if rank>0"")\n        self.num_tasks = num_tasks\n        self.rank = rank\n        # Handle deprecation of parameterization - TODO: Remove in future release\n        self._register_load_state_dict_pre_hook(deprecate_task_noise_corr)\n\n    def _eval_corr_matrix(self):\n        tnc = self.task_noise_corr\n        fac_diag = torch.ones(*tnc.shape[:-1], self.num_tasks, device=tnc.device, dtype=tnc.dtype)\n        Cfac = torch.diag_embed(fac_diag)\n        Cfac[..., self.tidcs[0], self.tidcs[1]] = self.task_noise_corr\n        # squared rows must sum to one for this to be a correlation matrix\n        C = Cfac / Cfac.pow(2).sum(dim=-1, keepdim=True).sqrt()\n        return C @ C.transpose(-1, -2)\n\n    def _shaped_noise_covar(self, base_shape, *params):\n        if len(base_shape) >= 2:\n            *batch_shape, n, _ = base_shape\n        else:\n            *batch_shape, n = base_shape\n\n        # compute the noise covariance\n        if len(params) > 0:\n            shape = None\n        else:\n            shape = base_shape if len(base_shape) == 1 else base_shape[:-1]\n        noise_covar = self.noise_covar(*params, shape=shape)\n\n        if self.rank > 0:\n            # if rank > 0, compute the task correlation matrix\n            # TODO: This is inefficient, change repeat so it can repeat LazyTensors w/ multiple batch dimensions\n            task_corr = self._eval_corr_matrix()\n            exp_shape = torch.Size([*batch_shape, n]) + task_corr.shape[-2:]\n            task_corr_exp = lazify(task_corr.unsqueeze(-3).expand(exp_shape))\n            noise_sem = noise_covar.sqrt()\n            task_covar_blocks = MatmulLazyTensor(MatmulLazyTensor(noise_sem, task_corr_exp), noise_sem)\n        else:\n            # otherwise tasks are uncorrelated\n            task_covar_blocks = noise_covar\n\n        if len(batch_shape) == 1:\n            # TODO: Properly support general batch shapes in BlockDiagLazyTensor (no shape arithmetic)\n            tcb_eval = task_covar_blocks.evaluate()\n            task_covar = BlockDiagLazyTensor(lazify(tcb_eval), block_dim=-3)\n        else:\n            task_covar = BlockDiagLazyTensor(task_covar_blocks)\n\n        return task_covar\n\n    def forward(self, function_samples: Tensor, *params: Any, **kwargs: Any) -> base_distributions.Normal:\n        noise = self._shaped_noise_covar(function_samples.shape, *params, **kwargs).diag()\n        noise = noise.view(*noise.shape[:-1], *function_samples.shape[-2:])\n        return base_distributions.Independent(base_distributions.Normal(function_samples, noise.sqrt()), 1)\n\n\nclass MultitaskGaussianLikelihood(_MultitaskGaussianLikelihoodBase):\n    """"""\n    A convenient extension of the :class:`gpytorch.likelihoods.GaussianLikelihood` to the multitask setting that allows\n    for a full cross-task covariance structure for the noise. The fitted covariance matrix has rank `rank`.\n    If a strictly diagonal task noise covariance matrix is desired, then rank=0 should be set. (This option still\n    allows for a different `log_noise` parameter for each task.). This likelihood assumes homoskedastic noise.\n\n    Like the Gaussian likelihood, this object can be used with exact inference.\n    """"""\n\n    def __init__(\n        self,\n        num_tasks,\n        rank=0,\n        task_correlation_prior=None,\n        batch_shape=torch.Size(),\n        noise_prior=None,\n        noise_constraint=None,\n    ):\n        """"""\n        Args:\n            num_tasks (int): Number of tasks.\n\n            rank (int): The rank of the task noise covariance matrix to fit. If `rank` is set to 0,\n            then a diagonal covariance matrix is fit.\n\n            task_correlation_prior (:obj:`gpytorch.priors.Prior`): Prior to use over the task noise correlaton matrix.\n            Only used when `rank` > 0.\n\n        """"""\n        if noise_constraint is None:\n            noise_constraint = GreaterThan(1e-4)\n\n        noise_covar = MultitaskHomoskedasticNoise(\n            num_tasks=num_tasks, noise_prior=noise_prior, noise_constraint=noise_constraint, batch_shape=batch_shape\n        )\n        super().__init__(\n            num_tasks=num_tasks,\n            noise_covar=noise_covar,\n            rank=rank,\n            task_correlation_prior=task_correlation_prior,\n            batch_shape=batch_shape,\n        )\n\n        self.register_parameter(name=""raw_noise"", parameter=torch.nn.Parameter(torch.zeros(*batch_shape, 1)))\n        self.register_constraint(""raw_noise"", noise_constraint)\n\n    @property\n    def noise(self):\n        return self.raw_noise_constraint.transform(self.raw_noise)\n\n    @noise.setter\n    def noise(self, value):\n        self._set_noise(value)\n\n    def _set_noise(self, value):\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_noise)\n        self.initialize(raw_noise=self.raw_noise_constraint.inverse_transform(value))\n\n    def _shaped_noise_covar(self, base_shape, *params):\n        noise_covar = super()._shaped_noise_covar(base_shape, *params)\n        noise = self.noise\n        return noise_covar.add_diag(noise)\n\n\nclass MultitaskGaussianLikelihoodKronecker(_MultitaskGaussianLikelihoodBase):\n    """"""\n    A convenient extension of the :class:`gpytorch.likelihoods.GaussianLikelihood` to the multitask setting that allows\n    for a full cross-task covariance structure for the noise. The fitted covariance matrix has rank `rank`.\n    If a strictly diagonal task noise covariance matrix is desired, then rank=0 should be set. (This option still\n    allows for a different `noise` parameter for each task.)\n\n    Like the Gaussian likelihood, this object can be used with exact inference.\n\n    Note: This Likelihood is scheduled to be deprecated and replaced by an improved version of\n    `MultitaskGaussianLikelihood`. Use this only for compatibility with batched Multitask models.\n    """"""\n\n    def __init__(\n        self, num_tasks, rank=0, task_prior=None, batch_shape=torch.Size(), noise_prior=None, noise_constraint=None\n    ):\n        """"""\n        Args:\n            num_tasks (int): Number of tasks.\n\n            rank (int): The rank of the task noise covariance matrix to fit. If `rank` is set to 0,\n            then a diagonal covariance matrix is fit.\n\n            task_prior (:obj:`gpytorch.priors.Prior`): Prior to use over the task noise covariance matrix if\n            `rank` > 0, or a prior over the log of just the diagonal elements, if `rank` == 0.\n\n        """"""\n        super(Likelihood, self).__init__()\n        if noise_constraint is None:\n            noise_constraint = GreaterThan(1e-4)\n        self.register_parameter(name=""raw_noise"", parameter=torch.nn.Parameter(torch.zeros(*batch_shape, 1)))\n        if rank == 0:\n            self.register_parameter(\n                name=""raw_task_noises"", parameter=torch.nn.Parameter(torch.zeros(*batch_shape, num_tasks))\n            )\n            if task_prior is not None:\n                raise RuntimeError(""Cannot set a `task_prior` if rank=0"")\n        else:\n            self.register_parameter(\n                name=""task_noise_covar_factor"", parameter=torch.nn.Parameter(torch.randn(*batch_shape, num_tasks, rank))\n            )\n            if task_prior is not None:\n                self.register_prior(""MultitaskErrorCovariancePrior"", task_prior, self._eval_covar_matrix)\n        self.num_tasks = num_tasks\n        self.rank = rank\n\n        self.register_constraint(""raw_noise"", noise_constraint)\n\n    @property\n    def noise(self):\n        return self.raw_noise_constraint.transform(self.raw_noise)\n\n    @noise.setter\n    def noise(self, value):\n        self._set_noise(value)\n\n    def _set_noise(self, value):\n        self.initialize(raw_noise=self.raw_noise_constraint.inverse_transform(value))\n\n    def _eval_covar_matrix(self):\n        covar_factor = self.task_noise_covar_factor\n        noise = self.noise\n        D = noise * torch.eye(self.num_tasks, dtype=noise.dtype, device=noise.device)\n        return covar_factor.matmul(covar_factor.transpose(-1, -2)) + D\n\n    def marginal(self, function_dist, *params, **kwargs):\n        r""""""\n        Adds the task noises to the diagonal of the covariance matrix of the supplied\n        :obj:`gpytorch.distributions.MultivariateNormal` or :obj:`gpytorch.distributions.MultitaskMultivariateNormal`,\n        in case of `rank` == 0. Otherwise, adds a rank `rank` covariance matrix to it.\n\n        To accomplish this, we form a new :obj:`gpytorch.lazy.KroneckerProductLazyTensor` between :math:`I_{n}`,\n        an identity matrix with size equal to the data and a (not necessarily diagonal) matrix containing the task\n        noises :math:`D_{t}`.\n\n        We also incorporate a shared `noise` parameter from the base\n        :class:`gpytorch.likelihoods.GaussianLikelihood` that we extend.\n\n        The final covariance matrix after this method is then :math:`K + D_{t} \\otimes I_{n} + \\sigma^{2}I_{nt}`.\n\n        Args:\n            function_dist (:obj:`gpytorch.distributions.MultitaskMultivariateNormal`): Random variable whose covariance\n                matrix is a :obj:`gpytorch.lazy.LazyTensor` we intend to augment.\n        Returns:\n            :obj:`gpytorch.distributions.MultitaskMultivariateNormal`: A new random variable whose covariance\n            matrix is a :obj:`gpytorch.lazy.LazyTensor` with :math:`D_{t} \\otimes I_{n}` and :math:`\\sigma^{2}I_{nt}`\n            added.\n        """"""\n        mean, covar = function_dist.mean, function_dist.lazy_covariance_matrix\n\n        if self.rank == 0:\n            task_noises = self.raw_noise_constraint.transform(self.raw_task_noises)\n            task_var_lt = DiagLazyTensor(task_noises)\n            dtype, device = task_noises.dtype, task_noises.device\n        else:\n            task_noise_covar_factor = self.task_noise_covar_factor\n            task_var_lt = RootLazyTensor(task_noise_covar_factor)\n            dtype, device = task_noise_covar_factor.dtype, task_noise_covar_factor.device\n\n        eye_lt = DiagLazyTensor(\n            torch.ones(*covar.batch_shape, covar.size(-1) // self.num_tasks, dtype=dtype, device=device)\n        )\n        task_var_lt = task_var_lt.expand(*covar.batch_shape, *task_var_lt.matrix_shape)\n\n        covar_kron_lt = KroneckerProductLazyTensor(eye_lt, task_var_lt)\n        covar = covar + covar_kron_lt\n\n        noise = self.noise\n        covar = add_diag(covar, noise)\n        return function_dist.__class__(mean, covar)\n\n\ndef deprecate_task_noise_corr(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n    if prefix + ""task_noise_corr_factor"" in state_dict:\n        # Remove after 1.0\n        warnings.warn(\n            ""Loading a deprecated parameterization of _MultitaskGaussianLikelihoodBase. Consider re-saving your model."",\n            OldVersionWarning,\n        )\n        # construct the task correlation matrix from the factors using the old parameterization\n        corr_factor = state_dict.pop(prefix + ""task_noise_corr_factor"").squeeze(0)\n        corr_diag = state_dict.pop(prefix + ""task_noise_corr_diag"").squeeze(0)\n        num_tasks, rank = corr_factor.shape[-2:]\n        M = corr_factor.matmul(corr_factor.transpose(-1, -2))\n        idx = torch.arange(M.shape[-1], dtype=torch.long, device=M.device)\n        M[..., idx, idx] += corr_diag\n        sem_inv = 1 / torch.diagonal(M, dim1=-2, dim2=-1).sqrt().unsqueeze(-1)\n        C = M * sem_inv.matmul(sem_inv.transpose(-1, -2))\n        # perform a Cholesky decomposition and extract the required entries\n        L = torch.cholesky(C)\n        tidcs = torch.tril_indices(num_tasks, rank)[:, 1:]\n        task_noise_corr = L[..., tidcs[0], tidcs[1]]\n        state_dict[prefix + ""task_noise_corr""] = task_noise_corr\n'"
gpytorch/likelihoods/noise_models.py,15,"b'#!/usr/bin/env python3\n\nfrom typing import Any, Optional\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn import Parameter\n\nfrom .. import settings\nfrom ..constraints import GreaterThan\nfrom ..distributions import MultivariateNormal\nfrom ..lazy import DiagLazyTensor, ZeroLazyTensor\nfrom ..module import Module\nfrom ..utils.broadcasting import _mul_broadcast_shape\n\n\nclass Noise(Module):\n    pass\n\n\nclass _HomoskedasticNoiseBase(Noise):\n    def __init__(self, noise_prior=None, noise_constraint=None, batch_shape=torch.Size(), num_tasks=1):\n        super().__init__()\n        if noise_constraint is None:\n            noise_constraint = GreaterThan(1e-4)\n\n        self.register_parameter(name=""raw_noise"", parameter=Parameter(torch.zeros(*batch_shape, num_tasks)))\n        if noise_prior is not None:\n            self.register_prior(""noise_prior"", noise_prior, lambda: self.noise, lambda v: self._set_noise(v))\n\n        self.register_constraint(""raw_noise"", noise_constraint)\n\n    @property\n    def noise(self):\n        return self.raw_noise_constraint.transform(self.raw_noise)\n\n    @noise.setter\n    def noise(self, value: Tensor) -> None:\n        self._set_noise(value)\n\n    def _set_noise(self, value: Tensor) -> None:\n        if not torch.is_tensor(value):\n            value = torch.as_tensor(value).to(self.raw_noise)\n        self.initialize(raw_noise=self.raw_noise_constraint.inverse_transform(value))\n\n    def forward(self, *params: Any, shape: Optional[torch.Size] = None, **kwargs: Any) -> DiagLazyTensor:\n        """"""In the homoskedastic case, the parameters are only used to infer the required shape.\n        Here are the possible scenarios:\n        - non-batched noise, non-batched input, non-MT -> noise_diag shape is `n`\n        - non-batched noise, non-batched input, MT -> noise_diag shape is `nt`\n        - non-batched noise, batched input, non-MT -> noise_diag shape is `b x n` with b\' the broadcasted batch shape\n        - non-batched noise, batched input, MT -> noise_diag shape is `b x nt`\n        - batched noise, non-batched input, non-MT -> noise_diag shape is `b x n`\n        - batched noise, non-batched input, MT -> noise_diag shape is `b x nt`\n        - batched noise, batched input, non-MT -> noise_diag shape is `b\' x n`\n        - batched noise, batched input, MT -> noise_diag shape is `b\' x nt`\n        where `n` is the number of evaluation points and `t` is the number of tasks (i.e. `num_tasks` of self.noise).\n        So bascially the shape is always `b\' x nt`, with `b\'` appropriately broadcast from the noise parameter and\n        input batch shapes. `n` and the input batch shape are determined either from the shape arg or from the params\n        input. For this it is sufficient to take in a single `shape` arg, with the convention that shape[:-1] is the\n        batch shape of the input, and shape[-1] is `n`.\n\n        If a ""noise"" kwarg (a Tensor) is provided, this noise is used directly.\n        """"""\n        if ""noise"" in kwargs:\n            return DiagLazyTensor(kwargs.get(""noise""))\n        if shape is None:\n            p = params[0] if torch.is_tensor(params[0]) else params[0][0]\n            shape = p.shape if len(p.shape) == 1 else p.shape[:-1]\n        noise = self.noise\n        *batch_shape, n = shape\n        noise_batch_shape = noise.shape[:-1] if noise.dim() > 1 else torch.Size()\n        num_tasks = noise.shape[-1]\n        batch_shape = _mul_broadcast_shape(noise_batch_shape, batch_shape)\n        noise = noise.unsqueeze(-2)\n        noise_diag = noise.expand(*batch_shape, n, num_tasks).contiguous()\n        if num_tasks == 1:\n            noise_diag = noise_diag.view(*batch_shape, n)\n        return DiagLazyTensor(noise_diag)\n\n\nclass HomoskedasticNoise(_HomoskedasticNoiseBase):\n    def __init__(self, noise_prior=None, noise_constraint=None, batch_shape=torch.Size()):\n        super().__init__(\n            noise_prior=noise_prior, noise_constraint=noise_constraint, batch_shape=batch_shape, num_tasks=1\n        )\n\n\nclass MultitaskHomoskedasticNoise(_HomoskedasticNoiseBase):\n    def __init__(self, num_tasks, noise_prior=None, noise_constraint=None, batch_shape=torch.Size()):\n        super().__init__(\n            noise_prior=noise_prior, noise_constraint=noise_constraint, batch_shape=batch_shape, num_tasks=num_tasks\n        )\n\n\nclass HeteroskedasticNoise(Noise):\n    def __init__(self, noise_model, noise_indices=None, noise_constraint=None):\n        if noise_constraint is None:\n            noise_constraint = GreaterThan(1e-4)\n        super().__init__()\n        self.noise_model = noise_model\n        self._noise_constraint = noise_constraint\n        self._noise_indices = noise_indices\n\n    def forward(\n        self,\n        *params: Any,\n        batch_shape: Optional[torch.Size] = None,\n        shape: Optional[torch.Size] = None,\n        noise: Optional[Tensor] = None,\n    ) -> DiagLazyTensor:\n        if noise is not None:\n            return DiagLazyTensor(noise)\n        training = self.noise_model.training  # keep track of mode\n        self.noise_model.eval()  # we want the posterior prediction of the noise model\n        with settings.detach_test_caches(False), settings.debug(False):\n            if len(params) == 1 and not torch.is_tensor(params[0]):\n                output = self.noise_model(*params[0])\n            else:\n                output = self.noise_model(*params)\n        self.noise_model.train(training)\n        if not isinstance(output, MultivariateNormal):\n            raise NotImplementedError(""Currently only noise models that return a MultivariateNormal are supported"")\n        # note: this also works with MultitaskMultivariateNormal, where this\n        # will return a batched DiagLazyTensors of size n x num_tasks x num_tasks\n        noise_diag = output.mean if self._noise_indices is None else output.mean[..., self._noise_indices]\n        return DiagLazyTensor(self._noise_constraint.transform(noise_diag))\n\n\nclass FixedGaussianNoise(Module):\n    def __init__(self, noise: Tensor) -> None:\n        super().__init__()\n        self.noise = noise\n\n    def forward(\n        self, *params: Any, shape: Optional[torch.Size] = None, noise: Optional[Tensor] = None, **kwargs: Any\n    ) -> DiagLazyTensor:\n        if shape is None:\n            p = params[0] if torch.is_tensor(params[0]) else params[0][0]\n            shape = p.shape if len(p.shape) == 1 else p.shape[:-1]\n\n        if noise is not None:\n            return DiagLazyTensor(noise)\n        elif shape[-1] == self.noise.shape[-1]:\n            return DiagLazyTensor(self.noise)\n        else:\n            return ZeroLazyTensor()\n\n    def _apply(self, fn):\n        self.noise = fn(self.noise)\n        return super(FixedGaussianNoise, self)._apply(fn)\n'"
gpytorch/likelihoods/softmax_likelihood.py,1,"b'#!/usr/bin/env python3\n\nimport warnings\n\nimport torch\n\nfrom ..distributions import Distribution, MultitaskMultivariateNormal, base_distributions\nfrom .likelihood import Likelihood\n\n\nclass SoftmaxLikelihood(Likelihood):\n    """"""\n    Implements the Softmax (multiclass) likelihood used for GP classification.\n    """"""\n\n    def __init__(self, num_features=None, num_classes=None, mixing_weights=True, mixing_weights_prior=None, **kwargs):\n        super().__init__()\n        if num_classes is None:\n            raise ValueError(""num_classes is required"")\n        self.num_classes = num_classes\n        if mixing_weights:\n            self.num_features = num_features\n            if num_features is None:\n                raise ValueError(""num_features is required with mixing weights"")\n            self.register_parameter(\n                name=""mixing_weights"",\n                parameter=torch.nn.Parameter(torch.randn(num_classes, num_features).div_(num_features)),\n            )\n            if mixing_weights_prior is not None:\n                self.register_prior(""mixing_weights_prior"", mixing_weights_prior, ""mixing_weights"")\n        else:\n            self.num_features = num_classes\n            self.mixing_weights = None\n\n    def forward(self, function_samples, *params, **kwargs):\n        num_data, num_features = function_samples.shape[-2:]\n\n        # Catch legacy mode\n        if num_data == self.num_features:\n            warnings.warn(\n                ""The input to SoftmaxLikelihood should be a MultitaskMultivariateNormal (num_data x num_tasks). ""\n                ""Batch MultivariateNormal inputs (num_tasks x num_data) will be deprectated."",\n                DeprecationWarning,\n            )\n            function_samples = function_samples.transpose(-1, -2)\n            num_data, num_features = function_samples.shape[-2:]\n\n        if num_features != self.num_features:\n            raise RuntimeError(""There should be %d features"" % self.num_features)\n\n        if self.mixing_weights is not None:\n            mixed_fs = function_samples @ self.mixing_weights.t()  # num_classes x num_data\n        else:\n            mixed_fs = function_samples\n        res = base_distributions.Categorical(logits=mixed_fs)\n        return res\n\n    def __call__(self, function, *params, **kwargs):\n        if isinstance(function, Distribution) and not isinstance(function, MultitaskMultivariateNormal):\n            warnings.warn(\n                ""The input to SoftmaxLikelihood should be a MultitaskMultivariateNormal (num_data x num_tasks). ""\n                ""Batch MultivariateNormal inputs (num_tasks x num_data) will be deprectated."",\n                DeprecationWarning,\n            )\n            function = MultitaskMultivariateNormal.from_batch_mvn(function)\n        return super().__call__(function, *params, **kwargs)\n'"
gpytorch/means/__init__.py,0,"b'#!/usr/bin/env python3\n\nfrom .constant_mean import ConstantMean\nfrom .constant_mean_grad import ConstantMeanGrad\nfrom .linear_mean import LinearMean\nfrom .mean import Mean\nfrom .multitask_mean import MultitaskMean\nfrom .zero_mean import ZeroMean\n\n__all__ = [""Mean"", ""ConstantMean"", ""ConstantMeanGrad"", ""LinearMean"", ""MultitaskMean"", ""ZeroMean""]\n'"
gpytorch/means/constant_mean.py,2,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils.broadcasting import _mul_broadcast_shape\nfrom .mean import Mean\n\n\nclass ConstantMean(Mean):\n    def __init__(self, prior=None, batch_shape=torch.Size(), **kwargs):\n        super(ConstantMean, self).__init__()\n        self.batch_shape = batch_shape\n        self.register_parameter(name=""constant"", parameter=torch.nn.Parameter(torch.zeros(*batch_shape, 1)))\n        if prior is not None:\n            self.register_prior(""mean_prior"", prior, ""constant"")\n\n    def forward(self, input):\n        if input.shape[:-2] == self.batch_shape:\n            return self.constant.expand(input.shape[:-1])\n        else:\n            return self.constant.expand(_mul_broadcast_shape(input.shape[:-1], self.constant.shape))\n'"
gpytorch/means/constant_mean_grad.py,2,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils.broadcasting import _mul_broadcast_shape\nfrom .mean import Mean\n\n\nclass ConstantMeanGrad(Mean):\n    def __init__(self, prior=None, batch_shape=torch.Size(), **kwargs):\n        super(ConstantMeanGrad, self).__init__()\n        self.batch_shape = batch_shape\n        self.register_parameter(name=""constant"", parameter=torch.nn.Parameter(torch.zeros(*batch_shape, 1)))\n        if prior is not None:\n            self.register_prior(""mean_prior"", prior, ""constant"")\n\n    def forward(self, input):\n        batch_shape = _mul_broadcast_shape(self.batch_shape, input.shape[:-2])\n        mean = self.constant.unsqueeze(-1).expand(*batch_shape, input.size(-2), input.size(-1) + 1).contiguous()\n        mean[..., 1:] = 0\n        return mean\n'"
gpytorch/means/linear_mean.py,3,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom .mean import Mean\n\n\nclass LinearMean(Mean):\n    def __init__(self, input_size, batch_shape=torch.Size(), bias=True):\n        super().__init__()\n        self.register_parameter(name=""weights"", parameter=torch.nn.Parameter(torch.randn(*batch_shape, input_size, 1)))\n        if bias:\n            self.register_parameter(name=""bias"", parameter=torch.nn.Parameter(torch.randn(*batch_shape, 1)))\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        res = x.matmul(self.weights).squeeze(-1)\n        if self.bias is not None:\n            res = res + self.bias\n        return res\n'"
gpytorch/means/mean.py,0,"b'#!/usr/bin/env python3\n\nfrom ..module import Module\n\n\nclass Mean(Module):\n    """"""\n    Mean function.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        raise NotImplementedError()\n\n    def __call__(self, x):\n        # Add a last dimension\n        if x.ndimension() == 1:\n            x = x.unsqueeze(1)\n\n        res = super(Mean, self).__call__(x)\n\n        return res\n'"
gpytorch/means/multitask_mean.py,5,"b'#!/usr/bin/env python3\n\nfrom copy import deepcopy\n\nimport torch\nfrom torch.nn import ModuleList\n\nfrom .mean import Mean\n\n\nclass MultitaskMean(Mean):\n    """"""\n    Convenience :class:`gpytorch.means.Mean` implementation for defining a different mean for each task in a multitask\n    model. Expects a list of `num_tasks` different mean functions, each of which is applied to the given data in\n    :func:`~gpytorch.means.MultitaskMean.forward` and returned as an `n x t` matrix of means, one for each task.\n    """"""\n\n    def __init__(self, base_means, num_tasks):\n        """"""\n        Args:\n            base_means (:obj:`list` or :obj:`gpytorch.means.Mean`): If a list, each mean is applied to the data.\n                If a single mean (or a list containing a single mean), that mean is copied `t` times.\n            num_tasks (int): Number of tasks. If base_means is a list, this should equal its length.\n        """"""\n        super(MultitaskMean, self).__init__()\n\n        if isinstance(base_means, Mean):\n            base_means = [base_means]\n\n        if not isinstance(base_means, list) or (len(base_means) != 1 and len(base_means) != num_tasks):\n            raise RuntimeError(""base_means should be a list of means of length either 1 or num_tasks"")\n\n        if len(base_means) == 1:\n            base_means = base_means + [deepcopy(base_means[0]) for i in range(num_tasks - 1)]\n\n        self.base_means = ModuleList(base_means)\n        self.num_tasks = num_tasks\n\n    def forward(self, input):\n        """"""\n        Evaluate each mean in self.base_means on the input data, and return as an `n x t` matrix of means.\n        """"""\n        return torch.cat([sub_mean(input).unsqueeze(-1) for sub_mean in self.base_means], dim=-1)\n'"
gpytorch/means/zero_mean.py,1,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom .mean import Mean\n\n\nclass ZeroMean(Mean):\n    def forward(self, input):\n        return torch.zeros(input.shape[:-1], dtype=input.dtype, device=input.device)\n'"
gpytorch/mlls/__init__.py,0,"b'#!/usr/bin/env python3\n\nimport warnings\n\nfrom .added_loss_term import AddedLossTerm\nfrom .deep_approximate_mll import DeepApproximateMLL\nfrom .exact_marginal_log_likelihood import ExactMarginalLogLikelihood\nfrom .gamma_robust_variational_elbo import GammaRobustVariationalELBO\nfrom .inducing_point_kernel_added_loss_term import InducingPointKernelAddedLossTerm\nfrom .marginal_log_likelihood import MarginalLogLikelihood\nfrom .noise_model_added_loss_term import NoiseModelAddedLossTerm\nfrom .predictive_log_likelihood import PredictiveLogLikelihood\nfrom .sum_marginal_log_likelihood import SumMarginalLogLikelihood\nfrom .variational_elbo import VariationalELBO\n\n\n# Deprecated for 0.4 release\nclass VariationalMarginalLogLikelihood(VariationalELBO):\n    def __init__(self, *args, **kwargs):\n        # Remove after 1.0\n        warnings.warn(\n            ""VariationalMarginalLogLikelihood is deprecated. Please use VariationalELBO instead."", DeprecationWarning\n        )\n        super().__init__(*args, **kwargs)\n\n\nclass VariationalELBOEmpirical(VariationalELBO):\n    def __init__(self, *args, **kwargs):\n        # Remove after 1.0\n        warnings.warn(""VariationalELBOEmpirical is deprecated. Please use VariationalELBO instead."", DeprecationWarning)\n        super().__init__(*args, **kwargs)\n\n\n__all__ = [\n    ""AddedLossTerm"",\n    ""DeepApproximateMLL"",\n    ""ExactMarginalLogLikelihood"",\n    ""InducingPointKernelAddedLossTerm"",\n    ""MarginalLogLikelihood"",\n    ""NoiseModelAddedLossTerm"",\n    ""PredictiveLogLikelihood"",\n    ""GammaRobustVariationalELBO"",\n    ""SumMarginalLogLikelihood"",\n    ""VariationalELBO"",\n]\n'"
gpytorch/mlls/_approximate_mll.py,8,"b'#!/usr/bin/env python3\n\nfrom abc import ABC, abstractmethod\n\nimport torch\n\nfrom .marginal_log_likelihood import MarginalLogLikelihood\n\n\nclass _ApproximateMarginalLogLikelihood(MarginalLogLikelihood, ABC):\n    r""""""\n    An approximate marginal log likelihood (typically a bound) for approximate GP models.\n    We expect that :attr:`model` is a :obj:`gpytorch.models.ApproximateGP`.\n\n    Args:\n        :attr:`likelihood` (:obj:`gpytorch.likelihoods.Likelihood`):\n            The likelihood for the model\n        :attr:`model` (:obj:`gpytorch.models.ApproximateGP`):\n            The approximate GP model\n        :attr:`num_data` (int):\n            The total number of training data points (necessary for SGD)\n        :attr:`beta` (float - default 1.):\n            A multiplicative factor for the KL divergence term.\n            Setting it to 1 (default) recovers true variational inference\n            (as derived in `Scalable Variational Gaussian Process Classification`_).\n            Setting it to anything less than 1 reduces the regularization effect of the model\n            (similarly to what was proposed in `the beta-VAE paper`_).\n        :attr:`combine_terms` (bool):\n            Whether or not to sum the expected NLL with the KL terms (default True)\n    """"""\n\n    def __init__(self, likelihood, model, num_data, beta=1.0, combine_terms=True):\n        super().__init__(likelihood, model)\n        self.combine_terms = combine_terms\n        self.num_data = num_data\n        self.beta = beta\n\n    @abstractmethod\n    def _log_likelihood_term(self, approximate_dist_f, target, **kwargs):\n        raise NotImplementedError\n\n    def forward(self, approximate_dist_f, target, **kwargs):\n        r""""""\n        Computes the Variational ELBO given :math:`q(\\mathbf f)` and `\\mathbf y`.\n        Calling this function will call the likelihood\'s `expected_log_prob` function.\n\n        Args:\n            :attr:`approximate_dist_f` (:obj:`gpytorch.distributions.MultivariateNormal`):\n                :math:`q(\\mathbf f)` the outputs of the latent function (the :obj:`gpytorch.models.ApproximateGP`)\n            :attr:`target` (`torch.Tensor`):\n                :math:`\\mathbf y` The target values\n            :attr:`**kwargs`:\n                Additional arguments passed to the likelihood\'s `expected_log_prob` function.\n        """"""\n        # Get likelihood term and KL term\n        num_batch = approximate_dist_f.event_shape[0]\n        log_likelihood = self._log_likelihood_term(approximate_dist_f, target, **kwargs).div(num_batch)\n        kl_divergence = self.model.variational_strategy.kl_divergence().div(self.num_data / self.beta)\n\n        # Add any additional registered loss terms\n        added_loss = torch.zeros_like(log_likelihood)\n        had_added_losses = False\n        for added_loss_term in self.model.added_loss_terms():\n            added_loss.add_(added_loss_term.loss())\n            had_added_losses = True\n\n        # Log prior term\n        log_prior = torch.zeros_like(log_likelihood)\n        for _, prior, closure, _ in self.named_priors():\n            log_prior.add_(prior.log_prob(closure()).sum().div(self.num_data))\n\n        if self.combine_terms:\n            return log_likelihood - kl_divergence + log_prior - added_loss\n        else:\n            if had_added_losses:\n                return log_likelihood, kl_divergence, log_prior.div(self.num_data), added_loss\n            else:\n                return log_likelihood, kl_divergence, log_prior.div(self.num_data)\n'"
gpytorch/mlls/added_loss_term.py,0,b'#!/usr/bin/env python3\n\n\nclass AddedLossTerm(object):\n    def loss(self):\n        raise NotImplementedError\n'
gpytorch/mlls/deep_approximate_mll.py,3,"b'from ._approximate_mll import _ApproximateMarginalLogLikelihood\n\n\nclass DeepApproximateMLL(_ApproximateMarginalLogLikelihood):\n    """"""\n    A wrapper to make a GPyTorch approximate marginal log likelihoods compatible with Deep GPs.\n\n    Example:\n        >>> deep_mll = gpytorch.mlls.DeepApproximateMLL(\n        >>>     gpytorch.mlls.VariationalELBO(likelihood, model, num_data=1000)\n        >>> )\n\n    :param ~gpytorch.mlls._ApproximateMarginalLogLikelihood base_mll: The base\n        approximate MLL\n    """"""\n\n    def __init__(self, base_mll):\n        if not base_mll.combine_terms:\n            raise ValueError(\n                ""The base marginal log likelihood object should combine terms ""\n                ""when used in conjunction with a DeepApproximateMLL.""\n            )\n        super().__init__(base_mll.likelihood, base_mll.model, num_data=base_mll.num_data, beta=base_mll.beta)\n        self.base_mll = base_mll\n\n    def _log_likelihood_term(self, approximate_dist_f, target, **kwargs):\n        return self.base_mll._log_likelihood_term(approximate_dist_f, target, **kwargs).mean(0)\n\n    def forward(self, approximate_dist_f, target, **kwargs):\n        return self.base_mll.forward(approximate_dist_f, target, **kwargs).mean(0)\n'"
gpytorch/mlls/exact_marginal_log_likelihood.py,11,"b'#!/usr/bin/env python3\n\nfrom ..distributions import MultivariateNormal\nfrom ..likelihoods import _GaussianLikelihoodBase\nfrom .marginal_log_likelihood import MarginalLogLikelihood\n\n\nclass ExactMarginalLogLikelihood(MarginalLogLikelihood):\n    """"""\n    The exact marginal log likelihood (MLL) for an exact Gaussian process with a\n    Gaussian likelihood.\n\n    .. note::\n        This module will not work with anything other than a :obj:`~gpytorch.likelihoods.GaussianLikelihood`\n        and a :obj:`~gpytorch.models.ExactGP`. It also cannot be used in conjunction with\n        stochastic optimization.\n\n    :param ~gpytorch.likelihoods.GaussianLikelihood likelihood: The Gaussian likelihood for the model\n    :param ~gpytorch.models.ExactGP model: The exact GP model\n\n    Example:\n        >>> # model is a gpytorch.models.ExactGP\n        >>> # likelihood is a gpytorch.likelihoods.Likelihood\n        >>> mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n        >>>\n        >>> output = model(train_x)\n        >>> loss = -mll(output, train_y)\n        >>> loss.backward()\n    """"""\n\n    def __init__(self, likelihood, model):\n        if not isinstance(likelihood, _GaussianLikelihoodBase):\n            raise RuntimeError(""Likelihood must be Gaussian for exact inference"")\n        super(ExactMarginalLogLikelihood, self).__init__(likelihood, model)\n\n    def forward(self, function_dist, target, *params):\n        r""""""\n        Computes the MLL given :math:`p(\\mathbf f)` and :math:`\\mathbf y`.\n\n        :param ~gpytorch.distributions.MultivariateNormal function_dist: :math:`p(\\mathbf f)`\n            the outputs of the latent function (the :obj:`gpytorch.models.ExactGP`)\n        :param torch.Tensor target: :math:`\\mathbf y` The target values\n        :rtype: torch.Tensor\n        :return: Exact MLL. Output shape corresponds to batch shape of the model/input data.\n        """"""\n        if not isinstance(function_dist, MultivariateNormal):\n            raise RuntimeError(""ExactMarginalLogLikelihood can only operate on Gaussian random variables"")\n\n        # Get the log prob of the marginal distribution\n        output = self.likelihood(function_dist, *params)\n        res = output.log_prob(target)\n\n        # Add additional terms (SGPR / learned inducing points, heteroskedastic likelihood models)\n        for added_loss_term in self.model.added_loss_terms():\n            res = res.add(added_loss_term.loss(*params))\n\n        # Add log probs of priors on the (functions of) parameters\n        for _, prior, closure, _ in self.named_priors():\n            res.add_(prior.log_prob(closure()).sum())\n\n        # Scale by the amount of data we have\n        num_data = target.size(-1)\n        return res.div_(num_data)\n\n    def pyro_factor(self, output, target, *params):\n        import pyro\n\n        mll = self(output, target, *params)\n        pyro.factor(""gp_mll"", mll)\n        return mll\n'"
gpytorch/mlls/gamma_robust_variational_elbo.py,9,"b'#!/usr/bin/env python3\n\nimport math\n\nimport numpy as np\nimport torch\n\nfrom ..likelihoods import _GaussianLikelihoodBase\nfrom ._approximate_mll import _ApproximateMarginalLogLikelihood\n\n\nclass GammaRobustVariationalELBO(_ApproximateMarginalLogLikelihood):\n    r""""""\n    An alternative to the variational evidence lower bound (ELBO), proposed by `Knoblauch, 2019`_.\n    It is derived by replacing the log-likelihood term in the ELBO with a `\\gamma` divergence:\n\n    .. math::\n\n       \\begin{align*}\n          \\mathcal{L}_{\\gamma} &=\n          \\sum_{i=1}^N \\mathbb{E}_{q( \\mathbf u)} \\left[\n            -\\frac{\\gamma}{\\gamma - 1}\n            \\frac{\n                p( y_i \\! \\mid \\! \\mathbf u, x_i)^{\\gamma - 1}\n            }{\n                \\int p(y \\mid \\mathbf u, x_i)^{\\gamma} \\: dy\n            }\n          \\right] - \\beta \\: \\text{KL} \\left[ q( \\mathbf u) \\Vert p( \\mathbf u) \\right]\n       \\end{align*}\n\n    where :math:`N` is the number of datapoints, :math:`\\gamma` is a hyperparameter,\n    :math:`q(\\mathbf u)` is the variational distribution for\n    the inducing function values, and :math:`p(\\mathbf u)` is the prior distribution for the inducing function\n    values.\n\n    :math:`\\beta` is a scaling constant for the KL divergence.\n\n    .. note::\n        This module will only work with :obj:`~gpytorch.likelihoods.GaussianLikelihood`.\n\n    :param ~gpytorch.likelihoods.GaussianLikelihood likelihood: The likelihood for the model\n    :param ~gpytorch.models.ApproximateGP model: The approximate GP model\n    :param int num_data: The total number of training data points (necessary for SGD)\n    :param float beta: (optional, default=1.) A multiplicative factor for the KL divergence term.\n        Setting it to anything less than 1 reduces the regularization effect of the model\n        (similarly to what was proposed in `the beta-VAE paper`_).\n    :param float gamma: (optional, default=1.03) The :math:`\\gamma`-divergence hyperparameter.\n    :param bool combine_terms: (default=True): Whether or not to sum the\n        expected NLL with the KL terms (default True)\n\n    Example:\n        >>> # model is a gpytorch.models.ApproximateGP\n        >>> # likelihood is a gpytorch.likelihoods.Likelihood\n        >>> mll = gpytorch.mlls.GammaRobustVariationalELBO(likelihood, model, num_data=100, beta=0.5, gamma=1.03)\n        >>>\n        >>> output = model(train_x)\n        >>> loss = -mll(output, train_y)\n        >>> loss.backward()\n\n    .. _Knoblauch, 2019:\n        https://arxiv.org/pdf/1904.02303.pdf\n    .. _Knoblauch, Jewson, Damoulas 2019:\n        https://arxiv.org/pdf/1904.02063.pdf\n    """"""\n\n    def __init__(self, likelihood, model, gamma=1.03, *args, **kwargs):\n        if not isinstance(likelihood, _GaussianLikelihoodBase):\n            raise RuntimeError(""Likelihood must be Gaussian for exact inference"")\n        super().__init__(likelihood, model, *args, **kwargs)\n        if gamma <= 1.0:\n            raise ValueError(""gamma should be > 1.0"")\n        self.gamma = gamma\n\n    def _log_likelihood_term(self, variational_dist_f, target, *args, **kwargs):\n        shifted_gamma = self.gamma - 1\n\n        muf, varf = variational_dist_f.mean, variational_dist_f.variance\n\n        # Get noise from likelihood\n        noise = self.likelihood._shaped_noise_covar(muf.shape, *args, **kwargs).diag()\n        # Potentially reshape the noise to deal with the multitask case\n        noise = noise.view(*noise.shape[:-1], *variational_dist_f.event_shape)\n\n        # adapted from https://github.com/JeremiasKnoblauch/GVIPublic/\n        mut = shifted_gamma * target / noise + muf / varf\n        sigmat = 1.0 / (shifted_gamma / noise + 1.0 / varf)\n        log_integral = -0.5 * shifted_gamma * torch.log(2.0 * math.pi * noise) - 0.5 * np.log1p(shifted_gamma)\n        log_tempered = (\n            -math.log(shifted_gamma)\n            - 0.5 * shifted_gamma * torch.log(2.0 * math.pi * noise)\n            - 0.5 * torch.log1p(shifted_gamma * varf / noise)\n            - 0.5 * (shifted_gamma * target.pow(2.0) / noise)\n            - 0.5 * muf.pow(2.0) / varf\n            + 0.5 * mut.pow(2.0) * sigmat\n        )\n\n        factor = log_tempered + shifted_gamma / self.gamma * log_integral\n        factor = self.gamma * factor.exp()\n\n        # Do appropriate summation for multitask Gaussian likelihoods\n        num_event_dim = len(variational_dist_f.event_shape)\n        if num_event_dim > 1:\n            factor = factor.sum(list(range(-1, -num_event_dim, -1)))\n\n        return factor.sum(-1)\n'"
gpytorch/mlls/inducing_point_kernel_added_loss_term.py,0,"b'#!/usr/bin/env python3\n\nfrom .added_loss_term import AddedLossTerm\n\n\nclass InducingPointKernelAddedLossTerm(AddedLossTerm):\n    def __init__(self, variational_dist, prior_dist, likelihood):\n        self.prior_dist = prior_dist\n        self.variational_dist = variational_dist\n        self.likelihood = likelihood\n\n    def loss(self, *params):\n        prior_covar = self.prior_dist.lazy_covariance_matrix\n        variational_covar = self.variational_dist.lazy_covariance_matrix\n        diag = prior_covar.diag() - variational_covar.diag()\n        shape = prior_covar.shape[:-1]\n        noise_diag = self.likelihood._shaped_noise_covar(shape, *params).diag()\n        return 0.5 * (diag / noise_diag).sum()\n'"
gpytorch/mlls/marginal_log_likelihood.py,3,"b'#!/usr/bin/env python3\n\nfrom ..models import GP\nfrom ..module import Module\n\n\nclass MarginalLogLikelihood(Module):\n    r""""""\n    These are modules to compute (or approximate/bound) the marginal log likelihood\n    (MLL) of the GP model when applied to data.  I.e., given a GP :math:`f \\sim\n    \\mathcal{GP}(\\mu, K)`, and data :math:`\\mathbf X, \\mathbf y`, these modules\n    compute/approximate\n\n    .. math::\n\n       \\begin{equation*}\n          \\mathcal{L} = p_f(\\mathbf y \\! \\mid \\! \\mathbf X)\n          = \\int p \\left( \\mathbf y \\! \\mid \\! f(\\mathbf X) \\right) \\: p(f(\\mathbf X) \\! \\mid \\! \\mathbf X) \\: d f\n       \\end{equation*}\n\n    This is computed exactly when the GP inference is computed exactly (e.g. regression w/ a Gaussian likelihood).\n    It is approximated/bounded for GP models that use approximate inference.\n\n    These models are typically used as the ""loss"" functions for GP models (though note that the output of\n    these functions must be negated for optimization).\n    """"""\n\n    def __init__(self, likelihood, model):\n        super(MarginalLogLikelihood, self).__init__()\n        if not isinstance(model, GP):\n            raise RuntimeError(\n                ""All MarginalLogLikelihood objects must be given a GP object as a model. If you are ""\n                ""using a more complicated model involving a GP, pass the underlying GP object as the ""\n                ""model, not a full PyTorch module.""\n            )\n        self.likelihood = likelihood\n        self.model = model\n\n    def forward(self, output, target, **kwargs):\n        r""""""\n        Computes the MLL given :math:`p(\\mathbf f)` and `\\mathbf y`\n\n        :param ~gpytorch.distributions.MultivariateNormal output: the outputs of the latent function\n            (the :obj:`~gpytorch.models.GP`)\n        :param torch.Tensor target: :math:`\\mathbf y` The target values\n        :param dict kwargs: Additional arguments to pass to the likelihood\'s :attr:`forward` function.\n        """"""\n        raise NotImplementedError\n\n    def pyro_factor(self, output, target):\n        """"""\n        As forward, but register the MLL with pyro using the pyro.factor primitive.\n        """"""\n        raise NotImplementedError\n'"
gpytorch/mlls/noise_model_added_loss_term.py,0,"b'#!/usr/bin/env python3\n\nfrom .added_loss_term import AddedLossTerm\n\n\nclass NoiseModelAddedLossTerm(AddedLossTerm):\n    def __init__(self, noise_model):\n        from .exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n\n        self.noise_mll = ExactMarginalLogLikelihood(noise_model.likelihood, noise_model)\n\n    def loss(self, *params):\n        output = self.noise_mll.model(*params)\n        targets = self.noise_mll.model.train_targets\n        return self.noise_mll(output, targets)\n'"
gpytorch/mlls/predictive_log_likelihood.py,12,"b'#!/usr/bin/env python3\n\nfrom ._approximate_mll import _ApproximateMarginalLogLikelihood\n\n\nclass PredictiveLogLikelihood(_ApproximateMarginalLogLikelihood):\n    r""""""\n    An alternative objective function for approximate GPs, proposed in `Jankowiak et al., 2019`_.\n    It typically produces better predictive variances than the :obj:`gpytorch.mlls.VariationalELBO` objective.\n\n    .. math::\n\n       \\begin{align*}\n          \\mathcal{L}_\\text{ELBO} &=\n          \\mathbb{E}_{p_\\text{data}( y, \\mathbf x )} \\left[\n            \\log p( y \\! \\mid \\! \\mathbf x)\n          \\right] - \\beta \\: \\text{KL} \\left[ q( \\mathbf u) \\Vert p( \\mathbf u) \\right]\n          \\\\\n          &\\approx \\sum_{i=1}^N \\log \\mathbb{E}_{q(\\mathbf u)} \\left[\n            \\int p( y_i \\! \\mid \\! f_i) p(f_i \\! \\mid \\! \\mathbf u, \\mathbf x_i) \\: d f_i\n          \\right] - \\beta \\: \\text{KL} \\left[ q( \\mathbf u) \\Vert p( \\mathbf u) \\right]\n       \\end{align*}\n\n    where :math:`N` is the total number of datapoints, :math:`q(\\mathbf u)` is the variational distribution for\n    the inducing function values, and :math:`p(\\mathbf u)` is the prior distribution for the inducing function\n    values.\n\n    :math:`\\beta` is a scaling constant that reduces the regularization effect of the KL\n    divergence. Setting :math:`\\beta=1` (default) results in an objective that can be motivated by a connection\n    to Stochastic Expectation Propagation (see `Jankowiak et al., 2019`_ for details).\n\n    .. note::\n        This objective is very similar to the variational ELBO.\n        The only difference is that the :math:`log` occurs *outside* the expectation :math:`\\mathbb{E}_{q(\\mathbf u)}`.\n        This difference results in very different predictive performance (see `Jankowiak et al., 2019`_).\n\n    :param ~gpytorch.likelihoods.Likelihood likelihood: The likelihood for the model\n    :param ~gpytorch.models.ApproximateGP model: The approximate GP model\n    :param int num_data: The total number of training data points (necessary for SGD)\n    :param float beta: (optional, default=1.) A multiplicative factor for the KL divergence term.\n        Setting it to anything less than 1 reduces the regularization effect of the model\n        (similarly to what was proposed in `the beta-VAE paper`_).\n    :param bool combine_terms: (default=True): Whether or not to sum the\n        expected NLL with the KL terms (default True)\n\n    Example:\n        >>> # model is a gpytorch.models.ApproximateGP\n        >>> # likelihood is a gpytorch.likelihoods.Likelihood\n        >>> mll = gpytorch.mlls.PredictiveLogLikelihood(likelihood, model, num_data=100, beta=0.5)\n        >>>\n        >>> output = model(train_x)\n        >>> loss = -mll(output, train_y)\n        >>> loss.backward()\n\n    .. _Jankowiak et al., 2019:\n        https://arxiv.org/abs/1910.07123\n    """"""\n\n    def _log_likelihood_term(self, approximate_dist_f, target, **kwargs):\n        return self.likelihood.log_marginal(target, approximate_dist_f, **kwargs).sum(-1)\n\n    def forward(self, approximate_dist_f, target, **kwargs):\n        r""""""\n        Computes the predictive cross entropy given :math:`q(\\mathbf f)` and :math:`\\mathbf y`.\n        Calling this function will call the likelihood\'s\n        :meth:`~gpytorch.likelihoods.Likelihood.forward` function.\n\n        :param ~gpytorch.distributions.MultivariateNormal variational_dist_f: :math:`q(\\mathbf f)`\n            the outputs of the latent function (the :obj:`gpytorch.models.ApproximateGP`)\n        :param torch.Tensor target: :math:`\\mathbf y` The target values\n        :param kwargs: Additional arguments passed to the\n            likelihood\'s :meth:`~gpytorch.likelihoods.Likelihood.forward` function.\n        :rtype: torch.Tensor\n        :return: Predictive log likelihood. Output shape corresponds to batch shape of the model/input data.\n        """"""\n        return super().forward(approximate_dist_f, target, **kwargs)\n'"
gpytorch/mlls/sum_marginal_log_likelihood.py,2,"b'#! /usr/bin/env python3\n\nfrom torch.nn import ModuleList\n\nfrom gpytorch.mlls import ExactMarginalLogLikelihood, MarginalLogLikelihood\n\n\nclass SumMarginalLogLikelihood(MarginalLogLikelihood):\n    """"""Sum of marginal log likelihoods, to be used with Multi-Output models.\n\n    Args:\n        likelihood: A MultiOutputLikelihood\n        model: A MultiOutputModel\n        mll_cls: The Marginal Log Likelihood class (default: ExactMarginalLogLikelihood)\n\n    In case the model outputs are independent, this provives the MLL of the multi-output model.\n\n    """"""\n\n    def __init__(self, likelihood, model, mll_cls=ExactMarginalLogLikelihood):\n        super().__init__(model.likelihood, model)\n        self.mlls = ModuleList([mll_cls(mdl.likelihood, mdl) for mdl in model.models])\n\n    def forward(self, outputs, targets, *params):\n        """"""\n        Args:\n            outputs: (Iterable[MultivariateNormal]) - the outputs of the latent function\n            targets: (Iterable[Tensor]) - the target values\n            params: (Iterable[Iterable[Tensor]]) - the arguments to be passed through\n                (e.g. parameters in case of heteroskedastic likelihoods)\n        """"""\n        if len(params) == 0:\n            sum_mll = sum(mll(output, target) for mll, output, target in zip(self.mlls, outputs, targets))\n        else:\n            sum_mll = sum(\n                mll(output, target, *iparams)\n                for mll, output, target, iparams in zip(self.mlls, outputs, targets, params)\n            )\n        return sum_mll.div_(len(self.mlls))\n'"
gpytorch/mlls/variational_elbo.py,11,"b'#!/usr/bin/env python3\n\nfrom ._approximate_mll import _ApproximateMarginalLogLikelihood\n\n\nclass VariationalELBO(_ApproximateMarginalLogLikelihood):\n    r""""""\n    The variational evidence lower bound (ELBO). This is used to optimize\n    variational Gaussian processes (with or without stochastic optimization).\n\n    .. math::\n\n       \\begin{align*}\n          \\mathcal{L}_\\text{ELBO} &=\n          \\mathbb{E}_{p_\\text{data}( y, \\mathbf x )} \\left[\n            \\mathbb{E}_{p(f \\mid \\mathbf u, \\mathbf x) q(\\mathbf u)} \\left[  \\log p( y \\! \\mid \\! f) \\right]\n          \\right] - \\beta \\: \\text{KL} \\left[ q( \\mathbf u) \\Vert p( \\mathbf u) \\right]\n          \\\\\n          &\\approx \\sum_{i=1}^N \\mathbb{E}_{q( f_i)} \\left[\n            \\log p( y_i \\! \\mid \\! f_i) \\right] - \\beta \\: \\text{KL} \\left[ q( \\mathbf u) \\Vert p( \\mathbf u) \\right]\n       \\end{align*}\n\n    where :math:`N` is the number of datapoints, :math:`q(\\mathbf u)` is the variational distribution for\n    the inducing function values, :math:`q(f_i)` is the marginal of\n    :math:`p(f_i \\mid \\mathbf u, \\mathbf x_i) q(\\mathbf u)`,\n    and :math:`p(\\mathbf u)` is the prior distribution for the inducing function values.\n\n    :math:`\\beta` is a scaling constant that reduces the regularization effect of the KL\n    divergence. Setting :math:`\\beta=1` (default) results in the true variational ELBO.\n\n    For more information on this derivation, see `Scalable Variational Gaussian Process Classification`_\n    (Hensman et al., 2015).\n\n    :param ~gpytorch.likelihoods.Likelihood likelihood: The likelihood for the model\n    :param ~gpytorch.models.ApproximateGP model: The approximate GP model\n    :param int num_data: The total number of training data points (necessary for SGD)\n    :param float beta: (optional, default=1.) A multiplicative factor for the KL divergence term.\n        Setting it to 1 (default) recovers true variational inference\n        (as derived in `Scalable Variational Gaussian Process Classification`_).\n        Setting it to anything less than 1 reduces the regularization effect of the model\n        (similarly to what was proposed in `the beta-VAE paper`_).\n    :param bool combine_terms: (default=True): Whether or not to sum the\n        expected NLL with the KL terms (default True)\n\n    Example:\n        >>> # model is a gpytorch.models.ApproximateGP\n        >>> # likelihood is a gpytorch.likelihoods.Likelihood\n        >>> mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=100, beta=0.5)\n        >>>\n        >>> output = model(train_x)\n        >>> loss = -mll(output, train_y)\n        >>> loss.backward()\n\n    .. _Scalable Variational Gaussian Process Classification:\n        http://proceedings.mlr.press/v38/hensman15.pdf\n    .. _the beta-VAE paper:\n        https://openreview.net/pdf?id=Sy2fzU9gl\n    """"""\n\n    def _log_likelihood_term(self, variational_dist_f, target, **kwargs):\n        return self.likelihood.expected_log_prob(target, variational_dist_f, **kwargs).sum(-1)\n\n    def forward(self, variational_dist_f, target, **kwargs):\n        r""""""\n        Computes the Variational ELBO given :math:`q(\\mathbf f)` and :math:`\\mathbf y`.\n        Calling this function will call the likelihood\'s :meth:`~gpytorch.likelihoods.Likelihood.expected_log_prob`\n        function.\n\n        :param ~gpytorch.distributions.MultivariateNormal variational_dist_f: :math:`q(\\mathbf f)`\n            the outputs of the latent function (the :obj:`gpytorch.models.ApproximateGP`)\n        :param torch.Tensor target: :math:`\\mathbf y` The target values\n        :param kwargs: Additional arguments passed to the\n            likelihood\'s :meth:`~gpytorch.likelihoods.Likelihood.expected_log_prob` function.\n        :rtype: torch.Tensor\n        :return: Variational ELBO. Output shape corresponds to batch shape of the model/input data.\n        """"""\n        return super().forward(variational_dist_f, target, **kwargs)\n'"
gpytorch/models/__init__.py,0,"b'#!/usr/bin/env python3\n\nimport warnings\n\nfrom . import deep_gps, pyro\nfrom .approximate_gp import ApproximateGP\nfrom .exact_gp import ExactGP\nfrom .gp import GP\nfrom .model_list import AbstractModelList, IndependentModelList\nfrom .pyro import PyroGP\n\n# Alternative name for ApproximateGP\nVariationalGP = ApproximateGP\n\n\n# Deprecated for 0.4 release\nclass AbstractVariationalGP(ApproximateGP):\n    # Remove after 1.0\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""AbstractVariationalGP has been renamed to ApproximateGP."", DeprecationWarning)\n        super().__init__(*args, **kwargs)\n\n\n# Deprecated for 0.4 release\nclass PyroVariationalGP(ApproximateGP):\n    # Remove after 1.0\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""PyroVariationalGP has been renamed to PyroGP."", DeprecationWarning)\n        super().__init__(*args, **kwargs)\n\n\n__all__ = [\n    ""AbstractModelList"",\n    ""ApproximateGP"",\n    ""ExactGP"",\n    ""GP"",\n    ""IndependentModelList"",\n    ""PyroGP"",\n    ""VariationalGP"",\n    ""deep_gps"",\n    ""pyro"",\n]\n'"
gpytorch/models/approximate_gp.py,10,"b'#!/usr/bin/env python3\n\nfrom .gp import GP\nfrom .pyro import _PyroMixin  # This will only contain functions if Pyro is installed\n\n\nclass ApproximateGP(GP, _PyroMixin):\n    r""""""\n    The base class for any Gaussian process latent function to be used in conjunction\n    with approximate inference (typically stochastic variational inference).\n    This base class can be used to implement most inducing point methods where the\n    variational parameters are learned directly.\n\n    :param ~gpytorch.variational._VariationalStrategy variational_strategy: The strategy that determines\n        how the model marginalizes over the variational distribution (over inducing points)\n        to produce the approximate posterior distribution (over data)\n\n    The :meth:`forward` function should describe how to compute the prior latent distribution\n    on a given input. Typically, this will involve a mean and kernel function.\n    The result must be a :obj:`~gpytorch.distributions.MultivariateNormal`.\n\n    Example:\n        >>> class MyVariationalGP(gpytorch.models.PyroGP):\n        >>>     def __init__(self, variational_strategy):\n        >>>         super().__init__(variational_strategy)\n        >>>         self.mean_module = gpytorch.means.ZeroMean()\n        >>>         self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n        >>>\n        >>>     def forward(self, x):\n        >>>         mean = self.mean_module(x)\n        >>>         covar = self.covar_module(x)\n        >>>         return gpytorch.distributions.MultivariateNormal(mean, covar)\n        >>>\n        >>> # variational_strategy = ...\n        >>> model = MyVariationalGP(variational_strategy)\n        >>> likelihood = gpytorch.likelihoods.GaussianLikelihood()\n        >>>\n        >>> # optimization loop for variational parameters...\n        >>>\n        >>> # test_x = ...;\n        >>> model(test_x)  # Returns the approximate GP latent function at test_x\n        >>> likelihood(model(test_x))  # Returns the (approximate) predictive posterior distribution at test_x\n    """"""\n\n    def __init__(self, variational_strategy):\n        super().__init__()\n        self.variational_strategy = variational_strategy\n\n    def forward(self, x):\n        raise NotImplementedError\n\n    def pyro_guide(self, input, beta=1.0, name_prefix=""""):\n        r""""""\n        (For Pyro integration only). The component of a `pyro.guide` that\n        corresponds to drawing samples from the latent GP function.\n\n        :param torch.Tensor input: The inputs :math:`\\mathbf X`.\n        :param float beta: (default=1.) How much to scale the :math:`\\text{KL} [ q(\\mathbf f) \\Vert p(\\mathbf f) ]`\n            term by.\n        :param str name_prefix: (default="""") A name prefix to prepend to pyro sample sites.\n        """"""\n        return super().pyro_guide(input, beta=beta, name_prefix=name_prefix)\n\n    def pyro_model(self, input, beta=1.0, name_prefix=""""):\n        r""""""\n        (For Pyro integration only). The component of a `pyro.model` that\n        corresponds to drawing samples from the latent GP function.\n\n        :param torch.Tensor input: The inputs :math:`\\mathbf X`.\n        :param float beta: (default=1.) How much to scale the :math:`\\text{KL} [ q(\\mathbf f) \\Vert p(\\mathbf f) ]`\n            term by.\n        :param str name_prefix: (default="""") A name prefix to prepend to pyro sample sites.\n        :return: samples from :math:`q(\\mathbf f)`\n        :rtype: torch.Tensor\n        """"""\n        return super().pyro_model(input, beta=beta, name_prefix=name_prefix)\n\n    def __call__(self, inputs, prior=False, **kwargs):\n        if inputs.dim() == 1:\n            inputs = inputs.unsqueeze(-1)\n        return self.variational_strategy(inputs, prior=prior)\n'"
gpytorch/models/exact_gp.py,24,"b'#!/usr/bin/env python3\n\nimport warnings\nfrom copy import deepcopy\n\nimport torch\n\nfrom .. import settings\nfrom ..distributions import MultivariateNormal\nfrom ..likelihoods import _GaussianLikelihoodBase\nfrom ..utils.broadcasting import _mul_broadcast_shape\nfrom ..utils.warnings import GPInputWarning\nfrom .exact_prediction_strategies import prediction_strategy\nfrom .gp import GP\n\n\nclass ExactGP(GP):\n    r""""""\n    The base class for any Gaussian process latent function to be used in conjunction\n    with exact inference.\n\n    :param torch.Tensor train_inputs: (size n x d) The training features :math:`\\mathbf X`.\n    :param torch.Tensor train_targets: (size n) The training targets :math:`\\mathbf y`.\n    :param ~gpytorch.likelihoods.GaussianLikelihood likelihood: The Gaussian likelihood that defines\n        the observational distribution. Since we\'re using exact inference, the likelihood must be Gaussian.\n\n    The :meth:`forward` function should describe how to compute the prior latent distribution\n    on a given input. Typically, this will involve a mean and kernel function.\n    The result must be a :obj:`~gpytorch.distributions.MultivariateNormal`.\n\n    Calling this model will return the posterior of the latent Gaussian process when conditioned\n    on the training data. The output will be a :obj:`~gpytorch.distributions.MultivariateNormal`.\n\n    Example:\n        >>> class MyGP(gpytorch.models.ExactGP):\n        >>>     def __init__(self, train_x, train_y, likelihood):\n        >>>         super().__init__(train_x, train_y, likelihood)\n        >>>         self.mean_module = gpytorch.means.ZeroMean()\n        >>>         self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n        >>>\n        >>>     def forward(self, x):\n        >>>         mean = self.mean_module(x)\n        >>>         covar = self.covar_module(x)\n        >>>         return gpytorch.distributions.MultivariateNormal(mean, covar)\n        >>>\n        >>> # train_x = ...; train_y = ...\n        >>> likelihood = gpytorch.likelihoods.GaussianLikelihood()\n        >>> model = MyGP(train_x, train_y, likelihood)\n        >>>\n        >>> # test_x = ...;\n        >>> model(test_x)  # Returns the GP latent function at test_x\n        >>> likelihood(model(test_x))  # Returns the (approximate) predictive posterior distribution at test_x\n    """"""\n\n    def __init__(self, train_inputs, train_targets, likelihood):\n        if train_inputs is not None and torch.is_tensor(train_inputs):\n            train_inputs = (train_inputs,)\n        if train_inputs is not None and not all(torch.is_tensor(train_input) for train_input in train_inputs):\n            raise RuntimeError(""Train inputs must be a tensor, or a list/tuple of tensors"")\n        if not isinstance(likelihood, _GaussianLikelihoodBase):\n            raise RuntimeError(""ExactGP can only handle Gaussian likelihoods"")\n\n        super(ExactGP, self).__init__()\n        if train_inputs is not None:\n            self.train_inputs = tuple(tri.unsqueeze(-1) if tri.ndimension() == 1 else tri for tri in train_inputs)\n            self.train_targets = train_targets\n        else:\n            self.train_inputs = None\n            self.train_targets = None\n        self.likelihood = likelihood\n\n        self.prediction_strategy = None\n\n    @property\n    def train_targets(self):\n        return self._train_targets\n\n    @train_targets.setter\n    def train_targets(self, value):\n        object.__setattr__(self, ""_train_targets"", value)\n\n    def _apply(self, fn):\n        if self.train_inputs is not None:\n            self.train_inputs = tuple(fn(train_input) for train_input in self.train_inputs)\n            self.train_targets = fn(self.train_targets)\n        return super(ExactGP, self)._apply(fn)\n\n    def local_load_samples(self, samples_dict, memo, prefix):\n        """"""\n        Replace the model\'s learned hyperparameters with samples from a posterior distribution.\n        """"""\n        # Pyro always puts the samples in the first batch dimension\n        num_samples = next(iter(samples_dict.values())).size(0)\n        self.train_inputs = tuple(tri.unsqueeze(0).expand(num_samples, *tri.shape) for tri in self.train_inputs)\n        self.train_targets = self.train_targets.unsqueeze(0).expand(num_samples, *self.train_targets.shape)\n        super().local_load_samples(samples_dict, memo, prefix)\n\n    def set_train_data(self, inputs=None, targets=None, strict=True):\n        """"""\n        Set training data (does not re-fit model hyper-parameters).\n\n        :param torch.Tensor inputs: The new training inputs.\n        :param torch.Tensor targets: The new training targets.\n        :param bool strict: (default True) If `True`, the new inputs and\n            targets must have the same shape, dtype, and device\n            as the current inputs and targets. Otherwise, any shape/dtype/device are allowed.\n        """"""\n        if inputs is not None:\n            if torch.is_tensor(inputs):\n                inputs = (inputs,)\n            inputs = tuple(input_.unsqueeze(-1) if input_.ndimension() == 1 else input_ for input_ in inputs)\n            if strict:\n                for input_, t_input in zip(inputs, self.train_inputs or (None,)):\n                    for attr in {""shape"", ""dtype"", ""device""}:\n                        expected_attr = getattr(t_input, attr, None)\n                        found_attr = getattr(input_, attr, None)\n                        if expected_attr != found_attr:\n                            msg = ""Cannot modify {attr} of inputs (expected {e_attr}, found {f_attr}).""\n                            msg = msg.format(attr=attr, e_attr=expected_attr, f_attr=found_attr)\n                            raise RuntimeError(msg)\n            self.train_inputs = inputs\n        if targets is not None:\n            if strict:\n                for attr in {""shape"", ""dtype"", ""device""}:\n                    expected_attr = getattr(self.train_targets, attr, None)\n                    found_attr = getattr(targets, attr, None)\n                    if expected_attr != found_attr:\n                        msg = ""Cannot modify {attr} of targets (expected {e_attr}, found {f_attr}).""\n                        msg = msg.format(attr=attr, e_attr=expected_attr, f_attr=found_attr)\n                        raise RuntimeError(msg)\n            self.train_targets = targets\n        self.prediction_strategy = None\n\n    def get_fantasy_model(self, inputs, targets, **kwargs):\n        """"""\n        Returns a new GP model that incorporates the specified inputs and targets as new training data.\n\n        Using this method is more efficient than updating with `set_train_data` when the number of inputs is relatively\n        small, because any computed test-time caches will be updated in linear time rather than computed from scratch.\n\n        .. note::\n            If `targets` is a batch (e.g. `b x m`), then the GP returned from this method will be a batch mode GP.\n            If `inputs` is of the same (or lesser) dimension as `targets`, then it is assumed that the fantasy points\n            are the same for each target batch.\n\n        :param torch.Tensor inputs: (`b1 x ... x bk x m x d` or `f x b1 x ... x bk x m x d`) Locations of fantasy\n            observations.\n        :param torch.Tensor targets: (`b1 x ... x bk x m` or `f x b1 x ... x bk x m`) Labels of fantasy observations.\n        :return: An `ExactGP` model with `n + m` training examples, where the `m` fantasy examples have been added\n            and all test-time caches have been updated.\n        :rtype: ~gpytorch.models.ExactGP\n        """"""\n        if self.prediction_strategy is None:\n            raise RuntimeError(\n                ""Fantasy observations can only be added after making predictions with a model so that ""\n                ""all test independent caches exist. Call the model on some data first!""\n            )\n\n        model_batch_shape = self.train_inputs[0].shape[:-2]\n\n        if self.train_targets.dim() > len(model_batch_shape) + 1:\n            raise RuntimeError(""Cannot yet add fantasy observations to multitask GPs, but this is coming soon!"")\n\n        if not isinstance(inputs, list):\n            inputs = [inputs]\n\n        inputs = [i.unsqueeze(-1) if i.ndimension() == 1 else i for i in inputs]\n\n        target_batch_shape = targets.shape[:-1]\n        input_batch_shape = inputs[0].shape[:-2]\n        tbdim, ibdim = len(target_batch_shape), len(input_batch_shape)\n\n        if not (tbdim == ibdim + 1 or tbdim == ibdim):\n            raise RuntimeError(\n                f""Unsupported batch shapes: The target batch shape ({target_batch_shape}) must have either the ""\n                f""same dimension as or one more dimension than the input batch shape ({input_batch_shape})""\n            )\n\n        # Check whether we can properly broadcast batch dimensions\n        err_msg = (\n            f""Model batch shape ({model_batch_shape}) and target batch shape ""\n            f""({target_batch_shape}) are not broadcastable.""\n        )\n        _mul_broadcast_shape(model_batch_shape, target_batch_shape, error_msg=err_msg)\n\n        if len(model_batch_shape) > len(input_batch_shape):\n            input_batch_shape = model_batch_shape\n        if len(model_batch_shape) > len(target_batch_shape):\n            target_batch_shape = model_batch_shape\n\n        # If input has no fantasy batch dimension but target does, we can save memory and computation by not\n        # computing the covariance for each element of the batch. Therefore we don\'t expand the inputs to the\n        # size of the fantasy model here - this is done below, after the evaluation and fast fantasy update\n        train_inputs = [tin.expand(input_batch_shape + tin.shape[-2:]) for tin in self.train_inputs]\n        train_targets = self.train_targets.expand(target_batch_shape + self.train_targets.shape[-1:])\n\n        full_inputs = [\n            torch.cat([train_input, input.expand(input_batch_shape + input.shape[-2:])], dim=-2)\n            for train_input, input in zip(train_inputs, inputs)\n        ]\n        full_targets = torch.cat([train_targets, targets.expand(target_batch_shape + targets.shape[-1:])], dim=-1)\n\n        try:\n            fantasy_kwargs = {""noise"": kwargs.pop(""noise"")}\n        except KeyError:\n            fantasy_kwargs = {}\n\n        full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)\n\n        # Copy model without copying training data or prediction strategy (since we\'ll overwrite those)\n        old_pred_strat = self.prediction_strategy\n        old_train_inputs = self.train_inputs\n        old_train_targets = self.train_targets\n        old_likelihood = self.likelihood\n        self.prediction_strategy = None\n        self.train_inputs = None\n        self.train_targets = None\n        self.likelihood = None\n        new_model = deepcopy(self)\n        self.prediction_strategy = old_pred_strat\n        self.train_inputs = old_train_inputs\n        self.train_targets = old_train_targets\n        self.likelihood = old_likelihood\n\n        new_model.likelihood = old_likelihood.get_fantasy_likelihood(**fantasy_kwargs)\n        new_model.prediction_strategy = old_pred_strat.get_fantasy_strategy(\n            inputs, targets, full_inputs, full_targets, full_output, **fantasy_kwargs\n        )\n\n        # if the fantasies are at the same points, we need to expand the inputs for the new model\n        if tbdim == ibdim + 1:\n            new_model.train_inputs = [fi.expand(target_batch_shape + fi.shape[-2:]) for fi in full_inputs]\n        else:\n            new_model.train_inputs = full_inputs\n        new_model.train_targets = full_targets\n\n        return new_model\n\n    def train(self, mode=True):\n        if mode:\n            self.prediction_strategy = None\n        return super(ExactGP, self).train(mode)\n\n    def _load_from_state_dict(\n        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n    ):\n        self.prediction_strategy = None\n        super()._load_from_state_dict(\n            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n        )\n\n    def __call__(self, *args, **kwargs):\n        train_inputs = list(self.train_inputs) if self.train_inputs is not None else []\n        inputs = [i.unsqueeze(-1) if i.ndimension() == 1 else i for i in args]\n\n        # Training mode: optimizing\n        if self.training:\n            if self.train_inputs is None:\n                raise RuntimeError(\n                    ""train_inputs, train_targets cannot be None in training mode. ""\n                    ""Call .eval() for prior predictions, or call .set_train_data() to add training data.""\n                )\n            if settings.debug.on():\n                if not all(torch.equal(train_input, input) for train_input, input in zip(train_inputs, inputs)):\n                    raise RuntimeError(""You must train on the training inputs!"")\n            res = super().__call__(*inputs, **kwargs)\n            return res\n\n        # Prior mode\n        elif settings.prior_mode.on() or self.train_inputs is None or self.train_targets is None:\n            full_inputs = args\n            full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)\n            if settings.debug().on():\n                if not isinstance(full_output, MultivariateNormal):\n                    raise RuntimeError(""ExactGP.forward must return a MultivariateNormal"")\n            return full_output\n\n        # Posterior mode\n        else:\n            if settings.debug.on():\n                if all(torch.equal(train_input, input) for train_input, input in zip(train_inputs, inputs)):\n                    warnings.warn(\n                        ""The input matches the stored training data. Did you forget to call model.train()?"",\n                        GPInputWarning,\n                    )\n\n            # Get the terms that only depend on training data\n            if self.prediction_strategy is None:\n                train_output = super().__call__(*train_inputs, **kwargs)\n\n                # Create the prediction strategy for\n                self.prediction_strategy = prediction_strategy(\n                    train_inputs=train_inputs,\n                    train_prior_dist=train_output,\n                    train_labels=self.train_targets,\n                    likelihood=self.likelihood,\n                )\n\n            # Concatenate the input to the training input\n            full_inputs = []\n            batch_shape = train_inputs[0].shape[:-2]\n            for train_input, input in zip(train_inputs, inputs):\n                # Make sure the batch shapes agree for training/test data\n                if batch_shape != train_input.shape[:-2]:\n                    batch_shape = _mul_broadcast_shape(batch_shape, train_input.shape[:-2])\n                    train_input = train_input.expand(*batch_shape, *train_input.shape[-2:])\n                if batch_shape != input.shape[:-2]:\n                    batch_shape = _mul_broadcast_shape(batch_shape, input.shape[:-2])\n                    train_input = train_input.expand(*batch_shape, *train_input.shape[-2:])\n                    input = input.expand(*batch_shape, *input.shape[-2:])\n                full_inputs.append(torch.cat([train_input, input], dim=-2))\n\n            # Get the joint distribution for training/test data\n            full_output = super(ExactGP, self).__call__(*full_inputs, **kwargs)\n            if settings.debug().on():\n                if not isinstance(full_output, MultivariateNormal):\n                    raise RuntimeError(""ExactGP.forward must return a MultivariateNormal"")\n            full_mean, full_covar = full_output.loc, full_output.lazy_covariance_matrix\n\n            # Determine the shape of the joint distribution\n            batch_shape = full_output.batch_shape\n            joint_shape = full_output.event_shape\n            tasks_shape = joint_shape[1:]  # For multitask learning\n            test_shape = torch.Size([joint_shape[0] - self.prediction_strategy.train_shape[0], *tasks_shape])\n\n            # Make the prediction\n            with settings._use_eval_tolerance():\n                predictive_mean, predictive_covar = self.prediction_strategy.exact_prediction(full_mean, full_covar)\n\n            # Reshape predictive mean to match the appropriate event shape\n            predictive_mean = predictive_mean.view(*batch_shape, *test_shape).contiguous()\n            return full_output.__class__(predictive_mean, predictive_covar)\n'"
gpytorch/models/exact_prediction_strategies.py,32,"b'#!/usr/bin/env python3\n\nimport functools\nimport string\n\nimport torch\n\nfrom .. import settings\nfrom ..lazy import (\n    BatchRepeatLazyTensor,\n    InterpolatedLazyTensor,\n    LazyEvaluatedKernelTensor,\n    MatmulLazyTensor,\n    NonLazyTensor,\n    RootLazyTensor,\n    SumLazyTensor,\n    ZeroLazyTensor,\n    delazify,\n    lazify,\n)\nfrom ..utils.cholesky import psd_safe_cholesky\nfrom ..utils.interpolation import left_interp, left_t_interp\nfrom ..utils.memoize import add_to_cache, cached\n\n\ndef clear_cache_hook(module, *args, **kwargs):\n    module._memoize_cache = {}\n\n\ndef prediction_strategy(train_inputs, train_prior_dist, train_labels, likelihood):\n    train_train_covar = train_prior_dist.lazy_covariance_matrix\n    if isinstance(train_train_covar, LazyEvaluatedKernelTensor):\n        cls = train_train_covar.kernel.prediction_strategy\n    else:\n        cls = DefaultPredictionStrategy\n    return cls(train_inputs, train_prior_dist, train_labels, likelihood)\n\n\nclass DefaultPredictionStrategy(object):\n    def __init__(self, train_inputs, train_prior_dist, train_labels, likelihood, root=None, inv_root=None):\n        # Flatten the training labels\n        train_shape = train_prior_dist.event_shape\n        train_labels = train_labels.view(*train_labels.shape[: -len(train_shape)], train_shape.numel())\n\n        self.train_inputs = train_inputs\n        self.train_prior_dist = train_prior_dist\n        self.train_labels = train_labels\n        self.likelihood = likelihood\n        self._last_test_train_covar = None\n        mvn = self.likelihood(train_prior_dist, train_inputs)\n        self.lik_train_train_covar = mvn.lazy_covariance_matrix\n\n        if root is not None:\n            add_to_cache(self.lik_train_train_covar, ""root_decomposition"", RootLazyTensor(root))\n\n        if inv_root is not None:\n            add_to_cache(self.lik_train_train_covar, ""root_inv_decomposition"", RootLazyTensor(inv_root))\n\n    def __deepcopy__(self, memo):\n        # deepcopying prediction strategies of a model evaluated on inputs that require gradients fails\n        # with RuntimeError (Only Tensors created explicitly by the user (graph leaves) support the deepcopy\n        # protocol at the moment). Overwriting this method make sure that the prediction strategies of a\n        # model are set to None upon deepcopying.\n        pass\n\n    def _exact_predictive_covar_inv_quad_form_cache(self, train_train_covar_inv_root, test_train_covar):\n        """"""\n        Computes a cache for K_X*X (K_XX + sigma^2 I)^-1 K_X*X if possible. By default, this does no work and returns\n        the first argument.\n\n        Args:\n            train_train_covar_inv_root (:obj:`torch.tensor`): a root of (K_XX + sigma^2 I)^-1\n            test_train_covar (:obj:`torch.tensor`): the observed noise (from the likelihood)\n\n        Returns\n            - A precomputed cache\n        """"""\n        res = train_train_covar_inv_root\n        if settings.detach_test_caches.on():\n            res = res.detach()\n\n        if res.grad_fn is not None:\n            wrapper = functools.partial(clear_cache_hook, self)\n            functools.update_wrapper(wrapper, clear_cache_hook)\n            res.grad_fn.register_hook(wrapper)\n\n        return res\n\n    def _exact_predictive_covar_inv_quad_form_root(self, precomputed_cache, test_train_covar):\n        r""""""\n        Computes :math:`K_{X^{*}X} S` given a precomputed cache\n        Where :math:`S` is a tensor such that :math:`SS^{\\top} = (K_{XX} + \\sigma^2 I)^{-1}`\n\n        Args:\n            precomputed_cache (:obj:`torch.tensor`): What was computed in _exact_predictive_covar_inv_quad_form_cache\n            test_train_covar (:obj:`torch.tensor`): The observed noise (from the likelihood)\n\n        Returns\n            :obj:`~gpytorch.lazy.LazyTensor`: :math:`K_{X^{*}X} S`\n        """"""\n        # Here the precomputed cache represents S,\n        # where S S^T = (K_XX + sigma^2 I)^-1\n        return test_train_covar.matmul(precomputed_cache)\n\n    def get_fantasy_strategy(self, inputs, targets, full_inputs, full_targets, full_output, **kwargs):\n        """"""\n        Returns a new PredictionStrategy that incorporates the specified inputs and targets as new training data.\n\n        This method is primary responsible for updating the mean and covariance caches. To add fantasy data to a\n        GP model, use the :meth:`~gpytorch.models.ExactGP.get_fantasy_model` method.\n\n        Args:\n            - :attr:`inputs` (Tensor `b1 x ... x bk x m x d` or `f x b1 x ... x bk x m x d`): Locations of fantasy\n                observations.\n            - :attr:`targets` (Tensor `b1 x ... x bk x m` or `f x b1 x ... x bk x m`): Labels of fantasy observations.\n            - :attr:`full_inputs` (Tensor `b1 x ... x bk x n+m x d` or `f x b1 x ... x bk x n+m x d`): Training data\n                concatenated with fantasy inputs\n            - :attr:`full_targets` (Tensor `b1 x ... x bk x n+m` or `f x b1 x ... x bk x n+m`): Training labels\n                concatenated with fantasy labels.\n            - :attr:`full_output` (:class:`gpytorch.distributions.MultivariateNormal`): Prior called on full_inputs\n\n        Returns:\n            - :class:`DefaultPredictionStrategy`\n                A `DefaultPredictionStrategy` model with `n + m` training examples, where the `m` fantasy examples have\n                been added and all test-time caches have been updated.\n        """"""\n        full_mean, full_covar = full_output.mean, full_output.lazy_covariance_matrix\n\n        batch_shape = full_inputs[0].shape[:-2]\n\n        full_mean = full_mean.view(*batch_shape, -1)\n        num_train = self.num_train\n\n        # Evaluate fant x train and fant x fant covariance matrices, leave train x train unevaluated.\n        fant_fant_covar = full_covar[..., num_train:, num_train:]\n        fant_mean = full_mean[..., num_train:]\n        mvn = self.train_prior_dist.__class__(fant_mean, fant_fant_covar)\n        fant_likelihood = self.likelihood.get_fantasy_likelihood(**kwargs)\n        mvn_obs = fant_likelihood(mvn, inputs, **kwargs)\n\n        fant_fant_covar = mvn_obs.covariance_matrix\n        fant_train_covar = delazify(full_covar[..., num_train:, :num_train])\n\n        self.fantasy_inputs = inputs\n        self.fantasy_targets = targets\n\n        r""""""\n        Compute a new mean cache given the old mean cache.\n\n        We have \\alpha = K^{-1}y, and we want to solve [K U; U\' S][a; b] = [y; y_f], where U\' is fant_train_covar,\n        S is fant_fant_covar, and y_f is (targets - fant_mean)\n\n        To do this, we solve the bordered linear system of equations for [a; b]:\n            AQ = U  # Q = fant_solve\n            [S - U\'Q]b = y_f - U\'\\alpha   ==> b = [S - U\'Q]^{-1}(y_f - U\'\\alpha)\n            a = \\alpha - Qb\n        """"""\n        # Get cached K inverse decomp. (or compute if we somehow don\'t already have the covariance cache)\n        K_inverse = self.lik_train_train_covar.root_inv_decomposition()\n        fant_solve = K_inverse.matmul(fant_train_covar.transpose(-2, -1))\n\n        # Solve for ""b"", the lower portion of the *new* \\\\alpha corresponding to the fantasy points.\n        schur_complement = fant_fant_covar - fant_train_covar.matmul(fant_solve)\n\n        # we\'d like to use a less hacky approach for the following, but einsum can be much faster than\n        # than unsqueezing/squeezing here (esp. in backward passes), unfortunately it currenlty has some\n        # issues with broadcasting: https://github.com/pytorch/pytorch/issues/15671\n        prefix = string.ascii_lowercase[: max(fant_train_covar.dim() - self.mean_cache.dim() - 1, 0)]\n        ftcm = torch.einsum(prefix + ""...yz,...z->"" + prefix + ""...y"", [fant_train_covar, self.mean_cache])\n\n        small_system_rhs = targets - fant_mean - ftcm\n        small_system_rhs = small_system_rhs.unsqueeze(-1)\n        # Schur complement of a spd matrix is guaranteed to be positive definite\n        schur_cholesky = psd_safe_cholesky(schur_complement, jitter=settings.cholesky_jitter.value())\n        fant_cache_lower = torch.cholesky_solve(small_system_rhs, schur_cholesky)\n\n        # Get ""a"", the new upper portion of the cache corresponding to the old training points.\n        fant_cache_upper = self.mean_cache.unsqueeze(-1) - fant_solve.matmul(fant_cache_lower)\n\n        fant_cache_upper = fant_cache_upper.squeeze(-1)\n        fant_cache_lower = fant_cache_lower.squeeze(-1)\n\n        # New mean cache.\n        fant_mean_cache = torch.cat((fant_cache_upper, fant_cache_lower), dim=-1)\n\n        """"""\n        Compute a new covariance cache given the old covariance cache.\n\n        We have access to K \\\\approx LL\' and K^{-1} \\\\approx R^{-1}R^{-T}, where L and R are low rank matrices\n        resulting from Lanczos (see the LOVE paper).\n\n        To update R^{-1}, we first update L:\n            [K U; U\' S] = [L 0; A B][L\' A\'; 0 B\']\n        Solving this matrix equation, we get:\n            K = LL\' ==>       L = L\n            U = LA\' ==>       A = UR^{-1}\n            S = AA\' + BB\' ==> B = cholesky(S - AA\')\n\n        Once we\'ve computed Z = [L 0; A B], we have that the new kernel matrix [K U; U\' S] \\approx ZZ\'. Therefore,\n        we can form a pseudo-inverse of Z directly to approximate [K U; U\' S]^{-1/2}.\n        """"""\n        # [K U; U\' S] = [L 0; lower_left schur_root]\n        batch_shape = fant_train_covar.shape[:-2]\n\n        L_inverse = self.covar_cache\n        L = delazify(self.lik_train_train_covar.root_decomposition().root)\n        m, n = L.shape[-2:]\n\n        lower_left = fant_train_covar.matmul(L_inverse)\n        schur = fant_fant_covar - lower_left.matmul(lower_left.transpose(-2, -1))\n        schur_root = psd_safe_cholesky(schur, jitter=settings.cholesky_jitter.value())\n\n        # Form new root Z = [L 0; lower_left schur_root]\n        num_fant = schur_root.size(-2)\n        m, n = L.shape[-2:]\n        new_root = torch.zeros(*batch_shape, m + num_fant, n + num_fant, device=L.device, dtype=L.dtype)\n        new_root[..., :m, :n] = L\n        new_root[..., m:, :n] = lower_left\n        new_root[..., m:, n:] = schur_root\n\n        # Use pseudo-inverse of Z as new inv root\n        Q, R = torch.qr(new_root)\n        Rdiag = torch.diagonal(R, dim1=-2, dim2=-1)\n        # if R is almost singular, add jitter (Rdiag is a view, so this works)\n        zeroish = Rdiag.abs() < 1e-6\n        if torch.any(zeroish):\n            # can\'t use in-place operation here b/c it would mess up backward pass\n            # haven\'t found a more elegant way to add a jitter diagonal yet...\n            jitter_diag = 1e-6 * torch.sign(Rdiag) * zeroish.to(Rdiag)\n            R = R + torch.diag_embed(jitter_diag)\n        new_covar_cache = torch.triangular_solve(Q.transpose(-2, -1), R)[0].transpose(-2, -1)\n\n        # Expand inputs accordingly if necessary (for fantasies at the same points)\n        if full_inputs[0].dim() <= full_targets.dim():\n            fant_batch_shape = full_targets.shape[:1]\n            n_batch = len(full_mean.shape[:-1])\n            repeat_shape = fant_batch_shape + torch.Size([1] * n_batch)\n            full_inputs = [fi.expand(fant_batch_shape + fi.shape) for fi in full_inputs]\n            full_mean = full_mean.expand(fant_batch_shape + full_mean.shape)\n            full_covar = BatchRepeatLazyTensor(full_covar, repeat_shape)\n            new_root = BatchRepeatLazyTensor(NonLazyTensor(new_root), repeat_shape)\n            # no need to repeat the covar cache, broadcasting will do the right thing\n\n        # Create new DefaultPredictionStrategy object\n        fant_strat = self.__class__(\n            train_inputs=full_inputs,\n            train_prior_dist=self.train_prior_dist.__class__(full_mean, full_covar),\n            train_labels=full_targets,\n            likelihood=fant_likelihood,\n            root=new_root,\n            inv_root=new_covar_cache,\n        )\n        fant_strat._memoize_cache = {""mean_cache"": fant_mean_cache, ""covar_cache"": new_covar_cache}\n\n        return fant_strat\n\n    @property\n    @cached(name=""covar_cache"")\n    def covar_cache(self):\n        train_train_covar = self.lik_train_train_covar\n        train_train_covar_inv_root = delazify(train_train_covar.root_inv_decomposition().root)\n        return self._exact_predictive_covar_inv_quad_form_cache(train_train_covar_inv_root, self._last_test_train_covar)\n\n    @property\n    @cached(name=""mean_cache"")\n    def mean_cache(self):\n        mvn = self.likelihood(self.train_prior_dist, self.train_inputs)\n        train_mean, train_train_covar = mvn.loc, mvn.lazy_covariance_matrix\n\n        train_labels_offset = (self.train_labels - train_mean).unsqueeze(-1)\n        mean_cache = train_train_covar.inv_matmul(train_labels_offset).squeeze(-1)\n\n        if settings.detach_test_caches.on():\n            mean_cache = mean_cache.detach()\n\n        if mean_cache.grad_fn is not None:\n            wrapper = functools.partial(clear_cache_hook, self)\n            functools.update_wrapper(wrapper, clear_cache_hook)\n            mean_cache.grad_fn.register_hook(wrapper)\n\n        return mean_cache\n\n    @property\n    def num_train(self):\n        return self.train_prior_dist.event_shape.numel()\n\n    @property\n    def train_shape(self):\n        return self.train_prior_dist.event_shape\n\n    def exact_prediction(self, joint_mean, joint_covar):\n        # Find the components of the distribution that contain test data\n        test_mean = joint_mean[..., self.num_train :]\n        # For efficiency - we can make things more efficient\n        if joint_covar.size(-1) <= settings.max_eager_kernel_size.value():\n            test_covar = joint_covar[..., self.num_train :, :].evaluate()\n            test_test_covar = test_covar[..., self.num_train :]\n            test_train_covar = test_covar[..., : self.num_train]\n        else:\n            test_test_covar = joint_covar[..., self.num_train :, self.num_train :]\n            test_train_covar = joint_covar[..., self.num_train :, : self.num_train]\n\n        return (\n            self.exact_predictive_mean(test_mean, test_train_covar),\n            self.exact_predictive_covar(test_test_covar, test_train_covar),\n        )\n\n    def exact_predictive_mean(self, test_mean, test_train_covar):\n        """"""\n        Computes the posterior predictive covariance of a GP\n\n        Args:\n            test_mean (:obj:`torch.tensor`): The test prior mean\n            test_train_covar (:obj:`gpytorch.lazy.LazyTensor`): Covariance matrix between test and train inputs\n\n        Returns:\n            :obj:`torch.tensor`: The predictive posterior mean of the test points\n        """"""\n        # NOTE TO FUTURE SELF:\n        # You **cannot* use addmv here, because test_train_covar may not actually be a non lazy tensor even for an exact\n        # GP, and using addmv requires you to delazify test_train_covar, which is obviously a huge no-no!\n        res = (test_train_covar @ self.mean_cache.unsqueeze(-1)).squeeze(-1)\n        res = res + test_mean\n\n        return res\n\n    def exact_predictive_covar(self, test_test_covar, test_train_covar):\n        """"""\n        Computes the posterior predictive covariance of a GP\n\n        Args:\n            test_train_covar (:obj:`gpytorch.lazy.LazyTensor`): Covariance matrix between test and train inputs\n            test_test_covar (:obj:`gpytorch.lazy.LazyTensor`): Covariance matrix between test inputs\n\n        Returns:\n            :obj:`gpytorch.lazy.LazyTensor`: A LazyTensor representing the predictive posterior covariance of the\n                                               test points\n        """"""\n        if settings.fast_pred_var.on():\n            self._last_test_train_covar = test_train_covar\n\n        if settings.skip_posterior_variances.on():\n            return ZeroLazyTensor(*test_test_covar.size())\n\n        if settings.fast_pred_var.off():\n            dist = self.train_prior_dist.__class__(\n                torch.zeros_like(self.train_prior_dist.mean), self.train_prior_dist.lazy_covariance_matrix\n            )\n            if settings.detach_test_caches.on():\n                train_train_covar = self.likelihood(dist, self.train_inputs).lazy_covariance_matrix.detach()\n            else:\n                train_train_covar = self.likelihood(dist, self.train_inputs).lazy_covariance_matrix\n\n            test_train_covar = delazify(test_train_covar)\n            train_test_covar = test_train_covar.transpose(-1, -2)\n            covar_correction_rhs = train_train_covar.inv_matmul(train_test_covar)\n            # For efficiency\n            if torch.is_tensor(test_test_covar):\n                # We can use addmm in the 2d case\n                if test_test_covar.dim() == 2:\n                    return lazify(\n                        torch.addmm(test_test_covar, test_train_covar, covar_correction_rhs, beta=1, alpha=-1)\n                    )\n                else:\n                    return lazify(test_test_covar + test_train_covar @ covar_correction_rhs.mul(-1))\n            # In other cases - we\'ll use the standard infrastructure\n            else:\n                return test_test_covar + MatmulLazyTensor(test_train_covar, covar_correction_rhs.mul(-1))\n\n        precomputed_cache = self.covar_cache\n        covar_inv_quad_form_root = self._exact_predictive_covar_inv_quad_form_root(precomputed_cache, test_train_covar)\n        if torch.is_tensor(test_test_covar):\n            return lazify(\n                torch.add(\n                    test_test_covar, covar_inv_quad_form_root @ covar_inv_quad_form_root.transpose(-1, -2), alpha=-1\n                )\n            )\n        else:\n            return test_test_covar + MatmulLazyTensor(\n                covar_inv_quad_form_root, covar_inv_quad_form_root.transpose(-1, -2).mul(-1)\n            )\n\n\nclass InterpolatedPredictionStrategy(DefaultPredictionStrategy):\n    def __init__(self, train_inputs, train_prior_dist, train_labels, likelihood):\n        super().__init__(train_inputs, train_prior_dist, train_labels, likelihood)\n        self.train_prior_dist = self.train_prior_dist.__class__(\n            self.train_prior_dist.mean, self.train_prior_dist.lazy_covariance_matrix.evaluate_kernel()\n        )\n\n    def _exact_predictive_covar_inv_quad_form_cache(self, train_train_covar_inv_root, test_train_covar):\n        train_interp_indices = test_train_covar.right_interp_indices\n        train_interp_values = test_train_covar.right_interp_values\n        base_lazy_tensor = test_train_covar.base_lazy_tensor\n        base_size = base_lazy_tensor.size(-1)\n        res = base_lazy_tensor.matmul(\n            left_t_interp(train_interp_indices, train_interp_values, train_train_covar_inv_root, base_size)\n        )\n        return res\n\n    def _exact_predictive_covar_inv_quad_form_root(self, precomputed_cache, test_train_covar):\n        # Here the precomputed cache represents K_UU W S,\n        # where S S^T = (K_XX + sigma^2 I)^-1\n        test_interp_indices = test_train_covar.left_interp_indices\n        test_interp_values = test_train_covar.left_interp_values\n        res = left_interp(test_interp_indices, test_interp_values, precomputed_cache)\n        return res\n\n    def get_fantasy_strategy(self, inputs, targets, full_inputs, full_targets, full_output, **kwargs):\n        raise NotImplementedError(\n            ""Fantasy observation updates not yet supported for models using InterpolatedLazyTensors""\n        )\n\n    @property\n    @cached(name=""mean_cache"")\n    def mean_cache(self):\n        train_train_covar = self.train_prior_dist.lazy_covariance_matrix\n        train_interp_indices = train_train_covar.left_interp_indices\n        train_interp_values = train_train_covar.left_interp_values\n\n        mvn = self.likelihood(self.train_prior_dist, self.train_inputs)\n        train_mean, train_train_covar_with_noise = mvn.mean, mvn.lazy_covariance_matrix\n\n        mean_diff = (self.train_labels - train_mean).unsqueeze(-1)\n        train_train_covar_inv_labels = train_train_covar_with_noise.inv_matmul(mean_diff)\n\n        # New root factor\n        base_size = train_train_covar.base_lazy_tensor.size(-1)\n        mean_cache = train_train_covar.base_lazy_tensor.matmul(\n            left_t_interp(train_interp_indices, train_interp_values, train_train_covar_inv_labels, base_size)\n        )\n\n        # Prevent backprop through this variable\n        if settings.detach_test_caches.on():\n            return mean_cache.detach()\n        else:\n            return mean_cache\n\n    @property\n    @cached(name=""covar_cache"")\n    def covar_cache(self):\n        # Get inverse root\n        train_train_covar = self.train_prior_dist.lazy_covariance_matrix\n        train_interp_indices = train_train_covar.left_interp_indices\n        train_interp_values = train_train_covar.left_interp_values\n\n        # Get probe vectors for inverse root\n        num_probe_vectors = settings.fast_pred_var.num_probe_vectors()\n        num_inducing = train_train_covar.base_lazy_tensor.size(-1)\n        vector_indices = torch.randperm(num_inducing).type_as(train_interp_indices)\n        probe_vector_indices = vector_indices[:num_probe_vectors]\n        test_vector_indices = vector_indices[num_probe_vectors : 2 * num_probe_vectors]\n\n        probe_interp_indices = probe_vector_indices.unsqueeze(1)\n        probe_test_interp_indices = test_vector_indices.unsqueeze(1)\n        dtype = train_train_covar.dtype\n        device = train_train_covar.device\n        probe_interp_values = torch.ones(num_probe_vectors, 1, dtype=dtype, device=device)\n\n        batch_shape = train_train_covar.base_lazy_tensor.batch_shape\n        probe_vectors = InterpolatedLazyTensor(\n            train_train_covar.base_lazy_tensor,\n            train_interp_indices.expand(*batch_shape, *train_interp_indices.shape[-2:]),\n            train_interp_values.expand(*batch_shape, *train_interp_values.shape[-2:]),\n            probe_interp_indices.expand(*batch_shape, *probe_interp_indices.shape[-2:]),\n            probe_interp_values.expand(*batch_shape, *probe_interp_values.shape[-2:]),\n        ).evaluate()\n        test_vectors = InterpolatedLazyTensor(\n            train_train_covar.base_lazy_tensor,\n            train_interp_indices.expand(*batch_shape, *train_interp_indices.shape[-2:]),\n            train_interp_values.expand(*batch_shape, *train_interp_values.shape[-2:]),\n            probe_test_interp_indices.expand(*batch_shape, *probe_test_interp_indices.shape[-2:]),\n            probe_interp_values.expand(*batch_shape, *probe_interp_values.shape[-2:]),\n        ).evaluate()\n\n        # Put data through the likelihood\n        dist = self.train_prior_dist.__class__(\n            torch.zeros_like(self.train_prior_dist.mean), self.train_prior_dist.lazy_covariance_matrix\n        )\n        train_train_covar_plus_noise = self.likelihood(dist, self.train_inputs).lazy_covariance_matrix\n\n        # Get inverse root\n        train_train_covar_inv_root = train_train_covar_plus_noise.root_inv_decomposition(\n            probe_vectors, test_vectors\n        ).root\n        train_train_covar_inv_root = train_train_covar_inv_root.evaluate()\n\n        # New root factor\n        root = self._exact_predictive_covar_inv_quad_form_cache(train_train_covar_inv_root, self._last_test_train_covar)\n\n        # Precomputed factor\n        if settings.fast_pred_samples.on():\n            inside = train_train_covar.base_lazy_tensor + RootLazyTensor(root).mul(-1)\n            inside_root = inside.root_decomposition().root.evaluate()\n            # Prevent backprop through this variable\n            if settings.detach_test_caches.on():\n                inside_root = inside_root.detach()\n            covar_cache = inside_root, None\n        else:\n            # Prevent backprop through this variable\n            if settings.detach_test_caches.on():\n                root = root.detach()\n            covar_cache = None, root\n\n        return covar_cache\n\n    def exact_prediction(self, joint_mean, joint_covar):\n        # Find the components of the distribution that contain test data\n        test_mean = joint_mean[..., self.num_train :]\n        test_test_covar = joint_covar[..., self.num_train :, self.num_train :].evaluate_kernel()\n        test_train_covar = joint_covar[..., self.num_train :, : self.num_train].evaluate_kernel()\n\n        return (\n            self.exact_predictive_mean(test_mean, test_train_covar),\n            self.exact_predictive_covar(test_test_covar, test_train_covar),\n        )\n\n    def exact_predictive_mean(self, test_mean, test_train_covar):\n        precomputed_cache = self.mean_cache\n        test_interp_indices = test_train_covar.left_interp_indices\n        test_interp_values = test_train_covar.left_interp_values\n        res = left_interp(test_interp_indices, test_interp_values, precomputed_cache).squeeze(-1) + test_mean\n        return res\n\n    def exact_predictive_covar(self, test_test_covar, test_train_covar):\n        if settings.fast_pred_var.off() and settings.fast_pred_samples.off():\n            return super(InterpolatedPredictionStrategy, self).exact_predictive_covar(test_test_covar, test_train_covar)\n\n        self._last_test_train_covar = test_train_covar\n        test_interp_indices = test_train_covar.left_interp_indices\n        test_interp_values = test_train_covar.left_interp_values\n\n        precomputed_cache = self.covar_cache\n        if (settings.fast_pred_samples.on() and precomputed_cache[0] is None) or (\n            settings.fast_pred_samples.off() and precomputed_cache[1] is None\n        ):\n            self._memoize_cache.pop(""covar_cache"")\n            precomputed_cache = self.covar_cache\n\n        # Compute the exact predictive posterior\n        if settings.fast_pred_samples.on():\n            res = self._exact_predictive_covar_inv_quad_form_root(precomputed_cache[0], test_train_covar)\n            res = RootLazyTensor(res)\n        else:\n            root = left_interp(test_interp_indices, test_interp_values, precomputed_cache[1])\n            res = test_test_covar + RootLazyTensor(root).mul(-1)\n        return res\n\n\nclass SumPredictionStrategy(DefaultPredictionStrategy):\n    @property\n    def _sub_strategies(self):\n        sub_strategies = []\n        for lazy_tensor in self.train_prior_dist.lazy_covariance_matrix.evaluate_kernel().lazy_tensors:\n            pred_strat = prediction_strategy(\n                self.train_inputs,\n                self.train_prior_dist.__class__(self.train_prior_dist.mean, lazy_tensor),\n                self.train_labels,\n                self.likelihood,\n            )\n            sub_strategies.append(pred_strat)\n\n        return sub_strategies\n\n    def _exact_predictive_covar_inv_quad_form_cache(self, train_train_covar_inv_root, test_train_covar):\n        test_train_covar = test_train_covar.evaluate_kernel()\n        if not isinstance(test_train_covar, SumLazyTensor):\n            return super(SumPredictionStrategy, self)._exact_predictive_covar_inv_quad_form_cache(\n                train_train_covar_inv_root, test_train_covar\n            )\n        else:\n            return tuple(\n                sub_strat._exact_predictive_covar_inv_quad_form_cache(train_train_covar_inv_root, test_train_covar_comp)\n                for sub_strat, test_train_covar_comp in zip(self._sub_strategies, test_train_covar.lazy_tensors)\n            )\n\n    def _exact_predictive_covar_inv_quad_form_root(self, precomputed_cache, test_train_covar):\n        # Here the precomputed cache is a list\n        # where each component in the list is the precomputed cache for each component lazy tensor\n        test_train_covar = test_train_covar.evaluate_kernel()\n        if not isinstance(test_train_covar, SumLazyTensor):\n            return super(SumPredictionStrategy, self)._exact_predictive_covar_inv_quad_form_root(\n                precomputed_cache, test_train_covar\n            )\n        else:\n            return sum(\n                sub_strat._exact_predictive_covar_inv_quad_form_root(cache_comp, test_train_covar_comp)\n                for sub_strat, cache_comp, test_train_covar_comp in zip(\n                    self._sub_strategies, precomputed_cache, test_train_covar.evaluate_kernel().lazy_tensors\n                )\n            )\n'"
gpytorch/models/gp.py,0,b'#!/usr/bin/env python3\n\nfrom ..module import Module\n\n\nclass GP(Module):\n    pass\n'
gpytorch/models/model_list.py,4,"b'#! /usr/bin/env python3\n\nfrom abc import ABC, abstractproperty\n\nimport torch\nfrom torch.nn import ModuleList\n\nfrom gpytorch.likelihoods import LikelihoodList\nfrom gpytorch.models import GP\n\n\nclass AbstractModelList(GP, ABC):\n    @abstractproperty\n    def num_outputs(self):\n        """"""The model\'s number of outputs""""""\n        pass\n\n    def forward_i(self, i, *args, **kwargs):\n        """"""Forward restricted to the i-th output only""""""\n        raise NotImplementedError\n\n    def likelihood_i(self, i, *args, **kwargs):\n        """"""Evaluate likelihood of the i-th output only""""""\n        raise NotImplementedError\n\n\nclass IndependentModelList(AbstractModelList):\n    def __init__(self, *models):\n        super().__init__()\n        self.models = ModuleList(models)\n        for m in models:\n            if not hasattr(m, ""likelihood""):\n                raise ValueError(\n                    ""IndependentModelList currently only supports models that have a likelihood (e.g. ExactGPs)""\n                )\n        self.likelihood = LikelihoodList(*[m.likelihood for m in models])\n\n    @property\n    def num_outputs(self):\n        return len(self.models)\n\n    def forward_i(self, i, *args, **kwargs):\n        return self.models[i].forward(*args, **kwargs)\n\n    def likelihood_i(self, i, *args, **kwargs):\n        return self.likelihood.likelihoods[i](*args, **kwargs)\n\n    def forward(self, *args, **kwargs):\n        return [model.forward(*args_, **kwargs) for model, args_ in zip(self.models, _get_tensor_args(*args))]\n\n    def get_fantasy_model(self, inputs, targets, **kwargs):\n        """"""\n        Returns a new GP model that incorporates the specified inputs and targets as new training data.\n\n        This is a simple wrapper that creates fantasy models for each of the models in the model list,\n        and returns the same class of fantasy models.\n\n        Args:\n            - :attr:`inputs`: List of locations of fantasy observations, one for each model.\n            - :attr:`targets` List of labels of fantasy observations, one for each model.\n\n        Returns:\n            - :class:`IndependentModelList`\n                An `IndependentModelList` model, where each sub-model is the fantasy model of the respective\n                sub-model in the original model at the corresponding input locations / labels.\n        """"""\n\n        if ""noise"" in kwargs:\n            noise = kwargs.pop(""noise"")\n            kwargs = [{**kwargs, ""noise"": noise_} if noise_ is not None else kwargs for noise_ in noise]\n        else:\n            kwargs = [kwargs] * len(inputs)\n\n        fantasy_models = [\n            model.get_fantasy_model(*inputs_, *targets_, **kwargs_)\n            for model, inputs_, targets_, kwargs_ in zip(\n                self.models, _get_tensor_args(*inputs), _get_tensor_args(*targets), kwargs\n            )\n        ]\n        return self.__class__(*fantasy_models)\n\n    def __call__(self, *args, **kwargs):\n        return [model.__call__(*args_, **kwargs) for model, args_ in zip(self.models, _get_tensor_args(*args))]\n\n    @property\n    def train_inputs(self):\n        return [model.train_inputs for model in self.models]\n\n    @property\n    def train_targets(self):\n        return [model.train_targets for model in self.models]\n\n\ndef _get_tensor_args(*args):\n    for arg in args:\n        if torch.is_tensor(arg):\n            yield (arg,)\n        else:\n            yield arg\n'"
gpytorch/priors/__init__.py,0,"b'#!/usr/bin/env python3\n\nfrom .horseshoe_prior import HorseshoePrior\nfrom .lkj_prior import LKJCholeskyFactorPrior, LKJCovariancePrior, LKJPrior\nfrom .prior import Prior\nfrom .smoothed_box_prior import SmoothedBoxPrior\nfrom .torch_priors import GammaPrior, LogNormalPrior, MultivariateNormalPrior, NormalPrior, UniformPrior\n\n# from .wishart_prior import InverseWishartPrior, WishartPrior\n\n\n__all__ = [\n    ""Prior"",\n    ""GammaPrior"",\n    ""HorseshoePrior"",\n    ""LKJPrior"",\n    ""LKJCholeskyFactorPrior"",\n    ""LKJCovariancePrior"",\n    ""LogNormalPrior"",\n    ""MultivariateNormalPrior"",\n    ""NormalPrior"",\n    ""SmoothedBoxPrior"",\n    ""UniformPrior"",\n    # ""InverseWishartPrior"",\n    # ""WishartPrior"",\n]\n'"
gpytorch/priors/horseshoe_prior.py,9,"b'#!/usr/bin/env python3\n\nimport math\nfrom numbers import Number\n\nimport torch\nfrom torch.distributions import HalfCauchy, Normal, constraints\nfrom torch.nn import Module as TModule\n\nfrom gpytorch.priors.prior import Prior\n\n\nclass HorseshoePrior(Prior):\n    """"""Horseshoe prior.\n\n    There is no analytical form for the horeshoe prior\'s pdf, but it\n    satisfies a tight bound of the form `lb(x) <= pdf(x) <= ub(x)`, where\n\n        lb(x) = K/2 * log(1 + 4 * (scale / x) ** 2)\n        ub(x) = K * log(1 + 2 * (scale / x) ** 2)\n\n    with `K = 1 / sqrt(2 pi^3)`. Here, we simply use\n\n        pdf(x) ~ (lb(x) + ub(x)) / 2\n\n    Reference: C. M. Carvalho, N. G. Polson, and J. G. Scott.\n        The horseshoe estimator for sparse signals. Biometrika, 2010.\n    """"""\n\n    arg_constraints = {""scale"": constraints.positive}\n    support = constraints.real\n    _validate_args = True\n\n    def __init__(self, scale, validate_args=False, transform=None):\n        TModule.__init__(self)\n        if isinstance(scale, Number):\n            scale = torch.tensor(float(scale))\n        self.K = 1 / math.sqrt(2 * math.pi ** 3)\n        self.scale = scale\n        super().__init__(scale.shape, validate_args=validate_args)\n        # now need to delete to be able to register buffer\n        del self.scale\n        self.register_buffer(""scale"", scale)\n        self._transform = transform\n\n    def log_prob(self, X):\n        A = (self.scale / self.transform(X)) ** 2\n        lb = self.K / 2 * torch.log(1 + 4 * A)\n        ub = self.K * torch.log(1 + 2 * A)\n        return torch.log((lb + ub) / 2)\n\n    def rsample(self, sample_shape=torch.Size([])):\n        local_shrinkage = HalfCauchy(1).rsample(self.scale.shape)\n        param_sample = Normal(0, local_shrinkage * self.scale).rsample(sample_shape)\n        return param_sample\n\n    def expand(self, expand_shape, _instance=None):\n        batch_shape = torch.Size(expand_shape)\n        return HorseshoePrior(self.scale.expand(batch_shape))\n'"
gpytorch/priors/lkj_prior.py,17,"b'#!/usr/bin/env python3\n\nimport math\nfrom numbers import Number\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.nn import Module as TModule\n\nfrom .prior import Prior\n\n\nclass LKJPrior(Prior):\n    r""""""LKJ prior over n x n (positive definite) correlation matrices\n\n    .. math:\n\n        \\begin{equation*}\n            pdf(\\Sigma) ~ |\\Sigma| ^ (\\eta  - 1)\n        \\end{equation*}\n\n    where :math:`\\eta > 0` is a shape parameter.\n\n    Reference: Bayesian Data Analysis, 3rd ed., Gelman et al., p. 576\n    """"""\n\n    arg_constraints = {""n"": constraints.positive_integer, ""eta"": constraints.positive}\n    # TODO: move correlation matrix validation upstream into pytorch\n    support = constraints.positive_definite\n    _validate_args = True\n\n    def __init__(self, n, eta, validate_args=False):\n        TModule.__init__(self)\n        if not isinstance(n, int) or n < 1:\n            raise ValueError(""n must be a positive integer"")\n        if isinstance(eta, Number):\n            eta = torch.tensor(float(eta))\n        self.n = torch.tensor(n, dtype=torch.long, device=eta.device)\n        batch_shape = eta.shape\n        event_shape = torch.Size([n, n])\n        # Normalization constant(s)\n        i = torch.arange(n, dtype=eta.dtype, device=eta.device)\n        C = (((2 * eta.view(-1, 1) - 2 + i) * i).sum(1) * math.log(2)).view_as(eta)\n        C += n * torch.sum(2 * torch.lgamma(i / 2 + 1) - torch.lgamma(i + 2))\n        # need to assign values before registering as buffers to make argument validation work\n        self.eta = eta\n        self.C = C\n        super(LKJPrior, self).__init__(batch_shape, event_shape, validate_args=validate_args)\n        # now need to delete to be able to register buffer\n        del self.eta, self.C\n        self.register_buffer(""eta"", eta)\n        self.register_buffer(""C"", C)\n\n    def log_prob(self, X):\n        if any(s != self.n for s in X.shape[-2:]):\n            raise ValueError(""Correlation matrix is not of size n={}"".format(self.n.item()))\n        if not _is_valid_correlation_matrix(X):\n            raise ValueError(""Input is not a valid correlation matrix"")\n        log_diag_sum = X.cholesky(upper=True).diagonal(dim1=-2, dim2=-1).log().sum(-1)\n        return self.C + (self.eta - 1) * 2 * log_diag_sum\n\n\nclass LKJCholeskyFactorPrior(LKJPrior):\n    r""""""LKJ prior over n x n (positive definite) Cholesky-decomposed\n    correlation matrices\n\n    .. math:\n\n        \\begin{equation*}\n            pdf(\\Sigma) ~ |\\Sigma| ^ (\\eta  - 1)\n        \\end{equation*}\n\n    where :math:`\\eta > 0` is a shape parameter and n is the dimension of the\n    correlation matrix.\n\n    LKJCholeskyFactorPrior is different from LKJPrior in that it accepts the\n    Cholesky factor of the correlation matrix to compute probabilities.\n    """"""\n\n    support = constraints.lower_cholesky\n\n    def log_prob(self, X):\n        if any(s != self.n for s in X.shape[-2:]):\n            raise ValueError(""Cholesky factor is not of size n={}"".format(self.n.item()))\n        if not _is_valid_correlation_matrix_cholesky_factor(X):\n            raise ValueError(""Input is not a Cholesky factor of a valid correlation matrix"")\n        log_diag_sum = torch.diagonal(X, dim1=-2, dim2=-1).log().sum(-1)\n        return self.C + (self.eta - 1) * 2 * log_diag_sum\n\n\nclass LKJCovariancePrior(LKJPrior):\n    """"""LKJCovariancePrior combines an LKJ prior over the correlation matrix\n    and a user-specified prior over marginal standard deviations to return a\n    prior over the full covariance matrix.\n\n    Usage: LKJCovariancePrior(n, eta, sd_prior), where\n        n is a positive integer, the size of the covariance matrix,\n        eta is a positive shape parameter for the LKJPrior over correlations, and\n        sd_prior is a scalar Prior over nonnegative numbers, which is used for\n        each of the n marginal standard deviations on the covariance matrix.\n    """"""\n\n    def __init__(self, n, eta, sd_prior, validate_args=False):\n        if not isinstance(sd_prior, Prior):\n            raise ValueError(""sd_prior must be an instance of Prior"")\n        if not isinstance(n, int):\n            raise ValueError(""n must be an integer"")\n        if sd_prior.event_shape not in {torch.Size([1]), torch.Size([n])}:\n            raise ValueError(""sd_prior must have event_shape 1 or n"")\n        correlation_prior = LKJPrior(n=n, eta=eta, validate_args=validate_args)\n        if sd_prior.batch_shape != correlation_prior.batch_shape:\n            raise ValueError(""sd_prior must have same batch_shape as eta"")\n        TModule.__init__(self)\n        super(LKJPrior, self).__init__(\n            correlation_prior.batch_shape, correlation_prior.event_shape, validate_args=False\n        )\n        self.correlation_prior = correlation_prior\n        self.sd_prior = sd_prior\n\n    def log_prob(self, X):\n        marginal_var = torch.diagonal(X, dim1=-2, dim2=-1)\n        if not torch.all(marginal_var >= 0):\n            raise ValueError(""Variance(s) cannot be negative"")\n        marginal_sd = marginal_var.sqrt()\n        sd_diag_mat = _batch_form_diag(1 / marginal_sd)\n        correlations = torch.matmul(torch.matmul(sd_diag_mat, X), sd_diag_mat)\n        log_prob_corr = self.correlation_prior.log_prob(correlations)\n        log_prob_sd = self.sd_prior.log_prob(marginal_sd)\n        return log_prob_corr + log_prob_sd\n\n\ndef _batch_form_diag(tsr):\n    """"""Form diagonal matrices in batch mode.""""""\n    eye = torch.eye(tsr.shape[-1], dtype=tsr.dtype, device=tsr.device)\n    M = tsr.unsqueeze(-1).expand(tsr.shape + tsr.shape[-1:])\n    return eye * M\n\n\ndef _is_valid_correlation_matrix(Sigma, tol=1e-6):\n    """"""Check if supplied matrix is a valid correlation matrix\n\n    A matrix is a valid correlation matrix if it is positive semidefinite, and\n    if all diagonal elements are equal to 1.\n\n    Args:\n        Sigma: A n x n correlation matrix, or a batch of b correlation matrices\n            with shape b x n x n\n        tol: The tolerance with which to check unit value of the diagonal elements\n\n    Returns:\n        True if Sigma is a valid correlation matrix, False otherwise (in batch\n            mode, all matrices in the batch need to be valid correlation matrices)\n\n    """"""\n    pdef = torch.all(constraints.positive_definite.check(Sigma))\n    return pdef and all(torch.all(torch.abs(S.diag() - 1) < tol) for S in Sigma.view(-1, *Sigma.shape[-2:]))\n\n\ndef _is_valid_correlation_matrix_cholesky_factor(L, tol=1e-6):\n    """"""Check if supplied matrix is a Cholesky factor of a valid correlation matrix\n\n    A matrix is a Cholesky fator of a valid correlation matrix if it is lower\n    triangular, has positive diagonal, and unit row-sum\n\n    Args:\n        L: A n x n lower-triangular matrix, or a batch of b lower-triangular\n            matrices with shape b x n x n\n        tol: The tolerance with which to check positivity of the diagonal and\n            unit-sum of the rows\n\n    Returns:\n        True if L is a Cholesky factor of a valid correlation matrix, False\n            otherwise (in batch mode, all matrices in the batch need to be\n            Cholesky factors of valid correlation matrices)\n\n    """"""\n    unit_row_length = torch.all((torch.norm(L, dim=-1) - 1).abs() < tol)\n    return unit_row_length and torch.all(constraints.lower_cholesky.check(L))\n'"
gpytorch/priors/prior.py,2,"b'#!/usr/bin/env python3\n\nfrom abc import ABC\n\nfrom torch.nn import Module\n\nfrom ..distributions import Distribution\n\n\nclass Prior(Distribution, Module, ABC):\n    """"""\n    Base class for Priors in GPyTorch.\n    In GPyTorch, a parameter can be assigned a prior by passing it as the `prior` argument to\n    :func:`~gpytorch.module.register_parameter`. GPyTorch performs internal bookkeeping of priors,\n    and for each parameter with a registered prior includes the log probability of the parameter under its\n    respective prior in computing the Marginal Log-Likelihood.\n    """"""\n\n    def transform(self, x):\n        return self._transform(x) if self._transform is not None else x\n\n    def log_prob(self, x):\n        """"""Returns the log-probability of the parameter value under the prior.""""""\n        return super(Prior, self).log_prob(self.transform(x))\n'"
gpytorch/priors/smoothed_box_prior.py,7,"b'#!/usr/bin/env python3\n\nimport math\nfrom numbers import Number\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.distributions.utils import broadcast_all\nfrom torch.nn import Module as TModule\n\nfrom .prior import Prior\nfrom .torch_priors import NormalPrior\n\n\nclass SmoothedBoxPrior(Prior):\n    r""""""A smoothed approximation of a uniform prior.\n\n    Has full support on the reals and is differentiable everywhere.\n\n    .. math::\n\n        \\begin{equation*}\n            B = {x: a_i <= x_i <= b_i}\n            d(x, B) = min_{x\' in B} |x - x\'|\n            pdf(x) ~ exp(- d(x, B)**2 / sqrt(2 * sigma^2))\n        \\end{equation*}\n\n    """"""\n\n    arg_constraints = {""sigma"": constraints.positive, ""a"": constraints.real, ""b"": constraints.real}\n    support = constraints.real\n    _validate_args = True\n\n    def __init__(self, a, b, sigma=0.01, validate_args=False, transform=None):\n        TModule.__init__(self)\n        _a = torch.tensor(float(a)) if isinstance(a, Number) else a\n        _a = _a.view(-1) if _a.dim() < 1 else _a\n        _a, _b, _sigma = broadcast_all(_a, b, sigma)\n        if not torch.all(constraints.less_than(_b).check(_a)):\n            raise ValueError(""must have that a < b (element-wise)"")\n        # TODO: Proper argument validation including broadcasting\n        batch_shape, event_shape = _a.shape[:-1], _a.shape[-1:]\n        # need to assign values before registering as buffers to make argument validation work\n        self.a, self.b, self.sigma = _a, _b, _sigma\n        super(SmoothedBoxPrior, self).__init__(batch_shape, event_shape, validate_args=validate_args)\n        # now need to delete to be able to register buffer\n        del self.a, self.b, self.sigma\n        self.register_buffer(""a"", _a)\n        self.register_buffer(""b"", _b)\n        self.register_buffer(""sigma"", _sigma)\n        self.tails = NormalPrior(torch.zeros_like(_a), _sigma, validate_args=validate_args)\n        self._transform = transform\n\n    @property\n    def _c(self):\n        return (self.a + self.b) / 2\n\n    @property\n    def _r(self):\n        return (self.b - self.a) / 2\n\n    @property\n    def _M(self):\n        # normalization factor to make this a probability distribution\n        return torch.log(1 + (self.b - self.a) / (math.sqrt(2 * math.pi) * self.sigma))\n\n    def log_prob(self, x):\n        return self._log_prob(self.transform(x))\n\n    def _log_prob(self, x):\n        # x = ""distances from box`""\n        X = ((x - self._c).abs_() - self._r).clamp(min=0)\n        return (self.tails.log_prob(X) - self._M).sum(-1)\n'"
gpytorch/priors/torch_priors.py,7,"b'#!/usr/bin/env python3\n\nimport torch\nfrom torch.distributions import Gamma, LogNormal, MultivariateNormal, Normal, Uniform\nfrom torch.nn import Module as TModule\n\nfrom .prior import Prior\nfrom .utils import _bufferize_attributes, _del_attributes\n\nMVN_LAZY_PROPERTIES = (""covariance_matrix"", ""scale_tril"", ""precision_matrix"")\n\n\nclass NormalPrior(Prior, Normal):\n    """"""\n    Normal (Gaussian) Prior\n\n    pdf(x) = (2 * pi * sigma^2)^-0.5 * exp(-(x - mu)^2 / (2 * sigma^2))\n\n    where mu is the mean and sigma^2 is the variance.\n    """"""\n\n    def __init__(self, loc, scale, validate_args=False, transform=None):\n        TModule.__init__(self)\n        Normal.__init__(self, loc=loc, scale=scale, validate_args=validate_args)\n        _bufferize_attributes(self, (""loc"", ""scale""))\n        self._transform = transform\n\n    def expand(self, batch_shape):\n        batch_shape = torch.Size(batch_shape)\n        return NormalPrior(self.loc.expand(batch_shape), self.scale.expand(batch_shape))\n\n\nclass LogNormalPrior(Prior, LogNormal):\n    """"""\n    Log Normal prior.\n    """"""\n\n    def __init__(self, loc, scale, validate_args=None, transform=None):\n        TModule.__init__(self)\n        LogNormal.__init__(self, loc=loc, scale=scale, validate_args=validate_args)\n        self._transform = transform\n\n    def expand(self, batch_shape):\n        batch_shape = torch.Size(batch_shape)\n        return LogNormalPrior(self.loc.expand(batch_shape), self.scale.expand(batch_shape))\n\n\nclass UniformPrior(Prior, Uniform):\n    """"""\n    Uniform prior.\n    """"""\n\n    def __init__(self, a, b, validate_args=None, transform=None):\n        TModule.__init__(self)\n        Uniform.__init__(self, a, b, validate_args=validate_args)\n        self._transform = transform\n\n    def expand(self, batch_shape):\n        batch_shape = torch.Size(batch_shape)\n        return UniformPrior(self.low.expand(batch_shape), self.high.expand(batch_shape))\n\n\nclass GammaPrior(Prior, Gamma):\n    """"""Gamma Prior parameterized by concentration and rate\n\n    pdf(x) = beta^alpha / Gamma(alpha) * x^(alpha - 1) * exp(-beta * x)\n\n    were alpha > 0 and beta > 0 are the concentration and rate parameters, respectively.\n    """"""\n\n    def __init__(self, concentration, rate, validate_args=False, transform=None):\n        TModule.__init__(self)\n        Gamma.__init__(self, concentration=concentration, rate=rate, validate_args=validate_args)\n        _bufferize_attributes(self, (""concentration"", ""rate""))\n        self._transform = transform\n\n    def expand(self, batch_shape):\n        batch_shape = torch.Size(batch_shape)\n        return GammaPrior(self.concentration.expand(batch_shape), self.rate.expand(batch_shape))\n\n    def __call__(self, *args, **kwargs):\n        return super(Gamma, self).__call__(*args, **kwargs)\n\n\nclass MultivariateNormalPrior(Prior, MultivariateNormal):\n    """"""Multivariate Normal prior\n\n    pdf(x) = det(2 * pi * Sigma)^-0.5 * exp(-0.5 * (x - mu)\' Sigma^-1 (x - mu))\n\n    where mu is the mean and Sigma > 0 is the covariance matrix.\n    """"""\n\n    def __init__(\n        self, loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=False, transform=None\n    ):\n        TModule.__init__(self)\n        MultivariateNormal.__init__(\n            self,\n            loc=loc,\n            covariance_matrix=covariance_matrix,\n            precision_matrix=precision_matrix,\n            scale_tril=scale_tril,\n            validate_args=validate_args,\n        )\n        _bufferize_attributes(self, (""loc"", ""_unbroadcasted_scale_tril""))\n        self._transform = transform\n\n    def cuda(self, device=None):\n        """"""Applies module-level cuda() call and resets all lazy properties""""""\n        module = self._apply(lambda t: t.cuda(device))\n        _del_attributes(module, MVN_LAZY_PROPERTIES)\n        return module\n\n    def cpu(self):\n        """"""Applies module-level cpu() call and resets all lazy properties""""""\n        module = self._apply(lambda t: t.cpu())\n        _del_attributes(module, MVN_LAZY_PROPERTIES)\n        return module\n\n    def expand(self, batch_shape):\n        batch_shape = torch.Size(batch_shape)\n        cov_shape = batch_shape + self.event_shape\n        new_loc = self.loc.expand(batch_shape)\n        new_scale_tril = self.scale_tril.expand(cov_shape)\n\n        return MultivariateNormalPrior(loc=new_loc, scale_tril=new_scale_tril)\n'"
gpytorch/priors/utils.py,0,"b'#!/usr/bin/env python3\n\n\ndef _bufferize_attributes(module, attributes):\n    attr_clones = {attr: getattr(module, attr).clone() for attr in attributes}\n    for attr, value in attr_clones.items():\n        delattr(module, attr)\n        module.register_buffer(attr, value)\n\n\ndef _del_attributes(module, attributes, raise_on_error=False):\n    for attr in attributes:\n        try:\n            delattr(module, attr)\n        except AttributeError as e:\n            if raise_on_error:\n                raise e\n    return module\n'"
gpytorch/priors/wishart_prior.py,21,"b'#!/usr/bin/env python3\n\nimport math\nfrom numbers import Number\n\nimport torch\nfrom torch.distributions import constraints\nfrom torch.nn import Module as TModule\n\nfrom .prior import Prior\n\n\nclass WishartPrior(Prior):\n    """"""Wishart prior over n x n positive definite matrices\n\n    pdf(Sigma) ~ |Sigma|^(nu - n - 1)/2 * exp(-0.5 * Trace(K^-1 Sigma))\n\n    where nu > n - 1 are the degrees of freedom and K > 0 is the p x p scale matrix\n\n    Reference: A. Shah, A. G. Wilson, and Z. Ghahramani. Student-t Processes as\n        Alternatives to Gaussian Processes. ArXiv e-prints, Feb. 2014.\n    """"""\n\n    arg_constraints = {""K_inv"": constraints.positive_definite, ""nu"": constraints.positive}\n    support = constraints.positive_definite\n    _validate_args = True\n\n    def __init__(self, nu, K, validate_args=False):\n        TModule.__init__(self)\n        if K.dim() < 2:\n            raise ValueError(""K must be at least 2-dimensional"")\n        n = K.shape[-1]\n        if K.shape[-2] != K.shape[-1]:\n            raise ValueError(""K must be square"")\n        if isinstance(nu, Number):\n            nu = torch.tensor(float(nu))\n        if torch.any(nu <= n):\n            raise ValueError(""Must have nu > n - 1"")\n        self.n = torch.tensor(n, dtype=torch.long, device=nu.device)\n        batch_shape = nu.shape\n        event_shape = torch.Size([n, n])\n        # normalization constant\n        logdetK = torch.logdet(K)\n        C = -(nu / 2) * (logdetK + n * math.log(2)) - torch.mvlgamma(nu / 2, n)\n        K_inv = torch.inverse(K)\n        # need to assign values before registering as buffers to make argument validation work\n        self.nu = nu\n        self.K_inv = K_inv\n        self.C = C\n        super(WishartPrior, self).__init__(batch_shape, event_shape, validate_args=validate_args)\n        # now need to delete to be able to register buffer\n        del self.nu, self.K_inv, self.C\n        self.register_buffer(""nu"", nu)\n        self.register_buffer(""K_inv"", K_inv)\n        self.register_buffer(""C"", C)\n\n    def log_prob(self, X):\n        # I\'m sure this could be done more elegantly\n        logdetp = torch.logdet(X)\n        Kinvp = torch.matmul(self.K_inv, X)\n        trKinvp = torch.diagonal(Kinvp, dim1=-2, dim2=-1).sum(-1)\n        return self.C + 0.5 * (self.nu - self.n - 1) * logdetp - trKinvp\n\n\nclass InverseWishartPrior(Prior):\n    """"""Inverse Wishart prior over n x n positive definite matrices\n\n    pdf(Sigma) ~ |Sigma|^-(nu + 2 * n)/2 * exp(-0.5 * Trace(K Sigma^-1))\n\n    where nu > 0 are the degrees of freedom and K > 0 is the p x p scale matrix\n\n    Reference: A. Shah, A. G. Wilson, and Z. Ghahramani. Student-t Processes as\n        Alternatives to Gaussian Processes. ArXiv e-prints, Feb. 2014.\n    """"""\n\n    arg_constraints = {""K"": constraints.positive_definite, ""nu"": constraints.positive}\n    support = constraints.positive_definite\n    _validate_args = True\n\n    def __init__(self, nu, K, validate_args=False):\n        TModule.__init__(self)\n        if K.dim() < 2:\n            raise ValueError(""K must be at least 2-dimensional"")\n        n = K.shape[-1]\n        if isinstance(nu, Number):\n            nu = torch.tensor(float(nu))\n        if torch.any(nu <= 0):\n            raise ValueError(""Must have nu > 0"")\n        self.n = torch.tensor(n, dtype=torch.long, device=nu.device)\n        batch_shape = nu.shape\n        event_shape = torch.Size([n, n])\n        # normalization constant\n        c = (nu + n - 1) / 2\n        logdetK = torch.logdet(K)\n        C = c * (logdetK - n * math.log(2)) - torch.mvlgamma(c, n)\n        # need to assign values before registering as buffers to make argument validation work\n        self.nu = nu\n        self.K = K\n        self.C = C\n        super(InverseWishartPrior, self).__init__(batch_shape, event_shape, validate_args=validate_args)\n        # now need to delete to be able to register buffer\n        del self.nu, self.K, self.C\n        self.register_buffer(""nu"", nu)\n        self.register_buffer(""K"", K)\n        self.register_buffer(""C"", C)\n\n    def log_prob(self, X):\n        logdetp = torch.logdet(X)\n        pinvK = torch.solve(self.K, X)[0]\n        trpinvK = torch.diagonal(pinvK, dim1=-2, dim2=-1).sum(-1)  # trace in batch mode\n        return self.C - 0.5 * ((self.nu + 2 * self.n) * logdetp + trpinvK)\n'"
gpytorch/test/__init__.py,0,b'#!/usr/bin/env python3\n'
gpytorch/test/base_kernel_test_case.py,24,"b'#!/usr/bin/env python3\n\nfrom abc import abstractmethod\n\nimport torch\n\n\nclass BaseKernelTestCase(object):\n    @abstractmethod\n    def create_kernel_no_ard(self, **kwargs):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def create_kernel_ard(self, num_dims, **kwargs):\n        raise NotImplementedError()\n\n    def create_data_no_batch(self):\n        return torch.randn(50, 10)\n\n    def create_data_single_batch(self):\n        return torch.randn(2, 50, 2)\n\n    def create_data_double_batch(self):\n        return torch.randn(3, 2, 50, 2)\n\n    def test_active_dims_list(self):\n        kernel = self.create_kernel_no_ard(active_dims=[0, 2, 4, 6])\n        x = self.create_data_no_batch()\n        covar_mat = kernel(x).evaluate_kernel().evaluate()\n        kernel_basic = self.create_kernel_no_ard()\n        covar_mat_actual = kernel_basic(x[:, [0, 2, 4, 6]]).evaluate_kernel().evaluate()\n\n        self.assertLess(torch.norm(covar_mat - covar_mat_actual) / covar_mat_actual.norm(), 1e-4)\n\n    def test_active_dims_range(self):\n        active_dims = list(range(3, 9))\n        kernel = self.create_kernel_no_ard(active_dims=active_dims)\n        x = self.create_data_no_batch()\n        covar_mat = kernel(x).evaluate_kernel().evaluate()\n        kernel_basic = self.create_kernel_no_ard()\n        covar_mat_actual = kernel_basic(x[:, active_dims]).evaluate_kernel().evaluate()\n\n        self.assertLess(torch.norm(covar_mat - covar_mat_actual) / covar_mat_actual.norm(), 1e-4)\n\n    def test_no_batch_kernel_single_batch_x_no_ard(self):\n        kernel = self.create_kernel_no_ard()\n        x = self.create_data_single_batch()\n        batch_covar_mat = kernel(x).evaluate_kernel().evaluate()\n\n        actual_mat_1 = kernel(x[0]).evaluate_kernel().evaluate()\n        actual_mat_2 = kernel(x[1]).evaluate_kernel().evaluate()\n        actual_covar_mat = torch.cat([actual_mat_1.unsqueeze(0), actual_mat_2.unsqueeze(0)])\n\n        self.assertLess(torch.norm(batch_covar_mat - actual_covar_mat) / actual_covar_mat.norm(), 1e-4)\n\n    def test_single_batch_kernel_single_batch_x_no_ard(self):\n        kernel = self.create_kernel_no_ard(batch_shape=torch.Size([]))\n        x = self.create_data_single_batch()\n        batch_covar_mat = kernel(x).evaluate_kernel().evaluate()\n\n        actual_mat_1 = kernel(x[0]).evaluate_kernel().evaluate()\n        actual_mat_2 = kernel(x[1]).evaluate_kernel().evaluate()\n        actual_covar_mat = torch.cat([actual_mat_1.unsqueeze(0), actual_mat_2.unsqueeze(0)])\n\n        self.assertLess(torch.norm(batch_covar_mat - actual_covar_mat) / actual_covar_mat.norm(), 1e-4)\n\n    def test_no_batch_kernel_double_batch_x_no_ard(self):\n        kernel = self.create_kernel_no_ard(batch_shape=torch.Size([]))\n        x = self.create_data_double_batch()\n        batch_covar_mat = kernel(x).evaluate_kernel().evaluate()\n\n        ij_actual_covars = []\n        for i in range(x.size(0)):\n            i_actual_covars = []\n            for j in range(x.size(1)):\n                i_actual_covars.append(kernel(x[i, j]).evaluate_kernel().evaluate())\n            ij_actual_covars.append(torch.cat([ac.unsqueeze(0) for ac in i_actual_covars]))\n\n        actual_covar_mat = torch.cat([ac.unsqueeze(0) for ac in ij_actual_covars])\n\n        self.assertLess(torch.norm(batch_covar_mat - actual_covar_mat) / actual_covar_mat.norm(), 1e-4)\n\n    def test_no_batch_kernel_double_batch_x_ard(self):\n        try:\n            kernel = self.create_kernel_ard(num_dims=2, batch_shape=torch.Size([]))\n        except NotImplementedError:\n            return\n\n        x = self.create_data_double_batch()\n        batch_covar_mat = kernel(x).evaluate_kernel().evaluate()\n\n        ij_actual_covars = []\n        for i in range(x.size(0)):\n            i_actual_covars = []\n            for j in range(x.size(1)):\n                i_actual_covars.append(kernel(x[i, j]).evaluate_kernel().evaluate())\n            ij_actual_covars.append(torch.cat([ac.unsqueeze(0) for ac in i_actual_covars]))\n\n        actual_covar_mat = torch.cat([ac.unsqueeze(0) for ac in ij_actual_covars])\n\n        self.assertLess(torch.norm(batch_covar_mat - actual_covar_mat) / actual_covar_mat.norm(), 2e-4)\n\n    def test_smoke_double_batch_kernel_double_batch_x_no_ard(self):\n        kernel = self.create_kernel_no_ard(batch_shape=torch.Size([3, 2]))\n        x = self.create_data_double_batch()\n        batch_covar_mat = kernel(x).evaluate_kernel().evaluate()\n        return batch_covar_mat\n\n    def test_smoke_double_batch_kernel_double_batch_x_ard(self):\n        try:\n            kernel = self.create_kernel_ard(num_dims=2, batch_shape=torch.Size([3, 2]))\n        except NotImplementedError:\n            return\n\n        x = self.create_data_double_batch()\n        batch_covar_mat = kernel(x).evaluate_kernel().evaluate()\n        return batch_covar_mat\n\n    def test_kernel_getitem_single_batch(self):\n        kernel = self.create_kernel_no_ard(batch_shape=torch.Size([2]))\n        x = self.create_data_single_batch()\n\n        res1 = kernel(x).evaluate()[0]  # Result of first kernel on first batch of data\n\n        new_kernel = kernel[0]\n        res2 = new_kernel(x[0]).evaluate()  # Should also be result of first kernel on first batch of data.\n\n        self.assertLess(torch.norm(res1 - res2) / res1.norm(), 1e-4)\n\n    def test_kernel_getitem_double_batch(self):\n        kernel = self.create_kernel_no_ard(batch_shape=torch.Size([3, 2]))\n        x = self.create_data_double_batch()\n\n        res1 = kernel(x).evaluate()[0, 1]  # Result of first kernel on first batch of data\n\n        new_kernel = kernel[0, 1]\n        res2 = new_kernel(x[0, 1]).evaluate()  # Should also be result of first kernel on first batch of data.\n\n        self.assertLess(torch.norm(res1 - res2) / res1.norm(), 1e-4)\n'"
gpytorch/test/base_likelihood_test_case.py,32,"b'#!/usr/bin/env python3\n\nfrom abc import abstractmethod\n\nimport torch\nfrom torch.distributions import Distribution\n\nimport gpytorch\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.likelihoods import Likelihood\n\nfrom .base_test_case import BaseTestCase\n\n\nclass BaseLikelihoodTestCase(BaseTestCase):\n    @abstractmethod\n    def create_likelihood(self, **kwargs):\n        raise NotImplementedError()\n\n    def _create_conditional_input(self, batch_shape=torch.Size()):\n        return torch.randn(*batch_shape, 5)\n\n    def _create_marginal_input(self, batch_shape=torch.Size()):\n        mat = torch.randn(*batch_shape, 5, 5)\n        eye = torch.diag_embed(torch.ones(*batch_shape, 5))\n        return MultivariateNormal(torch.randn(*batch_shape, 5), mat @ mat.transpose(-1, -2) + eye)\n\n    def _create_targets(self, batch_shape=torch.Size()):\n        return torch.randn(*batch_shape, 5)\n\n    def _test_conditional(self, batch_shape):\n        likelihood = self.create_likelihood()\n        likelihood.max_plate_nesting += len(batch_shape)\n        input = self._create_conditional_input(batch_shape)\n        output = likelihood(input)\n\n        self.assertTrue(isinstance(output, Distribution))\n        self.assertEqual(output.sample().shape, input.shape)\n\n    def _test_log_marginal(self, batch_shape):\n        likelihood = self.create_likelihood()\n        likelihood.max_plate_nesting += len(batch_shape)\n        input = self._create_marginal_input(batch_shape)\n        target = self._create_targets(batch_shape)\n        with gpytorch.settings.num_likelihood_samples(512):\n            output = likelihood.log_marginal(target, input)\n\n        self.assertTrue(torch.is_tensor(output))\n        self.assertEqual(output.shape, batch_shape + torch.Size([5]))\n        with gpytorch.settings.num_likelihood_samples(512):\n            default_log_prob = Likelihood.log_marginal(likelihood, target, input)\n        self.assertAllClose(output, default_log_prob, rtol=0.25)\n\n    def _test_log_prob(self, batch_shape):\n        likelihood = self.create_likelihood()\n        likelihood.max_plate_nesting += len(batch_shape)\n        input = self._create_marginal_input(batch_shape)\n        target = self._create_targets(batch_shape)\n        with gpytorch.settings.num_likelihood_samples(512):\n            output = likelihood.expected_log_prob(target, input)\n\n        self.assertTrue(torch.is_tensor(output))\n        self.assertEqual(output.shape, batch_shape + torch.Size([5]))\n        with gpytorch.settings.num_likelihood_samples(512):\n            default_log_prob = Likelihood.expected_log_prob(likelihood, target, input)\n        self.assertAllClose(output, default_log_prob, rtol=0.25)\n\n    def _test_marginal(self, batch_shape):\n        likelihood = self.create_likelihood()\n        likelihood.max_plate_nesting += len(batch_shape)\n        input = self._create_marginal_input(batch_shape)\n        output = likelihood(input)\n\n        self.assertTrue(isinstance(output, Distribution))\n        self.assertEqual(output.sample().shape[-len(input.sample().shape) :], input.sample().shape)\n\n        # Compare against default implementation\n        with gpytorch.settings.num_likelihood_samples(30000):\n            default = Likelihood.marginal(likelihood, input)\n        # print(output.mean, default.mean)\n        default_mean = default.mean\n        actual_mean = output.mean\n        if default_mean.dim() > actual_mean.dim():\n            default_mean = default_mean.mean(0)\n        self.assertAllClose(default_mean, actual_mean, rtol=0.25, atol=0.25)\n\n    def test_nonbatch(self):\n        self._test_conditional(batch_shape=torch.Size([]))\n        self._test_log_marginal(batch_shape=torch.Size([]))\n        self._test_log_prob(batch_shape=torch.Size([]))\n        self._test_marginal(batch_shape=torch.Size([]))\n\n    def test_batch(self):\n        self._test_conditional(batch_shape=torch.Size([3]))\n        self._test_log_marginal(batch_shape=torch.Size([3]))\n        self._test_log_prob(batch_shape=torch.Size([3]))\n        self._test_marginal(batch_shape=torch.Size([3]))\n\n    def test_multi_batch(self):\n        self._test_conditional(batch_shape=torch.Size([2, 3]))\n        self._test_log_marginal(batch_shape=torch.Size([2, 3]))\n        self._test_log_prob(batch_shape=torch.Size([2, 3]))\n        self._test_marginal(batch_shape=torch.Size([2, 3]))\n'"
gpytorch/test/base_mean_test_case.py,11,"b'#!/usr/bin/env python3\n\nfrom abc import abstractmethod\n\nimport torch\n\nfrom .base_test_case import BaseTestCase\n\n\nclass BaseMeanTestCase(BaseTestCase):\n    batch_shape = None\n\n    @abstractmethod\n    def create_mean(self, **kwargs):\n        raise NotImplementedError()\n\n    def test_forward_vec(self):\n        test_x = torch.randn(4)\n        mean = self.create_mean()\n        if self.__class__.batch_shape is None:\n            self.assertEqual(mean(test_x).shape, torch.Size([4]))\n        else:\n            self.assertEqual(mean(test_x).shape, torch.Size([*self.__class__.batch_shape, 4]))\n\n    def test_forward_mat(self):\n        test_x = torch.randn(4, 3)\n        mean = self.create_mean()\n        if self.__class__.batch_shape is None:\n            self.assertEqual(mean(test_x).shape, torch.Size([4]))\n        else:\n            self.assertEqual(mean(test_x).shape, torch.Size([*self.__class__.batch_shape, 4]))\n\n    def test_forward_mat_batch(self):\n        test_x = torch.randn(3, 4, 3)\n        mean = self.create_mean()\n        if self.__class__.batch_shape is None:\n            self.assertEqual(mean(test_x).shape, torch.Size([3, 4]))\n        else:\n            self.assertEqual(mean(test_x).shape, torch.Size([*self.__class__.batch_shape, 4]))\n\n    def test_forward_mat_multi_batch(self):\n        test_x = torch.randn(2, 3, 4, 3)\n        mean = self.create_mean()\n        self.assertEqual(mean(test_x).shape, torch.Size([2, 3, 4]))\n'"
gpytorch/test/base_test_case.py,15,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nfrom abc import ABC\n\nimport torch\n\n\nclass BaseTestCase(ABC):\n    def setUp(self):\n        if hasattr(self.__class__, ""seed""):\n            seed = self.__class__.seed\n            if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n                self.rng_state = torch.get_rng_state()\n                torch.manual_seed(seed)\n                if torch.cuda.is_available():\n                    torch.cuda.manual_seed_all(seed)\n                random.seed(seed)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def assertAllClose(self, tensor1, tensor2, rtol=1e-4, atol=1e-5, equal_nan=False):\n        if not tensor1.shape == tensor2.shape:\n            raise ValueError(f""tensor1 ({tensor1.shape}) and tensor2 ({tensor2.shape}) do not have the same shape."")\n\n        if torch.allclose(tensor1, tensor2, rtol=rtol, atol=atol, equal_nan=equal_nan):\n            return True\n\n        if not equal_nan:\n            if not torch.equal(tensor1, tensor1):\n                raise AssertionError(f""tensor1 ({tensor1.shape}) contains NaNs"")\n            if not torch.equal(tensor2, tensor2):\n                raise AssertionError(f""tensor2 ({tensor2.shape}) contains NaNs"")\n\n        rtol_diff = (torch.abs(tensor1 - tensor2) / torch.abs(tensor2)).view(-1)\n        rtol_diff = rtol_diff[torch.isfinite(rtol_diff)]\n        rtol_max = rtol_diff.max().item()\n\n        atol_diff = (torch.abs(tensor1 - tensor2) - torch.abs(tensor2).mul(rtol)).view(-1)\n        atol_diff = atol_diff[torch.isfinite(atol_diff)]\n        atol_max = atol_diff.max().item()\n\n        raise AssertionError(\n            f""tensor1 ({tensor1.shape}) and tensor2 ({tensor2.shape}) are not close enough. \\n""\n            f""max rtol: {rtol_max:0.8f}\\t\\tmax atol: {atol_max:0.8f}""\n        )\n\n    def assertEqual(self, item1, item2):\n        if torch.is_tensor(item1) and torch.is_tensor(item2):\n            if torch.equal(item1, item2):\n                return True\n            else:\n                raise AssertionError(f""{item1} does not equal {item2}."")\n        elif torch.is_tensor(item1) or torch.is_tensor(item2):\n            raise AssertionError(f""item1 ({type(item1)}) and item2 ({type(item2)}) are not the same type."")\n        elif item1 == item2:\n            return True\n        elif type(item1) != type(item2):\n            raise AssertionError(f""item1 ({type(item1)}) and item2 ({type(item2)}) are not the same type."")\n        else:\n            raise AssertionError(f""tensor1 ({item1}) does not equal tensor2 ({item2})."")\n'"
gpytorch/test/lazy_tensor_test_case.py,72,"b'#!/usr/bin/env python3\n\nimport math\nfrom abc import abstractmethod\nfrom itertools import combinations, product\nfrom unittest.mock import MagicMock, patch\n\nimport torch\n\nimport gpytorch\n\nfrom .base_test_case import BaseTestCase\n\n\ndef _ensure_symmetric_grad(grad):\n    """"""\n    A gradient-hook hack to ensure that symmetric matrix gradients are symmetric\n    """"""\n    res = torch.add(grad, grad.transpose(-1, -2)).mul(0.5)\n    return res\n\n\nclass RectangularLazyTensorTestCase(BaseTestCase):\n    @abstractmethod\n    def create_lazy_tensor(self):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def evaluate_lazy_tensor(self):\n        raise NotImplementedError()\n\n    def _test_matmul(self, rhs):\n        lazy_tensor = self.create_lazy_tensor().requires_grad_(True)\n        lazy_tensor_copy = lazy_tensor.clone().detach_().requires_grad_(True)\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor_copy)\n\n        res = lazy_tensor.matmul(rhs)\n        actual = evaluated.matmul(rhs)\n        self.assertAllClose(res, actual)\n\n        grad = torch.randn_like(res)\n        res.backward(gradient=grad)\n        actual.backward(gradient=grad)\n        for arg, arg_copy in zip(lazy_tensor.representation(), lazy_tensor_copy.representation()):\n            if arg_copy.grad is not None:\n                self.assertAllClose(arg.grad, arg_copy.grad, rtol=1e-3)\n\n    def test_matmul_vec(self):\n        lazy_tensor = self.create_lazy_tensor()\n        rhs = torch.randn(lazy_tensor.size(-1))\n\n        # We skip this test if we\'re dealing with batch LazyTensors\n        # They shouldn\'t multiply by a vec\n        if lazy_tensor.ndimension() > 2:\n            return\n        else:\n            return self._test_matmul(rhs)\n\n    def test_matmul_matrix(self):\n        lazy_tensor = self.create_lazy_tensor()\n        rhs = torch.randn(*lazy_tensor.batch_shape, lazy_tensor.size(-1), 4)\n        return self._test_matmul(rhs)\n\n    def test_matmul_matrix_broadcast(self):\n        lazy_tensor = self.create_lazy_tensor()\n\n        # Right hand size has one more batch dimension\n        batch_shape = torch.Size((3, *lazy_tensor.batch_shape))\n        rhs = torch.randn(*batch_shape, lazy_tensor.size(-1), 4)\n        self._test_matmul(rhs)\n\n        if lazy_tensor.ndimension() > 2:\n            # Right hand size has one fewer batch dimension\n            batch_shape = torch.Size(lazy_tensor.batch_shape[1:])\n            rhs = torch.randn(*batch_shape, lazy_tensor.size(-1), 4)\n            self._test_matmul(rhs)\n\n            # Right hand size has a singleton dimension\n            batch_shape = torch.Size((*lazy_tensor.batch_shape[:-1], 1))\n            rhs = torch.randn(*batch_shape, lazy_tensor.size(-1), 4)\n            self._test_matmul(rhs)\n\n    def test_constant_mul(self):\n        lazy_tensor = self.create_lazy_tensor()\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n        self.assertAllClose((lazy_tensor * 5).evaluate(), evaluated * 5)\n\n    def test_evaluate(self):\n        lazy_tensor = self.create_lazy_tensor()\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n        self.assertAllClose(lazy_tensor.evaluate(), evaluated)\n\n    def test_getitem(self):\n        lazy_tensor = self.create_lazy_tensor()\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n\n        # Non-batch case\n        if lazy_tensor.ndimension() == 2:\n            res = lazy_tensor[1]\n            actual = evaluated[1]\n            self.assertAllClose(res, actual)\n            res = lazy_tensor[0:2].evaluate()\n            actual = evaluated[0:2]\n            self.assertAllClose(res, actual)\n            res = lazy_tensor[:, 0:2].evaluate()\n            actual = evaluated[:, 0:2]\n            self.assertAllClose(res, actual)\n            res = lazy_tensor[0:2, :].evaluate()\n            actual = evaluated[0:2, :]\n            self.assertAllClose(res, actual)\n            res = lazy_tensor[..., 0:2].evaluate()\n            actual = evaluated[..., 0:2]\n            self.assertAllClose(res, actual)\n            res = lazy_tensor[0:2, ...].evaluate()\n            actual = evaluated[0:2, ...]\n            self.assertAllClose(res, actual)\n            res = lazy_tensor[..., 0:2, 2]\n            actual = evaluated[..., 0:2, 2]\n            self.assertAllClose(res, actual)\n            res = lazy_tensor[0:2, ..., 2]\n            actual = evaluated[0:2, ..., 2]\n            self.assertAllClose(res, actual)\n\n        # Batch case\n        else:\n            res = lazy_tensor[1].evaluate()\n            actual = evaluated[1]\n            self.assertAllClose(res, actual)\n            res = lazy_tensor[0:2].evaluate()\n            actual = evaluated[0:2]\n            self.assertAllClose(res, actual)\n            res = lazy_tensor[:, 0:2].evaluate()\n            actual = evaluated[:, 0:2]\n            self.assertAllClose(res, actual)\n\n            for batch_index in product([1, slice(0, 2, None)], repeat=(lazy_tensor.dim() - 2)):\n                res = lazy_tensor.__getitem__((*batch_index, slice(0, 1, None), slice(0, 2, None))).evaluate()\n                actual = evaluated.__getitem__((*batch_index, slice(0, 1, None), slice(0, 2, None)))\n                self.assertAllClose(res, actual)\n                res = lazy_tensor.__getitem__((*batch_index, 1, slice(0, 2, None)))\n                actual = evaluated.__getitem__((*batch_index, 1, slice(0, 2, None)))\n                self.assertAllClose(res, actual)\n                res = lazy_tensor.__getitem__((*batch_index, slice(1, None, None), 2))\n                actual = evaluated.__getitem__((*batch_index, slice(1, None, None), 2))\n                self.assertAllClose(res, actual)\n\n            # Ellipsis\n            res = lazy_tensor.__getitem__((Ellipsis, slice(1, None, None), 2))\n            actual = evaluated.__getitem__((Ellipsis, slice(1, None, None), 2))\n            self.assertAllClose(res, actual)\n            res = lazy_tensor.__getitem__((slice(1, None, None), Ellipsis, 2))\n            actual = evaluated.__getitem__((slice(1, None, None), Ellipsis, 2))\n            self.assertAllClose(res, actual)\n\n    def test_getitem_tensor_index(self):\n        lazy_tensor = self.create_lazy_tensor()\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n\n        # Non-batch case\n        if lazy_tensor.ndimension() == 2:\n            index = (torch.tensor([0, 0, 1, 2]), torch.tensor([0, 1, 0, 2]))\n            res, actual = lazy_tensor[index], evaluated[index]\n            self.assertAllClose(res, actual)\n            index = (torch.tensor([0, 0, 1, 2]), slice(None, None, None))\n            res, actual = gpytorch.delazify(lazy_tensor[index]), evaluated[index]\n            self.assertAllClose(res, actual)\n            index = (slice(None, None, None), torch.tensor([0, 0, 1, 2]))\n            res, actual = gpytorch.delazify(lazy_tensor[index]), evaluated[index]\n            self.assertAllClose(res, actual)\n            index = (torch.tensor([0, 0, 1, 2]), Ellipsis)\n            res, actual = gpytorch.delazify(lazy_tensor[index]), evaluated[index]\n            self.assertAllClose(res, actual)\n            index = (Ellipsis, torch.tensor([0, 0, 1, 2]))\n            res, actual = gpytorch.delazify(lazy_tensor[index]), evaluated[index]\n            self.assertAllClose(res, actual)\n            index = (Ellipsis, torch.tensor([0, 0, 1, 2]), torch.tensor([0, 1, 0, 2]))\n            res, actual = lazy_tensor[index], evaluated[index]\n            self.assertAllClose(res, actual)\n\n        # Batch case\n        else:\n            for batch_index in product(\n                [torch.tensor([0, 1, 1, 0]), slice(None, None, None)], repeat=(lazy_tensor.dim() - 2)\n            ):\n                index = (*batch_index, torch.tensor([0, 1, 0, 2]), torch.tensor([1, 2, 0, 1]))\n                res, actual = lazy_tensor[index], evaluated[index]\n                self.assertAllClose(res, actual)\n                index = (*batch_index, torch.tensor([0, 1, 0, 2]), slice(None, None, None))\n                res, actual = gpytorch.delazify(lazy_tensor[index]), evaluated[index]\n                self.assertAllClose(res, actual)\n                index = (*batch_index, slice(None, None, None), torch.tensor([0, 1, 2, 1]))\n                res, actual = gpytorch.delazify(lazy_tensor[index]), evaluated[index]\n                self.assertAllClose(res, actual)\n                index = (*batch_index, slice(None, None, None), slice(None, None, None))\n                res, actual = lazy_tensor[index].evaluate(), evaluated[index]\n                self.assertAllClose(res, actual)\n\n            # Ellipsis\n            res = lazy_tensor.__getitem__((Ellipsis, torch.tensor([0, 1, 0, 2]), torch.tensor([1, 2, 0, 1])))\n            actual = evaluated.__getitem__((Ellipsis, torch.tensor([0, 1, 0, 2]), torch.tensor([1, 2, 0, 1])))\n            self.assertAllClose(res, actual)\n            res = gpytorch.delazify(\n                lazy_tensor.__getitem__((torch.tensor([0, 1, 0, 1]), Ellipsis, torch.tensor([1, 2, 0, 1])))\n            )\n            actual = evaluated.__getitem__((torch.tensor([0, 1, 0, 1]), Ellipsis, torch.tensor([1, 2, 0, 1])))\n            self.assertAllClose(res, actual)\n\n    def test_permute(self):\n        lazy_tensor = self.create_lazy_tensor()\n        if lazy_tensor.dim() >= 4:\n            evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n            dims = torch.randperm(lazy_tensor.dim() - 2).tolist()\n            res = lazy_tensor.permute(*dims, -2, -1).evaluate()\n            actual = evaluated.permute(*dims, -2, -1)\n            self.assertAllClose(res, actual)\n\n    def test_quad_form_derivative(self):\n        lazy_tensor = self.create_lazy_tensor().requires_grad_(True)\n        lazy_tensor_clone = lazy_tensor.clone().detach_().requires_grad_(True)\n        left_vecs = torch.randn(*lazy_tensor.batch_shape, lazy_tensor.size(-2), 2)\n        right_vecs = torch.randn(*lazy_tensor.batch_shape, lazy_tensor.size(-1), 2)\n\n        deriv_custom = lazy_tensor._quad_form_derivative(left_vecs, right_vecs)\n        deriv_auto = gpytorch.lazy.LazyTensor._quad_form_derivative(lazy_tensor_clone, left_vecs, right_vecs)\n\n        for dc, da in zip(deriv_custom, deriv_auto):\n            self.assertAllClose(dc, da)\n\n    def test_sum(self):\n        lazy_tensor = self.create_lazy_tensor()\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n\n        self.assertAllClose(lazy_tensor.sum(-1), evaluated.sum(-1))\n        self.assertAllClose(lazy_tensor.sum(-2), evaluated.sum(-2))\n        if lazy_tensor.ndimension() > 2:\n            self.assertAllClose(lazy_tensor.sum(-3).evaluate(), evaluated.sum(-3))\n        if lazy_tensor.ndimension() > 3:\n            self.assertAllClose(lazy_tensor.sum(-4).evaluate(), evaluated.sum(-4))\n\n    def test_transpose_batch(self):\n        lazy_tensor = self.create_lazy_tensor()\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n\n        if lazy_tensor.dim() >= 4:\n            for i, j in combinations(range(lazy_tensor.dim() - 2), 2):\n                res = lazy_tensor.transpose(i, j).evaluate()\n                actual = evaluated.transpose(i, j)\n                self.assertAllClose(res, actual, rtol=1e-4, atol=1e-5)\n\n\nclass LazyTensorTestCase(RectangularLazyTensorTestCase):\n    should_test_sample = False\n    skip_slq_tests = False\n    should_call_cg = True\n    should_call_lanczos = True\n\n    def _test_inv_matmul(self, rhs, lhs=None, cholesky=False):\n        lazy_tensor = self.create_lazy_tensor().requires_grad_(True)\n        lazy_tensor_copy = lazy_tensor.clone().detach_().requires_grad_(True)\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor_copy)\n        evaluated.register_hook(_ensure_symmetric_grad)\n\n        # Create a test right hand side and left hand side\n        rhs.requires_grad_(True)\n        rhs_copy = rhs.clone().detach().requires_grad_(True)\n        if lhs is not None:\n            lhs.requires_grad_(True)\n            lhs_copy = lhs.clone().detach().requires_grad_(True)\n\n        _wrapped_cg = MagicMock(wraps=gpytorch.utils.linear_cg)\n        with patch(""gpytorch.utils.linear_cg"", new=_wrapped_cg) as linear_cg_mock:\n            with gpytorch.settings.max_cholesky_size(math.inf if cholesky else 0), gpytorch.settings.cg_tolerance(1e-4):\n                # Perform the inv_matmul\n                if lhs is not None:\n                    res = lazy_tensor.inv_matmul(rhs, lhs)\n                    actual = lhs_copy @ evaluated.inverse() @ rhs_copy\n                else:\n                    res = lazy_tensor.inv_matmul(rhs)\n                    actual = evaluated.inverse().matmul(rhs_copy)\n                self.assertAllClose(res, actual, rtol=0.02, atol=1e-5)\n\n                # Perform backward pass\n                grad = torch.randn_like(res)\n                res.backward(gradient=grad)\n                actual.backward(gradient=grad)\n                for arg, arg_copy in zip(lazy_tensor.representation(), lazy_tensor_copy.representation()):\n                    if arg_copy.grad is not None:\n                        self.assertAllClose(arg.grad, arg_copy.grad, rtol=0.03, atol=1e-5)\n                self.assertAllClose(rhs.grad, rhs_copy.grad, rtol=0.03, atol=1e-5)\n                if lhs is not None:\n                    self.assertAllClose(lhs.grad, lhs_copy.grad, rtol=0.03, atol=1e-5)\n\n            # Determine if we\'ve called CG or not\n            if not cholesky and self.__class__.should_call_cg:\n                self.assertTrue(linear_cg_mock.called)\n            else:\n                self.assertFalse(linear_cg_mock.called)\n\n    def _test_inv_quad_logdet(self, reduce_inv_quad=True, cholesky=False):\n        if not self.__class__.skip_slq_tests:\n            # Forward\n            lazy_tensor = self.create_lazy_tensor()\n            evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n            flattened_evaluated = evaluated.view(-1, *lazy_tensor.matrix_shape)\n\n            vecs = torch.randn(*lazy_tensor.batch_shape, lazy_tensor.size(-1), 3, requires_grad=True)\n            vecs_copy = vecs.clone().detach_().requires_grad_(True)\n\n            _wrapped_cg = MagicMock(wraps=gpytorch.utils.linear_cg)\n            with patch(""gpytorch.utils.linear_cg"", new=_wrapped_cg) as linear_cg_mock:\n                with gpytorch.settings.num_trace_samples(256), gpytorch.settings.max_cholesky_size(\n                    math.inf if cholesky else 0\n                ), gpytorch.settings.cg_tolerance(1e-5):\n\n                    res_inv_quad, res_logdet = lazy_tensor.inv_quad_logdet(\n                        inv_quad_rhs=vecs, logdet=True, reduce_inv_quad=reduce_inv_quad\n                    )\n\n            actual_inv_quad = evaluated.inverse().matmul(vecs_copy).mul(vecs_copy).sum(-2)\n            if reduce_inv_quad:\n                actual_inv_quad = actual_inv_quad.sum(-1)\n            actual_logdet = torch.cat(\n                [torch.logdet(flattened_evaluated[i]).unsqueeze(0) for i in range(lazy_tensor.batch_shape.numel())]\n            ).view(lazy_tensor.batch_shape)\n\n            self.assertAllClose(res_inv_quad, actual_inv_quad, rtol=0.01, atol=0.01)\n            self.assertAllClose(res_logdet, actual_logdet, rtol=0.2, atol=0.03)\n\n            if not cholesky and self.__class__.should_call_cg:\n                self.assertTrue(linear_cg_mock.called)\n            else:\n                self.assertFalse(linear_cg_mock.called)\n\n    def test_add_diag(self):\n        lazy_tensor = self.create_lazy_tensor()\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n\n        other_diag = torch.tensor(1.5)\n        res = lazy_tensor.add_diag(other_diag).evaluate()\n        actual = evaluated + torch.eye(evaluated.size(-1)).view(\n            *[1 for _ in range(lazy_tensor.dim() - 2)], evaluated.size(-1), evaluated.size(-1)\n        ).repeat(*lazy_tensor.batch_shape, 1, 1).mul(1.5)\n        self.assertAllClose(res, actual)\n\n        other_diag = torch.tensor([1.5])\n        res = lazy_tensor.add_diag(other_diag).evaluate()\n        actual = evaluated + torch.eye(evaluated.size(-1)).view(\n            *[1 for _ in range(lazy_tensor.dim() - 2)], evaluated.size(-1), evaluated.size(-1)\n        ).repeat(*lazy_tensor.batch_shape, 1, 1).mul(1.5)\n        self.assertAllClose(res, actual)\n\n        other_diag = torch.randn(lazy_tensor.size(-1)).pow(2)\n        res = lazy_tensor.add_diag(other_diag).evaluate()\n        actual = evaluated + other_diag.diag().repeat(*lazy_tensor.batch_shape, 1, 1)\n        self.assertAllClose(res, actual)\n\n        for sizes in product([1, None], repeat=(lazy_tensor.dim() - 2)):\n            batch_shape = [lazy_tensor.batch_shape[i] if size is None else size for i, size in enumerate(sizes)]\n            other_diag = torch.randn(*batch_shape, lazy_tensor.size(-1)).pow(2)\n            res = lazy_tensor.add_diag(other_diag).evaluate()\n            actual = evaluated.clone().detach()\n            for i in range(other_diag.size(-1)):\n                actual[..., i, i] = actual[..., i, i] + other_diag[..., i]\n            self.assertAllClose(res, actual, rtol=1e-2, atol=1e-5)\n\n    def test_diag(self):\n        lazy_tensor = self.create_lazy_tensor()\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n\n        res = lazy_tensor.diag()\n        actual = evaluated.diagonal(dim1=-2, dim2=-1)\n        actual = actual.view(*lazy_tensor.batch_shape, -1)\n        self.assertAllClose(res, actual, rtol=1e-2, atol=1e-5)\n\n    def test_inv_matmul_vector(self, cholesky=False):\n        lazy_tensor = self.create_lazy_tensor()\n        rhs = torch.randn(lazy_tensor.size(-1))\n\n        # We skip this test if we\'re dealing with batch LazyTensors\n        # They shouldn\'t multiply by a vec\n        if lazy_tensor.ndimension() > 2:\n            return\n        else:\n            return self._test_inv_matmul(rhs)\n\n    def test_inv_matmul_vector_with_left(self, cholesky=False):\n        lazy_tensor = self.create_lazy_tensor()\n        rhs = torch.randn(lazy_tensor.size(-1))\n        lhs = torch.randn(6, lazy_tensor.size(-1))\n\n        # We skip this test if we\'re dealing with batch LazyTensors\n        # They shouldn\'t multiply by a vec\n        if lazy_tensor.ndimension() > 2:\n            return\n        else:\n            return self._test_inv_matmul(rhs, lhs=lhs)\n\n    def test_inv_matmul_vector_with_left_cholesky(self):\n        lazy_tensor = self.create_lazy_tensor()\n        rhs = torch.randn(*lazy_tensor.batch_shape, lazy_tensor.size(-1), 5)\n        lhs = torch.randn(*lazy_tensor.batch_shape, 6, lazy_tensor.size(-1))\n        return self._test_inv_matmul(rhs, lhs=lhs, cholesky=True)\n\n    def test_inv_matmul_matrix(self, cholesky=False):\n        lazy_tensor = self.create_lazy_tensor()\n        rhs = torch.randn(*lazy_tensor.batch_shape, lazy_tensor.size(-1), 5)\n        return self._test_inv_matmul(rhs, cholesky=cholesky)\n\n    def test_inv_matmul_matrix_cholesky(self):\n        return self.test_inv_matmul_matrix(cholesky=True)\n\n    def test_inv_matmul_matrix_with_left(self):\n        lazy_tensor = self.create_lazy_tensor()\n        rhs = torch.randn(*lazy_tensor.batch_shape, lazy_tensor.size(-1), 5)\n        lhs = torch.randn(*lazy_tensor.batch_shape, 3, lazy_tensor.size(-1))\n        return self._test_inv_matmul(rhs, lhs=lhs)\n\n    def test_inv_matmul_matrix_broadcast(self):\n        lazy_tensor = self.create_lazy_tensor()\n\n        # Right hand size has one more batch dimension\n        batch_shape = torch.Size((3, *lazy_tensor.batch_shape))\n        rhs = torch.randn(*batch_shape, lazy_tensor.size(-1), 5)\n        self._test_inv_matmul(rhs)\n\n        if lazy_tensor.ndimension() > 2:\n            # Right hand size has one fewer batch dimension\n            batch_shape = torch.Size(lazy_tensor.batch_shape[1:])\n            rhs = torch.randn(*batch_shape, lazy_tensor.size(-1), 5)\n            self._test_inv_matmul(rhs)\n\n            # Right hand size has a singleton dimension\n            batch_shape = torch.Size((*lazy_tensor.batch_shape[:-1], 1))\n            rhs = torch.randn(*batch_shape, lazy_tensor.size(-1), 5)\n            self._test_inv_matmul(rhs)\n\n    def test_inv_quad_logdet(self):\n        return self._test_inv_quad_logdet(reduce_inv_quad=False, cholesky=False)\n\n    def test_inv_quad_logdet_no_reduce(self):\n        return self._test_inv_quad_logdet(reduce_inv_quad=True, cholesky=False)\n\n    def test_inv_quad_logdet_no_reduce_cholesky(self):\n        return self._test_inv_quad_logdet(reduce_inv_quad=True, cholesky=True)\n\n    def test_prod(self):\n        with gpytorch.settings.fast_computations(covar_root_decomposition=False):\n            lazy_tensor = self.create_lazy_tensor()\n            evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n\n            if lazy_tensor.ndimension() > 2:\n                self.assertAllClose(lazy_tensor.prod(-3).evaluate(), evaluated.prod(-3), atol=1e-2, rtol=1e-2)\n            if lazy_tensor.ndimension() > 3:\n                self.assertAllClose(lazy_tensor.prod(-4).evaluate(), evaluated.prod(-4), atol=1e-2, rtol=1e-2)\n\n    def test_root_decomposition(self, cholesky=False):\n        _wrapped_lanczos = MagicMock(wraps=gpytorch.utils.lanczos.lanczos_tridiag)\n        with patch(""gpytorch.utils.lanczos.lanczos_tridiag"", new=_wrapped_lanczos) as lanczos_mock:\n            lazy_tensor = self.create_lazy_tensor()\n            test_mat = torch.randn(*lazy_tensor.batch_shape, lazy_tensor.size(-1), 5)\n            with gpytorch.settings.max_cholesky_size(math.inf if cholesky else 0):\n                root_approx = lazy_tensor.root_decomposition()\n                res = root_approx.matmul(test_mat)\n                actual = lazy_tensor.matmul(test_mat)\n                self.assertAllClose(res, actual, rtol=0.05)\n\n            # Make sure that we\'re calling the correct function\n            if not cholesky and self.__class__.should_call_lanczos:\n                self.assertTrue(lanczos_mock.called)\n            else:\n                self.assertFalse(lanczos_mock.called)\n\n    def test_root_decomposition_cholesky(self):\n        return self.test_root_decomposition(cholesky=True)\n\n    def test_root_inv_decomposition(self):\n        lazy_tensor = self.create_lazy_tensor()\n        root_approx = lazy_tensor.root_inv_decomposition()\n\n        test_mat = torch.randn(*lazy_tensor.batch_shape, lazy_tensor.size(-1), 5)\n\n        res = root_approx.matmul(test_mat)\n        actual = lazy_tensor.inv_matmul(test_mat)\n        self.assertAllClose(res, actual, rtol=0.05, atol=0.02)\n\n    def test_sample(self):\n        if self.__class__.should_test_sample:\n            lazy_tensor = self.create_lazy_tensor()\n            evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n\n            samples = lazy_tensor.zero_mean_mvn_samples(50000)\n            sample_covar = samples.unsqueeze(-1).matmul(samples.unsqueeze(-2)).mean(0)\n            self.assertAllClose(sample_covar, evaluated, rtol=0.3, atol=0.3)\n'"
gpytorch/test/model_test_case.py,12,"b'#!/usr/bin/env python3\n\nfrom abc import abstractmethod\n\nimport torch\n\nimport gpytorch\n\n\nclass BaseModelTestCase(object):\n    @abstractmethod\n    def create_model(self, train_x, train_y, likelihood):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def create_test_data(self):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def create_likelihood_and_labels(self):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def create_batch_test_data(self, batch_shape=torch.Size([3])):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def create_batch_likelihood_and_labels(self, batch_shape=torch.Size([3])):\n        raise NotImplementedError()\n\n    def test_forward_train(self):\n        data = self.create_test_data()\n        likelihood, labels = self.create_likelihood_and_labels()\n        model = self.create_model(data, labels, likelihood)\n        model.train()\n        output = model(data)\n        self.assertTrue(output.lazy_covariance_matrix.dim() == 2)\n        self.assertTrue(output.lazy_covariance_matrix.size(-1) == data.size(-2))\n        self.assertTrue(output.lazy_covariance_matrix.size(-2) == data.size(-2))\n\n    def test_batch_forward_train(self):\n        batch_data = self.create_batch_test_data()\n        likelihood, labels = self.create_batch_likelihood_and_labels()\n        model = self.create_model(batch_data, labels, likelihood)\n        model.train()\n        output = model(batch_data)\n        self.assertTrue(output.lazy_covariance_matrix.dim() == 3)\n        self.assertTrue(output.lazy_covariance_matrix.size(-1) == batch_data.size(-2))\n        self.assertTrue(output.lazy_covariance_matrix.size(-2) == batch_data.size(-2))\n\n    def test_multi_batch_forward_train(self):\n        batch_data = self.create_batch_test_data(batch_shape=torch.Size([2, 3]))\n        likelihood, labels = self.create_batch_likelihood_and_labels(batch_shape=torch.Size([2, 3]))\n        model = self.create_model(batch_data, labels, likelihood)\n        model.train()\n        output = model(batch_data)\n        self.assertTrue(output.lazy_covariance_matrix.dim() == 4)\n        self.assertTrue(output.lazy_covariance_matrix.size(-1) == batch_data.size(-2))\n        self.assertTrue(output.lazy_covariance_matrix.size(-2) == batch_data.size(-2))\n\n    def test_forward_eval(self):\n        data = self.create_test_data()\n        likelihood, labels = self.create_likelihood_and_labels()\n        model = self.create_model(data, labels, likelihood)\n        model.eval()\n        output = model(data)\n        self.assertTrue(output.lazy_covariance_matrix.dim() == 2)\n        self.assertTrue(output.lazy_covariance_matrix.size(-1) == data.size(-2))\n        self.assertTrue(output.lazy_covariance_matrix.size(-2) == data.size(-2))\n\n    def test_batch_forward_eval(self):\n        batch_data = self.create_batch_test_data()\n        likelihood, labels = self.create_batch_likelihood_and_labels()\n        model = self.create_model(batch_data, labels, likelihood)\n        model.eval()\n        output = model(batch_data)\n        self.assertTrue(output.lazy_covariance_matrix.dim() == 3)\n        self.assertTrue(output.lazy_covariance_matrix.size(-1) == batch_data.size(-2))\n        self.assertTrue(output.lazy_covariance_matrix.size(-2) == batch_data.size(-2))\n\n    def test_multi_batch_forward_eval(self):\n        batch_data = self.create_batch_test_data(batch_shape=torch.Size([2, 3]))\n        likelihood, labels = self.create_batch_likelihood_and_labels(batch_shape=torch.Size([2, 3]))\n        model = self.create_model(batch_data, labels, likelihood)\n        model.eval()\n        output = model(batch_data)\n        self.assertTrue(output.lazy_covariance_matrix.dim() == 4)\n        self.assertTrue(output.lazy_covariance_matrix.size(-1) == batch_data.size(-2))\n        self.assertTrue(output.lazy_covariance_matrix.size(-2) == batch_data.size(-2))\n\n\nclass VariationalModelTestCase(BaseModelTestCase):\n    def test_backward_train(self):\n        data = self.create_test_data()\n        likelihood, labels = self.create_likelihood_and_labels()\n        model = self.create_model(data, labels, likelihood)\n        mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=labels.size(-1))\n        model.train()\n        likelihood.train()\n\n        # We\'ll just do one step of gradient descent to mix up the params a bit\n        optimizer = torch.optim.Adam([{""params"": model.parameters()}, {""params"": likelihood.parameters()}], lr=0.01)\n\n        output = model(data)\n        loss = -mll(output, labels)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        output = model(data)\n        loss = -mll(output, labels)\n        loss.backward()\n\n        for _, param in model.named_parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for _, param in likelihood.named_parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        optimizer.step()\n\n    def test_batch_backward_train(self, batch_shape=torch.Size([3])):\n        data = self.create_batch_test_data(batch_shape)\n        likelihood, labels = self.create_batch_likelihood_and_labels(batch_shape)\n        model = self.create_model(data, labels, likelihood)\n        mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=labels.size(-1))\n        model.train()\n        likelihood.train()\n\n        # We\'ll just do one step of gradient descent to mix up the params a bit\n        optimizer = torch.optim.Adam([{""params"": model.parameters()}, {""params"": likelihood.parameters()}], lr=0.01)\n\n        output = model(data)\n        loss = -mll(output, labels).sum()\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        output = model(data)\n        loss = -mll(output, labels).sum()\n        loss.backward()\n\n        for _, param in model.named_parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for _, param in likelihood.named_parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        optimizer.step()\n\n    def test_multi_batch_backward_train(self, batch_shape=torch.Size([2, 3])):\n        return self.test_batch_backward_train(batch_shape=batch_shape)\n'"
gpytorch/test/utils.py,4,"b'#!/usr/bin/env python3\n\nfrom contextlib import contextmanager\nfrom typing import Generator\n\nimport torch\n\n\ndef approx_equal(self, other, epsilon=1e-4):\n    """"""\n    Determines if two tensors are approximately equal\n    Args:\n        - self: tensor\n        - other: tensor\n    Returns:\n        - bool\n    """"""\n    if self.size() != other.size():\n        raise RuntimeError(\n            ""Size mismatch between self ({self}) and other ({other})"".format(self=self.size(), other=other.size())\n        )\n    return torch.max((self - other).abs()) <= epsilon\n\n\ndef get_cuda_max_memory_allocations() -> int:\n    """"""Get the `max_memory_allocated` for each cuda device""""""\n    return torch.tensor([torch.cuda.max_memory_allocated(i) for i in range(torch.cuda.device_count())])\n\n\n@contextmanager\ndef least_used_cuda_device() -> Generator:\n    """"""Contextmanager for automatically selecting the cuda device\n    with the least allocated memory""""""\n    mem_allocs = get_cuda_max_memory_allocations()\n    least_used_device = torch.argmin(mem_allocs).item()\n    with torch.cuda.device(least_used_device):\n        yield\n'"
gpytorch/test/variational_test_case.py,47,"b'#!/usr/bin/env python3\n\nimport warnings\nfrom abc import abstractproperty\nfrom unittest.mock import MagicMock, patch\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.utils.warnings import ExtraComputationWarning\n\nfrom .base_test_case import BaseTestCase\n\n\nclass VariationalTestCase(BaseTestCase):\n    def _make_model_and_likelihood(\n        self,\n        num_inducing=16,\n        batch_shape=torch.Size([]),\n        inducing_batch_shape=torch.Size([]),\n        strategy_cls=gpytorch.variational.VariationalStrategy,\n        distribution_cls=gpytorch.variational.CholeskyVariationalDistribution,\n        constant_mean=True,\n    ):\n        class _SVGPRegressionModel(gpytorch.models.ApproximateGP):\n            def __init__(self, inducing_points):\n                variational_distribution = distribution_cls(num_inducing, batch_shape=batch_shape)\n                variational_strategy = strategy_cls(\n                    self, inducing_points, variational_distribution, learn_inducing_locations=True\n                )\n                super().__init__(variational_strategy)\n                if constant_mean:\n                    self.mean_module = gpytorch.means.ConstantMean()\n                    self.mean_module.initialize(constant=1.0)\n                else:\n                    self.mean_module = gpytorch.means.ZeroMean()\n                self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n            def forward(self, x):\n                mean_x = self.mean_module(x)\n                covar_x = self.covar_module(x)\n                latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n                return latent_pred\n\n        inducing_points = torch.randn(num_inducing, 2).repeat(*inducing_batch_shape, 1, 1)\n        return _SVGPRegressionModel(inducing_points), self.likelihood_cls()\n\n    def _training_iter(\n        self, model, likelihood, batch_shape=torch.Size([]), mll_cls=gpytorch.mlls.VariationalELBO, cuda=False\n    ):\n        train_x = torch.randn(*batch_shape, 32, 2).clamp(-2.5, 2.5)\n        train_y = torch.linspace(-1, 1, self.event_shape[0])\n        train_y = train_y.view(self.event_shape[0], *([1] * (len(self.event_shape) - 1)))\n        train_y = train_y.expand(*self.event_shape)\n        mll = mll_cls(likelihood, model, num_data=train_x.size(-2))\n        if cuda:\n            train_x = train_x.cuda()\n            train_y = train_y.cuda()\n            model = model.cuda()\n            likelihood = likelihood.cuda()\n\n        # Single optimization iteration\n        model.train()\n        likelihood.train()\n        with warnings.catch_warnings(record=True) as ws:\n            output = model(train_x)\n            loss = -mll(output, train_y)\n            loss.sum().backward()\n\n        # Make sure we have gradients for all parameters\n        for _, param in model.named_parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for _, param in likelihood.named_parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n\n        # Make sure there were no warnings\n        self.assertFalse(any(issubclass(w.category, ExtraComputationWarning) for w in ws))\n        return output, loss\n\n    def _eval_iter(self, model, batch_shape=torch.Size([]), cuda=False):\n        test_x = torch.randn(*batch_shape, 32, 2).clamp(-2.5, 2.5)\n        if cuda:\n            test_x = test_x.cuda()\n            model = model.cuda()\n\n        # Single optimization iteration\n        model.eval()\n        with warnings.catch_warnings(record=True) as ws, torch.no_grad():\n            output = model(test_x)\n\n        # Make sure there were no warnings\n        self.assertFalse(any(issubclass(w.category, ExtraComputationWarning) for w in ws))\n        return output\n\n    @abstractproperty\n    def batch_shape(self):\n        raise NotImplementedError\n\n    @abstractproperty\n    def distribution_cls(self):\n        raise NotImplementedError\n\n    @property\n    def event_shape(self):\n        return torch.Size([32])\n\n    @property\n    def likelihood_cls(self):\n        return gpytorch.likelihoods.GaussianLikelihood\n\n    @abstractproperty\n    def mll_cls(self):\n        raise NotImplementedError\n\n    @abstractproperty\n    def strategy_cls(self):\n        raise NotImplementedError\n\n    @property\n    def cuda(self):\n        return False\n\n    def test_eval_iteration(\n        self,\n        data_batch_shape=None,\n        inducing_batch_shape=None,\n        model_batch_shape=None,\n        eval_data_batch_shape=None,\n        expected_batch_shape=None,\n    ):\n        # Batch shapes\n        model_batch_shape = model_batch_shape if model_batch_shape is not None else self.batch_shape\n        data_batch_shape = data_batch_shape if data_batch_shape is not None else self.batch_shape\n        inducing_batch_shape = inducing_batch_shape if inducing_batch_shape is not None else self.batch_shape\n        expected_batch_shape = expected_batch_shape if expected_batch_shape is not None else self.batch_shape\n        eval_data_batch_shape = eval_data_batch_shape if eval_data_batch_shape is not None else self.batch_shape\n\n        # Mocks\n        _wrapped_cholesky = MagicMock(wraps=torch.cholesky)\n        _wrapped_cg = MagicMock(wraps=gpytorch.utils.linear_cg)\n        _cholesky_mock = patch(""torch.cholesky"", new=_wrapped_cholesky)\n        _cg_mock = patch(""gpytorch.utils.linear_cg"", new=_wrapped_cg)\n\n        # Make model and likelihood\n        model, likelihood = self._make_model_and_likelihood(\n            batch_shape=model_batch_shape,\n            inducing_batch_shape=inducing_batch_shape,\n            distribution_cls=self.distribution_cls,\n            strategy_cls=self.strategy_cls,\n        )\n\n        # Do one forward pass\n        self._training_iter(model, likelihood, data_batch_shape, mll_cls=self.mll_cls, cuda=self.cuda)\n\n        # Now do evaluatioj\n        with _cholesky_mock as cholesky_mock, _cg_mock as cg_mock:\n            # Iter 1\n            _ = self._eval_iter(model, eval_data_batch_shape, cuda=self.cuda)\n            output = self._eval_iter(model, eval_data_batch_shape, cuda=self.cuda)\n            self.assertEqual(output.batch_shape, expected_batch_shape)\n            self.assertEqual(output.event_shape, self.event_shape)\n            return cg_mock, cholesky_mock\n\n    def test_eval_smaller_pred_batch(self):\n        return self.test_eval_iteration(\n            model_batch_shape=(torch.Size([3, 4]) + self.batch_shape),\n            inducing_batch_shape=(torch.Size([3, 1]) + self.batch_shape),\n            data_batch_shape=(torch.Size([3, 4]) + self.batch_shape),\n            eval_data_batch_shape=(torch.Size([4]) + self.batch_shape),\n            expected_batch_shape=(torch.Size([3, 4]) + self.batch_shape),\n        )\n\n    def test_eval_larger_pred_batch(self):\n        return self.test_eval_iteration(\n            model_batch_shape=(torch.Size([4]) + self.batch_shape),\n            inducing_batch_shape=(self.batch_shape),\n            data_batch_shape=(torch.Size([4]) + self.batch_shape),\n            eval_data_batch_shape=(torch.Size([3, 4]) + self.batch_shape),\n            expected_batch_shape=(torch.Size([3, 4]) + self.batch_shape),\n        )\n\n    def test_training_iteration(\n        self,\n        data_batch_shape=None,\n        inducing_batch_shape=None,\n        model_batch_shape=None,\n        expected_batch_shape=None,\n        constant_mean=True,\n    ):\n        # Batch shapes\n        model_batch_shape = model_batch_shape if model_batch_shape is not None else self.batch_shape\n        data_batch_shape = data_batch_shape if data_batch_shape is not None else self.batch_shape\n        inducing_batch_shape = inducing_batch_shape if inducing_batch_shape is not None else self.batch_shape\n        expected_batch_shape = expected_batch_shape if expected_batch_shape is not None else self.batch_shape\n\n        # Mocks\n        _wrapped_cholesky = MagicMock(wraps=torch.cholesky)\n        _wrapped_cg = MagicMock(wraps=gpytorch.utils.linear_cg)\n        _cholesky_mock = patch(""torch.cholesky"", new=_wrapped_cholesky)\n        _cg_mock = patch(""gpytorch.utils.linear_cg"", new=_wrapped_cg)\n\n        # Make model and likelihood\n        model, likelihood = self._make_model_and_likelihood(\n            batch_shape=model_batch_shape,\n            inducing_batch_shape=inducing_batch_shape,\n            distribution_cls=self.distribution_cls,\n            strategy_cls=self.strategy_cls,\n            constant_mean=constant_mean,\n        )\n\n        # Do forward pass\n        with _cholesky_mock as cholesky_mock, _cg_mock as cg_mock:\n            # Iter 1\n            self.assertEqual(model.variational_strategy.variational_params_initialized.item(), 0)\n            self._training_iter(model, likelihood, data_batch_shape, mll_cls=self.mll_cls, cuda=self.cuda)\n            self.assertEqual(model.variational_strategy.variational_params_initialized.item(), 1)\n            # Iter 2\n            output, loss = self._training_iter(\n                model, likelihood, data_batch_shape, mll_cls=self.mll_cls, cuda=self.cuda\n            )\n            self.assertEqual(output.batch_shape, expected_batch_shape)\n            self.assertEqual(output.event_shape, self.event_shape)\n            self.assertEqual(loss.shape, expected_batch_shape)\n            return cg_mock, cholesky_mock\n\n    def test_training_iteration_batch_inducing(self):\n        return self.test_training_iteration(\n            model_batch_shape=(torch.Size([3]) + self.batch_shape),\n            data_batch_shape=self.batch_shape,\n            inducing_batch_shape=(torch.Size([3]) + self.batch_shape),\n            expected_batch_shape=(torch.Size([3]) + self.batch_shape),\n        )\n\n    def test_training_iteration_batch_data(self):\n        return self.test_training_iteration(\n            model_batch_shape=self.batch_shape,\n            inducing_batch_shape=self.batch_shape,\n            data_batch_shape=(torch.Size([3]) + self.batch_shape),\n            expected_batch_shape=(torch.Size([3]) + self.batch_shape),\n        )\n\n    def test_training_iteration_batch_model(self):\n        return self.test_training_iteration(\n            model_batch_shape=(torch.Size([3]) + self.batch_shape),\n            inducing_batch_shape=self.batch_shape,\n            data_batch_shape=self.batch_shape,\n            expected_batch_shape=(torch.Size([3]) + self.batch_shape),\n        )\n\n    def test_training_all_batch_zero_mean(self):\n        return self.test_training_iteration(\n            model_batch_shape=(torch.Size([3, 4]) + self.batch_shape),\n            inducing_batch_shape=(torch.Size([3, 1]) + self.batch_shape),\n            data_batch_shape=(torch.Size([4]) + self.batch_shape),\n            expected_batch_shape=(torch.Size([3, 4]) + self.batch_shape),\n            constant_mean=False,\n        )\n'"
gpytorch/utils/__init__.py,0,"b'#!/usr/bin/env python3\n\nfrom . import broadcasting, cholesky, fft, grid, interpolation, lanczos, pivoted_cholesky, quadrature, sparse, warnings\nfrom .linear_cg import linear_cg\nfrom .memoize import cached\nfrom .stochastic_lq import StochasticLQ\n\n\ndef prod(items):\n    """"""\n    """"""\n    if len(items):\n        res = items[0]\n        for item in items[1:]:\n            res = res * item\n        return res\n    else:\n        return 1\n\n\n__all__ = [\n    ""broadcasting"",\n    ""cached"",\n    ""linear_cg"",\n    ""StochasticLQ"",\n    ""cholesky"",\n    ""fft"",\n    ""grid"",\n    ""interpolation"",\n    ""lanczos"",\n    ""pivoted_cholesky"",\n    ""prod"",\n    ""quadrature"",\n    ""sparse"",\n    ""warnings"",\n]\n'"
gpytorch/utils/broadcasting.py,3,"b'#!/usr/bin/env python3\n\nimport torch\n\n\ndef _mul_broadcast_shape(*shapes, error_msg=None):\n    """"""Compute dimension suggested by multiple tensor indices (supports broadcasting)""""""\n\n    # Pad each shape so they have the same number of dimensions\n    num_dims = max(len(shape) for shape in shapes)\n    shapes = tuple([1] * (num_dims - len(shape)) + list(shape) for shape in shapes)\n\n    # Make sure that each dimension agrees in size\n    final_size = []\n    for size_by_dim in zip(*shapes):\n        non_singleton_sizes = tuple(size for size in size_by_dim if size != 1)\n        if len(non_singleton_sizes):\n            if any(size != non_singleton_sizes[0] for size in non_singleton_sizes):\n                if error_msg is None:\n                    raise RuntimeError(""Shapes are not broadcastable for mul operation"")\n                else:\n                    raise RuntimeError(error_msg)\n            final_size.append(non_singleton_sizes[0])\n        # In this case - all dimensions are singleton sizes\n        else:\n            final_size.append(1)\n\n    return torch.Size(final_size)\n\n\ndef _matmul_broadcast_shape(shape_a, shape_b, error_msg=None):\n    """"""Compute dimension of matmul operation on shapes (supports broadcasting)""""""\n    m, n, p = shape_a[-2], shape_a[-1], shape_b[-1]\n\n    if len(shape_b) == 1:\n        if n != p:\n            if error_msg is None:\n                raise RuntimeError(f""Incompatible dimensions for matmul: {shape_a} and {shape_b}"")\n            else:\n                raise RuntimeError(error_msg)\n        return shape_a[:-1]\n\n    if n != shape_b[-2]:\n        if error_msg is None:\n            raise RuntimeError(f""Incompatible dimensions for matmul: {shape_a} and {shape_b}"")\n        else:\n            raise RuntimeError(error_msg)\n\n    tail_shape = torch.Size([m, p])\n\n    # Figure out batch shape\n    batch_shape_a = shape_a[:-2]\n    batch_shape_b = shape_b[:-2]\n    if batch_shape_a == batch_shape_b:\n        bc_shape = batch_shape_a\n    else:\n        bc_shape = _mul_broadcast_shape(batch_shape_a, batch_shape_b)\n    return bc_shape + tail_shape\n\n\ndef _pad_with_singletons(obj, num_singletons_before=0, num_singletons_after=0):\n    """"""\n    Pad obj with singleton dimensions on the left and right\n\n    Example:\n        >>> x = torch.randn(10, 5)\n        >>> _pad_width_singletons(x, 2, 3).shape\n        >>> # [1, 1, 10, 5, 1, 1, 1]\n    """"""\n    new_shape = [1] * num_singletons_before + list(obj.shape) + [1] * num_singletons_after\n    return obj.view(*new_shape)\n'"
gpytorch/utils/cholesky.py,6,"b'#!/usr/bin/env python3\n\nimport warnings\n\nimport torch\n\nfrom .errors import NanError\nfrom .warnings import NumericalWarning\n\n\ndef psd_safe_cholesky(A, upper=False, out=None, jitter=None):\n    """"""Compute the Cholesky decomposition of A. If A is only p.s.d, add a small jitter to the diagonal.\n    Args:\n        :attr:`A` (Tensor):\n            The tensor to compute the Cholesky decomposition of\n        :attr:`upper` (bool, optional):\n            See torch.cholesky\n        :attr:`out` (Tensor, optional):\n            See torch.cholesky\n        :attr:`jitter` (float, optional):\n            The jitter to add to the diagonal of A in case A is only p.s.d. If omitted, chosen\n            as 1e-6 (float) or 1e-8 (double)\n    """"""\n    try:\n        L = torch.cholesky(A, upper=upper, out=out)\n        return L\n    except RuntimeError as e:\n        isnan = torch.isnan(A)\n        if isnan.any():\n            raise NanError(\n                f""cholesky_cpu: {isnan.sum().item()} of {A.numel()} elements of the {A.shape} tensor are NaN.""\n            )\n\n        if jitter is None:\n            jitter = 1e-6 if A.dtype == torch.float32 else 1e-8\n        Aprime = A.clone()\n        jitter_prev = 0\n        for i in range(3):\n            jitter_new = jitter * (10 ** i)\n            Aprime.diagonal(dim1=-2, dim2=-1).add_(jitter_new - jitter_prev)\n            jitter_prev = jitter_new\n            try:\n                L = torch.cholesky(Aprime, upper=upper, out=out)\n                warnings.warn(f""A not p.d., added jitter of {jitter_new} to the diagonal"", NumericalWarning)\n                return L\n            except RuntimeError:\n                continue\n        raise e\n'"
gpytorch/utils/deprecation.py,2,"b'#!/usr/bin/env python3\n\nimport functools\nimport warnings\nfrom unittest.mock import MagicMock\n\nimport torch\n\n# TODO: Use bool instead of uint8 dtype once pytorch #21113 is in stable release\nif isinstance(torch, MagicMock):\n    bool_compat = torch.uint8\nelse:\n    bool_compat = (torch.ones(1) > 0).dtype\n\n\nclass DeprecationError(Exception):\n    pass\n\n\ndef _deprecated_function_for(old_function_name, function):\n    @functools.wraps(function)\n    def _deprecated_function(*args, **kwargs):\n        warnings.warn(\n            ""The `{}` function is deprecated. Use `{}` instead"".format(old_function_name, function.__name__),\n            DeprecationWarning,\n        )\n        return function(*args, **kwargs)\n\n    return _deprecated_function\n\n\ndef _deprecate_kwarg(kwargs, old_kw, new_kw, new_kw_value):\n    old_kwarg = kwargs.get(old_kw)\n    if old_kwarg is not None:\n        warnings.warn(""The `{}` argument is deprecated. Use `{}` instead."".format(old_kw, new_kw), DeprecationWarning)\n        if new_kw_value is not None:\n            raise ValueError(""Cannot set both `{}` and `{}`"".format(old_kw, new_kw))\n        return old_kwarg\n    return new_kw_value\n\n\ndef _deprecate_kwarg_with_transform(kwargs, old_kw, new_kw, new_kw_value, transform):\n    old_kwarg = kwargs.get(old_kw)\n    if old_kwarg is not None:\n        warnings.warn(""The `{}` argument is deprecated. Use `{}` instead."".format(old_kw, new_kw), DeprecationWarning)\n        return transform(old_kwarg)\n    return new_kw_value\n\n\ndef _deprecated_renamed_method(cls, old_method_name, new_method_name):\n    def _deprecated_method(self, *args, **kwargs):\n        warnings.warn(\n            ""The `{}` method is deprecated. Use `{}` instead"".format(old_method_name, new_method_name),\n            DeprecationWarning,\n        )\n        return getattr(self, new_method_name)(*args, **kwargs)\n\n    _deprecated_method.__name__ = old_method_name\n    setattr(cls, old_method_name, _deprecated_method)\n    return cls\n\n\ndef _deprecate_renamed_methods(cls, **renamed_methods):\n    for old_method_name, new_method_name in renamed_methods.items():\n        _deprecated_renamed_method(cls, old_method_name, new_method_name)\n    return cls\n'"
gpytorch/utils/errors.py,0,"b'#!/usr/bin/env python3\n\n\nclass NanError(RuntimeError):\n    pass\n\n\n__all__ = [""NanError""]\n'"
gpytorch/utils/fft.py,2,"b'#!/usr/bin/env python3\n\nimport torch\n\n\ndef fft1(input):\n    complex_input = torch.stack((input, torch.zeros_like(input)), dim=-1)\n    return complex_input.fft(1)\n\n\ndef ifft1(input):\n    complex_output = input.ifft(1)\n    real_ind = torch.tensor(0, dtype=torch.long, device=input.device)\n    return complex_output.index_select(-1, real_ind).squeeze(-1)\n'"
gpytorch/utils/getitem.py,16,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom .. import settings\nfrom .broadcasting import _mul_broadcast_shape, _pad_with_singletons\n\n# A slice that does nothing to a dimension\n_noop_index = slice(None, None, None)\n\n\ndef _compute_getitem_size(obj, indices):\n    """"""\n    Given an object and a tuple of indices, computes the final size of the\n    Indices is a tuple containing ints, slices, and tensors\n\n    .. note::\n        The length of indices must match the dimensionality of obj\n\n    Args:\n        obj - tensor or LazyTensor\n        indices - tuple of ints, slices, tensors\n\n    Returns:\n        :class:`torch.Size`\n    """"""\n    if obj.dim() != len(indices):\n        raise RuntimeError(\n            ""_compute_getitem_size assumes that obj (size: {}) and indices (len: {}) have the ""\n            ""same dimensionality."".format(obj.shape, len(indices))\n        )\n\n    final_shape = []\n    tensor_idx = None\n    tensor_idx_shape = None\n    slice_after_tensor_idx = False\n\n    for i, (size, idx) in enumerate(zip(obj.shape, indices)):\n        # Handle slice: that dimension gets downsized\n        if isinstance(idx, slice):\n            if idx == _noop_index:\n                final_shape.append(size)\n            else:\n                final_shape.append(len(range(*idx.indices(size))))\n\n            # If we don\'t have a continuous set of tensor indices, then the tensor indexed part\n            # goes to the front\n            if tensor_idx is not None:\n                slice_after_tensor_idx = True\n\n        # Handle int: we ""lose"" that dimension\n        elif isinstance(idx, int):\n            if settings.debug.on():\n                try:\n                    range(size)[idx]\n                except IndexError:\n                    raise IndexError(\n                        ""index element {} ({}) is invalid: out of range for obj of size ""\n                        ""{}."".format(i, idx, obj.shape)\n                    )\n\n        # Handle tensor index - this one is complicated\n        elif torch.is_tensor(idx):\n            if tensor_idx_shape is None:\n                tensor_idx_shape = idx.shape\n                tensor_idx = len(final_shape)\n\n            # If we don\'t have a continuous set of tensor indices, then the tensor indexed part\n            # goes to the front\n            else:\n                try:\n                    tensor_idx_shape = _mul_broadcast_shape(tensor_idx_shape, idx.shape)\n                except RuntimeError:\n                    raise IndexError(\n                        ""Incompatible tensor indices in index - got shapes of {} ."".format(\n                            [idx.shape for idx in indices if torch.is_tensor(idx)]\n                        )\n                    )\n\n                if slice_after_tensor_idx:\n                    tensor_idx = 0\n\n    # If we don\'t have a continuous set of tensor indices, then the tensor indexed part\n    # goes to the front\n    if tensor_idx is not None:\n        final_shape = final_shape[:tensor_idx] + list(tensor_idx_shape) + final_shape[tensor_idx:]\n\n    return torch.Size(final_shape)\n\n\ndef _convert_indices_to_tensors(obj, indices):\n    """"""\n    Given an index made up of tensors/slices/ints, returns a tensor-only index that has the\n    same outcome as the original index (when applied to the obj)\n\n    .. note::\n        The length of indices must match the dimensionality of obj\n\n    Args:\n        obj - tensor or LazyTensor\n        indices - tuple of slices, tensors, ints\n\n    Returns:\n        tuple of tensor indices (shapes of tensors will involve broadcasting)\n\n    Example:\n        >>> x = torch.randn(3, 6, 4)\n        >>> _convert_indices_to_tensors(x, (torch.tensor([0, 1]), 2, slice(None, None, None)))\n        >>> # (torch.tensor([[[0]], [[1]]]), torch.tensor([[[2]]]), torch.tensor([[[0, 1, 2, 3]]]))\n    """"""\n    slice_indices = tuple(index for index in indices if isinstance(index, slice))\n    tensor_indices = tuple(index for index in indices if torch.is_tensor(index))\n    tensor_index_shape = _mul_broadcast_shape(*[tensor_index.shape for tensor_index in tensor_indices])\n\n    # How many dimensions will the new tensor index have?\n    num_final_dims = len(slice_indices) + len(tensor_index_shape)\n    # Determine if the tensor index is being moved to the front\n    tensor_index_moved_to_start = _is_tensor_index_moved_to_start(indices)\n\n    # These are counters of the number of singleton dimensions that we need to append to\n    # the left and right of the indices that we\'re converting to tensor indices\n    num_singletons_before = len(tensor_index_shape) if tensor_index_moved_to_start else 0\n    num_singletons_after = (num_final_dims - len(tensor_index_shape)) if tensor_index_moved_to_start else num_final_dims\n    # These are counters of the number of singleton dimensions that we need to append to\n    # the left and right of the indices that are currently tensor indices\n    num_singletons_before_tensor = 0 if tensor_index_moved_to_start else None\n    num_singletons_after_tensor = (num_final_dims - len(tensor_index_shape)) if tensor_index_moved_to_start else None\n\n    # Compute the size suggested by the tensor indices\n    new_indices = []\n    for dim, index in enumerate(indices):\n        # slice - the tensor index will represent the slice\n        if isinstance(index, slice):\n            num_singletons_after -= 1\n            new_index = torch.arange(0, obj.size(dim), device=obj.device)[index]\n            new_index = _pad_with_singletons(new_index, num_singletons_before, num_singletons_after)\n            num_singletons_before += 1\n\n        # int - the tensor index will have only one element\n        elif isinstance(index, int):\n            new_index = torch.tensor(index, dtype=torch.long, device=obj.device)\n            new_index = _pad_with_singletons(new_index, num_singletons_before, num_singletons_after)\n\n        elif torch.is_tensor(index):\n            # If this is the first tensor index we\'ve seen, and we aren\'t moving all tensor indices to the start\n            # Then let\'s mark how much padding we need for subsequent tensor indices\n            if num_singletons_before_tensor is None:\n                num_singletons_after -= len(tensor_index_shape)\n                num_singletons_before_tensor = num_singletons_before\n                num_singletons_after_tensor = num_singletons_after\n                num_singletons_before += len(tensor_index_shape)\n            new_index = _pad_with_singletons(index, num_singletons_before_tensor, num_singletons_after_tensor)\n\n        new_indices.append(new_index)\n\n    return tuple(new_indices)\n\n\ndef _equal_indices(a, b):\n    """"""\n    Helper which checks whether two index components (int, slice, tensor) are equal\n    """"""\n    if torch.is_tensor(a) and torch.is_tensor(b):\n        return torch.equal(a, b)\n    elif not torch.is_tensor(a) and not torch.is_tensor(b):\n        return a == b\n    else:\n        return False\n\n\ndef _is_noop_index(index):\n    """"""\n    Determine if a given index is a noop (e.g. "":"")\n    """"""\n    return isinstance(index, slice) and index == _noop_index\n\n\ndef _is_tensor_index_moved_to_start(indices):\n    """"""\n    Given an index, determine if the indexed part of the getitem is moved to the zero\'th dimension\n    """"""\n    has_tensor_index = False\n    continuous_tensor_index = True\n\n    if torch.is_tensor(indices[0]):\n        return True\n\n    for index in indices[1:]:\n        if torch.is_tensor(index):\n            if not has_tensor_index:\n                has_tensor_index = True\n            elif not continuous_tensor_index:\n                return True\n\n        elif isinstance(index, slice):\n            if has_tensor_index:\n                continuous_tensor_index = False\n\n    return False\n'"
gpytorch/utils/grid.py,10,"b'#!/usr/bin/env python3\n\nimport math\nfrom typing import List, Tuple\n\nimport torch\n\n\ndef scale_to_bounds(x, lower_bound, upper_bound):\n    """"""\n    Scale the input data so that it lies in between the lower and upper bounds.\n\n    Args:\n        :attr:`x` (Tensor `n` or `b x n`):\n            the input\n        :attr:`lower_bound` (float)\n        :attr:`upper_bound` (float)\n\n    Returns:\n        :obj:`torch.Tensor`\n    """"""\n    # Scale features so they fit inside grid bounds\n    min_val = x.min()\n    max_val = x.max()\n    diff = max_val - min_val\n    x = (x - min_val) * (0.95 * (upper_bound - lower_bound) / diff) + 0.95 * lower_bound\n    return x\n\n\ndef choose_grid_size(train_inputs, ratio=1.0, kronecker_structure=True):\n    """"""\n    Given some training inputs, determine a good grid size for KISS-GP.\n\n    Args:\n        :attr:`train_inputs` (Tensor `n` or `n x d` or `b x n x d`):\n            training data\n        :attr:`ratio` (float, optional):\n            Ratio - number of grid points to the amount of data (default: 1.)\n        :attr:`kronecker_structure` (bool, default=True):\n            Whether or not the model will use Kronecker structure in the grid\n            (set to True unless there is an additive or product decomposition in the prior)\n\n    Returns:\n        :obj:`int`\n    """"""\n    # Scale features so they fit inside grid bounds\n    num_data = train_inputs.numel() if train_inputs.dim() == 1 else train_inputs.size(-2)\n    num_dim = 1 if train_inputs.dim() == 1 else train_inputs.size(-1)\n    if kronecker_structure:\n        return int(ratio * math.pow(num_data, 1.0 / num_dim))\n    else:\n        return ratio * num_data\n\n\ndef convert_legacy_grid(grid: torch.Tensor) -> List[torch.Tensor]:\n    return [grid[:, i] for i in range(grid.size(-1))]\n\n\ndef create_data_from_grid(grid: List[torch.Tensor]) -> torch.Tensor:\n    """"""\n    Args:\n        :attr:`grid` (List[Tensor])\n            Each Tensor is a 1D set of increments for the grid in that dimension\n    Returns:\n        `grid_data` (Tensor)\n            Returns the set of points on the grid going by column-major order\n            (due to legacy reasons).\n    """"""\n    if torch.is_tensor(grid):\n        grid = convert_legacy_grid(grid)\n    ndims = len(grid)\n    assert all(axis.dim() == 1 for axis in grid)\n    projections = torch.meshgrid(*grid)\n    grid_tensor = torch.stack(projections, axis=-1)\n    # Note that if we did\n    #     grid_data = grid_tensor.reshape(-1, ndims)\n    # instead, we would be iterating through the points of our grid from the\n    # last data dimension to the first data dimension. However, due to legacy\n    # reasons, we need to iterate from the first data dimension to the last data\n    # dimension when creating grid_data\n    grid_data = grid_tensor.permute(*(reversed(range(ndims + 1)))).reshape(ndims, -1).transpose(0, 1)\n    return grid_data\n\n\ndef create_grid(\n    grid_sizes: List[int], grid_bounds: List[Tuple[float, float]], extend: bool = True, device=""cpu"", dtype=torch.float,\n) -> List[torch.Tensor]:\n    """"""\n    Creates a grid represented by a list of 1D Tensors representing the\n    projections of the grid into each dimension\n\n    If `extend`, we extend the grid by two points past the specified boundary\n    which can be important for getting good grid interpolations\n    """"""\n    grid = []\n    for i in range(len(grid_bounds)):\n        grid_diff = float(grid_bounds[i][1] - grid_bounds[i][0]) / (grid_sizes[i] - 2)\n        if extend:\n            proj = torch.linspace(\n                grid_bounds[i][0] - grid_diff, grid_bounds[i][1] + grid_diff, grid_sizes[i], device=device, dtype=dtype,\n            )\n        else:\n            proj = torch.linspace(grid_bounds[i][0], grid_bounds[i][1], grid_sizes[i], device=device, dtype=dtype,)\n        grid.append(proj)\n    return grid\n'"
gpytorch/utils/interpolation.py,23,"b'#!/usr/bin/env python3\n\nfrom functools import reduce\nfrom operator import mul\nfrom typing import List\n\nimport torch\n\nfrom .broadcasting import _matmul_broadcast_shape\nfrom .grid import convert_legacy_grid\n\n\nclass Interpolation(object):\n    def _cubic_interpolation_kernel(self, scaled_grid_dist):\n        """"""\n        Computes the interpolation kernel u() for points X given the scaled\n        grid distances:\n                                    (X-x_{t})/s\n        where s is the distance between neighboring grid points. Note that,\n        in this context, the word ""kernel"" is not used to mean a covariance\n        function as in the rest of the package. For more details, see the\n        original paper Keys et al., 1989, equation (4).\n\n        scaled_grid_dist should be an n-by-g matrix of distances, where the\n        (ij)th element is the distance between the ith data point in X and the\n        jth element in the grid.\n\n        Note that, although this method ultimately expects a scaled distance matrix,\n        it is only intended to be used on single dimensional data.\n        """"""\n        U = scaled_grid_dist.abs()\n        res = torch.zeros(U.size(), dtype=U.dtype, device=U.device)\n\n        U_lt_1 = 1 - U.floor().clamp(0, 1)  # U, if U < 1, 0 otherwise\n        res = res + (((1.5 * U - 2.5).mul(U)).mul(U) + 1) * U_lt_1\n\n        # u(s) = -0.5|s|^3 + 2.5|s|^2 - 4|s| + 2 when 1 < |s| < 2\n        U_ge_1_le_2 = 1 - U_lt_1  # U, if U <= 1 <= 2, 0 otherwise\n        res = res + (((-0.5 * U + 2.5).mul(U) - 4).mul(U) + 2) * U_ge_1_le_2\n        return res\n\n    def interpolate(self, x_grid: List[torch.Tensor], x_target: torch.Tensor, interp_points=range(-2, 2), eps=1e-10):\n        if torch.is_tensor(x_grid):\n            x_grid = convert_legacy_grid(x_grid)\n        num_target_points = x_target.size(0)\n        num_dim = x_target.size(-1)\n        assert num_dim == len(x_grid)\n\n        grid_sizes = [len(x_grid[i]) for i in range(num_dim)]\n        # Do some boundary checking, # min/max along each dimension\n        x_target_max = x_target.max(0)[0]\n        x_target_min = x_target.min(0)[0]\n        grid_mins = torch.stack([x_grid[i].min() for i in range(num_dim)], dim=0).to(x_target_min)\n        grid_maxs = torch.stack([x_grid[i].max() for i in range(num_dim)], dim=0).to(x_target_max)\n\n        lt_min_mask = (x_target_min - grid_mins).lt(-1e-7)\n        gt_max_mask = (x_target_max - grid_maxs).gt(1e-7)\n        if lt_min_mask.sum().item():\n            first_out_of_range = lt_min_mask.nonzero().squeeze(1)[0].item()\n            raise RuntimeError(\n                (\n                    ""Received data that was out of bounds for the specified grid. ""\n                    ""Grid bounds were ({0:.3f}, {0:.3f}), but min = {0:.3f}, ""\n                    ""max = {0:.3f}""\n                ).format(\n                    grid_mins[first_out_of_range].item(),\n                    grid_maxs[first_out_of_range].item(),\n                    x_target_min[first_out_of_range].item(),\n                    x_target_max[first_out_of_range].item(),\n                )\n            )\n        if gt_max_mask.sum().item():\n            first_out_of_range = gt_max_mask.nonzero().squeeze(1)[0].item()\n            raise RuntimeError(\n                (\n                    ""Received data that was out of bounds for the specified grid. ""\n                    ""Grid bounds were ({0:.3f}, {0:.3f}), but min = {0:.3f}, ""\n                    ""max = {0:.3f}""\n                ).format(\n                    grid_mins[first_out_of_range].item(),\n                    grid_maxs[first_out_of_range].item(),\n                    x_target_min[first_out_of_range].item(),\n                    x_target_max[first_out_of_range].item(),\n                )\n            )\n\n        # Now do interpolation\n        interp_points = torch.tensor(interp_points, dtype=x_grid[0].dtype, device=x_grid[0].device)\n        interp_points_flip = interp_points.flip(0)  # [1, 0, -1, -2]\n\n        num_coefficients = len(interp_points)\n\n        interp_values = torch.ones(\n            num_target_points, num_coefficients ** num_dim, dtype=x_grid[0].dtype, device=x_grid[0].device\n        )\n        interp_indices = torch.zeros(\n            num_target_points, num_coefficients ** num_dim, dtype=torch.long, device=x_grid[0].device\n        )\n\n        for i in range(num_dim):\n            num_grid_points = x_grid[i].size(0)\n            grid_delta = (x_grid[i][1] - x_grid[i][0]).clamp_min_(eps)\n            # left-bounding grid point in index space\n            lower_grid_pt_idxs = torch.floor((x_target[:, i] - x_grid[i][0]) / grid_delta)\n            # distance from that left-bounding grid point, again in index space\n            lower_pt_rel_dists = (x_target[:, i] - x_grid[i][0]) / grid_delta - lower_grid_pt_idxs\n            lower_grid_pt_idxs = lower_grid_pt_idxs - interp_points.max()  # ends up being the left-most (relevant) pt\n            lower_grid_pt_idxs.detach_()\n\n            if len(lower_grid_pt_idxs.shape) == 0:\n                lower_grid_pt_idxs = lower_grid_pt_idxs.unsqueeze(0)\n\n            # get the interp. coeff. based on distances to interpolating points\n            scaled_dist = lower_pt_rel_dists.unsqueeze(-1) + interp_points_flip.unsqueeze(-2)\n            dim_interp_values = self._cubic_interpolation_kernel(scaled_dist)\n\n            # Find points who\'s closest lower grid point is the first grid point\n            # This corresponds to a boundary condition that we must fix manually.\n            left_boundary_pts = (lower_grid_pt_idxs < 0).nonzero()\n            num_left = len(left_boundary_pts)\n\n            if num_left > 0:\n                left_boundary_pts.squeeze_(1)\n                x_grid_first = x_grid[i][:num_coefficients].unsqueeze(1).t().expand(num_left, num_coefficients)\n\n                grid_targets = x_target.select(1, i)[left_boundary_pts].unsqueeze(1).expand(num_left, num_coefficients)\n                dists = torch.abs(x_grid_first - grid_targets)\n                closest_from_first = torch.min(dists, 1)[1]\n\n                for j in range(num_left):\n                    dim_interp_values[left_boundary_pts[j], :] = 0\n                    dim_interp_values[left_boundary_pts[j], closest_from_first[j]] = 1\n                    lower_grid_pt_idxs[left_boundary_pts[j]] = 0\n\n            right_boundary_pts = (lower_grid_pt_idxs > num_grid_points - num_coefficients).nonzero()\n            num_right = len(right_boundary_pts)\n\n            if num_right > 0:\n                right_boundary_pts.squeeze_(1)\n                x_grid_last = x_grid[i][-num_coefficients:].unsqueeze(1).t().expand(num_right, num_coefficients)\n\n                grid_targets = x_target.select(1, i)[right_boundary_pts].unsqueeze(1)\n                grid_targets = grid_targets.expand(num_right, num_coefficients)\n                dists = torch.abs(x_grid_last - grid_targets)\n                closest_from_last = torch.min(dists, 1)[1]\n\n                for j in range(num_right):\n                    dim_interp_values[right_boundary_pts[j], :] = 0\n                    dim_interp_values[right_boundary_pts[j], closest_from_last[j]] = 1\n                    lower_grid_pt_idxs[right_boundary_pts[j]] = num_grid_points - num_coefficients\n\n            offset = (interp_points - interp_points.min()).long().unsqueeze(-2)\n            dim_interp_indices = lower_grid_pt_idxs.long().unsqueeze(-1) + offset  # indices of corresponding ind. pts.\n\n            n_inner_repeat = num_coefficients ** i\n            n_outer_repeat = num_coefficients ** (num_dim - i - 1)\n            # index_coeff = num_grid_points ** (num_dim - i - 1)  # TODO: double check\n            index_coeff = reduce(mul, grid_sizes[i + 1 :], 1)  # Think this is right...\n            dim_interp_indices = dim_interp_indices.unsqueeze(-1).repeat(1, n_inner_repeat, n_outer_repeat)\n            dim_interp_values = dim_interp_values.unsqueeze(-1).repeat(1, n_inner_repeat, n_outer_repeat)\n            # compute the lexicographical position of the indices in the d-dimensional grid points\n            interp_indices = interp_indices.add(dim_interp_indices.view(num_target_points, -1).mul(index_coeff))\n            interp_values = interp_values.mul(dim_interp_values.view(num_target_points, -1))\n\n        return interp_indices, interp_values\n\n\ndef left_interp(interp_indices, interp_values, rhs):\n    """"""\n    """"""\n    is_vector = rhs.ndimension() == 1\n\n    if is_vector:\n        res = rhs.index_select(0, interp_indices.view(-1)).view(*interp_values.size())\n        res = res.mul(interp_values)\n        res = res.sum(-1)\n        return res\n\n    else:\n        num_rows, num_interp = interp_indices.shape[-2:]\n        num_data, num_columns = rhs.shape[-2:]\n        interp_shape = torch.Size((*interp_indices.shape[:-1], num_data))\n        output_shape = _matmul_broadcast_shape(interp_shape, rhs.shape)\n        batch_shape = output_shape[:-2]\n\n        interp_indices_expanded = interp_indices.unsqueeze(-1).expand(*batch_shape, num_rows, num_interp, num_columns)\n        interp_values_expanded = interp_values.unsqueeze(-1).expand(*batch_shape, num_rows, num_interp, num_columns)\n        rhs_expanded = rhs.unsqueeze(-2).expand(*batch_shape, num_data, num_interp, num_columns)\n        res = rhs_expanded.gather(-3, interp_indices_expanded).mul(interp_values_expanded)\n        return res.sum(-2)\n\n\ndef left_t_interp(interp_indices, interp_values, rhs, output_dim):\n    """"""\n    """"""\n    from .. import dsmm\n\n    is_vector = rhs.ndimension() == 1\n    if is_vector:\n        rhs = rhs.unsqueeze(-1)\n\n    # Multiply the rhs by the interp_values\n    # This multiplication here will give us the ability to perform backprop\n    values = rhs.unsqueeze(-2) * interp_values.unsqueeze(-1)\n\n    # Define a bunch of sizes\n    num_data, num_interp = interp_values.shape[-2:]\n    num_cols = rhs.size(-1)\n    interp_shape = torch.Size((*interp_indices.shape[:-2], output_dim, num_data))\n    output_shape = _matmul_broadcast_shape(interp_shape, rhs.shape)\n    batch_shape = output_shape[:-2]\n    batch_size = batch_shape.numel()\n\n    # Using interp_indices, create a sparse matrix that will sum up the values\n    interp_indices = interp_indices.expand(*batch_shape, *interp_indices.shape[-2:]).contiguous()\n    batch_indices = torch.arange(0, batch_size, dtype=torch.long, device=values.device).unsqueeze_(1)\n    batch_indices = batch_indices.repeat(1, num_data * num_interp)\n    column_indices = torch.arange(0, num_data * num_interp, dtype=torch.long, device=values.device).unsqueeze_(1)\n    column_indices = column_indices.repeat(batch_size, 1)\n    summing_matrix_indices = torch.stack([batch_indices.view(-1), interp_indices.view(-1), column_indices.view(-1)], 0)\n    summing_matrix_values = torch.ones(\n        batch_size * num_data * num_interp, dtype=interp_values.dtype, device=interp_values.device\n    )\n    size = torch.Size((batch_size, output_dim, num_data * num_interp))\n    type_name = summing_matrix_values.type().split(""."")[-1]  # e.g. FloatTensor\n    if interp_values.is_cuda:\n        cls = getattr(torch.cuda.sparse, type_name)\n    else:\n        cls = getattr(torch.sparse, type_name)\n    summing_matrix = cls(summing_matrix_indices, summing_matrix_values, size)\n\n    # Sum up the values appropriately by performing sparse matrix multiplication\n    values = values.reshape(batch_size, num_data * num_interp, num_cols)\n    res = dsmm(summing_matrix, values)\n\n    res = res.view(*batch_shape, *res.shape[-2:])\n    if is_vector:\n        res = res.squeeze(-1)\n    return res\n'"
gpytorch/utils/lanczos.py,12,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom .. import settings\n\n\ndef lanczos_tridiag(\n    matmul_closure,\n    max_iter,\n    dtype,\n    device,\n    matrix_shape,\n    batch_shape=torch.Size(),\n    init_vecs=None,\n    num_init_vecs=1,\n    tol=1e-5,\n):\n    """"""\n    """"""\n    # Determine batch mode\n    multiple_init_vecs = False\n\n    if not callable(matmul_closure):\n        raise RuntimeError(\n            ""matmul_closure should be a function callable object that multiples a (Lazy)Tensor ""\n            ""by a vector. Got a {} instead."".format(matmul_closure.__class__.__name__)\n        )\n\n    # Get initial probe ectors - and define if not available\n    if init_vecs is None:\n        init_vecs = torch.randn(matrix_shape[-1], num_init_vecs, dtype=dtype, device=device)\n        init_vecs = init_vecs.expand(*batch_shape, matrix_shape[-1], num_init_vecs)\n\n    else:\n        if settings.debug.on():\n            if dtype != init_vecs.dtype:\n                raise RuntimeError(\n                    ""Supplied dtype {} and init_vecs.dtype {} do not agree!"".format(dtype, init_vecs.dtype)\n                )\n            if device != init_vecs.device:\n                raise RuntimeError(\n                    ""Supplied device {} and init_vecs.device {} do not agree!"".format(device, init_vecs.device)\n                )\n            if batch_shape != init_vecs.shape[:-2]:\n                raise RuntimeError(\n                    ""batch_shape {} and init_vecs.shape {} do not agree!"".format(batch_shape, init_vecs.shape)\n                )\n            if matrix_shape[-1] != init_vecs.size(-2):\n                raise RuntimeError(\n                    ""matrix_shape {} and init_vecs.shape {} do not agree!"".format(matrix_shape, init_vecs.shape)\n                )\n\n        num_init_vecs = init_vecs.size(-1)\n\n    # Define some constants\n    num_iter = min(max_iter, matrix_shape[-1])\n    dim_dimension = -2\n\n    # Create storage for q_mat, alpha,and beta\n    # q_mat - batch version of Q - orthogonal matrix of decomp\n    # alpha - batch version main diagonal of T\n    # beta - batch version of off diagonal of T\n    q_mat = torch.zeros(num_iter, *batch_shape, matrix_shape[-1], num_init_vecs, dtype=dtype, device=device)\n    t_mat = torch.zeros(num_iter, num_iter, *batch_shape, num_init_vecs, dtype=dtype, device=device)\n\n    # Begin algorithm\n    # Initial Q vector: q_0_vec\n    q_0_vec = init_vecs / torch.norm(init_vecs, 2, dim=dim_dimension).unsqueeze(dim_dimension)\n    q_mat[0].copy_(q_0_vec)\n\n    # Initial alpha value: alpha_0\n    r_vec = matmul_closure(q_0_vec)\n    alpha_0 = q_0_vec.mul(r_vec).sum(dim_dimension)\n\n    # Initial beta value: beta_0\n    r_vec.sub_(alpha_0.unsqueeze(dim_dimension).mul(q_0_vec))\n    beta_0 = torch.norm(r_vec, 2, dim=dim_dimension)\n\n    # Copy over alpha_0 and beta_0 to t_mat\n    t_mat[0, 0].copy_(alpha_0)\n    t_mat[0, 1].copy_(beta_0)\n    t_mat[1, 0].copy_(beta_0)\n\n    # Compute the first new vector\n    q_mat[1].copy_(r_vec.div_(beta_0.unsqueeze(dim_dimension)))\n\n    # Now we start the iteration\n    for k in range(1, num_iter):\n        # Get previous values\n        q_prev_vec = q_mat[k - 1]\n        q_curr_vec = q_mat[k]\n        beta_prev = t_mat[k, k - 1].unsqueeze(dim_dimension)\n\n        # Compute next alpha value\n        r_vec = matmul_closure(q_curr_vec) - q_prev_vec.mul(beta_prev)\n        alpha_curr = q_curr_vec.mul(r_vec).sum(dim_dimension, keepdim=True)\n        # Copy over to t_mat\n        t_mat[k, k].copy_(alpha_curr.squeeze(dim_dimension))\n\n        # Copy over alpha_curr, beta_curr to t_mat\n        if (k + 1) < num_iter:\n            # Compute next residual value\n            r_vec.sub_(alpha_curr.mul(q_curr_vec))\n            # Full reorthogonalization: r <- r - Q (Q^T r)\n            correction = r_vec.unsqueeze(0).mul(q_mat[: k + 1]).sum(dim_dimension, keepdim=True)\n            correction = q_mat[: k + 1].mul(correction).sum(0)\n            r_vec.sub_(correction)\n            r_vec_norm = torch.norm(r_vec, 2, dim=dim_dimension, keepdim=True)\n            r_vec.div_(r_vec_norm)\n\n            # Get next beta value\n            beta_curr = r_vec_norm.squeeze_(dim_dimension)\n            # Update t_mat with new beta value\n            t_mat[k, k + 1].copy_(beta_curr)\n            t_mat[k + 1, k].copy_(beta_curr)\n\n            # Run more reorthoganilzation if necessary\n            inner_products = q_mat[: k + 1].mul(r_vec.unsqueeze(0)).sum(dim_dimension)\n            could_reorthogonalize = False\n            for _ in range(10):\n                if not torch.sum(inner_products > tol):\n                    could_reorthogonalize = True\n                    break\n                correction = r_vec.unsqueeze(0).mul(q_mat[: k + 1]).sum(dim_dimension, keepdim=True)\n                correction = q_mat[: k + 1].mul(correction).sum(0)\n                r_vec.sub_(correction)\n                r_vec_norm = torch.norm(r_vec, 2, dim=dim_dimension, keepdim=True)\n                r_vec.div_(r_vec_norm)\n                inner_products = q_mat[: k + 1].mul(r_vec.unsqueeze(0)).sum(dim_dimension)\n\n            # Update q_mat with new q value\n            q_mat[k + 1].copy_(r_vec)\n\n            if torch.sum(beta_curr.abs() > 1e-6) == 0 or not could_reorthogonalize:\n                break\n\n    # Now let\'s transpose q_mat, t_mat intot the correct shape\n    num_iter = k + 1\n\n    # num_init_vecs x batch_shape x matrix_shape[-1] x num_iter\n    q_mat = q_mat[: num_iter + 1].permute(-1, *range(1, 1 + len(batch_shape)), -2, 0).contiguous()\n    # num_init_vecs x batch_shape x num_iter x num_iter\n    t_mat = t_mat[: num_iter + 1, : num_iter + 1].permute(-1, *range(2, 2 + len(batch_shape)), 0, 1).contiguous()\n\n    # If we weren\'t in batch mode, remove batch dimension\n    if not multiple_init_vecs:\n        q_mat.squeeze_(0)\n        t_mat.squeeze_(0)\n\n    # We\'re done!\n    return q_mat, t_mat\n\n\ndef lanczos_tridiag_to_diag(t_mat):\n    """"""\n    Given a num_init_vecs x num_batch x k x k tridiagonal matrix t_mat,\n    returns a num_init_vecs x num_batch x k set of eigenvalues\n    and a num_init_vecs x num_batch x k x k set of eigenvectors.\n\n    TODO: make the eigenvalue computations done in batch mode.\n    """"""\n    orig_device = t_mat.device\n    if t_mat.size(-1) < 32:\n        retr = torch.symeig(t_mat.cpu(), eigenvectors=True)\n    else:\n        retr = torch.symeig(t_mat, eigenvectors=True)\n\n    evals, evecs = retr\n    mask = evals.ge(0)\n    evecs = evecs * mask.type_as(evecs).unsqueeze(-2)\n    evals = evals.masked_fill_(~mask, 1)\n\n    return evals.to(orig_device), evecs.to(orig_device)\n'"
gpytorch/utils/linear_cg.py,39,"b'#!/usr/bin/env python3\n\nimport warnings\n\nimport torch\n\nfrom .. import settings\nfrom .deprecation import bool_compat\nfrom .warnings import NumericalWarning\n\n\ndef _default_preconditioner(x):\n    return x.clone()\n\n\n@torch.jit.script\ndef _jit_linear_cg_updates(\n    result, alpha, residual_inner_prod, eps, beta, residual, precond_residual, mul_storage, is_zero, curr_conjugate_vec\n):\n    # # Update result\n    # # result_{k} = result_{k-1} + alpha_{k} p_vec_{k-1}\n    result = torch.addcmul(result, alpha, curr_conjugate_vec, out=result)\n\n    # beta_{k} = (precon_residual{k}^T r_vec_{k}) / (precon_residual{k-1}^T r_vec_{k-1})\n    beta.resize_as_(residual_inner_prod).copy_(residual_inner_prod)\n    torch.mul(residual, precond_residual, out=mul_storage)\n    torch.sum(mul_storage, -2, keepdim=True, out=residual_inner_prod)\n\n    # Do a safe division here\n    torch.lt(beta, eps, out=is_zero)\n    beta.masked_fill_(is_zero, 1)\n    torch.div(residual_inner_prod, beta, out=beta)\n    beta.masked_fill_(is_zero, 0)\n\n    # Update curr_conjugate_vec\n    # curr_conjugate_vec_{k} = precon_residual{k} + beta_{k} curr_conjugate_vec_{k-1}\n    curr_conjugate_vec.mul_(beta).add_(precond_residual)\n\n\n@torch.jit.script\ndef _jit_linear_cg_updates_no_precond(\n    mvms,\n    result,\n    has_converged,\n    alpha,\n    residual_inner_prod,\n    eps,\n    beta,\n    residual,\n    precond_residual,\n    mul_storage,\n    is_zero,\n    curr_conjugate_vec,\n):\n    torch.mul(curr_conjugate_vec, mvms, out=mul_storage)\n    torch.sum(mul_storage, dim=-2, keepdim=True, out=alpha)\n\n    # Do a safe division here\n    torch.lt(alpha, eps, out=is_zero)\n    alpha.masked_fill_(is_zero, 1)\n    torch.div(residual_inner_prod, alpha, out=alpha)\n    alpha.masked_fill_(is_zero, 0)\n\n    # We\'ll cancel out any updates by setting alpha=0 for any vector that has already converged\n    alpha.masked_fill_(has_converged, 0)\n\n    # Update residual\n    # residual_{k} = residual_{k-1} - alpha_{k} mat p_vec_{k-1}\n    torch.addcmul(residual, -alpha, mvms, out=residual)\n\n    # Update precond_residual\n    # precon_residual{k} = M^-1 residual_{k}\n    precond_residual = residual.clone()\n\n    _jit_linear_cg_updates(\n        result,\n        alpha,\n        residual_inner_prod,\n        eps,\n        beta,\n        residual,\n        precond_residual,\n        mul_storage,\n        is_zero,\n        curr_conjugate_vec,\n    )\n\n\ndef linear_cg(\n    matmul_closure,\n    rhs,\n    n_tridiag=0,\n    tolerance=None,\n    eps=1e-10,\n    stop_updating_after=1e-10,\n    max_iter=None,\n    max_tridiag_iter=None,\n    initial_guess=None,\n    preconditioner=None,\n):\n    """"""\n    Implements the linear conjugate gradients method for (approximately) solving systems of the form\n\n        lhs result = rhs\n\n    for positive definite and symmetric matrices.\n\n    Args:\n      - matmul_closure - a function which performs a left matrix multiplication with lhs_mat\n      - rhs - the right-hand side of the equation\n      - n_tridiag - returns a tridiagonalization of the first n_tridiag columns of rhs\n      - tolerance - stop the solve when the max residual is less than this\n      - eps - noise to add to prevent division by zero\n      - stop_updating_after - will stop updating a vector after this residual norm is reached\n      - max_iter - the maximum number of CG iterations\n      - max_tridiag_iter - the maximum size of the tridiagonalization matrix\n      - initial_guess - an initial guess at the solution `result`\n      - precondition_closure - a functions which left-preconditions a supplied vector\n\n    Returns:\n      result - a solution to the system (if n_tridiag is 0)\n      result, tridiags - a solution to the system, and corresponding tridiagonal matrices (if n_tridiag > 0)\n    """"""\n    # Unsqueeze, if necesasry\n    is_vector = rhs.ndimension() == 1\n    if is_vector:\n        rhs = rhs.unsqueeze(-1)\n\n    # Some default arguments\n    if max_iter is None:\n        max_iter = settings.max_cg_iterations.value()\n    if max_tridiag_iter is None:\n        max_tridiag_iter = settings.max_lanczos_quadrature_iterations.value()\n    if initial_guess is None:\n        initial_guess = torch.zeros_like(rhs)\n    if tolerance is None:\n        if settings._use_eval_tolerance.on():\n            tolerance = settings.eval_cg_tolerance.value()\n        else:\n            tolerance = settings.cg_tolerance.value()\n    if preconditioner is None:\n        preconditioner = _default_preconditioner\n        precond = False\n    else:\n        precond = True\n\n    # If we are running m CG iterations, we obviously can\'t get more than m Lanczos coefficients\n    if max_tridiag_iter > max_iter:\n        raise RuntimeError(""Getting a tridiagonalization larger than the number of CG iterations run is not possible!"")\n\n    # Check matmul_closure object\n    if torch.is_tensor(matmul_closure):\n        matmul_closure = matmul_closure.matmul\n    elif not callable(matmul_closure):\n        raise RuntimeError(""matmul_closure must be a tensor, or a callable object!"")\n\n    # Get some constants\n    batch_shape = rhs.shape[:-2]\n    num_rows = rhs.size(-2)\n    n_iter = min(max_iter, num_rows) if settings.terminate_cg_by_size.on() else max_iter\n    n_tridiag_iter = min(max_tridiag_iter, num_rows)\n    eps = torch.tensor(eps, dtype=rhs.dtype, device=rhs.device)\n\n    # Get the norm of the rhs - used for convergence checks\n    # Here we\'re going to make almost-zero norms actually be 1 (so we don\'t get divide-by-zero issues)\n    # But we\'ll store which norms were actually close to zero\n    rhs_norm = rhs.norm(2, dim=-2, keepdim=True)\n    rhs_is_zero = rhs_norm.lt(eps)\n    rhs_norm = rhs_norm.masked_fill_(rhs_is_zero, 1)\n\n    # Let\'s normalize. We\'ll un-normalize afterwards\n    rhs = rhs.div(rhs_norm)\n\n    # residual: residual_{0} = b_vec - lhs x_{0}\n    residual = rhs - matmul_closure(initial_guess)\n\n    # result <- x_{0}\n    result = initial_guess.expand_as(residual).contiguous()\n\n    # Check for NaNs\n    if not torch.equal(residual, residual):\n        raise RuntimeError(""NaNs encountered when trying to perform matrix-vector multiplication"")\n\n    # Sometime we\'re lucky and the preconditioner solves the system right away\n    # Check for convergence\n    residual_norm = residual.norm(2, dim=-2, keepdim=True)\n    has_converged = torch.lt(residual_norm, stop_updating_after)\n\n    if has_converged.all() and not n_tridiag:\n        n_iter = 0  # Skip the iteration!\n\n    # Otherwise, let\'s define precond_residual and curr_conjugate_vec\n    else:\n        # precon_residual{0} = M^-1 residual_{0}\n        precond_residual = preconditioner(residual)\n        curr_conjugate_vec = precond_residual\n        residual_inner_prod = precond_residual.mul(residual).sum(-2, keepdim=True)\n\n        # Define storage matrices\n        mul_storage = torch.empty_like(residual)\n        alpha = torch.empty(*batch_shape, 1, rhs.size(-1), dtype=residual.dtype, device=residual.device)\n        beta = torch.empty_like(alpha)\n        is_zero = torch.empty(*batch_shape, 1, rhs.size(-1), dtype=bool_compat, device=residual.device)\n\n    # Define tridiagonal matrices, if applicable\n    if n_tridiag:\n        t_mat = torch.zeros(\n            n_tridiag_iter, n_tridiag_iter, *batch_shape, n_tridiag, dtype=alpha.dtype, device=alpha.device\n        )\n        alpha_tridiag_is_zero = torch.empty(*batch_shape, n_tridiag, dtype=bool_compat, device=t_mat.device)\n        alpha_reciprocal = torch.empty(*batch_shape, n_tridiag, dtype=t_mat.dtype, device=t_mat.device)\n        prev_alpha_reciprocal = torch.empty_like(alpha_reciprocal)\n        prev_beta = torch.empty_like(alpha_reciprocal)\n\n    update_tridiag = True\n    last_tridiag_iter = 0\n\n    # It\'s conceivable we reach the tolerance on the last iteration, so can\'t just check iteration number.\n    tolerance_reached = False\n\n    # Start the iteration\n    for k in range(n_iter):\n        # Get next alpha\n        # alpha_{k} = (residual_{k-1}^T precon_residual{k-1}) / (p_vec_{k-1}^T mat p_vec_{k-1})\n        mvms = matmul_closure(curr_conjugate_vec)\n        if precond:\n            torch.mul(curr_conjugate_vec, mvms, out=mul_storage)\n            torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n\n            # Do a safe division here\n            torch.lt(alpha, eps, out=is_zero)\n            alpha.masked_fill_(is_zero, 1)\n            torch.div(residual_inner_prod, alpha, out=alpha)\n            alpha.masked_fill_(is_zero, 0)\n\n            # We\'ll cancel out any updates by setting alpha=0 for any vector that has already converged\n            alpha.masked_fill_(has_converged, 0)\n\n            # Update residual\n            # residual_{k} = residual_{k-1} - alpha_{k} mat p_vec_{k-1}\n            residual = torch.addcmul(residual, alpha, mvms, value=-1, out=residual)\n\n            # Update precond_residual\n            # precon_residual{k} = M^-1 residual_{k}\n            precond_residual = preconditioner(residual)\n\n            _jit_linear_cg_updates(\n                result,\n                alpha,\n                residual_inner_prod,\n                eps,\n                beta,\n                residual,\n                precond_residual,\n                mul_storage,\n                is_zero,\n                curr_conjugate_vec,\n            )\n        else:\n            _jit_linear_cg_updates_no_precond(\n                mvms,\n                result,\n                has_converged,\n                alpha,\n                residual_inner_prod,\n                eps,\n                beta,\n                residual,\n                precond_residual,\n                mul_storage,\n                is_zero,\n                curr_conjugate_vec,\n            )\n\n        torch.norm(residual, 2, dim=-2, keepdim=True, out=residual_norm)\n        residual_norm.masked_fill_(rhs_is_zero, 0)\n        torch.lt(residual_norm, stop_updating_after, out=has_converged)\n\n        if k >= 10 and bool(residual_norm.mean() < tolerance) and not (n_tridiag and k < n_tridiag_iter):\n            tolerance_reached = True\n            break\n\n        # Update tridiagonal matrices, if applicable\n        if n_tridiag and k < n_tridiag_iter and update_tridiag:\n            alpha_tridiag = alpha.squeeze_(-2).narrow(-1, 0, n_tridiag)\n            beta_tridiag = beta.squeeze_(-2).narrow(-1, 0, n_tridiag)\n            torch.eq(alpha_tridiag, 0, out=alpha_tridiag_is_zero)\n            alpha_tridiag.masked_fill_(alpha_tridiag_is_zero, 1)\n            torch.reciprocal(alpha_tridiag, out=alpha_reciprocal)\n            alpha_tridiag.masked_fill_(alpha_tridiag_is_zero, 0)\n\n            if k == 0:\n                t_mat[k, k].copy_(alpha_reciprocal)\n            else:\n                torch.addcmul(alpha_reciprocal, prev_beta, prev_alpha_reciprocal, out=t_mat[k, k])\n                torch.mul(prev_beta.sqrt_(), prev_alpha_reciprocal, out=t_mat[k, k - 1])\n                t_mat[k - 1, k].copy_(t_mat[k, k - 1])\n\n                if t_mat[k - 1, k].max() < 1e-6:\n                    update_tridiag = False\n\n            last_tridiag_iter = k\n\n            prev_alpha_reciprocal.copy_(alpha_reciprocal)\n            prev_beta.copy_(beta_tridiag)\n\n    # Un-normalize\n    result = result.mul(rhs_norm)\n\n    if not tolerance_reached and n_iter > 0:\n        warnings.warn(\n            ""CG terminated in {} iterations with average residual norm {}""\n            "" which is larger than the tolerance of {} specified by""\n            "" gpytorch.settings.cg_tolerance.""\n            "" If performance is affected, consider raising the maximum number of CG iterations by running code in""\n            "" a gpytorch.settings.max_cg_iterations(value) context."".format(k + 1, residual_norm.mean(), tolerance),\n            NumericalWarning,\n        )\n\n    if is_vector:\n        result = result.squeeze(-1)\n\n    if n_tridiag:\n        t_mat = t_mat[: last_tridiag_iter + 1, : last_tridiag_iter + 1]\n        return result, t_mat.permute(-1, *range(2, 2 + len(batch_shape)), 0, 1).contiguous()\n    else:\n        return result\n'"
gpytorch/utils/memoize.py,0,"b'#!/usr/bin/env python3\n\nimport functools\n\n\ndef add_to_cache(obj, name, val):\n    """"""Add a result to the cache of an object.""""""\n    if not hasattr(obj, ""_memoize_cache""):\n        obj._memoize_cache = dict()\n    obj._memoize_cache[name] = val\n    return obj\n\n\ndef get_from_cache(obj, name):\n    """"""Get an item from the cache.""""""\n    if not is_in_cache(obj, name):\n        raise RuntimeError(""Object does not have item {} stored in cache."".format(name))\n    return obj._memoize_cache[name]\n\n\ndef is_in_cache(obj, name):\n    return hasattr(obj, ""_memoize_cache"") and name in obj._memoize_cache\n\n\ndef cached(method=None, name=None):\n    """"""A decorator allowing for specifying the name of a cache, allowing it to be modified elsewhere.""""""\n    if method is None:\n        return functools.partial(cached, name=name)\n\n    @functools.wraps(method)\n    def g(self, *args, **kwargs):\n        cache_name = name if name is not None else method\n        if not is_in_cache(self, cache_name):\n            add_to_cache(self, cache_name, method(self, *args, **kwargs))\n        return get_from_cache(self, cache_name)\n\n    return g\n\n\ndef is_cached(self, name):\n    """"""\n    Determine if a cached item has been computed\n    """"""\n    return hasattr(self, ""_memoize_cache"") and name in self._memoize_cache.keys()\n'"
gpytorch/utils/pivoted_cholesky.py,12,"b""#!/usr/bin/env python3\n\nimport torch\n\nfrom .. import settings\n\n\ndef pivoted_cholesky(matrix, max_iter, error_tol=None):\n    from ..lazy import lazify, LazyTensor\n\n    batch_shape = matrix.shape[:-2]\n    matrix_shape = matrix.shape[-2:]\n\n    if error_tol is None:\n        error_tol = settings.preconditioner_tolerance.value()\n\n    # Need to get diagonals. This is easy if it's a LazyTensor, since\n    # LazyTensor.diag() operates in batch mode.\n    matrix = lazify(matrix)\n    matrix_diag = matrix._approx_diag()\n\n    # Make sure max_iter isn't bigger than the matrix\n    max_iter = min(max_iter, matrix_shape[-1])\n\n    # What we're returning\n    L = torch.zeros(*batch_shape, max_iter, matrix_shape[-1], dtype=matrix.dtype, device=matrix.device)\n    orig_error = torch.max(matrix_diag, dim=-1)[0]\n    errors = torch.norm(matrix_diag, 1, dim=-1) / orig_error\n\n    # The permutation\n    permutation = torch.arange(0, matrix_shape[-1], dtype=torch.long, device=matrix_diag.device)\n    permutation = permutation.repeat(*batch_shape, 1)\n\n    # Get batch indices\n    batch_iters = [\n        torch.arange(0, size, dtype=torch.long, device=matrix_diag.device)\n        .unsqueeze_(-1)\n        .repeat(torch.Size(batch_shape[:i]).numel(), torch.Size(batch_shape[i + 1 :]).numel())\n        .view(-1)\n        for i, size in enumerate(batch_shape)\n    ]\n\n    m = 0\n    while (m == 0) or (m < max_iter and torch.max(errors) > error_tol):\n        permuted_diags = torch.gather(matrix_diag, -1, permutation[..., m:])\n        max_diag_values, max_diag_indices = torch.max(permuted_diags, -1)\n\n        max_diag_indices = max_diag_indices + m\n\n        # Swap pi_m and pi_i in each row, where pi_i is the element of the permutation\n        # corresponding to the max diagonal element\n        old_pi_m = permutation[..., m].clone()\n        permutation[..., m].copy_(permutation.gather(-1, max_diag_indices.unsqueeze(-1)).squeeze_(-1))\n        permutation.scatter_(-1, max_diag_indices.unsqueeze(-1), old_pi_m.unsqueeze(-1))\n        pi_m = permutation[..., m].contiguous()\n\n        L_m = L[..., m, :]  # Will be all zeros -- should we use torch.zeros?\n        L_m.scatter_(-1, pi_m.unsqueeze(-1), max_diag_values.sqrt().unsqueeze_(-1))\n\n        row = matrix[(*batch_iters, pi_m.view(-1), slice(None, None, None))]\n        if isinstance(row, LazyTensor):\n            row = row.evaluate()\n        row = row.view(*batch_shape, matrix_shape[-1])\n\n        if m + 1 < matrix_shape[-1]:\n            pi_i = permutation[..., m + 1 :].contiguous()\n\n            L_m_new = row.gather(-1, pi_i)\n            if m > 0:\n                L_prev = L[..., :m, :].gather(-1, pi_i.unsqueeze(-2).repeat(*(1 for _ in batch_shape), m, 1))\n                update = L[..., :m, :].gather(-1, pi_m.view(*pi_m.shape, 1, 1).repeat(*(1 for _ in batch_shape), m, 1))\n                L_m_new -= torch.sum(update * L_prev, dim=-2)\n\n            L_m_new /= L_m.gather(-1, pi_m.unsqueeze(-1))\n            L_m.scatter_(-1, pi_i, L_m_new)\n\n            matrix_diag_current = matrix_diag.gather(-1, pi_i)\n            matrix_diag.scatter_(-1, pi_i, matrix_diag_current - L_m_new ** 2)\n            L[..., m, :] = L_m\n\n            errors = torch.norm(matrix_diag.gather(-1, pi_i), 1, dim=-1) / orig_error\n        m = m + 1\n\n    return L[..., :m, :].transpose(-1, -2).contiguous()\n"""
gpytorch/utils/quadrature.py,5,"b'#!/usr/bin/env python3\n\nimport math\n\nimport numpy as np\nimport torch\nfrom torch.nn import Module\n\nfrom .. import settings\nfrom .broadcasting import _pad_with_singletons\n\n\nclass GaussHermiteQuadrature1D(Module):\n    """"""\n    Implements Gauss-Hermite quadrature for integrating a function with respect to several 1D Gaussian distributions\n    in batch mode. Within GPyTorch, this is useful primarily for computing expected log likelihoods for variational\n    inference.\n\n    This is implemented as a Module because Gauss-Hermite quadrature has a set of locations and weights that it\n    should initialize one time, but that should obey parent calls to .cuda(), .double() etc.\n    """"""\n\n    def __init__(self, num_locs=settings.num_gauss_hermite_locs.value()):\n        super().__init__()\n        self.num_locs = num_locs\n\n        locations, weights = self._locs_and_weights(num_locs)\n\n        self.locations = locations\n        self.weights = weights\n\n    def _apply(self, fn):\n        self.locations = fn(self.locations)\n        self.weights = fn(self.weights)\n        return super(GaussHermiteQuadrature1D, self)._apply(fn)\n\n    def _locs_and_weights(self, num_locs):\n        """"""\n        Get locations and weights for Gauss-Hermite quadrature. Note that this is **not** intended to be used\n        externally, because it directly creates tensors with no knowledge of a device or dtype to cast to.\n\n        Instead, create a GaussHermiteQuadrature1D object and get the locations and weights from buffers.\n        """"""\n        locations, weights = np.polynomial.hermite.hermgauss(num_locs)\n        locations = torch.Tensor(locations)\n        weights = torch.Tensor(weights)\n        return locations, weights\n\n    def forward(self, func, gaussian_dists):\n        """"""\n        Runs Gauss-Hermite quadrature on the callable func, integrating against the Gaussian distributions specified\n        by gaussian_dists.\n\n        Args:\n            - func (callable): Function to integrate\n            - gaussian_dists (Distribution): Either a MultivariateNormal whose covariance is assumed to be diagonal\n                or a :obj:`torch.distributions.Normal`.\n        Returns:\n            - Result of integrating func against each univariate Gaussian in gaussian_dists.\n        """"""\n        means = gaussian_dists.mean\n        variances = gaussian_dists.variance\n\n        locations = _pad_with_singletons(self.locations, num_singletons_before=0, num_singletons_after=means.dim())\n\n        shifted_locs = torch.sqrt(2.0 * variances) * locations + means\n        log_probs = func(shifted_locs)\n        weights = _pad_with_singletons(self.weights, num_singletons_before=0, num_singletons_after=log_probs.dim() - 1)\n\n        res = (1 / math.sqrt(math.pi)) * (log_probs * weights)\n        res = res.sum(tuple(range(self.locations.dim())))\n\n        return res\n'"
gpytorch/utils/sparse.py,36,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom .broadcasting import _matmul_broadcast_shape\n\n\ndef make_sparse_from_indices_and_values(interp_indices, interp_values, num_rows):\n    """"""\n    This produces a sparse tensor with a fixed number of non-zero entries in each column.\n\n    Args:\n        - interp_indices - Tensor (batch_size) x num_cols x n_nonzero_entries\n            A matrix which has the indices of the nonzero_entries for each column\n        - interp_values - Tensor (batch_size) x num_cols x n_nonzero_entries\n            The corresponding values\n        - num_rows - the number of rows in the result matrix\n\n    Returns:\n        - SparseTensor - (batch_size) x num_cols x num_rows\n    """"""\n\n    if not torch.is_tensor(interp_indices):\n        raise RuntimeError(""interp_indices and interp_values should be tensors"")\n\n    # Is it batch mode?\n    batch_shape = interp_values.shape[:-2]\n    n_target_points, n_coefficients = interp_values.shape[-2:]\n\n    # Index tensor\n    batch_tensors = []\n    for i, batch_size in enumerate(batch_shape):\n        batch_tensor = torch.arange(0, batch_size, dtype=torch.long, device=interp_values.device)\n        batch_tensor = (\n            batch_tensor.unsqueeze_(1)\n            .repeat(batch_shape[:i].numel(), batch_shape[i + 1 :].numel() * n_target_points * n_coefficients)\n            .view(-1)\n        )\n        batch_tensors.append(batch_tensor)\n\n    row_tensor = torch.arange(0, n_target_points, dtype=torch.long, device=interp_values.device)\n    row_tensor = row_tensor.unsqueeze_(1).repeat(batch_shape.numel(), n_coefficients).view(-1)\n    index_tensor = torch.stack([*batch_tensors, interp_indices.reshape(-1), row_tensor], 0)\n\n    # Value tensor\n    value_tensor = interp_values.reshape(-1)\n    nonzero_indices = value_tensor.nonzero()\n    if nonzero_indices.storage():\n        nonzero_indices.squeeze_()\n        index_tensor = index_tensor.index_select(1, nonzero_indices)\n        value_tensor = value_tensor.index_select(0, nonzero_indices)\n    else:\n        index_tensor = index_tensor.resize_(interp_indices.dim(), 1).zero_()\n        value_tensor = value_tensor.resize_(1).zero_()\n\n    # Make the sparse tensor\n    type_name = value_tensor.type().split(""."")[-1]  # e.g. FloatTensor\n    interp_size = torch.Size((*batch_shape, num_rows, n_target_points))\n    if index_tensor.is_cuda:\n        cls = getattr(torch.cuda.sparse, type_name)\n    else:\n        cls = getattr(torch.sparse, type_name)\n    res = cls(index_tensor, value_tensor, interp_size)\n\n    # Wrap things as a variable, if necessary\n    return res\n\n\ndef bdsmm(sparse, dense):\n    """"""\n    Batch dense-sparse matrix multiply\n    """"""\n    # Make the batch sparse matrix into a block-diagonal matrix\n    if sparse.ndimension() > 2:\n        # Expand the tensors to account for broadcasting\n        output_shape = _matmul_broadcast_shape(sparse.shape, dense.shape)\n        expanded_sparse_shape = output_shape[:-2] + sparse.shape[-2:]\n        unsqueezed_sparse_shape = [1 for _ in range(len(output_shape) - sparse.dim())] + list(sparse.shape)\n        repeat_sizes = tuple(\n            output_size // sparse_size\n            for output_size, sparse_size in zip(expanded_sparse_shape, unsqueezed_sparse_shape)\n        )\n        sparse = sparse_repeat(sparse, *repeat_sizes)\n        dense = dense.expand(*output_shape[:-2], dense.size(-2), dense.size(-1))\n\n        # Figure out how much need to be added to the row/column indices to create\n        # a block-diagonal matrix\n        *batch_shape, num_rows, num_cols = sparse.shape\n        batch_size = torch.Size(batch_shape).numel()\n        batch_multiplication_factor = torch.tensor(\n            [torch.Size(batch_shape[i + 1 :]).numel() for i in range(len(batch_shape))],\n            dtype=torch.long,\n            device=sparse.device,\n        )\n        if batch_multiplication_factor.is_cuda:\n            batch_assignment = (sparse._indices()[:-2].float().t() @ batch_multiplication_factor.float()).long()\n        else:\n            batch_assignment = sparse._indices()[:-2].t() @ batch_multiplication_factor\n\n        # Create block-diagonal sparse tensor\n        indices = sparse._indices()[-2:].clone()\n        indices[0].add_(batch_assignment, alpha=num_rows)\n        indices[1].add_(batch_assignment, alpha=num_cols)\n        sparse_2d = torch.sparse_coo_tensor(\n            indices,\n            sparse._values(),\n            torch.Size((batch_size * num_rows, batch_size * num_cols)),\n            dtype=sparse._values().dtype,\n            device=sparse._values().device,\n        )\n\n        dense_2d = dense.reshape(batch_size * num_cols, -1)\n        res = torch.dsmm(sparse_2d, dense_2d)\n        res = res.view(*batch_shape, num_rows, -1)\n        return res\n\n    elif dense.dim() > 2:\n        *batch_shape, num_rows, num_cols = dense.size()\n        batch_size = torch.Size(batch_shape).numel()\n        dense = dense.view(batch_size, num_rows, num_cols)\n        res = torch.dsmm(sparse, dense.transpose(0, 1).reshape(-1, batch_size * num_cols))\n        res = res.view(-1, batch_size, num_cols)\n        res = res.transpose(0, 1).reshape(*batch_shape, -1, num_cols)\n        return res\n\n    else:\n        return torch.dsmm(sparse, dense)\n\n\ndef sparse_eye(size):\n    """"""\n    Returns the identity matrix as a sparse matrix\n    """"""\n    indices = torch.arange(0, size).long().unsqueeze(0).expand(2, size)\n    values = torch.tensor(1.0).expand(size)\n    cls = getattr(torch.sparse, values.type().split(""."")[-1])\n    return cls(indices, values, torch.Size([size, size]))\n\n\ndef sparse_getitem(sparse, idxs):\n    """"""\n    """"""\n    if not isinstance(idxs, tuple):\n        idxs = (idxs,)\n\n    if not sparse.ndimension() <= 2:\n        raise RuntimeError(""Must be a 1d or 2d sparse tensor"")\n\n    if len(idxs) > sparse.ndimension():\n        raise RuntimeError(""Invalid index for %d-order tensor"" % sparse.ndimension())\n\n    indices = sparse._indices()\n    values = sparse._values()\n    size = list(sparse.size())\n\n    for i, idx in list(enumerate(idxs))[::-1]:\n        if isinstance(idx, int):\n            del size[i]\n            mask = indices[i].eq(idx)\n            if torch.any(mask):\n                new_indices = torch.zeros(\n                    indices.size(0) - 1, torch.sum(mask), dtype=indices.dtype, device=indices.device\n                )\n                for j in range(indices.size(0)):\n                    if i > j:\n                        new_indices[j].copy_(indices[j][mask])\n                    elif i < j:\n                        new_indices[j - 1].copy_(indices[j][mask])\n                indices = new_indices\n                values = values[mask]\n            else:\n                indices.resize_(indices.size(0) - 1, 1).zero_()\n                values.resize_(1).zero_()\n\n            if not len(size):\n                return sum(values)\n\n        elif isinstance(idx, slice):\n            start, stop, step = idx.indices(size[i])\n            size = list(size[:i]) + [stop - start] + list(size[i + 1 :])\n            if step != 1:\n                raise RuntimeError(""Slicing with step is not supported"")\n            mask = indices[i].lt(stop) & indices[i].ge(start)\n            if torch.any(mask):\n                new_indices = torch.zeros(indices.size(0), torch.sum(mask), dtype=indices.dtype, device=indices.device)\n                for j in range(indices.size(0)):\n                    new_indices[j].copy_(indices[j][mask])\n                new_indices[i].sub_(start)\n                indices = new_indices\n                values = values[mask]\n            else:\n                indices.resize_(indices.size(0), 1).zero_()\n                values.resize_(1).zero_()\n\n        else:\n            raise RuntimeError(""Unknown index type"")\n\n    return torch.sparse_coo_tensor(indices, values, torch.Size(size), dtype=values.dtype, device=values.device)\n\n\ndef sparse_repeat(sparse, *repeat_sizes):\n    """"""\n    """"""\n    if len(repeat_sizes) == 1 and isinstance(repeat_sizes, tuple):\n        repeat_sizes = repeat_sizes[0]\n\n    if len(repeat_sizes) > len(sparse.shape):\n        num_new_dims = len(repeat_sizes) - len(sparse.shape)\n        new_indices = sparse._indices()\n        new_indices = torch.cat(\n            [\n                torch.zeros(num_new_dims, new_indices.size(1), dtype=new_indices.dtype, device=new_indices.device),\n                new_indices,\n            ],\n            0,\n        )\n        sparse = torch.sparse_coo_tensor(\n            new_indices,\n            sparse._values(),\n            torch.Size((*[1 for _ in range(num_new_dims)], *sparse.shape)),\n            dtype=sparse.dtype,\n            device=sparse.device,\n        )\n\n    for i, repeat_size in enumerate(repeat_sizes):\n        if repeat_size > 1:\n            new_indices = sparse._indices().repeat(1, repeat_size)\n            adding_factor = torch.arange(0, repeat_size, dtype=new_indices.dtype, device=new_indices.device).unsqueeze_(\n                1\n            )\n            new_indices[i].view(repeat_size, -1).add_(adding_factor)\n            sparse = torch.sparse_coo_tensor(\n                new_indices,\n                sparse._values().repeat(repeat_size),\n                torch.Size((*sparse.shape[:i], repeat_size * sparse.size(i), *sparse.shape[i + 1 :])),\n                dtype=sparse.dtype,\n                device=sparse.device,\n            )\n\n    return sparse\n\n\ndef to_sparse(dense):\n    """"""\n    """"""\n    mask = dense.ne(0)\n    indices = mask.nonzero()\n    if indices.storage():\n        values = dense[mask]\n    else:\n        indices = indices.resize_(1, dense.ndimension()).zero_()\n        values = torch.tensor(0, dtype=dense.dtype, device=dense.device)\n\n    # Construct sparse tensor\n    klass = getattr(torch.sparse, dense.type().split(""."")[-1])\n    res = klass(indices.t(), values, dense.size())\n    if dense.is_cuda:\n        res = res.cuda()\n    return res\n'"
gpytorch/utils/stochastic_lq.py,4,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom .lanczos import lanczos_tridiag\n\n\nclass StochasticLQ(object):\n    """"""\n    Implements an approximate log determinant calculation for symmetric positive definite matrices\n    using stochastic Lanczos quadrature. For efficient calculation of derivatives, We additionally\n    compute the trace of the inverse using the same probe vector the log determinant was computed\n    with. For more details, see Dong et al. 2017 (in submission).\n    """"""\n\n    def __init__(self, max_iter=15, num_random_probes=10):\n        """"""\n        The nature of stochastic Lanczos quadrature is that the calculation of tr(f(A)) is both inaccurate and\n        stochastic. An instance of StochasticLQ has two parameters that control these tradeoffs. Increasing either\n        parameter increases the running time of the algorithm.\n\n        Args:\n            - cls - Tensor constructor - to ensure correct type (default - default tensor)\n            - max_iter (scalar) - The number of Lanczos iterations to perform. Increasing this makes the estimate of\n                tr(f(A)) more accurate in expectation -- that is, the average value returned has lower error.\n            - num_random_probes (scalar) - The number of random probes to use in the stochastic trace estimation.\n                Increasing this makes the estimate of tr(f(A)) lower variance -- that is, the value\n                returned is more consistent.\n        """"""\n        self.max_iter = max_iter\n        self.num_random_probes = num_random_probes\n\n    def lanczos_batch(self, matmul_closure, rhs_vectors):\n        return lanczos_tridiag(\n            matmul_closure,\n            self.max_iter,\n            init_vecs=rhs_vectors,\n            dtype=rhs_vectors.dtype,\n            device=rhs_vectors.device,\n            batch_shape=rhs_vectors.shape[-2:],\n            matrix_shape=torch.Size((rhs_vectors.size(-2), rhs_vectors.size(-2))),\n        )\n\n    def evaluate(self, matrix_shape, eigenvalues, eigenvectors, funcs):\n        r""""""\n        Computes tr(f(A)) for an arbitrary list of functions, where f(A) is equivalent to applying the function\n        elementwise to the eigenvalues of A, i.e., if A = V\\LambdaV^{T}, then f(A) = Vf(\\Lambda)V^{T}, where\n        f(\\Lambda) is applied elementwise.\n        Note that calling this function with a list of functions to apply is significantly more efficient than\n        calling it multiple times with one function -- each additional function after the first requires negligible\n        additional computation.\n\n        Args:\n            - matrix_shape (torch.Size()) - size of underlying matrix (not including batch dimensions)\n            - eigenvalues (Tensor n_probes x ...batch_shape x k) - batches of eigenvalues from Lanczos tridiag mats\n            - eigenvectors (Tensor n_probes x ...batch_shape x k x k) - batches of eigenvectors from "" "" ""\n            - funcs (list of closures) - A list of functions [f_1,...,f_k]. tr(f_i(A)) is computed for each function.\n                Each function in the closure should expect to take a torch vector of eigenvalues as input and apply\n                the function elementwise. For example, to compute logdet(A) = tr(log(A)), [lambda x: x.log()] would\n                be a reasonable value of funcs.\n\n        Returns:\n            - results (list of scalars) - The trace of each supplied function applied to the matrix, e.g.,\n                [tr(f_1(A)),tr(f_2(A)),...,tr(f_k(A))].\n        """"""\n        batch_shape = torch.Size(eigenvalues.shape[1:-1])\n        results = [torch.zeros(batch_shape, dtype=eigenvalues.dtype, device=eigenvalues.device) for _ in funcs]\n        num_random_probes = eigenvalues.size(0)\n        for j in range(num_random_probes):\n            # These are (num_batch x k) and (num_batch x k x k)\n            eigenvalues_for_probe = eigenvalues[j]\n            eigenvectors_for_probe = eigenvectors[j]\n            for i, func in enumerate(funcs):\n                # First component of eigenvecs is (num_batch x k)\n                eigenvecs_first_component = eigenvectors_for_probe[..., 0, :]\n                func_eigenvalues = func(eigenvalues_for_probe)\n\n                dot_products = (eigenvecs_first_component.pow(2) * func_eigenvalues).sum(-1)\n                results[i] = results[i] + matrix_shape[-1] / float(num_random_probes) * dot_products\n\n        return results\n'"
gpytorch/utils/toeplitz.py,8,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..utils import broadcasting, fft\n\n\ndef toeplitz(toeplitz_column, toeplitz_row):\n    """"""\n    Constructs tensor version of toeplitz matrix from column vector\n    Args:\n        - toeplitz_column (vector n) - column of toeplitz matrix\n        - toeplitz_row (vector n-1) - row of toeplitz matrix\n    Returns:\n        - Matrix (n x n) - matrix representation\n    """"""\n    if toeplitz_column.ndimension() != 1:\n        raise RuntimeError(""toeplitz_column must be a vector."")\n\n    if toeplitz_row.ndimension() != 1:\n        raise RuntimeError(""toeplitz_row must be a vector."")\n\n    if toeplitz_column[0] != toeplitz_row[0]:\n        raise RuntimeError(\n            ""The first column and first row of the Toeplitz matrix should have ""\n            ""the same first otherwise the value of T[0,0] is ambiguous. ""\n            ""Got: c[0]={} and r[0]={}"".format(toeplitz_column[0], toeplitz_row[0])\n        )\n\n    if len(toeplitz_column) != len(toeplitz_row):\n        raise RuntimeError(""c and r should have the same length "" ""(Toeplitz matrices are necessarily square)."")\n\n    if type(toeplitz_column) != type(toeplitz_row):\n        raise RuntimeError(""toeplitz_column and toeplitz_row should be the same type."")\n\n    if len(toeplitz_column) == 1:\n        return toeplitz_column.view(1, 1)\n\n    res = torch.empty(\n        len(toeplitz_column), len(toeplitz_column), dtype=toeplitz_column.dtype, device=toeplitz_column.device\n    )\n    for i, val in enumerate(toeplitz_column):\n        for j in range(len(toeplitz_column) - i):\n            res[j + i, j] = val\n    for i, val in list(enumerate(toeplitz_row))[1:]:\n        for j in range(len(toeplitz_row) - i):\n            res[j, j + i] = val\n    return res\n\n\ndef sym_toeplitz(toeplitz_column):\n    """"""\n    Constructs tensor version of symmetric toeplitz matrix from column vector\n    Args:\n        - toeplitz_column (vector n) - column of Toeplitz matrix\n    Returns:\n        - Matrix (n x n) - matrix representation\n    """"""\n    return toeplitz(toeplitz_column, toeplitz_column)\n\n\ndef toeplitz_getitem(toeplitz_column, toeplitz_row, i, j):\n    """"""\n    Gets the (i,j)th entry of a Toeplitz matrix T.\n    Args:\n        - toeplitz_column (vector n) - column of Toeplitz matrix\n        - toeplitz_row (vector n) - row of Toeplitz matrix\n        - i (scalar) - row of entry to get\n        - j (scalar) - column of entry to get\n    Returns:\n        - T[i,j], where T is the Toeplitz matrix specified by c and r.\n    """"""\n    index = i - j\n    if index < 0:\n        return toeplitz_row[abs(index)]\n    else:\n        return toeplitz_column[index]\n\n\ndef sym_toeplitz_getitem(toeplitz_column, i, j):\n    """"""\n    Gets the (i,j)th entry of a symmetric Toeplitz matrix T.\n    Args:\n        - toeplitz_column (vector n) - column of symmetric Toeplitz matrix\n        - i (scalar) - row of entry to get\n        - j (scalar) - column of entry to get\n    Returns:\n        - T[i,j], where T is the Toeplitz matrix specified by c and r.\n    """"""\n    return toeplitz_getitem(toeplitz_column, toeplitz_column, i, j)\n\n\ndef toeplitz_matmul(toeplitz_column, toeplitz_row, tensor):\n    """"""\n    Performs multiplication T * M where the matrix T is Toeplitz.\n    Args:\n        - toeplitz_column (vector n or b x n) - First column of the Toeplitz matrix T.\n        - toeplitz_row (vector n or b x n) - First row of the Toeplitz matrix T.\n        - tensor (matrix n x p or b x n x p) - Matrix or vector to multiply the Toeplitz matrix with.\n    Returns:\n        - tensor (n x p or b x n x p) - The result of the matrix multiply T * M.\n    """"""\n    if toeplitz_column.size() != toeplitz_row.size():\n        raise RuntimeError(""c and r should have the same length (Toeplitz matrices are necessarily square)."")\n\n    toeplitz_shape = torch.Size((*toeplitz_column.shape, toeplitz_row.size(-1)))\n    output_shape = broadcasting._matmul_broadcast_shape(toeplitz_shape, tensor.shape)\n    broadcasted_t_shape = output_shape[:-1] if tensor.dim() > 1 else output_shape\n\n    if tensor.ndimension() == 1:\n        tensor = tensor.unsqueeze(-1)\n    toeplitz_column = toeplitz_column.expand(*broadcasted_t_shape).reshape(-1, toeplitz_column.size(-1))\n    toeplitz_row = toeplitz_row.expand(*broadcasted_t_shape).reshape(-1, toeplitz_row.size(-1))\n    tensor = tensor.expand(*output_shape).reshape(-1, *tensor.shape[-2:])\n\n    if not torch.equal(toeplitz_column[:, 0], toeplitz_row[:, 0]):\n        raise RuntimeError(\n            ""The first column and first row of the Toeplitz matrix should have ""\n            ""the same first element, otherwise the value of T[0,0] is ambiguous. ""\n            ""Got: c[0]={} and r[0]={}"".format(toeplitz_column[0], toeplitz_row[0])\n        )\n\n    if type(toeplitz_column) != type(toeplitz_row) or type(toeplitz_column) != type(tensor):\n        raise RuntimeError(""The types of all inputs to ToeplitzMV must match."")\n\n    batch_size, orig_size, num_rhs = tensor.size()\n    r_reverse = toeplitz_row[:, 1:].flip(dims=(1,))\n\n    c_r_rev = torch.zeros(batch_size, orig_size + r_reverse.size(1), dtype=tensor.dtype, device=tensor.device)\n    c_r_rev[:, :orig_size] = toeplitz_column\n    c_r_rev[:, orig_size:] = r_reverse\n\n    temp_tensor = torch.zeros(\n        batch_size, 2 * orig_size - 1, num_rhs, dtype=toeplitz_column.dtype, device=toeplitz_column.device\n    )\n    temp_tensor[:, :orig_size, :] = tensor\n\n    fft_M = fft.fft1(temp_tensor.transpose(1, 2).contiguous())\n    fft_c = fft.fft1(c_r_rev).unsqueeze(1).expand_as(fft_M)\n    fft_product = torch.zeros_like(fft_M)\n\n    fft_product[:, :, :, 0].addcmul_(fft_c[:, :, :, 0], fft_M[:, :, :, 0])\n    fft_product[:, :, :, 0].addcmul_(fft_c[:, :, :, 1], fft_M[:, :, :, 1], value=-1)\n    fft_product[:, :, :, 1].addcmul_(fft_c[:, :, :, 1], fft_M[:, :, :, 0])\n    fft_product[:, :, :, 1].addcmul_(fft_c[:, :, :, 0], fft_M[:, :, :, 1])\n\n    output = fft.ifft1(fft_product).transpose(1, 2)\n    output = output[:, :orig_size, :]\n\n    output = output.view(*output_shape)\n    return output\n\n\ndef sym_toeplitz_matmul(toeplitz_column, tensor):\n    """"""\n    Performs a matrix-matrix multiplication TM where the matrix T is symmetric Toeplitz.\n    Args:\n        - toeplitz_column (vector n) - First column of the symmetric Toeplitz matrix T.\n        - matrix (matrix n x p) - Matrix or vector to multiply the Toeplitz matrix with.\n    Returns:\n        - tensor\n    """"""\n    return toeplitz_matmul(toeplitz_column, toeplitz_column, tensor)\n\n\ndef sym_toeplitz_derivative_quadratic_form(left_vectors, right_vectors):\n    r""""""\n    Given a left vector v1 and a right vector v2, computes the quadratic form:\n                                v1\'*(dT/dc_i)*v2\n    for all i, where dT/dc_i is the derivative of the Toeplitz matrix with respect to\n    the ith element of its first column. Note that dT/dc_i is the same for any symmetric\n    Toeplitz matrix T, so we do not require it as an argument.\n\n    In particular, dT/dc_i is given by:\n                                [0 0; I_{m-i+1} 0] + [0 I_{m-i+1}; 0 0]\n    where I_{m-i+1} is the (m-i+1) dimensional identity matrix. In other words, dT/dc_i\n    for i=1..m is the matrix with ones on the ith sub- and superdiagonal.\n\n    Args:\n        - left_vectors (vector m or matrix s x m) - s left vectors u[j] in the quadratic form.\n        - right_vectors (vector m or matrix s x m) - s right vectors v[j] in the quadratic form.\n    Returns:\n        - vector m - a vector so that the ith element is the result of \\sum_j(u[j]*(dT/dc_i)*v[j])\n    """"""\n    if left_vectors.ndimension() == 1:\n        left_vectors = left_vectors.unsqueeze(1)\n        right_vectors = right_vectors.unsqueeze(1)\n\n    batch_shape = left_vectors.shape[:-2]\n    toeplitz_size = left_vectors.size(-2)\n    num_vectors = left_vectors.size(-1)\n\n    left_vectors = left_vectors.transpose(-1, -2).contiguous()\n    right_vectors = right_vectors.transpose(-1, -2).contiguous()\n\n    columns = torch.zeros_like(left_vectors)\n    columns[..., 0] = left_vectors[..., 0]\n    res = toeplitz_matmul(columns, left_vectors, right_vectors.unsqueeze(-1))\n    rows = left_vectors.flip(dims=(-1,))\n    columns[..., 0] = rows[..., 0]\n    res += toeplitz_matmul(columns, rows, torch.flip(right_vectors, dims=(-1,)).unsqueeze(-1))\n\n    res = res.reshape(*batch_shape, num_vectors, toeplitz_size).sum(-2)\n    res[..., 0] -= (left_vectors * right_vectors).view(*batch_shape, -1).sum(-1)\n\n    return res\n'"
gpytorch/utils/transforms.py,3,"b'#!/usr/bin/env python3\n\nimport torch\n\n\ndef inv_softplus(x):\n    return x + torch.log(-torch.expm1(-x))\n\n\ndef inv_sigmoid(x):\n    return torch.log(x) - torch.log(1 - x)\n\n\ndef _get_inv_param_transform(param_transform, inv_param_transform=None):\n    reg_inv_tf = TRANSFORM_REGISTRY.get(param_transform, None)\n    if reg_inv_tf is None:\n        if inv_param_transform is None:\n            raise RuntimeError(""Must specify inv_param_transform for custom param_transforms"")\n        return inv_param_transform\n    elif inv_param_transform is not None and reg_inv_tf != inv_param_transform:\n        raise RuntimeError(""TODO"")\n    return reg_inv_tf\n\n\nTRANSFORM_REGISTRY = {torch.exp: torch.log, torch.nn.functional.softplus: inv_softplus, torch.sigmoid: inv_sigmoid}\n'"
gpytorch/utils/warnings.py,2,"b'#!/usr/bin/env python3\n\n\nclass ExtraComputationWarning(UserWarning):\n    """"""\n    Warning thrown when a GP model does extra computation that it is not designed to do.\n    This is mostly designed for :obj:`~gpytorch.variational.UnwhitenedVariationalStrategy`, which\n    should cache most of its solves up front.\n    """"""\n\n    pass\n\n\nclass GPInputWarning(UserWarning):\n    """"""\n    Warning thrown when a GP model receives an unexpected input.\n    For example, when an :obj:`~gpytorch.models.ExactGP` in eval mode receives the training data as input.\n    """"""\n\n    pass\n\n\nclass NumericalWarning(RuntimeWarning):\n    """"""\n    Warning thrown when convergence criteria are not met, or when comptuations require extra stability.\n    """"""\n\n    pass\n\n\nclass OldVersionWarning(UserWarning):\n    """"""\n    Warning thrown when loading a saved model from an outdated version of GPyTorch.\n    """"""\n\n    pass\n'"
gpytorch/variational/__init__.py,0,"b'#!/usr/bin/env python3\n\nfrom ._variational_distribution import _VariationalDistribution\nfrom ._variational_strategy import _VariationalStrategy\nfrom .additive_grid_interpolation_variational_strategy import AdditiveGridInterpolationVariationalStrategy\nfrom .cholesky_variational_distribution import CholeskyVariationalDistribution\nfrom .delta_variational_distribution import DeltaVariationalDistribution\nfrom .grid_interpolation_variational_strategy import GridInterpolationVariationalStrategy\nfrom .mean_field_variational_distribution import MeanFieldVariationalDistribution\nfrom .multitask_variational_strategy import MultitaskVariationalStrategy\nfrom .orthogonally_decoupled_variational_strategy import OrthogonallyDecoupledVariationalStrategy\nfrom .unwhitened_variational_strategy import UnwhitenedVariationalStrategy\nfrom .variational_strategy import VariationalStrategy\nfrom .whitened_variational_strategy import WhitenedVariationalStrategy\n\n__all__ = [\n    ""_VariationalStrategy"",\n    ""AdditiveGridInterpolationVariationalStrategy"",\n    ""GridInterpolationVariationalStrategy"",\n    ""MultitaskVariationalStrategy"",\n    ""OrthogonallyDecoupledVariationalStrategy"",\n    ""VariationalStrategy"",\n    ""UnwhitenedVariationalStrategy"",\n    ""WhitenedVariationalStrategy"",\n    ""_VariationalDistribution"",\n    ""CholeskyVariationalDistribution"",\n    ""MeanFieldVariationalDistribution"",\n    ""DeltaVariationalDistribution"",\n]\n'"
gpytorch/variational/_variational_distribution.py,3,"b'#!/usr/bin/env python3\n\nimport warnings\nfrom abc import ABC, abstractmethod\n\nimport torch\n\nfrom ..module import Module\n\n\nclass _VariationalDistribution(Module, ABC):\n    r""""""\n    Abstract base class for all Variational Distributions.\n    """"""\n\n    def __init__(self, num_inducing_points, batch_shape=torch.Size([]), mean_init_std=1e-3):\n        super().__init__()\n        self.num_inducing_points = num_inducing_points\n        self.batch_shape = batch_shape\n        self.mean_init_std = mean_init_std\n\n    def forward(self):\n        r""""""\n        Constructs and returns the variational distribution\n\n        :rtype: :obj:`~gpytorch.distributions.MultivariateNormal`\n        :return: The distribution :math:q(\\mathbf u)""\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def initialize_variational_distribution(self, prior_dist):\n        r""""""\n        Method for initializing the variational distribution, based on the prior distribution.\n\n        :param ~gpytorch.distribution.Distribution prior_dist: The prior distribution :math:`p(\\mathbf u)`.\n        """"""\n        raise NotImplementedError\n\n    def __call__(self):\n        try:\n            return self.forward()\n        # Remove after 1.0\n        except NotImplementedError:\n            warnings.warn(\n                ""_VariationalDistribution.variational_distribution is deprecated. ""\n                ""Please implement a `forward` method instead."",\n                DeprecationWarning,\n            )\n            return self.variational_distribution\n\n    def __getattr__(self, attr):\n        # Remove after 1.0\n        if attr == ""variational_distribution"":\n            warnings.warn(\n                ""_VariationalDistribution.variational_distribution is deprecated. ""\n                ""To get q(u), call the _VariationalDistribution object instead."",\n                DeprecationWarning,\n            )\n            return self.forward()\n        else:\n            return super().__getattr__(attr)\n'"
gpytorch/variational/_variational_strategy.py,12,"b'#!/usr/bin/env python3\n\nfrom abc import ABC, abstractproperty\n\nimport torch\n\nfrom .. import settings\nfrom ..distributions import Delta, MultivariateNormal\nfrom ..module import Module\nfrom ..utils.broadcasting import _mul_broadcast_shape\nfrom ..utils.memoize import cached\n\n\nclass _VariationalStrategy(Module, ABC):\n    """"""\n    Abstract base class for all Variational Strategies.\n    """"""\n\n    def __init__(self, model, inducing_points, variational_distribution, learn_inducing_locations=True):\n        super().__init__()\n\n        # Model\n        object.__setattr__(self, ""model"", model)\n\n        # Inducing points\n        inducing_points = inducing_points.clone()\n        if inducing_points.dim() == 1:\n            inducing_points = inducing_points.unsqueeze(-1)\n        if learn_inducing_locations:\n            self.register_parameter(name=""inducing_points"", parameter=torch.nn.Parameter(inducing_points))\n        else:\n            self.register_buffer(""inducing_points"", inducing_points)\n\n        # Variational distribution\n        self._variational_distribution = variational_distribution\n        self.register_buffer(""variational_params_initialized"", torch.tensor(0))\n\n    @abstractproperty\n    @cached(name=""prior_distribution_memo"")\n    def prior_distribution(self):\n        r""""""\n        The :func:`~gpytorch.variational.VariationalStrategy.prior_distribution` method determines how to compute the\n        GP prior distribution of the inducing points, e.g. :math:`p(u) \\sim N(\\mu(X_u), K(X_u, X_u))`. Most commonly,\n        this is done simply by calling the user defined GP prior on the inducing point data directly.\n\n        :rtype: :obj:`~gpytorch.distributions.MultivariateNormal`\n        :return: The distribution :math:`p( \\mathbf u)`\n        """"""\n        raise NotImplementedError\n\n    @property\n    @cached(name=""variational_distribution_memo"")\n    def variational_distribution(self):\n        return self._variational_distribution()\n\n    def forward(self, x, inducing_points, inducing_values, variational_inducing_covar=None):\n        r""""""\n        The :func:`~gpytorch.variational.VariationalStrategy.forward` method determines how to marginalize out the\n        inducing point function values. Specifically, forward defines how to transform a variational distribution\n        over the inducing point values, :math:`q(u)`, in to a variational distribution over the function values at\n        specified locations x, :math:`q(f|x)`, by integrating :math:`\\int p(f|x, u)q(u)du`\n\n        :param torch.Tensor x: Locations :math:`\\mathbf X` to get the\n            variational posterior of the function values at.\n        :param torch.Tensor inducing_points: Locations :math:`\\mathbf Z` of the inducing points\n        :param torch.Tensor inducing_values: Samples of the inducing function values :math:`\\mathbf u`\n            (or the mean of the distribution :math:`q(\\mathbf u)` if q is a Gaussian.\n        :param ~gpytorch.lazy.LazyTensor variational_inducing_covar: If the distribuiton :math:`q(\\mathbf u)`\n            is Gaussian, then this variable is the covariance matrix of that Gaussian. Otherwise, it will be\n            :attr:`None`.\n\n        :rtype: :obj:`~gpytorch.distributions.MultivariateNormal`\n        :return: The distribution :math:`q( \\mathbf f(\\mathbf X))`\n        """"""\n        raise NotImplementedError\n\n    def kl_divergence(self):\n        r""""""\n        Compute the KL divergence between the variational inducing distribution :math:`q(\\mathbf u)`\n        and the prior inducing distribution :math:`p(\\mathbf u)`.\n\n        :rtype: torch.Tensor\n        """"""\n        with settings.max_preconditioner_size(0):\n            kl_divergence = torch.distributions.kl.kl_divergence(self.variational_distribution, self.prior_distribution)\n        return kl_divergence\n\n    def train(self, mode=True):\n        # Make sure we are clearing the cache if we change modes\n        if (self.training and not mode) or mode:\n            if hasattr(self, ""_memoize_cache""):\n                delattr(self, ""_memoize_cache"")\n        return super().train(mode=mode)\n\n    def __call__(self, x, prior=False):\n        # If we\'re in prior mode, then we\'re done!\n        if prior:\n            return self.model.forward(x)\n\n        # Delete previously cached items from the training distribution\n        if self.training:\n            if hasattr(self, ""_memoize_cache""):\n                delattr(self, ""_memoize_cache"")\n                self._memoize_cache = dict()\n        # (Maybe) initialize variational distribution\n        if not self.variational_params_initialized.item():\n            prior_dist = self.prior_distribution\n            self._variational_distribution.initialize_variational_distribution(prior_dist)\n            self.variational_params_initialized.fill_(1)\n\n        # Ensure inducing_points and x are the same size\n        inducing_points = self.inducing_points\n        if inducing_points.shape[:-2] != x.shape[:-2]:\n            batch_shape = _mul_broadcast_shape(inducing_points.shape[:-2], x.shape[:-2])\n            inducing_points = inducing_points.expand(*batch_shape, *inducing_points.shape[-2:])\n            x = x.expand(*batch_shape, *x.shape[-2:])\n\n        # Get p(u)/q(u)\n        variational_dist_u = self.variational_distribution\n\n        # Get q(f)\n        if isinstance(variational_dist_u, MultivariateNormal):\n            return super().__call__(\n                x,\n                inducing_points,\n                inducing_values=variational_dist_u.mean,\n                variational_inducing_covar=variational_dist_u.lazy_covariance_matrix,\n            )\n        elif isinstance(variational_dist_u, Delta):\n            return super().__call__(\n                x, inducing_points, inducing_values=variational_dist_u.mean, variational_inducing_covar=None\n            )\n        else:\n            raise RuntimeError(\n                f""Invalid variational distribuition ({type(variational_dist_u)}). ""\n                ""Expected a multivariate normal or a delta distribution.""\n            )\n'"
gpytorch/variational/additive_grid_interpolation_variational_strategy.py,1,"b'#!/usr/bin/env python3\n\n\nimport torch\n\nfrom ..distributions import Delta, MultivariateNormal\nfrom ..variational.grid_interpolation_variational_strategy import GridInterpolationVariationalStrategy\n\n\nclass AdditiveGridInterpolationVariationalStrategy(GridInterpolationVariationalStrategy):\n    def __init__(\n        self, model, grid_size, grid_bounds, num_dim, variational_distribution, mixing_params=False, sum_output=True\n    ):\n        super(AdditiveGridInterpolationVariationalStrategy, self).__init__(\n            model, grid_size, grid_bounds, variational_distribution\n        )\n        self.num_dim = num_dim\n        self.sum_output = sum_output\n        # Mixing parameters\n        if mixing_params:\n            self.register_parameter(name=""mixing_params"", parameter=torch.nn.Parameter(torch.ones(num_dim) / num_dim))\n\n    @property\n    def prior_distribution(self):\n        """"""\n        If desired, models can compare the input to forward to inducing_points and use a GridKernel for space\n        efficiency.\n\n        However, when using a default VariationalDistribution which has an O(m^2) space complexity anyways, we find that\n        GridKernel is typically not worth it due to the moderate slow down of using FFTs.\n        """"""\n        out = super(AdditiveGridInterpolationVariationalStrategy, self).prior_distribution\n        mean = out.mean.repeat(self.num_dim, 1)\n        covar = out.lazy_covariance_matrix.repeat(self.num_dim, 1, 1)\n        return MultivariateNormal(mean, covar)\n\n    def _compute_grid(self, inputs):\n        num_data, num_dim = inputs.size()\n        inputs = inputs.transpose(0, 1).reshape(-1, 1)\n        interp_indices, interp_values = super(AdditiveGridInterpolationVariationalStrategy, self)._compute_grid(inputs)\n        interp_indices = interp_indices.view(num_dim, num_data, -1)\n        interp_values = interp_values.view(num_dim, num_data, -1)\n\n        if hasattr(self, ""mixing_params""):\n            interp_values = interp_values.mul(self.mixing_params.unsqueeze(1).unsqueeze(2))\n        return interp_indices, interp_values\n\n    def forward(self, x, inducing_points, inducing_values, variational_inducing_covar=None):\n        if x.ndimension() == 1:\n            x = x.unsqueeze(-1)\n        elif x.ndimension() != 2:\n            raise RuntimeError(""AdditiveGridInterpolationVariationalStrategy expects a 2d tensor."")\n\n        num_data, num_dim = x.size()\n        if num_dim != self.num_dim:\n            raise RuntimeError(""The number of dims should match the number specified."")\n\n        output = super().forward(x, inducing_points, inducing_values, variational_inducing_covar)\n        if self.sum_output:\n            if variational_inducing_covar is not None:\n                mean = output.mean.sum(0)\n                covar = output.lazy_covariance_matrix.sum(-3)\n                return MultivariateNormal(mean, covar)\n            else:\n                return Delta(output.mean.sum(0))\n        else:\n            return output\n'"
gpytorch/variational/cholesky_variational_distribution.py,9,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..distributions import MultivariateNormal\nfrom ..lazy import CholLazyTensor\nfrom ._variational_distribution import _VariationalDistribution\n\n\nclass CholeskyVariationalDistribution(_VariationalDistribution):\n    """"""\n    A :obj:`~gpytorch.variational._VariationalDistribution` that is defined to be a multivariate normal distribution\n    with a full covariance matrix.\n\n    The most common way this distribution is defined is to parameterize it in terms of a mean vector and a covariance\n    matrix. In order to ensure that the covariance matrix remains positive definite, we only consider the lower\n    triangle.\n\n    :param int num_inducing_points: Size of the variational distribution. This implies that the variational mean\n        should be this size, and the variational covariance matrix should have this many rows and columns.\n    :param torch.Size batch_shape: (Optional.) Specifies an optional batch size\n        for the variational parameters. This is useful for example when doing additive variational inference.\n    :param float mean_init_std: (default=1e-3) Standard deviation of gaussian noise to add to the mean initialization.\n    """"""\n\n    def __init__(self, num_inducing_points, batch_shape=torch.Size([]), mean_init_std=1e-3, **kwargs):\n        super().__init__(num_inducing_points=num_inducing_points, batch_shape=batch_shape, mean_init_std=mean_init_std)\n        mean_init = torch.zeros(num_inducing_points)\n        covar_init = torch.eye(num_inducing_points, num_inducing_points)\n        mean_init = mean_init.repeat(*batch_shape, 1)\n        covar_init = covar_init.repeat(*batch_shape, 1, 1)\n\n        self.register_parameter(name=""variational_mean"", parameter=torch.nn.Parameter(mean_init))\n        self.register_parameter(name=""chol_variational_covar"", parameter=torch.nn.Parameter(covar_init))\n\n    def forward(self):\n        chol_variational_covar = self.chol_variational_covar\n        dtype = chol_variational_covar.dtype\n        device = chol_variational_covar.device\n\n        # First make the cholesky factor is upper triangular\n        lower_mask = torch.ones(self.chol_variational_covar.shape[-2:], dtype=dtype, device=device).tril(0)\n        chol_variational_covar = chol_variational_covar.mul(lower_mask)\n\n        # Now construct the actual matrix\n        variational_covar = CholLazyTensor(chol_variational_covar)\n        return MultivariateNormal(self.variational_mean, variational_covar)\n\n    def initialize_variational_distribution(self, prior_dist):\n        self.variational_mean.data.copy_(prior_dist.mean)\n        self.variational_mean.data.add_(torch.randn_like(prior_dist.mean), alpha=self.mean_init_std)\n        self.chol_variational_covar.data.copy_(prior_dist.lazy_covariance_matrix.cholesky().evaluate())\n'"
gpytorch/variational/delta_variational_distribution.py,6,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..distributions import Delta\nfrom ._variational_distribution import _VariationalDistribution\n\n\nclass DeltaVariationalDistribution(_VariationalDistribution):\n    """"""\n    This :obj:`~gpytorch.variational._VariationalDistribution` object replaces a variational distribution\n    with a single particle. It is equivalent to doing MAP inference.\n\n    :param int num_inducing_points: Size of the variational distribution. This implies that the variational mean\n        should be this size.\n    :param torch.Size batch_shape: (Optional.) Specifies an optional batch size\n        for the variational parameters. This is useful for example when doing additive variational inference.\n    :param float mean_init_std: (default=1e-3) Standard deviation of gaussian noise to add to the mean initialization.\n    """"""\n\n    def __init__(self, num_inducing_points, batch_shape=torch.Size([]), mean_init_std=1e-3, **kwargs):\n        super().__init__(num_inducing_points=num_inducing_points, batch_shape=batch_shape, mean_init_std=mean_init_std)\n        mean_init = torch.zeros(num_inducing_points)\n        mean_init = mean_init.repeat(*batch_shape, 1)\n        self.register_parameter(name=""variational_mean"", parameter=torch.nn.Parameter(mean_init))\n\n    def forward(self):\n        return Delta(self.variational_mean)\n\n    def initialize_variational_distribution(self, prior_dist):\n        self.variational_mean.data.copy_(prior_dist.mean)\n        self.variational_mean.data.add_(torch.randn_like(prior_dist.mean), alpha=self.mean_init_std)\n'"
gpytorch/variational/grid_interpolation_variational_strategy.py,5,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..distributions import MultivariateNormal\nfrom ..lazy import InterpolatedLazyTensor\nfrom ..utils.broadcasting import _mul_broadcast_shape\nfrom ..utils.interpolation import Interpolation, left_interp\nfrom ..utils.memoize import cached\nfrom ._variational_strategy import _VariationalStrategy\n\n\nclass GridInterpolationVariationalStrategy(_VariationalStrategy):\n    r""""""\n    This strategy constrains the inducing points to a grid and applies a deterministic\n    relationship between :math:`\\mathbf f` and :math:`\\mathbf u`.\n    It was introduced by `Wilson et al. (2016)`_.\n\n    Here, the inducing points are not learned. Instead, the strategy\n    automatically creates inducing points based on a set of grid sizes and grid\n    bounds.\n\n    .. _Wilson et al. (2016):\n        https://arxiv.org/abs/1611.00336\n\n    :param ~gpytorch.models.ApproximateGP model: Model this strategy is applied to.\n        Typically passed in when the VariationalStrategy is created in the\n        __init__ method of the user defined model.\n    :param int grid_size: Size of the grid\n    :param list grid_bounds: Bounds of each dimension of the grid (should be a list of (float, float) tuples)\n    :param ~gpytorch.variational.VariationalDistribution variational_distribution: A\n        VariationalDistribution object that represents the form of the variational distribution :math:`q(\\mathbf u)`\n    """"""\n\n    def __init__(self, model, grid_size, grid_bounds, variational_distribution):\n        grid = torch.zeros(grid_size, len(grid_bounds))\n        for i in range(len(grid_bounds)):\n            grid_diff = float(grid_bounds[i][1] - grid_bounds[i][0]) / (grid_size - 2)\n            grid[:, i] = torch.linspace(grid_bounds[i][0] - grid_diff, grid_bounds[i][1] + grid_diff, grid_size)\n\n        inducing_points = torch.zeros(int(pow(grid_size, len(grid_bounds))), len(grid_bounds))\n        prev_points = None\n        for i in range(len(grid_bounds)):\n            for j in range(grid_size):\n                inducing_points[j * grid_size ** i : (j + 1) * grid_size ** i, i].fill_(grid[j, i])\n                if prev_points is not None:\n                    inducing_points[j * grid_size ** i : (j + 1) * grid_size ** i, :i].copy_(prev_points)\n            prev_points = inducing_points[: grid_size ** (i + 1), : (i + 1)]\n\n        super(GridInterpolationVariationalStrategy, self).__init__(\n            model, inducing_points, variational_distribution, learn_inducing_locations=False\n        )\n        object.__setattr__(self, ""model"", model)\n\n        self.register_buffer(""grid"", grid)\n\n    def _compute_grid(self, inputs):\n        n_data, n_dimensions = inputs.size(-2), inputs.size(-1)\n        batch_shape = inputs.shape[:-2]\n\n        inputs = inputs.reshape(-1, n_dimensions)\n        interp_indices, interp_values = Interpolation().interpolate(self.grid, inputs)\n        interp_indices = interp_indices.view(*batch_shape, n_data, -1)\n        interp_values = interp_values.view(*batch_shape, n_data, -1)\n\n        if (interp_indices.dim() - 2) != len(self._variational_distribution.batch_shape):\n            batch_shape = _mul_broadcast_shape(interp_indices.shape[:-2], self._variational_distribution.batch_shape)\n            interp_indices = interp_indices.expand(*batch_shape, *interp_indices.shape[-2:])\n            interp_values = interp_values.expand(*batch_shape, *interp_values.shape[-2:])\n        return interp_indices, interp_values\n\n    @property\n    @cached(name=""prior_distribution_memo"")\n    def prior_distribution(self):\n        out = self.model.forward(self.inducing_points)\n        res = MultivariateNormal(out.mean, out.lazy_covariance_matrix.add_jitter())\n        return res\n\n    def forward(self, x, inducing_points, inducing_values, variational_inducing_covar=None):\n        if variational_inducing_covar is None:\n            raise RuntimeError(\n                ""GridInterpolationVariationalStrategy is only compatible with Gaussian variational ""\n                f""distributions. Got ({self.variational_distribution.__class__.__name__}.""\n            )\n\n        variational_distribution = self.variational_distribution\n\n        # Get interpolations\n        interp_indices, interp_values = self._compute_grid(x)\n\n        # Compute test mean\n        # Left multiply samples by interpolation matrix\n        predictive_mean = left_interp(interp_indices, interp_values, inducing_values.unsqueeze(-1))\n        predictive_mean = predictive_mean.squeeze(-1)\n\n        # Compute test covar\n        predictive_covar = InterpolatedLazyTensor(\n            variational_distribution.lazy_covariance_matrix,\n            interp_indices,\n            interp_values,\n            interp_indices,\n            interp_values,\n        )\n        output = MultivariateNormal(predictive_mean, predictive_covar)\n        return output\n'"
gpytorch/variational/mean_field_variational_distribution.py,11,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..distributions import MultivariateNormal\nfrom ..lazy import DiagLazyTensor\nfrom ._variational_distribution import _VariationalDistribution\n\n\nclass MeanFieldVariationalDistribution(_VariationalDistribution):\n    """"""\n    A :obj:`~gpytorch.variational._VariationalDistribution` that is defined to be a multivariate normal distribution\n    with a diagonal covariance matrix. This will not be as flexible/expressive as a\n    :obj:`~gpytorch.variational.CholeskyVariationalDistribution`.\n\n    :param int num_inducing_points: Size of the variational distribution. This implies that the variational mean\n        should be this size, and the variational covariance matrix should have this many rows and columns.\n    :param torch.Size batch_shape: (Optional.) Specifies an optional batch size\n        for the variational parameters. This is useful for example when doing additive variational inference.\n    :param float mean_init_std: (default=1e-3) Standard deviation of gaussian noise to add to the mean initialization.\n    """"""\n\n    def __init__(self, num_inducing_points, batch_shape=torch.Size([]), mean_init_std=1e-3, **kwargs):\n        super().__init__(num_inducing_points=num_inducing_points, batch_shape=batch_shape, mean_init_std=mean_init_std)\n        mean_init = torch.zeros(num_inducing_points)\n        covar_init = torch.ones(num_inducing_points)\n        mean_init = mean_init.repeat(*batch_shape, 1)\n        covar_init = covar_init.repeat(*batch_shape, 1)\n\n        self.register_parameter(name=""variational_mean"", parameter=torch.nn.Parameter(mean_init))\n        self.register_parameter(name=""_variational_stddev"", parameter=torch.nn.Parameter(covar_init))\n\n    @property\n    def variational_stddev(self):\n        # TODO: if we don\'t multiply self._variational_stddev by a mask of one, Pyro models fail\n        # not sure where this bug is occuring (in Pyro or PyTorch)\n        # throwing this in as a hotfix for now - we should investigate later\n        mask = torch.ones_like(self._variational_stddev)\n        return self._variational_stddev.mul(mask).abs().clamp_min(1e-8)\n\n    def forward(self):\n        # TODO: if we don\'t multiply self._variational_stddev by a mask of one, Pyro models fail\n        # not sure where this bug is occuring (in Pyro or PyTorch)\n        # throwing this in as a hotfix for now - we should investigate later\n        mask = torch.ones_like(self._variational_stddev)\n        variational_covar = DiagLazyTensor(self._variational_stddev.mul(mask).pow(2))\n        return MultivariateNormal(self.variational_mean, variational_covar)\n\n    def initialize_variational_distribution(self, prior_dist):\n        self.variational_mean.data.copy_(prior_dist.mean)\n        self.variational_mean.data.add_(torch.randn_like(prior_dist.mean), alpha=self.mean_init_std)\n        self._variational_stddev.data.copy_(prior_dist.stddev)\n'"
gpytorch/variational/multitask_variational_strategy.py,3,"b'#!/usr/bin/env python3\n\nfrom ..distributions import MultitaskMultivariateNormal\nfrom ..module import Module\nfrom ._variational_strategy import _VariationalStrategy\n\n\nclass MultitaskVariationalStrategy(_VariationalStrategy):\n    """"""\n    MultitaskVariationalStrategy wraps an existing :obj:`~gpytorch.variational.VariationalStrategy`\n    to product a :obj:`~gpytorch.variational.MultitaskMultivariateNormal` distribution.\n    This is useful for multi-output variational models.\n\n    The base variational strategy is assumed to operate on a batch of GPs. One of the batch\n    dimensions corresponds to the multiple tasks.\n\n    :param ~gpytorch.variational.VariationalStrategy base_variational_strategy: Base variational strategy\n    :param int task_dim: (default=-1) Which batch dimension is the task dimension\n    """"""\n\n    def __init__(self, base_variational_strategy, num_tasks, task_dim=-1):\n        Module.__init__(self)\n        self.base_variational_strategy = base_variational_strategy\n        self.task_dim = task_dim\n        self.num_tasks = num_tasks\n\n    @property\n    def prior_distribution(self):\n        return self.base_variational_strategy.prior_distribution\n\n    @property\n    def variational_distribution(self):\n        return self.base_variational_strategy.variational_distribution\n\n    @property\n    def variational_params_initialized(self):\n        return self.base_variational_strategy.variational_params_initialized\n\n    def kl_divergence(self):\n        return super().kl_divergence().sum(dim=-1)\n\n    def __call__(self, x, prior=False):\n        function_dist = self.base_variational_strategy(x, prior=prior)\n        if (\n            self.task_dim > 0\n            and self.task_dim > len(function_dist.batch_shape)\n            or self.task_dim < 0\n            and self.task_dim + len(function_dist.batch_shape) < 0\n        ):\n            return MultitaskMultivariateNormal.from_repeated_mvn(function_dist, num_tasks=self.num_tasks)\n        else:\n            function_dist = MultitaskMultivariateNormal.from_batch_mvn(function_dist, task_dim=self.task_dim)\n            assert function_dist.event_shape[-1] == self.num_tasks\n            return function_dist\n'"
gpytorch/variational/orthogonally_decoupled_variational_strategy.py,9,"b'#!/usr/bin/env python3\n\nimport torch\n\nfrom ..distributions import MultivariateNormal\nfrom ..utils.memoize import cached\nfrom ._variational_strategy import _VariationalStrategy\nfrom .delta_variational_distribution import DeltaVariationalDistribution\n\n\nclass OrthogonallyDecoupledVariationalStrategy(_VariationalStrategy):\n    r""""""\n    Implements orthogonally decoupled VGPs as defined in `Salimbeni et al. (2018)`_.\n    This variational strategy uses a different set of inducing points for the mean and covariance functions.\n    The idea is to use more inducing points for the (computationally efficient) mean and fewer inducing points for the\n    (computationally expensive) covaraince.\n\n    This variational strategy defines the inducing points/:obj:`~gpytorch.variational._VariationalDistribution`\n    for the mean function.\n    It then wraps a different :obj:`~gpytorch.variational._VariationalStrategy` which\n    defines the covariance inducing points.\n\n    Example:\n        >>> mean_inducing_points = torch.randn(1000, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n        >>> covar_inducing_points = torch.randn(100, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n        >>>\n        >>> covar_variational_strategy = gpytorch.variational.VariationalStrategy(\n        >>>     model, covar_inducing_points,\n        >>>     gpytorch.variational.CholeskyVariationalDistribution(covar_inducing_points.size(-2)),\n        >>>     learn_inducing_locations=True\n        >>> )\n        >>>\n        >>> variational_strategy = gpytorch.variational.OrthogonallyDecoupledVariationalStrategy(\n        >>>     covar_variational_strategy, mean_inducing_points,\n        >>>     gpytorch.variational.DeltaVariationalDistribution(mean_inducing_points.size(-2)),\n        >>> )\n\n    .. _Salimbeni et al. (2018):\n        https://arxiv.org/abs/1809.08820\n    """"""\n\n    def __init__(self, model, inducing_points, variational_distribution):\n        if not isinstance(variational_distribution, DeltaVariationalDistribution):\n            raise NotImplementedError(\n                ""OrthogonallyDecoupledVariationalStrategy currently works with DeltaVariationalDistribution""\n            )\n\n        super().__init__(model, inducing_points, variational_distribution, learn_inducing_locations=True)\n        self.base_variational_strategy = model\n\n    @property\n    @cached(name=""prior_distribution_memo"")\n    def prior_distribution(self):\n        out = self.model(self.inducing_points)\n        res = MultivariateNormal(out.mean, out.lazy_covariance_matrix.add_jitter())\n        return res\n\n    def forward(self, x, inducing_points, inducing_values, variational_inducing_covar=None):\n        if variational_inducing_covar is not None:\n            raise NotImplementedError(\n                ""OrthogonallyDecoupledVariationalStrategy currently works with DeltaVariationalDistribution""\n            )\n\n        num_data = x.size(-2)\n        full_output = self.model(torch.cat([x, inducing_points], dim=-2))\n        full_mean = full_output.mean\n        full_covar = full_output.lazy_covariance_matrix\n\n        if self.training:\n            induc_mean = full_mean[..., num_data:]\n            induc_induc_covar = full_covar[..., num_data:, num_data:]\n            self._memoize_cache[""prior_distribution_memo""] = MultivariateNormal(induc_mean, induc_induc_covar)\n\n        test_mean = full_mean[..., :num_data]\n        data_induc_covar = full_covar[..., :num_data, num_data:]\n        predictive_mean = (data_induc_covar @ inducing_values.unsqueeze(-1)).squeeze(-1).add(test_mean)\n        predictive_covar = full_covar[..., :num_data, :num_data]\n\n        # Return the distribution\n        return MultivariateNormal(predictive_mean, predictive_covar)\n\n    def kl_divergence(self):\n        mean = self.variational_distribution.mean\n        induc_induc_covar = self.prior_distribution.lazy_covariance_matrix\n        kl = self.model.kl_divergence() + ((induc_induc_covar @ mean.unsqueeze(-1)).squeeze(-1) * mean).sum(-1).mul(0.5)\n        return kl\n'"
gpytorch/variational/unwhitened_variational_strategy.py,15,"b'#!/usr/bin/env python3\n\nimport math\n\nimport torch\n\nfrom .. import settings\nfrom ..distributions import MultivariateNormal\nfrom ..lazy import (\n    CachedCGLazyTensor,\n    CholLazyTensor,\n    DiagLazyTensor,\n    PsdSumLazyTensor,\n    RootLazyTensor,\n    ZeroLazyTensor,\n    delazify,\n)\nfrom ..utils.broadcasting import _mul_broadcast_shape\nfrom ..utils.cholesky import psd_safe_cholesky\nfrom ..utils.memoize import cached\nfrom ._variational_strategy import _VariationalStrategy\n\n\nclass UnwhitenedVariationalStrategy(_VariationalStrategy):\n    r""""""\n    Similar to :obj:`~gpytorch.variational.VariationalStrategy`, but does not perform the\n    whitening operation. In almost all cases :obj:`~gpytorch.variational.VariationalStrategy`\n    is preferable, with a few exceptions:\n\n    - When the inducing points are exactly equal to the training points (i.e. :math:`\\mathbf Z = \\mathbf X`).\n      Unwhitened models are faster in this case.\n\n    - When the number of inducing points is very large (e.g. >2000). Unwhitened models can use CG for faster\n      computation.\n\n    :param ~gpytorch.models.ApproximateGP model: Model this strategy is applied to.\n        Typically passed in when the VariationalStrategy is created in the\n        __init__ method of the user defined model.\n    :param torch.Tensor inducing_points: Tensor containing a set of inducing\n        points to use for variational inference.\n    :param ~gpytorch.variational.VariationalDistribution variational_distribution: A\n        VariationalDistribution object that represents the form of the variational distribution :math:`q(\\mathbf u)`\n    :param bool learn_inducing_points: (optional, default True): Whether or not\n        the inducing point locations :math:`\\mathbf Z` should be learned (i.e. are they\n        parameters of the model).\n    """"""\n\n    @cached(name=""cholesky_factor"")\n    def _cholesky_factor(self, induc_induc_covar):\n        # Maybe used - if we\'re not using CG\n        L = psd_safe_cholesky(delazify(induc_induc_covar), jitter=settings.cholesky_jitter.value())\n        return L\n\n    @property\n    @cached(name=""prior_distribution_memo"")\n    def prior_distribution(self):\n        out = self.model.forward(self.inducing_points)\n        res = MultivariateNormal(out.mean, out.lazy_covariance_matrix.add_jitter())\n        return res\n\n    def forward(self, x, inducing_points, inducing_values, variational_inducing_covar=None):\n        # If our points equal the inducing points, we\'re done\n        if torch.equal(x, inducing_points):\n            if variational_inducing_covar is None:\n                raise RuntimeError\n            else:\n                return MultivariateNormal(inducing_values, variational_inducing_covar)\n\n        # Otherwise, we have to marginalize\n        num_induc = inducing_points.size(-2)\n        full_inputs = torch.cat([inducing_points, x], dim=-2)\n        full_output = self.model.forward(full_inputs)\n        full_mean, full_covar = full_output.mean, full_output.lazy_covariance_matrix\n\n        # Mean terms\n        test_mean = full_mean[..., num_induc:]\n        induc_mean = full_mean[..., :num_induc]\n        mean_diff = (inducing_values - induc_mean).unsqueeze(-1)\n\n        # Covariance terms\n        induc_induc_covar = full_covar[..., :num_induc, :num_induc].add_jitter()\n        induc_data_covar = full_covar[..., :num_induc, num_induc:].evaluate()\n        data_data_covar = full_covar[..., num_induc:, num_induc:]\n\n        # If we\'re less than a certain size, we\'ll compute the Cholesky decomposition of induc_induc_covar\n        cholesky = False\n        if settings.fast_computations.log_prob.off() or (num_induc <= settings.max_cholesky_size.value()):\n            induc_induc_covar = CholLazyTensor(self._cholesky_factor(induc_induc_covar))\n            cholesky = True\n\n        # If we are making predictions and don\'t need variances, we can do things very quickly.\n        if not self.training and settings.skip_posterior_variances.on():\n            if not hasattr(self, ""_mean_cache""):\n                # For now: run variational inference without a preconditioner\n                # The preconditioner screws things up for some reason\n                with settings.max_preconditioner_size(0):\n                    self._mean_cache = induc_induc_covar.inv_matmul(mean_diff).detach()\n            predictive_mean = torch.add(\n                test_mean, induc_data_covar.transpose(-2, -1).matmul(self._mean_cache).squeeze(-1)\n            )\n            predictive_covar = ZeroLazyTensor(test_mean.size(-1), test_mean.size(-1))\n            return MultivariateNormal(predictive_mean, predictive_covar)\n\n        # Expand everything to the right size\n        shapes = [mean_diff.shape[:-1], induc_data_covar.shape[:-1], induc_induc_covar.shape[:-1]]\n        if variational_inducing_covar is not None:\n            root_variational_covar = variational_inducing_covar.root_decomposition().root.evaluate()\n            shapes.append(root_variational_covar.shape[:-1])\n        shape = _mul_broadcast_shape(*shapes)\n        mean_diff = mean_diff.expand(*shape, mean_diff.size(-1))\n        induc_data_covar = induc_data_covar.expand(*shape, induc_data_covar.size(-1))\n        induc_induc_covar = induc_induc_covar.expand(*shape, induc_induc_covar.size(-1))\n        if variational_inducing_covar is not None:\n            root_variational_covar = root_variational_covar.expand(*shape, root_variational_covar.size(-1))\n\n        # Cache the CG results\n        # For now: run variational inference without a preconditioner\n        # The preconditioner screws things up for some reason\n        with settings.max_preconditioner_size(0):\n            # Cache the CG results\n            if variational_inducing_covar is None:\n                left_tensors = mean_diff\n            else:\n                left_tensors = torch.cat([mean_diff, root_variational_covar], -1)\n\n            with torch.no_grad():\n                eager_rhs = torch.cat([left_tensors, induc_data_covar], -1)\n                solve, probe_vecs, probe_vec_norms, probe_vec_solves, tmats = CachedCGLazyTensor.precompute_terms(\n                    induc_induc_covar,\n                    eager_rhs.detach(),\n                    logdet_terms=(not cholesky),\n                    include_tmats=(not settings.skip_logdet_forward.on() and not cholesky),\n                )\n                eager_rhss = [\n                    eager_rhs.detach(),\n                    eager_rhs[..., left_tensors.size(-1) :].detach(),\n                    eager_rhs[..., : left_tensors.size(-1)].detach(),\n                ]\n                solves = [\n                    solve.detach(),\n                    solve[..., left_tensors.size(-1) :].detach(),\n                    solve[..., : left_tensors.size(-1)].detach(),\n                ]\n                if settings.skip_logdet_forward.on():\n                    eager_rhss.append(torch.cat([probe_vecs, left_tensors], -1))\n                    solves.append(torch.cat([probe_vec_solves, solve[..., : left_tensors.size(-1)]], -1))\n            induc_induc_covar = CachedCGLazyTensor(\n                induc_induc_covar,\n                eager_rhss=eager_rhss,\n                solves=solves,\n                probe_vectors=probe_vecs,\n                probe_vector_norms=probe_vec_norms,\n                probe_vector_solves=probe_vec_solves,\n                probe_vector_tmats=tmats,\n            )\n\n        # Cache the kernel matrix with the cached CG calls\n        if self.training:\n            self._memoize_cache[""prior_distribution_memo""] = MultivariateNormal(induc_mean, induc_induc_covar)\n\n        # Compute predictive mean\n        inv_products = induc_induc_covar.inv_matmul(induc_data_covar, left_tensors.transpose(-1, -2))\n        predictive_mean = torch.add(test_mean, inv_products[..., 0, :])\n\n        # Compute covariance\n        if self.training:\n            interp_data_data_var, _ = induc_induc_covar.inv_quad_logdet(\n                induc_data_covar, logdet=False, reduce_inv_quad=False\n            )\n            data_covariance = DiagLazyTensor((data_data_covar.diag() - interp_data_data_var).clamp(0, math.inf))\n        else:\n            neg_induc_data_data_covar = torch.matmul(\n                induc_data_covar.transpose(-1, -2).mul(-1), induc_induc_covar.inv_matmul(induc_data_covar)\n            )\n            data_covariance = data_data_covar + neg_induc_data_data_covar\n        predictive_covar = PsdSumLazyTensor(RootLazyTensor(inv_products[..., 1:, :].transpose(-1, -2)), data_covariance)\n\n        # Done!\n        return MultivariateNormal(predictive_mean, predictive_covar)\n'"
gpytorch/variational/variational_strategy.py,13,"b'#!/usr/bin/env python3\n\nimport warnings\n\nimport torch\n\nfrom .. import settings\nfrom ..distributions import MultivariateNormal\nfrom ..lazy import DiagLazyTensor, MatmulLazyTensor, RootLazyTensor, SumLazyTensor, delazify\nfrom ..settings import trace_mode\nfrom ..utils.cholesky import psd_safe_cholesky\nfrom ..utils.memoize import cached\nfrom ..utils.warnings import OldVersionWarning\nfrom ._variational_strategy import _VariationalStrategy\n\n\ndef _ensure_updated_strategy_flag_set(\n    state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n):\n    device = state_dict[list(state_dict.keys())[0]].device\n    if prefix + ""updated_strategy"" not in state_dict:\n        state_dict[prefix + ""updated_strategy""] = torch.tensor(False, device=device)\n        warnings.warn(\n            ""You have loaded a variational GP model (using `VariationalStrategy`) from a previous version of ""\n            ""GPyTorch. We have updated the parameters of your model to work with the new version of ""\n            ""`VariationalStrategy` that uses whitened parameters.\\nYour model will work as expected, but we ""\n            ""recommend that you re-save your model."",\n            OldVersionWarning,\n        )\n\n\nclass VariationalStrategy(_VariationalStrategy):\n    r""""""\n    The standard variational strategy, as defined by `Hensman et al. (2015)`_.\n    This strategy takes a set of :math:`m \\ll n` inducing points :math:`\\mathbf Z`\n    and applies an approximate distribution :math:`q( \\mathbf u)` over their function values.\n    (Here, we use the common notation :math:`\\mathbf u = f(\\mathbf Z)`.\n    The approximate function distribution for any abitrary input :math:`\\mathbf X` is given by:\n\n    .. math::\n\n        q( f(\\mathbf X) ) = \\int p( f(\\mathbf X) \\mid \\mathbf u) q(\\mathbf u) \\: d\\mathbf u\n\n    This variational strategy uses ""whitening"" to accelerate the optimization of the variational\n    parameters. See `Matthews (2017)`_ for more info.\n\n    :param ~gpytorch.models.ApproximateGP model: Model this strategy is applied to.\n        Typically passed in when the VariationalStrategy is created in the\n        __init__ method of the user defined model.\n    :param torch.Tensor inducing_points: Tensor containing a set of inducing\n        points to use for variational inference.\n    :param ~gpytorch.variational.VariationalDistribution variational_distribution: A\n        VariationalDistribution object that represents the form of the variational distribution :math:`q(\\mathbf u)`\n    :param bool learn_inducing_points: (optional, default True): Whether or not\n        the inducing point locations :math:`\\mathbf Z` should be learned (i.e. are they\n        parameters of the model).\n\n    .. _Hensman et al. (2015):\n        http://proceedings.mlr.press/v38/hensman15.pdf\n    .. _Matthews (2017):\n        https://www.repository.cam.ac.uk/handle/1810/278022\n    """"""\n\n    def __init__(self, model, inducing_points, variational_distribution, learn_inducing_locations=True):\n        super().__init__(model, inducing_points, variational_distribution, learn_inducing_locations)\n        self.register_buffer(""updated_strategy"", torch.tensor(True))\n        self._register_load_state_dict_pre_hook(_ensure_updated_strategy_flag_set)\n\n    @cached(name=""cholesky_factor"")\n    def _cholesky_factor(self, induc_induc_covar):\n        L = psd_safe_cholesky(delazify(induc_induc_covar).double(), jitter=settings.cholesky_jitter.value())\n        return L\n\n    @property\n    @cached(name=""prior_distribution_memo"")\n    def prior_distribution(self):\n        zeros = torch.zeros_like(self.variational_distribution.mean)\n        ones = torch.ones_like(zeros)\n        res = MultivariateNormal(zeros, DiagLazyTensor(ones))\n        return res\n\n    def forward(self, x, inducing_points, inducing_values, variational_inducing_covar=None):\n        # Compute full prior distribution\n        full_inputs = torch.cat([inducing_points, x], dim=-2)\n        full_output = self.model.forward(full_inputs)\n        full_covar = full_output.lazy_covariance_matrix\n\n        # Covariance terms\n        num_induc = inducing_points.size(-2)\n        test_mean = full_output.mean[..., num_induc:]\n        induc_induc_covar = full_covar[..., :num_induc, :num_induc].add_jitter()\n        induc_data_covar = full_covar[..., :num_induc, num_induc:].evaluate()\n        data_data_covar = full_covar[..., num_induc:, num_induc:]\n\n        # Compute interpolation terms\n        # K_ZZ^{-1/2} K_ZX\n        # K_ZZ^{-1/2} \\mu_Z\n        L = self._cholesky_factor(induc_induc_covar)\n        if L.shape != induc_induc_covar.shape:\n            # Aggressive caching can cause nasty shape incompatibilies when evaluating with different batch shapes\n            del self._memoize_cache[""cholesky_factor""]\n            L = self._cholesky_factor(induc_induc_covar)\n        interp_term = torch.triangular_solve(induc_data_covar.double(), L, upper=False)[0].to(full_inputs.dtype)\n\n        # Compute the mean of q(f)\n        # k_XZ K_ZZ^{-1/2} (m - K_ZZ^{-1/2} \\mu_Z) + \\mu_X\n        predictive_mean = (\n            torch.matmul(\n                interp_term.transpose(-1, -2), (inducing_values - self.prior_distribution.mean).unsqueeze(-1)\n            ).squeeze(-1)\n            + test_mean\n        )\n\n        # Compute the covariance of q(f)\n        # K_XX + k_XZ K_ZZ^{-1/2} (S - I) K_ZZ^{-1/2} k_ZX\n        middle_term = self.prior_distribution.lazy_covariance_matrix.mul(-1)\n        if variational_inducing_covar is not None:\n            middle_term = SumLazyTensor(variational_inducing_covar, middle_term)\n\n        if trace_mode.on():\n            predictive_covar = (\n                data_data_covar.add_jitter(1e-4).evaluate()\n                + interp_term.transpose(-1, -2) @ middle_term.evaluate() @ interp_term\n            )\n        else:\n            predictive_covar = SumLazyTensor(\n                data_data_covar.add_jitter(1e-4),\n                MatmulLazyTensor(interp_term.transpose(-1, -2), middle_term @ interp_term),\n            )\n\n        # Return the distribution\n        return MultivariateNormal(predictive_mean, predictive_covar)\n\n    def __call__(self, x, prior=False):\n        if not self.updated_strategy.item() and not prior:\n            with torch.no_grad():\n                # Get unwhitened p(u)\n                prior_function_dist = self(self.inducing_points, prior=True)\n                prior_mean = prior_function_dist.loc\n                L = self._cholesky_factor(prior_function_dist.lazy_covariance_matrix.add_jitter())\n\n                # Temporarily turn off noise that\'s added to the mean\n                orig_mean_init_std = self._variational_distribution.mean_init_std\n                self._variational_distribution.mean_init_std = 0.0\n\n                # Change the variational parameters to be whitened\n                variational_dist = self.variational_distribution\n                whitened_mean = (\n                    torch.triangular_solve((variational_dist.loc - prior_mean).unsqueeze(-1).double(), L, upper=False)[\n                        0\n                    ]\n                    .squeeze(-1)\n                    .to(variational_dist.loc.dtype)\n                )\n                whitened_covar = RootLazyTensor(\n                    torch.triangular_solve(\n                        variational_dist.lazy_covariance_matrix.root_decomposition().root.evaluate().double(),\n                        L,\n                        upper=False,\n                    )[0].to(variational_dist.loc.dtype)\n                )\n                whitened_variational_distribution = variational_dist.__class__(whitened_mean, whitened_covar)\n                self._variational_distribution.initialize_variational_distribution(whitened_variational_distribution)\n\n                # Reset the random noise parameter of the model\n                self._variational_distribution.mean_init_std = orig_mean_init_std\n\n                # Reset the cache\n                if hasattr(self, ""_memoize_cache""):\n                    delattr(self, ""_memoize_cache"")\n                    self._memoize_cache = dict()\n\n                # Mark that we have updated the variational strategy\n                self.updated_strategy.fill_(True)\n\n        return super().__call__(x, prior=prior)\n'"
gpytorch/variational/whitened_variational_strategy.py,13,"b'#!/usr/bin/env python3\n\nimport math\nimport warnings\n\nimport torch\n\nfrom .. import settings\nfrom ..distributions import MultivariateNormal\nfrom ..lazy import (\n    BatchRepeatLazyTensor,\n    CachedCGLazyTensor,\n    CholLazyTensor,\n    DiagLazyTensor,\n    MatmulLazyTensor,\n    PsdSumLazyTensor,\n    RootLazyTensor,\n)\nfrom ..module import Module\nfrom ..utils.memoize import cached\nfrom .unwhitened_variational_strategy import UnwhitenedVariationalStrategy\n\n\n# Remove after 1.0\nclass WhitenedVariationalStrategy(UnwhitenedVariationalStrategy):\n    def __init__(self, model, inducing_points, variational_distribution, learn_inducing_locations=True):\n        warnings.warn(\n            ""WhitenedVariationalStrategy is deprecated. Please use VariationalStrategy instead."", DeprecationWarning\n        )\n        super().__init__(model, inducing_points, variational_distribution, learn_inducing_locations)\n\n    @cached(name=""logdet_memo"")\n    def prior_covar_logdet(self):\n        return -self.prior_distribution.lazy_covariance_matrix.logdet()\n\n    @cached(name=""covar_trace_memo"")\n    def covar_trace(self):\n        variational_covar = self.variational_distribution.covariance_matrix\n        prior_covar = self.prior_distribution.covariance_matrix\n        batch_shape = prior_covar.shape[:-2]\n        return (variational_covar * prior_covar).view(*batch_shape, -1).sum(-1)\n\n    @cached(name=""mean_diff_inv_quad_memo"")\n    def mean_diff_inv_quad(self):\n        prior_mean = self.prior_distribution.mean\n        prior_covar = self.prior_distribution.lazy_covariance_matrix\n        variational_mean = self.variational_distribution.mean\n        return prior_covar.inv_quad(variational_mean - prior_mean)\n\n    def kl_divergence(self):\n        variational_dist_u = self.variational_distribution\n        prior_dist = self.prior_distribution\n        kl_divergence = 0.5 * sum(\n            [\n                # log|k| - log|S|\n                # = log|K| - log|K var_dist_covar K|\n                # = -log|K| - log|var_dist_covar|\n                self.prior_covar_logdet(),\n                -variational_dist_u.lazy_covariance_matrix.logdet(),\n                # tr(K^-1 S) = tr(K^1 K var_dist_covar K) = tr(K var_dist_covar)\n                self.covar_trace(),\n                # (m - \\mu u)^T K^-1 (m - \\mu u)\n                # = (K^-1 (m - \\mu u)) K (K^1 (m - \\mu u))\n                # = (var_dist_mean)^T K (var_dist_mean)\n                self.mean_diff_inv_quad(),\n                # d\n                -prior_dist.event_shape.numel(),\n            ]\n        )\n\n        return kl_divergence\n\n    def initialize_variational_dist(self):\n        prior_dist = self.prior_distribution\n        inv_prior_dist = torch.distributions.MultivariateNormal(\n            prior_dist.mean,\n            prior_dist.lazy_covariance_matrix.add_jitter()\n            .evaluate()\n            .double()\n            .inverse()\n            .type_as(prior_dist.covariance_matrix),\n        )\n        self.variational_distribution.initialize_variational_distribution(inv_prior_dist)\n\n    def forward(self, x):\n        r""""""\n        The :func:`~gpytorch.variational.VariationalStrategy.forward` method determines how to marginalize out the\n        inducing point function values. Specifically, forward defines how to transform a variational distribution\n        over the inducing point values, :math:`q(u)`, in to a variational distribution over the function values at\n        specified locations x, :math:`q(f|x)`, by integrating :math:`\\int p(f|x, u)q(u)du`\n\n        :param torch.Tensor x: Locations x to get the variational posterior of the function values at.\n        :rtype: ~gpytorch.distributions.MultivariateNormal\n        :return: The distribution :math:`q(f|x)`\n        """"""\n        variational_dist = self.variational_distribution\n        inducing_points = self.inducing_points\n        if inducing_points.dim() < x.dim():\n            inducing_points = inducing_points.expand(*x.shape[:-2], *inducing_points.shape[-2:])\n        if len(variational_dist.batch_shape) < x.dim() - 2:\n            variational_dist = variational_dist.expand(x.shape[:-2])\n\n        # If our points equal the inducing points, we\'re done\n        if torch.equal(x, inducing_points):\n            # De-whiten the prior covar\n            prior_covar = self.prior_distribution.lazy_covariance_matrix\n            if isinstance(variational_dist.lazy_covariance_matrix, RootLazyTensor):\n                predictive_covar = RootLazyTensor(prior_covar @ variational_dist.lazy_covariance_matrix.root.evaluate())\n            else:\n                predictive_covar = MatmulLazyTensor(prior_covar @ variational_dist.covariance_matrix, prior_covar)\n\n            # Cache some values for the KL divergence\n            if self.training:\n                self._mean_diff_inv_quad_memo, self._logdet_memo = prior_covar.inv_quad_logdet(\n                    (variational_dist.mean - self.prior_distribution.mean), logdet=True\n                )\n\n            return MultivariateNormal(variational_dist.mean, predictive_covar)\n\n        # Otherwise, we have to marginalize\n        else:\n            num_induc = inducing_points.size(-2)\n            full_inputs = torch.cat([inducing_points, x], dim=-2)\n            full_output = self.model.forward(full_inputs)\n            full_mean, full_covar = full_output.mean, full_output.lazy_covariance_matrix\n\n            # Mean terms\n            test_mean = full_mean[..., num_induc:]\n            induc_mean = full_mean[..., :num_induc]\n            mean_diff = (variational_dist.mean - induc_mean).unsqueeze(-1)\n\n            # Covariance terms\n            induc_induc_covar = full_covar[..., :num_induc, :num_induc].add_jitter()\n            induc_data_covar = full_covar[..., :num_induc, num_induc:].evaluate()\n            data_data_covar = full_covar[..., num_induc:, num_induc:]\n\n            # If we\'re less than a certain size, we\'ll compute the Cholesky decomposition of induc_induc_covar\n            cholesky = False\n            if settings.fast_computations.log_prob.off() or (num_induc <= settings.max_cholesky_size.value()):\n                induc_induc_covar = CholLazyTensor(induc_induc_covar.cholesky())\n                cholesky = True\n\n            # Cache the CG results\n            # Do not use preconditioning for whitened VI, as it does not seem to improve performance.\n            with settings.max_preconditioner_size(0):\n                with torch.no_grad():\n                    eager_rhs = torch.cat([induc_data_covar, mean_diff], -1)\n                    solve, probe_vecs, probe_vec_norms, probe_vec_solves, tmats = CachedCGLazyTensor.precompute_terms(\n                        induc_induc_covar,\n                        eager_rhs.detach(),\n                        logdet_terms=(not cholesky),\n                        include_tmats=(not settings.skip_logdet_forward.on() and not cholesky),\n                    )\n                    eager_rhss = [eager_rhs.detach()]\n                    solves = [solve.detach()]\n                    if settings.skip_logdet_forward.on() and self.training:\n                        eager_rhss.append(torch.cat([probe_vecs, eager_rhs], -1))\n                        solves.append(torch.cat([probe_vec_solves, solve[..., : eager_rhs.size(-1)]], -1))\n                    elif not self.training:\n                        eager_rhss.append(eager_rhs[..., :-1])\n                        solves.append(solve[..., :-1])\n\n                induc_induc_covar = CachedCGLazyTensor(\n                    induc_induc_covar,\n                    eager_rhss=eager_rhss,\n                    solves=solves,\n                    probe_vectors=probe_vecs,\n                    probe_vector_norms=probe_vec_norms,\n                    probe_vector_solves=probe_vec_solves,\n                    probe_vector_tmats=tmats,\n                )\n\n            # Compute some terms that will be necessary for the predicitve covariance and KL divergence\n            if self.training:\n                interp_data_data_var_plus_mean_diff_inv_quad, logdet = induc_induc_covar.inv_quad_logdet(\n                    torch.cat([induc_data_covar, mean_diff], -1), logdet=True, reduce_inv_quad=False\n                )\n                interp_data_data_var = interp_data_data_var_plus_mean_diff_inv_quad[..., :-1]\n                mean_diff_inv_quad = interp_data_data_var_plus_mean_diff_inv_quad[..., -1]\n\n            # Compute predictive mean\n            predictive_mean = torch.add(\n                test_mean,\n                induc_induc_covar.inv_matmul(mean_diff, left_tensor=induc_data_covar.transpose(-1, -2)).squeeze(-1),\n            )\n\n            # Compute the predictive covariance\n            is_root_lt = isinstance(variational_dist.lazy_covariance_matrix, RootLazyTensor)\n            is_repeated_root_lt = isinstance(\n                variational_dist.lazy_covariance_matrix, BatchRepeatLazyTensor\n            ) and isinstance(variational_dist.lazy_covariance_matrix.base_lazy_tensor, RootLazyTensor)\n            if is_root_lt:\n                predictive_covar = RootLazyTensor(\n                    induc_data_covar.transpose(-1, -2) @ variational_dist.lazy_covariance_matrix.root.evaluate()\n                )\n            elif is_repeated_root_lt:\n                predictive_covar = RootLazyTensor(\n                    induc_data_covar.transpose(-1, -2)\n                    @ variational_dist.lazy_covariance_matrix.root_decomposition().root.evaluate()\n                )\n            else:\n                predictive_covar = MatmulLazyTensor(\n                    induc_data_covar.transpose(-1, -2), predictive_covar @ induc_data_covar\n                )\n\n            if self.training:\n                data_covariance = DiagLazyTensor((data_data_covar.diag() - interp_data_data_var).clamp(0, math.inf))\n            else:\n                neg_induc_data_data_covar = torch.matmul(\n                    induc_data_covar.transpose(-1, -2).mul(-1), induc_induc_covar.inv_matmul(induc_data_covar)\n                )\n                data_covariance = data_data_covar + neg_induc_data_data_covar\n            predictive_covar = PsdSumLazyTensor(predictive_covar, data_covariance)\n\n            # Save the logdet, mean_diff_inv_quad, prior distribution for the ELBO\n            if self.training:\n                self._memoize_cache[""prior_distribution_memo""] = MultivariateNormal(induc_mean, induc_induc_covar)\n                self._memoize_cache[""logdet_memo""] = -logdet\n                self._memoize_cache[""mean_diff_inv_quad_memo""] = mean_diff_inv_quad\n\n            return MultivariateNormal(predictive_mean, predictive_covar)\n\n    def __call__(self, x, prior=False):\n        # If we\'re in prior mode, then we\'re done!\n        if prior:\n            return self.model.forward(x)\n\n        # Delete previously cached items from the training distribution\n        if self.training:\n            if hasattr(self, ""_memoize_cache""):\n                delattr(self, ""_memoize_cache"")\n                self._memoize_cache = dict()\n        # (Maybe) initialize variational distribution\n        if not self.variational_params_initialized.item():\n            prior_dist = self.prior_distribution\n            self._variational_distribution.initialize_variational_distribution(prior_dist)\n            self.variational_params_initialized.fill_(1)\n\n        return Module.__call__(self, x)\n'"
test/constraints/__init__.py,0,b'#!/usr/bin/env python3\n'
test/constraints/test_constraints.py,47,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\nfrom torch import sigmoid\nfrom torch.nn.functional import softplus\n\nimport gpytorch\nfrom gpytorch.test.base_test_case import BaseTestCase\n\n\n# Basic exact GP model for testing parameter + constraint name resolution\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass TestInterval(unittest.TestCase, BaseTestCase):\n    def test_transform_float_bounds(self):\n        constraint = gpytorch.constraints.Interval(1.0, 5.0)\n\n        v = torch.tensor(-3.0)\n\n        value = constraint.transform(v)\n        actual_value = ((5.0 - 1.0) * sigmoid(v)) + 1.0\n\n        self.assertAllClose(value, actual_value)\n\n    def test_inverse_transform_float_bounds(self):\n        constraint = gpytorch.constraints.Interval(1.0, 5.0)\n\n        v = torch.tensor(-3.0)\n\n        value = constraint.inverse_transform(constraint.transform(v))\n\n        self.assertAllClose(v, value)\n\n    def test_transform_tensor_bounds(self):\n        constraint = gpytorch.constraints.Interval(torch.tensor([1.0, 2.0]), torch.tensor([3.0, 4.0]))\n\n        v = torch.tensor([-3.0, -2.0])\n\n        value = constraint.transform(v)\n        actual_value = v.clone()\n        actual_value[0] = (3.0 - 1.0) * sigmoid(v[0]) + 1.0\n        actual_value[1] = (4.0 - 2.0) * sigmoid(v[1]) + 2.0\n\n        self.assertAllClose(value, actual_value)\n\n    def test_inverse_transform_tensor_bounds(self):\n        constraint = gpytorch.constraints.Interval(torch.tensor([1.0, 2.0]), torch.tensor([3.0, 4.0]))\n\n        v = torch.tensor([-3.0, -2.0])\n\n        value = constraint.inverse_transform(constraint.transform(v))\n\n        self.assertAllClose(v, value)\n\n    def test_initial_value(self):\n        constraint = gpytorch.constraints.Interval(1.0, 5.0, transform=None, initial_value=3.0)\n        lkhd = gpytorch.likelihoods.GaussianLikelihood(noise_constraint=constraint)\n        self.assertEqual(lkhd.noise.item(), 3.0)\n\n\nclass TestGreaterThan(unittest.TestCase, BaseTestCase):\n    def test_transform_float_greater_than(self):\n        constraint = gpytorch.constraints.GreaterThan(1.0)\n\n        v = torch.tensor(-3.0)\n\n        value = constraint.transform(v)\n        actual_value = softplus(v) + 1.0\n\n        self.assertAllClose(value, actual_value)\n\n    def test_transform_tensor_greater_than(self):\n        constraint = gpytorch.constraints.GreaterThan([1.0, 2.0])\n\n        v = torch.tensor([-3.0, -2.0])\n\n        value = constraint.transform(v)\n        actual_value = v.clone()\n        actual_value[0] = softplus(v[0]) + 1.0\n        actual_value[1] = softplus(v[1]) + 2.0\n\n        self.assertAllClose(value, actual_value)\n\n    def test_inverse_transform_float_greater_than(self):\n        constraint = gpytorch.constraints.GreaterThan(1.0)\n\n        v = torch.tensor(-3.0)\n\n        value = constraint.inverse_transform(constraint.transform(v))\n\n        self.assertAllClose(value, v)\n\n    def test_inverse_transform_tensor_greater_than(self):\n        constraint = gpytorch.constraints.GreaterThan([1.0, 2.0])\n\n        v = torch.tensor([-3.0, -2.0])\n\n        value = constraint.inverse_transform(constraint.transform(v))\n\n        self.assertAllClose(value, v)\n\n\nclass TestLessThan(unittest.TestCase, BaseTestCase):\n    def test_transform_float_less_than(self):\n        constraint = gpytorch.constraints.LessThan(1.0)\n\n        v = torch.tensor(-3.0)\n\n        value = constraint.transform(v)\n        actual_value = -softplus(-v) + 1.0\n\n        self.assertAllClose(value, actual_value)\n\n    def test_transform_tensor_less_than(self):\n        constraint = gpytorch.constraints.LessThan([1.0, 2.0])\n\n        v = torch.tensor([-3.0, -2.0])\n\n        value = constraint.transform(v)\n        actual_value = v.clone()\n        actual_value[0] = -softplus(-v[0]) + 1.0\n        actual_value[1] = -softplus(-v[1]) + 2.0\n\n        self.assertAllClose(value, actual_value)\n\n    def test_inverse_transform_float_less_than(self):\n        constraint = gpytorch.constraints.LessThan(1.0)\n\n        v = torch.tensor(-3.0)\n\n        value = constraint.inverse_transform(constraint.transform(v))\n\n        self.assertAllClose(value, v)\n\n    def test_inverse_transform_tensor_less_than(self):\n        constraint = gpytorch.constraints.LessThan([1.0, 2.0])\n\n        v = torch.tensor([-3.0, -2.0])\n\n        value = constraint.inverse_transform(constraint.transform(v))\n\n        self.assertAllClose(value, v)\n\n\nclass TestPositive(unittest.TestCase, BaseTestCase):\n    def test_transform_float_positive(self):\n        constraint = gpytorch.constraints.Positive()\n\n        v = torch.tensor(-3.0)\n\n        value = constraint.transform(v)\n        actual_value = softplus(v)\n\n        self.assertAllClose(value, actual_value)\n\n    def test_transform_tensor_positive(self):\n        constraint = gpytorch.constraints.Positive()\n\n        v = torch.tensor([-3.0, -2.0])\n\n        value = constraint.transform(v)\n        actual_value = v.clone()\n        actual_value[0] = softplus(v[0])\n        actual_value[1] = softplus(v[1])\n\n        self.assertAllClose(value, actual_value)\n\n    def test_inverse_transform_float_positive(self):\n        constraint = gpytorch.constraints.Positive()\n\n        v = torch.tensor(-3.0)\n\n        value = constraint.inverse_transform(constraint.transform(v))\n\n        self.assertAllClose(value, v)\n\n    def test_inverse_transform_tensor_positive(self):\n        constraint = gpytorch.constraints.Positive()\n\n        v = torch.tensor([-3.0, -2.0])\n\n        value = constraint.inverse_transform(constraint.transform(v))\n\n        self.assertAllClose(value, v)\n\n\nclass TestConstraintNaming(unittest.TestCase, BaseTestCase):\n    def test_constraint_by_name(self):\n        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n        model = ExactGPModel(None, None, likelihood)\n\n        constraint = model.constraint_for_parameter_name(""likelihood.noise_covar.raw_noise"")\n        self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan)\n\n        constraint = model.constraint_for_parameter_name(""covar_module.base_kernel.raw_lengthscale"")\n        self.assertIsInstance(constraint, gpytorch.constraints.Positive)\n\n        constraint = model.constraint_for_parameter_name(""mean_module.constant"")\n        self.assertIsNone(constraint)\n\n    def test_named_parameters_and_constraints(self):\n        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n        model = ExactGPModel(None, None, likelihood)\n\n        for name, _param, constraint in model.named_parameters_and_constraints():\n            if name == ""likelihood.noise_covar.raw_noise"":\n                self.assertIsInstance(constraint, gpytorch.constraints.GreaterThan)\n            elif name == ""mean_module.constant"":\n                self.assertIsNone(constraint)\n            elif name == ""covar_module.raw_outputscale"":\n                self.assertIsInstance(constraint, gpytorch.constraints.Positive)\n            elif name == ""covar_module.base_kernel.raw_lengthscale"":\n                self.assertIsInstance(constraint, gpytorch.constraints.Positive)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/distributions/__init__.py,0,b'#!/usr/bin/env python3\n'
test/distributions/test_delta.py,11,"b'#!/usr/bin/env python3\n# Mostly copied from https://raw.githubusercontent.com/pyro-ppl/pyro/dev/tests/distributions/test_delta.py\n\nimport unittest\n\nimport numpy as np\nimport torch\n\nimport gpytorch.distributions as dist\nfrom gpytorch.test.base_test_case import BaseTestCase\n\n\nclass TestDelta(BaseTestCase, unittest.TestCase):\n    def setUp(self):\n        self.v = torch.tensor([3.0])\n        self.vs = torch.tensor([[0.0], [1.0], [2.0], [3.0]])\n        self.vs_expanded = self.vs.expand(4, 3)\n        self.test_data = torch.tensor([[3.0], [3.0], [3.0]])\n        self.batch_test_data_1 = torch.arange(0.0, 4.0).unsqueeze(1).expand(4, 3)\n        self.batch_test_data_2 = torch.arange(4.0, 8.0).unsqueeze(1).expand(4, 3)\n        self.batch_test_data_3 = torch.Tensor([[3.0], [3.0], [3.0], [3.0]])\n        self.expected_support = [[[0.0], [1.0], [2.0], [3.0]]]\n        self.expected_support_non_vec = [[3.0]]\n        self.analytic_mean = 3.0\n        self.analytic_var = 0.0\n        self.n_samples = 10\n\n    def test_log_prob_sum(self):\n        log_px_torch = dist.Delta(self.v).log_prob(self.test_data).sum()\n        self.assertEqual(log_px_torch.item(), 0)\n\n    def test_batch_log_prob(self):\n        log_px_torch = dist.Delta(self.vs_expanded).log_prob(self.batch_test_data_1).data\n        self.assertEqual(log_px_torch.sum().item(), 0)\n        log_px_torch = dist.Delta(self.vs_expanded).log_prob(self.batch_test_data_2).data\n        self.assertEqual(log_px_torch.sum().item(), float(""-inf""))\n\n    def test_batch_log_prob_shape(self):\n        assert dist.Delta(self.vs).log_prob(self.batch_test_data_3).size() == (4, 1)\n        assert dist.Delta(self.v).log_prob(self.batch_test_data_3).size() == (4, 1)\n\n    def test_mean_and_var(self):\n        torch_samples = [dist.Delta(self.v).sample().detach().cpu().numpy() for _ in range(self.n_samples)]\n        torch_mean = np.mean(torch_samples)\n        torch_var = np.var(torch_samples)\n        self.assertEqual(torch_mean, self.analytic_mean)\n        self.assertEqual(torch_var, self.analytic_var)\n'"
test/distributions/test_multitask_multivariate_normal.py,125,"b'#!/usr/bin/env python3\n\nimport math\nimport os\nimport random\nimport unittest\n\nimport torch\n\nfrom gpytorch.distributions import MultitaskMultivariateNormal, MultivariateNormal\nfrom gpytorch.lazy import DiagLazyTensor\nfrom gpytorch.test.base_test_case import BaseTestCase\nfrom gpytorch.test.utils import least_used_cuda_device\n\n\nclass TestMultiTaskMultivariateNormal(BaseTestCase, unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(1)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(1)\n            random.seed(1)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_multitask_multivariate_normal_exceptions(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.tensor([0, 1], device=device, dtype=dtype)\n            covmat = torch.eye(2, device=device, dtype=dtype)\n            with self.assertRaises(RuntimeError):\n                MultitaskMultivariateNormal(mean=mean, covariance_matrix=covmat)\n\n    def test_multitask_multivariate_normal_exceptions_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_multitask_multivariate_normal_exceptions(cuda=True)\n\n    def test_multitask_multivariate_normal(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.tensor([[0, 1], [2, 3], [4, 5]], dtype=dtype, device=device)\n            variance = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=dtype, device=device)\n\n            # interleaved\n            covmat = variance.view(-1).diag()\n            mtmvn = MultitaskMultivariateNormal(mean=mean, covariance_matrix=covmat)\n            self.assertTrue(torch.equal(mtmvn.mean, mean))\n            self.assertTrue(torch.allclose(mtmvn.variance, variance))\n            self.assertTrue(torch.allclose(mtmvn.scale_tril, covmat.sqrt()))\n            self.assertTrue(mtmvn.event_shape == torch.Size([3, 2]))\n            self.assertTrue(mtmvn.batch_shape == torch.Size())\n            mvn_plus1 = mtmvn + 1\n            self.assertTrue(torch.equal(mvn_plus1.mean, mtmvn.mean + 1))\n            self.assertTrue(torch.equal(mvn_plus1.covariance_matrix, mtmvn.covariance_matrix))\n            mvn_times2 = mtmvn * 2\n            self.assertTrue(torch.equal(mvn_times2.mean, mtmvn.mean * 2))\n            self.assertTrue(torch.equal(mvn_times2.covariance_matrix, mtmvn.covariance_matrix * 4))\n            mvn_divby2 = mtmvn / 2\n            self.assertTrue(torch.equal(mvn_divby2.mean, mtmvn.mean / 2))\n            self.assertTrue(torch.equal(mvn_divby2.covariance_matrix, mtmvn.covariance_matrix / 4))\n            self.assertAlmostEqual(mtmvn.entropy().item(), 11.80326, places=4)\n            self.assertAlmostEqual(\n                mtmvn.log_prob(torch.zeros(3, 2, device=device, dtype=dtype)).item(), -14.52826, places=4\n            )\n            logprob = mtmvn.log_prob(torch.zeros(2, 3, 2, device=device, dtype=dtype))\n            logprob_expected = -14.52826 * torch.ones(2, device=device, dtype=dtype)\n            self.assertTrue(torch.allclose(logprob, logprob_expected))\n            conf_lower, conf_upper = mtmvn.confidence_region()\n            self.assertTrue(torch.allclose(conf_lower, mtmvn.mean - 2 * mtmvn.stddev))\n            self.assertTrue(torch.allclose(conf_upper, mtmvn.mean + 2 * mtmvn.stddev))\n            self.assertTrue(mtmvn.sample().shape == torch.Size([3, 2]))\n            self.assertTrue(mtmvn.sample(torch.Size([3])).shape == torch.Size([3, 3, 2]))\n            self.assertTrue(mtmvn.sample(torch.Size([3, 4])).shape == torch.Size([3, 4, 3, 2]))\n\n            # non-interleaved\n            covmat = variance.transpose(-1, -2).reshape(-1).diag()\n            mtmvn = MultitaskMultivariateNormal(mean=mean, covariance_matrix=covmat, interleaved=False)\n            self.assertTrue(torch.equal(mtmvn.mean, mean))\n            self.assertTrue(torch.allclose(mtmvn.variance, variance))\n            self.assertTrue(torch.allclose(mtmvn.scale_tril, covmat.sqrt()))\n            self.assertTrue(mtmvn.event_shape == torch.Size([3, 2]))\n            self.assertTrue(mtmvn.batch_shape == torch.Size())\n\n    def test_multitask_multivariate_normal_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_multitask_multivariate_normal(cuda=True)\n\n    def test_multitask_multivariate_normal_batch(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.tensor([[0, 1], [2, 3], [4, 5]], dtype=dtype, device=device).repeat(2, 1, 1)\n            variance = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=dtype, device=device).repeat(2, 1, 1)\n\n            # interleaved\n            covmat = variance.view(2, 1, -1) * torch.eye(6, device=device, dtype=dtype)\n            mtmvn = MultitaskMultivariateNormal(mean=mean, covariance_matrix=covmat)\n            self.assertTrue(torch.equal(mtmvn.mean, mean))\n            self.assertTrue(torch.allclose(mtmvn.variance, variance))\n            self.assertTrue(torch.allclose(mtmvn.scale_tril, covmat.sqrt()))\n            self.assertTrue(mtmvn.event_shape == torch.Size([3, 2]))\n            self.assertTrue(mtmvn.batch_shape == torch.Size([2]))\n            mvn_plus1 = mtmvn + 1\n            self.assertTrue(torch.equal(mvn_plus1.mean, mtmvn.mean + 1))\n            self.assertTrue(torch.equal(mvn_plus1.covariance_matrix, mtmvn.covariance_matrix))\n            mvn_times2 = mtmvn * 2\n            self.assertTrue(torch.equal(mvn_times2.mean, mtmvn.mean * 2))\n            self.assertTrue(torch.equal(mvn_times2.covariance_matrix, mtmvn.covariance_matrix * 4))\n            mvn_divby2 = mtmvn / 2\n            self.assertTrue(torch.equal(mvn_divby2.mean, mtmvn.mean / 2))\n            self.assertTrue(torch.equal(mvn_divby2.covariance_matrix, mtmvn.covariance_matrix / 4))\n            self.assertTrue(torch.allclose(mtmvn.entropy(), 11.80326 * torch.ones(2, device=device, dtype=dtype)))\n            logprob = mtmvn.log_prob(torch.zeros(2, 3, 2, device=device, dtype=dtype))\n            logprob_expected = -14.52826 * torch.ones(2, device=device, dtype=dtype)\n            self.assertTrue(torch.allclose(logprob, logprob_expected))\n            logprob = mtmvn.log_prob(torch.zeros(3, 2, 3, 2, device=device, dtype=dtype))\n            logprob_expected = -14.52826 * torch.ones(3, 2, device=device, dtype=dtype)\n            self.assertTrue(torch.allclose(logprob, logprob_expected))\n            conf_lower, conf_upper = mtmvn.confidence_region()\n            self.assertTrue(torch.allclose(conf_lower, mtmvn.mean - 2 * mtmvn.stddev))\n            self.assertTrue(torch.allclose(conf_upper, mtmvn.mean + 2 * mtmvn.stddev))\n            self.assertTrue(mtmvn.sample().shape == torch.Size([2, 3, 2]))\n            self.assertTrue(mtmvn.sample(torch.Size([3])).shape == torch.Size([3, 2, 3, 2]))\n            self.assertTrue(mtmvn.sample(torch.Size([3, 4])).shape == torch.Size([3, 4, 2, 3, 2]))\n\n            # non-interleaved\n            covmat = variance.transpose(-1, -2).reshape(2, 1, -1) * torch.eye(6, device=device, dtype=dtype)\n            mtmvn = MultitaskMultivariateNormal(mean=mean, covariance_matrix=covmat, interleaved=False)\n            self.assertTrue(torch.equal(mtmvn.mean, mean))\n            self.assertTrue(torch.allclose(mtmvn.variance, variance))\n            self.assertTrue(torch.allclose(mtmvn.scale_tril, covmat.sqrt()))\n            self.assertTrue(mtmvn.event_shape == torch.Size([3, 2]))\n            self.assertTrue(mtmvn.batch_shape == torch.Size([2]))\n\n    def test_multitask_multivariate_normal_batch_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_multitask_multivariate_normal_batch(cuda=True)\n\n    def test_multivariate_normal_correlated_samples(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.tensor([[0, 1], [2, 3], [4, 5]], dtype=dtype, device=device)\n            variance = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=dtype, device=device)\n            covmat = variance.view(-1).diag()\n            mtmvn = MultitaskMultivariateNormal(mean=mean, covariance_matrix=covmat)\n            base_samples = mtmvn.get_base_samples(torch.Size([3, 4]))\n            self.assertTrue(mtmvn.sample(base_samples=base_samples).shape == torch.Size([3, 4, 3, 2]))\n            base_samples = mtmvn.get_base_samples()\n            self.assertTrue(mtmvn.sample(base_samples=base_samples).shape == torch.Size([3, 2]))\n\n    def test_multivariate_normal_correlated_samples_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_multivariate_normal_correlated_samples(cuda=True)\n\n    def test_multivariate_normal_batch_correlated_samples(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.tensor([[0, 1], [2, 3], [4, 5]], dtype=dtype, device=device).repeat(2, 1, 1)\n            variance = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=dtype, device=device).repeat(2, 1, 1)\n            covmat = variance.view(2, 1, -1) * torch.eye(6, device=device, dtype=dtype)\n            mtmvn = MultitaskMultivariateNormal(mean=mean, covariance_matrix=covmat)\n            base_samples = mtmvn.get_base_samples(torch.Size((3, 4)))\n            self.assertTrue(mtmvn.sample(base_samples=base_samples).shape == torch.Size([3, 4, 2, 3, 2]))\n            base_samples = mtmvn.get_base_samples()\n            self.assertTrue(mtmvn.sample(base_samples=base_samples).shape == torch.Size([2, 3, 2]))\n\n    def test_multivariate_normal_batch_correlated_samples_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_multivariate_normal_batch_correlated_samples(cuda=True)\n\n    def test_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.randn(4, 3, device=device, dtype=dtype)\n            var = torch.randn(12, device=device, dtype=dtype).abs_()\n            values = mean + 0.5\n            diffs = (values - mean).view(-1)\n\n            res = MultitaskMultivariateNormal(mean, DiagLazyTensor(var)).log_prob(values)\n            actual = -0.5 * (math.log(math.pi * 2) * 12 + var.log().sum() + (diffs / var * diffs).sum())\n            self.assertLess((res - actual).div(res).abs().item(), 1e-2)\n\n            mean = torch.randn(3, 4, 3, device=device, dtype=dtype)\n            var = torch.randn(3, 12, device=device, dtype=dtype).abs_()\n            values = mean + 0.5\n            diffs = (values - mean).view(3, -1)\n\n            res = MultitaskMultivariateNormal(mean, DiagLazyTensor(var)).log_prob(values)\n            actual = -0.5 * (math.log(math.pi * 2) * 12 + var.log().sum(-1) + (diffs / var * diffs).sum(-1))\n            self.assertLess((res - actual).div(res).abs().norm(), 1e-2)\n\n    def test_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_log_prob(cuda=True)\n\n    def test_multitask_from_batch(self):\n        mean = torch.randn(2, 3)\n        variance = torch.randn(2, 3)\n        mvn = MultivariateNormal(mean, DiagLazyTensor(variance))\n        mmvn = MultitaskMultivariateNormal.from_batch_mvn(mvn, task_dim=-1)\n        self.assertTrue(isinstance(mmvn, MultitaskMultivariateNormal))\n        self.assertEqual(mmvn.batch_shape, torch.Size([]))\n        self.assertEqual(mmvn.event_shape, torch.Size([3, 2]))\n        self.assertEqual(mmvn.covariance_matrix.shape, torch.Size([6, 6]))\n        self.assertEqual(mmvn.mean, mean.transpose(-1, -2))\n        self.assertEqual(mmvn.variance, variance.transpose(-1, -2))\n\n        mean = torch.randn(2, 4, 3)\n        variance = torch.randn(2, 4, 3)\n        mvn = MultivariateNormal(mean, DiagLazyTensor(variance))\n        mmvn = MultitaskMultivariateNormal.from_batch_mvn(mvn, task_dim=0)\n        self.assertTrue(isinstance(mmvn, MultitaskMultivariateNormal))\n        self.assertEqual(mmvn.batch_shape, torch.Size([4]))\n        self.assertEqual(mmvn.event_shape, torch.Size([3, 2]))\n        self.assertEqual(mmvn.covariance_matrix.shape, torch.Size([4, 6, 6]))\n        self.assertEqual(mmvn.mean, mean.permute(1, 2, 0))\n        self.assertEqual(mmvn.variance, variance.permute(1, 2, 0))\n\n    def test_multitask_from_repeat(self):\n        mean = torch.randn(2, 3)\n        variance = torch.randn(2, 3)\n        mvn = MultivariateNormal(mean, DiagLazyTensor(variance))\n        mmvn = MultitaskMultivariateNormal.from_repeated_mvn(mvn, num_tasks=4)\n        self.assertTrue(isinstance(mmvn, MultitaskMultivariateNormal))\n        self.assertEqual(mmvn.batch_shape, torch.Size([2]))\n        self.assertEqual(mmvn.event_shape, torch.Size([3, 4]))\n        self.assertEqual(mmvn.covariance_matrix.shape, torch.Size([2, 12, 12]))\n        for i in range(4):\n            self.assertEqual(mmvn.mean[..., i], mean)\n            self.assertEqual(mmvn.variance[..., i], variance)\n\n    def test_from_independent_mvns(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            # Test non-batch mode mvns\n            n_tasks = 2\n            n = 4\n            mvns = [\n                MultivariateNormal(\n                    mean=torch.randn(4, device=device, dtype=dtype),\n                    covariance_matrix=DiagLazyTensor(torch.randn(n, device=device, dtype=dtype).abs_()),\n                )\n                for i in range(n_tasks)\n            ]\n            mvn = MultitaskMultivariateNormal.from_independent_mvns(mvns=mvns)\n            expected_mean_shape = [n, n_tasks]\n            expected_covar_shape = [n * n_tasks] * 2\n            self.assertEqual(list(mvn.mean.shape), expected_mean_shape)\n            self.assertEqual(list(mvn.covariance_matrix.shape), expected_covar_shape)\n\n            # Test batch mode mvns\n            b = 3\n            mvns = [\n                MultivariateNormal(\n                    mean=torch.randn(b, n, device=device, dtype=dtype),\n                    covariance_matrix=DiagLazyTensor(torch.randn(b, n, device=device, dtype=dtype).abs_()),\n                )\n                for i in range(n_tasks)\n            ]\n            mvn = MultitaskMultivariateNormal.from_independent_mvns(mvns=mvns)\n            self.assertEqual(list(mvn.mean.shape), [b] + expected_mean_shape)\n            self.assertEqual(list(mvn.covariance_matrix.shape), [b] + expected_covar_shape)\n\n    def test_from_independent_mvns_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_from_independent_mvns(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/distributions/test_multivariate_normal.py,104,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\n\nimport torch\nfrom torch.distributions import MultivariateNormal as TMultivariateNormal\n\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.lazy import DiagLazyTensor, LazyTensor, NonLazyTensor\nfrom gpytorch.test.base_test_case import BaseTestCase\nfrom gpytorch.test.utils import least_used_cuda_device\n\n\nclass TestMultivariateNormal(BaseTestCase, unittest.TestCase):\n    seed = 1\n\n    def test_multivariate_normal_non_lazy(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.tensor([0, 1, 2], device=device, dtype=dtype)\n            covmat = torch.diag(torch.tensor([1, 0.75, 1.5], device=device, dtype=dtype))\n            mvn = MultivariateNormal(mean=mean, covariance_matrix=covmat, validate_args=True)\n            self.assertTrue(torch.is_tensor(mvn.covariance_matrix))\n            self.assertIsInstance(mvn.lazy_covariance_matrix, LazyTensor)\n            self.assertAllClose(mvn.variance, torch.diag(covmat))\n            self.assertAllClose(mvn.scale_tril, covmat.sqrt())\n            mvn_plus1 = mvn + 1\n            self.assertAllClose(mvn_plus1.mean, mvn.mean + 1)\n            self.assertAllClose(mvn_plus1.covariance_matrix, mvn.covariance_matrix)\n            mvn_times2 = mvn * 2\n            self.assertAllClose(mvn_times2.mean, mvn.mean * 2)\n            self.assertAllClose(mvn_times2.covariance_matrix, mvn.covariance_matrix * 4)\n            mvn_divby2 = mvn / 2\n            self.assertAllClose(mvn_divby2.mean, mvn.mean / 2)\n            self.assertAllClose(mvn_divby2.covariance_matrix, mvn.covariance_matrix / 4)\n            self.assertAlmostEqual(mvn.entropy().item(), 4.3157, places=4)\n            self.assertAlmostEqual(mvn.log_prob(torch.zeros(3, device=device, dtype=dtype)).item(), -4.8157, places=4)\n            logprob = mvn.log_prob(torch.zeros(2, 3, device=device, dtype=dtype))\n            logprob_expected = torch.tensor([-4.8157, -4.8157], device=device, dtype=dtype)\n            self.assertAllClose(logprob, logprob_expected)\n            conf_lower, conf_upper = mvn.confidence_region()\n            self.assertAllClose(conf_lower, mvn.mean - 2 * mvn.stddev)\n            self.assertAllClose(conf_upper, mvn.mean + 2 * mvn.stddev)\n            self.assertTrue(mvn.sample().shape == torch.Size([3]))\n            self.assertTrue(mvn.sample(torch.Size([2])).shape == torch.Size([2, 3]))\n            self.assertTrue(mvn.sample(torch.Size([2, 4])).shape == torch.Size([2, 4, 3]))\n\n    def test_multivariate_normal_non_lazy_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_multivariate_normal_non_lazy(cuda=True)\n\n    def test_multivariate_normal_batch_non_lazy(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.tensor([0, 1, 2], device=device, dtype=dtype)\n            covmat = torch.diag(torch.tensor([1, 0.75, 1.5], device=device, dtype=dtype))\n            mvn = MultivariateNormal(\n                mean=mean.repeat(2, 1), covariance_matrix=covmat.repeat(2, 1, 1), validate_args=True\n            )\n            self.assertTrue(torch.is_tensor(mvn.covariance_matrix))\n            self.assertIsInstance(mvn.lazy_covariance_matrix, LazyTensor)\n            self.assertAllClose(mvn.variance, covmat.diag().repeat(2, 1))\n            self.assertAllClose(mvn.scale_tril, torch.diag(covmat.diag().sqrt()).repeat(2, 1, 1))\n            mvn_plus1 = mvn + 1\n            self.assertAllClose(mvn_plus1.mean, mvn.mean + 1)\n            self.assertAllClose(mvn_plus1.covariance_matrix, mvn.covariance_matrix)\n            mvn_times2 = mvn * 2\n            self.assertAllClose(mvn_times2.mean, mvn.mean * 2)\n            self.assertAllClose(mvn_times2.covariance_matrix, mvn.covariance_matrix * 4)\n            mvn_divby2 = mvn / 2\n            self.assertAllClose(mvn_divby2.mean, mvn.mean / 2)\n            self.assertAllClose(mvn_divby2.covariance_matrix, mvn.covariance_matrix / 4)\n            self.assertAllClose(mvn.entropy(), 4.3157 * torch.ones(2, device=device, dtype=dtype))\n            logprob = mvn.log_prob(torch.zeros(2, 3, device=device, dtype=dtype))\n            logprob_expected = -4.8157 * torch.ones(2, device=device, dtype=dtype)\n            self.assertAllClose(logprob, logprob_expected)\n            logprob = mvn.log_prob(torch.zeros(2, 2, 3, device=device, dtype=dtype))\n            logprob_expected = -4.8157 * torch.ones(2, 2, device=device, dtype=dtype)\n            self.assertAllClose(logprob, logprob_expected)\n            conf_lower, conf_upper = mvn.confidence_region()\n            self.assertAllClose(conf_lower, mvn.mean - 2 * mvn.stddev)\n            self.assertAllClose(conf_upper, mvn.mean + 2 * mvn.stddev)\n            self.assertTrue(mvn.sample().shape == torch.Size([2, 3]))\n            self.assertTrue(mvn.sample(torch.Size([2])).shape == torch.Size([2, 2, 3]))\n            self.assertTrue(mvn.sample(torch.Size([2, 4])).shape == torch.Size([2, 4, 2, 3]))\n\n    def test_multivariate_normal_batch_non_lazy_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_multivariate_normal_batch_non_lazy(cuda=True)\n\n    def test_multivariate_normal_lazy(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.tensor([0, 1, 2], device=device, dtype=dtype)\n            covmat = torch.diag(torch.tensor([1, 0.75, 1.5], device=device, dtype=dtype))\n            covmat_chol = torch.cholesky(covmat)\n            mvn = MultivariateNormal(mean=mean, covariance_matrix=NonLazyTensor(covmat))\n            self.assertTrue(torch.is_tensor(mvn.covariance_matrix))\n            self.assertIsInstance(mvn.lazy_covariance_matrix, LazyTensor)\n            self.assertAllClose(mvn.variance, torch.diag(covmat))\n            self.assertAllClose(mvn.covariance_matrix, covmat)\n            self.assertAllClose(mvn._unbroadcasted_scale_tril, covmat_chol)\n            mvn_plus1 = mvn + 1\n            self.assertAllClose(mvn_plus1.mean, mvn.mean + 1)\n            self.assertAllClose(mvn_plus1.covariance_matrix, mvn.covariance_matrix)\n            self.assertAllClose(mvn_plus1._unbroadcasted_scale_tril, covmat_chol)\n            mvn_times2 = mvn * 2\n            self.assertAllClose(mvn_times2.mean, mvn.mean * 2)\n            self.assertAllClose(mvn_times2.covariance_matrix, mvn.covariance_matrix * 4)\n            self.assertAllClose(mvn_times2._unbroadcasted_scale_tril, covmat_chol * 2)\n            mvn_divby2 = mvn / 2\n            self.assertAllClose(mvn_divby2.mean, mvn.mean / 2)\n            self.assertAllClose(mvn_divby2.covariance_matrix, mvn.covariance_matrix / 4)\n            self.assertAllClose(mvn_divby2._unbroadcasted_scale_tril, covmat_chol / 2)\n            # TODO: Add tests for entropy, log_prob, etc. - this an issue b/c it\n            # uses using root_decomposition which is not very reliable\n            # self.assertAlmostEqual(mvn.entropy().item(), 4.3157, places=4)\n            # self.assertAlmostEqual(mvn.log_prob(torch.zeros(3)).item(), -4.8157, places=4)\n            # self.assertTrue(\n            #     torch.allclose(\n            #         mvn.log_prob(torch.zeros(2, 3)), -4.8157 * torch.ones(2))\n            #     )\n            # )\n            conf_lower, conf_upper = mvn.confidence_region()\n            self.assertAllClose(conf_lower, mvn.mean - 2 * mvn.stddev)\n            self.assertAllClose(conf_upper, mvn.mean + 2 * mvn.stddev)\n            self.assertTrue(mvn.sample().shape == torch.Size([3]))\n            self.assertTrue(mvn.sample(torch.Size([2])).shape == torch.Size([2, 3]))\n            self.assertTrue(mvn.sample(torch.Size([2, 4])).shape == torch.Size([2, 4, 3]))\n\n    def test_multivariate_normal_lazy_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_multivariate_normal_lazy(cuda=True)\n\n    def test_multivariate_normal_batch_lazy(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.tensor([0, 1, 2], device=device, dtype=dtype).repeat(2, 1)\n            covmat = torch.diag(torch.tensor([1, 0.75, 1.5], device=device, dtype=dtype)).repeat(2, 1, 1)\n            covmat_chol = torch.cholesky(covmat)\n            mvn = MultivariateNormal(mean=mean, covariance_matrix=NonLazyTensor(covmat))\n            self.assertTrue(torch.is_tensor(mvn.covariance_matrix))\n            self.assertIsInstance(mvn.lazy_covariance_matrix, LazyTensor)\n            self.assertAllClose(mvn.variance, torch.diagonal(covmat, dim1=-2, dim2=-1))\n            self.assertAllClose(mvn._unbroadcasted_scale_tril, covmat_chol)\n            mvn_plus1 = mvn + 1\n            self.assertAllClose(mvn_plus1.mean, mvn.mean + 1)\n            self.assertAllClose(mvn_plus1.covariance_matrix, mvn.covariance_matrix)\n            self.assertAllClose(mvn_plus1._unbroadcasted_scale_tril, covmat_chol)\n            mvn_times2 = mvn * 2\n            self.assertAllClose(mvn_times2.mean, mvn.mean * 2)\n            self.assertAllClose(mvn_times2.covariance_matrix, mvn.covariance_matrix * 4)\n            self.assertAllClose(mvn_times2._unbroadcasted_scale_tril, covmat_chol * 2)\n            mvn_divby2 = mvn / 2\n            self.assertAllClose(mvn_divby2.mean, mvn.mean / 2)\n            self.assertAllClose(mvn_divby2.covariance_matrix, mvn.covariance_matrix / 4)\n            self.assertAllClose(mvn_divby2._unbroadcasted_scale_tril, covmat_chol / 2)\n            # TODO: Add tests for entropy, log_prob, etc. - this an issue b/c it\n            # uses using root_decomposition which is not very reliable\n            # self.assertTrue(torch.allclose(mvn.entropy(), 4.3157 * torch.ones(2)))\n            # self.assertTrue(\n            #     torch.allclose(mvn.log_prob(torch.zeros(2, 3)), -4.8157 * torch.ones(2))\n            # )\n            # self.assertTrue(\n            #     torch.allclose(mvn.log_prob(torch.zeros(2, 2, 3)), -4.8157 * torch.ones(2, 2))\n            # )\n            conf_lower, conf_upper = mvn.confidence_region()\n            self.assertAllClose(conf_lower, mvn.mean - 2 * mvn.stddev)\n            self.assertAllClose(conf_upper, mvn.mean + 2 * mvn.stddev)\n            self.assertTrue(mvn.sample().shape == torch.Size([2, 3]))\n            self.assertTrue(mvn.sample(torch.Size([2])).shape == torch.Size([2, 2, 3]))\n            self.assertTrue(mvn.sample(torch.Size([2, 4])).shape == torch.Size([2, 4, 2, 3]))\n\n    def test_multivariate_normal_batch_lazy_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_multivariate_normal_batch_lazy(cuda=True)\n\n    def test_multivariate_normal_correlated_samples(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.tensor([0, 1, 2], device=device, dtype=dtype)\n            covmat = torch.diag(torch.tensor([1, 0.75, 1.5], device=device, dtype=dtype))\n            mvn = MultivariateNormal(mean=mean, covariance_matrix=NonLazyTensor(covmat))\n            base_samples = mvn.get_base_samples(torch.Size([3, 4]))\n            self.assertTrue(mvn.sample(base_samples=base_samples).shape == torch.Size([3, 4, 3]))\n            base_samples = mvn.get_base_samples()\n            self.assertTrue(mvn.sample(base_samples=base_samples).shape == torch.Size([3]))\n\n    def test_multivariate_normal_correlated_samples_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_multivariate_normal_correlated_samples(cuda=True)\n\n    def test_multivariate_normal_batch_correlated_samples(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.tensor([0, 1, 2], device=device, dtype=dtype)\n            covmat = torch.diag(torch.tensor([1, 0.75, 1.5], device=device, dtype=dtype))\n            mvn = MultivariateNormal(mean=mean.repeat(2, 1), covariance_matrix=NonLazyTensor(covmat).repeat(2, 1, 1))\n            base_samples = mvn.get_base_samples(torch.Size((3, 4)))\n            self.assertTrue(mvn.sample(base_samples=base_samples).shape == torch.Size([3, 4, 2, 3]))\n            base_samples = mvn.get_base_samples()\n            self.assertTrue(mvn.sample(base_samples=base_samples).shape == torch.Size([2, 3]))\n\n    def test_multivariate_normal_batch_correlated_samples_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_multivariate_normal_batch_correlated_samples(cuda=True)\n\n    def test_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean = torch.randn(4, device=device, dtype=dtype)\n            var = torch.randn(4, device=device, dtype=dtype).abs_()\n            values = torch.randn(4, device=device, dtype=dtype)\n\n            res = MultivariateNormal(mean, DiagLazyTensor(var)).log_prob(values)\n            actual = TMultivariateNormal(mean, torch.eye(4, device=device, dtype=dtype) * var).log_prob(values)\n            self.assertLess((res - actual).div(res).abs().item(), 1e-2)\n\n            mean = torch.randn(3, 4, device=device, dtype=dtype)\n            var = torch.randn(3, 4, device=device, dtype=dtype).abs_()\n            values = torch.randn(3, 4, device=device, dtype=dtype)\n\n            res = MultivariateNormal(mean, DiagLazyTensor(var)).log_prob(values)\n            actual = TMultivariateNormal(\n                mean, var.unsqueeze(-1) * torch.eye(4, device=device, dtype=dtype).repeat(3, 1, 1)\n            ).log_prob(values)\n            self.assertLess((res - actual).div(res).abs().norm(), 1e-2)\n\n    def test_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_log_prob(cuda=True)\n\n    def test_kl_divergence(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            mean0 = torch.randn(4, device=device, dtype=dtype)\n            mean1 = mean0 + 1\n            var0 = torch.randn(4, device=device, dtype=dtype).abs_()\n            var1 = var0 * math.exp(2)\n\n            dist_a = MultivariateNormal(mean0, DiagLazyTensor(var0))\n            dist_b = MultivariateNormal(mean1, DiagLazyTensor(var0))\n            dist_c = MultivariateNormal(mean0, DiagLazyTensor(var1))\n\n            res = torch.distributions.kl.kl_divergence(dist_a, dist_a)\n            actual = 0.0\n            self.assertLess((res - actual).abs().item(), 1e-2)\n\n            res = torch.distributions.kl.kl_divergence(dist_b, dist_a)\n            actual = var0.reciprocal().sum().div(2.0)\n            self.assertLess((res - actual).div(res).abs().item(), 1e-2)\n\n            res = torch.distributions.kl.kl_divergence(dist_a, dist_c)\n            actual = 0.5 * (8 - 4 + 4 * math.exp(-2))\n            self.assertLess((res - actual).div(res).abs().item(), 1e-2)\n\n    def test_kl_divergence_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_kl_divergence(cuda=True)\n\n    def test_getitem(self):\n        shape = (2, 4, 3, 2)\n        cov = torch.randn(*shape, shape[1])\n        cov = cov @ cov.transpose(-1, -2)\n        mean = torch.randn(*shape)\n        dist = MultivariateNormal(mean, cov)\n        dist_cov = dist.covariance_matrix\n\n        d = dist[1]\n        assert torch.equal(d.mean, dist.mean[1])\n        self.assertAllClose(d.covariance_matrix, dist_cov[1])\n\n        d = dist[..., 1]\n        assert torch.equal(d.mean, dist.mean[..., 1])\n        cov = dist_cov[..., 1, 1]\n        self.assertAllClose(d.covariance_matrix, cov.unsqueeze(-1) * torch.eye(shape[-2]))\n\n        d = dist[:, [2, 3], :, 1:]\n        assert torch.equal(d.mean, dist.mean[:, [2, 3], :, 1:])\n        self.assertAllClose(d.covariance_matrix, dist_cov[:, [2, 3], :, 1:, 1:])\n\n        d = dist[:, :, ..., [0, 1, 1, 0]]\n        assert torch.equal(d.mean, dist.mean[..., [0, 1, 1, 0]])\n        self.assertAllClose(d.covariance_matrix, dist_cov[..., [0, 1, 1, 0], :][..., [0, 1, 1, 0]])\n\n        d = dist[1, 2, 2, ...]\n        assert torch.equal(d.mean, dist.mean[1, 2, 2, :])\n        self.assertAllClose(d.covariance_matrix, dist_cov[1, 2, 2, :, :])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/__init__.py,0,b'#!/usr/bin/env python3\n'
test/examples/test_batch_gp_regression.py,50,"b'#!/usr/bin/env python3\n\nimport math\nimport os\nimport random\nimport unittest\n\nimport gpytorch\nimport torch\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom torch import optim\n\n\n# Batch training test: Let\'s learn hyperparameters on a sine dataset, but test on a sine dataset and a cosine dataset\n# in parallel.\ntrain_x1 = torch.linspace(0, 2, 11).unsqueeze(-1)\ntrain_y1 = torch.sin(train_x1 * (2 * math.pi)).squeeze()\ntrain_y1.add_(torch.randn_like(train_y1).mul_(0.01))\ntest_x1 = torch.linspace(0, 2, 51).unsqueeze(-1)\ntest_y1 = torch.sin(test_x1 * (2 * math.pi)).squeeze()\n\ntrain_x2 = torch.linspace(0, 1, 11).unsqueeze(-1)\ntrain_y2 = torch.sin(train_x2 * (2 * math.pi)).squeeze()\ntrain_y2.add_(torch.randn_like(train_y2).mul_(0.01))\ntest_x2 = torch.linspace(0, 1, 51).unsqueeze(-1)\ntest_y2 = torch.sin(test_x2 * (2 * math.pi)).squeeze()\n\n# Combined sets of data\ntrain_x12 = torch.cat((train_x1.unsqueeze(0), train_x2.unsqueeze(0)), dim=0).contiguous()\ntrain_y12 = torch.cat((train_y1.unsqueeze(0), train_y2.unsqueeze(0)), dim=0).contiguous()\ntest_x12 = torch.cat((test_x1.unsqueeze(0), test_x2.unsqueeze(0)), dim=0).contiguous()\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_inputs, train_targets, likelihood, batch_shape=torch.Size()):\n        super(ExactGPModel, self).__init__(train_inputs, train_targets, likelihood)\n        self.mean_module = ConstantMean(batch_shape=batch_shape, prior=gpytorch.priors.SmoothedBoxPrior(-1, 1))\n        self.covar_module = ScaleKernel(\n            RBFKernel(\n                batch_shape=batch_shape,\n                lengthscale_prior=gpytorch.priors.NormalPrior(\n                    loc=torch.zeros(*batch_shape, 1, 1), scale=torch.ones(*batch_shape, 1, 1)\n                ),\n            ),\n            batch_shape=batch_shape,\n            outputscale_prior=gpytorch.priors.SmoothedBoxPrior(-2, 2),\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestBatchGPRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_train_on_single_set_test_on_batch(self):\n        # We\'re manually going to set the hyperparameters to something they shouldn\'t be\n        likelihood = GaussianLikelihood()\n        gp_model = ExactGPModel(train_x1, train_y1, likelihood)\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(gp_model.parameters(), lr=0.1)\n        optimizer.n_iter = 0\n        for _ in range(75):\n            optimizer.zero_grad()\n            output = gp_model(train_x1)\n            loss = -mll(output, train_y1).sum()\n            loss.backward()\n            optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n        # Test the model\n        gp_model.eval()\n        likelihood.eval()\n\n        # First test on non-batch\n        non_batch_predictions = likelihood(gp_model(test_x1))\n        preds1 = non_batch_predictions.mean\n        mean_abs_error1 = torch.mean(torch.abs(test_y1 - preds1))\n        self.assertLess(mean_abs_error1.squeeze().item(), 0.1)\n\n        # Make predictions for both sets of test points, and check MAEs.\n        batch_predictions = likelihood(gp_model(test_x12))\n        preds1 = batch_predictions.mean[0]\n        preds2 = batch_predictions.mean[1]\n        mean_abs_error1 = torch.mean(torch.abs(test_y1 - preds1))\n        mean_abs_error2 = torch.mean(torch.abs(test_y2 - preds2))\n        self.assertLess(mean_abs_error1.squeeze().item(), 0.1)\n        self.assertLess(mean_abs_error2.squeeze().item(), 0.1)\n\n        # Smoke test for non-batch mode derivatives failing\n        test_x_param = torch.nn.Parameter(test_x1.data)\n        batch_predictions = likelihood(gp_model(test_x_param))\n        batch_predictions.mean.sum().backward()\n        self.assertTrue(test_x_param.grad is not None)\n\n        # Smoke test for batch mode derivatives failing\n        test_x_param = torch.nn.Parameter(test_x12.data)\n        batch_predictions = likelihood(gp_model(test_x_param))\n        batch_predictions.mean.sum().backward()\n        self.assertTrue(test_x_param.grad is not None)\n\n    def test_train_on_batch_test_on_batch(self):\n        # We\'re manually going to set the hyperparameters to something they shouldn\'t be\n        likelihood = GaussianLikelihood(\n            noise_prior=gpytorch.priors.NormalPrior(loc=torch.zeros(2), scale=torch.ones(2)),\n            batch_shape=torch.Size([2]),\n        )\n        gp_model = ExactGPModel(train_x12, train_y12, likelihood, batch_shape=torch.Size([2]))\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(gp_model.parameters(), lr=0.1)\n        for _ in range(50):\n            optimizer.zero_grad()\n            output = gp_model(train_x12)\n            loss = -mll(output, train_y12, train_x12).sum()\n            loss.backward()\n            optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n        # Test the model\n        gp_model.eval()\n        likelihood.eval()\n\n        # First test on non-batch\n        non_batch_predictions = likelihood(gp_model(test_x1))\n        preds1 = non_batch_predictions.mean\n        mean_abs_error1 = torch.mean(torch.abs(test_y1 - preds1[0]))\n        self.assertLess(mean_abs_error1.squeeze().item(), 0.1)\n\n        # Make predictions for both sets of test points, and check MAEs.\n        batch_predictions = likelihood(gp_model(test_x12))\n        preds1 = batch_predictions.mean[0]\n        preds2 = batch_predictions.mean[1]\n        mean_abs_error1 = torch.mean(torch.abs(test_y1 - preds1))\n        mean_abs_error2 = torch.mean(torch.abs(test_y2 - preds2))\n        self.assertLess(mean_abs_error1.squeeze().item(), 0.1)\n        self.assertLess(mean_abs_error2.squeeze().item(), 0.1)\n\n        # Smoke test for batch mode derivatives failing\n        test_x_param = torch.nn.Parameter(test_x12.data)\n        batch_predictions = likelihood(gp_model(test_x_param))\n        batch_predictions.mean.sum().backward()\n        self.assertTrue(test_x_param.grad is not None)\n\n        # Smoke test for non-batch mode derivatives failing\n        test_x_param = torch.nn.Parameter(test_x1.data)\n        batch_predictions = likelihood(gp_model(test_x_param))\n        batch_predictions.mean.sum().backward()\n        self.assertTrue(test_x_param.grad is not None)\n\n    def test_train_on_batch_test_on_batch_shared_hypers_over_batch(self):\n        # We\'re manually going to set the hyperparameters to something they shouldn\'t be\n        likelihood = GaussianLikelihood(\n            noise_prior=gpytorch.priors.NormalPrior(loc=torch.zeros(2), scale=torch.ones(2))\n        )\n        gp_model = ExactGPModel(train_x12, train_y12, likelihood)\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(gp_model.parameters(), lr=0.1)\n        for _ in range(50):\n            optimizer.zero_grad()\n            output = gp_model(train_x12)\n            loss = -mll(output, train_y12, train_x12).sum()\n            loss.backward()\n            optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n        # Test the model\n        gp_model.eval()\n        likelihood.eval()\n\n        # First test on non-batch\n        non_batch_predictions = likelihood(gp_model(test_x1))\n        preds1 = non_batch_predictions.mean\n        mean_abs_error1 = torch.mean(torch.abs(test_y1 - preds1[0]))\n        self.assertLess(mean_abs_error1.squeeze().item(), 0.1)\n\n        # Make predictions for both sets of test points, and check MAEs.\n        batch_predictions = likelihood(gp_model(test_x12))\n        preds1 = batch_predictions.mean[0]\n        preds2 = batch_predictions.mean[1]\n        mean_abs_error1 = torch.mean(torch.abs(test_y1 - preds1))\n        mean_abs_error2 = torch.mean(torch.abs(test_y2 - preds2))\n        self.assertLess(mean_abs_error1.squeeze().item(), 0.1)\n        self.assertLess(mean_abs_error2.squeeze().item(), 0.1)\n\n        # Smoke test for batch mode derivatives failing\n        test_x_param = torch.nn.Parameter(test_x12.data)\n        batch_predictions = likelihood(gp_model(test_x_param))\n        batch_predictions.mean.sum().backward()\n        self.assertTrue(test_x_param.grad is not None)\n\n        # Smoke test for non-batch mode derivatives failing\n        test_x_param = torch.nn.Parameter(test_x1.data)\n        batch_predictions = likelihood(gp_model(test_x_param))\n        batch_predictions.mean.sum().backward()\n        self.assertTrue(test_x_param.grad is not None)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_batch_multitask_gp_regression.py,40,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport math\nimport torch\nimport unittest\n\nimport gpytorch\nfrom torch import optim\nfrom gpytorch.kernels import RBFKernel, MultitaskKernel\nfrom gpytorch.means import ConstantMean, MultitaskMean\nfrom gpytorch.likelihoods import MultitaskGaussianLikelihoodKronecker\nfrom gpytorch.distributions import MultitaskMultivariateNormal\n\n\n# Batch training test: Let\'s learn hyperparameters on a sine dataset, but test on a sine dataset and a cosine dataset\n# in parallel.\ntrain_x1 = torch.linspace(0, 1, 11).unsqueeze(-1)\ntrain_y1 = torch.cat([torch.sin(train_x1 * (2 * math.pi)), torch.cos(train_x1 * (2 * math.pi))], 1)\ntest_x1 = torch.linspace(0, 1, 51).unsqueeze(-1)\ntest_y1 = torch.cat([torch.sin(test_x1 * (2 * math.pi)), torch.cos(test_x1 * (2 * math.pi))], 1)\n\ntrain_x2 = torch.linspace(0, 1, 11).unsqueeze(-1)\ntrain_y2 = torch.cat([torch.sin(train_x2 * (2 * math.pi)), torch.cos(train_x2 * (2 * math.pi))], 1)\ntest_x2 = torch.linspace(0, 1, 51).unsqueeze(-1)\ntest_y2 = torch.cat([torch.sin(test_x2 * (2 * math.pi)), torch.cos(test_x2 * (2 * math.pi))], 1)\n\n# Combined sets of data\ntrain_x12 = torch.cat((train_x1.unsqueeze(0), train_x2.unsqueeze(0)), dim=0).contiguous()\ntrain_y12 = torch.cat((train_y1.unsqueeze(0), train_y2.unsqueeze(0)), dim=0).contiguous()\ntest_x12 = torch.cat((test_x1.unsqueeze(0), test_x2.unsqueeze(0)), dim=0).contiguous()\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_inputs, train_targets, likelihood, batch_shape=torch.Size()):\n        super(ExactGPModel, self).__init__(train_inputs, train_targets, likelihood)\n        self.mean_module = MultitaskMean(\n            ConstantMean(batch_shape=batch_shape, prior=gpytorch.priors.SmoothedBoxPrior(-1, 1)), num_tasks=2\n        )\n        self.covar_module = MultitaskKernel(\n            RBFKernel(\n                batch_shape=batch_shape,\n                lengthscale_prior=gpytorch.priors.NormalPrior(loc=torch.tensor(0.0), scale=torch.tensor(1.0)),\n            ),\n            num_tasks=2,\n            rank=1,\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultitaskMultivariateNormal(mean_x, covar_x)\n\n\nclass TestBatchMultitaskGPRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_train_on_single_set_test_on_batch(self):\n        # We\'re manually going to set the hyperparameters to something they shouldn\'t be\n        likelihood = MultitaskGaussianLikelihoodKronecker(\n            noise_prior=gpytorch.priors.NormalPrior(loc=torch.zeros(1), scale=torch.ones(1)), num_tasks=2\n        )\n        gp_model = ExactGPModel(train_x1, train_y1, likelihood)\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n        optimizer.n_iter = 0\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(gp_model.parameters(), lr=0.1)\n        for _ in range(50):\n            optimizer.zero_grad()\n            output = gp_model(train_x1)\n            loss = -mll(output, train_y1).sum()\n            loss.backward()\n            optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n        # Test the model\n        gp_model.eval()\n        likelihood.eval()\n\n        # Make predictions for both sets of test points, and check MAEs.\n        batch_predictions = likelihood(gp_model(test_x12))\n        preds1 = batch_predictions.mean[0]\n        preds2 = batch_predictions.mean[1]\n        mean_abs_error1 = torch.mean(torch.abs(test_y1 - preds1))\n        mean_abs_error2 = torch.mean(torch.abs(test_y2 - preds2))\n        self.assertLess(mean_abs_error1.squeeze().item(), 0.05)\n        self.assertLess(mean_abs_error2.squeeze().item(), 0.05)\n\n    def test_train_on_batch_test_on_batch(self):\n        # We\'re manually going to set the hyperparameters to something they shouldn\'t be\n        likelihood = MultitaskGaussianLikelihoodKronecker(\n            noise_prior=gpytorch.priors.NormalPrior(loc=torch.zeros(2), scale=torch.ones(2)),\n            batch_shape=torch.Size([2]),\n            num_tasks=2,\n        )\n        gp_model = ExactGPModel(train_x12, train_y12, likelihood, batch_shape=torch.Size([2]))\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(gp_model.parameters(), lr=0.1)\n        for _ in range(50):\n            optimizer.zero_grad()\n            output = gp_model(train_x12)\n            loss = -mll(output, train_y12).sum()\n            loss.backward()\n            optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n        # Test the model\n        gp_model.eval()\n        likelihood.eval()\n\n        # Make predictions for both sets of test points, and check MAEs.\n        batch_predictions = likelihood(gp_model(test_x12))\n        preds1 = batch_predictions.mean[0]\n        preds2 = batch_predictions.mean[1]\n        mean_abs_error1 = torch.mean(torch.abs(test_y1 - preds1))\n        mean_abs_error2 = torch.mean(torch.abs(test_y2 - preds2))\n        self.assertLess(mean_abs_error1.squeeze().item(), 0.05)\n        self.assertLess(mean_abs_error2.squeeze().item(), 0.05)\n\n    def test_train_on_batch_test_on_batch_shared_hypers_over_batch(self):\n        # We\'re manually going to set the hyperparameters to something they shouldn\'t be\n        likelihood = MultitaskGaussianLikelihoodKronecker(\n            noise_prior=gpytorch.priors.NormalPrior(loc=torch.zeros(2), scale=torch.ones(2)),\n            batch_shape=torch.Size(),\n            num_tasks=2,\n        )\n        gp_model = ExactGPModel(train_x12, train_y12, likelihood, batch_shape=torch.Size())\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(gp_model.parameters(), lr=0.1)\n        for _ in range(50):\n            optimizer.zero_grad()\n            output = gp_model(train_x12)\n            loss = -mll(output, train_y12).sum()\n            loss.backward()\n            optimizer.step()\n\n        for param in gp_model.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for param in likelihood.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n\n        # Test the model\n        gp_model.eval()\n        likelihood.eval()\n\n        # Make predictions for both sets of test points, and check MAEs.\n        batch_predictions = likelihood(gp_model(test_x12))\n        preds1 = batch_predictions.mean[0]\n        preds2 = batch_predictions.mean[1]\n        mean_abs_error1 = torch.mean(torch.abs(test_y1 - preds1))\n        mean_abs_error2 = torch.mean(torch.abs(test_y2 - preds2))\n        self.assertLess(mean_abs_error1.squeeze().item(), 0.05)\n        self.assertLess(mean_abs_error2.squeeze().item(), 0.05)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_batch_svgp_gp_regression.py,32,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport unittest\nfrom math import pi\n\nimport gpytorch\nimport torch\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.test.utils import least_used_cuda_device\nfrom gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\nfrom torch import optim\n\n\ndef train_data(cuda=False):\n    train_x = torch.linspace(0, 1, 260).unsqueeze(-1)\n    train_y_cos = torch.cos(train_x * (2 * pi)).squeeze() + 0.01 * torch.randn(260)\n    train_y_sin = torch.sin(train_x * (2 * pi)).squeeze() + 0.01 * torch.randn(260)\n\n    # Make train_x (2 x 260 x 1) and train_y (2 x 260)\n    train_x = torch.cat([train_x, train_x], dim=1).transpose(-2, 1).unsqueeze(-1)\n    train_y = torch.cat([train_y_cos.unsqueeze(-1), train_y_sin.unsqueeze(-1)], dim=1).transpose(-2, -1)\n    if cuda:\n        return train_x.cuda(), train_y.cuda()\n    else:\n        return train_x, train_y\n\n\nclass SVGPRegressionModel(ApproximateGP):\n    def __init__(self, inducing_points):\n        variational_distribution = CholeskyVariationalDistribution(\n            inducing_points.size(-2), batch_shape=torch.Size([2])\n        )\n        variational_strategy = VariationalStrategy(\n            self, inducing_points, variational_distribution, learn_inducing_locations=True\n        )\n        super(SVGPRegressionModel, self).__init__(variational_strategy)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(\n            gpytorch.kernels.RBFKernel(lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(0.001, 1.0, sigma=0.1))\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n        return latent_pred\n\n\nclass TestSVGPRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_regression_error(self):\n        train_x, train_y = train_data()\n        likelihood = GaussianLikelihood()\n        inducing_points = torch.linspace(0, 1, 25).unsqueeze(-1).repeat(2, 1, 1)\n        model = SVGPRegressionModel(inducing_points)\n        mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(-1))\n\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n        optimizer = optim.Adam([{""params"": model.parameters()}, {""params"": likelihood.parameters()}], lr=0.01)\n        for _ in range(180):\n            optimizer.zero_grad()\n            output = model(train_x)\n            loss = -mll(output, train_y)\n            loss = loss.sum()\n            loss.backward()\n            optimizer.step()\n\n        for param in model.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for param in likelihood.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n\n        # Set back to eval mode\n        model.eval()\n        likelihood.eval()\n        test_preds = likelihood(model(train_x)).mean.squeeze()\n        mean_abs_error = torch.mean(torch.abs(train_y[0, :] - test_preds[0, :]) / 2)\n        mean_abs_error2 = torch.mean(torch.abs(train_y[1, :] - test_preds[1, :]) / 2)\n        self.assertLess(mean_abs_error.item(), 1e-1)\n        self.assertLess(mean_abs_error2.item(), 1e-1)\n\n    def test_regression_error_shared_inducing_locations(self):\n        train_x, train_y = train_data()\n        likelihood = GaussianLikelihood()\n        inducing_points = torch.linspace(0, 1, 25).unsqueeze(-1)\n        model = SVGPRegressionModel(inducing_points)\n        mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(-1))\n\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n        optimizer = optim.Adam([{""params"": model.parameters()}, {""params"": likelihood.parameters()}], lr=0.01)\n        for _ in range(200):\n            optimizer.zero_grad()\n            output = model(train_x)\n            loss = -mll(output, train_y)\n            loss = loss.sum()\n            loss.backward()\n            optimizer.step()\n\n        for param in model.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for param in likelihood.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n\n        # Set back to eval mode\n        model.eval()\n        likelihood.eval()\n        test_preds = likelihood(model(train_x)).mean.squeeze()\n        mean_abs_error = torch.mean(torch.abs(train_y[0, :] - test_preds[0, :]) / 2)\n        mean_abs_error2 = torch.mean(torch.abs(train_y[1, :] - test_preds[1, :]) / 2)\n        self.assertLess(mean_abs_error.item(), 1e-1)\n        self.assertLess(mean_abs_error2.item(), 1e-1)\n\n    def test_regression_error_cuda(self):\n        if not torch.cuda.is_available():\n            return\n        with least_used_cuda_device():\n            train_x, train_y = train_data(cuda=True)\n            likelihood = GaussianLikelihood().cuda()\n            inducing_points = torch.linspace(0, 1, 25).unsqueeze(-1).repeat(2, 1, 1)\n            model = SVGPRegressionModel(inducing_points).cuda()\n            mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(-1))\n\n            # Find optimal model hyperparameters\n            model.train()\n            likelihood.train()\n            optimizer = optim.Adam([{""params"": model.parameters()}, {""params"": likelihood.parameters()}], lr=0.01)\n            for _ in range(150):\n                optimizer.zero_grad()\n                output = model(train_x)\n                loss = -mll(output, train_y)\n                loss = loss.sum()\n                loss.backward()\n                optimizer.step()\n\n            for param in model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Set back to eval mode\n            model.eval()\n            likelihood.eval()\n            test_preds = likelihood(model(train_x)).mean.squeeze()\n            mean_abs_error = torch.mean(torch.abs(train_y[0, :] - test_preds[0, :]) / 2)\n            mean_abs_error2 = torch.mean(torch.abs(train_y[1, :] - test_preds[1, :]) / 2)\n            self.assertLess(mean_abs_error.item(), 1e-1)\n            self.assertLess(mean_abs_error2.item(), 1e-1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_decoupled_svgp_regression.py,21,"b'#!/usr/bin/env python3\n\nimport math\nimport warnings\nimport unittest\nfrom unittest.mock import MagicMock, patch\n\nimport gpytorch\nimport torch\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.test.base_test_case import BaseTestCase\nfrom gpytorch.utils.warnings import ExtraComputationWarning\nfrom torch import optim\n\n\ndef train_data():\n    train_x = torch.linspace(0, 1, 260)\n    train_y = torch.cos(train_x * (2 * math.pi))\n    return train_x, train_y\n\n\nclass SVGPRegressionModel(ApproximateGP):\n    def __init__(self, inducing_points, base_inducing_points):\n        base_variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n            base_inducing_points.size(-1)\n        )\n        variational_distribution = gpytorch.variational.DeltaVariationalDistribution(inducing_points.size(-1))\n        variational_strategy = gpytorch.variational.OrthogonallyDecoupledVariationalStrategy(\n            gpytorch.variational.VariationalStrategy(\n                self, base_inducing_points, base_variational_distribution, learn_inducing_locations=True,\n            ), inducing_points, variational_distribution\n        )\n        super(SVGPRegressionModel, self).__init__(variational_strategy)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n        return latent_pred\n\n\nclass TestSVGPRegression(BaseTestCase, unittest.TestCase):\n    seed = 0\n\n    def test_regression_error(\n        self,\n        mll_cls=gpytorch.mlls.VariationalELBO,\n        distribution_cls=gpytorch.variational.CholeskyVariationalDistribution,\n    ):\n        train_x, train_y = train_data()\n        likelihood = GaussianLikelihood()\n        model = SVGPRegressionModel(torch.linspace(0, 1, 128), torch.linspace(0, 1, 16))\n        mll = mll_cls(likelihood, model, num_data=len(train_y))\n\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n        optimizer = optim.Adam([{""params"": model.parameters()}, {""params"": likelihood.parameters()}], lr=0.01)\n\n        _wrapped_cg = MagicMock(wraps=gpytorch.utils.linear_cg)\n        _cg_mock = patch(""gpytorch.utils.linear_cg"", new=_wrapped_cg)\n        with warnings.catch_warnings(record=True) as ws, _cg_mock as cg_mock:\n            for _ in range(75):\n                optimizer.zero_grad()\n                output = model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.step()\n\n            for param in model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Set back to eval mode\n            model.eval()\n            likelihood.eval()\n            test_preds = likelihood(model(train_x)).mean.squeeze()\n            mean_abs_error = torch.mean(torch.abs(train_y - test_preds) / 2)\n            self.assertLess(mean_abs_error.item(), 1e-1)\n\n            # Make sure CG was called (or not), and no warnings were thrown\n            self.assertFalse(cg_mock.called)\n            self.assertFalse(any(issubclass(w.category, ExtraComputationWarning) for w in ws))\n\n    def test_predictive_ll_regression_error(self):\n        return self.test_regression_error(\n            mll_cls=gpytorch.mlls.PredictiveLogLikelihood,\n            distribution_cls=gpytorch.variational.MeanFieldVariationalDistribution,\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_fixed_noise_fanatasy_updates.py,28,"b'#!/usr/bin/env python3\n\nimport unittest\nfrom math import exp, pi\n\nimport gpytorch\nimport torch\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import FixedNoiseGaussianLikelihood\nfrom gpytorch.likelihoods.noise_models import FixedGaussianNoise\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.test.base_test_case import BaseTestCase\nfrom gpytorch.test.utils import least_used_cuda_device\nfrom torch import optim\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_inputs, train_targets, likelihood):\n        super(ExactGPModel, self).__init__(train_inputs, train_targets, likelihood)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1, 1))\n        self.covar_module = ScaleKernel(RBFKernel(lengthscale_prior=SmoothedBoxPrior(exp(-3), exp(3), sigma=0.1)))\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestFixedNoiseFantasies(BaseTestCase, unittest.TestCase):\n    seed = 1\n\n    def _get_data(self, cuda=False, num_data=11, add_noise=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        # Simple training data: let\'s try to learn a sine function\n        train_x = torch.linspace(0, 1, num_data, device=device)\n        train_y = torch.sin(train_x * (2 * pi))\n        if add_noise:\n            train_y.add_(torch.randn_like(train_x).mul_(0.1))\n        test_x = torch.linspace(0, 1, 51, device=device)\n        test_y = torch.sin(test_x * (2 * pi))\n        return train_x, test_x, train_y, test_y\n\n    def test_fixed_noise_fantasy_updates_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_fixed_noise_fantasy_updates(cuda=True)\n\n    def test_fixed_noise_fantasy_updates(self, cuda=False):\n        train_x, test_x, train_y, test_y = self._get_data(cuda=cuda)\n        noise = torch.full_like(train_y, 2e-4)\n        test_noise = torch.full_like(test_y, 3e-4)\n\n        likelihood = FixedNoiseGaussianLikelihood(noise)\n        gp_model = ExactGPModel(train_x, train_y, likelihood)\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n        gp_model.covar_module.base_kernel.initialize(lengthscale=exp(1))\n        gp_model.mean_module.initialize(constant=0)\n\n        if cuda:\n            gp_model.cuda()\n            likelihood.cuda()\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.15)\n        for _ in range(50):\n            optimizer.zero_grad()\n            with gpytorch.settings.debug(False):\n                output = gp_model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.step()\n\n        for param in gp_model.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        optimizer.step()\n\n        train_x.requires_grad = True\n        gp_model.set_train_data(train_x, train_y)\n        with gpytorch.settings.fast_pred_var(), gpytorch.settings.detach_test_caches(False):\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n            test_function_predictions = likelihood(gp_model(test_x), noise=test_noise)\n            test_function_predictions.mean.sum().backward()\n\n            real_fant_x_grad = train_x.grad[5:].clone()\n            train_x.grad = None\n            train_x.requires_grad = False\n            gp_model.set_train_data(train_x, train_y)\n\n            # Cut data down, and then add back via the fantasy interface\n            gp_model.set_train_data(train_x[:5], train_y[:5], strict=False)\n            gp_model.likelihood.noise_covar = FixedGaussianNoise(noise=noise[:5])\n            likelihood(gp_model(test_x), noise=test_noise)\n\n            fantasy_x = train_x[5:].clone().detach().requires_grad_(True)\n            fant_model = gp_model.get_fantasy_model(fantasy_x, train_y[5:], noise=noise[5:])\n\n            fant_function_predictions = likelihood(fant_model(test_x), noise=test_noise)\n\n            self.assertAllClose(test_function_predictions.mean, fant_function_predictions.mean, atol=1e-4)\n\n            fant_function_predictions.mean.sum().backward()\n            self.assertTrue(fantasy_x.grad is not None)\n\n            relative_error = torch.norm(real_fant_x_grad - fantasy_x.grad) / fantasy_x.grad.norm()\n            self.assertLess(relative_error, 15e-1)  # This was only passing by a hair before\n\n    def test_fixed_noise_fantasy_updates_batch_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_fixed_noise_fantasy_updates_batch(cuda=True)\n\n    def test_fixed_noise_fantasy_updates_batch(self, cuda=False):\n        train_x, test_x, train_y, test_y = self._get_data(cuda=cuda)\n        noise = torch.full_like(train_y, 2e-4)\n        test_noise = torch.full_like(test_y, 3e-4)\n\n        likelihood = FixedNoiseGaussianLikelihood(noise)\n        gp_model = ExactGPModel(train_x, train_y, likelihood)\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n        gp_model.covar_module.base_kernel.initialize(lengthscale=exp(1))\n        gp_model.mean_module.initialize(constant=0)\n\n        if cuda:\n            gp_model.cuda()\n            likelihood.cuda()\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.15)\n        for _ in range(50):\n            optimizer.zero_grad()\n            with gpytorch.settings.debug(False):\n                output = gp_model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.step()\n\n        for param in gp_model.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        optimizer.step()\n\n        with gpytorch.settings.fast_pred_var():\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n            test_function_predictions = likelihood(gp_model(test_x), noise=test_noise)\n\n            # Cut data down, and then add back via the fantasy interface\n            gp_model.set_train_data(train_x[:5], train_y[:5], strict=False)\n            gp_model.likelihood.noise_covar = FixedGaussianNoise(noise=noise[:5])\n            likelihood(gp_model(test_x), noise=test_noise)\n\n            fantasy_x = train_x[5:].clone().unsqueeze(0).unsqueeze(-1).repeat(3, 1, 1).requires_grad_(True)\n            fantasy_y = train_y[5:].unsqueeze(0).repeat(3, 1)\n            fant_model = gp_model.get_fantasy_model(fantasy_x, fantasy_y, noise=noise[5:].unsqueeze(0).repeat(3, 1))\n            fant_function_predictions = likelihood(fant_model(test_x), noise=test_noise)\n\n            self.assertAllClose(test_function_predictions.mean, fant_function_predictions.mean[0], atol=1e-4)\n\n            fant_function_predictions.mean.sum().backward()\n            self.assertTrue(fantasy_x.grad is not None)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_grid_gp_regression.py,24,"b'#!/usr/bin/env python3\n\nimport warnings\n\nimport math\nimport os\nimport random\nimport unittest\n\nimport gpytorch\nimport torch\nfrom gpytorch.test.utils import least_used_cuda_device\nfrom gpytorch.utils.warnings import GPInputWarning\nfrom torch import optim\n\n\ndef make_data(grid, cuda=False):\n    train_x = gpytorch.utils.grid.create_data_from_grid(grid)\n    train_y = torch.sin((train_x.sum(-1)) * (2 * math.pi)) + torch.randn_like(train_x[:, 0]).mul(0.01)\n    n = 20\n    test_x = torch.zeros(int(pow(n, 2)), 2)\n    for i in range(n):\n        for j in range(n):\n            test_x[i * n + j][0] = float(i) / (n - 1)\n            test_x[i * n + j][1] = float(j) / (n - 1)\n    test_y = torch.sin(((test_x.sum(-1)) * (2 * math.pi)))\n    if cuda:\n        train_x = train_x.cuda()\n        train_y = train_y.cuda()\n        test_x = test_x.cuda()\n        test_y = test_y.cuda()\n    return train_x, train_y, test_x, test_y\n\n\nclass GridGPRegressionModel(gpytorch.models.ExactGP):\n    def __init__(self, grid, train_x, train_y, likelihood):\n        super(GridGPRegressionModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.GridKernel(gpytorch.kernels.RBFKernel(), grid=grid)\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass TestGridGPRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_grid_gp_mean_abs_error(self, num_dim=1, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        grid_bounds = [(0, 1)] if num_dim == 1 else [(0, 1), (0, 2)]\n        grid_size = 25\n        grid = torch.zeros(grid_size, len(grid_bounds), device=device)\n        for i in range(len(grid_bounds)):\n            grid_diff = float(grid_bounds[i][1] - grid_bounds[i][0]) / (grid_size - 2)\n            grid[:, i] = torch.linspace(\n                grid_bounds[i][0] - grid_diff, grid_bounds[i][1] + grid_diff, grid_size, device=device\n            )\n\n        train_x, train_y, test_x, test_y = make_data(grid, cuda=cuda)\n        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n        gp_model = GridGPRegressionModel(grid, train_x, train_y, likelihood)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        if cuda:\n            gp_model.cuda()\n            likelihood.cuda()\n\n        # Optimize the model\n        gp_model.train()\n        likelihood.train()\n\n        optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n        optimizer.n_iter = 0\n        with gpytorch.settings.debug(True):\n            for _ in range(20):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for name, param in gp_model.named_parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n            # Make sure we don\'t get GP input warnings for testing on training data\n            warnings.simplefilter(""ignore"", GPInputWarning)\n\n            train_preds = likelihood(gp_model(train_x)).mean\n            mean_abs_error = torch.mean(torch.abs(train_y - train_preds))\n\n        self.assertLess(mean_abs_error.squeeze().item(), 0.3)\n\n    def test_grid_gp_mean_abs_error_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_grid_gp_mean_abs_error(cuda=True)\n\n    def test_grid_gp_mean_abs_error_2d(self):\n        self.test_grid_gp_mean_abs_error(num_dim=2)\n\n    def test_grid_gp_mean_abs_error_2d_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_grid_gp_mean_abs_error(cuda=True, num_dim=2)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_hadamard_multitask_gp_regression.py,27,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport unittest\nfrom math import exp, pi\n\nimport gpytorch\nimport torch\nfrom gpytorch.kernels import IndexKernel, RBFKernel\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import LKJCovariancePrior, SmoothedBoxPrior\nfrom gpytorch.distributions import MultivariateNormal\nfrom torch import optim\n\n# Simple training data: let\'s try to learn a sine function\ntrain_x = torch.linspace(0, 1, 100)\ny1_inds = torch.zeros(100, dtype=torch.long)\ny2_inds = torch.ones(100, dtype=torch.long)\ntrain_y1 = torch.sin(train_x * (2 * pi)) + torch.randn_like(train_x).mul_(1e-2)\ntrain_y2 = torch.cos(train_x * (2 * pi)) + torch.randn_like(train_x).mul_(1e-2)\n\ntest_x = torch.linspace(0, 1, 51)\ny1_inds_test = torch.zeros(51, dtype=torch.long)\ny2_inds_test = torch.ones(51, dtype=torch.long)\ntest_y1 = torch.sin(test_x * (2 * pi))\ntest_y2 = torch.cos(test_x * (2 * pi))\n\n\nclass HadamardMultitaskGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(HadamardMultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n        # Default bounds on mean are (-1e10, 1e10)\n        self.mean_module = ConstantMean()\n        # We use the very common RBF kernel\n        self.covar_module = RBFKernel()\n        # We learn an IndexKernel for 2 tasks\n        # (so we\'ll actually learn 2x2=4 tasks with correlations)\n        sd_prior = SmoothedBoxPrior(exp(-4), exp(4))\n        cov_prior = LKJCovariancePrior(n=2, eta=1, sd_prior=sd_prior)\n        self.task_covar_module = IndexKernel(num_tasks=2, rank=1, prior=cov_prior)\n\n    def forward(self, x, i):\n        # Get predictive mean\n        mean_x = self.mean_module(x)\n        # Get all covariances, we\'ll look up the task-speicific ones\n        covar_x = self.covar_module(x)\n        # # Get the covariance for task i\n        covar_i = self.task_covar_module(i)\n        covar_xi = covar_x.mul(covar_i)\n        return MultivariateNormal(mean_x, covar_xi)\n\n\nclass TestHadamardMultitaskGPRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_multitask_gp_mean_abs_error(self):\n        likelihood = GaussianLikelihood(noise_prior=SmoothedBoxPrior(-6, 6))\n        gp_model = HadamardMultitaskGPModel(\n            (torch.cat([train_x, train_x]), torch.cat([y1_inds, y2_inds])), torch.cat([train_y1, train_y2]), likelihood\n        )\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Optimize the model\n        gp_model.train()\n        likelihood.eval()\n        optimizer = optim.Adam(gp_model.parameters(), lr=0.01)\n        for _ in range(100):\n            optimizer.zero_grad()\n            output = gp_model(torch.cat([train_x, train_x]), torch.cat([y1_inds, y2_inds]))\n            loss = -mll(output, torch.cat([train_y1, train_y2]))\n            loss.backward()\n            optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n        # Test the model\n        gp_model.eval()\n        likelihood.eval()\n        test_preds_task_1 = likelihood(gp_model(test_x, y1_inds_test)).mean\n        mean_abs_error_task_1 = torch.mean(torch.abs(test_y1 - test_preds_task_1))\n\n        self.assertLess(mean_abs_error_task_1.item(), 0.1)\n\n        test_preds_task_2 = likelihood(gp_model(test_x, y2_inds_test)).mean\n        mean_abs_error_task_2 = torch.mean(torch.abs(test_y2 - test_preds_task_2))\n\n        self.assertLess(mean_abs_error_task_2.item(), 0.1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_independent_multitask_gp_regression.py,25,"b'#!/usr/bin/env python3\n\nimport math\nimport os\nimport random\nimport unittest\n\nimport gpytorch\nimport torch\nfrom gpytorch.distributions import MultivariateNormal, MultitaskMultivariateNormal\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import MultitaskGaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom torch import optim\n\n\n# Batch training test: Let\'s learn hyperparameters on a sine dataset, but test on a sine dataset and a cosine dataset\n# in parallel.\ntrain_x = torch.linspace(0, 2, 51).unsqueeze(-1)\ntest_x = torch.linspace(0, 2, 11).unsqueeze(-1)\ntrain_y1 = torch.sin(train_x * (2 * math.pi)).squeeze()\ntrain_y1.add_(torch.randn_like(train_y1).mul_(0.01))\ntest_y1 = torch.sin(test_x * (2 * math.pi)).squeeze()\ntrain_y2 = torch.sin(train_x * (2 * math.pi)).squeeze()\ntrain_y2.add_(torch.randn_like(train_y2).mul_(0.01))\ntest_y2 = torch.sin(test_x * (2 * math.pi)).squeeze()\n\n# Combined sets of data\ntrain_y12 = torch.stack((train_y1, train_y2), dim=-1).contiguous()\ntest_y12 = torch.stack((test_y1, test_y2), dim=-1).contiguous()\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_inputs, train_targets, likelihood, batch_shape=torch.Size([2])):\n        super(ExactGPModel, self).__init__(train_inputs, train_targets, likelihood)\n        self.mean_module = ConstantMean(batch_shape=batch_shape)\n        self.covar_module = ScaleKernel(RBFKernel(batch_shape=batch_shape), batch_shape=batch_shape)\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultitaskMultivariateNormal.from_batch_mvn(MultivariateNormal(mean_x, covar_x))\n\n\nclass TestIndependentMultitaskGPRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_train_and_eval(self):\n        # We\'re manually going to set the hyperparameters to something they shouldn\'t be\n        likelihood = MultitaskGaussianLikelihood(num_tasks=2)\n        gp_model = ExactGPModel(train_x, train_y12, likelihood)\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(gp_model.parameters(), lr=0.1)\n        optimizer.n_iter = 0\n        for _ in range(75):\n            optimizer.zero_grad()\n            output = gp_model(train_x)\n            loss = -mll(output, train_y12).sum()\n            loss.backward()\n            optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n        # Test the model\n        gp_model.eval()\n        likelihood.eval()\n\n        # Make predictions for both sets of test points, and check MAEs.\n        with torch.no_grad(), gpytorch.settings.max_eager_kernel_size(1):\n            batch_predictions = likelihood(gp_model(test_x))\n            preds1 = batch_predictions.mean[:, 0]\n            preds2 = batch_predictions.mean[:, 1]\n            mean_abs_error1 = torch.mean(torch.abs(test_y1 - preds1))\n            mean_abs_error2 = torch.mean(torch.abs(test_y2 - preds2))\n            self.assertLess(mean_abs_error1.squeeze().item(), 0.01)\n            self.assertLess(mean_abs_error2.squeeze().item(), 0.01)\n\n            # Smoke test for getting predictive uncertainties\n            lower, upper = batch_predictions.confidence_region()\n            self.assertEqual(lower.shape, test_y12.shape)\n            self.assertEqual(upper.shape, test_y12.shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_kissgp_additive_classification.py,19,"b'#!/usr/bin/env python3\n\nfrom math import exp\n\nimport os\nimport random\nimport torch\nimport unittest\n\nimport gpytorch\nfrom torch import optim\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import BernoulliLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.variational import AdditiveGridInterpolationVariationalStrategy, CholeskyVariationalDistribution\n\nn = 64\ntrain_x = torch.zeros(n ** 2, 2)\ntrain_x[:, 0].copy_(torch.linspace(-1, 1, n).repeat(n))\ntrain_x[:, 1].copy_(torch.linspace(-1, 1, n).unsqueeze(1).repeat(1, n).view(-1))\ntrain_y = train_x[:, 0].abs().lt(0.5).float()\ntrain_y = train_y * (train_x[:, 1].abs().lt(0.5)).float()\ntrain_y = train_y.float()\n\n\nclass GPClassificationModel(ApproximateGP):\n    def __init__(self, grid_size=16, grid_bounds=([-1, 1],)):\n        variational_distribution = CholeskyVariationalDistribution(num_inducing_points=16, batch_shape=torch.Size([2]))\n        variational_strategy = AdditiveGridInterpolationVariationalStrategy(\n            self,\n            grid_size=grid_size,\n            grid_bounds=grid_bounds,\n            num_dim=2,\n            variational_distribution=variational_distribution,\n        )\n        super(GPClassificationModel, self).__init__(variational_strategy)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1e-5, 1e-5))\n        self.covar_module = ScaleKernel(\n            RBFKernel(ard_num_dims=1, lengthscale_prior=SmoothedBoxPrior(exp(-5), exp(6), sigma=0.1)),\n            outputscale_prior=SmoothedBoxPrior(exp(-5), exp(6), sigma=0.1),\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        latent_pred = MultivariateNormal(mean_x, covar_x)\n        return latent_pred\n\n\nclass TestKISSGPAdditiveClassification(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_kissgp_classification_error(self):\n        with gpytorch.settings.use_toeplitz(False), gpytorch.settings.max_preconditioner_size(5):\n            model = GPClassificationModel()\n            likelihood = BernoulliLikelihood()\n            mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(train_y))\n\n            # Find optimal model hyperparameters\n            model.train()\n            likelihood.train()\n\n            optimizer = optim.Adam(model.parameters(), lr=0.15)\n            optimizer.n_iter = 0\n            for _ in range(25):\n                optimizer.zero_grad()\n                # Get predictive output\n                output = model(train_x)\n                # Calc loss and backprop gradients\n                loss = -mll(output, train_y).sum()\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Set back to eval mode\n            model.eval()\n            likelihood.eval()\n\n            test_preds = model(train_x).mean.ge(0.5).float()\n            mean_abs_error = torch.mean(torch.abs(train_y - test_preds) / 2)\n\n        self.assertLess(mean_abs_error.squeeze().item(), 0.15)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_kissgp_additive_regression.py,19,"b'#!/usr/bin/env python3\n\nimport random\nimport os\nimport torch\nimport unittest\n\nimport gpytorch\nfrom torch import optim\nfrom gpytorch.kernels import RBFKernel, AdditiveStructureKernel, GridInterpolationKernel, ScaleKernel\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.means import ZeroMean\nfrom gpytorch.distributions import MultivariateNormal\n\nn = 20\ntrain_x = torch.zeros(pow(n, 2), 2)\nfor i in range(n):\n    for j in range(n):\n        train_x[i * n + j][0] = float(i) / (n - 1)\n        train_x[i * n + j][1] = float(j) / (n - 1)\ntrain_y = torch.sin(train_x[:, 0]) + torch.cos(train_x[:, 1])\ntrain_y = train_y + torch.randn_like(train_y).div_(20.0)\n\nm = 10\ntest_x = torch.zeros(pow(m, 2), 2)\nfor i in range(m):\n    for j in range(m):\n        test_x[i * m + j][0] = float(i) / (m - 1)\n        test_x[i * m + j][1] = float(j) / (m - 1)\ntest_y = torch.sin(test_x[:, 0]) + torch.cos(test_x[:, 1])\n\n\n# All tests that pass with the exact kernel should pass with the interpolated kernel.\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = ZeroMean()\n        self.base_covar_module = ScaleKernel(RBFKernel())\n        self.covar_module = AdditiveStructureKernel(\n            GridInterpolationKernel(self.base_covar_module, grid_size=100, num_dims=1), num_dims=2\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestKISSGPAdditiveRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(1)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(1)\n            random.seed(1)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_kissgp_gp_mean_abs_error(self):\n        likelihood = GaussianLikelihood()\n        gp_model = GPRegressionModel(train_x, train_y, likelihood)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        with gpytorch.settings.max_preconditioner_size(10), gpytorch.settings.max_cg_iterations(50):\n            with gpytorch.settings.fast_pred_var():\n                # Optimize the model\n                gp_model.train()\n                likelihood.train()\n\n                optimizer = optim.Adam(gp_model.parameters(), lr=0.01)\n                optimizer.n_iter = 0\n                for _ in range(15):\n                    optimizer.zero_grad()\n                    output = gp_model(train_x)\n                    loss = -mll(output, train_y)\n                    loss.backward()\n                    optimizer.n_iter += 1\n                    optimizer.step()\n\n                    for param in gp_model.parameters():\n                        self.assertTrue(param.grad is not None)\n                        self.assertGreater(param.grad.norm().item(), 0)\n                    for param in likelihood.parameters():\n                        self.assertTrue(param.grad is not None)\n                        self.assertGreater(param.grad.norm().item(), 0)\n\n                # Test the model\n                gp_model.eval()\n                likelihood.eval()\n\n                test_preds = likelihood(gp_model(test_x)).mean\n                mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n                self.assertLess(mean_abs_error.squeeze().item(), 0.2)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_kissgp_dkl_regression.py,21,"b'#!/usr/bin/env python3\n\nfrom math import exp, pi\n\nimport os\nimport random\nimport torch\nimport unittest\n\nimport gpytorch\nfrom torch import optim, nn\nfrom gpytorch.kernels import RBFKernel, GridInterpolationKernel, ScaleKernel\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.distributions import MultivariateNormal\n\n\n# Simple training data: let\'s try to learn a sine function,\n# but with KISS-GP let\'s use 100 training examples.\ndef make_data(cuda=False):\n    train_x = torch.linspace(0, 1, 100)\n    train_y = torch.sin(train_x * (2 * pi))\n    test_x = torch.linspace(0, 1, 51)\n    test_y = torch.sin(test_x * (2 * pi))\n    if cuda:\n        train_x = train_x.cuda()\n        train_y = train_y.cuda()\n        test_x = test_x.cuda()\n        test_y = test_y.cuda()\n    return train_x, train_y, test_x, test_y\n\n\ndata_dim = 1\n\n\nclass SmallFeatureExtractor(nn.Sequential):\n    def __init__(self):\n        super(SmallFeatureExtractor, self).__init__()\n        self.add_module(""linear1"", nn.Linear(data_dim, 10))\n        self.add_module(""relu3"", nn.ReLU())\n        self.add_module(""linear4"", nn.Linear(10, 1))\n\n\nfeature_extractor = SmallFeatureExtractor()\n\n\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1e-5, 1e-5))\n        self.base_covar_module = ScaleKernel(RBFKernel(lengthscale_prior=SmoothedBoxPrior(exp(-5), exp(6), sigma=0.1)))\n        self.covar_module = GridInterpolationKernel(self.base_covar_module, grid_size=50, num_dims=1)\n        self.feature_extractor = feature_extractor\n\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        mean_x = self.mean_module(features)\n        covar_x = self.covar_module(features)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestDKLRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_dkl_gp_mean_abs_error(self):\n        train_x, train_y, test_x, test_y = make_data()\n        likelihood = GaussianLikelihood()\n        gp_model = GPRegressionModel(train_x, train_y, likelihood)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Optimize the model\n        gp_model.train()\n        likelihood.train()\n\n        optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n        optimizer.n_iter = 0\n        with gpytorch.settings.debug(False):\n            for _ in range(30):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n\n            test_preds = likelihood(gp_model(test_x)).mean\n            mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n\n            self.assertLess(mean_abs_error.squeeze().item(), 0.15)\n\n    def test_dkl_gp_fast_pred_var(self):\n        with gpytorch.settings.fast_pred_var(), gpytorch.settings.debug(False):\n            train_x, train_y, test_x, test_y = make_data()\n            likelihood = GaussianLikelihood()\n            gp_model = GPRegressionModel(train_x, train_y, likelihood)\n            mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n            # Optimize the model\n            gp_model.train()\n            likelihood.train()\n\n            optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n            optimizer.n_iter = 0\n            for _ in range(30):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n            # Set the cache\n            test_function_predictions = likelihood(gp_model(train_x))\n\n            # Now bump up the likelihood to something huge\n            # This will make it easy to calculate the variance\n            likelihood.raw_noise.data.fill_(3)\n            test_function_predictions = likelihood(gp_model(train_x))\n\n            noise = likelihood.noise\n            var_diff = (test_function_predictions.variance - noise).abs()\n            self.assertLess(torch.max(var_diff / noise), 0.15)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_kissgp_gp_classification.py,17,"b'#!/usr/bin/env python3\n\nfrom math import exp, pi\n\nimport os\nimport random\nimport torch\nimport unittest\n\nimport gpytorch\nfrom torch import optim\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import BernoulliLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.distributions import MultivariateNormal\n\n\ntrain_x = torch.linspace(0, 1, 10)\ntrain_y = torch.sign(torch.cos(train_x * (16 * pi))).add(1).div(2)\n\n\nclass GPClassificationModel(gpytorch.models.ApproximateGP):\n    def __init__(self, grid_size=32, grid_bounds=[(0, 1)]):\n        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n            num_inducing_points=int(pow(grid_size, len(grid_bounds)))\n        )\n        variational_strategy = gpytorch.variational.GridInterpolationVariationalStrategy(\n            self, grid_size=grid_size, grid_bounds=grid_bounds, variational_distribution=variational_distribution\n        )\n        super(GPClassificationModel, self).__init__(variational_strategy)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-5, 5))\n        self.covar_module = ScaleKernel(\n            RBFKernel(lengthscale_prior=SmoothedBoxPrior(exp(-5), exp(6), sigma=0.1)),\n            outputscale_prior=SmoothedBoxPrior(exp(-5), exp(6), sigma=0.1),\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        latent_pred = MultivariateNormal(mean_x, covar_x)\n        return latent_pred\n\n\nclass TestKISSGPClassification(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_kissgp_classification_error(self):\n        model = GPClassificationModel()\n        likelihood = BernoulliLikelihood()\n        mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(train_y))\n\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n\n        optimizer = optim.SGD(model.parameters(), lr=0.01)\n        optimizer.n_iter = 0\n        for _ in range(200):\n            optimizer.zero_grad()\n            output = model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.n_iter += 1\n            optimizer.step()\n\n        for _, param in model.named_parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n\n        for param in likelihood.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n\n        # Set back to eval mode\n        model.eval()\n        likelihood.eval()\n        test_preds = likelihood(model(train_x)).mean.ge(0.5).float()\n        mean_abs_error = torch.mean(torch.abs(train_y - test_preds) / 2)\n        self.assertLess(mean_abs_error.squeeze().item(), 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_kissgp_gp_regression.py,26,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport unittest\nfrom math import exp, pi\n\nimport gpytorch\nimport torch\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.kernels import GridInterpolationKernel, RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.test.utils import least_used_cuda_device\nfrom torch import optim\n\n\n# Simple training data: let\'s try to learn a sine function,\n# but with KISS-GP let\'s use 100 training examples.\ndef make_data(cuda=False):\n    train_x = torch.linspace(0, 1, 100)\n    train_y = torch.sin(train_x * (2 * pi))\n    test_x = torch.linspace(0, 1, 51)\n    test_y = torch.sin(test_x * (2 * pi))\n    if cuda:\n        train_x = train_x.cuda()\n        train_y = train_y.cuda()\n        test_x = test_x.cuda()\n        test_y = test_y.cuda()\n    return train_x, train_y, test_x, test_y\n\n\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1e-5, 1e-5))\n        self.base_covar_module = ScaleKernel(RBFKernel(lengthscale_prior=SmoothedBoxPrior(exp(-5), exp(6), sigma=0.1)))\n        self.covar_module = GridInterpolationKernel(self.base_covar_module, grid_size=50, num_dims=1)\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestKISSGPRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_kissgp_gp_mean_abs_error(self):\n        train_x, train_y, test_x, test_y = make_data()\n        likelihood = GaussianLikelihood()\n        gp_model = GPRegressionModel(train_x, train_y, likelihood)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Optimize the model\n        gp_model.train()\n        likelihood.train()\n\n        optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n        optimizer.n_iter = 0\n        with gpytorch.settings.debug(False):\n            for _ in range(25):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n\n            test_preds = likelihood(gp_model(test_x)).mean\n            mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n\n        self.assertLess(mean_abs_error.squeeze().item(), 0.05)\n\n    def test_kissgp_gp_fast_pred_var(self):\n        with gpytorch.settings.fast_pred_var(), gpytorch.settings.debug(False):\n            train_x, train_y, test_x, test_y = make_data()\n            likelihood = GaussianLikelihood()\n            gp_model = GPRegressionModel(train_x, train_y, likelihood)\n            mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n            # Optimize the model\n            gp_model.train()\n            likelihood.train()\n\n            optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n            optimizer.n_iter = 0\n            for _ in range(25):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n            # Set the cache\n            test_function_predictions = likelihood(gp_model(train_x))\n\n            # Now bump up the likelihood to something huge\n            # This will make it easy to calculate the variance\n            likelihood.raw_noise.data.fill_(3)\n            test_function_predictions = likelihood(gp_model(train_x))\n\n            noise = likelihood.noise\n            var_diff = (test_function_predictions.variance - noise).abs()\n            self.assertLess(torch.max(var_diff / noise), 0.05)\n\n    def test_kissgp_gp_mean_abs_error_cuda(self):\n        if not torch.cuda.is_available():\n            return\n        with least_used_cuda_device():\n            train_x, train_y, test_x, test_y = make_data(cuda=True)\n            likelihood = GaussianLikelihood().cuda()\n            gp_model = GPRegressionModel(train_x, train_y, likelihood).cuda()\n            mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n            # Optimize the model\n            gp_model.train()\n            likelihood.train()\n\n            optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n            optimizer.n_iter = 0\n            with gpytorch.settings.debug(False):\n                for _ in range(25):\n                    optimizer.zero_grad()\n                    output = gp_model(train_x)\n                    loss = -mll(output, train_y)\n                    loss.backward()\n                    optimizer.n_iter += 1\n                    optimizer.step()\n\n                for param in gp_model.parameters():\n                    self.assertTrue(param.grad is not None)\n                    self.assertGreater(param.grad.norm().item(), 0)\n                for param in likelihood.parameters():\n                    self.assertTrue(param.grad is not None)\n                    self.assertGreater(param.grad.norm().item(), 0)\n\n                # Test the model\n                gp_model.eval()\n                likelihood.eval()\n                test_preds = likelihood(gp_model(test_x)).mean\n                mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n\n            self.assertLess(mean_abs_error.squeeze().item(), 0.02)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_kissgp_kronecker_product_classification.py,18,"b'#!/usr/bin/env python3\n\nfrom math import exp\n\nimport os\nimport random\nimport torch\nimport unittest\n\nimport gpytorch\nfrom torch import optim\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.likelihoods import BernoulliLikelihood\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.distributions import MultivariateNormal\n\nn = 4\ntrain_x = torch.zeros(pow(n, 2), 2)\ntrain_y = torch.zeros(pow(n, 2))\nfor i in range(n):\n    for j in range(n):\n        train_x[i * n + j][0] = float(i) / (n - 1)\n        train_x[i * n + j][1] = float(j) / (n - 1)\n        train_y[i * n + j] = pow(-1, int(i / 2) + int(j / 2))\ntrain_x = train_x\ntrain_y = train_y.add(1).div(2)\n\n\nclass GPClassificationModel(gpytorch.models.ApproximateGP):\n    def __init__(self, grid_size=6, grid_bounds=[(-0.33, 1.33), (-0.33, 1.33)]):\n        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n            num_inducing_points=int(pow(grid_size, len(grid_bounds)))\n        )\n        variational_strategy = gpytorch.variational.GridInterpolationVariationalStrategy(\n            self, grid_size=grid_size, grid_bounds=grid_bounds, variational_distribution=variational_distribution\n        )\n        super(GPClassificationModel, self).__init__(variational_strategy)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1e-5, 1e-5))\n        self.covar_module = ScaleKernel(\n            RBFKernel(ard_num_dims=2, lengthscale_prior=SmoothedBoxPrior(exp(-2.5), exp(3), sigma=0.1))\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        latent_pred = MultivariateNormal(mean_x, covar_x)\n        return latent_pred\n\n\nclass TestKISSGPKroneckerProductClassification(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_kissgp_classification_error(self):\n        model = GPClassificationModel()\n        likelihood = BernoulliLikelihood()\n        mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(train_y))\n\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n\n        with gpytorch.settings.max_preconditioner_size(5):\n            optimizer = optim.Adam(model.parameters(), lr=0.15)\n            optimizer.n_iter = 0\n            for _ in range(100):\n                optimizer.zero_grad()\n                output = model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Set back to eval mode\n            model.eval()\n            likelihood.eval()\n\n            test_preds = model(train_x).mean.ge(0.5).float()\n            mean_abs_error = torch.mean(torch.abs(train_y - test_preds) / 2)\n            self.assertLess(mean_abs_error.squeeze().item(), 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_kissgp_kronecker_product_regression.py,20,"b'#!/usr/bin/env python3\n\nfrom math import exp, pi\n\nimport os\nimport random\nimport torch\nimport unittest\n\nimport gpytorch\nfrom gpytorch.kernels import RBFKernel, GridInterpolationKernel, ScaleKernel\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.distributions import MultivariateNormal\nfrom torch import optim\n\n# Simple training data: let\'s try to learn a sine function,\n# but with KISS-GP let\'s use 100 training examples.\nn = 40\ntrain_x = torch.zeros(pow(n, 2), 2)\nfor i in range(n):\n    for j in range(n):\n        train_x[i * n + j][0] = float(i) / (n - 1)\n        train_x[i * n + j][1] = float(j) / (n - 1)\ntrain_x = train_x\ntrain_y = torch.sin(((train_x[:, 0] + train_x[:, 1]) * (2 * pi)))\ntrain_y = train_y + torch.randn_like(train_y).mul_(0.01)\n\nm = 10\ntest_x = torch.zeros(pow(m, 2), 2)\nfor i in range(m):\n    for j in range(m):\n        test_x[i * m + j][0] = float(i) / (m - 1)\n        test_x[i * m + j][1] = float(j) / (m - 1)\ntest_x = test_x\ntest_y = torch.sin((test_x[:, 0] + test_x[:, 1]) * (2 * pi))\ntest_y = test_y + torch.randn_like(test_y).mul_(0.01)\n\n\n# All tests that pass with the exact kernel should pass with the interpolated kernel.\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1, 1))\n        self.base_covar_module = ScaleKernel(\n            RBFKernel(ard_num_dims=2, lengthscale_prior=SmoothedBoxPrior(exp(-3), exp(3), sigma=0.1))\n        )\n        self.covar_module = GridInterpolationKernel(self.base_covar_module, grid_size=64, num_dims=2)\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestKISSGPKroneckerProductRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_kissgp_gp_mean_abs_error(self):\n        likelihood = GaussianLikelihood()\n        gp_model = GPRegressionModel(train_x, train_y, likelihood)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Optimize the model\n        gp_model.train()\n        likelihood.train()\n\n        with gpytorch.settings.max_preconditioner_size(5), gpytorch.settings.use_toeplitz(False):\n            optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n            optimizer.n_iter = 0\n            for _ in range(8):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n\n            test_preds = likelihood(gp_model(test_x)).mean\n            mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n            self.assertLess(mean_abs_error.squeeze().item(), 0.2)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_kissgp_multiplicative_regression.py,18,"b'#!/usr/bin/env python3\n\nfrom math import pi\n\nimport os\nimport random\nimport torch\nimport unittest\n\nimport gpytorch\nfrom torch import optim\nfrom gpytorch.kernels import RBFKernel, ProductStructureKernel, GridInterpolationKernel, ScaleKernel\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.distributions import MultivariateNormal\n\n\n# Simple training data: let\'s try to learn a sine function,\n# but with KISS-GP let\'s use 100 training examples.\nn = 30\ntrain_x = torch.zeros(pow(n, 2), 2)\nfor i in range(n):\n    for j in range(n):\n        train_x[i * n + j][0] = float(i) / (n - 1)\n        train_x[i * n + j][1] = float(j) / (n - 1)\ntrain_x = train_x\ntrain_y = (torch.sin(train_x[:, 0]) + torch.cos(train_x[:, 1])) * (2 * pi)\n\nm = 10\ntest_x = torch.zeros(pow(m, 2), 2)\nfor i in range(m):\n    for j in range(m):\n        test_x[i * m + j][0] = float(i) / (m - 1)\n        test_x[i * m + j][1] = float(j) / (m - 1)\ntest_x = test_x\ntest_y = (torch.sin(test_x[:, 0]) + torch.cos(test_x[:, 1])) * (2 * pi)\n\n\n# All tests that pass with the exact kernel should pass with the interpolated kernel.\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1, 1))\n        self.base_covar_module = ScaleKernel(RBFKernel())\n        self.covar_module = ProductStructureKernel(\n            GridInterpolationKernel(self.base_covar_module, grid_size=100, num_dims=1), num_dims=2\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestKISSGPMultiplicativeRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_kissgp_gp_mean_abs_error(self):\n        likelihood = GaussianLikelihood()\n        gp_model = GPRegressionModel(train_x, train_y, likelihood)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Optimize the model\n        gp_model.train()\n        likelihood.train()\n\n        optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.2)\n        optimizer.n_iter = 0\n        for _ in range(15):\n            optimizer.zero_grad()\n            output = gp_model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.n_iter += 1\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            optimizer.step()\n\n        # Test the model\n        gp_model.eval()\n        likelihood.eval()\n\n        with gpytorch.settings.fast_pred_var():\n            test_preds = likelihood(gp_model(test_x)).mean\n        mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n        self.assertLess(mean_abs_error.squeeze().item(), 0.15)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_kissgp_variational_regression.py,23,"b'#!/usr/bin/env python3\n\nfrom math import exp, pi\n\nimport os\nimport random\nimport torch\nimport unittest\n\nimport gpytorch\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.distributions import MultivariateNormal\nfrom torch.utils.data import TensorDataset, DataLoader\n\n\n# Simple training data: let\'s try to learn a sine function,\n# but with KISS-GP let\'s use 100 training examples.\ndef make_data():\n    train_x = torch.linspace(0, 1, 1000)\n    train_y = torch.sin(train_x * (4 * pi)) + torch.randn(train_x.size()) * 0.2\n    test_x = torch.linspace(0.02, 1, 51)\n    test_y = torch.sin(test_x * (4 * pi))\n    return train_x, train_y, test_x, test_y\n\n\nclass GPRegressionModel(gpytorch.models.ApproximateGP):\n    def __init__(self, grid_size=20, grid_bounds=[(-0.1, 1.1)]):\n        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n            num_inducing_points=int(pow(grid_size, len(grid_bounds)))\n        )\n        variational_strategy = gpytorch.variational.GridInterpolationVariationalStrategy(\n            self, grid_size=grid_size, grid_bounds=grid_bounds, variational_distribution=variational_distribution\n        )\n        super(GPRegressionModel, self).__init__(variational_strategy)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-10, 10))\n        self.covar_module = ScaleKernel(RBFKernel(lengthscale_prior=SmoothedBoxPrior(exp(-3), exp(6), sigma=0.1)))\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestKISSGPVariationalRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_kissgp_gp_mean_abs_error(self):\n        train_x, train_y, test_x, test_y = make_data()\n        train_dataset = TensorDataset(train_x, train_y)\n        train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64)\n\n        model = GPRegressionModel()\n        likelihood = GaussianLikelihood()\n        mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(train_y))\n        # We use SGD here, rather than Adam\n        # Emperically, we find that SGD is better for variational regression\n        optimizer = torch.optim.Adam([{""params"": model.parameters()}, {""params"": likelihood.parameters()}], lr=0.01)\n\n        # Our loss object\n        # We\'re using the VariationalELBO object\n        mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n\n        # The training loop\n        def train(n_epochs=15):\n            # We use a Learning rate scheduler from PyTorch to lower the learning rate during optimization\n            # We\'re going to drop the learning rate by 1/10 after 3/4 of training\n            # This helps the model converge to a minimum\n            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[0.75 * n_epochs], gamma=0.1)\n\n            for _ in range(n_epochs):\n                for x_batch, y_batch in train_loader:\n                    x_batch = x_batch.float()\n                    y_batch = y_batch.float()\n                    optimizer.zero_grad()\n                    output = model(x_batch)\n                    loss = -mll(output, y_batch)\n                    loss.backward()\n                    optimizer.step()\n\n                scheduler.step()\n        train()\n\n        for _, param in model.named_parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for param in likelihood.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n\n            # Test the model\n            model.eval()\n            likelihood.eval()\n\n            test_preds = likelihood(model(test_x)).mean\n            mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n\n        self.assertLess(mean_abs_error.squeeze().item(), 0.1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_kissgp_white_noise_regression.py,31,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport unittest\nimport warnings\nfrom math import exp, pi\n\nimport gpytorch\nimport torch\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.kernels import GridInterpolationKernel, RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import FixedNoiseGaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.test.utils import least_used_cuda_device\nfrom gpytorch.utils.warnings import GPInputWarning\nfrom torch import optim\n\n\n# Simple training data: let\'s try to learn a sine function,\n# but with KISS-GP let\'s use 100 training examples.\ndef make_data(cuda=False):\n    train_x = torch.linspace(0, 1, 100)\n    train_y = torch.sin(train_x * (2 * pi))\n    test_x = torch.linspace(0, 1, 51)\n    test_y = torch.sin(test_x * (2 * pi))\n    if cuda:\n        train_x = train_x.cuda()\n        train_y = train_y.cuda()\n        test_x = test_x.cuda()\n        test_y = test_y.cuda()\n    return train_x, train_y, test_x, test_y\n\n\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1e-5, 1e-5))\n        self.base_covar_module = ScaleKernel(RBFKernel(lengthscale_prior=SmoothedBoxPrior(exp(-5), exp(6), sigma=0.1)))\n        self.grid_covar_module = GridInterpolationKernel(self.base_covar_module, grid_size=50, num_dims=1)\n        self.covar_module = self.grid_covar_module\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestKISSGPWhiteNoiseRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_kissgp_gp_mean_abs_error(self):\n        # This test throws a warning because the fixed noise likelihood gets the wrong input\n        warnings.simplefilter(""ignore"", GPInputWarning)\n\n        train_x, train_y, test_x, test_y = make_data()\n        likelihood = FixedNoiseGaussianLikelihood(torch.ones(100) * 0.001)\n        gp_model = GPRegressionModel(train_x, train_y, likelihood)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Optimize the model\n        gp_model.train()\n        likelihood.train()\n\n        optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n        optimizer.n_iter = 0\n        with gpytorch.settings.debug(False):\n            for _ in range(25):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n\n            test_preds = likelihood(gp_model(test_x)).mean\n            mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n\n        self.assertLess(mean_abs_error.squeeze().item(), 0.05)\n\n    def test_kissgp_gp_fast_pred_var(self):\n        with gpytorch.settings.fast_pred_var(), gpytorch.settings.debug(False):\n            train_x, train_y, test_x, test_y = make_data()\n            likelihood = FixedNoiseGaussianLikelihood(torch.ones(100) * 0.001)\n            gp_model = GPRegressionModel(train_x, train_y, likelihood)\n            mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n            # Optimize the model\n            gp_model.train()\n            likelihood.train()\n\n            optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n            optimizer.n_iter = 0\n            for _ in range(25):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n            # Set the cache\n            test_function_predictions = likelihood(gp_model(train_x))\n\n            # Now bump up the likelihood to something huge\n            # This will make it easy to calculate the variance\n            likelihood.noise = torch.ones(100) * 3.0\n            test_function_predictions = likelihood(gp_model(train_x))\n\n            noise = likelihood.noise\n            var_diff = (test_function_predictions.variance - noise).abs()\n            self.assertLess(torch.max(var_diff / noise), 0.05)\n\n    def test_kissgp_gp_mean_abs_error_cuda(self):\n        if not torch.cuda.is_available():\n            return\n        with least_used_cuda_device():\n            train_x, train_y, test_x, test_y = make_data(cuda=True)\n            likelihood = FixedNoiseGaussianLikelihood(torch.ones(100) * 0.001).cuda()\n            gp_model = GPRegressionModel(train_x, train_y, likelihood).cuda()\n            mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n            # Optimize the model\n            gp_model.train()\n            likelihood.train()\n\n            optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n            optimizer.n_iter = 0\n            with gpytorch.settings.debug(False):\n                for _ in range(25):\n                    optimizer.zero_grad()\n                    output = gp_model(train_x)\n                    loss = -mll(output, train_y)\n                    loss.backward()\n                    optimizer.n_iter += 1\n                    optimizer.step()\n\n                for param in gp_model.parameters():\n                    self.assertTrue(param.grad is not None)\n                    self.assertGreater(param.grad.norm().item(), 0)\n                for param in likelihood.parameters():\n                    self.assertTrue(param.grad is not None)\n                    self.assertGreater(param.grad.norm().item(), 0)\n\n                # Test the model\n                gp_model.eval()\n                likelihood.eval()\n                test_preds = likelihood(gp_model(test_x)).mean\n                mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n\n            self.assertLess(mean_abs_error.squeeze().item(), 0.02)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_kronecker_multitask_gp_regression.py,21,"b'#!/usr/bin/env python3\n\nfrom math import pi\n\nimport os\nimport random\nimport torch\nimport unittest\n\nimport gpytorch\nfrom gpytorch.kernels import RBFKernel, MultitaskKernel\nfrom gpytorch.means import ConstantMean, MultitaskMean\nfrom gpytorch.likelihoods import MultitaskGaussianLikelihood\nfrom gpytorch.distributions import MultitaskMultivariateNormal\n\n\n# Simple training data: let\'s try to learn a sine function\ntrain_x = torch.linspace(0, 1, 100)\n\n# y1 function is sin(2*pi*x) with noise N(0, 0.04)\ntrain_y1 = torch.sin(train_x * (2 * pi)) + torch.randn(train_x.size()) * 0.1\n# y2 function is cos(2*pi*x) with noise N(0, 0.04)\ntrain_y2 = torch.cos(train_x * (2 * pi)) + torch.randn(train_x.size()) * 0.1\n\n# Create a train_y which interleaves the two\ntrain_y = torch.stack([train_y1, train_y2], -1)\n\n\nclass MultitaskGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = MultitaskMean(ConstantMean(), num_tasks=2)\n        self_covar_module = RBFKernel()\n        self.covar_module = MultitaskKernel(self_covar_module, num_tasks=2, rank=2)\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultitaskMultivariateNormal(mean_x, covar_x)\n\n\nclass TestKroneckerMultiTaskGPRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_multitask_gp_mean_abs_error(self):\n        likelihood = MultitaskGaussianLikelihood(num_tasks=2)\n        model = MultitaskGPModel(train_x, train_y, likelihood)\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n\n        # Use the adam optimizer\n        optimizer = torch.optim.Adam([{""params"": model.parameters()}], lr=0.1)  # Includes GaussianLikelihood parameters\n\n        # ""Loss"" for GPs - the marginal log likelihood\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n        n_iter = 50\n        for _ in range(n_iter):\n            # Zero prev backpropped gradients\n            optimizer.zero_grad()\n            # Make predictions from training data\n            # Again, note feeding duplicated x_data and indices indicating which task\n            output = model(train_x)\n            # TODO: Fix this view call!!\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.step()\n\n        # Test the model\n        model.eval()\n        likelihood.eval()\n        test_x = torch.linspace(0, 1, 51)\n        test_y1 = torch.sin(test_x * (2 * pi))\n        test_y2 = torch.cos(test_x * (2 * pi))\n        test_preds = likelihood(model(test_x)).mean\n        mean_abs_error_task_1 = torch.mean(torch.abs(test_y1 - test_preds[:, 0]))\n        mean_abs_error_task_2 = torch.mean(torch.abs(test_y2 - test_preds[:, 1]))\n\n        self.assertLess(mean_abs_error_task_1.squeeze().item(), 0.05)\n        self.assertLess(mean_abs_error_task_2.squeeze().item(), 0.05)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_kronecker_multitask_ski_gp_regression.py,23,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport unittest\nfrom math import pi\n\nimport gpytorch\nimport torch\nfrom gpytorch.distributions import MultitaskMultivariateNormal\nfrom gpytorch.kernels import GridInterpolationKernel, MultitaskKernel, RBFKernel\nfrom gpytorch.likelihoods import MultitaskGaussianLikelihood\nfrom gpytorch.means import ConstantMean, MultitaskMean\nfrom gpytorch.test.utils import least_used_cuda_device\n\n\nclass MultitaskGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = MultitaskMean(ConstantMean(), num_tasks=2)\n        self.data_covar_module = GridInterpolationKernel(RBFKernel(), grid_size=100, num_dims=1)\n        self.covar_module = MultitaskKernel(self.data_covar_module, num_tasks=2, rank=1)\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultitaskMultivariateNormal(mean_x, covar_x)\n\n\nclass TestKroneckerMultiTaskKISSGPRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def _get_data(self, cuda=False):\n        # Simple training data: let\'s try to learn a sine function\n        train_x = torch.linspace(0, 1, 100, device=torch.device(""cuda"") if cuda else torch.device(""cpu""))\n        # y1 function is sin(2*pi*x) with noise N(0, 0.04)\n        train_y1 = torch.sin(train_x * (2 * pi)) + torch.randn_like(train_x) * 0.1\n        # y2 function is cos(2*pi*x) with noise N(0, 0.04)\n        train_y2 = torch.cos(train_x * (2 * pi)) + torch.randn_like(train_x) * 0.1\n        # Create a train_y which interleaves the two\n        train_y = torch.stack([train_y1, train_y2], -1)\n        return train_x, train_y\n\n    def test_multitask_gp_mean_abs_error(self, cuda=False):\n        train_x, train_y = self._get_data(cuda=cuda)\n        likelihood = MultitaskGaussianLikelihood(num_tasks=2)\n        model = MultitaskGPModel(train_x, train_y, likelihood)\n\n        if cuda:\n            model.cuda()\n\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n\n        # Use the adam optimizer\n        optimizer = torch.optim.Adam([{""params"": model.parameters()}], lr=0.1)  # Includes GaussianLikelihood parameters\n\n        # ""Loss"" for GPs - the marginal log likelihood\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n        n_iter = 50\n        for _ in range(n_iter):\n            # Zero prev backpropped gradients\n            optimizer.zero_grad()\n            # Make predictions from training data\n            # Again, note feeding duplicated x_data and indices indicating which task\n            output = model(train_x)\n            # TODO: Fix this view call!!\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.step()\n\n        # Test the model\n        model.eval()\n        likelihood.eval()\n\n        test_x = torch.linspace(0, 1, 51, device=torch.device(""cuda"") if cuda else torch.device(""cpu""))\n        test_y1 = torch.sin(test_x * (2 * pi))\n        test_y2 = torch.cos(test_x * (2 * pi))\n        test_preds = likelihood(model(test_x)).mean\n        mean_abs_error_task_1 = torch.mean(torch.abs(test_y1 - test_preds[:, 0]))\n        mean_abs_error_task_2 = torch.mean(torch.abs(test_y2 - test_preds[:, 1]))\n\n        self.assertLess(mean_abs_error_task_1.squeeze().item(), 0.05)\n        self.assertLess(mean_abs_error_task_2.squeeze().item(), 0.05)\n\n    def test_multitask_gp_mean_abs_error_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_multitask_gp_mean_abs_error(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_lcm_kernel_regression.py,18,"b'#!/usr/bin/env python3\n\nimport math\nimport torch\nimport unittest\n\nimport gpytorch\nfrom gpytorch.kernels import RBFKernel, MultitaskKernel, LCMKernel\nfrom gpytorch.means import ConstantMean, MultitaskMean\nfrom gpytorch.likelihoods import MultitaskGaussianLikelihood\nfrom gpytorch.distributions import MultitaskMultivariateNormal\n\n\nclass MultitaskGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = MultitaskMean(ConstantMean(), num_tasks=2)\n        self.base_kernel_list = [RBFKernel()]\n        self.covar_module = LCMKernel(self.base_kernel_list, num_tasks=2, rank=1)\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultitaskMultivariateNormal(mean_x, covar_x)\n\n\nclass MultitaskGPModel_ICM(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(MultitaskGPModel_ICM, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = MultitaskMean(ConstantMean(), num_tasks=2)\n        self.base_kernel = RBFKernel()\n        self.covar_module = MultitaskKernel(self.base_kernel, num_tasks=2, rank=1)\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultitaskMultivariateNormal(mean_x, covar_x)\n\n\nclass TestLCMKernelRegression(unittest.TestCase):\n    def test_lcm_icm_equivalence(self):\n        # Training points are every 0.1 in [0,1] (note that they\'re the same for both tasks)\n        train_x = torch.linspace(0, 1, 100)\n        # y1 function is sin(2*pi*x) with noise N(0, 0.04)\n        train_y1 = torch.sin(train_x.data * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2\n        # y2 function is cos(2*pi*x) with noise N(0, 0.04)\n        train_y2 = torch.cos(train_x.data * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2\n        # Create a train_y which interleaves the two\n        train_y = torch.stack([train_y1, train_y2], -1)\n\n        likelihood = MultitaskGaussianLikelihood(num_tasks=2)\n        model = MultitaskGPModel(train_x, train_y, likelihood)\n\n        # Use the adam optimizer\n        optimizer = torch.optim.Adam([{""params"": model.parameters()}], lr=0.1)  # Includes GaussianLikelihood parameters\n        model.train()\n        likelihood.train()\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n        n_iter = 50\n        for _ in range(n_iter):\n            optimizer.zero_grad()\n            output = model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.step()\n        model.eval()\n        likelihood.eval()\n\n        # Make predictions for LCM\n        with torch.no_grad():\n            test_x = torch.linspace(0, 1, 51)\n            observed_pred = likelihood(model(test_x))\n            mean = observed_pred.mean\n\n        model_icm = MultitaskGPModel_ICM(train_x, train_y, likelihood)\n        likelihood = MultitaskGaussianLikelihood(num_tasks=2)\n        model_icm.train()\n        likelihood.train()\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model_icm)\n        optimizer = torch.optim.Adam(\n            [{""params"": model_icm.parameters()}], lr=0.1  # Includes GaussianLikelihood parameters\n        )\n        for _ in range(n_iter):\n            optimizer.zero_grad()\n            output = model_icm(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.step()\n        model_icm.eval()\n        likelihood.eval()\n\n        # Make predictions for ICM\n        with torch.no_grad():\n            test_x = torch.linspace(0, 1, 51)\n            observed_pred_icm = likelihood(model_icm(test_x))\n            mean_icm = observed_pred_icm.mean\n\n        # Make sure predictions from LCM with one base kernel and ICM are the same.\n        self.assertLess((mean - mean_icm).pow(2).mean(), 1e-2)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_model_list_gp_regression.py,17,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\n\nimport gpytorch\nimport torch\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import GaussianLikelihood, LikelihoodList\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.mlls import SumMarginalLogLikelihood\nfrom gpytorch.models import IndependentModelList\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.test.utils import least_used_cuda_device\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_inputs, train_targets, likelihood):\n        super(ExactGPModel, self).__init__(train_inputs, train_targets, likelihood)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1, 1))\n        self.covar_module = ScaleKernel(\n            RBFKernel(lengthscale_prior=SmoothedBoxPrior(math.exp(-3), math.exp(3), sigma=0.1))\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestModelListGPRegression(unittest.TestCase):\n    def test_simple_model_list_gp_regression(self, cuda=False):\n        train_x1 = torch.linspace(0, 0.95, 25) + 0.05 * torch.rand(25)\n        train_x2 = torch.linspace(0, 0.95, 15) + 0.05 * torch.rand(15)\n\n        train_y1 = torch.sin(train_x1 * (2 * math.pi)) + 0.2 * torch.randn_like(train_x1)\n        train_y2 = torch.cos(train_x2 * (2 * math.pi)) + 0.2 * torch.randn_like(train_x2)\n\n        likelihood1 = GaussianLikelihood()\n        model1 = ExactGPModel(train_x1, train_y1, likelihood1)\n\n        likelihood2 = GaussianLikelihood()\n        model2 = ExactGPModel(train_x2, train_y2, likelihood2)\n\n        model = IndependentModelList(model1, model2)\n        likelihood = LikelihoodList(model1.likelihood, model2.likelihood)\n\n        if cuda:\n            model = model.cuda()\n\n        model.train()\n        likelihood.train()\n\n        mll = SumMarginalLogLikelihood(likelihood, model)\n\n        optimizer = torch.optim.Adam([{""params"": model.parameters()}], lr=0.1)\n\n        for _ in range(10):\n            optimizer.zero_grad()\n            output = model(*model.train_inputs)\n            loss = -mll(output, model.train_targets)\n            loss.backward()\n            optimizer.step()\n\n        model.eval()\n        likelihood.eval()\n\n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n            test_x = torch.linspace(0, 1, 10, device=torch.device(""cuda"") if cuda else torch.device(""cpu""))\n            outputs_f = model(test_x, test_x)\n            predictions_obs_noise = likelihood(*outputs_f)\n\n        self.assertIsInstance(outputs_f, list)\n        self.assertEqual(len(outputs_f), 2)\n        self.assertIsInstance(predictions_obs_noise, list)\n        self.assertEqual(len(predictions_obs_noise), 2)\n\n    def test_simple_model_list_gp_regression_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_simple_model_list_gp_regression(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_pyro_integration.py,66,"b'#!/usr/bin/env python3\n\nfrom math import pi\nimport torch\nimport time\nimport unittest\nimport gpytorch\nfrom gpytorch.test.base_test_case import BaseTestCase\n\n\ntry:\n    import pyro\n\n    class ClusterGaussianLikelihood(gpytorch.likelihoods.Likelihood):\n        def __init__(self, num_tasks, num_clusters, name_prefix=str(time.time()), reparam=False):\n            super().__init__()\n            self.register_buffer(""prior_cluster_logits"", torch.zeros(num_tasks, num_clusters))\n            self.register_parameter(\n                ""variational_cluster_logits"", torch.nn.Parameter(torch.randn(num_tasks, num_clusters))\n            )\n            self.register_parameter(""raw_noise"", torch.nn.Parameter(torch.tensor(0.0)))\n            self.reparam = reparam\n            if self.reparam:\n                self.register_buffer(""temp"", torch.tensor(1.0))\n            self.num_tasks = num_tasks\n            self.num_clusters = num_clusters\n            self.name_prefix = name_prefix\n            self.max_plate_nesting = 1\n\n        @property\n        def noise(self):\n            return torch.nn.functional.softplus(self.raw_noise)\n\n        def _cluster_dist(self, logits):\n            if self.reparam:\n                dist = pyro.distributions.RelaxedOneHotCategorical(temperature=self.temp, logits=logits).to_event(1)\n            else:\n                dist = pyro.distributions.OneHotCategorical(logits=logits).to_event(1)\n            return dist\n\n        def pyro_guide(self, function_dist, target):\n            pyro.sample(self.name_prefix + "".cluster_logits"", self._cluster_dist(self.variational_cluster_logits))\n            return super().pyro_guide(function_dist, target)\n\n        def pyro_model(self, function_dist, target):\n            cluster_assignment_samples = pyro.sample(\n                self.name_prefix + "".cluster_logits"", self._cluster_dist(self.prior_cluster_logits)\n            )\n            return super().pyro_model(function_dist, target, cluster_assignment_samples=cluster_assignment_samples)\n\n        def forward(self, function_samples, cluster_assignment_samples=None):\n            if cluster_assignment_samples is None:\n                cluster_assignment_samples = pyro.sample(\n                    self.name_prefix + "".cluster_logits"", self._cluster_dist(self.variational_cluster_logits)\n                )\n            res = pyro.distributions.Normal(\n                loc=(function_samples.unsqueeze(-2) * cluster_assignment_samples).sum(-1), scale=self.noise.sqrt()\n            ).to_event(1)\n            return res\n\n    class ClusterMultitaskGPModel(gpytorch.models.pyro.PyroGP):\n        def __init__(self, train_x, train_y, num_functions=2, reparam=False):\n            num_data = train_y.size(-2)\n\n            # Define all the variational stuff\n            inducing_points = torch.linspace(0, 1, 64).unsqueeze(-1)\n            variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n                num_inducing_points=inducing_points.size(-2), batch_shape=torch.Size([num_functions])\n            )\n            variational_strategy = gpytorch.variational.MultitaskVariationalStrategy(\n                gpytorch.variational.VariationalStrategy(self, inducing_points, variational_distribution),\n                num_tasks=num_functions,\n            )\n\n            # Standard initializtation\n            likelihood = ClusterGaussianLikelihood(\n                train_y.size(-1), num_functions, name_prefix=""likelihood"", reparam=reparam\n            )\n            super().__init__(variational_strategy, likelihood, num_data=num_data, name_prefix=str(time.time()))\n            self.likelihood = likelihood\n            self.num_functions = num_functions\n\n            # Mean, covar\n            self.mean_module = gpytorch.means.ZeroMean()\n            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n        def forward(self, x):\n            mean_x = self.mean_module(x)\n            covar_x = self.covar_module(x)\n            res = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n            return res\n\n    class SVGPModel(gpytorch.models.pyro.PyroGP):\n        def __init__(self, train_x, train_y):\n            num_data = train_y.size(-1)\n\n            # Define all the variational stuff\n            inducing_points = torch.linspace(0, 1, 64).unsqueeze(-1)\n            variational_distribution = gpytorch.variational.MeanFieldVariationalDistribution(inducing_points.size(-2))\n            variational_strategy = gpytorch.variational.VariationalStrategy(\n                self, inducing_points, variational_distribution\n            )\n\n            # Standard initializtation\n            likelihood = gpytorch.likelihoods.GaussianLikelihood()\n            super().__init__(variational_strategy, likelihood, num_data=num_data, name_prefix=str(time.time()))\n            self.likelihood = likelihood\n\n            # Mean, covar\n            self.mean_module = gpytorch.means.ZeroMean()\n            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n        def forward(self, x):\n            mean_x = self.mean_module(x)\n            covar_x = self.covar_module(x)\n            res = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n            return res\n\n    class LowLevelInterfaceClusterMultitaskGPModel(gpytorch.models.ApproximateGP):\n        def __init__(self, train_x, train_y, num_functions=2):\n            # Define all the variational stuff\n            inducing_points = torch.linspace(0, 1, 64).unsqueeze(-1)\n            variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n                num_inducing_points=inducing_points.size(-2), batch_shape=torch.Size([num_functions])\n            )\n            variational_strategy = gpytorch.variational.MultitaskVariationalStrategy(\n                gpytorch.variational.VariationalStrategy(self, inducing_points, variational_distribution),\n                num_tasks=num_functions,\n            )\n            super().__init__(variational_strategy)\n\n            # Mean, covar\n            self.mean_module = gpytorch.means.ZeroMean()\n            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n            # Values to store\n            self.name_prefix = ""llcmgp""\n            self.num_data, self.num_tasks = train_y.shape\n            self.num_functions = num_functions\n\n            # Define likelihood stuff\n            self.register_parameter(\n                ""variational_logits"", torch.nn.Parameter(torch.randn(self.num_tasks, self.num_functions))\n            )\n            self.register_parameter(""raw_noise"", torch.nn.Parameter(torch.tensor(0.0)))\n\n        @property\n        def noise(self):\n            return torch.nn.functional.softplus(self.raw_noise)\n\n        def forward(self, x):\n            mean_x = self.mean_module(x)\n            covar_x = self.covar_module(x)\n            res = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n            return res\n\n        def guide(self, x, y):\n            function_dist = self.pyro_guide(x, name_prefix=self.name_prefix)\n            pyro.sample(\n                self.name_prefix + "".cluster_logits"",\n                pyro.distributions.OneHotCategorical(logits=self.variational_logits).to_event(1),\n            )\n            with pyro.plate(self.name_prefix + "".output_values_plate"", function_dist.batch_shape[-1], dim=-1):\n                pyro.sample(self.name_prefix + "".f"", function_dist)\n\n        def model(self, x, y):\n            pyro.module(self.name_prefix + "".gp"", self)\n\n            # Draw sample from q(f)\n            function_dist = self.pyro_model(x, name_prefix=self.name_prefix)\n\n            # Draw samples of cluster assignments\n            cluster_assignment_samples = pyro.sample(\n                self.name_prefix + "".cluster_logits"",\n                pyro.distributions.OneHotCategorical(logits=torch.zeros(self.num_tasks, self.num_functions)).to_event(\n                    1\n                ),\n            )\n\n            # Sample from observation distribution\n            with pyro.plate(self.name_prefix + "".output_values_plate"", function_dist.batch_shape[-1], dim=-1):\n                function_samples = pyro.sample(self.name_prefix + "".f"", function_dist)\n                obs_dist = pyro.distributions.Normal(\n                    loc=(function_samples.unsqueeze(-2) * cluster_assignment_samples).sum(-1), scale=self.noise.sqrt()\n                ).to_event(1)\n                with pyro.poutine.scale(scale=(self.num_data / y.size(-2))):\n                    return pyro.sample(self.name_prefix + "".y"", obs_dist, obs=y)\n\n    class TestPyroIntegration(BaseTestCase, unittest.TestCase):\n        seed = 1\n\n        def test_simple_high_level_interface(self):\n            # Simple training data: let\'s try to learn a sine function\n            train_x = torch.linspace(0, 1, 100)\n            train_y = torch.sin(train_x * (2 * pi)) + torch.randn(train_x.size()) * 0.03\n\n            model = SVGPModel(train_x, train_y)\n            # Find optimal model hyperparameters\n            model.train()\n\n            # Use the adam optimizer\n            optimizer = pyro.optim.Adam({""lr"": 0.02})\n            elbo = pyro.infer.TraceMeanField_ELBO(num_particles=16, vectorize_particles=True, retain_graph=True)\n            svi = pyro.infer.SVI(model.model, model.guide, optimizer, elbo)\n\n            n_iter = 250\n            for _ in range(n_iter):\n                svi.step(train_x, train_y)\n\n            # Test the model\n            with torch.no_grad(), gpytorch.settings.num_likelihood_samples(128):\n                model.eval()\n                test_x = torch.linspace(0, 1, 51)\n                test_y = torch.sin(test_x * (2 * pi))\n                test_preds = model.likelihood(model(test_x)).mean\n                mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n                self.assertLess(mean_abs_error.squeeze().item(), 0.15)\n\n        def test_high_level_interface(self, mean_field=False):\n            # Simple training data: let\'s try to learn sine and cosine functions\n            train_x = torch.linspace(0, 1, 100)\n\n            # y1 and y4 functions are sin(2*pi*x) with noise N(0, 0.05)\n            train_y1 = torch.sin(train_x * (2 * pi)) + torch.randn(train_x.size()) * 0.03\n            train_y4 = torch.sin(train_x * (2 * pi)) + torch.randn(train_x.size()) * 0.03\n            # y2 and y3 functions are -sin(2*pi*x) with noise N(0, 0.05)\n            train_y2 = -torch.sin(train_x * (2 * pi)) + torch.randn(train_x.size()) * 0.03\n            train_y3 = -torch.sin(train_x * (2 * pi)) + torch.randn(train_x.size()) * 0.03\n            # Create a train_y which interleaves the four\n            train_y = torch.stack([train_y1, train_y2, train_y3, train_y4], -1)\n\n            model = ClusterMultitaskGPModel(train_x, train_y, reparam=mean_field)\n            # Find optimal model hyperparameters\n            model.train()\n\n            # Use the adam optimizer\n            optimizer = pyro.optim.Adam({""lr"": 0.03})\n            if mean_field:\n                elbo = pyro.infer.Trace_ELBO(num_particles=16, vectorize_particles=True, retain_graph=True)\n            else:\n                elbo = pyro.infer.Trace_ELBO(num_particles=16, vectorize_particles=True, retain_graph=True)\n            svi = pyro.infer.SVI(model.model, model.guide, optimizer, elbo)\n\n            n_iter = 200\n            for _ in range(n_iter):\n                svi.step(train_x, train_y)\n\n            # Test the model\n            with torch.no_grad(), gpytorch.settings.num_likelihood_samples(128):\n                model.eval()\n                test_x = torch.linspace(0, 1, 51)\n                test_y1 = torch.sin(test_x * (2 * pi))\n                test_y2 = -torch.sin(test_x * (2 * pi))\n                test_y3 = -torch.sin(test_x * (2 * pi))\n                test_y4 = torch.sin(test_x * (2 * pi))\n                test_preds = model.likelihood(model(test_x)).mean\n                mean_abs_error_task_1 = torch.mean(torch.abs(test_y1 - test_preds.mean(0)[:, 0]))\n                mean_abs_error_task_2 = torch.mean(torch.abs(test_y2 - test_preds.mean(0)[:, 1]))\n                mean_abs_error_task_3 = torch.mean(torch.abs(test_y3 - test_preds.mean(0)[:, 2]))\n                mean_abs_error_task_4 = torch.mean(torch.abs(test_y4 - test_preds.mean(0)[:, 3]))\n\n            self.assertLess(mean_abs_error_task_1.squeeze().item(), 0.15)\n            self.assertLess(mean_abs_error_task_2.squeeze().item(), 0.15)\n            self.assertLess(mean_abs_error_task_3.squeeze().item(), 0.15)\n            self.assertLess(mean_abs_error_task_4.squeeze().item(), 0.15)\n\n        def test_high_level_interface_mean_field(self):\n            return self.test_high_level_interface(mean_field=True)\n\n        def test_low_level_interface(self):\n            # Simple training data: let\'s try to learn sine and cosine functions\n            train_x = torch.linspace(0, 1, 100)\n\n            # y1 and y4 functions are sin(2*pi*x) with noise N(0, 0.05)\n            train_y1 = torch.sin(train_x * (2 * pi)) + torch.randn(train_x.size()) * 0.03\n            train_y4 = torch.sin(train_x * (2 * pi)) + torch.randn(train_x.size()) * 0.03\n            # y2 and y3 functions are -sin(2*pi*x) with noise N(0, 0.05)\n            train_y2 = -torch.sin(train_x * (2 * pi)) + torch.randn(train_x.size()) * 0.03\n            train_y3 = -torch.sin(train_x * (2 * pi)) + torch.randn(train_x.size()) * 0.03\n            # Create a train_y which interleaves the four\n            train_y = torch.stack([train_y1, train_y2, train_y3, train_y4], -1)\n            model = LowLevelInterfaceClusterMultitaskGPModel(train_x, train_y)\n            # Find optimal model hyperparameters\n            model.train()\n\n            # Use the adam optimizer\n            optimizer = pyro.optim.Adam({""lr"": 0.03})\n            elbo = pyro.infer.Trace_ELBO(num_particles=16, vectorize_particles=True, retain_graph=True)\n            svi = pyro.infer.SVI(model.model, model.guide, optimizer, elbo)\n\n            n_iter = 200\n            for _ in range(n_iter):\n                svi.step(train_x, train_y)\n\n            # Test the model inference\n            cluster_1_idx = model.variational_logits[0].max(dim=-1)[1].item()\n            cluster_2_idx = 1 - cluster_1_idx\n            cluster_probs = torch.softmax(model.variational_logits, dim=-1)\n            self.assertGreater(cluster_probs[0, cluster_1_idx].squeeze().item(), 0.9)\n            self.assertGreater(cluster_probs[1, cluster_2_idx].squeeze().item(), 0.9)\n            self.assertGreater(cluster_probs[2, cluster_2_idx].squeeze().item(), 0.9)\n            self.assertGreater(cluster_probs[3, cluster_1_idx].squeeze().item(), 0.9)\n\n\nexcept ImportError:\n    pass\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_sgpr_regression.py,30,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport unittest\nimport warnings\nfrom math import exp, pi\n\nimport gpytorch\nimport torch\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.kernels import InducingPointKernel, RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.test.utils import least_used_cuda_device\nfrom gpytorch.utils.warnings import NumericalWarning\nfrom torch import optim\n\n\n# Simple training data: let\'s try to learn a sine function,\n# but with SGPR\n# let\'s use 100 training examples.\ndef make_data(cuda=False):\n    train_x = torch.linspace(0, 1, 100)\n    train_y = torch.sin(train_x * (2 * pi))\n    train_y.add_(torch.randn_like(train_y), alpha=1e-2)\n    test_x = torch.rand(51)\n    test_y = torch.sin(test_x * (2 * pi))\n    if cuda:\n        train_x = train_x.cuda()\n        train_y = train_y.cuda()\n        test_x = test_x.cuda()\n        test_y = test_y.cuda()\n    return train_x, train_y, test_x, test_y\n\n\nclass GPRegressionModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1e-5, 1e-5))\n        self.base_covar_module = ScaleKernel(RBFKernel(lengthscale_prior=SmoothedBoxPrior(exp(-5), exp(6), sigma=0.1)))\n        self.covar_module = InducingPointKernel(\n            self.base_covar_module, inducing_points=torch.linspace(0, 1, 32), likelihood=likelihood\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestSGPRRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_sgpr_mean_abs_error(self):\n        # Suppress numerical warnings\n        warnings.simplefilter(""ignore"", NumericalWarning)\n\n        train_x, train_y, test_x, test_y = make_data()\n        likelihood = GaussianLikelihood()\n        gp_model = GPRegressionModel(train_x, train_y, likelihood)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Optimize the model\n        gp_model.train()\n        likelihood.train()\n\n        optimizer = optim.Adam(gp_model.parameters(), lr=0.1)\n        for _ in range(30):\n            optimizer.zero_grad()\n            output = gp_model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.step()\n\n        for param in gp_model.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for param in likelihood.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n\n        # Test the model\n        gp_model.eval()\n        likelihood.eval()\n\n        test_preds = likelihood(gp_model(test_x)).mean\n        mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n\n        self.assertLess(mean_abs_error.squeeze().item(), 0.05)\n\n    def test_sgpr_fast_pred_var(self):\n        # Suppress numerical warnings\n        warnings.simplefilter(""ignore"", NumericalWarning)\n\n        train_x, train_y, test_x, test_y = make_data()\n        likelihood = GaussianLikelihood()\n        gp_model = GPRegressionModel(train_x, train_y, likelihood)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Optimize the model\n        gp_model.train()\n        likelihood.train()\n\n        optimizer = optim.Adam(gp_model.parameters(), lr=0.1)\n        for _ in range(50):\n            optimizer.zero_grad()\n            output = gp_model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.step()\n\n        for param in gp_model.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for param in likelihood.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n\n        # Test the model\n        gp_model.eval()\n        likelihood.eval()\n\n        with gpytorch.settings.max_preconditioner_size(5), gpytorch.settings.max_cg_iterations(50):\n            with gpytorch.settings.fast_pred_var(True):\n                fast_var = gp_model(test_x).variance\n                fast_var_cache = gp_model(test_x).variance\n                self.assertLess(torch.max((fast_var_cache - fast_var).abs()), 1e-3)\n\n            with gpytorch.settings.fast_pred_var(False):\n                slow_var = gp_model(test_x).variance\n\n        self.assertLess(torch.max((fast_var_cache - slow_var).abs()), 1e-3)\n\n    def test_sgpr_mean_abs_error_cuda(self):\n        # Suppress numerical warnings\n        warnings.simplefilter(""ignore"", NumericalWarning)\n\n        if not torch.cuda.is_available():\n            return\n        with least_used_cuda_device():\n            train_x, train_y, test_x, test_y = make_data(cuda=True)\n            likelihood = GaussianLikelihood().cuda()\n            gp_model = GPRegressionModel(train_x, train_y, likelihood).cuda()\n            mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n            # Optimize the model\n            gp_model.train()\n            likelihood.train()\n\n            optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n            optimizer.n_iter = 0\n            for _ in range(25):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n            test_preds = likelihood(gp_model(test_x)).mean\n            mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n\n            self.assertLess(mean_abs_error.squeeze().item(), 0.02)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_simple_gp_classification.py,13,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport unittest\nfrom math import pi\n\nimport gpytorch\nimport torch\nfrom gpytorch.likelihoods import BernoulliLikelihood\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.variational import CholeskyVariationalDistribution, UnwhitenedVariationalStrategy\nfrom torch import optim\n\n\ndef train_data():\n    train_x = torch.linspace(0, 1, 10)\n    train_y = torch.sign(torch.cos(train_x * (4 * pi))).add(1).div(2)\n    return train_x, train_y\n\n\nclass GPClassificationModel(ApproximateGP):\n    def __init__(self, train_x):\n        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n        variational_strategy = UnwhitenedVariationalStrategy(\n            self, train_x, variational_distribution, learn_inducing_locations=False\n        )\n        super(GPClassificationModel, self).__init__(variational_strategy)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n        return latent_pred\n\n\nclass TestSimpleGPClassification(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_classification_error(self):\n        train_x, train_y = train_data()\n        likelihood = BernoulliLikelihood()\n        model = GPClassificationModel(train_x)\n        mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(train_y))\n\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n        optimizer = optim.Adam(model.parameters(), lr=0.1)\n        optimizer.n_iter = 0\n        for _ in range(75):\n            optimizer.zero_grad()\n            output = model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.n_iter += 1\n            optimizer.step()\n\n        for param in model.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for param in likelihood.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n\n        # Set back to eval mode\n        model.eval()\n        likelihood.eval()\n        test_preds = likelihood(model(train_x)).mean.round()\n        mean_abs_error = torch.mean(torch.abs(train_y - test_preds) / 2)\n        self.assertLess(mean_abs_error.item(), 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_simple_gp_regression.py,64,"b'#!/usr/bin/env python3\n\nimport unittest\nfrom math import exp, pi\n\nimport gpytorch\nimport torch\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior, UniformPrior\nfrom gpytorch.constraints import Positive\nfrom gpytorch.test.base_test_case import BaseTestCase\nfrom gpytorch.test.utils import least_used_cuda_device\nfrom torch import optim\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_inputs, train_targets, likelihood):\n        super(ExactGPModel, self).__init__(train_inputs, train_targets, likelihood)\n        self.mean_module = ConstantMean()\n        self.covar_module = ScaleKernel(RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestSimpleGPRegression(BaseTestCase, unittest.TestCase):\n    seed = 1\n\n    def _get_data(self, cuda=False, num_data=11, add_noise=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        # Simple training data: let\'s try to learn a sine function\n        train_x = torch.linspace(0, 1, num_data, device=device)\n        train_y = torch.sin(train_x * (2 * pi))\n        if add_noise:\n            train_y.add_(torch.randn_like(train_x).mul_(0.1))\n        test_x = torch.linspace(0, 1, 51, device=device)\n        test_y = torch.sin(test_x * (2 * pi))\n        return train_x, test_x, train_y, test_y\n\n    def test_prior(self, cuda=False):\n        train_x, test_x, train_y, test_y = self._get_data(cuda=cuda)\n        # We\'re manually going to set the hyperparameters to be ridiculous\n        likelihood = GaussianLikelihood(\n            noise_prior=SmoothedBoxPrior(exp(-3), exp(3), sigma=0.1),\n            noise_constraint=Positive(),  # Prior for this test is looser than default bound\n        )\n        gp_model = ExactGPModel(None, None, likelihood)\n        # Update lengthscale prior to accommodate extreme parameters\n        gp_model.covar_module.base_kernel.register_prior(\n            ""lengthscale_prior"", SmoothedBoxPrior(exp(-10), exp(10), sigma=0.5), ""raw_lengthscale""\n        )\n        gp_model.mean_module.initialize(constant=1.5)\n        gp_model.covar_module.base_kernel.initialize(lengthscale=1)\n        likelihood.initialize(noise=0)\n\n        if cuda:\n            gp_model.cuda()\n            likelihood.cuda()\n\n        # Compute posterior distribution\n        gp_model.eval()\n        likelihood.eval()\n\n        # The model should predict in prior mode\n        function_predictions = likelihood(gp_model(train_x))\n        correct_variance = gp_model.covar_module.outputscale + likelihood.noise\n\n        self.assertAllClose(function_predictions.mean, torch.full_like(function_predictions.mean, fill_value=1.5))\n        self.assertAllClose(\n            function_predictions.variance, correct_variance.squeeze().expand_as(function_predictions.variance)\n        )\n\n    def test_prior_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_prior(cuda=True)\n\n    def test_recursive_initialize(self, cuda=False):\n        train_x, test_x, train_y, test_y = self._get_data(cuda=cuda)\n\n        likelihood_1 = GaussianLikelihood()\n        gp_model_1 = ExactGPModel(train_x, train_y, likelihood_1)\n\n        likelihood_2 = GaussianLikelihood()\n        gp_model_2 = ExactGPModel(train_x, train_y, likelihood_2)\n\n        gp_model_1.initialize(**{""likelihood.noise"": 1e-2, ""covar_module.base_kernel.lengthscale"": 1e-1})\n        gp_model_2.likelihood.initialize(noise=1e-2)\n        gp_model_2.covar_module.base_kernel.initialize(lengthscale=1e-1)\n        self.assertTrue(torch.equal(gp_model_1.likelihood.noise, gp_model_2.likelihood.noise))\n        self.assertTrue(\n            torch.equal(\n                gp_model_1.covar_module.base_kernel.lengthscale, gp_model_2.covar_module.base_kernel.lengthscale\n            )\n        )\n\n    def test_posterior_latent_gp_and_likelihood_without_optimization(self, cuda=False):\n        train_x, test_x, train_y, test_y = self._get_data(cuda=cuda)\n        # We\'re manually going to set the hyperparameters to be ridiculous\n        likelihood = GaussianLikelihood(noise_constraint=Positive())  # This test actually wants a noise < 1e-4\n        gp_model = ExactGPModel(train_x, train_y, likelihood)\n        gp_model.covar_module.base_kernel.initialize(lengthscale=exp(-15))\n        likelihood.initialize(noise=exp(-15))\n\n        if cuda:\n            gp_model.cuda()\n            likelihood.cuda()\n\n        # Compute posterior distribution\n        gp_model.eval()\n        likelihood.eval()\n\n        # Let\'s see how our model does, conditioned with weird hyperparams\n        # The posterior should fit all the data\n        with gpytorch.settings.debug(False):\n            function_predictions = likelihood(gp_model(train_x))\n\n        self.assertAllClose(function_predictions.mean, train_y)\n        self.assertAllClose(function_predictions.variance, torch.zeros_like(function_predictions.variance))\n\n        # It shouldn\'t fit much else though\n        test_function_predictions = gp_model(torch.tensor([1.1]).type_as(test_x))\n\n        self.assertAllClose(test_function_predictions.mean, torch.zeros_like(test_function_predictions.mean))\n        self.assertAllClose(\n            test_function_predictions.variance,\n            gp_model.covar_module.outputscale.expand_as(test_function_predictions.variance),\n        )\n\n    def test_posterior_latent_gp_and_likelihood_without_optimization_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_posterior_latent_gp_and_likelihood_without_optimization(cuda=True)\n\n    def test_gp_posterior_mean_skip_variances_fast_cuda(self):\n        if not torch.cuda.is_available():\n            return\n        with least_used_cuda_device():\n            train_x, test_x, train_y, _ = self._get_data(cuda=True)\n            likelihood = GaussianLikelihood()\n            gp_model = ExactGPModel(train_x, train_y, likelihood)\n\n            gp_model.cuda()\n            likelihood.cuda()\n\n            # Compute posterior distribution\n            gp_model.eval()\n            likelihood.eval()\n\n            with gpytorch.settings.skip_posterior_variances(True):\n                mean_skip_var = gp_model(test_x).mean\n            mean = gp_model(test_x).mean\n            likelihood_mean = likelihood(gp_model(test_x)).mean\n\n            self.assertTrue(torch.allclose(mean_skip_var, mean))\n            self.assertTrue(torch.allclose(mean_skip_var, likelihood_mean))\n\n    def test_gp_posterior_mean_skip_variances_slow_cuda(self):\n        if not torch.cuda.is_available():\n            return\n        with least_used_cuda_device():\n            train_x, test_x, train_y, _ = self._get_data(cuda=True)\n            likelihood = GaussianLikelihood()\n            gp_model = ExactGPModel(train_x, train_y, likelihood)\n\n            gp_model.cuda()\n            likelihood.cuda()\n\n            # Compute posterior distribution\n            gp_model.eval()\n            likelihood.eval()\n\n            with gpytorch.settings.fast_pred_var(False):\n                with gpytorch.settings.skip_posterior_variances(True):\n                    mean_skip_var = gp_model(test_x).mean\n                mean = gp_model(test_x).mean\n                likelihood_mean = likelihood(gp_model(test_x)).mean\n            self.assertTrue(torch.allclose(mean_skip_var, mean))\n            self.assertTrue(torch.allclose(mean_skip_var, likelihood_mean))\n\n    def test_gp_posterior_single_training_point_smoke_test(self):\n        train_x, test_x, train_y, _ = self._get_data()\n        train_x = train_x[0].unsqueeze(-1).unsqueeze(-1)\n        train_y = train_y[0].unsqueeze(-1)\n        likelihood = GaussianLikelihood()\n        gp_model = ExactGPModel(train_x, train_y, likelihood)\n\n        gp_model.eval()\n        likelihood.eval()\n\n        with gpytorch.settings.fast_pred_var():\n            preds = gp_model(test_x)\n            single_mean = preds.mean\n            single_variance = preds.variance\n\n        self.assertFalse(torch.any(torch.isnan(single_variance)))\n        self.assertFalse(torch.any(torch.isnan(single_mean)))\n\n        gp_model.train()\n        gp_model.eval()\n\n        preds = gp_model(test_x)\n        single_mean = preds.mean\n        single_variance = preds.variance\n\n        self.assertFalse(torch.any(torch.isnan(single_variance)))\n        self.assertFalse(torch.any(torch.isnan(single_mean)))\n\n    def test_posterior_latent_gp_and_likelihood_with_optimization(self, cuda=False, checkpoint=0):\n        train_x, test_x, train_y, test_y = self._get_data(\n            cuda=cuda, num_data=(1000 if checkpoint else 11), add_noise=bool(checkpoint)\n        )\n        # We\'re manually going to set the hyperparameters to something they shouldn\'t be\n        likelihood = GaussianLikelihood(noise_prior=SmoothedBoxPrior(exp(-3), exp(3), sigma=0.1))\n        gp_model = ExactGPModel(train_x, train_y, likelihood)\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n        gp_model.covar_module.base_kernel.initialize(lengthscale=exp(1))\n        gp_model.mean_module.initialize(constant=0)\n        likelihood.initialize(noise=exp(1))\n\n        if cuda:\n            gp_model.cuda()\n            likelihood.cuda()\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.15)\n        with gpytorch.beta_features.checkpoint_kernel(checkpoint), gpytorch.settings.fast_pred_var():\n            for _ in range(20 if checkpoint else 50):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            optimizer.step()\n\n        # Test the model\n        gp_model.eval()\n        likelihood.eval()\n        with gpytorch.settings.skip_posterior_variances(True):\n            test_function_predictions = likelihood(gp_model(test_x))\n        mean_abs_error = torch.mean(torch.abs(test_y - test_function_predictions.mean))\n\n        self.assertLess(mean_abs_error.item(), 0.05)\n\n    def test_gp_with_checkpointing(self, cuda=False):\n        return self.test_posterior_latent_gp_and_likelihood_with_optimization(cuda=cuda, checkpoint=250)\n\n    def test_fantasy_updates_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_fantasy_updates(cuda=True)\n\n    def test_fantasy_updates(self, cuda=False):\n        train_x, test_x, train_y, test_y = self._get_data(cuda=cuda)\n        # We\'re manually going to set the hyperparameters to something they shouldn\'t be\n        likelihood = GaussianLikelihood()\n        gp_model = ExactGPModel(train_x, train_y, likelihood)\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n        gp_model.covar_module.base_kernel.initialize(lengthscale=exp(1))\n        gp_model.mean_module.initialize(constant=0)\n        likelihood.initialize(noise=exp(1))\n\n        if cuda:\n            gp_model.cuda()\n            likelihood.cuda()\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.15)\n        for _ in range(50):\n            optimizer.zero_grad()\n            with gpytorch.settings.debug(False):\n                output = gp_model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.step()\n\n        for param in gp_model.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for param in likelihood.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        optimizer.step()\n\n        train_x.requires_grad = True\n        gp_model.set_train_data(train_x, train_y)\n        with gpytorch.settings.fast_pred_var(), gpytorch.settings.detach_test_caches(False):\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n            test_function_predictions = likelihood(gp_model(test_x))\n            test_function_predictions.mean.sum().backward()\n\n            real_fant_x_grad = train_x.grad[5:].clone()\n            train_x.grad = None\n            train_x.requires_grad = False\n            gp_model.set_train_data(train_x, train_y)\n\n            # Cut data down, and then add back via the fantasy interface\n            gp_model.set_train_data(train_x[:5], train_y[:5], strict=False)\n            likelihood(gp_model(test_x))\n\n            fantasy_x = train_x[5:].clone().detach().requires_grad_(True)\n            fant_model = gp_model.get_fantasy_model(fantasy_x, train_y[5:])\n            fant_function_predictions = likelihood(fant_model(test_x))\n\n            self.assertAllClose(test_function_predictions.mean, fant_function_predictions.mean, atol=1e-4)\n\n            fant_function_predictions.mean.sum().backward()\n            self.assertTrue(fantasy_x.grad is not None)\n\n            relative_error = torch.norm(real_fant_x_grad - fantasy_x.grad) / fantasy_x.grad.norm()\n            self.assertLess(relative_error, 15e-1)  # This was only passing by a hair before\n\n    def test_fantasy_updates_batch_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_fantasy_updates_batch(cuda=True)\n\n    def test_fantasy_updates_batch(self, cuda=False):\n        train_x, test_x, train_y, test_y = self._get_data(cuda=cuda)\n        # We\'re manually going to set the hyperparameters to something they shouldn\'t be\n        likelihood = GaussianLikelihood()\n        gp_model = ExactGPModel(train_x, train_y, likelihood)\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n        gp_model.covar_module.base_kernel.initialize(lengthscale=exp(1))\n        gp_model.mean_module.initialize(constant=0)\n        likelihood.initialize(noise=exp(1))\n\n        if cuda:\n            gp_model.cuda()\n            likelihood.cuda()\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.15)\n        for _ in range(50):\n            optimizer.zero_grad()\n            with gpytorch.settings.debug(False):\n                output = gp_model(train_x)\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.step()\n\n        for param in gp_model.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for param in likelihood.parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        optimizer.step()\n\n        with gpytorch.settings.fast_pred_var():\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n            test_function_predictions = likelihood(gp_model(test_x))\n\n            # Cut data down, and then add back via the fantasy interface\n            gp_model.set_train_data(train_x[:5], train_y[:5], strict=False)\n            likelihood(gp_model(test_x))\n\n            fantasy_x = train_x[5:].clone().unsqueeze(0).unsqueeze(-1).repeat(3, 1, 1).requires_grad_(True)\n            fantasy_y = train_y[5:].unsqueeze(0).repeat(3, 1)\n            fant_model = gp_model.get_fantasy_model(fantasy_x, fantasy_y)\n            fant_function_predictions = likelihood(fant_model(test_x))\n\n            self.assertAllClose(test_function_predictions.mean, fant_function_predictions.mean[0], atol=1e-4)\n\n            fant_function_predictions.mean.sum().backward()\n            self.assertTrue(fantasy_x.grad is not None)\n\n    def test_posterior_latent_gp_and_likelihood_with_optimization_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_posterior_latent_gp_and_likelihood_with_optimization(cuda=True)\n\n    def test_posterior_with_exact_computations(self):\n        with gpytorch.settings.fast_computations(covar_root_decomposition=False, log_prob=False):\n            self.test_posterior_latent_gp_and_likelihood_with_optimization(cuda=False)\n\n    def test_posterior_with_exact_computations_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                with gpytorch.settings.fast_computations(covar_root_decomposition=False, log_prob=False):\n                    self.test_posterior_latent_gp_and_likelihood_with_optimization(cuda=True)\n\n    def test_posterior_latent_gp_and_likelihood_fast_pred_var(self, cuda=False):\n        train_x, test_x, train_y, test_y = self._get_data(cuda=cuda)\n        with gpytorch.settings.fast_pred_var(), gpytorch.settings.debug(False):\n            # We\'re manually going to set the hyperparameters to\n            # something they shouldn\'t be\n            likelihood = GaussianLikelihood(noise_prior=SmoothedBoxPrior(exp(-3), exp(3), sigma=0.1))\n            gp_model = ExactGPModel(train_x, train_y, likelihood)\n            mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n            gp_model.covar_module.base_kernel.initialize(lengthscale=exp(1))\n            gp_model.mean_module.initialize(constant=0)\n            likelihood.initialize(noise=exp(1))\n\n            if cuda:\n                gp_model.cuda()\n                likelihood.cuda()\n\n            # Find optimal model hyperparameters\n            gp_model.train()\n            likelihood.train()\n            optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n            for _ in range(50):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            optimizer.step()\n\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n            # Set the cache\n            test_function_predictions = likelihood(gp_model(train_x))\n\n            # Now bump up the likelihood to something huge\n            # This will make it easy to calculate the variance\n            likelihood.noise_covar.raw_noise.data.fill_(3)\n            test_function_predictions = likelihood(gp_model(train_x))\n\n            noise = likelihood.noise_covar.noise\n            var_diff = (test_function_predictions.variance - noise).abs()\n\n            self.assertLess(torch.max(var_diff / noise), 0.05)\n\n    def test_pyro_sampling(self):\n        try:\n            import pyro  # noqa\n            from pyro.infer.mcmc import NUTS, MCMC\n        except ImportError:\n            return\n        train_x, test_x, train_y, test_y = self._get_data(cuda=False)\n        likelihood = GaussianLikelihood(noise_constraint=gpytorch.constraints.Positive())\n        gp_model = ExactGPModel(train_x, train_y, likelihood)\n\n        # Register normal GPyTorch priors\n        gp_model.mean_module.register_prior(""mean_prior"", UniformPrior(-1, 1), ""constant"")\n        gp_model.covar_module.base_kernel.register_prior(""lengthscale_prior"", UniformPrior(0.01, 0.5), ""lengthscale"")\n        gp_model.covar_module.register_prior(""outputscale_prior"", UniformPrior(1, 2), ""outputscale"")\n        likelihood.register_prior(""noise_prior"", UniformPrior(0.05, 0.3), ""noise"")\n\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        def pyro_model(x, y):\n            gp_model.pyro_sample_from_prior()\n            output = gp_model(x)\n            mll.pyro_factor(output, y)\n            return y\n\n        nuts_kernel = NUTS(pyro_model, adapt_step_size=True)\n        mcmc_run = MCMC(nuts_kernel, num_samples=3, warmup_steps=20)\n        mcmc_run.run(train_x, train_y)\n\n        gp_model.pyro_load_from_samples(mcmc_run.get_samples())\n\n        gp_model.eval()\n        expanded_test_x = test_x.unsqueeze(-1).repeat(3, 1, 1)\n        output = gp_model(expanded_test_x)\n\n        self.assertEqual(output.mean.size(0), 3)\n\n        # All 3 samples should do reasonably well on a noiseless dataset.\n        self.assertLess(torch.norm(output.mean[0] - test_y) / test_y.norm(), 0.2)\n        self.assertLess(torch.norm(output.mean[1] - test_y) / test_y.norm(), 0.2)\n        self.assertLess(torch.norm(output.mean[2] - test_y) / test_y.norm(), 0.2)\n\n    def test_posterior_latent_gp_and_likelihood_fast_pred_var_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_posterior_latent_gp_and_likelihood_fast_pred_var(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_spectral_mixture_gp_regression.py,24,"b'#!/usr/bin/env python3\n\nfrom math import exp, pi\nfrom collections import OrderedDict\n\nimport os\nimport random\nimport torch\nimport unittest\n\nimport gpytorch\nfrom torch import optim\nfrom gpytorch.kernels import SpectralMixtureKernel\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.distributions import MultivariateNormal\n\n\n# Simple training data: let\'s try to learn a sine function\ntrain_x = torch.linspace(0, 1, 15)\ntrain_y = torch.sin(train_x * (2 * pi))\n\n# Spectral mixture kernel should be able to train on\n# data up to x=0.75, but test on data up to x=2\ntest_x = torch.linspace(0, 1.5, 51)\ntest_y = torch.sin(test_x * (2 * pi))\n\ngood_state_dict = OrderedDict(\n    [\n        (""likelihood.log_noise"", torch.tensor([-5.0])),\n        (""mean_module.constant"", torch.tensor([0.4615])),\n        (""covar_module.log_mixture_weights"", torch.tensor([-0.7277, -15.1212, -0.5511, -6.3787]).unsqueeze(0)),\n        (\n            ""covar_module.log_mixture_means"",\n            torch.tensor([[-0.1201], [0.6013], [-3.7319], [0.2380]]).unsqueeze(0).unsqueeze(-2),\n        ),\n        (\n            ""covar_module.log_mixture_scales"",\n            torch.tensor([[-1.9713], [2.6217], [-3.9268], [-4.7071]]).unsqueeze(0).unsqueeze(-2),\n        ),\n    ]\n)\n\n\nclass SpectralMixtureGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(SpectralMixtureGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1, 1))\n        self.covar_module = SpectralMixtureKernel(num_mixtures=4, ard_num_dims=1)\n        self.covar_module.initialize_from_data(train_x, train_y)\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestSpectralMixtureGPRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            seed = 4\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(seed)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(seed)\n            random.seed(seed)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_spectral_mixture_gp_mean_abs_error(self):\n        likelihood = GaussianLikelihood(noise_prior=SmoothedBoxPrior(exp(-5), exp(3), sigma=0.1))\n        gp_model = SpectralMixtureGPModel(train_x, train_y, likelihood)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n\n        # Optimize the model\n        gp_model.train()\n        likelihood.train()\n        optimizer = optim.SGD(list(gp_model.parameters()), lr=0.1)\n        optimizer.n_iter = 0\n\n        with gpytorch.settings.num_trace_samples(100):\n            for _ in range(150):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            optimizer.step()\n\n            gp_model.load_state_dict(good_state_dict, strict=False)\n\n            # Test the model\n        with torch.no_grad(), gpytorch.settings.max_cg_iterations(100):\n            gp_model.eval()\n            likelihood.eval()\n            test_preds = likelihood(gp_model(test_x)).mean\n            mean_abs_error = torch.mean(torch.abs(test_y - test_preds))\n\n        # The spectral mixture kernel should be trivially able to\n        # extrapolate the sine function.\n        self.assertLess(mean_abs_error.squeeze().item(), 0.2)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_svgp_gp_classification.py,17,"b'#!/usr/bin/env python3\n\nimport math\nimport warnings\nimport unittest\nfrom unittest.mock import MagicMock, patch\n\nimport gpytorch\nimport torch\nfrom gpytorch.likelihoods import BernoulliLikelihood\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.test.base_test_case import BaseTestCase\nfrom gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\nfrom gpytorch.utils.warnings import ExtraComputationWarning\nfrom torch import optim\n\n\ndef train_data(cuda=False):\n    train_x = torch.linspace(0, 1, 260)\n    train_y = torch.cos(train_x * (2 * math.pi)).gt(0).float()\n    if cuda:\n        return train_x.cuda(), train_y.cuda()\n    else:\n        return train_x, train_y\n\n\nclass SVGPClassificationModel(ApproximateGP):\n    def __init__(self, inducing_points):\n        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(-1))\n        variational_strategy = VariationalStrategy(\n            self, inducing_points, variational_distribution, learn_inducing_locations=True\n        )\n        super(SVGPClassificationModel, self).__init__(variational_strategy)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(\n            gpytorch.kernels.RBFKernel(lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(0.001, 1.0, sigma=0.1))\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n        return latent_pred\n\n\nclass TestSVGPClassification(BaseTestCase, unittest.TestCase):\n    seed = 1\n\n    def test_classification_error(self, cuda=False, mll_cls=gpytorch.mlls.VariationalELBO):\n        train_x, train_y = train_data(cuda=cuda)\n        likelihood = BernoulliLikelihood()\n        model = SVGPClassificationModel(torch.linspace(0, 1, 25))\n        mll = mll_cls(likelihood, model, num_data=len(train_y))\n        if cuda:\n            likelihood = likelihood.cuda()\n            model = model.cuda()\n            mll = mll.cuda()\n\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n        optimizer = optim.Adam([{""params"": model.parameters()}, {""params"": likelihood.parameters()}], lr=0.1)\n\n        _wrapped_cg = MagicMock(wraps=gpytorch.utils.linear_cg)\n        _cg_mock = patch(""gpytorch.utils.linear_cg"", new=_wrapped_cg)\n        with warnings.catch_warnings(record=True) as ws, _cg_mock as cg_mock:\n            for _ in range(400):\n                optimizer.zero_grad()\n                output = model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.step()\n\n            for param in model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Set back to eval mode\n            model.eval()\n            likelihood.eval()\n            test_preds = likelihood(model(train_x)).mean.squeeze().round().float()\n            mean_abs_error = torch.mean(torch.ne(train_y, test_preds).float())\n            self.assertLess(mean_abs_error.item(), 2e-1)\n\n            # Make sure CG was called (or not), and no warnings were thrown\n            self.assertFalse(cg_mock.called)\n            self.assertFalse(any(issubclass(w.category, ExtraComputationWarning) for w in ws))\n\n    def test_predictive_ll_classification_error(self):\n        return self.test_classification_error(mll_cls=gpytorch.mlls.PredictiveLogLikelihood)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_svgp_gp_regression.py,27,"b'#!/usr/bin/env python3\n\nimport math\nimport warnings\nimport unittest\nfrom pathlib import Path\nfrom unittest.mock import MagicMock, patch\n\nimport gpytorch\nimport torch\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.test.base_test_case import BaseTestCase\nfrom gpytorch.test.utils import least_used_cuda_device\nfrom gpytorch.utils.warnings import ExtraComputationWarning, OldVersionWarning\nfrom torch import optim\n\n\ndef train_data(cuda=False):\n    train_x = torch.linspace(0, 1, 260)\n    train_y = torch.cos(train_x * (2 * math.pi))\n    if cuda:\n        return train_x.cuda(), train_y.cuda()\n    else:\n        return train_x, train_y\n\n\nclass SVGPRegressionModel(ApproximateGP):\n    def __init__(self, inducing_points, distribution_cls):\n        variational_distribution = distribution_cls(inducing_points.size(-1))\n        variational_strategy = gpytorch.variational.VariationalStrategy(\n            self, inducing_points, variational_distribution, learn_inducing_locations=True\n        )\n        super(SVGPRegressionModel, self).__init__(variational_strategy)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n        return latent_pred\n\n\nclass TestSVGPRegression(BaseTestCase, unittest.TestCase):\n    seed = 0\n\n    def test_loading_old_model(self):\n        train_x, train_y = train_data(cuda=False)\n        likelihood = GaussianLikelihood()\n        model = SVGPRegressionModel(torch.linspace(0, 1, 25), gpytorch.variational.CholeskyVariationalDistribution)\n        data_file = Path(__file__).parent.joinpath(""old_variational_strategy_model.pth"").resolve()\n        state_dicts = torch.load(data_file)\n        likelihood.load_state_dict(state_dicts[""likelihood""], strict=False)\n\n        # Ensure we get a warning\n        with warnings.catch_warnings(record=True) as ws:\n            # Makes sure warnings we catch don\'t cause `-w error` to fail\n            warnings.simplefilter(""always"", OldVersionWarning)\n\n            model.load_state_dict(state_dicts[""model""])\n            self.assertTrue(any(issubclass(w.category, OldVersionWarning) for w in ws))\n\n        with torch.no_grad():\n            model.eval()\n            likelihood.eval()\n            test_preds = likelihood(model(train_x)).mean.squeeze()\n            mean_abs_error = torch.mean(torch.abs(train_y - test_preds) / 2)\n            self.assertLess(mean_abs_error.item(), 1e-1)\n\n    def test_regression_error(\n        self,\n        cuda=False,\n        mll_cls=gpytorch.mlls.VariationalELBO,\n        distribution_cls=gpytorch.variational.CholeskyVariationalDistribution,\n    ):\n        train_x, train_y = train_data(cuda=cuda)\n        likelihood = GaussianLikelihood()\n        model = SVGPRegressionModel(torch.linspace(0, 1, 25), distribution_cls)\n        mll = mll_cls(likelihood, model, num_data=len(train_y))\n        if cuda:\n            likelihood = likelihood.cuda()\n            model = model.cuda()\n            mll = mll.cuda()\n\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n        optimizer = optim.Adam([{""params"": model.parameters()}, {""params"": likelihood.parameters()}], lr=0.01)\n\n        _wrapped_cg = MagicMock(wraps=gpytorch.utils.linear_cg)\n        _cg_mock = patch(""gpytorch.utils.linear_cg"", new=_wrapped_cg)\n        with warnings.catch_warnings(record=True) as ws, _cg_mock as cg_mock:\n            for _ in range(150):\n                optimizer.zero_grad()\n                output = model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.step()\n\n            for param in model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Set back to eval mode\n            model.eval()\n            likelihood.eval()\n            test_preds = likelihood(model(train_x)).mean.squeeze()\n            mean_abs_error = torch.mean(torch.abs(train_y - test_preds) / 2)\n            self.assertLess(mean_abs_error.item(), 1e-1)\n\n            # Make sure CG was called (or not), and no warnings were thrown\n            self.assertFalse(cg_mock.called)\n            self.assertFalse(any(issubclass(w.category, ExtraComputationWarning) for w in ws))\n\n    def test_predictive_ll_regression_error(self):\n        return self.test_regression_error(\n            mll_cls=gpytorch.mlls.PredictiveLogLikelihood,\n            distribution_cls=gpytorch.variational.MeanFieldVariationalDistribution,\n        )\n\n    def test_predictive_ll_regression_error_delta(self):\n        return self.test_regression_error(\n            mll_cls=gpytorch.mlls.PredictiveLogLikelihood,\n            distribution_cls=gpytorch.variational.DeltaVariationalDistribution,\n        )\n\n    def test_robust_regression_error(self):\n        return self.test_regression_error(mll_cls=gpytorch.mlls.GammaRobustVariationalELBO)\n\n    def test_regression_error_cuda(self):\n        if not torch.cuda.is_available():\n            return\n        with least_used_cuda_device():\n            return self.test_regression_error(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_unwhitened_svgp_regression.py,14,"b'#!/usr/bin/env python3\n\nimport math\nimport warnings\nimport unittest\n\nimport gpytorch\nimport torch\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.test.base_test_case import BaseTestCase\nfrom gpytorch.utils.warnings import ExtraComputationWarning\nfrom torch import optim\n\n\ndef train_data(cuda=False):\n    train_x = torch.linspace(0, 1, 260)\n    train_y = torch.cos(train_x * (2 * math.pi))\n    if cuda:\n        return train_x.cuda(), train_y.cuda()\n    else:\n        return train_x, train_y\n\n\nclass SVGPRegressionModel(ApproximateGP):\n    def __init__(self, inducing_points, distribution_cls):\n        variational_distribution = distribution_cls(inducing_points.size(-1))\n        variational_strategy = gpytorch.variational.UnwhitenedVariationalStrategy(\n            self, inducing_points, variational_distribution, learn_inducing_locations=True\n        )\n        super(SVGPRegressionModel, self).__init__(variational_strategy)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n        return latent_pred\n\n\nclass TestSVGPRegression(BaseTestCase, unittest.TestCase):\n    seed = 0\n\n    def test_regression_error(\n        self,\n        cuda=False,\n        mll_cls=gpytorch.mlls.VariationalELBO,\n        distribution_cls=gpytorch.variational.CholeskyVariationalDistribution,\n    ):\n        train_x, train_y = train_data(cuda=cuda)\n        likelihood = GaussianLikelihood()\n        model = SVGPRegressionModel(torch.linspace(0, 1, 25), distribution_cls)\n        mll = mll_cls(likelihood, model, num_data=len(train_y))\n        if cuda:\n            likelihood = likelihood.cuda()\n            model = model.cuda()\n            mll = mll.cuda()\n\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n        optimizer = optim.Adam([{""params"": model.parameters()}, {""params"": likelihood.parameters()}], lr=0.01)\n\n        with warnings.catch_warnings(record=True) as ws:\n            for _ in range(200):\n                optimizer.zero_grad()\n                output = model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.step()\n\n            for param in model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n\n            # Set back to eval mode\n            model.eval()\n            likelihood.eval()\n            test_preds = likelihood(model(train_x)).mean.squeeze()\n            mean_abs_error = torch.mean(torch.abs(train_y - test_preds) / 2)\n            self.assertLess(mean_abs_error.item(), 1e-1)\n\n            # Make sure CG was called (or not), and no warnings were thrown\n            self.assertFalse(any(issubclass(w.category, ExtraComputationWarning) for w in ws))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/examples/test_white_noise_regression.py,35,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport unittest\nimport warnings\nfrom math import exp, pi\n\nimport gpytorch\nimport torch\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.kernels import RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import GaussianLikelihood, FixedNoiseGaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.test.utils import least_used_cuda_device\nfrom gpytorch.utils.warnings import GPInputWarning\nfrom torch import optim\n\n\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_inputs, train_targets, likelihood):\n        super(ExactGPModel, self).__init__(train_inputs, train_targets, likelihood)\n        self.mean_module = ConstantMean(prior=SmoothedBoxPrior(-1, 1))\n        self.rbf_covar_module = RBFKernel(lengthscale_prior=SmoothedBoxPrior(exp(-3), exp(3), sigma=0.1))\n        self.covar_module = ScaleKernel(self.rbf_covar_module)\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultivariateNormal(mean_x, covar_x)\n\n\nclass TestWhiteNoiseGPRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(1)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(1)\n            random.seed(1)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def _get_data(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        # Simple training data: let\'s try to learn a sine function\n        train_x = torch.linspace(0, 1, 11, device=device)\n        train_y = torch.sin(train_x * (2 * pi))\n        test_x = torch.linspace(0, 1, 51, device=device)\n        test_y = torch.sin(test_x * (2 * pi))\n        return train_x, test_x, train_y, test_y\n\n    def test_posterior_latent_gp_and_likelihood_without_optimization(self, cuda=False):\n        train_x, test_x, train_y, test_y = self._get_data(cuda=cuda)\n        with gpytorch.settings.debug(False):\n            # We\'re manually going to set the hyperparameters to be ridiculous\n            likelihood = FixedNoiseGaussianLikelihood(torch.ones(11) * 1e-8)\n            gp_model = ExactGPModel(train_x, train_y, likelihood)\n            # Update lengthscale prior to accommodate extreme parameters\n            gp_model.rbf_covar_module.initialize(lengthscale=exp(-6))\n            gp_model.mean_module.initialize(constant=0)\n\n            if cuda:\n                gp_model.cuda()\n                likelihood.cuda()\n\n            # Compute posterior distribution\n            gp_model.eval()\n            likelihood.eval()\n\n            # Let\'s see how our model does, conditioned with weird hyperparams\n            # The posterior should fit all the data\n            function_predictions = likelihood(gp_model(train_x))\n\n            self.assertLess(torch.norm(function_predictions.mean - train_y), 1e-3)\n            self.assertLess(torch.norm(function_predictions.variance), 5e-3)\n\n            # It shouldn\'t fit much else though\n            test_function_predictions = gp_model(torch.tensor([1.1]).type_as(test_x))\n\n            self.assertLess(torch.norm(test_function_predictions.mean - 0), 1e-4)\n            self.assertLess(torch.norm(test_function_predictions.variance - gp_model.covar_module.outputscale), 1e-4)\n\n    def test_posterior_latent_gp_and_likelihood_without_optimization_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_posterior_latent_gp_and_likelihood_without_optimization(cuda=True)\n\n    def test_posterior_latent_gp_and_likelihood_with_optimization(self, cuda=False):\n        # This test throws a warning because the fixed noise likelihood gets the wrong input\n        warnings.simplefilter(""ignore"", GPInputWarning)\n\n        train_x, test_x, train_y, test_y = self._get_data(cuda=cuda)\n        # We\'re manually going to set the hyperparameters to something they shouldn\'t be\n        likelihood = FixedNoiseGaussianLikelihood(torch.ones(11) * 0.001)\n        gp_model = ExactGPModel(train_x, train_y, likelihood)\n        mll = gpytorch.ExactMarginalLogLikelihood(likelihood, gp_model)\n        gp_model.rbf_covar_module.initialize(lengthscale=exp(1))\n        gp_model.mean_module.initialize(constant=0)\n\n        if cuda:\n            gp_model.cuda()\n            likelihood.cuda()\n\n        # Find optimal model hyperparameters\n        gp_model.train()\n        likelihood.train()\n\n        optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n        optimizer.n_iter = 0\n        with gpytorch.settings.debug(False):\n            for _ in range(75):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            optimizer.step()\n\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n            test_function_predictions = likelihood(gp_model(test_x))\n            mean_abs_error = torch.mean(torch.abs(test_y - test_function_predictions.mean))\n\n        self.assertLess(mean_abs_error.squeeze().item(), 0.05)\n\n    def test_posterior_latent_gp_and_likelihood_with_optimization_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_posterior_latent_gp_and_likelihood_with_optimization(cuda=True)\n\n    def test_posterior_latent_gp_and_likelihood_fast_pred_var(self, cuda=False):\n        train_x, test_x, train_y, test_y = self._get_data(cuda=cuda)\n        with gpytorch.settings.fast_pred_var(), gpytorch.settings.debug(False):\n            # We\'re manually going to set the hyperparameters to something they shouldn\'t be\n            likelihood = GaussianLikelihood(noise_prior=SmoothedBoxPrior(exp(-3), exp(3), sigma=0.1))\n            gp_model = ExactGPModel(train_x, train_y, likelihood)\n            mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n            gp_model.rbf_covar_module.initialize(lengthscale=exp(1))\n            gp_model.mean_module.initialize(constant=0)\n            likelihood.initialize(noise=exp(1))\n\n            if cuda:\n                gp_model.cuda()\n                likelihood.cuda()\n\n            # Find optimal model hyperparameters\n            gp_model.train()\n            likelihood.train()\n            optimizer = optim.Adam(list(gp_model.parameters()) + list(likelihood.parameters()), lr=0.1)\n            optimizer.n_iter = 0\n            for _ in range(50):\n                optimizer.zero_grad()\n                output = gp_model(train_x)\n                loss = -mll(output, train_y)\n                loss.backward()\n                optimizer.n_iter += 1\n                optimizer.step()\n\n            for param in gp_model.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            for param in likelihood.parameters():\n                self.assertTrue(param.grad is not None)\n                self.assertGreater(param.grad.norm().item(), 0)\n            optimizer.step()\n\n            # Test the model\n            gp_model.eval()\n            likelihood.eval()\n            # Set the cache\n            test_function_predictions = likelihood(gp_model(train_x))\n\n            # Now bump up the likelihood to something huge\n            # This will make it easy to calculate the variance\n            likelihood.raw_noise.data.fill_(3)\n            test_function_predictions = likelihood(gp_model(train_x))\n\n            noise = likelihood.noise\n            var_diff = (test_function_predictions.variance - noise).abs()\n\n            self.assertLess(torch.max(var_diff / noise), 0.05)\n\n    def test_posterior_latent_gp_and_likelihood_fast_pred_var_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_posterior_latent_gp_and_likelihood_fast_pred_var(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/functions/__init__.py,0,b'#!/usr/bin/env python3\n'
test/functions/test_dsmm.py,83,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport gpytorch\n\n\nclass TestDSMM(unittest.TestCase):\n    def test_forward(self):\n        i = torch.tensor([[0, 1, 1], [2, 0, 2]], dtype=torch.long)\n        v = torch.tensor([3, 4, 5], dtype=torch.float)\n        sparse = torch.sparse.FloatTensor(i, v, torch.Size([2, 3]))\n        dense = torch.randn(3, 3)\n\n        res = gpytorch.dsmm(sparse, dense)\n        actual = torch.mm(sparse.to_dense(), dense)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_forward_batch(self):\n        i = torch.tensor([[0, 0, 0, 1, 1, 1], [0, 1, 1, 0, 1, 1], [2, 0, 2, 2, 0, 2]], dtype=torch.long)\n        v = torch.tensor([3, 4, 5, 6, 7, 8], dtype=torch.float)\n        sparse = torch.sparse.FloatTensor(i, v, torch.Size([2, 2, 3]))\n        dense = torch.randn(2, 3, 3)\n\n        res = gpytorch.dsmm(sparse, dense)\n        actual = torch.matmul(sparse.to_dense(), dense)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_forward_multi_batch(self):\n        i = torch.tensor(\n            [[0, 1, 1, 0, 0, 1], [0, 0, 0, 1, 1, 1], [0, 1, 1, 0, 1, 1], [2, 0, 2, 2, 0, 2]], dtype=torch.long\n        )\n        v = torch.tensor([3, 4, 5, 6, 7, 8], dtype=torch.float)\n        sparse = torch.sparse.FloatTensor(i, v, torch.Size([2, 2, 2, 3]))\n        dense = torch.randn(2, 2, 3, 3)\n\n        res = gpytorch.dsmm(sparse, dense)\n        actual = torch.matmul(sparse.to_dense(), dense)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_backward(self):\n        i = torch.tensor([[0, 1, 1], [2, 0, 2]], dtype=torch.long)\n        v = torch.tensor([3, 4, 5], dtype=torch.float)\n        sparse = torch.sparse.FloatTensor(i, v, torch.Size([2, 3]))\n        dense = torch.randn(3, 4, requires_grad=True)\n        dense_copy = dense.clone().detach().requires_grad_(True)\n        grad_output = torch.randn(2, 4)\n\n        res = gpytorch.dsmm(sparse, dense)\n        res.backward(grad_output)\n        actual = torch.mm(sparse.to_dense(), dense_copy)\n        actual.backward(grad_output)\n        self.assertLess(torch.norm(dense.grad - dense_copy.grad).item(), 1e-5)\n\n    def test_backward_batch(self):\n        i = torch.tensor([[0, 0, 0, 1, 1, 1], [0, 1, 1, 0, 1, 1], [2, 0, 2, 2, 0, 2]], dtype=torch.long)\n        v = torch.tensor([3, 4, 5, 6, 7, 8], dtype=torch.float)\n        sparse = torch.sparse.FloatTensor(i, v, torch.Size([2, 2, 3]))\n        dense = torch.randn(2, 3, 4, requires_grad=True)\n        dense_copy = dense.clone().detach().requires_grad_(True)\n        grad_output = torch.randn(2, 2, 4)\n\n        res = gpytorch.dsmm(sparse, dense)\n        res.backward(grad_output)\n        actual = torch.matmul(sparse.to_dense(), dense_copy)\n        actual.backward(grad_output)\n        self.assertLess(torch.norm(dense.grad - dense_copy.grad).item(), 1e-5)\n\n    def test_backward_multi_batch(self):\n        i = torch.tensor(\n            [[0, 1, 1, 0, 0, 1], [0, 0, 0, 1, 1, 1], [0, 1, 1, 0, 1, 1], [2, 0, 2, 2, 0, 2]], dtype=torch.long\n        )\n        v = torch.tensor([3, 4, 5, 6, 7, 8], dtype=torch.float)\n        sparse = torch.sparse.FloatTensor(i, v, torch.Size([2, 2, 2, 3]))\n        dense = torch.randn(2, 2, 3, 4, requires_grad=True)\n        dense_copy = dense.clone().detach().requires_grad_(True)\n        grad_output = torch.randn(2, 2, 2, 4)\n\n        res = gpytorch.dsmm(sparse, dense)\n        res.backward(grad_output)\n        actual = torch.matmul(sparse.to_dense(), dense_copy)\n        actual.backward(grad_output)\n        self.assertLess(torch.norm(dense.grad - dense_copy.grad).item(), 1e-5)\n\n    def test_broadcast_rhs(self):\n        i = torch.tensor([[0, 1, 1, 0, 1, 1], [2, 0, 2, 2, 0, 2]], dtype=torch.long)\n        v = torch.tensor([3, 4, 5, 6, 7, 8], dtype=torch.float)\n        sparse = torch.sparse.FloatTensor(i, v, torch.Size([2, 3]))\n        dense = torch.randn(4, 2, 3, 4, requires_grad=True)\n        dense_copy = dense.clone().detach().requires_grad_(True)\n\n        res = gpytorch.dsmm(sparse, dense)\n        actual = torch.matmul(sparse.to_dense(), dense_copy)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        grad_output = torch.randn(4, 2, 2, 4)\n        res.backward(grad_output)\n        actual.backward(grad_output)\n        self.assertLess(torch.norm(dense.grad - dense_copy.grad).item(), 1e-5)\n\n        i = torch.tensor([[0, 0, 0, 1, 1, 1], [0, 1, 1, 0, 1, 1], [2, 0, 2, 2, 0, 2]], dtype=torch.long)\n        v = torch.tensor([3, 4, 5, 6, 7, 8], dtype=torch.float)\n        sparse = torch.sparse.FloatTensor(i, v, torch.Size([2, 2, 3]))\n        dense = torch.randn(4, 2, 3, 4, requires_grad=True)\n        dense_copy = dense.clone().detach().requires_grad_(True)\n\n        res = gpytorch.dsmm(sparse, dense)\n        actual = torch.matmul(sparse.to_dense(), dense_copy)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        grad_output = torch.randn(4, 2, 2, 4)\n        res.backward(grad_output)\n        actual.backward(grad_output)\n        self.assertLess(torch.norm(dense.grad - dense_copy.grad).item(), 1e-5)\n\n    def test_broadcast_sparse(self):\n        i = torch.tensor([[0, 0, 0, 1, 1, 1], [0, 1, 1, 0, 1, 1], [2, 0, 2, 2, 0, 2]], dtype=torch.long)\n        v = torch.tensor([3, 4, 5, 6, 7, 8], dtype=torch.float)\n        sparse = torch.sparse.FloatTensor(i, v, torch.Size([2, 2, 3]))\n        dense = torch.randn(3, 4, requires_grad=True)\n        dense_copy = dense.clone().detach().requires_grad_(True)\n\n        res = gpytorch.dsmm(sparse, dense)\n        actual = torch.matmul(sparse.to_dense(), dense_copy)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        grad_output = torch.randn(2, 2, 4)\n        res.backward(grad_output)\n        actual.backward(grad_output)\n        self.assertLess(torch.norm(dense.grad - dense_copy.grad).item(), 1e-5)\n\n    def test_broadcast_singleton(self):\n        i = torch.tensor([[0, 0, 0, 1, 1, 1], [0, 1, 1, 0, 1, 1], [2, 0, 2, 2, 0, 2]], dtype=torch.long)\n        v = torch.tensor([3, 4, 5, 6, 7, 8], dtype=torch.float)\n        sparse = torch.sparse.FloatTensor(i, v, torch.Size([2, 2, 3]))\n        dense = torch.randn(1, 3, 4, requires_grad=True)\n        dense_copy = dense.clone().detach().requires_grad_(True)\n\n        res = gpytorch.dsmm(sparse, dense)\n        actual = torch.matmul(sparse.to_dense(), dense_copy)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        grad_output = torch.randn(2, 2, 4)\n        res.backward(grad_output)\n        actual.backward(grad_output)\n        self.assertLess(torch.norm(dense.grad - dense_copy.grad).item(), 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/functions/test_inv_matmul.py,9,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch import settings\nfrom gpytorch.lazy import NonLazyTensor\nfrom gpytorch.test.base_test_case import BaseTestCase\n\n\ndef _ensure_symmetric_grad(grad):\n    """"""\n    A gradient-hook hack to ensure that symmetric matrix gradients are symmetric\n    """"""\n    res = torch.add(grad, grad.transpose(-1, -2)).mul(0.5)\n    return res\n\n\nclass TestInvMatmulNonBatch(BaseTestCase, unittest.TestCase):\n    seed = 0\n\n    def _create_mat(self):\n        mat = torch.randn(8, 8)\n        mat = mat @ mat.transpose(-1, -2)\n        return mat\n\n    def test_inv_matmul_vec(self):\n        mat = self._create_mat().detach().requires_grad_(True)\n        if mat.dim() > 2:  # This isn\'t a feature for batch mode\n            return\n        mat_copy = mat.detach().clone().requires_grad_(True)\n        mat_copy.register_hook(_ensure_symmetric_grad)\n        vec = torch.randn(mat.size(-1)).detach().requires_grad_(True)\n        vec_copy = vec.detach().clone().requires_grad_(True)\n\n        # Forward\n        with settings.terminate_cg_by_size(False):\n            res = NonLazyTensor(mat).inv_matmul(vec)\n            actual = mat_copy.inverse().matmul(vec_copy)\n            self.assertAllClose(res, actual)\n\n            # Backward\n            grad_output = torch.randn_like(vec)\n            res.backward(gradient=grad_output)\n            actual.backward(gradient=grad_output)\n            self.assertAllClose(mat.grad, mat_copy.grad)\n            self.assertAllClose(vec.grad, vec_copy.grad)\n\n    def test_inv_matmul_multiple_vecs(self):\n        mat = self._create_mat().detach().requires_grad_(True)\n        mat_copy = mat.detach().clone().requires_grad_(True)\n        mat_copy.register_hook(_ensure_symmetric_grad)\n        vecs = torch.randn(*mat.shape[:-2], mat.size(-1), 4).detach().requires_grad_(True)\n        vecs_copy = vecs.detach().clone().requires_grad_(True)\n\n        # Forward\n        with settings.terminate_cg_by_size(False):\n            res = NonLazyTensor(mat).inv_matmul(vecs)\n            actual = mat_copy.inverse().matmul(vecs_copy)\n            self.assertAllClose(res, actual)\n\n            # Backward\n            grad_output = torch.randn_like(vecs)\n            res.backward(gradient=grad_output)\n            actual.backward(gradient=grad_output)\n            self.assertAllClose(mat.grad, mat_copy.grad)\n            self.assertAllClose(vecs.grad, vecs_copy.grad)\n\n\nclass TestInvMatmulBatch(TestInvMatmulNonBatch):\n    seed = 0\n\n    def _create_mat(self):\n        mats = torch.randn(2, 8, 8)\n        mats = mats @ mats.transpose(-1, -2)\n        return mats\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/functions/test_inv_quad.py,45,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport unittest\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.lazy import NonLazyTensor\n\n\nclass TestInvQuadNonBatch(unittest.TestCase):\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def setUp(self):\n        if os.getenv(""unlock_seed"") is None or os.getenv(""unlock_seed"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n        mat = torch.randn(4, 4)\n        mat = mat @ mat.transpose(-1, -2)\n        mat.div_(5).add_(torch.eye(4))\n        vecs = torch.randn(5, 4, 6)\n        vec = torch.randn(4)\n        vecs = torch.randn(4, 8)\n        self.mat = mat.detach().clone().requires_grad_(True)\n        self.mat_clone = mat.detach().clone().requires_grad_(True)\n        self.vec = vec.detach().clone().requires_grad_(True)\n        self.vec_clone = vec.detach().clone().requires_grad_(True)\n        self.vecs = vecs.detach().clone().requires_grad_(True)\n        self.vecs_clone = vecs.detach().clone().requires_grad_(True)\n\n    def test_inv_quad_vector(self):\n        # Forward pass\n        actual_inv_quad = self.mat_clone.inverse().matmul(self.vec_clone).mul(self.vec_clone).sum()\n        with gpytorch.settings.num_trace_samples(1000):\n            non_lazy_tsr = NonLazyTensor(self.mat)\n            res_inv_quad = non_lazy_tsr.inv_quad(self.vec)\n\n        self.assertAlmostEqual(res_inv_quad.item(), actual_inv_quad.item(), places=1)\n\n        # Backward\n        actual_inv_quad.backward()\n        res_inv_quad.backward(retain_graph=True)\n\n        self.assertLess(torch.max((self.mat_clone.grad - self.mat.grad).abs()).item(), 1e-1)\n        self.assertLess(torch.max((self.vec_clone.grad - self.vec.grad).abs()).item(), 1e-1)\n\n    def test_inv_quad_many_vectors(self):\n        # Forward pass\n        actual_inv_quad = self.mat_clone.inverse().matmul(self.vecs_clone).mul(self.vecs_clone).sum()\n        with gpytorch.settings.num_trace_samples(1000):\n            non_lazy_tsr = NonLazyTensor(self.mat)\n            res_inv_quad = non_lazy_tsr.inv_quad(self.vecs)\n        self.assertAlmostEqual(res_inv_quad.item(), actual_inv_quad.item(), places=1)\n\n        # Backward\n        actual_inv_quad.backward()\n        res_inv_quad.backward(retain_graph=True)\n\n        self.assertLess(torch.max((self.mat_clone.grad - self.mat.grad).abs()).item(), 1e-1)\n        self.assertLess(torch.max((self.vecs_clone.grad - self.vecs.grad).abs()).item(), 1e-1)\n\n\nclass TestInvQuadBatch(unittest.TestCase):\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def setUp(self):\n        if os.getenv(""unlock_seed"") is None or os.getenv(""unlock_seed"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(1)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(1)\n            random.seed(1)\n\n        mats = torch.randn(5, 4, 4)\n        mats = mats @ mats.transpose(-1, -2)\n        mats.div_(5).add_(torch.eye(4).unsqueeze_(0))\n        vecs = torch.randn(5, 4, 6)\n        self.mats = mats.detach().clone().requires_grad_(True)\n        self.mats_clone = mats.detach().clone().requires_grad_(True)\n        self.vecs = vecs.detach().clone().requires_grad_(True)\n        self.vecs_clone = vecs.detach().clone().requires_grad_(True)\n\n    def test_inv_quad_many_vectors(self):\n        # Forward pass\n        actual_inv_quad = (\n            torch.cat([mat.inverse().unsqueeze(0) for mat in self.mats_clone])\n            .matmul(self.vecs_clone)\n            .mul(self.vecs_clone)\n            .sum(2)\n            .sum(1)\n        )\n        with gpytorch.settings.num_trace_samples(2000):\n            non_lazy_tsr = NonLazyTensor(self.mats)\n            res_inv_quad = non_lazy_tsr.inv_quad(self.vecs)\n\n        self.assertEqual(res_inv_quad.shape, actual_inv_quad.shape)\n        self.assertLess(torch.max((res_inv_quad - actual_inv_quad).abs()).item(), 1e-1)\n\n        # Backward\n        inv_quad_grad_output = torch.randn(5, dtype=torch.float)\n        actual_inv_quad.backward(gradient=inv_quad_grad_output)\n        res_inv_quad.backward(gradient=inv_quad_grad_output, retain_graph=True)\n\n        self.assertLess(torch.max((self.mats_clone.grad - self.mats.grad).abs()).item(), 1e-1)\n        self.assertLess(torch.max((self.vecs_clone.grad - self.vecs.grad).abs()).item(), 1e-1)\n\n\nclass TestInvQuadMultiBatch(unittest.TestCase):\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def setUp(self):\n        if os.getenv(""unlock_seed"") is None or os.getenv(""unlock_seed"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n        mats = torch.randn(2, 3, 4, 4)\n        mats = mats @ mats.transpose(-1, -2)\n        mats.div_(5).add_(torch.eye(4).view(1, 1, 4, 4))\n        vecs = torch.randn(2, 3, 4, 6)\n        self.mats = mats.detach().clone().requires_grad_(True)\n        self.mats_clone = mats.detach().clone().requires_grad_(True)\n        self.vecs = vecs.detach().clone().requires_grad_(True)\n        self.vecs_clone = vecs.detach().clone().requires_grad_(True)\n\n    def test_inv_quad_many_vectors(self):\n        # Forward pass\n        flattened_mats = self.mats_clone.view(-1, *self.mats_clone.shape[-2:])\n        actual_inv_quad = (\n            torch.cat([mat.inverse().unsqueeze(0) for mat in flattened_mats])\n            .view(self.mats_clone.shape)\n            .matmul(self.vecs_clone)\n            .mul(self.vecs_clone)\n            .sum(-2)\n            .sum(-1)\n        )\n\n        with gpytorch.settings.num_trace_samples(2000):\n            non_lazy_tsr = NonLazyTensor(self.mats)\n            res_inv_quad = non_lazy_tsr.inv_quad(self.vecs)\n\n        self.assertEqual(res_inv_quad.shape, actual_inv_quad.shape)\n        self.assertLess(torch.max((res_inv_quad - actual_inv_quad).abs()).item(), 1e-1)\n\n        # Backward\n        inv_quad_grad_output = torch.randn(2, 3, dtype=torch.float)\n        actual_inv_quad.backward(gradient=inv_quad_grad_output)\n        res_inv_quad.backward(gradient=inv_quad_grad_output, retain_graph=True)\n\n        self.assertLess(torch.max((self.mats_clone.grad - self.mats.grad).abs()).item(), 1e-1)\n        self.assertLess(torch.max((self.vecs_clone.grad - self.vecs.grad).abs()).item(), 1e-1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/functions/test_inv_quad_log_det.py,21,"b'#!/usr/bin/env python3\n\nimport unittest\nfrom unittest.mock import MagicMock, patch\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.lazy import RootLazyTensor\nfrom gpytorch.test.base_test_case import BaseTestCase\n\n\nclass TestInvQuadLogDetNonBatch(BaseTestCase, unittest.TestCase):\n    seed = 0\n    matrix_shape = torch.Size((4, 4))\n\n    def _test_inv_quad_logdet(self, inv_quad_rhs=None, logdet=False, improper_logdet=False, add_diag=False):\n        # Set up\n        mat = torch.randn(*self.__class__.matrix_shape).requires_grad_(True)\n        mat_clone = mat.detach().clone().requires_grad_(True)\n\n        if inv_quad_rhs is not None:\n            inv_quad_rhs.requires_grad_(True)\n            inv_quad_rhs_clone = inv_quad_rhs.detach().clone().requires_grad_(True)\n\n        # Compute actual values\n        actual_tensor = mat_clone @ mat_clone.transpose(-1, -2)\n\n        if add_diag:\n            actual_tensor.diagonal(dim1=-2, dim2=-1).add_(1.0)\n\n        if inv_quad_rhs is not None:\n            actual_inv_quad = actual_tensor.inverse().matmul(inv_quad_rhs_clone).mul(inv_quad_rhs_clone)\n            actual_inv_quad = actual_inv_quad.sum([-1, -2]) if inv_quad_rhs.dim() >= 2 else actual_inv_quad.sum()\n        if logdet:\n            flattened_tensor = actual_tensor.view(-1, *actual_tensor.shape[-2:])\n            logdets = torch.cat([mat.logdet().unsqueeze(0) for mat in flattened_tensor])\n            if actual_tensor.dim() > 2:\n                actual_logdet = logdets.view(*actual_tensor.shape[:-2])\n            else:\n                actual_logdet = logdets.squeeze()\n\n        # Compute values with LazyTensor\n        _wrapped_cg = MagicMock(wraps=gpytorch.utils.linear_cg)\n        with gpytorch.settings.num_trace_samples(2000), gpytorch.settings.max_cholesky_size(\n            0\n        ), gpytorch.settings.cg_tolerance(1e-5), gpytorch.settings.skip_logdet_forward(improper_logdet), patch(\n            ""gpytorch.utils.linear_cg"", new=_wrapped_cg\n        ) as linear_cg_mock:\n            lazy_tensor = RootLazyTensor(mat)\n\n            if add_diag:\n                lazy_tensor = lazy_tensor.add_jitter(1.0)\n\n            res_inv_quad, res_logdet = lazy_tensor.inv_quad_logdet(inv_quad_rhs=inv_quad_rhs, logdet=logdet)\n\n        # Compare forward pass\n        if inv_quad_rhs is not None:\n            self.assertAllClose(res_inv_quad, actual_inv_quad, rtol=1e-2)\n        if logdet:\n            if improper_logdet:\n                self.assertAlmostEqual(res_logdet.norm().item(), 0)\n            else:\n                self.assertAllClose(res_logdet, actual_logdet, rtol=1e-1, atol=2e-1)\n\n        # Backward\n        if inv_quad_rhs is not None:\n            actual_inv_quad.sum().backward(retain_graph=True)\n            res_inv_quad.sum().backward(retain_graph=True)\n        if logdet:\n            actual_logdet.sum().backward()\n            res_logdet.sum().backward()\n\n        self.assertAllClose(mat_clone.grad, mat.grad, rtol=1e-1, atol=2e-1)\n        if inv_quad_rhs is not None:\n            self.assertAllClose(inv_quad_rhs.grad, inv_quad_rhs_clone.grad, rtol=1e-2)\n\n        # Make sure CG was called\n        self.assertTrue(linear_cg_mock.called)\n\n    def test_inv_quad_logdet_vector(self):\n        rhs = torch.randn(self.matrix_shape[-1])\n        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True)\n\n    def test_precond_inv_quad_logdet_vector(self):\n        rhs = torch.randn(self.matrix_shape[-1])\n        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True, add_diag=True)\n\n    def test_inv_quad_only_vector(self):\n        rhs = torch.randn(self.matrix_shape[-1])\n        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=False)\n\n    def test_precond_inv_quad_only_vector(self):\n        rhs = torch.randn(self.matrix_shape[-1])\n        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=False, add_diag=True)\n\n    def test_inv_quad_logdet_many_vectors(self):\n        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True)\n\n    def test_precond_inv_quad_logdet_many_vectors(self):\n        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True, add_diag=True)\n\n    def test_inv_quad_logdet_many_vectors_improper(self):\n        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True, improper_logdet=True)\n\n    def test_precond_inv_quad_logdet_many_vectors_improper(self):\n        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=True, improper_logdet=True, add_diag=True)\n\n    def test_inv_quad_only_many_vectors(self):\n        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=False)\n\n    def test_precond_inv_quad_only_many_vectors(self):\n        rhs = torch.randn(*self.matrix_shape[:-1], 5)\n        self._test_inv_quad_logdet(inv_quad_rhs=rhs, logdet=False, add_diag=True)\n\n\nclass TestInvQuadLogDetBatch(TestInvQuadLogDetNonBatch):\n    seed = 0\n    matrix_shape = torch.Size((3, 4, 4))\n\n    def test_inv_quad_logdet_vector(self):\n        pass\n\n    def test_precond_inv_quad_logdet_vector(self):\n        pass\n\n    def test_inv_quad_only_vector(self):\n        pass\n\n    def test_precond_inv_quad_only_vector(self):\n        pass\n\n\nclass TestInvQuadLogDetMultiBatch(TestInvQuadLogDetBatch):\n    seed = 0\n    matrix_shape = torch.Size((2, 3, 4, 4))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/functions/test_log_normal_cdf.py,9,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\n\nimport torch\nfrom torch import nn\n\nimport gpytorch\n\n\nclass TestLogNormalCDF(unittest.TestCase):\n    def test_forward(self):\n        inputs = torch.tensor([-6, -5, -3, -1, 0, 1, 3, 5], dtype=torch.float)\n        output = gpytorch.log_normal_cdf(inputs)\n\n        # Answers should be reasonable for small values\n        self.assertLess(math.fabs(output[0] + 20.7368), 1e-4)\n        self.assertLess(math.fabs(output[1] + 15), 0.1)\n        self.assertLess(math.fabs(output[2] + 6.6), 0.01)\n        self.assertLess(math.fabs(output[3] + 1.841), 0.001)\n\n        # Should be very accurate for positive values\n        self.assertLess(math.fabs(output[4] + 0.693147), 1e-4)\n        self.assertLess(math.fabs(output[5] + 0.1727), 1e-4)\n        self.assertLess(math.fabs(output[6] + 0.00135081), 1e-4)\n        self.assertLess(math.fabs(output[7] + 2.86652e-7), 1e-4)\n\n    def test_backward(self):\n        inputs = nn.Parameter(torch.tensor([-6, -5, -3, -1, 0, 1, 3, 5], dtype=torch.float))\n        output = gpytorch.log_normal_cdf(inputs)\n        output.backward(torch.ones(8))\n\n        gradient = inputs.grad\n        expected_gradient = torch.tensor(\n            [6.1585, 5.1865, 3.2831, 1.5251, 0.7979, 0.2876, 0.0044, 0.0000], dtype=torch.float\n        )\n\n        # Should be reasonable for small values\n        for d in torch.abs(gradient[:3] - expected_gradient[:3]):\n            self.assertLess(d, 5e-1)\n\n        # Should be very accurate for larger ones\n        for d in torch.abs(gradient[3:] - expected_gradient[3:]):\n            self.assertLess(d, 5e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/functions/test_matern_covariance.py,35,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\n\nimport torch\n\nimport gpytorch\n\n\ndef dist_func(x1, x2):\n    dist_module = gpytorch.kernels.kernel.Distance()\n    return dist_module._dist(x1, x2, postprocess=torch.tensor(False))\n\n\nclass TestMaternCovariance(unittest.TestCase):\n    def test_1_2_forward(self):\n        nu = 1 / 2\n        batch_size = (3, 2, 4)\n        x1 = torch.randn(*batch_size, 7, 9)\n        x2 = torch.randn(*batch_size, 6, 9)\n        # Doesn\'t support ARD\n        lengthscale = torch.randn(*batch_size).view(*batch_size, 1, 1) ** 2\n\n        res = gpytorch.functions.MaternCovariance().apply(x1, x2, lengthscale, nu, dist_func)\n        scaled_unitless_dist = math.sqrt(nu * 2) * dist_func(x1, x2).div(lengthscale)\n        exp_component = torch.exp(-scaled_unitless_dist)\n        actual = exp_component\n        self.assertTrue(torch.allclose(res, actual))\n\n    def test_1_2_backward(self):\n        nu = 1 / 2\n        batch_size = (3, 2, 4)\n        x1 = torch.randn(*batch_size, 7, 9, dtype=torch.float64)\n        x2 = torch.randn(*batch_size, 6, 9, dtype=torch.float64)\n        lengthscale = torch.randn(*batch_size, dtype=torch.float64, requires_grad=True).view(*batch_size, 1, 1) ** 2\n        f = lambda x1, x2, l: gpytorch.functions.MaternCovariance().apply(x1, x2, l, nu, dist_func)\n        try:\n            torch.autograd.gradcheck(f, (x1, x2, lengthscale))\n        except RuntimeError:\n            self.fail(f""Gradcheck failed"")\n\n    def test_3_2_forward(self):\n        nu = 3 / 2\n        batch_size = (3, 2, 4)\n        x1 = torch.randn(*batch_size, 7, 9)\n        x2 = torch.randn(*batch_size, 6, 9)\n        # Doesn\'t support ARD\n        lengthscale = torch.randn(*batch_size).view(*batch_size, 1, 1) ** 2\n\n        res = gpytorch.functions.MaternCovariance().apply(x1, x2, lengthscale, nu, dist_func)\n        scaled_unitless_dist = math.sqrt(nu * 2) * dist_func(x1, x2).div(lengthscale)\n        exp_component = torch.exp(-scaled_unitless_dist)\n        actual = exp_component * (1 + scaled_unitless_dist)\n        self.assertTrue(torch.allclose(res, actual))\n\n    def test_3_2_backward(self):\n        nu = 3 / 2\n        batch_size = (3, 2, 4)\n        x1 = torch.randn(*batch_size, 7, 9, dtype=torch.float64)\n        x2 = torch.randn(*batch_size, 6, 9, dtype=torch.float64)\n        lengthscale = torch.randn(*batch_size, dtype=torch.float64, requires_grad=True).view(*batch_size, 1, 1) ** 2\n        f = lambda x1, x2, l: gpytorch.functions.MaternCovariance().apply(x1, x2, l, nu, dist_func)\n        try:\n            torch.autograd.gradcheck(f, (x1, x2, lengthscale))\n        except RuntimeError:\n            self.fail(f""Gradcheck failed"")\n\n    def test_5_2_forward(self):\n        nu = 5 / 2\n        batch_size = (3, 2, 4)\n        x1 = torch.randn(*batch_size, 7, 9)\n        x2 = torch.randn(*batch_size, 6, 9)\n        # Doesn\'t support ARD\n        lengthscale = torch.randn(*batch_size).view(*batch_size, 1, 1) ** 2\n\n        res = gpytorch.functions.MaternCovariance().apply(x1, x2, lengthscale, nu, dist_func)\n        scaled_unitless_dist = math.sqrt(nu * 2) * dist_func(x1, x2).div(lengthscale)\n        exp_component = torch.exp(-scaled_unitless_dist)\n        actual = exp_component * (1 + scaled_unitless_dist + scaled_unitless_dist ** 2 / 3)\n        self.assertTrue(torch.allclose(res, actual))\n\n    def test_5_2_backward(self):\n        nu = 5 / 2\n        batch_size = (3, 2, 4)\n        x1 = torch.randn(*batch_size, 7, 9, dtype=torch.float64)\n        x2 = torch.randn(*batch_size, 6, 9, dtype=torch.float64)\n        lengthscale = torch.randn(*batch_size, dtype=torch.float64, requires_grad=True).view(*batch_size, 1, 1) ** 2\n        f = lambda x1, x2, l: gpytorch.functions.MaternCovariance().apply(x1, x2, l, nu, dist_func)\n        try:\n            torch.autograd.gradcheck(f, (x1, x2, lengthscale))\n        except RuntimeError:\n            self.fail(f""Gradcheck failed"")\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/functions/test_matmul.py,13,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import NonLazyTensor\nfrom gpytorch.test.utils import approx_equal\n\n\nclass TestMatmulNonBatch(unittest.TestCase):\n    def setUp(self):\n        mat = torch.tensor([[3, -1, 0], [-1, 3, 0], [0, 0, 3]], dtype=torch.float)\n        vec = torch.randn(3)\n        vecs = torch.randn(3, 4)\n        self.mat = mat.detach().clone().requires_grad_(True)\n        self.mat_copy = mat.detach().clone().requires_grad_(True)\n        self.vec = vec.detach().clone().requires_grad_(True)\n        self.vec_copy = vec.detach().clone().requires_grad_(True)\n        self.vecs = vecs.detach().clone().requires_grad_(True)\n        self.vecs_copy = vecs.detach().clone().requires_grad_(True)\n\n    def test_matmul_vec(self):\n        # Forward\n        res = NonLazyTensor(self.mat).matmul(self.vec)\n        actual = self.mat_copy.matmul(self.vec_copy)\n        self.assertTrue(approx_equal(res, actual))\n\n        # Backward\n        grad_output = torch.randn(3)\n        res.backward(gradient=grad_output)\n        actual.backward(gradient=grad_output)\n        self.assertTrue(approx_equal(self.mat_copy.grad, self.mat.grad))\n        self.assertTrue(approx_equal(self.vec_copy.grad, self.vec.grad))\n\n    def test_matmul_multiple_vecs(self):\n        # Forward\n        res = NonLazyTensor(self.mat).matmul(self.vecs)\n        actual = self.mat_copy.matmul(self.vecs_copy)\n        self.assertTrue(approx_equal(res, actual))\n\n        # Backward\n        grad_output = torch.randn(3, 4)\n        res.backward(gradient=grad_output)\n        actual.backward(gradient=grad_output)\n        self.assertTrue(approx_equal(self.mat_copy.grad, self.mat.grad))\n        self.assertTrue(approx_equal(self.vecs_copy.grad, self.vecs.grad))\n\n\nclass TestMatmulBatch(unittest.TestCase):\n    def setUp(self):\n        mats = torch.randn(2, 5, 3)\n        vecs = torch.randn(2, 3, 4)\n        self.mats = mats.detach().clone().requires_grad_(True)\n        self.mats_copy = mats.detach().clone().requires_grad_(True)\n        self.vecs = vecs.detach().clone().requires_grad_(True)\n        self.vecs_copy = vecs.detach().clone().requires_grad_(True)\n\n    def test_matmul_multiple_vecs(self):\n        # Forward\n        res = NonLazyTensor(self.mats).matmul(self.vecs)\n        actual = self.mats_copy.matmul(self.vecs_copy)\n        self.assertTrue(approx_equal(res, actual))\n\n        # Backward\n        grad_output = torch.randn(2, 5, 4)\n        res.backward(gradient=grad_output)\n        actual.backward(gradient=grad_output)\n        self.assertTrue(approx_equal(self.mats_copy.grad, self.mats.grad))\n        self.assertTrue(approx_equal(self.vecs_copy.grad, self.vecs.grad))\n\n\nclass TestMatmulMultiBatch(unittest.TestCase):\n    def setUp(self):\n        mats = torch.randn(3, 4, 5, 6)\n        vecs = torch.randn(3, 4, 6, 2)\n        self.mats = mats.detach().clone().requires_grad_(True)\n        self.mats_copy = mats.detach().clone().requires_grad_(True)\n        self.vecs = vecs.detach().clone().requires_grad_(True)\n        self.vecs_copy = vecs.detach().clone().requires_grad_(True)\n\n    def test_matmul_multiple_vecs(self):\n        # Forward\n        res = NonLazyTensor(self.mats).matmul(self.vecs)\n        actual = self.mats_copy.matmul(self.vecs_copy)\n        self.assertTrue(approx_equal(res, actual))\n\n        # Backward\n        grad_output = torch.randn(3, 4, 5, 2)\n        res.backward(gradient=grad_output)\n        actual.backward(gradient=grad_output)\n        self.assertTrue(approx_equal(self.mats_copy.grad, self.mats.grad))\n        self.assertTrue(approx_equal(self.vecs_copy.grad, self.vecs.grad))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/functions/test_rbf_covariance.py,12,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport gpytorch\n\n\ndef sq_dist_func(x1, x2):\n    dist_module = gpytorch.kernels.kernel.Distance()\n    return dist_module._sq_dist(x1, x2, postprocess=torch.tensor(False))\n\n\nclass TestRBFCovariance(unittest.TestCase):\n    def test_forward(self):\n        batch_size = (3, 2, 4)\n        x1 = torch.randn(*batch_size, 7, 9)\n        x2 = torch.randn(*batch_size, 6, 9)\n        # Doesn\'t support ARD\n        lengthscale = torch.randn(*batch_size).view(*batch_size, 1, 1) ** 2\n        res = gpytorch.functions.RBFCovariance().apply(x1, x2, lengthscale, sq_dist_func)\n        actual = sq_dist_func(x1, x2).div(-2 * lengthscale ** 2).exp()\n        self.assertTrue(torch.allclose(res, actual))\n\n    def test_backward(self):\n        batch_size = (3, 2, 4)\n        x1 = torch.randn(*batch_size, 7, 9, dtype=torch.float64)\n        x2 = torch.randn(*batch_size, 6, 9, dtype=torch.float64)\n        lengthscale = torch.randn(*batch_size, dtype=torch.float64, requires_grad=True).view(*batch_size, 1, 1) ** 2\n        f = lambda x1, x2, l: gpytorch.functions.RBFCovariance().apply(x1, x2, l, sq_dist_func)\n        try:\n            torch.autograd.gradcheck(f, (x1, x2, lengthscale))\n        except RuntimeError:\n            self.fail(""Gradcheck failed"")\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/functions/test_root_decomposition.py,10,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import NonLazyTensor\nfrom gpytorch.test.base_test_case import BaseTestCase\n\n\nclass TestRootDecomposition(BaseTestCase, unittest.TestCase):\n    seed = 0\n\n    def _create_mat(self):\n        mat = torch.randn(4, 4)\n        mat = mat @ mat.transpose(-1, -2)\n        mat.div_(5).add_(torch.eye(4))\n        return mat\n\n    def test_root_decomposition(self):\n        mat = self._create_mat().detach().requires_grad_(True)\n        mat_clone = mat.detach().clone().requires_grad_(True)\n\n        # Forward\n        root = NonLazyTensor(mat).root_decomposition().root.evaluate()\n        res = root.matmul(root.transpose(-1, -2))\n        self.assertAllClose(res, mat)\n\n        # Backward\n        sum([mat.trace() for mat in res.view(-1, mat.size(-2), mat.size(-1))]).backward()\n        sum([mat.trace() for mat in mat_clone.view(-1, mat.size(-2), mat.size(-1))]).backward()\n        self.assertAllClose(mat.grad, mat_clone.grad)\n\n    def test_root_inv_decomposition(self):\n        mat = self._create_mat().detach().requires_grad_(True)\n        mat_clone = mat.detach().clone().requires_grad_(True)\n\n        # Forward\n        probe_vectors = torch.randn(*mat.shape[:-2], 4, 5)\n        test_vectors = torch.randn(*mat.shape[:-2], 4, 5)\n        root = NonLazyTensor(mat).root_inv_decomposition(probe_vectors, test_vectors).root.evaluate()\n        res = root.matmul(root.transpose(-1, -2))\n        actual = mat_clone.inverse()\n        self.assertAllClose(res, actual)\n\n        # Backward\n        sum([mat.trace() for mat in res.view(-1, mat.size(-2), mat.size(-1))]).backward()\n        sum([mat.trace() for mat in actual.view(-1, mat.size(-2), mat.size(-1))]).backward()\n        self.assertAllClose(mat.grad, mat_clone.grad)\n\n\nclass TestRootDecompositionBatch(TestRootDecomposition):\n    seed = 0\n\n    def _create_mat(self):\n        mat = torch.randn(3, 4, 4)\n        mat = mat @ mat.transpose(-1, -2)\n        mat.div_(5).add_(torch.eye(4).unsqueeze_(0))\n        return mat\n\n\nclass TestRootDecompositionMultiBatch(TestRootDecomposition):\n    seed = 0\n\n    def _create_mat(self):\n        mat = torch.randn(2, 3, 4, 4)\n        mat = mat @ mat.transpose(-1, -2)\n        mat.div_(5).add_(torch.eye(4).unsqueeze_(0))\n        return mat\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/__init__.py,0,b'#!/usr/bin/env python3\n'
test/kernels/test_additive_kernel.py,47,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import AdditiveKernel, LinearKernel, ProductKernel, RBFKernel\n\n\nclass TestAdditiveKernel(unittest.TestCase):\n    def test_computes_product_of_radial_basis_function(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2], dtype=torch.float).view(2, 1)\n        lengthscale = 2\n\n        kernel_1 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_2 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel = kernel_1 * kernel_2\n\n        actual = torch.tensor([[16, 4], [4, 0], [64, 36]], dtype=torch.float)\n        actual = actual.mul_(-0.5).div_(lengthscale ** 2).exp() ** 2\n\n        kernel.eval()\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 2e-5)\n\n    def test_computes_sum_of_radial_basis_function(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2], dtype=torch.float).view(2, 1)\n        lengthscale = 2\n\n        kernel_1 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_2 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel = kernel_1 + kernel_2\n\n        actual = torch.tensor([[16, 4], [4, 0], [64, 36]], dtype=torch.float)\n        actual = actual.mul_(-0.5).div_(lengthscale ** 2).exp() * 2\n\n        kernel.eval()\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 2e-5)\n\n    def test_computes_sum_of_radial_basis_function_diag(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2, 2], dtype=torch.float).view(3, 1)\n\n        actual = torch.tensor([0.2702, 2.000, 0.0222])\n\n        lengthscale = 2.0\n\n        kernel_1 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_2 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel = kernel_1 + kernel_2\n        kernel.eval()\n\n        kernel.eval()\n        res = kernel(a, b, diag=True)\n        self.assertLess(torch.norm(res - actual), 1e-3)\n\n    def test_computes_sum_of_three_radial_basis_function_diag(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2, 2], dtype=torch.float).view(3, 1)\n\n        actual = torch.tensor([0.4060, 3.000, 0.0333])\n\n        lengthscale = 2.0\n\n        kernel_1 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_2 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_3 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel = kernel_1 + kernel_2 + kernel_3\n        kernel.eval()\n\n        res = kernel(a, b, diag=True)\n        self.assertLess(torch.norm(res - actual), 1e-3)\n\n    def test_computes_product_of_radial_basis_function_diag(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2, 2], dtype=torch.float).view(3, 1)\n\n        actual = torch.tensor([2.4788e-03, 1.000, 1.3710e-06])\n\n        lengthscale = 2.0\n\n        kernel_1 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_2 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_3 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel = kernel_1 * kernel_2 * kernel_3\n        kernel.eval()\n\n        kernel.eval()\n        res = kernel(a, b, diag=True)\n        self.assertLess(torch.norm(res - actual), 1e-3)\n\n    def test_computes_product_of_three_radial_basis_function_diag(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2, 2], dtype=torch.float).view(3, 1)\n\n        actual = torch.tensor([1.8316e-02, 1.000, 1.2341e-04])\n\n        lengthscale = 2.0\n\n        kernel_1 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_2 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel = kernel_1 * kernel_2\n        kernel.eval()\n\n        kernel.eval()\n        res = kernel(a, b, diag=True)\n        self.assertLess(torch.norm(res - actual), 1e-3)\n\n    def test_computes_product_of_three_radial_basis_function(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2], dtype=torch.float).view(2, 1)\n        lengthscale = 2\n\n        kernel_1 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_2 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_3 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel = ProductKernel(kernel_1, kernel_2, kernel_3)\n\n        actual = torch.tensor([[16, 4], [4, 0], [64, 36]], dtype=torch.float)\n        actual = actual.mul_(-0.5).div_(lengthscale ** 2).exp() ** 3\n\n        kernel.eval()\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 2e-5)\n\n    def test_computes_sum_of_three_radial_basis_function(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2], dtype=torch.float).view(2, 1)\n        lengthscale = 2\n\n        kernel_1 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_2 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_3 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel = AdditiveKernel(kernel_1, kernel_2, kernel_3)\n\n        actual = (\n            torch.tensor([[16, 4], [4, 0], [64, 36]], dtype=torch.float).mul_(-0.5).div_(lengthscale ** 2).exp() * 3\n        )\n\n        kernel.eval()\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 2e-5)\n\n    def test_computes_sum_radial_basis_function_gradient(self):\n        softplus = torch.nn.functional.softplus\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2, 2], dtype=torch.float).view(3, 1)\n        lengthscale = 2\n\n        param = math.log(math.exp(lengthscale) - 1) * torch.ones(3, 3)\n        param.requires_grad_()\n        diffs = a.expand(3, 3) - b.expand(3, 3).transpose(0, 1)\n        actual_output = (-0.5 * (diffs / softplus(param)) ** 2).exp()\n        actual_output.backward(torch.eye(3))\n        actual_param_grad = param.grad.sum() * 2\n\n        kernel_1 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_2 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel = kernel_1 + kernel_2\n        kernel.eval()\n\n        output = kernel(a, b).evaluate()\n        output.backward(gradient=torch.eye(3))\n        res = kernel.kernels[0].raw_lengthscale.grad + kernel.kernels[1].raw_lengthscale.grad\n        self.assertLess(torch.norm(res - actual_param_grad), 2e-5)\n\n    def test_computes_sum_three_radial_basis_function_gradient(self):\n        softplus = torch.nn.functional.softplus\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2, 2], dtype=torch.float).view(3, 1)\n        lengthscale = 2\n\n        param = math.log(math.exp(lengthscale) - 1) * torch.ones(3, 3)\n        param.requires_grad_()\n        diffs = a.expand(3, 3) - b.expand(3, 3).transpose(0, 1)\n        actual_output = (-0.5 * (diffs / softplus(param)) ** 2).exp()\n        actual_output.backward(torch.eye(3))\n        actual_param_grad = param.grad.sum() * 3\n\n        kernel_1 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_2 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel_3 = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel = AdditiveKernel(kernel_1, kernel_2, kernel_3)\n        kernel.eval()\n\n        output = kernel(a, b).evaluate()\n        output.backward(gradient=torch.eye(3))\n        res = (\n            kernel.kernels[0].raw_lengthscale.grad\n            + kernel.kernels[1].raw_lengthscale.grad\n            + kernel.kernels[2].raw_lengthscale.grad\n        )\n        self.assertLess(torch.norm(res - actual_param_grad), 2e-5)\n\n    def test_is_stationary(self):\n        kernel_1 = RBFKernel().initialize(lengthscale=1)\n        kernel_2 = RBFKernel().initialize(lengthscale=2)\n        kernel_3 = LinearKernel().initialize()\n\n        self.assertTrue((kernel_1 + kernel_2).is_stationary)\n        self.assertTrue((kernel_1 * kernel_2).is_stationary)\n        self.assertFalse((kernel_1 + kernel_3).is_stationary)\n        self.assertFalse((kernel_1 * kernel_3).is_stationary)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_arc_kernel.py,2,"b'#!/usr/bin/env python3\n\nimport unittest\n\nfrom gpytorch.kernels import ArcKernel, MaternKernel\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\n\nclass TestArcKernel(unittest.TestCase, BaseKernelTestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        return ArcKernel(base_kernel=MaternKernel(nu=0.5), **kwargs)\n\n    def create_kernel_ard(self, num_dims, **kwargs):\n        return ArcKernel(base_kernel=MaternKernel(nu=0.5), ard_num_dims=num_dims, **kwargs)\n'"
test/kernels/test_cosine_kernel.py,31,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import CosineKernel\n\n\nclass TestCosineKernel(unittest.TestCase):\n    def test_computes_periodic_function(self):\n        a = torch.tensor([[4, 1], [2, 2], [8, 0]], dtype=torch.float)\n        b = torch.tensor([[0, 0], [2, 1], [1, 0]], dtype=torch.float)\n        period = 1\n        kernel = CosineKernel().initialize(period_length=period)\n        kernel.eval()\n\n        actual = torch.zeros(3, 3)\n        for i in range(3):\n            for j in range(3):\n                actual[i, j] = torch.cos(math.pi * ((a[i] - b[j]) / period).norm(2, dim=-1))\n\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims\n        actual = torch.zeros(2, 3, 3)\n        for i in range(3):\n            for j in range(3):\n                for l in range(2):\n                    actual[l, i, j] = torch.cos(math.pi * ((a[i, l] - b[j, l]) / period))\n        res = kernel(a, b, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims + diag\n        res = kernel(a, b, last_dim_is_batch=True).diag()\n        actual = torch.cat([actual[i].diag().unsqueeze(0) for i in range(actual.size(0))])\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_batch(self):\n        a = torch.tensor([[4, 2, 8], [1, 2, 3]], dtype=torch.float).view(2, 3, 1)\n        b = torch.tensor([[0, 2, 1], [-1, 2, 0]], dtype=torch.float).view(2, 3, 1)\n        period = torch.tensor(1, dtype=torch.float).view(1, 1, 1)\n        kernel = CosineKernel().initialize(period_length=period)\n        kernel.eval()\n\n        actual = torch.zeros(2, 3, 3)\n        for k in range(2):\n            for i in range(3):\n                for j in range(3):\n                    actual[k, i, j] = torch.cos(math.pi * ((a[k, i] - b[k, j]) / period))\n\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_batch_separate(self):\n        a = torch.tensor([[[4, 1], [2, 2], [8, 0]], [[2, 5], [6, 1], [0, 1]]], dtype=torch.float)\n        b = torch.tensor([[[0, 0], [2, 1], [1, 0]], [[1, 1], [2, 3], [1, 0]]], dtype=torch.float)\n        period = torch.tensor([1, 2], dtype=torch.float).view(2, 1, 1)\n        kernel = CosineKernel(batch_shape=torch.Size([2])).initialize(period_length=period)\n        kernel.eval()\n\n        actual = torch.zeros(2, 3, 3)\n        for k in range(2):\n            for i in range(3):\n                for j in range(3):\n                    actual[k, i, j] = torch.cos(math.pi * ((a[k, i] - b[k, j]) / period[k]).norm(2, dim=-1))\n\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = torch.cat([actual[i].diag().unsqueeze(0) for i in range(actual.size(0))])\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims\n        actual = torch.zeros(2, 2, 3, 3)\n        for k in range(2):\n            for i in range(3):\n                for j in range(3):\n                    for l in range(2):\n                        actual[k, l, i, j] = torch.cos(math.pi * ((a[k, i, l] - b[k, j, l]) / period[k]))\n        res = kernel(a, b, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims + diag\n        res = kernel(a, b, last_dim_is_batch=True).diag()\n        actual = actual.diagonal(dim1=-2, dim2=-1)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_cylindrical_kernel.py,5,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import CylindricalKernel, MaternKernel\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\n\nclass TestCylindricalKernel(unittest.TestCase, BaseKernelTestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        return CylindricalKernel(5, MaternKernel(nu=2.5), **kwargs)\n\n    def create_data_no_batch(self):\n        return torch.rand(50, 10) / math.sqrt(10)\n\n    def create_data_single_batch(self):\n        return torch.rand(2, 50, 2) / math.sqrt(2)\n\n    def create_data_double_batch(self):\n        return torch.rand(3, 2, 50, 2) / math.sqrt(2)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_grid_interpolation_kernel.py,14,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import GridInterpolationKernel, RBFKernel\nfrom gpytorch.lazy import InterpolatedLazyTensor\n\n\nclass TestGridInterpolationKernel(unittest.TestCase):\n    def test_standard(self):\n        base_kernel = RBFKernel()\n        kernel = GridInterpolationKernel(base_kernel, num_dims=2, grid_size=128, grid_bounds=[(-1.2, 1.2)] * 2)\n\n        xs = torch.randn(5, 2).clamp(-1, 1)\n        interp_covar = kernel(xs, xs).evaluate_kernel()\n        self.assertIsInstance(interp_covar, InterpolatedLazyTensor)\n\n        xs = torch.randn(5, 2).clamp(-1, 1)\n        grid_eval = kernel(xs, xs).evaluate()\n        actual_eval = base_kernel(xs, xs).evaluate()\n        self.assertLess(torch.norm(grid_eval - actual_eval), 2e-5)\n\n        xs = torch.randn(3, 5, 2).clamp(-1, 1)\n        grid_eval = kernel(xs, xs).evaluate()\n        actual_eval = base_kernel(xs, xs).evaluate()\n        self.assertLess(torch.norm(grid_eval - actual_eval), 2e-5)\n\n    def test_batch_base_kernel(self):\n        base_kernel = RBFKernel(batch_shape=torch.Size([3]))\n        kernel = GridInterpolationKernel(base_kernel, num_dims=2, grid_size=128, grid_bounds=[(-1.2, 1.2)] * 2)\n\n        xs = torch.randn(5, 2).clamp(-1, 1)\n        grid_eval = kernel(xs, xs).evaluate()\n        actual_eval = base_kernel(xs, xs).evaluate()\n        self.assertLess(torch.norm(grid_eval - actual_eval), 2e-5)\n\n        xs = torch.randn(3, 5, 2).clamp(-1, 1)\n        grid_eval = kernel(xs, xs).evaluate()\n        actual_eval = base_kernel(xs, xs).evaluate()\n        self.assertLess(torch.norm(grid_eval - actual_eval), 2e-5)\n\n        xs = torch.randn(4, 3, 5, 2).clamp(-1, 1)\n        grid_eval = kernel(xs, xs).evaluate()\n        actual_eval = base_kernel(xs, xs).evaluate()\n        self.assertLess(torch.norm(grid_eval - actual_eval), 2e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_grid_kernel.py,9,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import GridKernel, LinearKernel, RBFKernel\nfrom gpytorch.lazy import KroneckerProductLazyTensor\nfrom gpytorch.utils.grid import create_data_from_grid\n\ngrid = [torch.linspace(0, 1, 5), torch.linspace(0, 2, 3)]\nd = len(grid)\ngrid_data = create_data_from_grid(grid)\n\n\nclass TestGridKernel(unittest.TestCase):\n    def test_grid_grid(self):\n        base_kernel = RBFKernel()\n        kernel = GridKernel(base_kernel, grid)\n        grid_covar = kernel(grid_data, grid_data).evaluate_kernel()\n        self.assertIsInstance(grid_covar, KroneckerProductLazyTensor)\n        grid_eval = kernel(grid_data, grid_data).evaluate()\n        actual_eval = base_kernel(grid_data, grid_data).evaluate()\n        self.assertLess(torch.norm(grid_eval - actual_eval), 2e-5)\n\n    def test_nongrid_grid(self):\n        base_kernel = RBFKernel()\n        data = torch.randn(5, d)\n        kernel = GridKernel(base_kernel, grid)\n        grid_eval = kernel(grid_data, data).evaluate()\n        actual_eval = base_kernel(grid_data, data).evaluate()\n        self.assertLess(torch.norm(grid_eval - actual_eval), 1e-5)\n\n    def test_nongrid_nongrid(self):\n        base_kernel = RBFKernel()\n        data = torch.randn(5, d)\n        kernel = GridKernel(base_kernel, grid)\n        grid_eval = kernel(data, data).evaluate()\n        actual_eval = base_kernel(data, data).evaluate()\n        self.assertLess(torch.norm(grid_eval - actual_eval), 1e-5)\n\n    def test_non_stationary_base(self):\n        base_kernel = LinearKernel()\n        with self.assertRaisesRegex(RuntimeError, ""The base_kernel for GridKernel must be stationary.""):\n            GridKernel(base_kernel, grid)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_linear_kernel.py,22,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import LinearKernel\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\n\nclass TestLinearKernel(unittest.TestCase, BaseKernelTestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        return LinearKernel(**kwargs)\n\n    def test_computes_linear_function_rectangular(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2, 1], dtype=torch.float).view(3, 1)\n\n        kernel = LinearKernel().initialize(variance=1.0)\n        kernel.eval()\n        actual = torch.matmul(a, b.t())\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-4)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-4)\n\n    def test_computes_linear_function_square(self):\n        a = torch.tensor([[4, 1], [2, 0], [8, 3]], dtype=torch.float)\n\n        kernel = LinearKernel().initialize(variance=3.14)\n        kernel.eval()\n        actual = torch.matmul(a, a.t()) * 3.14\n        res = kernel(a, a).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-4)\n\n        # diag\n        res = kernel(a, a).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-4)\n\n        # batch_dims\n        dim_group_a = a\n        dim_group_a = dim_group_a.permute(1, 0).reshape(-1, 3)\n        actual = 3.14 * torch.mul(dim_group_a.unsqueeze(-1), dim_group_a.unsqueeze(-2))\n        res = kernel(a, a, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-4)\n\n        # batch_dims + diag\n        res = kernel(a, a, last_dim_is_batch=True).diag()\n        actual = torch.cat([actual[i].diag().unsqueeze(0) for i in range(actual.size(0))])\n        self.assertLess(torch.norm(res - actual), 1e-4)\n\n    def test_computes_linear_function_square_batch(self):\n        a = torch.tensor([[[4, 1], [2, 0], [8, 3]], [[1, 1], [2, 1], [1, 3]]], dtype=torch.float)\n\n        kernel = LinearKernel().initialize(variance=1.0)\n        kernel.eval()\n        actual = torch.matmul(a, a.transpose(-1, -2))\n        res = kernel(a, a).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-4)\n\n        # diag\n        res = kernel(a, a).diag()\n        actual = torch.cat([actual[i].diag().unsqueeze(0) for i in range(actual.size(0))])\n        self.assertLess(torch.norm(res - actual), 1e-4)\n\n        # batch_dims\n        dim_group_a = a\n        dim_group_a = dim_group_a.transpose(-1, -2).unsqueeze(-1)\n        actual = dim_group_a.matmul(dim_group_a.transpose(-2, -1))\n        res = kernel(a, a, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-4)\n\n        # batch_dims + diag\n        res = kernel(a, a, last_dim_is_batch=True).diag()\n        actual = actual.diagonal(dim1=-2, dim2=-1)\n        self.assertLess(torch.norm(res - actual), 1e-4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_matern_kernel.py,48,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import MaternKernel\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\n\nclass TestMatern25BaseKernel(unittest.TestCase, BaseKernelTestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        return MaternKernel(nu=2.5, **kwargs)\n\n    def create_kernel_ard(self, num_dims, **kwargs):\n        return MaternKernel(nu=2.5, ard_num_dims=num_dims, **kwargs)\n\n\nclass TestMatern05BaseKernel(unittest.TestCase, BaseKernelTestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        return MaternKernel(nu=0.5, **kwargs)\n\n    def create_kernel_ard(self, num_dims, **kwargs):\n        return MaternKernel(nu=0.5, ard_num_dims=num_dims, **kwargs)\n\n\nclass TestMaternKernel(unittest.TestCase, BaseKernelTestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        return MaternKernel(nu=1.5, **kwargs)\n\n    def create_kernel_ard(self, num_dims, **kwargs):\n        return MaternKernel(nu=1.5, ard_num_dims=num_dims, **kwargs)\n\n    def test_forward_nu_1_over_2(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2], dtype=torch.float).view(2, 1)\n        lengthscale = 2\n\n        kernel = MaternKernel(nu=0.5).initialize(lengthscale=lengthscale)\n        kernel.eval()\n\n        actual = torch.tensor([[4, 2], [2, 0], [8, 6]], dtype=torch.float).div_(-lengthscale).exp()\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-3)\n\n    def test_forward_nu_3_over_2(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2], dtype=torch.float).view(2, 1)\n        lengthscale = 2\n\n        kernel = MaternKernel(nu=1.5).initialize(lengthscale=lengthscale)\n        kernel.eval()\n\n        dist = torch.tensor([[4, 2], [2, 0], [8, 6]], dtype=torch.float).mul_(math.sqrt(3) / lengthscale)\n        actual = (dist + 1).mul(torch.exp(-dist))\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-3)\n\n    def test_forward_nu_5_over_2(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2], dtype=torch.float).view(2, 1)\n        lengthscale = 2\n\n        kernel = MaternKernel(nu=2.5).initialize(lengthscale=lengthscale)\n        kernel.eval()\n\n        dist = torch.tensor([[4, 2], [2, 0], [8, 6]], dtype=torch.float).mul_(math.sqrt(5) / lengthscale)\n        actual = (dist ** 2 / 3 + dist + 1).mul(torch.exp(-dist))\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-3)\n\n    def test_ard(self):\n        a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float)\n        b = torch.tensor([[1, 4], [1, 4]], dtype=torch.float)\n        lengthscales = torch.tensor([1, 2], dtype=torch.float).view(1, 1, 2)\n\n        kernel = MaternKernel(nu=2.5, ard_num_dims=2)\n        kernel.initialize(lengthscale=lengthscales)\n        kernel.eval()\n\n        dist = torch.tensor([[1, 1], [2, 2]], dtype=torch.float)\n        dist.mul_(math.sqrt(5))\n        actual = (dist ** 2 / 3 + dist + 1).mul(torch.exp(-dist))\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-3)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims\n        dist = torch.tensor([[[0, 0], [2, 2]], [[1, 1], [0, 0]]], dtype=torch.float)\n        dist.mul_(math.sqrt(5))\n        actual = (dist ** 2 / 3 + dist + 1).mul(torch.exp(-dist))\n        res = kernel(a, b, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims + diag\n        res = kernel(a, b, last_dim_is_batch=True).diag()\n        actual = torch.cat([actual[i].diag().unsqueeze(0) for i in range(actual.size(0))])\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_ard_batch(self):\n        a = torch.tensor([[[1, 2, 3], [2, 4, 3]], [[2, -1, 2], [2, -1, 0]]], dtype=torch.float)\n        b = torch.tensor([[[1, 4, 3]], [[2, -1, 0]]], dtype=torch.float)\n        lengthscales = torch.tensor([[[1, 2, 1]]], dtype=torch.float)\n\n        kernel = MaternKernel(nu=2.5, batch_shape=torch.Size([2]), ard_num_dims=3)\n        kernel.initialize(lengthscale=lengthscales)\n        kernel.eval()\n\n        dist = torch.tensor([[[1], [1]], [[2], [0]]], dtype=torch.float).mul_(math.sqrt(5))\n        actual = (dist ** 2 / 3 + dist + 1).mul(torch.exp(-dist))\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-3)\n\n    def test_ard_separate_batch(self):\n        a = torch.tensor([[[1, 2, 3], [2, 4, 3]], [[2, -1, 2], [2, -1, 0]]], dtype=torch.float)\n        b = torch.tensor([[[1, 4, 3]], [[2, -1, 0]]], dtype=torch.float).repeat(1, 2, 1)\n        lengthscales = torch.tensor([[[1, 2, 1]], [[2, 1, 0.5]]], dtype=torch.float)\n\n        kernel = MaternKernel(nu=2.5, batch_shape=torch.Size([2]), ard_num_dims=3)\n        kernel.initialize(lengthscale=lengthscales)\n        kernel.eval()\n\n        dist = torch.tensor([[[1, 1], [1, 1]], [[4, 4], [0, 0]]], dtype=torch.float).mul_(math.sqrt(5))\n        actual = (dist ** 2 / 3 + dist + 1).mul(torch.exp(-dist))\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-3)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = torch.cat([actual[i].diag().unsqueeze(0) for i in range(actual.size(0))])\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims\n        dist = torch.tensor(\n            [\n                [[[0.0, 0.0], [1.0, 1.0]], [[0.0, 0.0], [0.0, 0.0]]],\n                [[[1.0, 1.0], [0.0, 0.0]], [[0.0, 0.0], [0.0, 0.0]]],\n                [[[0.0, 0.0], [0.0, 0.0]], [[4.0, 4.0], [0.0, 0.0]]],\n            ]\n        )\n\n        dist.mul_(math.sqrt(5))\n        dist = dist.view(3, 2, 2, 2).transpose(0, 1)\n        actual = (dist ** 2 / 3 + dist + 1).mul(torch.exp(-dist))\n        res = kernel(a, b, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims + diag\n        res = kernel(a, b, last_dim_is_batch=True).diag()\n        actual = actual.diagonal(dim1=-2, dim2=-1)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_newton_girard_additive_kernel.py,21,"b""#!/usr/bin/env python3\n\nfrom unittest import TestCase\n\nimport torch\n\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.kernels import AdditiveKernel, NewtonGirardAdditiveKernel, RBFKernel, ScaleKernel\nfrom gpytorch.likelihoods import GaussianLikelihood\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.mlls import ExactMarginalLogLikelihood\nfrom gpytorch.models import ExactGP\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\n\nclass TestNewtonGirardAdditiveKernel(TestCase, BaseKernelTestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        return NewtonGirardAdditiveKernel(RBFKernel(), 4, 2, **kwargs)\n\n    def create_kernel_ard(self, num_dims, **kwargs):\n        return NewtonGirardAdditiveKernel(RBFKernel(ard_num_dims=num_dims), num_dims, 2, **kwargs)\n\n    def test_degree1(self):\n        AddK = NewtonGirardAdditiveKernel(RBFKernel(ard_num_dims=3), 3, 1)\n        self.assertEqual(AddK.base_kernel.lengthscale.numel(), 3)\n        self.assertEqual(AddK.outputscale.numel(), 1)\n\n        testvals = torch.tensor([[1, 2, 3], [7, 5, 2]], dtype=torch.float)\n        add_k_val = AddK(testvals, testvals).evaluate()\n\n        manual_k = ScaleKernel(\n            AdditiveKernel(RBFKernel(active_dims=0), RBFKernel(active_dims=1), RBFKernel(active_dims=2))\n        )\n        manual_k.initialize(outputscale=1.0)\n        manual_add_k_val = manual_k(testvals, testvals).evaluate()\n\n        # np.testing.assert_allclose(add_k_val.detach().numpy(), manual_add_k_val.detach().numpy(), atol=1e-5)\n        self.assertTrue(torch.allclose(add_k_val, manual_add_k_val, atol=1e-5))\n\n    def test_degree2(self):\n        AddK = NewtonGirardAdditiveKernel(RBFKernel(ard_num_dims=3), 3, 2)\n        self.assertEqual(AddK.base_kernel.lengthscale.numel(), 3)\n        self.assertEqual(AddK.outputscale.numel(), 2)\n\n        testvals = torch.tensor([[1, 2, 3], [7, 5, 2]], dtype=torch.float)\n        add_k_val = AddK(testvals, testvals).evaluate()\n\n        manual_k1 = ScaleKernel(\n            AdditiveKernel(RBFKernel(active_dims=0), RBFKernel(active_dims=1), RBFKernel(active_dims=2))\n        )\n        manual_k1.initialize(outputscale=1 / 2)\n        manual_k2 = ScaleKernel(\n            AdditiveKernel(RBFKernel(active_dims=[0, 1]), RBFKernel(active_dims=[1, 2]), RBFKernel(active_dims=[0, 2]))\n        )\n        manual_k2.initialize(outputscale=1 / 2)\n        manual_k = AdditiveKernel(manual_k1, manual_k2)\n        manual_add_k_val = manual_k(testvals, testvals).evaluate()\n\n        # np.testing.assert_allclose(add_k_val.detach().numpy(), manual_add_k_val.detach().numpy(), atol=1e-5)\n        self.assertTrue(torch.allclose(add_k_val, manual_add_k_val, atol=1e-5))\n\n    def test_degree3(self):\n        # just make sure it doesn't break here.\n        AddK = NewtonGirardAdditiveKernel(RBFKernel(ard_num_dims=3), 3, 3)\n        self.assertEqual(AddK.base_kernel.lengthscale.numel(), 3)\n        self.assertEqual(AddK.outputscale.numel(), 3)\n\n        testvals = torch.tensor([[1, 2, 3], [7, 5, 2]], dtype=torch.float)\n        add_k_val = AddK(testvals, testvals).evaluate()\n\n        manual_k1 = ScaleKernel(\n            AdditiveKernel(RBFKernel(active_dims=0), RBFKernel(active_dims=1), RBFKernel(active_dims=2))\n        )\n        manual_k1.initialize(outputscale=1 / 3)\n        manual_k2 = ScaleKernel(\n            AdditiveKernel(RBFKernel(active_dims=[0, 1]), RBFKernel(active_dims=[1, 2]), RBFKernel(active_dims=[0, 2]))\n        )\n        manual_k2.initialize(outputscale=1 / 3)\n\n        manual_k3 = ScaleKernel(AdditiveKernel(RBFKernel()))\n        manual_k3.initialize(outputscale=1 / 3)\n        manual_k = AdditiveKernel(manual_k1, manual_k2, manual_k3)\n        manual_add_k_val = manual_k(testvals, testvals).evaluate()\n        # np.testing.assert_allclose(add_k_val.detach().numpy(), manual_add_k_val.detach().numpy(), atol=1e-5)\n        self.assertTrue(torch.allclose(add_k_val, manual_add_k_val, atol=1e-5))\n\n    def test_optimizing(self):\n        # This tests should pass so long as nothing breaks.\n        torch.random.manual_seed(1)\n        data = torch.randn(40, 4)\n        target = torch.sin(data).sum(dim=-1)\n        d = 4\n\n        AddK = NewtonGirardAdditiveKernel(RBFKernel(ard_num_dims=d), d, max_degree=3)\n\n        class TestGPModel(ExactGP):\n            def __init__(self, train_x, train_y, likelihood, kernel):\n                super().__init__(train_x, train_y, likelihood)\n                self.mean_module = ConstantMean()\n                self.covar_module = kernel\n\n            def forward(self, x):\n                mean_x = self.mean_module(x)\n                covar_x = self.covar_module(x)\n                return MultivariateNormal(mean_x, covar_x)\n\n        model = TestGPModel(data, target, GaussianLikelihood(), ScaleKernel(AddK))\n        optim = torch.optim.Adam(model.parameters(), lr=0.1)\n        mll = ExactMarginalLogLikelihood(model.likelihood, model)\n        model.train()\n        for i in range(2):\n            optim.zero_grad()\n            out = model(data)\n            loss = -mll(out, target)\n            loss.backward()\n            optim.step()\n\n    def test_ard(self):\n        base_k = RBFKernel(ard_num_dims=3)\n        base_k.initialize(lengthscale=[1.0, 2.0, 3.0])\n        AddK = NewtonGirardAdditiveKernel(base_k, 3, max_degree=1)\n\n        testvals = torch.tensor([[1, 2, 3], [7, 5, 2]], dtype=torch.float)\n        add_k_val = AddK(testvals, testvals).evaluate()\n\n        ks = []\n        for i in range(3):\n            k = RBFKernel(active_dims=i)\n            k.initialize(lengthscale=i + 1)\n            ks.append(k)\n        manual_k = ScaleKernel(AdditiveKernel(*ks))\n        manual_k.initialize(outputscale=1.0)\n        manual_add_k_val = manual_k(testvals, testvals).evaluate()\n\n        # np.testing.assert_allclose(add_k_val.detach().numpy(), manual_add_k_val.detach().numpy(), atol=1e-5)\n        self.assertTrue(torch.allclose(add_k_val, manual_add_k_val, atol=1e-5))\n\n    def test_diag(self):\n        AddK = NewtonGirardAdditiveKernel(RBFKernel(ard_num_dims=3), 3, 2)\n        self.assertEqual(AddK.base_kernel.lengthscale.numel(), 3)\n        self.assertEqual(AddK.outputscale.numel(), 2)\n\n        testvals = torch.tensor([[1, 2, 3], [7, 5, 2]], dtype=torch.float)\n        add_k_val = AddK(testvals, testvals).diag()\n\n        manual_k1 = ScaleKernel(\n            AdditiveKernel(RBFKernel(active_dims=0), RBFKernel(active_dims=1), RBFKernel(active_dims=2))\n        )\n        manual_k1.initialize(outputscale=1 / 2)\n        manual_k2 = ScaleKernel(\n            AdditiveKernel(RBFKernel(active_dims=[0, 1]), RBFKernel(active_dims=[1, 2]), RBFKernel(active_dims=[0, 2]))\n        )\n        manual_k2.initialize(outputscale=1 / 2)\n        manual_k = AdditiveKernel(manual_k1, manual_k2)\n        manual_add_k_val = manual_k(testvals, testvals).diag()\n\n        # np.testing.assert_allclose(add_k_val.detach().numpy(), manual_add_k_val.detach().numpy(), atol=1e-5)\n        self.assertTrue(torch.allclose(add_k_val, manual_add_k_val, atol=1e-5))\n"""
test/kernels/test_periodic_kernel.py,24,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import PeriodicKernel\n\n\nclass TestPeriodicKernel(unittest.TestCase):\n    def test_computes_periodic_function(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2], dtype=torch.float).view(2, 1)\n        lengthscale = 2\n        period = 3\n        kernel = PeriodicKernel().initialize(lengthscale=lengthscale, period_length=period)\n        kernel.eval()\n\n        actual = torch.zeros(3, 2)\n        for i in range(3):\n            for j in range(2):\n                val = 2 * torch.pow(torch.sin(math.pi * (a[i] - b[j]) / 3), 2) / lengthscale\n                actual[i, j] = torch.exp(-val).item()\n\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_batch(self):\n        a = torch.tensor([[4, 2, 8], [1, 2, 3]], dtype=torch.float).view(2, 3, 1)\n        b = torch.tensor([[0, 2], [-1, 2]], dtype=torch.float).view(2, 2, 1)\n        period = torch.tensor(1, dtype=torch.float).view(1, 1, 1)\n        lengthscale = torch.tensor(2, dtype=torch.float).view(1, 1, 1)\n        kernel = PeriodicKernel().initialize(lengthscale=lengthscale, period_length=period)\n        kernel.eval()\n\n        actual = torch.zeros(2, 3, 2)\n        for k in range(2):\n            for i in range(3):\n                for j in range(2):\n                    val = 2 * torch.pow(torch.sin(math.pi * (a[k, i] - b[k, j]) / period), 2) / lengthscale\n                    actual[k, i, j] = torch.exp(-val).item()\n\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_batch_separate(self):\n        a = torch.tensor([[4, 2, 8], [1, 2, 3]], dtype=torch.float).view(2, 3, 1)\n        b = torch.tensor([[0, 2], [-1, 2]], dtype=torch.float).view(2, 2, 1)\n        period = torch.tensor([1, 2], dtype=torch.float).view(2, 1, 1)\n        lengthscale = torch.tensor([2, 1], dtype=torch.float).view(2, 1, 1)\n        kernel = PeriodicKernel(batch_shape=torch.Size([2])).initialize(lengthscale=lengthscale, period_length=period)\n        kernel.eval()\n\n        actual = torch.zeros(2, 3, 2)\n        for k in range(2):\n            for i in range(3):\n                for j in range(2):\n                    val = 2 * torch.pow(torch.sin(math.pi * (a[k, i] - b[k, j]) / period[k]), 2) / lengthscale[k]\n                    actual[k, i, j] = torch.exp(-val).item()\n\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_polynomial_kernel.py,30,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import PolynomialKernel\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\n\nclass TestPolynomialKernel(unittest.TestCase, BaseKernelTestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        return PolynomialKernel(power=2, **kwargs)\n\n    def test_computes_quadratic_kernel(self):\n        a = torch.tensor([[4, 1], [2, 2], [8, 0]], dtype=torch.float)\n        b = torch.tensor([[0, 0], [2, 1], [1, 0]], dtype=torch.float)\n        kernel = PolynomialKernel(power=2)\n        kernel.eval()\n\n        actual = torch.zeros(3, 3)\n        for i in range(3):\n            for j in range(3):\n                actual[i, j] = (a[i].matmul(b[j]) + kernel.offset).pow(kernel.power)\n\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims\n        actual = torch.zeros(2, 3, 3)\n        for l in range(2):\n            actual[l] = kernel(a[:, l].unsqueeze(-1), b[:, l].unsqueeze(-1)).evaluate()\n\n        res = kernel(a, b, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims + diag\n        res = kernel(a, b, last_dim_is_batch=True).diag()\n        actual = torch.cat([actual[i].diag().unsqueeze(0) for i in range(actual.size(0))])\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_computes_cubic_kernel(self):\n        a = torch.tensor([[4, 1], [2, 2], [8, 0]], dtype=torch.float)\n        b = torch.tensor([[0, 0], [2, 1], [1, 0]], dtype=torch.float)\n        kernel = PolynomialKernel(power=3)\n        kernel.eval()\n\n        actual = torch.zeros(3, 3)\n        for i in range(3):\n            for j in range(3):\n                actual[i, j] = (a[i].matmul(b[j]) + kernel.offset).pow(kernel.power)\n\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims\n        actual = torch.zeros(2, 3, 3)\n        for l in range(2):\n            actual[l] = kernel(a[:, l].unsqueeze(-1), b[:, l].unsqueeze(-1)).evaluate()\n\n        res = kernel(a, b, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims + diag\n        res = kernel(a, b, last_dim_is_batch=True).diag()\n        actual = torch.cat([actual[i].diag().unsqueeze(0) for i in range(actual.size(0))])\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_quadratic_kernel_batch(self):\n        a = torch.tensor([[4, 2, 8], [1, 2, 3]], dtype=torch.float).view(2, 3, 1)\n        b = torch.tensor([[0, 2, 1], [-1, 2, 0]], dtype=torch.float).view(2, 3, 1)\n        kernel = PolynomialKernel(power=2, batch_shape=torch.Size([2])).initialize(offset=torch.rand(2, 1))\n        kernel.eval()\n\n        actual = torch.zeros(2, 3, 3)\n        for k in range(2):\n            for i in range(3):\n                for j in range(3):\n                    actual[k, i, j] = (a[k, i].matmul(b[k, j]) + kernel.offset[k]).pow(kernel.power)\n\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_cubic_kernel_batch(self):\n        a = torch.tensor([[4, 2, 8], [1, 2, 3]], dtype=torch.float).view(2, 3, 1)\n        b = torch.tensor([[0, 2, 1], [-1, 2, 0]], dtype=torch.float).view(2, 3, 1)\n        kernel = PolynomialKernel(power=3, batch_shape=torch.Size([2])).initialize(offset=torch.rand(2, 1))\n        kernel.eval()\n\n        actual = torch.zeros(2, 3, 3)\n        for k in range(2):\n            for i in range(3):\n                for j in range(3):\n                    actual[k, i, j] = (a[k, i].matmul(b[k, j]) + kernel.offset[k]).pow(kernel.power)\n\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_polynomial_kernel_grad.py,2,"b'#!/usr/bin/env python3\n\nimport unittest\n\nfrom gpytorch.kernels import PolynomialKernelGrad\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\n\nclass TestPolynomialKernel(unittest.TestCase, BaseKernelTestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        return PolynomialKernelGrad(power=2, **kwargs)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_rbf_kernel.py,56,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import RBFKernel\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\n\nclass TestRBFKernel(unittest.TestCase, BaseKernelTestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        return RBFKernel(**kwargs)\n\n    def create_kernel_ard(self, num_dims, **kwargs):\n        return RBFKernel(ard_num_dims=num_dims, **kwargs)\n\n    def test_ard(self):\n        a = torch.tensor([[1, 2], [2, 4]], dtype=torch.float)\n        b = torch.tensor([[1, 3], [0, 4]], dtype=torch.float)\n        lengthscales = torch.tensor([1, 2], dtype=torch.float).view(1, 2)\n\n        kernel = RBFKernel(ard_num_dims=2)\n        kernel.initialize(lengthscale=lengthscales)\n        kernel.eval()\n\n        scaled_a = a.div(lengthscales)\n        scaled_b = b.div(lengthscales)\n        actual = (scaled_a.unsqueeze(-2) - scaled_b.unsqueeze(-3)).pow(2).sum(dim=-1).mul_(-0.5).exp()\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # Diag\n        res = kernel(a, b).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims\n        actual = scaled_a.transpose(-1, -2).unsqueeze(-1) - scaled_b.transpose(-1, -2).unsqueeze(-2)\n        actual = actual.pow(2).mul_(-0.5).exp()\n        res = kernel(a, b, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims and diag\n        res = kernel(a, b, last_dim_is_batch=True).diag()\n        actual = actual.diagonal(dim1=-1, dim2=-2)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_ard_batch(self):\n        a = torch.tensor([[[1, 2, 3], [2, 4, 0]], [[-1, 1, 2], [2, 1, 4]]], dtype=torch.float)\n        b = torch.tensor([[[1, 3, 1]], [[2, -1, 0]]], dtype=torch.float).repeat(1, 2, 1)\n        lengthscales = torch.tensor([[[1, 2, 1]]], dtype=torch.float)\n\n        kernel = RBFKernel(batch_shape=torch.Size([2]), ard_num_dims=3)\n        kernel.initialize(lengthscale=lengthscales)\n        kernel.eval()\n\n        scaled_a = a.div(lengthscales)\n        scaled_b = b.div(lengthscales)\n        actual = (scaled_a.unsqueeze(-2) - scaled_b.unsqueeze(-3)).pow(2).sum(dim=-1).mul_(-0.5).exp()\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diagonal(dim1=-1, dim2=-2)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims\n        double_batch_a = scaled_a.transpose(-1, -2).unsqueeze(-1)\n        double_batch_b = scaled_b.transpose(-1, -2).unsqueeze(-2)\n        actual = double_batch_a - double_batch_b\n        actual = actual.pow(2).mul_(-0.5).exp()\n        res = kernel(a, b, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims and diag\n        res = kernel(a, b, last_dim_is_batch=True).diag()\n        actual = actual.diagonal(dim1=-2, dim2=-1)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_ard_separate_batch(self):\n        a = torch.tensor([[[1, 2, 3], [2, 4, 0]], [[-1, 1, 2], [2, 1, 4]]], dtype=torch.float)\n        b = torch.tensor([[[1, 3, 1]], [[2, -1, 0]]], dtype=torch.float).repeat(1, 2, 1)\n        lengthscales = torch.tensor([[[1, 2, 1]], [[2, 1, 0.5]]], dtype=torch.float)\n\n        kernel = RBFKernel(batch_shape=torch.Size([2]), ard_num_dims=3)\n        kernel.initialize(lengthscale=lengthscales)\n        kernel.eval()\n\n        scaled_a = a.div(lengthscales)\n        scaled_b = b.div(lengthscales)\n        actual = (scaled_a.unsqueeze(-2) - scaled_b.unsqueeze(-3)).pow(2).sum(dim=-1).mul_(-0.5).exp()\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diagonal(dim1=-1, dim2=-2)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_subset_active_compute_radial_basis_function(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        a_p = torch.tensor([1, 2, 3], dtype=torch.float).view(3, 1)\n        a = torch.cat((a, a_p), 1)\n        b = torch.tensor([0, 2, 4], dtype=torch.float).view(3, 1)\n        lengthscale = 2\n\n        kernel = RBFKernel(active_dims=[0])\n        kernel.initialize(lengthscale=lengthscale)\n        kernel.eval()\n\n        actual = torch.tensor([[16, 4, 0], [4, 0, 4], [64, 36, 16]], dtype=torch.float)\n        actual.mul_(-0.5).div_(lengthscale ** 2).exp_()\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_computes_radial_basis_function(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2, 4], dtype=torch.float).view(3, 1)\n        lengthscale = 2\n\n        kernel = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel.eval()\n\n        actual = torch.tensor([[16, 4, 0], [4, 0, 4], [64, 36, 16]], dtype=torch.float)\n        actual.mul_(-0.5).div_(lengthscale ** 2).exp_()\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_computes_radial_basis_function_gradient(self):\n        softplus = torch.nn.functional.softplus\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2, 2], dtype=torch.float).view(3, 1)\n        lengthscale = 2\n\n        kernel = RBFKernel().initialize(lengthscale=lengthscale)\n        kernel.eval()\n\n        param = math.log(math.exp(lengthscale) - 1) * torch.ones(3, 3)\n        param.requires_grad_()\n        diffs = a.expand(3, 3) - b.expand(3, 3).transpose(0, 1)\n        actual_output = (-0.5 * (diffs / softplus(param)) ** 2).exp()\n        actual_output.backward(gradient=torch.eye(3))\n        actual_param_grad = param.grad.sum()\n\n        output = kernel(a, b).evaluate()\n        output.backward(gradient=torch.eye(3))\n        res = kernel.raw_lengthscale.grad\n\n        self.assertLess(torch.norm(res - actual_param_grad), 1e-5)\n\n    def test_subset_active_computes_radial_basis_function_gradient(self):\n        softplus = torch.nn.functional.softplus\n        a_1 = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        a_p = torch.tensor([1, 2, 3], dtype=torch.float).view(3, 1)\n        a = torch.cat((a_1, a_p), 1)\n        b = torch.tensor([0, 2, 2], dtype=torch.float).view(3, 1)\n        lengthscale = 2\n\n        param = math.log(math.exp(lengthscale) - 1) * torch.ones(3, 3)\n        param.requires_grad_()\n        diffs = a_1.expand(3, 3) - b.expand(3, 3).transpose(0, 1)\n        actual_output = (-0.5 * (diffs / softplus(param)) ** 2).exp()\n        actual_output.backward(torch.eye(3))\n        actual_param_grad = param.grad.sum()\n\n        kernel = RBFKernel(active_dims=[0])\n        kernel.initialize(lengthscale=lengthscale)\n        kernel.eval()\n        output = kernel(a, b).evaluate()\n        output.backward(gradient=torch.eye(3))\n        res = kernel.raw_lengthscale.grad\n\n        self.assertLess(torch.norm(res - actual_param_grad), 1e-5)\n\n    def test_initialize_lengthscale(self):\n        kernel = RBFKernel()\n        kernel.initialize(lengthscale=3.14)\n        actual_value = torch.tensor(3.14).view_as(kernel.lengthscale)\n        self.assertLess(torch.norm(kernel.lengthscale - actual_value), 1e-5)\n\n    def test_initialize_lengthscale_batch(self):\n        kernel = RBFKernel(batch_shape=torch.Size([2]))\n        ls_init = torch.tensor([3.14, 4.13])\n        kernel.initialize(lengthscale=ls_init)\n        actual_value = ls_init.view_as(kernel.lengthscale)\n        self.assertLess(torch.norm(kernel.lengthscale - actual_value), 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_rbf_kernel_grad.py,16,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import RBFKernelGrad\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\n\nclass TestRBFKernelGrad(unittest.TestCase, BaseKernelTestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        return RBFKernelGrad(**kwargs)\n\n    def create_kernel_ard(self, num_dims, **kwargs):\n        return RBFKernelGrad(ard_num_dims=num_dims, **kwargs)\n\n    def test_kernel(self, cuda=False):\n        a = torch.tensor([[[1, 2], [2, 4]]], dtype=torch.float)\n        b = torch.tensor([[[1, 3], [0, 4]]], dtype=torch.float)\n\n        actual = torch.tensor(\n            [\n                [0.35321, 0, -0.73517, 0.0054977, 0.011443, -0.022886],\n                [0, 0.73517, 0, -0.011443, -0.012374, 0.047633],\n                [0.73517, 0, -0.79499, 0.022886, 0.047633, -0.083824],\n                [0.12476, 0.25967, 0.25967, 0.015565, 0.064793, 0],\n                [-0.25967, -0.2808, -0.54047, -0.064793, -0.23732, 0],\n                [-0.25967, -0.54047, -0.2808, 0, 0, 0.032396],\n            ]\n        )\n\n        kernel = RBFKernelGrad()\n\n        if cuda:\n            a = a.cuda()\n            b = b.cuda()\n            actual = actual.cuda()\n            kernel = kernel.cuda()\n\n        res = kernel(a, b).evaluate()\n\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_kernel_cuda(self):\n        if torch.cuda.is_available():\n            self.test_kernel(cuda=True)\n\n    def test_kernel_batch(self):\n        a = torch.tensor([[[1, 2, 3], [2, 4, 0]], [[-1, 1, 2], [2, 1, 4]]], dtype=torch.float)\n        b = torch.tensor([[[1, 3, 1]], [[2, -1, 0]]], dtype=torch.float).repeat(1, 2, 1)\n\n        kernel = RBFKernelGrad()\n        res = kernel(a, b).evaluate()\n\n        # Compute each batch separately\n        actual = torch.zeros(2, 8, 8)\n        actual[0, :, :] = kernel(a[0, :, :].squeeze(), b[0, :, :].squeeze()).evaluate()\n        actual[1, :, :] = kernel(a[1, :, :].squeeze(), b[1, :, :].squeeze()).evaluate()\n\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_initialize_lengthscale(self):\n        kernel = RBFKernelGrad()\n        kernel.initialize(lengthscale=3.14)\n        actual_value = torch.tensor(3.14).view_as(kernel.lengthscale)\n        self.assertLess(torch.norm(kernel.lengthscale - actual_value), 1e-5)\n\n    def test_initialize_lengthscale_batch(self):\n        kernel = RBFKernelGrad(batch_shape=torch.Size([2]))\n        ls_init = torch.tensor([3.14, 4.13])\n        kernel.initialize(lengthscale=ls_init)\n        actual_value = ls_init.view_as(kernel.lengthscale)\n        self.assertLess(torch.norm(kernel.lengthscale - actual_value), 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_rq_kernel.py,62,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import RQKernel\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\n\nclass TestRQKernel(unittest.TestCase, BaseKernelTestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        return RQKernel(**kwargs)\n\n    def create_kernel_ard(self, num_dims, **kwargs):\n        return RQKernel(ard_num_dims=num_dims, **kwargs)\n\n    def test_ard(self):\n        a = torch.tensor([[1, 2], [2, 4]], dtype=torch.float)\n        b = torch.tensor([[1, 3], [0, 4]], dtype=torch.float)\n        lengthscales = torch.tensor([1, 2], dtype=torch.float).view(1, 2)\n\n        kernel = RQKernel(ard_num_dims=2)\n        kernel.initialize(lengthscale=lengthscales)\n        kernel.initialize(alpha=3.0)\n        kernel.eval()\n\n        scaled_a = a.div(lengthscales)\n        scaled_b = b.div(lengthscales)\n        dist = (scaled_a.unsqueeze(-2) - scaled_b.unsqueeze(-3)).pow(2).sum(dim=-1)\n        actual = dist.div_(2 * kernel.alpha).add_(1.0).pow(-kernel.alpha)\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # Diag\n        res = kernel(a, b).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims\n        diff = scaled_a.transpose(-1, -2).unsqueeze(-1) - scaled_b.transpose(-1, -2).unsqueeze(-2)\n        actual = diff.pow(2).div_(2 * kernel.alpha).add_(1.0).pow(-kernel.alpha)\n        res = kernel(a, b, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims and diag\n        res = kernel(a, b, last_dim_is_batch=True).diag()\n        actual = actual.diagonal(dim1=-1, dim2=-2)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_ard_batch(self):\n        a = torch.tensor([[[1, 2, 3], [2, 4, 0]], [[-1, 1, 2], [2, 1, 4]]], dtype=torch.float)\n        b = torch.tensor([[[1, 3, 1]], [[2, -1, 0]]], dtype=torch.float).repeat(1, 2, 1)\n        lengthscales = torch.tensor([[[1, 2, 1]]], dtype=torch.float)\n\n        kernel = RQKernel(batch_shape=torch.Size([2]), ard_num_dims=3)\n        kernel.initialize(lengthscale=lengthscales)\n        kernel.initialize(alpha=3.0)\n        kernel.eval()\n\n        scaled_a = a.div(lengthscales)\n        scaled_b = b.div(lengthscales)\n        dist = (scaled_a.unsqueeze(-2) - scaled_b.unsqueeze(-3)).pow(2).sum(dim=-1)\n        actual = dist.div_(2 * kernel.alpha).add_(1.0).pow(-kernel.alpha)\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diagonal(dim1=-1, dim2=-2)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # # batch_dims\n        double_batch_a = scaled_a.transpose(-1, -2).unsqueeze(-1)\n        double_batch_b = scaled_b.transpose(-1, -2).unsqueeze(-2)\n        actual = double_batch_a - double_batch_b\n        alpha = kernel.alpha.view(2, 1, 1, 1)\n        actual = actual.pow_(2).div_(2 * alpha).add_(1.0).pow(-alpha)\n        res = kernel(a, b, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims and diag\n        res = kernel(a, b, last_dim_is_batch=True).diag()\n        actual = actual.diagonal(dim1=-2, dim2=-1)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_ard_separate_batch(self):\n        a = torch.tensor([[[1, 2, 3], [2, 4, 0]], [[-1, 1, 2], [2, 1, 4]]], dtype=torch.float)\n        b = torch.tensor([[[1, 3, 1]], [[2, -1, 0]]], dtype=torch.float).repeat(1, 2, 1)\n        lengthscales = torch.tensor([[[1, 2, 1]], [[2, 1, 0.5]]], dtype=torch.float)\n\n        kernel = RQKernel(batch_shape=torch.Size([2]), ard_num_dims=3)\n        kernel.initialize(lengthscale=lengthscales)\n        kernel.initialize(alpha=3.0)\n        kernel.eval()\n\n        scaled_a = a.div(lengthscales)\n        scaled_b = b.div(lengthscales)\n        dist = (scaled_a.unsqueeze(-2) - scaled_b.unsqueeze(-3)).pow(2).sum(dim=-1)\n        actual = dist.div_(2 * kernel.alpha).add_(1.0).pow(-kernel.alpha)\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diagonal(dim1=-1, dim2=-2)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_computes_rational_quadratic(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2, 4], dtype=torch.float).view(3, 1)\n        lengthscale = 2\n\n        kernel = RQKernel().initialize(lengthscale=lengthscale)\n        kernel.eval()\n\n        dist = torch.tensor([[16, 4, 0], [4, 0, 4], [64, 36, 16]], dtype=torch.float).div(lengthscale ** 2)\n        actual = dist.div_(2 * kernel.alpha).add_(1.0).pow(-kernel.alpha)\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_computes_rational_quadratic_gradient(self):\n        softplus = torch.nn.functional.softplus\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        b = torch.tensor([0, 2, 2], dtype=torch.float).view(3, 1)\n\n        kernel = RQKernel()\n        kernel.initialize(lengthscale=2.0)\n        kernel.initialize(alpha=3.0)\n        kernel.eval()\n\n        raw_lengthscale = torch.tensor(math.log(math.exp(2.0) - 1))\n        raw_lengthscale.requires_grad_()\n        raw_alpha = torch.tensor(math.log(math.exp(3.0) - 1))\n        raw_alpha.requires_grad_()\n        lengthscale, alpha = softplus(raw_lengthscale), softplus(raw_alpha)\n        dist = (a.expand(3, 3) - b.expand(3, 3).transpose(0, 1)).div(lengthscale).pow(2)\n        actual_output = dist.div(2 * alpha).add(1).pow(-alpha)\n        actual_output.backward(gradient=torch.eye(3))\n\n        output = kernel(a, b).evaluate()\n        output.backward(gradient=torch.eye(3))\n\n        res = kernel.raw_lengthscale.grad\n        self.assertLess(torch.norm(res - raw_lengthscale.grad), 1e-5)\n        res = kernel.raw_alpha.grad\n        self.assertLess(torch.norm(res - raw_alpha.grad), 1e-5)\n\n    def test_subset_active_compute_rational_quadratic(self):\n        a = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        a_p = torch.tensor([1, 2, 3], dtype=torch.float).view(3, 1)\n        a = torch.cat((a, a_p), 1)\n        b = torch.tensor([0, 2, 4], dtype=torch.float).view(3, 1)\n        lengthscale = 2\n\n        kernel = RQKernel(active_dims=[0])\n        kernel.initialize(lengthscale=lengthscale)\n        kernel.initialize(alpha=3.0)\n        kernel.eval()\n\n        actual = torch.tensor([[16, 4, 0], [4, 0, 4], [64, 36, 16]], dtype=torch.float)\n        actual.div_(lengthscale ** 2).div_(2 * kernel.alpha).add_(1).pow_(-kernel.alpha)\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_subset_active_computes_rational_quadratic_gradient(self):\n        softplus = torch.nn.functional.softplus\n        a_1 = torch.tensor([4, 2, 8], dtype=torch.float).view(3, 1)\n        a_p = torch.tensor([1, 2, 3], dtype=torch.float).view(3, 1)\n        a = torch.cat((a_1, a_p), 1)\n        b = torch.tensor([0, 2, 2], dtype=torch.float).view(3, 1)\n\n        kernel = RQKernel(active_dims=[0])\n        kernel.initialize(lengthscale=2.0)\n        kernel.initialize(alpha=3.0)\n        kernel.eval()\n\n        raw_lengthscale = torch.tensor(math.log(math.exp(2.0) - 1))\n        raw_lengthscale.requires_grad_()\n        raw_alpha = torch.tensor(math.log(math.exp(3.0) - 1))\n        raw_alpha.requires_grad_()\n        lengthscale, alpha = softplus(raw_lengthscale), softplus(raw_alpha)\n        dist = (a_1.expand(3, 3) - b.expand(3, 3).transpose(0, 1)).div(lengthscale).pow(2)\n        actual_output = dist.div(2 * alpha).add(1).pow(-alpha)\n        actual_output.backward(gradient=torch.eye(3))\n\n        output = kernel(a, b).evaluate()\n        output.backward(gradient=torch.eye(3))\n\n        res = kernel.raw_lengthscale.grad\n        self.assertLess(torch.norm(res - raw_lengthscale.grad), 1e-5)\n        res = kernel.raw_alpha.grad\n        self.assertLess(torch.norm(res - raw_alpha.grad), 1e-5)\n\n    def test_initialize_lengthscale(self):\n        kernel = RQKernel()\n        kernel.initialize(lengthscale=3.14)\n        actual_value = torch.tensor(3.14).view_as(kernel.lengthscale)\n        self.assertLess(torch.norm(kernel.lengthscale - actual_value), 1e-5)\n\n    def test_initialize_lengthscale_batch(self):\n        kernel = RQKernel(batch_shape=torch.Size([2]))\n        ls_init = torch.tensor([3.14, 4.13])\n        kernel.initialize(lengthscale=ls_init)\n        actual_value = ls_init.view_as(kernel.lengthscale)\n        self.assertLess(torch.norm(kernel.lengthscale - actual_value), 1e-5)\n\n    def test_initialize_alpha(self):\n        kernel = RQKernel()\n        kernel.initialize(alpha=3.0)\n        actual_value = torch.tensor(3.0).view_as(kernel.alpha)\n        self.assertLess(torch.norm(kernel.alpha - actual_value), 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_scale_kernel.py,30,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import LinearKernel, RBFKernel, ScaleKernel\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\n\nclass TestScaleKernel(BaseKernelTestCase, unittest.TestCase):\n    def create_kernel_no_ard(self, **kwargs):\n        base_kernel = RBFKernel()\n        kernel = ScaleKernel(base_kernel, **kwargs)\n        return kernel\n\n    def create_kernel_ard(self, num_dims, **kwargs):\n        base_kernel = RBFKernel(ard_num_dims=num_dims)\n        kernel = ScaleKernel(base_kernel, **kwargs)\n        return kernel\n\n    def test_ard(self):\n        a = torch.tensor([[1, 2], [2, 4]], dtype=torch.float)\n        b = torch.tensor([[1, 3], [0, 4]], dtype=torch.float)\n        lengthscales = torch.tensor([1, 2], dtype=torch.float).view(1, 2)\n\n        base_kernel = RBFKernel(ard_num_dims=2)\n        base_kernel.initialize(lengthscale=lengthscales)\n        kernel = ScaleKernel(base_kernel)\n        kernel.initialize(outputscale=torch.tensor([3], dtype=torch.float))\n        kernel.eval()\n\n        scaled_a = a.div(lengthscales)\n        scaled_b = b.div(lengthscales)\n        actual = (scaled_a.unsqueeze(-2) - scaled_b.unsqueeze(-3)).pow(2).sum(dim=-1).mul_(-0.5).exp()\n        actual.mul_(3)\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # Diag\n        res = kernel(a, b).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims\n        actual = scaled_a.transpose(-1, -2).unsqueeze(-1) - scaled_b.transpose(-1, -2).unsqueeze(-2)\n        actual = actual.pow(2).mul_(-0.5).exp().view(2, 2, 2)\n        actual.mul_(3)\n        res = kernel(a, b, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims and diag\n        res = kernel(a, b, last_dim_is_batch=True).diag()\n        actual = torch.cat([actual[i].diag().unsqueeze(0) for i in range(actual.size(0))])\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_ard_batch(self):\n        a = torch.tensor([[[1, 2, 3], [2, 4, 0]], [[-1, 1, 2], [2, 1, 4]]], dtype=torch.float)\n        b = torch.tensor([[[1, 3, 1]], [[2, -1, 0]]], dtype=torch.float).repeat(1, 2, 1)\n        lengthscales = torch.tensor([[[1, 2, 1]]], dtype=torch.float)\n\n        base_kernel = RBFKernel(batch_shape=torch.Size([2]), ard_num_dims=3)\n        base_kernel.initialize(lengthscale=lengthscales)\n        kernel = ScaleKernel(base_kernel, batch_shape=torch.Size([2]))\n        kernel.initialize(outputscale=torch.tensor([1, 2], dtype=torch.float))\n        kernel.eval()\n\n        scaled_a = a.div(lengthscales)\n        scaled_b = b.div(lengthscales)\n        actual = (scaled_a.unsqueeze(-2) - scaled_b.unsqueeze(-3)).pow(2).sum(dim=-1).mul_(-0.5).exp()\n        actual[1].mul_(2)\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = torch.cat([actual[i].diag().unsqueeze(0) for i in range(actual.size(0))])\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims\n        double_batch_a = scaled_a.transpose(-1, -2)\n        double_batch_b = scaled_b.transpose(-1, -2)\n        actual = double_batch_a.unsqueeze(-1) - double_batch_b.unsqueeze(-2)\n        actual = actual.pow(2).mul_(-0.5).exp()\n        actual[1, :, :, :].mul_(2)\n        res = kernel(a, b, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims and diag\n        res = kernel(a, b, last_dim_is_batch=True).diag()\n        actual = actual.diagonal(dim1=-2, dim2=-1)\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_initialize_outputscale(self):\n        kernel = ScaleKernel(RBFKernel())\n        kernel.initialize(outputscale=3.14)\n        actual_value = torch.tensor(3.14).view_as(kernel.outputscale)\n        self.assertLess(torch.norm(kernel.outputscale - actual_value), 1e-5)\n\n    def test_initialize_outputscale_batch(self):\n        kernel = ScaleKernel(RBFKernel(), batch_shape=torch.Size([2]))\n        ls_init = torch.tensor([3.14, 4.13])\n        kernel.initialize(outputscale=ls_init)\n        actual_value = ls_init.view_as(kernel.outputscale)\n        self.assertLess(torch.norm(kernel.outputscale - actual_value), 1e-5)\n\n    def test_stationary(self):\n        kernel = ScaleKernel(RBFKernel())\n        self.assertTrue(kernel.is_stationary)\n\n    def test_non_stationary(self):\n        kernel = ScaleKernel(LinearKernel())\n        self.assertFalse(kernel.is_stationary)\n\n    def test_inherit_active_dims(self):\n        lengthscales = torch.tensor([1, 1], dtype=torch.float)\n        base_kernel = RBFKernel(active_dims=(1, 2), ard_num_dims=2)\n        base_kernel.initialize(lengthscale=lengthscales)\n        kernel = ScaleKernel(base_kernel)\n        kernel.initialize(outputscale=torch.tensor([3], dtype=torch.float))\n        kernel.eval()\n        self.assertTrue(torch.all(kernel.active_dims == base_kernel.active_dims))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/test_spectral_mixture_kernel.py,28,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import SpectralMixtureKernel\n\n\nclass TestSpectralMixtureKernel(unittest.TestCase):\n    def test_standard(self):\n        a = torch.tensor([[0, 1], [2, 2], [2, 0]], dtype=torch.float)\n        means = torch.tensor([[1, 2], [2, 1]], dtype=torch.float)\n        scales = torch.tensor([[0.5, 0.25], [0.25, 0.5]], dtype=torch.float)\n        scales = scales.unsqueeze(1)\n        means = means.unsqueeze(1)\n        weights = torch.tensor([4, 2], dtype=torch.float)\n        kernel = SpectralMixtureKernel(num_mixtures=2, ard_num_dims=2)\n        kernel.initialize(mixture_weights=weights, mixture_means=means, mixture_scales=scales)\n        kernel.eval()\n\n        actual = torch.zeros(2, 3, 3, 2)\n        for i in range(3):\n            for j in range(3):\n                for k in range(2):\n                    new_term = torch.cos(2 * math.pi * (a[i] - a[j]) * means[k])\n                    new_term *= torch.exp(-2 * (math.pi * (a[i] - a[j])) ** 2 * scales[k] ** 2)\n                    actual[k, i, j] = new_term\n        actual = actual.prod(-1)\n        actual[0].mul_(weights[0])\n        actual[1].mul_(weights[1])\n        actual = actual.sum(0)\n\n        res = kernel(a, a).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, a).diag()\n        actual = actual.diag()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims\n        actual = torch.zeros(2, 3, 3, 2)\n        for i in range(3):\n            for j in range(3):\n                for k in range(2):\n                    new_term = torch.cos(2 * math.pi * (a[i] - a[j]) * means[k])\n                    new_term *= torch.exp(-2 * (math.pi * (a[i] - a[j])) ** 2 * scales[k] ** 2)\n                    actual[k, i, j] = new_term\n        actual[0].mul_(weights[0])\n        actual[1].mul_(weights[1])\n        actual = actual.sum(0)\n        actual = actual.permute(2, 0, 1)\n        res = kernel(a, a, last_dim_is_batch=True).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # batch_dims + diag\n        res = kernel(a, a, last_dim_is_batch=True).diag()\n        actual = torch.cat([actual[i].diag().unsqueeze(0) for i in range(actual.size(0))])\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n    def test_batch_separate(self):\n        a = torch.tensor([[4, 2, 8], [1, 2, 3]], dtype=torch.float).view(2, 3, 1)\n        b = torch.tensor([[0, 2, 1], [-1, 2, 0]], dtype=torch.float).view(2, 3, 1)\n        means = torch.tensor([[1, 2], [2, 3]], dtype=torch.float).view(2, 2, 1, 1)\n        scales = torch.tensor([[0.5, 0.25], [0.25, 1]], dtype=torch.float).view(2, 2, 1, 1)\n        weights = torch.tensor([[4, 2], [1, 2]], dtype=torch.float).view(2, 2)\n        kernel = SpectralMixtureKernel(batch_shape=torch.Size([2]), num_mixtures=2)\n        kernel.initialize(mixture_weights=weights, mixture_means=means, mixture_scales=scales)\n        kernel.eval()\n\n        actual = torch.zeros(2, 3, 3)\n        for l in range(2):\n            for k in range(2):\n                for i in range(3):\n                    for j in range(3):\n                        new_term = torch.cos(2 * math.pi * (a[l, i] - b[l, j]) * means[l, k])\n                        new_term *= torch.exp(-2 * (math.pi * (a[l, i] - b[l, j])) ** 2 * scales[l, k] ** 2)\n                        new_term *= weights[l, k]\n                        actual[l, i, j] += new_term.item()\n\n        res = kernel(a, b).evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n        # diag\n        res = kernel(a, b).diag()\n        actual = torch.cat([actual[i].diag().unsqueeze(0) for i in range(actual.size(0))])\n        self.assertLess(torch.norm(res - actual), 1e-5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/__init__.py,0,b'#!/usr/bin/env python3\n'
test/lazy/test_added_diag_lazy_tensor.py,19,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import AddedDiagLazyTensor, DiagLazyTensor, NonLazyTensor, RootLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestAddedDiagLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    should_test_sample = True\n\n    def create_lazy_tensor(self):\n        tensor = torch.randn(5, 5)\n        tensor = tensor.transpose(-1, -2).matmul(tensor).detach()\n        diag = torch.tensor([1.0, 2.0, 4.0, 2.0, 3.0], requires_grad=True)\n        return AddedDiagLazyTensor(NonLazyTensor(tensor), DiagLazyTensor(diag))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        diag = lazy_tensor._diag_tensor._diag\n        tensor = lazy_tensor._lazy_tensor.tensor\n        return tensor + diag.diag()\n\n\nclass TestAddedDiagLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 4\n    should_test_sample = True\n\n    def create_lazy_tensor(self):\n        tensor = torch.randn(3, 5, 5)\n        tensor = tensor.transpose(-1, -2).matmul(tensor).detach()\n        diag = torch.tensor(\n            [[1.0, 2.0, 4.0, 2.0, 3.0], [2.0, 1.0, 2.0, 1.0, 4.0], [1.0, 2.0, 2.0, 3.0, 4.0]], requires_grad=True\n        )\n        return AddedDiagLazyTensor(NonLazyTensor(tensor), DiagLazyTensor(diag))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        diag = lazy_tensor._diag_tensor._diag\n        tensor = lazy_tensor._lazy_tensor.tensor\n        return tensor + torch.diag_embed(diag, dim1=-2, dim2=-1)\n\n\nclass TestAddedDiagLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 4\n    # Because these LTs are large, we\'ll skil the big tests\n    should_test_sample = False\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        tensor = torch.randn(4, 3, 5, 5)\n        tensor = tensor.transpose(-1, -2).matmul(tensor).detach()\n        diag = (\n            torch.tensor(\n                [[1.0, 2.0, 4.0, 2.0, 3.0], [2.0, 1.0, 2.0, 1.0, 4.0], [1.0, 2.0, 2.0, 3.0, 4.0]], requires_grad=True\n            )\n            .repeat(4, 1, 1)\n            .detach()\n        )\n        return AddedDiagLazyTensor(NonLazyTensor(tensor), DiagLazyTensor(diag))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        diag = lazy_tensor._diag_tensor._diag\n        tensor = lazy_tensor._lazy_tensor.tensor\n        return tensor + torch.diag_embed(diag, dim1=-2, dim2=-1)\n\n\nclass TestAddedDiagLazyTensorPrecondOverride(unittest.TestCase):\n    def test_precond_solve(self):\n        seed = 4\n        torch.random.manual_seed(seed)\n\n        tensor = torch.randn(1000, 800)\n        diag = torch.abs(torch.randn(1000))\n\n        standard_lt = AddedDiagLazyTensor(RootLazyTensor(tensor), DiagLazyTensor(diag))\n        evals, evecs = torch.symeig(standard_lt.evaluate(), eigenvectors=True)\n\n        # this preconditioner is a simple example of near deflation\n        def nonstandard_preconditioner(self):\n            top_100_evecs = evecs[:, :100]\n            top_100_evals = evals[:100] + 0.2 * torch.randn(100)\n\n            precond_lt = RootLazyTensor(top_100_evecs @ torch.diag(top_100_evals ** 0.5))\n            logdet = top_100_evals.log().sum()\n\n            def precond_closure(rhs):\n                rhs2 = top_100_evecs.t() @ rhs\n                return top_100_evecs @ torch.diag(1.0 / top_100_evals) @ rhs2\n\n            return precond_closure, precond_lt, logdet\n\n        overrode_lt = AddedDiagLazyTensor(\n            RootLazyTensor(tensor), DiagLazyTensor(diag), preconditioner_override=nonstandard_preconditioner\n        )\n\n        # compute a solve - mostly to make sure that we can actually perform the solve\n        rhs = torch.randn(1000, 1)\n        standard_solve = standard_lt.inv_matmul(rhs)\n        overrode_solve = overrode_lt.inv_matmul(rhs)\n\n        # gut checking that our preconditioner is not breaking anything\n        self.assertEqual(standard_solve.shape, overrode_solve.shape)\n        self.assertLess(torch.norm(standard_solve - overrode_solve) / standard_solve.norm(), 1.0)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_batch_repeat_lazy_tensor.py,12,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch import lazify\nfrom gpytorch.lazy import BatchRepeatLazyTensor, ToeplitzLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase, RectangularLazyTensorTestCase\n\n\nclass TestBatchRepeatLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        toeplitz_column = torch.tensor([4, 0.1, 0.05, 0.01, 0.0], dtype=torch.float)\n        toeplitz_column.detach_()\n        return BatchRepeatLazyTensor(ToeplitzLazyTensor(toeplitz_column), torch.Size((3,)))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        evaluated = lazy_tensor.base_lazy_tensor.evaluate()\n        return evaluated.repeat(*lazy_tensor.batch_repeat, 1, 1)\n\n\nclass TestBatchRepeatLazyTensorNonSquare(RectangularLazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        rand_mat = torch.randn(25, 12, dtype=torch.float)\n        rand_mat.detach_()\n        return BatchRepeatLazyTensor(lazify(rand_mat), torch.Size((10,)))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        evaluated = lazy_tensor.base_lazy_tensor.evaluate()\n        return evaluated.repeat(*lazy_tensor.batch_repeat, 1, 1)\n\n\nclass TestBatchRepeatLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        toeplitz_column = torch.tensor([[4, 0, 0, 1], [3, 0, -0.5, -1]], dtype=torch.float)\n        toeplitz_column.detach_()\n        return BatchRepeatLazyTensor(ToeplitzLazyTensor(toeplitz_column), torch.Size((3,)))\n        return BatchRepeatLazyTensor(ToeplitzLazyTensor(toeplitz_column), torch.Size((3,)))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        evaluated = lazy_tensor.base_lazy_tensor.evaluate()\n        return evaluated.repeat(*lazy_tensor.batch_repeat, 1, 1)\n\n\nclass TestBatchRepeatLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        toeplitz_column = torch.tensor(\n            [[[4, 0, 0, 1], [3, 0, -0.5, -1]], [[2, 0.1, 0.01, 0.0], [3, 0, -0.1, -2]]], dtype=torch.float\n        )\n        toeplitz_column.detach_()\n        return BatchRepeatLazyTensor(ToeplitzLazyTensor(toeplitz_column), torch.Size((2, 3, 1, 4)))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        evaluated = lazy_tensor.base_lazy_tensor.evaluate()\n        return evaluated.repeat(*lazy_tensor.batch_repeat, 1, 1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_block_diag_lazy_tensor.py,11,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import BlockDiagLazyTensor, NonLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestBlockDiagLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    should_test_sample = True\n\n    def create_lazy_tensor(self):\n        blocks = torch.randn(8, 4, 4)\n        blocks = blocks.matmul(blocks.transpose(-1, -2))\n        blocks.add_(torch.eye(4, 4).unsqueeze_(0))\n        return BlockDiagLazyTensor(NonLazyTensor(blocks))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        blocks = lazy_tensor.base_lazy_tensor.tensor\n        actual = torch.zeros(32, 32)\n        for i in range(8):\n            actual[i * 4 : (i + 1) * 4, i * 4 : (i + 1) * 4] = blocks[i]\n        return actual\n\n\nclass TestBlockDiagLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    should_test_sample = True\n\n    def create_lazy_tensor(self):\n        blocks = torch.randn(2, 6, 4, 4)\n        blocks = blocks.matmul(blocks.transpose(-1, -2))\n        blocks.add_(torch.eye(4, 4))\n        return BlockDiagLazyTensor(NonLazyTensor(blocks), block_dim=2)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        blocks = lazy_tensor.base_lazy_tensor.tensor\n        actual = torch.zeros(2, 24, 24)\n        for i in range(2):\n            for j in range(6):\n                actual[i, j * 4 : (j + 1) * 4, j * 4 : (j + 1) * 4] = blocks[i, j]\n        return actual\n\n\nclass TestBlockDiagLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    should_test_sample = False\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        blocks = torch.randn(2, 6, 5, 4, 4)\n        blocks = blocks.matmul(blocks.transpose(-1, -2))\n        blocks.add_(torch.eye(4, 4))\n        blocks.detach_()\n        return BlockDiagLazyTensor(NonLazyTensor(blocks), block_dim=1)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        blocks = lazy_tensor.base_lazy_tensor.tensor\n        actual = torch.zeros(2, 5, 24, 24)\n        for i in range(2):\n            for j in range(6):\n                for k in range(5):\n                    actual[i, k, j * 4 : (j + 1) * 4, j * 4 : (j + 1) * 4] = blocks[i, k, j]\n        return actual\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_block_interleaved_lazy_tensor.py,11,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import BlockInterleavedLazyTensor, NonLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestBlockInterleavedLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    should_test_sample = True\n\n    def create_lazy_tensor(self):\n        blocks = torch.randn(8, 4, 4)\n        blocks = blocks.matmul(blocks.transpose(-1, -2))\n        blocks.add_(torch.eye(4, 4).unsqueeze_(0))\n        return BlockInterleavedLazyTensor(NonLazyTensor(blocks))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        blocks = lazy_tensor.base_lazy_tensor.tensor\n        actual = torch.zeros(32, 32)\n        for i in range(8):\n            for j in range(4):\n                for k in range(4):\n                    actual[j * 8 + i, k * 8 + i] = blocks[i, j, k]\n        return actual\n\n\nclass TestBlockInterleavedLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    should_test_sample = True\n\n    def create_lazy_tensor(self):\n        blocks = torch.randn(2, 6, 4, 4)\n        blocks = blocks.matmul(blocks.transpose(-1, -2))\n        blocks.add_(torch.eye(4, 4))\n        return BlockInterleavedLazyTensor(NonLazyTensor(blocks), block_dim=2)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        blocks = lazy_tensor.base_lazy_tensor.tensor\n        actual = torch.zeros(2, 24, 24)\n        for i in range(2):\n            for j in range(6):\n                for k in range(4):\n                    for l in range(4):\n                        actual[i, k * 6 + j, l * 6 + j] = blocks[i, j, k, l]\n        return actual\n\n\nclass TestBlockInterleavedLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    should_test_sample = False\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        blocks = torch.randn(2, 6, 5, 4, 4)\n        blocks = blocks.matmul(blocks.transpose(-1, -2))\n        blocks.add_(torch.eye(4, 4))\n        blocks.detach_()\n        return BlockInterleavedLazyTensor(NonLazyTensor(blocks), block_dim=1)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        blocks = lazy_tensor.base_lazy_tensor.tensor\n        actual = torch.zeros(2, 5, 24, 24)\n        for i in range(2):\n            for j in range(6):\n                for k in range(5):\n                    for l in range(4):\n                        for m in range(4):\n                            actual[i, k, l * 6 + j, m * 6 + j] = blocks[i, k, j, l, m]\n        return actual\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_cached_cg_lazy_tensor.py,42,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\nimport warnings\nfrom unittest.mock import MagicMock, patch\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.lazy import CachedCGLazyTensor, NonLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase, _ensure_symmetric_grad\nfrom gpytorch.utils.warnings import ExtraComputationWarning\n\n\nclass TestCachedCGLazyTensorNoLogdet(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self, with_solves=False, with_logdet=False):\n        mat = torch.randn(5, 6)\n        mat = mat.matmul(mat.transpose(-1, -2))\n        mat.requires_grad_(True)\n\n        lazy_tensor = NonLazyTensor(mat)\n        eager_rhs = torch.randn(5, 10).detach()\n        if with_solves:\n            with gpytorch.settings.num_trace_samples(1000 if with_logdet else 1):  # For inv_quad_logdet tests\n                solve, probe_vecs, probe_vec_norms, probe_vec_solves, tmats = CachedCGLazyTensor.precompute_terms(\n                    lazy_tensor, eager_rhs.detach(), logdet_terms=with_logdet\n                )\n                eager_rhss = [eager_rhs.detach(), eager_rhs[..., -2:-1].detach()]\n                solves = [solve.detach(), solve[..., -2:-1].detach()]\n        else:\n            eager_rhss = [eager_rhs]\n            solves = []\n            probe_vecs = torch.tensor([], dtype=mat.dtype, device=mat.device)\n            probe_vec_norms = torch.tensor([], dtype=mat.dtype, device=mat.device)\n            probe_vec_solves = torch.tensor([], dtype=mat.dtype, device=mat.device)\n            tmats = torch.tensor([], dtype=mat.dtype, device=mat.device)\n\n        return CachedCGLazyTensor(lazy_tensor, eager_rhss, solves, probe_vecs, probe_vec_norms, probe_vec_solves, tmats)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return lazy_tensor.base_lazy_tensor.tensor\n\n    def _test_inv_matmul(self, rhs, lhs=None, cholesky=False):\n        if cholesky:  # These tests don\'t make sense for CachedCGLazyTensor\n            return\n\n        lazy_tensor = self.create_lazy_tensor(with_solves=True).requires_grad_(True)\n        lazy_tensor_copy = lazy_tensor.clone().detach_().requires_grad_(True)\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor_copy)\n        evaluated.register_hook(_ensure_symmetric_grad)\n\n        # Rather than the supplied rhs and lhs,\n        # we\'ll replace them with ones that we\'ve precomputed solves for\n        rhs_orig = rhs\n        if rhs_orig.dim() == 1:\n            rhs = lazy_tensor.eager_rhss[0][..., -1].squeeze(-1).clone().detach().requires_grad_(True)\n            # Make sure we\'re setting this test up correctly\n            self.assertEqual(rhs_orig.shape, rhs.shape)\n        else:\n            if lhs is not None:\n                rhs = lazy_tensor.eager_rhss[0][..., 2:].clone().detach().requires_grad_(True)\n            else:\n                rhs = lazy_tensor.eager_rhss[0].clone().detach().requires_grad_(True)\n            # Make sure we\'re setting this test up correctly\n            self.assertEqual(rhs_orig.shape[:-1], rhs.shape[:-1])\n\n        lhs = lhs\n        if lhs is not None:\n            lhs_orig = lhs\n            if rhs_orig.dim() == 1:\n                lhs = lazy_tensor.eager_rhss[0][..., :-1].transpose(-1, -2).clone().detach().requires_grad_(True)\n            else:\n                lhs = lazy_tensor.eager_rhss[0][..., :2].transpose(-1, -2).clone().detach().requires_grad_(True)\n\n            # Make sure we\'re setting this test up correctly\n            self.assertEqual(lhs_orig.shape[:-2], lhs.shape[:-2])\n\n        # Create a test right hand side and left hand side\n        rhs.requires_grad_(True)\n        rhs_copy = rhs.clone().detach().requires_grad_(True)\n        if lhs is not None:\n            lhs.requires_grad_(True)\n            lhs_copy = lhs.clone().detach().requires_grad_(True)\n\n        _wrapped_cg = MagicMock(wraps=gpytorch.utils.linear_cg)\n        with patch(""gpytorch.utils.linear_cg"", new=_wrapped_cg) as linear_cg_mock:\n            with gpytorch.settings.max_cholesky_size(math.inf if cholesky else 0), gpytorch.settings.cg_tolerance(1e-4):\n                with warnings.catch_warnings(record=True) as ws:\n                    # Makes sure warnings we catch don\'t cause `-w error` to fail\n                    warnings.simplefilter(""always"", ExtraComputationWarning)\n\n                    # Perform the inv_matmul\n                    if lhs is not None:\n                        res = lazy_tensor.inv_matmul(rhs, lhs)\n                        actual = lhs_copy @ evaluated.inverse() @ rhs_copy\n                    else:\n                        res = lazy_tensor.inv_matmul(rhs)\n                        actual = evaluated.inverse().matmul(rhs_copy)\n                    self.assertAllClose(res, actual, rtol=0.02, atol=1e-5)\n                    self.assertFalse(any(issubclass(w.category, ExtraComputationWarning) for w in ws))\n\n                with warnings.catch_warnings(record=True) as ws:\n                    # Makes sure warnings we catch don\'t cause `-w error` to fail\n                    warnings.simplefilter(""always"", ExtraComputationWarning)\n\n                    # Perform backward pass\n                    grad = torch.randn_like(res)\n                    res.backward(gradient=grad)\n                    actual.backward(gradient=grad)\n                    for arg, arg_copy in zip(lazy_tensor.representation(), lazy_tensor_copy.representation()):\n                        if arg_copy.grad is not None:\n                            self.assertAllClose(arg.grad, arg_copy.grad, rtol=0.03, atol=1e-5)\n                    self.assertAllClose(rhs.grad, rhs_copy.grad, rtol=0.03, atol=1e-5)\n                    if lhs is not None:\n                        self.assertAllClose(lhs.grad, lhs_copy.grad, rtol=0.03, atol=1e-5)\n\n                    # Determine if we\'ve called CG or not\n                    # We shouldn\'t if we supplied a lhs\n                    if lhs is None:\n                        self.assertEqual(len([w for w in ws if issubclass(w.category, ExtraComputationWarning)]), 1)\n                        if not cholesky and self.__class__.should_call_cg:\n                            self.assertTrue(linear_cg_mock.called)\n                    else:\n                        self.assertFalse(any(issubclass(w.category, ExtraComputationWarning) for w in ws))\n                        self.assertFalse(linear_cg_mock.called)\n\n    def _test_inv_quad_logdet(self, reduce_inv_quad=True, cholesky=False):\n        with warnings.catch_warnings():\n            # Makes sure warnings we catch don\'t cause `-w error` to fail\n            warnings.simplefilter(""ignore"", ExtraComputationWarning)\n            super()._test_inv_quad_logdet(reduce_inv_quad=reduce_inv_quad, cholesky=cholesky)\n\n    def test_inv_matmul_vector(self):\n        # Skipping this test because it\'s not really necessary for CachedCGLazyTensor\n        # We\'ll only ever be performing inv_matmul against matrices ,r owhen a left hand side is supplied\n        pass\n\n    def test_inv_matmul_matrix_broadcast(self):\n        pass\n\n    def test_inv_quad_logdet(self):\n        pass\n\n    def test_inv_quad_logdet_no_reduce(self):\n        pass\n\n    def test_root_inv_decomposition(self):\n        lazy_tensor = self.create_lazy_tensor()\n        root_approx = lazy_tensor.root_inv_decomposition()\n\n        test_mat = lazy_tensor.eager_rhss[0].clone().detach()\n\n        res = root_approx.matmul(test_mat)\n        with warnings.catch_warnings():\n            # Makes sure warnings we catch don\'t cause `-w error` to fail\n            warnings.simplefilter(""ignore"", ExtraComputationWarning)\n            actual = lazy_tensor.inv_matmul(test_mat)\n        self.assertLess(torch.norm(res - actual) / actual.norm(), 0.1)\n\n\nclass TestCachedCGLazyTensor(TestCachedCGLazyTensorNoLogdet):\n    seed = 0\n\n    def test_inv_quad_logdet(self):\n        # Forward\n        lazy_tensor = self.create_lazy_tensor(with_solves=True, with_logdet=True)\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n        flattened_evaluated = evaluated.view(-1, *lazy_tensor.matrix_shape)\n\n        vecs = lazy_tensor.eager_rhss[0].clone().detach().requires_grad_(True)\n        vecs_copy = lazy_tensor.eager_rhss[0].clone().detach().requires_grad_(True)\n\n        with gpytorch.settings.num_trace_samples(128), warnings.catch_warnings(record=True) as ws:\n            res_inv_quad, res_logdet = lazy_tensor.inv_quad_logdet(inv_quad_rhs=vecs, logdet=True)\n            self.assertFalse(any(issubclass(w.category, ExtraComputationWarning) for w in ws))\n        res = res_inv_quad + res_logdet\n\n        actual_inv_quad = evaluated.inverse().matmul(vecs_copy).mul(vecs_copy).sum(-2).sum(-1)\n        actual_logdet = torch.cat(\n            [torch.logdet(flattened_evaluated[i]).unsqueeze(0) for i in range(lazy_tensor.batch_shape.numel())]\n        ).view(lazy_tensor.batch_shape)\n        actual = actual_inv_quad + actual_logdet\n\n        diff = (res - actual).abs() / actual.abs().clamp(1, math.inf)\n        self.assertLess(diff.max().item(), 15e-2)\n\n    def test_inv_quad_logdet_no_reduce(self):\n        # Forward\n        lazy_tensor = self.create_lazy_tensor(with_solves=True, with_logdet=True)\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor)\n        flattened_evaluated = evaluated.view(-1, *lazy_tensor.matrix_shape)\n\n        vecs = lazy_tensor.eager_rhss[0].clone().detach().requires_grad_(True)\n        vecs_copy = lazy_tensor.eager_rhss[0].clone().detach().requires_grad_(True)\n\n        with gpytorch.settings.num_trace_samples(128), warnings.catch_warnings(record=True) as ws:\n            res_inv_quad, res_logdet = lazy_tensor.inv_quad_logdet(\n                inv_quad_rhs=vecs, logdet=True, reduce_inv_quad=False\n            )\n            self.assertFalse(any(issubclass(w.category, ExtraComputationWarning) for w in ws))\n        res = res_inv_quad.sum(-1) + res_logdet\n\n        actual_inv_quad = evaluated.inverse().matmul(vecs_copy).mul(vecs_copy).sum(-2).sum(-1)\n        actual_logdet = torch.cat(\n            [torch.logdet(flattened_evaluated[i]).unsqueeze(0) for i in range(lazy_tensor.batch_shape.numel())]\n        ).view(lazy_tensor.batch_shape)\n        actual = actual_inv_quad + actual_logdet\n\n        diff = (res - actual).abs() / actual.abs().clamp(1, math.inf)\n        self.assertLess(diff.max().item(), 15e-2)\n\n\nclass TestCachedCGLazyTensorNoLogdetBatch(TestCachedCGLazyTensorNoLogdet):\n    seed = 0\n\n    def create_lazy_tensor(self, with_solves=False, with_logdet=False):\n        mat = torch.randn(3, 5, 6)\n        mat = mat.matmul(mat.transpose(-1, -2))\n        mat.requires_grad_(True)\n\n        lazy_tensor = NonLazyTensor(mat)\n        eager_rhs = torch.randn(3, 5, 10).detach()\n        if with_solves:\n            with gpytorch.settings.num_trace_samples(1000 if with_logdet else 1):  # For inv_quad_logdet tests\n                solve, probe_vecs, probe_vec_norms, probe_vec_solves, tmats = CachedCGLazyTensor.precompute_terms(\n                    lazy_tensor, eager_rhs.detach(), logdet_terms=with_logdet\n                )\n                eager_rhss = [eager_rhs.detach(), eager_rhs[..., -2:-1].detach()]\n                solves = [solve.detach(), solve[..., -2:-1].detach()]\n        else:\n            eager_rhss = [eager_rhs]\n            solves = []\n            probe_vecs = torch.tensor([], dtype=mat.dtype, device=mat.device)\n            probe_vec_norms = torch.tensor([], dtype=mat.dtype, device=mat.device)\n            probe_vec_solves = torch.tensor([], dtype=mat.dtype, device=mat.device)\n            tmats = torch.tensor([], dtype=mat.dtype, device=mat.device)\n\n        return CachedCGLazyTensor(lazy_tensor, eager_rhss, solves, probe_vecs, probe_vec_norms, probe_vec_solves, tmats)\n\n\nclass TestCachedCGLazyTensorBatch(TestCachedCGLazyTensor):\n    seed = 0\n\n    def create_lazy_tensor(self, with_solves=False, with_logdet=False):\n        mat = torch.randn(3, 5, 6)\n        mat = mat.matmul(mat.transpose(-1, -2))\n        mat.requires_grad_(True)\n\n        lazy_tensor = NonLazyTensor(mat)\n        eager_rhs = torch.randn(3, 5, 10).detach()\n        if with_solves:\n            with gpytorch.settings.num_trace_samples(1000 if with_logdet else 1):  # For inv_quad_logdet tests\n                solve, probe_vecs, probe_vec_norms, probe_vec_solves, tmats = CachedCGLazyTensor.precompute_terms(\n                    lazy_tensor, eager_rhs.detach(), logdet_terms=with_logdet\n                )\n                eager_rhss = [eager_rhs.detach(), eager_rhs[..., -2:-1].detach()]\n                solves = [solve.detach(), solve[..., -2:-1].detach()]\n        else:\n            eager_rhss = [eager_rhs]\n            solves = []\n            probe_vecs = torch.tensor([], dtype=mat.dtype, device=mat.device)\n            probe_vec_norms = torch.tensor([], dtype=mat.dtype, device=mat.device)\n            probe_vec_solves = torch.tensor([], dtype=mat.dtype, device=mat.device)\n            tmats = torch.tensor([], dtype=mat.dtype, device=mat.device)\n\n        return CachedCGLazyTensor(lazy_tensor, eager_rhss, solves, probe_vecs, probe_vec_norms, probe_vec_solves, tmats)\n\n\nclass TestCachedCGLazyTensorMultiBatch(TestCachedCGLazyTensor):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    should_test_sample = False\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self, with_solves=False, with_logdet=False):\n        mat = torch.randn(2, 3, 5, 6)\n        mat = mat.matmul(mat.transpose(-1, -2))\n        mat.requires_grad_(True)\n\n        lazy_tensor = NonLazyTensor(mat)\n        eager_rhs = torch.randn(2, 3, 5, 10).detach()\n        if with_solves:\n            with gpytorch.settings.num_trace_samples(1000 if with_logdet else 1):  # For inv_quad_logdet tests\n                solve, probe_vecs, probe_vec_norms, probe_vec_solves, tmats = CachedCGLazyTensor.precompute_terms(\n                    lazy_tensor, eager_rhs.detach(), logdet_terms=with_logdet\n                )\n                eager_rhss = [eager_rhs.detach(), eager_rhs[..., -2:-1].detach()]\n                solves = [solve.detach(), solve[..., -2:-1].detach()]\n        else:\n            eager_rhss = [eager_rhs]\n            solves = []\n            probe_vecs = torch.tensor([], dtype=mat.dtype, device=mat.device)\n            probe_vec_norms = torch.tensor([], dtype=mat.dtype, device=mat.device)\n            probe_vec_solves = torch.tensor([], dtype=mat.dtype, device=mat.device)\n            tmats = torch.tensor([], dtype=mat.dtype, device=mat.device)\n\n        return CachedCGLazyTensor(lazy_tensor, eager_rhss, solves, probe_vecs, probe_vec_norms, probe_vec_solves, tmats)\n'"
test/lazy/test_cat_lazy_tensor.py,7,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import CatLazyTensor, NonLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestCatLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 1\n\n    def create_lazy_tensor(self):\n        root = torch.randn(6, 7)\n        self.psd_mat = root.matmul(root.t())\n\n        slice1_mat = self.psd_mat[:2, :].requires_grad_()\n        slice2_mat = self.psd_mat[2:4, :].requires_grad_()\n        slice3_mat = self.psd_mat[4:6, :].requires_grad_()\n\n        slice1 = NonLazyTensor(slice1_mat)\n        slice2 = NonLazyTensor(slice2_mat)\n        slice3 = NonLazyTensor(slice3_mat)\n\n        return CatLazyTensor(slice1, slice2, slice3, dim=-2)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return self.psd_mat.detach().clone().requires_grad_()\n\n\nclass TestCatLazyTensorColumn(LazyTensorTestCase, unittest.TestCase):\n    seed = 1\n\n    def create_lazy_tensor(self):\n        root = torch.randn(6, 7)\n        self.psd_mat = root.matmul(root.t())\n\n        slice1_mat = self.psd_mat[:, :2].requires_grad_()\n        slice2_mat = self.psd_mat[:, 2:4].requires_grad_()\n        slice3_mat = self.psd_mat[:, 4:6].requires_grad_()\n\n        slice1 = NonLazyTensor(slice1_mat)\n        slice2 = NonLazyTensor(slice2_mat)\n        slice3 = NonLazyTensor(slice3_mat)\n\n        return CatLazyTensor(slice1, slice2, slice3, dim=-1)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return self.psd_mat.detach().clone().requires_grad_()\n\n\nclass TestCatLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        root = torch.randn(3, 6, 7)\n        self.psd_mat = root.matmul(root.transpose(-2, -1))\n\n        slice1_mat = self.psd_mat[..., :2, :].requires_grad_()\n        slice2_mat = self.psd_mat[..., 2:4, :].requires_grad_()\n        slice3_mat = self.psd_mat[..., 4:6, :].requires_grad_()\n\n        slice1 = NonLazyTensor(slice1_mat)\n        slice2 = NonLazyTensor(slice2_mat)\n        slice3 = NonLazyTensor(slice3_mat)\n\n        return CatLazyTensor(slice1, slice2, slice3, dim=-2)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return self.psd_mat.detach().clone().requires_grad_()\n\n\nclass TestCatLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        root = torch.randn(4, 3, 6, 7)\n        self.psd_mat = root.matmul(root.transpose(-2, -1))\n\n        slice1_mat = self.psd_mat[..., :2, :].requires_grad_()\n        slice2_mat = self.psd_mat[..., 2:4, :].requires_grad_()\n        slice3_mat = self.psd_mat[..., 4:6, :].requires_grad_()\n\n        slice1 = NonLazyTensor(slice1_mat)\n        slice2 = NonLazyTensor(slice2_mat)\n        slice3 = NonLazyTensor(slice3_mat)\n\n        return CatLazyTensor(slice1, slice2, slice3, dim=-2)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return self.psd_mat.detach().clone().requires_grad_()\n\n\nclass TestCatLazyTensorBatchCat(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        root = torch.randn(5, 3, 6, 7)\n        self.psd_mat = root.matmul(root.transpose(-2, -1))\n\n        slice1_mat = self.psd_mat[:2, ...].requires_grad_()\n        slice2_mat = self.psd_mat[2:3, ...].requires_grad_()\n        slice3_mat = self.psd_mat[3:, ...].requires_grad_()\n\n        slice1 = NonLazyTensor(slice1_mat)\n        slice2 = NonLazyTensor(slice2_mat)\n        slice3 = NonLazyTensor(slice3_mat)\n\n        return CatLazyTensor(slice1, slice2, slice3, dim=0)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return self.psd_mat.detach().clone().requires_grad_()\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_chol_lazy_tensor.py,10,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import CholLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestCholLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    should_test_sample = True\n    should_call_cg = False\n    should_call_lanczos = False\n\n    def create_lazy_tensor(self):\n        chol = torch.tensor(\n            [[3, 0, 0, 0, 0], [-1, 2, 0, 0, 0], [1, 4, 1, 0, 0], [0, 2, 3, 2, 0], [-4, -2, 1, 3, 4]],\n            dtype=torch.float,\n            requires_grad=True,\n        )\n        return CholLazyTensor(chol)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        chol = lazy_tensor.root.evaluate()\n        return chol.matmul(chol.transpose(-1, -2))\n\n\nclass TestCholLazyTensorBatch(TestCholLazyTensor):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        chol = torch.tensor(\n            [\n                [[3, 0, 0, 0, 0], [-1, 2, 0, 0, 0], [1, 4, 1, 0, 0], [0, 2, 3, 2, 0], [-4, -2, 1, 3, 4]],\n                [[2, 0, 0, 0, 0], [3, 1, 0, 0, 0], [-2, 3, 2, 0, 0], [-2, 1, -1, 3, 0], [-4, -4, 5, 2, 3]],\n            ],\n            dtype=torch.float,\n        )\n        chol.add_(torch.eye(5).unsqueeze(0))\n        chol.requires_grad_(True)\n        return CholLazyTensor(chol)\n\n\nclass TestCholLazyTensorMultiBatch(TestCholLazyTensor):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    should_test_sample = False\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        chol = torch.tensor(\n            [\n                [[3, 0, 0, 0, 0], [-1, 2, 0, 0, 0], [1, 4, 1, 0, 0], [0, 2, 3, 2, 0], [-4, -2, 1, 3, 4]],\n                [[2, 0, 0, 0, 0], [3, 1, 0, 0, 0], [-2, 3, 2, 0, 0], [-2, 1, -1, 3, 0], [-4, -4, 5, 2, 3]],\n            ],\n            dtype=torch.float,\n        )\n        chol = chol.repeat(3, 1, 1, 1)\n        chol[1].mul_(2)\n        chol[2].mul_(0.5)\n        chol.add_(torch.eye(5).unsqueeze_(0).unsqueeze_(0))\n        chol.requires_grad_(True)\n        return CholLazyTensor(chol)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_constant_mul_lazy_tensor.py,11,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import ToeplitzLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\nfrom gpytorch.utils.toeplitz import sym_toeplitz\n\n\nclass TestConstantMulLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        column = torch.tensor([5, 1, 2, 0], dtype=torch.float, requires_grad=True)\n        constant = 2.5\n        return ToeplitzLazyTensor(column) * constant\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        constant = lazy_tensor.expanded_constant\n        column = lazy_tensor.base_lazy_tensor.column\n        return sym_toeplitz(column) * constant\n\n\nclass TestConstantMulLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        column = torch.tensor([[5, 1, 2, 0]], dtype=torch.float).repeat(2, 1)\n        column.requires_grad_(True)\n        constant = torch.tensor([2.5, 1.0]).view(2, 1, 1)\n        return ToeplitzLazyTensor(column) * constant\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        constant = lazy_tensor.expanded_constant\n        column = lazy_tensor.base_lazy_tensor.column\n        return torch.cat([sym_toeplitz(column[0]).unsqueeze(0), sym_toeplitz(column[1]).unsqueeze(0)]) * constant.view(\n            2, 1, 1\n        )\n\n\nclass TestConstantMulLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    should_test_sample = False\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        column = torch.tensor([[5, 1, 2, 0]], dtype=torch.float).repeat(3, 2, 1)\n        column.requires_grad_(True)\n        constant = torch.randn(3, 2, 1, 1).abs()\n        return ToeplitzLazyTensor(column) * constant\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        constant = lazy_tensor.expanded_constant\n        toeplitz = lazy_tensor.base_lazy_tensor\n        return toeplitz.evaluate() * constant\n\n\nclass TestConstantMulLazyTensorMultiBatchBroadcastConstant(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    should_test_sample = False\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        column = torch.tensor([[5, 1, 2, 0]], dtype=torch.float).repeat(3, 2, 1)\n        column.requires_grad_(True)\n        constant = torch.randn(2, 1, 1).abs()\n        return ToeplitzLazyTensor(column) * constant\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        constant = lazy_tensor.expanded_constant\n        toeplitz = lazy_tensor.base_lazy_tensor\n        return toeplitz.evaluate() * constant\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_diag_lazy_tensor.py,7,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import DiagLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestDiagLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    should_test_sample = True\n    should_call_cg = False\n    should_call_lanczos = False\n\n    def create_lazy_tensor(self):\n        diag = torch.tensor([1.0, 2.0, 4.0, 2.0, 3.0], requires_grad=True)\n        return DiagLazyTensor(diag)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        diag = lazy_tensor._diag\n        return diag.diag()\n\n\nclass TestDiagLazyTensorBatch(TestDiagLazyTensor):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        diag = torch.tensor(\n            [[1.0, 2.0, 4.0, 2.0, 3.0], [2.0, 1.0, 2.0, 1.0, 4.0], [1.0, 2.0, 2.0, 3.0, 4.0]], requires_grad=True\n        )\n        return DiagLazyTensor(diag)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        diag = lazy_tensor._diag\n        return torch.cat([diag[i].diag().unsqueeze(0) for i in range(3)])\n\n\nclass TestDiagLazyTensorMultiBatch(TestDiagLazyTensor):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    should_test_sample = True\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        diag = torch.randn(6, 3, 5).pow_(2)\n        diag.requires_grad_(True)\n        return DiagLazyTensor(diag)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        diag = lazy_tensor._diag\n        flattened_diag = diag.view(-1, diag.size(-1))\n        res = torch.cat([flattened_diag[i].diag().unsqueeze(0) for i in range(18)])\n        return res.view(6, 3, 5, 5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_interpolated_lazy_tensor.py,28,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import InterpolatedLazyTensor, NonLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestInterpolatedLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 1\n    should_test_sample = True\n\n    def test_quad_form_derivative(self):\n        # InterpolatedLazyTensor\'s representation includes int variables (the interp. indices),\n        # so the default derivative doesn\'t apply\n        pass\n\n    def create_lazy_tensor(self):\n        left_interp_indices = torch.LongTensor([[0, 1], [2, 3], [3, 4], [4, 5]])\n        left_interp_values = torch.tensor([[0.1, 0.9], [1, 2], [0.5, 1], [1, 3]], dtype=torch.float)\n        left_interp_values.requires_grad = True\n        right_interp_indices = torch.LongTensor([[0, 1], [2, 3], [3, 4], [4, 5]])\n        right_interp_values = torch.tensor([[0.1, 0.9], [1, 2], [0.5, 1], [1, 3]], dtype=torch.float)\n        right_interp_values.requires_grad = True\n\n        base_tensor = torch.randn(6, 6)\n        base_tensor = base_tensor.t().matmul(base_tensor)\n        base_tensor.requires_grad = True\n        base_lazy_tensor = NonLazyTensor(base_tensor)\n\n        return InterpolatedLazyTensor(\n            base_lazy_tensor, left_interp_indices, left_interp_values, right_interp_indices, right_interp_values\n        )\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        left_matrix = torch.zeros(4, 6)\n        right_matrix = torch.zeros(4, 6)\n        left_matrix.scatter_(1, lazy_tensor.left_interp_indices, lazy_tensor.left_interp_values)\n        right_matrix.scatter_(1, lazy_tensor.right_interp_indices, lazy_tensor.right_interp_values)\n\n        base_tensor = lazy_tensor.base_lazy_tensor.tensor\n        actual = left_matrix.matmul(base_tensor).matmul(right_matrix.t())\n        return actual\n\n\nclass TestInterpolatedLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    should_test_sample = True\n\n    def test_quad_form_derivative(self):\n        # InterpolatedLazyTensor\'s representation includes int variables (the interp. indices),\n        # so the default derivative doesn\'t apply\n        pass\n\n    def create_lazy_tensor(self):\n        left_interp_indices = torch.LongTensor([[0, 1], [2, 3], [3, 4], [4, 5]]).repeat(5, 1, 1)\n        left_interp_values = torch.tensor([[0.1, 0.9], [1, 2], [0.5, 1], [1, 3]], dtype=torch.float).repeat(5, 1, 1)\n        left_interp_values.requires_grad = True\n        right_interp_indices = torch.LongTensor([[0, 1], [2, 3], [3, 4], [4, 5]]).repeat(5, 1, 1)\n        right_interp_values = torch.tensor([[0.1, 0.9], [1, 2], [0.5, 1], [1, 3]], dtype=torch.float).repeat(5, 1, 1)\n        right_interp_values.requires_grad = True\n\n        base_tensor = torch.randn(5, 6, 6)\n        base_tensor = base_tensor.transpose(-2, -1).matmul(base_tensor)\n        base_tensor.requires_grad = True\n        base_lazy_tensor = NonLazyTensor(base_tensor)\n\n        return InterpolatedLazyTensor(\n            base_lazy_tensor, left_interp_indices, left_interp_values, right_interp_indices, right_interp_values\n        )\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        left_matrix_comps = []\n        right_matrix_comps = []\n        for i in range(5):\n            left_matrix_comp = torch.zeros(4, 6)\n            right_matrix_comp = torch.zeros(4, 6)\n            left_matrix_comp.scatter_(1, lazy_tensor.left_interp_indices[i], lazy_tensor.left_interp_values[i])\n            right_matrix_comp.scatter_(1, lazy_tensor.right_interp_indices[i], lazy_tensor.right_interp_values[i])\n            left_matrix_comps.append(left_matrix_comp.unsqueeze(0))\n            right_matrix_comps.append(right_matrix_comp.unsqueeze(0))\n        left_matrix = torch.cat(left_matrix_comps)\n        right_matrix = torch.cat(right_matrix_comps)\n\n        base_tensor = lazy_tensor.base_lazy_tensor.tensor\n        actual = left_matrix.matmul(base_tensor).matmul(right_matrix.transpose(-1, -2))\n        return actual\n\n\nclass TestInterpolatedLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    should_test_sample = False\n    skip_slq_tests = True\n\n    def test_quad_form_derivative(self):\n        # InterpolatedLazyTensor\'s representation includes int variables (the interp. indices),\n        # so the default derivative doesn\'t apply\n        pass\n\n    def create_lazy_tensor(self):\n        left_interp_indices = torch.LongTensor([[0, 1], [2, 3], [3, 4], [4, 5]]).repeat(2, 5, 1, 1)\n        left_interp_values = torch.tensor([[0.1, 0.9], [1, 2], [0.5, 1], [1, 3]], dtype=torch.float).repeat(2, 5, 1, 1)\n        left_interp_values.requires_grad = True\n        right_interp_indices = torch.LongTensor([[0, 1], [2, 3], [3, 4], [4, 5]]).repeat(2, 5, 1, 1)\n        right_interp_values = torch.tensor([[0.1, 0.9], [1, 2], [0.5, 1], [1, 3]], dtype=torch.float).repeat(2, 5, 1, 1)\n        right_interp_values.requires_grad = True\n\n        base_tensor = torch.randn(5, 6, 6)\n        base_tensor = base_tensor.transpose(-2, -1).matmul(base_tensor)\n        base_lazy_tensor = NonLazyTensor(base_tensor)\n\n        return InterpolatedLazyTensor(\n            base_lazy_tensor, left_interp_indices, left_interp_values, right_interp_indices, right_interp_values\n        )\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        left_matrix_comps = []\n        right_matrix_comps = []\n        for i in range(2):\n            for j in range(5):\n                left_matrix_comp = torch.zeros(4, 6)\n                right_matrix_comp = torch.zeros(4, 6)\n                left_matrix_comp.scatter_(\n                    1, lazy_tensor.left_interp_indices[i, j], lazy_tensor.left_interp_values[i, j]\n                )\n                right_matrix_comp.scatter_(\n                    1, lazy_tensor.right_interp_indices[i, j], lazy_tensor.right_interp_values[i, j]\n                )\n                left_matrix_comps.append(left_matrix_comp.unsqueeze(0))\n                right_matrix_comps.append(right_matrix_comp.unsqueeze(0))\n        left_matrix = torch.cat(left_matrix_comps)\n        right_matrix = torch.cat(right_matrix_comps)\n        left_matrix = left_matrix.view(2, 5, 4, 6)\n        right_matrix = right_matrix.view(2, 5, 4, 6)\n\n        base_tensor = lazy_tensor.base_lazy_tensor.tensor\n        actual = left_matrix.matmul(base_tensor).matmul(right_matrix.transpose(-1, -2))\n        return actual\n\n\ndef empty_method(self):\n    pass\n\n\nclass TestInterpolatedLazyTensorRectangular(LazyTensorTestCase, unittest.TestCase):\n    def create_lazy_tensor(self):\n        itplzt = InterpolatedLazyTensor(NonLazyTensor(torch.rand(3, 4)))\n        return itplzt\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return lazy_tensor.base_lazy_tensor.tensor\n\n    # Disable tests meant for square matrices\n    test_add_diag = empty_method\n    test_diag = empty_method\n    test_inv_matmul_matrix = empty_method\n    test_inv_matmul_matrix_broadcast = empty_method\n    test_inv_matmul_matrix_cholesky = empty_method\n    test_inv_matmul_matrix_with_left = empty_method\n    test_inv_matmul_vector = empty_method\n    test_inv_matmul_vector_with_left = empty_method\n    test_inv_matmul_vector_with_left_cholesky = empty_method\n    test_inv_quad_logdet = empty_method\n    test_inv_quad_logdet_no_reduce = empty_method\n    test_inv_quad_logdet_no_reduce_cholesky = empty_method\n    test_quad_form_derivative = empty_method\n    test_root_decomposition = empty_method\n    test_root_decomposition_cholesky = empty_method\n    test_root_inv_decomposition = empty_method\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_kronecker_product_lazy_tensor.py,16,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import KroneckerProductLazyTensor, NonLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase, RectangularLazyTensorTestCase\n\n\ndef kron(a, b):\n    res = []\n    for i in range(a.size(-2)):\n        row_res = []\n        for j in range(a.size(-1)):\n            row_res.append(b * a[..., i, j].unsqueeze(-1).unsqueeze(-2))\n        res.append(torch.cat(row_res, -1))\n    return torch.cat(res, -2)\n\n\nclass TestKroneckerProductLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        a = torch.tensor([[4, 0, 2], [0, 3, -1], [2, -1, 3]], dtype=torch.float)\n        b = torch.tensor([[2, 1], [1, 2]], dtype=torch.float)\n        c = torch.tensor([[4, 0, 1, 0], [0, 4, -1, 0], [1, -1, 3, 0], [0, 0, 0, 4]], dtype=torch.float)\n        a.requires_grad_(True)\n        b.requires_grad_(True)\n        c.requires_grad_(True)\n        kp_lazy_tensor = KroneckerProductLazyTensor(NonLazyTensor(a), NonLazyTensor(b), NonLazyTensor(c))\n        return kp_lazy_tensor\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        res = kron(lazy_tensor.lazy_tensors[0].tensor, lazy_tensor.lazy_tensors[1].tensor)\n        res = kron(res, lazy_tensor.lazy_tensors[2].tensor)\n        return res\n\n\nclass TestKroneckerProductLazyTensorBatch(TestKroneckerProductLazyTensor):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        a = torch.tensor([[4, 0, 2], [0, 3, -1], [2, -1, 3]], dtype=torch.float).repeat(3, 1, 1)\n        b = torch.tensor([[2, 1], [1, 2]], dtype=torch.float).repeat(3, 1, 1)\n        c = torch.tensor([[4, 0, 1, 0], [0, 4, -1, 0], [1, -1, 3, 0], [0, 0, 0, 4]], dtype=torch.float).repeat(3, 1, 1)\n        a.requires_grad_(True)\n        b.requires_grad_(True)\n        c.requires_grad_(True)\n        kp_lazy_tensor = KroneckerProductLazyTensor(NonLazyTensor(a), NonLazyTensor(b), NonLazyTensor(c))\n        return kp_lazy_tensor\n\n\nclass TestKroneckerProductLazyTensorRectangular(RectangularLazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        a = torch.randn(2, 3, requires_grad=True)\n        b = torch.randn(5, 2, requires_grad=True)\n        c = torch.randn(6, 4, requires_grad=True)\n        kp_lazy_tensor = KroneckerProductLazyTensor(NonLazyTensor(a), NonLazyTensor(b), NonLazyTensor(c))\n        return kp_lazy_tensor\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        res = kron(lazy_tensor.lazy_tensors[0].tensor, lazy_tensor.lazy_tensors[1].tensor)\n        res = kron(res, lazy_tensor.lazy_tensors[2].tensor)\n        return res\n\n\nclass TestKroneckerProductLazyTensorRectangularMultiBatch(TestKroneckerProductLazyTensorRectangular):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        a = torch.randn(3, 4, 2, 3, requires_grad=True)\n        b = torch.randn(3, 4, 5, 2, requires_grad=True)\n        c = torch.randn(3, 4, 6, 4, requires_grad=True)\n        kp_lazy_tensor = KroneckerProductLazyTensor(NonLazyTensor(a), NonLazyTensor(b), NonLazyTensor(c))\n        return kp_lazy_tensor\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_lazy_evaluated_kernel_tensor.py,24,"b'#!/usr/bin/env python3\n\nimport math\nimport unittest\nfrom unittest.mock import MagicMock, patch\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase, _ensure_symmetric_grad\n\n\nclass TestLazyEvaluatedKernelTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        kern = gpytorch.kernels.RBFKernel()\n        mat1 = torch.randn(2, 5, 6)\n        mat2 = mat1.detach().clone()\n        return kern(mat1, mat2)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        with gpytorch.settings.lazily_evaluate_kernels(False):\n            return gpytorch.lazy.delazify(lazy_tensor.kernel(lazy_tensor.x1, lazy_tensor.x2))\n\n    def _test_matmul(self, rhs):\n        lazy_tensor = self.create_lazy_tensor().requires_grad_(True)\n        lazy_tensor_copy = lazy_tensor.clone().detach_().requires_grad_(True)\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor_copy)\n\n        res = lazy_tensor.matmul(rhs)\n        actual = evaluated.matmul(rhs)\n        self.assertAllClose(res, actual)\n\n        grad = torch.randn_like(res)\n        res.backward(gradient=grad)\n        actual.backward(gradient=grad)\n        for param, param_copy in zip(lazy_tensor.kernel.parameters(), lazy_tensor_copy.kernel.parameters()):\n            self.assertAllClose(param.grad, param_copy.grad, rtol=1e-3)\n        self.assertAllClose(\n            lazy_tensor.x1.grad + lazy_tensor.x2.grad, lazy_tensor_copy.x1.grad + lazy_tensor_copy.x2.grad, rtol=1e-3\n        )\n\n    def _test_inv_matmul(self, rhs, lhs=None, cholesky=False):\n        lazy_tensor = self.create_lazy_tensor().requires_grad_(True)\n        lazy_tensor_copy = lazy_tensor.clone().detach_().requires_grad_(True)\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor_copy)\n        evaluated.register_hook(_ensure_symmetric_grad)\n\n        # Create a test right hand side and left hand side\n        rhs.requires_grad_(True)\n        rhs_copy = rhs.clone().detach().requires_grad_(True)\n        if lhs is not None:\n            lhs.requires_grad_(True)\n            lhs_copy = lhs.clone().detach().requires_grad_(True)\n\n        _wrapped_cg = MagicMock(wraps=gpytorch.utils.linear_cg)\n        with patch(""gpytorch.utils.linear_cg"", new=_wrapped_cg) as linear_cg_mock:\n            with gpytorch.settings.max_cholesky_size(math.inf if cholesky else 0), gpytorch.settings.cg_tolerance(1e-4):\n                # Perform the inv_matmul\n                if lhs is not None:\n                    res = lazy_tensor.inv_matmul(rhs, lhs)\n                    actual = lhs_copy @ evaluated.inverse() @ rhs_copy\n                else:\n                    res = lazy_tensor.inv_matmul(rhs)\n                    actual = evaluated.inverse().matmul(rhs_copy)\n                self.assertAllClose(res, actual, rtol=0.02, atol=1e-5)\n\n                # Perform backward pass\n                grad = torch.randn_like(res)\n                res.backward(gradient=grad)\n                actual.backward(gradient=grad)\n                for param, param_copy in zip(lazy_tensor.kernel.parameters(), lazy_tensor_copy.kernel.parameters()):\n                    self.assertAllClose(param.grad, param_copy.grad, rtol=1e-3)\n                self.assertAllClose(\n                    lazy_tensor.x1.grad + lazy_tensor.x2.grad,\n                    lazy_tensor_copy.x1.grad + lazy_tensor_copy.x2.grad,\n                    rtol=1e-3,\n                )\n                self.assertAllClose(rhs.grad, rhs_copy.grad, rtol=0.03, atol=1e-5)\n                if lhs is not None:\n                    self.assertAllClose(lhs.grad, lhs_copy.grad, rtol=0.03, atol=1e-5)\n\n            # Determine if we\'ve called CG or not\n            if not cholesky and self.__class__.should_call_cg:\n                self.assertTrue(linear_cg_mock.called)\n            else:\n                self.assertFalse(linear_cg_mock.called)\n\n    def test_inv_matmul_matrix_with_checkpointing(self):\n        # Add one checkpointing test\n        lazy_tensor = self.create_lazy_tensor().requires_grad_(True)\n        lazy_tensor_copy = lazy_tensor.clone().detach_().requires_grad_(True)\n        evaluated = self.evaluate_lazy_tensor(lazy_tensor_copy)\n\n        test_vector = torch.randn(2, 5, 6)\n        test_vector_copy = test_vector.clone()\n        with gpytorch.beta_features.checkpoint_kernel(2):\n            res = lazy_tensor.inv_matmul(test_vector)\n            actual = evaluated.inverse().matmul(test_vector_copy)\n            self.assertLess(((res - actual).abs() / actual.abs().clamp(1, 1e5)).max().item(), 3e-1)\n\n            grad = torch.randn_like(res)\n            res.backward(gradient=grad)\n            actual.backward(gradient=grad)\n\n        for param, param_copy in zip(lazy_tensor.kernel.parameters(), lazy_tensor_copy.kernel.parameters()):\n            self.assertAllClose(param.grad, param_copy.grad, rtol=1e-3)\n        self.assertAllClose(\n            lazy_tensor.x1.grad + lazy_tensor.x2.grad, lazy_tensor_copy.x1.grad + lazy_tensor_copy.x2.grad, rtol=1e-3\n        )\n\n    def test_batch_getitem(self):\n        """"""Indexing was wrong when the kernel had more batch dimensions than the\n        data""""""\n        x1 = torch.randn(5, 6)\n        x2 = torch.randn(5, 6)\n        kern = gpytorch.kernels.RBFKernel(batch_shape=torch.Size([2]))\n        k = kern(x1, x2)\n        self.assertEqual(k.size(), torch.Size([2, 5, 5]))\n        self.assertEqual(k[..., :4, :3].size(), torch.Size([2, 4, 3]))\n\n    def test_getitem_tensor_index(self):\n        # Not supported a.t.m. with LazyEvaluatedKernelTensors\n        pass\n\n    def test_quad_form_derivative(self):\n        pass\n\n\nclass TestLazyEvaluatedKernelTensorMultitaskBatch(TestLazyEvaluatedKernelTensorBatch):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        kern = gpytorch.kernels.MultitaskKernel(gpytorch.kernels.RBFKernel(), num_tasks=3, rank=2)\n        mat1 = torch.randn(2, 5, 6)\n        mat2 = mat1.detach().clone()\n        return kern(mat1, mat2)\n\n    def test_inv_matmul_matrix_with_checkpointing(self):\n        pass\n\n\nclass TestLazyEvaluatedKernelTensorAdditive(TestLazyEvaluatedKernelTensorBatch):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        kern = gpytorch.kernels.AdditiveStructureKernel(gpytorch.kernels.RBFKernel(), num_dims=6)\n        mat1 = torch.randn(5, 6)\n        mat2 = mat1.detach().clone()\n        return kern(mat1, mat2)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        res = gpytorch.lazy.delazify(\n            gpytorch.Module.__call__(\n                lazy_tensor.kernel.base_kernel,\n                lazy_tensor.x1.transpose(-1, -2).unsqueeze(-1),\n                lazy_tensor.x2.transpose(-1, -2).unsqueeze(-1),\n            )\n        ).sum(0)\n        return res\n\n    def test_inv_matmul_matrix_with_checkpointing(self):\n        pass\n'"
test/lazy/test_matmul_lazy_tensor.py,8,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import MatmulLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase, RectangularLazyTensorTestCase\n\n\nclass TestMatmulLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 1\n\n    def create_lazy_tensor(self):\n        lhs = torch.randn(5, 6, requires_grad=True)\n        rhs = lhs.clone().detach().transpose(-1, -2)\n        covar = MatmulLazyTensor(lhs, rhs)\n        return covar\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return lazy_tensor.left_lazy_tensor.tensor.matmul(lazy_tensor.right_lazy_tensor.tensor)\n\n\nclass TestMatmulLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 3\n\n    def create_lazy_tensor(self):\n        lhs = torch.randn(5, 5, 6, requires_grad=True)\n        rhs = lhs.clone().detach().transpose(-1, -2)\n        covar = MatmulLazyTensor(lhs, rhs)\n        return covar\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return lazy_tensor.left_lazy_tensor.tensor.matmul(lazy_tensor.right_lazy_tensor.tensor)\n\n\nclass TestMatmulLazyTensorRectangular(RectangularLazyTensorTestCase, unittest.TestCase):\n    def create_lazy_tensor(self):\n        lhs = torch.randn(5, 3, requires_grad=True)\n        rhs = torch.randn(3, 6, requires_grad=True)\n        covar = MatmulLazyTensor(lhs, rhs)\n        return covar\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return lazy_tensor.left_lazy_tensor.tensor.matmul(lazy_tensor.right_lazy_tensor.tensor)\n\n\nclass TestMatmulLazyTensorRectangularMultiBatch(RectangularLazyTensorTestCase, unittest.TestCase):\n    def create_lazy_tensor(self):\n        lhs = torch.randn(2, 3, 5, 3, requires_grad=True)\n        rhs = torch.randn(2, 3, 3, 6, requires_grad=True)\n        covar = MatmulLazyTensor(lhs, rhs)\n        return covar\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return lazy_tensor.left_lazy_tensor.tensor.matmul(lazy_tensor.right_lazy_tensor.tensor)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_mul_lazy_tensor.py,14,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import RootLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\ndef make_random_mat(size, rank, batch_shape=torch.Size(())):\n    res = torch.randn(*batch_shape, size, rank)\n    return res\n\n\nclass TestMulLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 10\n\n    def create_lazy_tensor(self):\n        mat1 = make_random_mat(6, 6)\n        mat2 = make_random_mat(6, 6)\n        res = RootLazyTensor(mat1) * RootLazyTensor(mat2)\n        return res.add_diag(torch.tensor(2.0))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        diag_tensor = lazy_tensor._diag_tensor.evaluate()\n        res = torch.mul(\n            lazy_tensor._lazy_tensor.left_lazy_tensor.evaluate(), lazy_tensor._lazy_tensor.right_lazy_tensor.evaluate()\n        )\n        res = res + diag_tensor\n        return res\n\n\nclass TestMulLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 2\n\n    def create_lazy_tensor(self):\n        mat1 = make_random_mat(6, rank=6, batch_shape=torch.Size((2,)))\n        mat2 = make_random_mat(6, rank=6, batch_shape=torch.Size((2,)))\n        res = RootLazyTensor(mat1) * RootLazyTensor(mat2)\n        return res.add_diag(torch.tensor(2.0))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        diag_tensor = lazy_tensor._diag_tensor.evaluate()\n        res = torch.mul(\n            lazy_tensor._lazy_tensor.left_lazy_tensor.evaluate(), lazy_tensor._lazy_tensor.right_lazy_tensor.evaluate()\n        )\n        res = res + diag_tensor\n        return res\n\n\nclass TestMulLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 1\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        mat1 = make_random_mat(6, rank=6, batch_shape=torch.Size((2, 3)))\n        mat2 = make_random_mat(6, rank=6, batch_shape=torch.Size((2, 3)))\n        res = RootLazyTensor(mat1) * RootLazyTensor(mat2)\n        return res.add_diag(torch.tensor(0.5))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        diag_tensor = lazy_tensor._diag_tensor.evaluate()\n        res = torch.mul(\n            lazy_tensor._lazy_tensor.left_lazy_tensor.evaluate(), lazy_tensor._lazy_tensor.right_lazy_tensor.evaluate()\n        )\n        res = res + diag_tensor\n        return res\n\n    def test_inv_quad_logdet(self):\n        pass\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_non_lazy_tensor.py,11,"b""#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.lazy import NonLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestNonLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        mat = torch.randn(5, 6)\n        mat = mat.matmul(mat.transpose(-1, -2))\n        mat.requires_grad_(True)\n        return NonLazyTensor(mat)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return lazy_tensor.tensor\n\n    def test_root_decomposition_exact(self):\n        lazy_tensor = self.create_lazy_tensor()\n        test_mat = torch.randn(*lazy_tensor.batch_shape, lazy_tensor.size(-1), 5)\n        with gpytorch.settings.fast_computations(covar_root_decomposition=False):\n            root_approx = lazy_tensor.root_decomposition()\n            res = root_approx.matmul(test_mat)\n            actual = lazy_tensor.matmul(test_mat)\n            self.assertLess(torch.norm(res - actual) / actual.norm(), 0.1)\n\n\nclass TestNonLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        mat = torch.randn(3, 5, 6)\n        mat = mat.matmul(mat.transpose(-1, -2))\n        mat.requires_grad_(True)\n        return NonLazyTensor(mat)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return lazy_tensor.tensor\n\n    def test_root_decomposition_exact(self):\n        lazy_tensor = self.create_lazy_tensor()\n        test_mat = torch.randn(*lazy_tensor.batch_shape, lazy_tensor.size(-1), 5)\n        with gpytorch.settings.fast_computations(covar_root_decomposition=False):\n            root_approx = lazy_tensor.root_decomposition()\n            res = root_approx.matmul(test_mat)\n            actual = lazy_tensor.matmul(test_mat)\n            self.assertLess(torch.norm(res - actual) / actual.norm(), 0.1)\n\n\nclass TestNonLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    # Because these LTs are large, we'll skil the big tests\n    should_test_sample = False\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        mat = torch.randn(2, 3, 5, 6)\n        mat = mat.matmul(mat.transpose(-1, -2))\n        mat.requires_grad_(True)\n        return NonLazyTensor(mat)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return lazy_tensor.tensor\n"""
test/lazy/test_psd_sum_lazy_tensor.py,8,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import NonLazyTensor, PsdSumLazyTensor, ToeplitzLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestPsdSumLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    should_test_sample = True\n\n    def create_lazy_tensor(self):\n        c1 = torch.tensor([5, 1, 2, 0], dtype=torch.float, requires_grad=True)\n        t1 = ToeplitzLazyTensor(c1)\n        c2 = torch.tensor([6, 0, 1, -1], dtype=torch.float, requires_grad=True)\n        t2 = ToeplitzLazyTensor(c2)\n        return PsdSumLazyTensor(t1, t2)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        tensors = [lt.evaluate() for lt in lazy_tensor.lazy_tensors]\n        return sum(tensors)\n\n\nclass TestPsdSumLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    should_test_sample = True\n\n    def create_lazy_tensor(self):\n        c1 = torch.tensor([[2, 0.5, 0, 0], [5, 1, 2, 0]], dtype=torch.float, requires_grad=True)\n        t1 = ToeplitzLazyTensor(c1)\n        c2 = torch.tensor([[2, 0.5, 0, 0], [6, 0, 1, -1]], dtype=torch.float, requires_grad=True)\n        t2 = ToeplitzLazyTensor(c2)\n        return PsdSumLazyTensor(t1, t2)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        tensors = [lt.evaluate() for lt in lazy_tensor.lazy_tensors]\n        return sum(tensors)\n\n\nclass TestPsdSumLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    should_test_sample = False\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        mat1 = torch.randn(2, 3, 4, 4)\n        lt1 = NonLazyTensor(mat1 @ mat1.transpose(-1, -2))\n        mat2 = torch.randn(2, 3, 4, 4)\n        lt2 = NonLazyTensor(mat2 @ mat2.transpose(-1, -2))\n        return PsdSumLazyTensor(lt1, lt2)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        tensors = [lt.evaluate() for lt in lazy_tensor.lazy_tensors]\n        return sum(tensors)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_root_lazy_tensor.py,6,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import RootLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestRootLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    should_test_sample = True\n    should_call_lanczos = False\n\n    def create_lazy_tensor(self):\n        root = torch.randn(3, 5, requires_grad=True)\n        return RootLazyTensor(root)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        root = lazy_tensor.root.tensor\n        res = root.matmul(root.transpose(-1, -2))\n        return res\n\n\nclass TestRootLazyTensorBatch(TestRootLazyTensor):\n    seed = 1\n\n    def create_lazy_tensor(self):\n        root = torch.randn(3, 5, 5)\n        root.add_(torch.eye(5).unsqueeze(0))\n        root.requires_grad_(True)\n        return RootLazyTensor(root)\n\n\nclass TestRootLazyTensorMultiBatch(TestRootLazyTensor):\n    seed = 1\n    # Because these LTs are large, we\'ll skil the big tests\n    should_test_sample = False\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        root = torch.randn(4, 3, 5, 5)\n        root.requires_grad_(True)\n        return RootLazyTensor(root)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_sum_batch_lazy_tensor.py,5,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import NonLazyTensor, SumBatchLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestSumBatchLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 6\n    should_test_sample = True\n\n    def create_lazy_tensor(self):\n        blocks = torch.randn(12, 4, 4)\n        blocks = blocks.transpose(-1, -2).matmul(blocks)\n        blocks.requires_grad_(True)\n        return SumBatchLazyTensor(NonLazyTensor(blocks))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        blocks = lazy_tensor.base_lazy_tensor.tensor\n        return blocks.sum(0)\n\n\nclass TestSumBatchLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 6\n    should_test_sample = True\n\n    def create_lazy_tensor(self):\n        blocks = torch.randn(2, 6, 4, 4)\n        blocks = blocks.transpose(-1, -2).matmul(blocks)\n        blocks.requires_grad_(True)\n        return SumBatchLazyTensor(NonLazyTensor(blocks))\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        blocks = lazy_tensor.base_lazy_tensor.tensor\n        return blocks.view(2, 6, 4, 4).sum(1)\n\n\nclass TestSumBatchLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 6\n    # Because these LTs are large, we\'ll skil the big tests\n    should_test_sample = False\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        blocks = torch.randn(2, 3, 6, 4, 4)\n        blocks = blocks.transpose(-1, -2).matmul(blocks)\n        blocks.detach_()\n        return SumBatchLazyTensor(NonLazyTensor(blocks), block_dim=1)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        blocks = lazy_tensor.base_lazy_tensor.tensor\n        return blocks.sum(-3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_sum_lazy_tensor.py,16,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import ToeplitzLazyTensor, lazify\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestSumLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        c1 = torch.tensor([5, 1, 2, 0], dtype=torch.float, requires_grad=True)\n        t1 = ToeplitzLazyTensor(c1)\n        c2 = torch.tensor([6, 0, 1, -1], dtype=torch.float, requires_grad=True)\n        t2 = ToeplitzLazyTensor(c2)\n        return t1 + t2\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        tensors = [lt.evaluate() for lt in lazy_tensor.lazy_tensors]\n        return sum(tensors)\n\n\nclass TestSumLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        c1 = torch.tensor([[2, 0.5, 0, 0], [5, 1, 2, 0]], dtype=torch.float, requires_grad=True)\n        t1 = ToeplitzLazyTensor(c1)\n        c2 = torch.tensor([[2, 0.5, 0, 0], [6, 0, 1, -1]], dtype=torch.float, requires_grad=True)\n        t2 = ToeplitzLazyTensor(c2)\n        return t1 + t2\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        tensors = [lt.evaluate() for lt in lazy_tensor.lazy_tensors]\n        return sum(tensors)\n\n\nclass TestSumLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n    # Because these LTs are large, we\'ll skil the big tests\n    skip_slq_tests = True\n\n    def create_lazy_tensor(self):\n        c1 = torch.tensor(\n            [[[2, 0.5, 0, 0], [5, 1, 2, 0]], [[2, 0.5, 0, 0], [5, 1, 2, 0]]], dtype=torch.float, requires_grad=True,\n        )\n        t1 = ToeplitzLazyTensor(c1)\n        c2 = torch.tensor(\n            [[[2, 0.5, 0, 0], [5, 1, 2, 0]], [[2, 0.5, 0, 0], [6, 0, 1, -1]]], dtype=torch.float, requires_grad=True,\n        )\n        t2 = ToeplitzLazyTensor(c2)\n        return t1 + t2\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        tensors = [lt.evaluate() for lt in lazy_tensor.lazy_tensors]\n        return sum(tensors)\n\n\nclass TestSumLazyTensorBroadcasting(unittest.TestCase):\n    def test_broadcast_same_shape(self):\n        test1 = lazify(torch.randn(30, 30))\n\n        test2 = torch.randn(30, 30)\n        res = test1 + test2\n        final_res = res + test2\n\n        torch_res = res.evaluate() + test2\n\n        self.assertEqual(final_res.shape, torch_res.shape)\n        self.assertEqual((final_res.evaluate() - torch_res).sum(), 0.0)\n\n    def test_broadcast_tensor_shape(self):\n        test1 = lazify(torch.randn(30, 30))\n\n        test2 = torch.randn(30, 1)\n        res = test1 + test2\n        final_res = res + test2\n\n        torch_res = res.evaluate() + test2\n\n        self.assertEqual(final_res.shape, torch_res.shape)\n        self.assertEqual((final_res.evaluate() - torch_res).sum(), 0.0)\n\n    def test_broadcast_lazy_shape(self):\n        test1 = lazify(torch.randn(30, 1))\n\n        test2 = torch.randn(30, 30)\n        res = test1 + test2\n        final_res = res + test2\n\n        torch_res = res.evaluate() + test2\n\n        self.assertEqual(final_res.shape, torch_res.shape)\n        self.assertEqual((final_res.evaluate() - torch_res).sum(), 0.0)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_toeplitz_lazy_tensor.py,8,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport gpytorch.utils.toeplitz as toeplitz\nfrom gpytorch.lazy import ToeplitzLazyTensor\nfrom gpytorch.test.lazy_tensor_test_case import LazyTensorTestCase\n\n\nclass TestToeplitzLazyTensor(LazyTensorTestCase, unittest.TestCase):\n    seed = 1\n\n    def create_lazy_tensor(self):\n        toeplitz_column = torch.tensor([4, 0.5, 0, 1], dtype=torch.float, requires_grad=True)\n        return ToeplitzLazyTensor(toeplitz_column)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return toeplitz.sym_toeplitz(lazy_tensor.column)\n\n\nclass TestToeplitzLazyTensorBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        toeplitz_column = torch.tensor([[2, -1, 0.5, 0.25], [4, 0.5, 0, 1]], dtype=torch.float, requires_grad=True)\n        return ToeplitzLazyTensor(toeplitz_column)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return torch.cat(\n            [\n                toeplitz.sym_toeplitz(lazy_tensor.column[0]).unsqueeze(0),\n                toeplitz.sym_toeplitz(lazy_tensor.column[1]).unsqueeze(0),\n            ]\n        )\n\n\nclass TestToeplitzLazyTensorMultiBatch(LazyTensorTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_lazy_tensor(self):\n        toeplitz_column = torch.tensor([[2, -1, 0.5, 0.25], [4, 0.5, 0, 1]], dtype=torch.float)\n        toeplitz_column = toeplitz_column.repeat(3, 1, 1)\n        toeplitz_column.requires_grad_(True)\n        return ToeplitzLazyTensor(toeplitz_column)\n\n    def evaluate_lazy_tensor(self, lazy_tensor):\n        return torch.cat(\n            [\n                toeplitz.sym_toeplitz(lazy_tensor.column[0, 0]).unsqueeze(0),\n                toeplitz.sym_toeplitz(lazy_tensor.column[0, 1]).unsqueeze(0),\n                toeplitz.sym_toeplitz(lazy_tensor.column[1, 0]).unsqueeze(0),\n                toeplitz.sym_toeplitz(lazy_tensor.column[1, 1]).unsqueeze(0),\n                toeplitz.sym_toeplitz(lazy_tensor.column[2, 0]).unsqueeze(0),\n                toeplitz.sym_toeplitz(lazy_tensor.column[2, 1]).unsqueeze(0),\n            ]\n        ).view(3, 2, 4, 4)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/lazy/test_zero_lazy_tensor.py,39,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.lazy import ZeroLazyTensor\nfrom gpytorch.test.utils import approx_equal\n\n\nclass TestZeroLazyTensor(unittest.TestCase):\n    def test_evaluate(self):\n        lv = ZeroLazyTensor(5, 4, 3)\n        actual = torch.zeros(5, 4, 3)\n        res = lv.evaluate()\n        self.assertLess(torch.norm(res - actual), 1e-4)\n\n    def test_getitem(self):\n        lv = ZeroLazyTensor(5, 4, 3)\n\n        res_one = lv[0].evaluate()\n        self.assertLess(torch.norm(res_one - torch.zeros(4, 3)), 1e-4)\n        res_two = lv[:, 1, :]\n        self.assertLess(torch.norm(res_two - torch.zeros(5, 3)), 1e-4)\n        res_three = lv[:, :, 2]\n        self.assertLess(torch.norm(res_three - torch.zeros(5, 4)), 1e-4)\n\n    def test_getitem_complex(self):\n        lv = ZeroLazyTensor(5, 4, 3)\n\n        res_one = lv[[0, 1]].evaluate()\n        self.assertLess(torch.norm(res_one - torch.zeros(2, 4, 3)), 1e-4)\n        res_two = lv[:, [0, 1], :].evaluate()\n        self.assertLess(torch.norm(res_two - torch.zeros(5, 2, 3)), 1e-4)\n        res_three = lv[:, :, [0, 2]].evaluate()\n        self.assertLess(torch.norm(res_three - torch.zeros(5, 4, 2)), 1e-4)\n\n    def test_getitem_ellipsis(self):\n        lv = ZeroLazyTensor(5, 4, 3)\n\n        res_one = lv[[0, 1]].evaluate()\n        self.assertLess(torch.norm(res_one - torch.zeros(2, 4, 3)), 1e-4)\n        res_two = lv[:, [0, 1], ...].evaluate()\n        self.assertLess(torch.norm(res_two - torch.zeros(5, 2, 3)), 1e-4)\n        res_three = lv[..., [0, 2]].evaluate()\n        self.assertLess(torch.norm(res_three - torch.zeros(5, 4, 2)), 1e-4)\n\n    def test_get_item_tensor_index(self):\n        # Tests the default LV.__getitem__ behavior\n        lazy_tensor = ZeroLazyTensor(5, 5)\n        evaluated = lazy_tensor.evaluate()\n\n        index = (torch.tensor([0, 0, 1, 2]), torch.tensor([0, 1, 0, 2]))\n        self.assertTrue(approx_equal(lazy_tensor[index], evaluated[index]))\n        index = (torch.tensor([0, 0, 1, 2]), slice(None, None, None))\n        self.assertTrue(approx_equal(lazy_tensor[index].evaluate(), evaluated[index]))\n        index = (slice(None, None, None), torch.tensor([0, 0, 1, 2]))\n        self.assertTrue(approx_equal(lazy_tensor[index].evaluate(), evaluated[index]))\n        index = (Ellipsis, slice(None, None, None), torch.tensor([0, 0, 1, 2]))\n        self.assertTrue(approx_equal(lazy_tensor[index].evaluate(), evaluated[index]))\n        index = (Ellipsis, torch.tensor([0, 0, 1, 2]))\n        self.assertTrue(approx_equal(lazy_tensor[index].evaluate(), evaluated[index]))\n\n    def test_get_item_tensor_index_on_batch(self):\n        # Tests the default LV.__getitem__ behavior\n        lazy_tensor = ZeroLazyTensor(3, 5, 5)\n        evaluated = lazy_tensor.evaluate()\n\n        index = (torch.tensor([0, 1, 1, 0]), torch.tensor([0, 1, 0, 2]), torch.tensor([1, 2, 0, 1]))\n        self.assertTrue(approx_equal(lazy_tensor[index], evaluated[index]))\n        index = (torch.tensor([0, 1, 1, 0]), torch.tensor([0, 1, 0, 2]), slice(None, None, None))\n        self.assertTrue(approx_equal(lazy_tensor[index], evaluated[index]))\n        index = (torch.tensor([0, 1, 1]), slice(None, None, None), torch.tensor([0, 1, 2]))\n        self.assertTrue(approx_equal(lazy_tensor[index], evaluated[index]))\n        index = (slice(None, None, None), torch.tensor([0, 1, 1, 0]), torch.tensor([0, 1, 0, 2]))\n        self.assertTrue(approx_equal(lazy_tensor[index], evaluated[index]))\n        index = (torch.tensor([0, 0, 1, 1]), slice(None, None, None), slice(None, None, None))\n        self.assertTrue(approx_equal(lazy_tensor[index].evaluate(), evaluated[index]))\n        index = (slice(None, None, None), torch.tensor([0, 0, 1, 2]), torch.tensor([0, 0, 1, 1]))\n        self.assertTrue(approx_equal(lazy_tensor[index], evaluated[index]))\n        index = (torch.tensor([0, 1, 1, 0]), torch.tensor([0, 1, 0, 2]), slice(None, None, None))\n        self.assertTrue(approx_equal(lazy_tensor[index], evaluated[index]))\n        index = (torch.tensor([0, 0, 1, 0]), slice(None, None, None), torch.tensor([0, 0, 1, 1]))\n        self.assertTrue(approx_equal(lazy_tensor[index], evaluated[index]))\n        index = (Ellipsis, torch.tensor([0, 1, 1, 0]))\n        self.assertTrue(approx_equal(lazy_tensor[index].evaluate(), evaluated[index]))\n\n    def test_add_diag(self):\n        diag = torch.tensor(1.5)\n        res = ZeroLazyTensor(5, 5).add_diag(diag).evaluate()\n        actual = torch.eye(5).mul(1.5)\n        self.assertTrue(approx_equal(res, actual))\n\n        diag = torch.tensor([1.5])\n        res = ZeroLazyTensor(5, 5).add_diag(diag).evaluate()\n        actual = torch.eye(5).mul(1.5)\n        self.assertTrue(approx_equal(res, actual))\n\n        diag = torch.tensor([1.5, 1.3, 1.2, 1.1, 2.0])\n        res = ZeroLazyTensor(5, 5).add_diag(diag).evaluate()\n        actual = diag.diag()\n        self.assertTrue(approx_equal(res, actual))\n\n        diag = torch.tensor(1.5)\n        res = ZeroLazyTensor(2, 5, 5).add_diag(diag).evaluate()\n        actual = torch.eye(5).unsqueeze(0).repeat(2, 1, 1).mul(1.5)\n        self.assertTrue(approx_equal(res, actual))\n\n        diag = torch.tensor([1.5])\n        res = ZeroLazyTensor(2, 5, 5).add_diag(diag).evaluate()\n        actual = torch.eye(5).unsqueeze(0).repeat(2, 1, 1).mul(1.5)\n        self.assertTrue(approx_equal(res, actual))\n\n        diag = torch.tensor([1.5, 1.3, 1.2, 1.1, 2.0])\n        res = ZeroLazyTensor(2, 5, 5).add_diag(diag).evaluate()\n        actual = diag.diag().unsqueeze(0).repeat(2, 1, 1)\n        self.assertTrue(approx_equal(res, actual))\n\n        diag = torch.tensor([[1.5, 1.3, 1.2, 1.1, 2.0], [0, 1, 2, 1, 1]])\n        res = ZeroLazyTensor(2, 5, 5).add_diag(diag).evaluate()\n        actual = torch.cat([diag[0].diag().unsqueeze(0), diag[1].diag().unsqueeze(0)])\n        self.assertTrue(approx_equal(res, actual))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/likelihoods/__init__.py,0,b'#!/usr/bin/env python3\n'
test/likelihoods/test_bernoulli_likelihood.py,8,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.likelihoods import BernoulliLikelihood, _OneDimensionalLikelihood\nfrom gpytorch.test.base_likelihood_test_case import BaseLikelihoodTestCase\n\n\nclass TestBernoulliLikelihood(BaseLikelihoodTestCase, unittest.TestCase):\n    seed = 1\n\n    def _create_targets(self, batch_shape=torch.Size([])):\n        res = torch.randn(*batch_shape, 5).gt(0).float()\n        return res\n\n    def _test_log_marginal(self, batch_shape):\n        # Overwriting this because use use probit, not logit\n        # The values are close, but not exact\n        # So we use looser checks\n        likelihood = self.create_likelihood()\n        input = self._create_marginal_input(batch_shape)\n        target = self._create_targets(batch_shape)\n        output = likelihood.log_marginal(target, input)\n\n        self.assertTrue(torch.is_tensor(output))\n        self.assertEqual(output.shape, batch_shape + torch.Size([5]))\n        default_log_prob = _OneDimensionalLikelihood.log_marginal(likelihood, target, input)\n        self.assertAllClose(output.sum(-1), default_log_prob.sum(-1), rtol=0.25, atol=0.1)\n\n    def _test_log_prob(self, batch_shape):\n        # Overwriting this because use use probit, not logit\n        # The values are close, but not exact\n        # So we use looser checks\n        likelihood = self.create_likelihood()\n        input = self._create_marginal_input(batch_shape)\n        target = self._create_targets(batch_shape)\n        output = likelihood.expected_log_prob(target, input)\n\n        self.assertTrue(torch.is_tensor(output))\n        self.assertEqual(output.shape, batch_shape + torch.Size([5]))\n        default_log_prob = _OneDimensionalLikelihood.expected_log_prob(likelihood, target, input)\n        self.assertAllClose(output.sum(-1), default_log_prob.sum(-1), rtol=0.25, atol=0.1)\n\n    def create_likelihood(self):\n        return BernoulliLikelihood()\n'"
test/likelihoods/test_gaussian_likelihood.py,23,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.lazy import DiagLazyTensor\nfrom gpytorch.likelihoods import FixedNoiseGaussianLikelihood, GaussianLikelihood\nfrom gpytorch.likelihoods.noise_models import FixedGaussianNoise\nfrom gpytorch.test.base_likelihood_test_case import BaseLikelihoodTestCase\n\n\nclass TestGaussianLikelihood(BaseLikelihoodTestCase, unittest.TestCase):\n    seed = 0\n\n    def create_likelihood(self):\n        return GaussianLikelihood()\n\n\nclass TestGaussianLikelihoodBatch(TestGaussianLikelihood):\n    seed = 0\n\n    def create_likelihood(self):\n        return GaussianLikelihood(batch_shape=torch.Size([3]))\n\n    def test_nonbatch(self):\n        pass\n\n\nclass TestGaussianLikelihoodMultiBatch(TestGaussianLikelihood):\n    seed = 0\n\n    def create_likelihood(self):\n        return GaussianLikelihood(batch_shape=torch.Size([2, 3]))\n\n    def test_nonbatch(self):\n        pass\n\n    def test_batch(self):\n        pass\n\n\nclass TestFixedNoiseGaussianLikelihood(BaseLikelihoodTestCase, unittest.TestCase):\n    def create_likelihood(self):\n        noise = 0.1 + torch.rand(5)\n        return FixedNoiseGaussianLikelihood(noise=noise)\n\n    def test_fixed_noise_gaussian_likelihood(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            noise = 0.1 + torch.rand(4, device=device, dtype=dtype)\n            lkhd = FixedNoiseGaussianLikelihood(noise=noise)\n            # test basics\n            self.assertIsInstance(lkhd.noise_covar, FixedGaussianNoise)\n            self.assertTrue(torch.equal(noise, lkhd.noise))\n            new_noise = 0.1 + torch.rand(4, device=device, dtype=dtype)\n            lkhd.noise = new_noise\n            self.assertTrue(torch.equal(lkhd.noise, new_noise))\n            # test __call__\n            mean = torch.zeros(4, device=device, dtype=dtype)\n            covar = DiagLazyTensor(torch.ones(4, device=device, dtype=dtype))\n            mvn = MultivariateNormal(mean, covar)\n            out = lkhd(mvn)\n            self.assertTrue(torch.allclose(out.variance, 1 + new_noise))\n            # things should break if dimensions mismatch\n            mean = torch.zeros(5, device=device, dtype=dtype)\n            covar = DiagLazyTensor(torch.ones(5, device=device, dtype=dtype))\n            mvn = MultivariateNormal(mean, covar)\n            with self.assertWarns(UserWarning):\n                lkhd(mvn)\n            # test __call__ w/ observation noise\n            obs_noise = 0.1 + torch.rand(5, device=device, dtype=dtype)\n            out = lkhd(mvn, noise=obs_noise)\n            self.assertTrue(torch.allclose(out.variance, 1 + obs_noise))\n\n\nclass TestFixedNoiseGaussianLikelihoodBatch(BaseLikelihoodTestCase, unittest.TestCase):\n    def create_likelihood(self):\n        noise = 0.1 + torch.rand(3, 5)\n        return FixedNoiseGaussianLikelihood(noise=noise)\n\n    def test_nonbatch(self):\n        pass\n\n\nclass TestFixedNoiseGaussianLikelihoodMultiBatch(BaseLikelihoodTestCase, unittest.TestCase):\n    def create_likelihood(self):\n        noise = 0.1 + torch.rand(2, 3, 5)\n        return FixedNoiseGaussianLikelihood(noise=noise)\n\n    def test_nonbatch(self):\n        pass\n\n    def test_batch(self):\n        pass\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/likelihoods/test_general_multitask_gaussian_likelihood.py,18,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport unittest\nfrom math import pi\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.distributions import MultitaskMultivariateNormal\nfrom gpytorch.kernels import MultitaskKernel, RBFKernel\nfrom gpytorch.likelihoods import MultitaskGaussianLikelihoodKronecker\nfrom gpytorch.means import ConstantMean, MultitaskMean\n\n# Simple training data: let\'s try to learn a sine function\ntrain_x = torch.linspace(0, 1, 100)\nlatent_error = torch.randn(train_x.size()) * 0.5\n\n# y1 function is sin(2*pi*x) with noise N(0, 0.04)\ntrain_y1 = torch.sin(train_x * (2 * pi)) + latent_error + torch.randn(train_x.size()) * 0.1\n# y2 function is cos(2*pi*x) with noise N(0, 0.04)\ntrain_y2 = torch.cos(train_x * (2 * pi)) + latent_error + torch.randn(train_x.size()) * 0.1\n\n# Create a train_y which interleaves the two\ntrain_y = torch.stack([train_y1, train_y2], -1)\n\n\nclass MultitaskGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = MultitaskMean(ConstantMean(), num_tasks=2)\n        self_covar_module = RBFKernel()\n        self.covar_module = MultitaskKernel(self_covar_module, num_tasks=2, rank=1)\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return MultitaskMultivariateNormal(mean_x, covar_x)\n\n\nclass TestMultiTaskGPRegression(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_multitask_low_rank_noise_covar(self):\n        likelihood = MultitaskGaussianLikelihoodKronecker(num_tasks=2, rank=1)\n        model = MultitaskGPModel(train_x, train_y, likelihood)\n        # Find optimal model hyperparameters\n        model.train()\n        likelihood.train()\n\n        # Use the adam optimizer\n        optimizer = torch.optim.Adam([{""params"": model.parameters()}], lr=0.1)  # Includes GaussianLikelihood parameters\n\n        # ""Loss"" for GPs - the marginal log likelihood\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n        n_iter = 50\n        for _ in range(n_iter):\n            # Zero prev backpropped gradients\n            optimizer.zero_grad()\n            # Make predictions from training data\n            # Again, note feeding duplicated x_data and indices indicating which task\n            output = model(train_x)\n            # TODO: Fix this view call!!\n            loss = -mll(output, train_y)\n            loss.backward()\n            optimizer.step()\n\n        # Test the model\n        model.eval()\n        likelihood.eval()\n\n        num_tasks = 2\n        task_noise_covar_factor = likelihood.task_noise_covar_factor\n        noise = likelihood.noise\n        task_noise_covar = task_noise_covar_factor.matmul(\n            task_noise_covar_factor.transpose(-1, -2)\n        ) + noise * torch.eye(num_tasks)\n\n        self.assertGreater(task_noise_covar[0, 1].item(), 0.05)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/likelihoods/test_multitask_gaussian_likelihood.py,13,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.distributions import MultitaskMultivariateNormal\nfrom gpytorch.lazy import KroneckerProductLazyTensor, RootLazyTensor\nfrom gpytorch.likelihoods import MultitaskGaussianLikelihood\nfrom gpytorch.test.base_likelihood_test_case import BaseLikelihoodTestCase\n\n\nclass TestMultitaskGaussianLikelihood(BaseLikelihoodTestCase, unittest.TestCase):\n    seed = 2\n\n    def _create_conditional_input(self, batch_shape=torch.Size([])):\n        return torch.randn(*batch_shape, 5, 4)\n\n    def _create_marginal_input(self, batch_shape=torch.Size([])):\n        mat = torch.randn(*batch_shape, 5, 5)\n        mat2 = torch.randn(*batch_shape, 4, 4)\n        covar = KroneckerProductLazyTensor(RootLazyTensor(mat), RootLazyTensor(mat2))\n        return MultitaskMultivariateNormal(torch.randn(*batch_shape, 5, 4), covar)\n\n    def _create_targets(self, batch_shape=torch.Size([])):\n        return torch.randn(*batch_shape, 5, 4)\n\n    def create_likelihood(self):\n        return MultitaskGaussianLikelihood(num_tasks=4, rank=2)\n\n\nclass TestMultitaskGaussianLikelihoodBatch(TestMultitaskGaussianLikelihood):\n    seed = 0\n\n    def create_likelihood(self):\n        return MultitaskGaussianLikelihood(num_tasks=4, rank=2, batch_shape=torch.Size([3]))\n\n    def test_nonbatch(self):\n        pass\n'"
test/likelihoods/test_softmax_likelihood.py,16,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\nfrom torch.distributions import Distribution\n\nfrom gpytorch.distributions import MultitaskMultivariateNormal, MultivariateNormal\nfrom gpytorch.likelihoods import SoftmaxLikelihood\nfrom gpytorch.test.base_likelihood_test_case import BaseLikelihoodTestCase\n\n\nclass TestSoftmaxLikelihood(BaseLikelihoodTestCase, unittest.TestCase):\n    seed = 0\n\n    def _create_conditional_input(self, batch_shape=torch.Size([])):\n        return torch.randn(*batch_shape, 5, 6)\n\n    def _create_marginal_input(self, batch_shape=torch.Size([])):\n        mat = torch.randn(*batch_shape, 6, 5, 5)\n        return MultitaskMultivariateNormal.from_batch_mvn(\n            MultivariateNormal(torch.randn(*batch_shape, 6, 5), mat @ mat.transpose(-1, -2))\n        )\n\n    def _create_targets(self, batch_shape=torch.Size([])):\n        return torch.distributions.Categorical(probs=torch.tensor([0.25, 0.25, 0.25, 0.25])).sample(\n            torch.Size([*batch_shape, 5])\n        )\n\n    def create_likelihood(self):\n        return SoftmaxLikelihood(num_features=6, num_classes=4)\n\n    def _test_conditional(self, batch_shape):\n        likelihood = self.create_likelihood()\n        input = self._create_conditional_input(batch_shape)\n        output = likelihood(input)\n\n        self.assertIsInstance(output, Distribution)\n        self.assertEqual(output.sample().shape, torch.Size([*batch_shape, 5]))\n\n    def _test_log_prob(self, batch_shape):\n        likelihood = self.create_likelihood()\n        input = self._create_marginal_input(batch_shape)\n        target = self._create_targets(batch_shape)\n        output = likelihood.expected_log_prob(target, input)\n\n        self.assertTrue(torch.is_tensor(output))\n        self.assertEqual(output.shape, batch_shape + torch.Size([5]))\n\n    def _test_marginal(self, batch_shape):\n        likelihood = self.create_likelihood()\n        input = self._create_marginal_input(batch_shape)\n        output = likelihood(input)\n\n        self.assertTrue(isinstance(output, Distribution))\n        self.assertEqual(output.sample().shape[-len(batch_shape) - 1 :], torch.Size([*batch_shape, 5]))\n\n\nclass TestSoftmaxLikelihoodNoMixing(TestSoftmaxLikelihood):\n    seed = 0\n\n    def create_likelihood(self):\n        return SoftmaxLikelihood(num_features=6, num_classes=6, mixing_weights=False)\n'"
test/means/__init__.py,0,b'#!/usr/bin/env python3\n'
test/means/test_constant_mean.py,4,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.means import ConstantMean\nfrom gpytorch.test.base_mean_test_case import BaseMeanTestCase\n\n\nclass TestConstantMean(BaseMeanTestCase, unittest.TestCase):\n    def create_mean(self):\n        return ConstantMean()\n\n\nclass TestConstantMeanBatch(BaseMeanTestCase, unittest.TestCase):\n    batch_shape = torch.Size([3])\n\n    def create_mean(self):\n        return ConstantMean(batch_shape=self.__class__.batch_shape)\n\n\nclass TestConstantMeanMultiBatch(BaseMeanTestCase, unittest.TestCase):\n    batch_shape = torch.Size([2, 3])\n\n    def create_mean(self):\n        return ConstantMean(batch_shape=self.__class__.batch_shape)\n'"
test/means/test_constant_mean_grad.py,16,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.means import ConstantMeanGrad\nfrom gpytorch.test.base_mean_test_case import BaseMeanTestCase\n\n\nclass TestConstantMeanGrad(BaseMeanTestCase, unittest.TestCase):\n    batch_shape = None\n\n    def create_mean(self):\n        return ConstantMeanGrad(batch_shape=self.__class__.batch_shape or torch.Size())\n\n    def test_forward_vec(self):\n        test_x = torch.randn(4)\n        mean = self.create_mean()\n        if self.__class__.batch_shape is None:\n            self.assertEqual(mean(test_x).shape, torch.Size([4, 2]))\n        else:\n            self.assertEqual(mean(test_x).shape, torch.Size([*self.__class__.batch_shape, 4, 2]))\n        self.assertEqual(mean(test_x)[..., 1:].norm().item(), 0)\n\n    def test_forward_mat(self):\n        test_x = torch.randn(4, 3)\n        mean = self.create_mean()\n        if self.__class__.batch_shape is None:\n            self.assertEqual(mean(test_x).shape, torch.Size([4, 4]))\n        else:\n            self.assertEqual(mean(test_x).shape, torch.Size([*self.__class__.batch_shape, 4, 4]))\n        self.assertEqual(mean(test_x)[..., 1:].norm().item(), 0)\n\n    def test_forward_mat_batch(self):\n        test_x = torch.randn(3, 4, 3)\n        mean = self.create_mean()\n        if self.__class__.batch_shape is None:\n            self.assertEqual(mean(test_x).shape, torch.Size([3, 4, 4]))\n        else:\n            self.assertEqual(mean(test_x).shape, torch.Size([*self.__class__.batch_shape, 4, 4]))\n        self.assertEqual(mean(test_x)[..., 1:].norm().item(), 0)\n\n    def test_forward_mat_multi_batch(self):\n        test_x = torch.randn(2, 3, 4, 3)\n        mean = self.create_mean()\n        self.assertEqual(mean(test_x).shape, torch.Size([2, 3, 4, 4]))\n        self.assertEqual(mean(test_x)[..., 1:].norm().item(), 0)\n\n\nclass TestConstantMeanGradBatch(TestConstantMeanGrad):\n    batch_shape = torch.Size([3])\n\n\nclass TestConstantMeanGradMultiBatch(TestConstantMeanGrad):\n    batch_shape = torch.Size([2, 3])\n'"
test/means/test_linear_mean.py,13,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.means import LinearMean\nfrom gpytorch.test.base_mean_test_case import BaseMeanTestCase\n\n\nclass TestLinearMean(BaseMeanTestCase, unittest.TestCase):\n    def create_mean(self, input_size=1, batch_shape=torch.Size(), bias=True, **kwargs):\n        return LinearMean(input_size=input_size, batch_shape=batch_shape, bias=bias)\n\n    def forward_vec(self):\n        n = 4\n        test_x = torch.randn(n)\n        mean = self.create_mean(input_size=1)\n        self.assertEqual(mean(test_x).shape, torch.Size([4]))\n\n    def test_forward_mat(self):\n        n, d = 4, 5\n        test_x = torch.randn(n, d)\n        mean = self.create_mean(d)\n        self.assertEqual(mean(test_x).shape, torch.Size([n]))\n\n    def test_forward_mat_batch(self):\n        b, n, d = torch.Size([3]), 4, 5\n        test_x = torch.randn(*b, n, d)\n        mean = self.create_mean(d, b)\n        self.assertEqual(mean(test_x).shape, torch.Size([*b, n]))\n\n    def test_forward_mat_multi_batch(self):\n        b, n, d = torch.Size([2, 3]), 4, 5\n        test_x = torch.randn(*b, n, d)\n        mean = self.create_mean(d, b)\n        self.assertEqual(mean(test_x).shape, torch.Size([*b, n]))\n'"
test/means/test_multitask_mean.py,12,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.means import ConstantMean, MultitaskMean, ZeroMean\nfrom gpytorch.test.base_mean_test_case import BaseMeanTestCase\n\n\nclass TestMultitaskMean(BaseMeanTestCase, unittest.TestCase):\n    def create_mean(self):\n        return MultitaskMean([ConstantMean(), ZeroMean(), ZeroMean()], num_tasks=3)\n\n    def test_forward_vec(self):\n        test_x = torch.randn(4)\n        mean = self.create_mean()\n        self.assertEqual(mean(test_x).shape, torch.Size([4, 3]))\n        self.assertEqual(mean(test_x)[..., 1:].norm().item(), 0)\n\n    def test_forward_mat(self):\n        test_x = torch.randn(4, 3)\n        mean = self.create_mean()\n        self.assertEqual(mean(test_x).shape, torch.Size([4, 3]))\n        self.assertEqual(mean(test_x)[..., 1:].norm().item(), 0)\n\n    def test_forward_mat_batch(self):\n        test_x = torch.randn(3, 4, 3)\n        mean = self.create_mean()\n        self.assertEqual(mean(test_x).shape, torch.Size([3, 4, 3]))\n        self.assertEqual(mean(test_x)[..., 1:].norm().item(), 0)\n\n    def test_forward_mat_multi_batch(self):\n        test_x = torch.randn(2, 3, 4, 3)\n        mean = self.create_mean()\n        self.assertEqual(mean(test_x).shape, torch.Size([2, 3, 4, 3]))\n        self.assertEqual(mean(test_x)[..., 1:].norm().item(), 0)\n\n\nclass TestMultitaskMeanBatch(TestMultitaskMean):\n    def create_mean(self):\n        return MultitaskMean([ConstantMean(batch_shape=torch.Size([3])), ZeroMean(), ZeroMean()], num_tasks=3)\n\n    def test_forward_vec(self):\n        pass\n\n    def test_forward_mat(self):\n        pass\n\n\nclass TestMultitaskMeanMultiBatch(TestMultitaskMean):\n    def create_mean(self):\n        return MultitaskMean([ConstantMean(batch_shape=torch.Size([2, 3])), ZeroMean(), ZeroMean()], num_tasks=3)\n\n    def test_forward_vec(self):\n        pass\n\n    def test_forward_mat(self):\n        pass\n\n    def test_forward_mat_batch(self):\n        pass\n'"
test/means/test_zero_mean.py,2,"b'#!/usr/bin/env python3\n\nimport unittest\n\nfrom gpytorch.means import ZeroMean\nfrom gpytorch.test.base_mean_test_case import BaseMeanTestCase\n\n\nclass TestZeroMean(BaseMeanTestCase, unittest.TestCase):\n    def create_mean(self):\n        return ZeroMean()\n'"
test/models/__init__.py,0,b'#!/usr/bin/env python3\n'
test/models/test_exact_gp.py,29,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.models import ExactGP\nfrom gpytorch.test.model_test_case import BaseModelTestCase\n\n\nclass ExactGPModel(ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass InterpolatedExactGPModel(ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.GridInterpolationKernel(\n            gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()), grid_size=128, num_dims=1\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass SumExactGPModel(ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super().__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        covar_a = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n        covar_b = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))\n        self.covar_module = covar_a + covar_b\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n\nclass TestExactGP(BaseModelTestCase, unittest.TestCase):\n    def create_model(self, train_x, train_y, likelihood):\n        model = ExactGPModel(train_x, train_y, likelihood)\n        return model\n\n    def create_test_data(self):\n        return torch.randn(50, 1)\n\n    def create_likelihood_and_labels(self):\n        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n        labels = torch.randn(50) + 2\n        return likelihood, labels\n\n    def create_batch_test_data(self, batch_shape=torch.Size([3])):\n        return torch.randn(*batch_shape, 50, 1)\n\n    def create_batch_likelihood_and_labels(self, batch_shape=torch.Size([3])):\n        likelihood = gpytorch.likelihoods.GaussianLikelihood(batch_shape=batch_shape)\n        labels = torch.randn(*batch_shape, 50) + 2\n        return likelihood, labels\n\n    def test_forward_eval_fast(self):\n        with gpytorch.settings.max_eager_kernel_size(1), gpytorch.settings.fast_pred_var(True):\n            self.test_forward_eval()\n\n    def test_batch_forward_eval_fast(self):\n        with gpytorch.settings.max_eager_kernel_size(1), gpytorch.settings.fast_pred_var(True):\n            self.test_batch_forward_eval()\n\n    def test_multi_batch_forward_eval_fast(self):\n        with gpytorch.settings.max_eager_kernel_size(1), gpytorch.settings.fast_pred_var(True):\n            self.test_multi_batch_forward_eval()\n\n    def test_batch_forward_then_nonbatch_forward_eval(self):\n        batch_data = self.create_batch_test_data()\n        likelihood, labels = self.create_batch_likelihood_and_labels()\n        model = self.create_model(batch_data, labels, likelihood)\n        model.eval()\n        output = model(batch_data)\n\n        # Smoke test derivatives working\n        output.mean.sum().backward()\n\n        self.assertTrue(output.lazy_covariance_matrix.dim() == 3)\n        self.assertTrue(output.lazy_covariance_matrix.size(-1) == batch_data.size(-2))\n        self.assertTrue(output.lazy_covariance_matrix.size(-2) == batch_data.size(-2))\n\n        # Create non-batch data\n        data = self.create_test_data()\n        output = model(data)\n        self.assertTrue(output.lazy_covariance_matrix.dim() == 3)\n        self.assertTrue(output.lazy_covariance_matrix.size(-1) == data.size(-2))\n        self.assertTrue(output.lazy_covariance_matrix.size(-2) == data.size(-2))\n\n        # Smoke test derivatives working\n        output.mean.sum().backward()\n\n    def test_batch_forward_then_different_batch_forward_eval(self):\n        non_batch_data = self.create_test_data()\n        likelihood, labels = self.create_likelihood_and_labels()\n        model = self.create_model(non_batch_data, labels, likelihood)\n        model.eval()\n\n        # Batch size 3\n        batch_data = self.create_batch_test_data()\n        output = model(batch_data)\n        self.assertTrue(output.lazy_covariance_matrix.dim() == 3)\n        self.assertTrue(output.lazy_covariance_matrix.size(-1) == batch_data.size(-2))\n        self.assertTrue(output.lazy_covariance_matrix.size(-2) == batch_data.size(-2))\n\n        # Now Batch size 2\n        batch_data = self.create_batch_test_data(batch_shape=torch.Size([2]))\n        output = model(batch_data)\n        self.assertTrue(output.lazy_covariance_matrix.dim() == 3)\n        self.assertTrue(output.lazy_covariance_matrix.size(-1) == batch_data.size(-2))\n        self.assertTrue(output.lazy_covariance_matrix.size(-2) == batch_data.size(-2))\n\n        # Now 3 again\n        batch_data = self.create_batch_test_data()\n        output = model(batch_data)\n        self.assertTrue(output.lazy_covariance_matrix.dim() == 3)\n        self.assertTrue(output.lazy_covariance_matrix.size(-1) == batch_data.size(-2))\n        self.assertTrue(output.lazy_covariance_matrix.size(-2) == batch_data.size(-2))\n\n        # Now 1\n        batch_data = self.create_batch_test_data(batch_shape=torch.Size([1]))\n        output = model(batch_data)\n        self.assertTrue(output.lazy_covariance_matrix.dim() == 3)\n        self.assertTrue(output.lazy_covariance_matrix.size(-1) == batch_data.size(-2))\n        self.assertTrue(output.lazy_covariance_matrix.size(-2) == batch_data.size(-2))\n\n    def test_prior_mode(self):\n        train_data = self.create_test_data()\n        likelihood, labels = self.create_likelihood_and_labels()\n        prior_model = self.create_model(None, None, likelihood)\n        model = self.create_model(train_data, labels, likelihood)\n        prior_model.eval()\n        model.eval()\n\n        test_data = self.create_test_data()\n        prior_out = prior_model(test_data)\n        with gpytorch.settings.prior_mode(True):\n            prior_out_cm = model(test_data)\n        self.assertTrue(torch.allclose(prior_out.mean, prior_out_cm.mean))\n        self.assertTrue(torch.allclose(prior_out.covariance_matrix, prior_out_cm.covariance_matrix))\n\n\nclass TestInterpolatedExactGP(TestExactGP):\n    def create_model(self, train_x, train_y, likelihood):\n        model = InterpolatedExactGPModel(train_x, train_y, likelihood)\n        return model\n\n\nclass TestSumExactGP(TestExactGP):\n    def create_model(self, train_x, train_y, likelihood):\n        model = SumExactGPModel(train_x, train_y, likelihood)\n        return model\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/models/test_model_list.py,14,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.likelihoods import FixedNoiseGaussianLikelihood\nfrom gpytorch.models import IndependentModelList\n\nfrom .test_exact_gp import TestExactGP\n\n\nclass TestModelListGP(unittest.TestCase):\n    def create_model(self, fixed_noise=False):\n        data = TestExactGP.create_test_data(self)\n        likelihood, labels = TestExactGP.create_likelihood_and_labels(self)\n        if fixed_noise:\n            noise = 0.1 + 0.2 * torch.rand_like(labels)\n            likelihood = FixedNoiseGaussianLikelihood(noise)\n        return TestExactGP.create_model(self, data, labels, likelihood)\n\n    def test_forward_eval(self):\n        models = [self.create_model() for _ in range(2)]\n        model = IndependentModelList(*models)\n        model.eval()\n        model(torch.rand(3))\n\n    def test_forward_eval_fixed_noise(self):\n        models = [self.create_model(fixed_noise=True) for _ in range(2)]\n        model = IndependentModelList(*models)\n        model.eval()\n        model(torch.rand(3))\n\n    def test_get_fantasy_model(self):\n        models = [self.create_model() for _ in range(2)]\n        model = IndependentModelList(*models)\n        model.eval()\n        model(torch.rand(3), torch.rand(3))\n        fant_x = [torch.randn(2), torch.randn(3)]\n        fant_y = [torch.randn(2), torch.randn(3)]\n        fmodel = model.get_fantasy_model(fant_x, fant_y)\n        fmodel(torch.randn(4))\n\n    def test_get_fantasy_model_fixed_noise(self):\n        models = [self.create_model(fixed_noise=True) for _ in range(2)]\n        model = IndependentModelList(*models)\n        model.eval()\n        model(torch.rand(3), torch.rand(3))\n        fant_x = [torch.randn(2), torch.randn(3)]\n        fant_y = [torch.randn(2), torch.randn(3)]\n        fant_noise = [0.1 * torch.ones(2), 0.1 * torch.ones(3)]\n        fmodel = model.get_fantasy_model(fant_x, fant_y, noise=fant_noise)\n        fmodel(torch.randn(4))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/models/test_variational_gp.py,18,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.models import ApproximateGP\nfrom gpytorch.test.model_test_case import VariationalModelTestCase\nfrom gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n\n\nclass GPClassificationModel(ApproximateGP):\n    def __init__(self, train_x, use_inducing=False):\n        variational_distribution = CholeskyVariationalDistribution(train_x.size(-2), batch_shape=train_x.shape[:-2])\n        inducing_points = torch.randn(50, train_x.size(-1)) if use_inducing else train_x\n        strategy_cls = VariationalStrategy\n        variational_strategy = strategy_cls(\n            self, inducing_points, variational_distribution, learn_inducing_locations=use_inducing\n        )\n        super(GPClassificationModel, self).__init__(variational_strategy)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n        return latent_pred\n\n\nclass TestVariationalGP(VariationalModelTestCase, unittest.TestCase):\n    def create_model(self, train_x, train_y, likelihood):\n        model = GPClassificationModel(train_x)\n        return model\n\n    def create_test_data(self):\n        return torch.randn(50, 1)\n\n    def create_likelihood_and_labels(self):\n        likelihood = gpytorch.likelihoods.GaussianLikelihood()\n        labels = torch.randn(50) + 2\n        return likelihood, labels\n\n    def create_batch_test_data(self, batch_shape=torch.Size([3])):\n        return torch.randn(*batch_shape, 50, 1)\n\n    def create_batch_likelihood_and_labels(self, batch_shape=torch.Size([3])):\n        likelihood = gpytorch.likelihoods.GaussianLikelihood(batch_shape=batch_shape)\n        labels = torch.randn(*batch_shape, 50) + 2\n        return likelihood, labels\n\n\nclass TestSVGPVariationalGP(TestVariationalGP):\n    def create_model(self, train_x, train_y, likelihood):\n        model = GPClassificationModel(train_x, use_inducing=True)\n        return model\n\n    def test_backward_train_nochol(self):\n        with gpytorch.settings.max_cholesky_size(0):\n            self.test_backward_train()\n\n    def test_batch_backward_train_nochol(self):\n        with gpytorch.settings.max_cholesky_size(0):\n            self.test_batch_backward_train()\n\n    def test_multi_batch_backward_train_nochol(self):\n        with gpytorch.settings.max_cholesky_size(0):\n            self.test_multi_batch_backward_train()\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/priors/__init__.py,0,b'#!/usr/bin/env python3\n'
test/priors/test_gamma_prior.py,42,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\nfrom torch.distributions import Gamma\n\nfrom gpytorch.priors import GammaPrior\nfrom gpytorch.test.utils import least_used_cuda_device\n\n\nclass TestGammaPrior(unittest.TestCase):\n    def test_gamma_prior_to_gpu(self):\n        if torch.cuda.is_available():\n            prior = GammaPrior(1.0, 1.0).cuda()\n            self.assertEqual(prior.concentration.device.type, ""cuda"")\n            self.assertEqual(prior.rate.device.type, ""cuda"")\n\n    def test_gamma_prior_validate_args(self):\n        with self.assertRaises(ValueError):\n            GammaPrior(0, 1, validate_args=True)\n        with self.assertRaises(ValueError):\n            GammaPrior(1, 0, validate_args=True)\n\n    def test_gamma_prior_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        concentration = torch.tensor(1.0, device=device)\n        rate = torch.tensor(1.0, device=device)\n        prior = GammaPrior(concentration, rate)\n        dist = Gamma(concentration, rate)\n\n        t = torch.tensor(1.0, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.tensor([1.5, 0.5], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.tensor([[1.0, 0.5], [3.0, 0.25]], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n\n    def test_gamma_prior_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                return self.test_gamma_prior_log_prob(cuda=True)\n\n    def test_gamma_prior_log_prob_log_transform(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        concentration = torch.tensor(1.0, device=device)\n        rate = torch.tensor(1.0, device=device)\n        prior = GammaPrior(concentration, rate, transform=torch.exp)\n        dist = Gamma(concentration, rate)\n\n        t = torch.tensor(0.0, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t.exp())))\n        t = torch.tensor([-1, 0.5], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t.exp())))\n        t = torch.tensor([[-1, 0.5], [0.1, -2.0]], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t.exp())))\n\n    def test_gamma_prior_log_prob_log_transform_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                return self.test_gamma_prior_log_prob_log_transform(cuda=True)\n\n    def test_gamma_prior_batch_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n\n        concentration = torch.tensor([1.0, 2.0], device=device)\n        rate = torch.tensor([1.0, 2.0], device=device)\n        prior = GammaPrior(concentration, rate)\n        dist = Gamma(concentration, rate)\n        t = torch.ones(2, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.ones(2, 2, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.ones(3, device=device))\n\n        mean = torch.tensor([[1.0, 2.0], [0.5, 3.0]], device=device)\n        variance = torch.tensor([[1.0, 2.0], [0.5, 1.0]], device=device)\n        prior = GammaPrior(mean, variance)\n        dist = Gamma(mean, variance)\n        t = torch.ones(2, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.ones(2, 2, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.ones(3, device=device))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.ones(2, 3, device=device))\n\n    def test_gamma_prior_batch_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                return self.test_gamma_prior_batch_log_prob(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/priors/test_horseshoe_prior.py,23,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.priors import HorseshoePrior\n\n\nclass TestHorseshoePrior(unittest.TestCase):\n    def test_horseshoe_prior_to_gpu(self):\n        if torch.cuda.is_available():\n            prior = HorseshoePrior(0.1).cuda()\n            self.assertEqual(prior.scale.device.type, ""cuda"")\n\n    def test_horseshoe_prior_validate_args(self):\n        with self.assertRaises(ValueError):\n            HorseshoePrior(-0.1, validate_args=True)\n\n    def test_horseshoe_prior_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        scale = torch.tensor(0.1, device=device)\n        prior = HorseshoePrior(scale)\n        t = torch.rand(3, device=device)\n        prior.log_prob(t)\n\n    def test_horseshoe_prior_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            return self.test_horseshoe_prior_log_prob(cuda=True)\n\n    def test_horseshoe_prior_log_prob_log_transform(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        scale = torch.tensor(0.1, device=device)\n        prior = HorseshoePrior(scale)\n        prior_tf = HorseshoePrior(scale, transform=torch.exp)\n        t = torch.tensor(0.5, device=device)\n        self.assertTrue(torch.equal(prior_tf.log_prob(t), prior.log_prob(t.exp())))\n\n    def test_horseshoe_prior_log_prob_log_transform_cuda(self):\n        if torch.cuda.is_available():\n            return self.test_horseshoe_prior_log_prob_log_transform(cuda=True)\n\n    def test_horseshoe_prior_batch_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n\n        scale = torch.tensor([0.1, 0.25], device=device)\n        prior = HorseshoePrior(scale)\n        t = torch.ones(2, device=device)\n        prior.log_prob(t)\n        t = torch.ones(2, 2, device=device)\n        prior.log_prob(t)\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.ones(3, device=device))\n\n        scale = torch.tensor([[0.1, 0.25], [0.3, 1.0]], device=device)\n        prior = HorseshoePrior(scale)\n        t = torch.ones(2, device=device)\n        prior.log_prob(t)\n        t = torch.ones(2, 2, device=device)\n        prior.log_prob(t)\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.ones(3, device=device))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.ones(2, 3, device=device))\n\n    def test_horseshoe_prior_batch_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            return self.test_horseshoe_prior_batch_log_prob(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/priors/test_lkj_prior.py,72,"b'#!/usr/bin/env python3\n\nimport unittest\nfrom math import exp\n\nimport torch\n\nfrom gpytorch.priors import LKJCholeskyFactorPrior, LKJCovariancePrior, LKJPrior, SmoothedBoxPrior\nfrom gpytorch.test.utils import approx_equal, least_used_cuda_device\n\n\nclass TestLKJPrior(unittest.TestCase):\n    def test_lkj_prior_to_gpu(self):\n        if torch.cuda.is_available():\n            prior = LKJPrior(2, 1.0).cuda()\n            self.assertEqual(prior.eta.device.type, ""cuda"")\n            self.assertEqual(prior.C.device.type, ""cuda"")\n\n    def test_lkj_prior_validate_args(self):\n        LKJPrior(2, 1.0, validate_args=True)\n        with self.assertRaises(ValueError):\n            LKJPrior(1.5, 1.0, validate_args=True)\n        with self.assertRaises(ValueError):\n            LKJPrior(2, -1.0, validate_args=True)\n\n    def test_lkj_prior_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        prior = LKJPrior(2, torch.tensor(0.5, device=device))\n\n        S = torch.eye(2, device=device)\n        self.assertAlmostEqual(prior.log_prob(S).item(), -1.86942, places=4)\n        S = torch.stack([S, torch.tensor([[1.0, 0.5], [0.5, 1]], device=S.device)])\n        self.assertTrue(approx_equal(prior.log_prob(S), torch.tensor([-1.86942, -1.72558], device=S.device)))\n        with self.assertRaises(ValueError):\n            prior.log_prob(torch.eye(3, device=device))\n\n        # For eta=1.0 log_prob is flat over all covariance matrices\n        prior = LKJPrior(2, torch.tensor(1.0, device=device))\n        self.assertTrue(torch.all(prior.log_prob(S) == prior.C))\n\n    def test_lkj_prior_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_lkj_prior_log_prob(cuda=True)\n\n    def test_lkj_prior_batch_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        prior = LKJPrior(2, torch.tensor([0.5, 1.5], device=device))\n\n        S = torch.eye(2, device=device)\n        self.assertTrue(approx_equal(prior.log_prob(S), torch.tensor([-1.86942, -0.483129], device=S.device)))\n        S = torch.stack([S, torch.tensor([[1.0, 0.5], [0.5, 1]], device=S.device)])\n        self.assertTrue(approx_equal(prior.log_prob(S), torch.tensor([-1.86942, -0.62697], device=S.device)))\n        with self.assertRaises(ValueError):\n            prior.log_prob(torch.eye(3, device=device))\n\n    def test_lkj_prior_batch_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_lkj_prior_batch_log_prob(cuda=True)\n\n\nclass TestLKJCholeskyFactorPrior(unittest.TestCase):\n    def test_lkj_cholesky_factor_prior_to_gpu(self):\n        if torch.cuda.is_available():\n            prior = LKJCholeskyFactorPrior(2, 1.0).cuda()\n            self.assertEqual(prior.eta.device.type, ""cuda"")\n            self.assertEqual(prior.C.device.type, ""cuda"")\n\n    def test_lkj_cholesky_factor_prior_validate_args(self):\n        LKJCholeskyFactorPrior(2, 1.0, validate_args=True)\n        with self.assertRaises(ValueError):\n            LKJCholeskyFactorPrior(1.5, 1.0, validate_args=True)\n        with self.assertRaises(ValueError):\n            LKJCholeskyFactorPrior(2, -1.0, validate_args=True)\n\n    def test_lkj_cholesky_factor_prior_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        prior = LKJCholeskyFactorPrior(2, torch.tensor(0.5, device=device))\n        S = torch.eye(2, device=device)\n        S_chol = torch.cholesky(S)\n        self.assertAlmostEqual(prior.log_prob(S_chol).item(), -1.86942, places=4)\n        S = torch.stack([S, torch.tensor([[1.0, 0.5], [0.5, 1]], device=S_chol.device)])\n        S_chol = torch.stack([torch.cholesky(Si) for Si in S])\n        self.assertTrue(approx_equal(prior.log_prob(S_chol), torch.tensor([-1.86942, -1.72558], device=S_chol.device)))\n        with self.assertRaises(ValueError):\n            prior.log_prob(torch.eye(3, device=device))\n\n        # For eta=1.0 log_prob is flat over all covariance matrices\n        prior = LKJCholeskyFactorPrior(2, torch.tensor(1.0, device=device))\n        self.assertTrue(torch.all(prior.log_prob(S_chol) == prior.C))\n\n    def test_lkj_cholesky_factor_prior_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_lkj_cholesky_factor_prior_log_prob(cuda=True)\n\n    def test_lkj_cholesky_factor_prior_batch_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        prior = LKJCholeskyFactorPrior(2, torch.tensor([0.5, 1.5], device=device))\n\n        S = torch.eye(2, device=device)\n        S_chol = torch.cholesky(S)\n        self.assertTrue(approx_equal(prior.log_prob(S_chol), torch.tensor([-1.86942, -0.483129], device=S_chol.device)))\n        S = torch.stack([S, torch.tensor([[1.0, 0.5], [0.5, 1]], device=S.device)])\n        S_chol = torch.stack([torch.cholesky(Si) for Si in S])\n        self.assertTrue(approx_equal(prior.log_prob(S_chol), torch.tensor([-1.86942, -0.62697], device=S_chol.device)))\n        with self.assertRaises(ValueError):\n            prior.log_prob(torch.eye(3, device=device))\n\n    def test_lkj_cholesky_factor_prior_batch_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_lkj_cholesky_factor_prior_batch_log_prob(cuda=True)\n\n\nclass TestLKJCovariancePrior(unittest.TestCase):\n    def test_lkj_covariance_prior_to_gpu(self):\n        if torch.cuda.is_available():\n            sd_prior = SmoothedBoxPrior(exp(-1), exp(1))\n            prior = LKJCovariancePrior(2, 1.0, sd_prior).cuda()\n            self.assertEqual(prior.correlation_prior.eta.device.type, ""cuda"")\n            self.assertEqual(prior.correlation_prior.C.device.type, ""cuda"")\n            self.assertEqual(prior.sd_prior.a.device.type, ""cuda"")\n\n    def test_lkj_covariance_prior_validate_args(self):\n        sd_prior = SmoothedBoxPrior(exp(-1), exp(1), validate_args=True)\n        LKJCovariancePrior(2, 1.0, sd_prior)\n        with self.assertRaises(ValueError):\n            LKJCovariancePrior(1.5, 1.0, sd_prior, validate_args=True)\n        with self.assertRaises(ValueError):\n            LKJCovariancePrior(2, -1.0, sd_prior, validate_args=True)\n\n    def test_lkj_covariance_prior_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        sd_prior = SmoothedBoxPrior(exp(-1), exp(1))\n        if cuda:\n            sd_prior = sd_prior.cuda()\n        prior = LKJCovariancePrior(2, torch.tensor(0.5, device=device), sd_prior)\n        S = torch.eye(2, device=device)\n        self.assertAlmostEqual(prior.log_prob(S).item(), -3.59981, places=4)\n        S = torch.stack([S, torch.tensor([[1.0, 0.5], [0.5, 1]], device=S.device)])\n        self.assertTrue(approx_equal(prior.log_prob(S), torch.tensor([-3.59981, -3.45597], device=S.device)))\n        with self.assertRaises(ValueError):\n            prior.log_prob(torch.eye(3, device=device))\n\n        # For eta=1.0 log_prob is flat over all covariance matrices\n        prior = LKJCovariancePrior(2, torch.tensor(1.0, device=device), sd_prior)\n        marginal_sd = torch.diagonal(S, dim1=-2, dim2=-1).sqrt()\n        log_prob_expected = prior.correlation_prior.C + prior.sd_prior.log_prob(marginal_sd)\n        self.assertTrue(approx_equal(prior.log_prob(S), log_prob_expected))\n\n    def test_lkj_covariance_prior_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_lkj_covariance_prior_log_prob(cuda=True)\n\n    def test_lkj_covariance_prior_log_prob_hetsd(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        a = torch.tensor([exp(-1), exp(-2)], device=device)\n        b = torch.tensor([exp(1), exp(2)], device=device)\n        sd_prior = SmoothedBoxPrior(a, b)\n        prior = LKJCovariancePrior(2, torch.tensor(0.5, device=device), sd_prior)\n        S = torch.eye(2, device=device)\n        self.assertAlmostEqual(prior.log_prob(S).item(), -4.71958, places=4)\n        S = torch.stack([S, torch.tensor([[1.0, 0.5], [0.5, 1]], device=S.device)])\n        self.assertTrue(approx_equal(prior.log_prob(S), torch.tensor([-4.71958, -4.57574], device=S.device)))\n        with self.assertRaises(ValueError):\n            prior.log_prob(torch.eye(3, device=device))\n\n        # For eta=1.0 log_prob is flat over all covariance matrices\n        prior = LKJCovariancePrior(2, torch.tensor(1.0, device=device), sd_prior)\n        marginal_sd = torch.diagonal(S, dim1=-2, dim2=-1).sqrt()\n        log_prob_expected = prior.correlation_prior.C + prior.sd_prior.log_prob(marginal_sd)\n        self.assertTrue(approx_equal(prior.log_prob(S), log_prob_expected))\n\n    def test_lkj_covariance_prior_log_prob_hetsd_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_lkj_covariance_prior_log_prob_hetsd(cuda=True)\n\n    def test_lkj_covariance_prior_batch_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        v = torch.ones(2, 1, device=device)\n        sd_prior = SmoothedBoxPrior(exp(-1) * v, exp(1) * v)\n        prior = LKJCovariancePrior(2, torch.tensor([0.5, 1.5], device=device), sd_prior)\n\n        S = torch.eye(2, device=device)\n        self.assertTrue(approx_equal(prior.log_prob(S), torch.tensor([-3.59981, -2.21351], device=S.device)))\n        S = torch.stack([S, torch.tensor([[1.0, 0.5], [0.5, 1]], device=S.device)])\n        self.assertTrue(approx_equal(prior.log_prob(S), torch.tensor([-3.59981, -2.35735], device=S.device)))\n        with self.assertRaises(ValueError):\n            prior.log_prob(torch.eye(3, device=device))\n\n    def test_lkj_covariance_prior_batch_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_lkj_covariance_prior_batch_log_prob(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/priors/test_multivariate_normal_prior.py,47,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\nfrom torch.distributions import MultivariateNormal\n\nfrom gpytorch.priors import MultivariateNormalPrior\nfrom gpytorch.test.utils import least_used_cuda_device\n\n\nclass TestMultivariateNormalPrior(unittest.TestCase):\n    def test_multivariate_normal_prior_to_gpu(self):\n        if torch.cuda.is_available():\n            prior = MultivariateNormalPrior(torch.tensor([0.0, 1.0]), covariance_matrix=torch.eye(2)).cuda()\n            self.assertEqual(prior.loc.device.type, ""cuda"")\n            self.assertEqual(prior.covariance_matrix.device.type, ""cuda"")\n            self.assertEqual(prior.scale_tril.device.type, ""cuda"")\n            self.assertEqual(prior.precision_matrix.device.type, ""cuda"")\n\n    def test_multivariate_normal_prior_validate_args(self):\n        with self.assertRaises(ValueError):\n            mean = torch.tensor([0.0, 1.0])\n            cov = torch.tensor([[1.0, 2.0], [2.0, 0.5]])\n            MultivariateNormalPrior(mean, covariance_matrix=cov, validate_args=True)\n\n    def test_multivariate_normal_prior_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        mean = torch.tensor([0.0, 1.0], device=device)\n        cov = torch.eye(2, device=device)\n        prior = MultivariateNormalPrior(mean, covariance_matrix=cov)\n        dist = MultivariateNormal(mean, covariance_matrix=cov)\n\n        t = torch.tensor([-1, 0.5], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.tensor([[-1, 0.5], [1.5, -2.0]], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.zeros(3, device=device))\n\n    def test_multivariate_normal_prior_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                return self.test_multivariate_normal_prior_log_prob(cuda=True)\n\n    def test_multivariate_normal_prior_log_prob_log_transform(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        mean = torch.tensor([0.0, 1.0], device=device)\n        cov = torch.eye(2, device=device)\n        prior = MultivariateNormalPrior(mean, covariance_matrix=cov, transform=torch.exp)\n        dist = MultivariateNormal(mean, covariance_matrix=cov)\n\n        t = torch.tensor([-1, 0.5], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t.exp())))\n        t = torch.tensor([[-1, 0.5], [1.5, -2.0]], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t.exp())))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.zeros(3, device=device))\n\n    def test_multivariate_normal_prior_log_prob_log_transform_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                return self.test_multivariate_normal_prior_log_prob_log_transform(cuda=True)\n\n    def test_multivariate_normal_prior_batch_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n\n        mean = torch.tensor([[0.0, 1.0], [-0.5, 2.0]], device=device)\n        cov = torch.eye(2, device=device).repeat(2, 1, 1)\n        prior = MultivariateNormalPrior(mean, covariance_matrix=cov)\n        dist = MultivariateNormal(mean, covariance_matrix=cov)\n\n        t = torch.tensor([-1, 0.5], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.tensor([[-1, 0.5], [1.5, -2.0]], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.zeros(1, 3, device=device))\n\n        mean = torch.rand(3, 2, 2, device=device)\n        cov = torch.eye(2, device=device).repeat(3, 2, 1, 1)\n        prior = MultivariateNormalPrior(mean, covariance_matrix=cov)\n        dist = MultivariateNormal(mean, covariance_matrix=cov)\n\n        t = torch.rand(2, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.rand(2, 2, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.rand(3, 2, 2, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.rand(2, 3, 2, 2, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.rand(3, device=device))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.rand(3, 2, 3, device=device))\n\n    def test_multivariate_normal_prior_batch_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                return self.test_multivariate_normal_prior_batch_log_prob(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/priors/test_normal_prior.py,42,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\nfrom torch.distributions import Normal\n\nfrom gpytorch.priors import NormalPrior\nfrom gpytorch.test.utils import least_used_cuda_device\n\n\nclass TestNormalPrior(unittest.TestCase):\n    def test_normal_prior_to_gpu(self):\n        if torch.cuda.is_available():\n            prior = NormalPrior(0, 1).cuda()\n            self.assertEqual(prior.loc.device.type, ""cuda"")\n            self.assertEqual(prior.scale.device.type, ""cuda"")\n\n    def test_normal_prior_validate_args(self):\n        with self.assertRaises(ValueError):\n            NormalPrior(0, -1, validate_args=True)\n\n    def test_normal_prior_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        mean = torch.tensor(0.0, device=device)\n        variance = torch.tensor(1.0, device=device)\n        prior = NormalPrior(mean, variance)\n        dist = Normal(mean, variance)\n\n        t = torch.tensor(0.0, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.tensor([-1, 0.5], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.tensor([[-1, 0.5], [0.1, -2.0]], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n\n    def test_normal_prior_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                return self.test_normal_prior_log_prob(cuda=True)\n\n    def test_normal_prior_log_prob_log_transform(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        mean = torch.tensor(0.0, device=device)\n        variance = torch.tensor(1.0, device=device)\n        prior = NormalPrior(mean, variance, transform=torch.exp)\n        dist = Normal(mean, variance)\n\n        t = torch.tensor(0.0, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t.exp())))\n        t = torch.tensor([-1, 0.5], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t.exp())))\n        t = torch.tensor([[-1, 0.5], [0.1, -2.0]], device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t.exp())))\n\n    def test_normal_prior_log_prob_log_transform_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                return self.test_normal_prior_log_prob_log_transform(cuda=True)\n\n    def test_normal_prior_batch_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n\n        mean = torch.tensor([0.0, 1.0], device=device)\n        variance = torch.tensor([1.0, 2.0], device=device)\n        prior = NormalPrior(mean, variance)\n        dist = Normal(mean, variance)\n        t = torch.zeros(2, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.zeros(2, 2, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.zeros(3, device=device))\n\n        mean = torch.tensor([[0.0, 1.0], [-1.0, 2.0]], device=device)\n        variance = torch.tensor([[1.0, 2.0], [0.5, 1.0]], device=device)\n        prior = NormalPrior(mean, variance)\n        dist = Normal(mean, variance)\n        t = torch.zeros(2, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        t = torch.zeros(2, 2, device=device)\n        self.assertTrue(torch.equal(prior.log_prob(t), dist.log_prob(t)))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.zeros(3, device=device))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.zeros(2, 3, device=device))\n\n    def test_normal_prior_batch_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                return self.test_normal_prior_batch_log_prob(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/priors/test_smoothed_box_prior.py,27,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.priors import SmoothedBoxPrior\nfrom gpytorch.test.utils import approx_equal, least_used_cuda_device\n\n\nclass TestSmoothedBoxPrior(unittest.TestCase):\n    def test_smoothed_box_prior_to_gpu(self):\n        if torch.cuda.is_available():\n            prior = SmoothedBoxPrior(torch.zeros(2), torch.ones(2)).cuda()\n            self.assertEqual(prior.a.device.type, ""cuda"")\n            self.assertEqual(prior.b.device.type, ""cuda"")\n            self.assertEqual(prior.sigma.device.type, ""cuda"")\n            self.assertEqual(prior._c.device.type, ""cuda"")\n            self.assertEqual(prior._r.device.type, ""cuda"")\n            self.assertEqual(prior._M.device.type, ""cuda"")\n            self.assertEqual(prior.tails.loc.device.type, ""cuda"")\n            self.assertEqual(prior.tails.scale.device.type, ""cuda"")\n\n    def test_smoothed_box_prior_validate_args(self):\n        with self.assertRaises(ValueError):\n            SmoothedBoxPrior(torch.ones(2), torch.zeros(2), validate_args=True)\n\n    def test_smoothed_box_prior_log_prob(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        a, b = torch.zeros(2, device=device), torch.ones(2, device=device)\n        sigma = 0.1\n        prior = SmoothedBoxPrior(a, b, sigma)\n\n        self.assertTrue(torch.equal(prior.a, a))\n        self.assertTrue(torch.equal(prior.b, b))\n        self.assertTrue(torch.equal(prior.sigma, torch.full_like(prior.a, sigma)))\n        self.assertTrue(torch.all(approx_equal(prior._M, torch.full_like(prior.a, 1.6073))))\n\n        t = torch.tensor([0.5, 1.1], device=device)\n        self.assertAlmostEqual(prior.log_prob(t).item(), -0.9473, places=4)\n        t = torch.tensor([[0.5, 1.1], [0.1, 0.25]], device=device)\n        log_prob_expected = torch.tensor([-0.947347, -0.447347], device=t.device)\n        self.assertTrue(torch.all(approx_equal(prior.log_prob(t), log_prob_expected)))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.zeros(3, device=device))\n\n    def test_smoothed_box_prior_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                return self.test_smoothed_box_prior_log_prob(cuda=True)\n\n    def test_smoothed_box_prior_log_prob_log_transform(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        a, b = torch.zeros(2, device=device), torch.ones(2, device=device)\n        sigma = 0.1\n        prior = SmoothedBoxPrior(a, b, sigma, transform=torch.exp)\n\n        t = torch.tensor([0.5, 1.1], device=device).log()\n        self.assertAlmostEqual(prior.log_prob(t).item(), -0.9473, places=4)\n        t = torch.tensor([[0.5, 1.1], [0.1, 0.25]], device=device).log()\n        log_prob_expected = torch.tensor([-0.947347, -0.447347], device=t.device)\n        self.assertTrue(torch.all(approx_equal(prior.log_prob(t), log_prob_expected)))\n        with self.assertRaises(RuntimeError):\n            prior.log_prob(torch.ones(3, device=device))\n\n    def test_smoothed_box_prior_log_prob_log_transform_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                return self.test_smoothed_box_prior_log_prob_log_transform(cuda=True)\n\n    def test_smoothed_box_prior_batch_log_prob(self, cuda=False):\n        # TODO: Implement test for batch mode\n        pass\n\n    def test_smoothed_box_prior_batch_log_prob_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                return self.test_smoothed_box_prior_batch_log_prob(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/utils/__init__.py,0,b'#!/usr/bin/env python3\n'
test/utils/test_cholesky.py,32,"b'#!/usr/bin/env python3\n\nimport unittest\nimport warnings\n\nimport torch\n\nfrom gpytorch.test.base_test_case import BaseTestCase\nfrom gpytorch.test.utils import least_used_cuda_device\nfrom gpytorch.utils.cholesky import psd_safe_cholesky\nfrom gpytorch.utils.errors import NanError\nfrom gpytorch.utils.warnings import NumericalWarning\n\n\nclass TestPSDSafeCholesky(BaseTestCase, unittest.TestCase):\n    seed = 0\n\n    def _gen_test_psd(self):\n        return torch.tensor([[[0.25, -0.75], [-0.75, 2.25]], [[1.0, 1.0], [1.0, 1.0]]])\n\n    def test_psd_safe_cholesky_nan(self, cuda=False):\n        A = self._gen_test_psd().sqrt()\n        with self.assertRaises(NanError) as ctx:\n            psd_safe_cholesky(A)\n            self.assertTrue(""NaN"" in ctx.exception)\n\n    def test_psd_safe_cholesky_pd(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            for batch_mode in (False, True):\n                if batch_mode:\n                    A = self._gen_test_psd().to(device=device, dtype=dtype)\n                    D = torch.eye(2).type_as(A).unsqueeze(0).repeat(2, 1, 1)\n                else:\n                    A = self._gen_test_psd()[0].to(device=device, dtype=dtype)\n                    D = torch.eye(2).type_as(A)\n                A += D\n                # basic\n                L = torch.cholesky(A)\n                L_safe = psd_safe_cholesky(A)\n                self.assertTrue(torch.allclose(L, L_safe))\n                # upper\n                L = torch.cholesky(A, upper=True)\n                L_safe = psd_safe_cholesky(A, upper=True)\n                self.assertTrue(torch.allclose(L, L_safe))\n                # output tensors\n                L = torch.empty_like(A)\n                L_safe = torch.empty_like(A)\n                torch.cholesky(A, out=L)\n                psd_safe_cholesky(A, out=L_safe)\n                self.assertTrue(torch.allclose(L, L_safe))\n                # output tensors, upper\n                torch.cholesky(A, upper=True, out=L)\n                psd_safe_cholesky(A, upper=True, out=L_safe)\n                self.assertTrue(torch.allclose(L, L_safe))\n                # make sure jitter doesn\'t do anything if p.d.\n                L = torch.cholesky(A)\n                L_safe = psd_safe_cholesky(A, jitter=1e-2)\n                self.assertTrue(torch.allclose(L, L_safe))\n\n    def test_psd_safe_cholesky_pd_cuda(self, cuda=False):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_psd_safe_cholesky_pd(cuda=True)\n\n    def test_psd_safe_cholesky_psd(self, cuda=False):\n        device = torch.device(""cuda"") if cuda else torch.device(""cpu"")\n        for dtype in (torch.float, torch.double):\n            for batch_mode in (False, True):\n                if batch_mode:\n                    A = self._gen_test_psd().to(device=device, dtype=dtype)\n                else:\n                    A = self._gen_test_psd()[0].to(device=device, dtype=dtype)\n                idx = torch.arange(A.shape[-1], device=A.device)\n                # default values\n                Aprime = A.clone()\n                Aprime[..., idx, idx] += 1e-6 if A.dtype == torch.float32 else 1e-8\n                L_exp = torch.cholesky(Aprime)\n                with warnings.catch_warnings(record=True) as w:\n                    # Makes sure warnings we catch don\'t cause `-w error` to fail\n                    warnings.simplefilter(""always"", NumericalWarning)\n\n                    L_safe = psd_safe_cholesky(A)\n                    self.assertTrue(any(issubclass(w_.category, NumericalWarning) for w_ in w))\n                    self.assertTrue(any(""A not p.d., added jitter"" in str(w_.message) for w_ in w))\n                self.assertTrue(torch.allclose(L_exp, L_safe))\n                # user-defined value\n                Aprime = A.clone()\n                Aprime[..., idx, idx] += 1e-2\n                L_exp = torch.cholesky(Aprime)\n                with warnings.catch_warnings(record=True) as w:\n                    # Makes sure warnings we catch don\'t cause `-w error` to fail\n                    warnings.simplefilter(""always"", NumericalWarning)\n\n                    L_safe = psd_safe_cholesky(A, jitter=1e-2)\n                    self.assertTrue(any(issubclass(w_.category, NumericalWarning) for w_ in w))\n                    self.assertTrue(any(""A not p.d., added jitter"" in str(w_.message) for w_ in w))\n                self.assertTrue(torch.allclose(L_exp, L_safe))\n\n    def test_psd_safe_cholesky_psd_cuda(self, cuda=False):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_psd_safe_cholesky_psd(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/utils/test_fft.py,22,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom gpytorch.utils import fft\n\n\nclass TestFFT(unittest.TestCase):\n    def test_fft1_computes_fft_of_1d_input(self):\n        d = 8\n        input = torch.randn(d)\n\n        res = fft.fft1(input)\n        actual = np.fft.fft(input.numpy())\n        self.assertEqual(tuple(res.size()), (8, 2))\n\n        res_real = res[:, 0]\n        res_imag = res[:, 1]\n        actual_real = torch.from_numpy(actual.real).float()\n        actual_imag = torch.from_numpy(actual.imag).float()\n\n        assert torch.norm(res_real - actual_real) < 1e-5\n        assert torch.norm(res_imag - actual_imag) < 1e-5\n\n    def test_fft1_computes_fft_of_nd_input(self):\n        d = 8\n        input = torch.randn(3, 6, d)\n\n        res = fft.fft1(input)\n        actual = np.fft.fft(input.numpy())\n        self.assertEqual(tuple(res.size()), (3, 6, 8, 2))\n\n        res_real = res[:, :, :, 0]\n        res_imag = res[:, :, :, 1]\n        actual_real = torch.from_numpy(actual.real[:, :, :]).float()\n        actual_imag = torch.from_numpy(actual.imag[:, :, :]).float()\n\n        self.assertLess(torch.norm(res_real - actual_real), 1e-5)\n        self.assertLess(torch.norm(res_imag - actual_imag), 1e-5)\n\n    def test_fft1_returns_type_of_original_input(self):\n        d = 8\n        input = torch.randn(3, 6, d).double()\n\n        res = fft.fft1(input)\n        self.assertTrue(isinstance(res, torch.DoubleTensor))\n\n    def test_ifft1_computes_ifft_of_1d_input(self):\n        d = 8\n        input = torch.randn(d)\n\n        res = fft.fft1(input)\n        recon = fft.ifft1(res)\n        self.assertEqual(input.size(), recon.size())\n        self.assertLess(torch.norm(input - recon), 1e-5)\n\n    def test_ifft1_computes_ifft_of_1d_input_with_odd_size(self):\n        d = 9\n        input = torch.randn(d)\n\n        res = fft.fft1(input)\n        recon = fft.ifft1(res)\n        self.assertEqual(input.size(), recon.size())\n        self.assertLess(torch.norm(input - recon), 1e-5)\n\n    def test_ifft1_computes_ifft_of_2d_input(self):\n        d = 8\n        input = torch.randn(6, d)\n\n        res = fft.fft1(input)\n        recon = fft.ifft1(res)\n        self.assertEqual(input.size(), recon.size())\n        self.assertLess(torch.norm(input - recon), 1e-5)\n\n    def test_ifft1_returns_type_of_original_input(self):\n        d = 8\n        input = torch.randn(6, d)\n        res = fft.fft1(input).double()\n        recon = fft.ifft1(res)\n        self.assertEqual(input.size(), recon.size())\n        self.assertLess(torch.norm(input.double() - recon), 1e-5)\n        self.assertTrue(isinstance(res, torch.DoubleTensor))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/utils/test_getitem.py,8,"b'#!/usr/bin/env python3\n\nimport unittest\nfrom itertools import product\n\nimport torch\n\nfrom gpytorch.utils.getitem import _compute_getitem_size, _convert_indices_to_tensors\n\n\nclass TestGetitem(unittest.TestCase):\n    def test_compute_getitem_size(self):\n        a = torch.tensor(0.0).expand(5, 5, 5, 5, 5)\n\n        for indices in product([torch.tensor([0, 1, 1, 0]), slice(None, None, None), 1, slice(0, 2, None)], repeat=5):\n            res = _compute_getitem_size(a, indices)\n            actual = a[indices].shape\n            self.assertEqual(res, actual)\n\n    def test_convert_indices_to_tensors(self):\n        a = torch.randn(5, 5, 5, 5, 5)\n\n        for indices in product([torch.tensor([0, 1, 1, 0]), slice(None, None, None), 1, slice(0, 2, None)], repeat=5):\n            if not any(torch.is_tensor(index) for index in indices):\n                continue\n            new_indices = _convert_indices_to_tensors(a, indices)\n            self.assertTrue(all(torch.is_tensor(index) for index in new_indices))\n            self.assertTrue(torch.equal(a[indices], a[new_indices]))\n'"
test/utils/test_grid.py,10,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport gpytorch\n\n\nclass TestGrid(unittest.TestCase):\n    def test_scale_to_bounds(self):\n        """"""\n        """"""\n        x = torch.randn(100) * 50\n        res = gpytorch.utils.grid.scale_to_bounds(x, -1, 1)\n        self.assertGreater(res.min().item(), -1)\n        self.assertLess(res.max().item(), 1)\n\n    def test_choose_grid_size(self):\n        """"""\n        """"""\n        x = torch.randn(100)\n        grid_size = gpytorch.utils.grid.choose_grid_size(x, ratio=2.0)\n        self.assertEqual(grid_size, 200)\n\n        x = torch.randn(100, 1)\n        grid_size = gpytorch.utils.grid.choose_grid_size(x, ratio=2.0)\n        self.assertEqual(grid_size, 200)\n\n        x = torch.randn(10000, 2)\n        grid_size = gpytorch.utils.grid.choose_grid_size(x, ratio=2.0)\n        self.assertEqual(grid_size, 200)\n\n        x = torch.randn(16, 10000, 4)\n        grid_size = gpytorch.utils.grid.choose_grid_size(x, ratio=2.0)\n        self.assertEqual(grid_size, 20)\n'"
test/utils/test_interpolation.py,55,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.test.utils import approx_equal\nfrom gpytorch.utils.interpolation import Interpolation, left_interp, left_t_interp\n\n\nclass TestCubicInterpolation(unittest.TestCase):\n    def test_interpolation(self):\n        x = torch.linspace(0.01, 1, 100).unsqueeze(1)\n        grid = torch.linspace(-0.05, 1.05, 50).unsqueeze(1)\n        indices, values = Interpolation().interpolate(grid, x)\n        indices = indices.squeeze_(0)\n        values = values.squeeze_(0)\n        test_func_grid = grid.squeeze(1).pow(2)\n        test_func_x = x.pow(2).squeeze(-1)\n\n        interp_func_x = left_interp(indices, values, test_func_grid.unsqueeze(1)).squeeze()\n\n        self.assertTrue(approx_equal(interp_func_x, test_func_x))\n\n    def test_multidim_interpolation(self):\n        x = torch.tensor([[0.25, 0.45, 0.65, 0.85], [0.35, 0.375, 0.4, 0.425], [0.45, 0.5, 0.55, 0.6]]).t().contiguous()\n        grid = torch.linspace(0.0, 1.0, 11).unsqueeze(1).repeat(1, 3)\n\n        indices, values = Interpolation().interpolate(grid, x)\n\n        actual_indices = torch.cat(\n            [\n                torch.tensor(\n                    [\n                        [146, 147, 148, 149, 157, 158, 159, 160, 168, 169, 170, 171, 179],\n                        [389, 390, 391, 392, 400, 401, 402, 403, 411, 412, 413, 414, 422],\n                        [642, 643, 644, 645, 653, 654, 655, 656, 664, 665, 666, 667, 675],\n                        [885, 886, 887, 888, 896, 897, 898, 899, 907, 908, 909, 910, 918],\n                    ],\n                    dtype=torch.long,\n                ),\n                torch.tensor(\n                    [\n                        [180, 181, 182, 267, 268, 269, 270, 278, 279, 280, 281, 289, 290],\n                        [423, 424, 425, 510, 511, 512, 513, 521, 522, 523, 524, 532, 533],\n                        [676, 677, 678, 763, 764, 765, 766, 774, 775, 776, 777, 785, 786],\n                        [919, 920, 921, 1006, 1007, 1008, 1009, 1017, 1018, 1019, 1020, 1028, 1029],\n                    ],\n                    dtype=torch.long,\n                ),\n                torch.tensor(\n                    [\n                        [291, 292, 300, 301, 302, 303, 388, 389, 390, 391, 399, 400, 401],\n                        [534, 535, 543, 544, 545, 546, 631, 632, 633, 634, 642, 643, 644],\n                        [787, 788, 796, 797, 798, 799, 884, 885, 886, 887, 895, 896, 897],\n                        [1030, 1031, 1039, 1040, 1041, 1042, 1127, 1128, 1129, 1130, 1138, 1139, 1140],\n                    ],\n                    dtype=torch.long,\n                ),\n                torch.tensor(\n                    [\n                        [402, 410, 411, 412, 413, 421, 422, 423, 424, 509, 510, 511, 512],\n                        [645, 653, 654, 655, 656, 664, 665, 666, 667, 752, 753, 754, 755],\n                        [898, 906, 907, 908, 909, 917, 918, 919, 920, 1005, 1006, 1007, 1008],\n                        [1141, 1149, 1150, 1151, 1152, 1160, 1161, 1162, 1163, 1248, 1249, 1250, 1251],\n                    ],\n                    dtype=torch.long,\n                ),\n                torch.tensor(\n                    [\n                        [520, 521, 522, 523, 531, 532, 533, 534, 542, 543, 544, 545],\n                        [763, 764, 765, 766, 774, 775, 776, 777, 785, 786, 787, 788],\n                        [1016, 1017, 1018, 1019, 1027, 1028, 1029, 1030, 1038, 1039, 1040, 1041],\n                        [1259, 1260, 1261, 1262, 1270, 1271, 1272, 1273, 1281, 1282, 1283, 1284],\n                    ],\n                    dtype=torch.long,\n                ),\n            ],\n            1,\n        )\n        self.assertTrue(approx_equal(indices, actual_indices))\n\n        actual_values = torch.cat(\n            [\n                torch.tensor(\n                    [\n                        [-0.0002, 0.0022, 0.0022, -0.0002, 0.0022, -0.0198, -0.0198, 0.0022, 0.0022, -0.0198],\n                        [0.0000, 0.0015, 0.0000, 0.0000, -0.0000, -0.0142, -0.0000, -0.0000, -0.0000, -0.0542],\n                        [0.0000, -0.0000, -0.0000, 0.0000, 0.0039, -0.0352, -0.0352, 0.0039, 0.0000, -0.0000],\n                        [0.0000, 0.0044, 0.0000, 0.0000, -0.0000, -0.0542, -0.0000, -0.0000, -0.0000, -0.0142],\n                    ]\n                ),\n                torch.tensor(\n                    [\n                        [-0.0198, 0.0022, -0.0002, 0.0022, 0.0022, -0.0002, 0.0022, -0.0198, -0.0198, 0.0022],\n                        [-0.0000, -0.0000, 0.0000, 0.0044, 0.0000, 0.0000, -0.0000, -0.0132, -0.0000, -0.0000],\n                        [-0.0000, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, -0.0000, 0.0000, 0.0000, -0.0000],\n                        [-0.0000, -0.0000, 0.0000, 0.0015, 0.0000, 0.0000, -0.0000, -0.0396, -0.0000, -0.0000],\n                    ]\n                ),\n                torch.tensor(\n                    [\n                        [-0.0198, 0.1780, 0.1780, -0.0198, -0.0198, 0.1780, 0.1780, -0.0198, 0.0022, -0.0198],\n                        [0.0000, 0.1274, 0.0000, 0.0000, 0.0000, 0.4878, 0.0000, 0.0000, -0.0000, -0.0396],\n                        [-0.0352, 0.3164, 0.3164, -0.0352, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000],\n                        [0.0000, 0.4878, 0.0000, 0.0000, 0.0000, 0.1274, 0.0000, 0.0000, -0.0000, -0.0132],\n                    ]\n                ),\n                torch.tensor(\n                    [\n                        [-0.0198, 0.0022, 0.0022, -0.0198, -0.0198, 0.0022, -0.0198, 0.1780, 0.1780, -0.0198],\n                        [-0.0000, -0.0000, -0.0000, -0.0132, -0.0000, -0.0000, 0.0000, 0.1274, 0.0000, 0.0000],\n                        [0.0000, -0.0000, -0.0000, 0.0000, 0.0000, -0.0000, -0.0352, 0.3164, 0.3164, -0.0352],\n                        [-0.0000, -0.0000, -0.0000, -0.0396, -0.0000, -0.0000, 0.0000, 0.4878, 0.0000, 0.0000],\n                    ]\n                ),\n                torch.tensor(\n                    [\n                        [-0.0198, 0.1780, 0.1780, -0.0198, 0.0022, -0.0198, -0.0198, 0.0022, -0.0002, 0.0022],\n                        [0.0000, 0.4878, 0.0000, 0.0000, -0.0000, -0.0396, -0.0000, -0.0000, 0.0000, 0.0015],\n                        [-0.0000, 0.0000, 0.0000, -0.0000, -0.0000, 0.0000, 0.0000, -0.0000, 0.0000, -0.0000],\n                        [0.0000, 0.1274, 0.0000, 0.0000, -0.0000, -0.0132, -0.0000, -0.0000, 0.0000, 0.0044],\n                    ]\n                ),\n                torch.tensor(\n                    [\n                        [0.0022, -0.0002, 0.0022, -0.0198, -0.0198, 0.0022, 0.0022, -0.0198, -0.0198, 0.0022],\n                        [0.0000, 0.0000, -0.0000, -0.0142, -0.0000, -0.0000, -0.0000, -0.0542, -0.0000, -0.0000],\n                        [-0.0000, 0.0000, 0.0039, -0.0352, -0.0352, 0.0039, 0.0000, -0.0000, -0.0000, 0.0000],\n                        [0.0000, 0.0000, -0.0000, -0.0542, -0.0000, -0.0000, -0.0000, -0.0142, -0.0000, -0.0000],\n                    ]\n                ),\n                torch.tensor(\n                    [\n                        [-0.0002, 0.0022, 0.0022, -0.0002],\n                        [0.0000, 0.0044, 0.0000, 0.0000],\n                        [0.0000, -0.0000, -0.0000, 0.0000],\n                        [0.0000, 0.0015, 0.0000, 0.0000],\n                    ]\n                ),\n            ],\n            1,\n        )\n        self.assertTrue(approx_equal(values, actual_values))\n\n\nclass TestInterp(unittest.TestCase):\n    def setUp(self):\n        self.interp_indices = torch.tensor([[2, 3], [3, 4], [4, 5]], dtype=torch.long).repeat(3, 1)\n        self.interp_values = torch.tensor([[1, 2], [0.5, 1], [1, 3]], dtype=torch.float).repeat(3, 1)\n        self.interp_indices_2 = torch.tensor([[0, 1], [1, 2], [2, 3]], dtype=torch.long).repeat(3, 1)\n        self.interp_values_2 = torch.tensor([[1, 2], [2, 0.5], [1, 3]], dtype=torch.float).repeat(3, 1)\n        self.batch_interp_indices = torch.cat([self.interp_indices.unsqueeze(0), self.interp_indices_2.unsqueeze(0)], 0)\n        self.batch_interp_values = torch.cat([self.interp_values.unsqueeze(0), self.interp_values_2.unsqueeze(0)], 0)\n        self.interp_matrix = torch.tensor(\n            [\n                [0, 0, 1, 2, 0, 0],\n                [0, 0, 0, 0.5, 1, 0],\n                [0, 0, 0, 0, 1, 3],\n                [0, 0, 1, 2, 0, 0],\n                [0, 0, 0, 0.5, 1, 0],\n                [0, 0, 0, 0, 1, 3],\n                [0, 0, 1, 2, 0, 0],\n                [0, 0, 0, 0.5, 1, 0],\n                [0, 0, 0, 0, 1, 3],\n            ],\n            dtype=torch.float,\n        )\n\n        self.batch_interp_matrix = torch.tensor(\n            [\n                [\n                    [0, 0, 1, 2, 0, 0],\n                    [0, 0, 0, 0.5, 1, 0],\n                    [0, 0, 0, 0, 1, 3],\n                    [0, 0, 1, 2, 0, 0],\n                    [0, 0, 0, 0.5, 1, 0],\n                    [0, 0, 0, 0, 1, 3],\n                    [0, 0, 1, 2, 0, 0],\n                    [0, 0, 0, 0.5, 1, 0],\n                    [0, 0, 0, 0, 1, 3],\n                ],\n                [\n                    [1, 2, 0, 0, 0, 0],\n                    [0, 2, 0.5, 0, 0, 0],\n                    [0, 0, 1, 3, 0, 0],\n                    [1, 2, 0, 0, 0, 0],\n                    [0, 2, 0.5, 0, 0, 0],\n                    [0, 0, 1, 3, 0, 0],\n                    [1, 2, 0, 0, 0, 0],\n                    [0, 2, 0.5, 0, 0, 0],\n                    [0, 0, 1, 3, 0, 0],\n                ],\n            ],\n            dtype=torch.float,\n        )\n\n    def test_left_interp_on_a_vector(self):\n        vector = torch.randn(6)\n\n        res = left_interp(self.interp_indices, self.interp_values, vector)\n        actual = torch.matmul(self.interp_matrix, vector)\n        self.assertTrue(approx_equal(res, actual))\n\n    def test_left_t_interp_on_a_vector(self):\n        vector = torch.randn(9)\n\n        res = left_t_interp(self.interp_indices, self.interp_values, vector, 6)\n        actual = torch.matmul(self.interp_matrix.transpose(-1, -2), vector)\n        self.assertTrue(approx_equal(res, actual))\n\n    def test_batch_left_interp_on_a_vector(self):\n        vector = torch.randn(6)\n\n        actual = torch.matmul(self.batch_interp_matrix, vector.unsqueeze(-1).unsqueeze(0)).squeeze(-1)\n        res = left_interp(self.batch_interp_indices, self.batch_interp_values, vector)\n        self.assertTrue(approx_equal(res, actual))\n\n    def test_batch_left_t_interp_on_a_vector(self):\n        vector = torch.randn(9)\n\n        actual = torch.matmul(self.batch_interp_matrix.transpose(-1, -2), vector.unsqueeze(-1).unsqueeze(0)).squeeze(-1)\n        res = left_t_interp(self.batch_interp_indices, self.batch_interp_values, vector, 6)\n        self.assertTrue(approx_equal(res, actual))\n\n    def test_left_interp_on_a_matrix(self):\n        matrix = torch.randn(6, 3)\n\n        res = left_interp(self.interp_indices, self.interp_values, matrix)\n        actual = torch.matmul(self.interp_matrix, matrix)\n        self.assertTrue(approx_equal(res, actual))\n\n    def test_left_t_interp_on_a_matrix(self):\n        matrix = torch.randn(9, 3)\n\n        res = left_t_interp(self.interp_indices, self.interp_values, matrix, 6)\n        actual = torch.matmul(self.interp_matrix.transpose(-1, -2), matrix)\n        self.assertTrue(approx_equal(res, actual))\n\n    def test_batch_left_interp_on_a_matrix(self):\n        batch_matrix = torch.randn(6, 3)\n\n        res = left_interp(self.batch_interp_indices, self.batch_interp_values, batch_matrix)\n        actual = torch.matmul(self.batch_interp_matrix, batch_matrix.unsqueeze(0))\n        self.assertTrue(approx_equal(res, actual))\n\n    def test_batch_left_t_interp_on_a_matrix(self):\n        batch_matrix = torch.randn(9, 3)\n\n        res = left_t_interp(self.batch_interp_indices, self.batch_interp_values, batch_matrix, 6)\n        actual = torch.matmul(self.batch_interp_matrix.transpose(-1, -2), batch_matrix.unsqueeze(0))\n        self.assertTrue(approx_equal(res, actual))\n\n    def test_batch_left_interp_on_a_batch_matrix(self):\n        batch_matrix = torch.randn(2, 6, 3)\n\n        res = left_interp(self.batch_interp_indices, self.batch_interp_values, batch_matrix)\n        actual = torch.matmul(self.batch_interp_matrix, batch_matrix)\n        self.assertTrue(approx_equal(res, actual))\n\n    def test_batch_left_t_interp_on_a_batch_matrix(self):\n        batch_matrix = torch.randn(2, 9, 3)\n\n        res = left_t_interp(self.batch_interp_indices, self.batch_interp_values, batch_matrix, 6)\n        actual = torch.matmul(self.batch_interp_matrix.transpose(-1, -2), batch_matrix)\n        self.assertTrue(approx_equal(res, actual))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/utils/test_lanczos.py,4,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.test.utils import approx_equal\nfrom gpytorch.utils.lanczos import lanczos_tridiag\n\n\nclass TestLanczos(unittest.TestCase):\n    def test_lanczos(self):\n        size = 100\n        matrix = torch.randn(size, size)\n        matrix = matrix.matmul(matrix.transpose(-1, -2))\n        matrix.div_(matrix.norm())\n        matrix.add_(torch.ones(matrix.size(-1)).mul(1e-6).diag())\n        q_mat, t_mat = lanczos_tridiag(\n            matrix.matmul, max_iter=size, dtype=matrix.dtype, device=matrix.device, matrix_shape=matrix.shape\n        )\n\n        approx = q_mat.matmul(t_mat).matmul(q_mat.transpose(-1, -2))\n        self.assertTrue(approx_equal(approx, matrix))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/utils/test_linear_cg.py,30,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport unittest\n\nimport torch\n\nfrom gpytorch.utils.linear_cg import linear_cg\n\n\nclass TestLinearCG(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_cg(self):\n        size = 100\n        matrix = torch.randn(size, size, dtype=torch.float64)\n        matrix = matrix.matmul(matrix.transpose(-1, -2))\n        matrix.div_(matrix.norm())\n        matrix.add_(torch.eye(matrix.size(-1), dtype=torch.float64).mul_(1e-1))\n\n        rhs = torch.randn(size, 50, dtype=torch.float64)\n        solves = linear_cg(matrix.matmul, rhs=rhs, max_iter=size)\n\n        # Check cg\n        matrix_chol = matrix.cholesky()\n        actual = torch.cholesky_solve(rhs, matrix_chol)\n        self.assertTrue(torch.allclose(solves, actual, atol=1e-3, rtol=1e-4))\n\n    def test_cg_with_tridiag(self):\n        size = 10\n        matrix = torch.randn(size, size, dtype=torch.float64)\n        matrix = matrix.matmul(matrix.transpose(-1, -2))\n        matrix.div_(matrix.norm())\n        matrix.add_(torch.eye(matrix.size(-1), dtype=torch.float64).mul_(1e-1))\n\n        rhs = torch.randn(size, 50, dtype=torch.float64)\n        solves, t_mats = linear_cg(\n            matrix.matmul, rhs=rhs, n_tridiag=5, max_tridiag_iter=10, max_iter=size, tolerance=0, eps=1e-15\n        )\n\n        # Check cg\n        matrix_chol = matrix.cholesky()\n        actual = torch.cholesky_solve(rhs, matrix_chol)\n        self.assertTrue(torch.allclose(solves, actual, atol=1e-3, rtol=1e-4))\n\n        # Check tridiag\n        eigs = matrix.symeig()[0]\n        for i in range(5):\n            approx_eigs = t_mats[i].symeig()[0]\n            self.assertTrue(torch.allclose(eigs, approx_eigs, atol=1e-3, rtol=1e-4))\n\n    def test_batch_cg(self):\n        batch = 5\n        size = 100\n        matrix = torch.randn(batch, size, size, dtype=torch.float64)\n        matrix = matrix.matmul(matrix.transpose(-1, -2))\n        matrix.div_(matrix.norm())\n        matrix.add_(torch.eye(matrix.size(-1), dtype=torch.float64).mul_(1e-1))\n\n        rhs = torch.randn(batch, size, 50, dtype=torch.float64)\n        solves = linear_cg(matrix.matmul, rhs=rhs, max_iter=size)\n\n        # Check cg\n        matrix_chol = torch.cholesky(matrix)\n        actual = torch.cholesky_solve(rhs, matrix_chol)\n        self.assertTrue(torch.allclose(solves, actual, atol=1e-3, rtol=1e-4))\n\n    def test_batch_cg_with_tridiag(self):\n        batch = 5\n        size = 10\n        matrix = torch.randn(batch, size, size, dtype=torch.float64)\n        matrix = matrix.matmul(matrix.transpose(-1, -2))\n        matrix.div_(matrix.norm())\n        matrix.add_(torch.eye(matrix.size(-1), dtype=torch.float64).mul_(1e-1))\n\n        rhs = torch.randn(batch, size, 10, dtype=torch.float64)\n        solves, t_mats = linear_cg(\n            matrix.matmul, rhs=rhs, n_tridiag=8, max_iter=size, max_tridiag_iter=10, tolerance=0, eps=1e-30\n        )\n\n        # Check cg\n        matrix_chol = torch.cholesky(matrix)\n        actual = torch.cholesky_solve(rhs, matrix_chol)\n        self.assertTrue(torch.allclose(solves, actual, atol=1e-3, rtol=1e-4))\n\n        # Check tridiag\n        for i in range(5):\n            eigs = matrix[i].symeig()[0]\n            for j in range(8):\n                approx_eigs = t_mats[j, i].symeig()[0]\n                self.assertTrue(torch.allclose(eigs, approx_eigs, atol=1e-3, rtol=1e-4))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/utils/test_pivoted_cholesky.py,48,"b'#!/usr/bin/env python3\n\nimport math\nimport os\nimport random\nimport unittest\n\nimport torch\n\nfrom gpytorch import settings\nfrom gpytorch.kernels import RBFKernel\nfrom gpytorch.test.utils import approx_equal\nfrom gpytorch.utils import pivoted_cholesky\n\n\nclass TestPivotedCholesky(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_pivoted_cholesky(self):\n        size = 100\n        train_x = torch.linspace(0, 1, size)\n        covar_matrix = RBFKernel()(train_x, train_x).evaluate()\n        piv_chol = pivoted_cholesky.pivoted_cholesky(covar_matrix, 10)\n        covar_approx = piv_chol @ piv_chol.transpose(-1, -2)\n        self.assertTrue(approx_equal(covar_approx, covar_matrix, 2e-4))\n\n    def test_solve_qr(self, dtype=torch.float64, tol=1e-8):\n        size = 50\n        X = torch.rand((size, 2)).to(dtype=dtype)\n        y = torch.sin(torch.sum(X, 1)).unsqueeze(-1).to(dtype=dtype)\n        with settings.min_preconditioning_size(0):\n            noise = torch.DoubleTensor(size).uniform_(math.log(1e-3), math.log(1e-1)).exp_().to(dtype=dtype)\n            lazy_tsr = RBFKernel().to(dtype=dtype)(X).evaluate_kernel().add_diag(noise)\n            precondition_qr, _, logdet_qr = lazy_tsr._preconditioner()\n\n            F = lazy_tsr._piv_chol_self\n            M = noise.diag() + F.matmul(F.t())\n\n        x_exact = torch.solve(y, M)[0]\n        x_qr = precondition_qr(y)\n\n        self.assertTrue(approx_equal(x_exact, x_qr, tol))\n\n        logdet = 2 * torch.cholesky(M).diag().log().sum(-1)\n        self.assertTrue(approx_equal(logdet, logdet_qr, tol))\n\n    def test_solve_qr_constant_noise(self, dtype=torch.float64, tol=1e-8):\n        size = 50\n        X = torch.rand((size, 2)).to(dtype=dtype)\n        y = torch.sin(torch.sum(X, 1)).unsqueeze(-1).to(dtype=dtype)\n\n        with settings.min_preconditioning_size(0):\n            noise = 1e-2 * torch.ones(size, dtype=dtype)\n            lazy_tsr = RBFKernel().to(dtype=dtype)(X).evaluate_kernel().add_diag(noise)\n            precondition_qr, _, logdet_qr = lazy_tsr._preconditioner()\n\n            F = lazy_tsr._piv_chol_self\n        M = noise.diag() + F.matmul(F.t())\n\n        x_exact = torch.solve(y, M)[0]\n        x_qr = precondition_qr(y)\n\n        self.assertTrue(approx_equal(x_exact, x_qr, tol))\n\n        logdet = 2 * torch.cholesky(M).diag().log().sum(-1)\n        self.assertTrue(approx_equal(logdet, logdet_qr, tol))\n\n    def test_solve_qr_float32(self):\n        self.test_solve_qr(dtype=torch.float32, tol=1e-2)\n\n    def test_solve_qr_constant_noise_float32(self):\n        self.test_solve_qr_constant_noise(dtype=torch.float32, tol=1e-3)\n\n\nclass TestPivotedCholeskyBatch(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_pivoted_cholesky(self):\n        size = 100\n        train_x = torch.cat(\n            [torch.linspace(0, 1, size).unsqueeze(0), torch.linspace(0, 0.5, size).unsqueeze(0)], 0\n        ).unsqueeze(-1)\n        covar_matrix = RBFKernel()(train_x, train_x).evaluate()\n        piv_chol = pivoted_cholesky.pivoted_cholesky(covar_matrix, 10)\n        covar_approx = piv_chol @ piv_chol.transpose(-1, -2)\n\n        self.assertTrue(approx_equal(covar_approx, covar_matrix, 2e-4))\n\n\nclass TestPivotedCholeskyMultiBatch(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(0)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(0)\n            random.seed(0)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_pivoted_cholesky(self):\n        size = 100\n        train_x = torch.cat(\n            [\n                torch.linspace(0, 1, size).unsqueeze(0),\n                torch.linspace(0, 0.5, size).unsqueeze(0),\n                torch.linspace(0, 0.25, size).unsqueeze(0),\n                torch.linspace(0, 1.25, size).unsqueeze(0),\n                torch.linspace(0, 1.5, size).unsqueeze(0),\n                torch.linspace(0, 1, size).unsqueeze(0),\n                torch.linspace(0, 0.5, size).unsqueeze(0),\n                torch.linspace(0, 0.25, size).unsqueeze(0),\n                torch.linspace(0, 1.25, size).unsqueeze(0),\n                torch.linspace(0, 1.25, size).unsqueeze(0),\n                torch.linspace(0, 1.5, size).unsqueeze(0),\n                torch.linspace(0, 1, size).unsqueeze(0),\n            ],\n            0,\n        ).unsqueeze(-1)\n        covar_matrix = RBFKernel()(train_x, train_x).evaluate().view(2, 2, 3, size, size)\n        piv_chol = pivoted_cholesky.pivoted_cholesky(covar_matrix, 10)\n        covar_approx = piv_chol @ piv_chol.transpose(-1, -2)\n\n        self.assertTrue(approx_equal(covar_approx, covar_matrix, 2e-4))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/utils/test_quadrature.py,35,"b'#!/usr/bin/env python3\n\nimport os\nimport random\nimport unittest\n\nimport torch\n\nfrom gpytorch.distributions import MultivariateNormal\nfrom gpytorch.lazy import DiagLazyTensor\nfrom gpytorch.test.utils import least_used_cuda_device\nfrom gpytorch.utils.quadrature import GaussHermiteQuadrature1D\n\n\nclass TestQuadrature(unittest.TestCase):\n    def setUp(self):\n        if os.getenv(""UNLOCK_SEED"") is None or os.getenv(""UNLOCK_SEED"").lower() == ""false"":\n            self.rng_state = torch.get_rng_state()\n            torch.manual_seed(1)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed_all(1)\n            random.seed(1)\n\n    def tearDown(self):\n        if hasattr(self, ""rng_state""):\n            torch.set_rng_state(self.rng_state)\n\n    def test_gauss_hermite_quadrature_1D_normal_nonbatch(self, cuda=False):\n        func = lambda x: torch.sin(x)\n\n        means = torch.randn(10)\n        variances = torch.randn(10).abs()\n        quadrature = GaussHermiteQuadrature1D()\n\n        if cuda:\n            means = means.cuda()\n            variances = variances.cuda()\n            quadrature = quadrature.cuda()\n\n        dist = torch.distributions.Normal(means, variances.sqrt())\n\n        # Use quadrature\n        results = quadrature(func, dist)\n\n        # Use Monte-Carlo\n        samples = dist.rsample(torch.Size([20000]))\n        actual = func(samples).mean(0)\n\n        self.assertLess(torch.mean(torch.abs(actual - results)), 0.1)\n\n    def test_gauss_hermite_quadrature_1D_normal_nonbatch_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_gauss_hermite_quadrature_1D_normal_nonbatch(cuda=True)\n\n    def test_gauss_hermite_quadrature_1D_normal_batch(self, cuda=False):\n        func = lambda x: torch.sin(x)\n\n        means = torch.randn(3, 10)\n        variances = torch.randn(3, 10).abs()\n        quadrature = GaussHermiteQuadrature1D()\n\n        if cuda:\n            means = means.cuda()\n            variances = variances.cuda()\n            quadrature = quadrature.cuda()\n\n        dist = torch.distributions.Normal(means, variances.sqrt())\n\n        # Use quadrature\n        results = quadrature(func, dist)\n\n        # Use Monte-Carlo\n        samples = dist.rsample(torch.Size([20000]))\n        actual = func(samples).mean(0)\n\n        self.assertLess(torch.mean(torch.abs(actual - results)), 0.1)\n\n    def test_gauss_hermite_quadrature_1D_normal_batch_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_gauss_hermite_quadrature_1D_normal_nonbatch(cuda=True)\n\n    def test_gauss_hermite_quadrature_1D_mvn_nonbatch(self, cuda=False):\n        func = lambda x: torch.sin(x)\n\n        means = torch.randn(10)\n        variances = torch.randn(10).abs()\n\n        quadrature = GaussHermiteQuadrature1D()\n\n        if cuda:\n            means = means.cuda()\n            variances = variances.cuda()\n            quadrature = quadrature.cuda()\n\n        dist = MultivariateNormal(means, DiagLazyTensor(variances.sqrt()))\n\n        # Use quadrature\n        results = quadrature(func, dist)\n\n        # Use Monte-Carlo\n        samples = dist.rsample(torch.Size([20000]))\n        actual = func(samples).mean(0)\n\n        self.assertLess(torch.mean(torch.abs(actual - results)), 0.1)\n\n    def test_gauss_hermite_quadrature_1D_mvn_nonbatch_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_gauss_hermite_quadrature_1D_normal_nonbatch(cuda=True)\n\n    def test_gauss_hermite_quadrature_1D_mvn_batch(self, cuda=False):\n        func = lambda x: torch.sin(x)\n\n        means = torch.randn(3, 10)\n        variances = torch.randn(3, 10).abs()\n        quadrature = GaussHermiteQuadrature1D()\n\n        if cuda:\n            means = means.cuda()\n            variances = variances.cuda()\n            quadrature = quadrature.cuda()\n\n        dist = MultivariateNormal(means, DiagLazyTensor(variances.sqrt()))\n\n        # Use quadrature\n        results = quadrature(func, dist)\n\n        # Use Monte-Carlo\n        samples = dist.rsample(torch.Size([20000]))\n        actual = func(samples).mean(0)\n\n        self.assertLess(torch.mean(torch.abs(actual - results)), 0.1)\n\n    def test_gauss_hermite_quadrature_1D_mvn_batch_cuda(self):\n        if torch.cuda.is_available():\n            with least_used_cuda_device():\n                self.test_gauss_hermite_quadrature_1D_normal_nonbatch(cuda=True)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/utils/test_sparse.py,12,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.utils.sparse import sparse_eye, sparse_getitem, to_sparse\n\n\nclass TestSparse(unittest.TestCase):\n    def setUp(self):\n        self.indices = torch.tensor([[0, 1, 2, 3, 4], [2, 1, 0, 0, 1]], dtype=torch.long)\n        self.values = torch.tensor([3, 4, 5, 2, 6], dtype=torch.float)\n        self.sparse = torch.sparse.FloatTensor(self.indices, self.values, torch.Size((5, 3)))\n        self.dense = self.sparse.to_dense()\n\n    def test_sparse_eye(self):\n        res = sparse_eye(5)\n        actual = torch.eye(5)\n        self.assertTrue(torch.equal(res.to_dense(), actual))\n\n    def test_sparse_getitem_one_dim_int(self):\n        actual = self.dense[3]\n        res = sparse_getitem(self.sparse, 3)\n        self.assertTrue(torch.equal(actual, res.to_dense()))\n\n    def test_sparse_getitem_one_dim_slice(self):\n        actual = self.dense[2:4]\n        res = sparse_getitem(self.sparse, slice(2, 4))\n        self.assertTrue(torch.equal(actual, res.to_dense()))\n\n    def test_sparse_getitem_two_dim_int(self):\n        actual = self.dense[2, 1]\n        res = sparse_getitem(self.sparse, (2, 1))\n        self.assertEqual(actual, res)\n\n    def test_sparse_getitem_two_dim_int_slice(self):\n        actual = self.dense[:, 1]\n        res = sparse_getitem(self.sparse, (slice(None, None, None), 1))\n        self.assertTrue(torch.equal(actual, res.to_dense()))\n\n        actual = self.dense[1, :]\n        res = sparse_getitem(self.sparse, (1, slice(None, None, None)))\n        self.assertTrue(torch.equal(actual, res.to_dense()))\n\n    def test_sparse_getitem_two_dim_slice(self):\n        actual = self.dense[2:4, 1:3]\n        res = sparse_getitem(self.sparse, (slice(2, 4), slice(1, 3)))\n        self.assertTrue(torch.equal(actual, res.to_dense()))\n\n    def test_to_sparse(self):\n        actual = self.sparse\n        res = to_sparse(self.sparse.to_dense())\n        self.assertTrue(torch.equal(actual.to_dense(), res.to_dense()))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/utils/test_toeplitz.py,17,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch import utils\nfrom gpytorch.test.utils import approx_equal\n\n\nclass TestToeplitz(unittest.TestCase):\n    def test_sym_toeplitz_constructs_tensor_from_vector(self):\n        c = torch.tensor([1, 6, 4, 5], dtype=torch.float)\n\n        res = utils.toeplitz.sym_toeplitz(c)\n        actual = torch.tensor([[1, 6, 4, 5], [6, 1, 6, 4], [4, 6, 1, 6], [5, 4, 6, 1]], dtype=torch.float)\n\n        self.assertTrue(torch.equal(res, actual))\n\n    def test_toeplitz_matmul(self):\n        col = torch.tensor([1, 6, 4, 5], dtype=torch.float)\n        row = torch.tensor([1, 2, 1, 1], dtype=torch.float)\n        rhs_mat = torch.randn(4, 2)\n\n        # Actual\n        lhs_mat = utils.toeplitz.toeplitz(col, row)\n        actual = torch.matmul(lhs_mat, rhs_mat)\n\n        # Fast toeplitz\n        res = utils.toeplitz.toeplitz_matmul(col, row, rhs_mat)\n        self.assertTrue(approx_equal(res, actual))\n\n    def test_toeplitz_matmul_batch(self):\n        cols = torch.tensor([[1, 6, 4, 5], [2, 3, 1, 0], [1, 2, 3, 1]], dtype=torch.float)\n        rows = torch.tensor([[1, 2, 1, 1], [2, 0, 0, 1], [1, 5, 1, 0]], dtype=torch.float)\n\n        rhs_mats = torch.randn(3, 4, 2)\n\n        # Actual\n        lhs_mats = torch.zeros(3, 4, 4)\n        for i, (col, row) in enumerate(zip(cols, rows)):\n            lhs_mats[i].copy_(utils.toeplitz.toeplitz(col, row))\n        actual = torch.matmul(lhs_mats, rhs_mats)\n\n        # Fast toeplitz\n        res = utils.toeplitz.toeplitz_matmul(cols, rows, rhs_mats)\n        self.assertTrue(approx_equal(res, actual))\n\n    def test_toeplitz_matmul_batchmat(self):\n        col = torch.tensor([1, 6, 4, 5], dtype=torch.float)\n        row = torch.tensor([1, 2, 1, 1], dtype=torch.float)\n        rhs_mat = torch.randn(3, 4, 2)\n\n        # Actual\n        lhs_mat = utils.toeplitz.toeplitz(col, row)\n        actual = torch.matmul(lhs_mat.unsqueeze(0), rhs_mat)\n\n        # Fast toeplitz\n        res = utils.toeplitz.toeplitz_matmul(col.unsqueeze(0), row.unsqueeze(0), rhs_mat)\n        self.assertTrue(approx_equal(res, actual))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/variational/__init__.py,0,b''
test/variational/test_grid_interpolation_variational_strategy.py,22,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.test.variational_test_case import VariationalTestCase\n\n\nclass TestGridVariationalGP(VariationalTestCase, unittest.TestCase):\n    def _make_model_and_likelihood(\n        self,\n        num_inducing=8,\n        batch_shape=torch.Size([]),\n        inducing_batch_shape=torch.Size([]),\n        strategy_cls=gpytorch.variational.VariationalStrategy,\n        distribution_cls=gpytorch.variational.CholeskyVariationalDistribution,\n        constant_mean=True,\n    ):\n        class _SVGPRegressionModel(gpytorch.models.ApproximateGP):\n            def __init__(self):\n                variational_distribution = distribution_cls(num_inducing ** 2, batch_shape=batch_shape)\n                variational_strategy = strategy_cls(self, num_inducing, [(-3, 3), (-3, 3)], variational_distribution)\n                super().__init__(variational_strategy)\n                if constant_mean:\n                    self.mean_module = gpytorch.means.ConstantMean()\n                    self.mean_module.initialize(constant=1.0)\n                else:\n                    self.mean_module = gpytorch.means.ZeroMean()\n                self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n            def forward(self, x):\n                mean_x = self.mean_module(x)\n                covar_x = self.covar_module(x)\n                latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n                return latent_pred\n\n        return _SVGPRegressionModel(), self.likelihood_cls()\n\n    @property\n    def batch_shape(self):\n        return torch.Size([])\n\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.CholeskyVariationalDistribution\n\n    @property\n    def learn_inducing_locations(self):\n        return None\n\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.VariationalELBO\n\n    @property\n    def strategy_cls(self):\n        return gpytorch.variational.GridInterpolationVariationalStrategy\n\n    def test_training_iteration(self, *args, **kwargs):\n        with gpytorch.settings.max_cholesky_size(0), gpytorch.settings.use_toeplitz(False):\n            cg_mock, cholesky_mock = super().test_training_iteration(*args, **kwargs)\n        self.assertEqual(cg_mock.call_count, 2)  # One for each forward pass\n        if self.distribution_cls == gpytorch.variational.CholeskyVariationalDistribution:\n            self.assertEqual(cholesky_mock.call_count, 1)\n        else:\n            self.assertFalse(cholesky_mock.called)\n\n    def test_eval_iteration(self, *args, **kwargs):\n        with gpytorch.settings.max_cholesky_size(0):\n            cg_mock, cholesky_mock = super().test_eval_iteration(*args, **kwargs)\n        self.assertFalse(cg_mock.called)  # One to compute cache, that\'s it!\n        self.assertFalse(cholesky_mock.called)\n\n\nclass TestGridPredictiveGP(TestGridVariationalGP):\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.PredictiveLogLikelihood\n\n\nclass TestGridRobustVGP(TestGridVariationalGP):\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.GammaRobustVariationalELBO\n\n\nclass TestGridMeanFieldVariationalGP(TestGridVariationalGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.MeanFieldVariationalDistribution\n\n\nclass TestGridMeanFieldPredictiveGP(TestGridPredictiveGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.MeanFieldVariationalDistribution\n\n\nclass TestGridMeanFieldRobustVGP(TestGridRobustVGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.MeanFieldVariationalDistribution\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/variational/test_multitask_variational_strategy.py,16,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.test.variational_test_case import VariationalTestCase\n\n\ndef likelihood_cls():\n    return gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n\n\ndef strategy_cls(model, inducing_points, variational_distribution, learn_inducing_locations):\n    return gpytorch.variational.MultitaskVariationalStrategy(\n        gpytorch.variational.VariationalStrategy(\n            model, inducing_points, variational_distribution, learn_inducing_locations\n        ),\n        num_tasks=2,\n    )\n\n\nclass TestMultitaskVariationalGP(VariationalTestCase, unittest.TestCase):\n    @property\n    def batch_shape(self):\n        return torch.Size([2])\n\n    @property\n    def event_shape(self):\n        return torch.Size([32, 2])\n\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.CholeskyVariationalDistribution\n\n    @property\n    def likelihood_cls(self):\n        return likelihood_cls\n\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.VariationalELBO\n\n    @property\n    def strategy_cls(self):\n        return strategy_cls\n\n    def test_training_iteration(self, *args, expected_batch_shape=None, **kwargs):\n        expected_batch_shape = expected_batch_shape or self.batch_shape\n        expected_batch_shape = expected_batch_shape[:-1]\n        cg_mock, cholesky_mock = super().test_training_iteration(\n            *args, expected_batch_shape=expected_batch_shape, **kwargs\n        )\n        self.assertFalse(cg_mock.called)\n        self.assertEqual(cholesky_mock.call_count, 2)  # One for each forward pass\n\n    def test_eval_iteration(self, *args, expected_batch_shape=None, **kwargs):\n        expected_batch_shape = expected_batch_shape or self.batch_shape\n        expected_batch_shape = expected_batch_shape[:-1]\n        cg_mock, cholesky_mock = super().test_eval_iteration(*args, expected_batch_shape=expected_batch_shape, **kwargs)\n        self.assertFalse(cg_mock.called)\n        self.assertEqual(cholesky_mock.call_count, 1)  # One to compute cache, that\'s it!\n\n\nclass TestMultitaskPredictiveGP(TestMultitaskVariationalGP):\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.PredictiveLogLikelihood\n\n\nclass TestMultitaskRobustVGP(TestMultitaskVariationalGP):\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.GammaRobustVariationalELBO\n\n\nclass TestMeanFieldMultitaskVariationalGP(TestMultitaskVariationalGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.MeanFieldVariationalDistribution\n\n\nclass TestMeanFieldMultitaskPredictiveGP(TestMultitaskPredictiveGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.MeanFieldVariationalDistribution\n\n\nclass TestMeanFieldMultitaskRobustVGP(TestMultitaskRobustVGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.MeanFieldVariationalDistribution\n\n\nclass TestDeltaMultitaskVariationalGP(TestMultitaskVariationalGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.DeltaVariationalDistribution\n\n\nclass TestDeltaMultitaskPredictiveGP(TestMultitaskPredictiveGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.DeltaVariationalDistribution\n\n\nclass TestDeltaMultitaskRobustVGP(TestMultitaskRobustVGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.DeltaVariationalDistribution\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/variational/test_orthogonally_decoupled_variational_strategy.py,11,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.test.variational_test_case import VariationalTestCase\n\n\ndef likelihood_cls():\n    return gpytorch.likelihoods.GaussianLikelihood()\n\n\ndef strategy_cls(model, inducing_points, variational_distribution, learn_inducing_locations):\n    base_inducing_points = torch.randn(8, inducing_points.size(-1), device=inducing_points.device)\n    base_variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(8)\n    return gpytorch.variational.OrthogonallyDecoupledVariationalStrategy(\n        gpytorch.variational.VariationalStrategy(\n            model, base_inducing_points, base_variational_distribution, learn_inducing_locations\n        ),\n        inducing_points,\n        variational_distribution,\n    )\n\n\nclass TestOrthogonallyDecoupledVariationalGP(VariationalTestCase, unittest.TestCase):\n    @property\n    def batch_shape(self):\n        return torch.Size([])\n\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.DeltaVariationalDistribution\n\n    @property\n    def likelihood_cls(self):\n        return likelihood_cls\n\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.VariationalELBO\n\n    @property\n    def strategy_cls(self):\n        return strategy_cls\n\n    def test_training_iteration(self, *args, **kwargs):\n        cg_mock, cholesky_mock = super().test_training_iteration(*args, **kwargs)\n        self.assertFalse(cg_mock.called)\n        self.assertEqual(cholesky_mock.call_count, 3)  # One for each forward pass, and for computing prior dist\n\n    def test_eval_iteration(self, *args, **kwargs):\n        cg_mock, cholesky_mock = super().test_eval_iteration(*args, **kwargs)\n        self.assertFalse(cg_mock.called)\n        self.assertEqual(cholesky_mock.call_count, 1)  # One to compute cache, that\'s it!\n\n\nclass TestOrthogonallyDecoupledPredictiveGP(TestOrthogonallyDecoupledVariationalGP):\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.PredictiveLogLikelihood\n\n\nclass TestOrthogonallyDecoupledRobustVGP(TestOrthogonallyDecoupledVariationalGP):\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.GammaRobustVariationalELBO\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/variational/test_unwhitened_variational_strategy.py,21,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.test.variational_test_case import VariationalTestCase\n\n\nclass TestUnwhitenedVariationalGP(VariationalTestCase, unittest.TestCase):\n    @property\n    def batch_shape(self):\n        return torch.Size([])\n\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.CholeskyVariationalDistribution\n\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.VariationalELBO\n\n    @property\n    def strategy_cls(self):\n        return gpytorch.variational.UnwhitenedVariationalStrategy\n\n    def test_training_iteration(self, *args, **kwargs):\n        cg_mock, cholesky_mock = super().test_training_iteration(*args, **kwargs)\n        self.assertFalse(cg_mock.called)\n        if self.distribution_cls == gpytorch.variational.CholeskyVariationalDistribution:\n            self.assertEqual(cholesky_mock.call_count, 3)  # One for each forward pass, once for initialization\n        else:\n            self.assertEqual(cholesky_mock.call_count, 2)  # One for each forward pass\n\n    def test_eval_iteration(self, *args, **kwargs):\n        cg_mock, cholesky_mock = super().test_eval_iteration(*args, **kwargs)\n        self.assertFalse(cg_mock.called)\n        self.assertEqual(cholesky_mock.call_count, 1)  # One to compute cache, that\'s it!\n\n\nclass TestUnwhitenedVariationalGP_CG(TestUnwhitenedVariationalGP):\n    def test_training_iteration(self, *args, **kwargs):\n        with gpytorch.settings.max_cholesky_size(0):\n            cg_mock, cholesky_mock = VariationalTestCase.test_training_iteration(self, *args, **kwargs)\n        self.assertEqual(cg_mock.call_count, 2)  # One for each forward pass\n        if self.distribution_cls == gpytorch.variational.CholeskyVariationalDistribution:\n            self.assertEqual(cholesky_mock.call_count, 1)\n        else:\n            self.assertFalse(cholesky_mock.called)\n\n    def test_eval_iteration(self, *args, **kwargs):\n        with gpytorch.settings.max_cholesky_size(0):\n            cg_mock, cholesky_mock = VariationalTestCase.test_eval_iteration(self, *args, **kwargs)\n        self.assertEqual(cg_mock.call_count, 2)  # One for each forward pass\n        self.assertFalse(cholesky_mock.called)\n\n\nclass TestUnwhitenedVariationalGP_CG_NoLogDet(TestUnwhitenedVariationalGP_CG):\n    def test_training_iteration(self, *args, **kwargs):\n        with gpytorch.settings.skip_logdet_forward(True):\n            super().test_training_iteration(*args, **kwargs)\n\n    def test_eval_iteration(self, *args, **kwargs):\n        with gpytorch.settings.skip_logdet_forward(True):\n            super().test_eval_iteration(*args, **kwargs)\n\n\nclass TestUnwhitenedVariationalGP_CG_NoPosteriorVariance(TestUnwhitenedVariationalGP_CG):\n    def test_training_iteration(self, *args, **kwargs):\n        with gpytorch.settings.skip_posterior_variances(True):\n            super().test_training_iteration(*args, **kwargs)\n\n    def test_eval_iteration(self, *args, **kwargs):\n        with gpytorch.settings.skip_posterior_variances(True), gpytorch.settings.max_cholesky_size(0):\n            cg_mock, cholesky_mock = VariationalTestCase.test_eval_iteration(self, *args, **kwargs)\n        self.assertEqual(cg_mock.call_count, 1)  # One for the cache - and that\'s it!\n        self.assertFalse(cholesky_mock.called)\n\n\nclass TestUnwhitenedPredictiveGP(TestUnwhitenedVariationalGP):\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.PredictiveLogLikelihood\n\n\nclass TestUnwhitenedRobustVGP(TestUnwhitenedVariationalGP):\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.GammaRobustVariationalELBO\n\n\nclass TestUnwhitenedMeanFieldVariationalGP(TestUnwhitenedVariationalGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.MeanFieldVariationalDistribution\n\n\nclass TestUnwhitenedMeanFieldPredictiveGP(TestUnwhitenedPredictiveGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.MeanFieldVariationalDistribution\n\n\nclass TestUnwhitenedMeanFieldRobustVGP(TestUnwhitenedRobustVGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.MeanFieldVariationalDistribution\n\n\nclass TestUnwhitenedDeltaVariationalGP(TestUnwhitenedVariationalGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.DeltaVariationalDistribution\n\n\nclass TestUnwhitenedDeltaPredictiveGP(TestUnwhitenedPredictiveGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.DeltaVariationalDistribution\n\n\nclass TestUnwhitenedDeltaRobustVGP(TestUnwhitenedRobustVGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.DeltaVariationalDistribution\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/variational/test_variational_strategy.py,13,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.test.variational_test_case import VariationalTestCase\n\n\nclass TestVariationalGP(VariationalTestCase, unittest.TestCase):\n    @property\n    def batch_shape(self):\n        return torch.Size([])\n\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.CholeskyVariationalDistribution\n\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.VariationalELBO\n\n    @property\n    def strategy_cls(self):\n        return gpytorch.variational.VariationalStrategy\n\n    def test_training_iteration(self, *args, **kwargs):\n        cg_mock, cholesky_mock = super().test_training_iteration(*args, **kwargs)\n        self.assertFalse(cg_mock.called)\n        self.assertEqual(cholesky_mock.call_count, 2)  # One for each forward pass\n\n    def test_eval_iteration(self, *args, **kwargs):\n        cg_mock, cholesky_mock = super().test_eval_iteration(*args, **kwargs)\n        self.assertFalse(cg_mock.called)\n        self.assertEqual(cholesky_mock.call_count, 1)  # One to compute cache, that\'s it!\n\n\nclass TestPredictiveGP(TestVariationalGP):\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.PredictiveLogLikelihood\n\n\nclass TestRobustVGP(TestVariationalGP):\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.GammaRobustVariationalELBO\n\n\nclass TestMeanFieldVariationalGP(TestVariationalGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.MeanFieldVariationalDistribution\n\n\nclass TestMeanFieldPredictiveGP(TestPredictiveGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.MeanFieldVariationalDistribution\n\n\nclass TestMeanFieldRobustVGP(TestRobustVGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.MeanFieldVariationalDistribution\n\n\nclass TestDeltaVariationalGP(TestVariationalGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.DeltaVariationalDistribution\n\n\nclass TestDeltaPredictiveGP(TestPredictiveGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.DeltaVariationalDistribution\n\n\nclass TestDeltaRobustVGP(TestRobustVGP):\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.DeltaVariationalDistribution\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/variational/test_whitened_variational_strategy.py,32,"b'#!/usr/bin/env python3\n\nimport unittest\nimport warnings\nfrom unittest.mock import MagicMock, patch\n\nimport torch\n\nimport gpytorch\nfrom gpytorch.test.base_test_case import BaseTestCase\nfrom gpytorch.utils.warnings import ExtraComputationWarning\n\n\nclass TestVariationalGP(BaseTestCase, unittest.TestCase):\n    @property\n    def batch_shape(self):\n        return torch.Size([])\n\n    @property\n    def distribution_cls(self):\n        return gpytorch.variational.CholeskyVariationalDistribution\n\n    @property\n    def mll_cls(self):\n        return gpytorch.mlls.VariationalELBO\n\n    @property\n    def strategy_cls(self):\n        return gpytorch.variational.WhitenedVariationalStrategy\n\n    def _make_model_and_likelihood(\n        self,\n        num_inducing=16,\n        batch_shape=torch.Size([]),\n        inducing_batch_shape=torch.Size([]),\n        strategy_cls=gpytorch.variational.VariationalStrategy,\n        distribution_cls=gpytorch.variational.CholeskyVariationalDistribution,\n        constant_mean=True,\n    ):\n        # This class is deprecated - so we\'ll ignore these warnings\n        warnings.simplefilter(""always"", DeprecationWarning)\n\n        class _SVGPRegressionModel(gpytorch.models.ApproximateGP):\n            def __init__(self, inducing_points):\n                variational_distribution = distribution_cls(num_inducing, batch_shape=batch_shape)\n                variational_strategy = strategy_cls(\n                    self, inducing_points, variational_distribution, learn_inducing_locations=True\n                )\n                super().__init__(variational_strategy)\n                if constant_mean:\n                    self.mean_module = gpytorch.means.ConstantMean()\n                    self.mean_module.initialize(constant=1.0)\n                else:\n                    self.mean_module = gpytorch.means.ZeroMean()\n                self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n\n            def forward(self, x):\n                mean_x = self.mean_module(x)\n                covar_x = self.covar_module(x)\n                latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n                return latent_pred\n\n        inducing_points = torch.randn(num_inducing, 2).repeat(*inducing_batch_shape, 1, 1)\n        with warnings.catch_warnings(record=True):\n            return _SVGPRegressionModel(inducing_points), self.likelihood_cls()\n\n    def _training_iter(self, model, likelihood, batch_shape=torch.Size([]), mll_cls=gpytorch.mlls.VariationalELBO):\n        # This class is deprecated - so we\'ll ignore these warnings\n        warnings.simplefilter(""always"", DeprecationWarning)\n\n        train_x = torch.randn(*batch_shape, 32, 2).clamp(-2.5, 2.5)\n        train_y = torch.linspace(-1, 1, self.event_shape[0])\n        train_y = train_y.view(self.event_shape[0], *([1] * (len(self.event_shape) - 1)))\n        train_y = train_y.expand(*self.event_shape)\n        mll = mll_cls(likelihood, model, num_data=train_x.size(-2))\n\n        # Single optimization iteration\n        model.train()\n        likelihood.train()\n        with warnings.catch_warnings(record=True) as ws:\n            output = model(train_x)\n            loss = -mll(output, train_y)\n            loss.sum().backward()\n\n        # Make sure we have gradients for all parameters\n        for _, param in model.named_parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n        for _, param in likelihood.named_parameters():\n            self.assertTrue(param.grad is not None)\n            self.assertGreater(param.grad.norm().item(), 0)\n\n        # Make sure there were no warnings\n        self.assertFalse(any(issubclass(w.category, ExtraComputationWarning) for w in ws))\n        return output, loss\n\n    def _eval_iter(self, model, batch_shape=torch.Size([])):\n        # This class is deprecated - so we\'ll ignore these warnings\n        warnings.simplefilter(""always"", DeprecationWarning)\n\n        test_x = torch.randn(*batch_shape, 32, 2).clamp(-2.5, 2.5)\n\n        # Single optimization iteration\n        model.eval()\n        with warnings.catch_warnings(record=True) as ws, torch.no_grad():\n            output = model(test_x)\n\n        # Make sure there were no warnings\n        self.assertFalse(any(issubclass(w.category, ExtraComputationWarning) for w in ws))\n        return output\n\n    @property\n    def event_shape(self):\n        return torch.Size([32])\n\n    @property\n    def likelihood_cls(self):\n        return gpytorch.likelihoods.GaussianLikelihood\n\n    def test_eval_iteration(\n        self,\n        data_batch_shape=None,\n        inducing_batch_shape=None,\n        model_batch_shape=None,\n        eval_data_batch_shape=None,\n        expected_batch_shape=None,\n    ):\n        # This class is deprecated - so we\'ll ignore these warnings\n        warnings.simplefilter(""always"", DeprecationWarning)\n\n        # Batch shapes\n        model_batch_shape = model_batch_shape if model_batch_shape is not None else self.batch_shape\n        data_batch_shape = data_batch_shape if data_batch_shape is not None else self.batch_shape\n        inducing_batch_shape = inducing_batch_shape if inducing_batch_shape is not None else self.batch_shape\n        expected_batch_shape = expected_batch_shape if expected_batch_shape is not None else self.batch_shape\n        eval_data_batch_shape = eval_data_batch_shape if eval_data_batch_shape is not None else self.batch_shape\n\n        # Mocks\n        _wrapped_cholesky = MagicMock(wraps=torch.cholesky)\n        _wrapped_cg = MagicMock(wraps=gpytorch.utils.linear_cg)\n        _cholesky_mock = patch(""torch.cholesky"", new=_wrapped_cholesky)\n        _cg_mock = patch(""gpytorch.utils.linear_cg"", new=_wrapped_cg)\n\n        # Make model and likelihood\n        model, likelihood = self._make_model_and_likelihood(\n            batch_shape=model_batch_shape,\n            inducing_batch_shape=inducing_batch_shape,\n            distribution_cls=self.distribution_cls,\n            strategy_cls=self.strategy_cls,\n        )\n\n        # Do one forward pass\n        self._training_iter(model, likelihood, data_batch_shape, mll_cls=self.mll_cls)\n\n        # Now do evaluatioj\n        with _cholesky_mock as cholesky_mock, _cg_mock as cg_mock:\n            # Iter 1\n            _ = self._eval_iter(model, eval_data_batch_shape)\n            output = self._eval_iter(model, eval_data_batch_shape)\n            self.assertEqual(output.batch_shape, expected_batch_shape)\n            self.assertEqual(output.event_shape, self.event_shape)\n            return cg_mock, cholesky_mock\n\n    def test_training_iteration(\n        self,\n        data_batch_shape=None,\n        inducing_batch_shape=None,\n        model_batch_shape=None,\n        expected_batch_shape=None,\n        constant_mean=True,\n    ):\n        # This class is deprecated - so we\'ll ignore these warnings\n        warnings.simplefilter(""always"", DeprecationWarning)\n\n        # Batch shapes\n        model_batch_shape = model_batch_shape if model_batch_shape is not None else self.batch_shape\n        data_batch_shape = data_batch_shape if data_batch_shape is not None else self.batch_shape\n        inducing_batch_shape = inducing_batch_shape if inducing_batch_shape is not None else self.batch_shape\n        expected_batch_shape = expected_batch_shape if expected_batch_shape is not None else self.batch_shape\n\n        # Mocks\n        _wrapped_cholesky = MagicMock(wraps=torch.cholesky)\n        _wrapped_cg = MagicMock(wraps=gpytorch.utils.linear_cg)\n        _cholesky_mock = patch(""torch.cholesky"", new=_wrapped_cholesky)\n        _cg_mock = patch(""gpytorch.utils.linear_cg"", new=_wrapped_cg)\n\n        # Make model and likelihood\n        model, likelihood = self._make_model_and_likelihood(\n            batch_shape=model_batch_shape,\n            inducing_batch_shape=inducing_batch_shape,\n            distribution_cls=self.distribution_cls,\n            strategy_cls=self.strategy_cls,\n            constant_mean=constant_mean,\n        )\n\n        # Do forward pass\n        with _cholesky_mock as cholesky_mock, _cg_mock as cg_mock:\n            # Iter 1\n            self.assertEqual(model.variational_strategy.variational_params_initialized.item(), 0)\n            self._training_iter(model, likelihood, data_batch_shape, mll_cls=self.mll_cls)\n            self.assertEqual(model.variational_strategy.variational_params_initialized.item(), 1)\n            # Iter 2\n            output, loss = self._training_iter(model, likelihood, data_batch_shape, mll_cls=self.mll_cls)\n            self.assertEqual(output.batch_shape, expected_batch_shape)\n            self.assertEqual(output.event_shape, self.event_shape)\n            self.assertEqual(loss.shape, expected_batch_shape)\n            return cg_mock, cholesky_mock\n'"
gpytorch/kernels/keops/__init__.py,0,"b'from .matern_kernel import MaternKernel\nfrom .rbf_kernel import RBFKernel\n\n__all__ = [""MaternKernel"", ""RBFKernel""]\n'"
gpytorch/kernels/keops/keops_kernel.py,1,"b'from abc import abstractmethod\n\nimport torch\n\nfrom ..kernel import Kernel\n\ntry:\n    from pykeops.torch import LazyTensor as KEOLazyTensor\n\n    class KeOpsKernel(Kernel):\n        @abstractmethod\n        def covar_func(self, x1: torch.Tensor, x2: torch.Tensor) -> KEOLazyTensor:\n            raise NotImplementedError(""KeOpsKernels must define a covar_func method"")\n\n\nexcept ImportError:\n\n    class KeOpsKernel(Kernel):\n        def __init__(self, *args, **kwargs):\n            raise RuntimeError(""You must have KeOps installed to use a KeOpsKernel"")\n'"
gpytorch/kernels/keops/matern_kernel.py,3,"b'#!/usr/bin/env python3\nimport math\n\nimport torch\n\nfrom ...lazy import KeOpsLazyTensor\nfrom .keops_kernel import KeOpsKernel\n\ntry:\n    from pykeops.torch import LazyTensor as KEOLazyTensor\n\n    class MaternKernel(KeOpsKernel):\n        """"""\n        Implements the Matern kernel using KeOps as a driver for kernel matrix multiplies.\n\n        This class can be used as a drop in replacement for gpytorch.kernels.MaternKernel in most cases, and supports\n        the same arguments. There are currently a few limitations, for example a lack of batch mode support. However,\n        most other features like ARD will work.\n        """"""\n\n        has_lengthscale = True\n\n        def __init__(self, nu=2.5, **kwargs):\n            if nu not in {0.5, 1.5, 2.5}:\n                raise RuntimeError(""nu expected to be 0.5, 1.5, or 2.5"")\n            super(MaternKernel, self).__init__(**kwargs)\n            self.nu = nu\n\n        def covar_func(self, x1, x2, diag=False):\n            # TODO: x1 / x2 size checks are a work around for a very minor bug in KeOps.\n            # This bug is fixed on KeOps master, and we\'ll remove that part of the check\n            # when they cut a new release.\n            if diag or x1.size(-2) == 1 or x2.size(-2) == 1:\n                distance = self.covar_dist(x1, x2, diag=diag)\n                exp_component = torch.exp(-math.sqrt(self.nu * 2) * distance)\n\n                if self.nu == 0.5:\n                    constant_component = 1\n                elif self.nu == 1.5:\n                    constant_component = (math.sqrt(3) * distance).add(1)\n                elif self.nu == 2.5:\n                    constant_component = (math.sqrt(5) * distance).add(1).add(5.0 / 3.0 * distance ** 2)\n                return constant_component * exp_component\n            else:\n                with torch.autograd.enable_grad():\n                    x1_ = KEOLazyTensor(x1[..., :, None, :])\n                    x2_ = KEOLazyTensor(x2[..., None, :, :])\n\n                    distance = ((x1_ - x2_) ** 2).sum(-1).sqrt()\n                    exp_component = (-math.sqrt(self.nu * 2) * distance).exp()\n\n                    if self.nu == 0.5:\n                        constant_component = 1\n                    elif self.nu == 1.5:\n                        constant_component = (math.sqrt(3) * distance) + 1\n                    elif self.nu == 2.5:\n                        constant_component = (math.sqrt(5) * distance) + (1 + 5.0 / 3.0 * distance ** 2)\n\n                    return constant_component * exp_component\n\n        def forward(self, x1, x2, diag=False, **params):\n            mean = x1.reshape(-1, x1.size(-1)).mean(0)[(None,) * (x1.dim() - 1)]\n\n            x1_ = (x1 - mean).div(self.lengthscale)\n            x2_ = (x2 - mean).div(self.lengthscale)\n\n            if diag:\n                return self.covar_func(x1_, x2_, diag=True)\n\n            covar_func = lambda x1, x2, diag=False: self.covar_func(x1, x2, diag)\n            return KeOpsLazyTensor(x1_, x2_, covar_func)\n\n\nexcept ImportError:\n\n    class MaternKernel(KeOpsKernel):\n        def __init__(self, *args, **kwargs):\n            super().__init__()\n'"
gpytorch/kernels/keops/rbf_kernel.py,2,"b'import torch\n\nfrom ...lazy import KeOpsLazyTensor\nfrom ..rbf_kernel import postprocess_rbf\nfrom .keops_kernel import KeOpsKernel\n\ntry:\n    from pykeops.torch import LazyTensor as KEOLazyTensor\n\n    class RBFKernel(KeOpsKernel):\n        """"""\n        Implements the RBF kernel using KeOps as a driver for kernel matrix multiplies.\n\n        This class can be used as a drop in replacement for gpytorch.kernels.RBFKernel in most cases, and supports\n        the same arguments. There are currently a few limitations, for example a lack of batch mode support. However,\n        most other features like ARD will work.\n        """"""\n\n        has_lengthscale = True\n\n        def covar_func(self, x1, x2, diag=False):\n            # TODO: x1 / x2 size checks are a work around for a very minor bug in KeOps.\n            # This bug is fixed on KeOps master, and we\'ll remove that part of the check\n            # when they cut a new release.\n            if diag or x1.size(-2) == 1 or x2.size(-2) == 1:\n                return self.covar_dist(\n                    x1, x2, square_dist=True, diag=diag, dist_postprocess_func=postprocess_rbf, postprocess=True\n                )\n            else:\n                with torch.autograd.enable_grad():\n                    x1_ = KEOLazyTensor(x1[..., :, None, :])\n                    x2_ = KEOLazyTensor(x2[..., None, :, :])\n\n                    K = (-((x1_ - x2_) ** 2).sum(-1) / 2).exp()\n\n                    return K\n\n        def forward(self, x1, x2, diag=False, **params):\n            x1_ = x1.div(self.lengthscale)\n            x2_ = x2.div(self.lengthscale)\n            if diag:\n                return self.covar_func(x1_, x2_, diag=True)\n\n            covar_func = lambda x1, x2, diag=False: self.covar_func(x1, x2, diag)\n            return KeOpsLazyTensor(x1_, x2_, covar_func)\n\n\nexcept ImportError:\n\n    class RBFKernel(KeOpsKernel):\n        def __init__(self, *args, **kwargs):\n            super().__init__()\n'"
gpytorch/models/deep_gps/__init__.py,0,"b'#!/usr/bin/env python3\n\nimport warnings\n\nfrom .deep_gp import DeepGP, DeepGPLayer, DeepLikelihood\n\n\n# Deprecated for 1.0 release\nclass AbstractDeepGP(DeepGP):\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""AbstractDeepGP has been renamed to DeepGP."", DeprecationWarning)\n        super().__init__(*args, **kwargs)\n\n\n# Deprecated for 1.0 release\nclass AbstractDeepGPLayer(DeepGPLayer):\n    def __init__(self, *args, **kwargs):\n        warnings.warn(""AbstractDeepGPLayer has been renamed to DeepGPLayer."", DeprecationWarning)\n        super().__init__(*args, **kwargs)\n\n\n__all__ = [""DeepGPLayer"", ""DeepGP"", ""AbstractDeepGPLayer"", ""AbstractDeepGP"", ""DeepLikelihood""]\n'"
gpytorch/models/deep_gps/deep_gp.py,10,"b'import warnings\n\nimport torch\n\nfrom gpytorch import settings\nfrom gpytorch.distributions import MultitaskMultivariateNormal\nfrom gpytorch.lazy import BlockDiagLazyTensor\nfrom gpytorch.likelihoods import Likelihood\n\nfrom ..approximate_gp import ApproximateGP\nfrom ..gp import GP\n\n\nclass _DeepGPVariationalStrategy(object):\n    def __init__(self, model):\n        self.model = model\n\n    @property\n    def sub_variational_strategies(self):\n        if not hasattr(self, ""_sub_variational_strategies_memo""):\n            self._sub_variational_strategies_memo = [\n                module.variational_strategy for module in self.model.modules() if isinstance(module, ApproximateGP)\n            ]\n        return self._sub_variational_strategies_memo\n\n    def kl_divergence(self):\n        return sum(strategy.kl_divergence().sum() for strategy in self.sub_variational_strategies)\n\n\nclass DeepGPLayer(ApproximateGP):\n    """"""\n    Represents a layer in a deep GP where inference is performed via the doubly stochastic method of\n    Salimbeni et al., 2017. Upon calling, instead of returning a variational distribution q(f), returns samples\n    from the variational distribution.\n\n    See the documentation for __call__ below for more details below. Note that the behavior of __call__\n    will change to be much more elegant with multiple batch dimensions; however, the interface doesn\'t really\n    change.\n\n    :param ~gpytorch.variational.VariationalStrategy variational_strategy: Strategy for\n        changing q(u) -> q(f) (see other VI docs)\n    :param int input_dims`: Dimensionality of input data expected by each GP\n    :param int output_dims: (default None) Number of GPs in this layer, equivalent to\n        output dimensionality. If set to `None`, then the output dimension will be squashed.\n\n    Forward data through this hidden GP layer. The output is a MultitaskMultivariateNormal distribution\n    (or MultivariateNormal distribution is output_dims=None).\n\n    If the input is >=2 dimensional Tensor (e.g. `n x d`), we pass the input through each hidden GP,\n    resulting in a `n x h` multitask Gaussian distribution (where all of the `h` tasks represent an\n    output dimension and are independent from one another).  We then draw `s` samples from these Gaussians,\n    resulting in a `s x n x h` MultitaskMultivariateNormal distribution.\n\n    If the input is a >=3 dimensional Tensor, and the `are_samples=True` kwarg is set, then we assume that\n    the outermost batch dimension is a samples dimension. The output will have the same number of samples.\n    For example, a `s x b x n x d` input will result in a `s x b x n x h` MultitaskMultivariateNormal distribution.\n\n    The goal of these last two points is that if you have a tensor `x` that is `n x d`, then\n\n        >>> hidden_gp2(hidden_gp(x))\n\n    will just work, and return a tensor of size `s x n x h2`, where `h2` is the output dimensionality of\n    hidden_gp2. In this way, hidden GP layers are easily composable.\n    """"""\n\n    def __init__(self, variational_strategy, input_dims, output_dims):\n        super(DeepGPLayer, self).__init__(variational_strategy)\n        self.input_dims = input_dims\n        self.output_dims = output_dims\n\n    def forward(self, x):\n        raise NotImplementedError\n\n    def __call__(self, inputs, are_samples=False, **kwargs):\n        deterministic_inputs = not are_samples\n        if isinstance(inputs, MultitaskMultivariateNormal):\n            inputs = torch.distributions.Normal(loc=inputs.mean, scale=inputs.variance.sqrt()).rsample()\n            deterministic_inputs = False\n\n        if settings.debug.on():\n            if not torch.is_tensor(inputs):\n                raise ValueError(\n                    ""`inputs` should either be a MultitaskMultivariateNormal or a Tensor, got ""\n                    f""{inputs.__class__.__Name__}""\n                )\n\n            if inputs.size(-1) != self.input_dims:\n                raise RuntimeError(\n                    f""Input shape did not match self.input_dims. Got total feature dims [{inputs.size(-1)}],""\n                    f"" expected [{self.input_dims}]""\n                )\n\n        # Repeat the input for all possible outputs\n        if self.output_dims is not None:\n            inputs = inputs.unsqueeze(-3)\n            inputs = inputs.expand(*inputs.shape[:-3], self.output_dims, *inputs.shape[-2:])\n\n        # Now run samples through the GP\n        output = ApproximateGP.__call__(self, inputs)\n        if self.output_dims is not None:\n            mean = output.loc.transpose(-1, -2)\n            covar = BlockDiagLazyTensor(output.lazy_covariance_matrix, block_dim=-3)\n            output = MultitaskMultivariateNormal(mean, covar, interleaved=False)\n\n        # Maybe expand inputs?\n        if deterministic_inputs:\n            output = output.expand(torch.Size([settings.num_likelihood_samples.value()]) + output.batch_shape)\n\n        return output\n\n\nclass DeepGP(GP):\n    """"""\n    A container module to build a DeepGP.\n    This module should contain :obj:`~gpytorch.models.deep.DeepGPLayer`\n    modules, and can also contain other modules as well.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self.variational_strategy = _DeepGPVariationalStrategy(self)\n\n    def forward(self, x):\n        raise NotImplementedError\n\n\nclass DeepLikelihood(Likelihood):\n    """"""\n    A wrapper to make a GPyTorch likelihood compatible with Deep GPs\n\n    Example:\n        >>> deep_gaussian_likelihood = gpytorch.likelihoods.DeepLikelihood(gpytorch.likelihood.GaussianLikelihood)\n    """"""\n\n    def __init__(self, base_likelihood):\n        super().__init__()\n        warnings.warn(\n            ""DeepLikelihood is now deprecated. Use a standard likelihood in conjunction with a ""\n            ""gpytorch.mlls.DeepApproximateMLL. See the DeepGP example in our documentation."",\n            DeprecationWarning,\n        )\n        self.base_likelihood = base_likelihood\n\n    def expected_log_prob(self, observations, function_dist, *params, **kwargs):\n        return self.base_likelihood.expected_log_prob(observations, function_dist, *params, **kwargs).mean(dim=0)\n\n    def log_marginal(self, observations, function_dist, *params, **kwargs):\n        return self.base_likelihood.log_marginal(observations, function_dist, *params, **kwargs).mean(dim=0)\n\n    def forward(self, *args, **kwargs):\n        pass\n\n    def __call__(self, *args, **kwargs):\n        return self.base_likelihood.__call__(*args, **kwargs)\n'"
gpytorch/models/pyro/__init__.py,0,"b'#!/usr/bin/env python3\n\ntry:\n    from .pyro_gp import PyroGP\n    from ._pyro_mixin import _PyroMixin\nexcept ImportError:\n\n    class PyroGP(object):\n        def __init__(self, *args, **kwargs):\n            raise RuntimeError(""Cannot use a PyroGP because you dont have Pyro installed."")\n\n    class _PyroMixin(object):\n        def pyro_factors(self, *args, **kwargs):\n            raise RuntimeError(""Cannot call `pyro_factors` because you dont have Pyro installed."")\n\n        def pyro_guide(self, *args, **kwargs):\n            raise RuntimeError(""Cannot call `pyro_sample` because you dont have Pyro installed."")\n\n        def pyro_model(self, *args, **kwargs):\n            raise RuntimeError(""Cannot call `pyro_sample` because you dont have Pyro installed."")\n\n\n__all__ = [""PyroGP"", ""_PyroMixin""]\n'"
gpytorch/models/pyro/_pyro_mixin.py,2,"b'#!/usr/bin/env python3\n\nimport pyro\nimport torch\n\n\nclass _PyroMixin(object):\n    def pyro_guide(self, input, beta=1.0, name_prefix=""""):\n        # Inducing values q(u)\n        with pyro.poutine.scale(scale=beta):\n            pyro.sample(name_prefix + "".u"", self.variational_strategy.variational_distribution)\n\n        # Draw samples from q(f)\n        function_dist = self(input, prior=False)\n        function_dist = pyro.distributions.Normal(loc=function_dist.mean, scale=function_dist.stddev).to_event(\n            len(function_dist.event_shape) - 1\n        )\n        return function_dist.mask(False)\n\n    def pyro_model(self, input, beta=1.0, name_prefix=""""):\n        # Inducing values p(u)\n        with pyro.poutine.scale(scale=beta):\n            u_samples = pyro.sample(self.name_prefix + "".u"", self.variational_strategy.prior_distribution)\n\n        # Include term for GPyTorch priors\n        log_prior = torch.tensor(0.0, dtype=u_samples.dtype, device=u_samples.device)\n        for _, prior, closure, _ in self.named_priors():\n            log_prior.add_(prior.log_prob(closure()).sum().div(self.num_data))\n        pyro.factor(name_prefix + "".log_prior"", log_prior)\n\n        # Include factor for added loss terms\n        added_loss = torch.tensor(0.0, dtype=u_samples.dtype, device=u_samples.device)\n        for added_loss_term in self.added_loss_terms():\n            added_loss.add_(added_loss_term.loss())\n        pyro.factor(name_prefix + "".added_loss"", added_loss)\n\n        # Draw samples from p(f)\n        function_dist = self(input, prior=True)\n        function_dist = pyro.distributions.Normal(loc=function_dist.mean, scale=function_dist.stddev).to_event(\n            len(function_dist.event_shape) - 1\n        )\n        return function_dist.mask(False)\n'"
gpytorch/models/pyro/pyro_gp.py,10,"b'#!/usr/bin/env python3\n\nimport pyro\n\nfrom ..gp import GP\nfrom ._pyro_mixin import _PyroMixin\n\n\nclass PyroGP(GP, _PyroMixin):\n    """"""\n    A :obj:`~gpytorch.models.ApproximateGP` designed to work with Pyro.\n\n    This module makes it possible to include GP models with more complex probablistic models,\n    or to use likelihood functions with additional variational/approximate distributions.\n\n    The parameters of these models are learned using Pyro\'s inference tools, unlike other models\n    that optimize models with respect to a :obj:`~gpytorch.mlls.MarginalLogLikelihood`.\n    See `the Pyro examples <examples/09_Pyro_Integration/index.html>`_ for detailed examples.\n\n    Args:\n        :attr:`variational_strategy` (:obj:`~gpytorch.variational.VariationalStrategy`):\n            The variational strategy that defines the variational distribution and\n            the marginalization strategy.\n        :attr:`likelihood` (:obj:`~gpytorch.likelihoods.Likelihood`):\n            The likelihood for the model\n        :attr:`num_data` (int):\n            The total number of training data points (necessary for SGD)\n        :attr:`name_prefix` (str, optional):\n            A prefix to put in front of pyro sample/plate sites\n        :attr:`beta` (float - default 1.):\n            A multiplicative factor for the KL divergence term.\n            Setting it to 1 (default) recovers true variational inference\n            (as derived in `Scalable Variational Gaussian Process Classification`_).\n            Setting it to anything less than 1 reduces the regularization effect of the model\n            (similarly to what was proposed in `the beta-VAE paper`_).\n\n    Example:\n        >>> class MyVariationalGP(gpytorch.models.PyroGP):\n        >>>     # implementation\n        >>>\n        >>> # variational_strategy = ...\n        >>> likelihood = gpytorch.likelihoods.GaussianLikelihood()\n        >>> model = MyVariationalGP(variational_strategy, likelihood, train_y.size())\n        >>>\n        >>> optimizer = pyro.optim.Adam({""lr"": 0.01})\n        >>> elbo = pyro.infer.Trace_ELBO(num_particles=64, vectorize_particles=True)\n        >>> svi = pyro.infer.SVI(model.model, model.guide, optimizer, elbo)\n        >>>\n        >>> # Optimize variational parameters\n        >>> for _ in range(n_iter):\n        >>>    loss = svi.step(train_x, train_y)\n\n    .. _Scalable Variational Gaussian Process Classification:\n        http://proceedings.mlr.press/v38/hensman15.pdf\n    .. _the beta-VAE paper:\n        https://openreview.net/pdf?id=Sy2fzU9gl\n    """"""\n\n    def __init__(self, variational_strategy, likelihood, num_data, name_prefix="""", beta=1.0):\n        super().__init__()\n        self.variational_strategy = variational_strategy\n        self.name_prefix = name_prefix\n        self.likelihood = likelihood\n        self.num_data = num_data\n        self.beta = beta\n\n        # Set values for the likelihood\n        self.likelihood.num_data = num_data\n        self.likelihood.name_prefix = name_prefix\n\n    def guide(self, input, target, *args, **kwargs):\n        r""""""\n        Guide function for Pyro inference.\n        Includes the guide for the GP\'s likelihood function as well.\n\n        :param torch.Tensor input: :math:`\\mathbf X` The input values values\n        :param torch.Tensor target: :math:`\\mathbf y` The target values\n        :param args: Additional arguments passed to the likelihood\'s forward function.\n        :param kwargs: Additional keyword arguments passed to the likelihood\'s forward function.\n        """"""\n        # Get q(f)\n        function_dist = self.pyro_guide(input, beta=self.beta, name_prefix=self.name_prefix)\n        return self.likelihood.pyro_guide(function_dist, target, *args, **kwargs)\n\n    def model(self, input, target, *args, **kwargs):\n        r""""""\n        Model function for Pyro inference.\n        Includes the model for the GP\'s likelihood function as well.\n\n        :param torch.Tensor input: :math:`\\mathbf X` The input values values\n        :param torch.Tensor target: :math:`\\mathbf y` The target values\n        :param args: Additional arguments passed to the likelihood\'s forward function.\n        :param kwargs: Additional keyword arguments passed to the likelihood\'s forward function.\n        """"""\n        # Include module\n        pyro.module(self.name_prefix + "".gp"", self)\n\n        # Get p(f)\n        function_dist = self.pyro_model(input, beta=self.beta, name_prefix=self.name_prefix)\n        return self.likelihood.pyro_model(function_dist, target, *args, **kwargs)\n\n    def __call__(self, inputs, prior=False):\n        if inputs.dim() == 1:\n            inputs = inputs.unsqueeze(-1)\n        return self.variational_strategy(inputs, prior=prior)\n'"
test/kernels/keops/__init__.py,0,b''
test/kernels/keops/test_matern_kernel.py,14,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import MaternKernel as GMaternKernel\nfrom gpytorch.kernels.keops import MaternKernel\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\ntry:\n    import pykeops  # noqa\n\n    class TestMatern25KeOpsBaseKernel(unittest.TestCase, BaseKernelTestCase):\n        def create_kernel_no_ard(self, **kwargs):\n            return MaternKernel(nu=2.5, **kwargs)\n\n        def create_kernel_ard(self, num_dims, **kwargs):\n            return MaternKernel(nu=2.5, ard_num_dims=num_dims, **kwargs)\n\n    class TestMaternKeOpsKernel(unittest.TestCase):\n        def test_forward_x1_eq_x2(self, nu):\n            if not torch.cuda.is_available():\n                return\n\n            x1 = torch.randn(100, 3).cuda()\n\n            kern1 = MaternKernel(nu=nu).cuda()\n            kern2 = GMaternKernel(nu=nu).cuda()\n\n            k1 = kern1(x1, x1).evaluate()\n            k2 = kern2(x1, x1).evaluate()\n\n            self.assertLess(torch.norm(k1 - k2), 1e-4)\n\n        def test_forward_x1_neq_x2(self, nu):\n            if not torch.cuda.is_available():\n                return\n\n            x1 = torch.randn(100, 3).cuda()\n            x2 = torch.randn(50, 3).cuda()\n\n            kern1 = MaternKernel(nu=nu).cuda()\n            kern2 = GMaternKernel(nu=nu).cuda()\n\n            k1 = kern1(x1, x2).evaluate()\n            k2 = kern2(x1, x2).evaluate()\n\n            self.assertLess(torch.norm(k1 - k2), 1e-4)\n\n        def test_forward_nu25_x1_eq_x2(self):\n            return self.test_forward_x1_eq_x2(nu=2.5)\n\n        def test_forward_nu25_x1_neq_x2(self):\n            return self.test_forward_nu05_x1_neq_x2(nu=2.5)\n\n        def test_forward_nu15_x1_eq_x2(self):\n            return self.test_forward_x1_eq_x2(nu=1.5)\n\n        def test_forward_nu15_x1_neq_x2(self):\n            return self.test_forward_x1_neq_x2(nu=1.5)\n\n        def test_forward_nu05_x1_eq_x2(self):\n            return self.test_forward_x1_eq_x2(nu=0.5)\n\n        def test_forward_nu05_x1_neq_x2(self):\n            return self.test_forward_x1_neq_x2(nu=0.5)\n\n        def test_batch_matmul(self):\n            if not torch.cuda.is_available():\n                return\n\n            x1 = torch.randn(3, 2, 100, 3).cuda()\n            kern1 = MaternKernel(nu=2.5).cuda()\n            kern2 = GMaternKernel(nu=2.5).cuda()\n\n            rhs = torch.randn(3, 2, 100, 1).cuda()\n            res1 = kern1(x1, x1).matmul(rhs)\n            res2 = kern2(x1, x1).matmul(rhs)\n\n            self.assertLess(torch.norm(res1 - res2), 1e-4)\n\n\nexcept ImportError:\n    pass\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/kernels/keops/test_rbf_kernel.py,14,"b'#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nfrom gpytorch.kernels import RBFKernel as GRBFKernel\nfrom gpytorch.kernels.keops import RBFKernel\nfrom gpytorch.test.base_kernel_test_case import BaseKernelTestCase\n\ntry:\n    import pykeops  # noqa\n\n    class TestRBFKeOpsBaseKernel(unittest.TestCase, BaseKernelTestCase):\n        def create_kernel_no_ard(self, **kwargs):\n            return RBFKernel(**kwargs)\n\n        def create_kernel_ard(self, num_dims, **kwargs):\n            return RBFKernel(ard_num_dims=num_dims, **kwargs)\n\n    class TestRBFKeOpsKernel(unittest.TestCase):\n        def test_forward_x1_eq_x2(self):\n            if not torch.cuda.is_available():\n                return\n\n            x1 = torch.randn(100, 3).cuda()\n\n            kern1 = RBFKernel().cuda()\n            kern2 = GRBFKernel().cuda()\n\n            k1 = kern1(x1, x1).evaluate()\n            k2 = kern2(x1, x1).evaluate()\n\n            self.assertLess(torch.norm(k1 - k2), 1e-4)\n\n        def test_forward_x1_neq_x2(self):\n            if not torch.cuda.is_available():\n                return\n\n            x1 = torch.randn(100, 3).cuda()\n            x2 = torch.randn(50, 3).cuda()\n\n            kern1 = RBFKernel().cuda()\n            kern2 = GRBFKernel().cuda()\n\n            k1 = kern1(x1, x2).evaluate()\n            k2 = kern2(x1, x2).evaluate()\n\n            self.assertLess(torch.norm(k1 - k2), 1e-4)\n\n        def test_batch_matmul(self):\n            if not torch.cuda.is_available():\n                return\n\n            x1 = torch.randn(3, 2, 100, 3).cuda()\n            kern1 = RBFKernel().cuda()\n            kern2 = GRBFKernel().cuda()\n\n            rhs = torch.randn(3, 2, 100, 1).cuda()\n            res1 = kern1(x1, x1).matmul(rhs)\n            res2 = kern2(x1, x1).matmul(rhs)\n\n            self.assertLess(torch.norm(res1 - res2), 1e-4)\n\n\nexcept ImportError:\n    pass\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
