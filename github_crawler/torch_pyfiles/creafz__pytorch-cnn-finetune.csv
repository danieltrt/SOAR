file_path,api_count,code
setup.py,0,"b'import re\nimport os\n\nfrom setuptools import setup, find_packages\n\n\ndef get_version():\n    filepath = os.path.join(\n        os.path.dirname(__file__), \'cnn_finetune\', \'__init__.py\'\n    )\n    with open(filepath) as f:\n        return re.findall(r""__version__ = \'([\\d.\\w]+)\'"", f.read())[0]\n\n\ndef get_long_description():\n    base_dir = os.path.abspath(os.path.dirname(__file__))\n    with open(os.path.join(base_dir, \'README.md\')) as f:\n        return f.read()\n\n\nrequirements = [\n    \'torch\',\n    \'torchvision>=0.3.0\',\n    \'pretrainedmodels>=0.7.4\',\n    \'scipy\',  # required for torchvision\n    \'tqdm\',  # required for pretrainedmodels\n]\n\n\ntests_requirements = [\n    \'pytest\',\n    \'numpy\',\n    \'pytest-cov\',\n]\n\n\nsetup(\n    name=\'cnn_finetune\',\n    version=get_version(),\n    description=(\n        \'Fine-tune pretrained Convolutional Neural Networks with PyTorch\'\n    ),\n    long_description=get_long_description(),\n    long_description_content_type=\'text/markdown\',\n    author=\'Alex Parinov\',\n    author_email=\'creafz@gmail.com\',\n    url=\'https://github.com/creafz/pytorch-cnn-finetune\',\n    license=\'MIT\',\n    install_requires=requirements,\n    extras_require={\'tests\': tests_requirements},\n    python_requires=\'>=3.5\',\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Intended Audience :: Developers\',\n        \'Operating System :: OS Independent\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n    ],\n    packages=find_packages(exclude=[\'tests\', \'examples\']),\n)\n'"
cnn_finetune/__init__.py,0,"b""__version__ = '0.6.0'\n\n\nfrom cnn_finetune.contrib.torchvision import *\nfrom cnn_finetune.contrib.pretrainedmodels import *\nfrom cnn_finetune.base import make_model\n"""
cnn_finetune/base.py,1,"b'from abc import ABCMeta, abstractmethod\nfrom collections import namedtuple\nimport sys\nimport warnings\n\nimport torch\nfrom torch import nn\n\nfrom cnn_finetune.shims import no_grad_variable\nfrom cnn_finetune.utils import default, product\n\n\nModelInfo = namedtuple(\n    \'ModelInfo\',\n    [\'input_space\', \'input_size\', \'input_range\', \'mean\', \'std\']\n)\n\n# Global registry which is used to track wrappers for all model names.\nMODEL_REGISTRY = {}\n\n\nclass ModelRegistryMeta(type):\n    """"""Metaclass that registers all model names defined in model_names property\n    of a descendant class in the global MODEL_REGISTRY.\n    """"""\n\n    def __new__(mcls, name, bases, namespace, **kwargs):\n        cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n        if \'model_names\' in namespace:\n            for model_name in namespace[\'model_names\']:\n                # If model_name is already registered,\n                # override the wrapper definition and display a warning.\n                if model_name in MODEL_REGISTRY:\n                    current_class = ""<class \'{module}.{qualname}\'>"".format(\n                        module=namespace[\'__module__\'],\n                        qualname=namespace[\'__qualname__\'],\n                    )\n                    warnings.warn(\n                        ""{current_class} redefined model_name \'{model_name}\'""\n                        ""that was already registered by ""\n                        ""{previous_class}"".format(\n                            current_class=current_class,\n                            model_name=model_name,\n                            previous_class=MODEL_REGISTRY[model_name]\n                        )\n                    )\n                MODEL_REGISTRY[model_name] = cls\n        return cls\n\n\nclass ModelWrapperMeta(ABCMeta, ModelRegistryMeta):\n    """"""An intermediate class that allows usage of both\n    ABCMeta and ModelRegistryMeta simultaneously\n    """"""\n    pass\n\n\nclass ModelWrapperBase(nn.Module, metaclass=ModelWrapperMeta):\n    """"""Base class for all wrappers. To create a new wrapper you should\n    subclass it and add model names that are supported by the wrapper to\n    the model_names property. Those model names will be automatically\n    registered in the global MODEL_REGISTRY upon class initialization.\n    """"""\n\n    # If True an output of .features() call will be converted\n    # to a tensor of shape [B, C * H * W].\n    flatten_features_output = True\n\n    def __init__(self, *, model_name, num_classes, pretrained, dropout_p, pool,\n                 classifier_factory, use_original_classifier, input_size,\n                 original_model_state_dict, catch_output_size_exception):\n        super().__init__()\n\n        if num_classes < 1:\n            raise ValueError(\'num_classes should be greater or equal to 1\')\n\n        if use_original_classifier and classifier_factory:\n            raise ValueError(\n                \'You can\\\'t use classifier_factory when \'\n                \'use_original_classifier is set to True\'\n            )\n        self.check_args(\n            model_name=model_name,\n            num_classes=num_classes,\n            dropout_p=dropout_p,\n            pretrained=pretrained,\n            pool=pool,\n            classifier_fn=classifier_factory,\n            use_original_classifier=use_original_classifier,\n            input_size=input_size,\n        )\n\n        self.model_name = model_name\n        self.num_classes = num_classes\n        self.pretrained = pretrained\n        self.catch_output_size_exception = catch_output_size_exception\n\n        original_model = self.get_original_model()\n        if original_model_state_dict is not None:\n            original_model.load_state_dict(original_model_state_dict)\n\n        self._features = self.get_features(original_model)\n        self.dropout = nn.Dropout(p=dropout_p) if dropout_p else None\n        self.pool = self.get_pool() if pool is default else pool\n        self.input_size = input_size\n\n        if pretrained:\n            self.original_model_info = self.get_original_model_info(\n                original_model\n            )\n        else:\n            self.original_model_info = None\n\n        if input_size:\n            classifier_in_features = self.calculate_classifier_in_features(\n                original_model\n            )\n        else:\n            classifier_in_features = self.get_classifier_in_features(\n                original_model\n            )\n\n        if use_original_classifier:\n            classifier = self.get_original_classifier(original_model)\n        else:\n            if classifier_factory:\n                classifier = classifier_factory(\n                    classifier_in_features, num_classes,\n                )\n            else:\n                classifier = self.get_classifier(\n                    classifier_in_features, num_classes\n                )\n        self._classifier = classifier\n\n    @abstractmethod\n    def get_original_model(self):\n        # Should return a model that will be later passed to\n        # methods that will construct a model for fine-tuning.\n        pass\n\n    @abstractmethod\n    def get_features(self, original_model):\n        # Should return an instance of nn.Module that will be used as\n        # a feature extractor.\n        pass\n\n    @abstractmethod\n    def get_classifier_in_features(self, original_model):\n        # Should return a number of input features for classifier\n        # for a case when default pooling layer is being used.\n        pass\n\n    def get_original_model_info(self, original_model):\n        # Should return an instance of ModelInfo.\n        return None\n\n    def calculate_classifier_in_features(self, original_model):\n        # Runs forward pass through the feature extractor to get\n        # the number of input features for classifier.\n\n        with no_grad_variable(torch.zeros(1, 3, *self.input_size)) as input_var:\n            # Set model to the eval mode so forward pass\n            # won\'t affect BatchNorm statistics.\n            self.eval()\n            try:\n                output = self.features(input_var)\n                if self.pool is not None:\n                    output = self.pool(output)\n            except RuntimeError as e:\n                if (\n                    self.catch_output_size_exception\n                    and \'Output size is too small\' in str(e)\n                ):\n                    _, _, traceback = sys.exc_info()\n                    message = (\n                        \'Input size {input_size} is too small for this model. \'\n                        \'Try increasing the input size of images and \'\n                        \'change the value of input_size argument accordingly.\'\n                        .format(input_size=self.input_size)\n                    )\n                    raise RuntimeError(message).with_traceback(traceback)\n                else:\n                    raise e\n            self.train()\n            return product(output.size()[1:])\n\n    def check_args(self, **kwargs):\n        # Allows additional arguments checking by model wrappers.\n        pass\n\n    def get_pool(self):\n        # Returns default pooling layer for model. May return None to\n        # indicate absence of pooling layer in a model.\n        return nn.AdaptiveAvgPool2d(1)\n\n    def get_classifier(self, in_features, num_classes):\n        return nn.Linear(in_features, self.num_classes)\n\n    def get_original_classifier(self, original_model):\n        raise NotImplementedError()\n\n    def features(self, x):\n        return self._features(x)\n\n    def classifier(self, x):\n        return self._classifier(x)\n\n    def forward(self, x):\n        x = self.features(x)\n        if self.pool is not None:\n            x = self.pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        if self.flatten_features_output:\n            x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n\ndef make_model(\n    model_name,\n    num_classes,\n    pretrained=True,\n    dropout_p=None,\n    pool=default,\n    classifier_factory=None,\n    use_original_classifier=False,\n    input_size=None,\n    original_model_state_dict=None,\n    catch_output_size_exception=True,\n):\n    """"""\n    Args:\n        model_name (str): Name of the model.\n        num_classes (int): Number of classes for the classifier.\n        pretrained (bool, optional) If True uses ImageNet weights for the\n            original model.\n        dropout_p (float, optional) Dropout probability.\n        pool (nn.Module or None, optional) Custom pooling layer.\n        classifier_factory (callable, optional) Allows creating a custom\n            classifier instead of using nn.Linear. Should be a callable\n            that takes the number of input features and the number of classes\n            as arguments and returns a classifier module.\n        use_original_classifier  (bool, optional) If True uses classifier from\n            the original model.\n        input_size (tuple, optional) Input size of  images that will be\n            fed into the network. Should be a tuple containing (width, height)\n            in pixels. Required for architectures that use fully-connected\n            layers such as AlexNet or VGG.\n        original_model_state_dict (dict, optional): Dict containing\n            parameters for the original model.\n        catch_output_size_exception(boolean, optional): If True catch PyTorch\n            exceptions that contain \'Output size is too small\' message and\n            show a custom exception message about adjusting the input_size\n            value.\n    """"""\n    if model_name not in MODEL_REGISTRY:\n        raise ValueError(\n            \'model_name {model_name} not found. \'\n            \'Available model_name values: {model_names}\'.format(\n                model_name=model_name,\n                model_names=\', \'.join(MODEL_REGISTRY.keys())\n            )\n        )\n    wrapper = MODEL_REGISTRY[model_name]\n    return wrapper(\n        model_name=model_name,\n        num_classes=num_classes,\n        pretrained=pretrained,\n        dropout_p=dropout_p,\n        pool=pool,\n        classifier_factory=classifier_factory,\n        use_original_classifier=use_original_classifier,\n        input_size=input_size,\n        original_model_state_dict=original_model_state_dict,\n        catch_output_size_exception=catch_output_size_exception,\n    )\n'"
cnn_finetune/shims.py,3,"b'import torch\nfrom torch.autograd import Variable\n\n\n# Polyfill that disables gradient calculation for Variable.\n# Previous versions of PyTorch use volatile=True flag for variables\n# and the latest version uses torch.no_grad context manager.\n\ntry:\n\n    class no_grad_variable(torch.no_grad):\n\n        def __init__(self, *args, **kwargs):\n            super().__init__()\n            self.variable = Variable(*args, **kwargs)\n\n        def __enter__(self):\n            super().__enter__()\n            return self.variable\n\nexcept AttributeError:\n\n    class no_grad_variable:\n\n        def __init__(self, *args, **kwargs):\n            self.variable = Variable(*args, **kwargs, volatile=True)\n\n        def __enter__(self):\n            return self.variable\n\n        def __exit__(self, *args, **kwargs):\n            pass\n'"
cnn_finetune/utils.py,0,"b'import functools\nimport operator\n\n\ndefault = object()\n\n\ndef product(iterable):\n    return functools.reduce(operator.mul, iterable)\n'"
examples/__init__.py,0,b''
examples/cifar10.py,8,"b'""""""CIFAR10 example for cnn_finetune.\nBased on:\n- https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py\n- https://github.com/pytorch/examples/blob/master/mnist/main.py\n""""""\n\nimport argparse\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom cnn_finetune import make_model\n\nparser = argparse.ArgumentParser(description=\'cnn_finetune cifar 10 example\')\nparser.add_argument(\'--batch-size\', type=int, default=32, metavar=\'N\',\n                    help=\'input batch size for training (default: 32)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=64, metavar=\'N\',\n                    help=\'input batch size for testing (default: 64)\')\nparser.add_argument(\'--epochs\', type=int, default=100, metavar=\'N\',\n                    help=\'number of epochs to train (default: 100)\')\nparser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                    help=\'learning rate (default: 0.01)\')\nparser.add_argument(\'--momentum\', type=float, default=0.9, metavar=\'M\',\n                    help=\'SGD momentum (default: 0.9)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--log-interval\', type=int, default=100, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\nparser.add_argument(\'--model-name\', type=str, default=\'resnet50\', metavar=\'M\',\n                    help=\'model name (default: resnet50)\')\nparser.add_argument(\'--dropout-p\', type=float, default=0.2, metavar=\'D\',\n                    help=\'Dropout probability (default: 0.2)\')\n\nargs = parser.parse_args()\nuse_cuda = not args.no_cuda and torch.cuda.is_available()\ndevice = torch.device(\'cuda\' if use_cuda else \'cpu\')\n\n\ndef train(model, epoch, optimizer, train_loader, criterion=nn.CrossEntropyLoss()):\n    total_loss = 0\n    total_size = 0\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        total_loss += loss.item()\n        total_size += data.size(0)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tAverage loss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), total_loss / total_size))\n\n\ndef test(model, test_loader, criterion=nn.CrossEntropyLoss()):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += criterion(output, target).item()\n            pred = output.data.max(1, keepdim=True)[1]\n            correct += pred.eq(target.data.view_as(pred)).long().cpu().sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\ndef main():\n    \'\'\'Main function to run code in this script\'\'\'\n\n    model_name = args.model_name\n\n    if model_name == \'alexnet\':\n        raise ValueError(\'The input size of the CIFAR-10 data set (32x32) is too small for AlexNet\')\n\n    classes = (\n        \'plane\', \'car\', \'bird\', \'cat\', \'deer\',\n        \'dog\', \'frog\', \'horse\', \'ship\', \'truck\'\n    )\n\n    model = make_model(\n        model_name,\n        pretrained=True,\n        num_classes=len(classes),\n        dropout_p=args.dropout_p,\n        input_size=(32, 32) if model_name.startswith((\'vgg\', \'squeezenet\')) else None,\n    )\n    model = model.to(device)\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=model.original_model_info.mean,\n            std=model.original_model_info.std),\n    ])\n\n    train_set = torchvision.datasets.CIFAR10(\n        root=\'./data\', train=True, download=True, transform=transform\n    )\n    train_loader = torch.utils.data.DataLoader(\n        train_set, batch_size=args.batch_size, shuffle=True, num_workers=2\n    )\n    test_set = torchvision.datasets.CIFAR10(\n        root=\'./data\', train=False, download=True, transform=transform\n    )\n    test_loader = torch.utils.data.DataLoader(\n        test_set, args.test_batch_size, shuffle=False, num_workers=2\n    )\n\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n\n    # Use exponential decay for fine-tuning optimizer\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.975)\n\n    # Train\n    for epoch in range(1, args.epochs + 1):\n        # Decay Learning Rate\n        scheduler.step(epoch)\n        train(model, epoch, optimizer, train_loader)\n        test(model, test_loader)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tests/__init__.py,0,b''
tests/conftest.py,2,"b""import pytest\nimport torch\n\nfrom cnn_finetune.shims import no_grad_variable\n\n\n@pytest.fixture(scope='function', params=[(1, 3, 224, 224)])\ndef input_var(request):\n    size = request.param\n    torch.manual_seed(42)\n    with no_grad_variable(torch.rand(size)) as var:\n        yield var\n"""
tests/test_base.py,4,"b""import pytest\nimport torch\n\nfrom cnn_finetune import make_model\nfrom cnn_finetune.base import MODEL_REGISTRY\nfrom .utils import (\n    assert_iterable_length_and_type,\n    get_default_input_size_for_model,\n)\n\n\ndef test_load_state_dict():\n    torch.manual_seed(42)\n    model1 = make_model('resnet18', num_classes=10)\n    model1_state = model1.state_dict()\n\n    torch.manual_seed(84)\n    model2 = make_model('resnet18', num_classes=10)\n    model2_state = model2.state_dict()\n\n    assert not all(\n        torch.equal(weights1, weights2) for weights1, weights2\n        in zip(model1_state.values(), model2_state.values())\n    )\n\n    model2.load_state_dict(model1_state)\n    model2_state = model2.state_dict()\n\n    assert all(\n        torch.equal(weights1, weights2) for weights1, weights2 in\n        zip(model1_state.values(), model2_state.values())\n    )\n\n\ndef test_state_dict_features_and_classifier():\n    model = make_model('resnet18', num_classes=10)\n    model_state_keys = model.state_dict().keys()\n\n    assert '_classifier.weight' in model_state_keys\n    assert '_classifier.bias' in model_state_keys\n\n    features_keys = [\n        key for key in model_state_keys\n        if key.startswith('_features') and key.endswith(('weight', 'bias', 'running_mean', 'running_var'))\n    ]\n    assert len(features_keys) == 100\n\n\n@pytest.mark.parametrize('model_name', list(MODEL_REGISTRY.keys()))\ndef test_every_pretrained_model_has_model_info(model_name):\n    input_size = get_default_input_size_for_model(model_name)\n    model = make_model(\n        model_name,\n        pretrained=True,\n        num_classes=10,\n        dropout_p=0.5,\n        input_size=input_size,\n    )\n    original_model_info = model.original_model_info\n    assert original_model_info\n\n    assert_iterable_length_and_type(\n        original_model_info.input_space, 3, str\n    )\n    assert_iterable_length_and_type(\n        original_model_info.input_size, 3, int\n    )\n    assert_iterable_length_and_type(\n        original_model_info.input_range, 2, (int, float)\n    )\n    assert_iterable_length_and_type(\n        original_model_info.mean, 3, (int, float)\n    )\n    assert_iterable_length_and_type(\n        original_model_info.std, 3, (int, float)\n    )\n\n\n@pytest.mark.parametrize('model_name', list(MODEL_REGISTRY.keys()))\ndef test_models_without_pretrained_weights_dont_have_model_info(model_name):\n    input_size = get_default_input_size_for_model(model_name)\n    model = make_model(\n        model_name,\n        pretrained=False,\n        num_classes=10,\n        dropout_p=0.5,\n        input_size=input_size,\n    )\n    assert model.original_model_info is None\n\n\n@pytest.mark.parametrize('model_name', list(MODEL_REGISTRY.keys()))\n@pytest.mark.parametrize('pretrained', [True, False])\ndef test_make_model_with_specific_input_size(model_name, pretrained):\n    make_model(\n        model_name,\n        pretrained=pretrained,\n        num_classes=10,\n        dropout_p=0.5,\n        input_size=(256, 256),\n    )\n\n\ndef test_make_model_error_message_for_small_input_size():\n    expected_message_end = (\n        'Input size (8, 8) is too small for this model. Try increasing '\n        'the input size of images and change the value of input_size '\n        'argument accordingly.'\n    )\n    with pytest.raises(RuntimeError) as exc_info:\n        make_model('alexnet', pretrained=True, num_classes=10, input_size=(8, 8))\n    assert str(exc_info.value).endswith(expected_message_end)\n\n\ndef test_make_model_error_message_for_small_input_size_without_catching_exc():\n    unexpected_message_end = (\n        'Input size (8, 8) is too small for this model. Try increasing '\n        'the input size of images and change the value of input_size '\n        'argument accordingly.'\n    )\n    with pytest.raises(RuntimeError) as exc_info:\n        make_model(\n            'alexnet',\n            pretrained=True,\n            num_classes=10,\n            input_size=(8, 8),\n            catch_output_size_exception=False,\n        )\n    assert not str(exc_info.value).endswith(unexpected_message_end)\n\n\n@pytest.mark.parametrize('model_name', list(MODEL_REGISTRY.keys()))\ndef test_call_to_make_model_returns_pretrained_model_by_default(model_name):\n    input_size = get_default_input_size_for_model(model_name)\n    model = make_model(model_name, num_classes=10, input_size=input_size)\n    assert model.pretrained\n"""
tests/test_pretrained_models.py,1,"b""import types\n\nimport pytest\nimport torch.nn as nn\nimport pretrainedmodels\n\nfrom cnn_finetune import make_model\nfrom cnn_finetune.utils import default\nfrom .utils import (\n    assert_equal_model_outputs,\n    assert_almost_equal_model_outputs,\n    copy_module_weights\n)\n\n\n@pytest.mark.parametrize('model_name', ['resnext101_32x4d', 'resnext101_64x4d'])\n@pytest.mark.parametrize(['pool', 'assert_equal_outputs'], [\n    (nn.AvgPool2d((7, 7), (1, 1)), assert_equal_model_outputs),\n    (default, assert_almost_equal_model_outputs),\n])\ndef test_resnext_models(input_var, model_name, pool, assert_equal_outputs):\n    original_model = getattr(pretrainedmodels, model_name)(\n        pretrained='imagenet', num_classes=1000\n    )\n    finetune_model = make_model(\n        model_name,\n        num_classes=1000,\n        pool=pool,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.last_linear, finetune_model._classifier)\n    assert_equal_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('model_name', ['resnext101_32x4d', 'resnext101_64x4d'])\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_resnext_models_with_another_input_size(input_var, model_name):\n    model = make_model(model_name, num_classes=1000, pretrained=True)\n    model(input_var)\n\n\n@pytest.mark.parametrize(['pool', 'assert_equal_outputs'], [\n    (nn.AvgPool2d(11, stride=1, padding=0), assert_equal_model_outputs),\n    (default, assert_almost_equal_model_outputs),\n])\n@pytest.mark.parametrize('input_var', [(1, 3, 331, 331)], indirect=True)\ndef test_nasnetalarge_model(input_var, pool, assert_equal_outputs):\n    original_model = pretrainedmodels.nasnetalarge(\n        pretrained='imagenet', num_classes=1000\n    )\n    finetune_model = make_model(\n        'nasnetalarge',\n        num_classes=1000,\n        pool=pool,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.last_linear, finetune_model._classifier)\n    assert_equal_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_nasnetalarge_model_with_another_input_size(input_var):\n    model = make_model('nasnetalarge', num_classes=1000, pretrained=True)\n    model(input_var)\n\n\n@pytest.mark.parametrize(['pool', 'assert_equal_outputs'], [\n    (nn.AvgPool2d(7, stride=1, padding=0), assert_equal_model_outputs),\n    (default, assert_almost_equal_model_outputs),\n])\n@pytest.mark.parametrize('input_var', [(1, 3, 224, 224)], indirect=True)\ndef test_nasnetamobile_model(input_var, pool, assert_equal_outputs):\n    original_model = pretrainedmodels.nasnetamobile(\n        pretrained='imagenet', num_classes=1000\n    )\n    finetune_model = make_model(\n        'nasnetamobile',\n        num_classes=1000,\n        pool=pool,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.last_linear, finetune_model._classifier)\n    assert_equal_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_nasnetamobile_model_with_another_input_size(input_var):\n    model = make_model('nasnetamobile', num_classes=1000, pretrained=True)\n    model(input_var)\n\n\n@pytest.mark.parametrize(['pool', 'assert_equal_outputs'], [\n    (nn.AvgPool2d(8, count_include_pad=False), assert_equal_model_outputs),\n    (default, assert_almost_equal_model_outputs),\n])\n@pytest.mark.parametrize('input_var', [(1, 3, 299, 299)], indirect=True)\ndef test_inceptionresnetv2_model(input_var, pool, assert_equal_outputs):\n    original_model = pretrainedmodels.inceptionresnetv2(\n        pretrained='imagenet', num_classes=1000\n    )\n    finetune_model = make_model(\n        'inceptionresnetv2',\n        num_classes=1000,\n        pool=pool,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.last_linear, finetune_model._classifier)\n    assert_equal_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_inceptionresnetv2_model_with_another_input_size(input_var):\n    model = make_model('inceptionresnetv2', num_classes=1000, pretrained=True)\n    model(input_var)\n\n\n@pytest.mark.parametrize(\n    'model_name',\n    ['dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn131', 'dpn107']\n)\n@pytest.mark.parametrize(['pool', 'assert_equal_outputs'], [\n    (nn.AvgPool2d(kernel_size=7, stride=1), assert_equal_model_outputs),\n    (default, assert_almost_equal_model_outputs),\n])\ndef test_dpn_models(input_var, model_name, pool, assert_equal_outputs):\n    if model_name in {'dpn68b', 'dpn92', 'dpn107'}:\n        pretrained = 'imagenet+5k'\n    else:\n        pretrained = 'imagenet'\n    original_model = getattr(pretrainedmodels, model_name)(\n        pretrained=pretrained, num_classes=1000\n    )\n    finetune_model = make_model(\n        model_name,\n        num_classes=1000,\n        pool=pool,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.last_linear, finetune_model._classifier)\n    assert_equal_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize(\n    'model_name',\n    ['dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn131', 'dpn107']\n)\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_dpn_models_with_another_input_size(model_name, input_var):\n    model = make_model(model_name, num_classes=1000, pretrained=True)\n    model(input_var)\n\n\n@pytest.mark.parametrize(['pool', 'assert_equal_outputs'], [\n    (nn.AvgPool2d(8, count_include_pad=False), assert_equal_model_outputs),\n    (default, assert_almost_equal_model_outputs),\n])\n@pytest.mark.parametrize('input_var', [(1, 3, 299, 299)], indirect=True)\ndef test_inceptionv4_model(input_var, pool, assert_equal_outputs):\n    original_model = pretrainedmodels.inceptionv4(\n        pretrained='imagenet', num_classes=1000\n    )\n    finetune_model = make_model(\n        'inception_v4',\n        num_classes=1000,\n        pool=pool,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.last_linear, finetune_model._classifier)\n    assert_equal_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_inceptionv4_model_with_another_input_size(input_var):\n    model = make_model('inception_v4', num_classes=1000, pretrained=True)\n    model(input_var)\n\n\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_xception_model_with_another_input_size(input_var):\n    model = make_model('xception', num_classes=1000, pretrained=True)\n    model(input_var)\n\n\n@pytest.mark.parametrize(\n    'model_name',\n    [\n        'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n        'se_resnext50_32x4d', 'se_resnext101_32x4d',\n    ]\n)\n@pytest.mark.parametrize(['pool', 'assert_equal_outputs'], [\n    (nn.AvgPool2d((7, 7), (1, 1)), assert_equal_model_outputs),\n    (default, assert_almost_equal_model_outputs),\n])\ndef test_senet_models(input_var, model_name, pool, assert_equal_outputs):\n    original_model = getattr(pretrainedmodels, model_name)(\n        pretrained='imagenet', num_classes=1000\n    )\n    finetune_model = make_model(\n        model_name,\n        num_classes=1000,\n        pool=pool,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.last_linear, finetune_model._classifier)\n    assert_equal_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize(\n    'model_name',\n    [\n        'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n        'se_resnext50_32x4d', 'se_resnext101_32x4d',\n    ]\n)\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_senet_models_with_another_input_size(input_var, model_name):\n    model = make_model(model_name, num_classes=1000, pretrained=True)\n    model(input_var)\n\n\n@pytest.mark.parametrize(['pool', 'assert_equal_outputs'], [\n    (nn.AvgPool2d(11, stride=1, padding=0), assert_equal_model_outputs),\n    (default, assert_almost_equal_model_outputs),\n])\n@pytest.mark.parametrize('input_var', [(1, 3, 331, 331)], indirect=True)\ndef test_pnasnet5large_model(input_var, pool, assert_equal_outputs):\n    original_model = pretrainedmodels.pnasnet5large(\n        pretrained='imagenet', num_classes=1000\n    )\n    finetune_model = make_model(\n        'pnasnet5large',\n        num_classes=1000,\n        pool=pool,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.last_linear, finetune_model._classifier)\n    assert_equal_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_pnasnet5large_model_with_another_input_size(input_var):\n    model = make_model('pnasnet5large', num_classes=1000, pretrained=True)\n    model(input_var)\n\n\n@pytest.mark.parametrize(['pool', 'assert_equal_outputs'], [\n    (nn.AvgPool2d(9, stride=1), assert_equal_model_outputs),\n    (default, assert_almost_equal_model_outputs),\n])\n@pytest.mark.parametrize('input_var', [(1, 3, 331, 331)], indirect=True)\ndef test_polynet_model(input_var, pool, assert_equal_outputs):\n    original_model = pretrainedmodels.polynet(\n        pretrained='imagenet', num_classes=1000\n    )\n    finetune_model = make_model(\n        'polynet',\n        num_classes=1000,\n        pool=pool,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.last_linear, finetune_model._classifier)\n    assert_equal_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_polynet_model_with_another_input_size(input_var):\n    model = make_model('polynet', num_classes=1000, pretrained=True)\n    model(input_var)\n"""
tests/test_torchvision_models.py,1,"b""import pytest\nimport torch.nn as nn\nfrom torchvision import models as torchvision_models\n\nfrom cnn_finetune import make_model\nfrom cnn_finetune.utils import default\nfrom .utils import (\n    assert_equal_model_outputs,\n    assert_almost_equal_model_outputs,\n    copy_module_weights\n)\n\n\n@pytest.mark.parametrize(\n    'model_name',\n    ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d']\n)\ndef test_resnet_models(input_var, model_name):\n    original_model = getattr(torchvision_models, model_name)(pretrained=True)\n    finetune_model = make_model(\n        model_name,\n        num_classes=1000,\n        pool=default,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.fc, finetune_model._classifier)\n    assert_equal_model_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize(\n    'model_name',\n    ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d']\n)\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_resnet_models_with_another_input_size(input_var, model_name):\n    model = make_model(model_name, num_classes=1000, pretrained=True)\n    model(input_var)\n\n\n@pytest.mark.parametrize('model_name', ['densenet121', 'densenet169', 'densenet201',  'densenet161'])\ndef test_densenet_models(input_var, model_name):\n    original_model = getattr(torchvision_models, model_name)(pretrained=True)\n    finetune_model = make_model(\n        model_name,\n        num_classes=1000,\n        pool=default,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.classifier, finetune_model._classifier)\n    assert_equal_model_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('model_name', ['densenet121', 'densenet169', 'densenet201',  'densenet161'])\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_densenet_models_with_another_input_size(input_var, model_name):\n    model = make_model(model_name, num_classes=1000, pretrained=True)\n    model(input_var)\n\n\ndef test_alexnet_model_with_default_classifier(input_var):\n    original_model = torchvision_models.alexnet(pretrained=True)\n    original_model(input_var)\n    finetune_model = make_model(\n        'alexnet',\n        num_classes=1000,\n        use_original_classifier=True,\n        input_size=(224, 224),\n        pretrained=True,\n    )\n    assert_equal_model_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('pool', [default, nn.AdaptiveAvgPool2d(1)])\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_alexnet_model_with_another_input_size(input_var, pool):\n    model = make_model(\n        'alexnet',\n        num_classes=1000,\n        input_size=(256, 256),\n        pool=pool,\n        pretrained=True,\n    )\n    model(input_var)\n\n\n@pytest.mark.parametrize(\n    'model_name',\n    [\n        'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n        'vgg19_bn', 'vgg19'\n    ]\n)\ndef test_vgg_models_with_default_classifier(model_name, input_var):\n    original_model = getattr(torchvision_models, model_name)(pretrained=True)\n    finetune_model = make_model(\n        model_name,\n        num_classes=1000,\n        use_original_classifier=True,\n        input_size=(224, 224),\n        pretrained=True,\n    )\n    assert_equal_model_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize(\n    'model_name',\n    [\n        'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n        'vgg19_bn', 'vgg19'\n    ]\n)\n@pytest.mark.parametrize('pool', [default, nn.AdaptiveAvgPool2d(1)])\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_vgg_models_with_another_input_size(model_name, input_var, pool):\n    model = make_model(\n        model_name,\n        num_classes=1000,\n        input_size=(256, 256),\n        pool=pool,\n        pretrained=True,\n    )\n    model(input_var)\n\n\n@pytest.mark.parametrize('model_name', ['squeezenet1_0', 'squeezenet1_1'])\ndef test_squeezenet_models_with_original_classifier(model_name, input_var):\n    original_model = getattr(torchvision_models, model_name)(pretrained=True)\n    finetune_model = make_model(\n        model_name,\n        num_classes=1000,\n        use_original_classifier=True,\n        input_size=(224, 224),\n        pretrained=True,\n    )\n    assert_equal_model_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('model_name', ['squeezenet1_0', 'squeezenet1_1'])\n@pytest.mark.parametrize('pool', [default, nn.AdaptiveAvgPool2d(1)])\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_squeezenet_models_with_another_input_size(model_name, input_var, pool):\n    model = make_model(\n        model_name,\n        num_classes=1000,\n        input_size=(256, 256),\n        pool=pool,\n        pretrained=True,\n    )\n    model(input_var)\n\n\n@pytest.mark.parametrize('input_var', [(1, 3, 299, 299)], indirect=True)\ndef test_inception_v3_model(input_var):\n    original_model = torchvision_models.inception_v3(\n        pretrained=True,\n        transform_input=False,\n    )\n    finetune_model = make_model(\n        'inception_v3', num_classes=1000, pool=default, pretrained=True\n    )\n    copy_module_weights(original_model.fc, finetune_model._classifier)\n    assert_equal_model_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('input_var', [(1, 3, 350, 350)], indirect=True)\ndef test_inception_v3_model_with_another_input_size(input_var):\n    model = make_model('inception_v3', num_classes=1000, pretrained=True)\n    model(input_var)\n\n\n@pytest.mark.parametrize('input_var', [(1, 3, 299, 299)], indirect=True)\ndef test_googlenet_model(input_var):\n    original_model = torchvision_models.googlenet(\n        pretrained=True,\n        transform_input=False,\n    )\n    finetune_model = make_model(\n        'googlenet', num_classes=1000, pool=default, pretrained=True\n    )\n    copy_module_weights(original_model.fc, finetune_model._classifier)\n    assert_equal_model_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('input_var', [(1, 3, 350, 350)], indirect=True)\ndef test_googlenet_model_with_another_input_size(input_var):\n    model = make_model('googlenet', num_classes=1000, pretrained=True)\n    model(input_var)\n\n\ndef test_mobilenet_v2_model(input_var):\n    original_model = torchvision_models.mobilenet_v2(pretrained=True)\n    finetune_model = make_model(\n        'mobilenet_v2',\n        num_classes=1000,\n        pool=default,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.classifier[-1], finetune_model._classifier)\n    assert_equal_model_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_mobilenet_v2_model_with_another_input_size(input_var):\n    model = make_model('mobilenet_v2', num_classes=1000, pretrained=True)\n    model(input_var)\n\n\n@pytest.mark.parametrize('model_name', ['shufflenet_v2_x0_5', 'shufflenet_v2_x1_0'])\ndef test_shufflenet_v2_models_with_pretrained_weights(input_var, model_name):\n    original_model = getattr(torchvision_models, model_name)(pretrained=True)\n    finetune_model = make_model(\n        model_name,\n        num_classes=1000,\n        pool=default,\n        pretrained=True,\n    )\n    copy_module_weights(original_model.fc, finetune_model._classifier)\n    assert_equal_model_outputs(input_var, original_model, finetune_model)\n\n\n@pytest.mark.parametrize(\n    'model_name',\n    ['shufflenet_v2_x0_5', 'shufflenet_v2_x1_0']\n)\n@pytest.mark.parametrize('input_var', [(1, 3, 256, 256)], indirect=True)\ndef test_shufflenet_v2_models_with_another_input_size(input_var, model_name):\n    model = make_model(model_name, num_classes=1000, pretrained=True)\n    model(input_var)\n"""
tests/utils.py,1,"b""import numpy as np\nimport torch\n\n\ndef assert_equal_model_outputs(input_var, model1, model2):\n    model1.eval()\n    model2.eval()\n    model1_output = model1(input_var)\n    model2_output = model2(input_var)\n    assert torch.equal(model1_output, model2_output)\n\n\ndef assert_almost_equal_model_outputs(input_var, model1, model2):\n    model1.eval()\n    model2.eval()\n    model1_output = model1(input_var)\n    model2_output = model2(input_var)\n    assert np.all(\n        np.isclose(\n            model1_output.data.numpy(),\n            model2_output.data.numpy(),\n            rtol=1e-04,\n            atol=1e-06,\n        )\n    )\n\n\ndef copy_module_weights(from_module, to_module):\n    to_module.weight.data.copy_(from_module.weight.data)\n    to_module.bias.data.copy_(from_module.bias.data)\n\n\ndef assert_iterable_length_and_type(iterable, length, element_type):\n    assert len(iterable) == length\n    for element in iterable:\n        assert isinstance(element, element_type)\n\n\ndef get_default_input_size_for_model(model_name):\n    if (\n        model_name == 'alexnet'\n        or model_name.startswith('vgg')\n        or model_name.startswith('squeezenet')\n    ):\n        return 224, 224\n    return None\n"""
cnn_finetune/contrib/__init__.py,0,b''
cnn_finetune/contrib/pretrainedmodels.py,1,"b""import torch\nfrom torch import nn\nfrom torch.utils import model_zoo\nimport pretrainedmodels\nfrom pretrainedmodels.models.dpn import adaptive_avgmax_pool2d\nfrom pretrainedmodels.models.xception import Xception, pretrained_settings as xception_settings\n\nfrom cnn_finetune.base import ModelWrapperBase, ModelInfo\n\n\n__all__ = [\n    'ResNeXtWrapper', 'NasNetWrapper', 'InceptionResNetV2Wrapper',\n    'DPNWrapper', 'InceptionV4Wrapper', 'XceptionWrapper',\n    'NasNetMobileWrapper', 'SenetWrapper', 'PNasNetWrapper', 'PolyNetWrapper'\n]\n\n\nclass PretrainedModelsWrapper(ModelWrapperBase):\n\n    def get_original_model_info(self, original_model):\n        return ModelInfo(\n            input_space=original_model.input_space,\n            input_size=original_model.input_size,\n            input_range=original_model.input_range,\n            mean=original_model.mean,\n            std=original_model.std,\n        )\n\n    def get_original_model(self):\n        model = getattr(pretrainedmodels, self.model_name)\n        if self.pretrained:\n            model_kwargs = {'pretrained': 'imagenet', 'num_classes': 1000}\n        else:\n            model_kwargs = {'pretrained': None}\n        return model(**model_kwargs)\n\n    def get_features(self, original_model):\n        return original_model.features\n\n    def get_original_classifier(self, original_model):\n        return original_model.last_linear\n\n    def get_classifier_in_features(self, original_model):\n        return original_model.last_linear.in_features\n\n\nclass ResNeXtWrapper(PretrainedModelsWrapper):\n\n    model_names = ['resnext101_32x4d', 'resnext101_64x4d']\n\n\nclass NasNetWrapper(PretrainedModelsWrapper):\n\n    model_names = ['nasnetalarge']\n\n    def get_features(self, original_model):\n        features = nn.Module()\n        for name, module in list(original_model.named_children())[:-3]:\n            features.add_module(name, module)\n        return features\n\n    def features(self, x):\n        x_conv0 = self._features.conv0(x)\n        x_stem_0 = self._features.cell_stem_0(x_conv0)\n        x_stem_1 = self._features.cell_stem_1(x_conv0, x_stem_0)\n\n        x_cell_0 = self._features.cell_0(x_stem_1, x_stem_0)\n        x_cell_1 = self._features.cell_1(x_cell_0, x_stem_1)\n        x_cell_2 = self._features.cell_2(x_cell_1, x_cell_0)\n        x_cell_3 = self._features.cell_3(x_cell_2, x_cell_1)\n        x_cell_4 = self._features.cell_4(x_cell_3, x_cell_2)\n        x_cell_5 = self._features.cell_5(x_cell_4, x_cell_3)\n\n        x_reduction_cell_0 = self._features.reduction_cell_0(\n            x_cell_5, x_cell_4\n        )\n\n        x_cell_6 = self._features.cell_6(x_reduction_cell_0, x_cell_4)\n        x_cell_7 = self._features.cell_7(x_cell_6, x_reduction_cell_0)\n        x_cell_8 = self._features.cell_8(x_cell_7, x_cell_6)\n        x_cell_9 = self._features.cell_9(x_cell_8, x_cell_7)\n        x_cell_10 = self._features.cell_10(x_cell_9, x_cell_8)\n        x_cell_11 = self._features.cell_11(x_cell_10, x_cell_9)\n\n        x_reduction_cell_1 = self._features.reduction_cell_1(\n            x_cell_11, x_cell_10\n        )\n\n        x_cell_12 = self._features.cell_12(x_reduction_cell_1, x_cell_10)\n        x_cell_13 = self._features.cell_13(x_cell_12, x_reduction_cell_1)\n        x_cell_14 = self._features.cell_14(x_cell_13, x_cell_12)\n        x_cell_15 = self._features.cell_15(x_cell_14, x_cell_13)\n        x_cell_16 = self._features.cell_16(x_cell_15, x_cell_14)\n        x_cell_17 = self._features.cell_17(x_cell_16, x_cell_15)\n        x = self._features.relu(x_cell_17)\n        return x\n\n\nclass NasNetMobileWrapper(PretrainedModelsWrapper):\n\n    model_names = ['nasnetamobile']\n\n    def get_features(self, original_model):\n        features = nn.Module()\n        for name, module in list(original_model.named_children())[:-3]:\n            features.add_module(name, module)\n        return features\n\n    def features(self, input):\n        x_conv0 = self._features.conv0(input)\n        x_stem_0 = self._features.cell_stem_0(x_conv0)\n        x_stem_1 = self._features.cell_stem_1(x_conv0, x_stem_0)\n\n        x_cell_0 = self._features.cell_0(x_stem_1, x_stem_0)\n        x_cell_1 = self._features.cell_1(x_cell_0, x_stem_1)\n        x_cell_2 = self._features.cell_2(x_cell_1, x_cell_0)\n        x_cell_3 = self._features.cell_3(x_cell_2, x_cell_1)\n\n        x_reduction_cell_0 = self._features.reduction_cell_0(x_cell_3, x_cell_2)\n\n        x_cell_6 = self._features.cell_6(x_reduction_cell_0, x_cell_3)\n        x_cell_7 = self._features.cell_7(x_cell_6, x_reduction_cell_0)\n        x_cell_8 = self._features.cell_8(x_cell_7, x_cell_6)\n        x_cell_9 = self._features.cell_9(x_cell_8, x_cell_7)\n\n        x_reduction_cell_1 = self._features.reduction_cell_1(x_cell_9, x_cell_8)\n\n        x_cell_12 = self._features.cell_12(x_reduction_cell_1, x_cell_9)\n        x_cell_13 = self._features.cell_13(x_cell_12, x_reduction_cell_1)\n        x_cell_14 = self._features.cell_14(x_cell_13, x_cell_12)\n        x_cell_15 = self._features.cell_15(x_cell_14, x_cell_13)\n        x = self._features.relu(x_cell_15)\n        return x\n\n\nclass InceptionResNetV2Wrapper(PretrainedModelsWrapper):\n\n    model_names = ['inceptionresnetv2']\n\n    def get_features(self, original_model):\n        return nn.Sequential(*list(original_model.children())[:-2])\n\n\nclass DPNWrapper(PretrainedModelsWrapper):\n\n    model_names = ['dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn131', 'dpn107']\n\n    flatten_features_output = False\n\n    def get_original_model(self):\n        # The original model is always constructed with test_time_pool=True\n        model = getattr(pretrainedmodels, self.model_name)\n        if self.pretrained:\n            if self.model_name in {'dpn68b', 'dpn92', 'dpn107'}:\n                pretrained = 'imagenet+5k'\n            else:\n                pretrained = 'imagenet'\n            model_kwargs = {'pretrained': pretrained, 'num_classes': 1000}\n        else:\n            model_kwargs = {'pretrained': None}\n        return model(**model_kwargs)\n\n    def get_classifier_in_features(self, original_model):\n        return original_model.last_linear.in_channels\n\n    def get_classifier(self, in_features, num_classes):\n        return nn.Conv2d(in_features, num_classes, kernel_size=1, bias=True)\n\n    def classifier(self, x):\n        x = self._classifier(x)\n        if not self.training:\n            x = adaptive_avgmax_pool2d(x, pool_type='avgmax')\n        return x.view(x.size(0), -1)\n\n\nclass InceptionV4Wrapper(PretrainedModelsWrapper):\n\n    model_names = ['inception_v4']\n\n    def get_original_model(self):\n        if self.pretrained:\n            model_kwargs = {'pretrained': 'imagenet', 'num_classes': 1000}\n        else:\n            model_kwargs = {'pretrained': None}\n        return pretrainedmodels.inceptionv4(**model_kwargs)\n\n\nclass XceptionWrapper(PretrainedModelsWrapper):\n\n    model_names = ['xception']\n\n    def get_features(self, original_model):\n        return nn.Sequential(*list(original_model.children())[:-1])\n\n\nclass SenetWrapper(PretrainedModelsWrapper):\n\n    model_names = [\n        'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n        'se_resnext50_32x4d', 'se_resnext101_32x4d',\n    ]\n\n    def get_features(self, original_model):\n        return nn.Sequential(\n            original_model.layer0,\n            original_model.layer1,\n            original_model.layer2,\n            original_model.layer3,\n            original_model.layer4,\n        )\n\n\nclass PNasNetWrapper(PretrainedModelsWrapper):\n\n    model_names = ['pnasnet5large']\n\n    def get_features(self, original_model):\n        features = nn.Module()\n        for name, module in list(original_model.named_children())[:-3]:\n            features.add_module(name, module)\n        return features\n\n    def features(self, x):\n        x_conv_0 = self._features.conv_0(x)\n        x_stem_0 = self._features.cell_stem_0(x_conv_0)\n        x_stem_1 = self._features.cell_stem_1(x_conv_0, x_stem_0)\n        x_cell_0 = self._features.cell_0(x_stem_0, x_stem_1)\n        x_cell_1 = self._features.cell_1(x_stem_1, x_cell_0)\n        x_cell_2 = self._features.cell_2(x_cell_0, x_cell_1)\n        x_cell_3 = self._features.cell_3(x_cell_1, x_cell_2)\n        x_cell_4 = self._features.cell_4(x_cell_2, x_cell_3)\n        x_cell_5 = self._features.cell_5(x_cell_3, x_cell_4)\n        x_cell_6 = self._features.cell_6(x_cell_4, x_cell_5)\n        x_cell_7 = self._features.cell_7(x_cell_5, x_cell_6)\n        x_cell_8 = self._features.cell_8(x_cell_6, x_cell_7)\n        x_cell_9 = self._features.cell_9(x_cell_7, x_cell_8)\n        x_cell_10 = self._features.cell_10(x_cell_8, x_cell_9)\n        x_cell_11 = self._features.cell_11(x_cell_9, x_cell_10)\n        x = self._features.relu(x_cell_11)\n        return x\n\n\nclass PolyNetWrapper(PretrainedModelsWrapper):\n\n    model_names = ['polynet']\n\n    def get_features(self, original_model):\n        return nn.Sequential(*list(original_model.children())[:-3])\n"""
cnn_finetune/contrib/torchvision.py,0,"b""from torch import nn\nfrom torchvision import models as torchvision_models\n\nfrom cnn_finetune.base import ModelWrapperBase, ModelInfo\nfrom cnn_finetune.utils import default\n\n\n__all__ = [\n    'ResNetWrapper', 'DenseNetWrapper', 'AlexNetWrapper', 'VGGWrapper',\n    'SqueezeNetWrapper', 'InceptionV3Wrapper'\n]\n\n\nclass TorchvisionWrapper(ModelWrapperBase):\n\n    def get_original_model_info(self, original_model):\n        return ModelInfo(\n            input_space='RGB',\n            input_size=[3, 224, 224],\n            input_range=[0, 1],\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n        )\n\n    def get_original_model(self):\n        model = getattr(torchvision_models, self.model_name)\n        return model(pretrained=self.pretrained)\n\n    def get_original_classifier(self, original_model):\n        return original_model.classifier\n\n\nclass ResNetWrapper(TorchvisionWrapper):\n\n    model_names = ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d']\n\n    def get_features(self, original_model):\n        return nn.Sequential(*list(original_model.children())[:-2])\n\n    def get_classifier_in_features(self, original_model):\n        return original_model.fc.in_features\n\n    def get_original_classifier(self, original_model):\n        return original_model.fc\n\n\nclass DenseNetWrapper(TorchvisionWrapper):\n\n    model_names = [\n        'densenet121', 'densenet169', 'densenet201', 'densenet161'\n    ]\n\n    def get_features(self, original_model):\n        return nn.Sequential(*original_model.features, nn.ReLU(inplace=True))\n\n    def get_classifier_in_features(self, original_model):\n        return original_model.classifier.in_features\n\n\nclass NetWithFcClassifierWrapper(TorchvisionWrapper):\n\n    def check_args(\n        self,\n        model_name,\n        pool,\n        use_original_classifier,\n        input_size,\n        num_classes,\n        pretrained,\n        **kwargs\n    ):\n        super().check_args()\n        if input_size is None:\n            raise Exception(\n                'You must provide input_size, e.g. '\n                'make_model({model_name}, num_classes={num_classes}, '\n                'pretrained={pretrained}, input_size=(224, 224)'.format(\n                    model_name=model_name,\n                    num_classes=num_classes,\n                    pretrained=pretrained,\n                )\n            )\n\n        if use_original_classifier:\n            if pool is not None and pool is not default:\n                raise Exception(\n                    'You can\\'t use pool layer with the original classifier'\n                )\n            if input_size != (224, 224):\n                raise Exception(\n                    'For the original classifier '\n                    'input_size value must be (224, 224)'\n                )\n\n    def get_classifier_in_features(self, original_model):\n        return self.calculate_classifier_in_features(original_model)\n\n    def get_features(self, original_model):\n        return original_model.features\n\n    def get_pool(self):\n        return None\n\n\nclass AlexNetWrapper(NetWithFcClassifierWrapper):\n\n    model_names = ['alexnet']\n\n    def get_classifier(self, in_features, num_classes):\n        return nn.Sequential(\n            nn.Linear(in_features, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n\nclass VGGWrapper(NetWithFcClassifierWrapper):\n\n    model_names = [\n        'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn',\n        'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn'\n    ]\n\n    def get_classifier(self, in_features, num_classes):\n        return nn.Sequential(\n            nn.Linear(in_features, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, num_classes),\n        )\n\n\nclass SqueezeNetWrapper(TorchvisionWrapper):\n\n    model_names = ['squeezenet1_0', 'squeezenet1_1']\n\n    def get_features(self, original_model):\n        return original_model.features\n\n    def get_pool(self):\n        return None\n\n    def get_classifier_in_features(self, original_model):\n        return self.calculate_classifier_in_features(original_model)\n\n    def get_classifier(self, in_features, num_classes):\n        classifier = nn.Sequential(\n            nn.Conv2d(512, num_classes, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        return classifier\n\n    def forward(self, x):\n        x = self.features(x)\n        if self.pool is not None:\n            x = self.pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = self.classifier(x)\n        return x.view(x.size(0), self.num_classes)\n\n\nclass InceptionWrapper(ModelWrapperBase):\n    # aux_logits and transform_input parameters from the torchvision\n    # implementation are not supported\n\n    def get_original_model_info(self, original_model):\n        return ModelInfo(\n            input_space='RGB',\n            input_size=[3, 299, 299],\n            input_range=[0, 1],\n            mean=[0.5, 0.5, 0.5],\n            std=[0.5, 0.5, 0.5],\n        )\n\n    def get_original_model(self):\n        model = getattr(torchvision_models, self.model_name)\n        return model(pretrained=self.pretrained)\n\n    def get_original_classifier(self, original_model):\n        return original_model.fc\n\n    def get_classifier_in_features(self, original_model):\n        return original_model.fc.in_features\n\n\nclass InceptionV3Wrapper(InceptionWrapper):\n\n    model_names = ['inception_v3']\n\n    def get_features(self, original_model):\n        features = nn.Sequential(\n            original_model.Conv2d_1a_3x3,\n            original_model.Conv2d_2a_3x3,\n            original_model.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            original_model.Conv2d_3b_1x1,\n            original_model.Conv2d_4a_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            original_model.Mixed_5b,\n            original_model.Mixed_5c,\n            original_model.Mixed_5d,\n            original_model.Mixed_6a,\n            original_model.Mixed_6b,\n            original_model.Mixed_6c,\n            original_model.Mixed_6d,\n            original_model.Mixed_6e,\n            original_model.Mixed_7a,\n            original_model.Mixed_7b,\n            original_model.Mixed_7c,\n        )\n        return features\n\n\nclass GoogLeNetWrapper(InceptionWrapper):\n\n    model_names = ['googlenet']\n\n    def get_features(self, original_model):\n        features = nn.Sequential(\n            original_model.conv1,\n            original_model.maxpool1,\n            original_model.conv2,\n            original_model.conv3,\n            original_model.maxpool2,\n            original_model.inception3a,\n            original_model.inception3b,\n            original_model.maxpool3,\n            original_model.inception4a,\n            original_model.inception4b,\n            original_model.inception4c,\n            original_model.inception4d,\n            original_model.inception4e,\n            original_model.maxpool4,\n            original_model.inception5a,\n            original_model.inception5b,\n        )\n        return features\n\n\nclass MobileNetV2Wrapper(TorchvisionWrapper):\n\n    model_names = ['mobilenet_v2']\n\n    def get_features(self, original_model):\n        return original_model.features\n\n    def get_original_classifier(self, original_model):\n        return original_model.classifier[-1]\n\n    def get_classifier_in_features(self, original_model):\n        return original_model.classifier[-1].in_features\n\n\nclass ShuffleNetV2Wrapper(TorchvisionWrapper):\n\n    model_names = ['shufflenet_v2_x0_5', 'shufflenet_v2_x1_0']\n\n    def get_features(self, original_model):\n        features = nn.Sequential(\n            original_model.conv1,\n            original_model.maxpool,\n            original_model.stage2,\n            original_model.stage3,\n            original_model.stage4,\n            original_model.conv5,\n        )\n        return features\n\n    def get_original_classifier(self, original_model):\n        return original_model.fc\n\n    def get_classifier_in_features(self, original_model):\n        return original_model.fc.in_features\n"""
