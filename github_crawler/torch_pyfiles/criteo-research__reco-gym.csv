file_path,api_count,code
deterministic_test.py,0,"b'from copy import deepcopy\n\nimport gym\nimport numpy as np\n\nfrom recogym import Configuration\nfrom recogym import env_0_args, env_1_args, test_agent\nfrom recogym.agents import BanditCount, bandit_count_args\nfrom recogym.agents import BanditMFSquare, bandit_mf_square_args\nfrom recogym.agents import LogregMulticlassIpsAgent, logreg_multiclass_ips_args\nfrom recogym.agents import LogregPolyAgent, logreg_poly_args\nfrom recogym.agents import NnIpsAgent, nn_ips_args\nfrom recogym.agents import OrganicCount, organic_count_args\nfrom recogym.agents import OrganicUserEventCounterAgent, organic_user_count_args\nfrom recogym.agents import RandomAgent, random_args\n\n# Add a new environment here.\nenv_test = {\n    ""reco-gym-v1"": env_1_args,\n    ""reco-gym-v0"": env_0_args,\n}\n\nRandomSeed = 42\n\n# Add a new agent here.\nagent_test = {\n    \'prod2vec\': BanditMFSquare(Configuration(bandit_mf_square_args)),\n    \'logistic\': BanditCount(Configuration(bandit_count_args)),\n    \'randomagent\': RandomAgent(Configuration({\n        **random_args,\n        \'random_seed\': RandomSeed,\n    })),\n    \'logreg_multiclass_ips\': LogregMulticlassIpsAgent(Configuration({\n        **logreg_multiclass_ips_args,\n        \'select_randomly\': False,\n    })),\n    \'logreg_multiclass_ips R\': LogregMulticlassIpsAgent(Configuration({\n        **logreg_multiclass_ips_args,\n        \'select_randomly\': True,\n        \'random_seed\': RandomSeed,\n    })),\n    \'organic_counter\': OrganicCount(Configuration(organic_count_args)),\n    \'organic_user_counter\': OrganicUserEventCounterAgent(Configuration({\n        **organic_user_count_args,\n        \'select_randomly\': False,\n    })),\n    \'organic_user_counter R\': OrganicUserEventCounterAgent(Configuration({\n        **organic_user_count_args,\n        \'select_randomly\': True,\n        \'random_seed\': RandomSeed,\n    })),\n    \'logreg_poly\': LogregPolyAgent(Configuration({\n        **logreg_poly_args,\n        \'with_ips\': False,\n    })),\n    \'logreg_poly_ips\': LogregPolyAgent(Configuration({\n        **logreg_poly_args,\n        \'with_ips\': True,\n    })),\n}\neval_size = 5\norganic_size = 5\nsamples = 200  # Set a big value to train model based on classifications.\n\n\ndef is_env_deterministic(env, users=5):\n    c_env = deepcopy(env)\n    logs_a = c_env.generate_logs(num_offline_users=users, num_organic_offline_users=users)\n    c_env = deepcopy(env)\n    logs_b = c_env.generate_logs(num_offline_users=users, num_organic_offline_users=users)\n    return np.mean(logs_a[~ np.isnan(logs_b.v)].v == logs_b[~ np.isnan(logs_b.v)].v) == 1.\n    # return logs_a.equals(logs_b) # this can return false.. it isn\'t clear why atm ... most likely types differ..\n\n\nif __name__ == ""__main__"":\n\n    for env_name in env_test.keys():\n        env = gym.make(env_name)\n        env.init_gym(env_test[env_name])\n\n        if is_env_deterministic(env):\n            print(f""{env_name} appears deterministic"")\n        else:\n            print(f""{env_name} is NOT deterministic"")\n\n        for agent_name in agent_test.keys():\n            agent = agent_test[agent_name]\n            a = test_agent(deepcopy(env), deepcopy(agent), samples, eval_size, organic_size)\n            print(f""{agent_name} runs"")\n            b = test_agent(deepcopy(env), deepcopy(agent), samples, eval_size, organic_size)\n            if a == b:\n                print(f""{agent_name} appears deterministic"")\n            else:\n                print(f""{agent_name} is NOT deterministic"")\n'"
quality_test.py,0,"b'import subprocess\nimport tempfile\nimport nbformat\nimport unittest\n\n\nclass TestNotebookConsistency(unittest.TestCase):\n    @staticmethod\n    def _execute_notebook(path):\n        """"""\n        Execute a Jupyter Notebook from scratch and convert it into another Jupyter Notebook.\n           :returns a converted Jupyter Notebook\n        """"""\n        with tempfile.NamedTemporaryFile(suffix = "".ipynb"") as tmp_notebook:\n            args = [\n                ""jupyter"", ""nbconvert"",\n                ""--to"", ""notebook"",\n                ""--execute"",\n                ""--ExecutePreprocessor.timeout=3600"",\n                ""--output"", tmp_notebook.name,\n                path\n            ]\n            subprocess.check_call(args)\n\n            tmp_notebook.seek(0)\n            return nbformat.read(tmp_notebook, nbformat.current_nbformat)\n\n    @staticmethod\n    def _analise_notebook(notebook):\n        """"""\n        Analise notebook cell outputs.\n\n        The function goes through all cell outputs and finds either error or warning.\n\n        :returns a tuple of errors (0th) and warnings (1st)\n        """"""\n        errors = []\n        warnings = []\n        for cell in notebook.cells:\n            if \'outputs\' in cell:\n                for output in cell[\'outputs\']:\n                    if output.output_type == ""error"":\n                        errors.append(output)\n                    if output.output_type == ""warning"":\n                        warnings.append(output)\n        return errors, warnings\n\n    def test_notebooks(self, with_replacement = False):\n        """"""\n        Launch Jupyter Notebooks Tests\n\n        This function automates the process of validating of Jupyter Notebooks\n        via launching them from scratch and checking that throughout the launching session\n        no error/warning occurs.\n\n        Note #1: it is assumed that the current directory\n            is the same where a test file is located.\n        Note #2: the name of the Notebook should be defined without the extension `*.ipynb\'.\n\n            with_replacement: when the flag is set `True\' and a Jupyter Notebook\n                has successfully passed tests, the Notebook will be replaced\n                with a newly generated Notebook with all rendered data, graphs, etc..\n        """"""\n        for case in {\n            \'Getting Started\',\n            \'Compare Agents\',\n            \'Likelihood Agents\',\n            \'Inverse Propensity Score\',\n            \'Explore Exploit Evolution\',\n            \'Complex Time Behaviour\',\n            \'Pure Organic vs Bandit - Number of Online Users\',\n            \'Organic vs Likelihood\',\n            \'IPS vs Non-IPS\',\n            \'Epsilon Worse\',\n        }:\n            with self.subTest(i = case):\n                try:\n                    notebook = self._execute_notebook(case)\n                except Exception:\n                    self.fail(f""Case has not passed: {case}"")\n\n            errors, warnings = self._analise_notebook(notebook)\n            self.assertEqual(errors, [], f""Case \'{case}\': NOK -- Errors: {errors}"")\n            self.assertEqual(warnings, [], f""Case \'{case}\': NOK -- Warnings: {warnings}"")\n\n            if with_replacement and len(errors) == 0 and len(warnings) == 0:\n                with open(f""{case}.new.ipynb"", mode = \'w\') as file:\n                    file.seek(0)\n                    file.write(f""{notebook}"")\n\n            print(f""Case \'{case}\': OK"")\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
setup.py,0,"b'import pathlib\n\nfrom setuptools import setup, find_packages\n\n# The directory containing this file\nHERE = pathlib.Path(__file__).parent\n\n# The text of the README file\nREADME = (HERE / ""README.md"").read_text()\n\n# This call to setup() does all the work\nsetup(\n    name=""recogym"",\n    version=""0.1.2.4"",\n    description=""Open-AI gym reinforcement learning environment for recommendation"",\n    long_description=README,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/criteo-research/reco-gym"",\n    author=""Criteo AI Lab"",\n    license=""Apache License"",\n    classifiers=[\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n    ],\n    packages=find_packages(exclude=(""tests"",)),\n    include_package_data=True,\n    install_requires=[\n        ""absl-py"",\n        ""pandas"",\n        ""matplotlib"",\n        ""simplegeneric"",\n        ""gym"",\n        ""torch"",\n        ""tensorflow"",\n        ""numba"",\n        ""tqdm"",\n        ""datetime"",\n        ""jupyter"",\n        ""scipy"",\n        ""numpy"",\n        ""scikit-learn"",\n    ],\n)\n'"
sim_test.py,0,"b'import argparse\nimport datetime\nimport glob\nimport os\nimport types\n\nimport pandas as pd\n\nfrom recogym import (\n    competition_score,\n    AgentInit,\n)\n\nif __name__ == ""__main__"":\n    import tensorflow as tf2\n    print(f\'TensorFlow V2: {tf2.__version__}\')\n    import tensorflow.compat.v1 as tf1\n    print(f\'TensorFlow V2: {tf1.__version__}\')\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--P\', type=int, default=100, help=\'Number of products\')\n    parser.add_argument(\'--UO\', type=int, default=100, help=\'Number of organic users to train on\')\n    parser.add_argument(\'--U\', type=int, default=100, help=\'Number of users to train on\')\n    parser.add_argument(\'--Utest\', type=int, default=1000, help=\'Number of users to test\')\n    parser.add_argument(\'--seed\', type=int, default=100, help=\'Seed\')\n    parser.add_argument(\'--K\', type=int, default=20, help=\'Number of latent factors\')\n    parser.add_argument(\'--F\', type=int, default=20,\n                        help=\'Number of flips, how different is bandit from organic\')\n    parser.add_argument(\'--log_epsilon\', type=float, default=0.05,\n                        help=\'Pop logging policy epsilon\')\n    parser.add_argument(\'--sigma_omega\', type=float, default=0.01, help=\'sigma_omega\')\n    parser.add_argument(\'--entries_dir\', type=str, default=\'my_entries\',\n                        help=\'directory with agent files for a leaderboard of small baselines for P small try setting to leaderboard_entries\')\n    parser.add_argument(\'--with_cache\', type=bool, default=False,\n                        help=\'Do use cache for training data or not\')\n\n    args = parser.parse_args()\n\n    P, UO, U, Utest, seed, num_flips, K, sigma_omega, log_epsilon, entries_dir, with_cache = (\n        args.P,\n        args.UO,\n        args.U,\n        args.Utest,\n        args.seed,\n        args.F,\n        args.K,\n        args.sigma_omega,\n        args.log_epsilon,\n        args.entries_dir,\n        args.with_cache,\n    )\n\n    print(args)\n\n    adf = []\n    start = datetime.datetime.now()\n    os.environ[\'KMP_DUPLICATE_LIB_OK\'] = \'True\'\n\n    for agent_file in glob.glob(entries_dir + \'/*.py\'):\n        print(f\'Agent: {agent_file}\')\n        try:\n            tmp_module = types.ModuleType(\'tmp_module\')\n            exec(\n                open(agent_file).read(),\n                tmp_module.__dict__\n            )\n            if hasattr(tmp_module, \'TestAgent\'):\n                agent_class = tmp_module.TestAgent\n                agent_configs = tmp_module.test_agent_args\n                agent_name = \'Test Agent\'\n            else:\n                if hasattr(tmp_module, \'agent\'):\n                    for agent_key in tmp_module.agent.keys():\n                        agent_class = tmp_module.agent[agent_key][AgentInit.CTOR]\n                        agent_configs = tmp_module.agent[agent_key][AgentInit.DEF_ARGS]\n                        agent_name = agent_key\n                else:\n                    print(\'There is no Agent to test!\')\n                    continue\n\n            df = competition_score(\n                P,\n                UO,\n                U,\n                Utest,\n                seed,\n                K,\n                num_flips,\n                log_epsilon,\n                sigma_omega,\n                agent_class,\n                agent_configs,\n                agent_name,\n                with_cache\n            )\n\n            df = df.join(pd.DataFrame({\n                \'entry\': [agent_file]\n            }))\n\n            print(df)\n\n            adf.append(df)\n        except Exception as ex:\n            print(f\'Agent @ ""{agent_file}"" failed: {str(ex)}\')\n\n    out_dir = entries_dir + \'_\' + str(P) + \'_\' + str(U) + \'_\' + str(Utest) + \'_\' + str(start)\n    os.mkdir(out_dir)\n    fp = open(out_dir + \'/config.txt\', \'w\')\n    fp.write(str(args))\n    fp.close()\n\n    leaderboard = pd.concat(adf)\n    leaderboard = leaderboard.sort_values(by=\'q0.500\', ascending=False)\n    leaderboard.to_csv(out_dir + \'/leaderboard.csv\')\n'"
leaderboard_entries/bandit_count.py,0,"b""from recogym import build_agent_init\nfrom recogym.agents import BanditCount, bandit_count_args\n\nagent = build_agent_init('BanditCount', BanditCount, {**bandit_count_args})\n"""
leaderboard_entries/bandit_mf_square.py,0,"b""from recogym import build_agent_init\nfrom recogym.agents import BanditMFSquare, bandit_mf_square_args\n\nagent = build_agent_init('BanditMFsquare', BanditMFSquare, {**bandit_mf_square_args})\n"""
leaderboard_entries/context_bandit.py,0,"b""from recogym import build_agent_init\nfrom recogym.agents import LogregMulticlassIpsAgent, logreg_multiclass_ips_args\n\nagent = build_agent_init('Contextual Bandit', LogregMulticlassIpsAgent,\n                         {**logreg_multiclass_ips_args, })\n"""
leaderboard_entries/dump_agent.py,0,"b'import pandas as pd\nimport numpy as np\n\nfrom recogym import Configuration, build_agent_init\nfrom recogym.agents import OrganicUserEventCounterAgent, organic_user_count_args\n\ndump_agent_args = {\n    \'agent\': build_agent_init(\n        \'OrganicUserCount\',\n        OrganicUserEventCounterAgent,\n        {**organic_user_count_args}\n    )\n}\n\n\nclass DumpAgent(OrganicUserEventCounterAgent):\n    """"""\n    Dump Agent\n\n    This is the Agent that dumps all its `train\' and `act\' functions.\n    It used mostly for debugging purposes.\n    """"""\n\n    def __init__(self, config=Configuration(dump_agent_args)):\n        super(DumpAgent, self).__init__(config)\n        self.previous_action = None\n\n        self.data = {\n            \'case\': [],\n            \'t\': [],\n            \'u\': [],\n            \'z\': [],\n            \'v\': [],\n            \'a\': [],\n            \'c\': [],\n            \'ps\': [],\n            \'ps-a\': [],\n            \'done\': [],\n        }\n\n    def _dump(self, case, observation, action, reward, done):\n        def _dump_organic():\n            for session in observation.sessions():\n                self.data[\'case\'].append(case)\n                self.data[\'t\'].append(session[\'t\'])\n                self.data[\'u\'].append(session[\'u\'])\n                self.data[\'z\'].append(\'organic\')\n                self.data[\'v\'].append(session[\'v\'])\n                self.data[\'a\'].append(None)\n                self.data[\'c\'].append(None)\n                self.data[\'ps\'].append(None)\n                self.data[\'ps-a\'].append(None)\n                self.data[\'done\'].append(done)\n\n        def _dump_bandit():\n            if action:\n                self.data[\'case\'].append(case)\n                self.data[\'t\'].append(action[\'t\'])\n                self.data[\'u\'].append(action[\'u\'])\n                self.data[\'z\'].append(\'bandit\')\n                self.data[\'v\'].append(None)\n                self.data[\'a\'].append(action[\'a\'])\n                self.data[\'c\'].append(reward)\n                self.data[\'ps\'].append(action[\'ps\'])\n                self.data[\'ps-a\'].append(action[\'ps-a\'])\n                self.data[\'done\'].append(done)\n\n        if case == \'A\':\n            _dump_bandit()\n            _dump_organic()\n        else:\n            _dump_organic()\n            _dump_bandit()\n\n    def train(self, observation, action, reward, done=False):\n        self._dump(\'T\', observation, action, reward, done)\n        self.config.agent.train(observation, action, reward, done)\n\n    def act(self, observation, reward, done):\n        """"""Make a recommendation""""""\n        self._dump(\'A\', observation, self.previous_action, reward, done)\n        if done:\n            return None\n        else:\n            action = self.config.agent.act(observation, reward, done)\n            self.previous_action = action\n            return action\n\n    def reset(self):\n        super().reset()\n        self.config.agent.reset()\n        self.previous_action = None\n\n    def dump(self):\n        self.data[\'t\'] = np.array(self.data[\'t\'], dtype=np.float32)\n        self.data[\'u\'] = pd.array(self.data[\'u\'], dtype=pd.UInt16Dtype())\n        self.data[\'v\'] = pd.array(self.data[\'v\'], dtype=pd.UInt16Dtype())\n        self.data[\'a\'] = pd.array(self.data[\'a\'], dtype=pd.UInt16Dtype())\n        self.data[\'c\'] = np.array(self.data[\'c\'], dtype=np.float32)\n        return pd.DataFrame().from_dict(self.data)\n\n\nagent = build_agent_init(\'DumpAgent\', DumpAgent, {**dump_agent_args})\n'"
leaderboard_entries/likelihood.py,0,"b""from recogym import build_agent_init\nfrom recogym.agents import LogregPolyAgent, logreg_poly_args\n\nagent = build_agent_init('Likelihood', LogregPolyAgent, {**logreg_poly_args, })\n"""
leaderboard_entries/likelihood_with_time.py,0,"b""import numpy as np\n\nfrom recogym import build_agent_init\nfrom recogym.agents import LogregPolyAgent, logreg_poly_args\n\n\nagent = build_agent_init(\n    'LikelihoodWithTime',\n    LogregPolyAgent,\n    {\n        **logreg_poly_args,\n        'weight_history_function': lambda t: np.exp(-t)\n    }\n)\n"""
leaderboard_entries/organic_count.py,0,"b""from recogym import build_agent_init\nfrom recogym.agents import OrganicCount, organic_count_args\n\nagent = build_agent_init('OrganicCount', OrganicCount, {**organic_count_args})\n"""
leaderboard_entries/organic_user_count.py,0,"b""from recogym import build_agent_init\nfrom recogym.agents.organic_user_count import organic_user_count_args, OrganicUserEventCounterAgent\n\nagent = build_agent_init('OrganicUserCount', OrganicUserEventCounterAgent, {**organic_user_count_args})\n"""
leaderboard_entries/pytorch_CB.py,0,"b""from recogym import build_agent_init\nfrom recogym.agents import PyTorchMLRAgent, pytorch_mlr_args\npytorch_mlr_args['n_epochs'] = 30\npytorch_mlr_args['learning_rate'] = 0.01\n\nagent = build_agent_init('PyTorchMLRAgent', PyTorchMLRAgent, {**pytorch_mlr_args})\n"""
leaderboard_entries/pytorch_CB_log.py,0,"b""from recogym import build_agent_init\nfrom recogym.agents import PyTorchMLRAgent, pytorch_mlr_args\n\npytorch_mlr_args['n_epochs'] = 30\npytorch_mlr_args['learning_rate'] = 0.01\npytorch_mlr_args['logIPS'] = True\n\nagent = build_agent_init('PyTorchMLRAgent', PyTorchMLRAgent, {**pytorch_mlr_args})\n"""
leaderboard_entries/pytorch_likelihood.py,0,"b""from recogym import build_agent_init\nfrom recogym.agents import PyTorchMLRAgent, pytorch_mlr_args\n\npytorch_mlr_args['n_epochs'] = 30\npytorch_mlr_args['learning_rate'] = 0.01,\npytorch_mlr_args['ll_IPS'] = False,\npytorch_mlr_args['alpha'] = 1.0\nagent = build_agent_init('PyTorchMLRAgent', PyTorchMLRAgent, {**pytorch_mlr_args})\n"""
leaderboard_entries/rand_agent.py,0,"b""from recogym import build_agent_init\nfrom recogym.agents import RandomAgent, random_args\n\nagent = build_agent_init('RandomAgent', RandomAgent, {**random_args})\n"""
leaderboard_entries/reweighted_likelihood.py,0,"b""from recogym import build_agent_init\nfrom recogym.agents import LogregPolyAgent, logreg_poly_args\n\nagent = build_agent_init('Re-weighted', LogregPolyAgent, {**logreg_poly_args, 'with_ips': True, })\n"""
my_entries/test_agent.py,0,"b'import numpy as np\n\nfrom recogym import Configuration, build_agent_init, to_categorical\nfrom recogym.agents import Agent\n\ntest_agent_args = {\n    \'num_products\': 10,\n    \'with_ps_all\': False,\n}\n\n\nclass TestAgent(Agent):\n    """"""Organic counter agent""""""\n\n    def __init__(self, config = Configuration(test_agent_args)):\n        super(TestAgent, self).__init__(config)\n\n        self.co_counts = np.zeros((self.config.num_products, self.config.num_products))\n        self.corr = None\n\n    def act(self, observation, reward, done):\n        """"""Make a recommendation""""""\n\n        self.update_lpv(observation)\n\n        action = self.co_counts[self.last_product_viewed, :].argmax()\n        if self.config.with_ps_all:\n            ps_all = np.zeros(self.config.num_products)\n            ps_all[action] = 1.0\n        else:\n            ps_all = ()\n        return {\n            **super().act(observation, reward, done),\n            **{\n                \'a\': self.co_counts[self.last_product_viewed, :].argmax(),\n                \'ps\': 1.0,\n                \'ps-a\': ps_all,\n            },\n        }\n\n    def train(self, observation, action, reward, done = False):\n        """"""Train the model in an online fashion""""""\n        if observation.sessions():\n            A = to_categorical(\n                [session[\'v\'] for session in observation.sessions()],\n                self.config.num_products\n            )\n            B = A.sum(0).reshape((self.config.num_products, 1))\n            self.co_counts = self.co_counts + np.matmul(B, B.T)\n\n    def update_lpv(self, observation):\n        """"""updates the last product viewed based on the observation""""""\n        if observation.sessions():\n            self.last_product_viewed = observation.sessions()[-1][\'v\']\n\n\n'"
recogym/__init__.py,0,"b""from .envs import env_0_args, env_1_args\nfrom .envs import Observation\nfrom .envs import Configuration\nfrom .envs import Session\nfrom .envs import Context, DefaultContext\n\nfrom .constants import (\n    AgentStats,\n    AgentInit,\n    EvolutionCase,\n    TrainingApproach,\n    RoiMetrics\n)\nfrom .bench_agents import test_agent\n\nfrom .evaluate_agent import (\n    evaluate_agent,\n    build_agent_init,\n    gather_agent_stats,\n    plot_agent_stats,\n    gather_exploration_stats,\n    plot_evolution_stats,\n    plot_heat_actions,\n    plot_roi,\n    verify_agents,\n    verify_agents_IPS,\n    to_categorical\n)\n\nfrom .competition import competition_score\n\nfrom .envs.features.time.default_time_generator import DefaultTimeGenerator\nfrom .envs.features.time.normal_time_generator import NormalTimeGenerator\n\nfrom gym.envs.registration import register\n\nregister(\n    id = 'reco-gym-v0',\n    entry_point = 'recogym.envs:RecoEnv0'\n)\n\nregister(\n    id = 'reco-gym-v1',\n    entry_point = 'recogym.envs:RecoEnv1'\n)\n\n"""
recogym/bench_agents.py,0,"b'import datetime\nimport hashlib\nimport json\nimport os\nimport pickle\nimport time\nfrom copy import deepcopy\nfrom pathlib import Path\n\nimport numpy as np\nfrom scipy.stats.distributions import beta\nfrom tqdm import trange, tqdm\n\nfrom recogym import AgentStats, DefaultContext, Observation\nfrom recogym.envs.session import OrganicSessions\n\nCACHE_DIR = os.path.join(os.path.join(str(Path.home()), \'.reco-gym\'), \'cache\')\n\n\ndef _cache_file_name(env, num_organic_offline_users: int, num_offline_users: int) -> str:\n    unique_config_data = (\n        (\n            env.config.K,\n            (\n                str(type(env.config.agent)),\n            ),\n            env.config.change_omega_for_bandits,\n            env.config.normalize_beta,\n            env.config.num_clusters,\n            env.config.num_products,\n            env.config.num_users,\n            env.config.number_of_flips,\n            env.config.phi_var,\n            env.config.prob_bandit_to_organic,\n            env.config.prob_leave_bandit,\n            env.config.prob_leave_organic,\n            env.config.prob_organic_to_bandit,\n            env.config.random_seed,\n            env.config.sigma_mu_organic,\n            env.config.sigma_omega,\n            env.config.sigma_omega_initial,\n            env.config.with_ps_all,\n        ),\n        num_organic_offline_users,\n        num_offline_users\n    )\n    return f\'{hashlib.sha1(json.dumps(unique_config_data).encode()).hexdigest()}.pkl\'\n\n\ndef _cached_data(env, num_organic_offline_users: int, num_offline_users: int) -> str:\n    cache_file_name = _cache_file_name(env, num_organic_offline_users, num_offline_users)\n    file_path = os.path.join(CACHE_DIR, cache_file_name)\n    if not os.path.exists(CACHE_DIR):\n        os.makedirs(CACHE_DIR)\n    if os.path.exists(file_path):\n        with open(file_path, \'rb\') as file:\n            data = pickle.load(file, fix_imports=False)\n    else:\n        data = env.generate_logs(num_offline_users=num_offline_users,\n                                 num_organic_offline_users=num_organic_offline_users)\n        with open(file_path, \'wb\') as file:\n            pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL, fix_imports=False)\n    return data\n\n\ndef _collect_stats(args):\n    env = args[\'env\']\n    agent = args[\'agent\']\n    num_offline_users = args[\'num_offline_users\']\n    num_online_users = args[\'num_online_users\']\n    num_organic_offline_users = args[\'num_organic_offline_users\']\n    epoch_with_random_reset = args[\'epoch_with_random_reset\']\n    epoch = args[\'epoch\']\n    with_cache = args[\'with_cache\']\n\n    print(f""START: Agent Training #{epoch}"")\n\n    unique_user_id = 0\n    new_agent = deepcopy(agent)\n\n    print(f""START: Agent Training @ Epoch #{epoch}"")\n    start = time.time()\n\n    if epoch_with_random_reset:\n        train_env = deepcopy(env)\n        train_env.reset_random_seed(epoch)\n    else:\n        train_env = env\n\n    if with_cache:\n        data = _cached_data(train_env, num_organic_offline_users, num_offline_users)\n\n        def _train(observation, session, action, reward, time, done):\n            if observation:\n                assert session is not None\n            else:\n                observation = Observation(DefaultContext(time, current_user), session)\n            new_agent.train(observation, action, reward, done)\n            return None, OrganicSessions(), None, None\n\n        current_session = OrganicSessions()\n        last_observation = None\n        last_action = None\n        last_reward = None\n        last_time = None\n\n        current_user = None\n        with tqdm(total=data.shape[0], desc=\'Offline Logs\') as pbar:\n            for _, row in data.iterrows():\n                pbar.update()\n                t, u, z, v, a, c, ps, ps_a = row.values\n\n                if current_user is None:\n                    current_user = u\n\n                if current_user != u:\n                    last_observation, current_session, last_action, last_reward = _train(\n                        last_observation,\n                        current_session,\n                        last_action,\n                        last_reward,\n                        last_time,\n                        True\n                    )\n                    current_user = u\n\n                if last_action:\n                    last_observation, current_session, last_action, last_reward = _train(\n                        last_observation,\n                        current_session,\n                        last_action,\n                        last_reward,\n                        last_time,\n                        False\n                    )\n\n                if z == \'organic\':\n                    assert (not np.isnan(v))\n                    assert (np.isnan(a))\n                    assert (np.isnan(c))\n                    current_session.next(DefaultContext(t, u), np.int16(v))\n                else:\n                    last_observation = Observation(DefaultContext(t, u), current_session)\n                    current_session = OrganicSessions()\n                    assert (np.isnan(v))\n                    assert (not np.isnan(a))\n                    last_action = {\n                        \'t\': t,\n                        \'u\': u,\n                        \'a\': np.int16(a),\n                        \'ps\': ps,\n                        \'ps-a\': ps_a,\n                    }\n                    assert (not np.isnan(c))\n                    last_reward = c\n\n                last_time = t\n\n            _train(\n                last_observation,\n                current_session,\n                last_action,\n                last_reward,\n                last_time,\n                True\n            )\n    else:\n        # Offline Organic Training.\n        for _ in trange(num_organic_offline_users, desc=\'Organic Users\'):\n            train_env.reset(unique_user_id)\n            unique_user_id += 1\n            new_observation, _, _, _ = train_env.step(None)\n            new_agent.train(new_observation, None, None, True)\n\n        # Offline Organic and Bandit Training.\n        for _ in trange(num_offline_users, desc=\'Users\'):\n            train_env.reset(unique_user_id)\n            unique_user_id += 1\n            new_observation, _, done, reward = train_env.step(None)\n            while not done:\n                old_observation = new_observation\n                action, new_observation, reward, done, _ = train_env.step_offline(\n                    old_observation, reward, done\n                )\n                new_agent.train(old_observation, action, reward, False)\n            old_observation = new_observation\n            action, _, reward, done, _ = train_env.step_offline(\n                old_observation, reward, done\n            )\n            new_agent.train(old_observation, action, reward, True)\n    print(f""END: Agent Training @ Epoch #{epoch} ({time.time() - start}s)"")\n\n    # Online Testing.\n    print(f""START: Agent Evaluating @ Epoch #{epoch}"")\n    start = time.time()\n\n    if epoch_with_random_reset:\n        eval_env = deepcopy(env)\n        eval_env.reset_random_seed(epoch)\n    else:\n        eval_env = env\n\n    stat_data = eval_env.generate_logs(num_offline_users=num_online_users, agent=new_agent)\n    rewards = stat_data[~np.isnan(stat_data[\'a\'])][\'c\']\n    successes = np.sum(rewards)\n    failures = rewards.shape[0] - successes\n    print(f""END: Agent Evaluating @ Epoch #{epoch} ({time.time() - start}s)"")\n\n    return {\n        AgentStats.SUCCESSES: successes,\n        AgentStats.FAILURES: failures,\n    }\n\n\ndef test_agent(\n        env,\n        agent,\n        num_offline_users=1000,\n        num_online_users=100,\n        num_organic_offline_users=0,\n        num_epochs=1,\n        epoch_with_random_reset=False,\n        with_cache=False\n\n):\n    successes = 0\n    failures = 0\n\n    argss = [\n        {\n            \'env\': env,\n            \'agent\': agent,\n            \'num_offline_users\': num_offline_users,\n            \'num_online_users\': num_online_users,\n            \'num_organic_offline_users\': num_organic_offline_users,\n            \'epoch_with_random_reset\': epoch_with_random_reset,\n            \'epoch\': epoch,\n            \'with_cache\': with_cache,\n        }\n        for epoch in range(num_epochs)\n    ]\n\n    for result in [_collect_stats(args) for args in argss]:\n        successes += result[AgentStats.SUCCESSES]\n        failures += result[AgentStats.FAILURES]\n\n    return (\n        beta.ppf(0.500, successes + 1, failures + 1),\n        beta.ppf(0.025, successes + 1, failures + 1),\n        beta.ppf(0.975, successes + 1, failures + 1)\n    )\n'"
recogym/competition.py,0,"b""import datetime\n\nimport gym\nimport pandas as pd\nfrom recogym import (\n    Configuration,\n    env_1_args,\n    gather_agent_stats,\n    build_agent_init,\n    AgentStats\n)\nfrom recogym.agents import OrganicUserEventCounterAgent, organic_user_count_args\n\n\ndef competition_score(\n    num_products: int,\n    num_organic_users_to_train: int,\n    num_users_to_train: int,\n    num_users_to_score: int,\n    random_seed: int,\n    latent_factor: int,\n    num_flips: int,\n    log_epsilon: float,\n    sigma_omega: float,\n    agent_class,\n    agent_configs,\n    agent_name: str,\n    with_cache: bool,\n):\n    training_data_samples = tuple([num_users_to_train])\n    testing_data_samples = num_users_to_score\n    stat_epochs = 1\n    stat_epochs_new_random_seed = True\n\n    std_env_args = {\n        **env_1_args,\n        'random_seed': random_seed,\n        'num_products': num_products,\n        'K': latent_factor,\n        'sigma_omega': sigma_omega,\n        'number_of_flips': num_flips\n    }\n\n    env = gym.make('reco-gym-v1')\n\n    agent_stats = gather_agent_stats(\n        env,\n        std_env_args,\n        {\n            'agent': OrganicUserEventCounterAgent(Configuration({\n                **organic_user_count_args,\n                **std_env_args,\n                'select_randomly': True,\n                'epsilon': log_epsilon,\n                'num_products': num_products,\n            })),\n        },\n        {\n            **build_agent_init(\n                agent_name,\n                agent_class,\n                {\n                    **agent_configs,\n                    'num_products': num_products,\n                }\n            ),\n        },\n        training_data_samples,\n        testing_data_samples,\n        stat_epochs,\n        stat_epochs_new_random_seed,\n        num_organic_users_to_train,\n        with_cache\n    )\n    time_start = datetime.datetime.now()\n\n    q0_025 = []\n    q0_500 = []\n    q0_975 = []\n    for agent_name in agent_stats[AgentStats.AGENTS]:\n        agent_values = agent_stats[AgentStats.AGENTS][agent_name]\n        q0_025.append(agent_values[AgentStats.Q0_025][0])\n        q0_500.append(agent_values[AgentStats.Q0_500][0])\n        q0_975.append(agent_values[AgentStats.Q0_975][0])\n\n    time_end = datetime.datetime.now()\n    seconds = (time_end - time_start).total_seconds()\n\n    return pd.DataFrame(\n        {\n            'q0.025': q0_025,\n            'q0.500': q0_500,\n            'q0.975': q0_975,\n            'time': [seconds],\n        }\n    )\n\n"""
recogym/constants.py,0,"b'from enum import Enum\n\n\nclass AgentStats(Enum):\n    """"""\n    Agent Statistics\n    """"""\n\n    # Confidence Interval.\n    Q0_025 = 0\n    Q0_500 = 1\n    Q0_975 = 2\n\n    # Number of Samples (Users) in a Training Data.\n    SAMPLES = 3\n\n    AGENTS = 4\n    SUCCESSES = 5  # Amount of Clicks.\n    FAILURES = 6  # Amount of non CLicks.\n\n\nclass AgentInit(Enum):\n    """"""\n    Abstract data for Agent Initialisation.\n    """"""\n    CTOR = 0  # Agent Constructor.\n    DEF_ARGS = 1  # Default Agent Arguments.\n\n\nclass TrainingApproach(Enum):\n    """"""\n    Training Approach of Evolution of Environment (Explore/Exploit approach).\n    """"""\n    ALL_DATA = 0  # All training data should be used (accumulated).\n    SLIDING_WINDOW_ALL_DATA = 1  # Fixed amount of training data (sliding window).\n    ALL_EXPLORATION_DATA = 2  # All training data obtained during Exploration (accumulated).\n    SLIDING_WINDOW_EXPLORATION_DATA = 3  # Fixed amount of training data obtained during Exploration.\n    MOST_VALUABLE = 4  # The most valuable training data.\n    LAST_STEP = 5  # All data BUT obtained only during the last step (both Explore and Exploit).\n\n\nclass EvolutionCase(Enum):\n    """"""\n    Evolution Stats Data.\n    """"""\n    SUCCESS = 0\n    FAILURE = 1\n    ACTIONS = 2\n    SUCCESS_GREEDY = 3\n    FAILURE_GREEDY = 4\n\n\nclass RoiMetrics(Enum):\n    """"""\n    Return of Investment Data.\n    """"""\n    ROI_MEAN = 0\n    ROI_0_025 = 1\n    ROI_0_975 = 2\n    ROI_SUCCESS = 3\n    ROI_FAILURE = 4\n'"
recogym/evaluate_agent.py,0,"b'import multiprocessing\nimport time\nfrom copy import deepcopy\nfrom multiprocessing import Pool\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import beta\n\nimport recogym\nfrom recogym import (\n    AgentInit,\n    AgentStats,\n    Configuration,\n    EvolutionCase,\n    RoiMetrics,\n    TrainingApproach\n)\nfrom recogym.agents import EpsilonGreedy, epsilon_greedy_args\nfrom .envs.context import DefaultContext\nfrom .envs.observation import Observation\nfrom .envs.session import OrganicSessions\n\nEpsilonDelta = .02\nEpsilonSteps = 6  # Including epsilon = 0.0.\nEpsilonPrecision = 2\nEvolutionEpsilons = (0.00, 0.01, 0.02, 0.03, 0.05, 0.08)\n\nGraphCTRMin = 0.009\nGraphCTRMax = 0.021\n\n\n# from Keras\ndef to_categorical(y, num_classes=None, dtype=\'float32\'):\n    y = np.array(y, dtype=\'int\')\n    input_shape = y.shape\n    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n        input_shape = tuple(input_shape[:-1])\n    y = y.ravel()\n    if not num_classes:\n        num_classes = np.max(y) + 1\n    n = y.shape[0]\n    categorical = np.zeros((n, num_classes), dtype=dtype)\n    categorical[np.arange(n), y] = 1\n    output_shape = input_shape + (num_classes,)\n    categorical = np.reshape(categorical, output_shape)\n    return categorical\n\n\ndef evaluate_agent(\n        env,\n        agent,\n        num_initial_train_users=100,\n        num_step_users=1000,\n        num_steps=10,\n        training_approach=TrainingApproach.ALL_DATA,\n        sliding_window_samples=10000):\n    initial_agent = deepcopy(agent)\n\n    unique_user_id = 0\n    for u in range(num_initial_train_users):\n        env.reset(unique_user_id + u)\n        agent.reset()\n        new_observation, reward, done, _ = env.step(None)\n        while True:\n            old_observation = new_observation\n            action, new_observation, reward, done, _ = env.step_offline(\n                new_observation, reward, False\n            )\n            agent.train(old_observation, action, reward, done)\n            if done:\n                break\n    unique_user_id += num_initial_train_users\n\n    rewards = {\n        EvolutionCase.SUCCESS: [],\n        EvolutionCase.SUCCESS_GREEDY: [],\n        EvolutionCase.FAILURE: [],\n        EvolutionCase.FAILURE_GREEDY: [],\n        EvolutionCase.ACTIONS: dict()\n    }\n    training_agent = deepcopy(agent)\n    samples = 0\n\n    for action_id in range(env.config.num_products):\n        rewards[EvolutionCase.ACTIONS][action_id] = [0]\n\n    for step in range(num_steps):\n        successes = 0\n        successes_greedy = 0\n        failures = 0\n        failures_greedy = 0\n\n        for u in range(num_step_users):\n            env.reset(unique_user_id + u)\n            agent.reset()\n            new_observation, reward, done, _ = env.step(None)\n            while not done:\n                old_observation = new_observation\n                action = agent.act(old_observation, reward, done)\n                new_observation, reward, done, info = env.step(action[\'a\'])\n                samples += 1\n\n                should_update_training_data = False\n                if training_approach == TrainingApproach.ALL_DATA or training_approach == TrainingApproach.LAST_STEP:\n                    should_update_training_data = True\n                elif training_approach == TrainingApproach.SLIDING_WINDOW_ALL_DATA:\n                    should_update_training_data = samples % sliding_window_samples == 0\n                elif training_approach == TrainingApproach.ALL_EXPLORATION_DATA:\n                    should_update_training_data = not action[\'greedy\']\n                elif training_approach == TrainingApproach.SLIDING_WINDOW_EXPLORATION_DATA:\n                    should_update_training_data = (not action[\n                        \'greedy\']) and samples % sliding_window_samples == 0\n                else:\n                    assert False, f""Unknown Training Approach: {training_approach}""\n\n                if should_update_training_data:\n                    training_agent.train(old_observation, action, reward, done)\n\n                if reward:\n                    successes += 1\n                    if \'greedy\' in action and action[\'greedy\']:\n                        successes_greedy += 1\n                    rewards[EvolutionCase.ACTIONS][action[\'a\']][-1] += 1\n                else:\n                    if \'greedy\' in action and action[\'greedy\']:\n                        failures_greedy += 1\n                    failures += 1\n        unique_user_id += num_step_users\n\n        agent = training_agent\n        for action_id in range(env.config.num_products):\n            rewards[EvolutionCase.ACTIONS][action_id].append(0)\n\n        if training_approach == TrainingApproach.LAST_STEP:\n            training_agent = deepcopy(initial_agent)\n        else:\n            training_agent = deepcopy(agent)\n\n        rewards[EvolutionCase.SUCCESS].append(successes)\n        rewards[EvolutionCase.SUCCESS_GREEDY].append(successes_greedy)\n        rewards[EvolutionCase.FAILURE].append(failures)\n        rewards[EvolutionCase.FAILURE_GREEDY].append(failures_greedy)\n\n    return rewards\n\n\ndef build_agent_init(agent_key, ctor, def_args):\n    return {\n        agent_key: {\n            AgentInit.CTOR: ctor,\n            AgentInit.DEF_ARGS: def_args,\n        }\n    }\n\n\ndef _collect_stats(args):\n    """"""\n    Function that is executed in a separate process.\n\n    :param args: arguments of the process to be executed.\n\n    :return: a vector of CTR for these confidence values:\n        0th: Q0.500\n        1st: Q0.025\n        snd: Q0.975\n    """"""\n    start = time.time()\n    print(f""START: Num of Users: {args[\'num_offline_users\']}"")\n    stats = recogym.test_agent(\n        deepcopy(args[\'env\']),\n        deepcopy(args[\'agent\']),\n        args[\'num_offline_users\'],\n        args[\'num_online_users\'],\n        args[\'num_organic_offline_users\'],\n        args[\'num_epochs\'],\n        args[\'epoch_with_random_reset\'],\n        args[\'with_cache\'],\n    )\n    print(f""END: Num of Offline Users: {args[\'num_offline_users\']} ({time.time() - start}s)"")\n    return stats\n\n\ndef gather_agent_stats(\n        env,\n        env_args,\n        extra_env_args,\n        agents_init_data,\n        user_samples=(100, 1000, 2000, 3000, 5000, 8000, 10000, 13000, 14000, 15000),\n        num_online_users: int = 15000,\n        num_epochs: int = 1,\n        epoch_with_random_reset: bool = False,\n        num_organic_offline_users: int = 100,\n        with_cache: bool = False\n):\n    """"""\n    The function that gathers Agents statistics via evaluating Agent performance\n     under different Environment conditions.\n\n    :param env: the Environment where some changes should be introduced and where Agent stats should\n        be gathered.\n    :param env_args: Environment arguments (default ones).\n    :param extra_env_args: extra Environment conditions those alter default values.\n    :param agents_init_data: Agent initialisation data.\n        This is a dictionary that has the following structure:\n        {\n            \'<Agent Name>\': {\n                AgentInit.CTOR: <Constructor>,\n                AgentInit.DEF_ARG: <Default Arguments>,\n            }\n        }\n    :param user_samples: Number of Offline Users i.e. Users used to train a Model.\n    :param num_online_users: Number of Online Users i.e. Users used to validate a Model.\n    :param num_epochs: how many different epochs should be tried to gather stats?\n    :param epoch_with_random_reset: should be a Random Seed reset at each new epoch?\n    :param num_organic_offline_users: how many Organic only users should be used for training.\n    :param with_cache: is the cache used for training data or not?\n\n    :return: a dictionary with stats\n        {\n            AgentStats.SAMPLES: [<vector of training offline users used to train a model>]\n            AgentStats.AGENTS: {\n                \'<Agent Name>\': {\n                    AgentStats.Q0_025: [],\n                    AgentStats.Q0_500: [],\n                    AgentStats.Q0_975: [],\n                }\n            }\n        }\n    """"""\n    new_env_args = {\n        **env_args,\n        **extra_env_args,\n    }\n\n    new_env = deepcopy(env)\n    new_env.init_gym(new_env_args)\n\n    agents = build_agents(agents_init_data, new_env_args)\n\n    agent_stats = {\n        AgentStats.SAMPLES: user_samples,\n        AgentStats.AGENTS: dict(),\n    }\n\n    for agent_key in agents:\n        print(f""Agent: {agent_key}"")\n        stats = {\n            AgentStats.Q0_025: [],\n            AgentStats.Q0_500: [],\n            AgentStats.Q0_975: [],\n        }\n\n        with Pool(processes=multiprocessing.cpu_count()) as pool:\n            argss = [\n                {\n                    \'env\': new_env,\n                    \'agent\': agents[agent_key],\n                    \'num_offline_users\': num_offline_users,\n                    \'num_online_users\': num_online_users,\n                    \'num_organic_offline_users\': num_organic_offline_users,\n                    \'num_epochs\': num_epochs,\n                    \'epoch_with_random_reset\': epoch_with_random_reset,\n                    \'with_cache\': with_cache\n                }\n                for num_offline_users in user_samples\n            ]\n\n            for result in (\n                    [_collect_stats(args) for args in argss]\n                    if num_epochs == 1 else\n                    pool.map(_collect_stats, argss)\n            ):\n                stats[AgentStats.Q0_025].append(result[1])\n                stats[AgentStats.Q0_500].append(result[0])\n                stats[AgentStats.Q0_975].append(result[2])\n\n        agent_stats[AgentStats.AGENTS][agent_key] = stats\n\n    return agent_stats\n\n\ndef build_agents(agents_init_data, new_env_args):\n    agents = dict()\n    for agent_key in agents_init_data:\n        agent_init_data = agents_init_data[agent_key]\n        ctor = agent_init_data[AgentInit.CTOR]\n        def_args = agent_init_data[AgentInit.DEF_ARGS]\n        agents[agent_key] = ctor(\n            Configuration({\n                **def_args,\n                **new_env_args,\n            })\n        )\n    return agents\n\n\ndef generate_epsilons(epsilon_step=EpsilonDelta, iterations=EpsilonSteps):\n    return [0.00, 0.01, 0.02, 0.03, 0.05, 0.08]\n\n\ndef format_epsilon(epsilon):\n    return (""{0:."" + f""{EpsilonPrecision}"" + ""f}"").format(round(epsilon, EpsilonPrecision))\n\n\ndef _collect_evolution_stats(args):\n    """"""\n    Function that is executed in a separate process.\n\n    :param args: arguments of the process to be executed.\n\n    :return: a dictionary of Success/Failures of applying an Agent.\n    """"""\n    start = time.time()\n    epsilon = args[\'epsilon\']\n    epsilon_key = format_epsilon(epsilon)\n    print(f""START: \xce\xb5 = {epsilon_key}"")\n    num_evolution_steps = args[\'num_evolution_steps\']\n    rewards = recogym.evaluate_agent(\n        deepcopy(args[\'env\']),\n        args[\'agent\'],\n        args[\'num_initial_train_users\'],\n        args[\'num_step_users\'],\n        num_evolution_steps,\n        args[\'training_approach\']\n    )\n\n    assert (len(rewards[EvolutionCase.SUCCESS]) == len(rewards[EvolutionCase.FAILURE]))\n    assert (len(rewards[EvolutionCase.SUCCESS]) == num_evolution_steps)\n    print(f""END: \xce\xb5 = {epsilon_key} ({time.time() - start}s)"")\n\n    return {\n        epsilon_key: {\n            EvolutionCase.SUCCESS: rewards[EvolutionCase.SUCCESS],\n            EvolutionCase.SUCCESS_GREEDY: rewards[EvolutionCase.SUCCESS_GREEDY],\n            EvolutionCase.FAILURE: rewards[EvolutionCase.FAILURE],\n            EvolutionCase.FAILURE_GREEDY: rewards[EvolutionCase.FAILURE_GREEDY],\n            EvolutionCase.ACTIONS: rewards[EvolutionCase.ACTIONS]\n        }\n    }\n\n\ndef gather_exploration_stats(\n        env,\n        env_args,\n        extra_env_args,\n        agents_init_data,\n        training_approach,\n        num_initial_train_users=1000,\n        num_step_users=1000,\n        epsilons=EvolutionEpsilons,\n        num_evolution_steps=6\n):\n    """"""\n    A helper function that collects data regarding Agents evolution\n    under different values of epsilon for Epsilon-Greedy Selection Policy.\n\n    :param env: The Environment where evolution should be applied;\n         every time when a new step of the evolution is applied, the Environment is deeply copied\n         thus the Environment does not interferes with evolution steps.\n\n    :param env_args: Environment arguments (default ones).\n    :param extra_env_args: extra Environment conditions those alter default values.\n    :param agents_init_data: Agent initialisation data.\n        This is a dictionary that has the following structure:\n        {\n            \'<Agent Name>\': {\n                AgentInit.CTOR: <Constructor>,\n                AgentInit.DEF_ARG: <Default Arguments>,\n            }\n        }\n\n\n    :param training_approach:  A training approach applied in verification;\n     for mode details look at `TrainingApproach\' enum.\n\n    :param num_initial_train_users: how many users\' data should be used\n     to train an initial model BEFORE evolution steps.\n\n    :param num_step_users: how many users\' data should be used\n     at each evolution step.\n\n     :param epsilons: a list of epsilon values.\n\n    :param num_evolution_steps: how many evolution steps should be applied\n     for an Agent with Epsilon-Greedy Selection Policy.\n\n    :return a dictionary of Agent evolution statistics in the form:\n        {\n            \'Agent Name\': {\n                \'Epsilon Values\': {\n                    EvolutionCase.SUCCESS: [an array of clicks (for each ith step of evolution)]\n                    EvolutionCase.FAILURE: [an array of failure to draw a click (for each ith step of evolution)]\n                }\n            }\n        }\n    """"""\n    # A dictionary that stores all data of Agent evolution statistics.\n    # Key is Agent Name, value is statistics.\n    agent_evolution_stats = dict()\n\n    new_env_args = {\n        **env_args,\n        **extra_env_args,\n    }\n\n    new_env = deepcopy(env)\n    new_env.init_gym(new_env_args)\n\n    agents = build_agents(agents_init_data, new_env_args)\n\n    for agent_key in agents:\n        print(f""Agent: {agent_key}"")\n        agent_stats = dict()\n\n        with Pool(processes=multiprocessing.cpu_count()) as pool:\n            for result in pool.map(\n                    _collect_evolution_stats,\n                    [\n                        {\n                            \'epsilon\': epsilon,\n                            \'env\': new_env,\n                            \'agent\': EpsilonGreedy(\n                                Configuration({\n                                    **epsilon_greedy_args,\n                                    **new_env_args,\n                                    \'epsilon\': epsilon,\n                                }),\n                                deepcopy(agents[agent_key])\n                            ),\n                            \'num_initial_train_users\': num_initial_train_users,\n                            \'num_step_users\': num_step_users,\n                            \'num_evolution_steps\': num_evolution_steps,\n                            \'training_approach\': training_approach,\n                        }\n                        for epsilon in epsilons\n                    ]\n            ):\n                agent_stats = {\n                    **agent_stats,\n                    **result,\n                }\n\n        agent_evolution_stats[agent_key] = agent_stats\n\n    return agent_evolution_stats\n\n\ndef plot_agent_stats(agent_stats):\n    _, ax = plt.subplots(\n        1,\n        1,\n        figsize=(16, 8)\n    )\n\n    user_samples = agent_stats[AgentStats.SAMPLES]\n    for agent_key in agent_stats[AgentStats.AGENTS]:\n        stats = agent_stats[AgentStats.AGENTS][agent_key]\n\n        ax.fill_between(\n            user_samples,\n            stats[AgentStats.Q0_975],\n            stats[AgentStats.Q0_025],\n            alpha=.05\n        )\n\n        ax.plot(user_samples, stats[AgentStats.Q0_500])\n\n        ax.set_xlabel(\'Samples #\')\n        ax.set_ylabel(\'CTR\')\n        ax.legend([\n            ""$C^{CTR}_{0.5}$: "" + f""{agent_key}"" for agent_key in agent_stats[AgentStats.AGENTS]\n        ])\n\n    plt.show()\n\n\ndef plot_evolution_stats(\n        agent_evolution_stats,\n        max_agents_per_row=2,\n        epsilons=EvolutionEpsilons,\n        plot_min=GraphCTRMin,\n        plot_max=GraphCTRMax\n):\n    figs, axs = plt.subplots(\n        int(len(agent_evolution_stats) / max_agents_per_row),\n        max_agents_per_row,\n        figsize=(16, 10),\n        squeeze=False\n    )\n    labels = [(""$\\epsilon=$"" + format_epsilon(epsilon)) for epsilon in epsilons]\n\n    for (ix, agent_key) in enumerate(agent_evolution_stats):\n        ax = axs[int(ix / max_agents_per_row), int(ix % max_agents_per_row)]\n        agent_evolution_stat = agent_evolution_stats[agent_key]\n\n        ctr_means = []\n        for epsilon in epsilons:\n            epsilon_key = format_epsilon(epsilon)\n            evolution_stat = agent_evolution_stat[epsilon_key]\n\n            steps = []\n            ms = []\n            q0_025 = []\n            q0_975 = []\n\n            assert (len(evolution_stat[EvolutionCase.SUCCESS]) == len(\n                evolution_stat[EvolutionCase.FAILURE]))\n            for step in range(len(evolution_stat[EvolutionCase.SUCCESS])):\n                steps.append(step)\n                successes = evolution_stat[EvolutionCase.SUCCESS][step]\n                failures = evolution_stat[EvolutionCase.FAILURE][step]\n\n                ms.append(beta.ppf(0.5, successes + 1, failures + 1))\n                q0_025.append(beta.ppf(0.025, successes + 1, failures + 1))\n                q0_975.append(beta.ppf(0.975, successes + 1, failures + 1))\n\n            ctr_means.append(np.mean(ms))\n\n            ax.fill_between(\n                range(len(steps)),\n                q0_975,\n                q0_025,\n                alpha=.05\n            )\n            ax.plot(steps, ms)\n\n        ctr_means_mean = np.mean(ctr_means)\n        ctr_means_div = np.sqrt(np.var(ctr_means))\n        ax.set_title(\n            f""Agent: {agent_key}\\n""\n            + ""$\\hat{Q}^{CTR}_{0.5}=""\n            + ""{0:.5f}"".format(round(ctr_means_mean, 5))\n            + ""$, ""\n            + ""$\\hat{\\sigma}^{CTR}_{0.5}=""\n            + ""{0:.5f}"".format(round(ctr_means_div, 5))\n            + ""$""\n        )\n        ax.legend(labels)\n        ax.set_ylabel(\'CTR\')\n        ax.set_ylim([plot_min, plot_max])\n\n    plt.subplots_adjust(hspace=.5)\n    plt.show()\n\n\ndef plot_heat_actions(\n        agent_evolution_stats,\n        epsilons=EvolutionEpsilons\n):\n    max_epsilons_per_row = len(epsilons)\n    the_first_agent = next(iter(agent_evolution_stats.values()))\n    epsilon_steps = len(the_first_agent)\n    rows = int(len(agent_evolution_stats) * epsilon_steps / max_epsilons_per_row)\n    figs, axs = plt.subplots(\n        int(len(agent_evolution_stats) * epsilon_steps / max_epsilons_per_row),\n        max_epsilons_per_row,\n        figsize=(16, 4 * rows),\n        squeeze=False\n    )\n\n    for (ix, agent_key) in enumerate(agent_evolution_stats):\n        agent_evolution_stat = agent_evolution_stats[agent_key]\n        for (jx, epsilon_key) in enumerate(agent_evolution_stat):\n            flat_index = ix * epsilon_steps + jx\n            ax = axs[int(flat_index / max_epsilons_per_row), int(flat_index % max_epsilons_per_row)]\n\n            evolution_stat = agent_evolution_stat[epsilon_key]\n\n            action_stats = evolution_stat[EvolutionCase.ACTIONS]\n            total_actions = len(action_stats)\n            heat_data = []\n            for kx in range(total_actions):\n                heat_data.append(action_stats[kx])\n\n            heat_data = np.array(heat_data)\n            im = ax.imshow(heat_data)\n\n            ax.set_yticks(np.arange(total_actions))\n            ax.set_yticklabels([f""{action_id}"" for action_id in range(total_actions)])\n\n            ax.set_title(f""Agent: {agent_key}\\n$\\epsilon=${epsilon_key}"")\n\n            _ = ax.figure.colorbar(im, ax=ax)\n\n    plt.show()\n\n\ndef plot_roi(\n        agent_evolution_stats,\n        epsilons=EvolutionEpsilons,\n        max_agents_per_row=2\n):\n    """"""\n    A helper function that calculates Return of Investment (ROI) for applying Epsilon-Greedy Selection Policy.\n\n    :param agent_evolution_stats: statistic about Agent evolution collected in `build_exploration_data\'.\n\n    :param epsilons: a list of epsilon values.\n\n    :param max_agents_per_row: how many graphs should be drawn per a row\n\n    :return: a dictionary of Agent ROI after applying Epsilon-Greedy Selection Strategy in the following form:\n        {\n            \'Agent Name\': {\n                \'Epsilon Value\': {\n                    Metrics.ROI: [an array of ROIs for each ith step (starting from 1st step)]\n                }\n            }\n        }\n    """"""\n    figs, axs = plt.subplots(\n        int(len(agent_evolution_stats) / max_agents_per_row),\n        max_agents_per_row,\n        figsize=(16, 8),\n        squeeze=False\n    )\n    labels = [(""$\\epsilon=$"" + format_epsilon(epsilon)) for epsilon in epsilons if epsilon != 0.0]\n\n    agent_roi_stats = dict()\n\n    for (ix, agent_key) in enumerate(agent_evolution_stats):\n        ax = axs[int(ix / max_agents_per_row), int(ix % max_agents_per_row)]\n        agent_stat = agent_evolution_stats[agent_key]\n        zero_epsilon_key = format_epsilon(0)\n        zero_epsilon = agent_stat[zero_epsilon_key]\n        zero_success_evolutions = zero_epsilon[EvolutionCase.SUCCESS]\n        zero_failure_evolutions = zero_epsilon[EvolutionCase.FAILURE]\n        assert (len(zero_success_evolutions))\n\n        agent_stats = dict()\n        roi_mean_means = []\n        for epsilon in generate_epsilons():\n            if zero_epsilon_key == format_epsilon(epsilon):\n                continue\n\n            epsilon_key = format_epsilon(epsilon)\n            agent_stats[epsilon_key] = {\n                RoiMetrics.ROI_0_025: [],\n                RoiMetrics.ROI_MEAN: [],\n                RoiMetrics.ROI_0_975: [],\n            }\n            epsilon_evolutions = agent_stat[epsilon_key]\n            success_greedy_evolutions = epsilon_evolutions[EvolutionCase.SUCCESS_GREEDY]\n            failure_greedy_evolutions = epsilon_evolutions[EvolutionCase.FAILURE_GREEDY]\n            assert (len(success_greedy_evolutions) == len(failure_greedy_evolutions))\n            assert (len(zero_success_evolutions) == len(success_greedy_evolutions))\n            steps = []\n            roi_means = []\n            for step in range(1, len(epsilon_evolutions[EvolutionCase.SUCCESS])):\n                previous_zero_successes = zero_success_evolutions[step - 1]\n                previous_zero_failures = zero_failure_evolutions[step - 1]\n                current_zero_successes = zero_success_evolutions[step]\n                current_zero_failures = zero_failure_evolutions[step]\n                current_epsilon_greedy_successes = success_greedy_evolutions[step]\n                current_epsilon_greedy_failures = failure_greedy_evolutions[step]\n\n                def roi_with_confidence_interval(\n                        epsilon,\n                        previous_zero_successes,\n                        previous_zero_failures,\n                        current_zero_successes,\n                        current_zero_failures,\n                        current_epsilon_greedy_successes,\n                        current_epsilon_greedy_failures\n                ):\n                    def roi_formulae(\n                            epsilon,\n                            previous_zero,\n                            current_zero,\n                            current_epsilon_greedy\n                    ):\n                        current_gain = current_epsilon_greedy / (1 - epsilon) - current_zero\n                        roi = current_gain / (epsilon * previous_zero)\n                        return roi\n\n                    return {\n                        RoiMetrics.ROI_SUCCESS: roi_formulae(\n                            epsilon,\n                            previous_zero_successes,\n                            current_zero_successes,\n                            current_epsilon_greedy_successes\n                        ),\n                        RoiMetrics.ROI_FAILURE: roi_formulae(\n                            epsilon,\n                            previous_zero_failures,\n                            current_zero_failures,\n                            current_epsilon_greedy_failures\n                        )\n                    }\n\n                roi_mean = roi_with_confidence_interval(\n                    epsilon,\n                    previous_zero_successes,\n                    previous_zero_failures,\n                    current_zero_successes,\n                    current_zero_failures,\n                    current_epsilon_greedy_successes,\n                    current_epsilon_greedy_failures\n                )[RoiMetrics.ROI_SUCCESS]\n                agent_stats[epsilon_key][RoiMetrics.ROI_MEAN].append(roi_mean)\n\n                roi_means.append(roi_mean)\n\n                steps.append(step)\n\n            roi_mean_means.append(np.mean(roi_means))\n            ax.plot(steps, roi_means)\n\n        roi_means_mean = np.mean(roi_mean_means)\n        roi_means_div = np.sqrt(np.var(roi_mean_means))\n        ax.set_title(\n            ""$ROI_{t+1}$ of Agent: "" + f""\'{agent_key}\'\\n""\n            + ""$\\hat{\\mu}_{ROI}=""\n            + ""{0:.5f}"".format(round(roi_means_mean, 5))\n            + ""$, ""\n            + ""$\\hat{\\sigma}_{ROI}=""\n            + ""{0:.5f}"".format(round(roi_means_div, 5))\n            + ""$""\n        )\n        ax.legend(labels, loc=10)\n        ax.set_ylabel(\'ROI\')\n\n        agent_roi_stats[agent_key] = agent_stats\n\n    plt.subplots_adjust(hspace=.5)\n    plt.show()\n    return agent_roi_stats\n\n\ndef verify_agents(env, number_of_users, agents):\n    stat = {\n        \'Agent\': [],\n        \'0.025\': [],\n        \'0.500\': [],\n        \'0.975\': [],\n    }\n\n    for agent_id in agents:\n        stat[\'Agent\'].append(agent_id)\n        data = deepcopy(env).generate_logs(number_of_users, agents[agent_id])\n        bandits = data[data[\'z\'] == \'bandit\']\n        successes = bandits[bandits[\'c\'] == 1].shape[0]\n        failures = bandits[bandits[\'c\'] == 0].shape[0]\n        stat[\'0.025\'].append(beta.ppf(0.025, successes + 1, failures + 1))\n        stat[\'0.500\'].append(beta.ppf(0.500, successes + 1, failures + 1))\n        stat[\'0.975\'].append(beta.ppf(0.975, successes + 1, failures + 1))\n\n    return pd.DataFrame().from_dict(stat)\n\n\ndef evaluate_IPS(agent, reco_log):\n    ee = []\n    for u in range(max(reco_log.u)):\n        t = np.array(reco_log[reco_log[\'u\'] == u].t)\n        v = np.array(reco_log[reco_log[\'u\'] == u].v)\n        a = np.array(reco_log[reco_log[\'u\'] == u].a)\n        c = np.array(reco_log[reco_log[\'u\'] == u].c)\n        z = list(reco_log[reco_log[\'u\'] == u].z)\n        ps = np.array(reco_log[reco_log[\'u\'] == u].ps)\n\n        jj = 0\n\n        session = OrganicSessions()\n        agent.reset()\n        while True:\n            if jj >= len(z):\n                break\n            if z[jj] == \'organic\':\n                session.next(DefaultContext(t[jj], u), int(v[jj]))\n            else:\n                prob_policy = agent.act(Observation(DefaultContext(t[jj], u), session), 0, False)[\n                    \'ps-a\']\n                \n                if prob_policy!=():\n                    ee.append(c[jj] * prob_policy[int(a[jj])] / ps[jj])\n                session = OrganicSessions()\n            jj += 1\n    return ee\n\n\ndef evaluate_SNIPS(agent, reco_log):\n    rewards = []\n    p_ratio = []\n    for u in range(max(reco_log.u)):\n        t = np.array(reco_log[reco_log[\'u\'] == u].t)\n        v = np.array(reco_log[reco_log[\'u\'] == u].v)\n        a = np.array(reco_log[reco_log[\'u\'] == u].a)\n        c = np.array(reco_log[reco_log[\'u\'] == u].c)\n        z = list(reco_log[reco_log[\'u\'] == u].z)\n        ps = np.array(reco_log[reco_log[\'u\'] == u].ps)\n\n        jj = 0\n\n        session = OrganicSessions()\n        agent.reset()\n        while True:\n            if jj >= len(z):\n                break\n            if z[jj] == \'organic\':\n                session.next(DefaultContext(t[jj], u), int(v[jj]))\n            else:\n                prob_policy = agent.act(Observation(DefaultContext(t[jj], u), session), 0, False)[\n                    \'ps-a\']\n                rewards.append(c[jj])\n                p_ratio.append(prob_policy[int(a[jj])] / ps[jj])\n                session = OrganicSessions()\n            jj += 1\n    return rewards, p_ratio\n\n\ndef verify_agents_IPS(reco_log, agents):\n    stat = {\n        \'Agent\': [],\n        \'0.025\': [],\n        \'0.500\': [],\n        \'0.975\': [],\n    }\n\n    for agent_id in agents:\n        ee = evaluate_IPS(agents[agent_id], reco_log)\n        mean_ee = np.mean(ee)\n        se_ee = np.std(ee) / np.sqrt(len(ee))\n        stat[\'Agent\'].append(agent_id)\n        stat[\'0.025\'].append(mean_ee - 2 * se_ee)\n        stat[\'0.500\'].append(mean_ee)\n        stat[\'0.975\'].append(mean_ee + 2 * se_ee)\n    return pd.DataFrame().from_dict(stat)\n\n\ndef verify_agents_SNIPS(reco_log, agents):\n    stat = {\n        \'Agent\': [],\n        \'0.025\': [],\n        \'0.500\': [],\n        \'0.975\': [],\n    }\n\n    for agent_id in agents:\n        rewards, p_ratio = evaluate_SNIPS(agents[agent_id], reco_log)\n        ee = np.asarray(rewards) * np.asarray(p_ratio)\n        mean_ee = np.sum(ee) / np.sum(p_ratio)\n        se_ee = np.std(ee) / np.sqrt(len(ee))\n        stat[\'Agent\'].append(agent_id)\n        stat[\'0.025\'].append(mean_ee - 2 * se_ee)\n        stat[\'0.500\'].append(mean_ee)\n        stat[\'0.975\'].append(mean_ee + 2 * se_ee)\n    return pd.DataFrame().from_dict(stat)\n\n\ndef evaluate_recall_at_k(agent, reco_log, k=5):\n    hits = []\n    for u in range(max(reco_log.u)):\n        t = np.array(reco_log[reco_log[\'u\'] == u].t)\n        v = np.array(reco_log[reco_log[\'u\'] == u].v)\n        a = np.array(reco_log[reco_log[\'u\'] == u].a)\n        c = np.array(reco_log[reco_log[\'u\'] == u].c)\n        z = list(reco_log[reco_log[\'u\'] == u].z)\n        ps = np.array(reco_log[reco_log[\'u\'] == u].ps)\n\n        jj = 0\n\n        session = OrganicSessions()\n        agent.reset()\n        while True:\n            if jj >= len(z):\n                break\n            if z[jj] == \'organic\':\n                session.next(DefaultContext(t[jj], u), int(v[jj]))\n            else:\n                prob_policy = agent.act(Observation(DefaultContext(t[jj], u), session), 0, False)[\n                    \'ps-a\']\n                # Does the next session exist?\n                if (jj + 1) < len(z):\n                    # Is the next session organic?\n                    if z[jj + 1] == \'organic\':\n                        # Whas there no click for this bandit event?\n                        if not c[jj]:\n                            # Generate a top-K from the probability distribution over all actions\n                            top_k = set(np.argpartition(prob_policy, -k)[-k:])\n                            # Is the next seen item in the top-K?\n                            if v[jj + 1] in top_k:\n                                hits.append(1)\n                            else:\n                                hits.append(0)\n                session = OrganicSessions()\n            jj += 1\n    return hits\n\n\ndef verify_agents_recall_at_k(reco_log, agents, k=5):\n    stat = {\n        \'Agent\': [],\n        \'0.025\': [],\n        \'0.500\': [],\n        \'0.975\': [],\n    }\n\n    for agent_id in agents:\n        hits = evaluate_recall_at_k(agents[agent_id], reco_log, k=k)\n        mean_hits = np.mean(hits)\n        se_hits = np.std(hits) / np.sqrt(len(hits))\n        stat[\'Agent\'].append(agent_id)\n        stat[\'0.025\'].append(mean_hits - 2 * se_hits)\n        stat[\'0.500\'].append(mean_hits)\n        stat[\'0.975\'].append(mean_hits + 2 * se_hits)\n    return pd.DataFrame().from_dict(stat)\n\n\ndef plot_verify_agents(result):\n    fig, ax = plt.subplots()\n    ax.set_title(\'CTR Estimate for Different Agents\')\n    plt.errorbar(result[\'Agent\'],\n                 result[\'0.500\'],\n                 yerr=(result[\'0.500\'] - result[\'0.025\'],\n                       result[\'0.975\'] - result[\'0.500\']),\n                 fmt=\'o\',\n                 capsize=4)\n    plt.xticks(result[\'Agent\'], result[\'Agent\'], rotation=\'vertical\')\n    return fig\n'"
recogym/agents/__init__.py,0,"b'\n\n\nfrom .abstract import (\n    Agent,\n    FeatureProvider,\n    AbstractFeatureProvider,\n    ViewsFeaturesProvider,\n    Model,\n    ModelBasedAgent\n)\nfrom .bayesian_poly_vb import BayesianAgentVB\nfrom .bandit_mf import BanditMFSquare, bandit_mf_square_args\nfrom .bandit_count import BanditCount, bandit_count_args\nfrom .random_agent import RandomAgent, random_args\nfrom .organic_count import OrganicCount, organic_count_args\nfrom .organic_mf import OrganicMFSquare, organic_mf_square_args\nfrom .logreg_ips import LogregMulticlassIpsAgent, logreg_multiclass_ips_args\nfrom .nn_ips import NnIpsAgent, nn_ips_args\nfrom .epsilon_greedy import EpsilonGreedy, epsilon_greedy_args\nfrom .logreg_poly import LogregPolyAgent, logreg_poly_args\nfrom .organic_user_count import OrganicUserEventCounterAgent, organic_user_count_args\n\nfrom .pytorch_mlr import PyTorchMLRAgent, pytorch_mlr_args\n\n\n'"
recogym/agents/abstract.py,0,"b'import math\n\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse as sparse\nfrom tqdm import tqdm\n\n\nclass Agent:\n    """"""\n    This is an abstract Agent class.\n    The class defines an interface with methods those should be overwritten for a new Agent.\n    """"""\n\n    def __init__(self, config):\n        self.config = config\n\n    def act(self, observation, reward, done):\n        """"""An act method takes in an observation, which could either be\n           `None` or an Organic_Session (see recogym/session.py) and returns\n           a integer between 0 and num_products indicating which product the\n           agent recommends""""""\n        return {\n            \'t\': observation.context().time(),\n            \'u\': observation.context().user(),\n        }\n\n    def train(self, observation, action, reward, done=False):\n        """"""Use this function to update your model based on observation, action,\n            reward tuples""""""\n        pass\n\n    def reset(self):\n        pass\n\n\nclass ModelBuilder:\n    """"""\n    Model Builder\n\n    The class that collects data obtained during a set of sessions\n    (the data are collected via set of calls `train\' method)\n    and when it is decided that there is enough data to create a Model,\n    the Model Builder generates BOTH the Feature Provider and the Model.\n    Next time when the Model is to be used,\n    the Feature Provider generates a Feature Set\n    that is suitable for the Model.\n    """"""\n\n    def __init__(self, config):\n        self.config = config\n        self.data = None\n        self.reset()\n\n    def train(self, observation, action, reward, done):\n        """"""\n        Train a Model\n\n        The method should be called every time when a new training data should be added.\n        These data are used to train a Model.\n\n        :param observation: Organic Sessions\n        :param action: an Agent action for the Observation\n        :param reward: reward (click/no click)\n        :param done:\n\n        :return: nothing\n        """"""\n        assert (observation is not None)\n        assert (observation.sessions() is not None)\n        for session in observation.sessions():\n            self.data[\'t\'].append(session[\'t\'])\n            self.data[\'u\'].append(session[\'u\'])\n            self.data[\'z\'].append(\'organic\')\n            self.data[\'v\'].append(session[\'v\'])\n            self.data[\'a\'].append(None)\n            self.data[\'c\'].append(None)\n            self.data[\'ps\'].append(None)\n\n        if action:\n            self.data[\'t\'].append(action[\'t\'])\n            self.data[\'u\'].append(action[\'u\'])\n            self.data[\'z\'].append(\'bandit\')\n            self.data[\'v\'].append(None)\n            self.data[\'a\'].append(action[\'a\'])\n            self.data[\'c\'].append(reward)\n            self.data[\'ps\'].append(action[\'ps\'])\n\n    def build(self):\n        """"""\n        Build a Model\n\n        The function generates a tuple: (FeatureProvider, Model)\n        """"""\n        raise NotImplemented\n\n    def reset(self):\n        """"""\n        Reset Data\n\n        The method clears all previously collected training data.\n        """"""\n        self.data = {\n            \'t\': [],\n            \'u\': [],\n            \'z\': [],\n            \'v\': [],\n            \'a\': [],\n            \'c\': [],\n            \'ps\': [],\n        }\n\n\nclass Model:\n    """"""\n    Model\n\n    """"""\n\n    def __init__(self, config):\n        self.config = config\n\n    def act(self, observation, features):\n        return {\n            \'t\': observation.context().time(),\n            \'u\': observation.context().user(),\n        }\n\n    def reset(self):\n        pass\n\n\nclass FeatureProvider:\n    """"""\n    Feature Provider\n\n    The interface defines a set of methods used to:\n    * collect data from which should be generated a Feature Set\n    * generate a Feature Set suitable for a particular Model from previously collected data\n    """"""\n\n    def __init__(self, config):\n        self.config = config\n\n    def observe(self, observation):\n        """"""\n        Collect Observations\n\n        The data are collected for a certain user\n\n        :param observation:\n        :return:\n        """"""\n        raise NotImplemented\n\n    def features(self, observation):\n        """"""\n        Generate a Feature Set\n\n        :return: a Feature Set suitable for a certain Model\n        """"""\n        raise NotImplemented\n\n    def reset(self):\n        """"""\n        Reset\n\n        Clear all previously collected data.\n        :return: nothing\n        """"""\n        raise NotImplemented\n\n\nclass AbstractFeatureProvider(ModelBuilder):\n    """"""\n    Abstract Feature Provider\n\n    The Feature Provider that contains the common logic in\n    creation of a Feature Set that consists of:\n    * Views (count of Organic Events: Products views)\n    * Actions (Actions provided by an Agent)\n    * Propensity Scores: probability of selecting an Action by an Agent\n    * Delta (rewards: 1 -- there was a Click; 0 -- there was no)\n    """"""\n\n    def __init__(self, config, is_sparse=False):\n        super(AbstractFeatureProvider, self).__init__(config)\n        self.is_sparse = is_sparse\n\n    def train_data(self):\n        data = pd.DataFrame().from_dict(self.data)\n\n        features = []\n        actions = []\n        pss = []\n        deltas = []\n\n        with_history = hasattr(self.config, \'weight_history_function\')\n\n        for user_id in tqdm(data[\'u\'].unique(), desc=\'Train Data\'):\n            ix = 0\n            ixs = []\n            jxs = []\n            if with_history:\n                history = []\n\n            assert data[data[\'u\'] == user_id].shape[0] <= np.iinfo(np.int16).max\n\n            for _, user_datum in data[data[\'u\'] == user_id].iterrows():\n                assert (not math.isnan(user_datum[\'t\']))\n                if user_datum[\'z\'] == \'organic\':\n                    assert (math.isnan(user_datum[\'a\']))\n                    assert (math.isnan(user_datum[\'c\']))\n                    assert (not math.isnan(user_datum[\'v\']))\n\n                    view = np.int16(user_datum[\'v\'])\n\n                    if with_history:\n                        ixs.append(np.int16(ix))\n                        jxs.append(view)\n\n                        history.append(np.int16(user_datum[\'t\']))\n                        ix += 1\n                    else:\n                        jxs.append(view)\n                else:\n                    assert (user_datum[\'z\'] == \'bandit\')\n                    assert (not math.isnan(user_datum[\'a\']))\n                    assert (not math.isnan(user_datum[\'c\']))\n                    assert (math.isnan(user_datum[\'v\']))\n\n                    action = np.int16(user_datum[\'a\'])\n                    delta = np.int16(user_datum[\'c\'])\n                    ps = user_datum[\'ps\']\n                    time = np.int16(user_datum[\'t\'])\n\n                    if with_history:\n                        assert len(ixs) == len(jxs)\n                        views = sparse.coo_matrix(\n                            (np.ones(len(ixs), dtype=np.int16), (ixs, jxs)),\n                            shape=(len(ixs), self.config.num_products),\n                            dtype=np.int16\n                        )\n                        weights = self.config.weight_history_function(\n                            time - np.array(history)\n                        )\n                        weighted_views = views.multiply(weights[:, np.newaxis])\n                        features.append(\n                            sparse.coo_matrix(\n                                weighted_views.sum(axis=0, dtype=np.float32),\n                                copy=False\n                            )\n                        )\n                    else:\n                        views = sparse.coo_matrix(\n                            (\n                                np.ones(len(jxs), dtype=np.int16),\n                                (np.zeros(len(jxs)), jxs)\n                            ),\n                            shape=(1, self.config.num_products),\n                            dtype=np.int16\n                        )\n                        features.append(views)\n\n                    actions.append(action)\n                    deltas.append(delta)\n                    pss.append(ps)\n\n        out_features = sparse.vstack(features, format=\'csr\')\n        return (\n            (\n                out_features\n                if self.is_sparse else\n                np.array(out_features.todense(), dtype=np.float)\n            ),\n            np.array(actions, dtype=np.int16),\n            np.array(deltas),\n            np.array(pss)\n        )\n\n\nclass ModelBasedAgent(Agent):\n    """"""\n    Model Based Agent\n\n    This is a common implementation of the Agent that uses a certain Model when it acts.\n    The Agent implements all routines needed to interact with the Model, namely:\n    * training\n    * acting\n    """"""\n\n    def __init__(self, config, model_builder):\n        super(ModelBasedAgent, self).__init__(config)\n        self.model_builder = model_builder\n        self.feature_provider = None\n        self.model = None\n\n    def train(self, observation, action, reward, done=False):\n        self.model_builder.train(observation, action, reward, done)\n\n    def act(self, observation, reward, done):\n        if self.model is None:\n            assert (self.feature_provider is None)\n            self.feature_provider, self.model = self.model_builder.build()\n        self.feature_provider.observe(observation)\n        return {\n            **super().act(observation, reward, done),\n            **self.model.act(observation, self.feature_provider.features(observation)),\n        }\n\n    def reset(self):\n        if self.model is not None:\n            assert (self.feature_provider is not None)\n            self.feature_provider.reset()\n            self.model.reset()\n\n\nclass ViewsFeaturesProvider(FeatureProvider):\n    """"""\n    Views Feature Provider\n\n    This class provides Views of Products i.e. for all Products viewed by a users so far,\n    the class returns a vector where at the index that corresponds to the Product you shall find\n    amount of Views of that product.\n\n    E.G.:\n    Amount of Products is 5.\n    Then, the class returns the vector [0, 3, 7, 0, 2].\n    That means that Products with IDs were viewed the following amount of times:\n        * 0 --> 0\n        * 1 --> 3\n        * 2 --> 7\n        * 3 --> 0\n        * 4 --> 2\n    """"""\n\n    def __init__(self, config, is_sparse=False):\n        super(ViewsFeaturesProvider, self).__init__(config)\n        self.is_sparse = is_sparse\n        self.with_history = (\n                hasattr(self.config, \'weight_history_function\')\n                and\n                self.config.weight_history_function is not None\n        )\n        self.reset()\n\n    def observe(self, observation):\n        assert (observation is not None)\n        assert (observation.sessions() is not None)\n        for session in observation.sessions():\n            view = np.int16(session[\'v\'])\n            if self.with_history:\n                self.ixs.append(np.int16(self.ix))\n                self.jxs.append(view)\n                self.history.append(np.int16(session[\'t\']))\n                self.ix += 1\n            else:\n                self.views[0, view] += 1\n\n    def features(self, observation):\n        if self.with_history:\n            time = np.int16(observation.context().time())\n\n            weights = self.config.weight_history_function(\n                time - np.array(self.history)\n            )\n            weighted_views = self._views().multiply(weights[:, np.newaxis])\n            views = sparse.coo_matrix(\n                weighted_views.sum(axis=0, dtype=np.float32),\n                copy=False\n            )\n            if self.is_sparse:\n                return views\n            else:\n                return np.array(views.todense())\n        else:\n            return self._views()\n\n    def reset(self):\n        if self.with_history:\n            self.ix = 0\n            self.ixs = []\n            self.jxs = []\n            self.history = []\n        else:\n            if self.is_sparse:\n                self.views = sparse.lil_matrix(\n                    (1, self.config.num_products),\n                    dtype=np.int16\n                )\n            else:\n                self.views = np.zeros(\n                    (1, self.config.num_products),\n                    dtype=np.int16\n                )\n\n    def _views(self):\n        if self.with_history:\n            assert len(self.ixs) == len(self.jxs)\n            return sparse.coo_matrix(\n                (\n                    np.ones(len(self.ixs), dtype=np.int16),\n                    (self.ixs, self.jxs)\n                ),\n                shape=(len(self.ixs), self.config.num_products),\n                dtype=np.int16\n            )\n        else:\n            return self.views\n'"
recogym/agents/bandit_count.py,0,"b'import numpy as np\n\nfrom ..envs.configuration import Configuration\n\nfrom .abstract import Agent\n\nbandit_count_args = {\n    \'num_products\': 10,\n    \'with_ps_all\': False,\n}\n\n\nclass BanditCount(Agent):\n    """"""\n    Bandit Count\n\n    The Agent that selects an Action for the most frequently clicked Action before.\n    """"""\n\n    def __init__(self, config = Configuration(bandit_count_args)):\n        super(BanditCount, self).__init__(config)\n\n        self.pulls_a = np.zeros((self.config.num_products, self.config.num_products))\n        self.clicks_a = np.zeros((self.config.num_products, self.config.num_products))\n        self.last_product_viewed = None\n        self.ctr = (self.clicks_a + 1) / (self.pulls_a + 2)\n\n    def act(self, observation, reward, done):\n        """"""Make a recommendation""""""\n\n        self.update_lpv(observation)\n        action = self.ctr[self.last_product_viewed, :].argmax()\n\n        if self.config.with_ps_all:\n            ps_all = np.zeros(self.config.num_products)\n            ps_all[action] = 1.0\n        else:\n            ps_all = ()\n\n        return {\n            **super().act(observation, reward, done),\n            **{\n                \'a\': self.ctr[self.last_product_viewed, :].argmax(),\n                \'ps\': self.ctr[self.last_product_viewed, :][action],\n                \'ps-a\': ps_all,\n            },\n        }\n\n    def train(self, observation, action, reward, done = False):\n        """"""Train the model in an online fashion""""""\n\n        if action is not None and reward is not None:\n\n            ix = self.last_product_viewed\n            jx = action[\'a\']\n            self.update_lpv(observation)\n            self.pulls_a[ix, jx] += 1\n            self.clicks_a[ix, jx] += reward\n\n            self.ctr[ix, jx] = (\n                    (self.clicks_a[ix, jx] + 1) / (self.pulls_a[ix, jx] + 2)\n            )\n\n    def update_lpv(self, observation):\n        """"""Updates the last product viewed based on the observation""""""\n        if observation.sessions():\n            self.last_product_viewed = observation.sessions()[-1][\'v\']\n\n    def save(self, location):\n        """"""Save the state of the model to disk""""""\n\n        np.save(location + ""pulls_a.npy"", self.pulls_a)\n        np.save(location + ""clicks_a.npy"", self.clicks_a)\n\n    def load(self, location):\n        """"""Load the model state from disk""""""\n\n        self.pulls_a = np.load(location + ""pulls_a.npy"")\n        self.clicks_a = np.load(location + ""clicks_a.npy"")\n\n    def reset(self):\n        pass\n'"
recogym/agents/bandit_mf.py,4,"b'import torch\nimport numpy as np\nfrom torch import nn, optim, Tensor\n\nfrom ..envs.configuration import Configuration\n\nfrom .abstract import Agent\n\n# Default Arguments.\nbandit_mf_square_args = {\n    \'num_products\': 10,\n    \'embed_dim\': 5,\n    \'mini_batch_size\': 32,\n    \'loss_function\': nn.BCEWithLogitsLoss(),\n    \'optim_function\': optim.RMSprop,\n    \'learning_rate\': 0.01,\n    \'with_ps_all\': False,\n}\n\n\n# Model.\nclass BanditMFSquare(nn.Module, Agent):\n    def __init__(self, config = Configuration(bandit_mf_square_args)):\n        nn.Module.__init__(self)\n        Agent.__init__(self, config)\n\n        self.product_embedding = nn.Embedding(\n            self.config.num_products, self.config.embed_dim\n        )\n        self.user_embedding = nn.Embedding(\n            self.config.num_products, self.config.embed_dim\n        )\n\n        # Initializing optimizer type.\n        self.optimizer = self.config.optim_function(\n            self.parameters(), lr = self.config.learning_rate\n        )\n\n        self.last_product_viewed = None\n        self.curr_step = 0\n        self.train_data = ([], [], [])\n        self.all_products = np.arange(self.config.num_products)\n\n    def forward(self, products, users = None):\n        if users is None:\n            users = np.full(products.shape[0], self.last_product_viewed)\n\n        a = self.product_embedding(torch.LongTensor(products))\n        b = self.user_embedding(torch.LongTensor(users))\n\n        return torch.sum(a * b, dim = 1)\n\n    def get_logits(self):\n        """"""Returns vector of product recommendation logits""""""\n        return self.forward(self.all_products)\n\n    def update_lpv(self, observation):\n        """"""Updates the last product viewed based on the observation""""""\n        assert (observation is not None)\n        assert (observation.sessions() is not None)\n        if observation.sessions():\n            self.last_product_viewed = observation.sessions()[-1][\'v\']\n\n    def act(self, observation, reward, done):\n        with torch.no_grad():\n            # Update last product viewed.\n            self.update_lpv(observation)\n\n            # Get logits for all possible actions.\n            logits = self.get_logits()\n\n            # No exploration strategy, choose maximum logit.\n            action = logits.argmax().item()\n            if self.config.with_ps_all:\n                all_ps = np.zeros(self.config.num_products)\n                all_ps[action] = 1.0\n            else:\n                all_ps = ()\n\n            return {\n                **super().act(observation, reward, done),\n                **{\n                    \'a\': action,\n                    \'ps\': logits[action],\n                    \'ps-a\': all_ps,\n                },\n            }\n\n    def update_weights(self):\n        """"""Update weights of embedding matrices using mini batch of data""""""\n        if len(self.train_data[0]) != 0:\n            # Eliminate previous gradient.\n            self.optimizer.zero_grad()\n            assert len(self.train_data[0]) == len(self.train_data[1])\n            assert len(self.train_data[0]) == len(self.train_data[2])\n            lpvs, actions, rewards = self.train_data\n\n            # Calculating logit of action and last product viewed.\n            logit = self.forward(np.array(actions), np.array(lpvs))\n\n            # Converting reward into Tensor.\n            reward = Tensor(np.array(rewards))\n\n            # Calculating supervised loss.\n            loss = self.config.loss_function(logit, reward)\n            loss.backward()\n\n            # Update weight parameters.\n            self.optimizer.step()\n\n    def train(self, observation, action, reward, done = False):\n        # Update last product viewed.\n        self.update_lpv(observation)\n\n        # Increment step.\n        self.curr_step += 1\n\n        # Update weights of model once mini batch of data accumulated.\n        if self.curr_step % self.config.mini_batch_size == 0:\n            self.update_weights()\n            self.train_data = ([], [], [])\n        else:\n            if action is not None and reward is not None:\n                self.train_data[0].append(self.last_product_viewed)\n                self.train_data[1].append(action[\'a\'])\n                self.train_data[2].append(reward)\n'"
recogym/agents/bayesian_poly.py,0,"b'import numpy as np\nimport pystan\nfrom scipy.special import expit\n\nfrom recogym import Configuration\nfrom recogym.agents import (\n    AbstractFeatureProvider,\n    Model,\n    ModelBasedAgent,\n    ViewsFeaturesProvider\n)\nfrom recogym.agents.organic_count import to_categorical\n\nbayesian_poly_args = {\n    \'num_products\': 10,\n    \'random_seed\': np.random.randint(2 ** 31 - 1),\n\n    \'poly_degree\': 2,\n    \'max_iter\': 5000,\n    \'aa\': 1.,\n    \'bb\': 1.,\n    \'with_ps_all\': False,\n}\n\n\nclass BayesianModelBuilder(AbstractFeatureProvider):\n    def __init__(self, config):\n        super(BayesianModelBuilder, self).__init__(config)\n\n    def build(self):\n        class BayesianFeaturesProvider(ViewsFeaturesProvider):\n            """"""\n            """"""\n\n            def __init__(self, config):\n                super(BayesianFeaturesProvider, self).__init__(config)\n\n            def features(self, observation):\n                base_features = super().features(observation)\n                return base_features.reshape(1, self.config.num_products)\n\n        class BayesianRegressionModel(Model):\n            """"""\n            """"""\n\n            def __init__(self, config, Lambda):\n                super(BayesianRegressionModel, self).__init__(config)\n                self.Lambda = Lambda\n\n            def act(self, observation, features):\n                X = features\n                P = X.shape[1]\n                A = np.eye(P)\n                XA = np.kron(X, A)\n\n                action_proba = expit(np.matmul(XA, self.Lambda.T)).mean(1)\n                action = np.argmax(action_proba)\n                if self.config.with_ps_all:\n                    ps_all = np.zeros(self.config.num_products)\n                    ps_all[action] = 1.0\n                else:\n                    ps_all = ()\n                return {\n                    **super().act(observation, features),\n                    **{\n                        \'a\': action,\n                        \'ps\': 1.0,\n                        \'ps-a\': ps_all,\n                    },\n                }\n\n        features, actions, deltas, pss = self.train_data()\n\n        X = features\n        N = X.shape[0]\n        P = X.shape[1]\n        A = to_categorical(actions, P)\n        XA = np.array([np.kron(X[n, :], A[n, :]) for n in range(N)])\n        y = deltas  # clicks\n\n        Sigma = np.kron(self.config.aa * np.eye(P) + self.config.bb,\n                        self.config.aa * np.eye(P) + self.config.bb)\n        fit = pystan.stan(\'recogym/agents/model.stan\', data = {\n            \'N\': features.shape[0],\n            \'P\': features.shape[1],\n            \'XA\': XA,\n            \'y\': deltas,\n            \'Sigma\': Sigma\n        }, chains = 1)\n        s = fit.extract()\n        Lambda = s[\'lambda\']\n\n        return (\n            BayesianFeaturesProvider(self.config),  # Poly is a bad name ..\n            BayesianRegressionModel(self.config, Lambda)\n        )\n\n\nclass BayesianAgent(ModelBasedAgent):\n    """"""\n    Bayesian Agent.\n\n    Note: the agent utilises Stan to train a model.\n    """"""\n\n    def __init__(self, config = Configuration(bayesian_poly_args)):\n        super(BayesianAgent, self).__init__(\n            config,\n            BayesianModelBuilder(config)\n        )\n'"
recogym/agents/bayesian_poly_vb.py,0,"b'import numpy as np\nfrom scipy.special import expit\n\nfrom ..envs.configuration import Configuration\nfrom . import (\n    AbstractFeatureProvider,\n    Model,\n    ModelBasedAgent,\n    ViewsFeaturesProvider\n)\nfrom .organic_count import to_categorical\n\nbayesian_poly_args = {\n    \'num_products\': 10,\n    \'random_seed\': np.random.randint(2 ** 31 - 1),\n\n    \'poly_degree\': 2,\n    \'max_iter\': 5000,\n    \'aa\': 1.,\n    \'bb\': 1.,\n    \'with_ps_all\': False,\n}\n\nfrom scipy import rand\nfrom numpy.linalg import inv\n\n\n# Algorithm 6\n# http://www.maths.usyd.edu.au/u/jormerod/JTOpapers/Ormerod10.pdf\ndef JJ(zeta):\n    return 1. / (2. * zeta) * (1. / (1 + np.exp(-zeta)) - 0.5)\n\n\n# TODO replace explicit inv with linear solves\ndef bayesian_logistic(Psi, y, mu_beta, Sigma_beta, iter = 200):\n    zeta = rand(Psi.shape[0])\n    for _ in range(iter):\n        q_Sigma = inv(inv(Sigma_beta) + 2 * np.matmul(np.matmul(Psi.T, np.diag(JJ(zeta))), Psi))\n        q_mu = np.matmul(q_Sigma, (np.matmul(Psi.T, y - 0.5) + np.matmul(inv(Sigma_beta), mu_beta)))\n        zeta = np.sqrt(np.diag(np.matmul(np.matmul(Psi, q_Sigma + np.matmul(q_mu, q_mu.T)), Psi.T)))\n    return q_mu, q_Sigma\n\n\nfrom scipy.stats import multivariate_normal\n\n\nclass BayesianModelBuilderVB(AbstractFeatureProvider):\n    def __init__(self, config):\n        super(BayesianModelBuilderVB, self).__init__(config)\n\n    def build(self):\n        class BayesianFeaturesProviderVB(ViewsFeaturesProvider):\n            """"""\n            """"""\n\n            def __init__(self, config):\n                super(BayesianFeaturesProviderVB, self).__init__(config)\n\n            def features(self, observation):\n                base_features = super().features(observation)\n                return base_features.reshape(1, self.config.num_products)\n\n        class BayesianRegressionModelVB(Model):\n            """"""\n            """"""\n\n            def __init__(self, config, Lambda):\n                super(BayesianRegressionModelVB, self).__init__(config)\n                self.Lambda = Lambda\n\n            def act(self, observation, features):\n                X = features\n                P = X.shape[1]\n                A = np.eye(P)\n                XA = np.kron(X, A)\n\n                action_proba = expit(np.matmul(XA, self.Lambda.T)).mean(1)\n                action = np.argmax(action_proba)\n                if self.config.with_ps_all:\n                    ps_all = np.zeros(self.config.num_products)\n                    ps_all[action] = 1.0\n                else:\n                    ps_all = ()\n                return {\n                    **super().act(observation, features),\n                    **{\n                        \'a\': action,\n                        \'ps\': 1.0,\n                        \'ps-a\': ps_all,\n                    },\n                }\n\n        features, actions, deltas, pss = self.train_data()\n\n        X = features\n        N = X.shape[0]\n        P = X.shape[1]\n        A = to_categorical(actions, P)\n        XA = np.array([np.kron(X[n, :], A[n, :]) for n in range(N)])\n        y = deltas  # clicks\n\n        Sigma = np.kron(self.config.aa * np.eye(P) + self.config.bb,\n                        self.config.aa * np.eye(P) + self.config.bb)\n\n        q_mu, q_Sigma = bayesian_logistic(XA, y.reshape((N, 1)),\n                                          mu_beta = -6 * np.ones((P ** 2, 1)), Sigma_beta = Sigma)\n        Lambda = multivariate_normal.rvs(q_mu.reshape(P ** 2), q_Sigma, 1000)\n\n        # stan version of the above (seems to agree well)\n        # fit = pystan.stan(\'model.stan\', data = {\'N\': features.shape[0], \'P\': features.shape[1], \'XA\': XA, \'y\': y, \'Sigma\': Sigma}, chains = 1)\n        # s = fit.extract()\n        # Lambda = s[\'lambda\']\n\n        ### \n\n        return (\n            BayesianFeaturesProviderVB(self.config),  # Poly is a bad name ..\n            BayesianRegressionModelVB(self.config, Lambda)\n        )\n\n\nclass BayesianAgentVB(ModelBasedAgent):\n    """"""\n    Bayesian Agent.\n\n    Note: the agent utilises VB to train a model.\n    """"""\n\n    def __init__(self, config = Configuration(bayesian_poly_args)):\n        print(\'ffq\')\n        super(BayesianAgentVB, self).__init__(\n            config,\n            BayesianModelBuilderVB(config)\n        )\n'"
recogym/agents/epsilon_greedy.py,0,"b""from numpy.random.mtrand import RandomState\nimport numpy as np\n\nfrom .abstract import Agent\n\nepsilon_greedy_args = {\n    'epsilon': 0.01,\n    'random_seed': np.random.randint(2 ** 31 - 1),\n\n    # Select an Action that is ABSOLUTELY different to the Action\n    # that would have been selected in case when Epsilon-Greedy Policy Selection\n    # had not been applied.\n    'epsilon_pure_new': True,\n\n    # Try to select the worse case in epsilon-case.\n    'epsilon_select_worse': False,\n    'with_ps_all': False,\n}\n\n\nclass EpsilonGreedy(Agent):\n    def __init__(self, config, agent):\n        super(EpsilonGreedy, self).__init__(config)\n        self.agent = agent\n        self.rng = RandomState(self.config.random_seed)\n\n    def train(self, observation, action, reward, done = False):\n        self.agent.train(observation, action, reward, done)\n\n    def act(self, observation, reward, done):\n        greedy_action = self.agent.act(observation, reward, done)\n\n        if self.rng.choice([True, False], p = [self.config.epsilon, 1.0 - self.config.epsilon]):\n            if self.config.epsilon_select_worse:\n                product_probas = greedy_action['ps-a']\n                product_probas = (1.0 - product_probas)  # Inversion of probabilities.\n            else:\n                product_probas = np.ones(self.config.num_products)\n\n            if self.config.epsilon_pure_new:\n                product_probas[greedy_action['a']] = 0.0\n            product_probas = product_probas / np.sum(product_probas)\n            epsilon_action = self.rng.choice(\n                self.config.num_products,\n                p = product_probas\n            )\n            return {\n                **super().act(observation, reward, done),\n                **{\n                    'a': epsilon_action,\n                    'ps': self.config.epsilon * product_probas[epsilon_action],\n                    'ps-a': (\n                            self.config.epsilon * product_probas\n                            if self.config.with_ps_all else\n                            ()\n                    ),\n                    'greedy': False,\n                    'h0': greedy_action['a']\n                }\n            }\n        else:\n            return {\n                **greedy_action,\n                'greedy': True,\n                'ps': (1.0 - self.config.epsilon) * greedy_action['ps'],\n                'ps-a': (\n                    (1.0 - self.config.epsilon) * greedy_action['ps-a']\n                    if self.config.with_ps_all else\n                    ()\n                ),\n            }\n\n    def reset(self):\n        self.agent.reset()\n"""
recogym/agents/logreg_ips.py,0,"b'from numpy.random.mtrand import RandomState\nfrom sklearn.linear_model import LogisticRegression\n\nfrom .abstract import *\nfrom ..envs.configuration import Configuration\n\nlogreg_multiclass_ips_args = {\n    \'num_products\': 10,\n    \'number_of_flips\': 1,\n    \'random_seed\': np.random.randint(2 ** 31 - 1),\n\n    # Select a Product randomly with the the probability predicted by Multi-Class Logistic Regression.\n    \'select_randomly\': False,\n\n    \'poly_degree\': 2,\n    \'solver\': \'lbfgs\',\n    \'max_iter\': 5000,\n    \'with_ps_all\': False,\n}\n\n\nclass LogregMulticlassIpsModelBuilder(AbstractFeatureProvider):\n    """"""\n    Logistic Regression Multiclass Model Builder\n\n    The class that provides both:\n    * Logistic Regression Model\n    * Feature Provider that builds a Feature Set suitable for the Logistic Regression Model\n    """"""\n\n    def __init__(self, config):\n        super(LogregMulticlassIpsModelBuilder, self).__init__(config)\n        if config.select_randomly:\n            self.rng = RandomState(self.config.random_seed)\n\n    def build(self):\n        class LogregMulticlassViewsFeaturesProvider(ViewsFeaturesProvider):\n            """"""\n            Logistic Regression Multiclass Feature Provider\n            """"""\n\n            def __init__(self, config):\n                super(LogregMulticlassViewsFeaturesProvider, self).__init__(config, is_sparse=True)\n\n            def features(self, observation):\n                base_features = super().features(observation)\n                return base_features.reshape(1, self.config.num_products)\n\n        class LogregMulticlassModel(Model):\n            """"""\n            Logistic Regression Multiclass Model\n            """"""\n\n            def __init__(self, config, logreg):\n                super(LogregMulticlassModel, self).__init__(config)\n                self.logreg = logreg\n                if config.select_randomly:\n                    self.rng = RandomState(self.config.random_seed)\n\n            def act(self, observation, features):\n                if self.config.select_randomly:\n                    action_proba = self.logreg.predict_proba(features)[0, :]\n                    action = self.rng.choice(\n                        self.config.num_products,\n                        p=action_proba\n                    )\n                    ps = action_proba[action]\n                    if self.config.with_ps_all:\n                        all_ps = action_proba\n                    else:\n                        all_ps = ()\n                else:\n                    action = self.logreg.predict(features).item()\n                    ps = 1.0\n                    if self.config.with_ps_all:\n                        all_ps = np.zeros(self.config.num_products)\n                        all_ps[action] = 1.0\n                    else:\n                        all_ps = ()\n                return {\n                    **super().act(observation, features),\n                    **{\n                        \'a\': action,\n                        \'ps\': ps,\n                        \'ps-a\': all_ps,\n                    },\n                }\n\n        features, actions, deltas, pss = self.train_data()\n        weights = deltas / pss\n\n        logreg = LogisticRegression(\n            solver=self.config.solver,\n            max_iter=self.config.max_iter,\n            multi_class=\'multinomial\',\n            random_state=self.config.random_seed\n        )\n\n        lr = logreg.fit(features, actions, weights)\n\n        return (\n            LogregMulticlassViewsFeaturesProvider(self.config),\n            LogregMulticlassModel(self.config, lr)\n        )\n\n\nclass LogregMulticlassIpsAgent(ModelBasedAgent):\n    """"""\n    Logistic Regression Multiclass Agent (IPS version)\n    """"""\n\n    def __init__(self, config=Configuration(logreg_multiclass_ips_args)):\n        super(LogregMulticlassIpsAgent, self).__init__(\n            config,\n            LogregMulticlassIpsModelBuilder(config)\n        )\n'"
recogym/agents/logreg_poly.py,0,"b'import numpy as np\nfrom numba import njit\nfrom scipy import sparse\nfrom sklearn.linear_model import LogisticRegression\n\nfrom ..envs.configuration import Configuration\nfrom .abstract import (AbstractFeatureProvider, Model, ModelBasedAgent, ViewsFeaturesProvider)\n\nlogreg_poly_args = {\n    \'num_products\': 10,\n    \'random_seed\': np.random.randint(2 ** 31 - 1),\n\n    \'poly_degree\': 2,\n\n    \'with_ips\': False,\n    # Should deltas (rewards) be used to calculate weights?\n    # If delta should not be used as a IPS numerator, than `1.0\' is used.\n    \'ips_numerator_is_delta\': False,\n    # Should clipping be used to calculate Inverse Propensity Score?\n    \'ips_with_clipping\': False,\n    # Clipping value that limits the value of Inverse Propensity Score.\n    \'ips_clipping_value\': 10,\n\n    \'solver\': \'lbfgs\',\n    \'max_iter\': 5000,\n    \'with_ps_all\': False,\n}\n\n\n@njit(nogil = True, cache = True)\ndef _fast_kron_cols(data, indptr, indices, actions, num_products):\n    if indptr.shape[0] - 1 == actions.shape[0]:\n        col_size = data.shape[0]\n        kron_data = data\n        with_sliding_ix = True\n    else:\n        assert indptr.shape[0] - 1 == 1\n        col_size = data.shape[0] * actions.shape[0]\n        kron_data = np.kron(data, np.ones(num_products, dtype = np.int16))\n        with_sliding_ix = False\n\n    kron_rows = np.zeros(col_size)\n    kron_cols = np.zeros(col_size)\n    for ix, action in enumerate(actions):\n        if with_sliding_ix:\n            six = indptr[ix]\n            skix = six\n            eix = indptr[ix + 1]\n            ekix = eix\n        else:\n            six = indptr[0]\n            eix = indptr[1]\n            delta = eix - six\n            skix = ix * delta + six\n            ekix = ix * delta + eix\n\n        cols = indices[six:eix]\n        kron_rows[skix:ekix] = ix\n        kron_cols[skix:ekix] += cols + action * num_products\n\n    return kron_data, kron_rows, kron_cols\n\n\nclass SparsePolynomialFeatures:\n    def __init__(self, config: Configuration):\n        self.config = config\n\n    def transform(\n            self,\n            features: sparse.csr_matrix,\n            actions: np.ndarray\n    ) -> sparse.csr_matrix:\n        kron_data, kron_rows, kron_cols = _fast_kron_cols(\n            features.data,\n            features.indptr,\n            features.indices,\n            actions,\n            self.config.num_products\n        )\n        assert kron_data.shape[0] == kron_rows.shape[0]\n        assert kron_data.shape[0] == kron_cols.shape[0]\n        kron = sparse.coo_matrix(\n            (\n                kron_data,\n                (kron_rows, kron_cols)\n            ),\n            (actions.shape[0], self.config.num_products * self.config.num_products)\n        )\n\n        if features.shape[0] != actions.shape[0]:\n            features_data = np.tile(features.data, actions.shape[0])\n            features_cols = np.tile(features.indices, actions.shape[0])\n            feature_rows = np.repeat(np.arange(actions.shape[0]), features.nnz)\n            features = sparse.coo_matrix(\n                (\n                    features_data,\n                    (feature_rows, features_cols)\n                ),\n                (actions.shape[0], self.config.num_products),\n                dtype = np.int16\n            )\n\n        actions = sparse.coo_matrix(\n            (\n                actions,\n                (range(actions.shape[0]), actions)\n            ),\n            (actions.shape[0], self.config.num_products),\n            dtype = np.int16\n        )\n\n        assert features.shape[0] == actions.shape[0]\n        assert features.shape[0] == kron.shape[0]\n        assert features.shape[1] == self.config.num_products\n        assert actions.shape[1] == self.config.num_products\n\n        return sparse.hstack(\n            (features, actions, kron)\n        )\n\n\nclass LogregPolyModelBuilder(AbstractFeatureProvider):\n    def __init__(self, config, is_sparse=False):\n        super(LogregPolyModelBuilder, self).__init__(config, is_sparse=is_sparse)\n\n    def build(self):\n        class LogisticRegressionPolyFeaturesProvider(ViewsFeaturesProvider):\n            """"""\n            Logistic Regression Polynomial Feature Provider.\n            """"""\n\n            def __init__(self, config, poly):\n                super(LogisticRegressionPolyFeaturesProvider, self).__init__(config, is_sparse=True)\n                self.poly = poly\n                self.all_actions = np.arange(self.config.num_products)\n\n            def features(self, observation):\n                return self.poly.transform(\n                    super().features(observation).tocsr(),\n                    self.all_actions\n                )\n\n        class LogisticRegressionModel(Model):\n            """"""\n            Logistic Regression Model\n            """"""\n\n            def __init__(self, config, logreg):\n                super(LogisticRegressionModel, self).__init__(config)\n                self.logreg = logreg\n\n            def act(self, observation, features):\n                action_proba = self.logreg.predict_proba(features)[:, 1]\n                action = np.argmax(action_proba)\n                if self.config.with_ps_all:\n                    ps_all = np.zeros(self.config.num_products)\n                    ps_all[action] = 1.0\n                else:\n                    ps_all = ()\n                return {\n                    **super().act(observation, features),\n                    **{\n                        \'a\': action,\n                        \'ps\': 1.0,\n                        \'ps-a\': ps_all,\n                    },\n                }\n\n        features, actions, deltas, pss = self.train_data()\n\n        logreg = LogisticRegression(\n            solver = self.config.solver,\n            max_iter = self.config.max_iter,\n            random_state = self.config.random_seed,\n            n_jobs = -1\n        )\n\n        poly = SparsePolynomialFeatures(self.config)\n        features_poly = poly.transform(features, actions)\n\n        if self.config.with_ips:\n            ips_numerator = deltas if self.config.ips_numerator_is_delta else 1.0\n            weights = ips_numerator / pss\n            if self.config.ips_with_clipping:\n                weights = np.minimum(deltas / pss, self.config.ips_clipping_value)\n            lr = logreg.fit(features_poly, deltas, weights)\n        else:\n            lr = logreg.fit(features_poly, deltas)\n        print(\'Model was built!\')\n\n        return (\n            LogisticRegressionPolyFeaturesProvider(self.config, poly),\n            LogisticRegressionModel(self.config, lr)\n        )\n\n\nclass LogregPolyAgent(ModelBasedAgent):\n    """"""\n    Logistic Regression Polynomial Agent\n    """"""\n\n    def __init__(self, config = Configuration(logreg_poly_args)):\n        super(LogregPolyAgent, self).__init__(\n            config,\n            LogregPolyModelBuilder(config, is_sparse=True)\n        )\n'"
recogym/agents/nn_ips.py,7,"b'import numpy as np\nimport torch\nimport torch.optim as optim\n\nfrom numpy.random.mtrand import RandomState\nfrom torch import nn\n\nfrom .abstract import AbstractFeatureProvider, ViewsFeaturesProvider, Model, ModelBasedAgent\nfrom ..envs.configuration import Configuration\n\nnn_ips_args = {\n    \'num_products\': 10,\n    \'number_of_flips\': 1,\n    \'random_seed\': np.random.randint(2 ** 31 - 1),\n\n    # Select a Product randomly with the the probability predicted by Neural Network with IPS.\n    \'select_randomly\': False,\n\n    \'M\': 111,\n    \'learning_rate\': 0.01,\n    \'num_epochs\': 100,\n    \'num_hidden\': 20,\n    \'lambda_val\': 0.01,\n    \'with_ps_all\': False,\n}\n\n\nclass IpsLoss(nn.Module):\n    """"""\n    IPS Loss Function\n    """"""\n\n    def __init__(self, config):\n        super(IpsLoss, self).__init__()\n        self.config = config\n        self.clipping = nn.Threshold(self.config.M, self.config.M)\n\n    def forward(self, hx, h0, deltas):\n        u = self.clipping(hx / h0) * deltas\n        return torch.mean(u) + self.config.lambda_val * torch.sqrt(torch.var(u) / deltas.shape[0])\n\n\nclass NeuralNet(nn.Module):\n    """"""\n    Neural Network Model\n\n    This class implements a Neural Net model by using PyTorch.\n    """"""\n\n    def __init__(self, config):\n        super(NeuralNet, self).__init__()\n        self.config = config\n        self.model = nn.Sequential(\n            nn.Linear(self.config.num_products, self.config.num_hidden),\n            nn.Sigmoid(),\n            nn.Linear(self.config.num_hidden, self.config.num_hidden),\n            nn.Sigmoid(),\n            nn.Linear(self.config.num_hidden, self.config.num_products),\n            nn.Softmax(dim = 1)\n        )\n\n    def forward(self, features):\n        return self.model.forward(features)\n\n\nclass NnIpsModelBuilder(AbstractFeatureProvider):\n    """"""\n    Neural Net Inverse Propensity Score Model Builder\n    """"""\n\n    def __init__(self, config):\n        super(NnIpsModelBuilder, self).__init__(config)\n\n    def build(self):\n        model = NeuralNet(self.config)\n        criterion = IpsLoss(self.config)\n        optimizer = optim.SGD(model.parameters(), lr = self.config.learning_rate)\n\n        features, actions, deltas, pss = self.train_data()\n\n        deltas = deltas[:, np.newaxis] * np.ones((1, self.config.num_products))\n        pss = pss[:, np.newaxis] * np.ones((1, self.config.num_products))\n\n        for epoch in range(self.config.num_epochs):\n            optimizer.zero_grad()\n\n            loss = criterion(\n                model(torch.Tensor(features)),\n                torch.Tensor(pss),\n                torch.Tensor(-1.0 * deltas)\n            )\n            loss.backward()\n\n            optimizer.step()\n\n        class TorchFeatureProvider(ViewsFeaturesProvider):\n            def __init__(self, config):\n                super(TorchFeatureProvider, self).__init__(config, is_sparse=True)\n\n            def features(self, observation):\n                base_features = super().features(observation).reshape(1, self.config.num_products)\n                return torch.Tensor(base_features)\n\n        class TorchModel(Model):\n            def __init__(self, config, model):\n                super(TorchModel, self).__init__(config)\n                self.model = model\n                if self.config.select_randomly:\n                    self.rng = RandomState(self.config.random_seed)\n\n            def act(self, observation, features):\n                prob = self.model.forward(features)[0, :]\n                if self.config.select_randomly:\n                    prob = prob.detach().numpy()\n                    action = self.rng.choice(\n                        np.array(self.config.num_products),\n                        p = prob\n                    )\n                    if self.config.with_ps_all:\n                        ps_all = prob\n                    else:\n                        ps_all = ()\n                else:\n                    action = torch.argmax(prob).item()\n                    if self.config.with_ps_all:\n                        ps_all = np.zeros(self.config.num_products)\n                        ps_all[action] = 1.0\n                    else:\n                        ps_all = ()\n                return {\n                    **super().act(observation, features),\n                    **{\n                        \'a\': action,\n                        \'ps\': prob[action].item(),\n                        \'ps-a\': ps_all,\n                    },\n                }\n\n        return (\n            TorchFeatureProvider(self.config),\n            TorchModel(self.config, model)\n        )\n\n\nclass NnIpsAgent(ModelBasedAgent):\n    """"""\n    Neural Network Agent with IPS\n    """"""\n\n    def __init__(self, config = Configuration(nn_ips_args)):\n        super(NnIpsAgent, self).__init__(\n            config,\n            NnIpsModelBuilder(config)\n        )\n'"
recogym/agents/organic_count.py,0,"b'import numpy as np\n\nfrom .abstract import Agent\nfrom ..envs.configuration import Configuration\n\norganic_count_args = {\n    \'num_products\': 10,\n    \'with_ps_all\': False,\n}\n\n\n# From Keras.\ndef to_categorical(y, num_classes = None, dtype = \'float32\'):\n    """"""Converts a class vector (integers) to binary class matrix.\n    E.g. for use with categorical_crossentropy.\n    # Arguments\n        y: class vector to be converted into a matrix\n            (integers from 0 to num_classes).\n        num_classes: total number of classes.\n        dtype: The data type expected by the input, as a string\n            (`float32`, `float64`, `int32`...)\n    # Returns\n        A binary matrix representation of the input. The classes axis\n        is placed last.\n    """"""\n    y = np.array(y, dtype = \'int\')\n    input_shape = y.shape\n    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n        input_shape = tuple(input_shape[:-1])\n    y = y.ravel()\n    if not num_classes:\n        num_classes = np.max(y) + 1\n    n = y.shape[0]\n    categorical = np.zeros((n, num_classes), dtype = dtype)\n    categorical[np.arange(n), y] = 1\n    output_shape = input_shape + (num_classes,)\n    categorical = np.reshape(categorical, output_shape)\n    return categorical\n\n\nclass OrganicCount(Agent):\n    """"""\n    Organic Count\n\n    The Agent that selects an Action based on the most frequently viewed Product.\n    """"""\n\n    def __init__(self, config = Configuration(organic_count_args)):\n        super(OrganicCount, self).__init__(config)\n\n        self.co_counts = np.zeros((self.config.num_products, self.config.num_products))\n        self.corr = None\n\n    def act(self, observation, reward, done):\n        """"""Make a recommendation""""""\n\n        self.update_lpv(observation)\n\n        action = self.co_counts[self.last_product_viewed, :].argmax()\n        if self.config.with_ps_all:\n            ps_all = np.zeros(self.config.num_products)\n            ps_all[action] = 1.0\n        else:\n            ps_all = ()\n        return {\n            **super().act(observation, reward, done),\n            **{\n                \'a\': self.co_counts[self.last_product_viewed, :].argmax(),\n                \'ps\': 1.0,\n                \'ps-a\': ps_all,\n            },\n        }\n\n    def train(self, observation, action, reward, done = False):\n        """"""Train the model in an online fashion""""""\n        if observation.sessions():\n            A = to_categorical(\n                [session[\'v\'] for session in observation.sessions()],\n                self.config.num_products\n            )\n            B = A.sum(0).reshape((self.config.num_products, 1))\n            self.co_counts = self.co_counts + np.matmul(B, B.T)\n\n    def update_lpv(self, observation):\n        """"""updates the last product viewed based on the observation""""""\n        if observation.sessions():\n            self.last_product_viewed = observation.sessions()[-1][\'v\']\n'"
recogym/agents/organic_mf.py,9,"b'import numpy as np\nimport torch\n\nfrom ..envs.configuration import Configuration\nfrom .abstract import Agent\n\n# Default Arguments ----------------------------------------------------------\norganic_mf_square_args = {\n    \'num_products\': 10,\n    \'embed_dim\': 5,\n    \'mini_batch_size\': 32,\n    \'loss_function\': torch.nn.CrossEntropyLoss(),\n    \'optim_function\': torch.optim.RMSprop,\n    \'learning_rate\': 0.01,\n    \'with_ps_all\': False,\n}\n\n\n# Model ----------------------------------------------------------------------\nclass OrganicMFSquare(torch.nn.Module, Agent):\n    """"""\n    Organic Matrix Factorisation (Square)\n\n    The Agent that selects an Action from the model that performs\n     Organic Events matrix factorisation.\n    """"""\n\n    def __init__(self, config = Configuration(organic_mf_square_args)):\n        torch.nn.Module.__init__(self)\n        Agent.__init__(self, config)\n\n        self.product_embedding = torch.nn.Embedding(\n            self.config.num_products, self.config.embed_dim\n        )\n        self.output_layer = torch.nn.Linear(\n            self.config.embed_dim, self.config.num_products\n        )\n\n        # Initializing optimizer type.\n        self.optimizer = self.config.optim_function(\n            self.parameters(), lr = self.config.learning_rate\n        )\n\n        self.last_product_viewed = None\n        self.curr_step = 0\n        self.train_data = []\n        self.action = None\n\n    def forward(self, product):\n\n        product = torch.Tensor([product])\n\n        a = self.product_embedding(product.long())\n        b = self.output_layer(a)\n\n        return b\n\n    def act(self, observation, reward, done):\n        with torch.no_grad():\n            if observation is not None and len(observation.current_sessions) > 0:\n                logits = self.forward(observation.current_sessions[-1][\'v\'])\n\n                # No exploration strategy, choose maximum logit.\n                self.action = logits.argmax().item()\n\n            if self.config.with_ps_all:\n                all_ps = np.zeros(self.config.num_products)\n                all_ps[self.action] = 1.0\n            else:\n                all_ps = ()\n            return {\n                **super().act(observation, reward, done),\n                **{\n                    \'a\': self.action,\n                    \'ps\': 1.0,\n                    \'ps-a\': all_ps,\n                }\n            }\n\n    def update_weights(self):\n        """"""Update weights of embedding matrices using mini batch of data""""""\n        # Eliminate previous gradient.\n        self.optimizer.zero_grad()\n\n        for prods in self.train_data:\n            # Calculating logit of action and last product viewed.\n\n            # Loop over the number of products.\n            for i in range(len(prods) - 1):\n\n                logit = self.forward(prods[i][\'v\'])\n\n                # Converting label into Tensor.\n                label = torch.LongTensor([prods[i + 1][\'v\']])\n\n                # Calculating supervised loss.\n                loss = self.config.loss_function(logit, label)\n                loss.backward()\n\n        # Update weight parameters.\n        self.optimizer.step()\n\n    def train(self, observation, action, reward, done = False):\n        """"""Method to deal with the """"""\n\n        # Increment step.\n        self.curr_step += 1\n\n        # Update weights of model once mini batch of data accumulated.\n        if self.curr_step % self.config.mini_batch_size == 0:\n            self.update_weights()\n            self.train_data = []\n        else:\n            if observation is not None:\n                data = observation.current_sessions\n                self.train_data.append(data)\n'"
recogym/agents/organic_user_count.py,0,"b'import numpy as np\nfrom numpy.random.mtrand import RandomState\n\nfrom .abstract import AbstractFeatureProvider, ViewsFeaturesProvider, Model, ModelBasedAgent\nfrom ..envs.configuration import Configuration\n\norganic_user_count_args = {\n    \'num_products\': 10,\n    \'random_seed\': np.random.randint(2 ** 31 - 1),\n\n    # Select a Product randomly with the highest probability for the most frequently viewed product.\n    \'select_randomly\': True,\n\n    \'epsilon\': .0,\n    \'exploit_explore\': True,\n    \'reverse_pop\': False,\n\n    # Weight History Function: how treat each event back in time.\n    \'weight_history_function\': None,\n    \'with_ps_all\': False,\n\n    # reverse popularity.\n    \'reverse_pop\': False,\n\n    # Epsilon to add to user state - if none-zero, this ensures the policy has support over all products\n    \'epsilon\': 0.\n}\n\nclass OrganicUserEventCounterModelBuilder(AbstractFeatureProvider):\n    def __init__(self, config):\n        super(OrganicUserEventCounterModelBuilder, self).__init__(config)\n\n    def build(self):\n\n        class OrganicUserEventCounterModel(Model):\n            """"""\n            Organic Event Count Model (per a User).\n            """"""\n\n            def __init__(self, config):\n                super(OrganicUserEventCounterModel, self).__init__(config)\n                self.rng = RandomState(self.config.random_seed)\n\n\n            def act(self, observation, features):\n                features = features.flatten()\n                if self.config.exploit_explore:\n                    is_explore_case = self.rng.choice(\n                        [True, False],\n                        p=[self.config.epsilon, 1 - self.config.epsilon]\n                    )\n                    if is_explore_case:\n                        mask = features == 0\n                        features[mask] = 1\n                        features[~mask] = 0\n                    action_proba = features / np.sum(features)\n                else:\n                    features = self.config.epsilon + features\n                    action_proba = features / np.sum(features)\n\n                    if self.config.reverse_pop:\n                        action_proba = 1 - action_proba\n                        action_proba = action_proba / np.sum(action_proba)\n\n                if self.config.select_randomly:\n                    action = self.rng.choice(self.config.num_products, p=action_proba)\n                    if self.config.exploit_explore:\n                        ps = (\n                                (\n                                    self.config.epsilon\n                                    if is_explore_case else\n                                    1 - self.config.epsilon\n                                ) * action_proba[action]\n                        )\n                    else:\n                        ps = action_proba[action]\n                    if self.config.with_ps_all:\n                        ps_all = action_proba\n                    else:\n                        ps_all = ()\n                else:\n                    action = np.argmax(action_proba)\n                    ps = 1.0\n                    if self.config.with_ps_all:\n                        ps_all = np.zeros(self.config.num_products)\n                        ps_all[action] = 1.0\n                    else:\n                        ps_all = ()\n                return {\n                    **super().act(observation, features),\n                    **{\n                        \'a\': action,\n                        \'ps\': ps,\n                        \'ps-a\': ps_all,\n                    },\n                }\n\n        return (\n            ViewsFeaturesProvider(self.config, is_sparse=False),\n            OrganicUserEventCounterModel(self.config)\n        )\n\n\nclass OrganicUserEventCounterAgent(ModelBasedAgent):\n    """"""\n    Organic Event Counter Agent\n\n    The Agent that counts Organic views of Products (per a User)\n    and selects an Action for the most frequently shown Product.\n    """"""\n\n    def __init__(self, config=Configuration(organic_user_count_args)):\n        super(OrganicUserEventCounterAgent, self).__init__(\n            config,\n            OrganicUserEventCounterModelBuilder(config)\n        )'"
recogym/agents/pytorch_mlr.py,45,"b'import numpy as np\n#import tensorflow as tf\nimport torch\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom scipy.special import expit\n\nfrom recogym.agents import (\n    AbstractFeatureProvider,\n    Model,\n    ModelBasedAgent,\n    ViewsFeaturesProvider\n)\nfrom ..envs.configuration import Configuration\n\npytorch_mlr_args = {\n    \'n_epochs\': 30,\n    \'learning_rate\': 0.01,\n    \'random_seed\': np.random.randint(2 ** 31 - 1),\n    \'logIPS\': False,\n    \'variance_penalisation_strength\': .0,\n    \'clip_weights\': False,\n    \'alpha\': .0,\n    \'ll_IPS\': True\n}\n\nfrom numba import jit\nfrom tqdm import tqdm\n\n@jit(nopython=True)\ndef act_linear_model(X, W, P):\n    return np.argmax(X.dot(W.T).reshape(P))\n\nclass PyTorchMLRModelBuilder(AbstractFeatureProvider):\n    def __init__(self, config):\n        super(PyTorchMLRModelBuilder, self).__init__(config)\n\n    def build(self):\n        class PyTorchMLRFeaturesProvider(ViewsFeaturesProvider):\n            """"""\n            """"""\n\n            def __init__(self, config):\n                super(PyTorchMLRFeaturesProvider, self).__init__(config)\n\n            def features(self, observation):\n                base_features = super().features(observation)\n                return base_features.reshape(1, self.config.num_products)\n\n        class PyTorchMLRModel(Model):\n            """"""\n            """"""\n\n            def __init__(self, config, model):\n                super(PyTorchMLRModel, self).__init__(config)\n                self.model = model\n                self.W = model.weight.detach().numpy()\n\n            def act(self, observation, features):\n                # OWN CODE\n                #X = features\n                #P = X.shape[1]\n                #X = Variable(torch.Tensor(X))\n                #action_probs = self.model(X).detach().numpy().ravel()\n                #action = np.argmax(action_probs)\n                #ps_all = np.zeros(P)\n                #ps_all[action] = 1.0\n                #/OWN CODE\n                # NUMBA CODE\n                P = features.shape[1]\n                X = features.astype(np.float32)\n                action = act_linear_model(X,self.W,P)\n                ps_all = np.zeros(P)\n                ps_all[action] = 1.0\n                #/NUMBA CODE\n\n                return {\n                    **super().act(observation, features),\n                    **{\n                        \'a\': action,\n                        \'ps\': 1.0,\n                        \'ps-a\': ps_all,\n                    },\n                }\n\n        class MultinomialLogisticRegressionModel(torch.nn.Module):\n            def __init__(self, input_dim, output_dim):\n                super(MultinomialLogisticRegressionModel, self).__init__()\n                # Generate weights - initialise randomly\n                self.weight = torch.nn.Parameter(torch.Tensor(output_dim, input_dim))\n                torch.nn.init.kaiming_uniform_(self.weight, a = np.sqrt(5))\n                \n                # Experimental bias \n                self.bias = torch.nn.Parameter(torch.Tensor(1))\n                fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n                bound = 1 / np.sqrt(fan_in)\n                torch.nn.init.uniform_(self.bias, -bound, bound)\n\n            def forward(self, x):\n                # Compute linear transformation x.A.T\n                pred = F.linear(x, self.weight)\n                return pred\n        \n        class BinomialLogisticRegressionModel(torch.nn.Module):\n            def __init__(self, multinomial_model):\n                super(BinomialLogisticRegressionModel, self).__init__()\n                # Weights learned through the multinomial model\n                self.weight = multinomial_model.weight\n                self.bias = multinomial_model.bias\n\n            def pairwise_dot(self, x, a):\n                # Not just .matmul - compute pairwise dotproduct for action/context pairs\n                return (x*a).sum(dim=1)\n\n            def forward(self, x, a):\n                # Compute linear transformation x.A.T\n                return torch.sigmoid(self.pairwise_dot(x, self.weight[a,:]) + self.bias) # Bias is experimental TODO\n\n        # Get data\n        features, actions, deltas, pss = self.train_data()\n\n        # Extract data properly\n        X = features\n        N = X.shape[0]\n        P = X.shape[1]\n        A = actions\n        y = deltas\n\n        # Generate model \n        multinomial_model = MultinomialLogisticRegressionModel(P, P)\n        binomial_model = BinomialLogisticRegressionModel(multinomial_model)\n\n        # Compute clipping constant as ratio between 90th and 10th percentile\n        # As recommended in POEM\n        M = np.inf\n        if type(self.config.clip_weights) != bool:\n            M = self.config.clip_weights\n        elif self.config.clip_weights:\n            M = np.percentile(pss[y != 0], 90) / np.percentile(pss[y != 0], 10)\n        if self.config.clip_weights:\n            print(\'Clipping with constant M:\\t{0}\'.format(M))\n\n        # Convert data to torch objects - only clicks for learning multinomial model\n        X1 = Variable(torch.Tensor(X[y != 0]))\n        A1 = Variable(torch.LongTensor(A[y != 0]))\n        w1 = torch.Tensor(pss[y != 0] ** -1)\n        N1 = X1.shape[0]\n\n        # Convert data to torch objects - all samples for learning binomial model\n        X = Variable(torch.Tensor(X))\n        A = Variable(torch.LongTensor(A))\n        w = torch.Tensor(pss ** -1)\n        y = Variable(torch.Tensor(y))\n\n        # Binary cross-entropy as objective for the binomial model\n        binary_cross_entropy = torch.nn.BCELoss(reduction = \'none\')\n        \n        # LBFGS for optimisation\n        optimiser = FullBatchLBFGS(multinomial_model.parameters())\n\n        def closure():\n            # Reset gradients\n            optimiser.zero_grad()\n\n            # Check whether we\'re using the multinomial model\n            if self.config.alpha < 1.0:\n                # Compute action predictions for clicks\n                p_a = multinomial_model(X1)\n                # Turn these into probabilities through softmax\n                p_a = F.softmax(p_a, dim = 1)\n                # Only keep probabilities for the actions that were taken\n                p_a = torch.gather(p_a, 1, A1.unsqueeze(1))\n                p_a = p_a.reshape(-1) # FIXME HOTFIX BY MARTIN\n                # (Clipped) IPS Estimate of the reward\n                reward = torch.clamp(p_a * w1, max = M)\n                # Logged version\n                log_reward = torch.log(torch.clamp(p_a, min = 1e-10)) * w1\n\n                # Compute the estimated reward\n                #reward = torch.clamp(torch.gather(F.softmax(prob_a, dim = 1), 1, A1.unsqueeze(1)) * w1, max = M)\n                # Log if necessary\n                #if self.config.logIPS:\n                #    reward = torch.log(torch.clamp(reward, min = 1e-10))\n\n                # PyTorch likes to minimise - loss instead of reward\n                loss = -reward\n                log_loss = -log_reward\n\n                # If the variance regularisation strength is larger than zero\n                if self.config.variance_penalisation_strength:\n                    # Compute the expectation of the IPS estimate\n                    avg_weighted_loss = torch.mean(loss)\n                    # Compute the variance of the IPS estimate\n                    var = torch.sqrt(torch.sum((loss - avg_weighted_loss)**2) / (N1 - 1) / N1)\n                    # Reweight with lambda and add to the loss\n                    if self.config.logIPS:\n                        loss = log_loss.mean() + self.config.variance_penalisation_strength * var\n                    else:\n                        loss = loss.mean() + self.config.variance_penalisation_strength * var\n                else:\n                    # Compute the mean over all samples as the final loss\n                    if self.config.logIPS:\n                        loss = log_loss.mean()\n                    else:\n                        loss = loss.mean()\n            \n            # Check whether we\'re using the binomial model\n            if .0 < self.config.alpha:\n                # Let it generate predictions\n                prob_c = binomial_model(X, A)\n                # Negative log-likelihood as loss here - TODO check whether IPS reweighting here makes sense\n                nll = binary_cross_entropy(prob_c, y)\n                if self.config.ll_IPS:\n                    nll = nll * w\n                nll = nll.mean() #* rescaling_factor\n            \n            # A bit ugly but most efficient - check explicitly for loss combination\n            if self.config.alpha == .0:\n                pass\n            elif self.config.alpha == 1.0:\n                loss = nll\n            else:\n                loss = (1.0 - self.config.alpha) * loss + self.config.alpha * nll\n\n            return loss\n\n        # Initial loss\n        last_obj = closure()\n        max_epoch = 200\n        tol = 1e-10 # Magic number \n        max_patience, patience = 20, 0\n        checkpoints = []\n        #for epoch in tqdm(range(self.config.n_epochs)):\n        for epoch in tqdm(range(max_epoch)):\n            # Optimisation step\n            obj, _, _, _, _, _, _, _ = optimiser.step({\'closure\': closure, \'current_loss\': last_obj, \'max_ls\': 20})\n            # Check for convergence\n            #if (last_obj - obj) < tol and epoch > 10:\n            #    patience += 1\n            #    if patience >= max_patience:\n            #        print(\'Converged after {0} iterations. Final loss: {1}\'.format(epoch, obj))\n            #        break\n            #    else:\n            #        #print(\'Remaining patient at epoch {0}...\'.format(epoch))\n            #        pass\n            # Save last loss\n            last_obj = obj\n            if (epoch % 25) == 0:\n                checkpoints.append(last_obj)\n\n\n        #if epoch == (max_epoch - 1):\n        #    print(\'Not converged after {0} iterations. Final loss: {1}\'.format(max_epoch, last_obj))\n        print(\'Checkpoints: {0}\\nFinal loss: {1}\'.format(checkpoints, last_obj))\n\n        return (\n            PyTorchMLRFeaturesProvider(self.config),\n            PyTorchMLRModel(self.config, multinomial_model)\n        )\n\nclass PyTorchMLRAgent(ModelBasedAgent):\n    """"""\n    PyTorch-based multinomial logistic regression Agent.\n    """"""\n\n    def __init__(self, config = Configuration(pytorch_mlr_args)):\n        super(PyTorchMLRAgent, self).__init__(\n            config,\n            PyTorchMLRModelBuilder(config)\n        )\n\n\n# GRACIOUSLY TAKEN FROM https://github.com/hjmshi/PyTorch-LBFGS\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import reduce\nfrom copy import deepcopy\nfrom torch.optim import Optimizer\n\n#%% Helper Functions for L-BFGS\n\ndef is_legal(v):\n    """"""\n    Checks that tensor is not NaN or Inf.\n    Inputs:\n        v (tensor): tensor to be checked\n    """"""\n    legal = not torch.isnan(v).any() and not torch.isinf(v)\n\n    return legal\n\ndef polyinterp(points, x_min_bound=None, x_max_bound=None, plot=False):\n    """"""\n    Gives the minimizer and minimum of the interpolating polynomial over given points\n    based on function and derivative information. Defaults to bisection if no critical\n    points are valid.\n    Based on polyinterp.m Matlab function in minFunc by Mark Schmidt with some slight\n    modifications.\n    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n    Last edited 12/6/18.\n    Inputs:\n        points (nparray): two-dimensional array with each point of form [x f g]\n        x_min_bound (float): minimum value that brackets minimum (default: minimum of points)\n        x_max_bound (float): maximum value that brackets minimum (default: maximum of points)\n        plot (bool): plot interpolating polynomial\n    Outputs:\n        x_sol (float): minimizer of interpolating polynomial\n        F_min (float): minimum of interpolating polynomial\n    Note:\n      . Set f or g to np.nan if they are unknown\n    """"""\n    no_points = points.shape[0]\n    order = np.sum(1 - np.isnan(points[:,1:3]).astype(\'int\')) - 1\n\n    x_min = np.min(points[:, 0])\n    x_max = np.max(points[:, 0])\n\n    # compute bounds of interpolation area\n    if(x_min_bound is None):\n        x_min_bound = x_min\n    if(x_max_bound is None):\n        x_max_bound = x_max\n\n    # explicit formula for quadratic interpolation\n    if no_points == 2 and order == 2 and plot is False:\n        # Solution to quadratic interpolation is given by:\n        # a = -(f1 - f2 - g1(x1 - x2))/(x1 - x2)^2\n        # x_min = x1 - g1/(2a)\n        # if x1 = 0, then is given by:\n        # x_min = - (g1*x2^2)/(2(f2 - f1 - g1*x2))\n\n        if(points[0, 0] == 0):\n            x_sol = -points[0, 2]*points[1, 0]**2/(2*(points[1, 1] - points[0, 1] - points[0, 2]*points[1, 0]))\n        else:\n            a = -(points[0, 1] - points[1, 1] - points[0, 2]*(points[0, 0] - points[1, 0]))/(points[0, 0] - points[1, 0])**2\n            x_sol = points[0, 0] - points[0, 2]/(2*a)\n\n        x_sol = np.minimum(np.maximum(x_min_bound, x_sol), x_max_bound)\n\n    # explicit formula for cubic interpolation\n    elif no_points == 2 and order == 3 and plot is False:\n        # Solution to cubic interpolation is given by:\n        # d1 = g1 + g2 - 3((f1 - f2)/(x1 - x2))\n        # d2 = sqrt(d1^2 - g1*g2)\n        # x_min = x2 - (x2 - x1)*((g2 + d2 - d1)/(g2 - g1 + 2*d2))\n        d1 = points[0, 2] + points[1, 2] - 3*((points[0, 1] - points[1, 1])/(points[0, 0] - points[1, 0]))\n        d2 = np.sqrt(d1**2 - points[0, 2]*points[1, 2])\n        if np.isreal(d2):\n            x_sol = points[1, 0] - (points[1, 0] - points[0, 0])*((points[1, 2] + d2 - d1)/(points[1, 2] - points[0, 2] + 2*d2))\n            x_sol = np.minimum(np.maximum(x_min_bound, x_sol), x_max_bound)\n        else:\n            x_sol = (x_max_bound + x_min_bound)/2\n\n    # solve linear system\n    else:\n        # define linear constraints\n        A = np.zeros((0, order+1))\n        b = np.zeros((0, 1))\n\n        # add linear constraints on function values\n        for i in range(no_points):\n            if not np.isnan(points[i, 1]):\n                constraint = np.zeros((1, order+1))\n                for j in range(order, -1, -1):\n                    constraint[0, order - j] = points[i, 0]**j\n                A = np.append(A, constraint, 0)\n                b = np.append(b, points[i, 1])\n\n        # add linear constraints on gradient values\n        for i in range(no_points):\n            if not np.isnan(points[i, 2]):\n                constraint = np.zeros((1, order+1))\n                for j in range(order):\n                    constraint[0, j] = (order-j)*points[i,0]**(order-j-1)\n                A = np.append(A, constraint, 0)\n                b = np.append(b, points[i, 2])\n\n        # check if system is solvable\n        if(A.shape[0] != A.shape[1] or np.linalg.matrix_rank(A) != A.shape[0]):\n            x_sol = (x_min_bound + x_max_bound)/2\n            f_min = np.Inf\n        else:\n            # solve linear system for interpolating polynomial\n            coeff = np.linalg.solve(A, b)\n\n            # compute critical points\n            dcoeff = np.zeros(order)\n            for i in range(len(coeff) - 1):\n                dcoeff[i] = coeff[i]*(order-i)\n\n            crit_pts = np.array([x_min_bound, x_max_bound])\n            crit_pts = np.append(crit_pts, points[:, 0])\n\n            if not np.isinf(dcoeff).any():\n                roots = np.roots(dcoeff)\n                crit_pts = np.append(crit_pts, roots)\n\n            # test critical points\n            f_min = np.Inf\n            x_sol = (x_min_bound + x_max_bound)/2 # defaults to bisection\n            for crit_pt in crit_pts:\n                if np.isreal(crit_pt) and crit_pt >= x_min_bound and crit_pt <= x_max_bound:\n                    F_cp = np.polyval(coeff, crit_pt)\n                    if np.isreal(F_cp) and F_cp < f_min:\n                        x_sol = np.real(crit_pt)\n                        f_min = np.real(F_cp)\n\n            if(plot):\n                plt.figure()\n                x = np.arange(x_min_bound, x_max_bound, (x_max_bound - x_min_bound)/10000)\n                f = np.polyval(coeff, x)\n                plt.plot(x, f)\n                plt.plot(x_sol, f_min, \'x\')\n\n    return x_sol\n\n#%% L-BFGS Optimizer\n\nclass LBFGS(Optimizer):\n    """"""\n    Implements the L-BFGS algorithm. Compatible with multi-batch and full-overlap\n    L-BFGS implementations and (stochastic) Powell damping. Partly based on the\n    original L-BFGS implementation in PyTorch, Mark Schmidt\'s minFunc MATLAB code,\n    and Michael Overton\'s weak Wolfe line search MATLAB code.\n    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n    Last edited 12/6/18.\n    Warnings:\n      . Does not support per-parameter options and parameter groups.\n      . All parameters have to be on a single device.\n    Inputs:\n        lr (float): steplength or learning rate (default: 1)\n        history_size (int): update history size (default: 10)\n        line_search (str): designates line search to use (default: \'Wolfe\')\n            Options:\n                \'None\': uses steplength designated in algorithm\n                \'Armijo\': uses Armijo backtracking line search\n                \'Wolfe\': uses Armijo-Wolfe bracketing line search\n        dtype: data type (default: torch.float)\n        debug (bool): debugging mode\n    References:\n    [1] Berahas, Albert S., Jorge Nocedal, and Martin Tak\xc3\xa1c. ""A Multi-Batch L-BFGS\n        Method for Machine Learning."" Advances in Neural Information Processing\n        Systems. 2016.\n    [2] Bollapragada, Raghu, et al. ""A Progressive Batching L-BFGS Method for Machine\n        Learning."" International Conference on Machine Learning. 2018.\n    [3] Lewis, Adrian S., and Michael L. Overton. ""Nonsmooth Optimization via Quasi-Newton\n        Methods."" Mathematical Programming 141.1-2 (2013): 135-163.\n    [4] Liu, Dong C., and Jorge Nocedal. ""On the Limited Memory BFGS Method for\n        Large Scale Optimization."" Mathematical Programming 45.1-3 (1989): 503-528.\n    [5] Nocedal, Jorge. ""Updating Quasi-Newton Matrices With Limited Storage.""\n        Mathematics of Computation 35.151 (1980): 773-782.\n    [6] Nocedal, Jorge, and Stephen J. Wright. ""Numerical Optimization."" Springer New York,\n        2006.\n    [7] Schmidt, Mark. ""minFunc: Unconstrained Differentiable Multivariate Optimization\n        in Matlab."" Software available at http://www.cs.ubc.ca/~schmidtm/Software/minFunc.html\n        (2005).\n    [8] Schraudolph, Nicol N., Jin Yu, and Simon G\xc3\xbcnter. ""A Stochastic Quasi-Newton\n        Method for Online Convex Optimization."" Artificial Intelligence and Statistics.\n        2007.\n    [9] Wang, Xiao, et al. ""Stochastic Quasi-Newton Methods for Nonconvex Stochastic\n        Optimization."" SIAM Journal on Optimization 27.2 (2017): 927-956.\n    """"""\n\n    def __init__(self, params, lr=1, history_size=10, line_search=\'Wolfe\',\n                 dtype=torch.float, debug=False):\n\n        # ensure inputs are valid\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0 <= history_size:\n            raise ValueError(""Invalid history size: {}"".format(history_size))\n        if line_search not in [\'Armijo\', \'Wolfe\', \'None\']:\n            raise ValueError(""Invalid line search: {}"".format(line_search))\n\n        defaults = dict(lr=lr, history_size=history_size, line_search=line_search,\n                        dtype=dtype, debug=debug)\n        super(LBFGS, self).__init__(params, defaults)\n\n        if len(self.param_groups) != 1:\n            raise ValueError(""L-BFGS doesn\'t support per-parameter options ""\n                             ""(parameter groups)"")\n\n        self._params = self.param_groups[0][\'params\']\n        self._numel_cache = None\n\n        state = self.state[\'global_state\']\n        state.setdefault(\'n_iter\', 0)\n        state.setdefault(\'curv_skips\', 0)\n        state.setdefault(\'fail_skips\', 0)\n        state.setdefault(\'H_diag\',1)\n        state.setdefault(\'fail\', True)\n\n        state[\'old_dirs\'] = []\n        state[\'old_stps\'] = []\n\n    def _numel(self):\n        if self._numel_cache is None:\n            self._numel_cache = reduce(lambda total, p: total + p.numel(), self._params, 0)\n        return self._numel_cache\n\n    def _gather_flat_grad(self):\n        views = []\n        for p in self._params:\n            if p.grad is None:\n                view = p.data.new(p.data.numel()).zero_()\n            elif p.grad.data.is_sparse:\n                view = p.grad.data.to_dense().view(-1)\n            else:\n                view = p.grad.data.view(-1)\n            views.append(view)\n        return torch.cat(views, 0)\n\n    def _add_update(self, step_size, update):\n        offset = 0\n        for p in self._params:\n            numel = p.numel()\n            # view as to avoid deprecated pointwise semantics\n            p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n            offset += numel\n        assert offset == self._numel()\n\n    def _copy_params(self):\n        current_params = []\n        for param in self._params:\n            current_params.append(deepcopy(param.data))\n        return current_params\n\n    def _load_params(self, current_params):\n        i = 0\n        for param in self._params:\n            param.data[:] = current_params[i]\n            i += 1\n\n    def line_search(self, line_search):\n        """"""\n        Switches line search option.\n\n        Inputs:\n            line_search (str): designates line search to use\n                Options:\n                    \'None\': uses steplength designated in algorithm\n                    \'Armijo\': uses Armijo backtracking line search\n                    \'Wolfe\': uses Armijo-Wolfe bracketing line search\n\n        """"""\n\n        group = self.param_groups[0]\n        group[\'line_search\'] = line_search\n\n        return\n\n    def two_loop_recursion(self, vec):\n        """"""\n        Performs two-loop recursion on given vector to obtain Hv.\n        Inputs:\n            vec (tensor): 1-D tensor to apply two-loop recursion to\n        Output:\n            r (tensor): matrix-vector product Hv\n        """"""\n\n        group = self.param_groups[0]\n        history_size = group[\'history_size\']\n\n        state = self.state[\'global_state\']\n        old_dirs = state.get(\'old_dirs\')    # change in gradients\n        old_stps = state.get(\'old_stps\')    # change in iterates\n        H_diag = state.get(\'H_diag\')\n\n        # compute the product of the inverse Hessian approximation and the gradient\n        num_old = len(old_dirs)\n\n        if \'rho\' not in state:\n            state[\'rho\'] = [None] * history_size\n            state[\'alpha\'] = [None] * history_size\n        rho = state[\'rho\']\n        alpha = state[\'alpha\']\n\n        for i in range(num_old):\n            rho[i] = 1. / old_stps[i].dot(old_dirs[i])\n\n        q = vec\n        for i in range(num_old - 1, -1, -1):\n            alpha[i] = old_dirs[i].dot(q) * rho[i]\n            q.add_(-alpha[i], old_stps[i])\n\n        # multiply by initial Hessian\n        # r/d is the final direction\n        r = torch.mul(q, H_diag)\n        for i in range(num_old):\n            beta = old_stps[i].dot(r) * rho[i]\n            r.add_(alpha[i] - beta, old_dirs[i])\n\n        return r\n\n    def curvature_update(self, flat_grad, eps=1e-2, damping=False):\n        """"""\n        Performs curvature update.\n        Inputs:\n            flat_grad (tensor): 1-D tensor of flattened gradient for computing\n                gradient difference with previously stored gradient\n            eps (float): constant for curvature pair rejection or damping (default: 1e-2)\n            damping (bool): flag for using Powell damping (default: False)\n        """"""\n\n        assert len(self.param_groups) == 1\n\n        # load parameters\n        if(eps <= 0):\n            raise(ValueError(\'Invalid eps; must be positive.\'))\n\n        group = self.param_groups[0]\n        history_size = group[\'history_size\']\n        debug = group[\'debug\']\n\n        # variables cached in state (for tracing)\n        state = self.state[\'global_state\']\n        fail = state.get(\'fail\')\n\n        # check if line search failed\n        if not fail:\n\n            d = state.get(\'d\')\n            t = state.get(\'t\')\n            old_dirs = state.get(\'old_dirs\')\n            old_stps = state.get(\'old_stps\')\n            H_diag = state.get(\'H_diag\')\n            prev_flat_grad = state.get(\'prev_flat_grad\')\n            Bs = state.get(\'Bs\')\n\n            # compute y\'s\n            y = flat_grad.sub(prev_flat_grad)\n            s = d.mul(t)\n            sBs = s.dot(Bs)\n            ys = y.dot(s)  # y*s\n\n            # update L-BFGS matrix\n            if ys > eps*sBs or damping == True:\n\n                # perform Powell damping\n                if damping == True and ys < eps*sBs:\n                    if debug:\n                        print(\'Applying Powell damping...\')\n                    theta = ((1-eps)*sBs)/(sBs - ys)\n                    y = theta*y + (1-theta)*Bs\n\n                # updating memory\n                if len(old_dirs) == history_size:\n                    # shift history by one (limited-memory)\n                    old_dirs.pop(0)\n                    old_stps.pop(0)\n\n                # store new direction/step\n                old_dirs.append(s)\n                old_stps.append(y)\n\n                # update scale of initial Hessian approximation\n                H_diag = ys / y.dot(y)  # (y*y)\n\n                state[\'old_dirs\'] = old_dirs\n                state[\'old_stps\'] = old_stps\n                state[\'H_diag\'] = H_diag\n\n            else:\n                # save skip\n                state[\'curv_skips\'] += 1\n                if debug:\n                    print(\'Curvature pair skipped due to failed criterion\')\n\n        else:\n            # save skip\n            state[\'fail_skips\'] += 1\n            if debug:\n                print(\'Line search failed; curvature pair update skipped\')\n\n        return\n\n    def _step(self, p_k, g_Ok, g_Sk=None, options={}):\n        """"""\n        Performs a single optimization step.\n        Inputs:\n            p_k (tensor): 1-D tensor specifying search direction\n            g_Ok (tensor): 1-D tensor of flattened gradient over overlap O_k used\n                            for gradient differencing in curvature pair update\n            g_Sk (tensor): 1-D tensor of flattened gradient over full sample S_k\n                            used for curvature pair damping or rejection criterion,\n                            if None, will use g_Ok (default: None)\n            options (dict): contains options for performing line search\n        Options for Armijo backtracking line search:\n            \'closure\' (callable): reevaluates model and returns function value\n            \'current_loss\' (tensor): objective value at current iterate (default: F(x_k))\n            \'gtd\' (tensor): inner product g_Ok\'d in line search (default: g_Ok\'d)\n            \'eta\' (tensor): factor for decreasing steplength > 0 (default: 2)\n            \'c1\' (tensor): sufficient decrease constant in (0, 1) (default: 1e-4)\n            \'max_ls\' (int): maximum number of line search steps permitted (default: 10)\n            \'interpolate\' (bool): flag for using interpolation (default: True)\n            \'inplace\' (bool): flag for inplace operations (default: True)\n            \'ls_debug\' (bool): debugging mode for line search\n        Options for Wolfe line search:\n            \'closure\' (callable): reevaluates model and returns function value\n            \'current_loss\' (tensor): objective value at current iterate (default: F(x_k))\n            \'gtd\' (tensor): inner product g_Ok\'d in line search (default: g_Ok\'d)\n            \'eta\' (float): factor for extrapolation (default: 2)\n            \'c1\' (float): sufficient decrease constant in (0, 1) (default: 1e-4)\n            \'c2\' (float): curvature condition constant in (0, 1) (default: 0.9)\n            \'max_ls\' (int): maximum number of line search steps permitted (default: 10)\n            \'interpolate\' (bool): flag for using interpolation (default: True)\n            \'inplace\' (bool): flag for inplace operations (default: True)\n            \'ls_debug\' (bool): debugging mode for line search\n        Outputs (depends on line search):\n          . No line search:\n                t (float): steplength\n          . Armijo backtracking line search:\n                F_new (tensor): loss function at new iterate\n                t (tensor): final steplength\n                ls_step (int): number of backtracks\n                closure_eval (int): number of closure evaluations\n                desc_dir (bool): descent direction flag\n                    True: p_k is descent direction with respect to the line search\n                    function\n                    False: p_k is not a descent direction with respect to the line\n                    search function\n                fail (bool): failure flag\n                    True: line search reached maximum number of iterations, failed\n                    False: line search succeeded\n          . Wolfe line search:\n                F_new (tensor): loss function at new iterate\n                g_new (tensor): gradient at new iterate\n                t (float): final steplength\n                ls_step (int): number of backtracks\n                closure_eval (int): number of closure evaluations\n                grad_eval (int): number of gradient evaluations\n                desc_dir (bool): descent direction flag\n                    True: p_k is descent direction with respect to the line search\n                    function\n                    False: p_k is not a descent direction with respect to the line\n                    search function\n                fail (bool): failure flag\n                    True: line search reached maximum number of iterations, failed\n                    False: line search succeeded\n        Notes:\n          . If encountering line search failure in the deterministic setting, one\n            should try increasing the maximum number of line search steps max_ls.\n        """"""\n\n        assert len(self.param_groups) == 1\n\n        # load parameter options\n        group = self.param_groups[0]\n        lr = group[\'lr\']\n        line_search = group[\'line_search\']\n        dtype = group[\'dtype\']\n        debug = group[\'debug\']\n\n        # variables cached in state (for tracing)\n        state = self.state[\'global_state\']\n        d = state.get(\'d\')\n        t = state.get(\'t\')\n        prev_flat_grad = state.get(\'prev_flat_grad\')\n        Bs = state.get(\'Bs\')\n\n        # keep track of nb of iterations\n        state[\'n_iter\'] += 1\n\n        # set search direction\n        d = p_k\n\n        # modify previous gradient\n        if prev_flat_grad is None:\n            prev_flat_grad = g_Ok.clone()\n        else:\n            prev_flat_grad.copy_(g_Ok)\n\n        # set initial step size\n        t = lr\n\n        # closure evaluation counter\n        closure_eval = 0\n\n        if g_Sk is None:\n            g_Sk = g_Ok.clone()\n\n        # perform Armijo backtracking line search\n        if(line_search == \'Armijo\'):\n\n            # load options\n            if(options):\n                if(\'closure\' not in options.keys()):\n                    raise(ValueError(\'closure option not specified.\'))\n                else:\n                    closure = options[\'closure\']\n\n                if(\'gtd\' not in options.keys()):\n                    gtd = g_Ok.dot(d)\n                else:\n                    gtd = options[\'gtd\']\n\n                if(\'current_loss\' not in options.keys()):\n                    F_k = closure()\n                    closure_eval += 1\n                else:\n                    F_k = options[\'current_loss\']\n\n                if(\'eta\' not in options.keys()):\n                    eta = 2\n                elif(options[\'eta\'] <= 0):\n                    raise(ValueError(\'Invalid eta; must be positive.\'))\n                else:\n                    eta = options[\'eta\']\n\n                if(\'c1\' not in options.keys()):\n                    c1 = 1e-4\n                elif(options[\'c1\'] >= 1 or options[\'c1\'] <= 0):\n                    raise(ValueError(\'Invalid c1; must be strictly between 0 and 1.\'))\n                else:\n                    c1 = options[\'c1\']\n\n                if(\'max_ls\' not in options.keys()):\n                    max_ls = 10\n                elif(options[\'max_ls\'] <= 0):\n                    raise(ValueError(\'Invalid max_ls; must be positive.\'))\n                else:\n                    max_ls = options[\'max_ls\']\n\n                if(\'interpolate\' not in options.keys()):\n                    interpolate = True\n                else:\n                    interpolate = options[\'interpolate\']\n\n                if(\'inplace\' not in options.keys()):\n                    inplace = True\n                else:\n                    inplace = options[\'inplace\']\n\n                if(\'ls_debug\' not in options.keys()):\n                    ls_debug = False\n                else:\n                    ls_debug = options[\'ls_debug\']\n\n            else:\n                raise(ValueError(\'Options are not specified; need closure evaluating function.\'))\n\n            # initialize values\n            if(interpolate):\n                if(torch.cuda.is_available()):\n                    F_prev = torch.tensor(np.nan, dtype=dtype).cuda()\n                else:\n                    F_prev = torch.tensor(np.nan, dtype=dtype)\n\n            ls_step = 0\n            t_prev = 0 # old steplength\n            fail = False # failure flag\n\n            # begin print for debug mode\n            if ls_debug:\n                print(\'==================================== Begin Armijo line search ===================================\')\n                print(\'F(x): %.8e  g*d: %.8e\' %(F_k, gtd))\n\n            # check if search direction is descent direction\n            if gtd >= 0:\n                desc_dir = False\n                if debug:\n                    print(\'Not a descent direction!\')\n            else:\n                desc_dir = True\n\n            # store values if not in-place\n            if not inplace:\n                current_params = self._copy_params()\n\n            # update and evaluate at new point\n            self._add_update(t, d)\n            F_new = closure()\n            closure_eval += 1\n\n            # print info if debugging\n            if(ls_debug):\n                print(\'LS Step: %d  t: %.8e  F(x+td): %.8e  F-c1*t*g*d: %.8e  F(x): %.8e\'\n                      %(ls_step, t, F_new, F_k + c1*t*gtd, F_k))\n\n            # check Armijo condition\n            while F_new > F_k + c1*t*gtd or not is_legal(F_new):\n\n                # check if maximum number of iterations reached\n                if(ls_step >= max_ls):\n                    if inplace:\n                        self._add_update(-t, d)\n                    else:\n                        self._load_params(current_params)\n\n                    t = 0\n                    F_new = closure()\n                    closure_eval += 1\n                    fail = True\n                    break\n\n                else:\n                    # store current steplength\n                    t_new = t\n\n                    # compute new steplength\n\n                    # if first step or not interpolating, then multiply by factor\n                    if(ls_step == 0 or not interpolate or not is_legal(F_new)):\n                        t = t/eta\n\n                    # if second step, use function value at new point along with\n                    # gradient and function at current iterate\n                    elif(ls_step == 1 or not is_legal(F_prev)):\n                        t = polyinterp(np.array([[0, F_k.item(), gtd.item()], [t_new, F_new.item(), np.nan]]))\n\n                    # otherwise, use function values at new point, previous point,\n                    # and gradient and function at current iterate\n                    else:\n                        t = polyinterp(np.array([[0, F_k.item(), gtd.item()], [t_new, F_new.item(), np.nan],\n                                                [t_prev, F_prev.item(), np.nan]]))\n\n                    # if values are too extreme, adjust t\n                    if(interpolate):\n                        if(t < 1e-3*t_new):\n                            t = 1e-3*t_new\n                        elif(t > 0.6*t_new):\n                            t = 0.6*t_new\n\n                        # store old point\n                        F_prev = F_new\n                        t_prev = t_new\n\n                    # update iterate and reevaluate\n                    if inplace:\n                        self._add_update(t-t_new, d)\n                    else:\n                        self._load_params(current_params)\n                        self._add_update(t, d)\n\n                    F_new = closure()\n                    closure_eval += 1\n                    ls_step += 1 # iterate\n\n                    # print info if debugging\n                    if(ls_debug):\n                        print(\'LS Step: %d  t: %.8e  F(x+td):   %.8e  F-c1*t*g*d: %.8e  F(x): %.8e\'\n                              %(ls_step, t, F_new, F_k + c1*t*gtd, F_k))\n\n            # store Bs\n            if Bs is None:\n                Bs = (g_Sk.mul(-t)).clone()\n            else:\n                Bs.copy_(g_Sk.mul(-t))\n\n            # print final steplength\n            if ls_debug:\n                print(\'Final Steplength:\', t)\n                print(\'===================================== End Armijo line search ====================================\')\n\n            state[\'d\'] = d\n            state[\'prev_flat_grad\'] = prev_flat_grad\n            state[\'t\'] = t\n            state[\'Bs\'] = Bs\n            state[\'fail\'] = fail\n\n            return F_new, t, ls_step, closure_eval, desc_dir, fail\n\n        # perform weak Wolfe line search\n        elif(line_search == \'Wolfe\'):\n\n            # load options\n            if(options):\n                if(\'closure\' not in options.keys()):\n                    raise(ValueError(\'closure option not specified.\'))\n                else:\n                    closure = options[\'closure\']\n\n                if(\'current_loss\' not in options.keys()):\n                    F_k = closure()\n                    closure_eval += 1\n                else:\n                    F_k = options[\'current_loss\']\n\n                if(\'gtd\' not in options.keys()):\n                    gtd = g_Ok.dot(d)\n                else:\n                    gtd = options[\'gtd\']\n\n                if(\'eta\' not in options.keys()):\n                    eta = 2\n                elif(options[\'eta\'] <= 1):\n                    raise(ValueError(\'Invalid eta; must be greater than 1.\'))\n                else:\n                    eta = options[\'eta\']\n\n                if(\'c1\' not in options.keys()):\n                    c1 = 1e-4\n                elif(options[\'c1\'] >= 1 or options[\'c1\'] <= 0):\n                    raise(ValueError(\'Invalid c1; must be strictly between 0 and 1.\'))\n                else:\n                    c1 = options[\'c1\']\n\n                if(\'c2\' not in options.keys()):\n                    c2 = 0.9\n                elif(options[\'c2\'] >= 1 or options[\'c2\'] <= 0):\n                    raise(ValueError(\'Invalid c2; must be strictly between 0 and 1.\'))\n                elif(options[\'c2\'] <= c1):\n                    raise(ValueError(\'Invalid c2; must be strictly larger than c1.\'))\n                else:\n                    c2 = options[\'c2\']\n\n                if(\'max_ls\' not in options.keys()):\n                    max_ls = 10\n                elif(options[\'max_ls\'] <= 0):\n                    raise(ValueError(\'Invalid max_ls; must be positive.\'))\n                else:\n                    max_ls = options[\'max_ls\']\n\n                if(\'interpolate\' not in options.keys()):\n                    interpolate = True\n                else:\n                    interpolate = options[\'interpolate\']\n\n                if(\'inplace\' not in options.keys()):\n                    inplace = True\n                else:\n                    inplace = options[\'inplace\']\n\n                if(\'ls_debug\' not in options.keys()):\n                    ls_debug = False\n                else:\n                    ls_debug = options[\'ls_debug\']\n\n            else:\n                raise(ValueError(\'Options are not specified; need closure evaluating function.\'))\n\n            # initialize counters\n            ls_step = 0\n            grad_eval = 0 # tracks gradient evaluations\n            t_prev = 0 # old steplength\n\n            # initialize bracketing variables and flag\n            alpha = 0\n            beta = float(\'Inf\')\n            fail = False\n\n            # initialize values for line search\n            if(interpolate):\n                F_a = F_k\n                g_a = gtd\n\n                if(torch.cuda.is_available()):\n                    F_b = torch.tensor(np.nan, dtype=dtype).cuda()\n                    g_b = torch.tensor(np.nan, dtype=dtype).cuda()\n                else:\n                    F_b = torch.tensor(np.nan, dtype=dtype)\n                    g_b = torch.tensor(np.nan, dtype=dtype)\n\n            # begin print for debug mode\n            if ls_debug:\n                print(\'==================================== Begin Wolfe line search ====================================\')\n                print(\'F(x): %.8e  g*d: %.8e\' %(F_k, gtd))\n\n            # check if search direction is descent direction\n            if gtd >= 0:\n                desc_dir = False\n                if debug:\n                    print(\'Not a descent direction!\')\n            else:\n                desc_dir = True\n\n            # store values if not in-place\n            if not inplace:\n                current_params = self._copy_params()\n\n            # update and evaluate at new point\n            self._add_update(t, d)\n            F_new = closure()\n            closure_eval += 1\n\n            # main loop\n            while True:\n\n                # check if maximum number of line search steps have been reached\n                if(ls_step >= max_ls):\n                    if inplace:\n                        self._add_update(-t, d)\n                    else:\n                        self._load_params(current_params)\n\n                    t = 0\n                    F_new = closure()\n                    F_new.backward()\n                    g_new = self._gather_flat_grad()\n                    closure_eval += 1\n                    grad_eval += 1\n                    fail = True\n                    break\n\n                # print info if debugging\n                if(ls_debug):\n                    print(\'LS Step: %d  t: %.8e  alpha: %.8e  beta: %.8e\'\n                          %(ls_step, t, alpha, beta))\n                    print(\'Armijo:  F(x+td): %.8e  F-c1*t*g*d: %.8e  F(x): %.8e\'\n                          %(F_new, F_k + c1*t*gtd, F_k))\n\n                # check Armijo condition\n                if(F_new > F_k + c1*t*gtd):\n\n                    # set upper bound\n                    beta = t\n                    t_prev = t\n\n                    # update interpolation quantities\n                    if(interpolate):\n                        F_b = F_new\n                        if(torch.cuda.is_available()):\n                            g_b = torch.tensor(np.nan, dtype=dtype).cuda()\n                        else:\n                            g_b = torch.tensor(np.nan, dtype=dtype)\n\n                else:\n\n                    # compute gradient\n                    F_new.backward()\n                    g_new = self._gather_flat_grad()\n                    grad_eval += 1\n                    gtd_new = g_new.dot(d)\n\n                    # print info if debugging\n                    if(ls_debug):\n                        print(\'Wolfe: g(x+td)*d: %.8e  c2*g*d: %.8e  gtd: %.8e\'\n                              %(gtd_new, c2*gtd, gtd))\n\n                    # check curvature condition\n                    if(gtd_new < c2*gtd):\n\n                        # set lower bound\n                        alpha = t\n                        t_prev = t\n\n                        # update interpolation quantities\n                        if(interpolate):\n                            F_a = F_new\n                            g_a = gtd_new\n\n                    else:\n                        break\n\n                # compute new steplength\n\n                # if first step or not interpolating, then bisect or multiply by factor\n                if(not interpolate or not is_legal(F_b)):\n                    if(beta == float(\'Inf\')):\n                        t = eta*t\n                    else:\n                        t = (alpha + beta)/2.0\n\n                # otherwise interpolate between a and b\n                else:\n                    t = polyinterp(np.array([[alpha, F_a.item(), g_a.item()],[beta, F_b.item(), g_b.item()]]))\n\n                    # if values are too extreme, adjust t\n                    if(beta == float(\'Inf\')):\n                        if(t > 2*eta*t_prev):\n                            t = 2*eta*t_prev\n                        elif(t < eta*t_prev):\n                            t = eta*t_prev\n                    else:\n                        if(t < alpha + 0.2*(beta - alpha)):\n                            t = alpha + 0.2*(beta - alpha)\n                        elif(t > (beta - alpha)/2.0):\n                            t = (beta - alpha)/2.0\n\n                    # if we obtain nonsensical value from interpolation\n                    if(t <= 0):\n                        t = (beta - alpha)/2.0\n\n                # update parameters\n                if inplace:\n                    self._add_update(t - t_prev, d)\n                else:\n                    self._load_params(current_params)\n                    self._add_update(t, d)\n\n                # evaluate closure\n                F_new = closure()\n                closure_eval += 1\n                ls_step += 1\n\n            # store Bs\n            if Bs is None:\n                Bs = (g_Sk.mul(-t)).clone()\n            else:\n                Bs.copy_(g_Sk.mul(-t))\n\n            # print final steplength\n            if ls_debug:\n                print(\'Final Steplength:\', t)\n                print(\'===================================== End Wolfe line search =====================================\')\n\n            state[\'d\'] = d\n            state[\'prev_flat_grad\'] = prev_flat_grad\n            state[\'t\'] = t\n            state[\'Bs\'] = Bs\n            state[\'fail\'] = fail\n\n            return F_new, g_new, t, ls_step, closure_eval, grad_eval, desc_dir, fail\n\n        else:\n\n            # perform update\n            self._add_update(t, d)\n\n            # store Bs\n            if Bs is None:\n                Bs = (g_Sk.mul(-t)).clone()\n            else:\n                Bs.copy_(g_Sk.mul(-t))\n\n            state[\'d\'] = d\n            state[\'prev_flat_grad\'] = prev_flat_grad\n            state[\'t\'] = t\n            state[\'Bs\'] = Bs\n            state[\'fail\'] = False\n\n            return t\n\n    def step(self, p_k, g_Ok, g_Sk=None, options={}):\n        return self._step(p_k, g_Ok, g_Sk, options)\n\n#%% Full-Batch (Deterministic) L-BFGS Optimizer (Wrapper)\n\nclass FullBatchLBFGS(LBFGS):\n    """"""\n    Implements full-batch or deterministic L-BFGS algorithm. Compatible with\n    Powell damping. Can be used when evaluating a deterministic function and\n    gradient. Wraps the LBFGS optimizer. Performs the two-loop recursion,\n    updating, and curvature updating in a single step.\n    Implemented by: Hao-Jun Michael Shi and Dheevatsa Mudigere\n    Last edited 11/15/18.\n    Warnings:\n      . Does not support per-parameter options and parameter groups.\n      . All parameters have to be on a single device.\n    Inputs:\n        lr (float): steplength or learning rate (default: 1)\n        history_size (int): update history size (default: 10)\n        line_search (str): designates line search to use (default: \'Wolfe\')\n            Options:\n                \'None\': uses steplength designated in algorithm\n                \'Armijo\': uses Armijo backtracking line search\n                \'Wolfe\': uses Armijo-Wolfe bracketing line search\n        dtype: data type (default: torch.float)\n        debug (bool): debugging mode\n    """"""\n\n    def __init__(self, params, lr=1, history_size=10, line_search=\'Wolfe\',\n                 dtype=torch.float, debug=False):\n        super(FullBatchLBFGS, self).__init__(params, lr, history_size, line_search,\n             dtype, debug)\n\n    def step(self, options={}):\n        """"""\n        Performs a single optimization step.\n        Inputs:\n            options (dict): contains options for performing line search\n\n        General Options:\n            \'eps\' (float): constant for curvature pair rejection or damping (default: 1e-2)\n            \'damping\' (bool): flag for using Powell damping (default: False)\n        Options for Armijo backtracking line search:\n            \'closure\' (callable): reevaluates model and returns function value\n            \'current_loss\' (tensor): objective value at current iterate (default: F(x_k))\n            \'gtd\' (tensor): inner product g_Ok\'d in line search (default: g_Ok\'d)\n            \'eta\' (tensor): factor for decreasing steplength > 0 (default: 2)\n            \'c1\' (tensor): sufficient decrease constant in (0, 1) (default: 1e-4)\n            \'max_ls\' (int): maximum number of line search steps permitted (default: 10)\n            \'interpolate\' (bool): flag for using interpolation (default: True)\n            \'inplace\' (bool): flag for inplace operations (default: True)\n            \'ls_debug\' (bool): debugging mode for line search\n        Options for Wolfe line search:\n            \'closure\' (callable): reevaluates model and returns function value\n            \'current_loss\' (tensor): objective value at current iterate (default: F(x_k))\n            \'gtd\' (tensor): inner product g_Ok\'d in line search (default: g_Ok\'d)\n            \'eta\' (float): factor for extrapolation (default: 2)\n            \'c1\' (float): sufficient decrease constant in (0, 1) (default: 1e-4)\n            \'c2\' (float): curvature condition constant in (0, 1) (default: 0.9)\n            \'max_ls\' (int): maximum number of line search steps permitted (default: 10)\n            \'interpolate\' (bool): flag for using interpolation (default: True)\n            \'inplace\' (bool): flag for inplace operations (default: True)\n            \'ls_debug\' (bool): debugging mode for line search\n        Outputs (depends on line search):\n          . No line search:\n                t (float): steplength\n          . Armijo backtracking line search:\n                F_new (tensor): loss function at new iterate\n                t (tensor): final steplength\n                ls_step (int): number of backtracks\n                closure_eval (int): number of closure evaluations\n                desc_dir (bool): descent direction flag\n                    True: p_k is descent direction with respect to the line search\n                    function\n                    False: p_k is not a descent direction with respect to the line\n                    search function\n                fail (bool): failure flag\n                    True: line search reached maximum number of iterations, failed\n                    False: line search succeeded\n          . Wolfe line search:\n                F_new (tensor): loss function at new iterate\n                g_new (tensor): gradient at new iterate\n                t (float): final steplength\n                ls_step (int): number of backtracks\n                closure_eval (int): number of closure evaluations\n                grad_eval (int): number of gradient evaluations\n                desc_dir (bool): descent direction flag\n                    True: p_k is descent direction with respect to the line search\n                    function\n                    False: p_k is not a descent direction with respect to the line\n                    search function\n                fail (bool): failure flag\n                    True: line search reached maximum number of iterations, failed\n                    False: line search succeeded\n        Notes:\n          . If encountering line search failure in the deterministic setting, one\n            should try increasing the maximum number of line search steps max_ls.\n        """"""\n\n        # load options for damping and eps\n        if(\'damping\' not in options.keys()):\n            damping = False\n        else:\n            damping = options[\'damping\']\n\n        if(\'eps\' not in options.keys()):\n            eps = 1e-2\n        else:\n            eps = options[\'eps\']\n\n        # gather gradient\n        grad = self._gather_flat_grad()\n\n        # update curvature if after 1st iteration\n        state = self.state[\'global_state\']\n        if(state[\'n_iter\'] > 0):\n            self.curvature_update(grad, eps, damping)\n\n        # compute search direction\n        p = self.two_loop_recursion(-grad)\n\n        # take step\n        return self._step(p, grad, options=options)\n\n'"
recogym/agents/random_agent.py,0,"b'# Default Arguments.\nimport numpy as np\nfrom numpy.random.mtrand import RandomState\n\nfrom .abstract import Agent\nfrom ..envs.configuration import Configuration\n\nrandom_args = {\n    \'num_products\': 10,\n    \'random_seed\': np.random.randint(2 ** 31 - 1),\n    \'with_ps_all\': False,\n}\n\n\nclass RandomAgent(Agent):\n    """"""The world\'s simplest agent!""""""\n\n    def __init__(self, config = Configuration(random_args)):\n        super(RandomAgent, self).__init__(config)\n        self.rng = RandomState(config.random_seed)\n\n    def act(self, observation, reward, done):\n        return {\n            **super().act(observation, reward, done),\n            **{\n                \'a\': self.rng.choice(self.config.num_products),\n                \'ps\': 1.0 / float(self.config.num_products),\n                \'ps-a\': (\n                    np.ones(self.config.num_products) / self.config.num_products\n                    if self.config.with_ps_all else\n                    ()\n                ),\n            },\n        }\n'"
recogym/envs/__init__.py,0,"b'from .reco_env_v0 import RecoEnv0\nfrom .reco_env_v1 import RecoEnv1\n\nfrom .observation import Observation\nfrom .configuration import Configuration\nfrom .context import Context, DefaultContext\nfrom .session import Session\n\nfrom .reco_env_v0 import env_0_args\nfrom .reco_env_v1 import env_1_args\n'"
recogym/envs/abstract.py,0,"b'from abc import ABC\n\nimport gym\nimport numpy as np\nimport pandas as pd\nfrom gym.spaces import Discrete\nfrom numpy.random.mtrand import RandomState\nfrom scipy.special import expit as sigmoid\nfrom tqdm import trange\n\nfrom .configuration import Configuration\nfrom .context import DefaultContext\nfrom .features.time import DefaultTimeGenerator\nfrom .observation import Observation\nfrom .session import OrganicSessions\nfrom ..agents import Agent\n\n# Arguments shared between all environments.\n\nenv_args = {\n    \'num_products\': 10,\n    \'num_users\': 100,\n    \'random_seed\': np.random.randint(2 ** 31 - 1),\n    # Markov State Transition Probabilities.\n    \'prob_leave_bandit\': 0.01,\n    \'prob_leave_organic\': 0.01,\n    \'prob_bandit_to_organic\': 0.05,\n    \'prob_organic_to_bandit\': 0.25,\n    \'normalize_beta\': False,\n    \'with_ps_all\': False\n}\n\n\n# Static function for squashing values between 0 and 1.\ndef f(mat, offset=5):\n    """"""Monotonic increasing function as described in toy.pdf.""""""\n    return sigmoid(mat - offset)\n\n\n# Magic numbers for Markov states.\norganic = 0\nbandit = 1\nstop = 2\n\n\nclass AbstractEnv(gym.Env, ABC):\n\n    def __init__(self):\n        gym.Env.__init__(self)\n        ABC.__init__(self)\n\n        self.first_step = True\n        self.config = None\n        self.state = None\n        self.current_user_id = None\n        self.current_time = None\n        self.empty_sessions = OrganicSessions()\n\n    def reset_random_seed(self, epoch=0):\n        # Initialize Random State.\n        assert (self.config.random_seed is not None)\n        self.rng = RandomState(self.config.random_seed + epoch)\n\n    def init_gym(self, args):\n\n        self.config = Configuration(args)\n\n        # Defining Action Space.\n        self.action_space = Discrete(self.config.num_products)\n\n        if \'time_generator\' not in args:\n            self.time_generator = DefaultTimeGenerator(self.config)\n        else:\n            self.time_generator = self.config.time_generator\n\n        # Setting random seed for the first time.\n        self.reset_random_seed()\n\n        if \'agent\' not in args:\n            self.agent = None\n        else:\n            self.agent = self.config.agent\n\n        # Setting any static parameters such as transition probabilities.\n        self.set_static_params()\n\n        # Set random seed for second time, ensures multiple epochs possible.\n        self.reset_random_seed()\n\n    def reset(self, user_id=0):\n        # Current state.\n        self.first_step = True\n        self.state = organic  # Manually set first state as Organic.\n\n        self.time_generator.reset()\n        if self.agent:\n            self.agent.reset()\n\n        self.current_time = self.time_generator.new_time()\n        self.current_user_id = user_id\n\n        # Record number of times each product seen for static policy calculation.\n        self.organic_views = np.zeros(self.config.num_products)\n\n    def generate_organic_sessions(self):\n\n        # Initialize session.\n        session = OrganicSessions()\n\n        while self.state == organic:\n            # Add next product view.\n            self.update_product_view()\n            session.next(\n                DefaultContext(self.current_time, self.current_user_id),\n                self.product_view\n            )\n\n            # Update markov state.\n            self.update_state()\n\n        return session\n\n    def step(self, action_id):\n        """"""\n\n        Parameters\n        ----------\n        action_id : int between 1 and num_products indicating which\n                 product recommended (aka which ad shown)\n\n        Returns\n        -------\n        observation, reward, done, info : tuple\n            observation (tuple) :\n                a tuple of values (is_organic, product_view)\n                is_organic - True  if Markov state is `organic`,\n                             False if Markov state `bandit` or `stop`.\n                product_view - if Markov state is `organic` then it is an int\n                               between 1 and P where P is the number of\n                               products otherwise it is None.\n            reward (float) :\n                if the previous state was\n                    `bandit` - then reward is 1 if the user clicked on the ad\n                               you recommended otherwise 0\n                    `organic` - then reward is None\n            done (bool) :\n                whether it\'s time to reset the environment again.\n                An episode is over at the end of a user\'s timeline (all of\n                their organic and bandit sessions)\n            info (dict) :\n                 this is unused, it\'s always an empty dict\n        """"""\n\n        # No information to return.\n        info = {}\n\n        if self.first_step:\n            assert (action_id is None)\n            self.first_step = False\n            sessions = self.generate_organic_sessions()\n            return (\n                Observation(\n                    DefaultContext(\n                        self.current_time,\n                        self.current_user_id\n                    ),\n                    sessions\n                ),\n                None,\n                self.state == stop,\n                info\n            )\n\n        assert (action_id is not None)\n        # Calculate reward from action.\n        reward = self.draw_click(action_id)\n\n        self.update_state()\n\n        if reward == 1:\n            self.state = organic  # After a click, Organic Events always follow.\n\n        # Markov state dependent logic.\n        if self.state == organic:\n            sessions = self.generate_organic_sessions()\n        else:\n            sessions = self.empty_sessions\n\n        return (\n            Observation(\n                DefaultContext(self.current_time, self.current_user_id),\n                sessions\n            ),\n            reward,\n            self.state == stop,\n            info\n        )\n\n    def step_offline(self, observation, reward, done):\n        """"""Call step function wih the policy implemented by a particular Agent.""""""\n\n        if self.first_step:\n            action = None\n        else:\n            assert (hasattr(self, \'agent\'))\n            assert (observation is not None)\n            if self.agent:\n                action = self.agent.act(observation, reward, done)\n            else:\n                # Select a Product randomly.\n                action = {\n                    \'t\': observation.context().time(),\n                    \'u\': observation.context().user(),\n                    \'a\': np.int16(self.rng.choice(self.config.num_products)),\n                    \'ps\': 1.0 / self.config.num_products,\n                    \'ps-a\': (\n                        np.ones(self.config.num_products) / self.config.num_products\n                        if self.config.with_ps_all else\n                        ()\n                    ),\n                }\n\n        if done:\n            return (\n                action,\n                Observation(\n                    DefaultContext(self.current_time, self.current_user_id),\n                    self.empty_sessions\n                ),\n                0,\n                done,\n                None\n            )\n        else:\n            observation, reward, done, info = self.step(\n                action[\'a\'] if action is not None else None\n            )\n\n            return action, observation, reward, done, info\n\n    def generate_logs(\n            self,\n            num_offline_users: int,\n            agent: Agent = None,\n            num_organic_offline_users: int = 0\n    ):\n        """"""\n        Produce logs of applying an Agent in the Environment for the specified amount of Users.\n        If the Agent is not provided, then the default Agent is used that randomly selects an Action.\n        """"""\n\n        if agent:\n            old_agent = self.agent\n            self.agent = agent\n\n        data = {\n            \'t\': [],\n            \'u\': [],\n            \'z\': [],\n            \'v\': [],\n            \'a\': [],\n            \'c\': [],\n            \'ps\': [],\n            \'ps-a\': [],\n        }\n\n        def _store_organic(observation):\n            assert (observation is not None)\n            assert (observation.sessions() is not None)\n            for session in observation.sessions():\n                data[\'t\'].append(session[\'t\'])\n                data[\'u\'].append(session[\'u\'])\n                data[\'z\'].append(\'organic\')\n                data[\'v\'].append(session[\'v\'])\n                data[\'a\'].append(None)\n                data[\'c\'].append(None)\n                data[\'ps\'].append(None)\n                data[\'ps-a\'].append(None)\n\n        def _store_bandit(action, reward):\n            if action:\n                assert (reward is not None)\n                data[\'t\'].append(action[\'t\'])\n                data[\'u\'].append(action[\'u\'])\n                data[\'z\'].append(\'bandit\')\n                data[\'v\'].append(None)\n                data[\'a\'].append(action[\'a\'])\n                data[\'c\'].append(reward)\n                data[\'ps\'].append(action[\'ps\'])\n                data[\'ps-a\'].append(action[\'ps-a\'] if \'ps-a\' in action else ())\n\n        unique_user_id = 0\n        for _ in trange(num_organic_offline_users, desc=\'Organic Users\'):\n            self.reset(unique_user_id)\n            unique_user_id += 1\n            observation, _, _, _ = self.step(None)\n            _store_organic(observation)\n\n        for _ in trange(num_offline_users, desc=\'Users\'):\n            self.reset(unique_user_id)\n            unique_user_id += 1\n            observation, reward, done, _ = self.step(None)\n\n            while not done:\n                _store_organic(observation)\n                action, observation, reward, done, _ = self.step_offline(\n                    observation, reward, done\n                )\n                _store_bandit(action, reward)\n\n            _store_organic(observation)\n            action, _, reward, done, _ = self.step_offline(\n                observation, reward, done\n            )\n            assert done, \'Done must not be changed!\'\n            _store_bandit(action, reward)\n\n        data[\'t\'] = np.array(data[\'t\'], dtype=np.float32)\n        data[\'u\'] = pd.array(data[\'u\'], dtype=pd.UInt16Dtype())\n        data[\'v\'] = pd.array(data[\'v\'], dtype=pd.UInt16Dtype())\n        data[\'a\'] = pd.array(data[\'a\'], dtype=pd.UInt16Dtype())\n        data[\'c\'] = np.array(data[\'c\'], dtype=np.float32)\n\n        if agent:\n            self.agent = old_agent\n\n        return pd.DataFrame().from_dict(data)\n'"
recogym/envs/configuration.py,0,"b'class Configuration:\n    """"""\n    Configuration\n\n    That class defines Environment Configurations used in RecoGym.\n    The configurations are provided as a dictionary: key = value.\n    The value can be ANY type i.e. a complex object, function etc.\n\n    The class is immutable i.e. once an instance of that class is created,\n    no configuration can be changed.\n    """"""\n\n    def __init__(self, args):\n        # self.args = args\n        # Set all key word arguments as attributes.\n        for key in args:\n            super(Configuration, self).__setattr__(key, args[key])\n        Configuration.__slots__ = [key for key in args]\n\n    def __setattr__(self, key, value):\n        pass\n\n    def __deepcopy__(self, memodict={}):\n        return self\n'"
recogym/envs/context.py,0,"b'class Context:\n\n    def time(self):\n        raise NotImplemented\n\n    def user(self):\n        raise NotImplemented\n\n\nclass DefaultContext(Context):\n\n    def __init__(self, current_time, current_user_id):\n        super(DefaultContext, self).__init__()\n        self.current_time = current_time\n        self.current_user_id = current_user_id\n\n    def time(self):\n        return self.current_time\n\n    def user(self):\n        return self.current_user_id\n'"
recogym/envs/observation.py,0,"b'class Observation:\n    def __init__(self, context, sessions):\n        self.current_context = context\n        self.current_sessions = sessions\n\n    def context(self):\n        return self.current_context\n\n    def sessions(self):\n        return self.current_sessions\n'"
recogym/envs/reco_env_v0.py,0,"b'from numpy import array, sqrt, kron, eye, ones\n\nfrom .abstract import AbstractEnv, f, env_args\n\n# Default arguments for toy environment ------------------------------------\n\nenv_0_args = env_args\n\n# Users are grouped into distinct clusters to prevent mixing.\nenv_args[\'num_clusters\'] = 2\n\n# Variance of the difference between organic and bandit.\nenv_args[\'phi_var\'] = 0.1\n\n\n# Environment definition ----------------------------------------------------\nclass RecoEnv0(AbstractEnv):\n\n    def __init__(self):\n        super(RecoEnv0, self).__init__()\n\n    def set_static_params(self):\n        # State transition Matrix between Organic, Bandit, Leave\n        self.state_transition = array([\n            [0, self.config.prob_organic_to_bandit, self.config.prob_leave_organic],\n            [self.config.prob_bandit_to_organic, 0, self.config.prob_leave_organic],\n            [0.0, 0.0, 1.]\n        ])\n\n        self.state_transition[0, 0] = 1 - sum(self.state_transition[0, :])\n        self.state_transition[1, 1] = 1 - sum(self.state_transition[1, :])\n\n        # Organic Transition Matrix\n        cluster_ratio = int(self.config.num_products / self.config.num_clusters)\n        ones_mat = ones((cluster_ratio, cluster_ratio))\n\n        T = kron(eye(self.config.num_clusters), ones_mat)\n        T = T / kron(T.sum(1), ones((self.config.num_products, 1))).T\n        self.product_transition = T\n\n        # creating click probability matrix\n        self.phi = self.rng.normal(\n            scale=sqrt(self.config.phi_var),\n            size=(self.config.num_products, self.config.num_products)\n        )\n        self.click_probs = f(self.config.num_products / 5. * (T + T.T) + self.phi)\n\n        self.initial_product_probs = \\\n            ones((self.config.num_products)) / self.config.num_products\n\n    def reset(self, user_id=0):\n        super().reset(user_id)\n\n        # Current Organic product viewed, choose from initial probabilities\n        self.product_view = self.rng.choice(\n            self.config.num_products, p=self.initial_product_probs\n        )\n\n    def update_state(self):\n        """"""Update Markov state between `organic`, `bandit`, or `stop`""""""\n        self.state = self.rng.choice(3, p=self.state_transition[self.state, :])\n\n    def draw_click(self, recommendation):\n        p = self.click_probs[recommendation, self.product_view]\n        return self.rng.binomial(1, p)\n\n    def update_product_view(self):\n        probs = self.product_transition[self.product_view, :]\n        self.product_view = self.rng.choice(self.config.num_products, p=probs)\n'"
recogym/envs/reco_env_v1.py,0,"b'# Omega is the users latent representation of interests - vector of size K\n#     omega is initialised when you have new user with reset\n#     omega is updated at every timestep using timestep\n#\n# Gamma is the latent representation of organic products (matrix P by K)\n# softmax(Gamma omega) is the next item probabilities (organic)\n\n# Beta is the latent representation of response to actions (matrix P by K)\n# sigmoid(beta omega) is the ctr for each action\nimport numpy as np\nfrom numba import njit\n\nfrom .abstract import AbstractEnv, env_args, organic\n\n# Default arguments for toy environment ------------------------------------\n\n# inherit most arguments from abstract class\nenv_1_args = {\n    **env_args,\n    **{\n        \'K\': 5,\n        \'sigma_omega_initial\': 1,\n        \'sigma_omega\': 0.1,\n        \'number_of_flips\': 0,\n        \'sigma_mu_organic\': 3,\n        \'change_omega_for_bandits\': False,\n        \'normalize_beta\': False\n    }\n}\n\n\n@njit(nogil=True)\ndef sig(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\n\n# Maps behaviour into ctr - organic has real support ctr is on [0,1].\n@njit(nogil=True)\ndef ff(xx, aa=5, bb=2, cc=0.3, dd=2, ee=6):\n    # Magic numbers give a reasonable ctr of around 2%.\n    return sig(aa * sig(bb * sig(cc * xx) - dd) - ee)\n\n\n# Environment definition.\nclass RecoEnv1(AbstractEnv):\n\n    def __init__(self):\n        super(RecoEnv1, self).__init__()\n        self.cached_state_seed = None\n\n    def set_static_params(self):\n        # Initialise the state transition matrix which is 3 by 3\n        # high level transitions between organic, bandit and leave.\n        self.state_transition = np.array([\n            [0, self.config.prob_organic_to_bandit, self.config.prob_leave_organic],\n            [self.config.prob_bandit_to_organic, 0, self.config.prob_leave_organic],\n            [0.0, 0.0, 1.]\n        ])\n\n        self.state_transition[0, 0] = 1 - sum(self.state_transition[0, :])\n        self.state_transition[1, 1] = 1 - sum(self.state_transition[1, :])\n\n        # Initialise Gamma for all products (Organic).\n        self.Gamma = self.rng.normal(\n            size=(self.config.num_products, self.config.K)\n        )\n\n        # Initialise mu_organic.\n        self.mu_organic = self.rng.normal(\n            0, self.config.sigma_mu_organic,\n            size=(self.config.num_products, 1)\n        )\n\n        # Initialise beta, mu_bandit for all products (Bandit).\n        self.generate_beta(self.config.number_of_flips)\n\n    # Create a new user.\n    def reset(self, user_id=0):\n        super().reset(user_id)\n        self.omega = self.rng.normal(\n            0, self.config.sigma_omega_initial, size=(self.config.K, 1)\n        )\n\n    # Update user state to one of (organic, bandit, leave) and their omega (latent factor).\n    def update_state(self):\n        old_state = self.state\n        self.state = self.rng.choice(3, p=self.state_transition[self.state, :])\n        assert (hasattr(self, \'time_generator\'))\n        old_time = self.current_time\n        self.current_time = self.time_generator.new_time()\n        time_delta = self.current_time - old_time\n        omega_k = 1 if time_delta == 0 else time_delta\n\n        # And update omega.\n        if self.config.change_omega_for_bandits or self.state == organic:\n            self.omega = self.rng.normal(\n                self.omega,\n                self.config.sigma_omega * omega_k, size=(self.config.K, 1)\n            )\n        self.context_switch = old_state != self.state\n\n    # Sample a click as response to recommendation when user in bandit state\n    # click ~ Bernoulli().\n    def draw_click(self, recommendation):\n        # Personalised CTR for every recommended product.\n        if self.config.change_omega_for_bandits or self.context_switch:\n            self.cached_state_seed = (\n                    self.beta @ self.omega + self.mu_bandit\n            ).ravel()\n        assert self.cached_state_seed is not None\n        ctr = ff(self.cached_state_seed)\n        click = self.rng.choice(\n            [0, 1],\n            p=[1 - ctr[recommendation], ctr[recommendation]]\n        )\n        return click\n\n    # Sample the next organic product view.\n    def update_product_view(self):\n        log_uprob = (self.Gamma @ self.omega + self.mu_organic).ravel()\n        log_uprob = log_uprob - max(log_uprob)\n        uprob = np.exp(log_uprob)\n        self.product_view = np.int16(\n            self.rng.choice(\n                self.config.num_products,\n                p=uprob / uprob.sum()\n            )\n        )\n\n    def normalize_beta(self):\n        self.beta = self.beta / np.sqrt((self.beta ** 2).sum(1)[:, np.newaxis])\n\n    def generate_beta(self, number_of_flips):\n        """"""Create Beta by flipping Gamma, but flips are between similar items only""""""\n\n        if number_of_flips == 0:\n            self.beta = self.Gamma\n            self.mu_bandit = self.mu_organic\n            if self.config.normalize_beta:\n                self.normalize_beta()\n\n            return\n\n        P, K = self.Gamma.shape\n        index = np.arange(P)\n\n        prod_cov = self.Gamma @ self.Gamma.T\n        # We are always most correlated with ourselves so remove the diagonal.\n        prod_cov = prod_cov - np.diag(np.diag(prod_cov))\n\n        prod_cov_flat = prod_cov.flatten()\n\n        already_used = set()\n        flips = 0\n        for p in prod_cov_flat.argsort()[::-1]:  # Find the most correlated entries\n            # Convert flat indexes to 2d indexes\n            ii, jj = int(p / P), np.mod(p, P)\n            # Do flips between the most correlated entries\n            # provided neither the row or col were used before.\n            if not (ii in already_used or jj in already_used):\n                index[ii] = jj  # Do a flip.\n                index[jj] = ii\n                already_used.add(ii)\n                already_used.add(jj)\n                flips += 1\n\n                if flips == number_of_flips:\n                    break\n\n        self.beta = self.Gamma[index, :]\n        self.mu_bandit = self.mu_organic[index, :]\n\n        if self.config.normalize_beta:\n            self.normalize_beta()\n'"
recogym/envs/session.py,0,"b'class Session(list):\n    """"""Abstract Session class""""""\n\n    def to_strings(self, user_id, session_id):\n        """"""represent session as list of strings (one per event)""""""\n        user_id, session_id = str(user_id), str(session_id)\n        session_type = self.get_type()\n        strings = []\n        for event, product in self:\n            columns = [user_id, session_type, session_id, event, str(product)]\n            strings.append(\',\'.join(columns))\n        return strings\n\n    def get_type(self):\n        raise NotImplemented\n\n\nclass OrganicSessions(Session):\n    def __init__(self):\n        super(OrganicSessions, self).__init__()\n\n    def next(self, context, product):\n        self.append(\n            {\n                \'t\': context.time(),\n                \'u\': context.user(),\n                \'z\': \'pageview\',\n                \'v\': product\n            }\n        )\n\n    def get_type(self):\n        return \'organic\'\n\n    def get_views(self):\n        return [p for _, _, e, p in self if e == \'pageview\']\n'"
recogym/envs/features/__init__.py,0,"b'from .time import TimeGenerator, DefaultTimeGenerator\n'"
recogym/envs/features/time/__init__.py,0,b'from .time_generator import TimeGenerator\nfrom .default_time_generator import DefaultTimeGenerator\nfrom .normal_time_generator import NormalTimeGenerator\n'
recogym/envs/features/time/default_time_generator.py,0,"b'from .time_generator import TimeGenerator\n\n\nclass DefaultTimeGenerator(TimeGenerator):\n    """"""""""""\n    def __init__(self, config):\n        super(DefaultTimeGenerator, self).__init__(config)\n        self.current_time = 0\n\n    def new_time(self):\n        tmp_time = self.current_time\n        self.current_time += 1\n        return tmp_time\n\n    def reset(self):\n        self.current_time = 0\n'"
recogym/envs/features/time/normal_time_generator.py,0,"b'from numpy.random.mtrand import RandomState\nimport numpy as np\n\nfrom .time_generator import TimeGenerator\n\n\nclass NormalTimeGenerator(TimeGenerator):\n    """"""""""""\n    def __init__(self, config):\n        super(NormalTimeGenerator, self).__init__(config)\n        self.current_time = 0\n\n        if not hasattr(self.config, \'normal_time_mu\'):\n            self.normal_time_mu = 0\n        else:\n            self.normal_time_mu = self.config.normal_time_mu\n\n        if not hasattr(self.config, \'normal_time_sigma\'):\n            self.normal_time_sigma = 1\n        else:\n            self.normal_time_sigma = self.config.normal_time_sigma\n\n        self.rng = RandomState(config.random_seed)\n\n    def new_time(self):\n        tmp_time = self.current_time\n        self.current_time += np.abs(self.rng.normal(self.normal_time_mu, self.normal_time_sigma))\n        return tmp_time\n\n    def reset(self):\n        self.current_time = 0\n'"
recogym/envs/features/time/time_generator.py,0,"b'class TimeGenerator:\n    """"""""""""\n    def __init__(self, config):\n        self.config = config\n\n    def new_time(self):\n        raise NotImplemented\n\n    def reset(self):\n        raise NotImplemented\n'"
