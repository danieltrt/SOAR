file_path,api_count,code
pytorch/nlp/build_kaggle_dataset.py,0,"b'""""""Read, split and save the kaggle dataset for our model""""""\n\nimport csv\nimport os\nimport sys\n\n\ndef load_dataset(path_csv):\n    """"""Loads dataset into memory from csv file""""""\n    # Open the csv file, need to specify the encoding for python3\n    use_python3 = sys.version_info[0] >= 3\n    with (open(path_csv, encoding=""windows-1252"") if use_python3 else open(path_csv)) as f:\n        csv_file = csv.reader(f, delimiter=\',\')\n        dataset = []\n        words, tags = [], []\n\n        # Each line of the csv corresponds to one word\n        for idx, row in enumerate(csv_file):\n            if idx == 0: continue\n            sentence, word, pos, tag = row\n            # If the first column is non empty it means we reached a new sentence\n            if len(sentence) != 0:\n                if len(words) > 0:\n                    assert len(words) == len(tags)\n                    dataset.append((words, tags))\n                    words, tags = [], []\n            try:\n                word, tag = str(word), str(tag)\n                words.append(word)\n                tags.append(tag)\n            except UnicodeDecodeError as e:\n                print(""An exception was raised, skipping a word: {}"".format(e))\n                pass\n\n    return dataset\n\n\ndef save_dataset(dataset, save_dir):\n    """"""Writes sentences.txt and labels.txt files in save_dir from dataset\n\n    Args:\n        dataset: ([([""a"", ""cat""], [""O"", ""O""]), ...])\n        save_dir: (string)\n    """"""\n    # Create directory if it doesn\'t exist\n    print(""Saving in {}..."".format(save_dir))\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    # Export the dataset\n    with open(os.path.join(save_dir, \'sentences.txt\'), \'w\') as file_sentences:\n        with open(os.path.join(save_dir, \'labels.txt\'), \'w\') as file_labels:\n            for words, tags in dataset:\n                file_sentences.write(""{}\\n"".format("" "".join(words)))\n                file_labels.write(""{}\\n"".format("" "".join(tags)))\n    print(""- done."")\n\n\nif __name__ == ""__main__"":\n    # Check that the dataset exists (you need to make sure you haven\'t downloaded the `ner.csv`)\n    path_dataset = \'data/kaggle/ner_dataset.csv\'\n    msg = ""{} file not found. Make sure you have downloaded the right dataset"".format(path_dataset)\n    assert os.path.isfile(path_dataset), msg\n\n    # Load the dataset into memory\n    print(""Loading Kaggle dataset into memory..."")\n    dataset = load_dataset(path_dataset)\n    print(""- done."")\n\n    # Split the dataset into train, val and split (dummy split with no shuffle)\n    train_dataset = dataset[:int(0.7*len(dataset))]\n    val_dataset = dataset[int(0.7*len(dataset)) : int(0.85*len(dataset))]\n    test_dataset = dataset[int(0.85*len(dataset)):]\n\n    # Save the datasets to files\n    save_dataset(train_dataset, \'data/kaggle/train\')\n    save_dataset(val_dataset, \'data/kaggle/val\')\n    save_dataset(test_dataset, \'data/kaggle/test\')'"
pytorch/nlp/build_vocab.py,0,"b'""""""Build vocabularies of words and tags from datasets""""""\n\nimport argparse\nfrom collections import Counter\nimport json\nimport os\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--min_count_word\', default=1, help=""Minimum count for words in the dataset"", type=int)\nparser.add_argument(\'--min_count_tag\', default=1, help=""Minimum count for tags in the dataset"", type=int)\nparser.add_argument(\'--data_dir\', default=\'data/small\', help=""Directory containing the dataset"")\n\n# Hyper parameters for the vocab\nPAD_WORD = \'<pad>\'\nPAD_TAG = \'O\'\nUNK_WORD = \'UNK\'\n\n\ndef save_vocab_to_txt_file(vocab, txt_path):\n    """"""Writes one token per line, 0-based line id corresponds to the id of the token.\n\n    Args:\n        vocab: (iterable object) yields token\n        txt_path: (stirng) path to vocab file\n    """"""\n    with open(txt_path, ""w"") as f:\n        for token in vocab:\n            f.write(token + \'\\n\')\n            \n\ndef save_dict_to_json(d, json_path):\n    """"""Saves dict to json file\n\n    Args:\n        d: (dict)\n        json_path: (string) path to json file\n    """"""\n    with open(json_path, \'w\') as f:\n        d = {k: v for k, v in d.items()}\n        json.dump(d, f, indent=4)\n\n\ndef update_vocab(txt_path, vocab):\n    """"""Update word and tag vocabulary from dataset\n\n    Args:\n        txt_path: (string) path to file, one sentence per line\n        vocab: (dict or Counter) with update method\n\n    Returns:\n        dataset_size: (int) number of elements in the dataset\n    """"""\n    with open(txt_path) as f:\n        for i, line in enumerate(f):\n            vocab.update(line.strip().split(\' \'))\n\n    return i + 1\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n\n    # Build word vocab with train and test datasets\n    print(""Building word vocabulary..."")\n    words = Counter()\n    size_train_sentences = update_vocab(os.path.join(args.data_dir, \'train/sentences.txt\'), words)\n    size_dev_sentences = update_vocab(os.path.join(args.data_dir, \'val/sentences.txt\'), words)\n    size_test_sentences = update_vocab(os.path.join(args.data_dir, \'test/sentences.txt\'), words)\n    print(""- done."")\n\n    # Build tag vocab with train and test datasets\n    print(""Building tag vocabulary..."")\n    tags = Counter()\n    size_train_tags = update_vocab(os.path.join(args.data_dir, \'train/labels.txt\'), tags)\n    size_dev_tags = update_vocab(os.path.join(args.data_dir, \'val/labels.txt\'), tags)\n    size_test_tags = update_vocab(os.path.join(args.data_dir, \'test/labels.txt\'), tags)\n    print(""- done."")\n\n    # Assert same number of examples in datasets\n    assert size_train_sentences == size_train_tags\n    assert size_dev_sentences == size_dev_tags\n    assert size_test_sentences == size_test_tags\n\n    # Only keep most frequent tokens\n    words = [tok for tok, count in words.items() if count >= args.min_count_word]\n    tags = [tok for tok, count in tags.items() if count >= args.min_count_tag]\n\n    # Add pad tokens\n    if PAD_WORD not in words: words.append(PAD_WORD)\n    if PAD_TAG not in tags: tags.append(PAD_TAG)\n    \n    # add word for unknown words \n    words.append(UNK_WORD)\n\n    # Save vocabularies to file\n    print(""Saving vocabularies to file..."")\n    save_vocab_to_txt_file(words, os.path.join(args.data_dir, \'words.txt\'))\n    save_vocab_to_txt_file(tags, os.path.join(args.data_dir, \'tags.txt\'))\n    print(""- done."")\n\n    # Save datasets properties in json file\n    sizes = {\n        \'train_size\': size_train_sentences,\n        \'dev_size\': size_dev_sentences,\n        \'test_size\': size_test_sentences,\n        \'vocab_size\': len(words),\n        \'number_of_tags\': len(tags),\n        \'pad_word\': PAD_WORD,\n        \'pad_tag\': PAD_TAG,\n        \'unk_word\': UNK_WORD\n    }\n    save_dict_to_json(sizes, os.path.join(args.data_dir, \'dataset_params.json\'))\n\n    # Logging sizes\n    to_print = ""\\n"".join(""- {}: {}"".format(k, v) for k, v in sizes.items())\n    print(""Characteristics of the dataset:\\n{}"".format(to_print))\n'"
pytorch/nlp/evaluate.py,4,"b'""""""Evaluates the model""""""\n\nimport argparse\nimport logging\nimport os\n\nimport numpy as np\nimport torch\nimport utils\nimport model.net as net\nfrom model.data_loader import DataLoader\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--data_dir\', default=\'data/small\', help=""Directory containing the dataset"")\nparser.add_argument(\'--model_dir\', default=\'experiments/base_model\', help=""Directory containing params.json"")\nparser.add_argument(\'--restore_file\', default=\'best\', help=""name of the file in --model_dir \\\n                     containing weights to load"")\n\n\ndef evaluate(model, loss_fn, data_iterator, metrics, params, num_steps):\n    """"""Evaluate the model on `num_steps` batches.\n\n    Args:\n        model: (torch.nn.Module) the neural network\n        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n        data_iterator: (generator) a generator that generates batches of data and labels\n        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n        params: (Params) hyperparameters\n        num_steps: (int) number of batches to train on, each of size params.batch_size\n    """"""\n\n    # set model to evaluation mode\n    model.eval()\n\n    # summary for current eval loop\n    summ = []\n\n    # compute metrics over the dataset\n    for _ in range(num_steps):\n        # fetch the next evaluation batch\n        data_batch, labels_batch = next(data_iterator)\n        \n        # compute model output\n        output_batch = model(data_batch)\n        loss = loss_fn(output_batch, labels_batch)\n\n        # extract data from torch Variable, move to cpu, convert to numpy arrays\n        output_batch = output_batch.data.cpu().numpy()\n        labels_batch = labels_batch.data.cpu().numpy()\n\n        # compute all metrics on this batch\n        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n                         for metric in metrics}\n        summary_batch[\'loss\'] = loss.item()\n        summ.append(summary_batch)\n\n    # compute mean of all metrics in summary\n    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n    metrics_string = "" ; "".join(""{}: {:05.3f}"".format(k, v) for k, v in metrics_mean.items())\n    logging.info(""- Eval metrics : "" + metrics_string)\n    return metrics_mean\n\n\nif __name__ == \'__main__\':\n    """"""\n        Evaluate the model on the test set.\n    """"""\n    # Load the parameters\n    args = parser.parse_args()\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = utils.Params(json_path)\n\n    # use GPU if available\n    params.cuda = torch.cuda.is_available()     # use GPU is available\n\n    # Set the random seed for reproducible experiments\n    torch.manual_seed(230)\n    if params.cuda: torch.cuda.manual_seed(230)\n        \n    # Get the logger\n    utils.set_logger(os.path.join(args.model_dir, \'evaluate.log\'))\n\n    # Create the input data pipeline\n    logging.info(""Creating the dataset..."")\n\n    # load data\n    data_loader = DataLoader(args.data_dir, params)\n    data = data_loader.load_data([\'test\'], args.data_dir)\n    test_data = data[\'test\']\n\n    # specify the test set size\n    params.test_size = test_data[\'size\']\n    test_data_iterator = data_loader.data_iterator(test_data, params)\n\n    logging.info(""- done."")\n\n    # Define the model\n    model = net.Net(params).cuda() if params.cuda else net.Net(params)\n    \n    loss_fn = net.loss_fn\n    metrics = net.metrics\n    \n    logging.info(""Starting evaluation"")\n\n    # Reload weights from the saved file\n    utils.load_checkpoint(os.path.join(args.model_dir, args.restore_file + \'.pth.tar\'), model)\n\n    # Evaluate\n    num_steps = (params.test_size + 1) // params.batch_size\n    test_metrics = evaluate(model, loss_fn, test_data_iterator, metrics, params, num_steps)\n    save_path = os.path.join(args.model_dir, ""metrics_test_{}.json"".format(args.restore_file))\n    utils.save_dict_to_json(test_metrics, save_path)\n'"
pytorch/nlp/search_hyperparams.py,0,"b'""""""Peform hyperparemeters search""""""\n\nimport argparse\nimport os\nfrom subprocess import check_call\nimport sys\n\nimport utils\n\n\nPYTHON = sys.executable\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--parent_dir\', default=\'experiments/learning_rate\',\n                    help=\'Directory containing params.json\')\nparser.add_argument(\'--data_dir\', default=\'data/small\', help=""Directory containing the dataset"")\n\n\ndef launch_training_job(parent_dir, data_dir, job_name, params):\n    """"""Launch training of the model with a set of hyperparameters in parent_dir/job_name\n\n    Args:\n        model_dir: (string) directory containing config, weights and log\n        data_dir: (string) directory containing the dataset\n        params: (dict) containing hyperparameters\n    """"""\n    # Create a new folder in parent_dir with unique_name ""job_name""\n    model_dir = os.path.join(parent_dir, job_name)\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    # Write parameters in json file\n    json_path = os.path.join(model_dir, \'params.json\')\n    params.save(json_path)\n\n    # Launch training with this config\n    cmd = ""{python} train.py --model_dir={model_dir} --data_dir {data_dir}"".format(python=PYTHON, model_dir=model_dir,\n                                                                                   data_dir=data_dir)\n    print(cmd)\n    check_call(cmd, shell=True)\n\n\nif __name__ == ""__main__"":\n    # Load the ""reference"" parameters from parent_dir json file\n    args = parser.parse_args()\n    json_path = os.path.join(args.parent_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = utils.Params(json_path)\n\n    # Perform hypersearch over one parameter\n    learning_rates = [1e-4, 1e-3, 1e-2]\n\n    for learning_rate in learning_rates:\n        # Modify the relevant parameter in params\n        params.learning_rate = learning_rate\n\n        # Launch job (name has to be unique)\n        job_name = ""learning_rate_{}"".format(learning_rate)\n        launch_training_job(args.parent_dir, args.data_dir, job_name, params)\n'"
pytorch/nlp/synthesize_results.py,0,"b'""""""Aggregates results from the metrics_eval_best_weights.json in a parent folder""""""\n\nimport argparse\nimport json\nimport os\n\nfrom tabulate import tabulate\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--parent_dir\', default=\'experiments\',\n                    help=\'Directory containing results of experiments\')\n\n\ndef aggregate_metrics(parent_dir, metrics):\n    """"""Aggregate the metrics of all experiments in folder `parent_dir`.\n\n    Assumes that `parent_dir` contains multiple experiments, with their results stored in\n    `parent_dir/subdir/metrics_dev.json`\n\n    Args:\n        parent_dir: (string) path to directory containing experiments results\n        metrics: (dict) subdir -> {\'accuracy\': ..., ...}\n    """"""\n    # Get the metrics for the folder if it has results from an experiment\n    metrics_file = os.path.join(parent_dir, \'metrics_val_best_weights.json\')\n    if os.path.isfile(metrics_file):\n        with open(metrics_file, \'r\') as f:\n            metrics[parent_dir] = json.load(f)\n\n    # Check every subdirectory of parent_dir\n    for subdir in os.listdir(parent_dir):\n        if not os.path.isdir(os.path.join(parent_dir, subdir)):\n            continue\n        else:\n            aggregate_metrics(os.path.join(parent_dir, subdir), metrics)\n\n\ndef metrics_to_table(metrics):\n    # Get the headers from the first subdir. Assumes everything has the same metrics\n    headers = metrics[list(metrics.keys())[0]].keys()\n    table = [[subdir] + [values[h] for h in headers] for subdir, values in metrics.items()]\n    res = tabulate(table, headers, tablefmt=\'pipe\')\n\n    return res\n\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n\n    # Aggregate metrics from args.parent_dir directory\n    metrics = dict()\n    aggregate_metrics(args.parent_dir, metrics)\n    table = metrics_to_table(metrics)\n\n    # Display the table to terminal\n    print(table)\n\n    # Save results in parent_dir/results.md\n    save_file = os.path.join(args.parent_dir, ""results.md"")\n    with open(save_file, \'w\') as f:\n        f.write(table)'"
pytorch/nlp/train.py,8,"b'""""""Train the model""""""\n\nimport argparse\nimport logging\nimport os\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom tqdm import trange\n\nimport utils\nimport model.net as net\nfrom model.data_loader import DataLoader\nfrom evaluate import evaluate\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--data_dir\', default=\'data/small\',\n                    help=""Directory containing the dataset"")\nparser.add_argument(\'--model_dir\', default=\'experiments/base_model\',\n                    help=""Directory containing params.json"")\nparser.add_argument(\'--restore_file\', default=None,\n                    help=""Optional, name of the file in --model_dir containing weights to reload before \\\n                    training"")  # \'best\' or \'train\'\n\n\ndef train(model, optimizer, loss_fn, data_iterator, metrics, params, num_steps):\n    """"""Train the model on `num_steps` batches\n\n    Args:\n        model: (torch.nn.Module) the neural network\n        optimizer: (torch.optim) optimizer for parameters of model\n        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n        data_iterator: (generator) a generator that generates batches of data and labels\n        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n        params: (Params) hyperparameters\n        num_steps: (int) number of batches to train on, each of size params.batch_size\n    """"""\n\n    # set model to training mode\n    model.train()\n\n    # summary for current training loop and a running average object for loss\n    summ = []\n    loss_avg = utils.RunningAverage()\n\n    # Use tqdm for progress bar\n    t = trange(num_steps)\n    for i in t:\n        # fetch the next training batch\n        train_batch, labels_batch = next(data_iterator)\n\n        # compute model output and loss\n        output_batch = model(train_batch)\n        loss = loss_fn(output_batch, labels_batch)\n\n        # clear previous gradients, compute gradients of all variables wrt loss\n        optimizer.zero_grad()\n        loss.backward()\n\n        # performs updates using calculated gradients\n        optimizer.step()\n\n        # Evaluate summaries only once in a while\n        if i % params.save_summary_steps == 0:\n            # extract data from torch Variable, move to cpu, convert to numpy arrays\n            output_batch = output_batch.data.cpu().numpy()\n            labels_batch = labels_batch.data.cpu().numpy()\n\n            # compute all metrics on this batch\n            summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n                             for metric in metrics}\n            summary_batch[\'loss\'] = loss.item()\n            summ.append(summary_batch)\n\n        # update the average loss\n        loss_avg.update(loss.item())\n        t.set_postfix(loss=\'{:05.3f}\'.format(loss_avg()))\n\n    # compute mean of all metrics in summary\n    metrics_mean = {metric: np.mean([x[metric]\n                                     for x in summ]) for metric in summ[0]}\n    metrics_string = "" ; "".join(""{}: {:05.3f}"".format(k, v)\n                                for k, v in metrics_mean.items())\n    logging.info(""- Train metrics: "" + metrics_string)\n\n\ndef train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, restore_file=None):\n    """"""Train the model and evaluate every epoch.\n\n    Args:\n        model: (torch.nn.Module) the neural network\n        train_data: (dict) training data with keys \'data\' and \'labels\'\n        val_data: (dict) validaion data with keys \'data\' and \'labels\'\n        optimizer: (torch.optim) optimizer for parameters of model\n        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n        params: (Params) hyperparameters\n        model_dir: (string) directory containing config, weights and log\n        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n    """"""\n    # reload weights from restore_file if specified\n    if restore_file is not None:\n        restore_path = os.path.join(\n            args.model_dir, args.restore_file + \'.pth.tar\')\n        logging.info(""Restoring parameters from {}"".format(restore_path))\n        utils.load_checkpoint(restore_path, model, optimizer)\n\n    best_val_acc = 0.0\n\n    for epoch in range(params.num_epochs):\n        # Run one epoch\n        logging.info(""Epoch {}/{}"".format(epoch + 1, params.num_epochs))\n\n        # compute number of batches in one epoch (one full pass over the training set)\n        num_steps = (params.train_size + 1) // params.batch_size\n        train_data_iterator = data_loader.data_iterator(\n            train_data, params, shuffle=True)\n        train(model, optimizer, loss_fn, train_data_iterator,\n              metrics, params, num_steps)\n\n        # Evaluate for one epoch on validation set\n        num_steps = (params.val_size + 1) // params.batch_size\n        val_data_iterator = data_loader.data_iterator(\n            val_data, params, shuffle=False)\n        val_metrics = evaluate(\n            model, loss_fn, val_data_iterator, metrics, params, num_steps)\n\n        val_acc = val_metrics[\'accuracy\']\n        is_best = val_acc >= best_val_acc\n\n        # Save weights\n        utils.save_checkpoint({\'epoch\': epoch + 1,\n                               \'state_dict\': model.state_dict(),\n                               \'optim_dict\': optimizer.state_dict()},\n                              is_best=is_best,\n                              checkpoint=model_dir)\n\n        # If best_eval, best_save_path\n        if is_best:\n            logging.info(""- Found new best accuracy"")\n            best_val_acc = val_acc\n\n            # Save best val metrics in a json file in the model directory\n            best_json_path = os.path.join(\n                model_dir, ""metrics_val_best_weights.json"")\n            utils.save_dict_to_json(val_metrics, best_json_path)\n\n        # Save latest val metrics in a json file in the model directory\n        last_json_path = os.path.join(\n            model_dir, ""metrics_val_last_weights.json"")\n        utils.save_dict_to_json(val_metrics, last_json_path)\n\n\nif __name__ == \'__main__\':\n\n    # Load the parameters from json file\n    args = parser.parse_args()\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(\n        json_path), ""No json configuration file found at {}"".format(json_path)\n    params = utils.Params(json_path)\n\n    # use GPU if available\n    params.cuda = torch.cuda.is_available()\n\n    # Set the random seed for reproducible experiments\n    torch.manual_seed(230)\n    if params.cuda:\n        torch.cuda.manual_seed(230)\n\n    # Set the logger\n    utils.set_logger(os.path.join(args.model_dir, \'train.log\'))\n\n    # Create the input data pipeline\n    logging.info(""Loading the datasets..."")\n\n    # load data\n    data_loader = DataLoader(args.data_dir, params)\n    data = data_loader.load_data([\'train\', \'val\'], args.data_dir)\n    train_data = data[\'train\']\n    val_data = data[\'val\']\n\n    # specify the train and val dataset sizes\n    params.train_size = train_data[\'size\']\n    params.val_size = val_data[\'size\']\n\n    logging.info(""- done."")\n\n    # Define the model and optimizer\n    model = net.Net(params).cuda() if params.cuda else net.Net(params)\n    optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n\n    # fetch loss function and metrics\n    loss_fn = net.loss_fn\n    metrics = net.metrics\n\n    # Train the model\n    logging.info(""Starting training for {} epoch(s)"".format(params.num_epochs))\n    train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, args.model_dir,\n                       args.restore_file)\n'"
pytorch/nlp/utils.py,4,"b'import json\nimport logging\nimport os\nimport shutil\n\nimport torch\n\n\nclass Params():\n    """"""Class that loads hyperparameters from a json file.\n\n    Example:\n    ```\n    params = Params(json_path)\n    print(params.learning_rate)\n    params.learning_rate = 0.5  # change the value of learning_rate in params\n    ```\n    """"""\n\n    def __init__(self, json_path):\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    def save(self, json_path):\n        with open(json_path, \'w\') as f:\n            json.dump(self.__dict__, f, indent=4)\n\n    def update(self, json_path):\n        """"""Loads parameters from json file""""""\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    @property\n    def dict(self):\n        """"""Gives dict-like access to Params instance by `params.dict[\'learning_rate\']""""""\n        return self.__dict__\n\n\nclass RunningAverage():\n    """"""A simple class that maintains the running average of a quantity\n\n    Example:\n    ```\n    loss_avg = RunningAverage()\n    loss_avg.update(2)\n    loss_avg.update(4)\n    loss_avg() = 3\n    ```\n    """"""\n\n    def __init__(self):\n        self.steps = 0\n        self.total = 0\n\n    def update(self, val):\n        self.total += val\n        self.steps += 1\n\n    def __call__(self):\n        return self.total / float(self.steps)\n\n\ndef set_logger(log_path):\n    """"""Set the logger to log info in terminal and file `log_path`.\n\n    In general, it is useful to have a logger so that every output to the terminal is saved\n    in a permanent file. Here we save it to `model_dir/train.log`.\n\n    Example:\n    ```\n    logging.info(""Starting training..."")\n    ```\n\n    Args:\n        log_path: (string) where to log\n    """"""\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    if not logger.handlers:\n        # Logging to a file\n        file_handler = logging.FileHandler(log_path)\n        file_handler.setFormatter(logging.Formatter(\'%(asctime)s:%(levelname)s: %(message)s\'))\n        logger.addHandler(file_handler)\n\n        # Logging to console\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(logging.Formatter(\'%(message)s\'))\n        logger.addHandler(stream_handler)\n\n\ndef save_dict_to_json(d, json_path):\n    """"""Saves dict of floats in json file\n\n    Args:\n        d: (dict) of float-castable values (np.float, int, float, etc.)\n        json_path: (string) path to json file\n    """"""\n    with open(json_path, \'w\') as f:\n        # We need to convert the values to float for json (it doesn\'t accept np.array, np.float, )\n        d = {k: float(v) for k, v in d.items()}\n        json.dump(d, f, indent=4)\n\n\ndef save_checkpoint(state, is_best, checkpoint):\n    """"""Saves model and training parameters at checkpoint + \'last.pth.tar\'. If is_best==True, also saves\n    checkpoint + \'best.pth.tar\'\n\n    Args:\n        state: (dict) contains model\'s state_dict, may contain other keys such as epoch, optimizer state_dict\n        is_best: (bool) True if it is the best model seen till now\n        checkpoint: (string) folder where parameters are to be saved\n    """"""\n    filepath = os.path.join(checkpoint, \'last.pth.tar\')\n    if not os.path.exists(checkpoint):\n        print(""Checkpoint Directory does not exist! Making directory {}"".format(checkpoint))\n        os.mkdir(checkpoint)\n    else:\n        print(""Checkpoint Directory exists! "")\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'best.pth.tar\'))\n\n\ndef load_checkpoint(checkpoint, model, optimizer=None):\n    """"""Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n    optimizer assuming it is present in checkpoint.\n\n    Args:\n        checkpoint: (string) filename which needs to be loaded\n        model: (torch.nn.Module) model for which the parameters are loaded\n        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n    """"""\n    if not os.path.exists(checkpoint):\n        raise (""File doesn\'t exist {}"".format(checkpoint))\n    checkpoint = torch.load(checkpoint)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n\n    if optimizer:\n        optimizer.load_state_dict(checkpoint[\'optim_dict\'])\n\n    return checkpoint'"
pytorch/vision/build_dataset.py,0,"b'""""""Split the SIGNS dataset into train/val/test and resize images to 64x64.\n\nThe SIGNS dataset comes into the following format:\n    train_signs/\n        0_IMG_5864.jpg\n        ...\n    test_signs/\n        0_IMG_5942.jpg\n        ...\n\nOriginal images have size (3024, 3024).\nResizing to (64, 64) reduces the dataset size from 1.16 GB to 4.7 MB, and loading smaller images\nmakes training faster.\n\nWe already have a test set created, so we only need to split ""train_signs"" into train and val sets.\nBecause we don\'t have a lot of images and we want that the statistics on the val set be as\nrepresentative as possible, we\'ll take 20% of ""train_signs"" as val set.\n""""""\n\nimport argparse\nimport random\nimport os\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\nSIZE = 64\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--data_dir\', default=\'data/SIGNS\', help=""Directory with the SIGNS dataset"")\nparser.add_argument(\'--output_dir\', default=\'data/64x64_SIGNS\', help=""Where to write the new data"")\n\n\ndef resize_and_save(filename, output_dir, size=SIZE):\n    """"""Resize the image contained in `filename` and save it to the `output_dir`""""""\n    image = Image.open(filename)\n    # Use bilinear interpolation instead of the default ""nearest neighbor"" method\n    image = image.resize((size, size), Image.BILINEAR)\n    image.save(os.path.join(output_dir, filename.split(\'/\')[-1]))\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n\n    assert os.path.isdir(args.data_dir), ""Couldn\'t find the dataset at {}"".format(args.data_dir)\n\n    # Define the data directories\n    train_data_dir = os.path.join(args.data_dir, \'train_signs\')\n    test_data_dir = os.path.join(args.data_dir, \'test_signs\')\n\n    # Get the filenames in each directory (train and test)\n    filenames = os.listdir(train_data_dir)\n    filenames = [os.path.join(train_data_dir, f) for f in filenames if f.endswith(\'.jpg\')]\n\n    test_filenames = os.listdir(test_data_dir)\n    test_filenames = [os.path.join(test_data_dir, f) for f in test_filenames if f.endswith(\'.jpg\')]\n\n    # Split the images in \'train_signs\' into 80% train and 20% val\n    # Make sure to always shuffle with a fixed seed so that the split is reproducible\n    random.seed(230)\n    filenames.sort()\n    random.shuffle(filenames)\n\n    split = int(0.8 * len(filenames))\n    train_filenames = filenames[:split]\n    val_filenames = filenames[split:]\n\n    filenames = {\'train\': train_filenames,\n                 \'val\': val_filenames,\n                 \'test\': test_filenames}\n\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n    else:\n        print(""Warning: output dir {} already exists"".format(args.output_dir))\n\n    # Preprocess train, val and test\n    for split in [\'train\', \'val\', \'test\']:\n        output_dir_split = os.path.join(args.output_dir, \'{}_signs\'.format(split))\n        if not os.path.exists(output_dir_split):\n            os.mkdir(output_dir_split)\n        else:\n            print(""Warning: dir {} already exists"".format(output_dir_split))\n\n        print(""Processing {} data, saving preprocessed data to {}"".format(split, output_dir_split))\n        for filename in tqdm(filenames[split]):\n            resize_and_save(filename, output_dir_split, size=SIZE)\n\n    print(""Done building dataset"")\n'"
pytorch/vision/evaluate.py,6,"b'""""""Evaluates the model""""""\n\nimport argparse\nimport logging\nimport os\n\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nimport utils\nimport model.net as net\nimport model.data_loader as data_loader\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--data_dir\', default=\'data/64x64_SIGNS\',\n                    help=""Directory containing the dataset"")\nparser.add_argument(\'--model_dir\', default=\'experiments/base_model\',\n                    help=""Directory containing params.json"")\nparser.add_argument(\'--restore_file\', default=\'best\', help=""name of the file in --model_dir \\\n                     containing weights to load"")\n\n\ndef evaluate(model, loss_fn, dataloader, metrics, params):\n    """"""Evaluate the model on `num_steps` batches.\n\n    Args:\n        model: (torch.nn.Module) the neural network\n        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches data\n        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n        params: (Params) hyperparameters\n        num_steps: (int) number of batches to train on, each of size params.batch_size\n    """"""\n\n    # set model to evaluation mode\n    model.eval()\n\n    # summary for current eval loop\n    summ = []\n\n    # compute metrics over the dataset\n    for data_batch, labels_batch in dataloader:\n\n        # move to GPU if available\n        if params.cuda:\n            data_batch, labels_batch = data_batch.cuda(\n                non_blocking=True), labels_batch.cuda(non_blocking=True)\n        # fetch the next evaluation batch\n        data_batch, labels_batch = Variable(data_batch), Variable(labels_batch)\n\n        # compute model output\n        output_batch = model(data_batch)\n        loss = loss_fn(output_batch, labels_batch)\n\n        # extract data from torch Variable, move to cpu, convert to numpy arrays\n        output_batch = output_batch.data.cpu().numpy()\n        labels_batch = labels_batch.data.cpu().numpy()\n\n        # compute all metrics on this batch\n        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n                         for metric in metrics}\n        summary_batch[\'loss\'] = loss.item()\n        summ.append(summary_batch)\n\n    # compute mean of all metrics in summary\n    metrics_mean = {metric: np.mean([x[metric]\n                                     for x in summ]) for metric in summ[0]}\n    metrics_string = "" ; "".join(""{}: {:05.3f}"".format(k, v)\n                                for k, v in metrics_mean.items())\n    logging.info(""- Eval metrics : "" + metrics_string)\n    return metrics_mean\n\n\nif __name__ == \'__main__\':\n    """"""\n        Evaluate the model on the test set.\n    """"""\n    # Load the parameters\n    args = parser.parse_args()\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(\n        json_path), ""No json configuration file found at {}"".format(json_path)\n    params = utils.Params(json_path)\n\n    # use GPU if available\n    params.cuda = torch.cuda.is_available()     # use GPU is available\n\n    # Set the random seed for reproducible experiments\n    torch.manual_seed(230)\n    if params.cuda:\n        torch.cuda.manual_seed(230)\n\n    # Get the logger\n    utils.set_logger(os.path.join(args.model_dir, \'evaluate.log\'))\n\n    # Create the input data pipeline\n    logging.info(""Creating the dataset..."")\n\n    # fetch dataloaders\n    dataloaders = data_loader.fetch_dataloader([\'test\'], args.data_dir, params)\n    test_dl = dataloaders[\'test\']\n\n    logging.info(""- done."")\n\n    # Define the model\n    model = net.Net(params).cuda() if params.cuda else net.Net(params)\n\n    loss_fn = net.loss_fn\n    metrics = net.metrics\n\n    logging.info(""Starting evaluation"")\n\n    # Reload weights from the saved file\n    utils.load_checkpoint(os.path.join(\n        args.model_dir, args.restore_file + \'.pth.tar\'), model)\n\n    # Evaluate\n    test_metrics = evaluate(model, loss_fn, test_dl, metrics, params)\n    save_path = os.path.join(\n        args.model_dir, ""metrics_test_{}.json"".format(args.restore_file))\n    utils.save_dict_to_json(test_metrics, save_path)\n'"
pytorch/vision/search_hyperparams.py,0,"b'""""""Peform hyperparemeters search""""""\n\nimport argparse\nimport os\nfrom subprocess import check_call\nimport sys\n\nimport utils\n\n\nPYTHON = sys.executable\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--parent_dir\', default=\'experiments/learning_rate\',\n                    help=\'Directory containing params.json\')\nparser.add_argument(\'--data_dir\', default=\'data/64x64_SIGNS\', help=""Directory containing the dataset"")\n\n\ndef launch_training_job(parent_dir, data_dir, job_name, params):\n    """"""Launch training of the model with a set of hyperparameters in parent_dir/job_name\n\n    Args:\n        model_dir: (string) directory containing config, weights and log\n        data_dir: (string) directory containing the dataset\n        params: (dict) containing hyperparameters\n    """"""\n    # Create a new folder in parent_dir with unique_name ""job_name""\n    model_dir = os.path.join(parent_dir, job_name)\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    # Write parameters in json file\n    json_path = os.path.join(model_dir, \'params.json\')\n    params.save(json_path)\n\n    # Launch training with this config\n    cmd = ""{python} train.py --model_dir={model_dir} --data_dir {data_dir}"".format(python=PYTHON, model_dir=model_dir,\n                                                                                   data_dir=data_dir)\n    print(cmd)\n    check_call(cmd, shell=True)\n\n\nif __name__ == ""__main__"":\n    # Load the ""reference"" parameters from parent_dir json file\n    args = parser.parse_args()\n    json_path = os.path.join(args.parent_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = utils.Params(json_path)\n\n    # Perform hypersearch over one parameter\n    learning_rates = [1e-4, 1e-3, 1e-2]\n\n    for learning_rate in learning_rates:\n        # Modify the relevant parameter in params\n        params.learning_rate = learning_rate\n\n        # Launch job (name has to be unique)\n        job_name = ""learning_rate_{}"".format(learning_rate)\n        launch_training_job(args.parent_dir, args.data_dir, job_name, params)\n'"
pytorch/vision/synthesize_results.py,0,"b'""""""Aggregates results from the metrics_eval_best_weights.json in a parent folder""""""\n\nimport argparse\nimport json\nimport os\n\nfrom tabulate import tabulate\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--parent_dir\', default=\'experiments\',\n                    help=\'Directory containing results of experiments\')\n\n\ndef aggregate_metrics(parent_dir, metrics):\n    """"""Aggregate the metrics of all experiments in folder `parent_dir`.\n\n    Assumes that `parent_dir` contains multiple experiments, with their results stored in\n    `parent_dir/subdir/metrics_dev.json`\n\n    Args:\n        parent_dir: (string) path to directory containing experiments results\n        metrics: (dict) subdir -> {\'accuracy\': ..., ...}\n    """"""\n    # Get the metrics for the folder if it has results from an experiment\n    metrics_file = os.path.join(parent_dir, \'metrics_val_best_weights.json\')\n    if os.path.isfile(metrics_file):\n        with open(metrics_file, \'r\') as f:\n            metrics[parent_dir] = json.load(f)\n\n    # Check every subdirectory of parent_dir\n    for subdir in os.listdir(parent_dir):\n        if not os.path.isdir(os.path.join(parent_dir, subdir)):\n            continue\n        else:\n            aggregate_metrics(os.path.join(parent_dir, subdir), metrics)\n\n\ndef metrics_to_table(metrics):\n    # Get the headers from the first subdir. Assumes everything has the same metrics\n    headers = metrics[list(metrics.keys())[0]].keys()\n    table = [[subdir] + [values[h] for h in headers] for subdir, values in metrics.items()]\n    res = tabulate(table, headers, tablefmt=\'pipe\')\n\n    return res\n\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n\n    # Aggregate metrics from args.parent_dir directory\n    metrics = dict()\n    aggregate_metrics(args.parent_dir, metrics)\n    table = metrics_to_table(metrics)\n\n    # Display the table to terminal\n    print(table)\n\n    # Save results in parent_dir/results.md\n    save_file = os.path.join(args.parent_dir, ""results.md"")\n    with open(save_file, \'w\') as f:\n        f.write(table)'"
pytorch/vision/train.py,12,"b'""""""Train the model""""""\n\nimport argparse\nimport logging\nimport os\n\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom tqdm import tqdm\n\nimport utils\nimport model.net as net\nimport model.data_loader as data_loader\nfrom evaluate import evaluate\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--data_dir\', default=\'data/64x64_SIGNS\',\n                    help=""Directory containing the dataset"")\nparser.add_argument(\'--model_dir\', default=\'experiments/base_model\',\n                    help=""Directory containing params.json"")\nparser.add_argument(\'--restore_file\', default=None,\n                    help=""Optional, name of the file in --model_dir containing weights to reload before \\\n                    training"")  # \'best\' or \'train\'\n\n\ndef train(model, optimizer, loss_fn, dataloader, metrics, params):\n    """"""Train the model on `num_steps` batches\n\n    Args:\n        model: (torch.nn.Module) the neural network\n        optimizer: (torch.optim) optimizer for parameters of model\n        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n        params: (Params) hyperparameters\n        num_steps: (int) number of batches to train on, each of size params.batch_size\n    """"""\n\n    # set model to training mode\n    model.train()\n\n    # summary for current training loop and a running average object for loss\n    summ = []\n    loss_avg = utils.RunningAverage()\n\n    # Use tqdm for progress bar\n    with tqdm(total=len(dataloader)) as t:\n        for i, (train_batch, labels_batch) in enumerate(dataloader):\n            # move to GPU if available\n            if params.cuda:\n                train_batch, labels_batch = train_batch.cuda(\n                    non_blocking=True), labels_batch.cuda(non_blocking=True)\n            # convert to torch Variables\n            train_batch, labels_batch = Variable(\n                train_batch), Variable(labels_batch)\n\n            # compute model output and loss\n            output_batch = model(train_batch)\n            loss = loss_fn(output_batch, labels_batch)\n\n            # clear previous gradients, compute gradients of all variables wrt loss\n            optimizer.zero_grad()\n            loss.backward()\n\n            # performs updates using calculated gradients\n            optimizer.step()\n\n            # Evaluate summaries only once in a while\n            if i % params.save_summary_steps == 0:\n                # extract data from torch Variable, move to cpu, convert to numpy arrays\n                output_batch = output_batch.data.cpu().numpy()\n                labels_batch = labels_batch.data.cpu().numpy()\n\n                # compute all metrics on this batch\n                summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n                                 for metric in metrics}\n                summary_batch[\'loss\'] = loss.item()\n                summ.append(summary_batch)\n\n            # update the average loss\n            loss_avg.update(loss.item())\n\n            t.set_postfix(loss=\'{:05.3f}\'.format(loss_avg()))\n            t.update()\n\n    # compute mean of all metrics in summary\n    metrics_mean = {metric: np.mean([x[metric]\n                                     for x in summ]) for metric in summ[0]}\n    metrics_string = "" ; "".join(""{}: {:05.3f}"".format(k, v)\n                                for k, v in metrics_mean.items())\n    logging.info(""- Train metrics: "" + metrics_string)\n\n\ndef train_and_evaluate(model, train_dataloader, val_dataloader, optimizer, loss_fn, metrics, params, model_dir,\n                       restore_file=None):\n    """"""Train the model and evaluate every epoch.\n\n    Args:\n        model: (torch.nn.Module) the neural network\n        train_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n        val_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches validation data\n        optimizer: (torch.optim) optimizer for parameters of model\n        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n        params: (Params) hyperparameters\n        model_dir: (string) directory containing config, weights and log\n        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n    """"""\n    # reload weights from restore_file if specified\n    if restore_file is not None:\n        restore_path = os.path.join(\n            args.model_dir, args.restore_file + \'.pth.tar\')\n        logging.info(""Restoring parameters from {}"".format(restore_path))\n        utils.load_checkpoint(restore_path, model, optimizer)\n\n    best_val_acc = 0.0\n\n    for epoch in range(params.num_epochs):\n        # Run one epoch\n        logging.info(""Epoch {}/{}"".format(epoch + 1, params.num_epochs))\n\n        # compute number of batches in one epoch (one full pass over the training set)\n        train(model, optimizer, loss_fn, train_dataloader, metrics, params)\n\n        # Evaluate for one epoch on validation set\n        val_metrics = evaluate(model, loss_fn, val_dataloader, metrics, params)\n\n        val_acc = val_metrics[\'accuracy\']\n        is_best = val_acc >= best_val_acc\n\n        # Save weights\n        utils.save_checkpoint({\'epoch\': epoch + 1,\n                               \'state_dict\': model.state_dict(),\n                               \'optim_dict\': optimizer.state_dict()},\n                              is_best=is_best,\n                              checkpoint=model_dir)\n\n        # If best_eval, best_save_path\n        if is_best:\n            logging.info(""- Found new best accuracy"")\n            best_val_acc = val_acc\n\n            # Save best val metrics in a json file in the model directory\n            best_json_path = os.path.join(\n                model_dir, ""metrics_val_best_weights.json"")\n            utils.save_dict_to_json(val_metrics, best_json_path)\n\n        # Save latest val metrics in a json file in the model directory\n        last_json_path = os.path.join(\n            model_dir, ""metrics_val_last_weights.json"")\n        utils.save_dict_to_json(val_metrics, last_json_path)\n\n\nif __name__ == \'__main__\':\n\n    # Load the parameters from json file\n    args = parser.parse_args()\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(\n        json_path), ""No json configuration file found at {}"".format(json_path)\n    params = utils.Params(json_path)\n\n    # use GPU if available\n    params.cuda = torch.cuda.is_available()\n\n    # Set the random seed for reproducible experiments\n    torch.manual_seed(230)\n    if params.cuda:\n        torch.cuda.manual_seed(230)\n\n    # Set the logger\n    utils.set_logger(os.path.join(args.model_dir, \'train.log\'))\n\n    # Create the input data pipeline\n    logging.info(""Loading the datasets..."")\n\n    # fetch dataloaders\n    dataloaders = data_loader.fetch_dataloader(\n        [\'train\', \'val\'], args.data_dir, params)\n    train_dl = dataloaders[\'train\']\n    val_dl = dataloaders[\'val\']\n\n    logging.info(""- done."")\n\n    # Define the model and optimizer\n    model = net.Net(params).cuda() if params.cuda else net.Net(params)\n    optimizer = optim.Adam(model.parameters(), lr=params.learning_rate)\n\n    # fetch loss function and metrics\n    loss_fn = net.loss_fn\n    metrics = net.metrics\n\n    # Train the model\n    logging.info(""Starting training for {} epoch(s)"".format(params.num_epochs))\n    train_and_evaluate(model, train_dl, val_dl, optimizer, loss_fn, metrics, params, args.model_dir,\n                       args.restore_file)\n'"
pytorch/vision/utils.py,4,"b'import json\nimport logging\nimport os\nimport shutil\n\nimport torch\n\nclass Params():\n    """"""Class that loads hyperparameters from a json file.\n\n    Example:\n    ```\n    params = Params(json_path)\n    print(params.learning_rate)\n    params.learning_rate = 0.5  # change the value of learning_rate in params\n    ```\n    """"""\n\n    def __init__(self, json_path):\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    def save(self, json_path):\n        with open(json_path, \'w\') as f:\n            json.dump(self.__dict__, f, indent=4)\n            \n    def update(self, json_path):\n        """"""Loads parameters from json file""""""\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    @property\n    def dict(self):\n        """"""Gives dict-like access to Params instance by `params.dict[\'learning_rate\']""""""\n        return self.__dict__\n\n\nclass RunningAverage():\n    """"""A simple class that maintains the running average of a quantity\n    \n    Example:\n    ```\n    loss_avg = RunningAverage()\n    loss_avg.update(2)\n    loss_avg.update(4)\n    loss_avg() = 3\n    ```\n    """"""\n    def __init__(self):\n        self.steps = 0\n        self.total = 0\n    \n    def update(self, val):\n        self.total += val\n        self.steps += 1\n    \n    def __call__(self):\n        return self.total/float(self.steps)\n        \n    \ndef set_logger(log_path):\n    """"""Set the logger to log info in terminal and file `log_path`.\n\n    In general, it is useful to have a logger so that every output to the terminal is saved\n    in a permanent file. Here we save it to `model_dir/train.log`.\n\n    Example:\n    ```\n    logging.info(""Starting training..."")\n    ```\n\n    Args:\n        log_path: (string) where to log\n    """"""\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    if not logger.handlers:\n        # Logging to a file\n        file_handler = logging.FileHandler(log_path)\n        file_handler.setFormatter(logging.Formatter(\'%(asctime)s:%(levelname)s: %(message)s\'))\n        logger.addHandler(file_handler)\n\n        # Logging to console\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(logging.Formatter(\'%(message)s\'))\n        logger.addHandler(stream_handler)\n\n\ndef save_dict_to_json(d, json_path):\n    """"""Saves dict of floats in json file\n\n    Args:\n        d: (dict) of float-castable values (np.float, int, float, etc.)\n        json_path: (string) path to json file\n    """"""\n    with open(json_path, \'w\') as f:\n        # We need to convert the values to float for json (it doesn\'t accept np.array, np.float, )\n        d = {k: float(v) for k, v in d.items()}\n        json.dump(d, f, indent=4)\n\n\ndef save_checkpoint(state, is_best, checkpoint):\n    """"""Saves model and training parameters at checkpoint + \'last.pth.tar\'. If is_best==True, also saves\n    checkpoint + \'best.pth.tar\'\n\n    Args:\n        state: (dict) contains model\'s state_dict, may contain other keys such as epoch, optimizer state_dict\n        is_best: (bool) True if it is the best model seen till now\n        checkpoint: (string) folder where parameters are to be saved\n    """"""\n    filepath = os.path.join(checkpoint, \'last.pth.tar\')\n    if not os.path.exists(checkpoint):\n        print(""Checkpoint Directory does not exist! Making directory {}"".format(checkpoint))\n        os.mkdir(checkpoint)\n    else:\n        print(""Checkpoint Directory exists! "")\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'best.pth.tar\'))\n\n\ndef load_checkpoint(checkpoint, model, optimizer=None):\n    """"""Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n    optimizer assuming it is present in checkpoint.\n\n    Args:\n        checkpoint: (string) filename which needs to be loaded\n        model: (torch.nn.Module) model for which the parameters are loaded\n        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n    """"""\n    if not os.path.exists(checkpoint):\n        raise(""File doesn\'t exist {}"".format(checkpoint))\n    checkpoint = torch.load(checkpoint)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n\n    if optimizer:\n        optimizer.load_state_dict(checkpoint[\'optim_dict\'])\n\n    return checkpoint'"
tensorflow/nlp/build_kaggle_dataset.py,0,"b'""""""Read, split and save the kaggle dataset for our model""""""\n\nimport csv\nimport os\nimport sys\n\n\ndef load_dataset(path_csv):\n    """"""Loads dataset into memory from csv file""""""\n    # Open the csv file, need to specify the encoding for python3\n    use_python3 = sys.version_info[0] >= 3\n    with (open(path_csv, encoding=""windows-1252"") if use_python3 else open(path_csv)) as f:\n        csv_file = csv.reader(f, delimiter=\',\')\n        dataset = []\n        words, tags = [], []\n\n        # Each line of the csv corresponds to one word\n        for idx, row in enumerate(csv_file):\n            if idx == 0: continue\n            sentence, word, pos, tag = row\n            # If the first column is non empty it means we reached a new sentence\n            if len(sentence) != 0:\n                if len(words) > 0:\n                    assert len(words) == len(tags)\n                    dataset.append((words, tags))\n                    words, tags = [], []\n            try:\n                word, tag = str(word), str(tag)\n                words.append(word)\n                tags.append(tag)\n            except UnicodeDecodeError as e:\n                print(""An exception was raised, skipping a word: {}"".format(e))\n                pass\n\n    return dataset\n\n\ndef save_dataset(dataset, save_dir):\n    """"""Writes sentences.txt and labels.txt files in save_dir from dataset\n\n    Args:\n        dataset: ([([""a"", ""cat""], [""O"", ""O""]), ...])\n        save_dir: (string)\n    """"""\n    # Create directory if it doesn\'t exist\n    print(""Saving in {}..."".format(save_dir))\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    # Export the dataset\n    with open(os.path.join(save_dir, \'sentences.txt\'), \'w\') as file_sentences:\n        with open(os.path.join(save_dir, \'labels.txt\'), \'w\') as file_labels:\n            for words, tags in dataset:\n                file_sentences.write(""{}\\n"".format("" "".join(words)))\n                file_labels.write(""{}\\n"".format("" "".join(tags)))\n    print(""- done."")\n\n\nif __name__ == ""__main__"":\n    # Check that the dataset exists (you need to make sure you haven\'t downloaded the `ner.csv`)\n    path_dataset = \'data/kaggle/ner_dataset.csv\'\n    msg = ""{} file not found. Make sure you have downloaded the right dataset"".format(path_dataset)\n    assert os.path.isfile(path_dataset), msg\n\n    # Load the dataset into memory\n    print(""Loading Kaggle dataset into memory..."")\n    dataset = load_dataset(path_dataset)\n    print(""- done."")\n\n    # Split the dataset into train, dev and split (dummy split with no shuffle)\n    train_dataset = dataset[:int(0.7*len(dataset))]\n    dev_dataset = dataset[int(0.7*len(dataset)) : int(0.85*len(dataset))]\n    test_dataset = dataset[int(0.85*len(dataset)):]\n\n    # Save the datasets to files\n    save_dataset(train_dataset, \'data/kaggle/train\')\n    save_dataset(dev_dataset, \'data/kaggle/dev\')\n    save_dataset(test_dataset, \'data/kaggle/test\')'"
tensorflow/nlp/build_vocab.py,0,"b'""""""Build vocabularies of words and tags from datasets""""""\n\nimport argparse\nfrom collections import Counter\nimport json\nimport os\nimport sys\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--min_count_word\', default=1, help=""Minimum count for words in the dataset"",\n                    type=int)\nparser.add_argument(\'--min_count_tag\', default=1, help=""Minimum count for tags in the dataset"",\n                    type=int)\nparser.add_argument(\'--data_dir\', default=\'data/small\', help=""Directory containing the dataset"")\n\n# Hyper parameters for the vocab\nNUM_OOV_BUCKETS = 1 # number of buckets (= number of ids) for unknown words\nPAD_WORD = \'<pad>\'\nPAD_TAG = \'O\'\n\n\ndef save_vocab_to_txt_file(vocab, txt_path):\n    """"""Writes one token per line, 0-based line id corresponds to the id of the token.\n\n    Args:\n        vocab: (iterable object) yields token\n        txt_path: (stirng) path to vocab file\n    """"""\n    with open(txt_path, ""w"") as f:\n        f.write(""\\n"".join(token for token in vocab))\n\n\ndef save_dict_to_json(d, json_path):\n    """"""Saves dict to json file\n\n    Args:\n        d: (dict)\n        json_path: (string) path to json file\n    """"""\n    with open(json_path, \'w\') as f:\n        d = {k: v for k, v in d.items()}\n        json.dump(d, f, indent=4)\n\n\ndef update_vocab(txt_path, vocab):\n    """"""Update word and tag vocabulary from dataset\n\n    Args:\n        txt_path: (string) path to file, one sentence per line\n        vocab: (dict or Counter) with update method\n\n    Returns:\n        dataset_size: (int) number of elements in the dataset\n    """"""\n    with open(txt_path) as f:\n        for i, line in enumerate(f):\n            vocab.update(line.strip().split(\' \'))\n\n\n    return i + 1\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n\n    # Build word vocab with train and test datasets\n    print(""Building word vocabulary..."")\n    words = Counter()\n    size_train_sentences = update_vocab(os.path.join(args.data_dir, \'train/sentences.txt\'), words)\n    size_dev_sentences = update_vocab(os.path.join(args.data_dir, \'dev/sentences.txt\'), words)\n    size_test_sentences = update_vocab(os.path.join(args.data_dir, \'test/sentences.txt\'), words)\n    print(""- done."")\n\n    # Build tag vocab with train and test datasets\n    print(""Building tag vocabulary..."")\n    tags = Counter()\n    size_train_tags = update_vocab(os.path.join(args.data_dir, \'train/labels.txt\'), tags)\n    size_dev_tags = update_vocab(os.path.join(args.data_dir, \'dev/labels.txt\'), tags)\n    size_test_tags = update_vocab(os.path.join(args.data_dir, \'test/labels.txt\'), tags)\n    print(""- done."")\n\n    # Assert same number of examples in datasets\n    assert size_train_sentences == size_train_tags\n    assert size_dev_sentences == size_dev_tags\n    assert size_test_sentences == size_test_tags\n\n    # Only keep most frequent tokens\n    words = [tok for tok, count in words.items() if count >= args.min_count_word]\n    tags = [tok for tok, count in tags.items() if count >= args.min_count_tag]\n\n    # Add pad tokens\n    if PAD_WORD not in words: words.append(PAD_WORD)\n    if PAD_TAG not in tags: tags.append(PAD_TAG)\n\n    # Save vocabularies to file\n    print(""Saving vocabularies to file..."")\n    save_vocab_to_txt_file(words, os.path.join(args.data_dir, \'words.txt\'))\n    save_vocab_to_txt_file(tags, os.path.join(args.data_dir, \'tags.txt\'))\n    print(""- done."")\n\n    # Save datasets properties in json file\n    sizes = {\n        \'train_size\': size_train_sentences,\n        \'dev_size\': size_dev_sentences,\n        \'test_size\': size_test_sentences,\n        \'vocab_size\': len(words) + NUM_OOV_BUCKETS,\n        \'number_of_tags\': len(tags),\n        \'pad_word\': PAD_WORD,\n        \'pad_tag\': PAD_TAG,\n        \'num_oov_buckets\': NUM_OOV_BUCKETS\n    }\n    save_dict_to_json(sizes, os.path.join(args.data_dir, \'dataset_params.json\'))\n\n    # Logging sizes\n    to_print = ""\\n"".join(""- {}: {}"".format(k, v) for k, v in sizes.items())\n    print(""Characteristics of the dataset:\\n{}"".format(to_print))'"
tensorflow/nlp/evaluate.py,0,"b'""""""Evaluate the model""""""\n\nimport argparse\nimport logging\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom model.utils import Params\nfrom model.utils import set_logger\nfrom model.evaluation import evaluate\nfrom model.input_fn import input_fn\nfrom model.input_fn import load_dataset_from_text\nfrom model.model_fn import model_fn\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_dir\', default=\'experiments/base_model\',\n                    help=""Directory containing params.json"")\nparser.add_argument(\'--data_dir\', default=\'data/small\', help=""Directory containing the dataset"")\nparser.add_argument(\'--restore_from\', default=\'best_weights\',\n                    help=""Subdirectory of model dir or file containing the weights"")\n\nif __name__ == \'__main__\':\n    # Set the random seed for the whole graph\n    tf.set_random_seed(230)\n\n    # Load the parameters\n    args = parser.parse_args()\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = Params(json_path)\n\n    # Load the parameters from the dataset, that gives the size etc. into params\n    json_path = os.path.join(args.data_dir, \'dataset_params.json\')\n    assert os.path.isfile(json_path), ""No json file found at {}, run build.py"".format(json_path)\n    params.update(json_path)\n    num_oov_buckets = params.num_oov_buckets # number of buckets for unknown words\n\n    # Set the logger\n    set_logger(os.path.join(args.model_dir, \'evaluate.log\'))\n\n    # Get paths for vocabularies and dataset\n    path_words = os.path.join(args.data_dir, \'words.txt\')\n    path_tags = os.path.join(args.data_dir, \'tags.txt\')\n    path_eval_sentences = os.path.join(args.data_dir, \'dev/sentences.txt\')\n    path_eval_labels = os.path.join(args.data_dir, \'dev/labels.txt\')\n\n    # Load Vocabularies\n    words = tf.contrib.lookup.index_table_from_file(path_words, num_oov_buckets=num_oov_buckets)\n    tags = tf.contrib.lookup.index_table_from_file(path_tags)\n\n    # Create the input data pipeline\n    logging.info(""Creating the dataset..."")\n    test_sentences = load_dataset_from_text(path_eval_sentences, words)\n    test_labels = load_dataset_from_text(path_eval_labels, tags)\n\n    # Specify other parameters for the dataset and the model\n    params.eval_size = params.test_size\n    params.id_pad_word = words.lookup(tf.constant(params.pad_word))\n    params.id_pad_tag = tags.lookup(tf.constant(params.pad_tag))\n\n    # Create iterator over the test set\n    inputs = input_fn(\'eval\', test_sentences, test_labels, params)\n    logging.info(""- done."")\n\n    # Define the model\n    logging.info(""Creating the model..."")\n    model_spec = model_fn(\'eval\', inputs, params, reuse=False)\n    logging.info(""- done."")\n\n    logging.info(""Starting evaluation"")\n    evaluate(model_spec, args.model_dir, params, args.restore_from)\n'"
tensorflow/nlp/search_hyperparams.py,0,"b'""""""Peform hyperparemeters search""""""\n\nimport argparse\nimport os\nfrom subprocess import check_call\nimport sys\n\nfrom model.utils import Params\n\n\nPYTHON = sys.executable\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--parent_dir\', default=\'experiments/learning_rate\',\n                    help=""Directory containing params.json"")\nparser.add_argument(\'--data_dir\', default=\'data/small\',\n                    help=""Directory containing the dataset"")\n\n\ndef launch_training_job(parent_dir, data_dir, job_name, params):\n    """"""Launch training of the model with a set of hyperparameters in parent_dir/job_name\n\n    Args:\n        parent_dir: (string) directory containing config, weights and log\n        data_dir: (string) directory containing the dataset\n        params: (dict) containing hyperparameters\n    """"""\n    # Create a new folder in parent_dir with unique_name ""job_name""\n    model_dir = os.path.join(parent_dir, job_name)\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    # Write parameters in json file\n    json_path = os.path.join(model_dir, \'params.json\')\n    params.save(json_path)\n\n    # Launch training with this config\n    cmd = ""{python} train.py --model_dir {model_dir} --data_dir {data_dir}"".format(python=PYTHON,\n            model_dir=model_dir, data_dir=data_dir)\n    print(cmd)\n    check_call(cmd, shell=True)\n\n\nif __name__ == ""__main__"":\n    # Load the ""reference"" parameters from parent_dir json file\n    args = parser.parse_args()\n    json_path = os.path.join(args.parent_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = Params(json_path)\n\n    # Perform hypersearch over one parameter\n    learning_rates = [1e-4, 1e-3, 1e-2]\n\n    for learning_rate in learning_rates:\n        # Modify the relevant parameter in params\n        params.learning_rate = learning_rate\n\n        # Launch job (name has to be unique)\n        job_name = ""learning_rate_{}"".format(learning_rate)\n        launch_training_job(args.parent_dir, args.data_dir, job_name, params)\n'"
tensorflow/nlp/synthesize_results.py,0,"b'""""""Aggregates results from the metrics_eval_best_weights.json in a parent folder""""""\n\nimport argparse\nimport json\nimport os\n\nfrom tabulate import tabulate\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--parent_dir\', default=\'experiments\',\n                    help=\'Directory containing results of experiments\')\n\n\ndef aggregate_metrics(parent_dir, metrics):\n    """"""Aggregate the metrics of all experiments in folder `parent_dir`.\n\n    Assumes that `parent_dir` contains multiple experiments, with their results stored in\n    `parent_dir/subdir/metrics_dev.json`\n\n    Args:\n        parent_dir: (string) path to directory containing experiments results\n        metrics: (dict) subdir -> {\'accuracy\': ..., ...}\n    """"""\n    # Get the metrics for the folder if it has results from an experiment\n    metrics_file = os.path.join(parent_dir, \'metrics_eval_best_weights.json\')\n    if os.path.isfile(metrics_file):\n        with open(metrics_file, \'r\') as f:\n            metrics[parent_dir] = json.load(f)\n\n    # Check every subdirectory of parent_dir\n    for subdir in os.listdir(parent_dir):\n        if not os.path.isdir(os.path.join(parent_dir, subdir)):\n            continue\n        else:\n            aggregate_metrics(os.path.join(parent_dir, subdir), metrics)\n\n\ndef metrics_to_table(metrics):\n    # Get the headers from the first subdir. Assumes everything has the same metrics\n    headers = metrics[list(metrics.keys())[0]].keys()\n    table = [[subdir] + [values[h] for h in headers] for subdir, values in metrics.items()]\n    res = tabulate(table, headers, tablefmt=\'pipe\')\n\n    return res\n\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n\n    # Aggregate metrics from args.parent_dir directory\n    metrics = dict()\n    aggregate_metrics(args.parent_dir, metrics)\n    table = metrics_to_table(metrics)\n\n    # Display the table to terminal\n    print(table)\n\n    # Save results in parent_dir/results.md\n    save_file = os.path.join(args.parent_dir, ""results.md"")\n    with open(save_file, \'w\') as f:\n        f.write(table)\n'"
tensorflow/nlp/train.py,0,"b'""""""Train the model""""""\n\nimport argparse\nimport logging\nimport os\n\nimport tensorflow as tf\n\nfrom model.utils import Params\nfrom model.utils import set_logger\nfrom model.training import train_and_evaluate\nfrom model.input_fn import input_fn\nfrom model.input_fn import load_dataset_from_text\nfrom model.model_fn import model_fn\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_dir\', default=\'experiments/base_model\',\n                    help=""Directory containing params.json"")\nparser.add_argument(\'--data_dir\', default=\'data/small\', help=""Directory containing the dataset"")\nparser.add_argument(\'--restore_dir\', default=None,\n                    help=""Optional, directory containing weights to reload before training"")\n\n\nif __name__ == \'__main__\':\n    # Set the random seed for the whole graph for reproductible experiments\n    tf.set_random_seed(230)\n\n    # Load the parameters from the experiment params.json file in model_dir\n    args = parser.parse_args()\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = Params(json_path)\n\n    # Load the parameters from the dataset, that gives the size etc. into params\n    json_path = os.path.join(args.data_dir, \'dataset_params.json\')\n    assert os.path.isfile(json_path), ""No json file found at {}, run build_vocab.py"".format(json_path)\n    params.update(json_path)\n    num_oov_buckets = params.num_oov_buckets # number of buckets for unknown words\n\n    # Check that we are not overwriting some previous experiment\n    # Comment these lines if you are developing your model and don\'t care about overwritting\n    model_dir_has_best_weights = os.path.isdir(os.path.join(args.model_dir, ""best_weights""))\n    overwritting = model_dir_has_best_weights and args.restore_dir is None\n    assert not overwritting, ""Weights found in model_dir, aborting to avoid overwrite""\n\n    # Set the logger\n    set_logger(os.path.join(args.model_dir, \'train.log\'))\n\n    # Get paths for vocabularies and dataset\n    path_words = os.path.join(args.data_dir, \'words.txt\')\n    path_tags = os.path.join(args.data_dir, \'tags.txt\')\n    path_train_sentences = os.path.join(args.data_dir, \'train/sentences.txt\')\n    path_train_labels = os.path.join(args.data_dir, \'train/labels.txt\')\n    path_eval_sentences = os.path.join(args.data_dir, \'dev/sentences.txt\')\n    path_eval_labels = os.path.join(args.data_dir, \'dev/labels.txt\')\n\n    # Load Vocabularies\n    words = tf.contrib.lookup.index_table_from_file(path_words, num_oov_buckets=num_oov_buckets)\n    tags = tf.contrib.lookup.index_table_from_file(path_tags)\n\n    # Create the input data pipeline\n    logging.info(""Creating the datasets..."")\n    train_sentences = load_dataset_from_text(path_train_sentences, words)\n    train_labels = load_dataset_from_text(path_train_labels, tags)\n    eval_sentences = load_dataset_from_text(path_eval_sentences, words)\n    eval_labels = load_dataset_from_text(path_eval_labels, tags)\n\n    # Specify other parameters for the dataset and the model\n    params.eval_size = params.dev_size\n    params.buffer_size = params.train_size # buffer size for shuffling\n    params.id_pad_word = words.lookup(tf.constant(params.pad_word))\n    params.id_pad_tag = tags.lookup(tf.constant(params.pad_tag))\n\n    # Create the two iterators over the two datasets\n    train_inputs = input_fn(\'train\', train_sentences, train_labels, params)\n    eval_inputs = input_fn(\'eval\', eval_sentences, eval_labels, params)\n    logging.info(""- done."")\n\n    # Define the models (2 different set of nodes that share weights for train and eval)\n    logging.info(""Creating the model..."")\n    train_model_spec = model_fn(\'train\', train_inputs, params)\n    eval_model_spec = model_fn(\'eval\', eval_inputs, params, reuse=True)\n    logging.info(""- done."")\n\n    # Train the model\n    logging.info(""Starting training for {} epoch(s)"".format(params.num_epochs))\n    train_and_evaluate(train_model_spec, eval_model_spec, args.model_dir, params, args.restore_dir)'"
tensorflow/vision/build_dataset.py,0,"b'""""""Split the SIGNS dataset into train/dev/test and resize images to 64x64.\n\nThe SIGNS dataset comes in the following format:\n    train_signs/\n        0_IMG_5864.jpg\n        ...\n    test_signs/\n        0_IMG_5942.jpg\n        ...\n\nOriginal images have size (3024, 3024).\nResizing to (64, 64) reduces the dataset size from 1.16 GB to 4.7 MB, and loading smaller images\nmakes training faster.\n\nWe already have a test set created, so we only need to split ""train_signs"" into train and dev sets.\nBecause we don\'t have a lot of images and we want that the statistics on the dev set be as\nrepresentative as possible, we\'ll take 20% of ""train_signs"" as dev set.\n""""""\n\nimport argparse\nimport random\nimport os\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\n\nSIZE = 64\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--data_dir\', default=\'data/SIGNS\', help=""Directory with the SIGNS dataset"")\nparser.add_argument(\'--output_dir\', default=\'data/64x64_SIGNS\', help=""Where to write the new data"")\n\n\ndef resize_and_save(filename, output_dir, size=SIZE):\n    """"""Resize the image contained in `filename` and save it to the `output_dir`""""""\n    image = Image.open(filename)\n    # Use bilinear interpolation instead of the default ""nearest neighbor"" method\n    image = image.resize((size, size), Image.BILINEAR)\n    image.save(os.path.join(output_dir, filename.split(\'/\')[-1]))\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n\n    assert os.path.isdir(args.data_dir), ""Couldn\'t find the dataset at {}"".format(args.data_dir)\n\n    # Define the data directories\n    train_data_dir = os.path.join(args.data_dir, \'train_signs\')\n    test_data_dir = os.path.join(args.data_dir, \'test_signs\')\n\n    # Get the filenames in each directory (train and test)\n    filenames = os.listdir(train_data_dir)\n    filenames = [os.path.join(train_data_dir, f) for f in filenames if f.endswith(\'.jpg\')]\n\n    test_filenames = os.listdir(test_data_dir)\n    test_filenames = [os.path.join(test_data_dir, f) for f in test_filenames if f.endswith(\'.jpg\')]\n\n    # Split the images in \'train_signs\' into 80% train and 20% dev\n    # Make sure to always shuffle with a fixed seed so that the split is reproducible\n    random.seed(230)\n    filenames.sort()\n    random.shuffle(filenames)\n\n    split = int(0.8 * len(filenames))\n    train_filenames = filenames[:split]\n    dev_filenames = filenames[split:]\n\n    filenames = {\'train\': train_filenames,\n                 \'dev\': dev_filenames,\n                 \'test\': test_filenames}\n\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n    else:\n        print(""Warning: output dir {} already exists"".format(args.output_dir))\n\n    # Preprocess train, dev and test\n    for split in [\'train\', \'dev\', \'test\']:\n        output_dir_split = os.path.join(args.output_dir, \'{}_signs\'.format(split))\n        if not os.path.exists(output_dir_split):\n            os.mkdir(output_dir_split)\n        else:\n            print(""Warning: dir {} already exists"".format(output_dir_split))\n\n        print(""Processing {} data, saving preprocessed data to {}"".format(split, output_dir_split))\n        for filename in tqdm(filenames[split]):\n            resize_and_save(filename, output_dir_split, size=SIZE)\n\n    print(""Done building dataset"")\n'"
tensorflow/vision/evaluate.py,0,"b'""""""Evaluate the model""""""\n\nimport argparse\nimport logging\nimport os\n\nimport tensorflow as tf\n\nfrom model.input_fn import input_fn\nfrom model.model_fn import model_fn\nfrom model.evaluation import evaluate\nfrom model.utils import Params\nfrom model.utils import set_logger\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_dir\', default=\'experiments/test\',\n                    help=""Experiment directory containing params.json"")\nparser.add_argument(\'--data_dir\', default=\'data/64x64_SIGNS\',\n                    help=""Directory containing the dataset"")\nparser.add_argument(\'--restore_from\', default=\'best_weights\',\n                    help=""Subdirectory of model dir or file containing the weights"")\n\n\nif __name__ == \'__main__\':\n    # Set the random seed for the whole graph\n    tf.set_random_seed(230)\n\n    # Load the parameters\n    args = parser.parse_args()\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = Params(json_path)\n\n    # Set the logger\n    set_logger(os.path.join(args.model_dir, \'evaluate.log\'))\n\n    # Create the input data pipeline\n    logging.info(""Creating the dataset..."")\n    data_dir = args.data_dir\n    test_data_dir = os.path.join(data_dir, ""test_signs"")\n\n    # Get the filenames from the test set\n    test_filenames = os.listdir(test_data_dir)\n    test_filenames = [os.path.join(test_data_dir, f) for f in test_filenames if f.endswith(\'.jpg\')]\n\n    test_labels = [int(f.split(\'/\')[-1][0]) for f in test_filenames]\n\n    # specify the size of the evaluation set\n    params.eval_size = len(test_filenames)\n\n    # create the iterator over the dataset\n    test_inputs = input_fn(False, test_filenames, test_labels, params)\n\n    # Define the model\n    logging.info(""Creating the model..."")\n    model_spec = model_fn(\'eval\', test_inputs, params, reuse=False)\n\n    logging.info(""Starting evaluation"")\n    evaluate(model_spec, args.model_dir, params, args.restore_from)\n'"
tensorflow/vision/search_hyperparams.py,0,"b'""""""Peform hyperparemeters search""""""\n\nimport argparse\nimport os\nfrom subprocess import check_call\nimport sys\n\nfrom model.utils import Params\n\n\nPYTHON = sys.executable\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--parent_dir\', default=\'experiments/learning_rate\',\n                    help=""Directory containing params.json"")\nparser.add_argument(\'--data_dir\', default=\'data/64x64_SIGNS\',\n                    help=""Directory containing the dataset"")\n\n\ndef launch_training_job(parent_dir, data_dir, job_name, params):\n    """"""Launch training of the model with a set of hyperparameters in parent_dir/job_name\n\n    Args:\n        parent_dir: (string) directory containing config, weights and log\n        data_dir: (string) directory containing the dataset\n        params: (dict) containing hyperparameters\n    """"""\n    # Create a new folder in parent_dir with unique_name ""job_name""\n    model_dir = os.path.join(parent_dir, job_name)\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    # Write parameters in json file\n    json_path = os.path.join(model_dir, \'params.json\')\n    params.save(json_path)\n\n    # Launch training with this config\n    cmd = ""{python} train.py --model_dir {model_dir} --data_dir {data_dir}"".format(python=PYTHON,\n            model_dir=model_dir, data_dir=data_dir)\n    print(cmd)\n    check_call(cmd, shell=True)\n\n\nif __name__ == ""__main__"":\n    # Load the ""reference"" parameters from parent_dir json file\n    args = parser.parse_args()\n    json_path = os.path.join(args.parent_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = Params(json_path)\n\n    # Perform hypersearch over one parameter\n    learning_rates = [1e-4, 1e-3, 1e-2]\n\n    for learning_rate in learning_rates:\n        # Modify the relevant parameter in params\n        params.learning_rate = learning_rate\n\n        # Launch job (name has to be unique)\n        job_name = ""learning_rate_{}"".format(learning_rate)\n        launch_training_job(args.parent_dir, args.data_dir, job_name, params)\n'"
tensorflow/vision/synthesize_results.py,0,"b'""""""Aggregates results from the metrics_eval_best_weights.json in a parent folder""""""\n\nimport argparse\nimport json\nimport os\n\nfrom tabulate import tabulate\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--parent_dir\', default=\'experiments\',\n                    help=\'Directory containing results of experiments\')\n\n\ndef aggregate_metrics(parent_dir, metrics):\n    """"""Aggregate the metrics of all experiments in folder `parent_dir`.\n\n    Assumes that `parent_dir` contains multiple experiments, with their results stored in\n    `parent_dir/subdir/metrics_dev.json`\n\n    Args:\n        parent_dir: (string) path to directory containing experiments results\n        metrics: (dict) subdir -> {\'accuracy\': ..., ...}\n    """"""\n    # Get the metrics for the folder if it has results from an experiment\n    metrics_file = os.path.join(parent_dir, \'metrics_eval_best_weights.json\')\n    if os.path.isfile(metrics_file):\n        with open(metrics_file, \'r\') as f:\n            metrics[parent_dir] = json.load(f)\n\n    # Check every subdirectory of parent_dir\n    for subdir in os.listdir(parent_dir):\n        if not os.path.isdir(os.path.join(parent_dir, subdir)):\n            continue\n        else:\n            aggregate_metrics(os.path.join(parent_dir, subdir), metrics)\n\n\ndef metrics_to_table(metrics):\n    # Get the headers from the first subdir. Assumes everything has the same metrics\n    headers = metrics[list(metrics.keys())[0]].keys()\n    table = [[subdir] + [values[h] for h in headers] for subdir, values in metrics.items()]\n    res = tabulate(table, headers, tablefmt=\'pipe\')\n\n    return res\n\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n\n    # Aggregate metrics from args.parent_dir directory\n    metrics = dict()\n    aggregate_metrics(args.parent_dir, metrics)\n    table = metrics_to_table(metrics)\n\n    # Display the table to terminal\n    print(table)\n\n    # Save results in parent_dir/results.md\n    save_file = os.path.join(args.parent_dir, ""results.md"")\n    with open(save_file, \'w\') as f:\n        f.write(table)\n'"
tensorflow/vision/train.py,0,"b'""""""Train the model""""""\n\nimport argparse\nimport logging\nimport os\nimport random\n\nimport tensorflow as tf\n\nfrom model.input_fn import input_fn\nfrom model.utils import Params\nfrom model.utils import set_logger\nfrom model.utils import save_dict_to_json\nfrom model.model_fn import model_fn\nfrom model.training import train_and_evaluate\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_dir\', default=\'experiments/test\',\n                    help=""Experiment directory containing params.json"")\nparser.add_argument(\'--data_dir\', default=\'data/64x64_SIGNS\',\n                    help=""Directory containing the dataset"")\nparser.add_argument(\'--restore_from\', default=None,\n                    help=""Optional, directory or file containing weights to reload before training"")\n\n\nif __name__ == \'__main__\':\n    # Set the random seed for the whole graph for reproductible experiments\n    tf.set_random_seed(230)\n\n    # Load the parameters from json file\n    args = parser.parse_args()\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(\n        json_path), ""No json configuration file found at {}"".format(json_path)\n    params = Params(json_path)\n\n    # Check that we are not overwriting some previous experiment\n    # Comment these lines if you are developing your model and don\'t care about overwritting\n    model_dir_has_best_weights = os.path.isdir(\n        os.path.join(args.model_dir, ""best_weights""))\n    overwritting = model_dir_has_best_weights and args.restore_from is None\n    assert not overwritting, ""Weights found in model_dir, aborting to avoid overwrite""\n\n    # Set the logger\n    set_logger(os.path.join(args.model_dir, \'train.log\'))\n\n    # Create the input data pipeline\n    logging.info(""Creating the datasets..."")\n    data_dir = args.data_dir\n    train_data_dir = os.path.join(data_dir, ""train_signs"")\n    dev_data_dir = os.path.join(data_dir, ""dev_signs"")\n\n    # Get the filenames from the train and dev sets\n    train_filenames = [os.path.join(train_data_dir, f) for f in os.listdir(train_data_dir)\n                       if f.endswith(\'.jpg\')]\n    eval_filenames = [os.path.join(dev_data_dir, f) for f in os.listdir(dev_data_dir)\n                      if f.endswith(\'.jpg\')]\n\n    # Labels will be between 0 and 5 included (6 classes in total)\n    train_labels = [int(f.split(\'/\')[-1][0]) for f in train_filenames]\n    eval_labels = [int(f.split(\'/\')[-1][0]) for f in eval_filenames]\n\n    # Specify the sizes of the dataset we train on and evaluate on\n    params.train_size = len(train_filenames)\n    params.eval_size = len(eval_filenames)\n\n    # Create the two iterators over the two datasets\n    train_inputs = input_fn(True, train_filenames, train_labels, params)\n    eval_inputs = input_fn(False, eval_filenames, eval_labels, params)\n\n    # Define the model\n    logging.info(""Creating the model..."")\n    train_model_spec = model_fn(\'train\', train_inputs, params)\n    eval_model_spec = model_fn(\'eval\', eval_inputs, params, reuse=True)\n\n    # Train the model\n    logging.info(""Starting training for {} epoch(s)"".format(params.num_epochs))\n    train_and_evaluate(train_model_spec, eval_model_spec,\n                       args.model_dir, params, args.restore_from)\n'"
pytorch/nlp/model/__init__.py,0,b''
pytorch/nlp/model/data_loader.py,2,"b'import random\nimport numpy as np\nimport os\nimport sys\n\nimport torch\nfrom torch.autograd import Variable\n\nimport utils \n\n\nclass DataLoader(object):\n    """"""\n    Handles all aspects of the data. Stores the dataset_params, vocabulary and tags with their mappings to indices.\n    """"""\n    def __init__(self, data_dir, params):\n        """"""\n        Loads dataset_params, vocabulary and tags. Ensure you have run `build_vocab.py` on data_dir before using this\n        class.\n\n        Args:\n            data_dir: (string) directory containing the dataset\n            params: (Params) hyperparameters of the training process. This function modifies params and appends\n                    dataset_params (such as vocab size, num_of_tags etc.) to params.\n        """"""\n\n        # loading dataset_params\n        json_path = os.path.join(data_dir, \'dataset_params.json\')\n        assert os.path.isfile(json_path), ""No json file found at {}, run build_vocab.py"".format(json_path)\n        self.dataset_params = utils.Params(json_path)        \n        \n        # loading vocab (we require this to map words to their indices)\n        vocab_path = os.path.join(data_dir, \'words.txt\')\n        self.vocab = {}\n        with open(vocab_path) as f:\n            for i, l in enumerate(f.read().splitlines()):\n                self.vocab[l] = i\n        \n        # setting the indices for UNKnown words and PADding symbols\n        self.unk_ind = self.vocab[self.dataset_params.unk_word]\n        self.pad_ind = self.vocab[self.dataset_params.pad_word]\n                \n        # loading tags (we require this to map tags to their indices)\n        tags_path = os.path.join(data_dir, \'tags.txt\')\n        self.tag_map = {}\n        with open(tags_path) as f:\n            for i, t in enumerate(f.read().splitlines()):\n                self.tag_map[t] = i\n\n        # adding dataset parameters to param (e.g. vocab size, )\n        params.update(json_path)\n\n    def load_sentences_labels(self, sentences_file, labels_file, d):\n        """"""\n        Loads sentences and labels from their corresponding files. Maps tokens and tags to their indices and stores\n        them in the provided dict d.\n\n        Args:\n            sentences_file: (string) file with sentences with tokens space-separated\n            labels_file: (string) file with NER tags for the sentences in labels_file\n            d: (dict) a dictionary in which the loaded data is stored\n        """"""\n\n        sentences = []\n        labels = []\n\n        with open(sentences_file) as f:\n            for sentence in f.read().splitlines():\n                # replace each token by its index if it is in vocab\n                # else use index of UNK_WORD\n                s = [self.vocab[token] if token in self.vocab \n                     else self.unk_ind\n                     for token in sentence.split(\' \')]\n                sentences.append(s)\n        \n        with open(labels_file) as f:\n            for sentence in f.read().splitlines():\n                # replace each label by its index\n                l = [self.tag_map[label] for label in sentence.split(\' \')]\n                labels.append(l)        \n\n        # checks to ensure there is a tag for each token\n        assert len(labels) == len(sentences)\n        for i in range(len(labels)):\n            assert len(labels[i]) == len(sentences[i])\n\n        # storing sentences and labels in dict d\n        d[\'data\'] = sentences\n        d[\'labels\'] = labels\n        d[\'size\'] = len(sentences)\n\n    def load_data(self, types, data_dir):\n        """"""\n        Loads the data for each type in types from data_dir.\n\n        Args:\n            types: (list) has one or more of \'train\', \'val\', \'test\' depending on which data is required\n            data_dir: (string) directory containing the dataset\n\n        Returns:\n            data: (dict) contains the data with labels for each type in types\n\n        """"""\n        data = {}\n        \n        for split in [\'train\', \'val\', \'test\']:\n            if split in types:\n                sentences_file = os.path.join(data_dir, split, ""sentences.txt"")\n                labels_file = os.path.join(data_dir, split, ""labels.txt"")\n                data[split] = {}\n                self.load_sentences_labels(sentences_file, labels_file, data[split])\n\n        return data\n\n    def data_iterator(self, data, params, shuffle=False):\n        """"""\n        Returns a generator that yields batches data with labels. Batch size is params.batch_size. Expires after one\n        pass over the data.\n\n        Args:\n            data: (dict) contains data which has keys \'data\', \'labels\' and \'size\'\n            params: (Params) hyperparameters of the training process.\n            shuffle: (bool) whether the data should be shuffled\n\n        Yields:\n            batch_data: (Variable) dimension batch_size x seq_len with the sentence data\n            batch_labels: (Variable) dimension batch_size x seq_len with the corresponding labels\n\n        """"""\n\n        # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n        order = list(range(data[\'size\']))\n        if shuffle:\n            random.seed(230)\n            random.shuffle(order)\n\n        # one pass over data\n        for i in range((data[\'size\']+1)//params.batch_size):\n            # fetch sentences and tags\n            batch_sentences = [data[\'data\'][idx] for idx in order[i*params.batch_size:(i+1)*params.batch_size]]\n            batch_tags = [data[\'labels\'][idx] for idx in order[i*params.batch_size:(i+1)*params.batch_size]]\n\n            # compute length of longest sentence in batch\n            batch_max_len = max([len(s) for s in batch_sentences])\n\n            # prepare a numpy array with the data, initialising the data with pad_ind and all labels with -1\n            # initialising labels to -1 differentiates tokens with tags from PADding tokens\n            batch_data = self.pad_ind*np.ones((len(batch_sentences), batch_max_len))\n            batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n\n            # copy the data to the numpy array\n            for j in range(len(batch_sentences)):\n                cur_len = len(batch_sentences[j])\n                batch_data[j][:cur_len] = batch_sentences[j]\n                batch_labels[j][:cur_len] = batch_tags[j]\n\n            # since all data are indices, we convert them to torch LongTensors\n            batch_data, batch_labels = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n\n            # shift tensors to GPU if available\n            if params.cuda:\n                batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n\n            # convert them to Variables to record operations in the computational graph\n            batch_data, batch_labels = Variable(batch_data), Variable(batch_labels)\n    \n            yield batch_data, batch_labels\n'"
pytorch/nlp/model/net.py,7,"b'""""""Defines the neural network, losss function and metrics""""""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    """"""\n    This is the standard way to define your own network in PyTorch. You typically choose the components\n    (e.g. LSTMs, linear layers etc.) of your network in the __init__ function. You then apply these layers\n    on the input step-by-step in the forward function. You can use torch.nn.functional to apply functions\n    such as F.relu, F.sigmoid, F.softmax. Be careful to ensure your dimensions are correct after each step.\n\n    You are encouraged to have a look at the network in pytorch/vision/model/net.py to get a better sense of how\n    you can go about defining your own network.\n\n    The documentation for all the various components available to you is here: http://pytorch.org/docs/master/nn.html\n    """"""\n\n    def __init__(self, params):\n        """"""\n        We define an recurrent network that predicts the NER tags for each token in the sentence. The components\n        required are:\n\n        - an embedding layer: this layer maps each index in range(params.vocab_size) to a params.embedding_dim vector\n        - lstm: applying the LSTM on the sequential input returns an output for each token in the sentence\n        - fc: a fully connected layer that converts the LSTM output for each token to a distribution over NER tags\n\n        Args:\n            params: (Params) contains vocab_size, embedding_dim, lstm_hidden_dim\n        """"""\n        super(Net, self).__init__()\n\n        # the embedding takes as input the vocab_size and the embedding_dim\n        self.embedding = nn.Embedding(params.vocab_size, params.embedding_dim)\n\n        # the LSTM takes as input the size of its input (embedding_dim), its hidden size\n        # for more details on how to use it, check out the documentation\n        self.lstm = nn.LSTM(params.embedding_dim,\n                            params.lstm_hidden_dim, batch_first=True)\n\n        # the fully connected layer transforms the output to give the final output layer\n        self.fc = nn.Linear(params.lstm_hidden_dim, params.number_of_tags)\n\n    def forward(self, s):\n        """"""\n        This function defines how we use the components of our network to operate on an input batch.\n\n        Args:\n            s: (Variable) contains a batch of sentences, of dimension batch_size x seq_len, where seq_len is\n               the length of the longest sentence in the batch. For sentences shorter than seq_len, the remaining\n               tokens are PADding tokens. Each row is a sentence with each element corresponding to the index of\n               the token in the vocab.\n\n        Returns:\n            out: (Variable) dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n                 of each sentence.\n\n        Note: the dimensions after each step are provided\n        """"""\n        #                                -> batch_size x seq_len\n        # apply the embedding layer that maps each token to its embedding\n        # dim: batch_size x seq_len x embedding_dim\n        s = self.embedding(s)\n\n        # run the LSTM along the sentences of length seq_len\n        # dim: batch_size x seq_len x lstm_hidden_dim\n        s, _ = self.lstm(s)\n\n        # make the Variable contiguous in memory (a PyTorch artefact)\n        s = s.contiguous()\n\n        # reshape the Variable so that each row contains one token\n        # dim: batch_size*seq_len x lstm_hidden_dim\n        s = s.view(-1, s.shape[2])\n\n        # apply the fully connected layer and obtain the output (before softmax) for each token\n        s = self.fc(s)                   # dim: batch_size*seq_len x num_tags\n\n        # apply log softmax on each token\'s output (this is recommended over applying softmax\n        # since it is numerically more stable)\n        return F.log_softmax(s, dim=1)   # dim: batch_size*seq_len x num_tags\n\n\ndef loss_fn(outputs, labels):\n    """"""\n    Compute the cross entropy loss given outputs from the model and labels for all tokens. Exclude loss terms\n    for PADding tokens.\n\n    Args:\n        outputs: (Variable) dimension batch_size*seq_len x num_tags - log softmax output of the model\n        labels: (Variable) dimension batch_size x seq_len where each element is either a label in [0, 1, ... num_tag-1],\n                or -1 in case it is a PADding token.\n\n    Returns:\n        loss: (Variable) cross entropy loss for all tokens in the batch\n\n    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n          demonstrates how you can easily define a custom loss function.\n    """"""\n\n    # reshape labels to give a flat vector of length batch_size*seq_len\n    labels = labels.view(-1)\n\n    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n    mask = (labels >= 0).float()\n\n    # indexing with negative values is not supported. Since PADded tokens have label -1, we convert them to a positive\n    # number. This does not affect training, since we ignore the PADded tokens with the mask.\n    labels = labels % outputs.shape[1]\n\n    num_tokens = int(torch.sum(mask))\n\n    # compute cross entropy loss for all tokens (except PADding tokens), by multiplying with mask.\n    return -torch.sum(outputs[range(outputs.shape[0]), labels]*mask)/num_tokens\n\n\ndef accuracy(outputs, labels):\n    """"""\n    Compute the accuracy, given the outputs and labels for all tokens. Exclude PADding terms.\n\n    Args:\n        outputs: (np.ndarray) dimension batch_size*seq_len x num_tags - log softmax output of the model\n        labels: (np.ndarray) dimension batch_size x seq_len where each element is either a label in\n                [0, 1, ... num_tag-1], or -1 in case it is a PADding token.\n\n    Returns: (float) accuracy in [0,1]\n    """"""\n\n    # reshape labels to give a flat vector of length batch_size*seq_len\n    labels = labels.ravel()\n\n    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n    mask = (labels >= 0)\n\n    # np.argmax gives us the class predicted for each token by the model\n    outputs = np.argmax(outputs, axis=1)\n\n    # compare outputs with labels and divide by number of tokens (excluding PADding tokens)\n    return np.sum(outputs == labels)/float(np.sum(mask))\n\n\n# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\nmetrics = {\n    \'accuracy\': accuracy,\n    # could add more metrics such as accuracy for each token type\n}\n'"
pytorch/vision/model/__init__.py,0,b''
pytorch/vision/model/data_loader.py,3,"b'import random\nimport os\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n\n# borrowed from http://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n# and http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n# define a training image loader that specifies transforms on images. See documentation for more details.\ntrain_transformer = transforms.Compose([\n    transforms.Resize(64),  # resize the image to 64x64 (remove if images are already 64x64)\n    transforms.RandomHorizontalFlip(),  # randomly flip image horizontally\n    transforms.ToTensor()])  # transform it into a torch tensor\n\n# loader for evaluation, no horizontal flip\neval_transformer = transforms.Compose([\n    transforms.Resize(64),  # resize the image to 64x64 (remove if images are already 64x64)\n    transforms.ToTensor()])  # transform it into a torch tensor\n\n\nclass SIGNSDataset(Dataset):\n    """"""\n    A standard PyTorch definition of Dataset which defines the functions __len__ and __getitem__.\n    """"""\n    def __init__(self, data_dir, transform):\n        """"""\n        Store the filenames of the jpgs to use. Specifies transforms to apply on images.\n\n        Args:\n            data_dir: (string) directory containing the dataset\n            transform: (torchvision.transforms) transformation to apply on image\n        """"""\n        self.filenames = os.listdir(data_dir)\n        self.filenames = [os.path.join(data_dir, f) for f in self.filenames if f.endswith(\'.jpg\')]\n\n        self.labels = [int(os.path.split(filename)[-1][0]) for filename in self.filenames]\n        self.transform = transform\n\n    def __len__(self):\n        # return size of dataset\n        return len(self.filenames)\n\n    def __getitem__(self, idx):\n        """"""\n        Fetch index idx image and labels from dataset. Perform transforms on image.\n\n        Args:\n            idx: (int) index in [0, 1, ..., size_of_dataset-1]\n\n        Returns:\n            image: (Tensor) transformed image\n            label: (int) corresponding label of image\n        """"""\n        image = Image.open(self.filenames[idx])  # PIL image\n        image = self.transform(image)\n        return image, self.labels[idx]\n\n\ndef fetch_dataloader(types, data_dir, params):\n    """"""\n    Fetches the DataLoader object for each type in types from data_dir.\n\n    Args:\n        types: (list) has one or more of \'train\', \'val\', \'test\' depending on which data is required\n        data_dir: (string) directory containing the dataset\n        params: (Params) hyperparameters\n\n    Returns:\n        data: (dict) contains the DataLoader object for each type in types\n    """"""\n    dataloaders = {}\n\n    for split in [\'train\', \'val\', \'test\']:\n        if split in types:\n            path = os.path.join(data_dir, ""{}_signs"".format(split))\n\n            # use the train_transformer if training data, else use eval_transformer without random flip\n            if split == \'train\':\n                dl = DataLoader(SIGNSDataset(path, train_transformer), batch_size=params.batch_size, shuffle=True,\n                                        num_workers=params.num_workers,\n                                        pin_memory=params.cuda)\n            else:\n                dl = DataLoader(SIGNSDataset(path, eval_transformer), batch_size=params.batch_size, shuffle=False,\n                                num_workers=params.num_workers,\n                                pin_memory=params.cuda)\n\n            dataloaders[split] = dl\n\n    return dataloaders\n'"
pytorch/vision/model/net.py,6,"b'""""""Defines the neural network, losss function and metrics""""""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    """"""\n    This is the standard way to define your own network in PyTorch. You typically choose the components\n    (e.g. LSTMs, linear layers etc.) of your network in the __init__ function. You then apply these layers\n    on the input step-by-step in the forward function. You can use torch.nn.functional to apply functions\n\n    such as F.relu, F.sigmoid, F.softmax, F.max_pool2d. Be careful to ensure your dimensions are correct after each\n    step. You are encouraged to have a look at the network in pytorch/nlp/model/net.py to get a better sense of how\n    you can go about defining your own network.\n\n    The documentation for all the various components available o you is here: http://pytorch.org/docs/master/nn.html\n    """"""\n\n    def __init__(self, params):\n        """"""\n        We define an convolutional network that predicts the sign from an image. The components\n        required are:\n\n        - an embedding layer: this layer maps each index in range(params.vocab_size) to a params.embedding_dim vector\n        - lstm: applying the LSTM on the sequential input returns an output for each token in the sentence\n        - fc: a fully connected layer that converts the LSTM output for each token to a distribution over NER tags\n\n        Args:\n            params: (Params) contains num_channels\n        """"""\n        super(Net, self).__init__()\n        self.num_channels = params.num_channels\n        \n        # each of the convolution layers below have the arguments (input_channels, output_channels, filter_size,\n        # stride, padding). We also include batch normalisation layers that help stabilise training.\n        # For more details on how to use these layers, check out the documentation.\n        self.conv1 = nn.Conv2d(3, self.num_channels, 3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(self.num_channels)\n        self.conv2 = nn.Conv2d(self.num_channels, self.num_channels*2, 3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(self.num_channels*2)\n        self.conv3 = nn.Conv2d(self.num_channels*2, self.num_channels*4, 3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(self.num_channels*4)\n\n        # 2 fully connected layers to transform the output of the convolution layers to the final output\n        self.fc1 = nn.Linear(8*8*self.num_channels*4, self.num_channels*4)\n        self.fcbn1 = nn.BatchNorm1d(self.num_channels*4)\n        self.fc2 = nn.Linear(self.num_channels*4, 6)       \n        self.dropout_rate = params.dropout_rate\n\n    def forward(self, s):\n        """"""\n        This function defines how we use the components of our network to operate on an input batch.\n\n        Args:\n            s: (Variable) contains a batch of images, of dimension batch_size x 3 x 64 x 64 .\n\n        Returns:\n            out: (Variable) dimension batch_size x 6 with the log probabilities for the labels of each image.\n\n        Note: the dimensions after each step are provided\n        """"""\n        #                                                  -> batch_size x 3 x 64 x 64\n        # we apply the convolution layers, followed by batch normalisation, maxpool and relu x 3\n        s = self.bn1(self.conv1(s))                         # batch_size x num_channels x 64 x 64\n        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels x 32 x 32\n        s = self.bn2(self.conv2(s))                         # batch_size x num_channels*2 x 32 x 32\n        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels*2 x 16 x 16\n        s = self.bn3(self.conv3(s))                         # batch_size x num_channels*4 x 16 x 16\n        s = F.relu(F.max_pool2d(s, 2))                      # batch_size x num_channels*4 x 8 x 8\n\n        # flatten the output for each image\n        s = s.view(-1, 8*8*self.num_channels*4)             # batch_size x 8*8*num_channels*4\n\n        # apply 2 fully connected layers with dropout\n        s = F.dropout(F.relu(self.fcbn1(self.fc1(s))), \n            p=self.dropout_rate, training=self.training)    # batch_size x self.num_channels*4\n        s = self.fc2(s)                                     # batch_size x 6\n\n        # apply log softmax on each image\'s output (this is recommended over applying softmax\n        # since it is numerically more stable)\n        return F.log_softmax(s, dim=1)\n\n\ndef loss_fn(outputs, labels):\n    """"""\n    Compute the cross entropy loss given outputs and labels.\n\n    Args:\n        outputs: (Variable) dimension batch_size x 6 - output of the model\n        labels: (Variable) dimension batch_size, where each element is a value in [0, 1, 2, 3, 4, 5]\n\n    Returns:\n        loss (Variable): cross entropy loss for all images in the batch\n\n    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n          demonstrates how you can easily define a custom loss function.\n    """"""\n    num_examples = outputs.size()[0]\n    return -torch.sum(outputs[range(num_examples), labels])/num_examples\n\n\ndef accuracy(outputs, labels):\n    """"""\n    Compute the accuracy, given the outputs and labels for all images.\n\n    Args:\n        outputs: (np.ndarray) dimension batch_size x 6 - log softmax output of the model\n        labels: (np.ndarray) dimension batch_size, where each element is a value in [0, 1, 2, 3, 4, 5]\n\n    Returns: (float) accuracy in [0,1]\n    """"""\n    outputs = np.argmax(outputs, axis=1)\n    return np.sum(outputs==labels)/float(labels.size)\n\n\n# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\nmetrics = {\n    \'accuracy\': accuracy,\n    # could add more metrics such as accuracy for each token type\n}\n'"
tensorflow/nlp/model/__init__.py,0,b''
tensorflow/nlp/model/evaluation.py,0,"b'""""""Tensorflow utility functions for evaluation""""""\n\nimport logging\nimport os\n\nfrom tqdm import trange\nimport tensorflow as tf\n\nfrom model.utils import save_dict_to_json\n\n\ndef evaluate_sess(sess, model_spec, num_steps, writer=None, params=None):\n    """"""Train the model on `num_steps` batches.\n\n    Args:\n        sess: (tf.Session) current session\n        model_spec: (dict) contains the graph operations or nodes needed for training\n        num_steps: (int) train for this number of batches\n        writer: (tf.summary.FileWriter) writer for summaries. Is None if we don\'t log anything\n        params: (Params) hyperparameters\n    """"""\n    update_metrics = model_spec[\'update_metrics\']\n    eval_metrics = model_spec[\'metrics\']\n    global_step = tf.train.get_global_step()\n\n    # Load the evaluation dataset into the pipeline and initialize the metrics init op\n    sess.run(model_spec[\'iterator_init_op\'])\n    sess.run(model_spec[\'metrics_init_op\'])\n\n    # compute metrics over the dataset\n    for _ in range(num_steps):\n        sess.run(update_metrics)\n\n    # Get the values of the metrics\n    metrics_values = {k: v[0] for k, v in eval_metrics.items()}\n    metrics_val = sess.run(metrics_values)\n    metrics_string = "" ; "".join(""{}: {:05.3f}"".format(k, v) for k, v in metrics_val.items())\n    logging.info(""- Eval metrics: "" + metrics_string)\n\n    # Add summaries manually to writer at global_step_val\n    if writer is not None:\n        global_step_val = sess.run(global_step)\n        for tag, val in metrics_val.items():\n            summ = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=val)])\n            writer.add_summary(summ, global_step_val)\n\n    return metrics_val\n\n\ndef evaluate(model_spec, model_dir, params, restore_from):\n    """"""Evaluate the model\n\n    Args:\n        model_spec: (dict) contains the graph operations or nodes needed for evaluation\n        model_dir: (string) directory containing config, weights and log\n        params: (Params) contains hyperparameters of the model.\n                Must define: num_epochs, train_size, batch_size, eval_size, save_summary_steps\n        restore_from: (string) directory or file containing weights to restore the graph\n    """"""\n    # Initialize tf.Saver\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        # Initialize the lookup table\n        sess.run(model_spec[\'variable_init_op\'])\n\n        # Reload weights from the weights subdirectory\n        save_path = os.path.join(model_dir, restore_from)\n        if os.path.isdir(save_path):\n            save_path = tf.train.latest_checkpoint(save_path)\n        saver.restore(sess, save_path)\n\n        # Evaluate\n        num_steps = (params.eval_size + params.batch_size - 1) // params.batch_size\n        metrics = evaluate_sess(sess, model_spec, num_steps)\n        metrics_name = \'_\'.join(restore_from.split(\'/\'))\n        save_path = os.path.join(model_dir, ""metrics_test_{}.json"".format(metrics_name))\n        save_dict_to_json(metrics, save_path)\n'"
tensorflow/nlp/model/input_fn.py,0,"b'""""""Create the input data pipeline using `tf.data`""""""\n\nimport tensorflow as tf\n\n\ndef load_dataset_from_text(path_txt, vocab):\n    """"""Create tf.data Instance from txt file\n\n    Args:\n        path_txt: (string) path containing one example per line\n        vocab: (tf.lookuptable)\n\n    Returns:\n        dataset: (tf.Dataset) yielding list of ids of tokens for each example\n    """"""\n    # Load txt file, one example per line\n    dataset = tf.data.TextLineDataset(path_txt)\n\n    # Convert line into list of tokens, splitting by white space\n    dataset = dataset.map(lambda string: tf.string_split([string]).values)\n\n    # Lookup tokens to return their ids\n    dataset = dataset.map(lambda tokens: (vocab.lookup(tokens), tf.size(tokens)))\n\n    return dataset\n\n\ndef input_fn(mode, sentences, labels, params):\n    """"""Input function for NER\n\n    Args:\n        mode: (string) \'train\', \'eval\' or any other mode you can think of\n                     At training, we shuffle the data and have multiple epochs\n        sentences: (tf.Dataset) yielding list of ids of words\n        datasets: (tf.Dataset) yielding list of ids of tags\n        params: (Params) contains hyperparameters of the model (ex: `params.num_epochs`)\n\n    """"""\n    # Load all the dataset in memory for shuffling is training\n    is_training = (mode == \'train\')\n    buffer_size = params.buffer_size if is_training else 1\n\n    # Zip the sentence and the labels together\n    dataset = tf.data.Dataset.zip((sentences, labels))\n\n    # Create batches and pad the sentences of different length\n    padded_shapes = ((tf.TensorShape([None]),  # sentence of unknown size\n                      tf.TensorShape([])),     # size(words)\n                     (tf.TensorShape([None]),  # labels of unknown size\n                      tf.TensorShape([])))     # size(tags)\n\n    padding_values = ((params.id_pad_word,   # sentence padded on the right with id_pad_word\n                       0),                   # size(words) -- unused\n                      (params.id_pad_tag,    # labels padded on the right with id_pad_tag\n                       0))                   # size(tags) -- unused\n\n\n    dataset = (dataset\n        .shuffle(buffer_size=buffer_size)\n        .padded_batch(params.batch_size, padded_shapes=padded_shapes, padding_values=padding_values)\n        .prefetch(1)  # make sure you always have one batch ready to serve\n    )\n\n    # Create initializable iterator from this dataset so that we can reset at each epoch\n    iterator = dataset.make_initializable_iterator()\n\n    # Query the output of the iterator for input to the model\n    ((sentence, sentence_lengths), (labels, _)) = iterator.get_next()\n    init_op = iterator.initializer\n\n    # Build and return a dictionnary containing the nodes / ops\n    inputs = {\n        \'sentence\': sentence,\n        \'labels\': labels,\n        \'sentence_lengths\': sentence_lengths,\n        \'iterator_init_op\': init_op\n    }\n\n    return inputs\n'"
tensorflow/nlp/model/model_fn.py,0,"b'""""""Define the model.""""""\n\nimport tensorflow as tf\n\n\ndef build_model(mode, inputs, params):\n    """"""Compute logits of the model (output distribution)\n\n    Args:\n        mode: (string) \'train\', \'eval\', etc.\n        inputs: (dict) contains the inputs of the graph (features, labels...)\n                this can be `tf.placeholder` or outputs of `tf.data`\n        params: (Params) contains hyperparameters of the model (ex: `params.learning_rate`)\n\n    Returns:\n        output: (tf.Tensor) output of the model\n    """"""\n    sentence = inputs[\'sentence\']\n\n    if params.model_version == \'lstm\':\n        # Get word embeddings for each token in the sentence\n        embeddings = tf.get_variable(name=""embeddings"", dtype=tf.float32,\n                shape=[params.vocab_size, params.embedding_size])\n        sentence = tf.nn.embedding_lookup(embeddings, sentence)\n\n        # Apply LSTM over the embeddings\n        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(params.lstm_num_units)\n        output, _  = tf.nn.dynamic_rnn(lstm_cell, sentence, dtype=tf.float32)\n\n        # Compute logits from the output of the LSTM\n        logits = tf.layers.dense(output, params.number_of_tags)\n\n    else:\n        raise NotImplementedError(""Unknown model version: {}"".format(params.model_version))\n\n    return logits\n\n\ndef model_fn(mode, inputs, params, reuse=False):\n    """"""Model function defining the graph operations.\n\n    Args:\n        mode: (string) \'train\', \'eval\', etc.\n        inputs: (dict) contains the inputs of the graph (features, labels...)\n                this can be `tf.placeholder` or outputs of `tf.data`\n        params: (Params) contains hyperparameters of the model (ex: `params.learning_rate`)\n        reuse: (bool) whether to reuse the weights\n\n    Returns:\n        model_spec: (dict) contains the graph operations or nodes needed for training / evaluation\n    """"""\n    is_training = (mode == \'train\')\n    labels = inputs[\'labels\']\n    sentence_lengths = inputs[\'sentence_lengths\']\n\n    # -----------------------------------------------------------\n    # MODEL: define the layers of the model\n    with tf.variable_scope(\'model\', reuse=reuse):\n        # Compute the output distribution of the model and the predictions\n        logits = build_model(mode, inputs, params)\n        predictions = tf.argmax(logits, -1)\n\n    # Define loss and accuracy (we need to apply a mask to account for padding)\n    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n    mask = tf.sequence_mask(sentence_lengths)\n    losses = tf.boolean_mask(losses, mask)\n    loss = tf.reduce_mean(losses)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(labels, predictions), tf.float32))\n\n    # Define training step that minimizes the loss with the Adam optimizer\n    if is_training:\n        optimizer = tf.train.AdamOptimizer(params.learning_rate)\n        global_step = tf.train.get_or_create_global_step()\n        train_op = optimizer.minimize(loss, global_step=global_step)\n\n    # -----------------------------------------------------------\n    # METRICS AND SUMMARIES\n    # Metrics for evaluation using tf.metrics (average over whole dataset)\n    with tf.variable_scope(""metrics""):\n        metrics = {\n            \'accuracy\': tf.metrics.accuracy(labels=labels, predictions=predictions),\n            \'loss\': tf.metrics.mean(loss)\n        }\n\n    # Group the update ops for the tf.metrics\n    update_metrics_op = tf.group(*[op for _, op in metrics.values()])\n\n    # Get the op to reset the local variables used in tf.metrics\n    metric_variables = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=""metrics"")\n    metrics_init_op = tf.variables_initializer(metric_variables)\n\n    # Summaries for training\n    tf.summary.scalar(\'loss\', loss)\n    tf.summary.scalar(\'accuracy\', accuracy)\n\n    # -----------------------------------------------------------\n    # MODEL SPECIFICATION\n    # Create the model specification and return it\n    # It contains nodes or operations in the graph that will be used for training and evaluation\n    model_spec = inputs\n    variable_init_op = tf.group(*[tf.global_variables_initializer(), tf.tables_initializer()])\n    model_spec[\'variable_init_op\'] = variable_init_op\n    model_spec[""predictions""] = predictions\n    model_spec[\'loss\'] = loss\n    model_spec[\'accuracy\'] = accuracy\n    model_spec[\'metrics_init_op\'] = metrics_init_op\n    model_spec[\'metrics\'] = metrics\n    model_spec[\'update_metrics\'] = update_metrics_op\n    model_spec[\'summary_op\'] = tf.summary.merge_all()\n\n    if is_training:\n        model_spec[\'train_op\'] = train_op\n\n    return model_spec\n'"
tensorflow/nlp/model/training.py,0,"b'""""""Tensorflow utility functions for training""""""\n\nimport logging\nimport os\n\nfrom tqdm import trange\nimport tensorflow as tf\n\nfrom model.utils import save_dict_to_json\nfrom model.evaluation import evaluate_sess\n\n\ndef train_sess(sess, model_spec, num_steps, writer, params):\n    """"""Train the model on `num_steps` batches\n\n    Args:\n        sess: (tf.Session) current session\n        model_spec: (dict) contains the graph operations or nodes needed for training\n        num_steps: (int) train for this number of batches\n        writer: (tf.summary.FileWriter) writer for summaries\n        params: (Params) hyperparameters\n    """"""\n    # Get relevant graph operations or nodes needed for training\n    loss = model_spec[\'loss\']\n    train_op = model_spec[\'train_op\']\n    update_metrics = model_spec[\'update_metrics\']\n    metrics = model_spec[\'metrics\']\n    summary_op = model_spec[\'summary_op\']\n    global_step = tf.train.get_global_step()\n\n    # Load the training dataset into the pipeline and initialize the metrics local variables\n    sess.run(model_spec[\'iterator_init_op\'])\n    sess.run(model_spec[\'metrics_init_op\'])\n\n    # Use tqdm for progress bar\n    t = trange(num_steps)\n    for i in t:\n        # Evaluate summaries for tensorboard only once in a while\n        if i % params.save_summary_steps == 0:\n            # Perform a mini-batch update\n            _, _, loss_val, summ, global_step_val = sess.run([train_op, update_metrics, loss,\n                                                              summary_op, global_step])\n            # Write summaries for tensorboard\n            writer.add_summary(summ, global_step_val)\n        else:\n            _, _, loss_val = sess.run([train_op, update_metrics, loss])\n        # Log the loss in the tqdm progress bar\n        t.set_postfix(loss=\'{:05.3f}\'.format(loss_val))\n\n\n    metrics_values = {k: v[0] for k, v in metrics.items()}\n    metrics_val = sess.run(metrics_values)\n    metrics_string = "" ; "".join(""{}: {:05.3f}"".format(k, v) for k, v in metrics_val.items())\n    logging.info(""- Train metrics: "" + metrics_string)\n\n\ndef train_and_evaluate(train_model_spec, eval_model_spec, model_dir, params, restore_from=None):\n    """"""Train the model and evaluate every epoch.\n\n    Args:\n        train_model_spec: (dict) contains the graph operations or nodes needed for training\n        eval_model_spec: (dict) contains the graph operations or nodes needed for evaluation\n        model_dir: (string) directory containing config, weights and log\n        params: (Params) contains hyperparameters of the model.\n                Must define: num_epochs, train_size, batch_size, eval_size, save_summary_steps\n        restore_from: (string) directory or file containing weights to restore the graph\n    """"""\n    # Initialize tf.Saver instances to save weights during training\n    last_saver = tf.train.Saver() # will keep last 5 epochs\n    best_saver = tf.train.Saver(max_to_keep=1)  # only keep 1 best checkpoint (best on eval)\n    begin_at_epoch = 0\n\n    with tf.Session() as sess:\n        # Initialize model variables\n        sess.run(train_model_spec[\'variable_init_op\'])\n\n        # Reload weights from directory if specified\n        if restore_from is not None:\n            logging.info(""Restoring parameters from {}"".format(restore_from))\n            if os.path.isdir(restore_from):\n                restore_from = tf.train.latest_checkpoint(restore_from)\n                begin_at_epoch = int(restore_from.split(\'-\')[-1])\n            last_saver.restore(sess, restore_from)\n\n        # For tensorboard (takes care of writing summaries to files)\n        train_writer = tf.summary.FileWriter(os.path.join(model_dir, \'train_summaries\'), sess.graph)\n        eval_writer = tf.summary.FileWriter(os.path.join(model_dir, \'eval_summaries\'), sess.graph)\n\n        best_eval_acc = 0.0\n        for epoch in range(begin_at_epoch, begin_at_epoch + params.num_epochs):\n            # Run one epoch\n            logging.info(""Epoch {}/{}"".format(epoch + 1, begin_at_epoch + params.num_epochs))\n            # Compute number of batches in one epoch (one full pass over the training set)\n            num_steps = (params.train_size + params.batch_size - 1) // params.batch_size\n            train_sess(sess, train_model_spec, num_steps, train_writer, params)\n\n            # Save weights\n            last_save_path = os.path.join(model_dir, \'last_weights\', \'after-epoch\')\n            last_saver.save(sess, last_save_path, global_step=epoch + 1)\n\n            # Evaluate for one epoch on validation set\n            num_steps = (params.eval_size + params.batch_size - 1) // params.batch_size\n            metrics = evaluate_sess(sess, eval_model_spec, num_steps, eval_writer)\n\n            # If best_eval, best_save_path\n            eval_acc = metrics[\'accuracy\']\n            if eval_acc >= best_eval_acc:\n                # Store new best accuracy\n                best_eval_acc = eval_acc\n                # Save weights\n                best_save_path = os.path.join(model_dir, \'best_weights\', \'after-epoch\')\n                best_save_path = best_saver.save(sess, best_save_path, global_step=epoch + 1)\n                logging.info(""- Found new best accuracy, saving in {}"".format(best_save_path))\n                # Save best eval metrics in a json file in the model directory\n                best_json_path = os.path.join(model_dir, ""metrics_eval_best_weights.json"")\n                save_dict_to_json(metrics, best_json_path)\n\n            # Save latest eval metrics in a json file in the model directory\n            last_json_path = os.path.join(model_dir, ""metrics_eval_last_weights.json"")\n            save_dict_to_json(metrics, last_json_path)\n'"
tensorflow/nlp/model/utils.py,0,"b'""""""General utility functions""""""\n\nimport json\nimport logging\n\n\nclass Params():\n    """"""Class that loads hyperparameters from a json file.\n\n    Example:\n    ```\n    params = Params(json_path)\n    print(params.learning_rate)\n    params.learning_rate = 0.5  # change the value of learning_rate in params\n    ```\n    """"""\n\n    def __init__(self, json_path):\n        self.update(json_path)\n\n    def save(self, json_path):\n        """"""Saves parameters to json file""""""\n        with open(json_path, \'w\') as f:\n            json.dump(self.__dict__, f, indent=4)\n\n    def update(self, json_path):\n        """"""Loads parameters from json file""""""\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    @property\n    def dict(self):\n        """"""Gives dict-like access to Params instance by `params.dict[\'learning_rate\']`""""""\n        return self.__dict__\n\n\ndef set_logger(log_path):\n    """"""Sets the logger to log info in terminal and file `log_path`.\n\n    In general, it is useful to have a logger so that every output to the terminal is saved\n    in a permanent file. Here we save it to `model_dir/train.log`.\n\n    Example:\n    ```\n    logging.info(""Starting training..."")\n    ```\n\n    Args:\n        log_path: (string) where to log\n    """"""\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    if not logger.handlers:\n        # Logging to a file\n        file_handler = logging.FileHandler(log_path)\n        file_handler.setFormatter(logging.Formatter(\'%(asctime)s:%(levelname)s: %(message)s\'))\n        logger.addHandler(file_handler)\n\n        # Logging to console\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(logging.Formatter(\'%(message)s\'))\n        logger.addHandler(stream_handler)\n\n\ndef save_dict_to_json(d, json_path):\n    """"""Saves dict of floats in json file\n\n    Args:\n        d: (dict) of float-castable values (np.float, int, float, etc.)\n        json_path: (string) path to json file\n    """"""\n    with open(json_path, \'w\') as f:\n        # We need to convert the values to float for json (it doesn\'t accept np.array, np.float, )\n        d = {k: float(v) for k, v in d.items()}\n        json.dump(d, f, indent=4)\n'"
tensorflow/vision/model/__init__.py,0,b''
tensorflow/vision/model/evaluation.py,0,"b'""""""Tensorflow utility functions for evaluation""""""\n\nimport logging\nimport os\n\nfrom tqdm import trange\nimport tensorflow as tf\n\nfrom model.utils import save_dict_to_json\n\n\ndef evaluate_sess(sess, model_spec, num_steps, writer=None, params=None):\n    """"""Train the model on `num_steps` batches.\n\n    Args:\n        sess: (tf.Session) current session\n        model_spec: (dict) contains the graph operations or nodes needed for training\n        num_steps: (int) train for this number of batches\n        writer: (tf.summary.FileWriter) writer for summaries. Is None if we don\'t log anything\n        params: (Params) hyperparameters\n    """"""\n    update_metrics = model_spec[\'update_metrics\']\n    eval_metrics = model_spec[\'metrics\']\n    global_step = tf.train.get_global_step()\n\n    # Load the evaluation dataset into the pipeline and initialize the metrics init op\n    sess.run(model_spec[\'iterator_init_op\'])\n    sess.run(model_spec[\'metrics_init_op\'])\n\n    # compute metrics over the dataset\n    for _ in range(num_steps):\n        sess.run(update_metrics)\n\n    # Get the values of the metrics\n    metrics_values = {k: v[0] for k, v in eval_metrics.items()}\n    metrics_val = sess.run(metrics_values)\n    metrics_string = "" ; "".join(""{}: {:05.3f}"".format(k, v) for k, v in metrics_val.items())\n    logging.info(""- Eval metrics: "" + metrics_string)\n\n    # Add summaries manually to writer at global_step_val\n    if writer is not None:\n        global_step_val = sess.run(global_step)\n        for tag, val in metrics_val.items():\n            summ = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=val)])\n            writer.add_summary(summ, global_step_val)\n\n    return metrics_val\n\n\ndef evaluate(model_spec, model_dir, params, restore_from):\n    """"""Evaluate the model\n\n    Args:\n        model_spec: (dict) contains the graph operations or nodes needed for evaluation\n        model_dir: (string) directory containing config, weights and log\n        params: (Params) contains hyperparameters of the model.\n                Must define: num_epochs, train_size, batch_size, eval_size, save_summary_steps\n        restore_from: (string) directory or file containing weights to restore the graph\n    """"""\n    # Initialize tf.Saver\n    saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n        # Initialize the lookup table\n        sess.run(model_spec[\'variable_init_op\'])\n\n        # Reload weights from the weights subdirectory\n        save_path = os.path.join(model_dir, restore_from)\n        if os.path.isdir(save_path):\n            save_path = tf.train.latest_checkpoint(save_path)\n        saver.restore(sess, save_path)\n\n        # Evaluate\n        num_steps = (params.eval_size + params.batch_size - 1) // params.batch_size\n        metrics = evaluate_sess(sess, model_spec, num_steps)\n        metrics_name = \'_\'.join(restore_from.split(\'/\'))\n        save_path = os.path.join(model_dir, ""metrics_test_{}.json"".format(metrics_name))\n        save_dict_to_json(metrics, save_path)\n'"
tensorflow/vision/model/input_fn.py,0,"b'""""""Create the input data pipeline using `tf.data`""""""\n\nimport tensorflow as tf\n\n\ndef _parse_function(filename, label, size):\n    """"""Obtain the image from the filename (for both training and validation).\n\n    The following operations are applied:\n        - Decode the image from jpeg format\n        - Convert to float and to range [0, 1]\n    """"""\n    image_string = tf.read_file(filename)\n\n    # Don\'t use tf.image.decode_image, or the output shape will be undefined\n    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n\n    # This will convert to float values in [0, 1]\n    image = tf.image.convert_image_dtype(image_decoded, tf.float32)\n\n    resized_image = tf.image.resize_images(image, [size, size])\n\n    return resized_image, label\n\n\ndef train_preprocess(image, label, use_random_flip):\n    """"""Image preprocessing for training.\n\n    Apply the following operations:\n        - Horizontally flip the image with probability 1/2\n        - Apply random brightness and saturation\n    """"""\n    if use_random_flip:\n        image = tf.image.random_flip_left_right(image)\n\n    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n\n    # Make sure the image is still in [0, 1]\n    image = tf.clip_by_value(image, 0.0, 1.0)\n\n    return image, label\n\n\ndef input_fn(is_training, filenames, labels, params):\n    """"""Input function for the SIGNS dataset.\n\n    The filenames have format ""{label}_IMG_{id}.jpg"".\n    For instance: ""data_dir/2_IMG_4584.jpg"".\n\n    Args:\n        is_training: (bool) whether to use the train or test pipeline.\n                     At training, we shuffle the data and have multiple epochs\n        filenames: (list) filenames of the images, as [""data_dir/{label}_IMG_{id}.jpg""...]\n        labels: (list) corresponding list of labels\n        params: (Params) contains hyperparameters of the model (ex: `params.num_epochs`)\n    """"""\n    num_samples = len(filenames)\n    assert len(filenames) == len(labels), ""Filenames and labels should have same length""\n\n    # Create a Dataset serving batches of images and labels\n    # We don\'t repeat for multiple epochs because we always train and evaluate for one epoch\n    parse_fn = lambda f, l: _parse_function(f, l, params.image_size)\n    train_fn = lambda f, l: train_preprocess(f, l, params.use_random_flip)\n\n    if is_training:\n        dataset = (tf.data.Dataset.from_tensor_slices((tf.constant(filenames), tf.constant(labels)))\n            .shuffle(num_samples)  # whole dataset into the buffer ensures good shuffling\n            .map(parse_fn, num_parallel_calls=params.num_parallel_calls)\n            .map(train_fn, num_parallel_calls=params.num_parallel_calls)\n            .batch(params.batch_size)\n            .prefetch(1)  # make sure you always have one batch ready to serve\n        )\n    else:\n        dataset = (tf.data.Dataset.from_tensor_slices((tf.constant(filenames), tf.constant(labels)))\n            .map(parse_fn)\n            .batch(params.batch_size)\n            .prefetch(1)  # make sure you always have one batch ready to serve\n        )\n\n    # Create reinitializable iterator from dataset\n    iterator = dataset.make_initializable_iterator()\n    images, labels = iterator.get_next()\n    iterator_init_op = iterator.initializer\n\n    inputs = {\'images\': images, \'labels\': labels, \'iterator_init_op\': iterator_init_op}\n    return inputs\n'"
tensorflow/vision/model/model_fn.py,0,"b'""""""Define the model.""""""\n\nimport tensorflow as tf\n\n\ndef build_model(is_training, inputs, params):\n    """"""Compute logits of the model (output distribution)\n\n    Args:\n        is_training: (bool) whether we are training or not\n        inputs: (dict) contains the inputs of the graph (features, labels...)\n                this can be `tf.placeholder` or outputs of `tf.data`\n        params: (Params) hyperparameters\n\n    Returns:\n        output: (tf.Tensor) output of the model\n    """"""\n    images = inputs[\'images\']\n\n    assert images.get_shape().as_list() == [None, params.image_size, params.image_size, 3]\n\n    out = images\n    # Define the number of channels of each convolution\n    # For each block, we do: 3x3 conv -> batch norm -> relu -> 2x2 maxpool\n    num_channels = params.num_channels\n    bn_momentum = params.bn_momentum\n    channels = [num_channels, num_channels * 2, num_channels * 4, num_channels * 8]\n    for i, c in enumerate(channels):\n        with tf.variable_scope(\'block_{}\'.format(i+1)):\n            out = tf.layers.conv2d(out, c, 3, padding=\'same\')\n            if params.use_batch_norm:\n                out = tf.layers.batch_normalization(out, momentum=bn_momentum, training=is_training)\n            out = tf.nn.relu(out)\n            out = tf.layers.max_pooling2d(out, 2, 2)\n\n    assert out.get_shape().as_list() == [None, 4, 4, num_channels * 8]\n\n    out = tf.reshape(out, [-1, 4 * 4 * num_channels * 8])\n    with tf.variable_scope(\'fc_1\'):\n        out = tf.layers.dense(out, num_channels * 8)\n        if params.use_batch_norm:\n            out = tf.layers.batch_normalization(out, momentum=bn_momentum, training=is_training)\n        out = tf.nn.relu(out)\n    with tf.variable_scope(\'fc_2\'):\n        logits = tf.layers.dense(out, params.num_labels)\n\n    return logits\n\n\ndef model_fn(mode, inputs, params, reuse=False):\n    """"""Model function defining the graph operations.\n\n    Args:\n        mode: (string) can be \'train\' or \'eval\'\n        inputs: (dict) contains the inputs of the graph (features, labels...)\n                this can be `tf.placeholder` or outputs of `tf.data`\n        params: (Params) contains hyperparameters of the model (ex: `params.learning_rate`)\n        reuse: (bool) whether to reuse the weights\n\n    Returns:\n        model_spec: (dict) contains the graph operations or nodes needed for training / evaluation\n    """"""\n    is_training = (mode == \'train\')\n    labels = inputs[\'labels\']\n    labels = tf.cast(labels, tf.int64)\n\n    # -----------------------------------------------------------\n    # MODEL: define the layers of the model\n    with tf.variable_scope(\'model\', reuse=reuse):\n        # Compute the output distribution of the model and the predictions\n        logits = build_model(is_training, inputs, params)\n        predictions = tf.argmax(logits, 1)\n\n    # Define loss and accuracy\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(labels, predictions), tf.float32))\n\n    # Define training step that minimizes the loss with the Adam optimizer\n    if is_training:\n        optimizer = tf.train.AdamOptimizer(params.learning_rate)\n        global_step = tf.train.get_or_create_global_step()\n        if params.use_batch_norm:\n            # Add a dependency to update the moving mean and variance for batch normalization\n            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n                train_op = optimizer.minimize(loss, global_step=global_step)\n        else:\n            train_op = optimizer.minimize(loss, global_step=global_step)\n\n\n    # -----------------------------------------------------------\n    # METRICS AND SUMMARIES\n    # Metrics for evaluation using tf.metrics (average over whole dataset)\n    with tf.variable_scope(""metrics""):\n        metrics = {\n            \'accuracy\': tf.metrics.accuracy(labels=labels, predictions=tf.argmax(logits, 1)),\n            \'loss\': tf.metrics.mean(loss)\n        }\n\n    # Group the update ops for the tf.metrics\n    update_metrics_op = tf.group(*[op for _, op in metrics.values()])\n\n    # Get the op to reset the local variables used in tf.metrics\n    metric_variables = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=""metrics"")\n    metrics_init_op = tf.variables_initializer(metric_variables)\n\n    # Summaries for training\n    tf.summary.scalar(\'loss\', loss)\n    tf.summary.scalar(\'accuracy\', accuracy)\n    tf.summary.image(\'train_image\', inputs[\'images\'])\n\n    #TODO: if mode == \'eval\': ?\n    # Add incorrectly labeled images\n    mask = tf.not_equal(labels, predictions)\n\n    # Add a different summary to know how they were misclassified\n    for label in range(0, params.num_labels):\n        mask_label = tf.logical_and(mask, tf.equal(predictions, label))\n        incorrect_image_label = tf.boolean_mask(inputs[\'images\'], mask_label)\n        tf.summary.image(\'incorrectly_labeled_{}\'.format(label), incorrect_image_label)\n\n    # -----------------------------------------------------------\n    # MODEL SPECIFICATION\n    # Create the model specification and return it\n    # It contains nodes or operations in the graph that will be used for training and evaluation\n    model_spec = inputs\n    model_spec[\'variable_init_op\'] = tf.global_variables_initializer()\n    model_spec[""predictions""] = predictions\n    model_spec[\'loss\'] = loss\n    model_spec[\'accuracy\'] = accuracy\n    model_spec[\'metrics_init_op\'] = metrics_init_op\n    model_spec[\'metrics\'] = metrics\n    model_spec[\'update_metrics\'] = update_metrics_op\n    model_spec[\'summary_op\'] = tf.summary.merge_all()\n\n    if is_training:\n        model_spec[\'train_op\'] = train_op\n\n    return model_spec\n'"
tensorflow/vision/model/training.py,0,"b'""""""Tensorflow utility functions for training""""""\n\nimport logging\nimport os\n\nfrom tqdm import trange\nimport tensorflow as tf\n\nfrom model.utils import save_dict_to_json\nfrom model.evaluation import evaluate_sess\n\n\ndef train_sess(sess, model_spec, num_steps, writer, params):\n    """"""Train the model on `num_steps` batches\n\n    Args:\n        sess: (tf.Session) current session\n        model_spec: (dict) contains the graph operations or nodes needed for training\n        num_steps: (int) train for this number of batches\n        writer: (tf.summary.FileWriter) writer for summaries\n        params: (Params) hyperparameters\n    """"""\n    # Get relevant graph operations or nodes needed for training\n    loss = model_spec[\'loss\']\n    train_op = model_spec[\'train_op\']\n    update_metrics = model_spec[\'update_metrics\']\n    metrics = model_spec[\'metrics\']\n    summary_op = model_spec[\'summary_op\']\n    global_step = tf.train.get_global_step()\n\n    # Load the training dataset into the pipeline and initialize the metrics local variables\n    sess.run(model_spec[\'iterator_init_op\'])\n    sess.run(model_spec[\'metrics_init_op\'])\n\n    # Use tqdm for progress bar\n    t = trange(num_steps)\n    for i in t:\n        # Evaluate summaries for tensorboard only once in a while\n        if i % params.save_summary_steps == 0:\n            # Perform a mini-batch update\n            _, _, loss_val, summ, global_step_val = sess.run([train_op, update_metrics, loss,\n                                                              summary_op, global_step])\n            # Write summaries for tensorboard\n            writer.add_summary(summ, global_step_val)\n        else:\n            _, _, loss_val = sess.run([train_op, update_metrics, loss])\n        # Log the loss in the tqdm progress bar\n        t.set_postfix(loss=\'{:05.3f}\'.format(loss_val))\n\n\n    metrics_values = {k: v[0] for k, v in metrics.items()}\n    metrics_val = sess.run(metrics_values)\n    metrics_string = "" ; "".join(""{}: {:05.3f}"".format(k, v) for k, v in metrics_val.items())\n    logging.info(""- Train metrics: "" + metrics_string)\n\n\ndef train_and_evaluate(train_model_spec, eval_model_spec, model_dir, params, restore_from=None):\n    """"""Train the model and evaluate every epoch.\n\n    Args:\n        train_model_spec: (dict) contains the graph operations or nodes needed for training\n        eval_model_spec: (dict) contains the graph operations or nodes needed for evaluation\n        model_dir: (string) directory containing config, weights and log\n        params: (Params) contains hyperparameters of the model.\n                Must define: num_epochs, train_size, batch_size, eval_size, save_summary_steps\n        restore_from: (string) directory or file containing weights to restore the graph\n    """"""\n    # Initialize tf.Saver instances to save weights during training\n    last_saver = tf.train.Saver() # will keep last 5 epochs\n    best_saver = tf.train.Saver(max_to_keep=1)  # only keep 1 best checkpoint (best on eval)\n    begin_at_epoch = 0\n\n    with tf.Session() as sess:\n        # Initialize model variables\n        sess.run(train_model_spec[\'variable_init_op\'])\n\n        # Reload weights from directory if specified\n        if restore_from is not None:\n            logging.info(""Restoring parameters from {}"".format(restore_from))\n            if os.path.isdir(restore_from):\n                restore_from = tf.train.latest_checkpoint(restore_from)\n                begin_at_epoch = int(restore_from.split(\'-\')[-1])\n            last_saver.restore(sess, restore_from)\n\n        # For tensorboard (takes care of writing summaries to files)\n        train_writer = tf.summary.FileWriter(os.path.join(model_dir, \'train_summaries\'), sess.graph)\n        eval_writer = tf.summary.FileWriter(os.path.join(model_dir, \'eval_summaries\'), sess.graph)\n\n        best_eval_acc = 0.0\n        for epoch in range(begin_at_epoch, begin_at_epoch + params.num_epochs):\n            # Run one epoch\n            logging.info(""Epoch {}/{}"".format(epoch + 1, begin_at_epoch + params.num_epochs))\n            # Compute number of batches in one epoch (one full pass over the training set)\n            num_steps = (params.train_size + params.batch_size - 1) // params.batch_size\n            train_sess(sess, train_model_spec, num_steps, train_writer, params)\n\n            # Save weights\n            last_save_path = os.path.join(model_dir, \'last_weights\', \'after-epoch\')\n            last_saver.save(sess, last_save_path, global_step=epoch + 1)\n\n            # Evaluate for one epoch on validation set\n            num_steps = (params.eval_size + params.batch_size - 1) // params.batch_size\n            metrics = evaluate_sess(sess, eval_model_spec, num_steps, eval_writer)\n\n            # If best_eval, best_save_path\n            eval_acc = metrics[\'accuracy\']\n            if eval_acc >= best_eval_acc:\n                # Store new best accuracy\n                best_eval_acc = eval_acc\n                # Save weights\n                best_save_path = os.path.join(model_dir, \'best_weights\', \'after-epoch\')\n                best_save_path = best_saver.save(sess, best_save_path, global_step=epoch + 1)\n                logging.info(""- Found new best accuracy, saving in {}"".format(best_save_path))\n                # Save best eval metrics in a json file in the model directory\n                best_json_path = os.path.join(model_dir, ""metrics_eval_best_weights.json"")\n                save_dict_to_json(metrics, best_json_path)\n\n            # Save latest eval metrics in a json file in the model directory\n            last_json_path = os.path.join(model_dir, ""metrics_eval_last_weights.json"")\n            save_dict_to_json(metrics, last_json_path)\n'"
tensorflow/vision/model/utils.py,0,"b'""""""General utility functions""""""\n\nimport json\nimport logging\n\n\nclass Params():\n    """"""Class that loads hyperparameters from a json file.\n\n    Example:\n    ```\n    params = Params(json_path)\n    print(params.learning_rate)\n    params.learning_rate = 0.5  # change the value of learning_rate in params\n    ```\n    """"""\n\n    def __init__(self, json_path):\n        self.update(json_path)\n\n    def save(self, json_path):\n        """"""Saves parameters to json file""""""\n        with open(json_path, \'w\') as f:\n            json.dump(self.__dict__, f, indent=4)\n\n    def update(self, json_path):\n        """"""Loads parameters from json file""""""\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    @property\n    def dict(self):\n        """"""Gives dict-like access to Params instance by `params.dict[\'learning_rate\']`""""""\n        return self.__dict__\n\n\ndef set_logger(log_path):\n    """"""Sets the logger to log info in terminal and file `log_path`.\n\n    In general, it is useful to have a logger so that every output to the terminal is saved\n    in a permanent file. Here we save it to `model_dir/train.log`.\n\n    Example:\n    ```\n    logging.info(""Starting training..."")\n    ```\n\n    Args:\n        log_path: (string) where to log\n    """"""\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    if not logger.handlers:\n        # Logging to a file\n        file_handler = logging.FileHandler(log_path)\n        file_handler.setFormatter(logging.Formatter(\'%(asctime)s:%(levelname)s: %(message)s\'))\n        logger.addHandler(file_handler)\n\n        # Logging to console\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(logging.Formatter(\'%(message)s\'))\n        logger.addHandler(stream_handler)\n\n\ndef save_dict_to_json(d, json_path):\n    """"""Saves dict of floats in json file\n\n    Args:\n        d: (dict) of float-castable values (np.float, int, float, etc.)\n        json_path: (string) path to json file\n    """"""\n    with open(json_path, \'w\') as f:\n        # We need to convert the values to float for json (it doesn\'t accept np.array, np.float, )\n        d = {k: float(v) for k, v in d.items()}\n        json.dump(d, f, indent=4)\n'"
