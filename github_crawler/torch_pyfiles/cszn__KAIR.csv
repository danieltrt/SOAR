file_path,api_count,code
main_test_dncnn.py,2,"b'import os.path\nimport logging\n\nimport numpy as np\nfrom datetime import datetime\nfrom collections import OrderedDict\nfrom scipy.io import loadmat\n\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_model\nfrom utils import utils_image as util\n\n\n\'\'\'\nSpyder (Python 3.6)\nPyTorch 1.1.0\nWindows 10 or Linux\n\nKai Zhang (cskaizhang@gmail.com)\ngithub: https://github.com/cszn/KAIR\n        https://github.com/cszn/DnCNN\n\n@article{zhang2017beyond,\n  title={Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising},\n  author={Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},\n  journal={IEEE Transactions on Image Processing},\n  volume={26},\n  number={7},\n  pages={3142--3155},\n  year={2017},\n  publisher={IEEE}\n}\n\n% If you have any question, please feel free to contact with me.\n% Kai Zhang (e-mail: cskaizhang@gmail.com; github: https://github.com/cszn)\n\nby Kai Zhang (12/Dec./2019)\n\'\'\'\n\n""""""\n# --------------------------------------------\n|--model_zoo          # model_zoo\n   |--dncnn_15        # model_name\n   |--dncnn_25\n   |--dncnn_50\n   |--dncnn_gray_blind\n   |--dncnn_color_blind\n   |--dncnn3\n|--testset            # testsets\n   |--set12           # testset_name\n   |--bsd68\n   |--cbsd68\n|--results            # results\n   |--set12_dncnn_15  # result_name = testset_name + \'_\' + model_name\n   |--set12_dncnn_25\n   |--bsd68_dncnn_15\n# --------------------------------------------\n""""""\n\n\ndef main():\n\n    # ----------------------------------------\n    # Preparation\n    # ----------------------------------------\n\n    noise_level_img = 25             # noise level for noisy image\n    model_name = \'dncnn_25\'          # \'dncnn_15\' | \'dncnn_25\' | \'dncnn_50\' | \'dncnn_gray_blind\' | \'dncnn_color_blind\' | \'dncnn3\'\n    testset_name = \'bsd68\'           # test set, \'bsd68\' | \'set12\'\n    need_degradation = True          # default: True\n    x8 = False                       # default: False, x8 to boost performance\n    show_img = False                 # default: False\n\n\n\n\n    task_current = \'dn\'       # \'dn\' for denoising | \'sr\' for super-resolution\n    sf = 1                    # unused for denoising\n    if \'color\' in model_name:\n        n_channels = 3        # fixed, 1 for grayscale image, 3 for color image \n    else:\n        n_channels = 1        # fixed for grayscale image \n    if model_name in [\'dncnn_gray_blind\', \'dncnn_color_blind\', \'dncnn3\']:\n        nb = 20               # fixed\n    else:\n        nb = 17               # fixed\n    model_pool = \'model_zoo\'  # fixed\n    testsets = \'testsets\'     # fixed\n    results = \'results\'       # fixed\n    result_name = testset_name + \'_\' + model_name     # fixed\n    border = sf if task_current == \'sr\' else 0        # shave boader to calculate PSNR and SSIM\n    model_path = os.path.join(model_pool, model_name+\'.pth\')\n\n    # ----------------------------------------\n    # L_path, E_path, H_path\n    # ----------------------------------------\n\n    L_path = os.path.join(testsets, testset_name) # L_path, for Low-quality images\n    H_path = L_path                               # H_path, for High-quality images\n    E_path = os.path.join(results, result_name)   # E_path, for Estimated images\n    util.mkdir(E_path)\n\n    if H_path == L_path:\n        need_degradation = True\n    logger_name = result_name\n    utils_logger.logger_info(logger_name, log_path=os.path.join(E_path, logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n\n    need_H = True if H_path is not None else False\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # ----------------------------------------\n    # load model\n    # ----------------------------------------\n\n    from models.network_dncnn import DnCNN as net\n    model = net(in_nc=n_channels, out_nc=n_channels, nc=64, nb=nb, act_mode=\'R\')\n    model.load_state_dict(torch.load(model_path), strict=True)\n    model.eval()\n    for k, v in model.named_parameters():\n        v.requires_grad = False\n    model = model.to(device)\n    logger.info(\'Model path: {:s}\'.format(model_path))\n    number_parameters = sum(map(lambda x: x.numel(), model.parameters()))\n    logger.info(\'Params number: {}\'.format(number_parameters))\n\n    test_results = OrderedDict()\n    test_results[\'psnr\'] = []\n    test_results[\'ssim\'] = []\n\n    logger.info(\'model_name:{}, image sigma:{}\'.format(model_name, noise_level_img))\n    logger.info(L_path)\n    L_paths = util.get_image_paths(L_path)\n    H_paths = util.get_image_paths(H_path) if need_H else None\n\n    for idx, img in enumerate(L_paths):\n\n        # ------------------------------------\n        # (1) img_L\n        # ------------------------------------\n\n        img_name, ext = os.path.splitext(os.path.basename(img))\n        # logger.info(\'{:->4d}--> {:>10s}\'.format(idx+1, img_name+ext))\n        img_L = util.imread_uint(img, n_channels=n_channels)\n        img_L = util.uint2single(img_L)\n\n        if need_degradation:  # degradation process\n            np.random.seed(seed=0)  # for reproducibility\n            img_L += np.random.normal(0, noise_level_img/255., img_L.shape)\n\n        util.imshow(util.single2uint(img_L), title=\'Noisy image with noise level {}\'.format(noise_level_img)) if show_img else None\n\n        img_L = util.single2tensor4(img_L)\n        img_L = img_L.to(device)\n\n        # ------------------------------------\n        # (2) img_E\n        # ------------------------------------\n\n        if not x8:\n            img_E = model(img_L)\n        else:\n            img_E = utils_model.test_mode(model, img_L, mode=3)\n\n        img_E = util.tensor2uint(img_E)\n\n        if need_H:\n\n            # --------------------------------\n            # (3) img_H\n            # --------------------------------\n\n            img_H = util.imread_uint(H_paths[idx], n_channels=n_channels)\n            img_H = img_H.squeeze()\n\n            # --------------------------------\n            # PSNR and SSIM\n            # --------------------------------\n\n            psnr = util.calculate_psnr(img_E, img_H, border=border)\n            ssim = util.calculate_ssim(img_E, img_H, border=border)\n            test_results[\'psnr\'].append(psnr)\n            test_results[\'ssim\'].append(ssim)\n            logger.info(\'{:s} - PSNR: {:.2f} dB; SSIM: {:.4f}.\'.format(img_name+ext, psnr, ssim))\n            util.imshow(np.concatenate([img_E, img_H], axis=1), title=\'Recovered / Ground-truth\') if show_img else None\n\n        # ------------------------------------\n        # save results\n        # ------------------------------------\n\n        util.imsave(img_E, os.path.join(E_path, img_name+ext))\n\n    if need_H:\n        ave_psnr = sum(test_results[\'psnr\']) / len(test_results[\'psnr\'])\n        ave_ssim = sum(test_results[\'ssim\']) / len(test_results[\'ssim\'])\n        logger.info(\'Average PSNR/SSIM(RGB) - {} - PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, ave_psnr, ave_ssim))\n\nif __name__ == \'__main__\':\n\n    main()\n'"
main_test_dncnn3_deblocking.py,2,"b'import os.path\nimport logging\n\nimport numpy as np\nfrom datetime import datetime\nfrom collections import OrderedDict\n\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_model\nfrom utils import utils_image as util\n#import os\n#os.environ[""KMP_DUPLICATE_LIB_OK""]=""TRUE""\n\n\n\'\'\'\nSpyder (Python 3.6)\nPyTorch 1.1.0\nWindows 10 or Linux\n\nKai Zhang (cskaizhang@gmail.com)\ngithub: https://github.com/cszn/KAIR\n        https://github.com/cszn/DnCNN\n\n@article{zhang2017beyond,\n  title={Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising},\n  author={Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},\n  journal={IEEE Transactions on Image Processing},\n  volume={26},\n  number={7},\n  pages={3142--3155},\n  year={2017},\n  publisher={IEEE}\n}\n\n% If you have any question, please feel free to contact with me.\n% Kai Zhang (e-mail: cskaizhang@gmail.com; github: https://github.com/cszn)\n\nby Kai Zhang (12/Dec./2019)\n\'\'\'\n\n""""""\n# --------------------------------------------\n|--model_zoo          # model_zoo\n   |--dncnn3          # model_name\n|--testset            # testsets\n   |--set12           # testset_name\n   |--bsd68\n|--results            # results\n   |--set12_dncnn3    # result_name = testset_name + \'_\' + model_name\n# --------------------------------------------\n""""""\n\n\ndef main():\n\n    # ----------------------------------------\n    # Preparation\n    # ----------------------------------------\n\n    model_name = \'dncnn3\'     # \'dncnn3\'- can be used for blind Gaussian denoising, JPEG deblocking (quality factor 5-100) and super-resolution (x234)\n\n    # important!\n    testset_name = \'bsd68\'    # test set, low-quality grayscale/color JPEG images\n    n_channels = 1            # set 1 for grayscale image, set 3 for color image\n\n\n    x8 = False                       # default: False, x8 to boost performance\n    testsets = \'testsets\'     # fixed\n    results = \'results\'       # fixed\n    result_name = testset_name + \'_\' + model_name # fixed\n    L_path = os.path.join(testsets, testset_name) # L_path, for Low-quality grayscale/Y-channel JPEG images\n    E_path = os.path.join(results, result_name)   # E_path, for Estimated images\n    util.mkdir(E_path)\n\n    model_pool = \'model_zoo\'  # fixed\n    model_path = os.path.join(model_pool, model_name+\'.pth\')\n    logger_name = result_name\n    utils_logger.logger_info(logger_name, log_path=os.path.join(E_path, logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # ----------------------------------------\n    # load model\n    # ----------------------------------------\n\n    from models.network_dncnn import DnCNN as net\n    model = net(in_nc=1, out_nc=1, nc=64, nb=20, act_mode=\'R\')\n    model.load_state_dict(torch.load(model_path), strict=True)\n    model.eval()\n    for k, v in model.named_parameters():\n        v.requires_grad = False\n    model = model.to(device)\n    logger.info(\'Model path: {:s}\'.format(model_path))\n    number_parameters = sum(map(lambda x: x.numel(), model.parameters()))\n    logger.info(\'Params number: {}\'.format(number_parameters))\n\n    logger.info(L_path)\n    L_paths = util.get_image_paths(L_path)\n\n    for idx, img in enumerate(L_paths):\n\n        # ------------------------------------\n        # (1) img_L\n        # ------------------------------------\n        img_name, ext = os.path.splitext(os.path.basename(img))\n        logger.info(\'{:->4d}--> {:>10s}\'.format(idx+1, img_name+ext))\n        img_L = util.imread_uint(img, n_channels=n_channels)\n        img_L = util.uint2single(img_L)\n        if n_channels == 3:\n            ycbcr = util.rgb2ycbcr(img_L, False)\n            img_L = ycbcr[..., 0:1]\n        img_L = util.single2tensor4(img_L)\n        img_L = img_L.to(device)\n\n        # ------------------------------------\n        # (2) img_E\n        # ------------------------------------\n        if not x8:\n            img_E = model(img_L)\n        else:\n            img_E = utils_model.test_mode(model, img_L, mode=3)\n\n        img_E = util.tensor2single(img_E)\n        if n_channels == 3:\n            ycbcr[..., 0] = img_E\n            img_E = util.ycbcr2rgb(ycbcr)\n        img_E = util.single2uint(img_E)\n\n        # ------------------------------------\n        # save results\n        # ------------------------------------\n        util.imsave(img_E, os.path.join(E_path, img_name+\'.png\'))\n\n\nif __name__ == \'__main__\':\n\n    main()\n'"
main_test_dpsr.py,4,"b'import os.path\nimport logging\nimport re\n\nimport numpy as np\nfrom collections import OrderedDict\n\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_model\n\n\n\'\'\'\nSpyder (Python 3.6)\nPyTorch 1.1.0\nWindows 10 or Linux\n\nKai Zhang (cskaizhang@gmail.com)\ngithub: https://github.com/cszn/KAIR\n        https://github.com/cszn/DPSR\n\n@inproceedings{zhang2019deep,\n  title={Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels},\n  author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={1671--1681},\n  year={2019}\n}\n\n% If you have any question, please feel free to contact with me.\n% Kai Zhang (e-mail: cskaizhang@gmail.com; github: https://github.com/cszn)\n\nby Kai Zhang (12/Dec./2019)\n\'\'\'\n\n""""""\n# --------------------------------------------\ntesting code for the super-resolver prior of DPSR\n# --------------------------------------------\n|--model_zoo             # model_zoo\n   |--dpsr_x2            # model_name, optimized for PSNR\n   |--dpsr_x3 \n   |--dpsr_x4\n   |--dpsr_x4_gan        # model_name, optimized for perceptual quality\n|--testset               # testsets\n   |--set5               # testset_name\n   |--srbsd68\n|--results               # results\n   |--set5_dpsr_x2       # result_name = testset_name + \'_\' + model_name\n   |--set5_dpsr_x3\n   |--set5_dpsr_x4\n   |--set5_dpsr_x4_gan\n   |--srbsd68_dpsr_x4_gan\n# --------------------------------------------\n""""""\n\n\ndef main():\n\n    # ----------------------------------------\n    # Preparation\n    # ----------------------------------------\n\n    noise_level_img = 0                  # default: 0, noise level for LR image\n    noise_level_model = noise_level_img  # noise level for model    \n    model_name = \'dpsr_x4_gan\'           # \'dpsr_x2\' | \'dpsr_x3\' | \'dpsr_x4\' | \'dpsr_x4_gan\'\n    testset_name = \'set5\'                # test set,  \'set5\' | \'srbsd68\'\n    need_degradation = True              # default: True\n    x8 = False                           # default: False, x8 to boost performance\n    sf = [int(s) for s in re.findall(r\'\\d+\', model_name)][0]  # scale factor\n    show_img = False                     # default: False\n\n\n\n    task_current = \'sr\'       # \'dn\' for denoising | \'sr\' for super-resolution\n    n_channels = 3            # fixed\n    nc = 96                   # fixed, number of channels\n    nb = 16                   # fixed, number of conv layers\n    model_pool = \'model_zoo\'  # fixed\n    testsets = \'testsets\'     # fixed\n    results = \'results\'       # fixed\n    result_name = testset_name + \'_\' + model_name\n    border = sf if task_current == \'sr\' else 0     # shave boader to calculate PSNR and SSIM\n    model_path = os.path.join(model_pool, model_name+\'.pth\')\n\n    # ----------------------------------------\n    # L_path, E_path, H_path\n    # ----------------------------------------\n\n    L_path = os.path.join(testsets, testset_name) # L_path, for Low-quality images\n    H_path = L_path                               # H_path, for High-quality images\n    E_path = os.path.join(results, result_name)   # E_path, for Estimated images\n    util.mkdir(E_path)\n\n    if H_path == L_path:\n        need_degradation = True\n    logger_name = result_name\n    utils_logger.logger_info(logger_name, log_path=os.path.join(E_path, logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n\n    need_H = True if H_path is not None else False\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # ----------------------------------------\n    # load model\n    # ----------------------------------------\n\n    from models.network_dpsr import MSRResNet_prior as net\n    model = net(in_nc=n_channels+1, out_nc=n_channels, nc=nc, nb=nb, upscale=sf, act_mode=\'R\', upsample_mode=\'pixelshuffle\')\n    model.load_state_dict(torch.load(model_path), strict=False)\n    model.eval()\n    for k, v in model.named_parameters():\n        v.requires_grad = False\n    model = model.to(device)\n    logger.info(\'Model path: {:s}\'.format(model_path))\n    number_parameters = sum(map(lambda x: x.numel(), model.parameters()))\n    logger.info(\'Params number: {}\'.format(number_parameters))\n\n    test_results = OrderedDict()\n    test_results[\'psnr\'] = []\n    test_results[\'ssim\'] = []\n    test_results[\'psnr_y\'] = []\n    test_results[\'ssim_y\'] = []\n\n    logger.info(\'model_name:{}, model sigma:{}, image sigma:{}\'.format(model_name, noise_level_img, noise_level_model))\n    logger.info(L_path)\n    L_paths = util.get_image_paths(L_path)\n    H_paths = util.get_image_paths(H_path) if need_H else None\n\n    for idx, img in enumerate(L_paths):\n\n        # ------------------------------------\n        # (1) img_L\n        # ------------------------------------\n\n        img_name, ext = os.path.splitext(os.path.basename(img))\n        # logger.info(\'{:->4d}--> {:>10s}\'.format(idx+1, img_name+ext))\n        img_L = util.imread_uint(img, n_channels=n_channels)\n        img_L = util.uint2single(img_L)\n\n        # degradation process, bicubic downsampling + Gaussian noise\n        if need_degradation:\n            img_L = util.modcrop(img_L, sf)\n            img_L = util.imresize_np(img_L, 1/sf)\n            np.random.seed(seed=0)  # for reproducibility\n            img_L += np.random.normal(0, noise_level_img/255., img_L.shape)\n\n        util.imshow(util.single2uint(img_L), title=\'LR image with noise level {}\'.format(noise_level_img)) if show_img else None\n\n        img_L = util.single2tensor4(img_L)\n        noise_level_map = torch.full((1, 1, img_L.size(2), img_L.size(3)), noise_level_model/255.).type_as(img_L)\n        img_L = torch.cat((img_L, noise_level_map), dim=1)\n        img_L = img_L.to(device)\n\n        # ------------------------------------\n        # (2) img_E\n        # ------------------------------------\n\n        if not x8:\n            img_E = model(img_L)\n        else:\n            img_E = utils_model.test_mode(model, img_L, mode=3, sf=sf)\n\n        img_E = util.tensor2uint(img_E)\n\n        if need_H:\n\n            # --------------------------------\n            # (3) img_H\n            # --------------------------------\n\n            img_H = util.imread_uint(H_paths[idx], n_channels=n_channels)\n            img_H = img_H.squeeze()\n            img_H = util.modcrop(img_H, sf)\n\n            # --------------------------------\n            # PSNR and SSIM\n            # --------------------------------\n\n            psnr = util.calculate_psnr(img_E, img_H, border=border)\n            ssim = util.calculate_ssim(img_E, img_H, border=border)\n            test_results[\'psnr\'].append(psnr)\n            test_results[\'ssim\'].append(ssim)\n            logger.info(\'{:s} - PSNR: {:.2f} dB; SSIM: {:.4f}.\'.format(img_name+ext, psnr, ssim))\n            util.imshow(np.concatenate([img_E, img_H], axis=1), title=\'Recovered / Ground-truth\') if show_img else None\n\n            if np.ndim(img_H) == 3:  # RGB image\n                img_E_y = util.rgb2ycbcr(img_E, only_y=True)\n                img_H_y = util.rgb2ycbcr(img_H, only_y=True)\n                psnr_y = util.calculate_psnr(img_E_y, img_H_y, border=border)\n                ssim_y = util.calculate_ssim(img_E_y, img_H_y, border=border)\n                test_results[\'psnr_y\'].append(psnr_y)\n                test_results[\'ssim_y\'].append(ssim_y)\n\n        # ------------------------------------\n        # save results\n        # ------------------------------------\n\n        util.imsave(img_E, os.path.join(E_path, img_name+\'.png\'))\n\n    if need_H:\n        ave_psnr = sum(test_results[\'psnr\']) / len(test_results[\'psnr\'])\n        ave_ssim = sum(test_results[\'ssim\']) / len(test_results[\'ssim\'])\n        logger.info(\'Average PSNR/SSIM(RGB) - {} - x{} --PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, sf, ave_psnr, ave_ssim))\n        if np.ndim(img_H) == 3:\n            ave_psnr_y = sum(test_results[\'psnr_y\']) / len(test_results[\'psnr_y\'])\n            ave_ssim_y = sum(test_results[\'ssim_y\']) / len(test_results[\'ssim_y\'])\n            logger.info(\'Average PSNR/SSIM( Y ) - {} - x{} - PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, sf, ave_psnr_y, ave_ssim_y))\n\nif __name__ == \'__main__\':\n\n    main()\n'"
main_test_fdncnn.py,4,"b'import os.path\nimport logging\n\nimport numpy as np\nfrom collections import OrderedDict\nfrom scipy.io import loadmat\n\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_model\nfrom utils import utils_image as util\n\n\n\'\'\'\nSpyder (Python 3.6)\nPyTorch 1.1.0\nWindows 10 or Linux\n\nKai Zhang (cskaizhang@gmail.com)\ngithub: https://github.com/cszn/KAIR\n        https://github.com/cszn/DnCNN\n        https://github.com/cszn/FFDNet\n\n@article{zhang2018ffdnet,\n  title={FFDNet: Toward a fast and flexible solution for CNN-based image denoising},\n  author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n  journal={IEEE Transactions on Image Processing},\n  volume={27},\n  number={9},\n  pages={4608--4622},\n  year={2018},\n  publisher={IEEE}\n}\n\n% If you have any question, please feel free to contact with me.\n% Kai Zhang (e-mail: cskaizhang@gmail.com; github: https://github.com/cszn)\n\nby Kai Zhang (12/Dec./2019)\n\'\'\'\n\n""""""\n# --------------------------------------------\n|--model_zoo             # model_zoo\n   |--fdncnn_color       # model_name, for color images\n   |--fdncnn_gray\n   |--fdncnn_color_clip  # for clipped uint8 color images\n   |--fdncnn_gray_clip\n|--testset               # testsets\n   |--set12              # testset_name\n   |--bsd68\n   |--cbsd68\n|--results               # results\n   |--set12_fdncnn_color # result_name = testset_name + \'_\' + model_name\n   |--set12_fdncnn_gray\n   |--cbsd68_fdncnn_color_clip\n# --------------------------------------------\n""""""\n\n\ndef main():\n\n    # ----------------------------------------\n    # Preparation\n    # ----------------------------------------\n\n    noise_level_img = 15                 # noise level for noisy image\n    noise_level_model = noise_level_img  # noise level for model\n    model_name = \'fdncnn_gray\'           # \'fdncnn_gray\' | \'fdncnn_color\' | \'fdncnn_color_clip\' | \'fdncnn_gray_clip\'\n    testset_name = \'bsd68\'               # test set,  \'bsd68\' | \'cbsd68\' | \'set12\'\n    need_degradation = True              # default: True\n    x8 = False                           # default: False, x8 to boost performance\n    show_img = False                     # default: Falsedefault: False\n\n\n\n\n    task_current = \'dn\'       # \'dn\' for denoising | \'sr\' for super-resolution\n    sf = 1                    # unused for denoising\n    if \'color\' in model_name:\n        n_channels = 3        # 3 for color image\n    else:\n        n_channels = 1        # 1 for grayscale image\n    if \'clip\' in model_name:\n        use_clip = True       # clip the intensities into range of [0, 1]\n    else:\n        use_clip = False\n    model_pool = \'model_zoo\'  # fixed\n    testsets = \'testsets\'     # fixed\n    results = \'results\'       # fixed\n    result_name = testset_name + \'_\' + model_name\n    border = sf if task_current == \'sr\' else 0     # shave boader to calculate PSNR and SSIM\n    model_path = os.path.join(model_pool, model_name+\'.pth\')\n\n    # ----------------------------------------\n    # L_path, E_path, H_path\n    # ----------------------------------------\n\n    L_path = os.path.join(testsets, testset_name) # L_path, for Low-quality images\n    H_path = L_path                               # H_path, for High-quality images\n    E_path = os.path.join(results, result_name)   # E_path, for Estimated images\n    util.mkdir(E_path)\n\n    if H_path == L_path:\n        need_degradation = True\n    logger_name = result_name\n    utils_logger.logger_info(logger_name, log_path=os.path.join(E_path, logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n\n    need_H = True if H_path is not None else False\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # ----------------------------------------\n    # load model\n    # ----------------------------------------\n\n    from models.network_dncnn import FDnCNN as net\n    model = net(in_nc=n_channels+1, out_nc=n_channels, nc=64, nb=20, act_mode=\'R\')\n    model.load_state_dict(torch.load(model_path), strict=True)\n    model.eval()\n    for k, v in model.named_parameters():\n        v.requires_grad = False\n    model = model.to(device)\n    logger.info(\'Model path: {:s}\'.format(model_path))\n    number_parameters = sum(map(lambda x: x.numel(), model.parameters()))\n    logger.info(\'Params number: {}\'.format(number_parameters))\n\n    test_results = OrderedDict()\n    test_results[\'psnr\'] = []\n    test_results[\'ssim\'] = []\n\n    logger.info(\'model_name:{}, model sigma:{}, image sigma:{}\'.format(model_name, noise_level_img, noise_level_model))\n    logger.info(L_path)\n    L_paths = util.get_image_paths(L_path)\n    H_paths = util.get_image_paths(H_path) if need_H else None\n\n    for idx, img in enumerate(L_paths):\n\n        # ------------------------------------\n        # (1) img_L\n        # ------------------------------------\n\n        img_name, ext = os.path.splitext(os.path.basename(img))\n        # logger.info(\'{:->4d}--> {:>10s}\'.format(idx+1, img_name+ext))\n        img_L = util.imread_uint(img, n_channels=n_channels)\n        img_L = util.uint2single(img_L)\n\n        if need_degradation:  # degradation process\n            np.random.seed(seed=0)  # for reproducibility\n            img_L += np.random.normal(0, noise_level_img/255., img_L.shape)\n        if use_clip:\n            img_L = util.uint2single(util.single2uint(img_L)) \n\n        util.imshow(util.single2uint(img_L), title=\'Noisy image with noise level {}\'.format(noise_level_img)) if show_img else None\n\n        img_L = util.single2tensor4(img_L)\n        noise_level_map = torch.ones((1, 1, img_L.size(2), img_L.size(3)), dtype=torch.float).mul_(noise_level_model/255.)\n        img_L = torch.cat((img_L, noise_level_map), dim=1)\n        img_L = img_L.to(device)\n\n        # ------------------------------------\n        # (2) img_E\n        # ------------------------------------\n\n        if not x8:\n            img_E = model(img_L)\n        else:\n            img_E = utils_model.test_mode(model, img_L, mode=3)\n\n        img_E = util.tensor2uint(img_E)\n\n        if need_H:\n\n            # --------------------------------\n            # (3) img_H\n            # --------------------------------\n\n            img_H = util.imread_uint(H_paths[idx], n_channels=n_channels)\n            img_H = img_H.squeeze()\n\n            # --------------------------------\n            # PSNR and SSIM\n            # --------------------------------\n\n            psnr = util.calculate_psnr(img_E, img_H, border=border)\n            ssim = util.calculate_ssim(img_E, img_H, border=border)\n            test_results[\'psnr\'].append(psnr)\n            test_results[\'ssim\'].append(ssim)\n            logger.info(\'{:s} - PSNR: {:.2f} dB; SSIM: {:.4f}.\'.format(img_name+ext, psnr, ssim))\n            util.imshow(np.concatenate([img_E, img_H], axis=1), title=\'Recovered / Ground-truth\') if show_img else None\n\n        # ------------------------------------\n        # save results\n        # ------------------------------------\n\n        util.imsave(img_E, os.path.join(E_path, img_name+ext))\n\n    if need_H:\n        ave_psnr = sum(test_results[\'psnr\']) / len(test_results[\'psnr\'])\n        ave_ssim = sum(test_results[\'ssim\']) / len(test_results[\'ssim\'])\n        logger.info(\'Average PSNR/SSIM(RGB) - {} - PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, ave_psnr, ave_ssim))\n\nif __name__ == \'__main__\':\n\n    main()\n'"
main_test_ffdnet.py,3,"b'import os.path\nimport logging\n\nimport numpy as np\nfrom collections import OrderedDict\n\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\n\n\n\'\'\'\nSpyder (Python 3.6)\nPyTorch 1.1.0\nWindows 10 or Linux\n\nKai Zhang (cskaizhang@gmail.com)\ngithub: https://github.com/cszn/KAIR\n        https://github.com/cszn/FFDNet\n\n@article{zhang2018ffdnet,\n  title={FFDNet: Toward a fast and flexible solution for CNN-based image denoising},\n  author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n  journal={IEEE Transactions on Image Processing},\n  volume={27},\n  number={9},\n  pages={4608--4622},\n  year={2018},\n  publisher={IEEE}\n}\n\n% If you have any question, please feel free to contact with me.\n% Kai Zhang (e-mail: cskaizhang@gmail.com; github: https://github.com/cszn)\n\nby Kai Zhang (12/Dec./2019)\n\'\'\'\n\n""""""\n# --------------------------------------------\n|--model_zoo             # model_zoo\n   |--ffdnet_gray        # model_name, for color images\n   |--ffdnet_color\n   |--ffdnet_color_clip  # for clipped uint8 color images\n   |--ffdnet_gray_clip\n|--testset               # testsets\n   |--set12              # testset_name\n   |--bsd68\n   |--cbsd68\n|--results               # results\n   |--set12_ffdnet_gray  # result_name = testset_name + \'_\' + model_name\n   |--set12_ffdnet_color\n   |--cbsd68_ffdnet_color_clip\n# --------------------------------------------\n""""""\n\n\ndef main():\n\n    # ----------------------------------------\n    # Preparation\n    # ----------------------------------------\n\n    noise_level_img = 15                 # noise level for noisy image\n    noise_level_model = noise_level_img  # noise level for model\n    model_name = \'ffdnet_gray\'           # \'ffdnet_gray\' | \'ffdnet_color\' | \'ffdnet_color_clip\' | \'ffdnet_gray_clip\'\n    testset_name = \'bsd68\'               # test set,  \'bsd68\' | \'cbsd68\' | \'set12\'\n    need_degradation = True              # default: True\n    show_img = False                     # default: False\n\n\n\n\n    task_current = \'dn\'       # \'dn\' for denoising | \'sr\' for super-resolution\n    sf = 1                    # unused for denoising\n    if \'color\' in model_name:\n        n_channels = 3        # setting for color image\n        nc = 96               # setting for color image\n        nb = 12               # setting for color image\n    else:\n        n_channels = 1        # setting for grayscale image\n        nc = 64               # setting for grayscale image\n        nb = 15               # setting for grayscale image\n    if \'clip\' in model_name:\n        use_clip = True       # clip the intensities into range of [0, 1]\n    else:\n        use_clip = False\n    model_pool = \'model_zoo\'  # fixed\n    testsets = \'testsets\'     # fixed\n    results = \'results\'       # fixed\n    result_name = testset_name + \'_\' + model_name\n    border = sf if task_current == \'sr\' else 0     # shave boader to calculate PSNR and SSIM\n    model_path = os.path.join(model_pool, model_name+\'.pth\')\n\n    # ----------------------------------------\n    # L_path, E_path, H_path\n    # ----------------------------------------\n\n    L_path = os.path.join(testsets, testset_name) # L_path, for Low-quality images\n    H_path = L_path                               # H_path, for High-quality images\n    E_path = os.path.join(results, result_name)   # E_path, for Estimated images\n    util.mkdir(E_path)\n\n    if H_path == L_path:\n        need_degradation = True\n    logger_name = result_name\n    utils_logger.logger_info(logger_name, log_path=os.path.join(E_path, logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n\n    need_H = True if H_path is not None else False\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # ----------------------------------------\n    # load model\n    # ----------------------------------------\n\n    from models.network_ffdnet import FFDNet as net\n    model = net(in_nc=n_channels, out_nc=n_channels, nc=nc, nb=nb, act_mode=\'R\')\n    model.load_state_dict(torch.load(model_path), strict=True)\n    model.eval()\n    for k, v in model.named_parameters():\n        v.requires_grad = False\n    model = model.to(device)\n    logger.info(\'Model path: {:s}\'.format(model_path))\n\n    test_results = OrderedDict()\n    test_results[\'psnr\'] = []\n    test_results[\'ssim\'] = []\n\n    logger.info(\'model_name:{}, model sigma:{}, image sigma:{}\'.format(model_name, noise_level_img, noise_level_model))\n    logger.info(L_path)\n    L_paths = util.get_image_paths(L_path)\n    H_paths = util.get_image_paths(H_path) if need_H else None\n\n    for idx, img in enumerate(L_paths):\n\n        # ------------------------------------\n        # (1) img_L\n        # ------------------------------------\n\n        img_name, ext = os.path.splitext(os.path.basename(img))\n        # logger.info(\'{:->4d}--> {:>10s}\'.format(idx+1, img_name+ext))\n        img_L = util.imread_uint(img, n_channels=n_channels)\n        img_L = util.uint2single(img_L)\n\n        if need_degradation:  # degradation process\n            np.random.seed(seed=0)  # for reproducibility\n            img_L += np.random.normal(0, noise_level_img/255., img_L.shape)\n            if use_clip:\n                img_L = util.uint2single(util.single2uint(img_L))\n\n        util.imshow(util.single2uint(img_L), title=\'Noisy image with noise level {}\'.format(noise_level_img)) if show_img else None\n\n        img_L = util.single2tensor4(img_L)\n        img_L = img_L.to(device)\n\n        sigma = torch.full((1,1,1,1), noise_level_model/255.).type_as(img_L)\n\n        # ------------------------------------\n        # (2) img_E\n        # ------------------------------------\n\n        img_E = model(img_L, sigma)\n        img_E = util.tensor2uint(img_E)\n\n        if need_H:\n\n            # --------------------------------\n            # (3) img_H\n            # --------------------------------\n            img_H = util.imread_uint(H_paths[idx], n_channels=n_channels)\n            img_H = img_H.squeeze()\n\n            # --------------------------------\n            # PSNR and SSIM\n            # --------------------------------\n\n            psnr = util.calculate_psnr(img_E, img_H, border=border)\n            ssim = util.calculate_ssim(img_E, img_H, border=border)\n            test_results[\'psnr\'].append(psnr)\n            test_results[\'ssim\'].append(ssim)\n            logger.info(\'{:s} - PSNR: {:.2f} dB; SSIM: {:.4f}.\'.format(img_name+ext, psnr, ssim))\n            util.imshow(np.concatenate([img_E, img_H], axis=1), title=\'Recovered / Ground-truth\') if show_img else None\n\n        # ------------------------------------\n        # save results\n        # ------------------------------------\n\n        util.imsave(img_E, os.path.join(E_path, img_name+ext))\n\n    if need_H:\n        ave_psnr = sum(test_results[\'psnr\']) / len(test_results[\'psnr\'])\n        ave_ssim = sum(test_results[\'ssim\']) / len(test_results[\'ssim\'])\n        logger.info(\'Average PSNR/SSIM(RGB) - {} - PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, ave_psnr, ave_ssim))\n\nif __name__ == \'__main__\':\n\n    main()\n'"
main_test_imdn.py,2,"b'import os.path\nimport logging\nimport re\n\nimport numpy as np\nfrom collections import OrderedDict\n\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_model\n\n\n\'\'\'\nSpyder (Python 3.6)\nPyTorch 1.1.0\nWindows 10 or Linux\n\nKai Zhang (cskaizhang@gmail.com)\ngithub: https://github.com/cszn/KAIR\n\nIf you have any question, please feel free to contact with me.\nKai Zhang (e-mail: cskaizhang@gmail.com)\n(github: https://github.com/cszn/KAIR)\n\nby Kai Zhang (12/Dec./2019)\n\'\'\'\n\n""""""\n# --------------------------------------------\n# simplified information multi-distillation\n# network (IMDN) for SR\n# --------------------------------------------\n@inproceedings{hui2019lightweight,\n  title={Lightweight Image Super-Resolution with Information Multi-distillation Network},\n  author={Hui, Zheng and Gao, Xinbo and Yang, Yunchu and Wang, Xiumei},\n  booktitle={Proceedings of the 27th ACM International Conference on Multimedia (ACM MM)},\n  pages={2024--2032},\n  year={2019}\n}\n@inproceedings{zhang2019aim,\n  title={AIM 2019 Challenge on Constrained Super-Resolution: Methods and Results},\n  author={Kai Zhang and Shuhang Gu and Radu Timofte and others},\n  booktitle={IEEE International Conference on Computer Vision Workshops},\n  year={2019}\n}\n# --------------------------------------------\n|--model_zoo                # model_zoo    \n   |--imdn_x4               # model_name, optimized for PSNR\n|--testset                  # testsets\n   |--set5                  # testset_name\n   |--srbsd68\n|--results                  # results\n   |--set5_imdn_x4          # result_name = testset_name + \'_\' + model_name\n# --------------------------------------------\n""""""\n\n\ndef main():\n\n    # ----------------------------------------\n    # Preparation\n    # ----------------------------------------\n\n    model_name = \'imdn_x4\'               # \'imdn_x4\'\n    testset_name = \'set5\'                # test set,  \'set5\' | \'srbsd68\'\n    need_degradation = True              # default: True\n    x8 = False                           # default: False, x8 to boost performance, default: False\n    sf = [int(s) for s in re.findall(r\'\\d+\', model_name)][0]  # scale factor\n    show_img = False                     # default: False\n\n\n\n\n    task_current = \'sr\'       # \'dn\' for denoising | \'sr\' for super-resolution\n    n_channels = 3            # fixed\n    model_pool = \'model_zoo\'  # fixed\n    testsets = \'testsets\'     # fixed\n    results = \'results\'       # fixed\n    noise_level_img = 0       # fixed: 0, noise level for LR image\n    result_name = testset_name + \'_\' + model_name\n    border = sf if task_current == \'sr\' else 0     # shave boader to calculate PSNR and SSIM\n    model_path = os.path.join(model_pool, model_name+\'.pth\')\n\n    # ----------------------------------------\n    # L_path, E_path, H_path\n    # ----------------------------------------\n\n    L_path = os.path.join(testsets, testset_name) # L_path, for Low-quality images\n    H_path = L_path                               # H_path, for High-quality images\n    E_path = os.path.join(results, result_name)   # E_path, for Estimated images\n    util.mkdir(E_path)\n\n    if H_path == L_path:\n        need_degradation = True\n    logger_name = result_name\n    utils_logger.logger_info(logger_name, log_path=os.path.join(E_path, logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n\n    need_H = True if H_path is not None else False\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # ----------------------------------------\n    # load model\n    # ----------------------------------------\n\n    from models.network_imdn import IMDN as net\n    model = net(in_nc=n_channels, out_nc=n_channels, nc=64, nb=8, upscale=4, act_mode=\'L\', upsample_mode=\'pixelshuffle\')\n\n    model.load_state_dict(torch.load(model_path), strict=True)\n    model.eval()\n    for k, v in model.named_parameters():\n        v.requires_grad = False\n    model = model.to(device)\n    logger.info(\'Model path: {:s}\'.format(model_path))\n    number_parameters = sum(map(lambda x: x.numel(), model.parameters()))\n    logger.info(\'Params number: {}\'.format(number_parameters))\n\n    test_results = OrderedDict()\n    test_results[\'psnr\'] = []\n    test_results[\'ssim\'] = []\n    test_results[\'psnr_y\'] = []\n    test_results[\'ssim_y\'] = []\n\n    logger.info(\'model_name:{}, image sigma:{}\'.format(model_name, noise_level_img))\n    logger.info(L_path)\n    L_paths = util.get_image_paths(L_path)\n    H_paths = util.get_image_paths(H_path) if need_H else None\n\n    for idx, img in enumerate(L_paths):\n\n        # ------------------------------------\n        # (1) img_L\n        # ------------------------------------\n\n        img_name, ext = os.path.splitext(os.path.basename(img))\n        # logger.info(\'{:->4d}--> {:>10s}\'.format(idx+1, img_name+ext))\n        img_L = util.imread_uint(img, n_channels=n_channels)\n        img_L = util.uint2single(img_L)\n\n        # degradation process, bicubic downsampling\n        if need_degradation:\n            img_L = util.modcrop(img_L, sf)\n            img_L = util.imresize_np(img_L, 1/sf)\n            # img_L = util.uint2single(util.single2uint(img_L))\n            # np.random.seed(seed=0)  # for reproducibility\n            # img_L += np.random.normal(0, noise_level_img/255., img_L.shape)\n\n        util.imshow(util.single2uint(img_L), title=\'LR image with noise level {}\'.format(noise_level_img)) if show_img else None\n\n        img_L = util.single2tensor4(img_L)\n        img_L = img_L.to(device)\n\n        # ------------------------------------\n        # (2) img_E\n        # ------------------------------------\n\n        if not x8:\n            img_E = model(img_L)\n        else:\n            img_E = utils_model.test_mode(model, img_L, mode=3, sf=sf)\n\n        img_E = util.tensor2uint(img_E)\n\n        if need_H:\n\n            # --------------------------------\n            # (3) img_H\n            # --------------------------------\n\n            img_H = util.imread_uint(H_paths[idx], n_channels=n_channels)\n            img_H = img_H.squeeze()\n            img_H = util.modcrop(img_H, sf)\n\n            # --------------------------------\n            # PSNR and SSIM\n            # --------------------------------\n\n            psnr = util.calculate_psnr(img_E, img_H, border=border)\n            ssim = util.calculate_ssim(img_E, img_H, border=border)\n            test_results[\'psnr\'].append(psnr)\n            test_results[\'ssim\'].append(ssim)\n            logger.info(\'{:s} - PSNR: {:.2f} dB; SSIM: {:.4f}.\'.format(img_name+ext, psnr, ssim))\n            util.imshow(np.concatenate([img_E, img_H], axis=1), title=\'Recovered / Ground-truth\') if show_img else None\n\n            if np.ndim(img_H) == 3:  # RGB image\n                img_E_y = util.rgb2ycbcr(img_E, only_y=True)\n                img_H_y = util.rgb2ycbcr(img_H, only_y=True)\n                psnr_y = util.calculate_psnr(img_E_y, img_H_y, border=border)\n                ssim_y = util.calculate_ssim(img_E_y, img_H_y, border=border)\n                test_results[\'psnr_y\'].append(psnr_y)\n                test_results[\'ssim_y\'].append(ssim_y)\n\n        # ------------------------------------\n        # save results\n        # ------------------------------------\n\n        util.imsave(img_E, os.path.join(E_path, img_name+\'.png\'))\n\n    if need_H:\n        ave_psnr = sum(test_results[\'psnr\']) / len(test_results[\'psnr\'])\n        ave_ssim = sum(test_results[\'ssim\']) / len(test_results[\'ssim\'])\n        logger.info(\'Average PSNR/SSIM(RGB) - {} - x{} --PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, sf, ave_psnr, ave_ssim))\n        if np.ndim(img_H) == 3:\n            ave_psnr_y = sum(test_results[\'psnr_y\']) / len(test_results[\'psnr_y\'])\n            ave_ssim_y = sum(test_results[\'ssim_y\']) / len(test_results[\'ssim_y\'])\n            logger.info(\'Average PSNR/SSIM( Y ) - {} - x{} - PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, sf, ave_psnr_y, ave_ssim_y))\n\nif __name__ == \'__main__\':\n\n    main()\n'"
main_test_ircnn_denoiser.py,2,"b'import os.path\nimport logging\n\nimport numpy as np\nfrom datetime import datetime\nfrom collections import OrderedDict\nfrom scipy.io import loadmat\n\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_model\nfrom utils import utils_image as util\n\n\n\'\'\'\nSpyder (Python 3.6)\nPyTorch 1.1.0\nWindows 10 or Linux\n\nKai Zhang (cskaizhang@gmail.com)\ngithub: https://github.com/cszn/KAIR\n        https://github.com/cszn/IRCNN\n\n@inproceedings{zhang2017learning,\ntitle={Learning deep CNN denoiser prior for image restoration},\nauthor={Zhang, Kai and Zuo, Wangmeng and Gu, Shuhang and Zhang, Lei},\nbooktitle={IEEE conference on computer vision and pattern recognition},\npages={3929--3938},\nyear={2017}\n}\n\n% If you have any question, please feel free to contact with me.\n% Kai Zhang (e-mail: cskaizhang@gmail.com; github: https://github.com/cszn)\n\nby Kai Zhang (12/Dec./2019)\n\'\'\'\n\n""""""\n# --------------------------------------------\n|--model_zoo          # model_zoo\n   |--ircnn_gray      # model_name\n   |--ircnn_color\n|--testset            # testsets\n   |--set12           # testset_name\n   |--bsd68\n   |--cbsd68\n|--results            # results\n   |--set12_ircnn_gray  # result_name = testset_name + \'_\' + model_name\n   |--cbsd68_ircnn_color\n# --------------------------------------------\n""""""\n\n\ndef main():\n\n    # ----------------------------------------\n    # Preparation\n    # ----------------------------------------\n    noise_level_img = 50             # noise level for noisy image\n    model_name = \'ircnn_gray\'        # \'ircnn_gray\' | \'ircnn_color\'\n    testset_name = \'set12\'          # test set, \'bsd68\' | \'set12\'\n    need_degradation = True          # default: True\n    x8 = False                       # default: False, x8 to boost performance\n    show_img = False                 # default: False\n    current_idx = min(24, np.int(np.ceil(noise_level_img/2)-1)) # current_idx+1 th denoiser\n\n\n    task_current = \'dn\'       # fixed, \'dn\' for denoising | \'sr\' for super-resolution\n    sf = 1                    # unused for denoising\n    if \'color\' in model_name:\n        n_channels = 3        # fixed, 1 for grayscale image, 3 for color image \n    else:\n        n_channels = 1        # fixed for grayscale image \n\n    model_pool = \'model_zoo\'  # fixed\n    testsets = \'testsets\'     # fixed\n    results = \'results\'       # fixed\n    result_name = testset_name + \'_\' + model_name     # fixed\n    border = sf if task_current == \'sr\' else 0        # shave boader to calculate PSNR and SSIM\n    model_path = os.path.join(model_pool, model_name+\'.pth\')\n\n    # ----------------------------------------\n    # L_path, E_path, H_path\n    # ----------------------------------------\n    L_path = os.path.join(testsets, testset_name) # L_path, for Low-quality images\n    H_path = L_path                               # H_path, for High-quality images\n    E_path = os.path.join(results, result_name)   # E_path, for Estimated images\n    util.mkdir(E_path)\n\n    if H_path == L_path:\n        need_degradation = True\n    logger_name = result_name\n    utils_logger.logger_info(logger_name, log_path=os.path.join(E_path, logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n\n    need_H = True if H_path is not None else False\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # ----------------------------------------\n    # load model\n    # ----------------------------------------\n    model25 = torch.load(model_path)\n    from models.network_dncnn import IRCNN as net\n    model = net(in_nc=n_channels, out_nc=n_channels, nc=64)\n    model.load_state_dict(model25[str(current_idx)], strict=True)\n    model.eval()\n    for _, v in model.named_parameters():\n        v.requires_grad = False\n    model = model.to(device)\n    logger.info(\'Model path: {:s}\'.format(model_path))\n    number_parameters = sum(map(lambda x: x.numel(), model.parameters()))\n    logger.info(\'Params number: {}\'.format(number_parameters))\n\n    test_results = OrderedDict()\n    test_results[\'psnr\'] = []\n    test_results[\'ssim\'] = []\n\n    logger.info(\'model_name:{}, image sigma:{}\'.format(model_name, noise_level_img))\n    logger.info(L_path)\n    L_paths = util.get_image_paths(L_path)\n    H_paths = util.get_image_paths(H_path) if need_H else None\n\n    for idx, img in enumerate(L_paths):\n\n        # ------------------------------------\n        # (1) img_L\n        # ------------------------------------\n        img_name, ext = os.path.splitext(os.path.basename(img))\n        # logger.info(\'{:->4d}--> {:>10s}\'.format(idx+1, img_name+ext))\n        img_L = util.imread_uint(img, n_channels=n_channels)\n        img_L = util.uint2single(img_L)\n\n        if need_degradation:  # degradation process\n            np.random.seed(seed=0)  # for reproducibility\n            img_L += np.random.normal(0, noise_level_img/255., img_L.shape)\n\n        util.imshow(util.single2uint(img_L), title=\'Noisy image with noise level {}\'.format(noise_level_img)) if show_img else None\n\n        img_L = util.single2tensor4(img_L)\n        img_L = img_L.to(device)\n\n        # ------------------------------------\n        # (2) img_E\n        # ------------------------------------\n        if not x8:\n            img_E = model(img_L)\n        else:\n            img_E = utils_model.test_mode(model, img_L, mode=3)\n\n        img_E = util.tensor2uint(img_E)\n\n        if need_H:\n\n            # --------------------------------\n            # (3) img_H\n            # --------------------------------\n            img_H = util.imread_uint(H_paths[idx], n_channels=n_channels)\n            img_H = img_H.squeeze()\n\n            # --------------------------------\n            # PSNR and SSIM\n            # --------------------------------\n            psnr = util.calculate_psnr(img_E, img_H, border=border)\n            ssim = util.calculate_ssim(img_E, img_H, border=border)\n            test_results[\'psnr\'].append(psnr)\n            test_results[\'ssim\'].append(ssim)\n            logger.info(\'{:s} - PSNR: {:.2f} dB; SSIM: {:.4f}.\'.format(img_name+ext, psnr, ssim))\n            util.imshow(np.concatenate([img_E, img_H], axis=1), title=\'Recovered / Ground-truth\') if show_img else None\n\n        # ------------------------------------\n        # save results\n        # ------------------------------------\n        util.imsave(img_E, os.path.join(E_path, img_name+ext))\n\n    if need_H:\n        ave_psnr = sum(test_results[\'psnr\']) / len(test_results[\'psnr\'])\n        ave_ssim = sum(test_results[\'ssim\']) / len(test_results[\'ssim\'])\n        logger.info(\'Average PSNR/SSIM(RGB) - {} - PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, ave_psnr, ave_ssim))\n\nif __name__ == \'__main__\':\n\n    main()\n'"
main_test_msrresnet.py,2,"b'import os.path\nimport logging\nimport re\n\nimport numpy as np\nfrom collections import OrderedDict\n\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_model\n\n\n\'\'\'\nSpyder (Python 3.6)\nPyTorch 1.1.0\nWindows 10 or Linux\n\nKai Zhang (cskaizhang@gmail.com)\ngithub: https://github.com/cszn/KAIR\n\nIf you have any question, please feel free to contact with me.\nKai Zhang (e-mail: cskaizhang@gmail.com)\n(github: https://github.com/cszn/KAIR)\n\nby Kai Zhang (12/Dec./2019)\n\'\'\'\n\n""""""\n# --------------------------------------------\ntesting demo for RRDB-ESRGAN\nhttps://github.com/xinntao/ESRGAN\n@inproceedings{wang2018esrgan,\n  title={Esrgan: Enhanced super-resolution generative adversarial networks},\n  author={Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Change Loy, Chen},\n  booktitle={European Conference on Computer Vision (ECCV)},\n  pages={0--0},\n  year={2018}\n}\n@inproceedings{ledig2017photo,\n  title={Photo-realistic single image super-resolution using a generative adversarial network},\n  author={Ledig, Christian and Theis, Lucas and Husz{\\\'a}r, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and others},\n  booktitle={IEEE conference on computer vision and pattern recognition},\n  pages={4681--4690},\n  year={2017}\n}\n# --------------------------------------------\n|--model_zoo                # model_zoo\n   |--msrresnet_x4_gan      # model_name, optimized for perceptual quality      \n   |--msrresnet_x4_psnr     # model_name, optimized for PSNR\n|--testset                  # testsets\n   |--set5                  # testset_name\n   |--srbsd68\n|--results                  # results\n   |--set5_msrresnet_x4_gan # result_name = testset_name + \'_\' + model_name\n   |--set5_msrresnet_x4_psnr\n# --------------------------------------------\n""""""\n\n\ndef main():\n\n    # ----------------------------------------\n    # Preparation\n    # ----------------------------------------\n\n    model_name = \'msrresnet_x4_psnr\'     # \'msrresnet_x4_gan\' | \'msrresnet_x4_psnr\'\n    testset_name = \'set5\'                # test set,  \'set5\' | \'srbsd68\'\n    need_degradation = True              # default: True\n    x8 = False                           # default: False, x8 to boost performance, default: False\n    sf = [int(s) for s in re.findall(r\'\\d+\', model_name)][0]  # scale factor\n    show_img = False                     # default: False\n\n\n\n\n    task_current = \'sr\'       # \'dn\' for denoising | \'sr\' for super-resolution\n    n_channels = 3            # fixed\n    model_pool = \'model_zoo\'  # fixed\n    testsets = \'testsets\'     # fixed\n    results = \'results\'       # fixed\n    noise_level_img = 0       # fixed: 0, noise level for LR image\n    result_name = testset_name + \'_\' + model_name\n    border = sf if task_current == \'sr\' else 0     # shave boader to calculate PSNR and SSIM\n    model_path = os.path.join(model_pool, model_name+\'.pth\')\n\n    # ----------------------------------------\n    # L_path, E_path, H_path\n    # ----------------------------------------\n\n    L_path = os.path.join(testsets, testset_name) # L_path, for Low-quality images\n    H_path = L_path                               # H_path, for High-quality images\n    E_path = os.path.join(results, result_name)   # E_path, for Estimated images\n    util.mkdir(E_path)\n\n    if H_path == L_path:\n        need_degradation = True\n    logger_name = result_name\n    utils_logger.logger_info(logger_name, log_path=os.path.join(E_path, logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n\n    need_H = True if H_path is not None else False\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # ----------------------------------------\n    # load model\n    # ----------------------------------------\n\n    from models.network_msrresnet import MSRResNet1 as net\n    model = net(in_nc=n_channels, out_nc=n_channels, nc=64, nb=16, upscale=4)\n    model.load_state_dict(torch.load(model_path), strict=True)\n    model.eval()\n    for k, v in model.named_parameters():\n        v.requires_grad = False\n    model = model.to(device)\n    logger.info(\'Model path: {:s}\'.format(model_path))\n    number_parameters = sum(map(lambda x: x.numel(), model.parameters()))\n    logger.info(\'Params number: {}\'.format(number_parameters))\n\n    test_results = OrderedDict()\n    test_results[\'psnr\'] = []\n    test_results[\'ssim\'] = []\n    test_results[\'psnr_y\'] = []\n    test_results[\'ssim_y\'] = []\n\n    logger.info(\'model_name:{}, image sigma:{}\'.format(model_name, noise_level_img))\n    logger.info(L_path)\n    L_paths = util.get_image_paths(L_path)\n    H_paths = util.get_image_paths(H_path) if need_H else None\n\n    for idx, img in enumerate(L_paths):\n\n        # ------------------------------------\n        # (1) img_L\n        # ------------------------------------\n\n        img_name, ext = os.path.splitext(os.path.basename(img))\n        # logger.info(\'{:->4d}--> {:>10s}\'.format(idx+1, img_name+ext))\n        img_L = util.imread_uint(img, n_channels=n_channels)\n        img_L = util.uint2single(img_L)\n\n        # degradation process, bicubic downsampling\n        if need_degradation:\n            img_L = util.modcrop(img_L, sf)\n            img_L = util.imresize_np(img_L, 1/sf)\n            # img_L = util.uint2single(util.single2uint(img_L))\n            # np.random.seed(seed=0)  # for reproducibility\n            # img_L += np.random.normal(0, noise_level_img/255., img_L.shape)\n\n        util.imshow(util.single2uint(img_L), title=\'LR image with noise level {}\'.format(noise_level_img)) if show_img else None\n\n        img_L = util.single2tensor4(img_L)\n        img_L = img_L.to(device)\n\n        # ------------------------------------\n        # (2) img_E\n        # ------------------------------------\n\n        if not x8:\n            img_E = model(img_L)\n        else:\n            img_E = utils_model.test_mode(model, img_L, mode=3, sf=sf)\n\n        img_E = util.tensor2uint(img_E)\n\n        if need_H:\n\n            # --------------------------------\n            # (3) img_H\n            # --------------------------------\n\n            img_H = util.imread_uint(H_paths[idx], n_channels=n_channels)\n            img_H = img_H.squeeze()\n            img_H = util.modcrop(img_H, sf)\n\n            # --------------------------------\n            # PSNR and SSIM\n            # --------------------------------\n\n            psnr = util.calculate_psnr(img_E, img_H, border=border)\n            ssim = util.calculate_ssim(img_E, img_H, border=border)\n            test_results[\'psnr\'].append(psnr)\n            test_results[\'ssim\'].append(ssim)\n            logger.info(\'{:s} - PSNR: {:.2f} dB; SSIM: {:.4f}.\'.format(img_name+ext, psnr, ssim))\n            util.imshow(np.concatenate([img_E, img_H], axis=1), title=\'Recovered / Ground-truth\') if show_img else None\n\n            if np.ndim(img_H) == 3:  # RGB image\n                img_E_y = util.rgb2ycbcr(img_E, only_y=True)\n                img_H_y = util.rgb2ycbcr(img_H, only_y=True)\n                psnr_y = util.calculate_psnr(img_E_y, img_H_y, border=border)\n                ssim_y = util.calculate_ssim(img_E_y, img_H_y, border=border)\n                test_results[\'psnr_y\'].append(psnr_y)\n                test_results[\'ssim_y\'].append(ssim_y)\n\n        # ------------------------------------\n        # save results\n        # ------------------------------------\n\n        util.imsave(img_E, os.path.join(E_path, img_name+\'.png\'))\n\n    if need_H:\n        ave_psnr = sum(test_results[\'psnr\']) / len(test_results[\'psnr\'])\n        ave_ssim = sum(test_results[\'ssim\']) / len(test_results[\'ssim\'])\n        logger.info(\'Average PSNR/SSIM(RGB) - {} - x{} --PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, sf, ave_psnr, ave_ssim))\n        if np.ndim(img_H) == 3:\n            ave_psnr_y = sum(test_results[\'psnr_y\']) / len(test_results[\'psnr_y\'])\n            ave_ssim_y = sum(test_results[\'ssim_y\']) / len(test_results[\'ssim_y\'])\n            logger.info(\'Average PSNR/SSIM( Y ) - {} - x{} - PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, sf, ave_psnr_y, ave_ssim_y))\n\nif __name__ == \'__main__\':\n\n    main()\n'"
main_test_rrdb.py,2,"b'import os.path\nimport logging\nimport re\n\nimport numpy as np\nfrom collections import OrderedDict\n\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_model\n\n\n\'\'\'\nSpyder (Python 3.6)\nPyTorch 1.1.0\nWindows 10 or Linux\n\nKai Zhang (cskaizhang@gmail.com)\ngithub: https://github.com/cszn/KAIR\n\nIf you have any question, please feel free to contact with me.\nKai Zhang (e-mail: cskaizhang@gmail.com)\n(github: https://github.com/cszn/KAIR)\n\nby Kai Zhang (12/Dec./2019)\n\'\'\'\n\n""""""\n# --------------------------------------------\ntesting demo for RRDB-ESRGAN\nhttps://github.com/xinntao/ESRGAN\n@inproceedings{wang2018esrgan,\n  title={Esrgan: Enhanced super-resolution generative adversarial networks},\n  author={Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Change Loy, Chen},\n  booktitle={European Conference on Computer Vision (ECCV)},\n  pages={0--0},\n  year={2018}\n}\n# --------------------------------------------\n|--model_zoo             # model_zoo\n   |--rrdb_x4_esrgan     # model_name, optimized for perceptual quality      \n   |--rrdb_x4_psnr       # model_name, optimized for PSNR\n|--testset               # testsets\n   |--set5               # testset_name\n   |--srbsd68\n|--results               # results\n   |--set5_rrdb_x4_esrgan# result_name = testset_name + \'_\' + model_name\n   |--set5_rrdb_x4_psnr \n# --------------------------------------------\n""""""\n\n\ndef main():\n\n    # ----------------------------------------\n    # Preparation\n    # ----------------------------------------\n\n    model_name = \'rrdb_x4_esrgan\'        # \'rrdb_x4_esrgan\' | \'rrdb_x4_psnr\'\n    testset_name = \'set5\'                # test set,  \'set5\' | \'srbsd68\'\n    need_degradation = True              # default: True\n    x8 = False                           # default: False, x8 to boost performance\n    sf = [int(s) for s in re.findall(r\'\\d+\', model_name)][0]  # scale factor\n    show_img = False                     # default: False\n\n\n\n\n    task_current = \'sr\'       # \'dn\' for denoising | \'sr\' for super-resolution\n    n_channels = 3            # fixed\n    model_pool = \'model_zoo\'  # fixed\n    testsets = \'testsets\'     # fixed\n    results = \'results\'       # fixed\n    noise_level_img = 0       # fixed: 0, noise level for LR image\n    result_name = testset_name + \'_\' + model_name\n    border = sf if task_current == \'sr\' else 0     # shave boader to calculate PSNR and SSIM\n    model_path = os.path.join(model_pool, model_name+\'.pth\')\n\n    # ----------------------------------------\n    # L_path, E_path, H_path\n    # ----------------------------------------\n\n    L_path = os.path.join(testsets, testset_name) # L_path, for Low-quality images\n    H_path = L_path                               # H_path, for High-quality images\n    E_path = os.path.join(results, result_name)   # E_path, for Estimated images\n    util.mkdir(E_path)\n\n    if H_path == L_path:\n        need_degradation = True\n    logger_name = result_name\n    utils_logger.logger_info(logger_name, log_path=os.path.join(E_path, logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n\n    need_H = True if H_path is not None else False\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # ----------------------------------------\n    # load model\n    # ----------------------------------------\n\n    from models.network_rrdb import RRDB as net\n    model = net(in_nc=n_channels, out_nc=n_channels, nc=64, nb=23, gc=32, upscale=4, act_mode=\'L\', upsample_mode=\'upconv\')\n    model.load_state_dict(torch.load(model_path), strict=False)\n    model.eval()\n    for k, v in model.named_parameters():\n        v.requires_grad = False\n    model = model.to(device)\n    logger.info(\'Model path: {:s}\'.format(model_path))\n    number_parameters = sum(map(lambda x: x.numel(), model.parameters()))\n    logger.info(\'Params number: {}\'.format(number_parameters))\n\n    test_results = OrderedDict()\n    test_results[\'psnr\'] = []\n    test_results[\'ssim\'] = []\n    test_results[\'psnr_y\'] = []\n    test_results[\'ssim_y\'] = []\n\n    logger.info(\'model_name:{}, image sigma:{}\'.format(model_name, noise_level_img))\n    logger.info(L_path)\n    L_paths = util.get_image_paths(L_path)\n    H_paths = util.get_image_paths(H_path) if need_H else None\n\n    for idx, img in enumerate(L_paths):\n\n        # ------------------------------------\n        # (1) img_L\n        # ------------------------------------\n\n        img_name, ext = os.path.splitext(os.path.basename(img))\n        # logger.info(\'{:->4d}--> {:>10s}\'.format(idx+1, img_name+ext))\n        img_L = util.imread_uint(img, n_channels=n_channels)\n        img_L = util.uint2single(img_L)\n\n        # degradation process, bicubic downsampling + Gaussian noise\n        if need_degradation:\n            img_L = util.modcrop(img_L, sf)\n            img_L = util.imresize_np(img_L, 1/sf)\n            # np.random.seed(seed=0)  # for reproducibility\n            # img_L += np.random.normal(0, noise_level_img/255., img_L.shape)\n\n        util.imshow(util.single2uint(img_L), title=\'LR image with noise level {}\'.format(noise_level_img)) if show_img else None\n\n        img_L = util.single2tensor4(img_L)\n        img_L = img_L.to(device)\n\n        # ------------------------------------\n        # (2) img_E\n        # ------------------------------------\n\n        if not x8:\n            img_E = model(img_L)\n        else:\n            img_E = utils_model.test_mode(model, img_L, mode=3, sf=sf)\n\n        img_E = util.tensor2uint(img_E)\n\n        if need_H:\n\n            # --------------------------------\n            # (3) img_H\n            # --------------------------------\n\n            img_H = util.imread_uint(H_paths[idx], n_channels=n_channels)\n            img_H = img_H.squeeze()\n            img_H = util.modcrop(img_H, sf)\n\n            # --------------------------------\n            # PSNR and SSIM\n            # --------------------------------\n\n            psnr = util.calculate_psnr(img_E, img_H, border=border)\n            ssim = util.calculate_ssim(img_E, img_H, border=border)\n            test_results[\'psnr\'].append(psnr)\n            test_results[\'ssim\'].append(ssim)\n            logger.info(\'{:s} - PSNR: {:.2f} dB; SSIM: {:.4f}.\'.format(img_name+ext, psnr, ssim))\n            util.imshow(np.concatenate([img_E, img_H], axis=1), title=\'Recovered / Ground-truth\') if show_img else None\n\n            if np.ndim(img_H) == 3:  # RGB image\n                img_E_y = util.rgb2ycbcr(img_E, only_y=True)\n                img_H_y = util.rgb2ycbcr(img_H, only_y=True)\n                psnr_y = util.calculate_psnr(img_E_y, img_H_y, border=border)\n                ssim_y = util.calculate_ssim(img_E_y, img_H_y, border=border)\n                test_results[\'psnr_y\'].append(psnr_y)\n                test_results[\'ssim_y\'].append(ssim_y)\n\n        # ------------------------------------\n        # save results\n        # ------------------------------------\n\n        util.imsave(img_E, os.path.join(E_path, img_name+\'.png\'))\n\n    if need_H:\n        ave_psnr = sum(test_results[\'psnr\']) / len(test_results[\'psnr\'])\n        ave_ssim = sum(test_results[\'ssim\']) / len(test_results[\'ssim\'])\n        logger.info(\'Average PSNR/SSIM(RGB) - {} - x{} --PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, sf, ave_psnr, ave_ssim))\n        if np.ndim(img_H) == 3:\n            ave_psnr_y = sum(test_results[\'psnr_y\']) / len(test_results[\'psnr_y\'])\n            ave_ssim_y = sum(test_results[\'ssim_y\']) / len(test_results[\'ssim_y\'])\n            logger.info(\'Average PSNR/SSIM( Y ) - {} - x{} - PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, sf, ave_psnr_y, ave_ssim_y))\n\nif __name__ == \'__main__\':\n\n    main()\n'"
main_test_srmd.py,4,"b'import os.path\nimport logging\nimport re\n\nimport numpy as np\nfrom collections import OrderedDict\nfrom scipy.io import loadmat\n\nimport torch\n\nfrom utils import utils_deblur\nfrom utils import utils_sisr as sr\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_model\n\n\n\'\'\'\nSpyder (Python 3.6)\nPyTorch 1.1.0\nWindows 10 or Linux\n\nKai Zhang (cskaizhang@gmail.com)\ngithub: https://github.com/cszn/KAIR\n        https://github.com/cszn/SRMD\n\n@inproceedings{zhang2018learning,\n  title={Learning a single convolutional super-resolution network for multiple degradations},\n  author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={3262--3271},\n  year={2018}\n}\n\n% If you have any question, please feel free to contact with me.\n% Kai Zhang (e-mail: cskaizhang@gmail.com; github: https://github.com/cszn)\n\nby Kai Zhang (12/Dec./2019)\n\'\'\'\n\n""""""\n# --------------------------------------------\n|--model_zoo             # model_zoo\n   |--srmdnf_x2          # model_name, for noise-free LR image SR\n   |--srmdnf_x3 \n   |--srmdnf_x4\n   |--srmd_x2            # model_name, for noisy LR image\n   |--srmd_x3 \n   |--srmd_x4\n|--testset               # testsets\n   |--set5               # testset_name\n   |--srbsd68\n|--results               # results\n   |--set5_srmdnf_x2     # result_name = testset_name + \'_\' + model_name\n   |--set5_srmdnf_x3\n   |--set5_srmdnf_x4\n   |--set5_srmd_x2\n   |--srbsd68_srmd_x2\n# --------------------------------------------\n""""""\n\n\ndef main():\n\n    # ----------------------------------------\n    # Preparation\n    # ----------------------------------------\n\n    noise_level_img = 0                  # default: 0, noise level for LR image\n    noise_level_model = noise_level_img  # noise level for model \n    model_name = \'srmdnf_x4\'             # \'srmd_x2\' | \'srmd_x3\' | \'srmd_x4\' | \'srmdnf_x2\' | \'srmdnf_x3\' | \'srmdnf_x4\'\n    testset_name = \'set5\'                # test set,  \'set5\' | \'srbsd68\'\n    sf = [int(s) for s in re.findall(r\'\\d+\', model_name)][0]  # scale factor\n    x8 = False                           # default: False, x8 to boost performance\n    need_degradation = True              # default: True, use degradation model to generate LR image\n    show_img = False                     # default: False\n\n\n\n\n    srmd_pca_path = os.path.join(\'kernels\', \'srmd_pca_matlab.mat\')\n    task_current = \'sr\'       # \'dn\' for denoising | \'sr\' for super-resolution\n    n_channels = 3            # fixed\n    in_nc = 18 if \'nf\' in model_name else 19\n    nc = 128                  # fixed, number of channels\n    nb = 12                   # fixed, number of conv layers\n    model_pool = \'model_zoo\'  # fixed\n    testsets = \'testsets\'     # fixed\n    results = \'results\'       # fixed\n    result_name = testset_name + \'_\' + model_name\n    border = sf if task_current == \'sr\' else 0     # shave boader to calculate PSNR and SSIM\n    model_path = os.path.join(model_pool, model_name+\'.pth\')\n\n    # ----------------------------------------\n    # L_path, E_path, H_path\n    # ----------------------------------------\n\n    L_path = os.path.join(testsets, testset_name) # L_path, for Low-quality images\n    H_path = L_path                               # H_path, for High-quality images\n    E_path = os.path.join(results, result_name)   # E_path, for Estimated images\n    util.mkdir(E_path)\n\n    if H_path == L_path:\n        need_degradation = True\n    logger_name = result_name\n    utils_logger.logger_info(logger_name, log_path=os.path.join(E_path, logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n\n    need_H = True if H_path is not None else False\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # ----------------------------------------\n    # load model\n    # ----------------------------------------\n\n    from models.network_srmd import SRMD as net\n    model = net(in_nc=in_nc, out_nc=n_channels, nc=nc, nb=nb, upscale=sf, act_mode=\'R\', upsample_mode=\'pixelshuffle\')\n    model.load_state_dict(torch.load(model_path), strict=False)\n    model.eval()\n    for k, v in model.named_parameters():\n        v.requires_grad = False\n    model = model.to(device)\n    logger.info(\'Model path: {:s}\'.format(model_path))\n    number_parameters = sum(map(lambda x: x.numel(), model.parameters()))\n    logger.info(\'Params number: {}\'.format(number_parameters))\n\n    test_results = OrderedDict()\n    test_results[\'psnr\'] = []\n    test_results[\'ssim\'] = []\n    test_results[\'psnr_y\'] = []\n    test_results[\'ssim_y\'] = []\n\n    logger.info(\'model_name:{}, model sigma:{}, image sigma:{}\'.format(model_name, noise_level_img, noise_level_model))\n    logger.info(L_path)\n    L_paths = util.get_image_paths(L_path)\n    H_paths = util.get_image_paths(H_path) if need_H else None\n\n    # ----------------------------------------\n    # kernel and PCA reduced feature\n    # ----------------------------------------\n\n    # kernel = sr.anisotropic_Gaussian(ksize=15, theta=np.pi, l1=4, l2=4)\n    kernel = utils_deblur.fspecial(\'gaussian\', 15, 0.01)  # Gaussian kernel, delta kernel 0.01\n\n    P = loadmat(srmd_pca_path)[\'P\']\n    degradation_vector = np.dot(P, np.reshape(kernel, (-1), order=""F""))\n    if \'nf\' not in model_name:  # noise-free SR\n        degradation_vector = np.append(degradation_vector, noise_level_model/255.)\n    degradation_vector = torch.from_numpy(degradation_vector).view(1, -1, 1, 1).float()\n\n    for idx, img in enumerate(L_paths):\n\n        # ------------------------------------\n        # (1) img_L\n        # ------------------------------------\n\n        img_name, ext = os.path.splitext(os.path.basename(img))\n        # logger.info(\'{:->4d}--> {:>10s}\'.format(idx+1, img_name+ext))\n        img_L = util.imread_uint(img, n_channels=n_channels)\n        img_L = util.uint2single(img_L)\n\n        # degradation process, blur + bicubic downsampling + Gaussian noise\n        if need_degradation:\n            img_L = util.modcrop(img_L, sf)\n            img_L = sr.srmd_degradation(img_L, kernel, sf)  # equivalent to bicubic degradation if kernel is a delta kernel\n            np.random.seed(seed=0)  # for reproducibility\n            img_L += np.random.normal(0, noise_level_img/255., img_L.shape)\n\n        util.imshow(util.single2uint(img_L), title=\'LR image with noise level {}\'.format(noise_level_img)) if show_img else None\n\n        img_L = util.single2tensor4(img_L)\n        degradation_map = degradation_vector.repeat(1, 1, img_L.size(-2), img_L.size(-1))\n        img_L = torch.cat((img_L, degradation_map), dim=1)\n        img_L = img_L.to(device)\n\n        # ------------------------------------\n        # (2) img_E\n        # ------------------------------------\n\n        if not x8:\n            img_E = model(img_L)\n        else:\n            img_E = utils_model.test_mode(model, img_L, mode=3, sf=sf)\n\n        img_E = util.tensor2uint(img_E)\n\n        if need_H:\n\n            # --------------------------------\n            # (3) img_H\n            # --------------------------------\n\n            img_H = util.imread_uint(H_paths[idx], n_channels=n_channels)\n            img_H = img_H.squeeze()\n            img_H = util.modcrop(img_H, sf)\n\n            # --------------------------------\n            # PSNR and SSIM\n            # --------------------------------\n\n            psnr = util.calculate_psnr(img_E, img_H, border=border)\n            ssim = util.calculate_ssim(img_E, img_H, border=border)\n            test_results[\'psnr\'].append(psnr)\n            test_results[\'ssim\'].append(ssim)\n            logger.info(\'{:s} - PSNR: {:.2f} dB; SSIM: {:.4f}.\'.format(img_name+ext, psnr, ssim))\n            util.imshow(np.concatenate([img_E, img_H], axis=1), title=\'Recovered / Ground-truth\') if show_img else None\n\n            if np.ndim(img_H) == 3:  # RGB image\n                img_E_y = util.rgb2ycbcr(img_E, only_y=True)\n                img_H_y = util.rgb2ycbcr(img_H, only_y=True)\n                psnr_y = util.calculate_psnr(img_E_y, img_H_y, border=border)\n                ssim_y = util.calculate_ssim(img_E_y, img_H_y, border=border)\n                test_results[\'psnr_y\'].append(psnr_y)\n                test_results[\'ssim_y\'].append(ssim_y)\n\n        # ------------------------------------\n        # save results\n        # ------------------------------------\n\n        util.imsave(img_E, os.path.join(E_path, img_name+\'.png\'))\n\n    if need_H:\n        ave_psnr = sum(test_results[\'psnr\']) / len(test_results[\'psnr\'])\n        ave_ssim = sum(test_results[\'ssim\']) / len(test_results[\'ssim\'])\n        logger.info(\'Average PSNR/SSIM(RGB) - {} - x{} --PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, sf, ave_psnr, ave_ssim))\n        if np.ndim(img_H) == 3:\n            ave_psnr_y = sum(test_results[\'psnr_y\']) / len(test_results[\'psnr_y\'])\n            ave_ssim_y = sum(test_results[\'ssim_y\']) / len(test_results[\'ssim_y\'])\n            logger.info(\'Average PSNR/SSIM( Y ) - {} - x{} - PSNR: {:.2f} dB; SSIM: {:.4f}\'.format(result_name, sf, ave_psnr_y, ave_ssim_y))\n\nif __name__ == \'__main__\':\n\n    main()\n'"
main_train_dncnn.py,3,"b'import os.path\nimport math\nimport argparse\nimport time\nimport random\nimport numpy as np\nfrom collections import OrderedDict\nimport logging\nimport torch\nfrom torch.utils.data import DataLoader\n\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_option as option\n\nfrom data.select_dataset import define_Dataset\nfrom models.select_model import define_Model\n\n\n\'\'\'\n# --------------------------------------------\n# training code for DnCNN\n# --------------------------------------------\n# Kai Zhang (cskaizhang@gmail.com)\n# github: https://github.com/cszn/KAIR\n#         https://github.com/cszn/DnCNN\n#\n# Reference:\n@article{zhang2017beyond,\n  title={Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising},\n  author={Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},\n  journal={IEEE Transactions on Image Processing},\n  volume={26},\n  number={7},\n  pages={3142--3155},\n  year={2017},\n  publisher={IEEE}\n}\n# --------------------------------------------\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\ndef main(json_path=\'options/train_dncnn.json\'):\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--1 (prepare opt)\n    # ----------------------------------------\n    \'\'\'\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-opt\', type=str, default=json_path, help=\'Path to option JSON file.\')\n\n    opt = option.parse(parser.parse_args().opt, is_train=True)\n    util.mkdirs((path for key, path in opt[\'path\'].items() if \'pretrained\' not in key))\n\n    # ----------------------------------------\n    # update opt\n    # ----------------------------------------\n    # -->-->-->-->-->-->-->-->-->-->-->-->-->-\n    init_iter, init_path_G = option.find_last_checkpoint(opt[\'path\'][\'models\'], net_type=\'G\')\n    opt[\'path\'][\'pretrained_netG\'] = init_path_G\n    current_step = init_iter\n\n    border = 0\n    # --<--<--<--<--<--<--<--<--<--<--<--<--<-\n\n    # ----------------------------------------\n    # save opt to  a \'../option.json\' file\n    # ----------------------------------------\n    option.save(opt)\n\n    # ----------------------------------------\n    # return None for missing key\n    # ----------------------------------------\n    opt = option.dict_to_nonedict(opt)\n\n    # ----------------------------------------\n    # configure logger\n    # ----------------------------------------\n    logger_name = \'train\'\n    utils_logger.logger_info(logger_name, os.path.join(opt[\'path\'][\'log\'], logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n    logger.info(option.dict2str(opt))\n\n    # ----------------------------------------\n    # seed\n    # ----------------------------------------\n    seed = opt[\'train\'][\'manual_seed\']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    logger.info(\'Random seed: {}\'.format(seed))\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--2 (creat dataloader)\n    # ----------------------------------------\n    \'\'\'\n\n    # ----------------------------------------\n    # 1) create_dataset\n    # 2) creat_dataloader for train and test\n    # ----------------------------------------\n    dataset_type = opt[\'datasets\'][\'train\'][\'dataset_type\']\n    for phase, dataset_opt in opt[\'datasets\'].items():\n        if phase == \'train\':\n            train_set = define_Dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt[\'dataloader_batch_size\']))\n            logger.info(\'Number of train images: {:,d}, iters: {:,d}\'.format(len(train_set), train_size))\n            train_loader = DataLoader(train_set,\n                                      batch_size=dataset_opt[\'dataloader_batch_size\'],\n                                      shuffle=dataset_opt[\'dataloader_shuffle\'],\n                                      num_workers=dataset_opt[\'dataloader_num_workers\'],\n                                      drop_last=True,\n                                      pin_memory=True)\n        elif phase == \'test\':\n            test_set = define_Dataset(dataset_opt)\n            test_loader = DataLoader(test_set, batch_size=1,\n                                     shuffle=False, num_workers=1,\n                                     drop_last=False, pin_memory=True)\n        else:\n            raise NotImplementedError(""Phase [%s] is not recognized."" % phase)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--3 (initialize model)\n    # ----------------------------------------\n    \'\'\'\n\n    model = define_Model(opt)\n\n    if opt[\'merge_bn\'] and current_step > opt[\'merge_bn_startpoint\']:\n        logger.info(\'^_^ -----merging bnorm----- ^_^\')\n        model.merge_bnorm_test()\n\n    logger.info(model.info_network())\n    model.init_train()\n    logger.info(model.info_params())\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--4 (main training)\n    # ----------------------------------------\n    \'\'\'\n\n    for epoch in range(1000000):  # keep running\n        for i, train_data in enumerate(train_loader):\n\n            current_step += 1\n\n            if dataset_type == \'dnpatch\' and current_step % 20000 == 0:  # for \'train400\'\n                train_loader.dataset.update_data()\n\n            # -------------------------------\n            # 1) update learning rate\n            # -------------------------------\n            model.update_learning_rate(current_step)\n\n            # -------------------------------\n            # 2) feed patch pairs\n            # -------------------------------\n            model.feed_data(train_data)\n\n            # -------------------------------\n            # 3) optimize parameters\n            # -------------------------------\n            model.optimize_parameters(current_step)\n\n            # -------------------------------\n            # merge bnorm\n            # -------------------------------\n            if opt[\'merge_bn\'] and opt[\'merge_bn_startpoint\'] == current_step:\n                logger.info(\'^_^ -----merging bnorm----- ^_^\')\n                model.merge_bnorm_train()\n                model.print_network()\n\n            # -------------------------------\n            # 4) training information\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_print\'] == 0:\n                logs = model.current_log()  # such as loss\n                message = \'<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> \'.format(epoch, current_step, model.current_learning_rate())\n                for k, v in logs.items():  # merge log information into message\n                    message += \'{:s}: {:.3e} \'.format(k, v)\n                logger.info(message)\n\n            # -------------------------------\n            # 5) save model\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_save\'] == 0:\n                logger.info(\'Saving the model.\')\n                model.save(current_step)\n\n            # -------------------------------\n            # 6) testing\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_test\'] == 0:\n\n                avg_psnr = 0.0\n                idx = 0\n\n                for test_data in test_loader:\n                    idx += 1\n                    image_name_ext = os.path.basename(test_data[\'L_path\'][0])\n                    img_name, ext = os.path.splitext(image_name_ext)\n\n                    img_dir = os.path.join(opt[\'path\'][\'images\'], img_name)\n                    util.mkdir(img_dir)\n\n                    model.feed_data(test_data)\n                    model.test()\n\n                    visuals = model.current_visuals()\n                    E_img = util.tensor2uint(visuals[\'E\'])\n                    H_img = util.tensor2uint(visuals[\'H\'])\n\n                    # -----------------------\n                    # save estimated image E\n                    # -----------------------\n                    save_img_path = os.path.join(img_dir, \'{:s}_{:d}.png\'.format(img_name, current_step))\n                    util.imsave(E_img, save_img_path)\n\n                    # -----------------------\n                    # calculate PSNR\n                    # -----------------------\n                    current_psnr = util.calculate_psnr(E_img, H_img, border=border)\n\n                    logger.info(\'{:->4d}--> {:>10s} | {:<4.2f}dB\'.format(idx, image_name_ext, current_psnr))\n\n                    avg_psnr += current_psnr\n\n                avg_psnr = avg_psnr / idx\n\n                # testing log\n                logger.info(\'<epoch:{:3d}, iter:{:8,d}, Average PSNR : {:<.2f}dB\\n\'.format(epoch, current_step, avg_psnr))\n\n    logger.info(\'Saving the final model.\')\n    model.save(\'latest\')\n    logger.info(\'End of training.\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
main_train_dpsr.py,3,"b'import os.path\nimport math\nimport argparse\nimport time\nimport random\nimport numpy as np\nfrom collections import OrderedDict\nimport logging\nfrom torch.utils.data import DataLoader\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_option as option\n\nfrom data.select_dataset import define_Dataset\nfrom models.select_model import define_Model\n\n\n\'\'\'\n# --------------------------------------------\n# training code for the sueper-resolver prior of DPSR\n# --------------------------------------------\n# Kai Zhang (cskaizhang@gmail.com)\n# github: https://github.com/cszn/KAIR\n#         https://github.com/cszn/DPSR\n#\n# Reference:\n@inproceedings{zhang2019deep,\n  title={Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels},\n  author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={1671--1681},\n  year={2019}\n}\n# --------------------------------------------\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\ndef main(json_path=\'options/train_dpsr.json\'):\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--1 (prepare opt)\n    # ----------------------------------------\n    \'\'\'\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-opt\', type=str, default=json_path, help=\'Path to option JSON file.\')\n\n    opt = option.parse(parser.parse_args().opt, is_train=True)\n    util.mkdirs((path for key, path in opt[\'path\'].items() if \'pretrained\' not in key))\n\n    # ----------------------------------------\n    # update opt\n    # ----------------------------------------\n    # -->-->-->-->-->-->-->-->-->-->-->-->-->-\n    init_iter, init_path_G = option.find_last_checkpoint(opt[\'path\'][\'models\'], net_type=\'G\')\n    opt[\'path\'][\'pretrained_netG\'] = init_path_G\n    current_step = init_iter\n\n    border = opt[\'scale\']\n    # --<--<--<--<--<--<--<--<--<--<--<--<--<-\n\n    # ----------------------------------------\n    # save opt to  a \'../option.json\' file\n    # ----------------------------------------\n    option.save(opt)\n\n    # ----------------------------------------\n    # return None for missing key\n    # ----------------------------------------\n    opt = option.dict_to_nonedict(opt)\n\n    # ----------------------------------------\n    # configure logger\n    # ----------------------------------------\n    logger_name = \'train\'\n    utils_logger.logger_info(logger_name, os.path.join(opt[\'path\'][\'log\'], logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n    logger.info(option.dict2str(opt))\n\n    # ----------------------------------------\n    # seed\n    # ----------------------------------------\n    seed = opt[\'train\'][\'manual_seed\']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    logger.info(\'Random seed: {}\'.format(seed))\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--2 (creat dataloader)\n    # ----------------------------------------\n    \'\'\'\n\n    # ----------------------------------------\n    # 1) create_dataset\n    # 2) creat_dataloader for train and test\n    # ----------------------------------------\n    for phase, dataset_opt in opt[\'datasets\'].items():\n        if phase == \'train\':\n            train_set = define_Dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt[\'dataloader_batch_size\']))\n            logger.info(\'Number of train images: {:,d}, iters: {:,d}\'.format(len(train_set), train_size))\n            train_loader = DataLoader(train_set,\n                                      batch_size=dataset_opt[\'dataloader_batch_size\'],\n                                      shuffle=dataset_opt[\'dataloader_shuffle\'],\n                                      num_workers=dataset_opt[\'dataloader_num_workers\'],\n                                      drop_last=True,\n                                      pin_memory=True)\n        elif phase == \'test\':\n            test_set = define_Dataset(dataset_opt)\n            test_loader = DataLoader(test_set, batch_size=1,\n                                     shuffle=False, num_workers=1,\n                                     drop_last=False, pin_memory=True)\n        else:\n            raise NotImplementedError(""Phase [%s] is not recognized."" % phase)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--3 (initialize model)\n    # ----------------------------------------\n    \'\'\'\n\n    model = define_Model(opt)\n\n#    if opt[\'merge_bn\'] and current_step > opt[\'merge_bn_startpoint\']:\n#        logger.info(\'^_^ -----merging bnorm----- ^_^\')\n#        model.merge_bnorm_test()\n\n    logger.info(model.info_network())\n    model.init_train()\n    logger.info(model.info_params())\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--4 (main training)\n    # ----------------------------------------\n    \'\'\'\n\n    for epoch in range(1000000):  # keep running\n        for i, train_data in enumerate(train_loader):\n\n            current_step += 1\n\n            # -------------------------------\n            # 1) update learning rate\n            # -------------------------------\n            model.update_learning_rate(current_step)\n\n            # -------------------------------\n            # 2) feed patch pairs\n            # -------------------------------\n            model.feed_data(train_data)\n\n            # -------------------------------\n            # 3) optimize parameters\n            # -------------------------------\n            model.optimize_parameters(current_step)\n\n            # -------------------------------\n            # merge bnorm\n            # -------------------------------\n#            if opt[\'merge_bn\'] and opt[\'merge_bn_startpoint\'] == current_step:\n#                logger.info(\'^_^ -----merging bnorm----- ^_^\')\n#                model.merge_bnorm_train()\n#                model.print_network()\n\n            # -------------------------------\n            # 4) training information\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_print\'] == 0:\n                logs = model.current_log()  # such as loss\n                message = \'<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> \'.format(epoch, current_step, model.current_learning_rate())\n                for k, v in logs.items():  # merge log information into message\n                    message += \'{:s}: {:.3e} \'.format(k, v)\n                logger.info(message)\n\n            # -------------------------------\n            # 5) save model\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_save\'] == 0:\n                logger.info(\'Saving the model.\')\n                model.save(current_step)\n\n            # -------------------------------\n            # 6) testing\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_test\'] == 0:\n\n                avg_psnr = 0.0\n                idx = 0\n\n                for test_data in test_loader:\n                    idx += 1\n                    image_name_ext = os.path.basename(test_data[\'L_path\'][0])\n                    img_name, ext = os.path.splitext(image_name_ext)\n\n                    img_dir = os.path.join(opt[\'path\'][\'images\'], img_name)\n                    util.mkdir(img_dir)\n\n                    model.feed_data(test_data)\n                    model.test()\n\n                    visuals = model.current_visuals()\n                    E_img = util.tensor2uint(visuals[\'E\'])\n                    H_img = util.tensor2uint(visuals[\'H\'])\n\n                    # -----------------------\n                    # save estimated image E\n                    # -----------------------\n                    save_img_path = os.path.join(img_dir, \'{:s}_{:d}.png\'.format(img_name, current_step))\n                    util.imsave(E_img, save_img_path)\n\n                    # -----------------------\n                    # calculate PSNR\n                    # -----------------------\n                    current_psnr = util.calculate_psnr(E_img, H_img, border=border)\n\n                    logger.info(\'{:->4d}--> {:>10s} | {:<4.2f}dB\'.format(idx, image_name_ext, current_psnr))\n\n                    avg_psnr += current_psnr\n\n                avg_psnr = avg_psnr / idx\n\n                # testing log\n                logger.info(\'<epoch:{:3d}, iter:{:8,d}, Average PSNR : {:<.2f}dB\\n\'.format(epoch, current_step, avg_psnr))\n\n    logger.info(\'Saving the final model.\')\n    model.save(\'latest\')\n    logger.info(\'End of training.\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
main_train_fdncnn.py,3,"b'import os.path\nimport math\nimport argparse\nimport time\nimport random\nimport numpy as np\nfrom collections import OrderedDict\nimport logging\nfrom torch.utils.data import DataLoader\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_option as option\n\nfrom data.select_dataset import define_Dataset\nfrom models.select_model import define_Model\n\n\n\'\'\'\n# --------------------------------------------\n# training code for Flexible DnCNN\n# a single model for a wide range of noise levels\n# --------------------------------------------\n# Kai Zhang (cskaizhang@gmail.com)\n# github: https://github.com/cszn/KAIR\n#         https://github.com/cszn/DnCNN\n#         https://github.com/cszn/FFDNet\n#\n# Reference:\n@article{zhang2017beyond,\n  title={Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising},\n  author={Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},\n  journal={IEEE Transactions on Image Processing},\n  volume={26},\n  number={7},\n  pages={3142--3155},\n  year={2017},\n  publisher={IEEE}\n}\n# --------------------------------------------\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\ndef main(json_path=\'options/train_fdncnn.json\'):\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--1 (prepare opt)\n    # ----------------------------------------\n    \'\'\'\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-opt\', type=str, default=json_path, help=\'Path to option JSON file.\')\n\n    opt = option.parse(parser.parse_args().opt, is_train=True)\n    util.mkdirs((path for key, path in opt[\'path\'].items() if \'pretrained\' not in key))\n\n    # ----------------------------------------\n    # update opt\n    # ----------------------------------------\n    # -->-->-->-->-->-->-->-->-->-->-->-->-->-\n    init_iter, init_path_G = option.find_last_checkpoint(opt[\'path\'][\'models\'], net_type=\'G\')\n    opt[\'path\'][\'pretrained_netG\'] = init_path_G\n    current_step = init_iter\n\n    border = 0\n    # --<--<--<--<--<--<--<--<--<--<--<--<--<-\n\n    # ----------------------------------------\n    # save opt to  a \'../option.json\' file\n    # ----------------------------------------\n    option.save(opt)\n\n    # ----------------------------------------\n    # return None for missing key\n    # ----------------------------------------\n    opt = option.dict_to_nonedict(opt)\n\n    # ----------------------------------------\n    # configure logger\n    # ----------------------------------------\n    logger_name = \'train\'\n    utils_logger.logger_info(logger_name, os.path.join(opt[\'path\'][\'log\'], logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n    logger.info(option.dict2str(opt))\n\n    # ----------------------------------------\n    # seed\n    # ----------------------------------------\n    seed = opt[\'train\'][\'manual_seed\']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    logger.info(\'Random seed: {}\'.format(seed))\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--2 (creat dataloader)\n    # ----------------------------------------\n    \'\'\'\n\n    # ----------------------------------------\n    # 1) create_dataset\n    # 2) creat_dataloader for train and test\n    # ----------------------------------------\n    for phase, dataset_opt in opt[\'datasets\'].items():\n        if phase == \'train\':\n            train_set = define_Dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt[\'dataloader_batch_size\']))\n            logger.info(\'Number of train images: {:,d}, iters: {:,d}\'.format(len(train_set), train_size))\n            train_loader = DataLoader(train_set,\n                                      batch_size=dataset_opt[\'dataloader_batch_size\'],\n                                      shuffle=dataset_opt[\'dataloader_shuffle\'],\n                                      num_workers=dataset_opt[\'dataloader_num_workers\'],\n                                      drop_last=True,\n                                      pin_memory=True)\n        elif phase == \'test\':\n            test_set = define_Dataset(dataset_opt)\n            test_loader = DataLoader(test_set, batch_size=1,\n                                     shuffle=False, num_workers=1,\n                                     drop_last=False, pin_memory=True)\n        else:\n            raise NotImplementedError(""Phase [%s] is not recognized."" % phase)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--3 (initialize model)\n    # ----------------------------------------\n    \'\'\'\n\n    model = define_Model(opt)\n\n    if opt[\'merge_bn\'] and current_step > opt[\'merge_bn_startpoint\']:\n        logger.info(\'^_^ -----merging bnorm----- ^_^\')\n        model.merge_bnorm_train()\n\n    logger.info(model.info_network())\n    model.init_train()\n    logger.info(model.info_params())\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--4 (main training)\n    # ----------------------------------------\n    \'\'\'\n\n    for epoch in range(1000000):  # keep running\n        for i, train_data in enumerate(train_loader):\n\n            current_step += 1\n\n            # -------------------------------\n            # 1) update learning rate\n            # -------------------------------\n            model.update_learning_rate(current_step)\n\n            # -------------------------------\n            # 2) feed patch pairs\n            # -------------------------------\n            model.feed_data(train_data)\n\n            # -------------------------------\n            # 3) optimize parameters\n            # -------------------------------\n            model.optimize_parameters(current_step)\n\n            # -------------------------------\n            # merge bnorm\n            # -------------------------------\n            if opt[\'merge_bn\'] and opt[\'merge_bn_startpoint\'] == current_step:\n                logger.info(\'^_^ -----merging bnorm----- ^_^\')\n                model.merge_bnorm_train()\n                model.print_network()\n\n            # -------------------------------\n            # 4) training information\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_print\'] == 0:\n                logs = model.current_log()  # such as loss\n                message = \'<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> \'.format(epoch, current_step, model.current_learning_rate())\n                for k, v in logs.items():  # merge log information into message\n                    message += \'{:s}: {:.3e} \'.format(k, v)\n                logger.info(message)\n\n            # -------------------------------\n            # 5) save model\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_save\'] == 0:\n                logger.info(\'Saving the model.\')\n                model.save(current_step)\n\n            # -------------------------------\n            # 6) testing\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_test\'] == 0:\n\n                avg_psnr = 0.0\n                idx = 0\n\n                for test_data in test_loader:\n                    idx += 1\n                    image_name_ext = os.path.basename(test_data[\'L_path\'][0])\n                    img_name, ext = os.path.splitext(image_name_ext)\n\n                    img_dir = os.path.join(opt[\'path\'][\'images\'], img_name)\n                    util.mkdir(img_dir)\n\n                    model.feed_data(test_data)\n                    model.test()\n\n                    visuals = model.current_visuals()\n                    E_img = util.tensor2uint(visuals[\'E\'])\n                    H_img = util.tensor2uint(visuals[\'H\'])\n\n                    # -----------------------\n                    # save estimated image E\n                    # -----------------------\n                    save_img_path = os.path.join(img_dir, \'{:s}_{:d}.png\'.format(img_name, current_step))\n                    util.imsave(E_img, save_img_path)\n\n                    # -----------------------\n                    # calculate PSNR\n                    # -----------------------\n                    current_psnr = util.calculate_psnr(E_img, H_img, border=border)\n\n                    logger.info(\'{:->4d}--> {:>10s} | {:<4.2f}dB\'.format(idx, image_name_ext, current_psnr))\n\n                    avg_psnr += current_psnr\n\n                avg_psnr = avg_psnr / idx\n\n                # testing log\n                logger.info(\'<epoch:{:3d}, iter:{:8,d}, Average PSNR : {:<.2f}dB\\n\'.format(epoch, current_step, avg_psnr))\n\n    logger.info(\'Saving the final model.\')\n    model.save(\'latest\')\n    logger.info(\'End of training.\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
main_train_ffdnet.py,3,"b'import os.path\nimport math\nimport argparse\nimport time\nimport random\nimport numpy as np\nfrom collections import OrderedDict\nimport logging\nfrom torch.utils.data import DataLoader\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_option as option\n\nfrom data.select_dataset import define_Dataset\nfrom models.select_model import define_Model\n\n\n\'\'\'\n# --------------------------------------------\n# training code for FFDNet\n# --------------------------------------------\n# Kai Zhang (cskaizhang@gmail.com)\n# github: https://github.com/cszn/KAIR\n#         https://github.com/cszn/FFDNet\n#\n# Reference:\n@article{zhang2018ffdnet,\n  title={FFDNet: Toward a fast and flexible solution for CNN-based image denoising},\n  author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n  journal={IEEE Transactions on Image Processing},\n  volume={27},\n  number={9},\n  pages={4608--4622},\n  year={2018},\n  publisher={IEEE}\n}\n# --------------------------------------------\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\ndef main(json_path=\'options/train_ffdnet.json\'):\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--1 (prepare opt)\n    # ----------------------------------------\n    \'\'\'\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-opt\', type=str, default=json_path, help=\'Path to option JSON file.\')\n\n    opt = option.parse(parser.parse_args().opt, is_train=True)\n    util.mkdirs((path for key, path in opt[\'path\'].items() if \'pretrained\' not in key))\n\n    # ----------------------------------------\n    # update opt\n    # ----------------------------------------\n    # -->-->-->-->-->-->-->-->-->-->-->-->-->-\n    init_iter, init_path_G = option.find_last_checkpoint(opt[\'path\'][\'models\'], net_type=\'G\')\n    opt[\'path\'][\'pretrained_netG\'] = init_path_G\n    current_step = init_iter\n\n    border = 0\n    # --<--<--<--<--<--<--<--<--<--<--<--<--<-\n\n    # ----------------------------------------\n    # save opt to  a \'../option.json\' file\n    # ----------------------------------------\n    option.save(opt)\n\n    # ----------------------------------------\n    # return None for missing key\n    # ----------------------------------------\n    opt = option.dict_to_nonedict(opt)\n\n    # ----------------------------------------\n    # configure logger\n    # ----------------------------------------\n    logger_name = \'train\'\n    utils_logger.logger_info(logger_name, os.path.join(opt[\'path\'][\'log\'], logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n    logger.info(option.dict2str(opt))\n\n    # ----------------------------------------\n    # seed\n    # ----------------------------------------\n    seed = opt[\'train\'][\'manual_seed\']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    logger.info(\'Random seed: {}\'.format(seed))\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--2 (creat dataloader)\n    # ----------------------------------------\n    \'\'\'\n\n    # ----------------------------------------\n    # 1) create_dataset\n    # 2) creat_dataloader for train and test\n    # ----------------------------------------\n    for phase, dataset_opt in opt[\'datasets\'].items():\n        if phase == \'train\':\n            train_set = define_Dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt[\'dataloader_batch_size\']))\n            logger.info(\'Number of train images: {:,d}, iters: {:,d}\'.format(len(train_set), train_size))\n            train_loader = DataLoader(train_set,\n                                      batch_size=dataset_opt[\'dataloader_batch_size\'],\n                                      shuffle=dataset_opt[\'dataloader_shuffle\'],\n                                      num_workers=dataset_opt[\'dataloader_num_workers\'],\n                                      drop_last=True,\n                                      pin_memory=True)\n        elif phase == \'test\':\n            test_set = define_Dataset(dataset_opt)\n            test_loader = DataLoader(test_set, batch_size=1,\n                                     shuffle=False, num_workers=1,\n                                     drop_last=False, pin_memory=True)\n        else:\n            raise NotImplementedError(""Phase [%s] is not recognized."" % phase)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--3 (initialize model)\n    # ----------------------------------------\n    \'\'\'\n\n    model = define_Model(opt)\n\n    if opt[\'merge_bn\'] and current_step > opt[\'merge_bn_startpoint\']:\n        logger.info(\'^_^ -----merging bnorm----- ^_^\')\n        model.merge_bnorm_test()\n\n    logger.info(model.info_network())\n    model.init_train()\n    logger.info(model.info_params())\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--4 (main training)\n    # ----------------------------------------\n    \'\'\'\n\n    for epoch in range(1000000):  # keep running\n        for i, train_data in enumerate(train_loader):\n\n            current_step += 1\n\n            # -------------------------------\n            # 1) update learning rate\n            # -------------------------------\n            model.update_learning_rate(current_step)\n\n            # -------------------------------\n            # 2) feed patch pairs\n            # -------------------------------\n            model.feed_data(train_data)\n\n            # -------------------------------\n            # 3) optimize parameters\n            # -------------------------------\n            model.optimize_parameters(current_step)\n\n            # -------------------------------\n            # merge bnorm\n            # -------------------------------\n            if opt[\'merge_bn\'] and opt[\'merge_bn_startpoint\'] == current_step:\n                logger.info(\'^_^ -----merging bnorm----- ^_^\')\n                model.merge_bnorm_train()\n                model.print_network()\n\n            # --------------------------\n            # 4) training information\n            # --------------------------\n            if current_step % opt[\'train\'][\'checkpoint_print\'] == 0:\n                logs = model.current_log()  # such as loss\n                message = \'<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> \'.format(epoch, current_step, model.current_learning_rate())\n                for k, v in logs.items():  # merge log information into message\n                    message += \'{:s}: {:.3e} \'.format(k, v)\n                logger.info(message)\n\n            # -------------------------------\n            # 5) save model\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_save\'] == 0:\n                logger.info(\'Saving the model.\')\n                model.save(current_step)\n\n            # -------------------------------\n            # 6) testing\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_test\'] == 0:\n\n                avg_psnr = 0.0\n                idx = 0\n\n                for test_data in test_loader:\n                    idx += 1\n                    image_name_ext = os.path.basename(test_data[\'L_path\'][0])\n                    img_name, ext = os.path.splitext(image_name_ext)\n\n                    img_dir = os.path.join(opt[\'path\'][\'images\'], img_name)\n                    util.mkdir(img_dir)\n\n                    model.feed_data(test_data)\n                    model.test()\n\n                    visuals = model.current_visuals()\n                    E_img = util.tensor2uint(visuals[\'E\'])\n                    H_img = util.tensor2uint(visuals[\'H\'])\n\n                    # -----------------------\n                    # save estimated image E\n                    # -----------------------\n                    save_img_path = os.path.join(img_dir, \'{:s}_{:d}.png\'.format(img_name, current_step))\n                    util.imsave(E_img, save_img_path)\n\n                    # -----------------------\n                    # calculate PSNR\n                    # -----------------------\n                    current_psnr = util.calculate_psnr(E_img, H_img, border=border)\n\n                    logger.info(\'{:->4d}--> {:>10s} | {:<4.2f}dB\'.format(idx, image_name_ext, current_psnr))\n\n                    avg_psnr += current_psnr\n\n                avg_psnr = avg_psnr / idx\n\n                # testing log\n                logger.info(\'<epoch:{:3d}, iter:{:8,d}, Average PSNR : {:<.2f}dB\\n\'.format(epoch, current_step, avg_psnr))\n\n    logger.info(\'Saving the final model.\')\n    model.save(\'latest\')\n    logger.info(\'End of training.\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
main_train_imdn.py,3,"b'import os.path\nimport math\nimport argparse\nimport time\nimport random\nimport numpy as np\nfrom collections import OrderedDict\nimport logging\nfrom torch.utils.data import DataLoader\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_option as option\n\nfrom data.select_dataset import define_Dataset\nfrom models.select_model import define_Model\n\n\n\'\'\'\n# --------------------------------------------\n# training code for IMDN super-resolution network\n# IMDN is the first place solution for AIM 2019 constrained SR\n# --------------------------------------------\n# Kai Zhang (cskaizhang@gmail.com)\n# github: https://github.com/cszn/KAIR\n#\n# References:\n@inproceedings{hui2019lightweight,\n  title={Lightweight Image Super-Resolution with Information Multi-distillation Network},\n  author={Hui, Zheng and Gao, Xinbo and Yang, Yunchu and Wang, Xiumei},\n  booktitle={Proceedings of the 27th ACM International Conference on Multimedia (ACM MM)},\n  pages={2024--2032},\n  year={2019}\n}\n@inproceedings{zhang2019aim,\n  title={AIM 2019 Challenge on Constrained Super-Resolution: Methods and Results},\n  author={Kai Zhang and Shuhang Gu and Radu Timofte and others},\n  booktitle={IEEE International Conference on Computer Vision Workshops},\n  year={2019}\n}\n# --------------------------------------------\n# https://github.com/Zheng222/IMDN\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\ndef main(json_path=\'options/train_imdn.json\'):\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--1 (prepare opt)\n    # ----------------------------------------\n    \'\'\'\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-opt\', type=str, default=json_path, help=\'Path to option JSON file.\')\n\n    opt = option.parse(parser.parse_args().opt, is_train=True)\n    util.mkdirs((path for key, path in opt[\'path\'].items() if \'pretrained\' not in key))\n\n    # ----------------------------------------\n    # update opt\n    # ----------------------------------------\n    # -->-->-->-->-->-->-->-->-->-->-->-->-->-\n    init_iter, init_path_G = option.find_last_checkpoint(opt[\'path\'][\'models\'], net_type=\'G\')\n    opt[\'path\'][\'pretrained_netG\'] = init_path_G\n    current_step = init_iter\n\n    border = opt[\'scale\']\n    # --<--<--<--<--<--<--<--<--<--<--<--<--<-\n\n    # ----------------------------------------\n    # save opt to  a \'../option.json\' file\n    # ----------------------------------------\n    option.save(opt)\n\n    # ----------------------------------------\n    # return None for missing key\n    # ----------------------------------------\n    opt = option.dict_to_nonedict(opt)\n\n    # ----------------------------------------\n    # configure logger\n    # ----------------------------------------\n    logger_name = \'train\'\n    utils_logger.logger_info(logger_name, os.path.join(opt[\'path\'][\'log\'], logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n    logger.info(option.dict2str(opt))\n\n    # ----------------------------------------\n    # seed\n    # ----------------------------------------\n    seed = opt[\'train\'][\'manual_seed\']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    logger.info(\'Random seed: {}\'.format(seed))\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--2 (creat dataloader)\n    # ----------------------------------------\n    \'\'\'\n\n    # ----------------------------------------\n    # 1) create_dataset\n    # 2) creat_dataloader for train and test\n    # ----------------------------------------\n    for phase, dataset_opt in opt[\'datasets\'].items():\n        if phase == \'train\':\n            train_set = define_Dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt[\'dataloader_batch_size\']))\n            logger.info(\'Number of train images: {:,d}, iters: {:,d}\'.format(len(train_set), train_size))\n            train_loader = DataLoader(train_set,\n                                      batch_size=dataset_opt[\'dataloader_batch_size\'],\n                                      shuffle=dataset_opt[\'dataloader_shuffle\'],\n                                      num_workers=dataset_opt[\'dataloader_num_workers\'],\n                                      drop_last=True,\n                                      pin_memory=True)\n        elif phase == \'test\':\n            test_set = define_Dataset(dataset_opt)\n            test_loader = DataLoader(test_set, batch_size=1,\n                                     shuffle=False, num_workers=1,\n                                     drop_last=False, pin_memory=True)\n        else:\n            raise NotImplementedError(""Phase [%s] is not recognized."" % phase)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--3 (initialize model)\n    # ----------------------------------------\n    \'\'\'\n\n    model = define_Model(opt)\n\n    logger.info(model.info_network())\n    model.init_train()\n    logger.info(model.info_params())\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--4 (main training)\n    # ----------------------------------------\n    \'\'\'\n\n    for epoch in range(1000000):  # keep running\n        for i, train_data in enumerate(train_loader):\n\n            current_step += 1\n\n            # -------------------------------\n            # 1) update learning rate\n            # -------------------------------\n            model.update_learning_rate(current_step)\n\n            # -------------------------------\n            # 2) feed patch pairs\n            # -------------------------------\n            model.feed_data(train_data)\n\n            # -------------------------------\n            # 3) optimize parameters\n            # -------------------------------\n            model.optimize_parameters(current_step)\n\n            # -------------------------------\n            # 4) training information\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_print\'] == 0:\n                logs = model.current_log()  # such as loss\n                message = \'<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> \'.format(epoch, current_step, model.current_learning_rate())\n                for k, v in logs.items():  # merge log information into message\n                    message += \'{:s}: {:.3e} \'.format(k, v)\n                logger.info(message)\n\n            # -------------------------------\n            # 5) save model\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_save\'] == 0:\n                logger.info(\'Saving the model.\')\n                model.save(current_step)\n\n            # -------------------------------\n            # 6) testing\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_test\'] == 0:\n\n                avg_psnr = 0.0\n                idx = 0\n\n                for test_data in test_loader:\n                    idx += 1\n                    image_name_ext = os.path.basename(test_data[\'L_path\'][0])\n                    img_name, ext = os.path.splitext(image_name_ext)\n\n                    img_dir = os.path.join(opt[\'path\'][\'images\'], img_name)\n                    util.mkdir(img_dir)\n\n                    model.feed_data(test_data)\n                    model.test()\n\n                    visuals = model.current_visuals()\n                    E_img = util.tensor2uint(visuals[\'E\'])\n                    H_img = util.tensor2uint(visuals[\'H\'])\n\n                    # -----------------------\n                    # save estimated image E\n                    # -----------------------\n                    save_img_path = os.path.join(img_dir, \'{:s}_{:d}.png\'.format(img_name, current_step))\n                    util.imsave(E_img, save_img_path)\n\n                    # -----------------------\n                    # calculate PSNR\n                    # -----------------------\n                    current_psnr = util.calculate_psnr(E_img, H_img, border=border)\n\n                    logger.info(\'{:->4d}--> {:>10s} | {:<4.2f}dB\'.format(idx, image_name_ext, current_psnr))\n\n                    avg_psnr += current_psnr\n\n                avg_psnr = avg_psnr / idx\n\n                # testing log\n                logger.info(\'<epoch:{:3d}, iter:{:8,d}, Average PSNR : {:<.2f}dB\\n\'.format(epoch, current_step, avg_psnr))\n\n    logger.info(\'Saving the final model.\')\n    model.save(\'latest\')\n    logger.info(\'End of training.\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
main_train_msrresnet_gan.py,3,"b'import os.path\nimport math\nimport argparse\nimport time\nimport random\nimport numpy as np\nfrom collections import OrderedDict\nimport logging\nfrom torch.utils.data import DataLoader\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_option as option\n\nfrom data.select_dataset import define_Dataset\nfrom models.select_model import define_Model\n\n\n\'\'\'\n# --------------------------------------------\n# training code for GAN-based model, such as ESRGAN, DPSRGAN\n# --------------------------------------------\n# Kai Zhang (cskaizhang@gmail.com)\n# github: https://github.com/cszn/KAIR\n# --------------------------------------------\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\ndef main(json_path=\'options/train_msrresnet_gan.json\'):\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--1 (prepare opt)\n    # ----------------------------------------\n    \'\'\'\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-opt\', type=str, default=json_path, help=\'Path to option JSON file.\')\n\n    opt = option.parse(parser.parse_args().opt, is_train=True)\n    util.mkdirs((path for key, path in opt[\'path\'].items() if \'pretrained\' not in key))\n\n    # ----------------------------------------\n    # update opt\n    # ----------------------------------------\n    # -->-->-->-->-->-->-->-->-->-->-->-->-->-\n    init_iterG, init_path_G = option.find_last_checkpoint(opt[\'path\'][\'models\'], net_type=\'G\')\n    init_iterD, init_path_D = option.find_last_checkpoint(opt[\'path\'][\'models\'], net_type=\'D\')\n    opt[\'path\'][\'pretrained_netG\'] = init_path_G\n    opt[\'path\'][\'pretrained_netD\'] = init_path_D\n    current_step = max(init_iterG, init_iterD)\n\n    # opt[\'path\'][\'pretrained_netG\'] = \'\'\n    # current_step = 0\n    border = opt[\'scale\']\n    # --<--<--<--<--<--<--<--<--<--<--<--<--<-\n\n    # ----------------------------------------\n    # save opt to  a \'../option.json\' file\n    # ----------------------------------------\n    option.save(opt)\n\n    # ----------------------------------------\n    # return None for missing key\n    # ----------------------------------------\n    opt = option.dict_to_nonedict(opt)\n\n    # ----------------------------------------\n    # configure logger\n    # ----------------------------------------\n    logger_name = \'train\'\n    utils_logger.logger_info(logger_name, os.path.join(opt[\'path\'][\'log\'], logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n    logger.info(option.dict2str(opt))\n\n    # ----------------------------------------\n    # seed\n    # ----------------------------------------\n    seed = opt[\'train\'][\'manual_seed\']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    logger.info(\'Random seed: {}\'.format(seed))\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--2 (creat dataloader)\n    # ----------------------------------------\n    \'\'\'\n\n    # ----------------------------------------\n    # 1) create_dataset\n    # 2) creat_dataloader for train and test\n    # ----------------------------------------\n    for phase, dataset_opt in opt[\'datasets\'].items():\n        if phase == \'train\':\n            train_set = define_Dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt[\'dataloader_batch_size\']))\n            logger.info(\'Number of train images: {:,d}, iters: {:,d}\'.format(len(train_set), train_size))\n            train_loader = DataLoader(train_set,\n                                      batch_size=dataset_opt[\'dataloader_batch_size\'],\n                                      shuffle=dataset_opt[\'dataloader_shuffle\'],\n                                      num_workers=dataset_opt[\'dataloader_num_workers\'],\n                                      drop_last=True,\n                                      pin_memory=True)\n        elif phase == \'test\':\n            test_set = define_Dataset(dataset_opt)\n            test_loader = DataLoader(test_set, batch_size=1,\n                                     shuffle=False, num_workers=1,\n                                     drop_last=False, pin_memory=True)\n        else:\n            raise NotImplementedError(""Phase [%s] is not recognized."" % phase)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--3 (initialize model)\n    # ----------------------------------------\n    \'\'\'\n\n    model = define_Model(opt)\n\n    model.init_train()\n    logger.info(model.info_network())\n    logger.info(model.info_params())\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--4 (main training)\n    # ----------------------------------------\n    \'\'\'\n\n    for epoch in range(1000000):  # keep running\n        for i, train_data in enumerate(train_loader):\n\n            current_step += 1\n\n            # -------------------------------\n            # 1) update learning rate\n            # -------------------------------\n            model.update_learning_rate(current_step)\n\n            # -------------------------------\n            # 2) feed patch pairs\n            # -------------------------------\n            model.feed_data(train_data)\n\n            # -------------------------------\n            # 3) optimize parameters\n            # -------------------------------\n            model.optimize_parameters(current_step)\n\n            # -------------------------------\n            # 4) training information\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_print\'] == 0:\n                logs = model.current_log()  # such as loss\n                message = \'<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> \'.format(epoch, current_step, model.current_learning_rate())\n                for k, v in logs.items():  # merge log information into message\n                    message += \'{:s}: {:.3e} \'.format(k, v)\n                logger.info(message)\n\n            # -------------------------------\n            # 5) save model\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_save\'] == 0:\n                logger.info(\'Saving the model.\')\n                model.save(current_step)\n\n            # -------------------------------\n            # 6) testing\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_test\'] == 0:\n\n                avg_psnr = 0.0\n                idx = 0\n\n                for test_data in test_loader:\n                    idx += 1\n                    image_name_ext = os.path.basename(test_data[\'L_path\'][0])\n                    img_name, ext = os.path.splitext(image_name_ext)\n\n                    img_dir = os.path.join(opt[\'path\'][\'images\'], img_name)\n                    util.mkdir(img_dir)\n\n                    model.feed_data(test_data)\n                    model.test()\n\n                    visuals = model.current_visuals()\n                    E_img = util.tensor2uint(visuals[\'E\'])\n                    H_img = util.tensor2uint(visuals[\'H\'])\n\n                    # -----------------------\n                    # save estimated image E\n                    # -----------------------\n                    save_img_path = os.path.join(img_dir, \'{:s}_{:d}.png\'.format(img_name, current_step))\n                    util.imsave(E_img, save_img_path)\n\n                    # -----------------------\n                    # calculate PSNR\n                    # -----------------------\n                    current_psnr = util.calculate_psnr(E_img, H_img, border=border)\n\n                    logger.info(\'{:->4d}--> {:>10s} | {:<4.2f}dB\'.format(idx, image_name_ext, current_psnr))\n\n                    avg_psnr += current_psnr\n\n                avg_psnr = avg_psnr / idx\n\n                # testing log\n                logger.info(\'<epoch:{:3d}, iter:{:8,d}, Average PSNR : {:<.2f}dB\\n\'.format(epoch, current_step, avg_psnr))\n\n    logger.info(\'Saving the final model.\')\n    model.save(\'latest\')\n    logger.info(\'End of training.\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
main_train_msrresnet_psnr.py,3,"b'import os.path\nimport math\nimport argparse\nimport time\nimport random\nimport numpy as np\nfrom collections import OrderedDict\nimport logging\nfrom torch.utils.data import DataLoader\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_option as option\n\nfrom data.select_dataset import define_Dataset\nfrom models.select_model import define_Model\n\n\n\'\'\'\n# --------------------------------------------\n# training code for MSRResNet\n# --------------------------------------------\n# Kai Zhang (cskaizhang@gmail.com)\n# github: https://github.com/cszn/KAIR\n# --------------------------------------------\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\ndef main(json_path=\'options/train_msrresnet_psnr.json\'):\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--1 (prepare opt)\n    # ----------------------------------------\n    \'\'\'\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-opt\', type=str, default=json_path, help=\'Path to option JSON file.\')\n\n    opt = option.parse(parser.parse_args().opt, is_train=True)\n    util.mkdirs((path for key, path in opt[\'path\'].items() if \'pretrained\' not in key))\n\n    # ----------------------------------------\n    # update opt\n    # ----------------------------------------\n    # -->-->-->-->-->-->-->-->-->-->-->-->-->-\n    init_iter, init_path_G = option.find_last_checkpoint(opt[\'path\'][\'models\'], net_type=\'G\')\n    opt[\'path\'][\'pretrained_netG\'] = init_path_G\n    current_step = init_iter\n\n    border = opt[\'scale\']\n    # --<--<--<--<--<--<--<--<--<--<--<--<--<-\n\n    # ----------------------------------------\n    # save opt to  a \'../option.json\' file\n    # ----------------------------------------\n    option.save(opt)\n\n    # ----------------------------------------\n    # return None for missing key\n    # ----------------------------------------\n    opt = option.dict_to_nonedict(opt)\n\n    # ----------------------------------------\n    # configure logger\n    # ----------------------------------------\n    logger_name = \'train\'\n    utils_logger.logger_info(logger_name, os.path.join(opt[\'path\'][\'log\'], logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n    logger.info(option.dict2str(opt))\n\n    # ----------------------------------------\n    # seed\n    # ----------------------------------------\n    seed = opt[\'train\'][\'manual_seed\']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    logger.info(\'Random seed: {}\'.format(seed))\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--2 (creat dataloader)\n    # ----------------------------------------\n    \'\'\'\n\n    # ----------------------------------------\n    # 1) create_dataset\n    # 2) creat_dataloader for train and test\n    # ----------------------------------------\n    for phase, dataset_opt in opt[\'datasets\'].items():\n        if phase == \'train\':\n            train_set = define_Dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt[\'dataloader_batch_size\']))\n            logger.info(\'Number of train images: {:,d}, iters: {:,d}\'.format(len(train_set), train_size))\n            train_loader = DataLoader(train_set,\n                                      batch_size=dataset_opt[\'dataloader_batch_size\'],\n                                      shuffle=dataset_opt[\'dataloader_shuffle\'],\n                                      num_workers=dataset_opt[\'dataloader_num_workers\'],\n                                      drop_last=True,\n                                      pin_memory=True)\n        elif phase == \'test\':\n            test_set = define_Dataset(dataset_opt)\n            test_loader = DataLoader(test_set, batch_size=1,\n                                     shuffle=False, num_workers=1,\n                                     drop_last=False, pin_memory=True)\n        else:\n            raise NotImplementedError(""Phase [%s] is not recognized."" % phase)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--3 (initialize model)\n    # ----------------------------------------\n    \'\'\'\n\n    model = define_Model(opt)\n\n    logger.info(model.info_network())\n    model.init_train()\n    logger.info(model.info_params())\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--4 (main training)\n    # ----------------------------------------\n    \'\'\'\n\n    for epoch in range(1000000):  # keep running\n        for i, train_data in enumerate(train_loader):\n\n            current_step += 1\n\n            # -------------------------------\n            # 1) update learning rate\n            # -------------------------------\n            model.update_learning_rate(current_step)\n\n            # -------------------------------\n            # 2) feed patch pairs\n            # -------------------------------\n            model.feed_data(train_data)\n\n            # -------------------------------\n            # 3) optimize parameters\n            # -------------------------------\n            model.optimize_parameters(current_step)\n\n            # -------------------------------\n            # 4) training information\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_print\'] == 0:\n                logs = model.current_log()  # such as loss\n                message = \'<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> \'.format(epoch, current_step, model.current_learning_rate())\n                for k, v in logs.items():  # merge log information into message\n                    message += \'{:s}: {:.3e} \'.format(k, v)\n                logger.info(message)\n\n            # -------------------------------\n            # 5) save model\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_save\'] == 0:\n                logger.info(\'Saving the model.\')\n                model.save(current_step)\n\n            # -------------------------------\n            # 6) testing\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_test\'] == 0:\n\n                avg_psnr = 0.0\n                idx = 0\n\n                for test_data in test_loader:\n                    idx += 1\n                    image_name_ext = os.path.basename(test_data[\'L_path\'][0])\n                    img_name, ext = os.path.splitext(image_name_ext)\n\n                    img_dir = os.path.join(opt[\'path\'][\'images\'], img_name)\n                    util.mkdir(img_dir)\n\n                    model.feed_data(test_data)\n                    model.test()\n\n                    visuals = model.current_visuals()\n                    E_img = util.tensor2uint(visuals[\'E\'])\n                    H_img = util.tensor2uint(visuals[\'H\'])\n\n                    # -----------------------\n                    # save estimated image E\n                    # -----------------------\n                    save_img_path = os.path.join(img_dir, \'{:s}_{:d}.png\'.format(img_name, current_step))\n                    util.imsave(E_img, save_img_path)\n\n                    # -----------------------\n                    # calculate PSNR\n                    # -----------------------\n                    current_psnr = util.calculate_psnr(E_img, H_img, border=border)\n\n                    logger.info(\'{:->4d}--> {:>10s} | {:<4.2f}dB\'.format(idx, image_name_ext, current_psnr))\n\n                    avg_psnr += current_psnr\n\n                avg_psnr = avg_psnr / idx\n\n                # testing log\n                logger.info(\'<epoch:{:3d}, iter:{:8,d}, Average PSNR : {:<.2f}dB\\n\'.format(epoch, current_step, avg_psnr))\n\n    logger.info(\'Saving the final model.\')\n    model.save(\'latest\')\n    logger.info(\'End of training.\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
main_train_rrdb_psnr.py,3,"b'import os.path\nimport math\nimport argparse\nimport time\nimport random\nimport numpy as np\nfrom collections import OrderedDict\nimport logging\nfrom torch.utils.data import DataLoader\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_option as option\n\nfrom data.select_dataset import define_Dataset\nfrom models.select_model import define_Model\n\n\n\'\'\'\n# --------------------------------------------\n# training code for RRDB_PSNR\n# --------------------------------------------\n# Kai Zhang (cskaizhang@gmail.com)\n# github: https://github.com/cszn/KAIR\n#\n# Reference:\n@inproceedings{wang2018esrgan,\n  title={Esrgan: Enhanced super-resolution generative adversarial networks},\n  author={Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Change Loy, Chen},\n  booktitle={European Conference on Computer Vision (ECCV)},\n  pages={0--0},\n  year={2018}\n}\n# --------------------------------------------\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\ndef main(json_path=\'options/train_rrdb_psnr.json\'):\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--1 (prepare opt)\n    # ----------------------------------------\n    \'\'\'\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-opt\', type=str, default=json_path, help=\'Path to option JSON file.\')\n\n    opt = option.parse(parser.parse_args().opt, is_train=True)\n    util.mkdirs((path for key, path in opt[\'path\'].items() if \'pretrained\' not in key))\n\n    # ----------------------------------------\n    # update opt\n    # ----------------------------------------\n    # -->-->-->-->-->-->-->-->-->-->-->-->-->-\n    init_iter, init_path_G = option.find_last_checkpoint(opt[\'path\'][\'models\'], net_type=\'G\')\n    opt[\'path\'][\'pretrained_netG\'] = init_path_G\n    current_step = init_iter\n\n    border = opt[\'scale\']\n    # --<--<--<--<--<--<--<--<--<--<--<--<--<-\n\n    # ----------------------------------------\n    # save opt to  a \'../option.json\' file\n    # ----------------------------------------\n    option.save(opt)\n\n    # ----------------------------------------\n    # return None for missing key\n    # ----------------------------------------\n    opt = option.dict_to_nonedict(opt)\n\n    # ----------------------------------------\n    # configure logger\n    # ----------------------------------------\n    logger_name = \'train\'\n    utils_logger.logger_info(logger_name, os.path.join(opt[\'path\'][\'log\'], logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n    logger.info(option.dict2str(opt))\n\n    # ----------------------------------------\n    # seed\n    # ----------------------------------------\n    seed = opt[\'train\'][\'manual_seed\']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    logger.info(\'Random seed: {}\'.format(seed))\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--2 (creat dataloader)\n    # ----------------------------------------\n    \'\'\'\n\n    # ----------------------------------------\n    # 1) create_dataset\n    # 2) creat_dataloader for train and test\n    # ----------------------------------------\n    for phase, dataset_opt in opt[\'datasets\'].items():\n        if phase == \'train\':\n            train_set = define_Dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt[\'dataloader_batch_size\']))\n            logger.info(\'Number of train images: {:,d}, iters: {:,d}\'.format(len(train_set), train_size))\n            train_loader = DataLoader(train_set,\n                                      batch_size=dataset_opt[\'dataloader_batch_size\'],\n                                      shuffle=dataset_opt[\'dataloader_shuffle\'],\n                                      num_workers=dataset_opt[\'dataloader_num_workers\'],\n                                      drop_last=True,\n                                      pin_memory=True)\n        elif phase == \'test\':\n            test_set = define_Dataset(dataset_opt)\n            test_loader = DataLoader(test_set, batch_size=1,\n                                     shuffle=False, num_workers=1,\n                                     drop_last=False, pin_memory=True)\n        else:\n            raise NotImplementedError(""Phase [%s] is not recognized."" % phase)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--3 (initialize model)\n    # ----------------------------------------\n    \'\'\'\n\n    model = define_Model(opt)\n\n    logger.info(model.info_network())\n    model.init_train()\n    logger.info(model.info_params())\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--4 (main training)\n    # ----------------------------------------\n    \'\'\'\n\n    for epoch in range(1000000):  # keep running\n        for i, train_data in enumerate(train_loader):\n\n            current_step += 1\n\n            # -------------------------------\n            # 1) update learning rate\n            # -------------------------------\n            model.update_learning_rate(current_step)\n\n            # -------------------------------\n            # 2) feed patch pairs\n            # -------------------------------\n            model.feed_data(train_data)\n\n            # -------------------------------\n            # 3) optimize parameters\n            # -------------------------------\n            model.optimize_parameters(current_step)\n\n            # -------------------------------\n            # 4) training information\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_print\'] == 0:\n                logs = model.current_log()  # such as loss\n                message = \'<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> \'.format(epoch, current_step, model.current_learning_rate())\n                for k, v in logs.items():  # merge log information into message\n                    message += \'{:s}: {:.3e} \'.format(k, v)\n                logger.info(message)\n\n            # -------------------------------\n            # 5) save model\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_save\'] == 0:\n                logger.info(\'Saving the model.\')\n                model.save(current_step)\n\n            # -------------------------------\n            # 6) testing\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_test\'] == 0:\n\n                avg_psnr = 0.0\n                idx = 0\n\n                for test_data in test_loader:\n                    idx += 1\n                    image_name_ext = os.path.basename(test_data[\'L_path\'][0])\n                    img_name, ext = os.path.splitext(image_name_ext)\n\n                    img_dir = os.path.join(opt[\'path\'][\'images\'], img_name)\n                    util.mkdir(img_dir)\n\n                    model.feed_data(test_data)\n                    model.test()\n\n                    visuals = model.current_visuals()\n                    E_img = util.tensor2uint(visuals[\'E\'])\n                    H_img = util.tensor2uint(visuals[\'H\'])\n\n                    # -----------------------\n                    # save estimated image E\n                    # -----------------------\n                    save_img_path = os.path.join(img_dir, \'{:s}_{:d}.png\'.format(img_name, current_step))\n                    util.imsave(E_img, save_img_path)\n\n                    # -----------------------\n                    # calculate PSNR\n                    # -----------------------\n                    current_psnr = util.calculate_psnr(E_img, H_img, border=border)\n\n                    logger.info(\'{:->4d}--> {:>10s} | {:<4.2f}dB\'.format(idx, image_name_ext, current_psnr))\n\n                    avg_psnr += current_psnr\n\n                avg_psnr = avg_psnr / idx\n\n                # testing log\n                logger.info(\'<epoch:{:3d}, iter:{:8,d}, Average PSNR : {:<.2f}dB\\n\'.format(epoch, current_step, avg_psnr))\n\n    logger.info(\'Saving the final model.\')\n    model.save(\'latest\')\n    logger.info(\'End of training.\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
main_train_srmd.py,4,"b'import os.path\nimport math\nimport argparse\nimport time\nimport random\nimport numpy as np\nfrom collections import OrderedDict\nimport logging\nfrom torch.utils.data import DataLoader\nimport torch\n\nfrom utils import utils_logger\nfrom utils import utils_image as util\nfrom utils import utils_option as option\nfrom utils import utils_sisr as sisr\n\nfrom data.select_dataset import define_Dataset\nfrom models.select_model import define_Model\n\n\n\'\'\'\n# --------------------------------------------\n# training code for SRMD\n# --------------------------------------------\n# Kai Zhang (cskaizhang@gmail.com)\n# github: https://github.com/cszn/KAIR\n#         https://github.com/cszn/SRMD\n#\n# Reference:\n@inproceedings{zhang2018learning,\n  title={Learning a single convolutional super-resolution network for multiple degradations},\n  author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={3262--3271},\n  year={2018}\n}\n# --------------------------------------------\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\ndef main(json_path=\'options/train_srmd.json\'):\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--1 (prepare opt)\n    # ----------------------------------------\n    \'\'\'\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-opt\', type=str, default=json_path, help=\'Path to option JSON file.\')\n\n    opt = option.parse(parser.parse_args().opt, is_train=True)\n    util.mkdirs((path for key, path in opt[\'path\'].items() if \'pretrained\' not in key))\n\n    # ----------------------------------------\n    # update opt\n    # ----------------------------------------\n    # -->-->-->-->-->-->-->-->-->-->-->-->-->-\n    init_iter, init_path_G = option.find_last_checkpoint(opt[\'path\'][\'models\'], net_type=\'G\')\n    opt[\'path\'][\'pretrained_netG\'] = init_path_G\n    current_step = init_iter\n\n    border = opt[\'scale\']\n    # --<--<--<--<--<--<--<--<--<--<--<--<--<-\n\n    # ----------------------------------------\n    # save opt to  a \'../option.json\' file\n    # ----------------------------------------\n    option.save(opt)\n\n    # ----------------------------------------\n    # return None for missing key\n    # ----------------------------------------\n    opt = option.dict_to_nonedict(opt)\n\n    # ----------------------------------------\n    # configure logger\n    # ----------------------------------------\n    logger_name = \'train\'\n    utils_logger.logger_info(logger_name, os.path.join(opt[\'path\'][\'log\'], logger_name+\'.log\'))\n    logger = logging.getLogger(logger_name)\n    logger.info(option.dict2str(opt))\n\n    # ----------------------------------------\n    # calculate PCA projection matrix\n    # ----------------------------------------\n    pca_matrix_path = os.path.join(\'kernels\', \'srmd_pca_pytorch.mat\')\n    if not os.path.exists(pca_matrix_path):\n        logger.info(\'calculating PCA projection matrix...\')\n        sisr.cal_pca_matrix(path=pca_matrix_path, ksize=15, l_max=10.0, dim_pca=15, num_samples=5000)\n        logger.info(\'done!\')\n    else:\n        logger.info(\'loading PCA projection matrix...\')\n\n    # ----------------------------------------\n    # seed\n    # ----------------------------------------\n    seed = opt[\'train\'][\'manual_seed\']\n    if seed is None:\n        seed = random.randint(1, 10000)\n    logger.info(\'Random seed: {}\'.format(seed))\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--2 (creat dataloader)\n    # ----------------------------------------\n    \'\'\'\n\n    # ----------------------------------------\n    # 1) create_dataset\n    # 2) creat_dataloader for train and test\n    # ----------------------------------------\n    for phase, dataset_opt in opt[\'datasets\'].items():\n        if phase == \'train\':\n            train_set = define_Dataset(dataset_opt)\n            train_size = int(math.ceil(len(train_set) / dataset_opt[\'dataloader_batch_size\']))\n            logger.info(\'Number of train images: {:,d}, iters: {:,d}\'.format(len(train_set), train_size))\n            train_loader = DataLoader(train_set,\n                                      batch_size=dataset_opt[\'dataloader_batch_size\'],\n                                      shuffle=dataset_opt[\'dataloader_shuffle\'],\n                                      num_workers=dataset_opt[\'dataloader_num_workers\'],\n                                      drop_last=True,\n                                      pin_memory=True)\n        elif phase == \'test\':\n            test_set = define_Dataset(dataset_opt)\n            test_loader = DataLoader(test_set, batch_size=1,\n                                     shuffle=False, num_workers=1,\n                                     drop_last=False, pin_memory=True)\n        else:\n            raise NotImplementedError(""Phase [%s] is not recognized."" % phase)\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--3 (initialize model)\n    # ----------------------------------------\n    \'\'\'\n\n    model = define_Model(opt)\n\n#    if opt[\'merge_bn\'] and current_step > opt[\'merge_bn_startpoint\']:\n#        logger.info(\'^_^ -----merging bnorm----- ^_^\')\n#        model.merge_bnorm_test()\n\n    logger.info(model.info_network())\n    model.init_train()\n    logger.info(model.info_params())\n\n    \'\'\'\n    # ----------------------------------------\n    # Step--4 (main training)\n    # ----------------------------------------\n    \'\'\'\n\n    for epoch in range(1000000):  # keep running\n        for i, train_data in enumerate(train_loader):\n\n            current_step += 1\n\n            # -------------------------------\n            # 1) update learning rate\n            # -------------------------------\n            model.update_learning_rate(current_step)\n\n            # -------------------------------\n            # 2) feed patch pairs\n            # -------------------------------\n            model.feed_data(train_data)\n\n            # -------------------------------\n            # 3) optimize parameters\n            # -------------------------------\n            model.optimize_parameters(current_step)\n\n            # -------------------------------\n            # merge bnorm\n            # -------------------------------\n#            if opt[\'merge_bn\'] and opt[\'merge_bn_startpoint\'] == current_step:\n#                logger.info(\'^_^ -----merging bnorm----- ^_^\')\n#                model.merge_bnorm_train()\n#                model.print_network()\n\n            # -------------------------------\n            # 4) training information\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_print\'] == 0:\n                logs = model.current_log()  # such as loss\n                message = \'<epoch:{:3d}, iter:{:8,d}, lr:{:.3e}> \'.format(epoch, current_step, model.current_learning_rate())\n                for k, v in logs.items():  # merge log information into message\n                    message += \'{:s}: {:.3e} \'.format(k, v)\n                logger.info(message)\n\n            # -------------------------------\n            # 5) save model\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_save\'] == 0:\n                logger.info(\'Saving the model.\')\n                model.save(current_step)\n\n            # -------------------------------\n            # 6) testing\n            # -------------------------------\n            if current_step % opt[\'train\'][\'checkpoint_test\'] == 0:\n\n                avg_psnr = 0.0\n                idx = 0\n\n                for test_data in test_loader:\n                    idx += 1\n                    image_name_ext = os.path.basename(test_data[\'L_path\'][0])\n                    img_name, ext = os.path.splitext(image_name_ext)\n\n                    img_dir = os.path.join(opt[\'path\'][\'images\'], img_name)\n                    util.mkdir(img_dir)\n\n                    model.feed_data(test_data)\n                    model.test()\n\n                    visuals = model.current_visuals()\n                    E_img = util.tensor2uint(visuals[\'E\'])\n                    H_img = util.tensor2uint(visuals[\'H\'])\n\n                    # -----------------------\n                    # save estimated image E\n                    # -----------------------\n                    save_img_path = os.path.join(img_dir, \'{:s}_{:d}.png\'.format(img_name, current_step))\n                    util.imsave(E_img, save_img_path)\n\n                    # -----------------------\n                    # calculate PSNR\n                    # -----------------------\n                    current_psnr = util.calculate_psnr(E_img, H_img, border=border)\n\n                    logger.info(\'{:->4d}--> {:>10s} | {:<4.2f}dB\'.format(idx, image_name_ext, current_psnr))\n\n                    avg_psnr += current_psnr\n\n                avg_psnr = avg_psnr / idx\n\n                # testing log\n                logger.info(\'<epoch:{:3d}, iter:{:8,d}, Average PSNR : {:<.2f}dB\\n\'.format(epoch, current_step, avg_psnr))\n\n    logger.info(\'Saving the final model.\')\n    model.save(\'latest\')\n    logger.info(\'End of training.\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
data/__init__.py,0,b'\n'
data/dataset_dncnn.py,2,"b'import os.path\nimport random\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nimport utils.utils_image as util\n\n\nclass DatasetDnCNN(data.Dataset):\n    """"""\n    # -----------------------------------------\n    # Get L/H for denosing on AWGN with fixed sigma.\n    # Only dataroot_H is needed.\n    # -----------------------------------------\n    # e.g., DnCNN\n    # -----------------------------------------\n    """"""\n\n    def __init__(self, opt):\n        super(DatasetDnCNN, self).__init__()\n        print(\'Dataset: Denosing on AWGN with fixed sigma. Only dataroot_H is needed.\')\n        self.opt = opt\n        self.n_channels = opt[\'n_channels\'] if opt[\'n_channels\'] else 3\n        self.patch_size = opt[\'H_size\'] if opt[\'H_size\'] else 64\n        self.sigma = opt[\'sigma\'] if opt[\'sigma\'] else 25\n        self.sigma_test = opt[\'sigma_test\'] if opt[\'sigma_test\'] else self.sigma\n\n        # ------------------------------------\n        # get path of H\n        # return None if input is None\n        # ------------------------------------\n        self.paths_H = util.get_image_paths(opt[\'dataroot_H\'])\n\n    def __getitem__(self, index):\n\n        # ------------------------------------\n        # get H image\n        # ------------------------------------\n        H_path = self.paths_H[index]\n        img_H = util.imread_uint(H_path, self.n_channels)\n\n        L_path = H_path\n\n        if self.opt[\'phase\'] == \'train\':\n            """"""\n            # --------------------------------\n            # get L/H patch pairs\n            # --------------------------------\n            """"""\n            H, W, _ = img_H.shape\n\n            # --------------------------------\n            # randomly crop the patch\n            # --------------------------------\n            rnd_h = random.randint(0, max(0, H - self.patch_size))\n            rnd_w = random.randint(0, max(0, W - self.patch_size))\n            patch_H = img_H[rnd_h:rnd_h + self.patch_size, rnd_w:rnd_w + self.patch_size, :]\n\n            # --------------------------------\n            # augmentation - flip, rotate\n            # --------------------------------\n            mode = np.random.randint(0, 8)\n            patch_H = util.augment_img(patch_H, mode=mode)\n\n            # --------------------------------\n            # HWC to CHW, numpy(uint) to tensor\n            # --------------------------------\n            img_H = util.uint2tensor3(patch_H)\n            img_L = img_H.clone()\n\n            # --------------------------------\n            # add noise\n            # --------------------------------\n            noise = torch.randn(img_L.size()).mul_(self.sigma/255.0)\n            img_L.add_(noise)\n\n        else:\n            """"""\n            # --------------------------------\n            # get L/H image pairs\n            # --------------------------------\n            """"""\n            img_H = util.uint2single(img_H)\n            img_L = np.copy(img_H)\n\n            # --------------------------------\n            # add noise\n            # --------------------------------\n            np.random.seed(seed=0)\n            img_L += np.random.normal(0, self.sigma_test/255.0, img_L.shape)\n\n            # --------------------------------\n            # HWC to CHW, numpy to tensor\n            # --------------------------------\n            img_L = util.single2tensor3(img_L)\n            img_H = util.single2tensor3(img_H)\n\n        return {\'L\': img_L, \'H\': img_H, \'H_path\': H_path, \'L_path\': L_path}\n\n    def __len__(self):\n        return len(self.paths_H)\n'"
data/dataset_dnpatch.py,2,"b'import random\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nimport utils.utils_image as util\n\n\nclass DatasetDnPatch(data.Dataset):\n    """"""\n    # -----------------------------------------\n    # Get L/H for denosing on AWGN with fixed sigma.\n    # ****Get all H patches first****\n    # Only dataroot_H is needed.\n    # -----------------------------------------\n    # e.g., DnCNN with BSD400\n    # -----------------------------------------\n    """"""\n\n    def __init__(self, opt):\n        super(DatasetDnPatch, self).__init__()\n        print(\'Get L/H for denosing on AWGN with fixed sigma. Only dataroot_H is needed.\')\n        self.opt = opt\n        self.n_channels = opt[\'n_channels\'] if opt[\'n_channels\'] else 3\n        self.patch_size = opt[\'H_size\'] if opt[\'H_size\'] else 64\n\n        self.sigma = opt[\'sigma\'] if opt[\'sigma\'] else 25\n        self.sigma_test = opt[\'sigma_test\'] if opt[\'sigma_test\'] else self.sigma\n\n        self.num_patches_per_image = opt[\'num_patches_per_image\'] if opt[\'num_patches_per_image\'] else 40\n        self.num_sampled = opt[\'num_sampled\'] if opt[\'num_sampled\'] else 3000\n\n        # ------------------------------------\n        # get paths of H\n        # ------------------------------------\n        self.paths_H = util.get_image_paths(opt[\'dataroot_H\'])\n        assert self.paths_H, \'Error: H path is empty.\'\n\n        # ------------------------------------\n        # number of sampled H images\n        # ------------------------------------\n        self.num_sampled = min(self.num_sampled, len(self.paths_H))\n\n        # ------------------------------------\n        # reserve space with zeros\n        # ------------------------------------\n        self.total_patches = self.num_sampled * self.num_patches_per_image\n        self.H_data = np.zeros([self.total_patches, self.patch_size, self.patch_size, self.n_channels], dtype=np.uint8)\n\n        # ------------------------------------\n        # update H patches\n        # ------------------------------------\n        self.update_data()\n\n    def update_data(self):\n        """"""\n        # ------------------------------------\n        # update whole H patches\n        # ------------------------------------\n        """"""\n        self.index_sampled = random.sample(range(0, len(self.paths_H), 1), self.num_sampled)\n        n_count = 0\n\n        for i in range(len(self.index_sampled)):\n            H_patches = self.get_patches(self.index_sampled[i])\n            for H_patch in H_patches:\n                self.H_data[n_count,:,:,:] = H_patch\n                n_count += 1\n\n        print(\'Training data updated! Total number of patches is:  %5.2f X %5.2f = %5.2f\\n\' % (len(self.H_data)//128, 128, len(self.H_data)))\n\n    def get_patches(self, index):\n        """"""\n        # ------------------------------------\n        # get H patches from an H image\n        # ------------------------------------\n        """"""\n        H_path = self.paths_H[index]\n        img_H = util.imread_uint(H_path, self.n_channels)  # uint format\n\n        H, W = img_H.shape[:2]\n\n        H_patches = []\n\n        num = self.num_patches_per_image\n        for _ in range(num):\n            rnd_h = random.randint(0, max(0, H - self.patch_size))\n            rnd_w = random.randint(0, max(0, W - self.patch_size))\n            H_patch = img_H[rnd_h:rnd_h + self.patch_size, rnd_w:rnd_w + self.patch_size, :]\n            H_patches.append(H_patch)\n\n        return H_patches\n\n    def __getitem__(self, index):\n\n        H_path = \'toy.png\'\n        if self.opt[\'phase\'] == \'train\':\n\n            patch_H = self.H_data[index]\n\n            # --------------------------------\n            # augmentation - flip and/or rotate\n            # --------------------------------\n            mode = np.random.randint(0, 8)\n            patch_H = util.augment_img(patch_H, mode=mode)\n\n            patch_H = util.uint2tensor3(patch_H)\n            patch_L = patch_H.clone()\n\n            # ------------------------------------\n            # add noise\n            # ------------------------------------\n            noise = torch.randn(patch_L.size()).mul_(self.sigma/255.0)\n            patch_L.add_(noise)\n\n        else:\n\n            H_path = self.paths_H[index]\n            img_H = util.imread_uint(H_path, self.n_channels)\n            img_H = util.uint2single(img_H)\n            img_L = np.copy(img_H)\n\n            # ------------------------------------\n            # add noise\n            # ------------------------------------\n            np.random.seed(seed=0)\n            img_L += np.random.normal(0, self.sigma_test/255.0, img_L.shape)\n            patch_L, patch_H = util.single2tensor3(img_L), util.single2tensor3(img_H)\n\n        L_path = H_path\n        return {\'L\': patch_L, \'H\': patch_H, \'L_path\': L_path, \'H_path\': H_path}\n\n    def __len__(self):\n        return len(self.H_data)\n'"
data/dataset_dpsr.py,8,"b'import random\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nimport utils.utils_image as util\n\n\nclass DatasetDPSR(data.Dataset):\n    \'\'\'\n    # -----------------------------------------\n    # Get L/H/M for noisy image SR.\n    # Only ""paths_H"" is needed, sythesize bicubicly downsampled L on-the-fly.\n    # -----------------------------------------\n    # e.g., SRResNet super-resolver prior for DPSR\n    # -----------------------------------------\n    \'\'\'\n\n    def __init__(self, opt):\n        super(DatasetDPSR, self).__init__()\n        self.opt = opt\n        self.n_channels = opt[\'n_channels\'] if opt[\'n_channels\'] else 3\n        self.sf = opt[\'scale\'] if opt[\'scale\'] else 4\n        self.patch_size = self.opt[\'H_size\'] if self.opt[\'H_size\'] else 96\n        self.L_size = self.patch_size // self.sf\n        self.sigma = opt[\'sigma\'] if opt[\'sigma\'] else [0, 50]\n        self.sigma_min, self.sigma_max = self.sigma[0], self.sigma[1]\n        self.sigma_test = opt[\'sigma_test\'] if opt[\'sigma_test\'] else 0\n\n        # ------------------------------------\n        # get paths of L/H\n        # ------------------------------------\n        self.paths_H = util.get_image_paths(opt[\'dataroot_H\'])\n        self.paths_L = util.get_image_paths(opt[\'dataroot_L\'])\n\n        assert self.paths_H, \'Error: H path is empty.\'\n\n    def __getitem__(self, index):\n\n        # ------------------------------------\n        # get H image\n        # ------------------------------------\n        H_path = self.paths_H[index]\n        img_H = util.imread_uint(H_path, self.n_channels)\n        img_H = util.uint2single(img_H)\n\n        # ------------------------------------\n        # modcrop for SR\n        # ------------------------------------\n        img_H = util.modcrop(img_H, self.sf)\n\n        # ------------------------------------\n        # sythesize L image via matlab\'s bicubic\n        # ------------------------------------\n        H, W, _ = img_H.shape\n        img_L = util.imresize_np(img_H, 1 / self.sf, True)\n\n        if self.opt[\'phase\'] == \'train\':\n            """"""\n            # --------------------------------\n            # get L/H patch pairs\n            # --------------------------------\n            """"""\n            H, W, C = img_L.shape\n\n            # --------------------------------\n            # randomly crop L patch\n            # --------------------------------\n            rnd_h = random.randint(0, max(0, H - self.L_size))\n            rnd_w = random.randint(0, max(0, W - self.L_size))\n            img_L = img_L[rnd_h:rnd_h + self.L_size, rnd_w:rnd_w + self.L_size, :]\n\n            # --------------------------------\n            # crop corresponding H patch\n            # --------------------------------\n            rnd_h_H, rnd_w_H = int(rnd_h * self.sf), int(rnd_w * self.sf)\n            img_H = img_H[rnd_h_H:rnd_h_H + self.patch_size, rnd_w_H:rnd_w_H + self.patch_size, :]\n\n            # --------------------------------\n            # augmentation - flip and/or rotate\n            # --------------------------------\n            mode = np.random.randint(0, 8)\n            img_L, img_H = util.augment_img(img_L, mode=mode), util.augment_img(img_H, mode=mode)\n\n            # --------------------------------\n            # get patch pairs\n            # --------------------------------\n            img_H, img_L = util.single2tensor3(img_H), util.single2tensor3(img_L)\n\n            # --------------------------------\n            # select noise level and get Gaussian noise\n            # --------------------------------\n            if random.random() < 0.1:\n                noise_level = torch.zeros(1).float()\n            else:\n                noise_level = torch.FloatTensor([np.random.uniform(self.sigma_min, self.sigma_max)])/255.0\n                # noise_level = torch.rand(1)*50/255.0\n                # noise_level = torch.min(torch.from_numpy(np.float32([7*np.random.chisquare(2.5)/255.0])),torch.Tensor([50./255.]))\n    \n        else:\n\n            img_H, img_L = util.single2tensor3(img_H), util.single2tensor3(img_L)\n\n            noise_level = torch.FloatTensor([self.sigma_test])\n\n        # ------------------------------------\n        # add noise\n        # ------------------------------------\n        noise = torch.randn(img_L.size()).mul_(noise_level).float()\n        img_L.add_(noise)\n\n        # ------------------------------------\n        # get noise level map M\n        # ------------------------------------\n        M_vector = noise_level.unsqueeze(1).unsqueeze(1)\n        M = M_vector.repeat(1, img_L.size()[-2], img_L.size()[-1])\n\n\n        """"""\n        # -------------------------------------\n        # concat L and noise level map M\n        # -------------------------------------\n        """"""\n        img_L = torch.cat((img_L, M), 0)\n\n\n        L_path = H_path\n\n        return {\'L\': img_L, \'H\': img_H, \'L_path\': L_path, \'H_path\': H_path}\n\n    def __len__(self):\n        return len(self.paths_H)\n'"
data/dataset_fdncnn.py,7,"b'import random\r\nimport numpy as np\r\nimport torch\r\nimport torch.utils.data as data\r\nimport utils.utils_image as util\r\n\r\n\r\nclass DatasetFDnCNN(data.Dataset):\r\n    """"""\r\n    # -----------------------------------------\r\n    # Get L/H/M for denosing on AWGN with a range of sigma.\r\n    # Only dataroot_H is needed.\r\n    # -----------------------------------------\r\n    # e.g., FDnCNN, H = f(cat(L, M)), M is noise level map\r\n    # -----------------------------------------\r\n    """"""\r\n\r\n    def __init__(self, opt):\r\n        super(DatasetFDnCNN, self).__init__()\r\n        self.opt = opt\r\n        self.n_channels = opt[\'n_channels\'] if opt[\'n_channels\'] else 3\r\n        self.patch_size = self.opt[\'H_size\'] if opt[\'H_size\'] else 64\r\n        self.sigma = opt[\'sigma\'] if opt[\'sigma\'] else [0, 75]\r\n        self.sigma_min, self.sigma_max = self.sigma[0], self.sigma[1]\r\n        self.sigma_test = opt[\'sigma_test\'] if opt[\'sigma_test\'] else 25\r\n\r\n        # -------------------------------------\r\n        # get the path of H, return None if input is None\r\n        # -------------------------------------\r\n        self.paths_H = util.get_image_paths(opt[\'dataroot_H\'])\r\n\r\n    def __getitem__(self, index):\r\n        # -------------------------------------\r\n        # get H image\r\n        # -------------------------------------\r\n        H_path = self.paths_H[index]\r\n        img_H = util.imread_uint(H_path, self.n_channels)\r\n\r\n        L_path = H_path\r\n\r\n        if self.opt[\'phase\'] == \'train\':\r\n            """"""\r\n            # --------------------------------\r\n            # get L/H/M patch pairs\r\n            # --------------------------------\r\n            """"""\r\n            H, W = img_H.shape[:2]\r\n\r\n            # ---------------------------------\r\n            # randomly crop the patch\r\n            # ---------------------------------\r\n            rnd_h = random.randint(0, max(0, H - self.patch_size))\r\n            rnd_w = random.randint(0, max(0, W - self.patch_size))\r\n            patch_H = img_H[rnd_h:rnd_h + self.patch_size, rnd_w:rnd_w + self.patch_size, :]\r\n\r\n            # ---------------------------------\r\n            # augmentation - flip, rotate\r\n            # ---------------------------------\r\n            mode = np.random.randint(0, 8)\r\n            patch_H = util.augment_img(patch_H, mode=mode)\r\n\r\n            # ---------------------------------\r\n            # HWC to CHW, numpy(uint) to tensor\r\n            # ---------------------------------\r\n            img_H = util.uint2tensor3(patch_H)\r\n            img_L = img_H.clone()\r\n\r\n            # ---------------------------------\r\n            # get noise level\r\n            # ---------------------------------\r\n            # noise_level = torch.FloatTensor([np.random.randint(self.sigma_min, self.sigma_max)])/255.0\r\n            noise_level = torch.FloatTensor([np.random.uniform(self.sigma_min, self.sigma_max)])/255.0\r\n\r\n            noise_level_map = torch.ones((1, img_L.size(1), img_L.size(2))).mul_(noise_level).float()  # torch.full((1, img_L.size(1), img_L.size(2)), noise_level)\r\n\r\n            # ---------------------------------\r\n            # add noise\r\n            # ---------------------------------\r\n            noise = torch.randn(img_L.size()).mul_(noise_level).float()\r\n            img_L.add_(noise)\r\n\r\n        else:\r\n            """"""\r\n            # --------------------------------\r\n            # get L/H/M image pairs\r\n            # --------------------------------\r\n            """"""\r\n            img_H = util.uint2single(img_H)\r\n            img_L = np.copy(img_H)\r\n            np.random.seed(seed=0)\r\n            img_L += np.random.normal(0, self.sigma_test/255.0, img_L.shape)\r\n            noise_level_map = torch.ones((1, img_L.shape[0], img_L.shape[1])).mul_(self.sigma_test/255.0).float()  # torch.full((1, img_L.size(1), img_L.size(2)), noise_level)\r\n\r\n            # ---------------------------------\r\n            # L/H image pairs\r\n            # ---------------------------------\r\n            img_H, img_L = util.single2tensor3(img_H), util.single2tensor3(img_L)\r\n\r\n        """"""\r\n        # -------------------------------------\r\n        # concat L and noise level map M\r\n        # -------------------------------------\r\n        """"""\r\n        img_L = torch.cat((img_L, noise_level_map), 0)\r\n\r\n        return {\'L\': img_L, \'H\': img_H, \'L_path\': L_path, \'H_path\': H_path}\r\n\r\n    def __len__(self):\r\n        return len(self.paths_H)\r\n'"
data/dataset_ffdnet.py,5,"b'import random\r\nimport numpy as np\r\nimport torch\r\nimport torch.utils.data as data\r\nimport utils.utils_image as util\r\n\r\n\r\nclass DatasetFFDNet(data.Dataset):\r\n    """"""\r\n    # -----------------------------------------\r\n    # Get L/H/M for denosing on AWGN with a range of sigma.\r\n    # Only dataroot_H is needed.\r\n    # -----------------------------------------\r\n    # e.g., FFDNet, H = f(L, sigma), sigma is noise level\r\n    # -----------------------------------------\r\n    """"""\r\n\r\n    def __init__(self, opt):\r\n        super(DatasetFFDNet, self).__init__()\r\n        self.opt = opt\r\n        self.n_channels = opt[\'n_channels\'] if opt[\'n_channels\'] else 3\r\n        self.patch_size = self.opt[\'H_size\'] if opt[\'H_size\'] else 64\r\n        self.sigma = opt[\'sigma\'] if opt[\'sigma\'] else [0, 75]\r\n        self.sigma_min, self.sigma_max = self.sigma[0], self.sigma[1]\r\n        self.sigma_test = opt[\'sigma_test\'] if opt[\'sigma_test\'] else 25\r\n\r\n        # -------------------------------------\r\n        # get the path of H, return None if input is None\r\n        # -------------------------------------\r\n        self.paths_H = util.get_image_paths(opt[\'dataroot_H\'])\r\n\r\n    def __getitem__(self, index):\r\n        # -------------------------------------\r\n        # get H image\r\n        # -------------------------------------\r\n        H_path = self.paths_H[index]\r\n        img_H = util.imread_uint(H_path, self.n_channels)\r\n\r\n        L_path = H_path\r\n\r\n        if self.opt[\'phase\'] == \'train\':\r\n            """"""\r\n            # --------------------------------\r\n            # get L/H/M patch pairs\r\n            # --------------------------------\r\n            """"""\r\n            H, W = img_H.shape[:2]\r\n\r\n            # ---------------------------------\r\n            # randomly crop the patch\r\n            # ---------------------------------\r\n            rnd_h = random.randint(0, max(0, H - self.patch_size))\r\n            rnd_w = random.randint(0, max(0, W - self.patch_size))\r\n            patch_H = img_H[rnd_h:rnd_h + self.patch_size, rnd_w:rnd_w + self.patch_size, :]\r\n\r\n            # ---------------------------------\r\n            # augmentation - flip, rotate\r\n            # ---------------------------------\r\n            mode = np.random.randint(0, 8)\r\n            patch_H = util.augment_img(patch_H, mode=mode)\r\n\r\n            # ---------------------------------\r\n            # HWC to CHW, numpy(uint) to tensor\r\n            # ---------------------------------\r\n            img_H = util.uint2tensor3(patch_H)\r\n            img_L = img_H.clone()\r\n\r\n            # ---------------------------------\r\n            # get noise level\r\n            # ---------------------------------\r\n            # noise_level = torch.FloatTensor([np.random.randint(self.sigma_min, self.sigma_max)])/255.0\r\n            noise_level = torch.FloatTensor([np.random.uniform(self.sigma_min, self.sigma_max)])/255.0\r\n\r\n            # ---------------------------------\r\n            # add noise\r\n            # ---------------------------------\r\n            noise = torch.randn(img_L.size()).mul_(noise_level).float()\r\n            img_L.add_(noise)\r\n\r\n        else:\r\n            """"""\r\n            # --------------------------------\r\n            # get L/H/sigma image pairs\r\n            # --------------------------------\r\n            """"""\r\n            img_H = util.uint2single(img_H)\r\n            img_L = np.copy(img_H)\r\n            np.random.seed(seed=0)\r\n            img_L += np.random.normal(0, self.sigma_test/255.0, img_L.shape)\r\n            noise_level = torch.FloatTensor([self.sigma_test/255.0])\r\n\r\n            # ---------------------------------\r\n            # L/H image pairs\r\n            # ---------------------------------\r\n            img_H, img_L = util.single2tensor3(img_H), util.single2tensor3(img_L)\r\n\r\n        noise_level = noise_level.unsqueeze(1).unsqueeze(1)\r\n\r\n\r\n        return {\'L\': img_L, \'H\': img_H, \'C\': noise_level, \'L_path\': L_path, \'H_path\': H_path}\r\n\r\n    def __len__(self):\r\n        return len(self.paths_H)\r\n'"
data/dataset_l.py,1,"b'import torch.utils.data as data\nimport utils.utils_image as util\n\n\nclass DatasetL(data.Dataset):\n    \'\'\'\n    # -----------------------------------------\n    # Get L in testing.\n    # Only ""dataroot_L"" is needed.\n    # -----------------------------------------\n    # -----------------------------------------\n    \'\'\'\n\n    def __init__(self, opt):\n        super(DatasetL, self).__init__()\n        print(\'Read L in testing. Only ""dataroot_L"" is needed.\')\n        self.opt = opt\n        self.n_channels = opt[\'n_channels\'] if opt[\'n_channels\'] else 3\n\n        # ------------------------------------\n        # get the path of L\n        # ------------------------------------\n        self.paths_L = util.get_image_paths(opt[\'dataroot_L\'])\n        assert self.paths_L, \'Error: L paths are empty.\'\n\n    def __getitem__(self, index):\n        L_path = None\n\n        # ------------------------------------\n        # get L image\n        # ------------------------------------\n        L_path = self.paths_L[index]\n        img_L = util.imread_uint(L_path, self.n_channels)\n\n        # ------------------------------------\n        # HWC to CHW, numpy to tensor\n        # ------------------------------------\n        img_L = util.uint2tensor3(img_L)\n\n        return {\'L\': img_L, \'L_path\': L_path}\n\n    def __len__(self):\n        return len(self.paths_L)\n'"
data/dataset_plain.py,1,"b'import random\nimport numpy as np\nimport torch.utils.data as data\nimport utils.utils_image as util\n\n\nclass DatasetPlain(data.Dataset):\n    \'\'\'\n    # -----------------------------------------\n    # Get L/H for image-to-image mapping.\n    # Both ""paths_L"" and ""paths_H"" are needed.\n    # -----------------------------------------\n    # e.g., train denoiser with L and H\n    # -----------------------------------------\n    \'\'\'\n\n    def __init__(self, opt):\n        super(DatasetPlain, self).__init__()\n        print(\'Get L/H for image-to-image mapping. Both ""paths_L"" and ""paths_H"" are needed.\')\n        self.opt = opt\n        self.n_channels = opt[\'n_channels\'] if opt[\'n_channels\'] else 3\n        self.patch_size = self.opt[\'H_size\'] if self.opt[\'H_size\'] else 64\n\n        # ------------------------------------\n        # get the path of L/H\n        # ------------------------------------\n        self.paths_H = util.get_image_paths(opt[\'dataroot_H\'])\n        self.paths_L = util.get_image_paths(opt[\'dataroot_L\'])\n\n        assert self.paths_H, \'Error: H path is empty.\'\n        assert self.paths_L, \'Error: L path is empty. Plain dataset assumes both L and H are given!\'\n        if self.paths_L and self.paths_H:\n            assert len(self.paths_L) == len(self.paths_H), \'L/H mismatch - {}, {}.\'.format(len(self.paths_L), len(self.paths_H))\n\n    def __getitem__(self, index):\n\n        # ------------------------------------\n        # get H image\n        # ------------------------------------\n        H_path = self.paths_H[index]\n        img_H = util.imread_uint(H_path, self.n_channels)\n\n        # ------------------------------------\n        # get L image\n        # ------------------------------------\n        L_path = self.paths_L[index]\n        img_L = util.imread_uint(L_path, self.n_channels)\n\n        # ------------------------------------\n        # if train, get L/H patch pair\n        # ------------------------------------\n        if self.opt[\'phase\'] == \'train\':\n\n            H, W, _ = img_H.shape\n\n            # --------------------------------\n            # randomly crop the patch\n            # --------------------------------\n            rnd_h = random.randint(0, max(0, H - self.patch_size))\n            rnd_w = random.randint(0, max(0, W - self.patch_size))\n            patch_L = img_L[rnd_h:rnd_h + self.patch_size, rnd_w:rnd_w + self.patch_size, :]\n            patch_H = img_H[rnd_h:rnd_h + self.patch_size, rnd_w:rnd_w + self.patch_size, :]\n\n            # --------------------------------\n            # augmentation - flip and/or rotate\n            # --------------------------------\n            mode = np.random.randint(0, 8)\n            patch_L, patch_H = util.image_augment(patch_L, mode=mode), util.image_augment(patch_H, mode=mode)\n\n            # --------------------------------\n            # HWC to CHW, numpy(uint) to tensor\n            # --------------------------------\n            img_L, img_H = util.uint2tensor3(patch_L), util.uint2tensor3(patch_H)\n\n        else:\n\n            # --------------------------------\n            # HWC to CHW, numpy(uint) to tensor\n            # --------------------------------\n            img_L, img_H = util.uint2tensor3(img_L), util.uint2tensor3(img_H)\n\n        return {\'L\': img_L, \'H\': img_H, \'L_path\': L_path, \'H_path\': H_path}\n\n    def __len__(self):\n        return len(self.paths_H)\n'"
data/dataset_plainpatch.py,1,"b'import os.path\nimport random\nimport numpy as np\nimport torch.utils.data as data\nimport utils.utils_image as util\n\n\n\nclass DatasetPlainPatch(data.Dataset):\n    \'\'\'\n    # -----------------------------------------\n    # Get L/H for image-to-image mapping.\n    # Both ""paths_L"" and ""paths_H"" are needed.\n    # -----------------------------------------\n    # e.g., train denoiser with L and H patches\n    # create a large patch dataset first\n    # -----------------------------------------\n    \'\'\'\n\n    def __init__(self, opt):\n        super(DatasetPlainPatch, self).__init__()\n        print(\'Get L/H for image-to-image mapping. Both ""paths_L"" and ""paths_H"" are needed.\')\n        self.opt = opt\n        self.n_channels = opt[\'n_channels\'] if opt[\'n_channels\'] else 3\n        self.patch_size = self.opt[\'H_size\'] if self.opt[\'H_size\'] else 64\n\n        self.num_patches_per_image = opt[\'num_patches_per_image\'] if opt[\'num_patches_per_image\'] else 40\n        self.num_sampled = opt[\'num_sampled\'] if opt[\'num_sampled\'] else 3000\n\n        # -------------------\n        # get the path of L/H\n        # -------------------\n        self.paths_H = util.get_image_paths(opt[\'dataroot_H\'])\n        self.paths_L = util.get_image_paths(opt[\'dataroot_L\'])\n\n        assert self.paths_H, \'Error: H path is empty.\'\n        assert self.paths_L, \'Error: L path is empty. This dataset uses L path, you can use dataset_dnpatchh\'\n        if self.paths_L and self.paths_H:\n            assert len(self.paths_L) == len(self.paths_H), \'H and L datasets have different number of images - {}, {}.\'.format(len(self.paths_L), len(self.paths_H))\n\n        # ------------------------------------\n        # number of sampled images\n        # ------------------------------------\n        self.num_sampled = min(self.num_sampled, len(self.paths_H))\n\n        # ------------------------------------\n        # reserve space with zeros\n        # ------------------------------------\n        self.total_patches = self.num_sampled * self.num_patches_per_image\n        self.H_data = np.zeros([self.total_patches, self.path_size, self.path_size, self.n_channels], dtype=np.uint8)\n        self.L_data = np.zeros([self.total_patches, self.path_size, self.path_size, self.n_channels], dtype=np.uint8)\n\n        # ------------------------------------\n        # update H patches\n        # ------------------------------------\n        self.update_data()\n\n\n    def update_data(self):\n        """"""\n        # ------------------------------------\n        # update whole L/H patches\n        # ------------------------------------\n        """"""\n        self.index_sampled = random.sample(range(0, len(self.paths_H), 1), self.num_sampled)\n        n_count = 0\n\n        for i in range(len(self.index_sampled)):\n            L_patches, H_patches = self.get_patches(self.index_sampled[i])\n            for (L_patch, H_patch) in zip(L_patches, H_patches):\n                self.L_data[n_count,:,:,:] = L_patch\n                self.H_data[n_count,:,:,:] = H_patch\n                n_count += 1\n\n        print(\'Training data updated! Total number of patches is:  %5.2f X %5.2f = %5.2f\\n\' % (len(self.H_data)//128, 128, len(self.H_data)))\n\n    def get_patches(self, index):\n        """"""\n        # ------------------------------------\n        # get L/H patches from L/H images\n        # ------------------------------------\n        """"""\n        L_path = self.paths_L[index]\n        H_path = self.paths_H[index]\n        img_L = util.imread_uint(L_path, self.n_channels)  # uint format\n        img_H = util.imread_uint(H_path, self.n_channels)  # uint format\n\n        H, W = img_H.shape[:2]\n\n        L_patches, H_patches = [], []\n\n        num = self.num_patches_per_image\n        for _ in range(num):\n            rnd_h = random.randint(0, max(0, H - self.path_size))\n            rnd_w = random.randint(0, max(0, W - self.path_size))\n            L_patch = img_L[rnd_h:rnd_h + self.path_size, rnd_w:rnd_w + self.path_size, :]\n            H_patch = img_H[rnd_h:rnd_h + self.path_size, rnd_w:rnd_w + self.path_size, :]\n            L_patches.append(L_patch)\n            H_patches.append(H_patch)\n\n        return L_patches, H_patches\n\n    def __getitem__(self, index):\n\n        if self.opt[\'phase\'] == \'train\':\n\n            patch_L, patch_H = self.L_data[index], self.H_data[index]\n\n            # --------------------------------\n            # augmentation - flip and/or rotate\n            # --------------------------------\n            mode = np.random.randint(0, 8)\n            patch_L = util.augment_img(patch_L, mode=mode)\n            patch_H = util.augment_img(patch_H, mode=mode)\n\n            patch_L, patch_H = util.uint2tensor3(patch_L), util.uint2tensor3(patch_H)\n\n        else:\n\n            L_path, H_path = self.paths_L[index], self.paths_H[index]\n            patch_L = util.imread_uint(L_path, self.n_channels)\n            patch_H = util.imread_uint(H_path, self.n_channels)\n\n            patch_L, patch_H = util.uint2tensor3(patch_L), util.uint2tensor3(patch_H)\n\n        return {\'L\': patch_L, \'H\': patch_H}\n\n\n    def __len__(self):\n        \n        return self.total_patches\n'"
data/dataset_sr.py,1,"b'import random\nimport numpy as np\nimport torch.utils.data as data\nimport utils.utils_image as util\n\n\nclass DatasetSR(data.Dataset):\n    \'\'\'\n    # -----------------------------------------\n    # Get L/H for SISR.\n    # If only ""paths_H"" is provided, sythesize bicubicly downsampled L on-the-fly.\n    # -----------------------------------------\n    # e.g., SRResNet\n    # -----------------------------------------\n    \'\'\'\n\n    def __init__(self, opt):\n        super(DatasetSR, self).__init__()\n        self.opt = opt\n        self.n_channels = opt[\'n_channels\'] if opt[\'n_channels\'] else 3\n        self.sf = opt[\'scale\'] if opt[\'scale\'] else 4\n        self.patch_size = self.opt[\'H_size\'] if self.opt[\'H_size\'] else 96\n        self.L_size = self.patch_size // self.sf\n\n        # ------------------------------------\n        # get paths of L/H\n        # ------------------------------------\n        self.paths_H = util.get_image_paths(opt[\'dataroot_H\'])\n        self.paths_L = util.get_image_paths(opt[\'dataroot_L\'])\n\n        assert self.paths_H, \'Error: H path is empty.\'\n        if self.paths_L and self.paths_H:\n            assert len(self.paths_L) == len(self.paths_H), \'L/H mismatch - {}, {}.\'.format(len(self.paths_L), len(self.paths_H))\n\n    def __getitem__(self, index):\n\n        L_path = None\n        # ------------------------------------\n        # get H image\n        # ------------------------------------\n        H_path = self.paths_H[index]\n        img_H = util.imread_uint(H_path, self.n_channels)\n        img_H = util.uint2single(img_H)\n\n        # ------------------------------------\n        # modcrop\n        # ------------------------------------\n        img_H = util.modcrop(img_H, self.sf)\n\n        # ------------------------------------\n        # get L image\n        # ------------------------------------\n        if self.paths_L:\n            # --------------------------------\n            # directly load L image\n            # --------------------------------\n            L_path = self.paths_L[index]\n            img_L = util.imread_uint(L_path, self.n_channels)\n            img_L = util.uint2single(img_L)\n\n        else:\n            # --------------------------------\n            # sythesize L image via matlab\'s bicubic\n            # --------------------------------\n            H, W = img_H.shape[:2]\n            img_L = util.imresize_np(img_H, 1 / self.sf, True)\n\n        # ------------------------------------\n        # if train, get L/H patch pair\n        # ------------------------------------\n        if self.opt[\'phase\'] == \'train\':\n\n            H, W, C = img_L.shape\n\n            # --------------------------------\n            # randomly crop the L patch\n            # --------------------------------\n            rnd_h = random.randint(0, max(0, H - self.L_size))\n            rnd_w = random.randint(0, max(0, W - self.L_size))\n            img_L = img_L[rnd_h:rnd_h + self.L_size, rnd_w:rnd_w + self.L_size, :]\n\n            # --------------------------------\n            # crop corresponding H patch\n            # --------------------------------\n            rnd_h_H, rnd_w_H = int(rnd_h * self.sf), int(rnd_w * self.sf)\n            img_H = img_H[rnd_h_H:rnd_h_H + self.patch_size, rnd_w_H:rnd_w_H + self.patch_size, :]\n\n            # --------------------------------\n            # augmentation - flip and/or rotate\n            # --------------------------------\n            mode = np.random.randint(0, 8)\n            img_L, img_H = util.augment_img(img_L, mode=mode), util.augment_img(img_H, mode=mode)\n\n        # ------------------------------------\n        # L/H pairs, HWC to CHW, numpy to tensor\n        # ------------------------------------\n        img_H, img_L = util.single2tensor3(img_H), util.single2tensor3(img_L)\n\n        if L_path is None:\n            L_path = H_path\n\n        return {\'L\': img_L, \'H\': img_H, \'L_path\': L_path, \'H_path\': H_path}\n\n    def __len__(self):\n        return len(self.paths_H)\n'"
data/dataset_srmd.py,11,"b'import random\nimport numpy as np\nimport torch\nimport torch.utils.data as data\nimport utils.utils_image as util\nfrom utils import utils_sisr\n\n\nimport hdf5storage\nimport os\n\n\nclass DatasetSRMD(data.Dataset):\n    \'\'\'\n    # -----------------------------------------\n    # Get L/H/M for noisy image SR with Gaussian kernels.\n    # Only ""paths_H"" is needed, sythesize bicubicly downsampled L on-the-fly.\n    # -----------------------------------------\n    # e.g., SRMD, H = f(L, kernel, sigma), sigma is noise level\n    # -----------------------------------------\n    \'\'\'\n\n    def __init__(self, opt):\n        super(DatasetSRMD, self).__init__()\n        self.opt = opt\n        self.n_channels = opt[\'n_channels\'] if opt[\'n_channels\'] else 3\n        self.sf = opt[\'scale\'] if opt[\'scale\'] else 4\n        self.patch_size = self.opt[\'H_size\'] if self.opt[\'H_size\'] else 96\n        self.L_size = self.patch_size // self.sf\n        self.sigma = opt[\'sigma\'] if opt[\'sigma\'] else [0, 50]\n        self.sigma_min, self.sigma_max = self.sigma[0], self.sigma[1]\n        self.sigma_test = opt[\'sigma_test\'] if opt[\'sigma_test\'] else 0\n\n        # -------------------------------------\n        # PCA projection matrix\n        # -------------------------------------\n        self.p = hdf5storage.loadmat(os.path.join(\'kernels\', \'srmd_pca_pytorch.mat\'))[\'p\']\n        self.ksize = int(np.sqrt(self.p.shape[-1]))  # kernel size\n\n        # ------------------------------------\n        # get paths of L/H\n        # ------------------------------------\n        self.paths_H = util.get_image_paths(opt[\'dataroot_H\'])\n        self.paths_L = util.get_image_paths(opt[\'dataroot_L\'])\n\n    def __getitem__(self, index):\n\n        # ------------------------------------\n        # get H image\n        # ------------------------------------\n        H_path = self.paths_H[index]\n        img_H = util.imread_uint(H_path, self.n_channels)\n        img_H = util.uint2single(img_H)\n\n        # ------------------------------------\n        # modcrop for SR\n        # ------------------------------------\n        img_H = util.modcrop(img_H, self.sf)\n\n        # ------------------------------------\n        # kernel\n        # ------------------------------------\n        if self.opt[\'phase\'] == \'train\':\n            l_max = 50\n            theta = np.pi*np.random.rand(1)\n            l1 = 0.1+l_max*np.random.rand(1)\n            l2 = 0.1+(l1-0.1)*np.random.rand(1)\n\n            kernel = utils_sisr.anisotropic_Gaussian(ksize=self.ksize, theta=theta[0], l1=l1[0], l2=l2[0])\n        else:\n            kernel = utils_sisr.anisotropic_Gaussian(ksize=self.ksize, theta=np.pi, l1=0.1, l2=0.1)\n\n        k = np.reshape(kernel, (-1), order=""F"")\n        k_reduced = np.dot(self.p, k)\n        k_reduced = torch.from_numpy(k_reduced).float()\n\n        # ------------------------------------\n        # sythesize L image via specified degradation model\n        # ------------------------------------\n        H, W, _ = img_H.shape\n        img_L = utils_sisr.srmd_degradation(img_H, kernel, self.sf)\n        img_L = np.float32(img_L)\n\n        if self.opt[\'phase\'] == \'train\':\n            """"""\n            # --------------------------------\n            # get L/H patch pairs\n            # --------------------------------\n            """"""\n            H, W, C = img_L.shape\n\n            # --------------------------------\n            # randomly crop L patch\n            # --------------------------------\n            rnd_h = random.randint(0, max(0, H - self.L_size))\n            rnd_w = random.randint(0, max(0, W - self.L_size))\n            img_L = img_L[rnd_h:rnd_h + self.L_size, rnd_w:rnd_w + self.L_size, :]\n\n            # --------------------------------\n            # crop corresponding H patch\n            # --------------------------------\n            rnd_h_H, rnd_w_H = int(rnd_h * self.sf), int(rnd_w * self.sf)\n            img_H = img_H[rnd_h_H:rnd_h_H + self.patch_size, rnd_w_H:rnd_w_H + self.patch_size, :]\n\n            # --------------------------------\n            # augmentation - flip and/or rotate\n            # --------------------------------\n            mode = np.random.randint(0, 8)\n            img_L, img_H = util.augment_img(img_L, mode=mode), util.augment_img(img_H, mode=mode)\n\n            # --------------------------------\n            # get patch pairs\n            # --------------------------------\n            img_H, img_L = util.single2tensor3(img_H), util.single2tensor3(img_L)\n\n            # --------------------------------\n            # select noise level and get Gaussian noise\n            # --------------------------------\n            if random.random() < 0.1:\n                noise_level = torch.zeros(1).float()\n            else:\n                noise_level = torch.FloatTensor([np.random.uniform(self.sigma_min, self.sigma_max)])/255.0\n                # noise_level = torch.rand(1)*50/255.0\n                # noise_level = torch.min(torch.from_numpy(np.float32([7*np.random.chisquare(2.5)/255.0])),torch.Tensor([50./255.]))\n    \n        else:\n\n            img_H, img_L = util.single2tensor3(img_H), util.single2tensor3(img_L)\n\n            noise_level = noise_level = torch.FloatTensor([self.sigma_test])\n\n        # ------------------------------------\n        # add noise\n        # ------------------------------------\n        noise = torch.randn(img_L.size()).mul_(noise_level).float()\n        img_L.add_(noise)\n\n        # ------------------------------------\n        # get degradation map M\n        # ------------------------------------\n        M_vector = torch.cat((k_reduced, noise_level), 0).unsqueeze(1).unsqueeze(1)\n        M = M_vector.repeat(1, img_L.size()[-2], img_L.size()[-1])\n\n        """"""\n        # -------------------------------------\n        # concat L and noise level map M\n        # -------------------------------------\n        """"""\n        \n        print(img_L.shape)\n        print(M.shape)\n        img_L = torch.cat((img_L, M), 0)\n\n\n        L_path = H_path\n\n        return {\'L\': img_L, \'H\': img_H, \'L_path\': L_path, \'H_path\': H_path}\n\n    def __len__(self):\n        return len(self.paths_H)\n'"
data/select_dataset.py,0,"b""\n\n'''\n# --------------------------------------------\n# select dataset\n# --------------------------------------------\n# Kai Zhang (github: https://github.com/cszn)\n# --------------------------------------------\n'''\n\n\ndef define_Dataset(dataset_opt):\n    dataset_type = dataset_opt['dataset_type'].lower()\n    if dataset_type in ['l', 'low-quality', 'input-only']:\n        from data.dataset_l import DatasetL as D\n\n    # -----------------------------------------\n    # denoising\n    # -----------------------------------------\n    elif dataset_type in ['dncnn', 'denoising']:\n        from data.dataset_dncnn import DatasetDnCNN as D\n\n    elif dataset_type in ['dnpatch']:\n        from data.dataset_dnpatch import DatasetDnPatch as D\n\n    elif dataset_type in ['ffdnet', 'denoising-noiselevel']:\n        from data.dataset_ffdnet import DatasetFFDNet as D\n\n    elif dataset_type in ['fdncnn', 'denoising-noiselevelmap']:\n        from data.dataset_fdncnn import DatasetFDnCNN as D\n\n    # -----------------------------------------\n    # super-resolution\n    # -----------------------------------------\n    elif dataset_type in ['sr', 'super-resolution']:\n        from data.dataset_sr import DatasetSR as D\n\n    elif dataset_type in ['srmd']:\n        from data.dataset_srmd import DatasetSRMD as D\n\n    elif dataset_type in ['dpsr', 'dnsr']:\n        from data.dataset_dpsr import DatasetDPSR as D\n\n    # -----------------------------------------\n    # common\n    # -----------------------------------------\n    elif dataset_type in ['plain']:\n        from data.dataset_plain import DatasetPlain as D\n\n    elif dataset_type in ['plainpatch']:\n        from data.dataset_plainpatch import DatasetPlainPatch as D\n\n    else:\n        raise NotImplementedError('Dataset [{:s}] is not found.'.format(dataset_type))\n\n    dataset = D(dataset_opt)\n    print('Dataset [{:s} - {:s}] is created.'.format(dataset.__class__.__name__, dataset_opt['name']))\n    return dataset\n"""
models/basicblock.py,13,"b'from collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\'\'\'\n# --------------------------------------------\n# Advanced nn.Sequential\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\ndef sequential(*args):\n    """"""Advanced nn.Sequential.\n\n    Args:\n        nn.Sequential, nn.Module\n\n    Returns:\n        nn.Sequential\n    """"""\n    if len(args) == 1:\n        if isinstance(args[0], OrderedDict):\n            raise NotImplementedError(\'sequential does not support OrderedDict input.\')\n        return args[0]  # No sequential is needed.\n    modules = []\n    for module in args:\n        if isinstance(module, nn.Sequential):\n            for submodule in module.children():\n                modules.append(submodule)\n        elif isinstance(module, nn.Module):\n            modules.append(module)\n    return nn.Sequential(*modules)\n\n\n\'\'\'\n# --------------------------------------------\n# Useful blocks\n# https://github.com/xinntao/BasicSR\n# --------------------------------\n# conv + normaliation + relu (conv)\n# (PixelUnShuffle)\n# (ConditionalBatchNorm2d)\n# concat (ConcatBlock)\n# sum (ShortcutBlock)\n# resblock (ResBlock)\n# Channel Attention (CA) Layer (CALayer)\n# Residual Channel Attention Block (RCABlock)\n# Residual Channel Attention Group (RCAGroup)\n# Residual Dense Block (ResidualDenseBlock_5C)\n# Residual in Residual Dense Block (RRDB)\n# --------------------------------------------\n\'\'\'\n\n\n# --------------------------------------------\n# return nn.Sequantial of (Conv + BN + ReLU)\n# --------------------------------------------\ndef conv(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True, mode=\'CBR\', negative_slope=0.2):\n    L = []\n    for t in mode:\n        if t == \'C\':\n            L.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias))\n        elif t == \'T\':\n            L.append(nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias))\n        elif t == \'B\':\n            L.append(nn.BatchNorm2d(out_channels, momentum=0.9, eps=1e-04, affine=True))\n        elif t == \'I\':\n            L.append(nn.InstanceNorm2d(out_channels, affine=True))\n        elif t == \'R\':\n            L.append(nn.ReLU(inplace=True))\n        elif t == \'r\':\n            L.append(nn.ReLU(inplace=False))\n        elif t == \'L\':\n            L.append(nn.LeakyReLU(negative_slope=negative_slope, inplace=True))\n        elif t == \'l\':\n            L.append(nn.LeakyReLU(negative_slope=negative_slope, inplace=False))\n        elif t == \'2\':\n            L.append(nn.PixelShuffle(upscale_factor=2))\n        elif t == \'3\':\n            L.append(nn.PixelShuffle(upscale_factor=3))\n        elif t == \'4\':\n            L.append(nn.PixelShuffle(upscale_factor=4))\n        elif t == \'U\':\n            L.append(nn.Upsample(scale_factor=2, mode=\'nearest\'))\n        elif t == \'u\':\n            L.append(nn.Upsample(scale_factor=3, mode=\'nearest\'))\n        elif t == \'v\':\n            L.append(nn.Upsample(scale_factor=4, mode=\'nearest\'))\n        elif t == \'M\':\n            L.append(nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=0))\n        elif t == \'A\':\n            L.append(nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=0))\n        else:\n            raise NotImplementedError(\'Undefined type: \'.format(t))\n    return sequential(*L)\n\n\n# --------------------------------------------\n# inverse of pixel_shuffle\n# --------------------------------------------\ndef pixel_unshuffle(input, upscale_factor):\n    r""""""Rearranges elements in a Tensor of shape :math:`(C, rH, rW)` to a\n    tensor of shape :math:`(*, r^2C, H, W)`.\n\n    Authors:\n        Zhaoyi Yan, https://github.com/Zhaoyi-Yan\n        Kai Zhang, https://github.com/cszn/FFDNet\n\n    Date:\n        01/Jan/2019\n    """"""\n    batch_size, channels, in_height, in_width = input.size()\n\n    out_height = in_height // upscale_factor\n    out_width = in_width // upscale_factor\n\n    input_view = input.contiguous().view(\n        batch_size, channels, out_height, upscale_factor,\n        out_width, upscale_factor)\n\n    channels *= upscale_factor ** 2\n    unshuffle_out = input_view.permute(0, 1, 3, 5, 2, 4).contiguous()\n    return unshuffle_out.view(batch_size, channels, out_height, out_width)\n\n\nclass PixelUnShuffle(nn.Module):\n    r""""""Rearranges elements in a Tensor of shape :math:`(C, rH, rW)` to a\n    tensor of shape :math:`(*, r^2C, H, W)`.\n\n    Authors:\n        Zhaoyi Yan, https://github.com/Zhaoyi-Yan\n        Kai Zhang, https://github.com/cszn/FFDNet\n\n    Date:\n        01/Jan/2019\n    """"""\n\n    def __init__(self, upscale_factor):\n        super(PixelUnShuffle, self).__init__()\n        self.upscale_factor = upscale_factor\n\n    def forward(self, input):\n        return pixel_unshuffle(input, self.upscale_factor)\n\n    def extra_repr(self):\n        return \'upscale_factor={}\'.format(self.upscale_factor)\n\n\n# --------------------------------------------\n# conditional batch norm\n# https://github.com/pytorch/pytorch/issues/8985#issuecomment-405080775\n# --------------------------------------------\nclass ConditionalBatchNorm2d(nn.Module):\n    def __init__(self, num_features, num_classes):\n        super().__init__()\n        self.num_features = num_features\n        self.bn = nn.BatchNorm2d(num_features, affine=False)\n        self.embed = nn.Embedding(num_classes, num_features * 2)\n        self.embed.weight.data[:, :num_features].normal_(1, 0.02)  # Initialise scale at N(1, 0.02)\n        self.embed.weight.data[:, num_features:].zero_()  # Initialise bias at 0\n\n    def forward(self, x, y):\n        out = self.bn(x)\n        gamma, beta = self.embed(y).chunk(2, 1)\n        out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n        return out\n\n\n# --------------------------------------------\n# Concat the output of a submodule to its input\n# --------------------------------------------\nclass ConcatBlock(nn.Module):\n    def __init__(self, submodule):\n        super(ConcatBlock, self).__init__()\n        self.sub = submodule\n\n    def forward(self, x):\n        output = torch.cat((x, self.sub(x)), dim=1)\n        return output\n\n    def __repr__(self):\n        return self.sub.__repr__() + \'concat\'\n\n\n# --------------------------------------------\n# sum the output of a submodule to its input\n# --------------------------------------------\nclass ShortcutBlock(nn.Module):\n    def __init__(self, submodule):\n        super(ShortcutBlock, self).__init__()\n\n        self.sub = submodule\n\n    def forward(self, x):\n        output = x + self.sub(x)\n        return output\n\n    def __repr__(self):\n        tmpstr = \'Identity + \\n|\'\n        modstr = self.sub.__repr__().replace(\'\\n\', \'\\n|\')\n        tmpstr = tmpstr + modstr\n        return tmpstr\n\n\n# --------------------------------------------\n# Res Block: x + conv(relu(conv(x)))\n# --------------------------------------------\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True, mode=\'CRC\', negative_slope=0.2):\n        super(ResBlock, self).__init__()\n\n        assert in_channels == out_channels, \'Only support in_channels==out_channels.\'\n        if mode[0] in [\'R\', \'L\']:\n            mode = mode[0].lower() + mode[1:]\n\n        self.res = conv(in_channels, out_channels, kernel_size, stride, padding, bias, mode, negative_slope)\n\n    def forward(self, x):\n        res = self.res(x)\n        return x + res\n\n\n# --------------------------------------------\n# simplified information multi-distillation block (IMDB)\n# x + conv1(concat(split(relu(conv(x)))x3))\n# --------------------------------------------\nclass IMDBlock(nn.Module):\n    """"""\n    @inproceedings{hui2019lightweight,\n      title={Lightweight Image Super-Resolution with Information Multi-distillation Network},\n      author={Hui, Zheng and Gao, Xinbo and Yang, Yunchu and Wang, Xiumei},\n      booktitle={Proceedings of the 27th ACM International Conference on Multimedia (ACM MM)},\n      pages={2024--2032},\n      year={2019}\n    }\n    @inproceedings{zhang2019aim,\n      title={AIM 2019 Challenge on Constrained Super-Resolution: Methods and Results},\n      author={Kai Zhang and Shuhang Gu and Radu Timofte and others},\n      booktitle={IEEE International Conference on Computer Vision Workshops},\n      year={2019}\n    }\n    """"""\n    def __init__(self, in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True, mode=\'CL\', d_rate=0.25, negative_slope=0.05):\n        super(IMDBlock, self).__init__()\n        self.d_nc = int(in_channels * d_rate)\n        self.r_nc = int(in_channels - self.d_nc)\n\n        assert mode[0] == \'C\', \'convolutional layer first\'\n\n        self.conv1 = conv(in_channels, in_channels, kernel_size, stride, padding, bias, mode, negative_slope)\n        self.conv2 = conv(self.r_nc, in_channels, kernel_size, stride, padding, bias, mode, negative_slope)\n        self.conv3 = conv(self.r_nc, in_channels, kernel_size, stride, padding, bias, mode, negative_slope)\n        self.conv4 = conv(self.r_nc, self.d_nc, kernel_size, stride, padding, bias, mode[0], negative_slope)\n        self.conv1x1 = conv(self.d_nc*4, out_channels, kernel_size=1, stride=1, padding=0, bias=bias, mode=mode[0], negative_slope=negative_slope)\n\n    def forward(self, x):\n        d1, r1 = torch.split(self.conv1(x), (self.d_nc, self.r_nc), dim=1)\n        d2, r2 = torch.split(self.conv2(r1), (self.d_nc, self.r_nc), dim=1)\n        d3, r3 = torch.split(self.conv3(r2), (self.d_nc, self.r_nc), dim=1)\n        d4 = self.conv4(r3)\n        res = self.conv1x1(torch.cat((d1, d2, d3, d4), dim=1))\n        return x + res\n\n\n# --------------------------------------------\n# Channel Attention (CA) Layer\n# --------------------------------------------\nclass CALayer(nn.Module):\n    def __init__(self, channel=64, reduction=16):\n        super(CALayer, self).__init__()\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_fc = nn.Sequential(\n                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.conv_fc(y)\n        return x * y\n\n\n# --------------------------------------------\n# Residual Channel Attention Block (RCAB)\n# --------------------------------------------\nclass RCABlock(nn.Module):\n    def __init__(self, in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True, mode=\'CRC\', reduction=16, negative_slope=0.2):\n        super(RCABlock, self).__init__()\n        assert in_channels == out_channels, \'Only support in_channels==out_channels.\'\n        if mode[0] in [\'R\',\'L\']:\n            mode = mode[0].lower() + mode[1:]\n\n        self.res = conv(in_channels, out_channels, kernel_size, stride, padding, bias, mode, negative_slope)\n        self.ca = CALayer(out_channels, reduction)\n\n    def forward(self, x):\n        res = self.res(x)\n        res = self.ca(res)\n        return res + x\n\n\n# --------------------------------------------\n# Residual Channel Attention Group (RG)\n# --------------------------------------------\nclass RCAGroup(nn.Module):\n    def __init__(self, in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True, mode=\'CRC\', reduction=16, nb=12, negative_slope=0.2):\n        super(RCAGroup, self).__init__()\n        assert in_channels == out_channels, \'Only support in_channels==out_channels.\'\n        if mode[0] in [\'R\',\'L\']:\n            mode = mode[0].lower() + mode[1:]\n\n        RG = [RCABlock(in_channels, out_channels, kernel_size, stride, padding, bias, mode, reduction, negative_slope)  for _ in range(nb)]\n        RG.append(conv(out_channels, out_channels, mode=\'C\'))\n        self.rg = nn.Sequential(*RG)  # self.rg = ShortcutBlock(nn.Sequential(*RG))\n\n    def forward(self, x):\n        res = self.rg(x)\n        return res + x\n\n\n# --------------------------------------------\n# Residual Dense Block\n# style: 5 convs\n# --------------------------------------------\nclass ResidualDenseBlock_5C(nn.Module):\n    def __init__(self, nc=64, gc=32, kernel_size=3, stride=1, padding=1, bias=True, mode=\'CR\', negative_slope=0.2):\n        super(ResidualDenseBlock_5C, self).__init__()\n        # gc: growth channel\n        self.conv1 = conv(nc, gc, kernel_size, stride, padding, bias, mode, negative_slope)\n        self.conv2 = conv(nc+gc, gc, kernel_size, stride, padding, bias, mode, negative_slope)\n        self.conv3 = conv(nc+2*gc, gc, kernel_size, stride, padding, bias, mode, negative_slope)\n        self.conv4 = conv(nc+3*gc, gc, kernel_size, stride, padding, bias, mode, negative_slope)\n        self.conv5 = conv(nc+4*gc, nc, kernel_size, stride, padding, bias, mode[:-1], negative_slope)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        x2 = self.conv2(torch.cat((x, x1), 1))\n        x3 = self.conv3(torch.cat((x, x1, x2), 1))\n        x4 = self.conv4(torch.cat((x, x1, x2, x3), 1))\n        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n        return x5.mul_(0.2) + x\n\n\n# --------------------------------------------\n# Residual in Residual Dense Block\n# 3x5c\n# --------------------------------------------\nclass RRDB(nn.Module):\n    def __init__(self, nc=64, gc=32, kernel_size=3, stride=1, padding=1, bias=True, mode=\'CR\', negative_slope=0.2):\n        super(RRDB, self).__init__()\n\n        self.RDB1 = ResidualDenseBlock_5C(nc, gc, kernel_size, stride, padding, bias, mode, negative_slope)\n        self.RDB2 = ResidualDenseBlock_5C(nc, gc, kernel_size, stride, padding, bias, mode, negative_slope)\n        self.RDB3 = ResidualDenseBlock_5C(nc, gc, kernel_size, stride, padding, bias, mode, negative_slope)\n\n    def forward(self, x):\n        out = self.RDB1(x)\n        out = self.RDB2(out)\n        out = self.RDB3(out)\n        return out.mul_(0.2) + x\n\n\n""""""\n# --------------------------------------------\n# Upsampler\n# Kai Zhang, https://github.com/cszn/KAIR\n# --------------------------------------------\n# upsample_pixelshuffle\n# upsample_upconv\n# upsample_convtranspose\n# --------------------------------------------\n""""""\n\n\n# --------------------------------------------\n# conv + subp (+ relu)\n# --------------------------------------------\ndef upsample_pixelshuffle(in_channels=64, out_channels=3, kernel_size=3, stride=1, padding=1, bias=True, mode=\'2R\', negative_slope=0.2):\n    assert len(mode)<4 and mode[0] in [\'2\', \'3\', \'4\'], \'mode examples: 2, 2R, 2BR, 3, ..., 4BR.\'\n    up1 = conv(in_channels, out_channels * (int(mode[0]) ** 2), kernel_size, stride, padding, bias, mode=\'C\'+mode, negative_slope=negative_slope)\n    return up1\n\n\n# --------------------------------------------\n# nearest_upsample + conv (+ R)\n# --------------------------------------------\ndef upsample_upconv(in_channels=64, out_channels=3, kernel_size=3, stride=1, padding=1, bias=True, mode=\'2R\', negative_slope=0.2):\n    assert len(mode)<4 and mode[0] in [\'2\', \'3\', \'4\'], \'mode examples: 2, 2R, 2BR, 3, ..., 4BR\'\n    if mode[0] == \'2\':\n        uc = \'UC\'\n    elif mode[0] == \'3\':\n        uc = \'uC\'\n    elif mode[0] == \'4\':\n        uc = \'vC\'\n    mode = mode.replace(mode[0], uc)\n    up1 = conv(in_channels, out_channels, kernel_size, stride, padding, bias, mode=mode, negative_slope=negative_slope)\n    return up1\n\n\n# --------------------------------------------\n# convTranspose (+ relu)\n# --------------------------------------------\ndef upsample_convtranspose(in_channels=64, out_channels=3, kernel_size=2, stride=2, padding=0, bias=True, mode=\'2R\', negative_slope=0.2):\n    assert len(mode)<4 and mode[0] in [\'2\', \'3\', \'4\'], \'mode examples: 2, 2R, 2BR, 3, ..., 4BR.\'\n    kernel_size = int(mode[0])\n    stride = int(mode[0])\n    mode = mode.replace(mode[0], \'T\')\n    up1 = conv(in_channels, out_channels, kernel_size, stride, padding, bias, mode, negative_slope)\n    return up1\n\n\n\'\'\'\n# --------------------------------------------\n# Downsampler\n# Kai Zhang, https://github.com/cszn/KAIR\n# --------------------------------------------\n# downsample_strideconv\n# downsample_maxpool\n# downsample_avgpool\n# --------------------------------------------\n\'\'\'\n\n\n# --------------------------------------------\n# strideconv (+ relu)\n# --------------------------------------------\ndef downsample_strideconv(in_channels=64, out_channels=64, kernel_size=2, stride=2, padding=0, bias=True, mode=\'2R\', negative_slope=0.2):\n    assert len(mode)<4 and mode[0] in [\'2\', \'3\', \'4\'], \'mode examples: 2, 2R, 2BR, 3, ..., 4BR.\'\n    kernel_size = int(mode[0])\n    stride = int(mode[0])\n    mode = mode.replace(mode[0], \'C\')\n    down1 = conv(in_channels, out_channels, kernel_size, stride, padding, bias, mode, negative_slope)\n    return down1\n\n\n# --------------------------------------------\n# maxpooling + conv (+ relu)\n# --------------------------------------------\ndef downsample_maxpool(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0, bias=True, mode=\'2R\', negative_slope=0.2):\n    assert len(mode)<4 and mode[0] in [\'2\', \'3\'], \'mode examples: 2, 2R, 2BR, 3, ..., 3BR.\'\n    kernel_size_pool = int(mode[0])\n    stride_pool = int(mode[0])\n    mode = mode.replace(mode[0], \'MC\')\n    pool = conv(kernel_size=kernel_size_pool, stride=stride_pool, mode=mode[0], negative_slope=negative_slope)\n    pool_tail = conv(in_channels, out_channels, kernel_size, stride, padding, bias, mode=mode[1:], negative_slope=negative_slope)\n    return sequential(pool, pool_tail)\n\n\n# --------------------------------------------\n# averagepooling + conv (+ relu)\n# --------------------------------------------\ndef downsample_avgpool(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True, mode=\'2R\', negative_slope=0.2):\n    assert len(mode)<4 and mode[0] in [\'2\', \'3\'], \'mode examples: 2, 2R, 2BR, 3, ..., 3BR.\'\n    kernel_size_pool = int(mode[0])\n    stride_pool = int(mode[0])\n    mode = mode.replace(mode[0], \'AC\')\n    pool = conv(kernel_size=kernel_size_pool, stride=stride_pool, mode=mode[0], negative_slope=negative_slope)\n    pool_tail = conv(in_channels, out_channels, kernel_size, stride, padding, bias, mode=mode[1:], negative_slope=negative_slope)\n    return sequential(pool, pool_tail)\n\n\n\'\'\'\n# --------------------------------------------\n# NonLocalBlock2D:\n# embedded_gaussian\n# +W(softmax(thetaXphi)Xg)\n# --------------------------------------------\n\'\'\'\n\n\n# --------------------------------------------\n# non-local block with embedded_gaussian\n# https://github.com/AlexHex7/Non-local_pytorch\n# --------------------------------------------\nclass NonLocalBlock2D(nn.Module):\n    def __init__(self, nc=64, kernel_size=1, stride=1, padding=0, bias=True, act_mode=\'B\', downsample=False, downsample_mode=\'maxpool\', negative_slope=0.2):\n\n        super(NonLocalBlock2D, self).__init__()\n\n        inter_nc = nc // 2\n        self.inter_nc = inter_nc\n        self.W = conv(inter_nc, nc, kernel_size, stride, padding, bias, mode=\'C\'+act_mode)\n        self.theta = conv(nc, inter_nc, kernel_size, stride, padding, bias, mode=\'C\')\n\n        if downsample:\n            if downsample_mode == \'avgpool\':\n                downsample_block = downsample_avgpool\n            elif downsample_mode == \'maxpool\':\n                downsample_block = downsample_maxpool\n            elif downsample_mode == \'strideconv\':\n                downsample_block = downsample_strideconv\n            else:\n                raise NotImplementedError(\'downsample mode [{:s}] is not found\'.format(downsample_mode))\n            self.phi = downsample_block(nc, inter_nc, kernel_size, stride, padding, bias, mode=\'2\')\n            self.g = downsample_block(nc, inter_nc, kernel_size, stride, padding, bias, mode=\'2\')\n        else:\n            self.phi = conv(nc, inter_nc, kernel_size, stride, padding, bias, mode=\'C\')\n            self.g = conv(nc, inter_nc, kernel_size, stride, padding, bias, mode=\'C\')\n\n    def forward(self, x):\n        \'\'\'\n        :param x: (b, c, t, h, w)\n        :return:\n        \'\'\'\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_nc, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = self.theta(x).view(batch_size, self.inter_nc, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n        phi_x = self.phi(x).view(batch_size, self.inter_nc, -1)\n        f = torch.matmul(theta_x, phi_x)\n        f_div_C = F.softmax(f, dim=-1)\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_nc, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        return z\n'"
models/loss.py,8,"b'import torch\nimport torch.nn as nn\n\n\n# --------------------------------------------\n# GAN loss: gan, ragan\n# --------------------------------------------\nclass GANLoss(nn.Module):\n    def __init__(self, gan_type, real_label_val=1.0, fake_label_val=0.0):\n        super(GANLoss, self).__init__()\n        self.gan_type = gan_type.lower()\n        self.real_label_val = real_label_val\n        self.fake_label_val = fake_label_val\n\n        if self.gan_type == \'gan\' or self.gan_type == \'ragan\':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif self.gan_type == \'lsgan\':\n            self.loss = nn.MSELoss()\n        elif self.gan_type == \'wgan-gp\':\n            def wgan_loss(input, target):\n                # target is boolean\n                return -1 * input.mean() if target else input.mean()\n\n            self.loss = wgan_loss\n        else:\n            raise NotImplementedError(\'GAN type [{:s}] is not found\'.format(self.gan_type))\n\n    def get_target_label(self, input, target_is_real):\n        if self.gan_type == \'wgan-gp\':\n            return target_is_real\n        if target_is_real:\n            return torch.empty_like(input).fill_(self.real_label_val)\n        else:\n            return torch.empty_like(input).fill_(self.fake_label_val)\n\n    def forward(self, input, target_is_real):\n        target_label = self.get_target_label(input, target_is_real)\n        loss = self.loss(input, target_label)\n        return loss\n\n\n# --------------------------------------------\n# TV loss\n# --------------------------------------------\nclass TVLoss(nn.Module):\n    def __init__(self, tv_loss_weight=1):\n        """"""\n        Total variation loss\n        https://github.com/jxgu1016/Total_Variation_Loss.pytorch\n        Args:\n            tv_loss_weight (int):\n        """"""\n        super(TVLoss, self).__init__()\n        self.tv_loss_weight = tv_loss_weight\n\n    def forward(self, x):\n        batch_size = x.size()[0]\n        h_x = x.size()[2]\n        w_x = x.size()[3]\n        count_h = self.tensor_size(x[:, :, 1:, :])\n        count_w = self.tensor_size(x[:, :, :, 1:])\n        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n\n    @staticmethod\n    def tensor_size(t):\n        return t.size()[1] * t.size()[2] * t.size()[3]\n\n\n# --------------------------------------------\n# GradientPenaltyLoss\n# --------------------------------------------\nclass GradientPenaltyLoss(nn.Module):\n    def __init__(self, device=torch.device(\'cpu\')):\n        super(GradientPenaltyLoss, self).__init__()\n        self.register_buffer(\'grad_outputs\', torch.Tensor())\n        self.grad_outputs = self.grad_outputs.to(device)\n\n    def get_grad_outputs(self, input):\n        if self.grad_outputs.size() != input.size():\n            self.grad_outputs.resize_(input.size()).fill_(1.0)\n        return self.grad_outputs\n\n    def forward(self, interp, interp_crit):\n        grad_outputs = self.get_grad_outputs(interp_crit)\n        grad_interp = torch.autograd.grad(outputs=interp_crit, inputs=interp, \\\n            grad_outputs=grad_outputs, create_graph=True, retain_graph=True, only_inputs=True)[0]\n        grad_interp = grad_interp.view(grad_interp.size(0), -1)\n        grad_interp_norm = grad_interp.norm(2, dim=1)\n\n        loss = ((grad_interp_norm - 1)**2).mean()\n        return loss'"
models/loss_ssim.py,7,"b'import torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nfrom math import exp\n\n""""""\n# ============================================\n# SSIM loss\n# https://github.com/Po-Hsun-Su/pytorch-ssim\n# ============================================\n""""""\n\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n    return gauss/gauss.sum()\n\n\ndef create_window(window_size, channel):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n    return window\n\n\ndef _ssim(img1, img2, window, window_size, channel, size_average=True):\n    mu1 = F.conv2d(img1, window, padding=window_size//2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size//2, groups=channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1*mu2\n\n    sigma1_sq = F.conv2d(img1*img1, window, padding=window_size//2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2*img2, window, padding=window_size//2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1*img2, window, padding=window_size//2, groups=channel) - mu1_mu2\n\n    C1 = 0.01**2\n    C2 = 0.03**2\n\n    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n    if size_average:\n        return ssim_map.mean()\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)\n\n\nclass SSIMLoss(torch.nn.Module):\n    def __init__(self, window_size=11, size_average=True):\n        super(SSIMLoss, self).__init__()\n        self.window_size = window_size\n        self.size_average = size_average\n        self.channel = 1\n        self.window = create_window(window_size, self.channel)\n\n    def forward(self, img1, img2):\n        (_, channel, _, _) = img1.size()\n        if channel == self.channel and self.window.data.type() == img1.data.type():\n            window = self.window\n        else:\n            window = create_window(self.window_size, channel)\n\n            if img1.is_cuda:\n                window = window.cuda(img1.get_device())\n            window = window.type_as(img1)\n\n            self.window = window\n            self.channel = channel\n\n        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n\n\ndef ssim(img1, img2, window_size=11, size_average=True):\n    (_, channel, _, _) = img1.size()\n    window = create_window(window_size, channel)\n    \n    if img1.is_cuda:\n        window = window.cuda(img1.get_device())\n    window = window.type_as(img1)\n    \n    return _ssim(img1, img2, window, window_size, channel, size_average)\n\n\nif __name__ == \'__main__\':\n    import cv2\n    from torch import optim\n    from skimage import io\n    npImg1 = cv2.imread(""einstein.png"")\n\n    img1 = torch.from_numpy(np.rollaxis(npImg1, 2)).float().unsqueeze(0)/255.0\n    img2 = torch.rand(img1.size())\n\n    if torch.cuda.is_available():\n        img1 = img1.cuda()\n        img2 = img2.cuda()\n\n    img1 = Variable(img1, requires_grad=False)\n    img2 = Variable(img2, requires_grad=True)\n\n    ssim_value = ssim(img1, img2).item()\n    print(""Initial ssim:"", ssim_value)\n\n    ssim_loss = SSIMLoss()\n    optimizer = optim.Adam([img2], lr=0.01)\n\n    while ssim_value < 0.99:\n        optimizer.zero_grad()\n        ssim_out = -ssim_loss(img1, img2)\n        ssim_value = -ssim_out.item()\n        print(\'{:<4.4f}\'.format(ssim_value))\n        ssim_out.backward()\n        optimizer.step()\n    img = np.transpose(img2.detach().cpu().squeeze().float().numpy(), (1,2,0))\n    io.imshow(np.uint8(np.clip(img*255, 0, 255)))\n'"
models/model_base.py,4,"b'import os\nimport torch\nimport torch.nn as nn\nfrom utils.utils_bnorm import merge_bn, tidy_sequential\n\n\nclass ModelBase():\n    def __init__(self, opt):\n        self.opt = opt                         # opt\n        self.save_dir = opt[\'path\'][\'models\']  # save models\n        self.device = torch.device(\'cuda\' if opt[\'gpu_ids\'] is not None else \'cpu\')\n        self.is_train = opt[\'is_train\']        # training or not\n        self.schedulers = []                   # schedulers\n\n    """"""\n    # ----------------------------------------\n    # Preparation before training with data\n    # Save model during training\n    # ----------------------------------------\n    """"""\n\n    def init_train(self):\n        pass\n\n    def load(self):\n        pass\n\n    def save(self, label):\n        pass\n\n    def define_loss(self):\n        pass\n\n    def define_optimizer(self):\n        pass\n\n    def define_scheduler(self):\n        pass\n\n    """"""\n    # ----------------------------------------\n    # Optimization during training with data\n    # Testing/evaluation\n    # ----------------------------------------\n    """"""\n\n    def feed_data(self, data):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def current_visuals(self):\n        pass\n\n    def current_losses(self):\n        pass\n\n    def update_learning_rate(self, n):\n        for scheduler in self.schedulers:\n            scheduler.step(n)\n\n    def current_learning_rate(self):\n        return self.schedulers[0].get_lr()[0]\n\n\n    """"""\n    # ----------------------------------------\n    # Information of net\n    # ----------------------------------------\n    """"""\n\n    def print_network(self):\n        pass\n\n    def info_network(self):\n        pass\n\n    def print_params(self):\n        pass\n\n    def info_params(self):\n        pass\n\n    # ----------------------------------------\n    # network name and number of parameters\n    # ----------------------------------------\n    def describe_network(self, network):\n        if isinstance(network, nn.DataParallel):\n            network = network.module\n        msg = \'\\n\'\n        msg += \'Networks name: {}\'.format(network.__class__.__name__) + \'\\n\'\n        msg += \'Params number: {}\'.format(sum(map(lambda x: x.numel(), network.parameters()))) + \'\\n\'\n        msg += \'Net structure:\\n{}\'.format(str(network)) + \'\\n\'\n        return msg\n\n    # ----------------------------------------\n    # parameters description\n    # ----------------------------------------\n    def describe_params(self, network):\n        if isinstance(network, nn.DataParallel):\n            network = network.module\n        msg = \'\\n\'\n        msg += \' | {:^6s} | {:^6s} | {:^6s} | {:^6s} || {:<20s}\'.format(\'mean\', \'min\', \'max\', \'std\', \'param_name\') + \'\\n\'\n        for name, param in network.state_dict().items():\n            if not \'num_batches_tracked\' in name:\n                v = param.data.clone().float()\n                msg += \' | {:>6.3f} | {:>6.3f} | {:>6.3f} | {:>6.3f} || {:s}\'.format(v.mean(), v.min(), v.max(), v.std(), name) + \'\\n\'\n        return msg\n\n    """"""\n    # ----------------------------------------\n    # Save prameters\n    # Load prameters\n    # ----------------------------------------\n    """"""\n\n    # ----------------------------------------\n    # save the state_dict of the network\n    # ----------------------------------------\n    def save_network(self, save_dir, network, network_label, iter_label):\n        save_filename = \'{}_{}.pth\'.format(iter_label, network_label)\n        save_path = os.path.join(save_dir, save_filename)\n        if isinstance(network, nn.DataParallel):\n            network = network.module\n        state_dict = network.state_dict()\n        for key, param in state_dict.items():\n            state_dict[key] = param.cpu()\n        torch.save(state_dict, save_path)\n\n    # ----------------------------------------\n    # load the state_dict of the network\n    # ----------------------------------------\n    def load_network(self, load_path, network, strict=True):\n        if isinstance(network, nn.DataParallel):\n            network = network.module\n        network.load_state_dict(torch.load(load_path), strict=strict)\n\n    """"""\n    # ----------------------------------------\n    # Merge Batch Normalization for training\n    # Merge Batch Normalization for testing\n    # ----------------------------------------\n    """"""\n\n    # ----------------------------------------\n    # merge bn during training\n    # ----------------------------------------\n    def merge_bnorm_train(self):\n        merge_bn(self.netG)\n        tidy_sequential(self.netG)\n        self.define_optimizer()\n        self.define_scheduler()\n\n    # ----------------------------------------\n    # merge bn before testing\n    # ----------------------------------------\n    def merge_bnorm_test(self):\n        merge_bn(self.netG)\n        tidy_sequential(self.netG)'"
models/model_gan.py,11,"b'from collections import OrderedDict\nimport torch\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nfrom torch.optim import Adam\nfrom torch.nn.parallel import DataParallel  # , DistributedDataParallel\n\nfrom models.select_network import define_G, define_D, define_F\nfrom models.model_base import ModelBase\nfrom models.loss import GANLoss\nfrom models.loss_ssim import SSIMLoss\n\n\nclass ModelGAN(ModelBase):\n    """"""Train with pixel-VGG-GAN loss""""""\n    def __init__(self, opt):\n        super(ModelGAN, self).__init__(opt)\n        # ------------------------------------\n        # define network\n        # ------------------------------------\n        self.netG = define_G(opt).to(self.device)\n        self.netG = DataParallel(self.netG)\n        if self.is_train:\n            self.netF = define_F(opt).to(self.device)\n            self.netD = define_D(opt).to(self.device)\n            self.netF = DataParallel(self.netF)\n            self.netD = DataParallel(self.netD)\n\n    """"""\n    # ----------------------------------------\n    # Preparation before training with data\n    # Save model during training\n    # ----------------------------------------\n    """"""\n\n    # ----------------------------------------\n    # initialize training\n    # ----------------------------------------\n    def init_train(self):\n        self.opt_train = self.opt[\'train\']    # training option\n        self.load()                           # load model\n        self.netG.train()                     # set training mode,for BN\n        self.netD.train()                     # set training mode,for BN\n        self.define_loss()                    # define loss\n        self.define_optimizer()               # define optimizer\n        self.define_scheduler()               # define scheduler\n        self.log_dict = OrderedDict()         # log\n\n    # ----------------------------------------\n    # load pre-trained G and D model\n    # ----------------------------------------\n    def load(self):\n        load_path_G = self.opt[\'path\'][\'pretrained_netG\']\n        if load_path_G is not None:\n            print(\'Loading model for G [{:s}] ...\'.format(load_path_G))\n            self.load_network(load_path_G, self.netG)\n        load_path_D = self.opt[\'path\'][\'pretrained_netD\']\n        if self.opt[\'is_train\'] and load_path_D is not None:\n            print(\'Loading model for D [{:s}] ...\'.format(load_path_D))\n            self.load_network(load_path_D, self.netD)\n\n    # ----------------------------------------\n    # save model\n    # ----------------------------------------\n    def save(self, iter_label):\n        self.save_network(self.save_dir, self.netG, \'G\', iter_label)\n        self.save_network(self.save_dir, self.netD, \'D\', iter_label)\n\n    # ----------------------------------------\n    # define loss\n    # ----------------------------------------\n    def define_loss(self):\n        # ------------------------------------\n        # G_loss\n        # ------------------------------------\n        if self.opt_train[\'G_lossfn_weight\'] > 0:\n            G_lossfn_type = self.opt_train[\'G_lossfn_type\']\n            if G_lossfn_type == \'l1\':\n                self.G_lossfn = nn.L1Loss().to(self.device)\n            elif G_lossfn_type == \'l2\':\n                self.G_lossfn = nn.MSELoss().to(self.device)\n            elif G_lossfn_type == \'l2sum\':\n                self.G_lossfn = nn.MSELoss(reduction=\'sum\').to(self.device)\n            elif G_lossfn_type == \'ssim\':\n                self.G_lossfn = SSIMLoss().to(self.device)\n            else:\n                raise NotImplementedError(\'Loss type [{:s}] is not found.\'.format(G_lossfn_type))\n            self.G_lossfn_weight = self.opt_train[\'G_lossfn_weight\']\n        else:\n            print(\'Do not use pixel loss.\')\n            self.G_lossfn = None\n\n        # ------------------------------------\n        # F_loss\n        # ------------------------------------\n        if self.opt_train[\'F_lossfn_weight\'] > 0:\n            F_lossfn_type = self.opt_train[\'F_lossfn_type\']\n            if F_lossfn_type == \'l1\':\n                self.F_lossfn = nn.L1Loss().to(self.device)\n            elif F_lossfn_type == \'l2\':\n                self.F_lossfn = nn.MSELoss().to(self.device)\n            else:\n                raise NotImplementedError(\'Loss type [{:s}] not recognized.\'.format(F_lossfn_type))\n            self.F_lossfn_weight = self.opt_train[\'F_lossfn_weight\']\n            self.netF = define_F(self.opt, use_bn=False).to(self.device)\n        else:\n            print(\'Do not use feature loss.\')\n            self.F_lossfn = None\n\n        # ------------------------------------\n        # D_loss\n        # ------------------------------------\n        self.D_lossfn = GANLoss(self.opt_train[\'gan_type\'], 1.0, 0.0).to(self.device)\n        self.D_lossfn_weight = self.opt_train[\'D_lossfn_weight\']\n\n        self.D_update_ratio = self.opt_train[\'D_update_ratio\'] if self.opt_train[\'D_update_ratio\'] else 1\n        self.D_init_iters = self.opt_train[\'D_init_iters\'] if self.opt_train[\'D_init_iters\'] else 0\n\n    # ----------------------------------------\n    # define optimizer, G and D\n    # ----------------------------------------\n    def define_optimizer(self):\n        G_optim_params = []\n        for k, v in self.netG.named_parameters():\n            if v.requires_grad:\n                G_optim_params.append(v)\n            else:\n                print(\'Params [{:s}] will not optimize.\'.format(k))\n\n        self.G_optimizer = Adam(G_optim_params, lr=self.opt_train[\'G_optimizer_lr\'], weight_decay=0)\n        self.D_optimizer = Adam(self.netD.parameters(), lr=self.opt_train[\'D_optimizer_lr\'], weight_decay=0)\n\n    # ----------------------------------------\n    # define scheduler, only ""MultiStepLR""\n    # ----------------------------------------\n    def define_scheduler(self):\n        self.schedulers.append(lr_scheduler.MultiStepLR(self.G_optimizer,\n                                                        self.opt_train[\'G_scheduler_milestones\'],\n                                                        self.opt_train[\'G_scheduler_gamma\']\n                                                        ))\n        self.schedulers.append(lr_scheduler.MultiStepLR(self.D_optimizer,\n                                                        self.opt_train[\'D_scheduler_milestones\'],\n                                                        self.opt_train[\'D_scheduler_gamma\']\n                                                        ))\n\n    """"""\n    # ----------------------------------------\n    # Optimization during training with data\n    # Testing/evaluation\n    # ----------------------------------------\n    """"""\n\n    # ----------------------------------------\n    # feed L/H data\n    # ----------------------------------------\n    def feed_data(self, data, need_H=True):\n        self.L = data[\'L\'].to(self.device)\n        if need_H:\n            self.H = data[\'H\'].to(self.device)\n            input_ref = data[\'ref\'] if \'ref\' in data else data[\'H\']\n            self.var_ref = input_ref.to(self.device)\n\n    # ----------------------------------------\n    # update parameters and get loss\n    # ----------------------------------------\n    def optimize_parameters(self, current_step):\n        # ------------------------------------\n        # optimize G\n        # ------------------------------------\n        for p in self.netD.parameters():\n            p.requires_grad = False\n\n        self.G_optimizer.zero_grad()\n        self.E = self.netG(self.L)\n        loss_G_total = 0\n\n        if current_step % self.D_update_ratio == 0 and current_step > self.D_init_iters:  # updata D first\n            if self.opt_train[\'G_lossfn_weight\'] > 0:\n                G_loss = self.G_lossfn_weight * self.G_lossfn(self.E, self.H)\n                loss_G_total += G_loss                 # 1) pixel loss\n            if self.opt_train[\'F_lossfn_weight\'] > 0:\n                real_fea = self.netF(self.H).detach()\n                fake_fea = self.netF(self.E)\n                F_loss = self.F_lossfn_weight * self.F_lossfn(fake_fea, real_fea)\n                loss_G_total += F_loss                 # 2) VGG feature loss\n\n            pred_g_fake = self.netD(self.E)\n            if self.opt[\'train\'][\'gan_type\'] == \'gan\':\n                D_loss = self.D_lossfn_weight * self.D_lossfn(pred_g_fake, True)\n            elif self.opt[\'train\'][\'gan_type\'] == \'ragan\':\n                pred_d_real = self.netD(self.var_ref).detach()\n                D_loss = self.D_lossfn_weight * (\n                    self.D_lossfn(pred_d_real - torch.mean(pred_g_fake), False) +\n                    self.D_lossfn(pred_g_fake - torch.mean(pred_d_real), True)) / 2\n            loss_G_total += D_loss                     # 3) GAN loss\n\n            loss_G_total.backward()\n            self.G_optimizer.step()\n\n        # ------------------------------------\n        # optimize D\n        # ------------------------------------\n        for p in self.netD.parameters():\n            p.requires_grad = True\n\n        self.D_optimizer.zero_grad()\n        loss_D_total = 0\n\n        pred_d_real = self.netD(self.var_ref)          # 1) real data\n        pred_d_fake = self.netD(self.E.detach())       # 2) fake data, detach to avoid BP to G\n        if self.opt[\'train\'][\'gan_type\'] == \'gan\':\n            l_d_real = self.D_lossfn(pred_d_real, True)\n            l_d_fake = self.D_lossfn(pred_d_fake, False)\n            loss_D_total = l_d_real + l_d_fake\n        elif self.opt[\'train\'][\'gan_type\'] == \'ragan\':\n            l_d_real = self.D_lossfn(pred_d_real - torch.mean(pred_d_fake), True)\n            l_d_fake = self.D_lossfn(pred_d_fake - torch.mean(pred_d_real), False)\n            loss_D_total = (l_d_real + l_d_fake) / 2\n\n        loss_D_total.backward()\n        self.D_optimizer.step()\n\n        # ------------------------------------\n        # record log\n        # ------------------------------------\n        if current_step % self.D_update_ratio == 0 and current_step > self.D_init_iters:\n            if self.opt_train[\'G_lossfn_weight\'] > 0:\n                self.log_dict[\'G_loss\'] = G_loss.item()  # /self.E.size()[0]\n            if self.opt_train[\'F_lossfn_weight\'] > 0:\n                self.log_dict[\'F_loss\'] = F_loss.item()  # /self.E.size()[0]\n            self.log_dict[\'D_loss\'] = D_loss.item()  # /self.E.size()[0]\n\n        self.log_dict[\'l_d_real\'] = l_d_real.item()  # /self.E.size()[0]\n        self.log_dict[\'l_d_fake\'] = l_d_fake.item()  # /self.E.size()[0]\n        self.log_dict[\'D_real\'] = torch.mean(pred_d_real.detach())\n        self.log_dict[\'D_fake\'] = torch.mean(pred_d_fake.detach())\n\n    # ----------------------------------------\n    # test and inference\n    # ----------------------------------------\n    def test(self):\n        self.netG.eval()\n        with torch.no_grad():\n            self.E = self.netG(self.L)\n        self.netG.train()\n\n    # ----------------------------------------\n    # get log_dict\n    # ----------------------------------------\n    def current_log(self):\n        return self.log_dict\n\n    # ----------------------------------------\n    # get L, E, H images\n    # ----------------------------------------\n    def current_visuals(self, need_H=True):\n        out_dict = OrderedDict()\n        out_dict[\'L\'] = self.L.detach()[0].float().cpu()\n        out_dict[\'E\'] = self.E.detach()[0].float().cpu()\n        if need_H:\n            out_dict[\'H\'] = self.H.detach()[0].float().cpu()\n        return out_dict\n\n    """"""\n    # ----------------------------------------\n    # Information of netG, netD and netF\n    # ----------------------------------------\n    """"""\n\n    # ----------------------------------------\n    # print network\n    # ----------------------------------------\n    def print_network(self):\n        msg = self.describe_network(self.netG)\n        print(msg)\n        if self.is_train:\n            msg = self.describe_network(self.netD)\n            print(msg)\n            if self.opt_train[\'F_lossfn_weight\'] > 0:\n                msg = self.describe_network(self.netF)\n                print(msg)\n\n    # ----------------------------------------\n    # print params\n    # ----------------------------------------\n    def print_params(self):\n        msg = self.describe_params(self.netG)\n        print(msg)\n\n    # ----------------------------------------\n    # network information\n    # ----------------------------------------\n    def info_network(self):\n        msg = self.describe_network(self.netG)\n        if self.is_train:\n            msg += self.describe_network(self.netD)\n            if self.opt_train[\'F_lossfn_weight\'] > 0:\n                msg += self.describe_network(self.netF)\n        return msg\n\n    # ----------------------------------------\n    # params information\n    # ----------------------------------------\n    def info_params(self):\n        msg = self.describe_params(self.netG)\n        return msg\n\n'"
models/model_plain.py,7,"b'from collections import OrderedDict\nimport torch\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nfrom torch.optim import Adam\nfrom torch.nn.parallel import DataParallel  # , DistributedDataParallel\n\nfrom models.select_network import define_G\nfrom models.model_base import ModelBase\nfrom models.loss_ssim import SSIMLoss\n\nfrom utils.utils_model import test_mode\nfrom utils.utils_regularizers import regularizer_orth, regularizer_clip\n\n\nclass ModelPlain(ModelBase):\n    """"""Train with pixel loss""""""\n    def __init__(self, opt):\n        super(ModelPlain, self).__init__(opt)\n        # ------------------------------------\n        # define network\n        # ------------------------------------\n        self.netG = define_G(opt).to(self.device)\n        self.netG = DataParallel(self.netG)\n\n    """"""\n    # ----------------------------------------\n    # Preparation before training with data\n    # Save model during training\n    # ----------------------------------------\n    """"""\n\n    # ----------------------------------------\n    # initialize training\n    # ----------------------------------------\n    def init_train(self):\n        self.opt_train = self.opt[\'train\']    # training option\n        self.load()                           # load model\n        self.netG.train()                     # set training mode,for BN\n        self.define_loss()                    # define loss\n        self.define_optimizer()               # define optimizer\n        self.define_scheduler()               # define scheduler\n        self.log_dict = OrderedDict()         # log\n\n    # ----------------------------------------\n    # load pre-trained G model\n    # ----------------------------------------\n    def load(self):\n        load_path_G = self.opt[\'path\'][\'pretrained_netG\']\n        if load_path_G is not None:\n            print(\'Loading model for G [{:s}] ...\'.format(load_path_G))\n            self.load_network(load_path_G, self.netG)\n\n    # ----------------------------------------\n    # save model\n    # ----------------------------------------\n    def save(self, iter_label):\n        self.save_network(self.save_dir, self.netG, \'G\', iter_label)\n\n    # ----------------------------------------\n    # define loss\n    # ----------------------------------------\n    def define_loss(self):\n        G_lossfn_type = self.opt_train[\'G_lossfn_type\']\n        if G_lossfn_type == \'l1\':\n            self.G_lossfn = nn.L1Loss().to(self.device)\n        elif G_lossfn_type == \'l2\':\n            self.G_lossfn = nn.MSELoss().to(self.device)\n        elif G_lossfn_type == \'l2sum\':\n            self.G_lossfn = nn.MSELoss(reduction=\'sum\').to(self.device)\n        elif G_lossfn_type == \'ssim\':\n            self.G_lossfn = SSIMLoss().to(self.device)\n        else:\n            raise NotImplementedError(\'Loss type [{:s}] is not found.\'.format(G_lossfn_type))\n        self.G_lossfn_weight = self.opt_train[\'G_lossfn_weight\']\n\n    # ----------------------------------------\n    # define optimizer\n    # ----------------------------------------\n    def define_optimizer(self):\n        G_optim_params = []\n        for k, v in self.netG.named_parameters():\n            if v.requires_grad:\n                G_optim_params.append(v)\n            else:\n                print(\'Params [{:s}] will not optimize.\'.format(k))\n        self.G_optimizer = Adam(G_optim_params, lr=self.opt_train[\'G_optimizer_lr\'], weight_decay=0)\n\n    # ----------------------------------------\n    # define scheduler, only ""MultiStepLR""\n    # ----------------------------------------\n    def define_scheduler(self):\n        self.schedulers.append(lr_scheduler.MultiStepLR(self.G_optimizer,\n                                                        self.opt_train[\'G_scheduler_milestones\'],\n                                                        self.opt_train[\'G_scheduler_gamma\']\n                                                        ))\n    """"""\n    # ----------------------------------------\n    # Optimization during training with data\n    # Testing/evaluation\n    # ----------------------------------------\n    """"""\n\n    # ----------------------------------------\n    # feed L/H data\n    # ----------------------------------------\n    def feed_data(self, data, need_H=True):\n        self.L = data[\'L\'].to(self.device)\n        if need_H:\n            self.H = data[\'H\'].to(self.device)\n\n    # ----------------------------------------\n    # update parameters and get loss\n    # ----------------------------------------\n    def optimize_parameters(self, current_step):\n        self.G_optimizer.zero_grad()\n        self.E = self.netG(self.L)\n        G_loss = self.G_lossfn_weight * self.G_lossfn(self.E, self.H)\n        G_loss.backward()\n\n        # ------------------------------------\n        # clip_grad\n        # ------------------------------------\n        # `clip_grad_norm` helps prevent the exploding gradient problem.\n        G_optimizer_clipgrad = self.opt_train[\'G_optimizer_clipgrad\'] if self.opt_train[\'G_optimizer_clipgrad\'] else 0\n        if G_optimizer_clipgrad > 0:\n            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=self.opt_train[\'G_optimizer_clipgrad\'], norm_type=2)\n\n        self.G_optimizer.step()\n\n        # ------------------------------------\n        # regularizer\n        # ------------------------------------\n        G_regularizer_orthstep = self.opt_train[\'G_regularizer_orthstep\'] if self.opt_train[\'G_regularizer_orthstep\'] else 0\n        if G_regularizer_orthstep > 0 and current_step % G_regularizer_orthstep == 0 and current_step % self.opt[\'train\'][\'checkpoint_save\'] != 0:\n            self.netG.apply(regularizer_orth)\n        G_regularizer_clipstep = self.opt_train[\'G_regularizer_clipstep\'] if self.opt_train[\'G_regularizer_clipstep\'] else 0\n        if G_regularizer_clipstep > 0 and current_step % G_regularizer_clipstep == 0 and current_step % self.opt[\'train\'][\'checkpoint_save\'] != 0:\n            self.netG.apply(regularizer_clip)\n\n        # self.log_dict[\'G_loss\'] = G_loss.item()/self.E.size()[0]  # if `reduction=\'sum\'`\n        self.log_dict[\'G_loss\'] = G_loss.item()\n \n    # ----------------------------------------\n    # test / inference\n    # ----------------------------------------\n    def test(self):\n        self.netG.eval()\n        with torch.no_grad():\n            self.E = self.netG(self.L)\n        self.netG.train()\n\n    # ----------------------------------------\n    # test / inference x8\n    # ----------------------------------------\n    def testx8(self):\n        self.netG.eval()\n        with torch.no_grad():\n            self.E = test_mode(self.netG, self.L, mode=3, sf=self.opt[\'scale\'], modulo=1)\n        self.netG.train()\n\n    # ----------------------------------------\n    # get log_dict\n    # ----------------------------------------\n    def current_log(self):\n        return self.log_dict\n\n    # ----------------------------------------\n    # get L, E, H image\n    # ----------------------------------------\n    def current_visuals(self, need_H=True):\n        out_dict = OrderedDict()\n        out_dict[\'L\'] = self.L.detach()[0].float().cpu()\n        out_dict[\'E\'] = self.E.detach()[0].float().cpu()\n        if need_H:\n            out_dict[\'H\'] = self.H.detach()[0].float().cpu()\n        return out_dict\n\n    # ----------------------------------------\n    # get L, E, H batch images\n    # ----------------------------------------\n    def current_results(self, need_H=True):\n        out_dict = OrderedDict()\n        out_dict[\'L\'] = self.L.detach().float().cpu()\n        out_dict[\'E\'] = self.E.detach().float().cpu()\n        if need_H:\n            out_dict[\'H\'] = self.H.detach().float().cpu()\n        return out_dict\n\n    """"""\n    # ----------------------------------------\n    # Information of netG\n    # ----------------------------------------\n    """"""\n\n    # ----------------------------------------\n    # print network\n    # ----------------------------------------\n    def print_network(self):\n        msg = self.describe_network(self.netG)\n        print(msg)\n\n    # ----------------------------------------\n    # print params\n    # ----------------------------------------\n    def print_params(self):\n        msg = self.describe_params(self.netG)\n        print(msg)\n\n    # ----------------------------------------\n    # network information\n    # ----------------------------------------\n    def info_network(self):\n        msg = self.describe_network(self.netG)\n        return msg\n\n    # ----------------------------------------\n    # params information\n    # ----------------------------------------\n    def info_params(self):\n        msg = self.describe_params(self.netG)\n        return msg\n'"
models/model_plain2.py,6,"b'from collections import OrderedDict\nimport torch\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nfrom torch.optim import Adam\nfrom torch.nn.parallel import DataParallel  # , DistributedDataParallel\n\nfrom models.select_network import define_G\nfrom models.model_base import ModelBase\nfrom models.loss_ssim import SSIMLoss\n\nfrom utils.utils_model import test_mode\nfrom utils.utils_regularizers import regularizer_orth, regularizer_clip\n\n\nclass ModelPlain2(ModelBase):\n    """"""Train with pixel loss""""""\n    def __init__(self, opt):\n        super(ModelPlain2, self).__init__(opt)\n        # ------------------------------------\n        # define network\n        # ------------------------------------\n        self.netG = define_G(opt).to(self.device)\n        self.netG = DataParallel(self.netG)\n\n    """"""\n    # ----------------------------------------\n    # Preparation before training with data\n    # Save model during training\n    # ----------------------------------------\n    """"""\n\n    # ----------------------------------------\n    # initialize training\n    # ----------------------------------------\n    def init_train(self):\n        self.opt_train = self.opt[\'train\']    # training option\n        self.load()                           # load model\n        self.netG.train()                     # set training mode,for BN\n        self.define_loss()                    # define loss\n        self.define_optimizer()               # define optimizer\n        self.define_scheduler()               # define scheduler\n        self.log_dict = OrderedDict()         # log\n\n    # ----------------------------------------\n    # load pre-trained G model\n    # ----------------------------------------\n    def load(self):\n        load_path_G = self.opt[\'path\'][\'pretrained_netG\']\n        if load_path_G is not None:\n            print(\'Loading model for G [{:s}] ...\'.format(load_path_G))\n            self.load_network(load_path_G, self.netG)\n\n    # ----------------------------------------\n    # save model\n    # ----------------------------------------\n    def save(self, iter_label):\n        self.save_network(self.save_dir, self.netG, \'G\', iter_label)\n\n    # ----------------------------------------\n    # define loss\n    # ----------------------------------------\n    def define_loss(self):\n        G_lossfn_type = self.opt_train[\'G_lossfn_type\']\n        if G_lossfn_type == \'l1\':\n            self.G_lossfn = nn.L1Loss().to(self.device)\n        elif G_lossfn_type == \'l2\':\n            self.G_lossfn = nn.MSELoss().to(self.device)\n        elif G_lossfn_type == \'l2sum\':\n            self.G_lossfn = nn.MSELoss(reduction=\'sum\').to(self.device)\n        elif G_lossfn_type == \'ssim\':\n            self.G_lossfn = SSIMLoss().to(self.device)\n        else:\n            raise NotImplementedError(\'Loss type [{:s}] is not found.\'.format(G_lossfn_type))\n        self.G_lossfn_weight = self.opt_train[\'G_lossfn_weight\']\n\n    # ----------------------------------------\n    # define optimizer\n    # ----------------------------------------\n    def define_optimizer(self):\n        G_optim_params = []\n        for k, v in self.netG.named_parameters():\n            if v.requires_grad:\n                G_optim_params.append(v)\n            else:\n                print(\'Params [{:s}] will not optimize.\'.format(k))\n        self.G_optimizer = Adam(G_optim_params, lr=self.opt_train[\'G_optimizer_lr\'], weight_decay=0)\n\n    # ----------------------------------------\n    # define scheduler, only ""MultiStepLR""\n    # ----------------------------------------\n    def define_scheduler(self):\n        self.schedulers.append(lr_scheduler.MultiStepLR(self.G_optimizer,\n                                                        self.opt_train[\'G_scheduler_milestones\'],\n                                                        self.opt_train[\'G_scheduler_gamma\']\n                                                        ))\n    """"""\n    # ----------------------------------------\n    # Optimization during training with data\n    # Testing/evaluation\n    # ----------------------------------------\n    """"""\n\n    # ----------------------------------------\n    # feed L/H data\n    # ----------------------------------------\n    def feed_data(self, data, need_H=True):\n        self.L = data[\'L\'].to(self.device)\n        self.C = data[\'C\'].to(self.device)\n        if need_H:\n            self.H = data[\'H\'].to(self.device)\n\n    # ----------------------------------------\n    # update parameters and get loss\n    # ----------------------------------------\n    def optimize_parameters(self, current_step):\n        self.G_optimizer.zero_grad()\n        self.E = self.netG(self.L, self.C)\n        G_loss = self.G_lossfn_weight * self.G_lossfn(self.E, self.H)\n        G_loss.backward()\n\n        # ------------------------------------\n        # clip_grad\n        # ------------------------------------\n        # `clip_grad_norm` helps prevent the exploding gradient problem.\n        G_optimizer_clipgrad = self.opt_train[\'G_optimizer_clipgrad\'] if self.opt_train[\'G_optimizer_clipgrad\'] else 0\n        if G_optimizer_clipgrad > 0:\n            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=self.opt_train[\'G_optimizer_clipgrad\'], norm_type=2)\n\n        self.G_optimizer.step()\n\n        # ------------------------------------\n        # regularizer\n        # ------------------------------------\n        G_regularizer_orthstep = self.opt_train[\'G_regularizer_orthstep\'] if self.opt_train[\'G_regularizer_orthstep\'] else 0\n        if G_regularizer_orthstep > 0 and current_step % G_regularizer_orthstep == 0 and current_step % self.opt[\'train\'][\'checkpoint_save\'] != 0:\n            self.netG.apply(regularizer_orth)\n        G_regularizer_clipstep = self.opt_train[\'G_regularizer_clipstep\'] if self.opt_train[\'G_regularizer_clipstep\'] else 0\n        if G_regularizer_clipstep > 0 and current_step % G_regularizer_clipstep == 0 and current_step % self.opt[\'train\'][\'checkpoint_save\'] != 0:\n            self.netG.apply(regularizer_clip)\n\n        self.log_dict[\'G_loss\'] = G_loss.item()  # /self.E.size()[0]\n\n    # ----------------------------------------\n    # test / inference\n    # ----------------------------------------\n    def test(self):\n        self.netG.eval()\n        with torch.no_grad():\n            self.E = self.netG(self.L, self.C)\n        self.netG.train()\n\n    # ----------------------------------------\n    # get log_dict\n    # ----------------------------------------\n    def current_log(self):\n        return self.log_dict\n\n    # ----------------------------------------\n    # get L, E, H image\n    # ----------------------------------------\n    def current_visuals(self, need_H=True):\n        out_dict = OrderedDict()\n        out_dict[\'L\'] = self.L.detach()[0].float().cpu()\n        out_dict[\'E\'] = self.E.detach()[0].float().cpu()\n        if need_H:\n            out_dict[\'H\'] = self.H.detach()[0].float().cpu()\n        return out_dict\n\n    # ----------------------------------------\n    # get L, E, H batch images\n    # ----------------------------------------\n    def current_results(self, need_H=True):\n        out_dict = OrderedDict()\n        out_dict[\'L\'] = self.L.detach().float().cpu()\n        out_dict[\'E\'] = self.E.detach().float().cpu()\n        if need_H:\n            out_dict[\'H\'] = self.H.detach().float().cpu()\n        return out_dict\n\n    """"""\n    # ----------------------------------------\n    # Information of netG\n    # ----------------------------------------\n    """"""\n\n    # ----------------------------------------\n    # print network\n    # ----------------------------------------\n    def print_network(self):\n        msg = self.describe_network(self.netG)\n        print(msg)\n\n    # ----------------------------------------\n    # print params\n    # ----------------------------------------\n    def print_params(self):\n        msg = self.describe_params(self.netG)\n        print(msg)\n\n    # ----------------------------------------\n    # network information\n    # ----------------------------------------\n    def info_network(self):\n        msg = self.describe_network(self.netG)\n        return msg\n\n    # ----------------------------------------\n    # params information\n    # ----------------------------------------\n    def info_params(self):\n        msg = self.describe_params(self.netG)\n        return msg\n'"
models/network_discriminator.py,10,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.utils import spectral_norm\nimport models.basicblock as B\n\n\n""""""\n# --------------------------------------------\n# Discriminator_VGG_96\n# Discriminator_VGG_128\n# Discriminator_VGG_192\n# Discriminator_VGG_128_SN\n# --------------------------------------------\n""""""\n\n\n# --------------------------------------------\n# VGG style Discriminator with 96x96 input\n# --------------------------------------------\nclass Discriminator_VGG_96(nn.Module):\n    def __init__(self, in_nc=3, base_nc=64, ac_type=\'BL\'):\n        super(Discriminator_VGG_96, self).__init__()\n        # features\n        # hxw, c\n        # 96, 64\n        conv0 = B.conv(in_nc, base_nc, kernel_size=3, mode=\'C\')\n        conv1 = B.conv(base_nc, base_nc, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 48, 64\n        conv2 = B.conv(base_nc, base_nc*2, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv3 = B.conv(base_nc*2, base_nc*2, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 24, 128\n        conv4 = B.conv(base_nc*2, base_nc*4, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv5 = B.conv(base_nc*4, base_nc*4, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 12, 256\n        conv6 = B.conv(base_nc*4, base_nc*8, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv7 = B.conv(base_nc*8, base_nc*8, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 6, 512\n        conv8 = B.conv(base_nc*8, base_nc*8, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv9 = B.conv(base_nc*8, base_nc*8, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 3, 512\n        self.features = B.sequential(conv0, conv1, conv2, conv3, conv4,\n                                     conv5, conv6, conv7, conv8, conv9)\n\n        # classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 3 * 3, 100), nn.LeakyReLU(0.2, True), nn.Linear(100, 1))\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n\n# --------------------------------------------\n# VGG style Discriminator with 128x128 input\n# --------------------------------------------\nclass Discriminator_VGG_128(nn.Module):\n    def __init__(self, in_nc=3, base_nc=64, ac_type=\'BL\'):\n        super(Discriminator_VGG_128, self).__init__()\n        # features\n        # hxw, c\n        # 128, 64\n        conv0 = B.conv(in_nc, base_nc, kernel_size=3, mode=\'C\')\n        conv1 = B.conv(base_nc, base_nc, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 64, 64\n        conv2 = B.conv(base_nc, base_nc*2, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv3 = B.conv(base_nc*2, base_nc*2, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 32, 128\n        conv4 = B.conv(base_nc*2, base_nc*4, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv5 = B.conv(base_nc*4, base_nc*4, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 16, 256\n        conv6 = B.conv(base_nc*4, base_nc*8, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv7 = B.conv(base_nc*8, base_nc*8, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 8, 512\n        conv8 = B.conv(base_nc*8, base_nc*8, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv9 = B.conv(base_nc*8, base_nc*8, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 4, 512\n        self.features = B.sequential(conv0, conv1, conv2, conv3, conv4,\n                                     conv5, conv6, conv7, conv8, conv9)\n\n        # classifier\n        self.classifier = nn.Sequential(nn.Linear(512 * 4 * 4, 100), \n                                        nn.LeakyReLU(0.2, True), \n                                        nn.Linear(100, 1))\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n\n# --------------------------------------------\n# VGG style Discriminator with 192x192 input\n# --------------------------------------------\nclass Discriminator_VGG_192(nn.Module):\n    def __init__(self, in_nc=3, base_nc=64, ac_type=\'BL\'):\n        super(Discriminator_VGG_192, self).__init__()\n        # features\n        # hxw, c\n        # 192, 64\n        conv0 = B.conv(in_nc, base_nc, kernel_size=3, mode=\'C\')\n        conv1 = B.conv(base_nc, base_nc, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 96, 64\n        conv2 = B.conv(base_nc, base_nc*2, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv3 = B.conv(base_nc*2, base_nc*2, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 48, 128\n        conv4 = B.conv(base_nc*2, base_nc*4, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv5 = B.conv(base_nc*4, base_nc*4, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 24, 256\n        conv6 = B.conv(base_nc*4, base_nc*8, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv7 = B.conv(base_nc*8, base_nc*8, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 12, 512\n        conv8 = B.conv(base_nc*8, base_nc*8, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv9 = B.conv(base_nc*8, base_nc*8, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 6, 512\n        conv10 = B.conv(base_nc*8, base_nc*8, kernel_size=3, stride=1, mode=\'C\'+ac_type)\n        conv11 = B.conv(base_nc*8, base_nc*8, kernel_size=4, stride=2, mode=\'C\'+ac_type)\n        # 3, 512\n        self.features = B.sequential(conv0, conv1, conv2, conv3, conv4, conv5,\n                                     conv6, conv7, conv8, conv9, conv10, conv11)\n\n        # classifier\n        self.classifier = nn.Sequential(nn.Linear(512 * 3 * 3, 100),\n                                        nn.LeakyReLU(0.2, True),\n                                        nn.Linear(100, 1))\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n\n# --------------------------------------------\n# SN-VGG style Discriminator with 128x128 input\n# --------------------------------------------\nclass Discriminator_VGG_128_SN(nn.Module):\n    def __init__(self):\n        super(Discriminator_VGG_128_SN, self).__init__()\n        # features\n        # hxw, c\n        # 128, 64\n        self.lrelu = nn.LeakyReLU(0.2, True)\n\n        self.conv0 = spectral_norm(nn.Conv2d(3, 64, 3, 1, 1))\n        self.conv1 = spectral_norm(nn.Conv2d(64, 64, 4, 2, 1))\n        # 64, 64\n        self.conv2 = spectral_norm(nn.Conv2d(64, 128, 3, 1, 1))\n        self.conv3 = spectral_norm(nn.Conv2d(128, 128, 4, 2, 1))\n        # 32, 128\n        self.conv4 = spectral_norm(nn.Conv2d(128, 256, 3, 1, 1))\n        self.conv5 = spectral_norm(nn.Conv2d(256, 256, 4, 2, 1))\n        # 16, 256\n        self.conv6 = spectral_norm(nn.Conv2d(256, 512, 3, 1, 1))\n        self.conv7 = spectral_norm(nn.Conv2d(512, 512, 4, 2, 1))\n        # 8, 512\n        self.conv8 = spectral_norm(nn.Conv2d(512, 512, 3, 1, 1))\n        self.conv9 = spectral_norm(nn.Conv2d(512, 512, 4, 2, 1))\n        # 4, 512\n\n        # classifier\n        self.linear0 = spectral_norm(nn.Linear(512 * 4 * 4, 100))\n        self.linear1 = spectral_norm(nn.Linear(100, 1))\n\n    def forward(self, x):\n        x = self.lrelu(self.conv0(x))\n        x = self.lrelu(self.conv1(x))\n        x = self.lrelu(self.conv2(x))\n        x = self.lrelu(self.conv3(x))\n        x = self.lrelu(self.conv4(x))\n        x = self.lrelu(self.conv5(x))\n        x = self.lrelu(self.conv6(x))\n        x = self.lrelu(self.conv7(x))\n        x = self.lrelu(self.conv8(x))\n        x = self.lrelu(self.conv9(x))\n        x = x.view(x.size(0), -1)\n        x = self.lrelu(self.linear0(x))\n        x = self.linear1(x)\n        return x\n\n\nif __name__ == \'__main__\':\n\n    x = torch.rand(1, 3, 96, 96)\n    net = Discriminator_VGG_96()\n    net.eval()\n    with torch.no_grad():\n        y = net(x)\n    print(y.size())\n\n    x = torch.rand(1, 3, 128, 128)\n    net = Discriminator_VGG_128()\n    net.eval()\n    with torch.no_grad():\n        y = net(x)\n    print(y.size())\n\n    x = torch.rand(1, 3, 192, 192)\n    net = Discriminator_VGG_192()\n    net.eval()\n    with torch.no_grad():\n        y = net(x)\n    print(y.size())\n\n    x = torch.rand(1, 3, 128, 128)\n    net = Discriminator_VGG_128_SN()\n    net.eval()\n    with torch.no_grad():\n        y = net(x)\n    print(y.size())\n\n    # run models/network_discriminator.py\n'"
models/network_dncnn.py,3,"b'\nimport torch.nn as nn\nimport models.basicblock as B\n\n\n""""""\n# --------------------------------------------\n# DnCNN (20 conv layers)\n# FDnCNN (20 conv layers)\n# IRCNN (7 conv layers)\n# --------------------------------------------\n# References:\n@article{zhang2017beyond,\n  title={Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising},\n  author={Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},\n  journal={IEEE Transactions on Image Processing},\n  volume={26},\n  number={7},\n  pages={3142--3155},\n  year={2017},\n  publisher={IEEE}\n}\n@article{zhang2018ffdnet,\n  title={FFDNet: Toward a fast and flexible solution for CNN-based image denoising},\n  author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n  journal={IEEE Transactions on Image Processing},\n  volume={27},\n  number={9},\n  pages={4608--4622},\n  year={2018},\n  publisher={IEEE}\n}\n# --------------------------------------------\n""""""\n\n\n# --------------------------------------------\n# DnCNN\n# --------------------------------------------\nclass DnCNN(nn.Module):\n    def __init__(self, in_nc=1, out_nc=1, nc=64, nb=17, act_mode=\'BR\'):\n        """"""\n        # ------------------------------------\n        in_nc: channel number of input\n        out_nc: channel number of output\n        nc: channel number\n        nb: total number of conv layers\n        act_mode: batch norm + activation function; \'BR\' means BN+ReLU.\n        # ------------------------------------\n        Batch normalization and residual learning are\n        beneficial to Gaussian denoising (especially\n        for a single noise level).\n        The residual of a noisy image corrupted by additive white\n        Gaussian noise (AWGN) follows a constant\n        Gaussian distribution which stablizes batch\n        normalization during training.\n        # ------------------------------------\n        """"""\n        super(DnCNN, self).__init__()\n        assert \'R\' in act_mode or \'L\' in act_mode, \'Examples of activation function: R, L, BR, BL, IR, IL\'\n        bias = True\n\n        m_head = B.conv(in_nc, nc, mode=\'C\'+act_mode[-1], bias=bias)\n        m_body = [B.conv(nc, nc, mode=\'C\'+act_mode, bias=bias) for _ in range(nb-2)]\n        m_tail = B.conv(nc, out_nc, mode=\'C\', bias=bias)\n\n        self.model = B.sequential(m_head, *m_body, m_tail)\n\n    def forward(self, x):\n        n = self.model(x)\n        return x-n\n\n\n# --------------------------------------------\n# IRCNN denoiser\n# --------------------------------------------\nclass IRCNN(nn.Module):\n    def __init__(self, in_nc=1, out_nc=1, nc=64):\n        """"""\n        # ------------------------------------\n        denoiser of IRCNN\n        in_nc: channel number of input\n        out_nc: channel number of output\n        nc: channel number\n        nb: total number of conv layers\n        act_mode: batch norm + activation function; \'BR\' means BN+ReLU.\n        # ------------------------------------\n        Batch normalization and residual learning are\n        beneficial to Gaussian denoising (especially\n        for a single noise level).\n        The residual of a noisy image corrupted by additive white\n        Gaussian noise (AWGN) follows a constant\n        Gaussian distribution which stablizes batch\n        normalization during training.\n        # ------------------------------------\n        """"""\n        super(IRCNN, self).__init__()\n        L =[]\n        L.append(nn.Conv2d(in_channels=in_nc, out_channels=nc, kernel_size=3, stride=1, padding=1, dilation=1, bias=True))\n        L.append(nn.ReLU(inplace=True))\n        L.append(nn.Conv2d(in_channels=nc, out_channels=nc, kernel_size=3, stride=1, padding=2, dilation=2, bias=True))\n        L.append(nn.ReLU(inplace=True))\n        L.append(nn.Conv2d(in_channels=nc, out_channels=nc, kernel_size=3, stride=1, padding=3, dilation=3, bias=True))\n        L.append(nn.ReLU(inplace=True))\n        L.append(nn.Conv2d(in_channels=nc, out_channels=nc, kernel_size=3, stride=1, padding=4, dilation=4, bias=True))\n        L.append(nn.ReLU(inplace=True))\n        L.append(nn.Conv2d(in_channels=nc, out_channels=nc, kernel_size=3, stride=1, padding=3, dilation=3, bias=True))\n        L.append(nn.ReLU(inplace=True))\n        L.append(nn.Conv2d(in_channels=nc, out_channels=nc, kernel_size=3, stride=1, padding=2, dilation=2, bias=True))\n        L.append(nn.ReLU(inplace=True))\n        L.append(nn.Conv2d(in_channels=nc, out_channels=out_nc, kernel_size=3, stride=1, padding=1, dilation=1, bias=True))\n        self.model = B.sequential(*L)\n\n    def forward(self, x):\n        n = self.model(x)\n        return x-n\n\n\n# --------------------------------------------\n# FDnCNN\n# --------------------------------------------\n# Compared with DnCNN, FDnCNN has three modifications:\n# 1) add noise level map as input\n# 2) remove residual learning and BN\n# 3) train with L1 loss\n# may need more training time, but will not reduce the final PSNR too much.\n# --------------------------------------------\nclass FDnCNN(nn.Module):\n    def __init__(self, in_nc=2, out_nc=1, nc=64, nb=20, act_mode=\'R\'):\n        """"""\n        in_nc: channel number of input\n        out_nc: channel number of output\n        nc: channel number\n        nb: total number of conv layers\n        act_mode: batch norm + activation function; \'BR\' means BN+ReLU.\n        """"""\n        super(FDnCNN, self).__init__()\n        assert \'R\' in act_mode or \'L\' in act_mode, \'Examples of activation function: R, L, BR, BL, IR, IL\'\n        bias = True\n\n        m_head = B.conv(in_nc, nc, mode=\'C\'+act_mode[-1], bias=bias)\n        m_body = [B.conv(nc, nc, mode=\'C\'+act_mode, bias=bias) for _ in range(nb-2)]\n        m_tail = B.conv(nc, out_nc, mode=\'C\', bias=bias)\n\n        self.model = B.sequential(m_head, *m_body, m_tail)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\nif __name__ == \'__main__\':\n    from utils import utils_model\n    import torch\n    model1 = DnCNN(in_nc=1, out_nc=1, nc=64, nb=20, act_mode=\'BR\')\n    print(utils_model.describe_model(model1))\n\n    model2 = FDnCNN(in_nc=2, out_nc=1, nc=64, nb=20, act_mode=\'R\')\n    print(utils_model.describe_model(model2))\n\n    x = torch.randn((1, 1, 240, 240))\n    x1 = model1(x)\n    print(x1.shape)\n\n    x = torch.randn((1, 2, 240, 240))\n    x2 = model2(x)\n    print(x2.shape)\n\n    #  run models/network_dncnn.py\n'"
models/network_dpsr.py,1,"b'import math\nimport torch.nn as nn\nimport models.basicblock as B\n\n\n""""""\n# --------------------------------------------\n# modified SRResNet\n#   -- MSRResNet_prior (for DPSR)\n# --------------------------------------------\nReferences:\n@inproceedings{zhang2019deep,\n  title={Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels},\n  author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={1671--1681},\n  year={2019}\n}\n@inproceedings{wang2018esrgan,\n  title={Esrgan: Enhanced super-resolution generative adversarial networks},\n  author={Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Change Loy, Chen},\n  booktitle={European Conference on Computer Vision (ECCV)},\n  pages={0--0},\n  year={2018}\n}\n@inproceedings{ledig2017photo,\n  title={Photo-realistic single image super-resolution using a generative adversarial network},\n  author={Ledig, Christian and Theis, Lucas and Husz{\\\'a}r, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and others},\n  booktitle={IEEE conference on computer vision and pattern recognition},\n  pages={4681--4690},\n  year={2017}\n}\n# --------------------------------------------\n""""""\n\n\n# --------------------------------------------\n# MSRResNet super-resolver prior for DPSR\n# https://github.com/cszn/DPSR\n# https://github.com/cszn/DPSR/blob/master/models/network_srresnet.py\n# --------------------------------------------\nclass MSRResNet_prior(nn.Module):\n    def __init__(self, in_nc=4, out_nc=3, nc=96, nb=16, upscale=4, act_mode=\'R\', upsample_mode=\'upconv\'):\n        super(MSRResNet_prior, self).__init__()\n        n_upscale = int(math.log(upscale, 2))\n        if upscale == 3:\n            n_upscale = 1\n\n        m_head = B.conv(in_nc, nc, mode=\'C\')\n\n        m_body = [B.ResBlock(nc, nc, mode=\'C\'+act_mode+\'C\') for _ in range(nb)]\n        m_body.append(B.conv(nc, nc, mode=\'C\'))\n\n        if upsample_mode == \'upconv\':\n            upsample_block = B.upsample_upconv\n        elif upsample_mode == \'pixelshuffle\':\n            upsample_block = B.upsample_pixelshuffle\n        elif upsample_mode == \'convtranspose\':\n            upsample_block = B.upsample_convtranspose\n        else:\n            raise NotImplementedError(\'upsample mode [{:s}] is not found\'.format(upsample_mode))\n        if upscale == 3:\n            m_uper = upsample_block(nc, nc, mode=\'3\'+act_mode)\n        else:\n            m_uper = [upsample_block(nc, nc, mode=\'2\'+act_mode) for _ in range(n_upscale)]\n\n        H_conv0 = B.conv(nc, nc, mode=\'C\'+act_mode)\n        H_conv1 = B.conv(nc, out_nc, bias=False, mode=\'C\')\n        m_tail = B.sequential(H_conv0, H_conv1)\n\n        self.model = B.sequential(m_head, B.ShortcutBlock(B.sequential(*m_body)), *m_uper, m_tail)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\n\nclass SRResNet(nn.Module):\n    def __init__(self, in_nc=3, out_nc=3, nc=64, nb=16, upscale=4, act_mode=\'R\', upsample_mode=\'upconv\'):\n        super(SRResNet, self).__init__()\n        n_upscale = int(math.log(upscale, 2))\n        if upscale == 3:\n            n_upscale = 1\n\n        m_head = B.conv(in_nc, nc, mode=\'C\')\n\n        m_body = [B.ResBlock(nc, nc, mode=\'C\'+act_mode+\'C\') for _ in range(nb)]\n        m_body.append(B.conv(nc, nc, mode=\'C\'))\n\n        if upsample_mode == \'upconv\':\n            upsample_block = B.upsample_upconv\n        elif upsample_mode == \'pixelshuffle\':\n            upsample_block = B.upsample_pixelshuffle\n        elif upsample_mode == \'convtranspose\':\n            upsample_block = B.upsample_convtranspose\n        else:\n            raise NotImplementedError(\'upsample mode [{:s}] is not found\'.format(upsample_mode))\n        if upscale == 3:\n            m_uper = upsample_block(nc, nc, mode=\'3\'+act_mode)\n        else:\n            m_uper = [upsample_block(nc, nc, mode=\'2\'+act_mode) for _ in range(n_upscale)]\n\n        H_conv0 = B.conv(nc, nc, mode=\'C\'+act_mode)\n        H_conv1 = B.conv(nc, out_nc, bias=False, mode=\'C\')\n        m_tail = B.sequential(H_conv0, H_conv1)\n\n        self.model = B.sequential(m_head, B.ShortcutBlock(B.sequential(*m_body)), *m_uper, m_tail)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x'"
models/network_feature.py,4,"b'import torch\nimport torch.nn as nn\nimport torchvision\n\n\n""""""\n# --------------------------------------------\n# VGG Feature Extractor\n# --------------------------------------------\n""""""\n\n# --------------------------------------------\n# VGG features\n# Assume input range is [0, 1]\n# --------------------------------------------\nclass VGGFeatureExtractor(nn.Module):\n    def __init__(self,\n                 feature_layer=34,\n                 use_bn=False,\n                 use_input_norm=True,\n                 device=torch.device(\'cpu\')):\n        super(VGGFeatureExtractor, self).__init__()\n        if use_bn:\n            model = torchvision.models.vgg19_bn(pretrained=True)\n        else:\n            model = torchvision.models.vgg19(pretrained=True)\n        self.use_input_norm = use_input_norm\n        if self.use_input_norm:\n            mean = torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n            # [0.485-1, 0.456-1, 0.406-1] if input in range [-1,1]\n            std = torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n            # [0.229*2, 0.224*2, 0.225*2] if input in range [-1,1]\n            self.register_buffer(\'mean\', mean)\n            self.register_buffer(\'std\', std)\n        self.features = nn.Sequential(*list(model.features.children())[:(feature_layer + 1)])\n        # No need to BP to variable\n        for k, v in self.features.named_parameters():\n            v.requires_grad = False\n\n    def forward(self, x):\n        if self.use_input_norm:\n            x = (x - self.mean) / self.std\n        output = self.features(x)\n        return output\n\n\n'"
models/network_ffdnet.py,6,"b'import numpy as np\nimport torch.nn as nn\nimport models.basicblock as B\nimport torch\n\n""""""\n# --------------------------------------------\n# FFDNet (15 or 12 conv layers)\n# --------------------------------------------\nReference:\n@article{zhang2018ffdnet,\n  title={FFDNet: Toward a fast and flexible solution for CNN-based image denoising},\n  author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n  journal={IEEE Transactions on Image Processing},\n  volume={27},\n  number={9},\n  pages={4608--4622},\n  year={2018},\n  publisher={IEEE}\n}\n""""""\n\n\n# --------------------------------------------\n# FFDNet\n# --------------------------------------------\nclass FFDNet(nn.Module):\n    def __init__(self, in_nc=1, out_nc=1, nc=64, nb=15, act_mode=\'R\'):\n        """"""\n        # ------------------------------------\n        in_nc: channel number of input\n        out_nc: channel number of output\n        nc: channel number\n        nb: total number of conv layers\n        act_mode: batch norm + activation function; \'BR\' means BN+ReLU.\n        # ------------------------------------\n        # ------------------------------------\n        """"""\n        super(FFDNet, self).__init__()\n        assert \'R\' in act_mode or \'L\' in act_mode, \'Examples of activation function: R, L, BR, BL, IR, IL\'\n        bias = True\n        sf = 2\n\n        self.m_down = B.PixelUnShuffle(upscale_factor=sf)\n\n        m_head = B.conv(in_nc*sf*sf+1, nc, mode=\'C\'+act_mode[-1], bias=bias)\n        m_body = [B.conv(nc, nc, mode=\'C\'+act_mode, bias=bias) for _ in range(nb-2)]\n        m_tail = B.conv(nc, out_nc*sf*sf, mode=\'C\', bias=bias)\n\n        self.model = B.sequential(m_head, *m_body, m_tail)\n\n        self.m_up = nn.PixelShuffle(upscale_factor=sf)\n\n    def forward(self, x, sigma):\n\n        h, w = x.size()[-2:]\n        paddingBottom = int(np.ceil(h/2)*2-h)\n        paddingRight = int(np.ceil(w/2)*2-w)\n        x = torch.nn.ReplicationPad2d((0, paddingRight, 0, paddingBottom))(x)\n\n        x = self.m_down(x)\n        # m = torch.ones(sigma.size()[0], sigma.size()[1], x.size()[-2], x.size()[-1]).type_as(x).mul(sigma)\n        m = sigma.repeat(1, 1, x.size()[-2], x.size()[-1])\n        x = torch.cat((x, m), 1)\n        x = self.model(x)\n        x = self.m_up(x)\n        \n        x = x[..., :h, :w]\n        return x\n\n\nif __name__ == \'__main__\':\n    from utils import utils_model\n    model = FFDNet(in_nc=1, out_nc=1, nc=64, nb=15, act_mode=\'R\')\n    print(utils_model.describe_model(model))\n\n    x = torch.randn((2,1,240,240))\n    sigma = torch.randn(2,1,1,1)\n    x = model(x, sigma)\n    print(x.shape)\n\n    #  run models/network_ffdnet.py\n\n\n'"
models/network_imdn.py,1,"b'import math\nimport torch.nn as nn\nimport models.basicblock as B\n\n\n""""""\n# --------------------------------------------\n# simplified information multi-distillation\n# network (IMDN) for SR\n# --------------------------------------------\nReferences:\n@inproceedings{hui2019lightweight,\n  title={Lightweight Image Super-Resolution with Information Multi-distillation Network},\n  author={Hui, Zheng and Gao, Xinbo and Yang, Yunchu and Wang, Xiumei},\n  booktitle={Proceedings of the 27th ACM International Conference on Multimedia (ACM MM)},\n  pages={2024--2032},\n  year={2019}\n}\n@inproceedings{zhang2019aim,\n  title={AIM 2019 Challenge on Constrained Super-Resolution: Methods and Results},\n  author={Kai Zhang and Shuhang Gu and Radu Timofte and others},\n  booktitle={IEEE International Conference on Computer Vision Workshops},\n  year={2019}\n}\n# --------------------------------------------\n""""""\n\n\n# --------------------------------------------\n# modified version, https://github.com/Zheng222/IMDN\n# first place solution for AIM 2019 challenge\n# --------------------------------------------\nclass IMDN(nn.Module):\n    def __init__(self, in_nc=3, out_nc=3, nc=64, nb=8, upscale=4, act_mode=\'L\', upsample_mode=\'pixelshuffle\', negative_slope=0.05):\n        """"""\n        in_nc: channel number of input\n        out_nc: channel number of output\n        nc: channel number\n        nb: number of residual blocks\n        upscale: up-scale factor\n        act_mode: activation function\n        upsample_mode: \'upconv\' | \'pixelshuffle\' | \'convtranspose\'\n        """"""\n        super(IMDN, self).__init__()\n        assert \'R\' in act_mode or \'L\' in act_mode, \'Examples of activation function: R, L, BR, BL, IR, IL\'\n\n        m_head = B.conv(in_nc, nc, mode=\'C\')\n        m_body = [B.IMDBlock(nc, nc, mode=\'C\'+act_mode, negative_slope=negative_slope) for _ in range(nb)]\n        m_body.append(B.conv(nc, nc, mode=\'C\'))\n\n        if upsample_mode == \'upconv\':\n            upsample_block = B.upsample_upconv\n        elif upsample_mode == \'pixelshuffle\':\n            upsample_block = B.upsample_pixelshuffle\n        elif upsample_mode == \'convtranspose\':\n            upsample_block = B.upsample_convtranspose\n        else:\n            raise NotImplementedError(\'upsample mode [{:s}] is not found\'.format(upsample_mode))\n\n        m_uper = upsample_block(nc, out_nc, mode=str(upscale))\n\n        self.model = B.sequential(m_head, B.ShortcutBlock(B.sequential(*m_body)), *m_uper)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n'"
models/network_msrresnet.py,3,"b'import math\nimport torch.nn as nn\nimport models.basicblock as B\nimport functools\nimport torch.nn.functional as F\nimport torch.nn.init as init\n\n\n""""""\n# --------------------------------------------\n# modified SRResNet\n#   -- MSRResNet0 (v0.0)\n#   -- MSRResNet1 (v0.1)\n# --------------------------------------------\nReferences:\n@inproceedings{wang2018esrgan,\n  title={Esrgan: Enhanced super-resolution generative adversarial networks},\n  author={Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Change Loy, Chen},\n  booktitle={European Concerence on Computer Vision (ECCV)},\n  pages={0--0},\n  year={2018}\n}\n@inproceedings{ledig2017photo,\n  title={Photo-realistic single image super-resolution using a generative adversarial network},\n  author={Ledig, Christian and Theis, Lucas and Husz{\\\'a}r, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and others},\n  booktitle={IEEE concerence on computer vision and pattern recognition},\n  pages={4681--4690},\n  year={2017}\n}\n# --------------------------------------------\n""""""\n\n\n# --------------------------------------------\n# modified SRResNet v0.0\n# https://github.com/xinntao/ESRGAN\n# --------------------------------------------\nclass MSRResNet0(nn.Module):\n    def __init__(self, in_nc=3, out_nc=3, nc=64, nb=16, upscale=4, act_mode=\'R\', upsample_mode=\'upconv\'):\n        """"""\n        in_nc: channel number of input\n        out_nc: channel number of output\n        nc: channel number\n        nb: number of residual blocks\n        upscale: up-scale factor\n        act_mode: activation function\n        upsample_mode: \'upconv\' | \'pixelshuffle\' | \'convtranspose\'\n        """"""\n        super(MSRResNet0, self).__init__()\n        assert \'R\' in act_mode or \'L\' in act_mode, \'Examples of activation function: R, L, BR, BL, IR, IL\'\n\n        n_upscale = int(math.log(upscale, 2))\n        if upscale == 3:\n            n_upscale = 1\n\n        m_head = B.conv(in_nc, nc, mode=\'C\')\n\n        m_body = [B.ResBlock(nc, nc, mode=\'C\'+act_mode+\'C\') for _ in range(nb)]\n        m_body.append(B.conv(nc, nc, mode=\'C\'))\n\n        if upsample_mode == \'upconv\':\n            upsample_block = B.upsample_upconv\n        elif upsample_mode == \'pixelshuffle\':\n            upsample_block = B.upsample_pixelshuffle\n        elif upsample_mode == \'convtranspose\':\n            upsample_block = B.upsample_convtranspose\n        else:\n            raise NotImplementedError(\'upsample mode [{:s}] is not found\'.format(upsample_mode))\n        if upscale == 3:\n            m_uper = upsample_block(nc, nc, mode=\'3\'+act_mode)\n        else:\n            m_uper = [upsample_block(nc, nc, mode=\'2\'+act_mode) for _ in range(n_upscale)]\n\n        H_conv0 = B.conv(nc, nc, mode=\'C\'+act_mode)\n        H_conv1 = B.conv(nc, out_nc, bias=False, mode=\'C\')\n        m_tail = B.sequential(H_conv0, H_conv1)\n\n        self.model = B.sequential(m_head, B.ShortcutBlock(B.sequential(*m_body)), *m_uper, m_tail)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\n# --------------------------------------------\n# modified SRResNet v0.1\n# https://github.com/xinntao/ESRGAN\n# --------------------------------------------\nclass MSRResNet1(nn.Module):\n    def __init__(self, in_nc=3, out_nc=3, nc=64, nb=16, upscale=4, act_mode=\'R\', upsample_mode=\'upconv\'):\n        super(MSRResNet1, self).__init__()\n        self.upscale = upscale\n\n        self.conv_first = nn.Conv2d(in_nc, nc, 3, 1, 1, bias=True)\n        basic_block = functools.partial(ResidualBlock_noBN, nc=nc)\n        self.recon_trunk = make_layer(basic_block, nb)\n\n        # upsampling\n        if self.upscale == 2:\n            self.upconv1 = nn.Conv2d(nc, nc * 4, 3, 1, 1, bias=True)\n            self.pixel_shuffle = nn.PixelShuffle(2)\n        elif self.upscale == 3:\n            self.upconv1 = nn.Conv2d(nc, nc * 9, 3, 1, 1, bias=True)\n            self.pixel_shuffle = nn.PixelShuffle(3)\n        elif self.upscale == 4:\n            self.upconv1 = nn.Conv2d(nc, nc * 4, 3, 1, 1, bias=True)\n            self.upconv2 = nn.Conv2d(nc, nc * 4, 3, 1, 1, bias=True)\n            self.pixel_shuffle = nn.PixelShuffle(2)\n\n        self.HRconv = nn.Conv2d(nc, nc, 3, 1, 1, bias=True)\n        self.conv_last = nn.Conv2d(nc, out_nc, 3, 1, 1, bias=True)\n\n        # activation function\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n        # initialization\n        initialize_weights([self.conv_first, self.upconv1, self.HRconv, self.conv_last], 0.1)\n        if self.upscale == 4:\n            initialize_weights(self.upconv2, 0.1)\n\n    def forward(self, x):\n        fea = self.lrelu(self.conv_first(x))\n        out = self.recon_trunk(fea)\n\n        if self.upscale == 4:\n            out = self.lrelu(self.pixel_shuffle(self.upconv1(out)))\n            out = self.lrelu(self.pixel_shuffle(self.upconv2(out)))\n        elif self.upscale == 3 or self.upscale == 2:\n            out = self.lrelu(self.pixel_shuffle(self.upconv1(out)))\n\n        out = self.conv_last(self.lrelu(self.HRconv(out)))\n        base = F.interpolate(x, scale_factor=self.upscale, mode=\'bilinear\', align_corners=False)\n        out += base\n        return out\n\n\ndef initialize_weights(net_l, scale=1):\n    if not isinstance(net_l, list):\n        net_l = [net_l]\n    for net in net_l:\n        for m in net.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, a=0, mode=\'fan_in\')\n                m.weight.data *= scale  # for residual block\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                init.kaiming_normal_(m.weight, a=0, mode=\'fan_in\')\n                m.weight.data *= scale\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias.data, 0.0)\n\n\ndef make_layer(block, n_layers):\n    layers = []\n    for _ in range(n_layers):\n        layers.append(block())\n    return nn.Sequential(*layers)\n\n\nclass ResidualBlock_noBN(nn.Module):\n    \'\'\'Residual block w/o BN\n    ---Conv-ReLU-Conv-+-\n     |________________|\n    \'\'\'\n\n    def __init__(self, nc=64):\n        super(ResidualBlock_noBN, self).__init__()\n        self.conv1 = nn.Conv2d(nc, nc, 3, 1, 1, bias=True)\n        self.conv2 = nn.Conv2d(nc, nc, 3, 1, 1, bias=True)\n\n        # initialization\n        initialize_weights([self.conv1, self.conv2], 0.1)\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.conv1(x), inplace=True)\n        out = self.conv2(out)\n        return identity + out\n'"
models/network_rrdb.py,1,"b'import math\nimport torch.nn as nn\nimport models.basicblock as B\n\n\n""""""\n# --------------------------------------------\n# SR network with Residual in Residual Dense Block (RRDB)\n# ""ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks""\n# --------------------------------------------\n""""""\n\n\nclass RRDB(nn.Module):\n    """"""\n    gc: number of growth channels\n    nb: number of RRDB\n    """"""\n    def __init__(self, in_nc=3, out_nc=3, nc=64, nb=23, gc=32, upscale=4, act_mode=\'L\', upsample_mode=\'upconv\'):\n        super(RRDB, self).__init__()\n        assert \'R\' in act_mode or \'L\' in act_mode, \'Examples of activation function: R, L, BR, BL, IR, IL\'\n\n        n_upscale = int(math.log(upscale, 2))\n        if upscale == 3:\n            n_upscale = 1\n\n        m_head = B.conv(in_nc, nc, mode=\'C\')\n\n        m_body = [B.RRDB(nc, gc=32, mode=\'C\'+act_mode) for _ in range(nb)]\n        m_body.append(B.conv(nc, nc, mode=\'C\'))\n\n        if upsample_mode == \'upconv\':\n            upsample_block = B.upsample_upconv\n        elif upsample_mode == \'pixelshuffle\':\n            upsample_block = B.upsample_pixelshuffle\n        elif upsample_mode == \'convtranspose\':\n            upsample_block = B.upsample_convtranspose\n        else:\n            raise NotImplementedError(\'upsample mode [{:s}] is not found\'.format(upsample_mode))\n\n        if upscale == 3:\n            m_uper = upsample_block(nc, nc, mode=\'3\'+act_mode)\n        else:\n            m_uper = [upsample_block(nc, nc, mode=\'2\'+act_mode) for _ in range(n_upscale)]\n\n        H_conv0 = B.conv(nc, nc, mode=\'C\'+act_mode)\n        H_conv1 = B.conv(nc, out_nc, mode=\'C\')\n        m_tail = B.sequential(H_conv0, H_conv1)\n\n        self.model = B.sequential(m_head, B.ShortcutBlock(B.sequential(*m_body)), *m_uper, m_tail)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n'"
models/network_srmd.py,4,"b'\nimport torch.nn as nn\nimport models.basicblock as B\nimport torch\n\n""""""\n# --------------------------------------------\n# SRMD (15 conv layers)\n# --------------------------------------------\nReference:\n@inproceedings{zhang2018learning,\n  title={Learning a single convolutional super-resolution network for multiple degradations},\n  author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={3262--3271},\n  year={2018}\n}\nhttp://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Learning_a_Single_CVPR_2018_paper.pdf\n""""""\n\n\n# --------------------------------------------\n# SRMD   (SRMD,   in_nc = 3+15+1 = 19)\n# SRMD   (SRMDNF, in_nc = 3+15   = 18)\n# --------------------------------------------\nclass SRMD(nn.Module):\n    def __init__(self, in_nc=19, out_nc=3, nc=128, nb=12, upscale=4, act_mode=\'R\', upsample_mode=\'pixelshuffle\'):\n        """"""\n        # ------------------------------------\n        in_nc: channel number of input, default: 3+15\n        out_nc: channel number of output\n        nc: channel number\n        nb: total number of conv layers\n        upscale: scale factor\n        act_mode: batch norm + activation function; \'BR\' means BN+ReLU\n        upsample_mode: default \'pixelshuffle\' = conv + pixelshuffle\n        # ------------------------------------\n        """"""\n        super(SRMD, self).__init__()\n        assert \'R\' in act_mode or \'L\' in act_mode, \'Examples of activation function: R, L, BR, BL, IR, IL\'\n        bias = True\n\n        if upsample_mode == \'upconv\':\n            upsample_block = B.upsample_upconv\n        elif upsample_mode == \'pixelshuffle\':\n            upsample_block = B.upsample_pixelshuffle\n        elif upsample_mode == \'convtranspose\':\n            upsample_block = B.upsample_convtranspose\n        else:\n            raise NotImplementedError(\'upsample mode [{:s}] is not found\'.format(upsample_mode))\n\n        m_head = B.conv(in_nc, nc, mode=\'C\'+act_mode[-1], bias=bias)\n        m_body = [B.conv(nc, nc, mode=\'C\'+act_mode, bias=bias) for _ in range(nb-2)]\n        m_tail = upsample_block(nc, out_nc, mode=str(upscale), bias=bias)\n\n        self.model = B.sequential(m_head, *m_body, m_tail)\n\n#    def forward(self, x, k_pca):\n#        m = k_pca.repeat(1, 1, x.size()[-2], x.size()[-1])\n#        x = torch.cat((x, m), 1)\n#        x = self.body(x)\n\n    def forward(self, x):\n\n        x = self.model(x)\n\n        return x\n\n\nif __name__ == \'__main__\':\n    from utils import utils_model\n    model = SRMD(in_nc=18, out_nc=3, nc=64, nb=15, upscale=4, act_mode=\'R\', upsample_mode=\'pixelshuffle\')\n    print(utils_model.describe_model(model))\n\n    x = torch.randn((2, 3, 100, 100))\n    k_pca = torch.randn(2, 15, 1, 1)\n    x = model(x, k_pca)\n    print(x.shape)\n\n    #  run models/network_srmd.py\n\n'"
models/network_usrnet.py,27,"b'import torch\nimport torch.nn as nn\nimport models.basicblock as B\nimport numpy as np\nfrom utils import utils_image as util\n\n\n""""""\n# --------------------------------------------\n# Kai Zhang (cskaizhang@gmail.com)\n@inproceedings{zhang2020deep,\n  title={Deep unfolding network for image super-resolution},\n  author={Zhang, Kai and Van Gool, Luc and Timofte, Radu},\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={0--0},\n  year={2020}\n}\n# --------------------------------------------\n""""""\n\n\n""""""\n# --------------------------------------------\n# basic functions\n# --------------------------------------------\n""""""\n\n\ndef splits(a, sf):\n    \'\'\'split a into sfxsf distinct blocks\n\n    Args:\n        a: NxCxWxHx2\n        sf: split factor\n\n    Returns:\n        b: NxCx(W/sf)x(H/sf)x2x(sf^2)\n    \'\'\'\n    b = torch.stack(torch.chunk(a, sf, dim=2), dim=5)\n    b = torch.cat(torch.chunk(b, sf, dim=3), dim=5)\n    return b\n\n\ndef c2c(x):\n    return torch.from_numpy(np.stack([np.float32(x.real), np.float32(x.imag)], axis=-1))\n\n\ndef r2c(x):\n    # convert real to complex\n    return torch.stack([x, torch.zeros_like(x)], -1)\n\n\ndef cdiv(x, y):\n    # complex division\n    a, b = x[..., 0], x[..., 1]\n    c, d = y[..., 0], y[..., 1]\n    cd2 = c**2 + d**2\n    return torch.stack([(a*c+b*d)/cd2, (b*c-a*d)/cd2], -1)\n\n\ndef crdiv(x, y):\n    # complex/real division\n    a, b = x[..., 0], x[..., 1]\n    return torch.stack([a/y, b/y], -1)\n\n\ndef csum(x, y):\n    # complex + real\n    return torch.stack([x[..., 0] + y, x[..., 1]], -1)\n\n\ndef cabs(x):\n    # modulus of a complex number\n    return torch.pow(x[..., 0]**2+x[..., 1]**2, 0.5)\n\n\ndef cabs2(x):\n    return x[..., 0]**2+x[..., 1]**2\n\n\ndef cmul(t1, t2):\n    \'\'\'complex multiplication\n\n    Args:\n        t1: NxCxHxWx2, complex tensor\n        t2: NxCxHxWx2\n\n    Returns:\n        output: NxCxHxWx2\n    \'\'\'\n    real1, imag1 = t1[..., 0], t1[..., 1]\n    real2, imag2 = t2[..., 0], t2[..., 1]\n    return torch.stack([real1 * real2 - imag1 * imag2, real1 * imag2 + imag1 * real2], dim=-1)\n\n\ndef cconj(t, inplace=False):\n    \'\'\'complex\'s conjugation\n\n    Args:\n        t: NxCxHxWx2\n\n    Returns:\n        output: NxCxHxWx2\n    \'\'\'\n    c = t.clone() if not inplace else t\n    c[..., 1] *= -1\n    return c\n\n\ndef rfft(t):\n    # Real-to-complex Discrete Fourier Transform\n    return torch.rfft(t, 2, onesided=False)\n\n\ndef irfft(t):\n    # Complex-to-real Inverse Discrete Fourier Transform\n    return torch.irfft(t, 2, onesided=False)\n\n\ndef fft(t):\n    # Complex-to-complex Discrete Fourier Transform\n    return torch.fft(t, 2)\n\n\ndef ifft(t):\n    # Complex-to-complex Inverse Discrete Fourier Transform\n    return torch.ifft(t, 2)\n\n\ndef p2o(psf, shape):\n    \'\'\'\n    Convert point-spread function to optical transfer function.\n    otf = p2o(psf) computes the Fast Fourier Transform (FFT) of the\n    point-spread function (PSF) array and creates the optical transfer\n    function (OTF) array that is not influenced by the PSF off-centering.\n\n    Args:\n        psf: NxCxhxw\n        shape: [H, W]\n\n    Returns:\n        otf: NxCxHxWx2\n    \'\'\'\n    otf = torch.zeros(psf.shape[:-2] + shape).type_as(psf)\n    otf[...,:psf.shape[2],:psf.shape[3]].copy_(psf)\n    for axis, axis_size in enumerate(psf.shape[2:]):\n        otf = torch.roll(otf, -int(axis_size / 2), dims=axis+2)\n    otf = torch.rfft(otf, 2, onesided=False)\n    n_ops = torch.sum(torch.tensor(psf.shape).type_as(psf) * torch.log2(torch.tensor(psf.shape).type_as(psf)))\n    otf[..., 1][torch.abs(otf[..., 1]) < n_ops*2.22e-16] = torch.tensor(0).type_as(psf)\n    return otf\n\n\ndef upsample(x, sf=3):\n    \'\'\'s-fold upsampler\n\n    Upsampling the spatial size by filling the new entries with zeros\n\n    x: tensor image, NxCxWxH\n    \'\'\'\n    st = 0\n    z = torch.zeros((x.shape[0], x.shape[1], x.shape[2]*sf, x.shape[3]*sf)).type_as(x)\n    z[..., st::sf, st::sf].copy_(x)\n    return z\n\n\ndef downsample(x, sf=3):\n    \'\'\'s-fold downsampler\n\n    Keeping the upper-left pixel for each distinct sfxsf patch and discarding the others\n\n    x: tensor image, NxCxWxH\n    \'\'\'\n    st = 0\n    return x[..., st::sf, st::sf]\n\n\ndef downsample_np(x, sf=3):\n    st = 0\n    return x[st::sf, st::sf, ...]\n\n\n""""""\n# --------------------------------------------\n# (1) Prior module; ResUNet: act as a non-blind denoiser\n# x_k = P(z_k, beta_k)\n# --------------------------------------------\n""""""\n\n\nclass ResUNet(nn.Module):\n    def __init__(self, in_nc=4, out_nc=3, nc=[64, 128, 256, 512], nb=2, act_mode=\'R\', downsample_mode=\'strideconv\', upsample_mode=\'convtranspose\'):\n        super(ResUNet, self).__init__()\n\n        self.m_head = B.conv(in_nc, nc[0], bias=False, mode=\'C\')\n\n        # downsample\n        if downsample_mode == \'avgpool\':\n            downsample_block = B.downsample_avgpool\n        elif downsample_mode == \'maxpool\':\n            downsample_block = B.downsample_maxpool\n        elif downsample_mode == \'strideconv\':\n            downsample_block = B.downsample_strideconv\n        else:\n            raise NotImplementedError(\'downsample mode [{:s}] is not found\'.format(downsample_mode))\n\n        self.m_down1 = B.sequential(*[B.ResBlock(nc[0], nc[0], bias=False, mode=\'C\'+act_mode+\'C\') for _ in range(nb)], downsample_block(nc[0], nc[1], bias=False, mode=\'2\'))\n        self.m_down2 = B.sequential(*[B.ResBlock(nc[1], nc[1], bias=False, mode=\'C\'+act_mode+\'C\') for _ in range(nb)], downsample_block(nc[1], nc[2], bias=False, mode=\'2\'))\n        self.m_down3 = B.sequential(*[B.ResBlock(nc[2], nc[2], bias=False, mode=\'C\'+act_mode+\'C\') for _ in range(nb)], downsample_block(nc[2], nc[3], bias=False, mode=\'2\'))\n\n        self.m_body  = B.sequential(*[B.ResBlock(nc[3], nc[3], bias=False, mode=\'C\'+act_mode+\'C\') for _ in range(nb)])\n\n        # upsample\n        if upsample_mode == \'upconv\':\n            upsample_block = B.upsample_upconv\n        elif upsample_mode == \'pixelshuffle\':\n            upsample_block = B.upsample_pixelshuffle\n        elif upsample_mode == \'convtranspose\':\n            upsample_block = B.upsample_convtranspose\n        else:\n            raise NotImplementedError(\'upsample mode [{:s}] is not found\'.format(upsample_mode))\n\n        self.m_up3 = B.sequential(upsample_block(nc[3], nc[2], bias=False, mode=\'2\'), *[B.ResBlock(nc[2], nc[2], bias=False, mode=\'C\'+act_mode+\'C\') for _ in range(nb)])\n        self.m_up2 = B.sequential(upsample_block(nc[2], nc[1], bias=False, mode=\'2\'), *[B.ResBlock(nc[1], nc[1], bias=False, mode=\'C\'+act_mode+\'C\') for _ in range(nb)])\n        self.m_up1 = B.sequential(upsample_block(nc[1], nc[0], bias=False, mode=\'2\'), *[B.ResBlock(nc[0], nc[0], bias=False, mode=\'C\'+act_mode+\'C\') for _ in range(nb)])\n\n        self.m_tail = B.conv(nc[0], out_nc, bias=False, mode=\'C\')\n\n    def forward(self, x):\n        \n        h, w = x.size()[-2:]\n        paddingBottom = int(np.ceil(h/8)*8-h)\n        paddingRight = int(np.ceil(w/8)*8-w)\n        x = nn.ReplicationPad2d((0, paddingRight, 0, paddingBottom))(x)\n\n        x1 = self.m_head(x)\n        x2 = self.m_down1(x1)\n        x3 = self.m_down2(x2)\n        x4 = self.m_down3(x3)\n        x = self.m_body(x4)\n        x = self.m_up3(x+x4)\n        x = self.m_up2(x+x3)\n        x = self.m_up1(x+x2)\n        x = self.m_tail(x+x1)\n\n        x = x[..., :h, :w]\n\n        return x\n\n\n""""""\n# --------------------------------------------\n# (2) Data module, closed-form solution\n# It is a trainable-parameter-free module  ^_^\n# z_k = D(x_{k-1}, s, k, y, alpha_k)\n# some can be pre-calculated\n# --------------------------------------------\n""""""\n\n\nclass DataNet(nn.Module):\n    def __init__(self):\n        super(DataNet, self).__init__()\n\n    def forward(self, x, FB, FBC, F2B, FBFy, alpha, sf):\n        FR = FBFy + torch.rfft(alpha*x, 2, onesided=False)\n        x1 = cmul(FB, FR)\n        FBR = torch.mean(splits(x1, sf), dim=-1, keepdim=False)\n        invW = torch.mean(splits(F2B, sf), dim=-1, keepdim=False)\n        invWBR = cdiv(FBR, csum(invW, alpha))\n        FCBinvWBR = cmul(FBC, invWBR.repeat(1, 1, sf, sf, 1))\n        FX = (FR-FCBinvWBR)/alpha.unsqueeze(-1)\n        Xest = torch.irfft(FX, 2, onesided=False)\n\n        return Xest\n\n\n""""""\n# --------------------------------------------\n# (3) Hyper-parameter module\n# --------------------------------------------\n""""""\n\n\nclass HyPaNet(nn.Module):\n    def __init__(self, in_nc=2, out_nc=8, channel=64):\n        super(HyPaNet, self).__init__()\n        self.mlp = nn.Sequential(\n                nn.Conv2d(in_nc, channel, 1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel, channel, 1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel, out_nc, 1, padding=0, bias=True),\n                nn.Softplus())\n\n    def forward(self, x):\n        x = self.mlp(x) + 1e-6\n        return x\n\n\n""""""\n# --------------------------------------------\n# main USRNet\n# deep unfolding super-resolution network\n# --------------------------------------------\n""""""\n\n\nclass USRNet(nn.Module):\n    def __init__(self, n_iter=8, h_nc=64, in_nc=4, out_nc=3, nc=[64, 128, 256, 512], nb=2, act_mode=\'R\', downsample_mode=\'strideconv\', upsample_mode=\'convtranspose\'):\n        super(USRNet, self).__init__()\n\n        self.d = DataNet()\n        self.p = ResUNet(in_nc=in_nc, out_nc=out_nc, nc=nc, nb=nb, act_mode=act_mode, downsample_mode=downsample_mode, upsample_mode=upsample_mode)\n        self.h = HyPaNet(in_nc=2, out_nc=n_iter*2, channel=h_nc)\n        self.n = n_iter\n\n    def forward(self, x, k, sf, sigma):\n        \'\'\'\n        x: tensor, NxCxWxH\n        k: tensor, Nx(1,3)xwxh\n        sf: integer, 1\n        sigma: tensor, Nx1x1x1\n        \'\'\'\n\n        # initialization & pre-calculation\n        w, h = x.shape[-2:]\n        FB = p2o(k, (w*sf, h*sf))\n        FBC = cconj(FB, inplace=False)\n        F2B = r2c(cabs2(FB))\n        STy = upsample(x, sf=sf)\n        FBFy = cmul(FBC, torch.rfft(STy, 2, onesided=False))\n        x = nn.functional.interpolate(x, scale_factor=sf, mode=\'nearest\')\n\n        # hyper-parameter, alpha & beta\n        ab = self.h(torch.cat((sigma, torch.tensor(sf).type_as(sigma).expand_as(sigma)), dim=1))\n\n        # unfolding\n        for i in range(self.n):\n            \n            x = self.d(x, FB, FBC, F2B, FBFy, ab[:, i:i+1, ...], sf)\n            x = self.p(torch.cat((x, ab[:, i+self.n:i+self.n+1, ...].repeat(1, 1, x.size(2), x.size(3))), dim=1))\n\n        return x\n'"
models/select_model.py,0,"b'\n""""""\n# --------------------------------------------\n# define training model\n# --------------------------------------------\n""""""\n\n\ndef define_Model(opt):\n    model = opt[\'model\']      # one input: L\n\n    if model == \'plain\':\n        from models.model_plain import ModelPlain as M\n\n    elif model == \'plain2\':  # two inputs: L, C\n        from models.model_plain2 import ModelPlain2 as M\n\n    elif model == \'gan\':     # one input: L\n        from models.model_gan import ModelGAN as M\n\n    else:\n        raise NotImplementedError(\'Model [{:s}] is not defined.\'.format(model))\n\n    m = M(opt)\n\n    print(\'Training model [{:s}] is created.\'.format(m.__class__.__name__))\n    return m\n'"
models/select_network.py,2,"b'import functools\nimport torch\nfrom torch.nn import init\n\n\n""""""\n# --------------------------------------------\n# select the network of G, D and F\n# --------------------------------------------\n""""""\n\n\n# --------------------------------------------\n# Generator, netG, G\n# --------------------------------------------\ndef define_G(opt):\n    opt_net = opt[\'netG\']\n    net_type = opt_net[\'net_type\']\n\n\n    # ----------------------------------------\n    # denoising task\n    # ----------------------------------------\n\n    # ----------------------------------------\n    # DnCNN\n    # ----------------------------------------\n    if net_type == \'dncnn\':\n        from models.network_dncnn import DnCNN as net\n        netG = net(in_nc=opt_net[\'in_nc\'],\n                   out_nc=opt_net[\'out_nc\'],\n                   nc=opt_net[\'nc\'],\n                   nb=opt_net[\'nb\'],  # total number of conv layers\n                   act_mode=opt_net[\'act_mode\'])\n\n    # ----------------------------------------\n    # Flexible DnCNN\n    # ----------------------------------------\n    elif net_type == \'fdncnn\':\n        from models.network_dncnn import FDnCNN as net\n        netG = net(in_nc=opt_net[\'in_nc\'],\n                   out_nc=opt_net[\'out_nc\'],\n                   nc=opt_net[\'nc\'],\n                   nb=opt_net[\'nb\'],  # total number of conv layers\n                   act_mode=opt_net[\'act_mode\'])\n\n    # ----------------------------------------\n    # FFDNet\n    # ----------------------------------------\n    elif net_type == \'ffdnet\':\n        from models.network_ffdnet import FFDNet as net\n        netG = net(in_nc=opt_net[\'in_nc\'],\n                   out_nc=opt_net[\'out_nc\'],\n                   nc=opt_net[\'nc\'],\n                   nb=opt_net[\'nb\'],\n                   act_mode=opt_net[\'act_mode\'])\n\n    # ----------------------------------------\n    # others\n    # ----------------------------------------\n\n    # ----------------------------------------\n    # super-resolution task\n    # ----------------------------------------\n\n    # ----------------------------------------\n    # SRMD\n    # ----------------------------------------\n    elif net_type == \'srmd\':\n        from models.network_srmd import SRMD as net\n        netG = net(in_nc=opt_net[\'in_nc\'],\n                   out_nc=opt_net[\'out_nc\'],\n                   nc=opt_net[\'nc\'],\n                   nb=opt_net[\'nb\'],\n                   upscale=opt_net[\'scale\'],\n                   act_mode=opt_net[\'act_mode\'],\n                   upsample_mode=opt_net[\'upsample_mode\'])\n\n    # ----------------------------------------\n    # super-resolver prior of DPSR\n    # ----------------------------------------\n    elif net_type == \'dpsr\':\n        from models.network_dpsr import MSRResNet_prior as net\n        netG = net(in_nc=opt_net[\'in_nc\'],\n                   out_nc=opt_net[\'out_nc\'],\n                   nc=opt_net[\'nc\'],\n                   nb=opt_net[\'nb\'],\n                   upscale=opt_net[\'scale\'],\n                   act_mode=opt_net[\'act_mode\'],\n                   upsample_mode=opt_net[\'upsample_mode\'])\n\n    # ----------------------------------------\n    # modified SRResNet v0.0\n    # ----------------------------------------\n    elif net_type == \'msrresnet0\':\n        from models.network_msrresnet import MSRResNet0 as net\n        netG = net(in_nc=opt_net[\'in_nc\'],\n                   out_nc=opt_net[\'out_nc\'],\n                   nc=opt_net[\'nc\'],\n                   nb=opt_net[\'nb\'],\n                   upscale=opt_net[\'scale\'],\n                   act_mode=opt_net[\'act_mode\'],\n                   upsample_mode=opt_net[\'upsample_mode\'])\n\n    # ----------------------------------------\n    # modified SRResNet v0.1\n    # ----------------------------------------\n    elif net_type == \'msrresnet1\':\n        from models.network_msrresnet import MSRResNet1 as net\n        netG = net(in_nc=opt_net[\'in_nc\'],\n                   out_nc=opt_net[\'out_nc\'],\n                   nc=opt_net[\'nc\'],\n                   nb=opt_net[\'nb\'],\n                   upscale=opt_net[\'scale\'],\n                   act_mode=opt_net[\'act_mode\'],\n                   upsample_mode=opt_net[\'upsample_mode\'])\n\n    # ----------------------------------------\n    # RRDB\n    # ----------------------------------------\n    elif net_type == \'rrdb\':  # RRDB\n        from models.network_rrdb import RRDB as net\n        netG = net(in_nc=opt_net[\'in_nc\'],\n                   out_nc=opt_net[\'out_nc\'],\n                   nc=opt_net[\'nc\'],\n                   nb=opt_net[\'nb\'],\n                   gc=opt_net[\'gc\'],\n                   upscale=opt_net[\'scale\'],\n                   act_mode=opt_net[\'act_mode\'],\n                   upsample_mode=opt_net[\'upsample_mode\'])\n\n    # ----------------------------------------\n    # IMDB\n    # ----------------------------------------\n    elif net_type == \'imdn\':  # IMDB\n        from models.network_imdn import IMDN as net\n        netG = net(in_nc=opt_net[\'in_nc\'],\n                   out_nc=opt_net[\'out_nc\'],\n                   nc=opt_net[\'nc\'],\n                   nb=opt_net[\'nb\'],\n                   upscale=opt_net[\'scale\'],\n                   act_mode=opt_net[\'act_mode\'],\n                   upsample_mode=opt_net[\'upsample_mode\'])\n\n    # ----------------------------------------\n    # others\n    # ----------------------------------------\n    # TODO\n\n    else:\n        raise NotImplementedError(\'netG [{:s}] is not found.\'.format(net_type))\n\n    # ----------------------------------------\n    # initialize weights\n    # ----------------------------------------\n    if opt[\'is_train\']:\n        init_weights(netG,\n                     init_type=opt_net[\'init_type\'],\n                     init_bn_type=opt_net[\'init_bn_type\'],\n                     gain=opt_net[\'init_gain\'])\n\n    return netG\n\n\n# --------------------------------------------\n# Discriminator, netD, D\n# --------------------------------------------\ndef define_D(opt):\n    opt_net = opt[\'netD\']\n    net_type = opt_net[\'net_type\']\n\n    # ----------------------------------------\n    # discriminator_vgg_96\n    # ----------------------------------------\n    if net_type == \'discriminator_vgg_96\':\n        from models.network_discriminator import Discriminator_VGG_96 as discriminator\n        netD = discriminator(in_nc=opt_net[\'in_nc\'],\n                             base_nc=opt_net[\'base_nc\'],\n                             ac_type=opt_net[\'act_mode\'])\n\n    # ----------------------------------------\n    # discriminator_vgg_128\n    # ----------------------------------------\n    elif net_type == \'discriminator_vgg_128\':\n        from models.network_discriminator import Discriminator_VGG_128 as discriminator\n        netD = discriminator(in_nc=opt_net[\'in_nc\'],\n                             base_nc=opt_net[\'base_nc\'],\n                             ac_type=opt_net[\'act_mode\'])\n\n    # ----------------------------------------\n    # discriminator_vgg_192\n    # ----------------------------------------\n    elif net_type == \'discriminator_vgg_192\':\n        from models.network_discriminator import Discriminator_VGG_192 as discriminator\n        netD = discriminator(in_nc=opt_net[\'in_nc\'],\n                             base_nc=opt_net[\'base_nc\'],\n                             ac_type=opt_net[\'act_mode\'])\n\n    # ----------------------------------------\n    # discriminator_vgg_128_SN\n    # ----------------------------------------\n    elif net_type == \'discriminator_vgg_128_SN\':\n        from models.network_discriminator import Discriminator_VGG_128_SN as discriminator\n        netD = discriminator()\n\n    else:\n        raise NotImplementedError(\'netD [{:s}] is not found.\'.format(net_type))\n\n    # ----------------------------------------\n    # initialize weights\n    # ----------------------------------------\n    init_weights(netD,\n                 init_type=opt_net[\'init_type\'],\n                 init_bn_type=opt_net[\'init_bn_type\'],\n                 gain=opt_net[\'init_gain\'])\n\n    return netD\n\n\n# --------------------------------------------\n# VGGfeature, netF, F\n# --------------------------------------------\ndef define_F(opt, use_bn=False):\n    device = torch.device(\'cuda\' if opt[\'gpu_ids\'] else \'cpu\')\n    from models.network_feature import VGGFeatureExtractor\n    # pytorch pretrained VGG19-54, before ReLU.\n    if use_bn:\n        feature_layer = 49\n    else:\n        feature_layer = 34\n    netF = VGGFeatureExtractor(feature_layer=feature_layer,\n                               use_bn=use_bn,\n                               use_input_norm=True,\n                               device=device)\n    netF.eval()  # No need to train, but need BP to input\n    return netF\n\n\n""""""\n# --------------------------------------------\n# weights initialization\n# --------------------------------------------\n""""""\n\n\ndef init_weights(net, init_type=\'xavier_uniform\', init_bn_type=\'uniform\', gain=1):\n    """"""\n    # Kai Zhang, https://github.com/cszn/KAIR\n    #\n    # Args:\n    #   init_type:\n    #       normal; normal; xavier_normal; xavier_uniform;\n    #       kaiming_normal; kaiming_uniform; orthogonal\n    #   init_bn_type:\n    #       uniform; constant\n    #   gain:\n    #       0.2\n    """"""\n    print(\'Initialization method [{:s} + {:s}], gain is [{:.2f}]\'.format(init_type, init_bn_type, gain))\n\n    def init_fn(m, init_type=\'xavier_uniform\', init_bn_type=\'uniform\', gain=1):\n        classname = m.__class__.__name__\n\n        if classname.find(\'Conv\') != -1 or classname.find(\'Linear\') != -1:\n\n            if init_type == \'normal\':\n                init.normal_(m.weight.data, 0, 0.1)\n                m.weight.data.clamp_(-1, 1).mul_(gain)\n\n            elif init_type == \'uniform\':\n                init.uniform_(m.weight.data, -0.2, 0.2)\n                m.weight.data.mul_(gain)\n\n            elif init_type == \'xavier_normal\':\n                init.xavier_normal_(m.weight.data, gain=gain)\n                m.weight.data.clamp_(-1, 1)\n\n            elif init_type == \'xavier_uniform\':\n                init.xavier_uniform_(m.weight.data, gain=gain)\n\n            elif init_type == \'kaiming_normal\':\n                init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\', nonlinearity=\'relu\')\n                m.weight.data.clamp_(-1, 1).mul_(gain)\n\n            elif init_type == \'kaiming_uniform\':\n                init.kaiming_uniform_(m.weight.data, a=0, mode=\'fan_in\', nonlinearity=\'relu\')\n                m.weight.data.mul_(gain)\n\n            elif init_type == \'orthogonal\':\n                init.orthogonal_(m.weight.data, gain=gain)\n\n            else:\n                raise NotImplementedError(\'Initialization method [{:s}] is not implemented\'.format(init_type))\n\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n        elif classname.find(\'BatchNorm2d\') != -1:\n\n            if init_bn_type == \'uniform\':  # preferred\n                if m.affine:\n                    init.uniform_(m.weight.data, 0.1, 1.0)\n                    init.constant_(m.bias.data, 0.0)\n            elif init_bn_type == \'constant\':\n                if m.affine:\n                    init.constant_(m.weight.data, 1.0)\n                    init.constant_(m.bias.data, 0.0)\n            else:\n                raise NotImplementedError(\'Initialization method [{:s}] is not implemented\'.format(init_bn_type))\n\n    fn = functools.partial(init_fn, init_type=init_type, init_bn_type=init_bn_type, gain=gain)\n    net.apply(fn)\n'"
utils/utils_bnorm.py,2,"b'import torch\nimport torch.nn as nn\n\n\n""""""\n# --------------------------------------------\n# Batch Normalization\n# --------------------------------------------\n\n# Kai Zhang (cskaizhang@gmail.com)\n# https://github.com/cszn\n# 01/Jan/2019\n# --------------------------------------------\n""""""\n\n\n# --------------------------------------------\n# remove/delete specified layer\n# --------------------------------------------\ndef deleteLayer(model, layer_type=nn.BatchNorm2d):\n    \'\'\' Kai Zhang, 11/Jan/2019.\n    \'\'\'\n    for k, m in list(model.named_children()):\n        if isinstance(m, layer_type):\n            del model._modules[k]\n        deleteLayer(m, layer_type)\n\n\n# --------------------------------------------\n# merge bn, ""conv+bn"" --> ""conv""\n# --------------------------------------------\ndef merge_bn(model):\n    \'\'\' Kai Zhang, 11/Jan/2019.\n    merge all \'Conv+BN\' (or \'TConv+BN\') into \'Conv\' (or \'TConv\')\n    based on https://github.com/pytorch/pytorch/pull/901\n    \'\'\'\n    prev_m = None\n    for k, m in list(model.named_children()):\n        if (isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d)) and (isinstance(prev_m, nn.Conv2d) or isinstance(prev_m, nn.Linear) or isinstance(prev_m, nn.ConvTranspose2d)):\n\n            w = prev_m.weight.data\n\n            if prev_m.bias is None:\n                zeros = torch.Tensor(prev_m.out_channels).zero_().type(w.type())\n                prev_m.bias = nn.Parameter(zeros)\n            b = prev_m.bias.data\n\n            invstd = m.running_var.clone().add_(m.eps).pow_(-0.5)\n            if isinstance(prev_m, nn.ConvTranspose2d):\n                w.mul_(invstd.view(1, w.size(1), 1, 1).expand_as(w))\n            else:\n                w.mul_(invstd.view(w.size(0), 1, 1, 1).expand_as(w))\n            b.add_(-m.running_mean).mul_(invstd)\n            if m.affine:\n                if isinstance(prev_m, nn.ConvTranspose2d):\n                    w.mul_(m.weight.data.view(1, w.size(1), 1, 1).expand_as(w))\n                else:\n                    w.mul_(m.weight.data.view(w.size(0), 1, 1, 1).expand_as(w))\n                b.mul_(m.weight.data).add_(m.bias.data)\n\n            del model._modules[k]\n        prev_m = m\n        merge_bn(m)\n\n\n# --------------------------------------------\n# add bn, ""conv"" --> ""conv+bn""\n# --------------------------------------------\ndef add_bn(model):\n    \'\'\' Kai Zhang, 11/Jan/2019.\n    \'\'\'\n    for k, m in list(model.named_children()):\n        if (isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear) or isinstance(m, nn.ConvTranspose2d)):\n            b = nn.BatchNorm2d(m.out_channels, momentum=0.1, affine=True)\n            b.weight.data.fill_(1)\n            new_m = nn.Sequential(model._modules[k], b)\n            model._modules[k] = new_m\n        add_bn(m)\n\n\n# --------------------------------------------\n# tidy model after removing bn\n# --------------------------------------------\ndef tidy_sequential(model):\n    \'\'\' Kai Zhang, 11/Jan/2019.\n    \'\'\'\n    for k, m in list(model.named_children()):\n        if isinstance(m, nn.Sequential):\n            if m.__len__() == 1:\n                model._modules[k] = m.__getitem__(0)\n        tidy_sequential(m)\n'"
utils/utils_deblur.py,0,"b'import numpy as np\r\nimport scipy\r\nfrom scipy import fftpack\r\n\r\n\r\n\'\'\'\r\n# --------------------------------------------\r\n# Kai Zhang (github: https://github.com/cszn)\r\n# 03/Mar/2019\r\n# --------------------------------------------\r\n\'\'\'\r\n\r\n# --------------------------------------------\r\n# get rho and sigma for dpsr\r\n# --------------------------------------------\r\ndef get_rho_sigma(sigma=2.55/255, iter_num=15):\r\n    \'\'\'\r\n    Kai Zhang (github: https://github.com/cszn)\r\n    03/03/2019\r\n    \'\'\'\r\n    modelSigma1 = 49.0\r\n    modelSigma2 = 2.55\r\n    modelSigmaS = np.logspace(np.log10(modelSigma1), np.log10(modelSigma2), iter_num)\r\n    sigmas = modelSigmaS/255.\r\n    mus = list(map(lambda x: (sigma**2)/(x**2)/3, sigmas))\r\n    rhos = mus\r\n    return rhos, sigmas\r\n\r\n\r\n# --------------------------------------------\r\n# HWC, get uperleft and denominator\r\n# --------------------------------------------\r\ndef get_uperleft_denominator(img, kernel):\r\n    \'\'\'\r\n    Kai Zhang (github: https://github.com/cszn)\r\n    03/03/2019\r\n    \'\'\'\r\n    V = psf2otf(kernel, img.shape[:2])   # discrete fourier transform of kernel\r\n    denominator = np.expand_dims(np.abs(V)**2, axis=2)  # Fourier transform of K transpose * K\r\n    upperleft = np.expand_dims(np.conj(V), axis=2) * np.fft.fft2(img, axes=[0, 1])\r\n    return upperleft, denominator\r\n\r\n\r\n# --------------------------------------------\r\n# otf2psf: not sure where I got this one from. \r\n# Maybe translated from Octave source code or whatever. It\'s just math.\r\n# --------------------------------------------\r\ndef otf2psf(otf, outsize=None):\r\n    insize = np.array(otf.shape)\r\n    psf = np.fft.ifftn(otf, axes=(0, 1))\r\n    for axis, axis_size in enumerate(insize):\r\n        psf = np.roll(psf, np.floor(axis_size / 2).astype(int), axis=axis)\r\n    if type(outsize) != type(None):\r\n        insize = np.array(otf.shape)\r\n        outsize = np.array(outsize)\r\n        n = max(np.size(outsize), np.size(insize))\r\n        # outsize = postpad(outsize(:), n, 1);\r\n        # insize = postpad(insize(:) , n, 1);\r\n        colvec_out = outsize.flatten().reshape((np.size(outsize), 1))\r\n        colvec_in = insize.flatten().reshape((np.size(insize), 1))\r\n        outsize = np.pad(colvec_out, ((0, max(0, n - np.size(colvec_out))), (0, 0)), mode=""constant"")\r\n        insize = np.pad(colvec_in, ((0, max(0, n - np.size(colvec_in))), (0, 0)), mode=""constant"")\r\n\r\n        pad = (insize - outsize) / 2\r\n        if np.any(pad < 0):\r\n            print(""otf2psf error: OUTSIZE must be smaller than or equal than OTF size"")\r\n        prepad = np.floor(pad)\r\n        postpad = np.ceil(pad)\r\n        dims_start = prepad.astype(int)\r\n        dims_end = (insize - postpad).astype(int)\r\n        for i in range(len(dims_start.shape)):\r\n            psf = np.take(psf, range(dims_start[i][0], dims_end[i][0]), axis=i)\r\n    n_ops = np.sum(otf.size * np.log2(otf.shape))\r\n    psf = np.real_if_close(psf, tol=n_ops)\r\n    return psf\r\n\r\n\r\n# --------------------------------------------\r\n# psf2otf copied/modified from https://github.com/aboucaud/pypher/blob/master/pypher/pypher.py\r\n# --------------------------------------------\r\ndef psf2otf(psf, shape=None):\r\n    """"""\r\n    Convert point-spread function to optical transfer function.\r\n    Compute the Fast Fourier Transform (FFT) of the point-spread\r\n    function (PSF) array and creates the optical transfer function (OTF)\r\n    array that is not influenced by the PSF off-centering.\r\n    By default, the OTF array is the same size as the PSF array.\r\n    To ensure that the OTF is not altered due to PSF off-centering, PSF2OTF\r\n    post-pads the PSF array (down or to the right) with zeros to match\r\n    dimensions specified in OUTSIZE, then circularly shifts the values of\r\n    the PSF array up (or to the left) until the central pixel reaches (1,1)\r\n    position.\r\n    Parameters\r\n    ----------\r\n    psf : `numpy.ndarray`\r\n        PSF array\r\n    shape : int\r\n        Output shape of the OTF array\r\n    Returns\r\n    -------\r\n    otf : `numpy.ndarray`\r\n        OTF array\r\n    Notes\r\n    -----\r\n    Adapted from MATLAB psf2otf function\r\n    """"""\r\n    if type(shape) == type(None):\r\n        shape = psf.shape\r\n    shape = np.array(shape)\r\n    if np.all(psf == 0):\r\n        # return np.zeros_like(psf)\r\n        return np.zeros(shape)\r\n    if len(psf.shape) == 1:\r\n        psf = psf.reshape((1, psf.shape[0]))\r\n    inshape = psf.shape\r\n    psf = zero_pad(psf, shape, position=\'corner\')\r\n    for axis, axis_size in enumerate(inshape):\r\n        psf = np.roll(psf, -int(axis_size / 2), axis=axis)\r\n    # Compute the OTF\r\n    otf = np.fft.fft2(psf, axes=(0, 1))\r\n    # Estimate the rough number of operations involved in the FFT\r\n    # and discard the PSF imaginary part if within roundoff error\r\n    # roundoff error  = machine epsilon = sys.float_info.epsilon\r\n    # or np.finfo().eps\r\n    n_ops = np.sum(psf.size * np.log2(psf.shape))\r\n    otf = np.real_if_close(otf, tol=n_ops)\r\n    return otf\r\n\r\n\r\ndef zero_pad(image, shape, position=\'corner\'):\r\n    """"""\r\n    Extends image to a certain size with zeros\r\n    Parameters\r\n    ----------\r\n    image: real 2d `numpy.ndarray`\r\n        Input image\r\n    shape: tuple of int\r\n        Desired output shape of the image\r\n    position : str, optional\r\n        The position of the input image in the output one:\r\n            * \'corner\'\r\n                top-left corner (default)\r\n            * \'center\'\r\n                centered\r\n    Returns\r\n    -------\r\n    padded_img: real `numpy.ndarray`\r\n        The zero-padded image\r\n    """"""\r\n    shape = np.asarray(shape, dtype=int)\r\n    imshape = np.asarray(image.shape, dtype=int)\r\n    if np.alltrue(imshape == shape):\r\n        return image\r\n    if np.any(shape <= 0):\r\n        raise ValueError(""ZERO_PAD: null or negative shape given"")\r\n    dshape = shape - imshape\r\n    if np.any(dshape < 0):\r\n        raise ValueError(""ZERO_PAD: target size smaller than source one"")\r\n    pad_img = np.zeros(shape, dtype=image.dtype)\r\n    idx, idy = np.indices(imshape)\r\n    if position == \'center\':\r\n        if np.any(dshape % 2 != 0):\r\n            raise ValueError(""ZERO_PAD: source and target shapes ""\r\n                             ""have different parity."")\r\n        offx, offy = dshape // 2\r\n    else:\r\n        offx, offy = (0, 0)\r\n    pad_img[idx + offx, idy + offy] = image\r\n    return pad_img\r\n\r\n\r\n\'\'\'\r\n# --------------------------------------------\r\n# Reducing boundary artifacts\r\n# --------------------------------------------\r\n\'\'\'\r\n\r\n\r\ndef opt_fft_size(n):\r\n    \'\'\'\r\n    # --------------------------------------------\r\n    Kai Zhang (github: https://github.com/cszn)\r\n    03/Mar/2019\r\n    #  opt_fft_size.m\r\n    # compute an optimal data length for Fourier transforms\r\n    # written by Sunghyun Cho (sodomau@postech.ac.kr)\r\n    # persistent opt_fft_size_LUT\r\n    # --------------------------------------------\r\n    \'\'\'\r\n\r\n    LUT_size = 2048\r\n    # print(""generate opt_fft_size_LUT"")\r\n    opt_fft_size_LUT = np.zeros(LUT_size)\r\n\r\n    e2 = 1\r\n    while e2 <= LUT_size:\r\n        e3 = e2\r\n        while e3 <= LUT_size:\r\n            e5 = e3\r\n            while e5 <= LUT_size:\r\n                e7 = e5\r\n                while e7 <= LUT_size:\r\n                    if e7 <= LUT_size:\r\n                        opt_fft_size_LUT[e7-1] = e7\r\n                    if e7*11 <= LUT_size:\r\n                        opt_fft_size_LUT[e7*11-1] = e7*11\r\n                    if e7*13 <= LUT_size:\r\n                        opt_fft_size_LUT[e7*13-1] = e7*13\r\n                    e7 = e7 * 7\r\n                e5 = e5 * 5\r\n            e3 = e3 * 3\r\n        e2 = e2 * 2\r\n\r\n    nn = 0\r\n    for i in range(LUT_size, 0, -1):\r\n        if opt_fft_size_LUT[i-1] != 0:\r\n            nn = i-1\r\n        else:\r\n            opt_fft_size_LUT[i-1] = nn+1\r\n\r\n    m = np.zeros(len(n))\r\n    for c in range(len(n)):\r\n        nn = n[c]\r\n        if nn <= LUT_size:\r\n            m[c] = opt_fft_size_LUT[nn-1]\r\n        else:\r\n            m[c] = -1\r\n    return m\r\n\r\n\r\ndef wrap_boundary_liu(img, img_size):\r\n\r\n    """"""\r\n    # --------------------------------------------\r\n    Reducing boundary artifacts in image deconvolution\r\n    Renting Liu, Jiaya Jia\r\n    ICIP 2008\r\n    # --------------------------------------------\r\n    """"""\r\n    if img.ndim == 2:\r\n        ret = wrap_boundary(img, img_size)\r\n    elif img.ndim == 3:\r\n        ret = [wrap_boundary(img[:, :, i], img_size) for i in range(3)]\r\n        ret = np.stack(ret, 2)\r\n    return ret\r\n\r\n\r\ndef wrap_boundary(img, img_size):\r\n\r\n    """"""\r\n    # --------------------------------------------\r\n    python code from:\r\n    https://github.com/ys-koshelev/nla_deblur/blob/90fe0ab98c26c791dcbdf231fe6f938fca80e2a0/boundaries.py\r\n    Reducing boundary artifacts in image deconvolution\r\n    Renting Liu, Jiaya Jia\r\n    ICIP 2008\r\n    # --------------------------------------------\r\n    """"""\r\n    (H, W) = np.shape(img)\r\n    H_w = int(img_size[0]) - H\r\n    W_w = int(img_size[1]) - W\r\n\r\n    # ret = np.zeros((img_size[0], img_size[1]));\r\n    alpha = 1\r\n    HG = img[:, :]\r\n\r\n    r_A = np.zeros((alpha*2+H_w, W))\r\n    r_A[:alpha, :] = HG[-alpha:, :]\r\n    r_A[-alpha:, :] = HG[:alpha, :]\r\n    a = np.arange(H_w)/(H_w-1)\r\n    # r_A(alpha+1:end-alpha, 1) = (1-a)*r_A(alpha,1) + a*r_A(end-alpha+1,1)\r\n    r_A[alpha:-alpha, 0] = (1-a)*r_A[alpha-1, 0] + a*r_A[-alpha, 0]\r\n    # r_A(alpha+1:end-alpha, end) = (1-a)*r_A(alpha,end) + a*r_A(end-alpha+1,end)\r\n    r_A[alpha:-alpha, -1] = (1-a)*r_A[alpha-1, -1] + a*r_A[-alpha, -1]\r\n\r\n    r_B = np.zeros((H, alpha*2+W_w))\r\n    r_B[:, :alpha] = HG[:, -alpha:]\r\n    r_B[:, -alpha:] = HG[:, :alpha]\r\n    a = np.arange(W_w)/(W_w-1)\r\n    r_B[0, alpha:-alpha] = (1-a)*r_B[0, alpha-1] + a*r_B[0, -alpha]\r\n    r_B[-1, alpha:-alpha] = (1-a)*r_B[-1, alpha-1] + a*r_B[-1, -alpha]\r\n\r\n    if alpha == 1:\r\n        A2 = solve_min_laplacian(r_A[alpha-1:, :])\r\n        B2 = solve_min_laplacian(r_B[:, alpha-1:])\r\n        r_A[alpha-1:, :] = A2\r\n        r_B[:, alpha-1:] = B2\r\n    else:\r\n        A2 = solve_min_laplacian(r_A[alpha-1:-alpha+1, :])\r\n        r_A[alpha-1:-alpha+1, :] = A2\r\n        B2 = solve_min_laplacian(r_B[:, alpha-1:-alpha+1])\r\n        r_B[:, alpha-1:-alpha+1] = B2\r\n    A = r_A\r\n    B = r_B\r\n\r\n    r_C = np.zeros((alpha*2+H_w, alpha*2+W_w))\r\n    r_C[:alpha, :] = B[-alpha:, :]\r\n    r_C[-alpha:, :] = B[:alpha, :]\r\n    r_C[:, :alpha] = A[:, -alpha:]\r\n    r_C[:, -alpha:] = A[:, :alpha]\r\n\r\n    if alpha == 1:\r\n        C2 = C2 = solve_min_laplacian(r_C[alpha-1:, alpha-1:])\r\n        r_C[alpha-1:, alpha-1:] = C2\r\n    else:\r\n        C2 = solve_min_laplacian(r_C[alpha-1:-alpha+1, alpha-1:-alpha+1])\r\n        r_C[alpha-1:-alpha+1, alpha-1:-alpha+1] = C2\r\n    C = r_C\r\n    # return C\r\n    A = A[alpha-1:-alpha-1, :]\r\n    B = B[:, alpha:-alpha]\r\n    C = C[alpha:-alpha, alpha:-alpha]\r\n    ret = np.vstack((np.hstack((img, B)), np.hstack((A, C))))\r\n    return ret\r\n\r\n\r\ndef solve_min_laplacian(boundary_image):\r\n    (H, W) = np.shape(boundary_image)\r\n\r\n    # Laplacian\r\n    f = np.zeros((H, W))\r\n    # boundary image contains image intensities at boundaries\r\n    boundary_image[1:-1, 1:-1] = 0\r\n    j = np.arange(2, H)-1\r\n    k = np.arange(2, W)-1\r\n    f_bp = np.zeros((H, W))\r\n    f_bp[np.ix_(j, k)] = -4*boundary_image[np.ix_(j, k)] + boundary_image[np.ix_(j, k+1)] + boundary_image[np.ix_(j, k-1)] + boundary_image[np.ix_(j-1, k)] + boundary_image[np.ix_(j+1, k)]\r\n    \r\n    del(j, k)\r\n    f1 = f - f_bp  # subtract boundary points contribution\r\n    del(f_bp, f)\r\n\r\n    # DST Sine Transform algo starts here\r\n    f2 = f1[1:-1,1:-1]\r\n    del(f1)\r\n\r\n    # compute sine tranform\r\n    if f2.shape[1] == 1:\r\n        tt = fftpack.dst(f2, type=1, axis=0)/2\r\n    else:\r\n        tt = fftpack.dst(f2, type=1)/2\r\n\r\n    if tt.shape[0] == 1:\r\n        f2sin = np.transpose(fftpack.dst(np.transpose(tt), type=1, axis=0)/2)\r\n    else:\r\n        f2sin = np.transpose(fftpack.dst(np.transpose(tt), type=1)/2) \r\n    del(f2)\r\n\r\n    # compute Eigen Values\r\n    [x, y] = np.meshgrid(np.arange(1, W-1), np.arange(1, H-1))\r\n    denom = (2*np.cos(np.pi*x/(W-1))-2) + (2*np.cos(np.pi*y/(H-1)) - 2)\r\n\r\n    # divide\r\n    f3 = f2sin/denom\r\n    del(f2sin, x, y)\r\n\r\n    # compute Inverse Sine Transform\r\n    if f3.shape[0] == 1:\r\n        tt = fftpack.idst(f3*2, type=1, axis=1)/(2*(f3.shape[1]+1))\r\n    else:\r\n        tt = fftpack.idst(f3*2, type=1, axis=0)/(2*(f3.shape[0]+1))\r\n    del(f3)\r\n    if tt.shape[1] == 1:\r\n        img_tt = np.transpose(fftpack.idst(np.transpose(tt)*2, type=1)/(2*(tt.shape[0]+1)))\r\n    else:\r\n        img_tt = np.transpose(fftpack.idst(np.transpose(tt)*2, type=1, axis=0)/(2*(tt.shape[1]+1)))\r\n    del(tt)\r\n\r\n    # put solution in inner points; outer points obtained from boundary image\r\n    img_direct = boundary_image\r\n    img_direct[1:-1, 1:-1] = 0\r\n    img_direct[1:-1, 1:-1] = img_tt\r\n    return img_direct\r\n\r\n\r\n""""""\r\n# --------------------------------------------\r\n# Created on Thu Jan 18 15:36:32 2018\r\n# @author: italo\r\n# https://github.com/ronaldosena/imagens-medicas-2/\r\n# blob/40171a6c259edec7827a6693a93955de2bd39e76/Aulas/aula_2_-_uniform_filter/matlab_fspecial.py\r\n# --------------------------------------------\r\n# h = fspecial(type)\r\n# h = fspecial(\'average\',hsize)\r\n# h = fspecial(\'disk\',radius)\r\n# h = fspecial(\'gaussian\',hsize,sigma)\r\n# h = fspecial(\'laplacian\',alpha)\r\n# h = fspecial(\'log\',hsize,sigma)\r\n# h = fspecial(\'motion\',len,theta)\r\n# h = fspecial(\'prewitt\')\r\n# h = fspecial(\'sobel\')\r\n# --------------------------------------------\r\n""""""\r\n\r\n\r\ndef fspecial_average(hsize=3):\r\n    """"""Smoothing filter""""""\r\n    return np.ones((hsize, hsize))/hsize**2\r\n\r\n\r\ndef fspecial_disk(radius):\r\n    """"""Disk filter""""""\r\n    raise(NotImplemented)\r\n    rad = 0.6\r\n    crad = np.ceil(rad-0.5)\r\n    [x, y] = np.meshgrid(np.arange(-crad, crad+1), np.arange(-crad, crad+1))\r\n    maxxy = np.zeros(x.shape)\r\n    maxxy[abs(x) >= abs(y)] = abs(x)[abs(x) >= abs(y)]\r\n    maxxy[abs(y) >= abs(x)] = abs(y)[abs(y) >= abs(x)]\r\n    minxy = np.zeros(x.shape)\r\n    minxy[abs(x) <= abs(y)] = abs(x)[abs(x) <= abs(y)]\r\n    minxy[abs(y) <= abs(x)] = abs(y)[abs(y) <= abs(x)]\r\n    m1 = (rad**2 <  (maxxy+0.5)**2 + (minxy-0.5)**2)*(minxy-0.5) +\\\r\n         (rad**2 >= (maxxy+0.5)**2 + (minxy-0.5)**2)*\\\r\n         np.sqrt((rad**2 + 0j) - (maxxy + 0.5)**2)\r\n    m2 = (rad**2 >  (maxxy-0.5)**2 + (minxy+0.5)**2)*(minxy+0.5) +\\\r\n         (rad**2 <= (maxxy-0.5)**2 + (minxy+0.5)**2)*\\\r\n         np.sqrt((rad**2 + 0j) - (maxxy - 0.5)**2)\r\n    h = None\r\n    return h\r\n\r\n\r\ndef fspecial_gaussian(hsize, sigma):\r\n    hsize = [hsize, hsize]\r\n    siz = [(hsize[0]-1.0)/2.0, (hsize[1]-1.0)/2.0]\r\n    std = sigma\r\n    [x, y] = np.meshgrid(np.arange(-siz[1], siz[1]+1), np.arange(-siz[0], siz[0]+1))\r\n    arg = -(x*x + y*y)/(2*std*std)\r\n    h = np.exp(arg)\r\n    h[h < scipy.finfo(float).eps * h.max()] = 0\r\n    sumh = h.sum()\r\n    if sumh != 0:\r\n        h = h/sumh\r\n    return h\r\n\r\n\r\ndef fspecial_laplacian(alpha):\r\n    alpha = max([0, min([alpha,1])])\r\n    h1 = alpha/(alpha+1)\r\n    h2 = (1-alpha)/(alpha+1)\r\n    h = [[h1, h2, h1], [h2, -4/(alpha+1), h2], [h1, h2, h1]]\r\n    h = np.array(h)\r\n    return h\r\n\r\n\r\ndef fspecial_log(hsize, sigma):\r\n    raise(NotImplemented)\r\n\r\n\r\ndef fspecial_motion(motion_len, theta):\r\n    raise(NotImplemented)\r\n\r\n\r\ndef fspecial_prewitt():\r\n    return np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]])\r\n\r\n\r\ndef fspecial_sobel():\r\n    return np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\r\n\r\n\r\ndef fspecial(filter_type, *args, **kwargs):\r\n    \'\'\'\r\n    python code from:\r\n    https://github.com/ronaldosena/imagens-medicas-2/blob/40171a6c259edec7827a6693a93955de2bd39e76/Aulas/aula_2_-_uniform_filter/matlab_fspecial.py\r\n    \'\'\'\r\n    if filter_type == \'average\':\r\n        return fspecial_average(*args, **kwargs)\r\n    if filter_type == \'disk\':\r\n        return fspecial_disk(*args, **kwargs)\r\n    if filter_type == \'gaussian\':\r\n        return fspecial_gaussian(*args, **kwargs)\r\n    if filter_type == \'laplacian\':\r\n        return fspecial_laplacian(*args, **kwargs)\r\n    if filter_type == \'log\':\r\n        return fspecial_log(*args, **kwargs)\r\n    if filter_type == \'motion\':\r\n        return fspecial_motion(*args, **kwargs)\r\n    if filter_type == \'prewitt\':\r\n        return fspecial_prewitt(*args, **kwargs)\r\n    if filter_type == \'sobel\':\r\n        return fspecial_sobel(*args, **kwargs)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    a = opt_fft_size([111])\r\n    print(a)\r\n\r\n    print(fspecial(\'gaussian\', 5, 1))'"
utils/utils_image.py,31,"b'import os\nimport math\nimport random\nimport numpy as np\nimport torch\nimport cv2\nfrom torchvision.utils import make_grid\nfrom datetime import datetime\n# import torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\n\'\'\'\n# --------------------------------------------\n# Kai Zhang (github: https://github.com/cszn)\n# 03/Mar/2019\n# --------------------------------------------\n# https://github.com/twhui/SRGAN-pyTorch\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\nIMG_EXTENSIONS = [\'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\', \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\', \'.tif\']\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef get_timestamp():\n    return datetime.now().strftime(\'%y%m%d-%H%M%S\')\n\n\ndef imshow(x, title=None, cbar=False, figsize=None):\n    plt.figure(figsize=figsize)\n    plt.imshow(np.squeeze(x), interpolation=\'nearest\', cmap=\'gray\')\n    if title:\n        plt.title(title)\n    if cbar:\n        plt.colorbar()\n    plt.show()\n\n\ndef surf(Z, cmap=\'rainbow\', figsize=None):\n    plt.figure(figsize=figsize)\n    ax3 = plt.axes(projection=\'3d\')\n\n    w, h = Z.shape[:2]\n    xx = np.arange(0,w,1)\n    yy = np.arange(0,h,1)\n    X, Y = np.meshgrid(xx, yy)\n    ax3.plot_surface(X,Y,Z,cmap=cmap)\n    #ax3.contour(X,Y,Z, zdim=\'z\',offset=-2\xef\xbc\x8ccmap=cmap)\n    plt.show()\n\n\n\'\'\'\n# --------------------------------------------\n# get image pathes\n# --------------------------------------------\n\'\'\'\n\n\ndef get_image_paths(dataroot):\n    paths = None  # return None if dataroot is None\n    if dataroot is not None:\n        paths = sorted(_get_paths_from_images(dataroot))\n    return paths\n\n\ndef _get_paths_from_images(path):\n    assert os.path.isdir(path), \'{:s} is not a valid directory\'.format(path)\n    images = []\n    for dirpath, _, fnames in sorted(os.walk(path)):\n        for fname in sorted(fnames):\n            if is_image_file(fname):\n                img_path = os.path.join(dirpath, fname)\n                images.append(img_path)\n    assert images, \'{:s} has no valid image file\'.format(path)\n    return images\n\n\n\'\'\'\n# --------------------------------------------\n# split large images into small images \n# --------------------------------------------\n\'\'\'\n\n\ndef patches_from_image(img, p_size=512, p_overlap=64, p_max=800):\n    w, h = img.shape[:2]\n    patches = []\n    if w > p_max and h > p_max:\n        w1 = list(np.arange(0, w-p_size, p_size-p_overlap, dtype=np.int))\n        h1 = list(np.arange(0, h-p_size, p_size-p_overlap, dtype=np.int))\n        w1.append(w-p_size)\n        h1.append(h-p_size)\n        # print(w1)\n        # print(h1)\n        for i in w1:\n            for j in h1:\n                patches.append(img[i:i+p_size, j:j+p_size,:])\n    else:\n        patches.append(img)\n\n    return patches\n\n\ndef imssave(imgs, img_path):\n    """"""\n    imgs: list, N images of size WxHxC\n    """"""\n    img_name, ext = os.path.splitext(os.path.basename(img_path))\n    for i, img in enumerate(imgs):\n        if img.ndim == 3:\n            img = img[:, :, [2, 1, 0]]\n        new_path = os.path.join(os.path.dirname(img_path), img_name+str(\'_{:04d}\'.format(i))+\'.png\')\n        cv2.imwrite(new_path, img)\n\n\ndef split_imageset(original_dataroot, taget_dataroot, n_channels=3, p_size=512, p_overlap=96, p_max=800):\n    """"""\n    split the large images from original_dataroot into small overlapped images with size (p_size)x(p_size), \n    and save them into taget_dataroot; only the images with larger size than (p_max)x(p_max)\n    will be splitted.\n\n    Args:\n        original_dataroot:\n        taget_dataroot:\n        p_size: size of small images\n        p_overlap: patch size in training is a good choice\n        p_max: images with smaller size than (p_max)x(p_max) keep unchanged.\n    """"""\n    paths = get_image_paths(original_dataroot)\n    for img_path in paths:\n        # img_name, ext = os.path.splitext(os.path.basename(img_path))\n        img = imread_uint(img_path, n_channels=n_channels)\n        patches = patches_from_image(img, p_size, p_overlap, p_max)\n        imssave(patches, os.path.join(taget_dataroot, os.path.basename(img_path)))\n        #if original_dataroot == taget_dataroot:\n        #del img_path\n\n\'\'\'\n# --------------------------------------------\n# makedir\n# --------------------------------------------\n\'\'\'\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef mkdirs(paths):\n    if isinstance(paths, str):\n        mkdir(paths)\n    else:\n        for path in paths:\n            mkdir(path)\n\n\ndef mkdir_and_rename(path):\n    if os.path.exists(path):\n        new_name = path + \'_archived_\' + get_timestamp()\n        print(\'Path already exists. Rename it to [{:s}]\'.format(new_name))\n        os.rename(path, new_name)\n    os.makedirs(path)\n\n\n\'\'\'\n# --------------------------------------------\n# read image from path\n# opencv is fast, but read BGR numpy image\n# --------------------------------------------\n\'\'\'\n\n\n# --------------------------------------------\n# get uint8 image of size HxWxn_channles (RGB)\n# --------------------------------------------\ndef imread_uint(path, n_channels=3):\n    #  input: path\n    # output: HxWx3(RGB or GGG), or HxWx1 (G)\n    if n_channels == 1:\n        img = cv2.imread(path, 0)  # cv2.IMREAD_GRAYSCALE\n        img = np.expand_dims(img, axis=2)  # HxWx1\n    elif n_channels == 3:\n        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  # BGR or G\n        if img.ndim == 2:\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # GGG\n        else:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # RGB\n    return img\n\n\n# --------------------------------------------\n# matlab\'s imwrite\n# --------------------------------------------\ndef imsave(img, img_path):\n    img = np.squeeze(img)\n    if img.ndim == 3:\n        img = img[:, :, [2, 1, 0]]\n    cv2.imwrite(img_path, img)\n\ndef imwrite(img, img_path):\n    img = np.squeeze(img)\n    if img.ndim == 3:\n        img = img[:, :, [2, 1, 0]]\n    cv2.imwrite(img_path, img)\n\n\n\n# --------------------------------------------\n# get single image of size HxWxn_channles (BGR)\n# --------------------------------------------\ndef read_img(path):\n    # read image by cv2\n    # return: Numpy float32, HWC, BGR, [0,1]\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)  # cv2.IMREAD_GRAYSCALE\n    img = img.astype(np.float32) / 255.\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    # some images have 4 channels\n    if img.shape[2] > 3:\n        img = img[:, :, :3]\n    return img\n\n\n\'\'\'\n# --------------------------------------------\n# image format conversion\n# --------------------------------------------\n# numpy(single) <--->  numpy(unit)\n# numpy(single) <--->  tensor\n# numpy(unit)   <--->  tensor\n# --------------------------------------------\n\'\'\'\n\n\n# --------------------------------------------\n# numpy(single) [0, 1] <--->  numpy(unit)\n# --------------------------------------------\n\n\ndef uint2single(img):\n\n    return np.float32(img/255.)\n\n\ndef single2uint(img):\n\n    return np.uint8((img.clip(0, 1)*255.).round())\n\n\ndef uint162single(img):\n\n    return np.float32(img/65535.)\n\n\ndef single2uint16(img):\n\n    return np.uint16((img.clip(0, 1)*65535.).round())\n\n\n# --------------------------------------------\n# numpy(unit) (HxWxC or HxW) <--->  tensor\n# --------------------------------------------\n\n\n# convert uint to 4-dimensional torch tensor\ndef uint2tensor4(img):\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255.).unsqueeze(0)\n\n\n# convert uint to 3-dimensional torch tensor\ndef uint2tensor3(img):\n    if img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().div(255.)\n\n\n# convert 2/3/4-dimensional torch tensor to uint\ndef tensor2uint(img):\n    img = img.data.squeeze().float().clamp_(0, 1).cpu().numpy()\n    if img.ndim == 3:\n        img = np.transpose(img, (1, 2, 0))\n    return np.uint8((img*255.0).round())\n\n\n# --------------------------------------------\n# numpy(single) (HxWxC) <--->  tensor\n# --------------------------------------------\n\n\n# convert single (HxWxC) to 3-dimensional torch tensor\ndef single2tensor3(img):\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float()\n\n\n# convert single (HxWxC) to 4-dimensional torch tensor\ndef single2tensor4(img):\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1).float().unsqueeze(0)\n\n\n# convert torch tensor to single\ndef tensor2single(img):\n    img = img.data.squeeze().float().cpu().numpy()\n    if img.ndim == 3:\n        img = np.transpose(img, (1, 2, 0))\n\n    return img\n\n# convert torch tensor to single\ndef tensor2single3(img):\n    img = img.data.squeeze().float().cpu().numpy()\n    if img.ndim == 3:\n        img = np.transpose(img, (1, 2, 0))\n    elif img.ndim == 2:\n        img = np.expand_dims(img, axis=2)\n    return img\n\n\ndef single2tensor5(img):\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1, 3).float().unsqueeze(0)\n\n\ndef single32tensor5(img):\n    return torch.from_numpy(np.ascontiguousarray(img)).float().unsqueeze(0).unsqueeze(0)\n\n\ndef single42tensor4(img):\n    return torch.from_numpy(np.ascontiguousarray(img)).permute(2, 0, 1, 3).float()\n\n\n# from skimage.io import imread, imsave\ndef tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):\n    \'\'\'\n    Converts a torch Tensor into an image Numpy array of BGR channel order\n    Input: 4D(B,(3/1),H,W), 3D(C,H,W), or 2D(H,W), any range, RGB channel order\n    Output: 3D(H,W,C) or 2D(H,W), [0,255], np.uint8 (default)\n    \'\'\'\n    tensor = tensor.squeeze().float().cpu().clamp_(*min_max)  # squeeze first, then clamp\n    tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])  # to range [0,1]\n    n_dim = tensor.dim()\n    if n_dim == 4:\n        n_img = len(tensor)\n        img_np = make_grid(tensor, nrow=int(math.sqrt(n_img)), normalize=False).numpy()\n        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n    elif n_dim == 3:\n        img_np = tensor.numpy()\n        img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))  # HWC, BGR\n    elif n_dim == 2:\n        img_np = tensor.numpy()\n    else:\n        raise TypeError(\n            \'Only support 4D, 3D and 2D tensor. But received with dimension: {:d}\'.format(n_dim))\n    if out_type == np.uint8:\n        img_np = (img_np * 255.0).round()\n        # Important. Unlike matlab, numpy.unit8() WILL NOT round by default.\n    return img_np.astype(out_type)\n\n\n\'\'\'\n# --------------------------------------------\n# Augmentation, flipe and/or rotate\n# --------------------------------------------\n# The following two are enough.\n# (1) augmet_img: numpy image of WxHxC or WxH\n# (2) augment_img_tensor4: tensor image 1xCxWxH\n# --------------------------------------------\n\'\'\'\n\n\ndef augment_img(img, mode=0):\n    \'\'\'Kai Zhang (github: https://github.com/cszn)\n    \'\'\'\n    if mode == 0:\n        return img\n    elif mode == 1:\n        return np.flipud(np.rot90(img))\n    elif mode == 2:\n        return np.flipud(img)\n    elif mode == 3:\n        return np.rot90(img, k=3)\n    elif mode == 4:\n        return np.flipud(np.rot90(img, k=2))\n    elif mode == 5:\n        return np.rot90(img)\n    elif mode == 6:\n        return np.rot90(img, k=2)\n    elif mode == 7:\n        return np.flipud(np.rot90(img, k=3))\n\n\ndef augment_img_tensor4(img, mode=0):\n    \'\'\'Kai Zhang (github: https://github.com/cszn)\n    \'\'\'\n    if mode == 0:\n        return img\n    elif mode == 1:\n        return img.rot90(1, [2, 3]).flip([2])\n    elif mode == 2:\n        return img.flip([2])\n    elif mode == 3:\n        return img.rot90(3, [2, 3])\n    elif mode == 4:\n        return img.rot90(2, [2, 3]).flip([2])\n    elif mode == 5:\n        return img.rot90(1, [2, 3])\n    elif mode == 6:\n        return img.rot90(2, [2, 3])\n    elif mode == 7:\n        return img.rot90(3, [2, 3]).flip([2])\n\n\ndef augment_img_tensor(img, mode=0):\n    \'\'\'Kai Zhang (github: https://github.com/cszn)\n    \'\'\'\n    img_size = img.size()\n    img_np = img.data.cpu().numpy()\n    if len(img_size) == 3:\n        img_np = np.transpose(img_np, (1, 2, 0))\n    elif len(img_size) == 4:\n        img_np = np.transpose(img_np, (2, 3, 1, 0))\n    img_np = augment_img(img_np, mode=mode)\n    img_tensor = torch.from_numpy(np.ascontiguousarray(img_np))\n    if len(img_size) == 3:\n        img_tensor = img_tensor.permute(2, 0, 1)\n    elif len(img_size) == 4:\n        img_tensor = img_tensor.permute(3, 2, 0, 1)\n\n    return img_tensor.type_as(img)\n\n\ndef augment_img_np3(img, mode=0):\n    if mode == 0:\n        return img\n    elif mode == 1:\n        return img.transpose(1, 0, 2)\n    elif mode == 2:\n        return img[::-1, :, :]\n    elif mode == 3:\n        img = img[::-1, :, :]\n        img = img.transpose(1, 0, 2)\n        return img\n    elif mode == 4:\n        return img[:, ::-1, :]\n    elif mode == 5:\n        img = img[:, ::-1, :]\n        img = img.transpose(1, 0, 2)\n        return img\n    elif mode == 6:\n        img = img[:, ::-1, :]\n        img = img[::-1, :, :]\n        return img\n    elif mode == 7:\n        img = img[:, ::-1, :]\n        img = img[::-1, :, :]\n        img = img.transpose(1, 0, 2)\n        return img\n\n\ndef augment_imgs(img_list, hflip=True, rot=True):\n    # horizontal flip OR rotate\n    hflip = hflip and random.random() < 0.5\n    vflip = rot and random.random() < 0.5\n    rot90 = rot and random.random() < 0.5\n\n    def _augment(img):\n        if hflip:\n            img = img[:, ::-1, :]\n        if vflip:\n            img = img[::-1, :, :]\n        if rot90:\n            img = img.transpose(1, 0, 2)\n        return img\n\n    return [_augment(img) for img in img_list]\n\n\n\'\'\'\n# --------------------------------------------\n# modcrop and shave\n# --------------------------------------------\n\'\'\'\n\n\ndef modcrop(img_in, scale):\n    # img_in: Numpy, HWC or HW\n    img = np.copy(img_in)\n    if img.ndim == 2:\n        H, W = img.shape\n        H_r, W_r = H % scale, W % scale\n        img = img[:H - H_r, :W - W_r]\n    elif img.ndim == 3:\n        H, W, C = img.shape\n        H_r, W_r = H % scale, W % scale\n        img = img[:H - H_r, :W - W_r, :]\n    else:\n        raise ValueError(\'Wrong img ndim: [{:d}].\'.format(img.ndim))\n    return img\n\n\ndef shave(img_in, border=0):\n    # img_in: Numpy, HWC or HW\n    img = np.copy(img_in)\n    h, w = img.shape[:2]\n    img = img[border:h-border, border:w-border]\n    return img\n\n\n\'\'\'\n# --------------------------------------------\n# image processing process on numpy image\n# channel_convert(in_c, tar_type, img_list):\n# rgb2ycbcr(img, only_y=True):\n# bgr2ycbcr(img, only_y=True):\n# ycbcr2rgb(img):\n# --------------------------------------------\n\'\'\'\n\n\ndef rgb2ycbcr(img, only_y=True):\n    \'\'\'same as matlab rgb2ycbcr\n    only_y: only return Y channel\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    \'\'\'\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    if only_y:\n        rlt = np.dot(img, [65.481, 128.553, 24.966]) / 255.0 + 16.0\n    else:\n        rlt = np.matmul(img, [[65.481, -37.797, 112.0], [128.553, -74.203, -93.786],\n                              [24.966, 112.0, -18.214]]) / 255.0 + [16, 128, 128]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef ycbcr2rgb(img):\n    \'\'\'same as matlab ycbcr2rgb\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    \'\'\'\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    rlt = np.matmul(img, [[0.00456621, 0.00456621, 0.00456621], [0, -0.00153632, 0.00791071],\n                          [0.00625893, -0.00318811, 0]]) * 255.0 + [-222.921, 135.576, -276.836]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef bgr2ycbcr(img, only_y=True):\n    \'\'\'bgr version of rgb2ycbcr\n    only_y: only return Y channel\n    Input:\n        uint8, [0, 255]\n        float, [0, 1]\n    \'\'\'\n    in_img_type = img.dtype\n    img.astype(np.float32)\n    if in_img_type != np.uint8:\n        img *= 255.\n    # convert\n    if only_y:\n        rlt = np.dot(img, [24.966, 128.553, 65.481]) / 255.0 + 16.0\n    else:\n        rlt = np.matmul(img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786],\n                              [65.481, -37.797, 112.0]]) / 255.0 + [16, 128, 128]\n    if in_img_type == np.uint8:\n        rlt = rlt.round()\n    else:\n        rlt /= 255.\n    return rlt.astype(in_img_type)\n\n\ndef channel_convert(in_c, tar_type, img_list):\n    # conversion among BGR, gray and y\n    if in_c == 3 and tar_type == \'gray\':  # BGR to gray\n        gray_list = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in img_list]\n        return [np.expand_dims(img, axis=2) for img in gray_list]\n    elif in_c == 3 and tar_type == \'y\':  # BGR to y\n        y_list = [bgr2ycbcr(img, only_y=True) for img in img_list]\n        return [np.expand_dims(img, axis=2) for img in y_list]\n    elif in_c == 1 and tar_type == \'RGB\':  # gray/y to BGR\n        return [cv2.cvtColor(img, cv2.COLOR_GRAY2BGR) for img in img_list]\n    else:\n        return img_list\n\n\n\'\'\'\n# --------------------------------------------\n# metric, PSNR and SSIM\n# --------------------------------------------\n\'\'\'\n\n\n# --------------------------------------------\n# PSNR\n# --------------------------------------------\ndef calculate_psnr(img1, img2, border=0):\n    # img1 and img2 have range [0, 255]\n    #img1 = img1.squeeze()\n    #img2 = img2.squeeze()\n    if not img1.shape == img2.shape:\n        raise ValueError(\'Input images must have the same dimensions.\')\n    h, w = img1.shape[:2]\n    img1 = img1[border:h-border, border:w-border]\n    img2 = img2[border:h-border, border:w-border]\n\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    mse = np.mean((img1 - img2)**2)\n    if mse == 0:\n        return float(\'inf\')\n    return 20 * math.log10(255.0 / math.sqrt(mse))\n\n\n# --------------------------------------------\n# SSIM\n# --------------------------------------------\ndef calculate_ssim(img1, img2, border=0):\n    \'\'\'calculate SSIM\n    the same outputs as MATLAB\'s\n    img1, img2: [0, 255]\n    \'\'\'\n    #img1 = img1.squeeze()\n    #img2 = img2.squeeze()\n    if not img1.shape == img2.shape:\n        raise ValueError(\'Input images must have the same dimensions.\')\n    h, w = img1.shape[:2]\n    img1 = img1[border:h-border, border:w-border]\n    img2 = img2[border:h-border, border:w-border]\n\n    if img1.ndim == 2:\n        return ssim(img1, img2)\n    elif img1.ndim == 3:\n        if img1.shape[2] == 3:\n            ssims = []\n            for i in range(3):\n                ssims.append(ssim(img1[:,:,i], img2[:,:,i]))\n            return np.array(ssims).mean()\n        elif img1.shape[2] == 1:\n            return ssim(np.squeeze(img1), np.squeeze(img2))\n    else:\n        raise ValueError(\'Wrong input image dimensions.\')\n\n\ndef ssim(img1, img2):\n    C1 = (0.01 * 255)**2\n    C2 = (0.03 * 255)**2\n\n    img1 = img1.astype(np.float64)\n    img2 = img2.astype(np.float64)\n    kernel = cv2.getGaussianKernel(11, 1.5)\n    window = np.outer(kernel, kernel.transpose())\n\n    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid\n    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n    mu1_sq = mu1**2\n    mu2_sq = mu2**2\n    mu1_mu2 = mu1 * mu2\n    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n                                                            (sigma1_sq + sigma2_sq + C2))\n    return ssim_map.mean()\n\n\n\'\'\'\n# --------------------------------------------\n# matlab\'s bicubic imresize (numpy and torch) [0, 1]\n# --------------------------------------------\n\'\'\'\n\n\n# matlab \'imresize\' function, now only support \'bicubic\'\ndef cubic(x):\n    absx = torch.abs(x)\n    absx2 = absx**2\n    absx3 = absx**3\n    return (1.5*absx3 - 2.5*absx2 + 1) * ((absx <= 1).type_as(absx)) + \\\n        (-0.5*absx3 + 2.5*absx2 - 4*absx + 2) * (((absx > 1)*(absx <= 2)).type_as(absx))\n\n\ndef calculate_weights_indices(in_length, out_length, scale, kernel, kernel_width, antialiasing):\n    if (scale < 1) and (antialiasing):\n        # Use a modified kernel to simultaneously interpolate and antialias- larger kernel width\n        kernel_width = kernel_width / scale\n\n    # Output-space coordinates\n    x = torch.linspace(1, out_length, out_length)\n\n    # Input-space coordinates. Calculate the inverse mapping such that 0.5\n    # in output space maps to 0.5 in input space, and 0.5+scale in output\n    # space maps to 1.5 in input space.\n    u = x / scale + 0.5 * (1 - 1 / scale)\n\n    # What is the left-most pixel that can be involved in the computation?\n    left = torch.floor(u - kernel_width / 2)\n\n    # What is the maximum number of pixels that can be involved in the\n    # computation?  Note: it\'s OK to use an extra pixel here; if the\n    # corresponding weights are all zero, it will be eliminated at the end\n    # of this function.\n    P = math.ceil(kernel_width) + 2\n\n    # The indices of the input pixels involved in computing the k-th output\n    # pixel are in row k of the indices matrix.\n    indices = left.view(out_length, 1).expand(out_length, P) + torch.linspace(0, P - 1, P).view(\n        1, P).expand(out_length, P)\n\n    # The weights used to compute the k-th output pixel are in row k of the\n    # weights matrix.\n    distance_to_center = u.view(out_length, 1).expand(out_length, P) - indices\n    # apply cubic kernel\n    if (scale < 1) and (antialiasing):\n        weights = scale * cubic(distance_to_center * scale)\n    else:\n        weights = cubic(distance_to_center)\n    # Normalize the weights matrix so that each row sums to 1.\n    weights_sum = torch.sum(weights, 1).view(out_length, 1)\n    weights = weights / weights_sum.expand(out_length, P)\n\n    # If a column in weights is all zero, get rid of it. only consider the first and last column.\n    weights_zero_tmp = torch.sum((weights == 0), 0)\n    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):\n        indices = indices.narrow(1, 1, P - 2)\n        weights = weights.narrow(1, 1, P - 2)\n    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):\n        indices = indices.narrow(1, 0, P - 2)\n        weights = weights.narrow(1, 0, P - 2)\n    weights = weights.contiguous()\n    indices = indices.contiguous()\n    sym_len_s = -indices.min() + 1\n    sym_len_e = indices.max() - in_length\n    indices = indices + sym_len_s - 1\n    return weights, indices, int(sym_len_s), int(sym_len_e)\n\n\n# --------------------------------------------\n# imresize for tensor image [0, 1]\n# --------------------------------------------\ndef imresize(img, scale, antialiasing=True):\n    # Now the scale should be the same for H and W\n    # input: img: pytorch tensor, CHW or HW [0,1]\n    # output: CHW or HW [0,1] w/o round\n    need_squeeze = True if img.dim() == 2 else False\n    if need_squeeze:\n        img.unsqueeze_(0)\n    in_C, in_H, in_W = img.size()\n    out_C, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n    kernel_width = 4\n    kernel = \'cubic\'\n\n    # Return the desired dimension order for performing the resize.  The\n    # strategy is to perform the resize first along the dimension with the\n    # smallest scale factor.\n    # Now we do not support this.\n\n    # get weights and indices\n    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n    # process H dimension\n    # symmetric copying\n    img_aug = torch.FloatTensor(in_C, in_H + sym_len_Hs + sym_len_He, in_W)\n    img_aug.narrow(1, sym_len_Hs, in_H).copy_(img)\n\n    sym_patch = img[:, :sym_len_Hs, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    img_aug.narrow(1, 0, sym_len_Hs).copy_(sym_patch_inv)\n\n    sym_patch = img[:, -sym_len_He:, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    img_aug.narrow(1, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n\n    out_1 = torch.FloatTensor(in_C, out_H, in_W)\n    kernel_width = weights_H.size(1)\n    for i in range(out_H):\n        idx = int(indices_H[i][0])\n        for j in range(out_C):\n            out_1[j, i, :] = img_aug[j, idx:idx + kernel_width, :].transpose(0, 1).mv(weights_H[i])\n\n    # process W dimension\n    # symmetric copying\n    out_1_aug = torch.FloatTensor(in_C, out_H, in_W + sym_len_Ws + sym_len_We)\n    out_1_aug.narrow(2, sym_len_Ws, in_W).copy_(out_1)\n\n    sym_patch = out_1[:, :, :sym_len_Ws]\n    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n    out_1_aug.narrow(2, 0, sym_len_Ws).copy_(sym_patch_inv)\n\n    sym_patch = out_1[:, :, -sym_len_We:]\n    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n    out_1_aug.narrow(2, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n\n    out_2 = torch.FloatTensor(in_C, out_H, out_W)\n    kernel_width = weights_W.size(1)\n    for i in range(out_W):\n        idx = int(indices_W[i][0])\n        for j in range(out_C):\n            out_2[j, :, i] = out_1_aug[j, :, idx:idx + kernel_width].mv(weights_W[i])\n    if need_squeeze:\n        out_2.squeeze_()\n    return out_2\n\n\n# --------------------------------------------\n# imresize for numpy image [0, 1]\n# --------------------------------------------\ndef imresize_np(img, scale, antialiasing=True):\n    # Now the scale should be the same for H and W\n    # input: img: Numpy, HWC or HW [0,1]\n    # output: HWC or HW [0,1] w/o round\n    img = torch.from_numpy(img)\n    need_squeeze = True if img.dim() == 2 else False\n    if need_squeeze:\n        img.unsqueeze_(2)\n\n    in_H, in_W, in_C = img.size()\n    out_C, out_H, out_W = in_C, math.ceil(in_H * scale), math.ceil(in_W * scale)\n    kernel_width = 4\n    kernel = \'cubic\'\n\n    # Return the desired dimension order for performing the resize.  The\n    # strategy is to perform the resize first along the dimension with the\n    # smallest scale factor.\n    # Now we do not support this.\n\n    # get weights and indices\n    weights_H, indices_H, sym_len_Hs, sym_len_He = calculate_weights_indices(\n        in_H, out_H, scale, kernel, kernel_width, antialiasing)\n    weights_W, indices_W, sym_len_Ws, sym_len_We = calculate_weights_indices(\n        in_W, out_W, scale, kernel, kernel_width, antialiasing)\n    # process H dimension\n    # symmetric copying\n    img_aug = torch.FloatTensor(in_H + sym_len_Hs + sym_len_He, in_W, in_C)\n    img_aug.narrow(0, sym_len_Hs, in_H).copy_(img)\n\n    sym_patch = img[:sym_len_Hs, :, :]\n    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n    img_aug.narrow(0, 0, sym_len_Hs).copy_(sym_patch_inv)\n\n    sym_patch = img[-sym_len_He:, :, :]\n    inv_idx = torch.arange(sym_patch.size(0) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(0, inv_idx)\n    img_aug.narrow(0, sym_len_Hs + in_H, sym_len_He).copy_(sym_patch_inv)\n\n    out_1 = torch.FloatTensor(out_H, in_W, in_C)\n    kernel_width = weights_H.size(1)\n    for i in range(out_H):\n        idx = int(indices_H[i][0])\n        for j in range(out_C):\n            out_1[i, :, j] = img_aug[idx:idx + kernel_width, :, j].transpose(0, 1).mv(weights_H[i])\n\n    # process W dimension\n    # symmetric copying\n    out_1_aug = torch.FloatTensor(out_H, in_W + sym_len_Ws + sym_len_We, in_C)\n    out_1_aug.narrow(1, sym_len_Ws, in_W).copy_(out_1)\n\n    sym_patch = out_1[:, :sym_len_Ws, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    out_1_aug.narrow(1, 0, sym_len_Ws).copy_(sym_patch_inv)\n\n    sym_patch = out_1[:, -sym_len_We:, :]\n    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n    out_1_aug.narrow(1, sym_len_Ws + in_W, sym_len_We).copy_(sym_patch_inv)\n\n    out_2 = torch.FloatTensor(out_H, out_W, in_C)\n    kernel_width = weights_W.size(1)\n    for i in range(out_W):\n        idx = int(indices_W[i][0])\n        for j in range(out_C):\n            out_2[:, i, j] = out_1_aug[:, idx:idx + kernel_width, j].mv(weights_W[i])\n    if need_squeeze:\n        out_2.squeeze_()\n\n    return out_2.numpy()\n\n\nif __name__ == \'__main__\':\n    img = imread_uint(\'test.bmp\', 3)\n#    img = uint2single(img)\n#    img_bicubic = imresize_np(img, 1/4)\n#    imshow(single2uint(img_bicubic))\n#\n#    img_tensor = single2tensor4(img)\n#    for i in range(8):\n#        imshow(np.concatenate((augment_img(img, i), tensor2single(augment_img_tensor4(img_tensor, i))), 1))\n    \n#    patches = patches_from_image(img, p_size=128, p_overlap=0, p_max=200)\n#    imssave(patches,\'a.png\')\n\n\n    \n    \n    \n    \n    \n'"
utils/utils_logger.py,0,"b'import sys\nimport datetime\nimport logging\n\n\n\'\'\'\n# --------------------------------------------\n# Kai Zhang (github: https://github.com/cszn)\n# 03/Mar/2019\n# --------------------------------------------\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\ndef log(*args, **kwargs):\n    print(datetime.datetime.now().strftime(""%Y-%m-%d %H:%M:%S:""), *args, **kwargs)\n\n\n\'\'\'\n# --------------------------------------------\n# logger\n# --------------------------------------------\n\'\'\'\n\n\ndef logger_info(logger_name, log_path=\'default_logger.log\'):\n    \'\'\' set up logger\n    modified by Kai Zhang (github: https://github.com/cszn)\n    \'\'\'\n    log = logging.getLogger(logger_name)\n    if log.hasHandlers():\n        print(\'LogHandlers exist!\')\n    else:\n        print(\'LogHandlers setup!\')\n        level = logging.INFO\n        formatter = logging.Formatter(\'%(asctime)s.%(msecs)03d : %(message)s\', datefmt=\'%y-%m-%d %H:%M:%S\')\n        fh = logging.FileHandler(log_path, mode=\'a\')\n        fh.setFormatter(formatter)\n        log.setLevel(level)\n        log.addHandler(fh)\n        # print(len(log.handlers))\n\n        sh = logging.StreamHandler()\n        sh.setFormatter(formatter)\n        log.addHandler(sh)\n\n\n\'\'\'\n# --------------------------------------------\n# print to file and std_out simultaneously\n# --------------------------------------------\n\'\'\'\n\n\nclass logger_print(object):\n    def __init__(self, log_path=""default.log""):\n        self.terminal = sys.stdout\n        self.log = open(log_path, \'a\')\n\n    def write(self, message):\n        self.terminal.write(message)\n        self.log.write(message)  # write the message\n\n    def flush(self):\n        pass\n'"
utils/utils_matconvnet.py,9,"b'# -*- coding: utf-8 -*-\r\nimport numpy as np\r\nimport torch\r\nfrom collections import OrderedDict\r\n\r\n# import scipy.io as io\r\nimport hdf5storage\r\n\r\n""""""\r\n# --------------------------------------------\r\n# Convert matconvnet SimpleNN model into pytorch model\r\n# --------------------------------------------\r\n# Kai Zhang (cskaizhang@gmail.com)\r\n# https://github.com/cszn\r\n# 28/Nov/2019\r\n# --------------------------------------------\r\n""""""\r\n\r\n\r\ndef weights2tensor(x, squeeze=False, in_features=None, out_features=None):\r\n    """"""Modified version of https://github.com/albanie/pytorch-mcn\r\n    Adjust memory layout and load weights as torch tensor\r\n    Args:\r\n        x (ndaray): a numpy array, corresponding to a set of network weights\r\n           stored in column major order\r\n        squeeze (bool) [False]: whether to squeeze the tensor (i.e. remove\r\n           singletons from the trailing dimensions. So after converting to\r\n           pytorch layout (C_out, C_in, H, W), if the shape is (A, B, 1, 1)\r\n           it will be reshaped to a matrix with shape (A,B).\r\n        in_features (int :: None): used to reshape weights for a linear block.\r\n        out_features (int :: None): used to reshape weights for a linear block.\r\n    Returns:\r\n        torch.tensor: a permuted sets of weights, matching the pytorch layout\r\n        convention\r\n    """"""\r\n    if x.ndim == 4:\r\n        x = x.transpose((3, 2, 0, 1))\r\n# for FFDNet, pixel-shuffle layer\r\n#        if x.shape[1]==13:\r\n#            x=x[:,[0,2,1,3,  4,6,5,7, 8,10,9,11, 12],:,:]\r\n#        if x.shape[0]==12:   \r\n#            x=x[[0,2,1,3,  4,6,5,7, 8,10,9,11],:,:,:]\r\n#        if x.shape[1]==5:\r\n#            x=x[:,[0,2,1,3,  4],:,:]\r\n#        if x.shape[0]==4:   \r\n#            x=x[[0,2,1,3],:,:,:]\r\n## for SRMD, pixel-shuffle layer\r\n#        if x.shape[0]==12:   \r\n#            x=x[[0,2,1,3,  4,6,5,7, 8,10,9,11],:,:,:]\r\n#        if x.shape[0]==27:\r\n#            x=x[[0,3,6,1,4,7,2,5,8, 0+9,3+9,6+9,1+9,4+9,7+9,2+9,5+9,8+9, 0+18,3+18,6+18,1+18,4+18,7+18,2+18,5+18,8+18],:,:,:]\r\n#        if x.shape[0]==48:   \r\n#            x=x[[0,4,8,12,1,5,9,13,2,6,10,14,3,7,11,15,  0+16,4+16,8+16,12+16,1+16,5+16,9+16,13+16,2+16,6+16,10+16,14+16,3+16,7+16,11+16,15+16,  0+32,4+32,8+32,12+32,1+32,5+32,9+32,13+32,2+32,6+32,10+32,14+32,3+32,7+32,11+32,15+32],:,:,:]\r\n\r\n    elif x.ndim == 3:  # add by Kai\r\n        x = x[:,:,:,None]\r\n        x = x.transpose((3, 2, 0, 1))\r\n    elif x.ndim == 2:\r\n        if x.shape[1] == 1:\r\n            x = x.flatten()\r\n    if squeeze:\r\n        if in_features and out_features:\r\n            x = x.reshape((out_features, in_features))\r\n        x = np.squeeze(x)\r\n    return torch.from_numpy(np.ascontiguousarray(x))\r\n\r\n\r\ndef save_model(network, save_path):\r\n    state_dict = network.state_dict()\r\n    for key, param in state_dict.items():\r\n        state_dict[key] = param.cpu()\r\n    torch.save(state_dict, save_path)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    \r\n    \r\n#    from utils import utils_logger\r\n#    import logging\r\n#    utils_logger.logger_info(\'a\', \'a.log\')\r\n#    logger = logging.getLogger(\'a\')\r\n#    \r\n    # mcn = hdf5storage.loadmat(\'/model_zoo/matfile/FFDNet_Clip_gray.mat\')\r\n    mcn = hdf5storage.loadmat(\'models/modelcolor.mat\')\r\n    \r\n    \r\n    #logger.info(mcn[\'CNNdenoiser\'][0][0][0][1][0][0][0][0])\r\n\r\n    mat_net = OrderedDict()\r\n    for idx in range(25):\r\n        mat_net[str(idx)] = OrderedDict()\r\n        count = -1\r\n        \r\n        print(idx)\r\n        for i in range(13):\r\n            \r\n            if mcn[\'CNNdenoiser\'][0][idx][0][i][0][0][0][0] == \'conv\':\r\n                \r\n                count += 1\r\n                w = mcn[\'CNNdenoiser\'][0][idx][0][i][0][1][0][0]\r\n               # print(w.shape)\r\n                w = weights2tensor(w)\r\n               # print(w.shape)\r\n                \r\n                b = mcn[\'CNNdenoiser\'][0][idx][0][i][0][1][0][1]\r\n                b = weights2tensor(b)\r\n                print(b.shape)\r\n\r\n                mat_net[str(idx)][\'model.{:d}.weight\'.format(count*2)] = w\r\n                mat_net[str(idx)][\'model.{:d}.bias\'.format(count*2)] = b\r\n\r\n    torch.save(mat_net, \'model_zoo/modelcolor.pth\')\r\n   \r\n\r\n\r\n#    from models.network_dncnn import IRCNN as net\r\n#    network = net(in_nc=3, out_nc=3, nc=64)\r\n#    state_dict = network.state_dict()\r\n#\r\n#    #show_kv(state_dict)\r\n#\r\n#    for i in range(len(mcn[\'net\'][0][0][0])):\r\n#        print(mcn[\'net\'][0][0][0][i][0][0][0][0])\r\n#\r\n#    count = -1\r\n#    mat_net = OrderedDict()\r\n#    for i in range(len(mcn[\'net\'][0][0][0])):\r\n#        if mcn[\'net\'][0][0][0][i][0][0][0][0] == \'conv\':\r\n#            \r\n#            count += 1\r\n#            w = mcn[\'net\'][0][0][0][i][0][1][0][0]\r\n#            print(w.shape)\r\n#            w = weights2tensor(w)\r\n#            print(w.shape)\r\n#            \r\n#            b = mcn[\'net\'][0][0][0][i][0][1][0][1]\r\n#            b = weights2tensor(b)\r\n#            print(b.shape)\r\n#            \r\n#            mat_net[\'model.{:d}.weight\'.format(count*2)] = w\r\n#            mat_net[\'model.{:d}.bias\'.format(count*2)] = b\r\n#\r\n#    torch.save(mat_net, \'E:/pytorch/KAIR_ongoing/model_zoo/ffdnet_gray_clip.pth\')\r\n#    \r\n#    \r\n#\r\n#    crt_net = torch.load(\'E:/pytorch/KAIR_ongoing/model_zoo/imdn_x4.pth\')\r\n#    def show_kv(net):\r\n#        for k, v in net.items():\r\n#            print(k)\r\n#\r\n#    show_kv(crt_net)\r\n\r\n\r\n#    from models.network_dncnn import DnCNN as net\r\n#    network = net(in_nc=2, out_nc=1, nc=64, nb=20, act_mode=\'R\')\r\n\r\n#    from models.network_srmd import SRMD as net\r\n#    #network = net(in_nc=1, out_nc=1, nc=64, nb=15, act_mode=\'R\')\r\n#    network = net(in_nc=19, out_nc=3, nc=128, nb=12, upscale=4, act_mode=\'R\', upsample_mode=\'pixelshuffle\')\r\n#    \r\n#    from models.network_rrdb import RRDB as net\r\n#    network = net(in_nc=3, out_nc=3, nc=64, nb=23, gc=32, upscale=4, act_mode=\'L\', upsample_mode=\'upconv\')\r\n#    \r\n#    state_dict = network.state_dict()\r\n#    for key, param in state_dict.items():\r\n#        print(key)\r\n#    from models.network_imdn import IMDN as net\r\n#    network = net(in_nc=3, out_nc=3, nc=64, nb=8, upscale=4, act_mode=\'L\', upsample_mode=\'pixelshuffle\')\r\n#    state_dict = network.state_dict()\r\n#    mat_net = OrderedDict()\r\n#    for ((key, param),(key2, param2)) in zip(state_dict.items(), crt_net.items()):\r\n#        mat_net[key] = param2\r\n#    torch.save(mat_net, \'model_zoo/imdn_x4_1.pth\') \r\n#        \r\n\r\n#    net_old = torch.load(\'net_old.pth\')\r\n#    def show_kv(net):\r\n#        for k, v in net.items():\r\n#            print(k)\r\n#\r\n#    show_kv(net_old)\r\n#    from models.network_dpsr import MSRResNet_prior as net\r\n#    model = net(in_nc=4, out_nc=3, nc=96, nb=16, upscale=4, act_mode=\'R\', upsample_mode=\'pixelshuffle\')\r\n#    state_dict = network.state_dict()\r\n#    net_new = OrderedDict()\r\n#    for ((key, param),(key_old, param_old)) in zip(state_dict.items(), net_old.items()):\r\n#        net_new[key] = param_old\r\n#    torch.save(net_new, \'net_new.pth\') \r\n\r\n\r\n   # print(key)\r\n      #  print(param.size())\r\n\r\n\r\n\r\n    # run utils/utils_matconvnet.py\r\n'"
utils/utils_model.py,14,"b'# -*- coding: utf-8 -*-\r\nimport numpy as np\r\nimport torch\r\nfrom utils import utils_image as util\r\nimport re\r\nimport glob\r\nimport os\r\n\r\n\r\n\'\'\'\r\n# --------------------------------------------\r\n# Model\r\n# --------------------------------------------\r\n# Kai Zhang (github: https://github.com/cszn)\r\n# 03/Mar/2019\r\n# --------------------------------------------\r\n\'\'\'\r\n\r\n\r\ndef find_last_checkpoint(save_dir, net_type=\'G\'):\r\n    """"""\r\n    # ---------------------------------------\r\n    # Kai Zhang (github: https://github.com/cszn)\r\n    # 03/Mar/2019\r\n    # ---------------------------------------\r\n    Args:\r\n        save_dir: model folder\r\n        net_type: \'G\' or \'D\'\r\n\r\n    Return:\r\n        init_iter: iteration number\r\n        init_path: model path\r\n    # ---------------------------------------\r\n    """"""\r\n    file_list = glob.glob(os.path.join(save_dir, \'*_{}.pth\'.format(net_type)))\r\n    if file_list:\r\n        iter_exist = []\r\n        for file_ in file_list:\r\n            iter_current = re.findall(r""(\\d+)_{}.pth"".format(net_type), file_)\r\n            iter_exist.append(int(iter_current[0]))\r\n        init_iter = max(iter_exist)\r\n        init_path = os.path.join(save_dir, \'{}_{}.pth\'.format(init_iter, net_type))\r\n    else:\r\n        init_iter = 0\r\n        init_path = None\r\n    return init_iter, init_path\r\n\r\n\r\ndef test_mode(model, L, mode=0, refield=32, min_size=256, sf=1, modulo=1):\r\n    \'\'\'\r\n    # ---------------------------------------\r\n    # Kai Zhang (github: https://github.com/cszn)\r\n    # 03/Mar/2019\r\n    # ---------------------------------------\r\n    Args:\r\n        model: trained model\r\n        L: input Low-quality image\r\n        mode:\r\n            (0) normal: test(model, L)\r\n            (1) pad: test_pad(model, L, modulo=16)\r\n            (2) split: test_split(model, L, refield=32, min_size=256, sf=1, modulo=1)\r\n            (3) x8: test_x8(model, L, modulo=1) ^_^\r\n            (4) split and x8: test_split_x8(model, L, refield=32, min_size=256, sf=1, modulo=1)\r\n        refield: effective receptive filed of the network, 32 is enough\r\n            useful when split, i.e., mode=2, 4\r\n        min_size: min_sizeXmin_size image, e.g., 256X256 image\r\n            useful when split, i.e., mode=2, 4\r\n        sf: scale factor for super-resolution, otherwise 1\r\n        modulo: 1 if split\r\n            useful when pad, i.e., mode=1\r\n\r\n    Returns:\r\n        E: estimated image\r\n    # ---------------------------------------\r\n    \'\'\'\r\n    if mode == 0:\r\n        E = test(model, L)\r\n    elif mode == 1:\r\n        E = test_pad(model, L, modulo, sf)\r\n    elif mode == 2:\r\n        E = test_split(model, L, refield, min_size, sf, modulo)\r\n    elif mode == 3:\r\n        E = test_x8(model, L, modulo, sf)\r\n    elif mode == 4:\r\n        E = test_split_x8(model, L, refield, min_size, sf, modulo)\r\n    return E\r\n\r\n\r\n\'\'\'\r\n# --------------------------------------------\r\n# normal (0)\r\n# --------------------------------------------\r\n\'\'\'\r\n\r\n\r\ndef test(model, L):\r\n    E = model(L)\r\n    return E\r\n\r\n\r\n\'\'\'\r\n# --------------------------------------------\r\n# pad (1)\r\n# --------------------------------------------\r\n\'\'\'\r\n\r\n\r\ndef test_pad(model, L, modulo=16, sf=1):\r\n    h, w = L.size()[-2:]\r\n    paddingBottom = int(np.ceil(h/modulo)*modulo-h)\r\n    paddingRight = int(np.ceil(w/modulo)*modulo-w)\r\n    L = torch.nn.ReplicationPad2d((0, paddingRight, 0, paddingBottom))(L)\r\n    E = model(L)\r\n    E = E[..., :h*sf, :w*sf]\r\n    return E\r\n\r\n\r\n\'\'\'\r\n# --------------------------------------------\r\n# split (function)\r\n# --------------------------------------------\r\n\'\'\'\r\n\r\n\r\ndef test_split_fn(model, L, refield=32, min_size=256, sf=1, modulo=1):\r\n    """"""\r\n    Args:\r\n        model: trained model\r\n        L: input Low-quality image\r\n        refield: effective receptive filed of the network, 32 is enough\r\n        min_size: min_sizeXmin_size image, e.g., 256X256 image\r\n        sf: scale factor for super-resolution, otherwise 1\r\n        modulo: 1 if split\r\n\r\n    Returns:\r\n        E: estimated result\r\n    """"""\r\n    h, w = L.size()[-2:]\r\n    if h*w <= min_size**2:\r\n        L = torch.nn.ReplicationPad2d((0, int(np.ceil(w/modulo)*modulo-w), 0, int(np.ceil(h/modulo)*modulo-h)))(L)\r\n        E = model(L)\r\n        E = E[..., :h*sf, :w*sf]\r\n    else:\r\n        top = slice(0, (h//2//refield+1)*refield)\r\n        bottom = slice(h - (h//2//refield+1)*refield, h)\r\n        left = slice(0, (w//2//refield+1)*refield)\r\n        right = slice(w - (w//2//refield+1)*refield, w)\r\n        Ls = [L[..., top, left], L[..., top, right], L[..., bottom, left], L[..., bottom, right]]\r\n\r\n        if h * w <= 4*(min_size**2):\r\n            Es = [model(Ls[i]) for i in range(4)]\r\n        else:\r\n            Es = [test_split_fn(model, Ls[i], refield=refield, min_size=min_size, sf=sf, modulo=modulo) for i in range(4)]\r\n\r\n        b, c = Es[0].size()[:2]\r\n        E = torch.zeros(b, c, sf * h, sf * w).type_as(L)\r\n\r\n        E[..., :h//2*sf, :w//2*sf] = Es[0][..., :h//2*sf, :w//2*sf]\r\n        E[..., :h//2*sf, w//2*sf:w*sf] = Es[1][..., :h//2*sf, (-w + w//2)*sf:]\r\n        E[..., h//2*sf:h*sf, :w//2*sf] = Es[2][..., (-h + h//2)*sf:, :w//2*sf]\r\n        E[..., h//2*sf:h*sf, w//2*sf:w*sf] = Es[3][..., (-h + h//2)*sf:, (-w + w//2)*sf:]\r\n    return E\r\n\r\n\r\n\'\'\'\r\n# --------------------------------------------\r\n# split (2)\r\n# --------------------------------------------\r\n\'\'\'\r\n\r\n\r\ndef test_split(model, L, refield=32, min_size=256, sf=1, modulo=1):\r\n    E = test_split_fn(model, L, refield=refield, min_size=min_size, sf=sf, modulo=modulo)\r\n    return E\r\n\r\n\r\n\'\'\'\r\n# --------------------------------------------\r\n# x8 (3)\r\n# --------------------------------------------\r\n\'\'\'\r\n\r\n\r\ndef test_x8(model, L, modulo=1, sf=1):\r\n    E_list = [test_pad(model, util.augment_img_tensor4(L, mode=i), modulo=modulo, sf=sf) for i in range(8)]\r\n    for i in range(len(E_list)):\r\n        if i == 3 or i == 5:\r\n            E_list[i] = util.augment_img_tensor4(E_list[i], mode=8 - i)\r\n        else:\r\n            E_list[i] = util.augment_img_tensor4(E_list[i], mode=i)\r\n    output_cat = torch.stack(E_list, dim=0)\r\n    E = output_cat.mean(dim=0, keepdim=False)\r\n    return E\r\n\r\n\r\n\'\'\'\r\n# --------------------------------------------\r\n# split and x8 (4)\r\n# --------------------------------------------\r\n\'\'\'\r\n\r\n\r\ndef test_split_x8(model, L, refield=32, min_size=256, sf=1, modulo=1):\r\n    E_list = [test_split_fn(model, util.augment_img_tensor4(L, mode=i), refield=refield, min_size=min_size, sf=sf, modulo=modulo) for i in range(8)]\r\n    for k, i in enumerate(range(len(E_list))):\r\n        if i==3 or i==5:\r\n            E_list[k] = util.augment_img_tensor4(E_list[k], mode=8-i)\r\n        else:\r\n            E_list[k] = util.augment_img_tensor4(E_list[k], mode=i)\r\n    output_cat = torch.stack(E_list, dim=0)\r\n    E = output_cat.mean(dim=0, keepdim=False)\r\n    return E\r\n\r\n\r\n\'\'\'\r\n# ^_^-^_^-^_^-^_^-^_^-^_^-^_^-^_^-^_^-^_^-^_^-\r\n# _^_^-^_^-^_^-^_^-^_^-^_^-^_^-^_^-^_^-^_^-^_^\r\n# ^_^-^_^-^_^-^_^-^_^-^_^-^_^-^_^-^_^-^_^-^_^-\r\n\'\'\'\r\n\r\n\r\n\'\'\'\r\n# --------------------------------------------\r\n# print\r\n# --------------------------------------------\r\n\'\'\'\r\n\r\n\r\n# --------------------------------------------\r\n# print model\r\n# --------------------------------------------\r\ndef print_model(model):\r\n    msg = describe_model(model)\r\n    print(msg)\r\n\r\n\r\n# --------------------------------------------\r\n# print params\r\n# --------------------------------------------\r\ndef print_params(model):\r\n    msg = describe_params(model)\r\n    print(msg)\r\n\r\n\r\n\'\'\'\r\n# --------------------------------------------\r\n# information\r\n# --------------------------------------------\r\n\'\'\'\r\n\r\n\r\n# --------------------------------------------\r\n# model inforation\r\n# --------------------------------------------\r\ndef info_model(model):\r\n    msg = describe_model(model)\r\n    return msg\r\n\r\n\r\n# --------------------------------------------\r\n# params inforation\r\n# --------------------------------------------\r\ndef info_params(model):\r\n    msg = describe_params(model)\r\n    return msg\r\n\r\n\r\n\'\'\'\r\n# --------------------------------------------\r\n# description\r\n# --------------------------------------------\r\n\'\'\'\r\n\r\n\r\n# --------------------------------------------\r\n# model name and total number of parameters\r\n# --------------------------------------------\r\ndef describe_model(model):\r\n    if isinstance(model, torch.nn.DataParallel):\r\n        model = model.module\r\n    msg = \'\\n\'\r\n    msg += \'models name: {}\'.format(model.__class__.__name__) + \'\\n\'\r\n    msg += \'Params number: {}\'.format(sum(map(lambda x: x.numel(), model.parameters()))) + \'\\n\'\r\n    msg += \'Net structure:\\n{}\'.format(str(model)) + \'\\n\'\r\n    return msg\r\n\r\n\r\n# --------------------------------------------\r\n# parameters description\r\n# --------------------------------------------\r\ndef describe_params(model):\r\n    if isinstance(model, torch.nn.DataParallel):\r\n        model = model.module\r\n    msg = \'\\n\'\r\n    msg += \' | {:^6s} | {:^6s} | {:^6s} | {:^6s} || {:<20s}\'.format(\'mean\', \'min\', \'max\', \'std\', \'param_name\') + \'\\n\'\r\n    for name, param in model.state_dict().items():\r\n        if not \'num_batches_tracked\' in name:\r\n            v = param.data.clone().float()\r\n            msg += \' | {:>6.3f} | {:>6.3f} | {:>6.3f} | {:>6.3f} || {:s}\'.format(v.mean(), v.min(), v.max(), v.std(), name) + \'\\n\'\r\n    return msg\r\n\r\n\r\nif __name__ == \'__main__\':\r\n\r\n    class Net(torch.nn.Module):\r\n        def __init__(self, in_channels=3, out_channels=3):\r\n            super(Net, self).__init__()\r\n            self.conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1)\r\n\r\n        def forward(self, x):\r\n            x = self.conv(x)\r\n            return x\r\n\r\n    start = torch.cuda.Event(enable_timing=True)\r\n    end = torch.cuda.Event(enable_timing=True)\r\n\r\n    model = Net()\r\n    model = model.eval()\r\n    print_model(model)\r\n    print_params(model)\r\n    x = torch.randn((2,3,401,401))\r\n    torch.cuda.empty_cache()\r\n    with torch.no_grad():\r\n        for mode in range(5):\r\n            y = test_mode(model, x, mode, refield=32, min_size=256, sf=1, modulo=1)\r\n            print(y.shape)\r\n\r\n    # run utils/utils_model.py'"
utils/utils_option.py,0,"b'import os\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport json\nimport re\nimport glob\n\n\n\'\'\'\n# --------------------------------------------\n# Kai Zhang (github: https://github.com/cszn)\n# 03/Mar/2019\n# --------------------------------------------\n# https://github.com/xinntao/BasicSR\n# --------------------------------------------\n\'\'\'\n\n\ndef get_timestamp():\n    return datetime.now().strftime(\'_%y%m%d_%H%M%S\')\n\n\ndef parse(opt_path, is_train=True):\n\n    # ----------------------------------------\n    # remove comments starting with \'//\'\n    # ----------------------------------------\n    json_str = \'\'\n    with open(opt_path, \'r\') as f:\n        for line in f:\n            line = line.split(\'//\')[0] + \'\\n\'\n            json_str += line\n\n    # ----------------------------------------\n    # initialize opt\n    # ----------------------------------------\n    opt = json.loads(json_str, object_pairs_hook=OrderedDict)\n\n    opt[\'opt_path\'] = opt_path\n    opt[\'is_train\'] = is_train\n\n    # ----------------------------------------\n    # set default\n    # ----------------------------------------\n    if \'merge_bn\' not in opt:\n        opt[\'merge_bn\'] = False\n        opt[\'merge_bn_startpoint\'] = -1\n\n    if \'scale\' not in opt:\n        opt[\'scale\'] = 1\n\n    # ----------------------------------------\n    # datasets\n    # ----------------------------------------\n    for phase, dataset in opt[\'datasets\'].items():\n        phase = phase.split(\'_\')[0]\n        dataset[\'phase\'] = phase\n        dataset[\'scale\'] = opt[\'scale\']  # broadcast\n        dataset[\'n_channels\'] = opt[\'n_channels\']  # broadcast\n        if \'dataroot_H\' in dataset and dataset[\'dataroot_H\'] is not None:\n            dataset[\'dataroot_H\'] = os.path.expanduser(dataset[\'dataroot_H\'])\n        if \'dataroot_L\' in dataset and dataset[\'dataroot_L\'] is not None:\n            dataset[\'dataroot_L\'] = os.path.expanduser(dataset[\'dataroot_L\'])\n\n    # ----------------------------------------\n    # path\n    # ----------------------------------------\n    for key, path in opt[\'path\'].items():\n        if path and key in opt[\'path\']:\n            opt[\'path\'][key] = os.path.expanduser(path)\n\n    path_task = os.path.join(opt[\'path\'][\'root\'], opt[\'task\'])\n    opt[\'path\'][\'task\'] = path_task\n    opt[\'path\'][\'log\'] = path_task\n    opt[\'path\'][\'options\'] = os.path.join(path_task, \'options\')\n\n    if is_train:\n        opt[\'path\'][\'models\'] = os.path.join(path_task, \'models\')\n        opt[\'path\'][\'images\'] = os.path.join(path_task, \'images\')\n    else:  # test\n        opt[\'path\'][\'images\'] = os.path.join(path_task, \'test_images\')\n\n    # ----------------------------------------\n    # network\n    # ----------------------------------------\n    opt[\'netG\'][\'scale\'] = opt[\'scale\'] if \'scale\' in opt else 1\n\n    # ----------------------------------------\n    # GPU devices\n    # ----------------------------------------\n    gpu_list = \',\'.join(str(x) for x in opt[\'gpu_ids\'])\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = gpu_list\n    print(\'export CUDA_VISIBLE_DEVICES=\' + gpu_list)\n\n    return opt\n\n\ndef find_last_checkpoint(save_dir, net_type=\'G\'):\n    """"""\n    Args: \n        save_dir: model folder\n        net_type: \'G\' or \'D\'\n\n    Return:\n        init_iter: iteration number\n        init_path: model path\n    """"""\n    file_list = glob.glob(os.path.join(save_dir, \'*_{}.pth\'.format(net_type)))\n    if file_list:\n        iter_exist = []\n        for file_ in file_list:\n            iter_current = re.findall(r""(\\d+)_{}.pth"".format(net_type), file_)\n            iter_exist.append(int(iter_current[0]))\n        init_iter = max(iter_exist)\n        init_path = os.path.join(save_dir, \'{}_{}.pth\'.format(init_iter, net_type))\n    else:\n        init_iter = 0\n        init_path = None\n    return init_iter, init_path\n\n\n\'\'\'\n# --------------------------------------------\n# convert the opt into json file\n# --------------------------------------------\n\'\'\'\n\n\ndef save(opt):\n    opt_path = opt[\'opt_path\']\n    opt_path_copy = opt[\'path\'][\'options\']\n    dirname, filename_ext = os.path.split(opt_path)\n    filename, ext = os.path.splitext(filename_ext)\n    dump_path = os.path.join(opt_path_copy, filename+get_timestamp()+ext)\n    with open(dump_path, \'w\') as dump_file:\n        json.dump(opt, dump_file, indent=2)\n\n\n\'\'\'\n# --------------------------------------------\n# dict to string for logger\n# --------------------------------------------\n\'\'\'\n\n\ndef dict2str(opt, indent_l=1):\n    msg = \'\'\n    for k, v in opt.items():\n        if isinstance(v, dict):\n            msg += \' \' * (indent_l * 2) + k + \':[\\n\'\n            msg += dict2str(v, indent_l + 1)\n            msg += \' \' * (indent_l * 2) + \']\\n\'\n        else:\n            msg += \' \' * (indent_l * 2) + k + \': \' + str(v) + \'\\n\'\n    return msg\n\n\n\'\'\'\n# --------------------------------------------\n# convert OrderedDict to NoneDict,\n# return None for missing key\n# --------------------------------------------\n\'\'\'\n\n\ndef dict_to_nonedict(opt):\n    if isinstance(opt, dict):\n        new_opt = dict()\n        for key, sub_opt in opt.items():\n            new_opt[key] = dict_to_nonedict(sub_opt)\n        return NoneDict(**new_opt)\n    elif isinstance(opt, list):\n        return [dict_to_nonedict(sub_opt) for sub_opt in opt]\n    else:\n        return opt\n\n\nclass NoneDict(dict):\n    def __missing__(self, key):\n        return None\n'"
utils/utils_params.py,8,"b""import torch\n\nimport torchvision\n\nfrom models import basicblock as B\n\ndef show_kv(net):\n    for k, v in net.items():\n        print(k)\n\n# should run train debug mode first to get an initial model\n#crt_net = torch.load('../../experiments/debug_SRResNet_bicx4_in3nf64nb16/models/8_G.pth')\n#\n#for k, v in crt_net.items():\n#    print(k)\n#for k, v in crt_net.items():\n#    if k in pretrained_net:\n#        crt_net[k] = pretrained_net[k]\n#        print('replace ... ', k)\n\n# x2 -> x4\n#crt_net['model.5.weight'] = pretrained_net['model.2.weight']\n#crt_net['model.5.bias'] = pretrained_net['model.2.bias']\n#crt_net['model.8.weight'] = pretrained_net['model.5.weight']\n#crt_net['model.8.bias'] = pretrained_net['model.5.bias']\n#crt_net['model.10.weight'] = pretrained_net['model.7.weight']\n#crt_net['model.10.bias'] = pretrained_net['model.7.bias']\n#torch.save(crt_net, '../pretrained_tmp.pth')\n\n# x2 -> x3\n'''\nin_filter = pretrained_net['model.2.weight'] # 256, 64, 3, 3\nnew_filter = torch.Tensor(576, 64, 3, 3)\nnew_filter[0:256, :, :, :] = in_filter\nnew_filter[256:512, :, :, :] = in_filter\nnew_filter[512:, :, :, :] = in_filter[0:576-512, :, :, :]\ncrt_net['model.2.weight'] = new_filter\n\nin_bias = pretrained_net['model.2.bias']  # 256, 64, 3, 3\nnew_bias = torch.Tensor(576)\nnew_bias[0:256] = in_bias\nnew_bias[256:512] = in_bias\nnew_bias[512:] = in_bias[0:576 - 512]\ncrt_net['model.2.bias'] = new_bias\n\ntorch.save(crt_net, '../pretrained_tmp.pth')\n'''\n\n# x2 -> x8\n'''\ncrt_net['model.5.weight'] = pretrained_net['model.2.weight']\ncrt_net['model.5.bias'] = pretrained_net['model.2.bias']\ncrt_net['model.8.weight'] = pretrained_net['model.2.weight']\ncrt_net['model.8.bias'] = pretrained_net['model.2.bias']\ncrt_net['model.11.weight'] = pretrained_net['model.5.weight']\ncrt_net['model.11.bias'] = pretrained_net['model.5.bias']\ncrt_net['model.13.weight'] = pretrained_net['model.7.weight']\ncrt_net['model.13.bias'] = pretrained_net['model.7.bias']\ntorch.save(crt_net, '../pretrained_tmp.pth')\n'''\n\n# x3/4/8 RGB -> Y\n\ndef rgb2gray_net(net, only_input=True):\n\n    if only_input:\n        in_filter = net['0.weight']\n        in_new_filter = in_filter[:,0,:,:]*0.2989 + in_filter[:,1,:,:]*0.587 + in_filter[:,2,:,:]*0.114\n        in_new_filter.unsqueeze_(1)\n        net['0.weight'] = in_new_filter\n\n#    out_filter = pretrained_net['model.13.weight']\n#    out_new_filter = out_filter[0, :, :, :] * 0.2989 + out_filter[1, :, :, :] * 0.587 + \\\n#        out_filter[2, :, :, :] * 0.114\n#    out_new_filter.unsqueeze_(0)\n#    crt_net['model.13.weight'] = out_new_filter\n#    out_bias = pretrained_net['model.13.bias']\n#    out_new_bias = out_bias[0] * 0.2989 + out_bias[1] * 0.587 + out_bias[2] * 0.114\n#    out_new_bias = torch.Tensor(1).fill_(out_new_bias)\n#    crt_net['model.13.bias'] = out_new_bias\n\n#    torch.save(crt_net, '../pretrained_tmp.pth')\n\n    return net\n\n\n\nif __name__ == '__main__':\n    \n    net = torchvision.models.vgg19(pretrained=True)\n    for k,v in net.features.named_parameters():\n        if k=='0.weight':\n            in_new_filter = v[:,0,:,:]*0.2989 + v[:,1,:,:]*0.587 + v[:,2,:,:]*0.114\n            in_new_filter.unsqueeze_(1)\n            v = in_new_filter\n            print(v.shape)\n            print(v[0,0,0,0])\n        if k=='0.bias':\n            in_new_bias = v\n            print(v[0])\n\n    print(net.features[0])\n\n    net.features[0] = B.conv(1, 64, mode='C') \n\n    print(net.features[0])\n    net.features[0].weight.data=in_new_filter\n    net.features[0].bias.data=in_new_bias\n\n    for k,v in net.features.named_parameters():\n        if k=='0.weight':\n            print(v[0,0,0,0])\n        if k=='0.bias':\n            print(v[0])\n\n\n\n\n\n   # rgb2gray_net(net)\n\n\n\n\n\n\n\n\n\n"""
utils/utils_regularizers.py,7,"b'import torch\nimport torch.nn as nn\n\n\n\'\'\'\n# --------------------------------------------\n# Kai Zhang (github: https://github.com/cszn)\n# 03/Mar/2019\n# --------------------------------------------\n\'\'\'\n\n\n# --------------------------------------------\n# SVD Orthogonal Regularization\n# --------------------------------------------\ndef regularizer_orth(m):\n    """"""\n    # ----------------------------------------\n    # SVD Orthogonal Regularization\n    # ----------------------------------------\n    # Applies regularization to the training by performing the\n    # orthogonalization technique described in the paper\n    # This function is to be called by the torch.nn.Module.apply() method,\n    # which applies svd_orthogonalization() to every layer of the model.\n    # usage: net.apply(regularizer_orth)\n    # ----------------------------------------\n    """"""\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        w = m.weight.data.clone()\n        c_out, c_in, f1, f2 = w.size()\n        # dtype = m.weight.data.type()\n        w = w.permute(2, 3, 1, 0).contiguous().view(f1*f2*c_in, c_out)\n        # self.netG.apply(svd_orthogonalization)\n        u, s, v = torch.svd(w)\n        s[s > 1.5] = s[s > 1.5] - 1e-4\n        s[s < 0.5] = s[s < 0.5] + 1e-4\n        w = torch.mm(torch.mm(u, torch.diag(s)), v.t())\n        m.weight.data = w.view(f1, f2, c_in, c_out).permute(3, 2, 0, 1)  # .type(dtype)\n    else:\n        pass\n\n\n# --------------------------------------------\n# SVD Orthogonal Regularization\n# --------------------------------------------\ndef regularizer_orth2(m):\n    """"""\n    # ----------------------------------------\n    # Applies regularization to the training by performing the\n    # orthogonalization technique described in the paper\n    # This function is to be called by the torch.nn.Module.apply() method,\n    # which applies svd_orthogonalization() to every layer of the model.\n    # usage: net.apply(regularizer_orth2)\n    # ----------------------------------------\n    """"""\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        w = m.weight.data.clone()\n        c_out, c_in, f1, f2 = w.size()\n        # dtype = m.weight.data.type()\n        w = w.permute(2, 3, 1, 0).contiguous().view(f1*f2*c_in, c_out)\n        u, s, v = torch.svd(w)\n        s_mean = s.mean()\n        s[s > 1.5*s_mean] = s[s > 1.5*s_mean] - 1e-4\n        s[s < 0.5*s_mean] = s[s < 0.5*s_mean] + 1e-4\n        w = torch.mm(torch.mm(u, torch.diag(s)), v.t())\n        m.weight.data = w.view(f1, f2, c_in, c_out).permute(3, 2, 0, 1)  # .type(dtype)\n    else:\n        pass\n\n\n\ndef regularizer_clip(m):\n    """"""\n    # ----------------------------------------\n    # usage: net.apply(regularizer_clip)\n    # ----------------------------------------\n    """"""\n    eps = 1e-4\n    c_min = -1.5\n    c_max = 1.5\n\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1 or classname.find(\'Linear\') != -1:\n        w = m.weight.data.clone()\n        w[w > c_max] -= eps\n        w[w < c_min] += eps\n        m.weight.data = w\n\n        if m.bias is not None:\n            b = m.bias.data.clone()\n            b[b > c_max] -= eps\n            b[b < c_min] += eps\n            m.bias.data = b\n\n#    elif classname.find(\'BatchNorm2d\') != -1:\n#\n#       rv = m.running_var.data.clone()\n#       rm = m.running_mean.data.clone()\n#\n#        if m.affine:\n#            m.weight.data\n#            m.bias.data\n'"
utils/utils_sisr.py,0,"b'# -*- coding: utf-8 -*-\r\nimport numpy as np\r\n\r\nfrom utils import utils_image as util\r\nfrom utils import utils_deblur\r\n\r\nfrom scipy import ndimage\r\nimport scipy\r\nimport scipy.stats as ss\r\nimport scipy.io as io\r\n\r\nfrom scipy.ndimage import filters, measurements, interpolation\r\nfrom scipy.interpolate import interp2d\r\n\r\n\r\n""""""\r\n# --------------------------------------------\r\n# Super-Resolution\r\n# --------------------------------------------\r\n#\r\n# Kai Zhang (cskaizhang@gmail.com)\r\n# https://github.com/cszn\r\n# 28/Nov/2019\r\n# --------------------------------------------\r\n""""""\r\n\r\n\r\n""""""\r\n# --------------------------------------------\r\n# anisotropic Gaussian kernels\r\n# --------------------------------------------\r\n""""""\r\n\r\n\r\ndef anisotropic_Gaussian(ksize=15, theta=np.pi, l1=6, l2=6):\r\n    """""" generate an anisotropic Gaussian kernel\r\n    Args:\r\n        ksize : e.g., 15, kernel size\r\n        theta : [0,  pi], rotation angle range\r\n        l1    : [0.1,50], scaling of eigenvalues\r\n        l2    : [0.1,l1], scaling of eigenvalues\r\n        If l1 = l2, will get an isotropic Gaussian kernel.\r\n\r\n    Returns:\r\n        k     : kernel\r\n    """"""\r\n\r\n    v = np.dot(np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]]), np.array([1., 0.]))\r\n    V = np.array([[v[0], v[1]], [v[1], -v[0]]])\r\n    D = np.array([[l1, 0], [0, l2]])\r\n    Sigma = np.dot(np.dot(V, D), np.linalg.inv(V))\r\n    k = gm_blur_kernel(mean=[0, 0], cov=Sigma, size=ksize)\r\n\r\n    return k\r\n\r\n\r\ndef gm_blur_kernel(mean, cov, size=15):\r\n    center = size / 2.0 + 0.5\r\n    k = np.zeros([size, size])\r\n    for y in range(size):\r\n        for x in range(size):\r\n            cy = y - center + 1\r\n            cx = x - center + 1\r\n            k[y, x] = ss.multivariate_normal.pdf([cx, cy], mean=mean, cov=cov)\r\n\r\n    k = k / np.sum(k)\r\n    return k\r\n\r\n\r\n""""""\r\n# --------------------------------------------\r\n# calculate PCA projection matrix\r\n# --------------------------------------------\r\n""""""\r\n\r\n\r\ndef get_pca_matrix(x, dim_pca=15):\r\n    """"""\r\n    Args:\r\n        x: 225x10000 matrix\r\n        dim_pca: 15\r\n\r\n    Returns:\r\n        pca_matrix: 15x225\r\n    """"""\r\n    C = np.dot(x, x.T)\r\n    w, v = scipy.linalg.eigh(C)\r\n    pca_matrix = v[:, -dim_pca:].T\r\n\r\n    return pca_matrix\r\n\r\n\r\ndef show_pca(x):\r\n    """"""\r\n    x: PCA projection matrix, e.g., 15x225\r\n    """"""\r\n    for i in range(x.shape[0]):\r\n        xc = np.reshape(x[i, :], (int(np.sqrt(x.shape[1])), -1), order=""F"")\r\n        util.surf(xc)\r\n\r\n\r\ndef cal_pca_matrix(path=\'PCA_matrix.mat\', ksize=15, l_max=12.0, dim_pca=15, num_samples=500):\r\n    kernels = np.zeros([ksize*ksize, num_samples], dtype=np.float32)\r\n    for i in range(num_samples):\r\n\r\n        theta = np.pi*np.random.rand(1)\r\n        l1    = 0.1+l_max*np.random.rand(1)\r\n        l2    = 0.1+(l1-0.1)*np.random.rand(1)\r\n\r\n        k = anisotropic_Gaussian(ksize=ksize, theta=theta[0], l1=l1[0], l2=l2[0])\r\n\r\n        # util.imshow(k)\r\n\r\n        kernels[:, i] = np.reshape(k, (-1), order=""F"")  # k.flatten(order=\'F\')\r\n\r\n    # io.savemat(\'k.mat\', {\'k\': kernels})\r\n\r\n    pca_matrix = get_pca_matrix(kernels, dim_pca=dim_pca)\r\n\r\n    io.savemat(path, {\'p\': pca_matrix})\r\n\r\n    return pca_matrix\r\n\r\n\r\n""""""\r\n# --------------------------------------------\r\n# kernel shift\r\n# --------------------------------------------\r\n""""""\r\n\r\n\r\ndef gen_kernel(k_size=np.array([15, 15]), scale_factor=np.array([4, 4]), min_var=0.6, max_var=10., noise_level=0):\r\n    """"""""\r\n    # modified version of https://github.com/assafshocher/BlindSR_dataset_generator\r\n    # Kai Zhang\r\n    # min_var = 0.175 * sf  # variance of the gaussian kernel will be sampled between min_var and max_var\r\n    # max_var = 2.5 * sf\r\n    """"""\r\n    # Set random eigen-vals (lambdas) and angle (theta) for COV matrix\r\n    lambda_1 = min_var + np.random.rand() * (max_var - min_var)\r\n    lambda_2 = min_var + np.random.rand() * (max_var - min_var)\r\n    theta = np.random.rand() * np.pi  # random theta\r\n    noise = -noise_level + np.random.rand(*k_size) * noise_level * 2\r\n\r\n    # Set COV matrix using Lambdas and Theta\r\n    LAMBDA = np.diag([lambda_1, lambda_2])\r\n    Q = np.array([[np.cos(theta), -np.sin(theta)],\r\n                  [np.sin(theta), np.cos(theta)]])\r\n    SIGMA = Q @ LAMBDA @ Q.T\r\n    INV_SIGMA = np.linalg.inv(SIGMA)[None, None, :, :]\r\n\r\n    # Set expectation position (shifting kernel for aligned image)\r\n    MU = k_size // 2 - 0.5*(scale_factor - 1) # - 0.5 * (scale_factor - k_size % 2)\r\n    MU = MU[None, None, :, None]\r\n\r\n    # Create meshgrid for Gaussian\r\n    [X,Y] = np.meshgrid(range(k_size[0]), range(k_size[1]))\r\n    Z = np.stack([X, Y], 2)[:, :, :, None]\r\n\r\n    # Calcualte Gaussian for every pixel of the kernel\r\n    ZZ = Z-MU\r\n    ZZ_t = ZZ.transpose(0,1,3,2)\r\n    raw_kernel = np.exp(-0.5 * np.squeeze(ZZ_t @ INV_SIGMA @ ZZ)) * (1 + noise)\r\n\r\n    # shift the kernel so it will be centered\r\n    #raw_kernel_centered = kernel_shift(raw_kernel, scale_factor)\r\n\r\n    # Normalize the kernel and return\r\n    #kernel = raw_kernel_centered / np.sum(raw_kernel_centered)\r\n    kernel = raw_kernel / np.sum(raw_kernel)\r\n    return kernel\r\n\r\n\r\n""""""\r\n# --------------------------------------------\r\n# degradation models\r\n# --------------------------------------------\r\n""""""\r\n\r\n\r\ndef bicubic_degradation(x, sf=3):\r\n    \'\'\'\r\n    Args:\r\n        x: HxWxC image, [0, 1]\r\n        sf: down-scale factor\r\n\r\n    Return:\r\n        bicubicly downsampled LR image\r\n    \'\'\'\r\n    x = util.imresize_np(x, scale=1/sf)\r\n    return x\r\n\r\n\r\ndef srmd_degradation(x, k, sf=3):\r\n    \'\'\' blur + bicubic downsampling\r\n\r\n    Args:\r\n        x: HxWxC image, [0, 1]\r\n        k: hxw, double\r\n        sf: down-scale factor\r\n\r\n    Return:\r\n        downsampled LR image\r\n\r\n    Reference:\r\n        @inproceedings{zhang2018learning,\r\n          title={Learning a single convolutional super-resolution network for multiple degradations},\r\n          author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\r\n          booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\r\n          pages={3262--3271},\r\n          year={2018}\r\n        }\r\n    \'\'\'\r\n    x = ndimage.filters.convolve(x, np.expand_dims(k, axis=2), mode=\'wrap\')  # \'nearest\' | \'mirror\'\r\n    x = bicubic_degradation(x, sf=sf)\r\n    return x\r\n\r\n\r\ndef dpsr_degradation(x, k, sf=3):\r\n\r\n    \'\'\' bicubic downsampling + blur\r\n\r\n    Args:\r\n        x: HxWxC image, [0, 1]\r\n        k: hxw, double\r\n        sf: down-scale factor\r\n\r\n    Return:\r\n        downsampled LR image\r\n\r\n    Reference:\r\n        @inproceedings{zhang2019deep,\r\n          title={Deep Plug-and-Play Super-Resolution for Arbitrary Blur Kernels},\r\n          author={Zhang, Kai and Zuo, Wangmeng and Zhang, Lei},\r\n          booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\r\n          pages={1671--1681},\r\n          year={2019}\r\n        }\r\n    \'\'\'\r\n    x = bicubic_degradation(x, sf=sf)\r\n    x = ndimage.filters.convolve(x, np.expand_dims(k, axis=2), mode=\'wrap\')\r\n    return x\r\n\r\n\r\ndef modcrop_np(img, sf):\r\n    \'\'\'\r\n    Args:\r\n        img: numpy image, WxH or WxHxC\r\n        sf: scale factor\r\n\r\n    Return:\r\n        cropped image\r\n    \'\'\'\r\n    w, h = img.shape[:2]\r\n    im = np.copy(img)\r\n    return im[:w - w % sf, :h - h % sf, ...]\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    img = util.imread_uint(\'test.bmp\', 3)\r\n\r\n#    img = util.uint2single(img)\r\n#    k = utils_deblur.fspecial(\'gaussian\', 7, 1.6)\r\n#\r\n#    for sf in [2, 3, 4]:\r\n#\r\n#        # modcrop\r\n#        img = modcrop_np(img, sf=sf)\r\n#\r\n#        # 1) bicubic degradation\r\n#        img_b = bicubic_degradation(img, sf=sf)\r\n#        print(img_b.shape)\r\n#\r\n#        # 2) srmd degradation\r\n#        img_s = srmd_degradation(img, k, sf=sf)\r\n#        print(img_s.shape)\r\n#\r\n#        # 3) dpsr degradation\r\n#        img_d = dpsr_degradation(img, k, sf=sf)\r\n#        print(img_d.shape)\r\n\r\n\r\n#    k = anisotropic_Gaussian(ksize=7, theta=0.25*np.pi, l1=0.01, l2=0.01)\r\n#    print(k)\r\n#    util.imshow(k*10)\r\n\r\n#    k = gen_kernel(k_size=np.array([15, 15]), scale_factor=np.array([4, 4]), min_var=0.8, max_var=10.8, noise_level=0.0)\r\n#    util.imshow(k*10)\r\n\r\n#    util.surf(k)\r\n\r\n    # PCA\r\n#    pca_matrix = cal_pca_matrix(ksize=15, l_max=10.0, dim_pca=15, num_samples=12500)\r\n#    print(pca_matrix.shape)\r\n#    show_pca(pca_matrix)\r\n    # run utils/utils_sisr.py\r\n'"
