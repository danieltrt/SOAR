file_path,api_count,code
examine_data.py,1,"b'"""""" Some data examination """"""\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_rollout():\n    """""" Plot a rollout """"""\n    from torch.utils.data import DataLoader\n    from data.loaders import RolloutSequenceDataset\n    dataloader = DataLoader(\n        RolloutSequenceDataset(\n            root=\'datasets/carracing\', seq_len=900,\n            transform=lambda x: x, buffer_size=10,\n            train=False),\n        batch_size=1, shuffle=True)\n\n    dataloader.dataset.load_next_buffer()\n\n    # setting up subplots\n    plt.subplot(2, 2, 1)\n    monitor_obs = plt.imshow(np.zeros((64, 64, 3)))\n    plt.subplot(2, 2, 2)\n    monitor_next_obs = plt.imshow(np.zeros((64, 64, 3)))\n    plt.subplot(2, 2, 3)\n    monitor_diff = plt.imshow(np.zeros((64, 64, 3)))\n\n    for data in dataloader:\n        obs_seq = data[0].numpy().squeeze()\n        action_seq = data[1].numpy().squeeze()\n        next_obs_seq = data[-1].numpy().squeeze()\n        for obs, action, next_obs in zip(obs_seq, action_seq, next_obs_seq):\n            monitor_obs.set_data(obs)\n            monitor_next_obs.set_data(next_obs)\n            monitor_diff.set_data(next_obs - obs)\n            print(action)\n            plt.pause(.01)\n        break\n\nif __name__ == \'__main__\':\n    plot_rollout()\n'"
test_controller.py,2,"b'"""""" Test controller """"""\nimport argparse\nfrom os.path import join, exists\nfrom utils.misc import RolloutGenerator\nimport torch\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--logdir\', type=str, help=\'Where models are stored.\')\nargs = parser.parse_args()\n\nctrl_file = join(args.logdir, \'ctrl\', \'best.tar\')\n\nassert exists(ctrl_file),\\\n    ""Controller was not trained...""\n\ndevice = torch.device(\'cpu\')\n\ngenerator = RolloutGenerator(args.logdir, device, 1000)\n\nwith torch.no_grad():\n    generator.rollout(None)\n'"
traincontroller.py,6,"b'""""""\nTraining a linear controller on latent + recurrent state\nwith CMAES.\n\nThis is a bit complex. num_workers slave threads are launched\nto process a queue filled with parameters to be evaluated.\n""""""\nimport argparse\nimport sys\nfrom os.path import join, exists\nfrom os import mkdir, unlink, listdir, getpid\nfrom time import sleep\nfrom torch.multiprocessing import Process, Queue\nimport torch\nimport cma\nfrom models import Controller\nfrom tqdm import tqdm\nimport numpy as np\nfrom utils.misc import RolloutGenerator, ASIZE, RSIZE, LSIZE\nfrom utils.misc import load_parameters\nfrom utils.misc import flatten_parameters\n\n# parsing\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--logdir\', type=str, help=\'Where everything is stored.\')\nparser.add_argument(\'--n-samples\', type=int, help=\'Number of samples used to obtain \'\n                    \'return estimate.\')\nparser.add_argument(\'--pop-size\', type=int, help=\'Population size.\')\nparser.add_argument(\'--target-return\', type=float, help=\'Stops once the return \'\n                    \'gets above target_return\')\nparser.add_argument(\'--display\', action=\'store_true\', help=""Use progress bars if ""\n                    ""specified."")\nparser.add_argument(\'--max-workers\', type=int, help=\'Maximum number of workers.\',\n                    default=32)\nargs = parser.parse_args()\n\n# Max number of workers. M\n\n# multiprocessing variables\nn_samples = args.n_samples\npop_size = args.pop_size\nnum_workers = min(args.max_workers, n_samples * pop_size)\ntime_limit = 1000\n\n# create tmp dir if non existent and clean it if existent\ntmp_dir = join(args.logdir, \'tmp\')\nif not exists(tmp_dir):\n    mkdir(tmp_dir)\nelse:\n    for fname in listdir(tmp_dir):\n        unlink(join(tmp_dir, fname))\n\n# create ctrl dir if non exitent\nctrl_dir = join(args.logdir, \'ctrl\')\nif not exists(ctrl_dir):\n    mkdir(ctrl_dir)\n\n\n################################################################################\n#                           Thread routines                                    #\n################################################################################\ndef slave_routine(p_queue, r_queue, e_queue, p_index):\n    """""" Thread routine.\n\n    Threads interact with p_queue, the parameters queue, r_queue, the result\n    queue and e_queue the end queue. They pull parameters from p_queue, execute\n    the corresponding rollout, then place the result in r_queue.\n\n    Each parameter has its own unique id. Parameters are pulled as tuples\n    (s_id, params) and results are pushed as (s_id, result).  The same\n    parameter can appear multiple times in p_queue, displaying the same id\n    each time.\n\n    As soon as e_queue is non empty, the thread terminate.\n\n    When multiple gpus are involved, the assigned gpu is determined by the\n    process index p_index (gpu = p_index % n_gpus).\n\n    :args p_queue: queue containing couples (s_id, parameters) to evaluate\n    :args r_queue: where to place results (s_id, results)\n    :args e_queue: as soon as not empty, terminate\n    :args p_index: the process index\n    """"""\n    # init routine\n    gpu = p_index % torch.cuda.device_count()\n    device = torch.device(\'cuda:{}\'.format(gpu) if torch.cuda.is_available() else \'cpu\')\n\n    # redirect streams\n    sys.stdout = open(join(tmp_dir, str(getpid()) + \'.out\'), \'a\')\n    sys.stderr = open(join(tmp_dir, str(getpid()) + \'.err\'), \'a\')\n\n    with torch.no_grad():\n        r_gen = RolloutGenerator(args.logdir, device, time_limit)\n\n        while e_queue.empty():\n            if p_queue.empty():\n                sleep(.1)\n            else:\n                s_id, params = p_queue.get()\n                r_queue.put((s_id, r_gen.rollout(params)))\n\n\n################################################################################\n#                Define queues and start workers                               #\n################################################################################\np_queue = Queue()\nr_queue = Queue()\ne_queue = Queue()\n\nfor p_index in range(num_workers):\n    Process(target=slave_routine, args=(p_queue, r_queue, e_queue, p_index)).start()\n\n\n################################################################################\n#                           Evaluation                                         #\n################################################################################\ndef evaluate(solutions, results, rollouts=100):\n    """""" Give current controller evaluation.\n\n    Evaluation is minus the cumulated reward averaged over rollout runs.\n\n    :args solutions: CMA set of solutions\n    :args results: corresponding results\n    :args rollouts: number of rollouts\n\n    :returns: minus averaged cumulated reward\n    """"""\n    index_min = np.argmin(results)\n    best_guess = solutions[index_min]\n    restimates = []\n\n    for s_id in range(rollouts):\n        p_queue.put((s_id, best_guess))\n\n    print(""Evaluating..."")\n    for _ in tqdm(range(rollouts)):\n        while r_queue.empty():\n            sleep(.1)\n        restimates.append(r_queue.get()[1])\n\n    return best_guess, np.mean(restimates), np.std(restimates)\n\n################################################################################\n#                           Launch CMA                                         #\n################################################################################\ncontroller = Controller(LSIZE, RSIZE, ASIZE)  # dummy instance\n\n# define current best and load parameters\ncur_best = None\nctrl_file = join(ctrl_dir, \'best.tar\')\nprint(""Attempting to load previous best..."")\nif exists(ctrl_file):\n    state = torch.load(ctrl_file, map_location={\'cuda:0\': \'cpu\'})\n    cur_best = - state[\'reward\']\n    controller.load_state_dict(state[\'state_dict\'])\n    print(""Previous best was {}..."".format(-cur_best))\n\nparameters = controller.parameters()\nes = cma.CMAEvolutionStrategy(flatten_parameters(parameters), 0.1,\n                              {\'popsize\': pop_size})\n\nepoch = 0\nlog_step = 3\nwhile not es.stop():\n    if cur_best is not None and - cur_best > args.target_return:\n        print(""Already better than target, breaking..."")\n        break\n\n    r_list = [0] * pop_size  # result list\n    solutions = es.ask()\n\n    # push parameters to queue\n    for s_id, s in enumerate(solutions):\n        for _ in range(n_samples):\n            p_queue.put((s_id, s))\n\n    # retrieve results\n    if args.display:\n        pbar = tqdm(total=pop_size * n_samples)\n    for _ in range(pop_size * n_samples):\n        while r_queue.empty():\n            sleep(.1)\n        r_s_id, r = r_queue.get()\n        r_list[r_s_id] += r / n_samples\n        if args.display:\n            pbar.update(1)\n    if args.display:\n        pbar.close()\n\n    es.tell(solutions, r_list)\n    es.disp()\n\n    # evaluation and saving\n    if epoch % log_step == log_step - 1:\n        best_params, best, std_best = evaluate(solutions, r_list)\n        print(""Current evaluation: {}"".format(best))\n        if not cur_best or cur_best > best:\n            cur_best = best\n            print(""Saving new best with value {}+-{}..."".format(-cur_best, std_best))\n            load_parameters(best_params, controller)\n            torch.save(\n                {\'epoch\': epoch,\n                 \'reward\': - cur_best,\n                 \'state_dict\': controller.state_dict()},\n                join(ctrl_dir, \'best.tar\'))\n        if - best > args.target_return:\n            print(""Terminating controller training with value {}..."".format(best))\n            break\n\n\n    epoch += 1\n\nes.result_pretty()\ne_queue.put(\'EOP\')\n'"
trainmdrnn.py,9,"b'"""""" Recurrent model training """"""\nimport argparse\nfrom functools import partial\nfrom os.path import join, exists\nfrom os import mkdir\nimport torch\nimport torch.nn.functional as f\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport numpy as np\nfrom tqdm import tqdm\nfrom utils.misc import save_checkpoint\nfrom utils.misc import ASIZE, LSIZE, RSIZE, RED_SIZE, SIZE\nfrom utils.learning import EarlyStopping\n## WARNING : THIS SHOULD BE REPLACED WITH PYTORCH 0.5\nfrom utils.learning import ReduceLROnPlateau\n\nfrom data.loaders import RolloutSequenceDataset\nfrom models.vae import VAE\nfrom models.mdrnn import MDRNN, gmm_loss\n\nparser = argparse.ArgumentParser(""MDRNN training"")\nparser.add_argument(\'--logdir\', type=str,\n                    help=""Where things are logged and models are loaded from."")\nparser.add_argument(\'--noreload\', action=\'store_true\',\n                    help=""Do not reload if specified."")\nparser.add_argument(\'--include_reward\', action=\'store_true\',\n                    help=""Add a reward modelisation term to the loss."")\nargs = parser.parse_args()\n\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n# constants\nBSIZE = 16\nSEQ_LEN = 32\nepochs = 30\n\n# Loading VAE\nvae_file = join(args.logdir, \'vae\', \'best.tar\')\nassert exists(vae_file), ""No trained VAE in the logdir...""\nstate = torch.load(vae_file)\nprint(""Loading VAE at epoch {} ""\n      ""with test error {}"".format(\n          state[\'epoch\'], state[\'precision\']))\n\nvae = VAE(3, LSIZE).to(device)\nvae.load_state_dict(state[\'state_dict\'])\n\n# Loading model\nrnn_dir = join(args.logdir, \'mdrnn\')\nrnn_file = join(rnn_dir, \'best.tar\')\n\nif not exists(rnn_dir):\n    mkdir(rnn_dir)\n\nmdrnn = MDRNN(LSIZE, ASIZE, RSIZE, 5)\nmdrnn.to(device)\noptimizer = torch.optim.RMSprop(mdrnn.parameters(), lr=1e-3, alpha=.9)\nscheduler = ReduceLROnPlateau(optimizer, \'min\', factor=0.5, patience=5)\nearlystopping = EarlyStopping(\'min\', patience=30)\n\n\nif exists(rnn_file) and not args.noreload:\n    rnn_state = torch.load(rnn_file)\n    print(""Loading MDRNN at epoch {} ""\n          ""with test error {}"".format(\n              rnn_state[""epoch""], rnn_state[""precision""]))\n    mdrnn.load_state_dict(rnn_state[""state_dict""])\n    optimizer.load_state_dict(rnn_state[""optimizer""])\n    scheduler.load_state_dict(state[\'scheduler\'])\n    earlystopping.load_state_dict(state[\'earlystopping\'])\n\n\n# Data Loading\ntransform = transforms.Lambda(\n    lambda x: np.transpose(x, (0, 3, 1, 2)) / 255)\ntrain_loader = DataLoader(\n    RolloutSequenceDataset(\'datasets/carracing\', SEQ_LEN, transform, buffer_size=30),\n    batch_size=BSIZE, num_workers=8, shuffle=True)\ntest_loader = DataLoader(\n    RolloutSequenceDataset(\'datasets/carracing\', SEQ_LEN, transform, train=False, buffer_size=10),\n    batch_size=BSIZE, num_workers=8)\n\ndef to_latent(obs, next_obs):\n    """""" Transform observations to latent space.\n\n    :args obs: 5D torch tensor (BSIZE, SEQ_LEN, ASIZE, SIZE, SIZE)\n    :args next_obs: 5D torch tensor (BSIZE, SEQ_LEN, ASIZE, SIZE, SIZE)\n\n    :returns: (latent_obs, latent_next_obs)\n        - latent_obs: 4D torch tensor (BSIZE, SEQ_LEN, LSIZE)\n        - next_latent_obs: 4D torch tensor (BSIZE, SEQ_LEN, LSIZE)\n    """"""\n    with torch.no_grad():\n        obs, next_obs = [\n            f.upsample(x.view(-1, 3, SIZE, SIZE), size=RED_SIZE,\n                       mode=\'bilinear\', align_corners=True)\n            for x in (obs, next_obs)]\n\n        (obs_mu, obs_logsigma), (next_obs_mu, next_obs_logsigma) = [\n            vae(x)[1:] for x in (obs, next_obs)]\n\n        latent_obs, latent_next_obs = [\n            (x_mu + x_logsigma.exp() * torch.randn_like(x_mu)).view(BSIZE, SEQ_LEN, LSIZE)\n            for x_mu, x_logsigma in\n            [(obs_mu, obs_logsigma), (next_obs_mu, next_obs_logsigma)]]\n    return latent_obs, latent_next_obs\n\ndef get_loss(latent_obs, action, reward, terminal,\n             latent_next_obs, include_reward: bool):\n    """""" Compute losses.\n\n    The loss that is computed is:\n    (GMMLoss(latent_next_obs, GMMPredicted) + MSE(reward, predicted_reward) +\n         BCE(terminal, logit_terminal)) / (LSIZE + 2)\n    The LSIZE + 2 factor is here to counteract the fact that the GMMLoss scales\n    approximately linearily with LSIZE. All losses are averaged both on the\n    batch and the sequence dimensions (the two first dimensions).\n\n    :args latent_obs: (BSIZE, SEQ_LEN, LSIZE) torch tensor\n    :args action: (BSIZE, SEQ_LEN, ASIZE) torch tensor\n    :args reward: (BSIZE, SEQ_LEN) torch tensor\n    :args latent_next_obs: (BSIZE, SEQ_LEN, LSIZE) torch tensor\n\n    :returns: dictionary of losses, containing the gmm, the mse, the bce and\n        the averaged loss.\n    """"""\n    latent_obs, action,\\\n        reward, terminal,\\\n        latent_next_obs = [arr.transpose(1, 0)\n                           for arr in [latent_obs, action,\n                                       reward, terminal,\n                                       latent_next_obs]]\n    mus, sigmas, logpi, rs, ds = mdrnn(action, latent_obs)\n    gmm = gmm_loss(latent_next_obs, mus, sigmas, logpi)\n    bce = f.binary_cross_entropy_with_logits(ds, terminal)\n    if include_reward:\n        mse = f.mse_loss(rs, reward)\n        scale = LSIZE + 2\n    else:\n        mse = 0\n        scale = LSIZE + 1\n    loss = (gmm + bce + mse) / scale\n    return dict(gmm=gmm, bce=bce, mse=mse, loss=loss)\n\n\ndef data_pass(epoch, train, include_reward): # pylint: disable=too-many-locals\n    """""" One pass through the data """"""\n    if train:\n        mdrnn.train()\n        loader = train_loader\n    else:\n        mdrnn.eval()\n        loader = test_loader\n\n    loader.dataset.load_next_buffer()\n\n    cum_loss = 0\n    cum_gmm = 0\n    cum_bce = 0\n    cum_mse = 0\n\n    pbar = tqdm(total=len(loader.dataset), desc=""Epoch {}"".format(epoch))\n    for i, data in enumerate(loader):\n        obs, action, reward, terminal, next_obs = [arr.to(device) for arr in data]\n\n        # transform obs\n        latent_obs, latent_next_obs = to_latent(obs, next_obs)\n\n        if train:\n            losses = get_loss(latent_obs, action, reward,\n                              terminal, latent_next_obs, include_reward)\n\n            optimizer.zero_grad()\n            losses[\'loss\'].backward()\n            optimizer.step()\n        else:\n            with torch.no_grad():\n                losses = get_loss(latent_obs, action, reward,\n                                  terminal, latent_next_obs, include_reward)\n\n        cum_loss += losses[\'loss\'].item()\n        cum_gmm += losses[\'gmm\'].item()\n        cum_bce += losses[\'bce\'].item()\n        cum_mse += losses[\'mse\'].item() if hasattr(losses[\'mse\'], \'item\') else \\\n            losses[\'mse\']\n\n        pbar.set_postfix_str(""loss={loss:10.6f} bce={bce:10.6f} ""\n                             ""gmm={gmm:10.6f} mse={mse:10.6f}"".format(\n                                 loss=cum_loss / (i + 1), bce=cum_bce / (i + 1),\n                                 gmm=cum_gmm / LSIZE / (i + 1), mse=cum_mse / (i + 1)))\n        pbar.update(BSIZE)\n    pbar.close()\n    return cum_loss * BSIZE / len(loader.dataset)\n\n\ntrain = partial(data_pass, train=True, include_reward=args.include_reward)\ntest = partial(data_pass, train=False, include_reward=args.include_reward)\n\ncur_best = None\nfor e in range(epochs):\n    train(e)\n    test_loss = test(e)\n    scheduler.step(test_loss)\n    earlystopping.step(test_loss)\n\n    is_best = not cur_best or test_loss < cur_best\n    if is_best:\n        cur_best = test_loss\n    checkpoint_fname = join(rnn_dir, \'checkpoint.tar\')\n    save_checkpoint({\n        ""state_dict"": mdrnn.state_dict(),\n        ""optimizer"": optimizer.state_dict(),\n        \'scheduler\': scheduler.state_dict(),\n        \'earlystopping\': earlystopping.state_dict(),\n        ""precision"": test_loss,\n        ""epoch"": e}, is_best, checkpoint_fname,\n                    rnn_file)\n\n    if earlystopping.stop:\n        print(""End of Training because of early stopping at epoch {}"".format(e))\n        break\n'"
trainvae.py,13,"b'"""""" Training VAE """"""\nimport argparse\nfrom os.path import join, exists\nfrom os import mkdir\n\nimport torch\nimport torch.utils.data\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\n\nfrom models.vae import VAE\n\nfrom utils.misc import save_checkpoint\nfrom utils.misc import LSIZE, RED_SIZE\n## WARNING : THIS SHOULD BE REPLACE WITH PYTORCH 0.5\nfrom utils.learning import EarlyStopping\nfrom utils.learning import ReduceLROnPlateau\nfrom data.loaders import RolloutObservationDataset\n\nparser = argparse.ArgumentParser(description=\'VAE Trainer\')\nparser.add_argument(\'--batch-size\', type=int, default=32, metavar=\'N\',\n                    help=\'input batch size for training (default: 32)\')\nparser.add_argument(\'--epochs\', type=int, default=1000, metavar=\'N\',\n                    help=\'number of epochs to train (default: 1000)\')\nparser.add_argument(\'--logdir\', type=str, help=\'Directory where results are logged\')\nparser.add_argument(\'--noreload\', action=\'store_true\',\n                    help=\'Best model is not reloaded if specified\')\nparser.add_argument(\'--nosamples\', action=\'store_true\',\n                    help=\'Does not save samples during training if specified\')\n\n\nargs = parser.parse_args()\ncuda = torch.cuda.is_available()\n\n\ntorch.manual_seed(123)\n# Fix numeric divergence due to bug in Cudnn\ntorch.backends.cudnn.benchmark = True\n\ndevice = torch.device(""cuda"" if cuda else ""cpu"")\n\n\ntransform_train = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((RED_SIZE, RED_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((RED_SIZE, RED_SIZE)),\n    transforms.ToTensor(),\n])\n\ndataset_train = RolloutObservationDataset(\'datasets/carracing\',\n                                          transform_train, train=True)\ndataset_test = RolloutObservationDataset(\'datasets/carracing\',\n                                         transform_test, train=False)\ntrain_loader = torch.utils.data.DataLoader(\n    dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=2)\ntest_loader = torch.utils.data.DataLoader(\n    dataset_test, batch_size=args.batch_size, shuffle=True, num_workers=2)\n\n\nmodel = VAE(3, LSIZE).to(device)\noptimizer = optim.Adam(model.parameters())\nscheduler = ReduceLROnPlateau(optimizer, \'min\', factor=0.5, patience=5)\nearlystopping = EarlyStopping(\'min\', patience=30)\n\n# Reconstruction + KL divergence losses summed over all elements and batch\ndef loss_function(recon_x, x, mu, logsigma):\n    """""" VAE loss function """"""\n    BCE = F.mse_loss(recon_x, x, size_average=False)\n\n    # see Appendix B from VAE paper:\n    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n    # https://arxiv.org/abs/1312.6114\n    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n    KLD = -0.5 * torch.sum(1 + 2 * logsigma - mu.pow(2) - (2 * logsigma).exp())\n    return BCE + KLD\n\n\ndef train(epoch):\n    """""" One training epoch """"""\n    model.train()\n    dataset_train.load_next_buffer()\n    train_loss = 0\n    for batch_idx, data in enumerate(train_loader):\n        data = data.to(device)\n        optimizer.zero_grad()\n        recon_batch, mu, logvar = model(data)\n        loss = loss_function(recon_batch, data, mu, logvar)\n        loss.backward()\n        train_loss += loss.item()\n        optimizer.step()\n        if batch_idx % 20 == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader),\n                loss.item() / len(data)))\n\n    print(\'====> Epoch: {} Average loss: {:.4f}\'.format(\n        epoch, train_loss / len(train_loader.dataset)))\n\n\ndef test():\n    """""" One test epoch """"""\n    model.eval()\n    dataset_test.load_next_buffer()\n    test_loss = 0\n    with torch.no_grad():\n        for data in test_loader:\n            data = data.to(device)\n            recon_batch, mu, logvar = model(data)\n            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n\n    test_loss /= len(test_loader.dataset)\n    print(\'====> Test set loss: {:.4f}\'.format(test_loss))\n    return test_loss\n\n# check vae dir exists, if not, create it\nvae_dir = join(args.logdir, \'vae\')\nif not exists(vae_dir):\n    mkdir(vae_dir)\n    mkdir(join(vae_dir, \'samples\'))\n\nreload_file = join(vae_dir, \'best.tar\')\nif not args.noreload and exists(reload_file):\n    state = torch.load(reload_file)\n    print(""Reloading model at epoch {}""\n          "", with test error {}"".format(\n              state[\'epoch\'],\n              state[\'precision\']))\n    model.load_state_dict(state[\'state_dict\'])\n    optimizer.load_state_dict(state[\'optimizer\'])\n    scheduler.load_state_dict(state[\'scheduler\'])\n    earlystopping.load_state_dict(state[\'earlystopping\'])\n\n\ncur_best = None\n\nfor epoch in range(1, args.epochs + 1):\n    train(epoch)\n    test_loss = test()\n    scheduler.step(test_loss)\n    earlystopping.step(test_loss)\n\n    # checkpointing\n    best_filename = join(vae_dir, \'best.tar\')\n    filename = join(vae_dir, \'checkpoint.tar\')\n    is_best = not cur_best or test_loss < cur_best\n    if is_best:\n        cur_best = test_loss\n\n    save_checkpoint({\n        \'epoch\': epoch,\n        \'state_dict\': model.state_dict(),\n        \'precision\': test_loss,\n        \'optimizer\': optimizer.state_dict(),\n        \'scheduler\': scheduler.state_dict(),\n        \'earlystopping\': earlystopping.state_dict()\n    }, is_best, filename, best_filename)\n\n\n\n    if not args.nosamples:\n        with torch.no_grad():\n            sample = torch.randn(RED_SIZE, LSIZE).to(device)\n            sample = model.decoder(sample).cpu()\n            save_image(sample.view(64, 3, RED_SIZE, RED_SIZE),\n                       join(vae_dir, \'samples/sample_\' + str(epoch) + \'.png\'))\n\n    if earlystopping.stop:\n        print(""End of Training because of early stopping at epoch {}"".format(epoch))\n        break\n'"
data/__init__.py,0,b'\n'
data/carracing.py,0,"b'""""""\nGenerating data from the CarRacing gym environment.\n!!! DOES NOT WORK ON TITANIC, DO IT AT HOME, THEN SCP !!!\n""""""\nimport argparse\nfrom os.path import join, exists\nimport gym\nimport numpy as np\nfrom utils.misc import sample_continuous_policy\n\ndef generate_data(rollouts, data_dir, noise_type): # pylint: disable=R0914\n    """""" Generates data """"""\n    assert exists(data_dir), ""The data directory does not exist...""\n\n    env = gym.make(""CarRacing-v0"")\n    seq_len = 1000\n\n    for i in range(rollouts):\n        env.reset()\n        env.env.viewer.window.dispatch_events()\n        if noise_type == \'white\':\n            a_rollout = [env.action_space.sample() for _ in range(seq_len)]\n        elif noise_type == \'brown\':\n            a_rollout = sample_continuous_policy(env.action_space, seq_len, 1. / 50)\n\n        s_rollout = []\n        r_rollout = []\n        d_rollout = []\n\n        t = 0\n        while True:\n            action = a_rollout[t]\n            t += 1\n\n            s, r, done, _ = env.step(action)\n            env.env.viewer.window.dispatch_events()\n            s_rollout += [s]\n            r_rollout += [r]\n            d_rollout += [done]\n            if done:\n                print(""> End of rollout {}, {} frames..."".format(i, len(s_rollout)))\n                np.savez(join(data_dir, \'rollout_{}\'.format(i)),\n                         observations=np.array(s_rollout),\n                         rewards=np.array(r_rollout),\n                         actions=np.array(a_rollout),\n                         terminals=np.array(d_rollout))\n                break\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--rollouts\', type=int, help=""Number of rollouts"")\n    parser.add_argument(\'--dir\', type=str, help=""Where to place rollouts"")\n    parser.add_argument(\'--policy\', type=str, choices=[\'white\', \'brown\'],\n                        help=\'Noise type used for action sampling.\',\n                        default=\'brown\')\n    args = parser.parse_args()\n    generate_data(args.rollouts, args.dir, args.policy)\n'"
data/generation_script.py,0,"b'""""""\nEncapsulate generate data to make it parallel\n""""""\nfrom os import makedirs\nfrom os.path import join\nimport argparse\nfrom multiprocessing import Pool\nfrom subprocess import call\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--rollouts\', type=int, help=""Total number of rollouts."")\nparser.add_argument(\'--threads\', type=int, help=""Number of threads"")\nparser.add_argument(\'--rootdir\', type=str, help=""Directory to store rollout ""\n                    ""directories of each thread"")\nparser.add_argument(\'--policy\', type=str, choices=[\'brown\', \'white\'],\n                    help=""Directory to store rollout directories of each thread"",\n                    default=\'brown\')\nargs = parser.parse_args()\n\nrpt = args.rollouts // args.threads + 1\n\ndef _threaded_generation(i):\n    tdir = join(args.rootdir, \'thread_{}\'.format(i))\n    makedirs(tdir, exist_ok=True)\n    cmd = [\'xvfb-run\', \'-s\', \'""-screen 0 1400x900x24""\']\n    cmd += [\'--server-num={}\'.format(i + 1)]\n    cmd += [""python"", ""-m"", ""data.carracing"", ""--dir"",\n            tdir, ""--rollouts"", str(rpt), ""--policy"", args.policy]\n    cmd = "" "".join(cmd)\n    print(cmd)\n    call(cmd, shell=True)\n    return True\n\n\nwith Pool(args.threads) as p:\n    p.map(_threaded_generation, range(args.threads))\n'"
data/loaders.py,2,"b'"""""" Some data loading utilities """"""\nfrom bisect import bisect\nfrom os import listdir\nfrom os.path import join, isdir\nfrom tqdm import tqdm\nimport torch\nimport torch.utils.data\nimport numpy as np\n\nclass _RolloutDataset(torch.utils.data.Dataset): # pylint: disable=too-few-public-methods\n    def __init__(self, root, transform, buffer_size=200, train=True): # pylint: disable=too-many-arguments\n        self._transform = transform\n\n        self._files = [\n            join(root, sd, ssd)\n            for sd in listdir(root) if isdir(join(root, sd))\n            for ssd in listdir(join(root, sd))]\n\n        if train:\n            self._files = self._files[:-600]\n        else:\n            self._files = self._files[-600:]\n\n        self._cum_size = None\n        self._buffer = None\n        self._buffer_fnames = None\n        self._buffer_index = 0\n        self._buffer_size = buffer_size\n\n    def load_next_buffer(self):\n        """""" Loads next buffer """"""\n        self._buffer_fnames = self._files[self._buffer_index:self._buffer_index + self._buffer_size]\n        self._buffer_index += self._buffer_size\n        self._buffer_index = self._buffer_index % len(self._files)\n        self._buffer = []\n        self._cum_size = [0]\n\n        # progress bar\n        pbar = tqdm(total=len(self._buffer_fnames),\n                    bar_format=\'{l_bar}{bar}| {n_fmt}/{total_fmt} {postfix}\')\n        pbar.set_description(""Loading file buffer ..."")\n\n        for f in self._buffer_fnames:\n            with np.load(f) as data:\n                self._buffer += [{k: np.copy(v) for k, v in data.items()}]\n                self._cum_size += [self._cum_size[-1] +\n                                   self._data_per_sequence(data[\'rewards\'].shape[0])]\n            pbar.update(1)\n        pbar.close()\n\n    def __len__(self):\n        # to have a full sequence, you need self.seq_len + 1 elements, as\n        # you must produce both an seq_len obs and seq_len next_obs sequences\n        if not self._cum_size:\n            self.load_next_buffer()\n        return self._cum_size[-1]\n\n    def __getitem__(self, i):\n        # binary search through cum_size\n        file_index = bisect(self._cum_size, i) - 1\n        seq_index = i - self._cum_size[file_index]\n        data = self._buffer[file_index]\n        return self._get_data(data, seq_index)\n\n    def _get_data(self, data, seq_index):\n        pass\n\n    def _data_per_sequence(self, data_length):\n        pass\n\n\nclass RolloutSequenceDataset(_RolloutDataset): # pylint: disable=too-few-public-methods\n    """""" Encapsulates rollouts.\n\n    Rollouts should be stored in subdirs of the root directory, in the form of npz files,\n    each containing a dictionary with the keys:\n        - observations: (rollout_len, *obs_shape)\n        - actions: (rollout_len, action_size)\n        - rewards: (rollout_len,)\n        - terminals: (rollout_len,), boolean\n\n     As the dataset is too big to be entirely stored in rams, only chunks of it\n     are stored, consisting of a constant number of files (determined by the\n     buffer_size parameter).  Once built, buffers must be loaded with the\n     load_next_buffer method.\n\n    Data are then provided in the form of tuples (obs, action, reward, terminal, next_obs):\n    - obs: (seq_len, *obs_shape)\n    - actions: (seq_len, action_size)\n    - reward: (seq_len,)\n    - terminal: (seq_len,) boolean\n    - next_obs: (seq_len, *obs_shape)\n\n    NOTE: seq_len < rollout_len in moste use cases\n\n    :args root: root directory of data sequences\n    :args seq_len: number of timesteps extracted from each rollout\n    :args transform: transformation of the observations\n    :args train: if True, train data, else test\n    """"""\n    def __init__(self, root, seq_len, transform, buffer_size=200, train=True): # pylint: disable=too-many-arguments\n        super().__init__(root, transform, buffer_size, train)\n        self._seq_len = seq_len\n\n    def _get_data(self, data, seq_index):\n        obs_data = data[\'observations\'][seq_index:seq_index + self._seq_len + 1]\n        obs_data = self._transform(obs_data.astype(np.float32))\n        obs, next_obs = obs_data[:-1], obs_data[1:]\n        action = data[\'actions\'][seq_index+1:seq_index + self._seq_len + 1]\n        action = action.astype(np.float32)\n        reward, terminal = [data[key][seq_index+1:\n                                      seq_index + self._seq_len + 1].astype(np.float32)\n                            for key in (\'rewards\', \'terminals\')]\n        # data is given in the form\n        # (obs, action, reward, terminal, next_obs)\n        return obs, action, reward, terminal, next_obs\n\n    def _data_per_sequence(self, data_length):\n        return data_length - self._seq_len\n\nclass RolloutObservationDataset(_RolloutDataset): # pylint: disable=too-few-public-methods\n    """""" Encapsulates rollouts.\n\n    Rollouts should be stored in subdirs of the root directory, in the form of npz files,\n    each containing a dictionary with the keys:\n        - observations: (rollout_len, *obs_shape)\n        - actions: (rollout_len, action_size)\n        - rewards: (rollout_len,)\n        - terminals: (rollout_len,), boolean\n\n     As the dataset is too big to be entirely stored in rams, only chunks of it\n     are stored, consisting of a constant number of files (determined by the\n     buffer_size parameter).  Once built, buffers must be loaded with the\n     load_next_buffer method.\n\n    Data are then provided in the form of images\n\n    :args root: root directory of data sequences\n    :args seq_len: number of timesteps extracted from each rollout\n    :args transform: transformation of the observations\n    :args train: if True, train data, else test\n    """"""\n    def _data_per_sequence(self, data_length):\n        return data_length\n\n    def _get_data(self, data, seq_index):\n        return self._transform(data[\'observations\'][seq_index])\n'"
envs/__init__.py,0,b''
envs/simulated_carracing.py,11,"b'""""""\nSimulated carracing environment.\n""""""\nimport argparse\nfrom os.path import join, exists\nimport torch\nfrom torch.distributions.categorical import Categorical\nimport gym\nfrom gym import spaces\nfrom models.vae import VAE\nfrom models.mdrnn import MDRNNCell\nfrom utils.misc import LSIZE, RSIZE, RED_SIZE\n\nimport numpy as np\n\nclass SimulatedCarracing(gym.Env): # pylint: disable=too-many-instance-attributes\n    """"""\n    Simulated Car Racing.\n\n    Gym environment using learnt VAE and MDRNN to simulate the\n    CarRacing-v0 environment.\n\n    :args directory: directory from which the vae and mdrnn are\n    loaded.\n    """"""\n    def __init__(self, directory):\n        vae_file = join(directory, \'vae\', \'best.tar\')\n        rnn_file = join(directory, \'mdrnn\', \'best.tar\')\n        assert exists(vae_file), ""No VAE model in the directory...""\n        assert exists(rnn_file), ""No MDRNN model in the directory...""\n\n        # spaces\n        self.action_space = spaces.Box(np.array([-1, 0, 0]), np.array([1, 1, 1]))\n        self.observation_space = spaces.Box(low=0, high=255, shape=(RED_SIZE, RED_SIZE, 3),\n                                            dtype=np.uint8)\n\n        # load VAE\n        vae = VAE(3, LSIZE)\n        vae_state = torch.load(vae_file, map_location=lambda storage, location: storage)\n        print(""Loading VAE at epoch {}, ""\n              ""with test error {}..."".format(\n                  vae_state[\'epoch\'], vae_state[\'precision\']))\n        vae.load_state_dict(vae_state[\'state_dict\'])\n        self._decoder = vae.decoder\n\n        # load MDRNN\n        self._rnn = MDRNNCell(32, 3, RSIZE, 5)\n        rnn_state = torch.load(rnn_file, map_location=lambda storage, location: storage)\n        print(""Loading MDRNN at epoch {}, ""\n              ""with test error {}..."".format(\n                  rnn_state[\'epoch\'], rnn_state[\'precision\']))\n        rnn_state_dict = {k.strip(\'_l0\'): v for k, v in rnn_state[\'state_dict\'].items()}\n        self._rnn.load_state_dict(rnn_state_dict)\n\n        # init state\n        self._lstate = torch.randn(1, LSIZE)\n        self._hstate = 2 * [torch.zeros(1, RSIZE)]\n\n        # obs\n        self._obs = None\n        self._visual_obs = None\n\n        # rendering\n        self.monitor = None\n        self.figure = None\n\n    def reset(self):\n        """""" Resetting """"""\n        import matplotlib.pyplot as plt\n        self._lstate = torch.randn(1, LSIZE)\n        self._hstate = 2 * [torch.zeros(1, RSIZE)]\n\n        # also reset monitor\n        if not self.monitor:\n            self.figure = plt.figure()\n            self.monitor = plt.imshow(\n                np.zeros((RED_SIZE, RED_SIZE, 3),\n                         dtype=np.uint8))\n\n    def step(self, action):\n        """""" One step forward """"""\n        with torch.no_grad():\n            action = torch.Tensor(action).unsqueeze(0)\n            mu, sigma, pi, r, d, n_h = self._rnn(action, self._lstate, self._hstate)\n            pi = pi.squeeze()\n            mixt = Categorical(torch.exp(pi)).sample().item()\n\n            self._lstate = mu[:, mixt, :] # + sigma[:, mixt, :] * torch.randn_like(mu[:, mixt, :])\n            self._hstate = n_h\n\n            self._obs = self._decoder(self._lstate)\n            np_obs = self._obs.numpy()\n            np_obs = np.clip(np_obs, 0, 1) * 255\n            np_obs = np.transpose(np_obs, (0, 2, 3, 1))\n            np_obs = np_obs.squeeze()\n            np_obs = np_obs.astype(np.uint8)\n            self._visual_obs = np_obs\n\n            return np_obs, r.item(), d.item() > 0\n\n    def render(self): # pylint: disable=arguments-differ\n        """""" Rendering """"""\n        import matplotlib.pyplot as plt\n        if not self.monitor:\n            self.figure = plt.figure()\n            self.monitor = plt.imshow(\n                np.zeros((RED_SIZE, RED_SIZE, 3),\n                         dtype=np.uint8))\n        self.monitor.set_data(self._visual_obs)\n        plt.pause(.01)\n\nif __name__ == \'__main__\':\n    # argument parsing\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--logdir\', type=str, help=\'Directory from which MDRNN and VAE are \'\n                        \'retrieved.\')\n    args = parser.parse_args()\n    env = SimulatedCarracing(args.logdir)\n\n    env.reset()\n    action = np.array([0., 0., 0.])\n\n    def on_key_press(event):\n        """""" Defines key pressed behavior """"""\n        if event.key == \'up\':\n            action[1] = 1\n        if event.key == \'down\':\n            action[2] = .8\n        if event.key == \'left\':\n            action[0] = -1\n        if event.key == \'right\':\n            action[0] = 1\n\n    def on_key_release(event):\n        """""" Defines key pressed behavior """"""\n        if event.key == \'up\':\n            action[1] = 0\n        if event.key == \'down\':\n            action[2] = 0\n        if event.key == \'left\' and action[0] == -1:\n            action[0] = 0\n        if event.key == \'right\' and action[0] == 1:\n            action[0] = 0\n\n    env.figure.canvas.mpl_connect(\'key_press_event\', on_key_press)\n    env.figure.canvas.mpl_connect(\'key_release_event\', on_key_release)\n    while True:\n        _, _, done = env.step(action)\n        env.render()\n        if done:\n            break\n'"
models/__init__.py,0,"b'"""""" Models package """"""\nfrom models.vae import VAE, Encoder, Decoder\nfrom models.mdrnn import MDRNN, MDRNNCell\nfrom models.controller import Controller\n\n__all__ = [\'VAE\', \'Encoder\', \'Decoder\',\n           \'MDRNN\', \'MDRNNCell\', \'Controller\']\n'"
models/controller.py,2,"b'"""""" Define controller """"""\nimport torch\nimport torch.nn as nn\n\nclass Controller(nn.Module):\n    """""" Controller """"""\n    def __init__(self, latents, recurrents, actions):\n        super().__init__()\n        self.fc = nn.Linear(latents + recurrents, actions)\n\n    def forward(self, *inputs):\n        cat_in = torch.cat(inputs, dim=1)\n        return self.fc(cat_in)\n'"
models/mdrnn.py,13,"b'""""""\nDefine MDRNN model, supposed to be used as a world model\non the latent space.\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as f\nfrom torch.distributions.normal import Normal\n\ndef gmm_loss(batch, mus, sigmas, logpi, reduce=True): # pylint: disable=too-many-arguments\n    """""" Computes the gmm loss.\n\n    Compute minus the log probability of batch under the GMM model described\n    by mus, sigmas, pi. Precisely, with bs1, bs2, ... the sizes of the batch\n    dimensions (several batch dimension are useful when you have both a batch\n    axis and a time step axis), gs the number of mixtures and fs the number of\n    features.\n\n    :args batch: (bs1, bs2, *, fs) torch tensor\n    :args mus: (bs1, bs2, *, gs, fs) torch tensor\n    :args sigmas: (bs1, bs2, *, gs, fs) torch tensor\n    :args logpi: (bs1, bs2, *, gs) torch tensor\n    :args reduce: if not reduce, the mean in the following formula is ommited\n\n    :returns:\n    loss(batch) = - mean_{i1=0..bs1, i2=0..bs2, ...} log(\n        sum_{k=1..gs} pi[i1, i2, ..., k] * N(\n            batch[i1, i2, ..., :] | mus[i1, i2, ..., k, :], sigmas[i1, i2, ..., k, :]))\n\n    NOTE: The loss is not reduced along the feature dimension (i.e. it should scale ~linearily\n    with fs).\n    """"""\n    batch = batch.unsqueeze(-2)\n    normal_dist = Normal(mus, sigmas)\n    g_log_probs = normal_dist.log_prob(batch)\n    g_log_probs = logpi + torch.sum(g_log_probs, dim=-1)\n    max_log_probs = torch.max(g_log_probs, dim=-1, keepdim=True)[0]\n    g_log_probs = g_log_probs - max_log_probs\n\n    g_probs = torch.exp(g_log_probs)\n    probs = torch.sum(g_probs, dim=-1)\n\n    log_prob = max_log_probs.squeeze() + torch.log(probs)\n    if reduce:\n        return - torch.mean(log_prob)\n    return - log_prob\n\nclass _MDRNNBase(nn.Module):\n    def __init__(self, latents, actions, hiddens, gaussians):\n        super().__init__()\n        self.latents = latents\n        self.actions = actions\n        self.hiddens = hiddens\n        self.gaussians = gaussians\n\n        self.gmm_linear = nn.Linear(\n            hiddens, (2 * latents + 1) * gaussians + 2)\n\n    def forward(self, *inputs):\n        pass\n\nclass MDRNN(_MDRNNBase):\n    """""" MDRNN model for multi steps forward """"""\n    def __init__(self, latents, actions, hiddens, gaussians):\n        super().__init__(latents, actions, hiddens, gaussians)\n        self.rnn = nn.LSTM(latents + actions, hiddens)\n\n    def forward(self, actions, latents): # pylint: disable=arguments-differ\n        """""" MULTI STEPS forward.\n\n        :args actions: (SEQ_LEN, BSIZE, ASIZE) torch tensor\n        :args latents: (SEQ_LEN, BSIZE, LSIZE) torch tensor\n\n        :returns: mu_nlat, sig_nlat, pi_nlat, rs, ds, parameters of the GMM\n        prediction for the next latent, gaussian prediction of the reward and\n        logit prediction of terminality.\n            - mu_nlat: (SEQ_LEN, BSIZE, N_GAUSS, LSIZE) torch tensor\n            - sigma_nlat: (SEQ_LEN, BSIZE, N_GAUSS, LSIZE) torch tensor\n            - logpi_nlat: (SEQ_LEN, BSIZE, N_GAUSS) torch tensor\n            - rs: (SEQ_LEN, BSIZE) torch tensor\n            - ds: (SEQ_LEN, BSIZE) torch tensor\n        """"""\n        seq_len, bs = actions.size(0), actions.size(1)\n\n        ins = torch.cat([actions, latents], dim=-1)\n        outs, _ = self.rnn(ins)\n        gmm_outs = self.gmm_linear(outs)\n\n        stride = self.gaussians * self.latents\n\n        mus = gmm_outs[:, :, :stride]\n        mus = mus.view(seq_len, bs, self.gaussians, self.latents)\n\n        sigmas = gmm_outs[:, :, stride:2 * stride]\n        sigmas = sigmas.view(seq_len, bs, self.gaussians, self.latents)\n        sigmas = torch.exp(sigmas)\n\n        pi = gmm_outs[:, :, 2 * stride: 2 * stride + self.gaussians]\n        pi = pi.view(seq_len, bs, self.gaussians)\n        logpi = f.log_softmax(pi, dim=-1)\n\n        rs = gmm_outs[:, :, -2]\n\n        ds = gmm_outs[:, :, -1]\n\n        return mus, sigmas, logpi, rs, ds\n\nclass MDRNNCell(_MDRNNBase):\n    """""" MDRNN model for one step forward """"""\n    def __init__(self, latents, actions, hiddens, gaussians):\n        super().__init__(latents, actions, hiddens, gaussians)\n        self.rnn = nn.LSTMCell(latents + actions, hiddens)\n\n    def forward(self, action, latent, hidden): # pylint: disable=arguments-differ\n        """""" ONE STEP forward.\n\n        :args actions: (BSIZE, ASIZE) torch tensor\n        :args latents: (BSIZE, LSIZE) torch tensor\n        :args hidden: (BSIZE, RSIZE) torch tensor\n\n        :returns: mu_nlat, sig_nlat, pi_nlat, r, d, next_hidden, parameters of\n        the GMM prediction for the next latent, gaussian prediction of the\n        reward, logit prediction of terminality and next hidden state.\n            - mu_nlat: (BSIZE, N_GAUSS, LSIZE) torch tensor\n            - sigma_nlat: (BSIZE, N_GAUSS, LSIZE) torch tensor\n            - logpi_nlat: (BSIZE, N_GAUSS) torch tensor\n            - rs: (BSIZE) torch tensor\n            - ds: (BSIZE) torch tensor\n        """"""\n        in_al = torch.cat([action, latent], dim=1)\n\n        next_hidden = self.rnn(in_al, hidden)\n        out_rnn = next_hidden[0]\n\n        out_full = self.gmm_linear(out_rnn)\n\n        stride = self.gaussians * self.latents\n\n        mus = out_full[:, :stride]\n        mus = mus.view(-1, self.gaussians, self.latents)\n\n        sigmas = out_full[:, stride:2 * stride]\n        sigmas = sigmas.view(-1, self.gaussians, self.latents)\n        sigmas = torch.exp(sigmas)\n\n        pi = out_full[:, 2 * stride:2 * stride + self.gaussians]\n        pi = pi.view(-1, self.gaussians)\n        logpi = f.log_softmax(pi, dim=-1)\n\n        r = out_full[:, -2]\n\n        d = out_full[:, -1]\n\n        return mus, sigmas, logpi, r, d, next_hidden\n'"
models/vae.py,3,"b'\n""""""\nVariational encoder model, used as a visual model\nfor our model of the world.\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Decoder(nn.Module):\n    """""" VAE decoder """"""\n    def __init__(self, img_channels, latent_size):\n        super(Decoder, self).__init__()\n        self.latent_size = latent_size\n        self.img_channels = img_channels\n\n        self.fc1 = nn.Linear(latent_size, 1024)\n        self.deconv1 = nn.ConvTranspose2d(1024, 128, 5, stride=2)\n        self.deconv2 = nn.ConvTranspose2d(128, 64, 5, stride=2)\n        self.deconv3 = nn.ConvTranspose2d(64, 32, 6, stride=2)\n        self.deconv4 = nn.ConvTranspose2d(32, img_channels, 6, stride=2)\n\n    def forward(self, x): # pylint: disable=arguments-differ\n        x = F.relu(self.fc1(x))\n        x = x.unsqueeze(-1).unsqueeze(-1)\n        x = F.relu(self.deconv1(x))\n        x = F.relu(self.deconv2(x))\n        x = F.relu(self.deconv3(x))\n        reconstruction = F.sigmoid(self.deconv4(x))\n        return reconstruction\n\nclass Encoder(nn.Module): # pylint: disable=too-many-instance-attributes\n    """""" VAE encoder """"""\n    def __init__(self, img_channels, latent_size):\n        super(Encoder, self).__init__()\n        self.latent_size = latent_size\n        #self.img_size = img_size\n        self.img_channels = img_channels\n\n        self.conv1 = nn.Conv2d(img_channels, 32, 4, stride=2)\n        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n        self.conv3 = nn.Conv2d(64, 128, 4, stride=2)\n        self.conv4 = nn.Conv2d(128, 256, 4, stride=2)\n\n        self.fc_mu = nn.Linear(2*2*256, latent_size)\n        self.fc_logsigma = nn.Linear(2*2*256, latent_size)\n\n\n    def forward(self, x): # pylint: disable=arguments-differ\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = x.view(x.size(0), -1)\n\n        mu = self.fc_mu(x)\n        logsigma = self.fc_logsigma(x)\n\n        return mu, logsigma\n\nclass VAE(nn.Module):\n    """""" Variational Autoencoder """"""\n    def __init__(self, img_channels, latent_size):\n        super(VAE, self).__init__()\n        self.encoder = Encoder(img_channels, latent_size)\n        self.decoder = Decoder(img_channels, latent_size)\n\n    def forward(self, x): # pylint: disable=arguments-differ\n        mu, logsigma = self.encoder(x)\n        sigma = logsigma.exp()\n        eps = torch.randn_like(sigma)\n        z = eps.mul(sigma).add_(mu)\n\n        recon_x = self.decoder(z)\n        return recon_x, mu, logsigma\n'"
tests/test_data.py,4,"b'"""""" Test data loading utilities """"""\nimport time\nimport unittest\nimport torch\nimport numpy as np\nfrom torchvision import transforms\nfrom data.loaders import RolloutSequenceDataset\nfrom data.loaders import RolloutObservationDataset\n\nclass TestData(unittest.TestCase):\n    """""" Test data loading """"""\n    def test_rollout_data(self):\n        """""" Test rollout sequence dataset """"""\n        transform = transforms.Lambda(lambda x: np.transpose(x, (0, 3, 1, 2)))\n        dataset = RolloutSequenceDataset(\'datasets/carracing\', 32, transform)\n        loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True,\n                                             num_workers=8)\n        dataset.load_next_buffer()\n        init_time = time.time()\n        for i, data in enumerate(loader):\n            if i == 150:\n                self.assertEqual(data[0].size(), torch.Size([8, 32, 3, 96, 96]))\n                break\n        end_time = time.time()\n        print(""WallTime: {}s"".format(end_time - init_time))\n\n    def test_observation_data(self):\n        """""" Test observation data """"""\n        transform = transforms.ToTensor()\n        dataset = RolloutObservationDataset(\'datasets/carracing\', transform)\n        loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True,\n                                             num_workers=8)\n        dataset.load_next_buffer()\n        init_time = time.time()\n        for i, data in enumerate(loader):\n            if i == 150:\n                self.assertEqual(data.size(), torch.Size([32, 3, 96, 96]))\n                break\n        end_time = time.time()\n        print(""WallTime: {}s"".format(end_time - init_time))\n'"
tests/test_envs.py,0,"b'""""""\nTesting environments\n(Non visual testing)\n""""""\nimport unittest\nfrom envs.simulated_carracing import SimulatedCarracing\nfrom utils.misc import sample_continuous_policy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nFPS = 50\n\nclass TestEnvs(unittest.TestCase):\n    """""" Test environments """"""\n    def test_simulated_carracing(self):\n        """""" Test simulated Car Racing """"""\n        env = SimulatedCarracing(\'logs/exp0\')\n        env.reset()\n        seq_len = 1000\n        actions = sample_continuous_policy(\n            env.action_space, seq_len, 1. / FPS)\n        for i in range(seq_len):\n            action = actions[i]\n            next_obs, reward, terminal = env.step(action)\n            env.render()\n            print(next_obs.shape, reward)\n            if terminal:\n                break\n'"
tests/test_gmm.py,13,"b'"""""" Test gmm loss """"""\nimport unittest\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as f\nfrom tqdm import tqdm\nfrom torch.distributions.categorical import Categorical\nfrom models.mdrnn import gmm_loss\n\nclass TestGMM(unittest.TestCase):\n    """""" Test GMMs """"""\n    def test_gmm_loss(self):\n        """""" Test case 1 """"""\n        n_samples = 10000\n\n        means = torch.Tensor([[0., 0.],\n                              [1., 1.],\n                              [-1., 1.]])\n        stds = torch.Tensor([[.03, .05],\n                             [.02, .1],\n                             [.1, .03]])\n        pi = torch.Tensor([.2, .3, .5])\n\n        cat_dist = Categorical(pi)\n        indices = cat_dist.sample((n_samples,)).long()\n        rands = torch.randn(n_samples, 2)\n\n        samples = means[indices] + rands * stds[indices]\n\n        class _model(nn.Module):\n            def __init__(self, gaussians):\n                super().__init__()\n                self.means = nn.Parameter(torch.Tensor(1, gaussians, 2).normal_())\n                self.pre_stds = nn.Parameter(torch.Tensor(1, gaussians, 2).normal_())\n                self.pi = nn.Parameter(torch.Tensor(1, gaussians).normal_())\n\n            def forward(self, *inputs):\n                return self.means, torch.exp(self.pre_stds), f.softmax(self.pi, dim=1)\n\n        model = _model(3)\n        optimizer = torch.optim.Adam(model.parameters())\n\n        iterations = 100000\n        log_step = iterations // 10\n        pbar = tqdm(total=iterations)\n        cum_loss = 0\n        for i in range(iterations):\n            batch = samples[torch.LongTensor(128).random_(0, n_samples)]\n            m, s, p = model.forward()\n            loss = gmm_loss(batch, m, s, p)\n            cum_loss += loss.item()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            pbar.set_postfix_str(""avg_loss={:10.6f}"".format(\n                cum_loss / (i + 1)))\n            pbar.update(1)\n            if i % log_step == log_step - 1:\n                print(m)\n                print(s)\n                print(p)\n'"
utils/__init__.py,0,b''
utils/learning.py,2,"b'"""""" Learning utilities """"""\nfrom functools import partial\nfrom torch.optim import Optimizer\n\nclass EarlyStopping(object): # pylint: disable=R0902\n    """"""\n    Gives a criterion to stop training when a given metric is not\n    improving anymore\n    Args:\n        mode (str): One of `min`, `max`. In `min` mode, training will\n            be stopped when the quantity monitored has stopped\n            decreasing; in `max` mode it will be stopped when the\n            quantity monitored has stopped increasing. Default: \'min\'.\n        patience (int): Number of epochs with no improvement after\n            which training is stopped. For example, if\n            `patience = 2`, then we will ignore the first 2 epochs\n            with no improvement, and will only stop learning after the\n            3rd epoch if the loss still hasn\'t improved then.\n            Default: 10.\n        threshold (float): Threshold for measuring the new optimum,\n            to only focus on significant changes. Default: 1e-4.\n        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\n            dynamic_threshold = best * ( 1 + threshold ) in \'max\'\n            mode or best * ( 1 - threshold ) in `min` mode.\n            In `abs` mode, dynamic_threshold = best + threshold in\n            `max` mode or best - threshold in `min` mode. Default: \'rel\'.\n\n    """"""\n\n    def __init__(self, mode=\'min\', patience=10, threshold=1e-4, threshold_mode=\'rel\'):\n        self.patience = patience\n        self.mode = mode\n        self.threshold = threshold\n        self.threshold_mode = threshold_mode\n        self.best = None\n        self.num_bad_epochs = None\n        self.mode_worse = None  # the worse value for the chosen mode\n        self.is_better = None\n        self.last_epoch = -1\n        self._init_is_better(mode=mode, threshold=threshold,\n                             threshold_mode=threshold_mode)\n        self._reset()\n\n    def _reset(self):\n        """"""Resets num_bad_epochs counter and cooldown counter.""""""\n        self.best = self.mode_worse\n        self.num_bad_epochs = 0\n\n    def step(self, metrics, epoch=None):\n        """""" Updates early stopping state """"""\n        current = metrics\n        if epoch is None:\n            epoch = self.last_epoch = self.last_epoch + 1\n        self.last_epoch = epoch\n\n        if self.is_better(current, self.best):\n            self.best = current\n            self.num_bad_epochs = 0\n        else:\n            self.num_bad_epochs += 1\n\n    @property\n    def stop(self):\n        """""" Should we stop learning? """"""\n        return self.num_bad_epochs > self.patience\n\n\n    def _cmp(self, mode, threshold_mode, threshold, a, best): # pylint: disable=R0913, R0201\n        if mode == \'min\' and threshold_mode == \'rel\':\n            rel_epsilon = 1. - threshold\n            return a < best * rel_epsilon\n\n        elif mode == \'min\' and threshold_mode == \'abs\':\n            return a < best - threshold\n\n        elif mode == \'max\' and threshold_mode == \'rel\':\n            rel_epsilon = threshold + 1.\n            return a > best * rel_epsilon\n\n        return a > best + threshold\n\n    def _init_is_better(self, mode, threshold, threshold_mode):\n        if mode not in {\'min\', \'max\'}:\n            raise ValueError(\'mode \' + mode + \' is unknown!\')\n        if threshold_mode not in {\'rel\', \'abs\'}:\n            raise ValueError(\'threshold mode \' + threshold_mode + \' is unknown!\')\n\n        if mode == \'min\':\n            self.mode_worse = float(\'inf\')\n        else:  # mode == \'max\':\n            self.mode_worse = (-float(\'inf\'))\n\n        self.is_better = partial(self._cmp, mode, threshold_mode, threshold)\n\n    def state_dict(self):\n        """""" Returns early stopping state """"""\n        return {key: value for key, value in self.__dict__.items() if key != \'is_better\'}\n\n    def load_state_dict(self, state_dict):\n        """""" Loads early stopping state """"""\n        self.__dict__.update(state_dict)\n        self._init_is_better(mode=self.mode, threshold=self.threshold,\n                             threshold_mode=self.threshold_mode)\n\n\n\n############################################################\n#### WARNING : THIS IS A TEMPORARY CODE WHICH HAS      #####\n####  TO BE REMOVED WITH PYTORCH 0.5                   #####\n#### IT IS COPY OF THE 0.5 VERSION OF THE LR SCHEDULER #####\n############################################################\nclass ReduceLROnPlateau(object): # pylint: disable=R0902\n    """"""Reduce learning rate when a metric has stopped improving.\n    Models often benefit from reducing the learning rate by a factor\n    of 2-10 once learning stagnates. This scheduler reads a metrics\n    quantity and if no improvement is seen for a \'patience\' number\n    of epochs, the learning rate is reduced.\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        mode (str): One of `min`, `max`. In `min` mode, lr will\n            be reduced when the quantity monitored has stopped\n            decreasing; in `max` mode it will be reduced when the\n            quantity monitored has stopped increasing. Default: \'min\'.\n        factor (float): Factor by which the learning rate will be\n            reduced. new_lr = lr * factor. Default: 0.1.\n        patience (int): Number of epochs with no improvement after\n            which learning rate will be reduced. For example, if\n            `patience = 2`, then we will ignore the first 2 epochs\n            with no improvement, and will only decrease the LR after the\n            3rd epoch if the loss still hasn\'t improved then.\n            Default: 10.\n        verbose (bool): If ``True``, prints a message to stdout for\n            each update. Default: ``False``.\n        threshold (float): Threshold for measuring the new optimum,\n            to only focus on significant changes. Default: 1e-4.\n        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\n            dynamic_threshold = best * ( 1 + threshold ) in \'max\'\n            mode or best * ( 1 - threshold ) in `min` mode.\n            In `abs` mode, dynamic_threshold = best + threshold in\n            `max` mode or best - threshold in `min` mode. Default: \'rel\'.\n        cooldown (int): Number of epochs to wait before resuming\n            normal operation after lr has been reduced. Default: 0.\n        min_lr (float or list): A scalar or a list of scalars. A\n            lower bound on the learning rate of all param groups\n            or each group respectively. Default: 0.\n        eps (float): Minimal decay applied to lr. If the difference\n            between new and old lr is smaller than eps, the update is\n            ignored. Default: 1e-8.\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = ReduceLROnPlateau(optimizer, \'min\')\n        >>> for epoch in range(10):\n        >>>     train(...)\n        >>>     val_loss = validate(...)\n        >>>     # Note that step should be called after validate()\n        >>>     scheduler.step(val_loss)\n    """"""\n\n    def __init__(self, optimizer, mode=\'min\', factor=0.1, patience=10, # pylint: disable=R0913\n                 verbose=False, threshold=1e-4, threshold_mode=\'rel\',\n                 cooldown=0, min_lr=0, eps=1e-8):\n\n        if factor >= 1.0:\n            raise ValueError(\'Factor should be < 1.0.\')\n        self.factor = factor\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError(\'{} is not an Optimizer\'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(min_lr, (list, tuple)):\n            if len(min_lr) != len(optimizer.param_groups):\n                raise ValueError(""expected {} min_lrs, got {}"".format(\n                    len(optimizer.param_groups), len(min_lr)))\n            self.min_lrs = list(min_lr)\n        else:\n            self.min_lrs = [min_lr] * len(optimizer.param_groups)\n\n        self.patience = patience\n        self.verbose = verbose\n        self.cooldown = cooldown\n        self.cooldown_counter = 0\n        self.mode = mode\n        self.threshold = threshold\n        self.threshold_mode = threshold_mode\n        self.best = None\n        self.num_bad_epochs = None\n        self.mode_worse = None  # the worse value for the chosen mode\n        self.is_better = None\n        self.eps = eps\n        self.last_epoch = -1\n        self._init_is_better(mode=mode, threshold=threshold,\n                             threshold_mode=threshold_mode)\n        self._reset()\n\n    def _reset(self):\n        """"""Resets num_bad_epochs counter and cooldown counter.""""""\n        self.best = self.mode_worse\n        self.cooldown_counter = 0\n        self.num_bad_epochs = 0\n\n    def step(self, metrics, epoch=None):\n        """""" Updates scheduler state """"""\n        current = metrics\n        if epoch is None:\n            epoch = self.last_epoch = self.last_epoch + 1\n        self.last_epoch = epoch\n\n        if self.is_better(current, self.best):\n            self.best = current\n            self.num_bad_epochs = 0\n        else:\n            self.num_bad_epochs += 1\n\n        if self.in_cooldown:\n            self.cooldown_counter -= 1\n            self.num_bad_epochs = 0  # ignore any bad epochs in cooldown\n\n        if self.num_bad_epochs > self.patience:\n            self._reduce_lr(epoch)\n            self.cooldown_counter = self.cooldown\n            self.num_bad_epochs = 0\n\n    def _reduce_lr(self, epoch):\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            old_lr = float(param_group[\'lr\'])\n            new_lr = max(old_lr * self.factor, self.min_lrs[i])\n            if old_lr - new_lr > self.eps:\n                param_group[\'lr\'] = new_lr\n                if self.verbose:\n                    print(\'Epoch {:5d}: reducing learning rate\'\n                          \' of group {} to {:.4e}.\'.format(epoch, i, new_lr))\n\n    @property\n    def in_cooldown(self):\n        """""" Are we on CD? """"""\n        return self.cooldown_counter > 0\n\n    def _cmp(self, mode, threshold_mode, threshold, a, best): # pylint: disable=R0913,R0201\n        if mode == \'min\' and threshold_mode == \'rel\':\n            rel_epsilon = 1. - threshold\n            return a < best * rel_epsilon\n\n        elif mode == \'min\' and threshold_mode == \'abs\':\n            return a < best - threshold\n\n        elif mode == \'max\' and threshold_mode == \'rel\':\n            rel_epsilon = threshold + 1.\n            return a > best * rel_epsilon\n\n        return a > best + threshold\n\n    def _init_is_better(self, mode, threshold, threshold_mode):\n        if mode not in {\'min\', \'max\'}:\n            raise ValueError(\'mode \' + mode + \' is unknown!\')\n        if threshold_mode not in {\'rel\', \'abs\'}:\n            raise ValueError(\'threshold mode \' + threshold_mode + \' is unknown!\')\n\n        if mode == \'min\':\n            self.mode_worse = float(\'inf\')\n        else:  # mode == \'max\':\n            self.mode_worse = (-float(\'inf\'))\n\n        self.is_better = partial(self._cmp, mode, threshold_mode, threshold)\n\n    def state_dict(self):\n        """""" Returns scheduler state """"""\n        return {key: value for key, value in self.__dict__.items()\n                if key not in {\'optimizer\', \'is_better\'}}\n\n    def load_state_dict(self, state_dict):\n        """""" Loads scheduler state """"""\n        self.__dict__.update(state_dict)\n        self._init_is_better(mode=self.mode, threshold=self.threshold,\n                             threshold_mode=self.threshold_mode)\n'"
utils/misc.py,7,"b'"""""" Various auxiliary utilities """"""\nimport math\nfrom os.path import join, exists\nimport torch\nfrom torchvision import transforms\nimport numpy as np\nfrom models import MDRNNCell, VAE, Controller\nimport gym\nimport gym.envs.box2d\n\n# A bit dirty: manually change size of car racing env\ngym.envs.box2d.car_racing.STATE_W, gym.envs.box2d.car_racing.STATE_H = 64, 64\n\n# Hardcoded for now\nASIZE, LSIZE, RSIZE, RED_SIZE, SIZE =\\\n    3, 32, 256, 64, 64\n\n# Same\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((RED_SIZE, RED_SIZE)),\n    transforms.ToTensor()\n])\n\ndef sample_continuous_policy(action_space, seq_len, dt):\n    """""" Sample a continuous policy.\n\n    Atm, action_space is supposed to be a box environment. The policy is\n    sampled as a brownian motion a_{t+1} = a_t + sqrt(dt) N(0, 1).\n\n    :args action_space: gym action space\n    :args seq_len: number of actions returned\n    :args dt: temporal discretization\n\n    :returns: sequence of seq_len actions\n    """"""\n    actions = [action_space.sample()]\n    for _ in range(seq_len):\n        daction_dt = np.random.randn(*actions[-1].shape)\n        actions.append(\n            np.clip(actions[-1] + math.sqrt(dt) * daction_dt,\n                    action_space.low, action_space.high))\n    return actions\n\ndef save_checkpoint(state, is_best, filename, best_filename):\n    """""" Save state in filename. Also save in best_filename if is_best. """"""\n    torch.save(state, filename)\n    if is_best:\n        torch.save(state, best_filename)\n\ndef flatten_parameters(params):\n    """""" Flattening parameters.\n\n    :args params: generator of parameters (as returned by module.parameters())\n\n    :returns: flattened parameters (i.e. one tensor of dimension 1 with all\n        parameters concatenated)\n    """"""\n    return torch.cat([p.detach().view(-1) for p in params], dim=0).cpu().numpy()\n\ndef unflatten_parameters(params, example, device):\n    """""" Unflatten parameters.\n\n    :args params: parameters as a single 1D np array\n    :args example: generator of parameters (as returned by module.parameters()),\n        used to reshape params\n    :args device: where to store unflattened parameters\n\n    :returns: unflattened parameters\n    """"""\n    params = torch.Tensor(params).to(device)\n    idx = 0\n    unflattened = []\n    for e_p in example:\n        unflattened += [params[idx:idx + e_p.numel()].view(e_p.size())]\n        idx += e_p.numel()\n    return unflattened\n\ndef load_parameters(params, controller):\n    """""" Load flattened parameters into controller.\n\n    :args params: parameters as a single 1D np array\n    :args controller: module in which params is loaded\n    """"""\n    proto = next(controller.parameters())\n    params = unflatten_parameters(\n        params, controller.parameters(), proto.device)\n\n    for p, p_0 in zip(controller.parameters(), params):\n        p.data.copy_(p_0)\n\nclass RolloutGenerator(object):\n    """""" Utility to generate rollouts.\n\n    Encapsulate everything that is needed to generate rollouts in the TRUE ENV\n    using a controller with previously trained VAE and MDRNN.\n\n    :attr vae: VAE model loaded from mdir/vae\n    :attr mdrnn: MDRNN model loaded from mdir/mdrnn\n    :attr controller: Controller, either loaded from mdir/ctrl or randomly\n        initialized\n    :attr env: instance of the CarRacing-v0 gym environment\n    :attr device: device used to run VAE, MDRNN and Controller\n    :attr time_limit: rollouts have a maximum of time_limit timesteps\n    """"""\n    def __init__(self, mdir, device, time_limit):\n        """""" Build vae, rnn, controller and environment. """"""\n        # Loading world model and vae\n        vae_file, rnn_file, ctrl_file = \\\n            [join(mdir, m, \'best.tar\') for m in [\'vae\', \'mdrnn\', \'ctrl\']]\n\n        assert exists(vae_file) and exists(rnn_file),\\\n            ""Either vae or mdrnn is untrained.""\n\n        vae_state, rnn_state = [\n            torch.load(fname, map_location={\'cuda:0\': str(device)})\n            for fname in (vae_file, rnn_file)]\n\n        for m, s in ((\'VAE\', vae_state), (\'MDRNN\', rnn_state)):\n            print(""Loading {} at epoch {} ""\n                  ""with test loss {}"".format(\n                      m, s[\'epoch\'], s[\'precision\']))\n\n        self.vae = VAE(3, LSIZE).to(device)\n        self.vae.load_state_dict(vae_state[\'state_dict\'])\n\n        self.mdrnn = MDRNNCell(LSIZE, ASIZE, RSIZE, 5).to(device)\n        self.mdrnn.load_state_dict(\n            {k.strip(\'_l0\'): v for k, v in rnn_state[\'state_dict\'].items()})\n\n        self.controller = Controller(LSIZE, RSIZE, ASIZE).to(device)\n\n        # load controller if it was previously saved\n        if exists(ctrl_file):\n            ctrl_state = torch.load(ctrl_file, map_location={\'cuda:0\': str(device)})\n            print(""Loading Controller with reward {}"".format(\n                ctrl_state[\'reward\']))\n            self.controller.load_state_dict(ctrl_state[\'state_dict\'])\n\n        self.env = gym.make(\'CarRacing-v0\')\n        self.device = device\n\n        self.time_limit = time_limit\n\n    def get_action_and_transition(self, obs, hidden):\n        """""" Get action and transition.\n\n        Encode obs to latent using the VAE, then obtain estimation for next\n        latent and next hidden state using the MDRNN and compute the controller\n        corresponding action.\n\n        :args obs: current observation (1 x 3 x 64 x 64) torch tensor\n        :args hidden: current hidden state (1 x 256) torch tensor\n\n        :returns: (action, next_hidden)\n            - action: 1D np array\n            - next_hidden (1 x 256) torch tensor\n        """"""\n        _, latent_mu, _ = self.vae(obs)\n        action = self.controller(latent_mu, hidden[0])\n        _, _, _, _, _, next_hidden = self.mdrnn(action, latent_mu, hidden)\n        return action.squeeze().cpu().numpy(), next_hidden\n\n    def rollout(self, params, render=False):\n        """""" Execute a rollout and returns minus cumulative reward.\n\n        Load :params: into the controller and execute a single rollout. This\n        is the main API of this class.\n\n        :args params: parameters as a single 1D np array\n\n        :returns: minus cumulative reward\n        """"""\n        # copy params into the controller\n        if params is not None:\n            load_parameters(params, self.controller)\n\n        obs = self.env.reset()\n\n        # This first render is required !\n        self.env.render()\n\n        hidden = [\n            torch.zeros(1, RSIZE).to(self.device)\n            for _ in range(2)]\n\n        cumulative = 0\n        i = 0\n        while True:\n            obs = transform(obs).unsqueeze(0).to(self.device)\n            action, hidden = self.get_action_and_transition(obs, hidden)\n            obs, reward, done, _ = self.env.step(action)\n\n            if render:\n                self.env.render()\n\n            cumulative += reward\n            if done or i > self.time_limit:\n                return - cumulative\n            i += 1\n'"
