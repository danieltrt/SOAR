file_path,api_count,code
imagenet.py,16,"b'\'\'\'\nTraining script for ImageNet\nCopyright (c) Wei YANG, 2017\n\'\'\'\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport random\nimport shutil\nimport time\nimport warnings\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nimport models.imagenet as customized_models\nfrom utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\nfrom utils.dataloaders import *\nfrom tensorboardX import SummaryWriter\n\ndefault_model_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\ncustomized_models_names = sorted(name for name in customized_models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(customized_models.__dict__[name]))\n\nfor name in customized_models.__dict__:\n    if name.islower() and not name.startswith(""__"") and callable(customized_models.__dict__[name]):\n        models.__dict__[name] = customized_models.__dict__[name]\n\nmodel_names = default_model_names + customized_models_names\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\nparser.add_argument(\'-d\', \'--data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--data-backend\', metavar=\'BACKEND\', default=\'pytorch\',\n                    choices=DATA_BACKEND_CHOICES)\nparser.add_argument(\'-a\', \'--arch\', metavar=\'ARCH\', default=\'resnet18\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: resnet18)\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--epochs\', default=90, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                    metavar=\'N\',\n                    help=\'mini-batch size (default: 256), this is the total \'\n                         \'batch size of all GPUs on the current node when \'\n                         \'using Data Parallel or Distributed Data Parallel\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\', dest=\'lr\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--wd\', \'--weight-decay\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\',\n                    dest=\'weight_decay\')\nparser.add_argument(\'-p\', \'--print-freq\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                    help=\'use pre-trained model\')\nparser.add_argument(\'--world-size\', default=-1, type=int,\n                    help=\'number of nodes for distributed training\')\nparser.add_argument(\'--rank\', default=-1, type=int,\n                    help=\'node rank for distributed training\')\nparser.add_argument(\'--dist-url\', default=\'tcp://224.66.41.62:23456\', type=str,\n                    help=\'url used to set up distributed training\')\nparser.add_argument(\'--dist-backend\', default=\'nccl\', type=str,\n                    help=\'distributed backend\')\nparser.add_argument(\'--seed\', default=None, type=int,\n                    help=\'seed for initializing training. \')\n\nparser.add_argument(\'--lr-decay\', type=str, default=\'step\',\n                    help=\'mode for learning rate decay\')\nparser.add_argument(\'--step\', type=int, default=30,\n                    help=\'interval for learning rate decay in step mode\')\nparser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[150, 225],\n                    help=\'decrease learning rate at these epochs.\')\nparser.add_argument(\'--gamma\', type=float, default=0.1,\n                    help=\'LR is multiplied by gamma on schedule.\')\nparser.add_argument(\'--warmup\', action=\'store_true\',\n                    help=\'set lower initial learning rate to warm up the training\')\n\nparser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoints\', type=str, metavar=\'PATH\',\n                    help=\'path to save checkpoint (default: checkpoints)\')\n\nparser.add_argument(\'--width-mult\', type=float, default=1.0, help=\'MobileNet model width multiplier.\')\nparser.add_argument(\'--input-size\', type=int, default=224, help=\'MobileNet model input resolution\')\nparser.add_argument(\'--weight\', default=\'\', type=str, metavar=\'WEIGHT\',\n                    help=\'path to pretrained weight (default: none)\')\n\n\nbest_prec1 = 0\n\n\ndef main():\n    global args, best_prec1\n    args = parser.parse_args()\n\n    if args.seed is not None:\n        random.seed(args.seed)\n        torch.manual_seed(args.seed)\n        cudnn.deterministic = True\n        warnings.warn(\'You have chosen to seed training. \'\n                      \'This will turn on the CUDNN deterministic setting, \'\n                      \'which can slow down your training considerably! \'\n                      \'You may see unexpected behavior when restarting \'\n                      \'from checkpoints.\')\n\n    args.distributed = args.world_size > 1\n\n    if args.distributed:\n        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n                                world_size=args.world_size)\n\n    # create model\n    print(""=> creating model \'{}\'"".format(args.arch))\n    model = models.__dict__[args.arch](width_mult=args.width_mult)\n\n    if not args.distributed:\n        if args.arch.startswith(\'alexnet\') or args.arch.startswith(\'vgg\'):\n            model.features = torch.nn.DataParallel(model.features)\n            model.cuda()\n        else:\n            model = torch.nn.DataParallel(model).cuda()\n    else:\n        model.cuda()\n        model = torch.nn.parallel.DistributedDataParallel(model)\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    # optionally resume from a checkpoint\n    title = \'ImageNet-\' + args.arch\n    if not os.path.isdir(args.checkpoint):\n        mkdir_p(args.checkpoint)\n\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_prec1 = checkpoint[\'best_prec1\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n            args.checkpoint = os.path.dirname(args.resume)\n            logger = Logger(os.path.join(args.checkpoint, \'log.txt\'), title=title, resume=True)\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        logger = Logger(os.path.join(args.checkpoint, \'log.txt\'), title=title)\n        logger.set_names([\'Learning Rate\', \'Train Loss\', \'Valid Loss\', \'Train Acc.\', \'Valid Acc.\'])\n\n\n    cudnn.benchmark = True\n\n    # Data loading code\n    if args.data_backend == \'pytorch\':\n        get_train_loader = get_pytorch_train_loader\n        get_val_loader = get_pytorch_val_loader\n    elif args.data_backend == \'dali-gpu\':\n        get_train_loader = get_dali_train_loader(dali_cpu=False)\n        get_val_loader = get_dali_val_loader()\n    elif args.data_backend == \'dali-cpu\':\n        get_train_loader = get_dali_train_loader(dali_cpu=True)\n        get_val_loader = get_dali_val_loader()\n\n    train_loader, train_loader_len = get_train_loader(args.data, args.batch_size, workers=args.workers, input_size=args.input_size)\n    val_loader, val_loader_len = get_val_loader(args.data, args.batch_size, workers=args.workers, input_size=args.input_size)\n\n    if args.evaluate:\n        from collections import OrderedDict\n        if os.path.isfile(args.weight):\n            print(""=> loading pretrained weight \'{}\'"".format(args.weight))\n            source_state = torch.load(args.weight)\n            target_state = OrderedDict()\n            for k, v in source_state.items():\n                if k[:7] != \'module.\':\n                    k = \'module.\' + k\n                target_state[k] = v\n            model.load_state_dict(target_state)\n        else:\n            print(""=> no weight found at \'{}\'"".format(args.weight))\n\n        validate(val_loader, val_loader_len, model, criterion)\n        return\n\n    # visualization\n    writer = SummaryWriter(os.path.join(args.checkpoint, \'logs\'))\n\n    for epoch in range(args.start_epoch, args.epochs):\n        if args.distributed:\n            train_sampler.set_epoch(epoch)\n\n        print(\'\\nEpoch: [%d | %d]\' % (epoch + 1, args.epochs))\n\n        # train for one epoch\n        train_loss, train_acc = train(train_loader, train_loader_len, model, criterion, optimizer, epoch)\n\n        # evaluate on validation set\n        val_loss, prec1 = validate(val_loader, val_loader_len, model, criterion)\n\n        lr = optimizer.param_groups[0][\'lr\']\n\n        # append logger file\n        logger.append([lr, train_loss, val_loss, train_acc, prec1])\n\n        # tensorboardX\n        writer.add_scalar(\'learning rate\', lr, epoch + 1)\n        writer.add_scalars(\'loss\', {\'train loss\': train_loss, \'validation loss\': val_loss}, epoch + 1)\n        writer.add_scalars(\'accuracy\', {\'train accuracy\': train_acc, \'validation accuracy\': prec1}, epoch + 1)\n\n        is_best = prec1 > best_prec1\n        best_prec1 = max(prec1, best_prec1)\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'arch\': args.arch,\n            \'state_dict\': model.state_dict(),\n            \'best_prec1\': best_prec1,\n            \'optimizer\' : optimizer.state_dict(),\n        }, is_best, checkpoint=args.checkpoint)\n\n    logger.close()\n    logger.plot()\n    savefig(os.path.join(args.checkpoint, \'log.eps\'))\n    writer.close()\n\n    print(\'Best accuracy:\')\n    print(best_prec1)\n\n\ndef train(train_loader, train_loader_len, model, criterion, optimizer, epoch):\n    bar = Bar(\'Processing\', max=train_loader_len)\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n        adjust_learning_rate(optimizer, epoch, i, train_loader_len)\n\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        target = target.cuda(non_blocking=True)\n\n        # compute output\n        output = model(input)\n        loss = criterion(output, target)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n        losses.update(loss.item(), input.size(0))\n        top1.update(prec1.item(), input.size(0))\n        top5.update(prec5.item(), input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                    batch=i + 1,\n                    size=train_loader_len,\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    top1=top1.avg,\n                    top5=top5.avg,\n                    )\n        bar.next()\n    bar.finish()\n    return (losses.avg, top1.avg)\n\n\ndef validate(val_loader, val_loader_len, model, criterion):\n    bar = Bar(\'Processing\', max=val_loader_len)\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target) in enumerate(val_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        target = target.cuda(non_blocking=True)\n\n        with torch.no_grad():\n            # compute output\n            output = model(input)\n            loss = criterion(output, target)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n        losses.update(loss.item(), input.size(0))\n        top1.update(prec1.item(), input.size(0))\n        top5.update(prec5.item(), input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}\'.format(\n                    batch=i + 1,\n                    size=val_loader_len,\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    top1=top1.avg,\n                    top5=top5.avg,\n                    )\n        bar.next()\n    bar.finish()\n    return (losses.avg, top1.avg)\n\n\ndef save_checkpoint(state, is_best, checkpoint=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'model_best.pth.tar\'))\n\n\nfrom math import cos, pi\ndef adjust_learning_rate(optimizer, epoch, iteration, num_iter):\n    lr = optimizer.param_groups[0][\'lr\']\n\n    warmup_epoch = 5 if args.warmup else 0\n    warmup_iter = warmup_epoch * num_iter\n    current_iter = iteration + epoch * num_iter\n    max_iter = args.epochs * num_iter\n\n    if args.lr_decay == \'step\':\n        lr = args.lr * (args.gamma ** ((current_iter - warmup_iter) // (max_iter - warmup_iter)))\n    elif args.lr_decay == \'cos\':\n        lr = args.lr * (1 + cos(pi * (current_iter - warmup_iter) / (max_iter - warmup_iter))) / 2\n    elif args.lr_decay == \'linear\':\n        lr = args.lr * (1 - (current_iter - warmup_iter) / (max_iter - warmup_iter))\n    elif args.lr_decay == \'schedule\':\n        count = sum([1 for s in args.schedule if s <= epoch])\n        lr = args.lr * pow(args.gamma, count)\n    else:\n        raise ValueError(\'Unknown lr mode {}\'.format(args.lr_decay))\n\n    if epoch < warmup_epoch:\n        lr = args.lr * current_iter / warmup_iter\n\n\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/__init__.py,0,"b'""""""Useful utils\n""""""\nfrom .misc import *\nfrom .logger import *\nfrom .visualize import *\nfrom .eval import *\n\n# progress bar\nimport os, sys\nsys.path.append(os.path.join(os.path.dirname(__file__), ""progress""))\nfrom progress.bar import Bar as Bar'"
utils/dataloaders.py,28,"b'import os\nimport torch\nimport numpy as np\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nDATA_BACKEND_CHOICES = [\'pytorch\']\ntry:\n    from nvidia.dali.plugin.pytorch import DALIClassificationIterator\n    from nvidia.dali.pipeline import Pipeline\n    import nvidia.dali.ops as ops\n    import nvidia.dali.types as types\n    DATA_BACKEND_CHOICES.append(\'dali-gpu\')\n    DATA_BACKEND_CHOICES.append(\'dali-cpu\')\nexcept ImportError:\n    print(""Please install DALI from https://www.github.com/NVIDIA/DALI to run this example."")\n\n\nclass HybridTrainPipe(Pipeline):\n    def __init__(self, batch_size, num_threads, device_id, data_dir, crop, dali_cpu=False):\n        super(HybridTrainPipe, self).__init__(batch_size, num_threads, device_id, seed = 12 + device_id)\n        if torch.distributed.is_initialized():\n            local_rank = torch.distributed.get_rank()\n            world_size = torch.distributed.get_world_size()\n        else:\n            local_rank = 0\n            world_size = 1\n\n        self.input = ops.FileReader(\n                file_root = data_dir,\n                shard_id = local_rank,\n                num_shards = world_size,\n                random_shuffle = True)\n\n        if dali_cpu:\n            dali_device = ""cpu""\n            self.decode = ops.HostDecoderRandomCrop(device=dali_device, output_type=types.RGB,\n                                                    random_aspect_ratio=[0.75, 4./3.],\n                                                    random_area=[0.08, 1.0],\n                                                    num_attempts=100)\n        else:\n            dali_device = ""gpu""\n            # This padding sets the size of the internal nvJPEG buffers to be able to handle all images from full-sized ImageNet\n            # without additional reallocations\n            self.decode = ops.nvJPEGDecoderRandomCrop(device=""mixed"", output_type=types.RGB, device_memory_padding=211025920, host_memory_padding=140544512,\n                                                      random_aspect_ratio=[0.75, 4./3.],\n                                                      random_area=[0.08, 1.0],\n                                                      num_attempts=100)\n\n        self.res = ops.Resize(device=dali_device, resize_x=crop, resize_y=crop, interp_type=types.INTERP_TRIANGULAR)\n        self.cmnp = ops.CropMirrorNormalize(device = ""gpu"",\n                                            output_dtype = types.FLOAT,\n                                            output_layout = types.NCHW,\n                                            crop = (crop, crop),\n                                            image_type = types.RGB,\n                                            mean = [0.485 * 255,0.456 * 255,0.406 * 255],\n                                            std = [0.229 * 255,0.224 * 255,0.225 * 255])\n        self.coin = ops.CoinFlip(probability = 0.5)\n\n    def define_graph(self):\n        rng = self.coin()\n        self.jpegs, self.labels = self.input(name = ""Reader"")\n        images = self.decode(self.jpegs)\n        images = self.res(images)\n        output = self.cmnp(images.gpu(), mirror = rng)\n        return [output, self.labels]\n\n\nclass HybridValPipe(Pipeline):\n    def __init__(self, batch_size, num_threads, device_id, data_dir, crop, size):\n        super(HybridValPipe, self).__init__(batch_size, num_threads, device_id, seed = 12 + device_id)\n        if torch.distributed.is_initialized():\n            local_rank = torch.distributed.get_rank()\n            world_size = torch.distributed.get_world_size()\n        else:\n            local_rank = 0\n            world_size = 1\n\n        self.input = ops.FileReader(\n                file_root = data_dir,\n                shard_id = local_rank,\n                num_shards = world_size,\n                random_shuffle = False)\n\n        self.decode = ops.nvJPEGDecoder(device = ""mixed"", output_type = types.RGB)\n        self.res = ops.Resize(device = ""gpu"", resize_shorter = size)\n        self.cmnp = ops.CropMirrorNormalize(device = ""gpu"",\n                output_dtype = types.FLOAT,\n                output_layout = types.NCHW,\n                crop = (crop, crop),\n                image_type = types.RGB,\n                mean = [0.485 * 255,0.456 * 255,0.406 * 255],\n                std = [0.229 * 255,0.224 * 255,0.225 * 255])\n\n    def define_graph(self):\n        self.jpegs, self.labels = self.input(name = ""Reader"")\n        images = self.decode(self.jpegs)\n        images = self.res(images)\n        output = self.cmnp(images)\n        return [output, self.labels]\n\n\nclass DALIWrapper(object):\n    def gen_wrapper(dalipipeline):\n        for data in dalipipeline:\n            input = data[0][""data""]\n            target = data[0][""label""].squeeze().cuda().long()\n            yield input, target\n        dalipipeline.reset()\n\n    def __init__(self, dalipipeline):\n        self.dalipipeline = dalipipeline\n\n    def __iter__(self):\n        return DALIWrapper.gen_wrapper(self.dalipipeline)\n\ndef get_dali_train_loader(dali_cpu=False):\n    def gdtl(data_path, batch_size, workers=5, _worker_init_fn=None):\n        if torch.distributed.is_initialized():\n            local_rank = torch.distributed.get_rank()\n            world_size = torch.distributed.get_world_size()\n        else:\n            local_rank = 0\n            world_size = 1\n\n        traindir = os.path.join(data_path, \'train\')\n\n        pipe = HybridTrainPipe(batch_size=batch_size, num_threads=workers,\n                device_id = local_rank,\n                data_dir = traindir, crop = 224, dali_cpu=dali_cpu)\n\n        pipe.build()\n        test_run = pipe.run()\n        train_loader = DALIClassificationIterator(pipe, size = int(pipe.epoch_size(""Reader"") / world_size))\n\n        return DALIWrapper(train_loader), int(pipe.epoch_size(""Reader"") / (world_size * batch_size))\n\n    return gdtl\n\n\ndef get_dali_val_loader():\n    def gdvl(data_path, batch_size, workers=5, _worker_init_fn=None):\n        if torch.distributed.is_initialized():\n            local_rank = torch.distributed.get_rank()\n            world_size = torch.distributed.get_world_size()\n        else:\n            local_rank = 0\n            world_size = 1\n\n        valdir = os.path.join(data_path, \'val\')\n\n        pipe = HybridValPipe(batch_size=batch_size, num_threads=workers,\n                device_id = local_rank,\n                data_dir = valdir,\n                crop = 224, size = 256)\n        pipe.build()\n        test_run = pipe.run()\n        val_loader = DALIClassificationIterator(pipe, size = int(pipe.epoch_size(""Reader"") / world_size), fill_last_batch=False)\n\n        return DALIWrapper(val_loader), int(pipe.epoch_size(""Reader"") / (world_size * batch_size))\n    return gdvl\n\n\ndef fast_collate(batch):\n    imgs = [img[0] for img in batch]\n    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n    w = imgs[0].size[0]\n    h = imgs[0].size[1]\n    tensor = torch.zeros( (len(imgs), 3, h, w), dtype=torch.uint8 )\n    for i, img in enumerate(imgs):\n        nump_array = np.asarray(img, dtype=np.uint8)\n        tens = torch.from_numpy(nump_array)\n        if(nump_array.ndim < 3):\n            nump_array = np.expand_dims(nump_array, axis=-1)\n        nump_array = np.rollaxis(nump_array, 2)\n\n        tensor[i] += torch.from_numpy(nump_array)\n\n    return tensor, targets\n\n\nclass PrefetchedWrapper(object):\n    def prefetched_loader(loader):\n        mean = torch.tensor([0.485 * 255, 0.456 * 255, 0.406 * 255]).cuda().view(1,3,1,1)\n        std = torch.tensor([0.229 * 255, 0.224 * 255, 0.225 * 255]).cuda().view(1,3,1,1)\n\n        stream = torch.cuda.Stream()\n        first = True\n\n        for next_input, next_target in loader:\n            with torch.cuda.stream(stream):\n                next_input = next_input.cuda(async=True)\n                next_target = next_target.cuda(async=True)\n                next_input = next_input.float()\n                next_input = next_input.sub_(mean).div_(std)\n\n            if not first:\n                yield input, target\n            else:\n                first = False\n\n            torch.cuda.current_stream().wait_stream(stream)\n            input = next_input\n            target = next_target\n\n        yield input, target\n\n    def __init__(self, dataloader):\n        self.dataloader = dataloader\n        self.epoch = 0\n\n    def __iter__(self):\n        if (self.dataloader.sampler is not None and\n            isinstance(self.dataloader.sampler,\n                       torch.utils.data.distributed.DistributedSampler)):\n\n            self.dataloader.sampler.set_epoch(self.epoch)\n        self.epoch += 1\n        return PrefetchedWrapper.prefetched_loader(self.dataloader)\n\ndef get_pytorch_train_loader(data_path, batch_size, workers=5, _worker_init_fn=None, input_size=224):\n    traindir = os.path.join(data_path, \'train\')\n    train_dataset = datasets.ImageFolder(\n            traindir,\n            transforms.Compose([\n                transforms.RandomResizedCrop(input_size),\n                transforms.RandomHorizontalFlip(),\n                ]))\n\n    if torch.distributed.is_initialized():\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n    else:\n        train_sampler = None\n\n    train_loader = torch.utils.data.DataLoader(\n            train_dataset, batch_size=batch_size, shuffle=(train_sampler is None),\n            num_workers=workers, worker_init_fn=_worker_init_fn, pin_memory=True, sampler=train_sampler, collate_fn=fast_collate)\n\n    return PrefetchedWrapper(train_loader), len(train_loader)\n\ndef get_pytorch_val_loader(data_path, batch_size, workers=5, _worker_init_fn=None, input_size=224):\n    valdir = os.path.join(data_path, \'val\')\n    val_dataset = datasets.ImageFolder(\n            valdir, transforms.Compose([\n                transforms.Resize(int(input_size / 0.875)),\n                transforms.CenterCrop(input_size),\n                ]))\n\n    if torch.distributed.is_initialized():\n        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n    else:\n        val_sampler = None\n\n    val_loader = torch.utils.data.DataLoader(\n            val_dataset,\n            sampler=val_sampler,\n            batch_size=batch_size, shuffle=False,\n            num_workers=workers, worker_init_fn=_worker_init_fn, pin_memory=True,\n            collate_fn=fast_collate)\n\n    return PrefetchedWrapper(val_loader), len(val_loader)\n'"
utils/eval.py,1,"b'from __future__ import print_function, absolute_import\nimport torch\n\n__all__ = [\'accuracy\']\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n'"
utils/logger.py,0,"b'# A simple torch style logger\n# (C) Wei YANG 2017\nfrom __future__ import absolute_import\nimport matplotlib.pyplot as plt\nimport os\nimport sys\nimport numpy as np\n\n__all__ = [\'Logger\', \'LoggerMonitor\', \'savefig\']\n\ndef savefig(fname, dpi=None):\n    dpi = 150 if dpi == None else dpi\n    plt.savefig(fname, dpi=dpi)\n    \ndef plot_overlap(logger, names=None):\n    names = logger.names if names == None else names\n    numbers = logger.numbers\n    for _, name in enumerate(names):\n        x = np.arange(len(numbers[name]))\n        plt.plot(x, np.asarray(numbers[name]))\n    return [logger.title + \'(\' + name + \')\' for name in names]\n\nclass Logger(object):\n    \'\'\'Save training process to log file with simple plot function.\'\'\'\n    def __init__(self, fpath, title=None, resume=False): \n        self.file = None\n        self.resume = resume\n        self.title = \'\' if title == None else title\n        if fpath is not None:\n            if resume: \n                self.file = open(fpath, \'r\') \n                name = self.file.readline()\n                self.names = name.rstrip().split(\'\\t\')\n                self.numbers = {}\n                for _, name in enumerate(self.names):\n                    self.numbers[name] = []\n\n                for numbers in self.file:\n                    numbers = numbers.rstrip().split(\'\\t\')\n                    for i in range(0, len(numbers)):\n                        self.numbers[self.names[i]].append(numbers[i])\n                self.file.close()\n                self.file = open(fpath, \'a\')  \n            else:\n                self.file = open(fpath, \'w\')\n\n    def set_names(self, names):\n        if self.resume: \n            pass\n        # initialize numbers as empty list\n        self.numbers = {}\n        self.names = names\n        for _, name in enumerate(self.names):\n            self.file.write(name)\n            self.file.write(\'\\t\')\n            self.numbers[name] = []\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n\n    def append(self, numbers):\n        assert len(self.names) == len(numbers), \'Numbers do not match names\'\n        for index, num in enumerate(numbers):\n            self.file.write(""{0:.6f}"".format(num))\n            self.file.write(\'\\t\')\n            self.numbers[self.names[index]].append(num)\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def plot(self, names=None):   \n        names = self.names if names == None else names\n        numbers = self.numbers\n        for _, name in enumerate(names):\n            x = np.arange(len(numbers[name]))\n            plt.plot(x, np.asarray(numbers[name]))\n        plt.legend([self.title + \'(\' + name + \')\' for name in names])\n        plt.grid(True)\n\n    def close(self):\n        if self.file is not None:\n            self.file.close()\n\nclass LoggerMonitor(object):\n    \'\'\'Load and visualize multiple logs.\'\'\'\n    def __init__ (self, paths):\n        \'\'\'paths is a distionary with {name:filepath} pair\'\'\'\n        self.loggers = []\n        for title, path in paths.items():\n            logger = Logger(path, title=title, resume=True)\n            self.loggers.append(logger)\n\n    def plot(self, names=None):\n        plt.figure()\n        plt.subplot(121)\n        legend_text = []\n        for logger in self.loggers:\n            legend_text += plot_overlap(logger, names)\n        plt.legend(legend_text, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n        plt.grid(True)\n                    \nif __name__ == \'__main__\':\n    # # Example\n    # logger = Logger(\'test.txt\')\n    # logger.set_names([\'Train loss\', \'Valid loss\',\'Test loss\'])\n\n    # length = 100\n    # t = np.arange(length)\n    # train_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # valid_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # test_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n\n    # for i in range(0, length):\n    #     logger.append([train_loss[i], valid_loss[i], test_loss[i]])\n    # logger.plot()\n\n    # Example: logger monitor\n    paths = {\n    \'resadvnet20\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet20/log.txt\', \n    \'resadvnet32\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet32/log.txt\',\n    \'resadvnet44\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet44/log.txt\',\n    }\n\n    field = [\'Valid Acc.\']\n\n    monitor = LoggerMonitor(paths)\n    monitor.plot(names=field)\n    savefig(\'test.eps\')\n'"
utils/misc.py,6,"b'\'\'\'Some helper functions for PyTorch, including:\n    - get_mean_and_std: calculate the mean and std value of dataset.\n    - msr_init: net parameter initialization.\n    - progress_bar: progress bar mimic xlua.progress.\n\'\'\'\nimport errno\nimport os\nimport sys\nimport time\nimport math\n\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\n__all__ = [\'get_mean_and_std\', \'init_params\', \'mkdir_p\', \'AverageMeter\']\n\n\ndef get_mean_and_std(dataset):\n    \'\'\'Compute the mean and std value of dataset.\'\'\'\n    dataloader = trainloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print(\'==> Computing mean and std..\')\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:,i,:,:].mean()\n            std[i] += inputs[:,i,:,:].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\ndef init_params(net):\n    \'\'\'Init layer parameters.\'\'\'\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode=\'fan_out\')\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\ndef mkdir_p(path):\n    \'\'\'make dir if not exist\'\'\'\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value\n       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n    """"""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count'"
utils/visualize.py,6,"b""import matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom .misc import *   \n\n__all__ = ['make_image', 'show_batch', 'show_mask', 'show_mask_single']\n\n# functions to show an image\ndef make_image(img, mean=(0,0,0), std=(1,1,1)):\n    for i in range(0, 3):\n        img[i] = img[i] * std[i] + mean[i]    # unnormalize\n    npimg = img.numpy()\n    return np.transpose(npimg, (1, 2, 0))\n\ndef gauss(x,a,b,c):\n    return torch.exp(-torch.pow(torch.add(x,-b),2).div(2*c*c)).mul(a)\n\ndef colorize(x):\n    ''' Converts a one-channel grayscale image to a color heatmap image '''\n    if x.dim() == 2:\n        torch.unsqueeze(x, 0, out=x)\n    if x.dim() == 3:\n        cl = torch.zeros([3, x.size(1), x.size(2)])\n        cl[0] = gauss(x,.5,.6,.2) + gauss(x,1,.8,.3)\n        cl[1] = gauss(x,1,.5,.3)\n        cl[2] = gauss(x,1,.2,.3)\n        cl[cl.gt(1)] = 1\n    elif x.dim() == 4:\n        cl = torch.zeros([x.size(0), 3, x.size(2), x.size(3)])\n        cl[:,0,:,:] = gauss(x,.5,.6,.2) + gauss(x,1,.8,.3)\n        cl[:,1,:,:] = gauss(x,1,.5,.3)\n        cl[:,2,:,:] = gauss(x,1,.2,.3)\n    return cl\n\ndef show_batch(images, Mean=(2, 2, 2), Std=(0.5,0.5,0.5)):\n    images = make_image(torchvision.utils.make_grid(images), Mean, Std)\n    plt.imshow(images)\n    plt.show()\n\n\ndef show_mask_single(images, mask, Mean=(2, 2, 2), Std=(0.5,0.5,0.5)):\n    im_size = images.size(2)\n\n    # save for adding mask\n    im_data = images.clone()\n    for i in range(0, 3):\n        im_data[:,i,:,:] = im_data[:,i,:,:] * Std[i] + Mean[i]    # unnormalize\n\n    images = make_image(torchvision.utils.make_grid(images), Mean, Std)\n    plt.subplot(2, 1, 1)\n    plt.imshow(images)\n    plt.axis('off')\n\n    # for b in range(mask.size(0)):\n    #     mask[b] = (mask[b] - mask[b].min())/(mask[b].max() - mask[b].min())\n    mask_size = mask.size(2)\n    # print('Max %f Min %f' % (mask.max(), mask.min()))\n    mask = (upsampling(mask, scale_factor=im_size/mask_size))\n    # mask = colorize(upsampling(mask, scale_factor=im_size/mask_size))\n    # for c in range(3):\n    #     mask[:,c,:,:] = (mask[:,c,:,:] - Mean[c])/Std[c]\n\n    # print(mask.size())\n    mask = make_image(torchvision.utils.make_grid(0.3*im_data+0.7*mask.expand_as(im_data)))\n    # mask = make_image(torchvision.utils.make_grid(0.3*im_data+0.7*mask), Mean, Std)\n    plt.subplot(2, 1, 2)\n    plt.imshow(mask)\n    plt.axis('off')\n\ndef show_mask(images, masklist, Mean=(2, 2, 2), Std=(0.5,0.5,0.5)):\n    im_size = images.size(2)\n\n    # save for adding mask\n    im_data = images.clone()\n    for i in range(0, 3):\n        im_data[:,i,:,:] = im_data[:,i,:,:] * Std[i] + Mean[i]    # unnormalize\n\n    images = make_image(torchvision.utils.make_grid(images), Mean, Std)\n    plt.subplot(1+len(masklist), 1, 1)\n    plt.imshow(images)\n    plt.axis('off')\n\n    for i in range(len(masklist)):\n        mask = masklist[i].data.cpu()\n        # for b in range(mask.size(0)):\n        #     mask[b] = (mask[b] - mask[b].min())/(mask[b].max() - mask[b].min())\n        mask_size = mask.size(2)\n        # print('Max %f Min %f' % (mask.max(), mask.min()))\n        mask = (upsampling(mask, scale_factor=im_size/mask_size))\n        # mask = colorize(upsampling(mask, scale_factor=im_size/mask_size))\n        # for c in range(3):\n        #     mask[:,c,:,:] = (mask[:,c,:,:] - Mean[c])/Std[c]\n\n        # print(mask.size())\n        mask = make_image(torchvision.utils.make_grid(0.3*im_data+0.7*mask.expand_as(im_data)))\n        # mask = make_image(torchvision.utils.make_grid(0.3*im_data+0.7*mask), Mean, Std)\n        plt.subplot(1+len(masklist), 1, i+2)\n        plt.imshow(mask)\n        plt.axis('off')\n\n\n\n# x = torch.zeros(1, 3, 3)\n# out = colorize(x)\n# out_im = make_image(out)\n# plt.imshow(out_im)\n# plt.show()"""
models/imagenet/__init__.py,0,b'from __future__ import absolute_import\nfrom .mobilenetv2 import *\n'
models/imagenet/mobilenetv2.py,1,"b'""""""\nCreates a MobileNetV2 Model as defined in:\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. (2018). \nMobileNetV2: Inverted Residuals and Linear Bottlenecks\narXiv preprint arXiv:1801.04381.\nimport from https://github.com/tonylins/pytorch-mobilenet-v2\n""""""\n\nimport torch.nn as nn\nimport math\n\n__all__ = [\'mobilenetv2\']\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    """"""\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    """"""\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef conv_3x3_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.identity = stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.identity:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        # setting of inverted residual blocks\n        self.cfgs = [\n            # t, c, n, s\n            [1,  16, 1, 1],\n            [6,  24, 2, 2],\n            [6,  32, 3, 2],\n            [6,  64, 4, 2],\n            [6,  96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        input_channel = _make_divisible(32 * width_mult, 4 if width_mult == 0.1 else 8)\n        layers = [conv_3x3_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        block = InvertedResidual\n        for t, c, n, s in self.cfgs:\n            output_channel = _make_divisible(c * width_mult, 4 if width_mult == 0.1 else 8)\n            for i in range(n):\n                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t))\n                input_channel = output_channel\n        self.features = nn.Sequential(*layers)\n        # building last several layers\n        output_channel = _make_divisible(1280 * width_mult, 4 if width_mult == 0.1 else 8) if width_mult > 1.0 else 1280\n        self.conv = conv_1x1_bn(input_channel, output_channel)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Linear(output_channel, num_classes)\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.conv(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\ndef mobilenetv2(**kwargs):\n    """"""\n    Constructs a MobileNet V2 model\n    """"""\n    return MobileNetV2(**kwargs)\n\n'"
