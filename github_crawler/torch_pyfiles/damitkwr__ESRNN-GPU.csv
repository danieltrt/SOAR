file_path,api_count,code
__init__.py,0,b''
es_rnn/DRNN.py,11,"b'# lovingly borrowed from https://github.com/zalandoresearch/pytorch-dilated-rnn\n\nimport torch\nimport torch.nn as nn\nimport torch.autograd as autograd\n\nuse_cuda = torch.cuda.is_available()\n\n\nclass DRNN(nn.Module):\n\n    def __init__(self, n_input, n_hidden, n_layers, dilations, dropout=0, cell_type=\'GRU\', batch_first=False):\n\n        super(DRNN, self).__init__()\n\n        self.dilations = dilations\n        self.cell_type = cell_type\n        self.batch_first = batch_first\n\n        layers = []\n        if self.cell_type == ""GRU"":\n            cell = nn.GRU\n        elif self.cell_type == ""RNN"":\n            cell = nn.RNN\n        elif self.cell_type == ""LSTM"":\n            cell = nn.LSTM\n        else:\n            raise NotImplementedError\n\n        for i in range(n_layers):\n            if i == 0:\n                c = cell(n_input, n_hidden, dropout=dropout)\n            else:\n                c = cell(n_hidden, n_hidden, dropout=dropout)\n            layers.append(c)\n        self.cells = nn.Sequential(*layers)\n\n    def forward(self, inputs, hidden=None):\n        if self.batch_first:\n            inputs = inputs.transpose(0, 1)\n        outputs = []\n        for i, (cell, dilation) in enumerate(zip(self.cells, self.dilations)):\n            if hidden is None:\n                inputs, _ = self.drnn_layer(cell, inputs, dilation)\n            else:\n                inputs, hidden[i] = self.drnn_layer(cell, inputs, dilation, hidden[i])\n\n            outputs.append(inputs[-dilation:])\n\n        if self.batch_first:\n            inputs = inputs.transpose(0, 1)\n        return inputs, outputs\n\n    def drnn_layer(self, cell, inputs, rate, hidden=None):\n\n        n_steps = len(inputs)\n        batch_size = inputs[0].size(0)\n        hidden_size = cell.hidden_size\n\n        inputs, dilated_steps = self._pad_inputs(inputs, n_steps, rate)\n        dilated_inputs = self._prepare_inputs(inputs, rate)\n\n        if hidden is None:\n            dilated_outputs, hidden = self._apply_cell(dilated_inputs, cell, batch_size, rate, hidden_size)\n        else:\n            hidden = self._prepare_inputs(hidden, rate)\n            dilated_outputs, hidden = self._apply_cell(dilated_inputs, cell, batch_size, rate, hidden_size,\n                                                       hidden=hidden)\n\n        splitted_outputs = self._split_outputs(dilated_outputs, rate)\n        outputs = self._unpad_outputs(splitted_outputs, n_steps)\n\n        return outputs, hidden\n\n    def _apply_cell(self, dilated_inputs, cell, batch_size, rate, hidden_size, hidden=None):\n        if hidden is None:\n            if self.cell_type == \'LSTM\':\n                c, m = self.init_hidden(batch_size * rate, hidden_size)\n                hidden = (c.unsqueeze(0), m.unsqueeze(0))\n            else:\n                hidden = self.init_hidden(batch_size * rate, hidden_size).unsqueeze(0)\n\n        dilated_outputs, hidden = cell(dilated_inputs, hidden)\n\n        return dilated_outputs, hidden\n\n    def _unpad_outputs(self, splitted_outputs, n_steps):\n        return splitted_outputs[:n_steps]\n\n    def _split_outputs(self, dilated_outputs, rate):\n        batchsize = dilated_outputs.size(1) // rate\n\n        blocks = [dilated_outputs[:, i * batchsize: (i + 1) * batchsize, :] for i in range(rate)]\n\n        interleaved = torch.stack((blocks)).transpose(1, 0).contiguous()\n        interleaved = interleaved.view(dilated_outputs.size(0) * rate,\n                                       batchsize,\n                                       dilated_outputs.size(2))\n        return interleaved\n\n    def _pad_inputs(self, inputs, n_steps, rate):\n        iseven = (n_steps % rate) == 0\n\n        if not iseven:\n            dilated_steps = n_steps // rate + 1\n\n            zeros_ = torch.zeros(dilated_steps * rate - inputs.size(0),\n                                 inputs.size(1),\n                                 inputs.size(2))\n            if use_cuda:\n                zeros_ = zeros_.cuda()\n\n            inputs = torch.cat((inputs, autograd.Variable(zeros_)))\n        else:\n            dilated_steps = n_steps // rate\n\n        return inputs, dilated_steps\n\n    def _prepare_inputs(self, inputs, rate):\n        dilated_inputs = torch.cat([inputs[j::rate, :, :] for j in range(rate)], 1)\n        return dilated_inputs\n\n    def init_hidden(self, batch_size, hidden_dim):\n        hidden = autograd.Variable(torch.zeros(batch_size, hidden_dim))\n        if use_cuda:\n            hidden = hidden.cuda()\n        if self.cell_type == ""LSTM"":\n            memory = autograd.Variable(torch.zeros(batch_size, hidden_dim))\n            if use_cuda:\n                memory = memory.cuda()\n            return hidden, memory\n        else:\n            return hidden\n\n\nif __name__ == \'__main__\':\n    n_inp = 10\n    n_hidden = 16\n    n_layers = 3\n\n    model = DRNN(n_inp, n_hidden, n_layers, cell_type=\'LSTM\')\n\n    test_x1 = torch.autograd.Variable(torch.randn(26, 2, n_inp))\n    test_x2 = torch.autograd.Variable(torch.randn(26, 2, n_inp))\n\n    out, hidden = model(test_x1)\n'"
es_rnn/__init__.py,0,b''
es_rnn/config.py,1,"b'from math import sqrt\n\nimport torch\n\n\ndef get_config(interval):\n    config = {\n        \'prod\': True,\n        \'device\': (""cuda"" if torch.cuda.is_available() else ""cpu""),\n        \'percentile\': 50,\n        \'training_percentile\': 45,\n        \'add_nl_layer\': True,\n        \'rnn_cell_type\': \'LSTM\',\n        \'learning_rate\': 1e-3,\n        \'learning_rates\': ((10, 1e-4)),\n        \'num_of_train_epochs\': 15,\n        \'num_of_categories\': 6,  # in data provided\n        \'batch_size\': 1024,\n        \'gradient_clipping\': 20,\n        \'c_state_penalty\': 0,\n        \'min_learning_rate\': 0.0001,\n        \'lr_ratio\': sqrt(10),\n        \'lr_tolerance_multip\': 1.005,\n        \'min_epochs_before_changing_lrate\': 2,\n        \'print_train_batch_every\': 5,\n        \'print_output_stats\': 3,\n        \'lr_anneal_rate\': 0.5,\n        \'lr_anneal_step\': 5\n    }\n\n    if interval == \'Quarterly\':\n        config.update({\n            \'chop_val\': 72,\n            \'variable\': ""Quarterly"",\n            \'dilations\': ((1, 2), (4, 8)),\n            \'state_hsize\': 40,\n            \'seasonality\': 4,\n            \'input_size\': 4,\n            \'output_size\': 8,\n            \'level_variability_penalty\': 80\n        })\n    elif interval == \'Monthly\':\n        config.update({\n            #     RUNTIME PARAMETERS\n            \'chop_val\': 72,\n            \'variable\': ""Monthly"",\n            \'dilations\': ((1, 3), (6, 12)),\n            \'state_hsize\': 50,\n            \'seasonality\': 12,\n            \'input_size\': 12,\n            \'output_size\': 18,\n            \'level_variability_penalty\': 50\n        })\n    elif interval == \'Daily\':\n        config.update({\n            #     RUNTIME PARAMETERS\n            \'chop_val\': 200,\n            \'variable\': ""Daily"",\n            \'dilations\': ((1, 7), (14, 28)),\n            \'state_hsize\': 50,\n            \'seasonality\': 7,\n            \'input_size\': 7,\n            \'output_size\': 14,\n            \'level_variability_penalty\': 50\n        })\n    elif interval == \'Yearly\':\n\n        config.update({\n            #     RUNTIME PARAMETERS\n            \'chop_val\': 25,\n            \'variable\': ""Yearly"",\n            \'dilations\': ((1, 2), (2, 6)),\n            \'state_hsize\': 30,\n            \'seasonality\': 1,\n            \'input_size\': 4,\n            \'output_size\': 6,\n            \'level_variability_penalty\': 0\n        })\n    else:\n        print(""I don\'t have that config. :("")\n\n    config[\'input_size_i\'] = config[\'input_size\']\n    config[\'output_size_i\'] = config[\'output_size\']\n    config[\'tau\'] = config[\'percentile\'] / 100\n    config[\'training_tau\'] = config[\'training_percentile\'] / 100\n\n    if not config[\'prod\']:\n        config[\'batch_size\'] = 10\n        config[\'num_of_train_epochs\'] = 15\n\n    return config\n'"
es_rnn/data_loading.py,5,"b'import numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\n\n\ndef read_file(file_location):\n    series = []\n    ids = []\n    with open(file_location, \'r\') as file:\n        data = file.read().split(""\\n"")\n\n    for i in range(1, len(data) - 1):\n    # for i in range(1, len(data)):\n        row = data[i].replace(\'""\', \'\').split(\',\')\n        series.append(np.array([float(j) for j in row[1:] if j != """"]))\n        ids.append(row[0])\n\n    series = np.array(series)\n    return series\n\n\ndef create_val_set(train, output_size):\n    val = []\n    for i in range(len(train)):\n        val.append(train[i][-output_size:])\n        train[i] = train[i][:-output_size]\n    return np.array(val)\n\n\ndef chop_series(train, chop_val):\n    # CREATE MASK FOR VALUES TO BE CHOPPED\n    train_len_mask = [True if len(i) >= chop_val else False for i in train]\n    # FILTER AND CHOP TRAIN\n    train = [train[i][-chop_val:] for i in range(len(train)) if train_len_mask[i]]\n    return train, train_len_mask\n\n\ndef create_datasets(train_file_location, test_file_location, output_size):\n    train = read_file(train_file_location)\n    test = read_file(test_file_location)\n    val = create_val_set(train, output_size)\n    return train, val, test\n\n\nclass SeriesDataset(Dataset):\n\n    def __init__(self, dataTrain, dataVal, dataTest, info, variable, chop_value, device):\n        dataTrain, mask = chop_series(dataTrain, chop_value)\n\n        self.dataInfoCatOHE = pd.get_dummies(info[info[\'SP\'] == variable][\'category\'])\n        self.dataInfoCatHeaders = np.array([i for i in self.dataInfoCatOHE.columns.values])\n        self.dataInfoCat = torch.from_numpy(self.dataInfoCatOHE[mask].values).float()\n        self.dataTrain = [torch.tensor(dataTrain[i]) for i in range(len(dataTrain))]  # ALREADY MASKED IN CHOP FUNCTION\n        self.dataVal = [torch.tensor(dataVal[i]) for i in range(len(dataVal)) if mask[i]]\n        self.dataTest = [torch.tensor(dataTest[i]) for i in range(len(dataTest)) if mask[i]]\n        self.device = device\n\n    def __len__(self):\n        return len(self.dataTrain)\n\n    def __getitem__(self, idx):\n        return self.dataTrain[idx].to(self.device), \\\n               self.dataVal[idx].to(self.device), \\\n               self.dataTest[idx].to(self.device), \\\n               self.dataInfoCat[idx].to(self.device), \\\n               idx\n\n\ndef collate_lines(seq_list):\n    train_, val_, test_, info_cat_, idx_ = zip(*seq_list)\n    train_lens = [len(seq) for seq in train_]\n    seq_order = sorted(range(len(train_lens)), key=train_lens.__getitem__, reverse=True)\n    train = [train_[i] for i in seq_order]\n    val = [val_[i] for i in seq_order]\n    test = [test_[i] for i in seq_order]\n    info_cat = [info_cat_[i] for i in seq_order]\n    idx = [idx_[i] for i in seq_order]\n    return train, val, test, info_cat, idx\n\n'"
es_rnn/loss_modules.py,19,"b""import torch\nimport torch.nn as nn\nimport numpy as np\n\n# Expression pinBallLoss(const Expression& out_ex, const Expression& actuals_ex) {//used by Dynet, learning loss function\n#   vector<Expression> losses;\n#   for (unsigned int indx = 0; indx<OUTPUT_SIZE; indx++) {\n#     auto forec = pick(out_ex, indx);\n#     auto actual = pick(actuals_ex, indx);\n#     if (as_scalar(actual.value()) > as_scalar(forec.value()))\n#       losses.push_back((actual - forec)*TRAINING_TAU);\n#     else\n#       losses.push_back((actual - forec)*(TRAINING_TAU - 1));\n#   }\n#   return sum(losses) / OUTPUT_SIZE * 2;\n# }\n\n#     as defined in the blog post --- https://eng.uber.com/m4-forecasting-competition/\n\n\nclass PinballLoss(nn.Module):\n\n    def __init__(self, training_tau, output_size, device):\n        super(PinballLoss, self).__init__()\n        self.training_tau = training_tau\n        self.output_size = output_size\n        self.device = device\n\n    def forward(self, predictions, actuals):\n        cond = torch.zeros_like(predictions).to(self.device)\n        loss = torch.sub(actuals, predictions).to(self.device)\n\n        less_than = torch.mul(loss, torch.mul(torch.gt(loss, cond).type(torch.FloatTensor).to(self.device),\n                                              self.training_tau))\n\n        greater_than = torch.mul(loss, torch.mul(torch.lt(loss, cond).type(torch.FloatTensor).to(self.device),\n                                                 (self.training_tau - 1)))\n\n        final_loss = torch.add(less_than, greater_than)\n        # losses = []\n        # for i in range(self.output_size):\n        #     prediction = predictions[i]\n        #     actual = actuals[i]\n        #     if actual > prediction:\n        #         losses.append((actual - prediction) * self.training_tau)\n        #     else:\n        #         losses.append((actual - prediction) * (self.training_tau - 1))\n        # loss = torch.Tensor(losses)\n        return torch.sum(final_loss) / self.output_size * 2\n\n\n# test1 = torch.rand(100)\n# test2 = torch.rand(100)\n# pb = PinballLoss(0.5, 100)\n# pb(test1, test2)\n\n\n### sMAPE\n\n# float sMAPE(vector<float>& out_vect, vector<float>& actuals_vect) {\n#   float sumf = 0;\n#   for (unsigned int indx = 0; indx<OUTPUT_SIZE; indx++) {\n#     auto forec = out_vect[indx];\n#     auto actual = actuals_vect[indx];\n#     sumf+=abs(forec-actual)/(abs(forec)+abs(actual));\n#   }\n#   return sumf / OUTPUT_SIZE * 200;\n# }\n\n\ndef non_sMAPE(predictions, actuals, output_size):\n    sumf = 0\n    for i in range(output_size):\n        prediction = predictions[i]\n        actual = actuals[i]\n        sumf += abs(prediction - actual) / (abs(prediction) + abs(actual))\n    return sumf / output_size * 200\n\n\ndef sMAPE(predictions, actuals, N):\n    predictions = predictions.float()\n    actuals = actuals.float()\n    sumf = torch.sum(torch.abs(predictions - actuals) / (torch.abs(predictions) + torch.abs(actuals)))\n    return ((2 * sumf) / N) * 100\n\n\ndef np_sMAPE(predictions, actuals, N):\n    predictions = torch.from_numpy(np.array(predictions))\n    actuals = torch.from_numpy(np.array(actuals))\n    return float(sMAPE(predictions, actuals, N))\n\n\n### wQuantLoss\n\n# float wQuantLoss(vector<float>& out_vect, vector<float>& actuals_vect) {\n#   float sumf = 0; float suma=0;\n#   for (unsigned int indx = 0; indx<OUTPUT_SIZE; indx++) {\n#     auto forec = out_vect[indx];\n#     auto actual = actuals_vect[indx];\n#     suma+= abs(actual);\n#     if (actual > forec)\n#       sumf = sumf + (actual - forec)*TAU;\n#     else\n#       sumf = sumf + (actual - forec)*(TAU - 1);\n#   }\n#   return sumf / suma * 200;\n# }\n\ndef wQuantLoss(predictions, actuals, output_size, training_tau):\n    sumf = 0\n    suma = 0\n    for i in range(output_size):\n        prediction = predictions[i]\n        actual = actuals[i]\n\n        suma += abs(actual)\n        if (actual > prediction):\n            sumf = sumf + (actual - prediction) * training_tau\n        else:\n            sumf = sumf + (actual - prediction) * (training_tau - 1)\n\n    return sumf / suma * 200\n\n\n# test1 = torch.rand(100)\n# test2 = torch.rand(100)\n# wQuantLoss(test1, test2, 100, 0.5)\n\n\n### ErrorFunc\n\n# float errorFunc(vector<float>& out_vect, vector<float>& actuals_vect) {\n#   if (PERCENTILE==50)\n#     return sMAPE(out_vect, actuals_vect);\n#   else\n#     return wQuantLoss(out_vect, actuals_vect);\n# }\n\ndef errorFunc(predictions, actuals, output_size, percentile):\n    if (percentile == 50):\n        return sMAPE(predictions, actuals, output_size)\n    else:\n        return wQuantLoss(predictions, actuals, output_size, percentile / 100)\n\n\n# test1 = torch.rand(100)\n# test2 = torch.rand(100)\n# print(errorFunc(test1, test2, 100, 48))\n# print(wQuantLoss(test1, test2, 100, 0.48))\n# print(errorFunc(test1, test2, 100, 50))\n# print(sMAPE(test1, test2, 100))\n\ndef main():\n    # Test vectorized calculation\n    test1 = torch.rand(100)\n    test2 = torch.rand(100)\n    cpu_loss = non_sMAPE(test1, test2, 100)\n    vec_loss = sMAPE(test1, test2, 100)\n\nif __name__ == '__main__':\n    main()\n"""
es_rnn/main.py,1,"b""import pandas as pd\nfrom torch.utils.data import DataLoader\nfrom es_rnn.data_loading import create_datasets, SeriesDataset\nfrom es_rnn.config import get_config\nfrom es_rnn.trainer import ESRNNTrainer\nfrom es_rnn.model import ESRNN\nimport time\n\nprint('loading config')\nconfig = get_config('Monthly')\n\nprint('loading data')\ninfo = pd.read_csv('../data/info.csv')\n\ntrain_path = '../data/Train/%s-train.csv' % (config['variable'])\ntest_path = '../data/Test/%s-test.csv' % (config['variable'])\n\ntrain, val, test = create_datasets(train_path, test_path, config['output_size'])\n\ndataset = SeriesDataset(train, val, test, info, config['variable'], config['chop_val'], config['device'])\ndataloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n\nrun_id = str(int(time.time()))\nmodel = ESRNN(num_series=len(dataset), config=config)\ntr = ESRNNTrainer(model, dataloader, run_id, config, ohe_headers=dataset.dataInfoCatHeaders)\ntr.train_epochs()\n"""
es_rnn/model.py,20,"b""import torch\nimport torch.nn as nn\nfrom es_rnn.DRNN import DRNN\n\n\nclass ESRNN(nn.Module):\n    def __init__(self, num_series, config):\n        super(ESRNN, self).__init__()\n        self.config = config\n        self.num_series = num_series\n        self.add_nl_layer = self.config['add_nl_layer']\n\n        init_lev_sms = []\n        init_seas_sms = []\n        init_seasonalities = []\n\n        for i in range(num_series):\n            init_lev_sms.append(nn.Parameter(torch.Tensor([0.5]), requires_grad=True))\n            init_seas_sms.append(nn.Parameter(torch.Tensor([0.5]), requires_grad=True))\n            init_seasonalities.append(nn.Parameter((torch.ones(config['seasonality']) * 0.5), requires_grad=True))\n\n        self.init_lev_sms = nn.ParameterList(init_lev_sms)\n        self.init_seas_sms = nn.ParameterList(init_seas_sms)\n        self.init_seasonalities = nn.ParameterList(init_seasonalities)\n\n        self.nl_layer = nn.Linear(config['state_hsize'],\n                                  config['state_hsize'])\n        self.act = nn.Tanh()\n        self.scoring = nn.Linear(config['state_hsize'], config['output_size'])\n\n        self.logistic = nn.Sigmoid()\n\n        self.resid_drnn = ResidualDRNN(self.config)\n\n    def forward(self, train, val, test, info_cat, idxs, testing=False):\n        # GET THE PER SERIES PARAMETERS\n        lev_sms = self.logistic(torch.stack([self.init_lev_sms[idx] for idx in idxs]).squeeze(1))\n        seas_sms = self.logistic(torch.stack([self.init_seas_sms[idx] for idx in idxs]).squeeze(1))\n        init_seasonalities = torch.stack([self.init_seasonalities[idx] for idx in idxs])\n\n        seasonalities = []\n        # PRIME SEASONALITY\n        for i in range(self.config['seasonality']):\n            seasonalities.append(torch.exp(init_seasonalities[:, i]))\n        seasonalities.append(torch.exp(init_seasonalities[:, 0]))\n\n        if testing:\n            train = torch.cat((train, val), dim=1)\n\n        train = train.float()\n\n        levs = []\n        log_diff_of_levels = []\n\n        levs.append(train[:, 0] / seasonalities[0])\n        for i in range(1, train.shape[1]):\n            # CALCULATE LEVEL FOR CURRENT TIMESTEP TO NORMALIZE RNN\n            new_lev = lev_sms * (train[:, i] / seasonalities[i]) + (1 - lev_sms) * levs[i - 1]\n            levs.append(new_lev)\n\n            # STORE DIFFERENCE TO PENALIZE LATER\n            log_diff_of_levels.append(torch.log(new_lev / levs[i - 1]))\n\n            # CALCULATE SEASONALITY TO DESEASONALIZE THE DATA FOR RNN\n            seasonalities.append(seas_sms * (train[:, i] / new_lev) + (1 - seas_sms) * seasonalities[i])\n\n        seasonalities_stacked = torch.stack(seasonalities).transpose(1, 0)\n        levs_stacked = torch.stack(levs).transpose(1, 0)\n\n        loss_mean_sq_log_diff_level = 0\n        if self.config['level_variability_penalty'] > 0:\n            sq_log_diff = torch.stack(\n                [(log_diff_of_levels[i] - log_diff_of_levels[i - 1]) ** 2 for i in range(1, len(log_diff_of_levels))])\n            loss_mean_sq_log_diff_level = torch.mean(sq_log_diff)\n\n        if self.config['output_size'] > self.config['seasonality']:\n            start_seasonality_ext = seasonalities_stacked.shape[1] - self.config['seasonality']\n            end_seasonality_ext = start_seasonality_ext + self.config['output_size'] - self.config['seasonality']\n            seasonalities_stacked = torch.cat((seasonalities_stacked, seasonalities_stacked[:, start_seasonality_ext:end_seasonality_ext]),\n                                              dim=1)\n\n        window_input_list = []\n        window_output_list = []\n        for i in range(self.config['input_size'] - 1, train.shape[1]):\n            input_window_start = i + 1 - self.config['input_size']\n            input_window_end = i + 1\n\n            train_deseas_window_input = train[:, input_window_start:input_window_end] / seasonalities_stacked[:,\n                                                                                        input_window_start:input_window_end]\n            train_deseas_norm_window_input = (train_deseas_window_input / levs_stacked[:, i].unsqueeze(1))\n            train_deseas_norm_cat_window_input = torch.cat((train_deseas_norm_window_input, info_cat), dim=1)\n            window_input_list.append(train_deseas_norm_cat_window_input)\n\n            output_window_start = i + 1\n            output_window_end = i + 1 + self.config['output_size']\n\n            if i < train.shape[1] - self.config['output_size']:\n                train_deseas_window_output = train[:, output_window_start:output_window_end] / \\\n                                             seasonalities_stacked[:, output_window_start:output_window_end]\n                train_deseas_norm_window_output = (train_deseas_window_output / levs_stacked[:, i].unsqueeze(1))\n                window_output_list.append(train_deseas_norm_window_output)\n\n        window_input = torch.cat([i.unsqueeze(0) for i in window_input_list], dim=0)\n        window_output = torch.cat([i.unsqueeze(0) for i in window_output_list], dim=0)\n\n        self.train()\n        network_pred = self.series_forward(window_input[:-self.config['output_size']])\n        network_act = window_output\n\n        self.eval()\n        network_output_non_train = self.series_forward(window_input)\n\n        # USE THE LAST VALUE OF THE NETWORK OUTPUT TO COMPUTE THE HOLDOUT PREDICTIONS\n        hold_out_output_reseas = network_output_non_train[-1] * seasonalities_stacked[:, -self.config['output_size']:]\n        hold_out_output_renorm = hold_out_output_reseas * levs_stacked[:, -1].unsqueeze(1)\n\n        hold_out_pred = hold_out_output_renorm * torch.gt(hold_out_output_renorm, 0).float()\n        hold_out_act = test if testing else val\n\n        hold_out_act_deseas = hold_out_act.float() / seasonalities_stacked[:, -self.config['output_size']:]\n        hold_out_act_deseas_norm = hold_out_act_deseas / levs_stacked[:, -1].unsqueeze(1)\n\n        self.train()\n        # RETURN JUST THE TRAINING INPUT RATHER THAN THE ENTIRE SET BECAUSE THE HOLDOUT IS BEING GENERATED WITH THE REST\n        return network_pred, \\\n               network_act, \\\n               (hold_out_pred, network_output_non_train), \\\n               (hold_out_act, hold_out_act_deseas_norm), \\\n               loss_mean_sq_log_diff_level\n\n    def series_forward(self, data):\n        data = self.resid_drnn(data)\n        if self.add_nl_layer:\n            data = self.nl_layer(data)\n            data = self.act(data)\n        data = self.scoring(data)\n        return data\n\n\nclass ResidualDRNN(nn.Module):\n    def __init__(self, config):\n        super(ResidualDRNN, self).__init__()\n        self.config = config\n\n        layers = []\n        for grp_num in range(len(self.config['dilations'])):\n\n            if grp_num == 0:\n                input_size = self.config['input_size'] + self.config['num_of_categories']\n            else:\n                input_size = self.config['state_hsize']\n\n            l = DRNN(input_size,\n                     self.config['state_hsize'],\n                     n_layers=len(self.config['dilations'][grp_num]),\n                     dilations=self.config['dilations'][grp_num],\n                     cell_type=self.config['rnn_cell_type'])\n\n            layers.append(l)\n\n        self.rnn_stack = nn.Sequential(*layers)\n\n    def forward(self, input_data):\n        for layer_num in range(len(self.rnn_stack)):\n            residual = input_data\n            out, _ = self.rnn_stack[layer_num](input_data)\n            if layer_num > 0:\n                out += residual\n            input_data = out\n        return out\n"""
es_rnn/trainer.py,6,"b'import os\nimport time\nimport numpy as np\nimport copy\nimport torch\nimport torch.nn as nn\nfrom es_rnn.loss_modules import PinballLoss, sMAPE, np_sMAPE\nfrom utils.logger import Logger\nimport pandas as pd\n\n\nclass ESRNNTrainer(nn.Module):\n    def __init__(self, model, dataloader, run_id, config, ohe_headers):\n        super(ESRNNTrainer, self).__init__()\n        self.model = model.to(config[\'device\'])\n        self.config = config\n        self.dl = dataloader\n        self.ohe_headers = ohe_headers\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config[\'learning_rate\'])\n        # self.optimizer = torch.optim.ASGD(self.model.parameters(), lr=config[\'learning_rate\'])\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer,\n                                                         step_size=config[\'lr_anneal_step\'],\n                                                         gamma=config[\'lr_anneal_rate\'])\n        self.criterion = PinballLoss(self.config[\'training_tau\'],\n                                     self.config[\'output_size\'] * self.config[\'batch_size\'], self.config[\'device\'])\n        self.epochs = 0\n        self.max_epochs = config[\'num_of_train_epochs\']\n        self.run_id = str(run_id)\n        self.prod_str = \'prod\' if config[\'prod\'] else \'dev\'\n        self.log = Logger(""../logs/train%s%s%s"" % (self.config[\'variable\'], self.prod_str, self.run_id))\n        self.csv_save_path = None\n\n    def train_epochs(self):\n        max_loss = 1e8\n        start_time = time.time()\n        for e in range(self.max_epochs):\n            self.scheduler.step()\n            epoch_loss = self.train()\n            if epoch_loss < max_loss:\n                self.save()\n            epoch_val_loss = self.val()\n            if e == 0:\n                file_path = os.path.join(self.csv_save_path, \'validation_losses.csv\')\n                with open(file_path, \'w\') as f:\n                    f.write(\'epoch,training_loss,validation_loss\\n\')\n            with open(file_path, \'a\') as f:\n                f.write(\',\'.join([str(e), str(epoch_loss), str(epoch_val_loss)]) + \'\\n\')\n        print(\'Total Training Mins: %5.2f\' % ((time.time()-start_time)/60))\n\n    def train(self):\n        self.model.train()\n        epoch_loss = 0\n        for batch_num, (train, val, test, info_cat, idx) in enumerate(self.dl):\n            start = time.time()\n            print(""Train_batch: %d"" % (batch_num + 1))\n            loss = self.train_batch(train, val, test, info_cat, idx)\n            epoch_loss += loss\n            end = time.time()\n            self.log.log_scalar(\'Iteration time\', end - start, batch_num + 1 * (self.epochs + 1))\n        epoch_loss = epoch_loss / (batch_num + 1)\n        self.epochs += 1\n\n        # LOG EPOCH LEVEL INFORMATION\n        print(\'[TRAIN]  Epoch [%d/%d]   Loss: %.4f\' % (\n            self.epochs, self.max_epochs, epoch_loss))\n        info = {\'loss\': epoch_loss}\n\n        self.log_values(info)\n        self.log_hists()\n\n        return epoch_loss\n\n    def train_batch(self, train, val, test, info_cat, idx):\n        self.optimizer.zero_grad()\n        network_pred, network_act, _, _, loss_mean_sq_log_diff_level = self.model(train, val,\n                                                                                  test, info_cat,\n                                                                                  idx)\n\n        loss = self.criterion(network_pred, network_act)\n        loss.backward()\n        nn.utils.clip_grad_value_(self.model.parameters(), self.config[\'gradient_clipping\'])\n        self.optimizer.step()\n        return float(loss)\n\n    def val(self):\n        self.model.eval()\n        with torch.no_grad():\n            acts = []\n            preds = []\n            info_cats = []\n\n            hold_out_loss = 0\n            for batch_num, (train, val, test, info_cat, idx) in enumerate(self.dl):\n                _, _, (hold_out_pred, network_output_non_train), \\\n                (hold_out_act, hold_out_act_deseas_norm), _ = self.model(train, val, test, info_cat, idx)\n                hold_out_loss += self.criterion(network_output_non_train.unsqueeze(0).float(),\n                                                hold_out_act_deseas_norm.unsqueeze(0).float())\n                acts.extend(hold_out_act.view(-1).cpu().detach().numpy())\n                preds.extend(hold_out_pred.view(-1).cpu().detach().numpy())\n                info_cats.append(info_cat.cpu().detach().numpy())\n            hold_out_loss = hold_out_loss / (batch_num + 1)\n\n            info_cat_overall = np.concatenate(info_cats, axis=0)\n            _hold_out_df = pd.DataFrame({\'acts\': acts, \'preds\': preds})\n            cats = [val for val in self.ohe_headers[info_cat_overall.argmax(axis=1)] for _ in\n                    range(self.config[\'output_size\'])]\n            _hold_out_df[\'category\'] = cats\n\n            overall_hold_out_df = copy.copy(_hold_out_df)\n            overall_hold_out_df[\'category\'] = [\'Overall\' for _ in cats]\n\n            overall_hold_out_df = pd.concat((_hold_out_df, overall_hold_out_df))\n            grouped_results = overall_hold_out_df.groupby([\'category\']).apply(\n                lambda x: np_sMAPE(x.preds, x.acts, x.shape[0]))\n\n            results = grouped_results.to_dict()\n            results[\'hold_out_loss\'] = float(hold_out_loss.detach().cpu())\n\n            self.log_values(results)\n\n            file_path = os.path.join(\'..\', \'grouped_results\', self.run_id, self.prod_str)\n            os.makedirs(file_path, exist_ok=True)\n\n            print(results)\n            grouped_path = os.path.join(file_path, \'grouped_results-{}.csv\'.format(self.epochs))\n            grouped_results.to_csv(grouped_path)\n            self.csv_save_path = file_path\n\n        return hold_out_loss.detach().cpu().item()\n\n    def save(self, save_dir=\'..\'):\n        print(\'Loss decreased, saving model!\')\n        file_path = os.path.join(save_dir, \'models\', self.run_id, self.prod_str)\n        model_path = os.path.join(file_path, \'model-{}.pyt\'.format(self.epochs))\n        os.makedirs(file_path, exist_ok=True)\n        torch.save({\'state_dict\': self.model.state_dict()}, model_path)\n\n    def log_values(self, info):\n\n        # SCALAR\n        for tag, value in info.items():\n            self.log.log_scalar(tag, value, self.epochs + 1)\n\n    def log_hists(self):\n        # HISTS\n        batch_params = dict()\n        for tag, value in self.model.named_parameters():\n            if value.grad is not None:\n                if ""init"" in tag:\n                    name, _ = tag.split(""."")\n                    if name not in batch_params.keys() or ""%s/grad"" % name not in batch_params.keys():\n                        batch_params[name] = []\n                        batch_params[""%s/grad"" % name] = []\n                    batch_params[name].append(value.data.cpu().numpy())\n                    batch_params[""%s/grad"" % name].append(value.grad.cpu().numpy())\n                else:\n                    tag = tag.replace(\'.\', \'/\')\n                    self.log.log_histogram(tag, value.data.cpu().numpy(), self.epochs + 1)\n                    self.log.log_histogram(tag + \'/grad\', value.grad.data.cpu().numpy(), self.epochs + 1)\n            else:\n                print(\'Not printing %s because it\\\'s not updating\' % tag)\n\n        for tag, v in batch_params.items():\n            vals = np.concatenate(np.array(v))\n            self.log.log_histogram(tag, vals, self.epochs + 1)\n'"
utils/__init__.py,0,b''
utils/helper_funcs.py,2,"b'import torch\n\n\ndef colwise_batch_mask(target_shape_tuple, target_lens):\n    # takes in (seq_len, B) shape and returns mask of same shape with ones up to the target lens\n    mask = torch.zeros(target_shape_tuple)\n    for i in range(target_shape_tuple[1]):\n        mask[:target_lens[i], i] = 1\n    return mask\n\n\ndef rowwise_batch_mask(target_shape_tuple, target_lens):\n    # takes in (B, seq_len) shape and returns mask of same shape with ones up to the target lens\n    mask = torch.zeros(target_shape_tuple)\n    for i in range(target_shape_tuple[0]):\n        mask[i, :target_lens[i]] = 1\n    return mask\n\n\ndef unpad_sequence(padded_sequence, lens):\n    seqs = []\n    for i in range(padded_sequence.size(1)):\n        seqs.append(padded_sequence[:lens[i], i])\n    return seqs\n\n'"
utils/logger.py,0,"b'import tensorflow as tf\nimport numpy as np\n\n\nclass Logger(object):\n    """"""Logging in tensorboard without tensorflow ops.""""""\n\n    def __init__(self, log_dir):\n        self.writer = tf.summary.FileWriter(log_dir)\n\n    def log_scalar(self, tag, value, step):\n        """"""Log a scalar variable.\n        Parameter\n        ----------\n        tag : Name of the scalar\n        value : value itself\n        step :  training iteration\n        """"""\n        # Notice we\'re using the Summary ""class"" instead of the ""tf.summary"" public API.\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n        self.writer.add_summary(summary, step)\n\n    def log_histogram(self, tag, values, step, bins=1000):\n        """"""Logs the histogram of a list/vector of values.""""""\n        # Convert to a numpy array\n        values = np.array(values)\n\n        # Create histogram using numpy\n        counts, bin_edges = np.histogram(values, bins=bins)\n\n        # Fill fields of histogram proto\n        hist = tf.HistogramProto()\n        hist.min = float(np.min(values))\n        hist.max = float(np.max(values))\n        hist.num = int(np.prod(values.shape))\n        hist.sum = float(np.sum(values))\n        hist.sum_squares = float(np.sum(values ** 2))\n\n        # Requires equal number as bins, where the first goes from -DBL_MAX to bin_edges[1]\n        # See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/summary.proto#L30\n        # Thus, we drop the start of the first bin\n        bin_edges = bin_edges[1:]\n\n        # Add bin edges and counts\n        for edge in bin_edges:\n            hist.bucket_limit.append(edge)\n        for c in counts:\n            hist.bucket.append(c)\n\n        # Create and write Summary\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n        self.writer.add_summary(summary, step)\n        self.writer.flush()'"
