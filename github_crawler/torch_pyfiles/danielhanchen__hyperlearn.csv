file_path,api_count,code
numba_compile.py,0,"b'\nprint(""******* Now compiling Numba and LLVM code..... *******"")\nprint(""******* This can be VERY SLOW. Please wait.... *******\\n""\n\t""Progress: |||||||||||||||"", end = """")\n\nfrom hyperlearn.numba.funcs import *\n\nprint(""|||||||||||||||"", end = """")\n\nfrom hyperlearn.utils import *\n\nprint(""|||||||||||||||"")\n\nfrom hyperlearn.stats import *\n\nprint(""******* Code has been successfully compiled!:) *******"")\n'"
setup.py,0,"b'\n\nfrom distutils.core import setup\nfrom setuptools import Extension\nfrom setuptools.command.install import install\nfrom setuptools import find_packages\nimport subprocess\n\n#from Cython.Build import cythonize\nfrom numpy import get_include\n#from Cython.Compiler import Options\nimport os\n\n# os.environ[\'CFLAGS\'] = \'-O3 -march=native -ffast-math -mtune=native -ftree-vectorize\'\n# os.environ[\'CXXFLAGS\'] = \'-O3 -march=native -ffast-math -mtune=native -ftree-vectorize\'\n# os.environ[\'CL\'] = \'/arch:AVX /arch:AVX2 /arch:SSE2 /arch:SSE /arch:ARMv7VE /arch:VFPv4\'\n\n# Options.docstrings = True\n# Options.generate_cleanup_code = True\n\ninstall_requires = [\n    \'numpy >= 1.13.0\',\n    \'torchvision >= 0.2.0\',\n    \'scikit-learn >= 0.18.0\',\n    \'scipy >= 1.0.0\',\n    \'pandas >= 0.21.0\',\n    \'torch >= 0.4.0\',\n    \'numba >= 0.37.0\',\n    \'psutil >= 4.0.0\',\n    ""cython >= 0.x"",\n    ]\n\ndependency_links = [\n    ]\n\n\ndesc = """"""\\\nHyperLearn\n\nFaster, Leaner Scikit Learn (Sklearn) morphed with Statsmodels & \nDeep Learning drop in substitute. Designed for big data, HyperLearn \ncan use 50%+ less memory, and runs 50%+ faster on some modules. \nWill have GPU support, and all modules are parallelized. \nWritten completely in PyTorch, Numba, Numpy, Pandas, Scipy & LAPACK.\n\nhttps://github.com/danielhanchen/hyperlearn\n""""""\n\n\n# class InstallLocalPackage(install):\n#     def run(self):\n#         install.run(self)\n#         print(""******* Now compiling C and Cython code..... *******"")\n\n#         subprocess.call(\n#             ""python setup.py build_ext --inplace"", shell = True,\n#             cwd = ""hyperlearn/cython""\n#         )\n\nUSE_CYTHON = False\next = \'.pyx\' if USE_CYTHON else \'.c\'\n\next_modules = [\n    # Extension(""hyperlearn.cython.base"", [""hyperlearn/cython/base""+ext]),\n    # Extension(""hyperlearn.cython.utils"", [""hyperlearn/cython/utils""+ext])\n]\n\n# if USE_CYTHON:\n#     ext_modules = cythonize(ext_modules,\n#         compiler_directives = {\n#             \'language_level\':3, \n#             \'boundscheck\':False, \n#             \'wraparound\':False,\n#             \'initializedcheck\':False, \n#             \'cdivision\':True,\n#             \'nonecheck\':False,\n#         },\n#         quiet = True,\n#         force = True,\n#     )\n\n\n## Contributed by themightyoarfish [6/1/19 Issue 13]\nkwargs = {\n    ""name"" : \'hyperlearn\',\n    ""version"" : \'0.0.1\',\n    ""author"" : \'Daniel Han-Chen & Others listed on Github\',\n    ""url"" : \'https://github.com/danielhanchen/hyperlearn\',\n    ""long_description"" : desc,\n    #""py_modules"" : [\'hyperlearn\'],\n    \'packages\' : find_packages(\'.\', include=[\'hyperlearn\']),\n    \'package_data\' : {\n        \'hyperlearn\': [\'LICENSE\', \'README.md\', \'CODE_OF_CONDUCT.md\', \'CONTRIBUTING.md\']\n        },\n    \'include_package_data\' : True,\n    ""install_requires"" : install_requires,\n    ""dependency_links"" : dependency_links,\n    \'zip_safe\' : False,\n    ""classifiers"" : [  # Optional\n    \'Development Status :: 1 - Planning\',\n\n    # Indicate who your project is intended for\n    \'Intended Audience :: Developers\',\n    \'Intended Audience :: Education\',\n    \'Intended Audience :: Science/Research\',\n\n    # Pick your license as you wish\n    \'License :: OSI Approved :: GNU General Public License v3 (GPLv3)\',\n\n    # Specify the Python versions you support here. In particular, ensure\n    # that you indicate whether you support Python 2, Python 3 or both.\n    \'Programming Language :: Python :: 3.4\',\n    \'Programming Language :: Python :: 3.5\',\n    \'Programming Language :: Python :: 3.6\',\n    \'Programming Language :: Python :: 3.7\',\n\n    \'Topic :: Scientific/Engineering\',\n    \'Topic :: Scientific/Engineering :: Mathematics\',\n    \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n    \'Topic :: Software Development\',\n    \'Topic :: Software Development :: Libraries\',\n    \'Topic :: Software Development :: Libraries :: Python Modules\',\n        ],\n    #""cmdclass"" : { \'install\': InstallLocalPackage },\n    ""ext_modules"" : ext_modules,\n    ""include_dirs"" : [get_include()],\n}\n\nprint(""#### Welcome to Umbra\'s HyperLearn! ####"")\nprint(""#### During installation, code will be compiled down to C / LLVM via Numba. ####"")\nprint(""#### This could mean you have to wait...... ####"")\nprint(""\\n#### You MUST have a C compiler AND MKL/LAPACK enabled Scipy. ####"")\nprint(""#### If you have Anaconda, then you are set to go! ####"")\n\n\nsetup(**kwargs)\n\nprint(""#### HyperLearn has been installed! ####"")\nprint(""\\n#### If you want to compile Numba code, please run:"")\nprint(""    >>>>   python numba_compile.py"")'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# HyperLearn documentation build configuration file, created by\n# sphinx-quickstart on Wed Nov 21 20:33:44 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n#sys.path.insert(0, os.path.abspath(\'..\'))\nsys.path.insert(0, os.path.abspath(\'..\'))\nsys.path.insert(0, os.path.abspath(\'../../\'))\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#sys.path.insert(0, os.path.abspath(\'.\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.napoleon\'\n]\nnapoleon_google_docstring = False\nnapoleon_use_param = False\nnapoleon_use_ivar = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'HyperLearn\'\ncopyright = \'2018, Daniel Han-Chen\'\nauthor = \'Daniel Han-Chen\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'0.0.2\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'1\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.\n# ""<project> v<release> documentation"" by default.\n#html_title = \'HyperLearn v1\'\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not None, a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to \'%b %d, %Y\'.\n#html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'h\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'r\', \'sv\', \'tr\', \'zh\'\n#html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# \'ja\' uses this config value.\n# \'zh\' user can custom change `jieba` dictionary path.\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'HyperLearndoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n\n# Latex figure (float) alignment\n#\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'HyperLearn.tex\', \'HyperLearn Documentation\',\n     \'Daniel Han-Chen\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'hyperlearn\', \'HyperLearn Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'HyperLearn\', \'HyperLearn Documentation\',\n     author, \'HyperLearn\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\'https://docs.python.org/\': None}\n\n'"
hyperlearn/base.py,0,"b'from torch import from_numpy as _Tensor, einsum as t_einsum, \\\n\t\t\t\ttranspose as __transpose, Tensor as typeTensor, stack as __stack, \\\n\t\t\t\tfloat32, int32\nfrom functools import wraps    \nfrom numpy import finfo as np_finfo, einsum as np_einsum, log as np_log, array as np_array\nfrom numpy import float32 as np_float32, float64 as np_float64, int32 as np_int32, \\\n\t\t\t\t\tint64 as np_int64, bool_ as np_bool, uint8 as np_uint8, ndarray, \\\n\t\t\t\t\tround as np_round\nfrom numpy import newaxis\nfrom sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin\nfrom inspect import isclass as isClass\nfrom torch import matmul as torch_dot, diag, ones\nfrom numpy import diag as np_diag, ones as np_ones # dot as np_dot\n\nUSE_NUMPY_EINSUM = True\nPRINT_ALL_WARNINGS = False\nUSE_GPU = False\nALPHA_DEFAULT = 0.00001\nUSE_NUMBA = True\n\n""""""\n------------------------------------------------------------\nType Checks\nUpdated 27/8/2018\n------------------------------------------------------------\n""""""\nFloatInt = (float, int)\nListTuple = (list, tuple)\nArrayType = (np_array, ndarray)\nKeysValues = ( type({}.keys()), type({}.values()), dict )\n\ndef isList(X):\n\treturn type(X) in ListTuple\n\ndef isArray(X):\n\treturn type(X) in ArrayType\n\ndef isIterable(X):\n\tif isArray(X):\n\t\tif len(X.shape) == 1: return True\n\treturn isList(X)\n\ndef isDict(X):\n\treturn type(X) in KeysValues\n\ndef array(X):\n\tif isDict(X):\n\t\tX = list(X)\n\treturn np_array(X)\n\ndef Tensor(X):\n\tif type(X) is typeTensor:\n\t\treturn X\n\tif isDict(X):\n\t\tX = list(X)\n\tif isList(X):\n\t\tX = np_array(X)\n\ttry:\n\t\tif X.dtype == np_bool:\n\t\t\tX = X.astype(np_uint8)\n\t\treturn _Tensor(X)\n\texcept:\n\t\treturn X\n\ndef isTensor(X):\n\treturn type(X) is typeTensor\n\n\ndef Tensors(*args):\n\tout = []\n\tfor x in args:\n\t\tif x is None: out.append(None)\n\t\tif isTensor(x):\n\t\t\tout.append(  x  )\n\t\telse:\n\t\t\tout.append(  Tensor(x)  )\n\treturn out\n\n\ndef Numpy(*args):\n\tout = []\n\tif isList(args[0]):\n\t\targs = args[0]\n\n\tfor x in args:\n\t\tif x is None: out.append(None)\n\t\tif isTensor(x):\n\t\t\tout.append(  x.numpy()  )\n\t\telse:\n\t\t\tout.append(  x  )\n\treturn out\n\n\ndef return_numpy(args):\n\targs = [args] if not isList(args) else args\n\tresult = Numpy(*args)\n\tif len(result) == 1:\n\t\treturn result[0]\n\telse:\n\t\treturn tuple(result)\n\ndef return_torch(args):\n\targs = [args] if not isList(args) else args\n\tresult = Tensors(*args)\n\tif len(result) == 1:\n\t\treturn result[0]\n\telse:\n\t\treturn tuple(result)\n\n""""""\n------------------------------------------------------------\nDecorators:\n\tcheck\nUpdated 31/8/2018\n------------------------------------------------------------\n""""""\n\ndef check(f):\n\t@wraps(f)\n\tdef wrapper(*args, **kwargs):\n\t\tif USE_GPU:\n\t\t\tif isClass(args[0]):\n\t\t\t\treturned = f(args[0], *Tensors(*args[1:]), **kwargs)\n\t\t\treturned = f(*Tensors(*args), **kwargs)\n\t\t\treturn return_torch(returned)\n\n\t\treturned = f(*args, **kwargs)\n\t\treturn return_numpy(returned)\n\treturn wrapper\n\n""""""\n------------------------------------------------------------\nMatrix Manipulation\n\t>>> Now can specify the backend either GPU or CPU\n\t>>> Note on CPU --> Numpy is considerably faster\n\t\twhen X(n,p) p>>n\nUpdated 30/8/2018\n------------------------------------------------------------\n""""""\n#dot = torch_dot if USE_GPU else np_dot\n\ndef T(X):\n\tA = X.reshape(-1,1) if len(X.shape) == 1 else X\n\tif USE_GPU: return A.t()\n\treturn A.T\n\ndef cast(X, dtype):\n\tif USE_GPU: return X.type(dtype)\n\treturn X.astype(dtype)\n\n#def ravel(X):\n#\tif USE_GPU: return X.unsqueeze(1)\n#\treturn X.ravel()\n\ndef constant(X):\n\tif USE_GPU: return X.item()\n\treturn X\n\ndef eps(X):\n\ttry:\n\t\treturn np_finfo(dtype(X)).eps\n\texcept:\n\t\treturn np_finfo(np_float64).eps\n\ndef resolution(X):\n\ttry:\n\t\treturn np_finfo(dtype(X)).resolution\n\texcept:\n\t\treturn np_finfo(np_float32).resolution\n\t\n\ndef dtype(tensor):\n\tstring = str(tensor.dtype)\n\tif \'float32\' in string: return np_float32\n\telif \'float64\' in string: return np_float64\n\telif \'int32\' in string: return np_int32\n\telif \'int64\' in string: return np_int64\n\telse: return np_float32\n\t\n\ndef stack(*args):\n\tif isList(args[0]): args = args[0]\n\ttoStack = Tensors(*args)\n\treturn __stack(toStack)\n\n""""""\n------------------------------------------------------------\nEINSUM, Einstein Notation Summation\nUpdated 28/8/2018\n------------------------------------------------------------\n""""""\ndef einsum(notation, *args, tensor = False):\n\tif USE_NUMPY_EINSUM:\n\t\targs = Numpy(*args)\n\t\tout = np_einsum(notation, *args)\n\telse:\n\t\targs = Tensors(*args)\n\t\ttry:\n\t\t\tout = t_einsum(notation, *args)\n\t\texcept:\n\t\t\tout = t_einsum(notation, args)\n\tif tensor:\n\t\treturn Tensor(out)\n\treturn out\n\n\ndef squareSum(X):\n\tif len(X.shape) == 1:\n\t\treturn einsum(\'i,i->\', X, X)\n\treturn einsum(\'ij,ij->i\', X, X )\n\n\ndef rowSum(X, Y = None, transpose_a = False):\n\tif Y is None:\n\t\treturn einsum(\'ij->i\',X)\n\tif transpose_a:\n\t\treturn einsum(\'ji,ij->i\', X , Y )\n\treturn einsum(\'ij,ij->i\', X , Y )\n\n\ndef diagSum(X, Y, transpose_a = False):\n\tif transpose_a:\n\t\treturn einsum(\'ji,ij->\', X , Y )\n\treturn einsum(\'ij,ij->\', X , Y )\n\n\n'"
hyperlearn/exceptions.py,0,"b""\n\nclass FutureExceedsMemory(BaseException):\n\tdef __init__(self, text = 'Operation done in the future uses more'\n\t\t\t\t\t\t\t\t' memory than what is free. HyperLearn'):\n\t\tself.text = text\n\tdef __str__(self):\n\t\treturn self.text\n\n\nclass PartialWrongShape(BaseException):\n\tdef __init__(self, text = 'Partial SVD or Eig needs the same number of'\n\t\t\t\t\t\t\t\t' columns in both the previous iteration and'\n\t\t\t\t\t\t\t\t' the future iteration. Currenlty, the number'\n\t\t\t\t\t\t\t\t' of columns is different.'):\n\t\tself.text = text\n\tdef __str__(self):\n\t\treturn self.text"""
hyperlearn/linalg.py,0,"b'\nfrom scipy.linalg import lu as _lu, qr as _qr\nfrom . import numba\nfrom numba import njit\nfrom .base import *\nfrom .utils import *\nfrom numpy import float32, float64\n\n\n__all__ = [\'cholesky\', \'invCholesky\', \'pinvCholesky\', \'cholSolve\',\n\t\t\t\'svd\', \'lu\', \'qr\', \n\t\t\t\'pinv\', \'pinvh\', \n\t\t\t\'eigh\', \'pinvEig\', \'eig\']\n\n\ndef cholesky(XTX, alpha = None, fast = True):\n\t""""""\n\tComputes the Cholesky Decompsition of a Hermitian Matrix\n\t(Positive Symmetric Matrix) giving a Upper Triangular Matrix.\n\t\n\tCholesky Decomposition is used as the default solver in HyperLearn,\n\tas it is super fast and allows regularization. HyperLearn\'s\n\timplementation also handles rank deficient and ill-conditioned\n\tmatrices perfectly with the help of the limiting behaivour of\n\tadding forced epsilon regularization.\n\t\n\tSpeed\n\t--------------\n\tIf USE_GPU:\n\t\tUses PyTorch\'s Cholesky. Speed is OK.\n\tIf CPU:\n\t\tUses Numpy\'s Fortran C based Cholesky.\n\t\tIf NUMBA is not installed, uses very fast LAPACK functions.\n\t\n\tStability\n\t--------------\n\tAlpha is added for regularization purposes. This prevents system\n\trounding errors and promises better convergence rates.\n\t""""""\n\tn,p = XTX.shape;  assert n==p\n\terror = 1\n\talpha = ALPHA_DEFAULT if alpha == None else alpha\n\told_alpha = 0\n\n\tdecomp = lapack(""potrf"", fast, ""cholesky"")\n\n\twhile error != 0:\n\t\tif PRINT_ALL_WARNINGS: \n\t\t\tprint(\'cholesky Alpha = {}\'.format(alpha))\n\n\t\t# Add epsilon jitter to diagonal. Note adding\n\t\t# np.eye(p)*alpha is slower and uses p^2 memory\n\t\t# whilst flattening uses only p memory.\n\t\taddDiagonal(XTX, alpha-old_alpha)\n\t\ttry:\n\t\t\tcho = decomp(XTX)\n\t\t\tif USE_NUMBA: \n\t\t\t\tcho = cho.T\n\t\t\t\terror = 0\n\t\t\telse:\n\t\t\t\tcho, error = cho\n\t\texcept: pass\n\t\tif error != 0:\n\t\t\told_alpha = alpha\n\t\t\talpha *= 10\n\n\taddDiagonal(XTX, -alpha)\n\treturn cho\n\n\ndef cholSolve(A, b, alpha = None):\n\t""""""\n\t[Added 20/10/2018]\n\tFaster than direct inverse solve. Finds coefficients in linear regression\n\tallowing A @ theta = b.\n\tNotice auto adds epsilon jitter if solver fails.\n\t""""""\n\tn,p = A.shape;\tassert n == p and b.shape[0] == n\n\terror = 1\n\talpha = ALPHA_DEFAULT if alpha is None else alpha\n\told_alpha = 0\n\n\tsolver = lapack(""potrs"")\n\n\twhile error != 0:\n\t\tif PRINT_ALL_WARNINGS: \n\t\t\tprint(\'cholSolve Alpha = {}\'.format(alpha))\n\n\t\t# Add epsilon jitter to diagonal. Note adding\n\t\t# np.eye(p)*alpha is slower and uses p^2 memory\n\t\t# whilst flattening uses only p memory.\n\t\taddDiagonal(A, alpha-old_alpha)\n\t\ttry:\n\t\t\tcoef, error = solver(A, b)\n\t\texcept: pass\n\t\tif error != 0:\n\t\t\told_alpha = alpha\n\t\t\talpha *= 10\n\n\taddDiagonal(A, -alpha)\n\treturn coef\n\n\ndef lu(X, L_only = False, U_only = False):\n\t""""""\n\t[Edited 8/11/2018 Changed to LAPACK LU if L/U only wanted]\n\tComputes the LU Decomposition of any matrix with pivoting.\n\tProvides L only or U only if specified.\n\n\tMuch faster than Scipy if only U/L wanted, and more memory efficient,\n\tsince data is altered inplace.\n\t""""""\n\tn, p = X.shape\n\tif L_only or U_only:\n\n\t\tA, P, __ = lapack(""getrf"")(X)\n\t\tif L_only:\n\t\t\tA, k = L_process(n, p, A)\n\t\t\t# inc = -1 means reverse order pivoting\n\t\t\tA = lapack(""laswp"")(a = A, piv = P, inc = -1, k1 = 0, k2 = k-1, overwrite_a = True)\n\t\telse:\n\t\t\tA = triu_process(n, p, A)\n\t\treturn A\n\telse:\n\t\treturn _lu(X, permute_l = True, check_finite = False)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef L_process(n, p, L):\n\t""""""\n\tAuxiliary function to modify data in place to only get L from LU decomposition.\n\t""""""\n\twide = (p > n)\n\tk = p\n\n\tif wide:\n\t\t# wide matrix\n\t\t# L get all n rows, but only n columns\n\t\tL = L[:, :n]\n\t\tk = n\n\n\t# tall / wide matrix\n\tfor i in range(k):\n\t\tli = L[i]\n\t\tli[i+1:] = 0\n\t\tli[i] = 1\n\t# Set diagonal to 1\n\treturn L, k\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef triu_process(n, p, U):\n\t""""""\n\tAuxiliary function to modify data in place to only get upper triangular\n\tpart of matrix. Used in QR (get R) and LU (get U) decompositon.\n\t""""""\n\ttall = (n > p)\n\tk = n\n\n\tif tall:\n\t\t# tall matrix\n\t\t# U get all p rows\n\t\tU = U[:p]\n\t\tk = p\n\n\tfor i in range(1, k):\n\t\tU[i, :i] = 0\n\treturn U\n\n\n\ndef qr(X, Q_only = False, R_only = False, overwrite = False):\n\t""""""\n\t[Edited 8/11/2018 Added Q, R only parameters. Faster than Numba]\n\t[Edited 9/11/2018 Made R only more memory efficient (no data copying)]\n\tComputes the reduced QR Decomposition of any matrix.\n\tUses optimized NUMBA QR if avaliable else use\'s Scipy\'s\n\tversion.\n\n\tProvides Q or R only if specified, and is must faster + more memory\n\tefficient since data is changed inplace.\n\t""""""\n\tif Q_only or R_only:\n\t\t# Compute R\n\t\tn, p = X.shape\n\t\tR, tau, __, __ = lapack(""geqrf"")(X, overwrite_a = overwrite)\n\n\t\tif Q_only:\n\t\t\tif p > n:\n\t\t\t\tR = R[:, :n]\n\t\t\t# Compute Q\n\t\t\tQ, __, __ = lapack(""orgqr"")(R, tau, overwrite_a = True)\n\t\t\treturn Q\n\t\telse:\n\t\t\t# Do nothing, as R is already computed.\n\t\t\tR = triu_process(n, p, R)\n\t\t\treturn R\n\n\tif USE_NUMBA: return numba.qr(X)\n\treturn _qr(X, mode = \'economic\', check_finite = False, overwrite_a = overwrite)\n\n\n\ndef invCholesky(X, fast = False):\n\t""""""\n\tComputes the Inverse of a Hermitian Matrix\n\t(Positive Symmetric Matrix) after provided with Cholesky\'s\n\tLower Triangular Matrix.\n\t\n\tThis is used in conjunction in solveCholesky, where the\n\tinverse of the covariance matrix is needed.\n\t\n\tSpeed\n\t--------------\n\tIf USE_GPU:\n\t\tUses PyTorch\'s Triangular Solve given identity matrix. Speed is OK.\n\tIf CPU:\n\t\tUses very fast LAPACK algorithms for triangular system inverses.\n\t\n\tStability\n\t--------------\n\tNote that LAPACK\'s single precision (float32) solver (strtri) is much more\n\tunstable than double (float64). So, default strtri is OFF.\n\tHowever, speeds are reduced by 50%.\n\t""""""\n\tassert X.shape[0] == X.shape[1]\n\tchoInv = lapack(""trtri"", fast)(X)[0]\n\n\treturn choInv @ choInv.T\n\n\t\n\ndef pinvCholesky(X, alpha = None, fast = False):\n\t""""""\n\tComputes the approximate pseudoinverse of any matrix using Cholesky Decomposition\n\tThis means X @ pinv(X) approx = eye(n).\n\t\n\tNote that this is super fast, and will be used in HyperLearn as the default\n\tpseudoinverse solver for any matrix. Care is taken to make the algorithm\n\tconverge, and this is done via forced epsilon regularization.\n\t\n\tHyperLearn\'s implementation also handles rank deficient and ill-conditioned\n\tmatrices perfectly with the help of the limiting behaivour of\n\tadding forced epsilon regularization.\n\t\n\tSpeed\n\t--------------\n\tIf USE_GPU:\n\t\tUses PyTorch\'s Cholesky. Speed is OK.\n\tIf CPU:\n\t\tUses Numpy\'s Fortran C based Cholesky.\n\t\tIf NUMBA is not installed, uses very fast LAPACK functions.\n\t\n\tStability\n\t--------------\n\tAlpha is added for regularization purposes. This prevents system\n\trounding errors and promises better convergence rates.\n\t""""""\n\tn,p = X.shape\n\tXT = X.T\n\tmemoryCovariance(X)\n\tcovariance = _XTX(XT) if n >= p else _XXT(XT)\n\t\n\tcho = cholesky(covariance, alpha = alpha, fast = fast)\n\tinv = invCholesky(cho, fast = fast)\n\t\n\treturn inv @ XT if n >= p else XT @ inv\n\n\n\ndef svd(X, fast = True, U_decision = False, transpose = True):\n\t""""""\n\t[Edited 9/11/2018 --> Modern Big Data Algorithms p/n ratio check]\n\tComputes the Singular Value Decomposition of any matrix.\n\tSo, X = U * S @ VT. Note will compute svd(X.T) if p > n.\n\tShould be 99% same result. This means this implementation\'s\n\ttime complexity is O[ min(np^2, n^2p) ]\n\t\n\tSpeed\n\t--------------\n\tIf USE_GPU:\n\t\tUses PyTorch\'s SVD. PyTorch uses (for now) a NON divide-n-conquer algo.\n\t\tSubmitted report to PyTorch:\n\t\thttps://github.com/pytorch/pytorch/issues/11174\n\tIf CPU:\n\t\tUses Numpy\'s Fortran C based SVD.\n\t\tIf NUMBA is not installed, uses divide-n-conqeur LAPACK functions.\n\tIf Transpose:\n\t\tWill compute if possible svd(X.T) instead of svd(X) if p > n.\n\t\tDefault setting is TRUE to maintain speed.\n\t\n\tStability\n\t--------------\n\tSVD_Flip is used for deterministic output. Does NOT follow Sklearn convention.\n\tThis flips the signs of U and VT, using VT_based decision.\n\t""""""\n\ttranspose = True if (transpose and X.shape[1] > X.shape[0]) else False\n\tX = _float(X)\n\tif transpose: \n\t\tX, U_decision = X.T, not U_decision\n\n\tn, p = X.shape\n\tratio = p/n\n\t#### TO DO: If memory usage exceeds LWORK, use GESVD\n\tif ratio >= 0.001:\n\t\tif USE_NUMBA:\n\t\t\tU, S, VT = numba.svd(X)\n\t\telse:\n\t\t\t#### TO DO: If memory usage exceeds LWORK, use GESVD\n\t\t\tU, S, VT, __ = lapack(""gesdd"", fast)(X, full_matrices = False)\n\telse:\n\t\tU, S, VT = lapack(""gesvd"", fast)(X, full_matrices = False)\n\t\t\n\tU, VT = svd_flip(U, VT, U_decision = U_decision)\n\t\n\tif transpose:\n\t\treturn VT.T, S, U.T\n\treturn U, S, VT\n\t\t\n\n\ndef pinv(X, alpha = None, fast = True):\n\t""""""\n\tComputes the pseudoinverse of any matrix.\n\tThis means X @ pinv(X) = eye(n).\n\t\n\tOptional alpha is used for regularization purposes.\n\t\n\tSpeed\n\t--------------\n\tIf USE_GPU:\n\t\tUses PyTorch\'s SVD. PyTorch uses (for now) a NON divide-n-conquer algo.\n\t\tSubmitted report to PyTorch:\n\t\thttps://github.com/pytorch/pytorch/issues/11174\n\tIf CPU:\n\t\tUses Numpy\'s Fortran C based SVD.\n\t\tIf NUMBA is not installed, uses divide-n-conqeur LAPACK functions.\n\t\n\tStability\n\t--------------\n\tCondition number is:\n\t\tfloat32 = 1e3 * eps * max(S)\n\t\tfloat64 = 1e6 * eps * max(S)\n\t""""""\n\tif alpha is not None: assert alpha >= 0\n\talpha = 0 if alpha is None else alpha\n\t\n\tU, S, VT = svd(X, fast = fast)\n\tU, S, VT = _svdCond(U, S, VT, alpha)\n\treturn (VT.T * S) @ U.T\n\t\n\n\ndef eigh(XTX, alpha = None, fast = True, svd = False, positive = False, qr = False):\n\t""""""\n\tComputes the Eigendecomposition of a Hermitian Matrix\n\t(Positive Symmetric Matrix).\n\t\n\tNote: Slips eigenvalues / eigenvectors with MAX first.\n\tScipy convention is MIN first, but MAX first is SVD convention.\n\t\n\tUses the fact that the matrix is special, and so time\n\tcomplexity is approximately reduced by 1/2 or more when\n\tcompared to full SVD.\n\n\tIf POSITIVE is True, then all negative eigenvalues will be set\n\tto zero, and return value will be VT and not V.\n\n\tIf SVD is True, then eigenvalues will be square rooted as well.\n\t\n\tSpeed\n\t--------------\n\tIf USE_GPU:\n\t\tUses PyTorch\'s EIGH. PyTorch uses (for now) a non divide-n-conquer algo.\n\tIf CPU:\n\t\tUses Numpy\'s Fortran C based EIGH.\n\t\tIf NUMBA is not installed, uses very fast divide-n-conqeur LAPACK functions.\n\t\tNote Scipy\'s EIGH as of now is NON divide-n-conquer.\n\t\tSubmitted report to Scipy:\n\t\thttps://github.com/scipy/scipy/issues/9212\n\t\t\n\tStability\n\t--------------\n\tAlpha is added for regularization purposes. This prevents system\n\trounding errors and promises better convergence rates.\n\n\tAlso uses eig_flip to flip the signs of the eigenvectors\n\tto ensure deterministic output.\n\t""""""\n\tn,p = XTX.shape\n\tassert n == p\n\terror = 1\n\talpha = ALPHA_DEFAULT if alpha is None else alpha\n\told_alpha = 0\n\t\n\tdecomp = lapack(""syevd"", fast, ""eigh"") if not qr else lapack(""syevr"", fast) \n\n\twhile error != 0:\n\t\tif PRINT_ALL_WARNINGS: \n\t\t\tprint(\'eigh Alpha = {}\'.format(alpha))\n\n\t\t# Add epsilon jitter to diagonal. Note adding\n\t\t# np.eye(p)*alpha is slower and uses p^2 memory\n\t\t# whilst flattening uses only p memory.\n\t\taddDiagonal(XTX, alpha-old_alpha)\n\t\ttry:\n\t\t\toutput = decomp(XTX)\n\t\t\tif USE_NUMBA: \n\t\t\t\tS2, V = output\n\t\t\t\terror = 0\n\t\t\telse: \n\t\t\t\tS2, V, error = output\n\t\texcept: pass\n\t\tif error != 0:\n\t\t\told_alpha = alpha\n\t\t\talpha *= 10\n\n\taddDiagonal(XTX, -alpha)\n\tS2, V = S2[::-1], eig_flip(V[:,::-1])\n\n\tif svd or positive: \n\t\tS2[S2 < 0] = 0.0\n\t\tV = V.T\n\tif svd:\n\t\tS2 **= 0.5\n\treturn S2, V\n\n\n_svd = svd\ndef eig(X, alpha = None, fast = True, U_decision = False, svd = False, stable = False):\n\t""""""\n\t[Edited 8/11/2018 Made QR-SVD even faster --> changed to n >= p from n >= 5/3p]\n\tComputes the Eigendecomposition of any matrix using either\n\tQR then SVD or just SVD. This produces much more stable solutions \n\tthat pure eigh(covariance), and thus will be necessary in some cases.\n\n\tIf STABLE is True, then EIGH will be bypassed, and instead SVD or QR/SVD\n\twill be used instead. This is to guarantee stability, since EIGH\n\tuses epsilon jitter along the diagonal of the covariance matrix.\n\t\n\tSpeed\n\t--------------\n\tIf n >= 5/3 * p:\n\t\tUses QR followed by SVD noticing that U is not needed.\n\t\tThis means Q @ U is not required, reducing work.\n\n\t\tNote Sklearn\'s Incremental PCA was used for the constant\n\t\t5/3 [`Matrix Computations, Third Edition, G. Holub and C. \n\t\tVan Loan, Chapter 5, section 5.4.4, pp 252-253.`]\n\n\tElse If n >= p:\n\t\tSVD is used, as QR would be slower.\n\n\tElse If n <= p:\n\t\tSVD Transpose is used svd(X.T)\n\n\tIf stable is False:\n\t\tEigh is used or SVD depending on the memory requirement.\n\t\t\n\tStability\n\t--------------\n\tEig is the most stable Eigendecomposition in HyperLearn. It\n\tsurpasses the stability of Eigh, as no epsilon jitter is added,\n\tunless specified when stable = False.\n\t""""""\n\tn,p = X.shape\n\tmemCheck = memoryXTX(X)\n\n\t# Note when p >= n, EIGH will return incorrect results, and hence HyperLearn\n\t# will default to SVD or QR/SVD\n\tif stable or not memCheck or p > n:\n\t\t# From Daniel Han-Chen\'s Modern Big Data Algorithms --> if n is larger\n\t\t# than p, then use QR. Old is >= 5/3*p.\n\t\tif n >= p:\n\t\t\t# Q, R = qr(X)\n\t\t\t# U, S, VT = svd(R)\n\t\t\t# S, VT is kept.\n\t\t\t__, S, V = _svd( qr(X, R_only = True), fast = fast, U_decision = U_decision)\n\t\telse:\n\t\t\t# Force turn on transpose:\n\t\t\t# either computes svd(X) or svd(X.T)\n\t\t\t# whichever is faster. [p >= n --> svd(X.T)]\n\t\t\t__, S, V = _svd(X, transpose = True, fast = fast, U_decision = U_decision)\n\t\tif not svd:\n\t\t\tS **= 2\n\t\t\tV = V.T\n\telse:\n\t\tS, V = eigh(_XTX(X.T), fast = fast, alpha = alpha)\n\t\tif svd:\n\t\t\tS **= 0.5\n\t\t\tV = V.T\n\t\t\t\n\treturn S, V\n\n\n\n\t\ndef pinvh(XTX, alpha = None, fast = True):\n\t""""""\n\tComputes the pseudoinverse of a Hermitian Matrix\n\t(Positive Symmetric Matrix) using Eigendecomposition.\n\t\n\tUses the fact that the matrix is special, and so time\n\tcomplexity is approximately reduced by 1/2 or more when\n\tcompared to full SVD.\n\t\n\tSpeed\n\t--------------\n\tIf USE_GPU:\n\t\tUses PyTorch\'s EIGH. PyTorch uses (for now) a non divide-n-conquer algo.\n\tIf CPU:\n\t\tUses Numpy\'s Fortran C based EIGH.\n\t\tIf NUMBA is not installed, uses very fast divide-n-conqeur LAPACK functions.\n\t\tNote Scipy\'s EIGH as of now is NON divide-n-conquer.\n\t\tSubmitted report to Scipy:\n\t\thttps://github.com/scipy/scipy/issues/9212\n\t\n\tStability\n\t--------------\n\tCondition number is:\n\t\tfloat32 = 1e3 * eps * max(abs(S))\n\t\tfloat64 = 1e6 * eps * max(abs(S))\n\t\t\n\tAlpha is added for regularization purposes. This prevents system\n\trounding errors and promises better convergence rates.\n\t""""""\n\tassert XTX.shape[0] == XTX.shape[1]\n\n\tS2, V = eigh(XTX, alpha = alpha, fast = fast)\n\tS2, V = _eighCond(S2, V)\n\treturn (V / S2) @ V.T\n\n\n\ndef pinvEig(X, alpha = None, fast = True):\n\t""""""\n\tComputes the approximate pseudoinverse of any matrix X\n\tusing Eigendecomposition on the covariance matrix XTX or XXT\n\t\n\tUses a special trick where:\n\t\tIf n >= p: X^-1 approx = (XT @ X)^-1 @ XT\n\t\tIf n < p:  X^-1 approx = XT @ (X @ XT)^-1\n\t\n\tSpeed\n\t--------------\n\tIf USE_GPU:\n\t\tUses PyTorch\'s EIGH. PyTorch uses (for now) a non divide-n-conquer algo.\n\tIf CPU:\n\t\tUses Numpy\'s Fortran C based EIGH.\n\t\tIf NUMBA is not installed, uses very fast divide-n-conqeur LAPACK functions.\n\t\tNote Scipy\'s EIGH as of now is NON divide-n-conquer.\n\t\tSubmitted report to Scipy:\n\t\thttps://github.com/scipy/scipy/issues/9212\n\t\n\tStability\n\t--------------\n\tCondition number is:\n\t\tfloat32 = 1e3 * eps * max(abs(S))\n\t\tfloat64 = 1e6 * eps * max(abs(S))\n\t\t\n\tAlpha is added for regularization purposes. This prevents system\n\trounding errors and promises better convergence rates.\n\t""""""\n\tn,p = X.shape\n\tXT = X.T\n\tmemoryCovariance(X)\n\tcovariance = _XTX(XT) if n >= p else _XXT(XT)\n\t\n\tS2, V = eigh(covariance, alpha = alpha, fast = fast)\n\tS2, V = _eighCond(S2, V)\n\tinv = (V / S2) @ V.T\n\n\treturn inv @ XT if n >= p else XT @ inv\n'"
hyperlearn/multiprocessing.py,1,"b'from torch.multiprocessing import Pool, cpu_count\nfrom .base import isIterable, isTensor\nfrom copy import copy\n\n__all__ = [\'ParallelReference\']\n\n\nclass ParallelReferenceError(BaseException):\n\tdef __init__(self, text = \'At least 1 item needs to be type (array, list), \'\n\t\t\t\t\t\t\t\t\'as that acts as the base item for which the \'\n\t\t\t\t\t\t\t\t\'multiprocessing functions.\'):\n\t\tself.text = text\n\tdef __str__(self):\n\t\treturn self.text\n\n""""""\n------------------------------------------------------------\nParallel_Reference\n\tThis module requires a list / vector to act as a ""reference""\n\nUpdated 27/8/2018\n------------------------------------------------------------\n""""""\nclass ParallelReference(BaseEstimator):\n\t\'\'\'\n\tParallel_Reference\'s syntax is like joblib.\n\tParallel_Reference(f, n_jobs = 1, reference = -1)\n\n\tEg: \n\t\tParallel_Reference(f)(X, [1,2,3], 1)\n\n\t\t[1,2,3] (index 1) acts as the reference, so f(X, 1), f(X, 2), ...\n\t\tis called.\n\n\tIf you want instead given f, X, just process f(X[0:10]), f(X[10:20]), ...\n\tmaybe use ParallelProcess instead.\n\t\'\'\'\n\t\n\tdef __init__(self, f, n_jobs = 1, reference = -1):\n\t\t\n\t\tself.count = cpu_count()\n\t\tassert type(n_jobs) is int\n\t\tassert type(reference) is int\n\t\t\n\t\tif n_jobs == -1 or n_jobs > self.count:\n\t\t\tself.n_jobs = self.count\n\t\telif n_jobs == \'fit\':\n\t\t\tself.n_jobs = None\n\t\telse:\n\t\t\tself.n_jobs = n_jobs\n\t\tself.f = f\n\t\tself.reference = reference\n\t\t\n\t\t\n\tdef __call__(self, *args, **kwargs):\n\t\t\n\t\tif self.n_jobs == 1:\n\t\t\targs = list(args)\n\t\t\tif self.reference > -1:\n\t\t\t\toutput = []\n\n\t\t\t\tfor j in copy(args[self.reference]):\n\t\t\t\t\targs[self.reference] = j\n\t\t\t\t\tout = self.f(*args)\n\t\t\t\t\toutput.append(out)\n\t\t\telse:\n\t\t\t\traise ParallelReferenceError()\n\t\telse:\n\t\t\toutput = self.multiprocess(*args, **kwargs)\n\n\t\treturn self.process(output)\n\t\t\n\t\n\tdef multiprocess(self, *args, **kwargs):\n\t\tfoundIter = False\n\t\tif self.reference > -1:\n\t\t\tfoundIter = self.reference\n\t\t\tlength = len(args[self.reference])\n\t\telse:\n\t\t\tfor i,x in enumerate(args):\n\t\t\t\tif isIterable(x):\n\t\t\t\t\tfoundIter = i\n\t\t\t\t\tlength = len(x)\n\t\t\t\t\tbreak\n\n\t\tif self.n_jobs is None:\n\t\t\tself.n_jobs = length if length > self.count else self.count\n\t\tself.n_jobs = length if self.count > length else self.n_jobs\n\t\t\t\n\t\tif foundIter is False:\n\t\t\traise ParallelReferenceError()\n\t\t\n\t\ttoCall = []\n\t\tfor i,x in enumerate(args):\n\t\t\tif foundIter == i:\n\t\t\t\ttoCall.append(x)\n\t\t\t\tcontinue\n\t\t\tif isTensor(x):\n\t\t\t\tx.share_memory_()\n\t\t\ttoCall.append([x]*length)\n\t\t\n\t\twith Pool(processes = self.n_jobs) as pool:\n\t\t\toutput = pool.starmap(self.f, zip(*toCall) )\n\t\treturn output\n\n\n\tdef process(self, output):\n\t\tfinalOutput = []\n\t\tfor i in range(len(output[0])):\n\t\t\teachVariable = []\n\t\t\tfor j in output:\n\t\t\t\teachVariable.append(j[i])\n\t\t\tfinalOutput.append(eachVariable)\n\t\treturn finalOutput\n\n\t\t'"
hyperlearn/numba.py,0,"b'\nfrom numpy import ones, eye, float32, float64, \\\n\t\t\t\tsum as __sum, arange as _arange, sign as __sign, uint as _uint, \\\n\t\t\t\tabs as __abs, minimum as _minimum, maximum as _maximum\nfrom numpy.linalg import svd as _svd, pinv as _pinv, eigh as _eigh, \\\n\t\t\t\t\tcholesky as _cholesky, lstsq as _lstsq, qr as _qr, \\\n\t\t\t\t\tnorm as _norm\nfrom numba import njit, prange\nfrom .base import USE_NUMBA\n\n__all__ = [\'svd\', \'pinv\', \'eigh\', \'cholesky\', \'lstsq\', \'qr\',\'norm\',\n\t\t\t\'mean\', \'_sum\', \'sign\', \'arange\', \'_abs\', \'minimum\', \'maximum\',\n\t\t\t\'multsum\', \'squaresum\', \'_sign\']\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef svd(X):\n\treturn _svd(X, full_matrices = False)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef pinv(X):\n\treturn _pinv(X)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef eigh(XTX):\n\treturn _eigh(XTX)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef cholesky(XTX):\n\treturn _cholesky(XTX)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef lstsq(X, y):\n\treturn _lstsq(X, y.astype(X.dtype))[0]\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef qr(X):\n\treturn _qr(X)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef norm(v, d = 2):\n\treturn _norm(v, d)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef _0mean(X, axis = 0):\n\treturn __sum(X, axis)/X.shape[axis]\n\n\ndef mean(X, axis = 0):\n\tif axis == 0 and X.flags[\'C_CONTIGUOUS\']:\n\t\treturn _0mean(X)\n\telse:\n\t\treturn X.mean(axis)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef sign(X):\n\treturn __sign(X)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef arange(i):\n\treturn _arange(i)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef _sum(X, axis = 0):\n\treturn __sum(X, axis)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef _abs(v):\n\treturn __abs(v)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef maximum(X, i):\n    return _maximum(X, i)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef minimum(X, i):\n    return _minimum(X, i)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef _min(a,b):\n    if a < b:\n        return a\n    return b\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef _max(a,b):\n    if a < b:\n        return b\n    return a\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef _sign(x):\n    if x < 0:\n        return -1\n    return 1\n\n\n@njit(fastmath = True, nogil = True, parallel = True)\ndef multsum(a,b):\n    s = a[0]*b[0]\n    for i in prange(1,len(a)):\n        s += a[i]*b[i]\n    return s\n\n\n@njit(fastmath = True, nogil = True, parallel = True)\ndef squaresum(v):\n\tif len(v.shape) == 1:\n\t    s = v[0]**2\n\t    for i in prange(1,len(v)):\n\t        s += v[i]**2\n\t# else:\n\n #    return s\n\n\n## TEST\nprint(""""""Note that first time import of HyperLearn will be slow, """"""\n\t\t""""""since NUMBA code has to be compiled to machine code """"""\n\t\t""""""for optimization purposes."""""")\n\ny32 = ones(2, dtype = float32)\ny64 = ones(2, dtype = float64)\n\n\nX = eye(2, dtype = float32)\nA = svd(X)\nA = eigh(X)\nA = cholesky(X)\nA = pinv(X)\nA = lstsq(X, y32)\nA = lstsq(X, y64)\nA = qr(X)\nA = norm(y32)\nA = norm(y64)\nA = mean(X)\nA = mean(y32)\nA = mean(y64)\nA = _sum(X)\nA = _sum(y32)\nA = _sum(y64)\nA = sign(X)\nA = arange(100)\nA = _abs(y32)\nA = _abs(y64)\nA = _abs(X)\nA = _abs(10.0)\nA = _abs(10)\nA = minimum(X, 0)\nA = minimum(y32, 0)\nA = minimum(y64, 0)\nA = maximum(X, 0)\nA = maximum(y32, 0)\nA = maximum(y64, 0)\nA = _min(0,1)\nA = _min(0.1,1.1)\nA = _max(0,1)\nA = _max(0.1,1.1)\nA = multsum(y32, y32)\nA = multsum(y32, y64)\nA = multsum(y64, y64)\nA = squaresum(y32)\nA = squaresum(X)\nA = squaresum(y64)\nA = _sign(-1)\nA = _sign(-1.2)\nA = _sign(1.2)\n\n\nX = eye(2, dtype = float64)\nA = svd(X)\nA = eigh(X)\nA = cholesky(X)\nA = pinv(X)\nA = lstsq(X, y32)\nA = lstsq(X, y64)\nA = qr(X)\nA = norm(y32, 2)\nA = norm(y64, 2)\nA = mean(X, 1)\nA = _sum(X)\nA = sign(X)\nA = _abs(X)\nA = maximum(X, 0)\nA = minimum(X, 0)\nA = squaresum(X)\n\n\nA = None\nX = None\ny32 = None\ny64 = None\n'"
hyperlearn/random.py,0,"b'\nfrom numpy.random import uniform as _uniform\nfrom numpy import tile, float32, zeros, newaxis\nfrom numba import njit\n\n\ndef uniform(left, right, n, p = None, dtype = float32):\n\t""""""\n\t[Added 6/11/2018]\n\n\tProduces pseudo-random uniform numbers between left and right range.\n\tNotice much more memory efficient than Numpy, as provides\n\ta DTYPE argument (float32 supported).\n\t""""""\n\tl, r = left/2, right/2\n\tdtype = zeros(1, dtype = dtype)\n\n\tif p == None:\n\t\t# Only 1 long vector --> easily made.\n\t\treturn uniform_vector(left, right, n, dtype)\n\n\tpart = uniform_vector(l, r, p, dtype)\n\tX = tile(part, (n, 1))\n\tadd = uniform_vector(l, r, n, dtype)[:, newaxis]\n\tmult = uniform_vector(0, 1, n, dtype)[:, newaxis]\n\treturn (X + add) * mult\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef uniform_vector(l, r, size, dtype):\n\tzero = zeros(size, dtype = dtype.dtype)\n\tfor j in range(size):\n\t\tzero[j] = _uniform(l, r)\n\treturn zero\n\n'"
hyperlearn/solvers.py,0,"b'\nfrom .linalg import *\nfrom .base import *\nfrom .utils import _svdCond, _float, _XTX, _XXT, fastDot\nfrom .numba import lstsq as _lstsq\nfrom .big_data import LSMR\nfrom numpy import newaxis, cumsum, sqrt, hstack\nfrom .big_data.randomized import randomizedSVD\nfrom .big_data.truncated import truncatedEig\n\n\n__all__ = [\'solve\', \'solveCholesky\', \'solveSVD\', \'solveEig\', \'solvePartial\', \'lstsq\',\n\t\t\t\'solveTLS\']\n\n\ndef solve(X, y, tol = 1e-6, condition_limit = 1e8, alpha = None, weights = None, copy = False,\n\t\t\tnon_negative = False, max_iter = None):\n\t""""""\n\t[As of 12/9/2018, an optional non_negative argument is added. Note as accurate as Scipy\'s NNLS,\n\tby copies ideas from gradient descent.]\n\n\t[NOTE: as of 12/9/2018, LSMR is default in HyperLearn, replacing the 2nd fastest Cholesky\n\tSolve. LSMR is 2-4 times faster, and uses N less memory]\n\n\t>>> WEIGHTS is an array of Weights for Weighted / Generalized Least Squares [default None]\n\t>>> theta = (XT*W*X)^-1*(XT*W*y)\n\n\tImplements extremely fast least squares LSMR using orthogonalization as seen in Scipy\'s LSMR and\n\thttps://arxiv.org/abs/1006.0758 [LSMR: An iterative algorithm for sparse least-squares problems]\n\tby David Fong, Michael Saunders.\n\n\tScipy\'s version of LSMR is surprisingly slow, as some slow design factors were used\n\t(ie np.sqrt(1 number) is slower than number**0.5, or min(a,b) is slower than using 1 if statement.)\n\n\tALPHA is provided for regularization purposes like Ridge Regression.\n\t\n\tSpeed\n\t--------------\n\tThis algorithm works well for Sparse Matrices as well, and the time complexity analysis is approx:\n\t\tX.T @ y   * min(n,p) times + 3 or so O(n) operations\n\t\t==> O(np)*min(n,p)\n\t\t==> either min(O(n^2p + n), O(np^2 + n))\n\t\t*** Note if Weights is present, complexity increases.\n\t\t\tInstead of fitting X^T*W*X, fits X^T/sqrt(W)\n\t\t\tSo, O(np+n) is needed extra.\n\n\tThis complexity is much better than Cholesky Solve which is the next fastest in HyperLearn.\n\tCholesky requires O(np^2) for XT * X, then Cholesky needs an extra 1/3*O(np^2), then inversion\n\ttakes another 1/3*(np^2), and finally (XT*y) needs O(np).\n\n\tSo Cholesky needs O(5/3np^2 + np) >> min(O(n^2p + n), O(np^2 + n))\n\n\tSo by factor analysis, expect LSMR to be approx 2 times faster or so.\n\n\tMemory\n\t--------------\n\tInterestingly, the Space Complexity is even more staggering. LSMR takes only maximum O(np^2) space\n\tfor the computation of XT * y + some overhead.\n\t\t*** Note if Weights is present, and COPY IS TRUE, then memory is DOUBLED.\n\t\t\tHence, try setting COPY to FALSE, memory will not change, and X will return back to its\n\t\t\toriginal state afterwards.\n\n\tCholesky requires XT * X space, which is already max O(n^2p) [which is huge].\n\tEssentially, Cholesky shines when P is large, but N is small. LSMR is good for large N, medium P\n\n\tWeighted Least Squares\n\t------------------------\n\tTheta_hat = (XT * W * X)^-1 * (XT * y)\n\tIn other words in gradient descent / iterative solves solve:\n\t\tX * sqrt(W) * theta_hat = y * sqrt(W)\n\n\t\tor: X*sqrt(W)  ==>  y*sqrt(W)\n\t""""""\n\n\tif len(y.shape) > 1:\n\t\tif y.shape[1] > 1:\n\t\t\tprint(""LSMR can only work on single Ys. Try fitting 2 or more models."")\n\t\t\treturn\n\n\n\tif weights is not None:\n\t\tif len(weights.shape) > 1:\n\t\t\tif weights.shape[1] > 1:\n\t\t\t\tprint(""Weights must be 1 dimensional."")\n\t\t\t\treturn\n\t\tweights = weights.ravel()**0.5\n\t\tW = weights[:,newaxis]\n\t\tif copy:\n\t\t\tX = X*W\n\t\t\ty = y*weights\n\t\telse:\n\t\t\tX *= W\n\t\t\ty *= weights\n\n\talpha = 0 if alpha is None else alpha\n\n\tgood = False\n\twhile not good:\n\t\ttheta_hat, good = LSMR(X, y, tol = tol, condition_limit = condition_limit, alpha = alpha,\n\t\t\t\t\t\t\t\tnon_negative = non_negative, max_iter = max_iter)\n\t\talpha = ALPHA_DEFAULT if alpha == 0 else alpha*10\n\n\t# Return X back to its original state\n\tif not copy and weights is not None:\n\t\tX /= W\n\t\ty /= weights\n\n\treturn theta_hat\n\n\t\n\n\ndef solveCholesky(X, y, alpha = None, fast = True):\n\t""""""\n\t[Added 23/9/2018 added matrix multiplication decisions (faster multiply)\n\t ie: if (XTX)^1(XTy) or ((XTX)^-1XT)y is faster]\n\t[Edited 20/10/2018 Major update - added LAPACK cholSolve --> 20% faster]\n\t[Edited 30/10/2018 Reduced RAM usage by clearing unused variables]\n\n\tComputes the Least Squares solution to X @ theta = y using Cholesky\n\tDecomposition. This is the default solver in HyperLearn.\n\t\n\tCholesky Solving is used as the 2nd default solver [as of 12/9/2018, default\n\thas been switched to LSMR (called solve)] in HyperLearn,\n\tas it is super fast and allows regularization. HyperLearn\'s\n\timplementation also handles rank deficient and ill-conditioned\n\tmatrices perfectly with the help of the limiting behaivour of\n\tadding forced epsilon regularization.\n\t\n\tOptional alpha is used for regularization purposes.\n\t\n\t|  Method   |   Operations    | Factor * np^2 |\n\t|-----------|-----------------|---------------|\n\t| Cholesky  |   1/3 * np^2    |      1/3      |\n\t|    QR     |   p^3/3 + np^2  |   1 - p/3n    |\n\t|    SVD    |   p^3   + np^2  |    1 - p/n    |\n\t\n\tSpeed\n\t--------------\n\tIf USE_GPU:\n\t\tUses PyTorch\'s Cholesky and Triangular Solve given identity matrix. \n\t\tSpeed is OK.\n\tIf CPU:\n\t\tUses Numpy\'s Fortran C based Cholesky.\n\t\tIf NUMBA is not installed, uses very fast LAPACK functions.\n\t\tAlso, uses very fast LAPACK algorithms for triangular system inverses.\n\t\n\tStability\n\t--------------\n\tNote that LAPACK\'s single precision (float32) solver (strtri) is much more\n\tunstable than double (float64). You might see stability problems if FAST = TRUE.\n\tSet it to FALSE if theres issues.\n\t""""""\n\tn,p = X.shape\n\tX, y = _float(X), _float(y)\n\tXT = X.T\n\tcovariance = _XTX(XT) if n >= p else _XXT(XT)\n\t\n\tcho = cholesky(covariance, alpha = alpha, fast = fast)\n\tdel covariance; covariance = None; # saving memory\n\n\n\tif n >= p:\n\t\t# Use spotrs solve from LAPACK\n\t\treturn cholSolve(cho, XT @ y, alpha = alpha)\n\telse:\n\t\tinv = invCholesky(cho, fast = fast)\n\t\treturn fastDot(XT, inv, y)\n\n\t#return fastDot(inv, XT, y) if n >= p else fastDot(XT, inv, y)\n\t\n\n\n\ndef solveSVD(X, y, n_components = None, alpha = None, fast = True):\n\t""""""\n\t[Edited 6/11/2018 Added n_components for Partial Solving]\n\tComputes the Least Squares solution to X @ theta = y using SVD.\n\tSlow, but most accurate. Specify n_components to reduce overfitting.\n\tHeurestic is 95% of variance is captured, if set to \'auto\'.\n\t\n\tOptional alpha is used for regularization purposes.\n\t\n\tSpeed\n\t--------------\n\tIf USE_GPU:\n\t\tUses PyTorch\'s SVD. PyTorch uses (for now) a NON divide-n-conquer algo.\n\t\tSubmitted report to PyTorch:\n\t\thttps://github.com/pytorch/pytorch/issues/11174\n\tIf CPU:\n\t\tUses Numpy\'s Fortran C based SVD.\n\t\tIf NUMBA is not installed, uses divide-n-conqeur LAPACK functions.\n\t\n\tStability\n\t--------------\n\tCondition number is:\n\t\tfloat32 = 1e3 * eps * max(S)\n\t\tfloat64 = 1e6 * eps * max(S)\n\t""""""\n\tif alpha != None: assert alpha >= 0\n\talpha = 0 if alpha is None else alpha\n\tX, y = _float(X), _float(y)\n\n\tU, S, VT = svd(X, fast = fast)\n\tU, S, VT = _svdCond(U, S, VT, alpha)\n\n\tif type(n_components) == float:\n\t\tif n_components > 1:\n\t\t\tn_components = int(n_components)\n\n\tif n_components == \'auto\' or type(n_components) == int:\n\t\t# Notice heuristic of 90% variance explained.\n\t\ts = S / S.sum()\n\t\ts = cumsum(s)\n\n\t\tif n_components == \'auto\':\n\t\t\tfor i in range(len(s)):\n\t\t\t\tif s[i] >= 0.9: break\n\t\telse:\n\t\t\ti = n_components\n\t\tU, S, VT = U[:,:i], S[:i], VT[:i]\n\n\treturn fastDot(VT.T * S,   U.T,   y)\n\n\n\ndef solvePartial(X, y, n_components = None, alpha = None, fast = True):\n\t""""""\n\t[Added 6/11/2018]\n\tComputes the Least Squares solution to X @ theta = y using Randomized SVD.\n\tMuch faster than normal SVD solving, and is not prone is overfitting.\n\t\n\tOptional alpha is used for regularization purposes.\n\t""""""\n\tif alpha != None: assert alpha >= 0\n\talpha = 0 if alpha is None else alpha\n\n\tif n_components == None or n_components == \'auto\':\n\t\t# will provide approx sqrt(p) - 1 components.\n\t\t# A heuristic, so not guaranteed to work.\n\t\tk = int(sqrt(X.shape[1]))-1\n\t\tif k <= 0: k = 1\n\telse:\n\t\tk = int(n_components) if n_components > 0 else 1\n\n\tX, y = _float(X), _float(y)\n\n\tU, S, VT = randomizedSVD(X, k)\n\tU, S, VT = _svdCond(U, S, VT, alpha)\n\t\n\treturn fastDot(VT.T * S,   U.T,   y)\n\n\n\ndef solveEig(X, y, alpha = None, fast = True):\n\t""""""\n\t[Edited 30/10/2018 Reduced RAM usage by clearing unused variables]\n\t\n\tComputes the Least Squares solution to X @ theta = y using\n\tEigendecomposition on the covariance matrix XTX or XXT.\n\tMedium speed and accurate, where this lies between\n\tSVD and Cholesky.\n\t\n\tOptional alpha is used for regularization purposes.\n\t\n\tSpeed\n\t--------------\n\tIf USE_GPU:\n\t\tUses PyTorch\'s EIGH. PyTorch uses (for now) a non divide-n-conquer algo.\n\t\tSubmitted report to PyTorch:\n\t\thttps://github.com/pytorch/pytorch/issues/11174\n\tIf CPU:\n\t\tUses Numpy\'s Fortran C based EIGH.\n\t\tIf NUMBA is not installed, uses divide-n-conqeur LAPACK functions.\n\t\tNote Scipy\'s EIGH as of now is NON divide-n-conquer.\n\t\tSubmitted report to Scipy:\n\t\thttps://github.com/scipy/scipy/issues/9212\n\t\n\tStability\n\t--------------\n\tCondition number is:\n\t\tfloat32 = 1e3 * eps * max(abs(S))\n\t\tfloat64 = 1e6 * eps * max(abs(S))\n\t\t\n\tAlpha is added for regularization purposes. This prevents system\n\trounding errors and promises better convergence rates.\n\t""""""\n\tn,p = X.shape\n\tX, y = _float(X), _float(y)\n\tXT = X.T\n\tcovariance = _XTX(XT) if n >= p else _XXT(XT)\n\t\n\tinv = pinvh(covariance, alpha = alpha, fast = fast)\n\tdel covariance; covariance = None; # saving memory\n\n\treturn fastDot(inv, XT, y) if n >= p else fastDot(XT, inv, y)\n\n\n\ndef lstsq(X, y):\n\t""""""\n\tReturns normal Least Squares solution using LAPACK and Numba if\n\tinstalled. PyTorch will default to Cholesky Solve.\n\t""""""\n\tX, y = _float(X), _float(y)\n\treturn _lstsq(X, y)\n\n\n\ndef solveTLS(X, y, solver = \'truncated\'):\n\t""""""\n\t[Added 6/11/2018]\n\tPerforms Total Least Squares based on the implementation in Wikipedia:\n\thttps://en.wikipedia.org/wiki/Total_least_squares.\n\tThe naming is rather deceptive, as it doesn\'t mean it\'ll yield better\n\tresults than pure SVD solving.\n\tNormal linear regression assumes Y|X has gaussian noise. TLS assumes this\n\tAND X|Y has noise.\n\n\tTwo solvers - full, truncated. Truncated is much much faster, as smallest\n\teigen component is needed. Full solver uses Eigendecomposition, which is\n\tmuch much slower, but more accurate.\n\t""""""\n\tp = X.shape[1]\n\tX, y = _float(X), _float(y)\n\tif len(y.shape) > 1:\n\t\tprint(""solveTLS works only on single column Ys."")\n\t\treturn\n\tZ = hstack((X, y[:, newaxis]))\n\n\tif solver == \'full\':\n\t\t__, V = eig(Z)\n\n\t\tVXY = V[:p, p]\n\t\tVYY = V[p:, p]\n\telse:\n\t\t# Uses truncatedEig\n\t\tV = truncatedEig(Z, 1, which = \'smallest\')[1].flatten()\n\t\tVXY = V[:p]\n\t\tVYY = V[p]\n\n\ttheta_hat = -VXY / VYY\n\n\treturn theta_hat\n\n'"
hyperlearn/stats.py,0,"b'from .base import *\nfrom scipy.stats import t as tdist\nfrom .numba import mean\n\n__all__ = [\'corr\', \'qr_stats\',\'svd_stats\',\'ridge_stats\']\n\n\ndef corr(X, y):\n    same = y is X\n    \n    _X = X - mean(X, 0)\n    _y = y - mean(y, 0) if not same else _X\n    \n    if len(X.shape) == 1:\n        _X2 = einsum(\'i,i\', _X, _X)**0.5\n    else:\n        _X2 = einsum(\'ij,ij->j\', _X, _X)**0.5\n        \n\n    if len(y.shape) == 1:\n        _y2 = einsum(\'i,i\',_y,_y)**0.5\n    else:\n        _y2 = einsum(\'ij,ij->j\',_y,_y)**0.5 if not same else _X2\n        _X2 = _X2[:,newaxis]\n        \n    corr = (_X.T @ _y) / _X2 /_y2\n    \n    return corr\n\n""""""\n------------------------------------------------------------\nQR_STATS\nUpdated 27/8/2018\n------------------------------------------------------------\n""""""\ndef qr_stats(Q, R):\n\t\'\'\'\n\tXTX^-1  =  RT * R\n\t\n\th = diag  Q * QT\n\t\n\tmean(h) used for normalized leverage\n\t\'\'\'\n\tXTX = T(R).matmul(R)\n\t_XTX = pinv(XTX)\n\t## Einsum is slow in pytorch so revert to numpy version\n\th = squareSum(Q) #einsum(\'ij,ij->i\', Q, Q )\n\th_mean = h.mean()\n\t\n\treturn _XTX, h, h_mean\n\n""""""\n------------------------------------------------------------\nSVD_STATS\nUpdated 27/8/2018\n------------------------------------------------------------\n""""""\ndef svd_stats(U, S, VT):\n\t\'\'\'\n\t\t\t\t  1\n\tXTX^-1 =  V ----- VT \n\t\t\t\t S^2\n\t\n\th = diag U * UT\n\t\n\tmean(h) used for normalized leverage\n\t\'\'\'\n\t_S2 = 1.0 / (S**2)\n\tVS = T(VT) * _S2\n\t_XTX = VS.matmul(VT)\n\th = squareSum(U) #einsum(\'ij,ij->i\', U, U )\n\th_mean = h.mean()\n\t\n\treturn _XTX, h, h_mean\n\n\n""""""\n------------------------------------------------------------\nRIDGE_STATS\nUpdated 27/8/2018\n------------------------------------------------------------\n""""""\ndef ridge_stats(U, S, VT, alpha = 1):\n\t\'\'\'\n\t\t\t\t\t\t\t   S^2\n\texp_theta_hat =  diag V --------- VT\n\t\t\t\t\t\t\tS^2 + aI\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t S^2\n\tvar_theta_hat =  diag V ------------- VT\n\t\t\t\t\t\t\t(S^2 + aI)^2\n\t\n\t\t\t\t\t1\n\tXTX^-1 =  V --------- VT\n\t\t\t\tS^2 + aI\n\t\t\t\t\n\t\t\t\t  S^2\n\th = diag U --------- UT\n\t\t\t\tS^2 + aI\n\t\n\tmean(h) used for normalized leverage\n\t\'\'\'\n\tV = T(VT)\n\tS2 = S**2\n\tS2_alpha = S2 + alpha\n\tS2_over_S2 = S2 / S2_alpha\n\t\n\tVS = V * S2_over_S2\n\texp_theta_hat = einsum(\'ij,ji->i\', VS, VT )     # Same as VS.dot(VT)\n\t\n\tV_S2 = VS / S2_alpha  # np_divide(S2,  np_square( S2 + alpha ) )\n\tvar_theta_hat = rowSum(V_S2, V) #einsum(\'ij,ij->i\',  V_S2  , V )   # Sams as np_multiply(   V,   V_S2 ).sum(1)\n\t\n\t_XTX = (V * (1.0 / S2_alpha )  ).matmul( VT )   # V  1/S^2 + a  VT\n\t\n\th = rowSum((U * S2_over_S2), U) #einsum(\'ij,ij->i\', (U * S2_over_S2), U )  # Same as np_multiply(  U*S2_over_S2, U ).sum(1)\n\th_mean = h.mean()\n\t\n\treturn exp_theta_hat, var_theta_hat, _XTX, h_mean\n'"
hyperlearn/utils.py,0,"b'\nfrom numpy import uint, newaxis, finfo, float32, float64, zeros\nfrom .numba import sign, arange\nfrom numba import njit, prange\nfrom psutil import virtual_memory\nfrom .exceptions import FutureExceedsMemory\nfrom scipy.linalg.blas import dsyrk, ssyrk\t\t# For XTX, XXT\nfrom scipy.linalg import lapack as _lapack\nfrom . import numba\nfrom .base import USE_NUMBA\n\n__all__ = [\'lapack\',\'svd_flip\', \'eig_flip\', \'_svdCond\', \'_eighCond\',\n\t\t\t\'memoryXTX\', \'memoryCovariance\', \'memorySVD\', \'_float\',\n\t\t\t\'traceXTX\', \'fastDot\', \'_XTX\', \'_XXT\',\n\t\t\t\'rowSum\', \'rowSum_A\',\'reflect\', \n\t\t\t\'addDiagonal\', \'setDiagonal\']\n\n_condition = {\'f\': 1e3, \'d\': 1e6}\n\n\nclass lapack():\n\t""""""\n\t[Added 11/11/2018] [Edited 13/11/2018 -> made into a class]\n\t[Edited 14/11/2018 -> fixed class]\n\tGet a LAPACK function based on the dtype(X). Acts like Scipy.\n\t""""""\n\tdef __init__(self, function, fast = True, numba = None):\n\t\tself.function = function\n\t\tself.fast = fast\n\t\tself.f = None\n\n\t\tif numba != None and USE_NUMBA:\n\t\t\ttry: f = eval(f\'numba.{function}\')\n\t\t\texcept: pass\n\t\t\tf = eval(f)\n\t\t\tself.f = f\n\n\tdef __repr__(self):\n\t\treturn f""Calls LAPACK or Numba function {self.function}""\n\n\tdef __call__(self, *args, **kwargs):\n\t\tif self.f == None:\n\t\t\tself.f = f""_lapack.d{self.function}""\n\t\t\t\n\t\t\tif len(args) == 0:\n\t\t\t\ta = next(iter(kwargs.values()))\n\t\t\telse:\n\t\t\t\ta = args[0]\n\t\t\t\n\t\t\tif a.dtype == np.float32 and self.fast:\n\t\t\t\tself.f = f""_lapack.s{self.function}""\n\t\t\tself.f = eval(self.f)\n\n\t\treturn self.f(*args, **kwargs)\n\t\t\n\n\ndef svd_flip(U, VT, U_decision = True):\n\t""""""\n\tFlips the signs of U and VT for SVD in order to force deterministic output.\n\n\tFollows Sklearn convention by looking at U\'s maximum in columns\n\tas default.\n\t""""""\n\tif U_decision:\n\t\tmax_abs_cols = abs(U).argmax(0)\n\t\tsigns = sign( U[max_abs_cols, arange(U.shape[1])  ]  )\n\telse:\n\t\t# rows of v, columns of u\n\t\tmax_abs_rows = abs(VT).argmax(1)\n\t\tsigns = sign( VT[  arange(VT.shape[0]) , max_abs_rows] )\n\n\tU *= signs\n\tVT *= signs[:,newaxis]\n\treturn U, VT\n\n\n\ndef eig_flip(V):\n\t""""""\n\tFlips the signs of V for Eigendecomposition in order to \n\tforce deterministic output.\n\n\tFollows Sklearn convention by looking at V\'s maximum in columns\n\tas default. This essentially mirrors svd_flip(U_decision = False)\n\t""""""\n\tmax_abs_rows = abs(V).argmax(0)\n\tsigns = sign( V[max_abs_rows, arange(V.shape[1]) ] )\n\tV *= signs\n\treturn V\n\n\n\ndef _svdCond(U, S, VT, alpha):\n\t""""""\n\tCondition number from Scipy.\n\tcond = 1e-3 / 1e-6  * eps * max(S)\n\t""""""\n\tt = S.dtype.char.lower()\n\tcond = (S > (_condition[t] * finfo(t).eps * S[0]))\n\trank = cond.sum()\n\t\n\tS /= (S**2 + alpha)\n\treturn U[:, :rank], S[:rank], VT[:rank]\n\n\n\ndef _eighCond(S2, V):\n\t""""""\n\tCondition number from Scipy.\n\tcond = 1e-3 / 1e-6  * eps * max(S2)\n\n\tNote that maximum could be either S2[-1] or S2[0]\n\tdepending on it\'s absolute magnitude.\n\t""""""\n\tt = S2.dtype.char.lower()\n\tabsS = abs(S2)\n\tmaximum = absS[0] if absS[0] >= absS[-1] else absS[-1]\n\n\tcond = (absS > (_condition[t] * finfo(t).eps * maximum) )\n\tS2 = S2[cond]\n\t\n\treturn S2, V[:, cond]\n\n\n\ndef memoryXTX(X):\n\t""""""\n\tComputes the memory usage for X.T @ X so that error messages\n\tcan be broadcast without submitting to a memory error.\n\t""""""\n\tfree = virtual_memory().free * 0.95\n\tbyte = 4 if \'32\' in str(X.dtype) else 8\n\tmemUsage = X.shape[1]**2 * byte\n\n\treturn memUsage < free\n\n\n\ndef memoryCovariance(X):\n\t""""""\n\tComputes the memory usage for X.T @ X or X @ X.T so that error messages\n\tcan be broadcast without submitting to a memory error.\n\t""""""\n\tn,p = X.shape\n\tfree = virtual_memory().free * 0.95\n\tbyte = 4 if \'32\' in str(X.dtype) else 8\n\tsize = p if n > p else n\n\t\n\tmemUsage = size**2 * byte\n\n\tif memUsage > free:\n\t\traise FutureExceedsMemory()\n\treturn\n\n\ndef memorySVD(X):\n\t""""""\n\tComputes the approximate memory usage of SVD(X) [transpose or not].\n\tHow it\'s computed:\n\t\tX = U * S * VT\n\t\tU(n,p) * S(p) * VT(p,p)\n\t\tThis means RAM usgae is np+p+p^2 approximately.\n\t### TODO: Divide N Conquer SVD vs old SVD\n\t""""""\n\tn,p = X.shape\n\tif n > p: n,p = p,n\n\tfree = virtual_memory().free * 0.95\n\tbyte = 4 if \'32\' in str(X.dtype) else 8\n\n\tU = n*p\n\tS = p\n\tVT = p*p\n\tmemUsage = (U+S+VT) * byte\n\n\treturn memUsage < free\n\n\n\ndef _float(data):\n\tdtype = str(data.dtype)\n\tif \'f\' not in dtype:\n\t\tif \'64\' in dtype:\n\t\t\treturn data.astype(float64)\n\t\treturn data.astype(float32)\n\treturn data\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef traceXTX(X):\n\t""""""\n\t[Edited 18/10/2018]\n\tOne drawback of truncated algorithms is that they can\'t output the correct\n\tvariance explained ratios, since the full eigenvalue decomp needs to be\n\tdone. However, using linear algebra, trace(XT*X) = sum(eigenvalues).\n\n\tSo, this function outputs the trace(XT*X) efficiently without computing\n\texplicitly XT*X.\n\n\tChanges --> now uses Numba which is approx 20% faster.\n\t""""""\n\tn, p = X.shape\n\ts = 0\n\tfor i in range(n):\n\t\tfor j in range(p):\n\t\t\txij = X[i,j]\n\t\t\ts += xij*xij\n\treturn s\n\n\n\ndef fastDot(A, B, C):\n\t""""""\n\t[Added 23/9/2018]\n\t[Updated 1/10/2018 Error in calculating which is faster]\n\tComputes a fast matrix multiplication of 3 matrices.\n\tEither performs (A @ B) @ C or A @ (B @ C) depending which is more\n\tefficient.\n\t""""""\n\tsize = A.shape\n\tn = size[0]\n\tp = size[1] if len(size) > 1 else 1\n\t\n\tsize = B.shape\n\tk = size[1] if len(size) > 1 else 1\n\t\n\tsize = C.shape\n\td = size[1] if len(size) > 1 else 1\n\t\n\t# Forward (A @ B) @ C\n\t# p*k*n + k*d*n = kn(p+d)\n\tforward = k*n*(p+d)\n\t\n\t# Backward A @ (B @ C)\n\t# p*d*n + k*d*p = pd(n+k)\n\tbackward = p*d*(n+k)\n\t\n\tif forward <= backward:\n\t\treturn (A @ B) @ C\n\treturn A @ (B @ C)\n\n\t\n\ndef _XTX(XT):\n\t""""""\n\t[Added 30/9/2018]\n\tComputes XT @ X much faster than naive XT @ X.\n\tNotice XT @ X is symmetric, hence instead of doing the\n\tfull matrix multiplication XT @ X which takes O(np^2) time,\n\tcompute only the upper triangular which takes slightly\n\tless time and memory.\n\t""""""\n\tif XT.dtype == float64:\n\t\treturn dsyrk(1, XT, trans = 0).T\n\treturn ssyrk(1, XT, trans = 0).T\n\n\n\ndef _XXT(XT):\n\t""""""\n\t[Added 30/9/2018]\n\tComputes X @ XT much faster than naive X @ XT.\n\tNotice X @ XT is symmetric, hence instead of doing the\n\tfull matrix multiplication X @ XT which takes O(pn^2) time,\n\tcompute only the upper triangular which takes slightly\n\tless time and memory.\n\t""""""\n\tif XT.dtype == float64:\n\t\treturn dsyrk(1, XT, trans = 1).T\n\treturn ssyrk(1, XT, trans = 1).T\n\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef rowSum_0(X, norm = False):\n\t""""""\n\t[Added 17/10/2018]\n\tComputes rowSum**2 for dense matrix efficiently, instead of using einsum\n\t""""""\n\tn, p = X.shape\n\tS = zeros(n, dtype = X.dtype)\n\n\tfor i in range(n):\n\t\ts = 0\n\t\tXi = X[i]\n\t\tfor j in range(p):\n\t\t\tXij = Xi[j]\n\t\t\ts += Xij*Xij\n\t\tS[i] = s\n\tif norm:\n\t\tS**=0.5\n\treturn S\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef rowSum_A(X, norm = False):\n\t""""""\n\t[Added 22/10/2018]\n\tComputes rowSum**2 for dense array efficiently, instead of using einsum\n\t""""""\n\tn = len(X)\n\ts = 0\n\tfor i in range(n):\n\t\ts += X[i]**2\n\tif norm:\n\t\ts **= 0.5\n\treturn s\n\n\ndef rowSum(X, norm = False):\n\t""""""\n\t[Added 22/10/2018]\n\tCombines rowSum for matrices and arrays.\n\t""""""\n\tif len(X.shape) > 1:\n\t\treturn rowSum_0(X, norm)\n\treturn rowSum_A(X, norm)\n\n\ndef _reflect(X):\n\t""""""\n\tSee reflect(X, n_jobs = N) documentation.\n\t""""""\n\tn = len(X)\n\tfor i in prange(1, n):\n\t\tXi = X[i]\n\t\tfor j in range(i):\n\t\t\tX[j, i] = Xi[j]\n\treturn X\nreflect_single = njit(_reflect, fastmath = True, nogil = True, cache = True)\nreflect_parallel = njit(_reflect, fastmath = True, nogil = True, parallel = True)\n\n\ndef reflect(X, n_jobs = 1):\n\t""""""\n\t[Added 15/10/2018] [Edited 18/10/2018]\n\tReflects lower triangular of matrix efficiently to upper.\n\tNotice much faster than say X += X.T or naive:\n\t\tfor i in range(n):\n\t\t\tfor j in range(i, n):\n\t\t\t\tX[i,j] = X[j,i]\n\tIn fact, it is much faster to perform vertically:\n\t\tfor i in range(1, n):\n\t\t\tXi = X[i]\n\t\t\tfor j in range(i):\n\t\t\t\tX[j,i] = Xi[j]\n\tThe trick is to notice X[i], which reduces array access.\n\t""""""\n\tX = reflect_parallel(X) if n_jobs != 1 else reflect_single(X)\n\treturn X\n\n\ndef addDiagonal(X, c = 1):\n\t""""""\n\t[Added 11/11/2018]\n\tAdd c to diagonal of matrix\n\t""""""\n\tn = X.shape[0]\n\tX.flat[::n+1] += c\n\ndef setDiagonal(X, c = 1):\n\t""""""\n\t[Added 11/11/2018]\n\tSet c to diagonal of matrix\n\t""""""\n\tn = X.shape[0]\n\tX.flat[::n+1] = c\n\n'"
hyperlearn_new/__init__.py,0,b'\n'
hyperlearn_new/base.py,0,"b'\nfrom .cython.base import process\n""""""\nCython equivalent of Hyperlearn\'s base.process, but this is much faster.\n>>> process(memcheck = {}, square = False, fractional = True)\n[Added 7/1/19]\n\nParameters\n-----------\nmemcheck:       Dictionary of memcheck arguments or 1 string. Used to check\n                whether the matrix X satifies the system\'s memory constraints.\nsquare:         Check whether the matrix X must be square.\nfractional:     (default = True). Whether to convert n_components float to int\n                (eg: 0.5 == 50%*p)\nReturns\n-----------\nWrapped up function - can call like normal functions.\n""""""\n\nfrom .cython.base import lapack\n""""""\nCalls LAPACK functions from Scipy. Use like this: lapack(""LAPACK function"")(""args"")\nFor example: lapack(""gesdd"")(X, full_matrices = False) for SVD. Types are automatically\ndetermined from X, so no need to specify the type of the matrix.\n>>> lapack(function, numba = None, turbo = True)\n""""""\n\nfrom .cython.base import blas\n""""""\nCalls BLAS functions from Scipy. Use like this: blas(""BLAS function"")(""args"")\nFor example: blas(""syrk"")(X) for symmetric matrix multiply. Types are automatically\ndetermined from X, so no need to specify the type of the matrix.\n>>> lapack(function, left = """")\n""""""\n\nfrom .cython.base import isComplex\n""""""\nGiven a numpy datatype, returns if it\'s a complex type.\n""""""\n\nfrom .cython.base import available_memory\n""""""\nReturns the current memory in MB for a system.\n""""""\n\nfrom .cython.base import isList\n\n'"
hyperlearn_new/cfuncs.py,0,"b'\n\nfrom .cython.base import isComplex, available_memory, isList\n\n\nfrom .cython.utils import epsilon\nfrom .cython.utils import svd_lwork, eigh_lwork, dot_left_right\nfrom .cython.utils import uinteger, integer\nfrom .cython.utils import min_ as _min, max_ as _max\n\nfrom .cython.utils import MAXIMUM, RAND\n\n'"
hyperlearn_new/linalg.py,0,"b'\nfrom .base import *\nfrom .utils import *\nimport scipy.linalg as scipy\nfrom .cfuncs import *\nfrom .numba import funcs as numba\n\n\n###\ndef dot(A, B, C, message = False):\n    """"""\n    Implements fast matrix multiplication of 3 matrices X = ABC\n    From left: X = (AB)C. From right: X = A(BC). This function\n    calculates which is faster, and outputs the result.\n    [Added 10/12/18] [Edited 13/12/18 Added left or right statement]\n    [Edited 20/12/18 Uses numba]\n\n    Parameters\n    -----------\n    A:          First matrix\n    B:          Multiplied with 2nd matrix\n    C:          Multiplied with 3rd matrix\n    message:    Default = False. If True, doesn\'t output result, but\n                outputs TRUE if left to right, else FALSE right to left.\n    Returns\n    -----------\n    (A@B@C or message)\n    """"""\n    n, a_b = A.shape    # A and B share sizes. Size of A determines\n                        # final number of rows\n    b_c = B.shape[1]\n    c = C.shape[1]      # final columns\n\n    left, right = dot_left_right(n, a_b, b_c, c)\n\n    if message:\n        return left <= right\n\n    if left <= right:\n        return A @ B @ C\n    return A @ (B @ C)\n\n\n\n###\ndef transpose(X, overwrite = True, dtype = None):\n    """"""\n    Provides X.T if dtype == float, else X.H (Conjugate transpose)\n    [Added 23/11/18]\n\n    Parameters\n    -----------\n    X :         Matrix to be decomposed. Has to be symmetric.\n    Overwrite:  If overwritten, then inplace operation.\n\n    Returns\n    -----------\n    X.T or X.H: Conjugate Tranpose (X)\n    """"""\n    if dtype is None:\n        dtype = X.dtype\n    if isComplex(dtype):\n        if overwrite:\n            return np.conjugate(X, out = X).T\n        return X.conj().T\n    return X.T\n\n\n###\ndef matmul(pattern, X, Y = None):\n    """"""\n    Using BLAS routines GEMM, SYRK, SYMM, multiplies 2 matrices together\n    assuming X has some special structure or the output is special. Supports\n    symmetric constructions: X.H @ X and X @ X.H; symmetric multiplies:\n    S @ Y.H and Y.H @ S where S is a symmetric matrix; general multiplies:\n    X @ Y and X.H @ Y.\n    [Added 28/11/18 Changed from transpose since it didn\'t work]\n    [Edited 1/12/18 Added S @ Y] [Edited 17/12/18 Added Y @ S]\n\n    Parameters\n    -----------\n    pattern:    Can include: X.H @ X | X @ X.H | S @ Y.H | \n                Y.H @ S | X @ Y | X.H @ Y | S @ Y | Y @ S\n    X:          Compulsory left side matrix.\n    Y:          Optional right side matrix.\n\n    Returns\n    -----------\n    out:        Special matrix output according to pattern.\n    """"""\n    pattern = pattern.upper().replace(\' \',\'\')\n    dtypeX = X.dtype\n    XT = X.T\n    n = X.shape[0]\n    isComplex_dtypeX = isComplex(dtypeX)\n    \n    if pattern == ""X.H@X"":\n        if isComplex_dtypeX:\n            # BLAS SYRK doesn\'t work\n            out = blas(""gemm"")(a = XT, b = XT, trans_a = 0, trans_b = 2, alpha = 1)\n            out = np.conjugate(out, out = out)\n        else:\n            # Use BLAS SYRK\n            out = blas(""syrk"")(a = XT, trans = 0, alpha = 1)\n\n    elif pattern == ""X@X.H"":\n        if isComplex_dtypeX:\n            # BLAS SYRK doesn\'t work\n            out = blas(""gemm"")(a = XT, b = XT, trans_a = 2, trans_b = 0, alpha = 1)\n            out = np.conjugate(out, out = out)\n        else:\n            # Use BLAS SYRK\n            out = blas(""syrk"")(a = XT, trans = 1, alpha = 1)\n\n    elif pattern == ""X.H@Y"":\n        if isComplex_dtypeX:\n            dtypeY = Y.dtype\n            if isComplex_dtypeY:\n                out = blas(""gemm"")(a = XT, b = Y.T, trans_a = 0, trans_b = 2, alpha = 1)\n            else:\n                out = XT @ Y\n            out = np.conjugate(out, out = out)\n        else:\n            out = XT @ Y\n\n    elif pattern == ""X@Y"":\n        out = X @ Y\n        \n    # Symmetric Multiply\n    # If it\'s F Contiguous, I assume it\'s UPPER. If not, it\'s transposed.\n    elif pattern == ""S@Y.H"":\n        dtypeY = Y.dtype\n        isComplex_dtypeY = isComplex(dtypeY)\n\n        if isComplex_dtypeY:\n            a = X if X.flags[""F_CONTIGUOUS""] else XT\n            # Symmetric doesn\'t work\n            out = blas(""gemm"")(a = a, b = Y, trans_b = 2, alpha = 1)\n        else:\n            YT = Y.T\n            if X.flags[""F_CONTIGUOUS""]:\n                out = blas(""symm"")(a = X, b = YT, side = 0, alpha = 1)\n            else:\n                out = blas(""symm"")(a = XT, b = YT, side = 0, alpha = 1, lower = 1)\n\n    elif pattern == ""Y.H@S"":\n        dtypeY = Y.dtype\n\n        if isComplex_dtypeY:\n            a = X if X.flags[""F_CONTIGUOUS""] else XT\n            # Symmetric doesn\'t work\n            out = blas(""gemm"")(a = Y, b = a, trans_a = 2, alpha = 1)\n        else:\n            YT = Y.T\n            if X.flags[""F_CONTIGUOUS""]:\n                out = blas(""symm"")(a = X, b = YT, side = 1, alpha = 1)\n            else:\n                out = blas(""symm"")(a = XT, b = YT, side = 1, alpha = 1, lower = 1)\n\n    elif pattern == ""Y@S"":\n        if X.flags[""F_CONTIGUOUS""]:\n            out = blas(""symm"")(a = X, b = Y, side = 1, alpha = 1)\n        else:\n            out = blas(""symm"")(a = XT, b = Y, side = 1, alpha = 1, lower = 1)\n\n    elif pattern == ""S@Y"":\n        if X.flags[""F_CONTIGUOUS""]:\n            out = blas(""symm"")(a = X, b = Y, side = 0, alpha = 1)\n        else:\n            out = blas(""symm"")(a = XT, b = Y, side = 0, alpha = 1, lower = 1)\n\n    else:\n        raise NameError(f""Pattern = {pattern} is not recognised."")\n    return out\n\n\n###\n@process(square = True, memcheck = ""columns"")\ndef cholesky(X, alpha = None, overwrite = False):\n    """"""\n    Uses Epsilon Jitter Solver to compute the Cholesky Decomposition\n    until success. Default alpha ridge regularization = 1e-6.\n    [Added 15/11/18] [Edited 16/11/18 Numpy is slower. Uses LAPACK only]\n    [Edited 18/11/18 Uses universal ""do_until_success""]\n\n    Parameters\n    -----------\n    X :         Matrix to be decomposed. Has to be symmetric.\n    alpha :     Ridge alpha regularization parameter. Default 1e-6\n    overwrite:  Whether to inplace change data.\n\n    Returns\n    -----------\n    U :         Upper triangular cholesky factor (U)\n    """"""\n    decomp = lapack(""potrf"")\n    U = do_until_success(\n        decomp, add_jitter, X.shape[0], overwrite, alpha, X\n        )\n    return U\n\n\n###\n@process(square = True, memcheck = ""columns"")\ndef cho_solve(X, rhs, alpha = None):\n    """"""\n    Given U from a cholesky decompostion and a RHS, find a least squares\n    solution.\n    [Added 15/11/18]\n\n    Parameters\n    -----------\n    X :         Cholesky Factor. Use cholesky first.\n    alpha :     Ridge alpha regularization parameter. Default 1e-6\n    turbo :     Boolean to use float32, rather than more accurate float64.\n\n    Returns\n    -----------\n    U :          Upper triangular cholesky factor (U)\n    """"""\n    theta = lapack(""potrs"")(X, rhs)[0]\n    return theta\n\n\n###\n@process(square = True, memcheck = ""squared"")\ndef cho_inv(X, turbo = True):\n    """"""\n    Computes an inverse to the Cholesky Decomposition.\n    [Added 17/11/18]\n\n    Parameters\n    -----------\n    X :         Upper Triangular Cholesky Factor U. Use cholesky first.\n    turbo :     Boolean to use float32, rather than more accurate float64.\n    \n    Returns\n    -----------\n    inv(U) :     Upper Triangular Inverse(X)\n    """"""\n    inv = lapack(""potri"", turbo)(X)\n    return inv[0]\n\n\n###\n@process(memcheck = ""full"")\ndef pinvc(X, alpha = None, turbo = True, overwrite = False):\n    """"""\n    Returns the Pseudoinverse of the matrix X using Cholesky Decomposition.\n    Fastest pinv(X) possible, and uses the Epsilon Jitter Algorithm to\n    guarantee convergence. Allows Ridge Regularization - default 1e-6.\n    [Added 17/11/18] [Edited 18/11/18 for speed - uses more BLAS]\n    [Edited 23/11/18 Added Complex support]\n\n    Parameters\n    -----------\n    X :         General matrix X.\n    alpha :     Ridge alpha regularization parameter. Default 1e-6\n    turbo :     Boolean to use float32, rather than more accurate float64.\n    overwrite:  Whether to overwrite intermmediate results. Will cause\n                alpha to be increased by a factor of 10.\n\n    Returns\n    -----------    \n    pinv(X) :   Pseudoinverse of X. Allows pinv(X) @ X = I if n >= p or X\n                @ pinv(X) = I for p > n.\n    """"""\n    n, p = X.shape\n    # determine under / over-determined\n    XTX = n > p\n    dtype = X.dtype\n\n    # get covariance or gram matrix\n    U = matmul(""X.H @ X"", X) if XTX else matmul(""X @ X.H"", X)\n\n    decomp = lapack(""potrf"")\n    U = do_until_success(\n        decomp, add_jitter, _min(n,p), overwrite, alpha, U\n        )\n    U = lapack(""potri"", turbo)(U, overwrite_c = True)[0]\n\n    # if XXT -> XT * (XXT)^-1\n    # if XTX -> (XTX)^-1 * XT\n    inv = matmul(""S @ Y.H"", U, X) if XTX else matmul(""Y.H @ S"", U, X)\n    return inv\n\n\n###\n_reflect = reflect\n@process(square = True, memcheck = ""full"")\ndef pinvch(X, alpha = None, turbo = True, overwrite = False, reflect = True):\n    """"""\n    Returns the inverse of a square Hermitian Matrix using Cholesky \n    Decomposition. Uses the Epsilon Jitter Algorithm to guarantee convergence. \n    Allows Ridge Regularization - default 1e-6.\n    [Added 19/11/18]\n\n    Parameters\n    -----------\n    X :         Upper Symmetric Matrix X\n    alpha :     Ridge alpha regularization parameter. Default 1e-6\n    turbo :     Boolean to use float32, rather than more accurate float64.\n    overwrite:  Whether to overwrite X inplace with pinvh.\n    reflect:    Output full matrix or 1/2 triangular\n\n    Returns\n    -----------    \n    pinv(X) :   Pseudoinverse of X. Allows pinv(X) @ X = I.\n    """"""\n    decomp = lapack(""potrf"")\n    U = do_until_success(\n        decomp, add_jitter, X.shape[0], overwrite, alpha, X\n        )\n    U = lapack(""potri"", turbo)(U, overwrite_c = True)[0]\n\n    return _reflect(U) if reflect else U\n\n\n###\n@process(memcheck = {""X"":""full"", ""L_only"":""same"", ""U_only"":""same""})\ndef lu(X, L_only = False, U_only = False, overwrite = False):\n    """"""\n    Computes the pivoted LU decomposition of a matrix. Optional to output\n    only L or U components with minimal memory copying.\n    [Added 16/11/18]\n\n    Parameters\n    -----------\n    X:          Matrix to be decomposed. Can be retangular.\n    L_only:     Output only L.\n    U_only:     Output only U.\n    overwrite:  Whether to directly alter the original matrix.\n\n    Returns\n    -----------    \n    (L,U) or (L) or (U)\n    """"""\n    n, p = X.shape\n    if L_only or U_only:\n        A, P, _ = lapack(""getrf"")(X, overwrite_a = overwrite)\n        if L_only:\n            A, k = L_process(n, p, A)\n            # inc = -1 means reverse order pivoting\n            A = lapack(""laswp"")(a = A, piv = P, inc = -1, k1 = 0, k2 = k-1, overwrite_a = True)\n        else:\n            # get only upper triangle\n            A = triu(n, p, A)\n        return A\n    else:\n        return scipy.lu(X, permute_l = True, check_finite = False, overwrite_a = overwrite)\n\n\n###\n@process(square = True, memcheck = ""same"")\ndef pinvl(X, alpha = None, turbo = True, overwrite = False):\n    """"""\n    Computes the pseudoinverse of a square matrix X using LU Decomposition.\n    Notice, it\'s much faster to use pinvc (Choleksy Inverse).\n    [Added 18/11/18] [Edited 26/11/18 Fixed ridge regularization]\n\n    Parameters\n    -----------\n    X:          Matrix to be decomposed. Must be square.\n    alpha:      Ridge alpha regularization parameter. Default 1e-6\n    turbo:      Boolean to use float32, rather than more accurate float64.\n    overwrite:  Whether to directly alter the original matrix.\n\n    Returns\n    -----------    \n    pinv(X):    Pseudoinverse of X. Allows pinv(X) @ X = I = X @ pinv(X) \n    """"""\n    n, p = X.shape\n    A, P, _ = lapack(""getrf"")(X, overwrite_a = overwrite)\n\n    inv = lapack(""getri"")\n    A = do_until_success(\n        inv, U_process, _min(n,p), True, alpha, lu = A, piv = P, overwrite_lu = True\n        ) \n    # overwrite shouldnt matter in first go\n    return A\n\n\n###\n@process(memcheck = {""X"":""full"", ""Q_only"":""same"", ""R_only"":""same""})\ndef qr(X, Q_only = False, R_only = False, overwrite = False):\n    """"""\n    Computes the reduced economic QR Decomposition of a matrix. Optional\n    to output only Q or R.\n    [Added 16/11/18] [Edited 28/11/18 Complex support]\n\n    Parameters\n    -----------\n    X:          Matrix to be decomposed. Can be retangular.\n    Q_only:     Output only Q.\n    R_only:     Output only R.\n    overwrite:  Whether to directly alter the original matrix.\n\n    Returns\n    -----------    \n    (Q,R) or (Q) or (R)\n    """"""\n    dtype = X.dtype\n\n    if Q_only or R_only:\n        n, p = X.shape\n        R, tau, _, _ = lapack(""geqrf"")(X, overwrite_a = overwrite)\n\n        if Q_only:\n            if p > n:\n                R = R[:, :n]\n            # Compute Q\n            if isComplex(dtype):\n                Q, _, _ = lapack(""ungqr"")(R, tau, overwrite_a = True)\n            else:\n                Q, _, _ = lapack(""orgqr"")(R, tau, overwrite_a = True)\n            return Q\n        else:\n            # get only upper triangle\n            R = triu(n, p, R)\n            return R\n\n    return numba.qr(X)\n\n\n###\n@process(memcheck = ""extended"")\ndef svd(X, U_decision = False, n_jobs = 1, conjugate = True, overwrite = False):\n    """"""\n    Computes the Singular Value Decomposition of a general matrix providing\n    X = U S VT. Notice VT (V transpose) is returned, and not V.\n    Also, by default, the signs of U and VT are swapped so that VT has the\n    sign of the maximum item as positive.\n\n    HyperLearn\'s SVD is optimized dramatically due to the findings made in\n    Modern Big Data Algorithms. If p/n >= 0.001, then GESDD is used. Else,\n    GESVD is used. Also, svd(XT) is used if it\'s faster, bringing the complexity\n    to O( min(np^2, n^2p) ).\n    [Added 19/11/18] [Edited 23/11/18 Added Complex support]\n    \n    Parameters\n    -----------\n    X:          Matrix to be decomposed. General matrix.\n    U_decision: Default = False. If True, uses max from U. If None. don\'t flip.\n    n_jobs:     Whether to use more >= 1 CPU\n    conjugate:  Whether to inplace conjugate but inplace return original.\n    overwrite:  Whether to conjugate transpose inplace.\n\n    Returns\n    -----------    \n    U:          Orthogonal Left Eigenvectors\n    S:          Descending Singular Values\n    VT:         Orthogonal Right Eigenvectors\n    """"""\n    n, p = X.shape\n    dtype = X.dtype\n    ifTranspose = p > n # p > n\n    isComplex_dtype = isComplex(dtype)\n\n    if ifTranspose: \n        X = transpose(X, conjugate, dtype)\n        U_decision = not U_decision\n        n, p = X.shape\n    byte = X.itemsize\n\n    gesdd, gesvd = svd_lwork(isComplex_dtype, byte, n, p)\n    free = available_memory()\n    if gesdd > free:\n        if gesvd > free:\n            raise MemoryError(f""GESVD requires {gesvd} MB, but {free} MB is free, ""\n    f""so an extra {gesvd-free} MB is required."")\n        gesdd = False\n    else:\n        gesdd = True\n\n    # Use GESDD from Numba or GESVD from LAPACK\n    ratio = p/n\n    # From Modern Big Data Algorithms -> GESVD better if matrix is very skinny\n    if ratio >= 0.001:\n        if overwrite:\n            U, S, VT, _ = lapack(""gesdd"")(X, full_matrices = False, overwrite_a = overwrite)\n        else:\n            U, S, VT = numba.svd(X, full_matrices = False)\n    else:\n        U, S, VT, _ = lapack(""gesvd"")(X, full_matrices = False, overwrite_a  = overwrite)\n        \n    # Return original X if X.H\n    if not overwrite and conjugate and isComplex_dtype:\n        transpose(X, True, dtype);\n    \n    # Flip if svd(X.T) was performed.\n    if ifTranspose:\n        U, VT = transpose(VT, True, dtype), transpose(U, True, dtype)\n\n    # In place flips sign according to max(abs(VT))\n    svd_flip(U, VT, U_decision = U_decision, n_jobs = n_jobs)\n\n    return U, S, VT\n\n\n###\n@process(memcheck = ""extended"")\ndef pinv(X, alpha = None, overwrite = False):\n    """"""\n    Returns the inverse of a general Matrix using SVD. Uses the Epsilon Jitter \n    Algorithm to guarantee convergence. Allows Ridge Regularization - default 1e-6.\n    [Added 21/11/18] [Edited 23/11/18 Added Complex support]\n\n    Parameters\n    -----------\n    X:          Upper Triangular Cholesky Factor U. Use cholesky.\n    alpha:      Ridge alpha regularization parameter. Default 1e-6.\n    overwrite:  Whether to directly alter the original matrix.\n\n    Returns\n    -----------    \n    pinv(X):    Pseudoinverse of X. Allows pinv(X) @ X = I.\n    """"""\n    dtype = X.dtype\n    U, S, VT = svd(X, U_decision = None, overwrite = overwrite)\n    U, _S, VT = svd_condition(U, S, VT, alpha)\n    return (transpose(VT, True, dtype) * _S) @ transpose(U, True, dtype)\n\n\n###\n@process(square = True, memcheck = ""extra"")\ndef eigh(X, U_decision = False, alpha = None, svd = False, n_jobs = 1, overwrite = False):\n    """"""\n    Returns sorted eigenvalues and eigenvectors from large to small of\n    a symmetric square matrix X. Follows SVD convention. Also flips\n    signs of eigenvectors using svd_flip. Uses the Epsilon Jitter \n    Algorithm to guarantee convergence. Allows Ridge Regularization\n    default 1e-6.\n    [Added 21/11/18] [Edited 24/11/18 Added Complex Support, Eigh alpha\n    set to 0 since Eigh errors are rare.]\n\n    Parameters\n    -----------\n    X:          Symmetric Square Matrix.\n    U_decision: Always set to False. Can choose None for no swapping.\n    alpha:      Ridge alpha regularization parameter. Default 1e-6.\n    svd:        Returns sqrt(W) and V.T\n    n_jobs:     Whether to use more >= 1 CPU\n    overwrite:  Whether to directly alter the original matrix.\n\n    Returns\n    -----------    \n    W:          Eigenvalues\n    V:          Eigenvectors\n    """"""\n    n = X.shape[0]\n    byte = X.itemsize\n    dtype = X.dtype\n    isComplex_dtype = isComplex(dtype)\n\n    evd, evr = eigh_lwork(isComplex_dtype, byte, n, n)\n   \n    free = available_memory()\n    if evd > free:\n        if evr > free:\n            raise MemoryError(f""SYEVR requires {evr} MB, but {free} MB is free, ""\n    f""so an extra {evr-free} MB is required."")\n        evd = False\n    else:\n        evd = True\n    \n    # From Modern Big Data Algorithms: SYEVD mostly faster than SYEVR\n    # contradicts MKL\'s findings\n    if evd:\n        decomp = lapack(""heevd"") if isComplex_dtype else lapack(""syevd"")\n        W, V = do_until_success(\n            decomp, add_jitter, n, overwrite, None, \n            a = X, lower = 0, overwrite_a = overwrite)\n    else:\n        decomp = lapack(""heevr"") if isComplex_dtype else lapack(""syevr"")\n        W, V = do_until_success(\n            decomp, add_jitter, n, overwrite, None, \n            a = X, uplo = ""U"", overwrite_a = overwrite)\n\n    # if svd -> return V.T and sqrt(S)\n    if svd:\n        W = eig_search(W, 0)\n\n    if U_decision is not None:\n        W, V = W[::-1], V[:,::-1]\n\n    if svd:\n        W **= 0.5\n        V = transpose(V, True, dtype)\n\n    # return with SVD convention: sort eigenvalues\n    svd_flip(None, V, U_decision = U_decision, n_jobs = n_jobs)\n\n    return W, V\n\n\n###\n_svd = svd\n@process(memcheck = ""extra"")\ndef eig(\n    X, U_decision = False, alpha = None, turbo = True, svd = False, \n    n_jobs = 1, conjugate = True, overwrite = False, use_svd = False):\n    """"""\n    Returns sorted eigenvalues and eigenvectors from large to small of\n    a general matrix X. Follows SVD convention. Also flips signs of \n    eigenvectors using svd_flip. Uses the Epsilon Jitter \n    Algorithm to guarantee convergence. Allows Ridge Regularization\n    default 1e-6.\n\n    According to [`Matrix Computations, Third Edition, G. Holub and C. \n    Van Loan, Chapter 5, section 5.4.4, pp 252-253.`], QR is better if\n    n >= 5/3p. In Modern Big Data Algorithms, I find QR is better for\n    all n > p.\n    [Added 21/11/18] [Edited 22/11/18 with turbo -> approximate\n    eigendecomposition when p >> n] [Edited 24/11/18 Added Complex Support]\n\n    Parameters\n    -----------\n    X:          General Matrix.\n    U_decision: Always set to False. Can choose None for no swapping.\n    alpha:      Ridge alpha regularization parameter. Default 1e-6.\n    turbo:      If True, if p >> n, then will output approximate eigenvectors\n                where V = (X.T @ U) / sqrt(W)\n    svd:        Returns sqrt(W) and V.T\n    n_jobs:     Whether to use more >= 1 CPU\n    conjugate:  Whether to inplace conjugate but inplace return original.\n    overwrite:  Whether to conjugate transpose inplace.\n    use_svd:    Use SVD instead of EIGH (slower, but more robust)\n\n    Returns\n    -----------\n    W:          Eigenvalues\n    V:          Eigenvectors\n    """"""\n    n, p = X.shape\n    byte = X.itemsize\n    dtype = X.dtype\n    isComplex_dtype = isComplex(dtype)\n\n    # check memory usage\n    free = available_memory()\n    evd, evr = eigh_lwork(isComplex_dtype, byte, n, p)\n    eigh_work = _min(evd, evr)\n\n    if eigh_work > free:\n        use_svd = True\n        # check SVD since less memory usage\n        # notice since QR used, upper triangular\n        MIN = _min(n, p)\n        gesdd, gesvd = svd_lwork(isComplex_dtype, byte, MIN, p)\n        gesddT, gesvdT = svd_lwork(isComplex_dtype, byte, p, MIN) # also check transpose\n        svd_work = min(gesdd, gesvd, eigh_work, gesddT, gesvdT)\n\n        if svd_work > free:\n            raise MemoryError(f""EIG requires {svd_work} MB, but {free} MB is free, ""\n    f""so an extra {svd_work-free} MB is required."")\n\n    if not use_svd:\n        # From Modern Big Data Algorithms for p >= 1.1n\n        if turbo and p >= 1.1*n:\n            # Form XXT\n            cov = matmul(""X @ X.H"", X)\n            W, V = eigh(cov, U_decision = None, overwrite = True) # overwrite doesn\'t matter\n\n            W, V = eig_condition(X, W, V)\n        else:\n        # Form XTX\n            cov = matmul(""X.H @ X"", X)\n            W, V = eigh(cov, U_decision = None, overwrite = True)\n        W, V = W[::-1], V[:,::-1]\n        \n    else:\n        _, W, V = _svd( qr(X, R_only = True), U_decision = None, overwrite = True)\n        if svd:\n            return W, V\n        W **= 2\n        V = transpose(V, True, dtype)\n\n    # revert matrix X back\n    if not overwrite and conjugate and isComplex(dtype):\n        transpose(X, True, dtype);\n\n    # if svd -> return V.T and sqrt(S)\n    if svd:\n        W = eig_search(W, 0)\n\n        W **= 0.5\n        V = transpose(V, True, dtype)\n\n    # return with SVD convention: flip signs\n    svd_flip(None, V, U_decision = U_decision, n_jobs = n_jobs)\n\n    return W, V\n\n\n###\n@process(square = True, memcheck = ""full"")\ndef pinvh(X, alpha = None, turbo = True, overwrite = False, reflect = True):\n    """"""\n    Returns the inverse of a square Hermitian Matrix using Eigendecomposition. \n    Uses the Epsilon Jitter Algorithm to guarantee convergence. \n    Allows Ridge Regularization - default 1e-6.\n    [Added 19/11/18]\n\n    Parameters\n    -----------\n    X :         Upper Symmetric Matrix X\n    alpha :     Ridge alpha regularization parameter. Default 1e-6\n    turbo :     Boolean to use float32, rather than more accurate float64.\n    overwrite:  Whether to overwrite X inplace with pinvh.\n    reflect:    Output full matrix or 1/2 triangular\n\n    Returns\n    -----------    \n    pinv(X) :   Pseudoinverse of X. Allows pinv(X) @ X = I.\n    """"""\n    W, V = eigh(X, U_decision = None, alpha = alpha, overwrite = overwrite)\n    \n    eps = epsilon(W)\n    above_cutoff = eigh_search(W, eps)\n    _W = 1.0 / W[above_cutoff]\n    V = V[:, above_cutoff]\n\n    inv = V * _W @ transpose(V)\n    return inv\n\n'"
hyperlearn_new/random.py,0,"b'\nfrom .numba.types import *\nimport numpy as np\nfrom .cfuncs import isComplex, isList, uinteger\n\n###\n@jit(**nogil)\ndef cov(size = 100, dtype = np.float32):\n\tout = np.zeros((size,size), dtype = dtype)\n\tdiag = np.random.randint(1, size**2, size = size)\n\t\n\tfor i in range(size):\n\t\tout[i, i] = diag[i]\n\n\tfor i in range(size-1):\n\t\tfor j in range(i+1,size):\n\t\t\trand = np.random.uniform(-size**2, size**2)\n\t\t\tout[i, j] = rand\n\t\t\tout[j, i] = rand\n\treturn out\n\n\n######\ndef random(left, right, shape = 1, dtype = np.float32, size = None):\n    """"""\n    Produces pseudo-random numbers between left and right range.\n    Notice much more memory efficient than Numpy, as provides\n    a DTYPE argument (float32 supported).\n    [Added 24/11/2018] [Edited 26/11/18 Inplace operations more\n    memory efficient] [Edited 28/11/18 Supports complex entries]\n\n    Parameters\n    -----------\n    left:       Lower bound\n    right:      Upper bound\n    shape:      Final shape of random matrix\n    dtype:      Data type: supports any type\n\n    Returns\n    -----------\n    Random Vector or Matrix\n    """"""\n    if size is not None:\n        shape = size\n    if type(shape) is int:\n        # Only 1 long vector --> easily made.\n        return uniform_vector(left, right, shape, dtype)\n    if len(shape) == 1:\n        return uniform_vector(left, right, shape[0], dtype)\n    n, p = shape\n\n    l, r = left/2, right/2\n\n    part = uniform_vector(l, r, p, dtype)\n    X = np.tile(part, (n, 1))\n    add = uniform_vector(l, r, n, dtype)[:, np.newaxis]\n    mult = uniform_vector(0, 1, p, dtype)\n    X *= mult\n    X += add\n\n    # If datatype is complex, produce complex entries as well\n    # if isComplex(dtype):\n    #   add = add.real\n    #   add = add*1j\n    #   segment = n // 4\n    #   for i in range(4):\n    #       np.random.shuffle(add)\n    #       left = i*segment\n    #       right = (i+1)*segment\n    #       X[left:right] += add[left:right]\n    return X\n\n\n######\n@jit(**nogil)\ndef uniform_vector(l, r, size, dtype):\n    zero = np.zeros(size, dtype = dtype)\n    for j in range(size):\n        zero[j] = np.random.uniform(l, r)\n    return zero\n\n######\n@jit(**nogil)\ndef shuffle(x):\n    np.random.shuffle(x)\n\n######\n@jit(bool_[:,::1](I64, I64), **nogil)\ndef boolean(n, p):\n    out = np.zeros((n,p), dtype = np.bool_)\n\n    cols = np.zeros(p, dtype = np.uint32)\n    for i in range(p):\n        cols[i] = i\n\n    half = p // 2\n    quarter = n // 4\n\n    for i in range(n):\n        if i % quarter == 0:\n            np.random.shuffle(cols)\n\n        l = np.random.randint(half)\n        r = l + half\n\n        if i % 3 == 0:\n            for j in range(l, r):\n                out[i, cols[j]] = True\n        else:\n            for j in range(r, l, -1):\n                out[i, cols[j]] = True\n    return out\n\n\n######\ndef randbool(size = 1):\n    """"""\n    Produces pseudo-random boolean numbers.\n    [Added 22/12/18]\n\n    Parameters\n    -----------\n    size:       Default = 1. Can be 1D or 2D shape.\n\n    Returns\n    -----------\n    Random Vector or Matrix\n    """"""\n    if isList(size):\n        n, p = size\n        out = boolean(*size)\n    else:\n        out = np.random.randint(0, 2, size, dtype = bool)\n    return out\n\n\n######\n@jit(**nogil)\ndef _choice_p(a, p, size = 1, replace = True):\n    """"""\n    Deterministic selection of items in an array according\n    to some probabilities p.\n    [Added 23/12/18]\n    """"""\n    n = a.size\n    sort = np.argsort(p) # Sort probabilities, making more probable\n                        # to appear first.\n    if replace:\n        counts = np.zeros(n, dtype = np.uint32)\n\n        # Get only top items that add to size\n        seen = 0\n        for i in range(n-1, 0, -1):\n            j = sort[i]\n            prob = np.ceil(p[j] * size)\n            counts[j] = prob\n            seen += prob\n            if seen >= size:\n                break\n\n        have = 0\n        j = n-1\n        out = np.zeros(size, dtype = a.dtype)\n\n        while have < size:\n            where = sort[j]\n            amount = counts[where]\n            out[have:have+amount] = a[where]\n            have += amount\n            j -= 1 \n    else:\n        length = size if size < n else n\n        out = a[sort[:length]]\n    return out\n\n\n######\n@jit(**nogil)\ndef _choice_a(a, size = 1, replace = True):\n    return np.random.choice(a, size = size, replace = replace)\n\n\n######\ndef choice(a, size = 1, replace = True, p = None):\n    """"""\n    Selects elements from an array a pseudo randomnly based on\n    possible probabilities p or just uniformally.\n    [Added 23/12/18]\n\n    Parameters\n    -----------\n    a:          Array input\n    size:       How many elements to randomnly get\n    replace:    Whether to get elements with replacement\n    p:          Probability distribution or just uniform\n\n    Returns\n    -----------\n    Vector of randomnly chosen elements\n    """"""\n    if p is not None:\n        out = _choice_p(a, p, size, replace)\n    else:\n        out = _choice_a(a, size, replace)\n    return out\n\n\n######\ndef randint(low, high = None, size = 1):\n    dtype = uinteger(size)\n    return np.random.randint(low, high, size = size, dtype = dtype)\n\n\n######\nfrom .cfuncs import MAXIMUM, RAND\n\ncycle = MAXIMUM()\nZmultf = 2.0 / cycle\nmult1 = 2**0.5\nmult2 = 2.3\n\n######\n@jit([(A32, I64, I64, U32), (A64, I64, I64, U32)], **nogil)\ndef normal_1(out, div, diff, x):\n    j = 0\n    normal = 2.0\n    while normal >= 1.0:\n        x = (214013 * x + 2531011) & cycle;  unif_1 = x*Zmultf - 1\n        x = (214013 * x + 2531011) & cycle;  unif_2 = x*Zmultf - 1\n        normal = unif_1*unif_1 + unif_2*unif_2\n    temp1 = unif_1*mult1*(1/(normal + 0.02) - 0.1);  temp2 = unif_2*2.3\n\n    for a in range(div):\n        out[j] = -temp1*.4;   j += 1;  out[j] = temp2;    j += 1;     # reflection\n        out[j] = -temp2*.75;  j += 1;  out[j] = temp1;    j += 1;\n        out[j] = -temp1*.75;  j += 1;  out[j] = temp2*.4; j += 1;\n        \n        normal = 2.0\n        while normal >= 1.0:\n            x = (214013 * x + 2531011) & cycle;  unif_1 = x*Zmultf - 1\n            x = (214013 * x + 2531011) & cycle;  unif_2 = x*Zmultf - 1\n            normal = unif_1*unif_1 + unif_2*unif_2\n        temp1 = unif_1*mult1*(1/(normal + 0.02) - 0.1);  temp2 = unif_2*2.3\n    if diff >= 1:   out[j] = temp1*.2;   j += 1;\n    if diff >= 2:   out[j] = -temp2*.2;  j += 1;\n    if diff >= 3:   out[j] = temp1*.7;   j += 1;\n    if diff >= 4:   out[j] = -temp2*.7;  j += 1;\n    if diff >= 5:   out[j] = (temp1+temp2)/2\n\n        \n######\n@jit(M32_(I64, I64, I64), **gil)\ndef normal_32_(n, p, seed):\n    x = uint32(seed)\n    size = n*p\n    out = np.zeros(size, dtype = np.float32)\n\n    diff = size % 6\n    div = size // 6\n    normal_1(out, div, diff, x)\n    return out.reshape((n, p))\n\n        \n######\n@jit(M32_(I64, I64, I64), **parallel)\ndef normal_parallel_32(n, p, seed):\n    x = uint32(seed)\n    out = np.zeros((n,p), dtype = np.float32)\n    div = p // 6\n    diff = p % 6\n    \n    for i in prange(n):  normal_1(out[i], div, diff, x+i)\n    return out\n\n\n######\n@jit(M32_(I64, I64, I64), **gil)\ndef normal_32(n, p, seed):\n    if n*p > 50000:\n        return normal_parallel_32(n, p, seed)\n    else:\n        return normal_32_(n, p, seed)\n        \n        \n######\n@jit(M64_(I64, I64, I64), **gil)\ndef normal_64_(n, p, seed):\n    x = uint32(seed)\n    size = n*p\n    out = np.zeros(size, dtype = np.float64)\n\n    diff = size % 6\n    div = size // 6\n    normal_1(out, div, diff, x)\n    return out.reshape((n, p))\n\n        \n######\n@jit(M64_(I64, I64, I64), **parallel)\ndef normal_parallel_64(n, p, seed):\n    x = uint32(seed)\n    out = np.zeros((n,p), dtype = np.float64)\n    div = p // 6\n    diff = p % 6\n    \n    for i in prange(n):  normal_1(out[i], div, diff, x+i)\n    return out\n\n\n######\n@jit(M64_(I64, I64, I64), **gil)\ndef normal_64(n, p, seed):\n    if n*p > 50000:\n        return normal_parallel_64(n, p, seed)\n    else:\n        return normal_64_(n, p, seed)\n\n\n######\ndef normal(mean = 0, std = 1, size = (100,3), dtype = np.float32, random_state = -1):\n    n, p = size\n    if dtype == np.float32:\n        return normal_32(n, p, RAND(random_state))\n    else:\n        return normal_64(n, p, RAND(random_state))\n\n'"
hyperlearn_new/setup.py,0,"b'\n# python setup.py build_ext --inplace\nfrom distutils.core import setup\nfrom Cython.Build import cythonize\nfrom numpy import get_include\nfrom Cython.Compiler import Options\nimport os\nos.environ[\'CFLAGS\'] = \'-O3 -march=native\'\nos.environ[\'CXXFLAGS\'] = \'-O3 -march=native\'\nos.environ[\'CL\'] = \'/arch:AVX /arch:AVX2 /arch:SSE2 /arch:SSE /arch:ARMv7VE /arch:VFPv4\'\n\nOptions.docstrings = True\nOptions.generate_cleanup_code = True\n\nsetup(\n    ext_modules = cythonize(""*.pyx"",\n        compiler_directives = {\n            \'language_level\':3, \n            \'boundscheck\':False, \n            \'wraparound\':False,\n            \'initializedcheck\':False, \n            \'cdivision\':True,\n            \'nonecheck\':False,\n        },\n        quiet = True,\n        force = True,\n    ),\n    include_dirs = [get_include()],\n)\n'"
hyperlearn_new/stats.py,0,"b'\nfrom .numba.funcs import mean, maximum\nfrom .numba.types import *\nfrom .utils import reflect as _reflect, col_norm\nimport numpy as np\nfrom . import linalg\n\n\ndef corr(X, y, reflect = False):\n    """"""\n    Provides the Pearson Correlation between X and y. y can be X,\n    and if that\'s the case, will use faster methods. Much faster\n    than Scipy\'s version, as HyperLearn is parallelised.\n    [Added 8/12/18]\n\n    Parameters\n    -----------\n    X:              Correlate y with this\n    y:              The main matrix or vector you want to correlate with\n    reflect:        Whether to output full correlations, or 1/2 filled\n                    upper triangular matrix. Default = False.\n    Returns\n    -----------\n    C:              Correlation matrix.\n    """"""\n    same = y is X\n    \n    _X = X - mean(X, 0)\n    _y = y - mean(y, 0) if not same else _X\n    \n    # Sometimes norm can be = 0\n    _X2 = maximum( col_norm(_X)**0.5, 1)\n    \n    if len(y.shape) == 1:\n        _y2 = col_norm(_y)**0.5\n        if _y2 == 0:\n            _y2 = 1\n    else:\n        _y2 = col_norm(_y)**0.5 if not same else _X2\n        _y2 = maximum(_y2, 1)\n        _X2 = _X2[:,np.newaxis]\n        \n    if same:\n        C = linalg.matmul(""X.H @ X"", _X)\n    else:        \n        C = _X.T @ _y\n    C /= _X2\n    C /= _y2\n    if same and reflect:\n        C = _reflect(C)\n    return C\n\n\n@jit([A32(M32_), A64(M64_)], **nogil)\ndef corr_sum(C):\n    """"""\n    Sums up all abs(correlation values). Used to find the\n    most ""important"" columns.\n    """"""\n    p = C.shape[0]\n    z = np.zeros(p, dtype = C.dtype)\n\n    for i in range(p):\n        for j in range(i+1, p):\n            c = abs(C[i,j])\n            z[i] += c\n            z[j] += c            \n    return z\n'"
hyperlearn_new/utils.py,0,"b'\nimport numpy as np\nfrom .base import *\nfrom .numba.types import *\nALPHA_DEFAULT32 = 1e-3\nALPHA_DEFAULT64 = 1e-6\n\n\n###\n@jit([ (M_32, A32), (M_64, A64) ], **gil)\ndef diag_set(X, new):\n    """"""\n    Sets the diagonal of a square matrix X to new.\n    [Added 5/1/19]\n    """""" \n    for i in range(X.shape[1]): X[i, i] = new[i]\n\n\n###\ndef do_until_success(\n    f, epsilon_f, size, overwrite = False, alpha = None, *args, **kwargs):\n    """"""\n    Epsilon Jitter Algorithm from Modern Big Data Algorithms. Forces certain\n    algorithms to converge via ridge regularization.\n    [Added 15/11/18] [Edited 25/11/18 Fixed Alpha setting, can now run in\n    approx 1 - 2 runs] [Edited 26/11/18 99% rounded accuracy in 1 run]\n    [Edited 14/12/18 Some overwrite errors] [Edited 21/12/18 If see a 0]\n\n    Parameters\n    -----------\n    f:          Function for solver\n    epsilon_f:  How to add epsilon\n    size:       Argument for epsilon_f\n    overwrite:  Whether to overwrite data matrix\n    alpha:      Ridge regularizer - default = 1e-6\n    """"""\n    if len(args) > 0:\n        X = args[0]\n    else:\n        X = next(iter(kwargs.values()))\n\n    # Default alpha\n    small = X.itemsize < 8\n    default = ALPHA_DEFAULT32 if small else ALPHA_DEFAULT64\n    \n    if alpha is None: alpha = 0\n\n    if overwrite:   alpha = _max(alpha, default)\n    else:           previous = X.diagonal().copy()\n\n    old_alpha = 0\n    error = 1\n    while error != 0:\n        epsilon_f(X, size, alpha - old_alpha, default)\n        #print(X.diagonal())\n        try:\n            out = f(*args, **kwargs)\n            if type(out) is tuple:\n                error = out[-1]\n                if len(out) == 2:\n                    out = out[0]\n                else:\n                    out = out[:-1]\n            else:\n                error = 0\n        except: \n            pass\n        if error != 0:\n            old_alpha = alpha\n            alpha *= 2\n\n            if alpha == 0:\n                alpha = ALPHA_DEFAULT32 if small else ALPHA_DEFAULT64\n            \n            #print(f""Epsilon Jitter Algorithm Restart with alpha = {alpha}."")\n            if overwrite:\n                #print(""Overwriting maybe have issues, please turn it off."")\n                overwrite = False\n\n    if not overwrite:\n        diag_set(X, previous)\n        #epsilon_f(X, size, -alpha)\n    return out\n\n\n###\n@jit([ (M_32, I64, F64, F64), (M_64, I64, F64, F64) ], **gil)\ndef add_jitter(X, size, alpha, default):\n    """"""\n    Epsilon Jitter Algorithm from Modern Big Data Algorithms. Forces certain\n    algorithms to converge via ridge regularization.\n    [Added 15/11/18] [Edited 25/11/18 for floating point errors]\n    [Edited 21/12/18 first pass checks what should be added]\n    """"""\n    jitter = alpha\n\n    for i in range(size):\n        old = X[i, i]\n        if old == 0 and jitter == 0:\n            jitter = default\n\n        new = old + jitter\n        while new - old < alpha:\n            jitter *= jitter\n            new = old + jitter            \n\n    for i in range(size):\n        X[i, i] += jitter\n\n\n\n### # output only U part. Overwrites LU matrix to save memory.\n@jit([ M32(I64, I64, M_32), M64(I64, I64, M_64) ], **nogil) \ndef triu(n, p, U):\n    tall = (n > p)\n    k = n\n\n    if tall:\n        # tall matrix\n        # U get all p rows\n        U = U[:p]\n        k = p\n\n    for i in range(1, k):\n        U[i, :i] = 0\n    return U\n\n\n\n### # Output only L part. Overwrites LU matrix to save memory.\n@jit([ Tuple((M32, I64))(I64, I64, M_32), Tuple((M64, I64))(I64, I64, M_64) ], **nogil)  \ndef L_process(n, p, L):\n    if p > n:\n        # wide matrix\n        # L get all n rows, but only n columns\n        L = L[:, :n]\n        k = n\n    else:\n        k = p\n\n    # tall / wide matrix\n    for i in range(k):\n        L[i, i+1:] = 0\n        L[i, i] = 1\n    # Set diagonal to 1\n    return L, k\n\n\n### # Force triangular matrix U to be invertible using ridge regularization\n@jit([ (M_32, I64, F64, F64), (M_64, I64, F64, F64) ], **nogil)\ndef U_process(A, size, alpha, default):\n    jitter = alpha\n\n    for i in range(size):\n        old = A[i, i]\n        if old == 0:\n            if jitter == 0:\n                jitter = default\n\n            new = jitter\n            while old < alpha:\n                jitter *= jitter\n                new = jitter\n\n    for i in range(size):\n        if A[i, i] == 0:\n            A[i, i] += jitter\n\n\n###\n@jit([ M32_(M32_), M64_(M64_) ], **nogil)\ndef _reflect(X):\n    n = X.shape[0]\n    for i in range(1, n):\n        for j in range(i):\n            X[j, i] = X[i, j]\n    return X\n###\n@jit([ M32_(M32_), M64_(M64_) ], **parallel)\ndef _reflect_parallel(X):\n    n = X.shape[0]\n    for i in prange(1, n):\n        for j in range(i):\n            X[j, i] = X[i, j]\n    return X\n\n###\ndef reflect(X):\n    n = X.shape[0]\n    x = X.T if X.flags[""F_CONTIGUOUS""] else X\n    f = _reflect_parallel if n > 5000 else _reflect\n    return f(x)\n\n\n###\n@jit([int8[::1](M_32, I64, I64), int8[::1](M_64, I64, I64),\n      int8[::1](M32_, I64, I64), int8[::1](M64_, I64, I64)], **nogil)   \ndef amax_0(X, n, p):\n    """"""\n    Finds the sign(X[:,abs(X).argmax(0)])\n    """"""\n    indices = np.ones(p, dtype = np.int8)\n    maximum = np.zeros(p, dtype = X.dtype)\n\n    for i in range(n):\n        for j in range(p):\n            Xij = X[i, j]\n            a = abs(Xij)\n            if a > maximum[j]:\n                maximum[j] = a\n                indices[j] = np.sign(Xij)\n    return indices\n\n###\n@jit([int8[::1](M_32, I64, I64), int8[::1](M_64, I64, I64),\n      int8[::1](M32_, I64, I64), int8[::1](M64_, I64, I64)], **parallel)   \ndef amax_0_parallel(X, n, p):\n    """"""\n    Finds the sign(X[:,abs(X).argmax(0)])\n    """"""\n    indices = np.ones(p, dtype = np.int8)\n    maximum = np.zeros(p, dtype = X.dtype)\n\n    for i in prange(n):\n        for j in range(p):\n            Xij = X[i, j]\n            a = abs(Xij)\n            if a > maximum[j]:\n                maximum[j] = a\n                indices[j] = np.sign(Xij)\n    return indices\n\n\n###\n@jit([int8[::1](M_32, I64, I64), int8[::1](M_64, I64, I64),\n      int8[::1](M32_, I64, I64), int8[::1](M64_, I64, I64)], **nogil)   \ndef amax_1(X, n, p):\n    """"""\n    Finds the sign(X[abs(X).argmax(1)])\n    """"""\n    indices = np.zeros(n, dtype = np.int8)\n    for i in range(n):\n        _max = 0\n        s = 1\n        for j in range(p):\n            Xij = X[i, j]\n            a = abs(Xij)\n            if a > _max:\n                _max = a\n                s = np.sign(Xij)\n        indices[i] = s\n    return indices\n\n\n###\n@jit([int8[::1](M_32, I64, I64), int8[::1](M_64, I64, I64),\n      int8[::1](M32_, I64, I64), int8[::1](M64_, I64, I64)], **parallel)   \ndef amax_1_parallel(X, n, p):\n    """"""\n    Finds the sign(X[abs(X).argmax(1)])\n    """"""\n    indices = np.zeros(n, dtype = np.int8)\n    for i in prange(n):\n        _max = 0\n        s = 1\n        for j in range(p):\n            Xij = X[i, j]\n            a = abs(Xij)\n            if a > _max:\n                _max = a\n                s = np.sign(Xij)\n        indices[i] = s\n    return indices\n\n\n###\ndef sign_max(X, axis = 0, n_jobs = 1):\n    """"""\n    [Added 19/11/2018] [Edited 24/11/2018 Uses NUMBA]\n    Returns the sign of the maximum absolute value of an axis of X.\n\n    input:      1 argument, 2 optional\n    ----------------------------------------------------------\n    X:          Matrix X to be processed. Must be 2D array.\n    axis:       Default = 0. 0 = column-wise. 1 = row-wise.\n    n_jobs:     Default = 1. Uses multiple CPUs if n*p > 20,000^2.\n\n    returns:    sign array of -1,1\n    ----------------------------------------------------------\n    """""" \n    n, p = X.shape\n    amax = f""amax_{axis}_parallel"" if (n_jobs != 1 and n*p > 4e8) else f""amax_{axis}""\n    return eval(amax)(X, n, p)\n\n\n###\ndef svd_flip(U = None, VT = None, U_decision = False, n_jobs = 1):\n    """"""\n    [Added 19/11/2018] [Edited 24/11/2018 Uses NUMBA]\n    Flips the signs of U and VT from a SVD or eigendecomposition.\n    Default opposite to Sklearn\'s U decision. HyperLearn uses\n    the maximum of rows of VT.\n\n    input:      2 argument, 1 optional\n    ----------------------------------------------------------\n    U:          U matrix from SVD\n    VT:         VT matrix (V transpose) from SVD\n    U_decision: Default = False. If True, uses max from U.\n\n    returns:    Nothing. Inplace updates U, VT\n    ----------------------------------------------------------\n    """"""\n    if U_decision is None: return\n\n    if U is not None:\n        if U_decision:\n            signs = sign_max(U, 0, n_jobs = n_jobs)\n        else:\n            signs = sign_max(VT, 1, n_jobs = n_jobs)\n        U *= signs\n        VT *= signs[:,np.newaxis]\n    else:\n        # Eig flip on eigenvectors\n        signs = sign_max(VT, 0, n_jobs = n_jobs)\n        VT *= signs\n\n\n###\n@jit([I64(A32, F64, I64), I64(A64, F64, I64)], **gil)\ndef svd_search(S, eps, size):\n    """"""\n    Determines the rank of a matrix via the singular values.\n    (Counts how many are larger than eps.)\n    """"""\n    rank = size-1\n    if S[rank] < eps:\n        rank -= 1\n        while rank > 0:\n            if S[rank] >= eps: break\n            rank -= 1\n    rank += 1\n    return rank\n\n\n###\ndef svd_condition(U, S, VT, alpha = None):\n    """"""\n    [Added 21/11/2018]\n    Uses Scipy\'s SVD condition number calculation to improve pseudoinverse\n    stability. Uses (1e3, 1e6) * eps(S) * S[0] as the condition number.\n    Everythimg below cond is set to 0.\n\n    input:      3 argument, 1 optional\n    ----------------------------------------------------------\n    U:          U matrix from SVD\n    S:          S diagonal array from SVD\n    VT:         VT matrix (V transpose) from SVD\n    alpha:      Default = None. Ridge regularization\n\n    returns:    U, S/(S+alpha), VT updated.\n    ----------------------------------------------------------\n    """"""\n    eps = epsilon(S)*S[0]\n\n    # Binary search O(logn) is not useful\n    # since most singular values are not going to be 0\n    size = S.size\n    rank = svd_search(S, eps, size)\n    if rank != size:\n        U, S, VT = U[:, :rank], S[:rank], VT[:rank]\n\n    # Check if alpha needs to be added onto the singular values\n    alphaUpdate = False\n    if alpha is not None:\n        if is32:\n            if alpha != ALPHA_DEFAULT32:\n                alphaUpdate = True\n        else:\n            if alpha != ALPHA_DEFAULT64:\n                alphaUpdate = True\n\n    if alphaUpdate:\n        S /= (S**2 + alpha)\n    else:\n        S = 1/S\n    return U, S, VT\n\n\n###\n@jit([A32(A32, F64), A64(A64, F64)], **gil)\ndef eig_search(W, eps):\n    """"""\n    Corrects the eigenvalues if they\'re smaller than 0.\n    """"""\n    for i in range(W.size):\n        if W[i] > 0: break\n        # else set to condition number\n        W[i] = eps\n    return W\n\n\n###\ndef eig_condition(X, W, V):\n    # Condition number just in case W[i] <= 0\n    eps = epsilon(W)\n    if W[-1] >= 0:\n        first = W[-1]**0.5 # eigenvalues are sorted ascending\n    else:\n        first = 0\n    eps *= first\n    eps **= 2 # since eigenvalues are squared of singular values\n\n    W = eig_search(W, eps)    \n\n    # X.H @ V / W**0.5\n    XT = X if X.flags[""F_CONTIGUOUS""] else X.T\n    \n    if isComplex(dtype):\n        if isComplex(dtypeY):\n            out = blas(""gemm"")(a = XT, b = V.T, trans_a = 0, trans_b = 2, alpha = 1)\n        else:\n            out = XT @ V\n        out = np.conjugate(out, out = out)\n    else:\n        out = XT @ V\n\n    out /= W**0.5\n    return W, out\n\n\n@jit([bool_[::1](A32, F64), bool_[::1](A64, F64)], **gil)\ndef eigh_search(W, eps):\n    w0 = abs(W[0])\n\n    last = W[-1]\n    negative = (last < 0)\n    w1 = abs(last)\n\n    cutoff = w0 if w0 > w1 else w1\n    cutoff *= eps\n    \n    size = len(W)\n    out = np.ones(size, dtype = np.bool_)\n    \n    if w0 < cutoff:\n        out[0] = False\n    if w1 < cutoff:\n        out[-1] = False\n    \n    # Check if abs(W) < eps\n    for i in range(1, size-1):\n        if abs(W[i]) < cutoff:\n            out[i] = False\n            \n    return out\n\n\n###\n@jit([A32(M32_), A64(M64_)], **nogil)\ndef _row_norm(X):\n    n, p = X.shape\n    norm = np.zeros(n, dtype = X.dtype)\n    \n    for i in range(n):\n        s = 0\n        for j in range(p):\n            xij = X[i, j]\n            xij *= xij\n            s += xij\n        norm[i] = s\n    return norm\n\n\n###\n@jit([A32(M32_), A64(M64_)], **nogil)\ndef _col_norm(X):\n    n, p = X.shape\n    norm = np.zeros(p, dtype = X.dtype)\n    \n    for i in range(n):\n        for j in range(p):\n            xij = X[i, j]\n            xij *= xij\n            norm[j] += xij\n    return norm\n\n###\n@jit([F64(A32), F64(A64)], **gil)\ndef normA(X):\n    s = 0\n    for i in range(X.size):\n        xi = X[i]\n        xi *= xi\n        s += xi\n    return s\n\n###\ndef col_norm(X):\n    if len(X.shape) > 1:\n        return _col_norm(X)\n    return normA(X)\n\n###\ndef row_norm(X):\n    if len(X.shape) > 1:\n        return _row_norm(X)\n    return normA(X)\n\n\n###\n@jit([F64(M32_), F64(M64_)], **gil)\ndef _frobenius_norm(X):\n    n, p = X.shape\n    norm = 0\n    \n    for i in range(n):\n        for j in range(p):\n            xij = X[i, j]\n            xij *= xij\n            norm += xij\n    return norm\n\n###\n@jit([F64(M_32), F64(M_64)], **gil)\ndef _frobenius_norm_symmetric(X):\n    n = X.shape[0]\n    norm = 0\n    diag = 0\n    \n    for i in range(n):\n        for j in range(i+1, n):\n            xij = X[i, j]\n            xij *= xij\n            norm += xij\n\n        xii = X[i, i]\n        xii *= xii\n        diag += xii\n    norm *= 2\n    norm += diag\n    return norm\n\n###\ndef frobenius_norm(X, symmetric = False):\n    """"""\n    Outputs the ||X||_F ^ 2 norm of a matrix. If symmetric,\n    then computes the norm on 1/2 of the matrix and multiplies\n    it by 2.\n    """"""\n    if len(X.shape) > 1:\n        if symmetric:\n            return _frobenius_norm_symmetric(X)\n        return _frobenius_norm(X)\n    return normA(X)\n\n\n###\n@jit([A32(A32), A64(A64)], **nogil)\ndef proportion(X):\n    X /= np.sum(X)\n    return X\n\n\n###\n@jit([M32_(M32_, I64[::1], I64, I64), M64_(M64_, I64[::1], I64, I64)], **nogil)\ndef gram_schmidt(X, P, n, k):\n    """"""\n    Modified stable Gram Schmidt process.\n    Gram-Schmidt Orthogonalization\n    Instructor: Ana Rita Pires (MIT 18.06SC).\n    Output is Q.T NOT Q. So you must transpose it.\n    """"""\n    Q = np.zeros((k, n), dtype = X.dtype)\n    Z = np.zeros(n, dtype = X.dtype)\n\n    for i in range(k):\n\n        x = Q[i]\n        col = P[i]\n        # Q[i] = X[:,P[i]]\n        for a in range(n):\n            x[a] = X[a, col]\n        \n        for j in range(i):\n            q = Q[j]\n            x -= np.vdot(x, q) * q\n            \n        norm = np.linalg.norm(x)\n        if norm == 0:\n            Q[i] = Z\n            Q[i,i] = 1\n        else:\n            x /= norm\n    return Q\n\n\n###\n@jit(**nogil)\ndef _unique_int(a):\n    """"""\n    Assumes a is just integers, and returning unique elements\n    will be much easier. Uses a quick boolean array instead of\n    a hash table.\n    """"""\n    seen = np.zeros(np.max(a)+1, dtype = np.bool_)\n    count = 0\n    \n    for i in range(a.size):\n        element = a[i]\n        curr = seen[element]\n        if not curr:\n            seen[element] = True\n            count += 1\n\n    out = np.zeros(count, dtype = a.dtype)\n    j = 0\n    # fill up array with uniques\n    for i in range(seen.size):\n        if seen[i]:\n            out[j] = i\n            j += 1\n            if j > count: break\n    return out\n\n###\n@jit(**nogil)\ndef _unique_count(a, dtype):\n    """"""\n    Returns the counts and unique values of an array a.\n    [Added 23/12/18]\n    """"""\n    maximum = np.max(a) + 1\n    seen = np.zeros(maximum, dtype = dtype)\n    count = 0\n    \n    for i in range(a.size):\n        element = a[i]\n        curr = seen[element]\n        if curr == 0: count += 1\n        seen[element] += 1\n\n    unique = np.zeros(count, dtype = a.dtype)\n    counts = np.zeros(count, dtype = np.uint32)\n    \n    j = 0\n    for i in range(seen.size):\n        curr = seen[i]\n        if curr > 0:\n            unique[j] = i\n            counts[j] = curr\n            j += 1\n            if j > count: break\n    return unique, counts\n\n###\n@jit(**gil)\ndef _unique_sorted_size(a):\n    """"""\n    Returns how many uniques in a sorted list.\n    [Added 23/12/18]\n    """"""\n    size = 1\n    i = 0\n    old = a[i]\n    i += 1\n    while i < a.size:\n        new = a[i]\n        if new != old:\n            size += 1\n            old = new\n        i += 1\n    return size\n\n###\n@jit(**gil)\ndef _unique_sorted(a):\n    """"""\n    Returns only unique elements in a sorted list.\n    [Added 23/12/18]\n    """"""\n    size = _unique_sorted_size(a)\n        \n    out = np.zeros(size, dtype = a.dtype)\n    i = 0\n    old = a[i]\n    out[i] = old\n    \n    i += 1\n    j = 1\n    length = a.size\n    while i < length:\n        new = a[i]\n        if new != old:\n            out[j] = new\n            old = new\n            j += 1\n            if j > length: break\n        i += 1\n    return out\n\n###\n@jit(**gil)\ndef _unique_sorted_count(a):\n    """"""\n    Returns unique elements and their counts in a sorted list.\n    [Added 23/12/18]\n    """"""\n    size = _unique_sorted_size(a)\n        \n    out = np.zeros(size, dtype = a.dtype)\n    counts = np.zeros(size, dtype = np.uint32)\n    i = 0\n    old = a[i]\n    out[i] = old\n    \n    i += 1\n    j = 0\n    length = a.size\n    while i < length:\n        new = a[i]\n\n        # Add 1 to count\n        counts[j] += 1\n        if new != old:\n            j += 1\n            if j > length: break\n            out[j] = new\n            old = new\n        i += 1\n\n    # Need to update last element since loop forgets it.\n    counts[-1] += 1\n    return out, counts\n\n\n###\ndef unique_int(a, return_counts = False):\n    """"""\n    Given a list of postive ints, returns the unique\n    elements accompanied with optional counts.\n    [Added 23/12/18]\n\n    Parameters\n    -----------\n    a:              Array of postive ints\n    return_counts:  Whether to return (unique, counts)\n    """"""\n    if return_counts:\n        return _unique_count(a, uinteger(a.size))\n    return _unique_int(a)\n\n\n###\ndef unique_sorted(a, return_counts = False):\n    """"""\n    Given a list of sorted elements, returns the unique\n    elements accompanied with optional counts.\n    [Added 23/12/18]\n\n    Parameters\n    -----------\n    a:              Array of sorted elements\n    return_counts:  Whether to return (unique, counts)\n    """"""\n    if return_counts:\n        return _unique_sorted_count(a)\n    return _unique_sorted(a)\n\n'"
hyperlearn/big_data/__init__.py,0,"b""\nfrom .truncated import truncatedSVD, truncatedEigh\nfrom .randomized import randomizedSVD\nfrom .lsmr import lsmr as LSMR\n\n__all__ = ['truncatedSVD', 'truncatedEigh',\n\t\t\t'randomizedSVD', 'LSMR']\n"""
hyperlearn/big_data/base.py,0,b'\n'
hyperlearn/big_data/incremental.py,0,"b'\nfrom numpy import vstack, newaxis, arange\nfrom ..linalg import svd, eigh, eig\nfrom .truncated import truncatedSVD, truncatedEigh\nfrom ..utils import memoryXTX\nfrom .randomized import randomizedSVD, randomizedEig\nfrom ..exceptions import PartialWrongShape\n\n\ndef _utilSVD(batch, S, VT, eig = False):\n\t""""""\n\tBatch (nrows, ncols)\n\tS (ncomponents)\n\tVT (rows = ncomponents, cols = ncols)\n\t\n\tCheck Batch(ncols) == VT(ncols) to check same number\n\t\tof columns or else error is provided.\n\t""""""\n\tif eig: \n\t\tVT, S = VT.T, S**0.5\n\tncomponents, ncols = VT.shape\n\tif batch.shape[1] != ncols:\n\t\traise PartialWrongShape()\n\n\tdata = vstack( ( S[:,newaxis]*VT , batch ) )\n\n\treturn data, VT.shape[0] , memoryXTX(data)\n\n\n\ndef partialSVD(batch, S, VT, ratio = 1, solver = \'full\', tol = None, max_iter = \'auto\'):\n\t""""""\n\tFits a partial SVD after given old singular values S\n\tand old components VT.\n\n\tNote that VT will be used as the number of old components,\n\tso when calling truncated or randomized, will output a\n\tspecific number of eigenvectors and singular values.\n\n\tChecks if new batch\'s size matches that of the old VT.\n\n\tNote that PartialSVD has different solvers. Either choose:\n\t\t1. full\n\t\t\tSolves full SVD on the data. This is the most\n\t\t\tstable and will guarantee the most robust results.\n\t\t\tYou can select the number of components to keep\n\t\t\twithin the model later.\n\n\t\t2. truncated\n\t\t\tThis keeps the top K right eigenvectors and top\n\t\t\tk right singular values, as determined by\n\t\t\tn_components. Note full SVD is not called for the\n\t\t\ttruncated case, but rather ARPACK is called.\n\n\t\t3. randomized\n\t\t\tSame as truncated, but instead of using ARPACK, uses\n\t\t\trandomized SVD.\n\n\tNotice how Batch = U @ S @ VT. However, partialSVD returns\n\tS, VT, and not U. In order to get U, you might consider using\n\tthe relation that X = U @ S @ VT, and approximating U by:\n\n\t\tX = U @ S @ VT\n\t\tX @ V = U @ S\n\t\t(X @ V)/S = U\n\n\t\tSo, U = (X @ V)/S, so you can output U from (X @ V)/S\n\n\t\tYou can also get U partially and slowly using reverseU.\n\t""""""\n\tdata, k, __ = _utilSVD(batch, S, VT, eig = False)\n\n\tif solver == \'full\':\n\t\tU, S, VT = svd(data)\n\telif solver == \'truncated\':\n\t\tU, S, VT = truncatedSVD(data, n_components = k, tol = tol)\n\telse:\n\t\tU, S, VT = randomizedSVD(data, n_components = k, max_iter = max_iter)\n\n\treturn U[k:,:k], S[:k], VT[:k]\n\n\n\n\ndef partialEig(batch, S2, V, ratio = 1, solver = \'full\', tol = None, max_iter = \'auto\'):\n\t""""""\n\tFits a partial Eigendecomp after given old eigenvalues S2\n\tand old eigenvector components V.\n\n\tNote that V will be used as the number of old components,\n\tso when calling truncated or randomized, will output a\n\tspecific number of eigenvectors and eigenvalues.\n\n\tChecks if new batch\'s size matches that of the old V.\n\n\tNote that PartialEig has different solvers. Either choose:\n\t\t1. full\n\t\t\tSolves full Eigendecompsition on the data. This is the most\n\t\t\tstable and will guarantee the most robust results.\n\t\t\tYou can select the number of components to keep\n\t\t\twithin the model later.\n\n\t\t2. truncated\n\t\t\tThis keeps the top K right eigenvectors and top\n\t\t\tk eigenvalues, as determined by n_components. Note full Eig\n\t\t\tis not called for the truncated case, but rather ARPACK is called.\n\n\t\t3. randomized\n\t\t\tSame as truncated, but instead of using ARPACK, uses\n\t\t\trandomized Eig.\n\t""""""\n\tdata, k, memCheck = _utilSVD(batch, S2, V, eig = True)\n\n\tif solver == \'full\':\n\t\tS2, V = eig(data, svd = False)\n\t\treturn S2, V\n\n\telif solver == \'truncated\':\n\t\tif memCheck:\n\t\t\tS2, V = truncatedEigh(data.T @ data, n_components = k, tol = tol)\n\t\telse:\n\t\t\t__, S2, V = truncatedSVD(data, n_components = k, tol = tol)\n\t\t\tS2**=2\n\t\t\tV = V.T\n\telse:\n\t\tS2, V = randomizedEig(data, n_components = k, max_iter = max_iter)\n\n\treturn S2[:k], V[:,:k]\n\n'"
hyperlearn/big_data/lsmr.py,0,"b'\nfrom ..numba import norm, _min, _max, _sign\nfrom numpy import infty, zeros, float32, float64\nfrom copy import copy\nfrom ..utils import _float\n\n\ndef Orthogonalize(a, b):\n\tA,B,_a,_b = abs(a), abs(b), _sign(a), _sign(b)\n\tif b == 0: \n\t\treturn _a, 0, A\n\telif a == 0:\n\t\treturn 0, _b, B\n\telif B > A:\n\t\ttau = a/b\n\t\ts = _b/(1+tau*tau)**0.5\n\t\tc = s*tau\n\t\tr = b/s\n\telse:\n\t\ttau = b/a\n\t\tc = _a/(1+tau*tau)**0.5\n\t\ts = c*tau\n\t\tr = a/c\n\treturn c,s,r\n\n\ndef floatType(dtype):\n\tdtype = str(dtype)\n\tif \'64\' in dtype:\n\t\treturn float64\n\treturn float32\n\n\n\ndef lsmr(X, y, tol = 1e-6, condition_limit = 1e8, alpha = 0, threshold = 1e12, non_negative = False,\n\t\t\tmax_iter = None):\n\t""""""\n\t[As of 12/9/2018, an optional non_negative argument is added. Note as accurate as Scipy\'s NNLS,\n\tby copies ideas from gradient descent.]\n\n\tImplements extremely fast least squares LSMR using orthogonalization as seen in Scipy\'s LSMR and\n\thttps://arxiv.org/abs/1006.0758 [LSMR: An iterative algorithm for sparse least-squares problems]\n\tby David Fong, Michael Saunders.\n\n\tScipy\'s version of LSMR is surprisingly slow, as some slow design factors were used\n\t(ie np.sqrt(1 number) is slower than number**0.5, or min(a,b) is slower than using 1 if statement.)\n\n\tALPHA is provided for regularization purposes like Ridge Regression.\n\n\tThis algorithm works well for Sparse Matrices as well, and the time complexity analysis is approx:\n\t\tX.T @ y   * min(n,p) times + 3 or so O(n) operations\n\t\t==> O(np)*min(n,p)\n\t\t==> either min(O(n^2p + n), O(np^2 + n))\n\n\tThis complexity is much better than Cholesky Solve which is the next fastest in HyperLearn.\n\tCholesky requires O(np^2) for XT * X, then Cholesky needs an extra 1/3*O(np^2), then inversion\n\ttakes another 1/3*(np^2), and finally (XT*y) needs O(np).\n\n\tSo Cholesky needs O(5/3np^2 + np) >> min(O(n^2p + n), O(np^2 + n))\n\n\tSo by factor analysis, expect LSMR to be approx 2 times faster or so.\n\tInterestingly, the Space Complexity is even more staggering. LSMR takes only maximum O(np^2) space\n\tfor the computation of XT * y + some overhead.\n\n\tCholesky requires XT * X space, which is already max O(n^2p) [which is huge].\n\tEssentially, Cholesky shines when P is large, but N is small. LSMR is good for large N, medium P\n\t""""""\n\tdamp = alpha\n\tcheck = False\n\ttry:\n\t\tX = X.A  # Convert Matrix to Array\n\t\tX = _float(X)\n\texcept:\n\t\tpass\n\tdtype = X.dtype\n\tY = y.ravel().copy()\n\tif y.dtype != dtype:\n\t\tY = Y.astype(dtype)\n\t# Y = Y.copy()\n\tn,p = X.shape\n\tmax_iter = _min(n, p) if max_iter is None else max_iter\n\n\tnorm_Y = norm(Y)\n\tzero = zeros(p, dtype = dtype)\n\ttheta_hat = zero.copy()\n\n\tbeta = copy(norm_Y)\n\tXT = X.T\n\n\tif beta > 0:\n\t\tY /= beta\n\t\tV = XT @ Y\n\t\talpha = norm(V)\n\telse:\n\t\tV = zeros(p, dtype = dtype)\n\t\talpha = 0\n\n\tif alpha > 0:\n\t\tV /= alpha\n\t\t\n\t# Initialize first iteration variables\n\tzeta_bar = alpha * beta\n\talpha_bar = alpha\n\trho, rho_bar, c_bar, s_bar = 1,1,1,0\n\tH = V.copy()\n\tH_bar = zero.copy()\n\n\n\t# Initialize variables for estimation of ||r||.\n\tbeta_dd = beta\n\tbeta_d, rho_d_old, tau_tilde_old = 0,1,0\n\ttheta_tilde, zeta, d = 0,0,0\n\n\n\t# Initialize variables for estimation of ||A|| and cond(A)\n\tnorm_X2 = alpha*alpha\n\tmax_r_bar, min_r_bar = 0, 1e100\n\tnorm_X, cond_X, norm_theta = alpha,1,0\n\n\n\t# Early stopping\n\tif condition_limit > 0:\n\t\tc_tol = 1/condition_limit\n\telse:\n\t\tc_tol = 0\n\tnorm_r = beta\n\n\n\t# Check if theta_hat == 0\n\tnormAB = alpha*beta\n\tif normAB == 0:\n\t\treturn theta_hat, True\n\n\n\t# Iteration Loop\n\tfor i in range(max_iter):\n\n\t\tY *= -alpha\n\t\tY += X @ V\n\t\tbeta = norm(Y)\n\n\t\tif beta > 0:\n\t\t\tY /= beta\n\t\t\tV *= -beta\n\t\t\tV += XT @ Y\n\t\t\talpha = norm(V)\n\n\t\t\tif alpha > 0:\n\t\t\t\tV /= alpha\n\n\n\t\tc_hat, s_hat, alpha_hat = Orthogonalize(alpha_bar, damp)\n\n\t\t# Plane rotation\n\t\trho_old = rho\n\t\tc, s, rho = Orthogonalize(alpha_hat, beta)\n\t\ttheta_new = s*alpha\n\t\talpha_bar = c*alpha\n\n\t\t# Plane rotation\n\t\trho_bar_old, zeta_old = rho_bar, zeta\n\t\ttheta_bar, rho_temp = s_bar*rho, c_bar*rho\n\n\t\tc_bar, s_bar, rho_bar = Orthogonalize(c_bar*rho, theta_new)\n\t\tzeta = c_bar*zeta_bar\n\t\tzeta_bar *= -s_bar\n\t\t\t\t\n\t\t# Update H, H_hat, theta_hat\n\t\tH_bar *= -theta_bar*rho/(rho_old*rho_bar_old)\n\t\tH_bar += H\n\t\ttheta_hat += (zeta/(rho*rho_bar))*H_bar\n\n\t\t# Just in case if coefficients exceed maximum\n\t\ttheta_hat[abs(theta_hat) > threshold] = 0.0\n\n\t\tif non_negative:\n\t\t\ttheta_hat[theta_hat < 0] = 0.0\n\n\t\tH *= -theta_new/rho\n\t\tH += V\n\n\t\t# Estimate of ||r||\n\t\tbeta_acute, beta_check = c_hat*beta_dd, -s_hat*beta_dd\n\n\t\t# Apply rotation\n\t\tbeta_hat = c*beta_acute\n\t\tbeta_dd = -s*beta_acute\n\n\t\t# Apply rotation\n\t\ttheta_tilde_old = theta_tilde\n\t\tc_tilde_old, s_tilde_old, rho_tilde_old = Orthogonalize(rho_old, theta_bar)\n\t\ttheta_tilde = s_tilde_old*rho_bar\n\n\t\trho_old = c_tilde_old*rho_bar\n\t\tbeta_d = c_tilde_old*beta_hat - s_tilde_old*beta_d\n\n\n\t\ttau_tilde_old = (zeta_old - theta_tilde_old*tau_tilde_old)/rho_tilde_old\n\t\td += beta_check*beta_check\n\n\t\tpartial = beta_d - ((zeta - theta_tilde*tau_tilde_old)/rho_old)\n\t\tnorm_r = (d+ partial*partial + beta_dd*beta_dd)**0.5\n\n\t\t# Estimate ||X||\n\t\tnorm_X2 += beta*beta\n\t\tnorm_X = norm_X2**0.5\n\t\tnorm_X2 += alpha*alpha\n\n\t\t# Condition(X)\n\t\tmax_r_bar = _max(max_r_bar, rho_bar_old)\n\t\tif i > 1:\n\t\t\tmin_r_bar = _min(max_r_bar, rho_bar_old)\n\t\tcond_X = _max(max_r_bar, rho_temp)/_min(min_r_bar, rho_temp)\n\n\t\t# Estimate other stuff\n\n\t\ttest_1 = norm_r/norm_Y\n\t\tX_r = norm_X * norm_r\n\n\n\t\tif X_r != 0:\n\t\t\ttest_2 = abs(zeta_bar) / X_r\t\t\t\t\t\t\t# norm_ar = abs(zeta_bar)\n\t\telse:\n\t\t\ttest_2 = infty\n\n\t\tif (1+test_2 <= 1):\n\t\t\tbreak\n\t\telif (test_2 <= tol):\n\t\t\tbreak\n\t\t\n\n\t\ttest_3 = 1/cond_X\n\t\tcheck = (1+test_3 <= 1)\n\n\t\tif check:\n\t\t\tbreak\n\t\telif (test_3 <= c_tol):\n\t\t\tbreak\n\n\n\t\tpartial = 1+(norm_X*norm(theta_hat)/norm_Y)\t\t\t\t\t\t# norm_theta = norm(theta)\n\t\tt1 = test_1/partial\n\n\t\tr_tol = tol*partial\n\n\t\tif (1+t1 <= 1):\n\t\t\tbreak\n\t\telif (test_1 <= r_tol):\n\t\t\tbreak\n\n\n\treturn theta_hat, not check\n\t# check is TRUE is good, FALSE is cond(X) is too large.\n\n\n'"
hyperlearn/big_data/randomized.py,0,"b'\nfrom ..linalg import lu, svd, qr, eig\nfrom numpy import random as _random, sqrt\nfrom numpy.linalg import norm\nfrom ..utils import _float, _svdCond, traceXTX, eig_flip, svd_flip\nfrom ..random import uniform\n\n# __all__ = [\'randomizedSVD\', \'randomizedEig\']\n\n\ndef randomized_projection(X, k, solver = \'lu\', max_iter = 4):\n\t""""""\n\t[Edited 8/11/2018 Added QR Q_only parameter]\n\tProjects X onto some random eigenvectors, then using a special\n\tvariant of Orthogonal Iteration, finds the closest orthogonal\n\trepresentation for X.\n\t\n\tSolver can be QR or LU or None.\n\t""""""\n\tn, p = X.shape\n\tif max_iter == \'auto\':\n\t\t# From Modern Big Data Algorithms --> seems like <= 4 is enough.\n\t\t_min = n if n <= p else p\n\t\tmax_iter = 5 if k < 0.1 * _min else 4\n\n\tQ = uniform(-5, 5, p, int(k), X.dtype)\n\tXT = X.T\n\n\t_solver =                       lambda x: lu(x, L_only = True)\n\tif solver == \'qr\': _solver =    lambda x: qr(x, Q_only = True)\n\telif solver == None: _solver = \tlambda x: x\n\n\tfor __ in range(max_iter):\n\t\tQ = _solver(XT @ _solver(X @ Q))\n\n\treturn qr(X @ Q, Q_only = True)\n\n\n\ndef randomizedSVD(X, n_components = 2, max_iter = \'auto\', solver = \'lu\', n_oversamples = 10):\n\t""""""\n\t[Edited 9/11/2018 Fixed SVD_flip]\n\tHyperLearn\'s Fast Randomized SVD is approx 10 - 30 % faster than\n\tSklearn\'s implementation depending on n_components and max_iter.\n\t\n\tUses NUMBA Jit accelerated functions when available, and tries to\n\treduce memory overhead by chaining operations.\n\t\n\tUses QR, LU or no solver to find the best SVD decomp. QR is most stable,\n\tbut can be 2x slower than LU.\n\n\t****n_oversamples = 10. This follows Sklearn convention to increase the chance\n\t\t\t\t\t\t\tof more accurate SVD.\n\t\n\tReferences\n\t--------------\n\t* Sklearn\'s RandomizedSVD\n\t\n\t* Finding structure with randomness: Stochastic algorithms for constructing\n\t  approximate matrix decompositions\n\t  Halko, et al., 2009 http://arxiv.org/abs/arXiv:0909.4061\n\n\t* A randomized algorithm for the decomposition of matrices\n\t  Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n\n\t* An implementation of a randomized algorithm for principal component\n\t  analysis\n\t  A. Szlam et al. 2014\n\t""""""\n\tn,p = X.shape\n\ttranspose = (n < p)\n\tX = X if not transpose else X.T\n\tX = _float(X)\n\n\tQ = randomized_projection(X, n_components + n_oversamples, solver, max_iter)\n\n\tU, S, VT = svd(Q.T @ X, U_decision = transpose, transpose = True)\n\tU = Q @ U\n\tif transpose:\n\t\tU, VT = VT.T, U.T\n\n\treturn U[:, :n_components], S[:n_components], VT[:n_components, :]\n\n\n\ndef randomizedEig(X, n_components = 2, max_iter = \'auto\', solver = \'lu\', n_oversamples = 10):\n\t""""""\n\t[Edited 9/11/2018 Fixed Eig_Flip]\n\tHyperLearn\'s Randomized Eigendecomposition is an extension of Sklearn\'s\n\trandomized SVD. HyperLearn notices that the computation of U is not necessary,\n\thence will use QR followed by SVD or just SVD depending on the situation.\n\n\tLikewise, solver = LU is default, and follows randomizedSVD\n\t\n\tReferences\n\t--------------\n\t* Sklearn\'s RandomizedSVD\n\t\n\t* Finding structure with randomness: Stochastic algorithms for constructing\n\t  approximate matrix decompositions\n\t  Halko, et al., 2009 http://arxiv.org/abs/arXiv:0909.4061\n\n\t* A randomized algorithm for the decomposition of matrices\n\t  Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n\n\t* An implementation of a randomized algorithm for principal component\n\t  analysis\n\t  A. Szlam et al. 2014\n\t""""""\n\tn,p = X.shape\n\ttranspose = (n < p)\n\tX = X if not transpose else X.T\n\tX = _float(X)\n\n\tQ = randomized_projection(X, n_components + n_oversamples, solver, max_iter)\n\n\tif transpose:\n\t\tV, S2, __ = svd(Q.T @ X, U_decision = transpose, transpose = True)\n\t\tV = Q @ V\n\t\tS2 **= 2\n\telse:\n\t\tS2, V = eig(Q.T @ X, U_decision = transpose)\n\treturn S2[:n_components], V[:, :n_components]\n\n\n\ndef randomizedPinv(X, n_components = None, alpha = None):\n\t""""""\n\t[Added 6/11/2018]\n\tImplements fast randomized pseudoinverse with regularization.\n\tCan be used as an approximation to the matrix inverse.\n\t""""""\n\tif alpha != None: assert alpha >= 0\n\talpha = 0 if alpha is None else alpha\n\n\tif n_components == None:\n\t\t# will provide approx sqrt(p) - 1 components.\n\t\t# A heuristic, so not guaranteed to work.\n\t\tk = int(sqrt(X.shape[1]))-1\n\t\tif k <= 0: k = 1\n\telse:\n\t\tk = int(n_components) if n_components > 0 else 1\n\n\tX = _float(X)\n\n\tU, S, VT = randomizedSVD(X, n_components)\n\tU, S, VT = _svdCond(U, S, VT, alpha)\n\t\n\treturn VT.T * S @ U.T\n\t'"
hyperlearn/big_data/truncated.py,0,"b'\nfrom scipy.sparse import linalg as sparse\nfrom numpy import finfo\nfrom ..utils import *\nfrom ..random import uniform\n\n__all__ = [\'truncatedEigh\', \'truncatedSVD\', \'truncatedEig\']\n\n\ndef truncatedEigh(XTX, n_components = 2, tol = None, svd = False, which = \'largest\'):\n\t""""""\n\t[Edited 6/11/2018 Added smallest / largest command]\n\tComputes the Truncated Eigendecomposition of a Hermitian Matrix\n\t(positive definite). K = 2 for default.\n\tReturn format is LARGEST eigenvalue first.\n\n\tIf SVD is True, then outputs S2**0.5 and sets negative S2 to 0\n\tand outputs VT and not V.\n\t\n\tSpeed\n\t--------------\n\tUses ARPACK from Scipy to compute the truncated decomp. Note that\n\tto make it slightly more stable and faster, follows Sklearn\'s\n\trandom intialization from -1 -> 1.\n\n\tAlso note tolerance is resolution(X), and NOT eps(X)\n\n\tMight switch to SPECTRA in the future.\n\t\n\tStability\n\t--------------\n\tEIGH FLIP is called to flip the eigenvector signs for deterministic\n\toutput.\n\t""""""\n\tn_components = int(n_components)\n\tn,p = XTX.shape\n\tdtype = XTX.dtype\n\tassert n == p\n\t\n\tif tol is None: tol = finfo(dtype).resolution\n\tsize = n if p >= n else p  # min(n,p)\n\tv = uniform(-1, 1, size, dtype = dtype)\n\n\tif which == \'largest\':\n\t\tS2, V = sparse.eigsh(XTX, k = n_components, tol = tol, v0 = v)\n\telse:\n\t\t# Uses shift invert mode to get smallest\n\t\tS2, V = sparse.eigsh(XTX, k = n_components, tol = tol, v0 = v, sigma = 0)\n\tV = eig_flip(V)\n\t\n\t# Note ARPACK provides SMALLEST to LARGEST S2. Hence, reverse.\n\tS2, V = S2[::-1], V[:,::-1]\n\n\tif svd:\n\t\tS2[S2 < 0] = 0.0\n\t\tS2 **= 0.5\n\t\treturn S2, V.T\n\treturn S2, V\n\n\ndef truncatedEig(X, n_components = 2, tol = None, svd = False, which = \'largest\'):\n\t""""""\n\t[Added 6/11/2018]\n\tComputes truncated eigendecomposition given any matrix X. Directly\n\tuses TruncatedSVD if memory is not enough, and returns eigen vectors/values.\n\tAlso argument for smallest eigen components are provided.\n\t""""""\n\tif memoryXTX(X):\n\t\tcovariance = _XTX(X.T)\n\t\tS, VT = truncatedEigh(covariance, n_components, tol, which = which, svd = svd)\n\telse:\n\t\t__, S, VT = truncatedSVD(X, n_components, tol, which = which)\n\t\tif svd:\n\t\t\treturn S, VT\n\t\tS **= 2\n\t\tVT = VT.T\n\treturn S, VT\n\n\n\ndef truncatedSVD(X, n_components = 2, tol = None, transpose = True, U_decision = False, which = \'largest\'):\n\t""""""\n\t[Edited 6/11/2018 Added which command - can get largest or smallest eigen components]\n\tComputes the Truncated SVD of any matrix. K = 2 for default.\n\tReturn format is LARGEST singular first first.\n\t\n\tSpeed\n\t--------------\n\tUses ARPACK from Scipy to compute the truncated decomp. Note that\n\tto make it slightly more stable and faster, follows Sklearn\'s\n\trandom intialization from -1 -> 1.\n\n\tAlso note tolerance is resolution(X), and NOT eps(X). Also note\n\tTRANSPOSE is True. This means instead of computing svd(X) if p > n,\n\tthen computing svd(X.T) is faster, but you must output VT.T, S, U.T\n\n\tMight switch to SPECTRA in the future.\n\t\n\tStability\n\t--------------\n\tSVD FLIP is called to flip the VT signs for deterministic\n\toutput. Note uses VT based decision and not U based decision.\n\tU_decision can be changed to TRUE for Sklearn convention\n\t""""""\n\tn_components = int(n_components)\n\tdtype = X.dtype\n\tn, p = X.shape\n\ttranspose = True if (transpose and p > n) else False\n\tif transpose: \n\t\tX, U_decision = X.T, not U_decision\n\n\tif tol is None: tol = finfo(dtype).resolution\n\tsize = n if p >= n else p  # min(n,p)\n\tv = uniform(-1, 1, size, dtype = dtype)\n\n\twhich = \'LM\' if which == \'largest\' else \'SM\'\n\tU, S, VT = sparse.svds(X, k = n_components, tol = tol, v0 = v, which = which)\n\n\t# Note ARPACK provides SMALLEST to LARGEST S. Hence, reverse.\n\tU, S, VT = U[:, ::-1], S[::-1], VT[::-1]\n\n\tU, VT = svd_flip(U, VT, U_decision = U_decision)\n\t\n\tif transpose:\n\t\treturn VT.T, S, U.T\n\treturn U, S, VT\n\n\n\ndef truncatedPinv(X, n_components = None, alpha = None):\n\t""""""\n\t[Added 6/11/2018]\n\tImplements fast truncated pseudoinverse with regularization.\n\tCan be used as an approximation to the matrix inverse.\n\t""""""\n\tif alpha != None: assert alpha >= 0\n\talpha = 0 if alpha is None else alpha\n\n\tif n_components == None:\n\t\t# will provide approx sqrt(p) - 1 components.\n\t\t# A heuristic, so not guaranteed to work.\n\t\tk = int(sqrt(X.shape[1]))-1\n\t\tif k <= 0: k = 1\n\telse:\n\t\tk = int(n_components) if n_components > 0 else 1\n\n\tX = _float(X)\n\n\tU, S, VT = truncatedSVD(X, n_components)\n\tU, S, VT = _svdCond(U, S, VT, alpha)\n\t\n\treturn VT.T * S @ U.T\n\t'"
hyperlearn/decomposition/NMF.py,0,"b""\nfrom ..numba import _min, _max, maximum, minimum, norm, njit, prange, squaresum\nfrom numpy import zeros, float32, float64\nfrom ..utils import _float, reflect, _XTX, _XXT\nfrom ..big_data.randomized import randomizedSVD\nfrom ..solvers import solveCholesky\n\n\ndef intialize_NMF(X, n_components = 2, eps = 1e-6, init = 'nndsvd', HT = True):\n\tU, S, VT = randomizedSVD(X, n_components = n_components)\n\n\tdtype = U.dtype\n\tW, H = zeros(U.shape, dtype), zeros(VT.shape, dtype)\n\tSa = S[0]**0.5\n\n\tW[:, 0] = Sa * abs(U[:, 0])\n\tH[0, :] = Sa * abs(VT[0, :])\n\n\tfor j in range(1, n_components):\n\t\ta, b = U[:,j], VT[j,:]\n\n\t\ta_p, b_p = maximum(a, 0), maximum(b, 0)\n\t\ta_n, b_n = abs(minimum(a, 0)), abs(minimum(b, 0))\n\n\t\ta_p_norm, b_p_norm = norm(a_p), norm(b_p)\n\t\ta_n_norm, b_n_norm = norm(a_n), norm(b_n)\n\n\t\tm_p, m_n = a_p_norm * b_p_norm, a_n_norm * b_n_norm\n\n\t\t# Update\n\t\tif m_p > m_n:\n\t\t\ta_p /= a_p_norm\n\t\t\tb_p /= b_p_norm\n\t\t\tu,v,sigma = a_p, b_p, m_p\n\t\telse:\n\t\t\ta_n /= a_n_norm\n\t\t\tb_n /= b_n_norm\n\t\t\tu,v,sigma = a_n, b_n, m_n\n\n\t\tlbd = (S[j] * sigma)**0.5\n\t\tW[:,j], H[j,:] = lbd*u, lbd*v\n\n\tW, H = maximum(W, 0), maximum(H, 0)\n\tif HT:\n\t\treturn W, H.T.copy(), X\n\treturn W, H, X\n\n\n\ndef update_CD_base(W, HHT, XHT, n, k, runs = 1):\n\tviolation = 0\n\tXHT *= -1\n\t\n\tfor t in prange(k):\n\t\t# Hessian\n\t\tH_part = HHT[t]\n\t\thess = H_part[t]\n\n\t\tif hess == 0:\n\t\t\tfor run in range(runs):\n\t\t\t\tfor i in prange(n):\n\t\t\t\t\tW_i = W[i]\n\t\t\t\t\tW_it = W_i[t]\n\t\t\t\t\t# gradient = GW[t, i] where GW = np.dot(W, HHt) - XHt\n\t\t\t\t\tgrad = XHT[i, t]\n\n\t\t\t\t\tfor r in prange(k): grad += H_part[r] * W_i[r]\n\t\t\t\t\t\n\t\t\t\t\t# projected gradient\n\t\t\t\t\tpg = _min(0., grad) if W_it == 0 else grad\n\t\t\t\t\tviolation += abs(pg)\n\n\t\telse:\n\t\t\tfor run in range(runs):\n\t\t\t\tfor i in prange(n):\n\t\t\t\t\tW_i = W[i]\n\t\t\t\t\tW_it = W_i[t]\n\t\t\t\t\t# gradient = GW[t, i] where GW = np.dot(W, HHt) - XHt\n\t\t\t\t\tgrad = XHT[i, t]\n\n\t\t\t\t\tfor r in prange(k): grad += H_part[r] * W_i[r]\n\t\t\t\t\t\n\t\t\t\t\t# projected gradient\n\t\t\t\t\tpg = _min(0., grad) if W_it == 0 else grad\n\t\t\t\t\tviolation += abs(pg)\n\t\t\t\t\t\n\t\t\t\t\tif grad != 0:\n\t\t\t\t\t\tW[i, t] = _max(W_it - grad / hess, 0.)\n\treturn violation\nupdate_CD = njit(update_CD_base, fastmath = True, nogil = True, cache = True)\nupdate_CD_parallel = njit(update_CD_base, fastmath = True, nogil = True, parallel = True)\n\n\n\ndef nmf_cd(X, n_components = 2, tol = 1e-4, max_iter = 200, init = 'nndsvd', speed = 1, n_jobs = 1):\n\tW, HT, X = intialize_NMF(X, n_components)\n\n\tXT = X.T\n\tn,k = W.shape\n\tp,k = HT.shape\n\n\tupdate_CD_i = update_CD_parallel if n_jobs != 1 else update_CD\n\n\tif speed != 1: \n\t\tmax_iter = _min(int(200/speed*1.5), 5)\n\n\tfor n_iter in range(max_iter):\n\t\t# Update W\n\t\t#HHT = reflect()\n\t\tviolation = update_CD_i(W, HT.T@HT, X@HT, n, k, speed)\n\t\t# Update H\n\t\tviolation += update_CD_i(HT, W.T@W, XT@W, p, k, speed)\n\t\t#loss.append(squareSum(X - W@HT.T).sum())\n\t\t\n\t\tif n_iter == 0:\n\t\t\tviolation_init = violation\n\n\t\tif violation_init == 0:\n\t\t\tbreak\n\n\t\tif violation / violation_init <= tol:\n\t\t\tbreak\n\treturn W, HT.T\n\n\n\ndef nmf_als(X, n_components = 2, max_iter = 100, init = 'nndsvd', alpha = None):\n\tW, H, X = intialize_NMF(X, n_components, HT = False)\n\tXT = X.T\n\tn = X.shape[0]\n\tpast_error = 1e100\n\n\tfor i in range(max_iter):\n\t\tH = maximum(solveCholesky(W, X, alpha = alpha), 0)\n\t\tW = maximum(solveCholesky(H.T, XT, alpha = alpha), 0).T\n\t\tif i % 10 == 0:\n\t\t\terror = squaresum(X - W@H)/n\n\t\t\tif error/past_error > 0.9:\n\t\t\t\tbreak\n\t\t\tpast_error = error\n\treturn W, H\n\n\n\n_X = zeros((2,2), float32)\n_XX = nmf_cd(_X, 1, max_iter = 1)\n_X = nmf_cd(_X, 1, max_iter = 1, n_jobs = -1)\n_X = zeros((2,2), float64)\n_XX = nmf_cd(_X, 1, max_iter = 1)\n_X = nmf_cd(_X, 1, max_iter = 1, n_jobs = -1)\n_X = None\n_XX = None\n"""
hyperlearn/decomposition/PCA.py,0,"b'\nfrom .base import _basePCA\nfrom ..base import *\nfrom ..linalg import svd, eigh, svd_flip\n\n\nclass PCA(_basePCA):\n    """"""\n    Principal Component Analysis reduces the dimensionality\n    of data by selecting the linear combinations of columns\n    or features which maximises the total variance.\n    \n    Speed\n    --------------\n    Note, SKLEARN\'s implementation is wasteful, as it uses\n    a full SVD solver, which is slow and painful.\n    \n    HyperLearn\'s implementation uses regularized Eigenvalue Decomposition,\n    but if solver = \'svd\', then full SVD is used.\n    \n    If USE_GPU:\n        Uses PyTorch\'s SVD (which is slow sadly), or EIGH. Speed is OK.\n    If CPU:\n        Uses Numpy\'s Fortran C based SVD or EIGH.\n        If NUMBA is not installed, uses very fast LAPACK functions.\n    \n    Stability\n    --------------\n    Alpha is added for regularization purposes. This prevents system\n    rounding errors and promises better convergence rates.\n    \n    Note svd_flip is NOT same as SKLEARN, hence output may have reversed\n    signs. V based decision is used as EIGH is faster, and U is not computed.\n    """"""\n    def __init__(self, n_components = 2, solver = \'eig\', alpha = None, fast = True,\n                centre = True):\n        self.decomp = self._fit_svd if solver == \'svd\' else self._fit_eig\n        \n        self.n_components, self.alpha, self.solver, self.fast, self.truncated, \\\n        self.centre    = n_components, alpha, solver, fast, False, centre\n        \n        \n    def _fit_svd(self, X):        \n        S2, VT = eig(X, fast = self.fast)\n        return S2, VT\n        \n        \n    def _fit_eig(self, X):\n        if X.shape[1] >= X.shape[0]:\n            # Drop back to SVD, as Eigendecomp would output U and not VT.\n            return self._fit_svd(X)\n        else:\n            S2, VT = eigh(X.T @ X, alpha = self.alpha, fast = self.fast,\n                            positive = True)\n            return S2, VT\n'"
hyperlearn/decomposition/__init__.py,0,"b""\nfrom .PCA import PCA\n\n__all__ = ['PCA']"""
hyperlearn/decomposition/base.py,0,"b'\nfrom sklearn.externals import six\nfrom abc import ABCMeta, abstractmethod\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom ..numba import mean\nfrom ..utils import _float\n\n\nclass _basePCA(six.with_metaclass(ABCMeta, BaseEstimator, TransformerMixin)):\n    """"""\n    Base PCA class. Do not use this for applications.\n    This is just a class wrapper for the other classes.\n\n    Implements TRANSFORM, INVERSE_TRANSFORM, FIT\n    """"""\n    def transform(self, X):\n        if hasattr(self, \'singular_values_\'):\n            if self.centre: X -= self.mean_\n                \n            X_transformed = X @ self.components_.T\n            \n            if self.centre: X += self.mean_\n            return X_transformed\n        else:\n            print(\'PCA has not been fit. Call .fit(X)\')\n            \n\n    def inverse_transform(self, X_transformed):\n        if hasattr(self, \'singular_values_\'):\n            X = X_transformed @ self.components_\n            if self.centre: X += self.mean_\n            return X\n        else:\n            print(\'PCA has not been fit. Call .fit(X)\')\n            \n            \n    def _store_components(self, S2, VT):\n        explained_variance_ = S2 / (self.n - 1)\n        total_var = explained_variance_.sum()\n\n        k = self.n_components\n        self.components_ = VT[:k]\n        self.explained_variance_ = explained_variance_[:k]\n        self.explained_variance_ratio_ = explained_variance_[:k] / total_var\n        self.singular_values_ = S2[:k]**0.5\n        \n        \n    def fit(self, X):\n        self.n, self.p = X.shape\n        if self.centre: \n            self.mean_ = mean(X)\n            X -= self.mean_  # Inplace mean removal\n            \n        S2, VT = self.decomp(X)\n        self._store_components(S2, VT)\n        \n        if self.centre: X += self.mean_  # Recover old X\n        return self\n'"
hyperlearn/discriminant_analysis/QDA.py,0,"b'\nfrom .multiprocessing import Parallel_Reference\nfrom .base import y_count\nfrom ..base import *\nfrom ..linalg import *\n\n\nclass QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):\n\n\tdef __init__(self, reg_param = 0.001, store_covariance = False,\n\t\t\t\tn_jobs = 1):\n\n\t\tassert type(reg_param) in FloatInt\n\t\tassert type(store_covariance) is bool\n\t\tassert type(n_jobs) is int\n\n\t\tself.reg_param = reg_param\n\t\tself.store_covariance = store_covariance\n\t\tself.n_jobs = n_jobs\n\n\t\tself.scalings_, self.rotations_, self.means_ = [], [], []\n\t\tself.log_scalings_, self.covariance_, self.scaled_rotations_ = [], [], []\n\n\n\t@n2t\n\tdef fit(self, X, y):\n\n\t\tr_1 = 1-self.reg_param\n\t\tY = row_np(y)\n\n\t\tself.classes_, self.priors_ = y_count(Y)\n\t\tself.priors_ /= X.shape[0]\n\n\t\tself.scalings_, self.log_scalings_, self.rotations_, \\\n\t\tself.means_, self.covariance_, self.scaled_rotations_ = \\\n\t\t\\\n\t\t\tParallel_Reference(QuadraticDiscriminantAnalysis_partial_fit,\n\t\t\t\tn_jobs = self.n_jobs, reference = 2)(\n\t\t\t\tX, Y, self.classes_, self.reg_param, r_1, \\\n\t\t\t\tself.store_covariance)\n\n\t\tif not self.store_covariance: self.covariance_ = []\n\n\t\tself.log_scalings_ = stack(self.log_scalings_)\n\t\treturn self\n\n\n\t@n2t\n\tdef decision_function(self, X):\n\t\tdistances = []\n\n\t\tfor VS, means in zip(self.scaled_rotations_, self.means_):\n\t\t\tpartial_X = (X - means).matmul(   VS   )\n\n\t\t\tdistances.append(  squareSum(partial_X) )\n\t\t\t#distances.append(  (partial_X**2).sum(1)  )\n\n\t\tdistances = T( stack(distances) )\n\t\tdecision = -0.5 * (distances + self.log_scalings_) + self.priors_\n\t\treturn decision\n\n\n\tdef predict_proba(self, X):\n\n\t\tdecision = self.decision_function(X)\n\n\t\tlikelihood = (decision - T(decision.max(1)[0])).exp()\n\t\tsum_softmax = T(  rowSum(likelihood)  )\n\t\t#sum_softmax = T(likelihood.sum(1))\n\t\tsoftmax = likelihood / sum_softmax\n\n\t\treturn softmax.numpy()\n\n\n\tdef predict_log_proba(self, X):\n\n\t\tprobas_ = self.predict_proba(X)\n\t\treturn np_log(probas_)\n\n\n\tdef predict(self, X):\n\n\t\tdecision = self.decision_function(X).argmax(1)\n\t\ty_hat = self.classes_.take(decision)\n\n\t\treturn y_hat\n\n\ndef QuadraticDiscriminantAnalysis_partial_fit(\n\tX, Y, x, reg_param, r_1, store_covariance):\n\n\tpartial_X = X[toTensor(Y == x)]\n\tpartial_mean = partial_X.mean(0)\n\tpartial_X -= partial_mean\n\n\tS, VT = _svd(partial_X, U = False)\n\tV = T(VT)\n\tscale = (S**2) / (len(partial_X) -1)\n\n\tscale = reg_param + (r_1 * scale)\n\n\tpartial_cov = None\n\tif store_covariance:\n\t\tpartial_cov = (V * scale).matmul(VT)\n\n\tscalings_ = scale\n\tlog_scalings_ = scale.log().sum()\n\t#rotations_ = V\n\tmeans_ = partial_mean\n\tscaled_rotations_ = V / scale**0.5\n\n\treturn scalings_, log_scalings_, None, means_, partial_cov, scaled_rotations_\n\n'"
hyperlearn/discriminant_analysis/__init__.py,0,"b""\nfrom .QDA import QuadraticDiscriminantAnalysis\n\n__all__ = ['QuadraticDiscriminantAnalysis']"""
hyperlearn/discriminant_analysis/base.py,0,"b'\nfrom collections import Counter\nfrom ..base import toTensor, array\n\ndef y_count(y):\n\tcounts = Counter(y)\n\tclasses = array(counts.keys())\n\tcounts = toTensor(counts.values()).type(float32)\n\treturn classes, counts\n\n'"
hyperlearn/impute/SVDImpute.py,0,"b'\nfrom numpy import nanmean, nanstd, log1p, isnan, sqrt, nanmin, nan\nfrom ..numba import minimum\nfrom ..linalg import eig\nfrom ..big_data.randomized import randomizedEig\nfrom ..big_data.incremental import partialSVD\n\n\ndef fit(X, n_components = \'auto\', standardise = True, copy = True):\n\t""""""\n\t[Added 31/10/2018] [Edited 2/11/2018 Fixed SVDImpute]\n\n\tFits a SVD onto the training data by projecting it to a lower space after\n\tbeing intially filled with column means. By default, n_components is\n\tdetermined automatically using log(p+1). Setting too low or too high mirrors\n\tmean imputation, and deletes the purpose of SVD imputation.\n\n\tReturns:\n\t1. S \tsingular values\n\t2. VT \teigenvectors\n\t+ mean, std, mins\n\t""""""\n\tn, p = X.shape\n\tk = int(sqrt(p)-1) if n_components in (\'auto\', None) else n_components        \n\tif k <= 0: k = 1\n\tif k >= p: k = p\n\n\tC = X.copy() if copy else X\n\tmask = isnan(X)\n\n\tif standardise:\n\t\tmean = nanmean(X, 0)\n\t\tstd = nanstd(X, 0)\n\t\tmins = nanmin(X, 0)\n\t\tstd[std == 0] = 1\n\t\tC -= mean\n\t\tC /= std\n\telse:\n\t\tmean, std, mins = None, None, None\n\tC[mask] = 0\n\n\tS, VT = randomizedEig(C, k)\n\tS **= 0.5\n\tVT = VT.T\n\n\tif copy == False:\n\t\tC[mask] = nan\n\t\tif standardise:\n\t\t\tC *= std\n\t\t\tC += mean\n\treturn S, VT, mean, std, mins, standardise\n\n\ndef transform(X, S, VT, mean, std, mins, standardise, copy = True):\n\t""""""\n\t[Added 31/10/2018] [Edited 2/11/2018 FIxed SVDImpute]\n\n\tThe fundamental advantage of HyperLearn\'s SVD imputation is that a .transform\n\tmethod is provided. I do not require seeing the whole matrix for imputation,\n\tand can calculate SVD incrementally via the Incremental Module.\n\t""""""\n\tn, p = X.shape\n\tD = X.copy() if copy else X\n\tmask = isnan(X)\n\n\tif standardise:\n\t\tD -= mean\n\t\tD /= std\n\tD[mask] = 0\n\t\n\tU, S, VT = partialSVD(D, S, VT, solver = \'randomized\')\n\treconstruction = U * S @ VT\n\tD[mask] = reconstruction[mask]\n\n\tif standardise:\n\t\tD *= std\n\t\tD += mean\n\t\tfor j in range(p):\n\t\t\tmin_ = mins[j]\n\t\t\twhat = D[:,j]\n\t\t\twhat[what < min_] = min_\n\n\tif copy == False:\n\t\tX[mask] = nan\n\t\tif standardise:\n\t\t\tX *= std\n\t\t\tX += mean\n\treturn D\n'"
hyperlearn/metrics/cosine.py,0,"b'\n\nfrom ..utils import _XXT, rowSum, reflect, setDiagonal\nfrom numpy import zeros, newaxis\nfrom numba import njit, prange\nfrom ..sparse.csr import div_1 ,mult_1, _XXT as _XXT_sparse, rowSum as rowSum_sparse\nfrom ..sparse.tcsr import _XXT as _XXT_triangular\n\n\n\ndef cosine_sim_triangular(N, D):\n\t""""""\n\t[Added 21/10/2018]\n\tQuickly performs X / norm_rows / norm_rows.T on the TCSR matrix.\n\t""""""\n\tn = len(N)\n\tmove = 0\n\t\n\t# loop *-2 and adds S[:, newaxis]\n\tfor i in prange(n-1):\n\t\ti1 = i+1\n\t\t\n\t\tleft = i*i1 // 2\n\t\ts = N[i1]\n\t\tfor j in range(left, left+i1):\t\t\t\n\t\t\t# div N[:, newaxis]\n\t\t\tD[j] /= s\n\t\n\t# loop div N[newaxis, :]\n\tfor a in prange(n-1):\n\t\ts = N[a]\n\n\t\tfor b in range(a, n-1):\n\t\t\t# div N[newaxis, :] or N\n\t\t\tc = b*(b+1) // 2 + a\n\t\t\tD[c] /= s\n\treturn D\ncosine_sim_triangular_single = njit(cosine_sim_triangular, fastmath = True, nogil = True, cache = True)\ncosine_sim_triangular_parallel = njit(cosine_sim_triangular, fastmath = True, nogil = True, parallel = True)\n\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef cosine_dis(XXT):\n\t""""""\n\t[Added 22/10/2018]\n\tPerforms XXT*-1 + 1 quickly on the lower triangular part.\n\t""""""\n\tn = len(XXT)\n\tfor i in range(n):\n\t\tfor j in range(i):\n\t\t\tXXT[i, j] *= -1\n\t\t\tXXT[i, j] += 1\n\treturn XXT\n\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef cosine_dis_triangular(D):\n\t""""""\n\t[Added 22/10/2018]\n\tPerforms XXT*-1 + 1 quickly on the TCSR.\n\t""""""\n\tD *= -1\n\tD += 1\n\treturn D\n\n\n\ndef cosine_similarity(X, Y = None, triangular = False, n_jobs = 1, copy = False):\n\t""""""\n\t[Added 20/10/2018] [Edited 22/201/2018]\n\t[Edited 22/10/2018 Added Y option]\n\tNote: when using Y, speed improvement is approx 5% only from Sklearn.\n\n\tCosine similarity is approx the same speed as Sklearn, but uses approx 10%\n\tless memory. One clear advantage is if you set triangular to TRUE, then it\'s faster.\n\t""""""\n\tnorm_rows = rowSum(X, norm = True)\n\n\tif Y is X:\n\t\t# Force algo to be triangular cosine rather than normal CS.\n\t\tY = None\n\n\tif Y is None:\n\t\tif copy:\n\t\t\tXXT = _XXT(X.T)\n\t\t\tXXT /= norm_rows[:, newaxis]\n\t\t\tXXT /= norm_rows #[newaxis, :]\n\t\telse:\n\t\t\tXXT = _XXT(  (X/norm_rows[:, newaxis]).T  )\n\n\t\tif not triangular:\n\t\t\tXXT = reflect(XXT, n_jobs)\n\n\t\t# diagonal is set to 1\n\t\tsetDiagonal(XXT, 1)\n\t\treturn XXT\n\telse:\n\t\tD = X @ Y.T\n\t\tD /= norm_rows[:, newaxis]\n\t\tD /= rowSum(Y, norm = True)\n\t\treturn D\n\n\n\ndef cosine_similarity_sparse(val, colPointer, rowIndices, n, p, triangular = False, dense_output = True,\n\tn_jobs = 1, copy = True):\n\t""""""\n\t[Added 20/10/2018] [Edited 21/10/2018]\n\tSlightly faster than Sklearn\'s Cosine Similarity implementation.\n\n\tIf dense_output is set to FALSE, then a TCSR Matrix (Triangular CSR Matrix) is\n\tprovided and not a CSR matrix. This has the advantage of using only 1/2n^2 - n\n\tmemory and not n^2 memory.\n\t""""""\n\tnorm_rows = rowSum_sparse(val, colPointer, rowIndices, norm = True)\n\n\tif dense_output:\n\t\tif copy:\n\t\t\tXXT = _XXT_sparse(val, colPointer, rowIndices, n, p, n_jobs)\n\t\t\tXXT /= norm_rows[:, newaxis]\n\t\t\tXXT /= norm_rows #[newaxis, :]\n\t\telse:\n\t\t\tval = div_1(val, colPointer, rowIndices, norm_rows, n, p, copy = False)\n\t\t\tXXT = _XXT_sparse(val, colPointer, rowIndices, n, p, n_jobs)\n\t\t\tval = mult_1(val, colPointer, rowIndices, norm_rows, n, p, copy = False)\n\n\t\tif not triangular: \n\t\t\tXXT = reflect(XXT, n_jobs)\n\n\t\t# diagonal is set to 1\n\t\tsetDiagonal(XXT, 1)\n\telse:\n\t\tXXT = _XXT_triangular(val, colPointer, rowIndices, n, p, n_jobs)\n\n\t\tXXT = cosine_triangular_parallel(norm_rows, XXT) if n_jobs != 1 else \\\n\t\t\tcosine_triangular_single(norm_rows, XXT)\n\treturn XXT\n\n\n\ndef cosine_distances(X, Y = None, triangular = False, n_jobs = 1, copy = False):\n\t""""""\n\t[Added 15/10/2018] [Edited 18/10/2018]\n\t[Edited 22/10/2018 Added Y option]\n\tNote: when using Y, speed improvement is approx 5-10% only from Sklearn.\n\n\tSlightly faster than Sklearn\'s Cosine Distances implementation.\n\tIf you set triangular to TRUE, the result is much much faster.\n\t(Approx 50% faster than Sklearn)\n\t""""""\n\tnorm_rows = rowSum(X, norm = True)\n\n\tif Y is X:\n\t\t# Force algo to be triangular cosine rather than normal CS.\n\t\tY = None\n\n\tif Y is None:\n\t\tif copy:\n\t\t\tXXT = _XXT(X.T)\n\t\t\tXXT /= norm_rows[:, newaxis]\n\t\t\tXXT /= norm_rows #[newaxis, :]\n\t\telse:\n\t\t\tXXT = _XXT(  (X/norm_rows[:, newaxis]).T  )\n\n\t\t# XXT*-1 + 1\n\t\tXXT = cosine_dis(XXT)\n\n\t\tif not triangular:\n\t\t\tXXT = reflect(XXT, n_jobs)\n\n\t\t# diagonal is set to 0 as zero distance between row i and i\n\t\tsetDiagonal(XXT, 0)\n\t\treturn XXT\n\telse:\n\t\tD = X @ Y.T\n\t\tD /= norm_rows[:, newaxis]\n\t\tD /= rowSum(Y, norm = True)\n\t\tD *= -1\n\t\tD += 1\n\t\treturn D\n\n\n\ndef cosine_distances_sparse(val, colPointer, rowIndices, n, p, triangular = False, dense_output = True,\n\tn_jobs = 1, copy = True):\n\t""""""\n\t[Added 22/10/2018]\n\tSlightly faster than Sklearn\'s Cosine Distances implementation.\n\n\tIf dense_output is set to FALSE, then a TCSR Matrix (Triangular CSR Matrix) is\n\tprovided and not a CSR matrix. This has the advantage of using only 1/2n^2 - n\n\tmemory and not n^2 memory.\n\t""""""\n\tnorm_rows = rowSum_sparse(val, colPointer, rowIndices, norm = True)\n\n\tif dense_output:\n\t\tif copy:\n\t\t\tXXT = _XXT_sparse(val, colPointer, rowIndices, n, p, n_jobs)\n\t\t\tXXT /= norm_rows[:, newaxis]\n\t\t\tXXT /= norm_rows #[newaxis, :]\n\t\telse:\n\t\t\tval = div_1(val, colPointer, rowIndices, norm_rows, n, p, copy = False)\n\t\t\tXXT = _XXT_sparse(val, colPointer, rowIndices, n, p, n_jobs)\n\t\t\tval = mult_1(val, colPointer, rowIndices, norm_rows, n, p, copy = False)\n\n\t\t# XXT*-1 + 1\n\t\tXXT = cosine_dis(XXT)\n\n\t\tif not triangular: \n\t\t\tXXT = reflect(XXT, n_jobs)\n\n\t\t# diagonal is set to 0 as zero distance between row i and i\n\t\tsetDiagonal(XXT, 0)\n\telse:\n\t\tXXT = _XXT_triangular(val, colPointer, rowIndices, n, p, n_jobs)\n\n\t\t# XXT*-1 + 1\n\t\tXXT = cosine_dis_triangular(XXT)\n\n\t\tXXT = cosine_triangular_parallel(norm_rows, XXT) if n_jobs != 1 else \\\n\t\t\tcosine_triangular_single(norm_rows, XXT)\n\treturn XXT'"
hyperlearn/metrics/euclidean.py,0,"b'\nfrom ..utils import _XXT, rowSum, reflect\nfrom numpy import zeros, newaxis\nfrom numba import njit, prange\nfrom ..sparse.csr import _XXT as _XXT_sparse, rowSum as rowSum_sparse\nfrom ..sparse.tcsr import _XXT as _XXT_triangular\nfrom ..numba import maximum\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef mult_minus2(XXT):\n\t""""""\n\t[Added 17/10/2018]\n\tQuickly multiplies XXT by -2. Uses notion that XXT is symmetric,\n\thence only lower triangular is multiplied.\n\t""""""\n\tn = len(XXT)\n\tfor i in range(n):\n\t\tfor j in range(i):\n\t\t\tXXT[i, j] *= -2\n\treturn XXT\n\n\ndef _maximum0(XXT, squared = True):\n\t""""""\n\t[Added 15/10/2018] [Edited 21/10/2018]\n\tComputes maxmimum(XXT, 0) faster. Much faster than Sklearn since uses \n\tthe notion that\tdistance(X, X) is symmetric.\n\n\tSteps:\n\t\tmaximum(XXT, 0)\n\t\t\tOptimised. Instead of n^2 operations, does n(n-1)/2 operations.\n\t""""""\n\tn = len(XXT)\n\t\n\tfor i in prange(n):\n\t\tXXT[i, i] = 0\n\t\tfor j in range(i):\n\t\t\tif XXT[i, j] < 0:\n\t\t\t\tXXT[i, j] = 0\n\t\t\tif not squared:\n\t\t\t\tXXT[i, j] **= 0.5\n\t\t\t\t\n\treturn XXT\nmaximum0 = njit(_maximum0, fastmath = True, nogil = True, cache = True)\nmaximum0_parallel = njit(_maximum0, fastmath = True, nogil = True, parallel = True)\n\n\n\ndef euclidean_triangular(S, D, squared = False):\n\t""""""\n\t[Added 21/10/2018]\n\tQuickly performs -2D + X^2 + X.T^2 on the TCSR matrix.\n\tAlso applies maximum(D, 0) and then square roots distances\n\tif required.\n\t""""""\n\t# Apply -2*D and add row-wise rowSum\n\tn = len(S)\n\tmove = 0\n\t\n\t# loop *-2 and adds S[:, newaxis]\n\tfor i in prange(n-1):\n\t\ti1 = i+1\n\t\t\n\t\tleft = i*i1 // 2\n\t\ts = S[i1]\n\t\tfor j in range(left, left+i1):\n\t\t\t# mult by -2\n\t\t\tD[j] *= -2\n\t\t\t\n\t\t\t# add S[:, newaxis]\n\t\t\tD[j] += s\n\t\n\t# loop adds S[newaxis, :]\n\tfor a in prange(n-1):\n\t\ts = S[a]\n\n\t\tfor b in range(a, n-1):\n\t\t\t# add S[newaxis, :] or S\n\t\t\tc = b*(b+1) // 2 + a\n\t\t\tD[c] += s\n\t\t\t\n\t\t\t# maximum(D, 0)\n\t\t\tif D[c] < 0:\n\t\t\t\tD[c] = 0\n\t\t\tif not squared:\n\t\t\t\tD[c] **= 0.5\n\treturn D\neuclidean_triangular_single = njit(euclidean_triangular, fastmath = True, nogil = True, cache = True)\neuclidean_triangular_parallel = njit(euclidean_triangular, fastmath = True, nogil = True, parallel = True)\n\n\n\ndef euclidean_distances(X, Y = None, triangular = False, squared = False, n_jobs = 1):\n\t""""""\n\t[Added 15/10/2018] [Edited 16/10/2018]\n\t[Edited 22/10/2018 Added Y option]\n\tNotice: parsing in Y will result in only 10% - 15% speed improvement, not 30%.\n\n\tMuch much faster than Sklearn\'s implementation. Approx not 30% faster. Probably\n\teven faster if using n_jobs = -1. Uses the idea that distance(X, X) is symmetric,\n\tand thus algorithm runs only on 1/2 triangular part.\n\n\tOld complexity:\n\t\tX @ XT \t\t\tn^2p\n\t\trowSum(X^2)\t\tnp\t\n\t\tXXT*-2\t\t\tn^2\n\t\tXXT+X^2\t\t\t2n^2\n\t\tmaximum(XXT,0)\tn^2\n\t\t\t\t\t\tn^2p + 4n^2 + np\n\tNew complexity:\n\t\tsym X @ XT \t\tn^2p/2\n\t\trowSum(X^2)\t\tnp\t\n\t\tsym XXT*-2\t\tn^2/2\t\n\t\tsym XXT+X^2\t\tn^2\n\t\tmaximum(XXT,0)\tn^2/2\n\t\t\t\t\t\tn^2p/2 + 2n^2 + np\n\n\tSo New complexity approx= 1/2(Old complexity)\n\t""""""\n\tS = rowSum(X)\n\tif Y is X:\n\t\t# if X == Y, then defaults to fast triangular L2 distance algo\n\t\tY = None\n\n\tif Y is None:\n\t\tXXT = _XXT(X.T)\n\t\tXXT = mult_minus2(XXT)\n\t\t\n\t\tXXT += S[:, newaxis]\n\t\tXXT += S #[newaxis,:]\n\t\t\n\t\tD = maximum0_parallel(XXT, squared) if n_jobs != 1 else maximum0(XXT, squared)\n\t\tif not triangular: \n\t\t\tD = reflect(XXT, n_jobs)\n\telse:\n\t\tD = X @ Y.T\n\t\tD *= -2\n\t\tD += S[:, newaxis]\n\t\tD += rowSum(Y)\n\t\tD = maximum(D, 0)\n\t\tif not squared:\n\t\t\tD **= 0.5\n\treturn D\n\n\ndef euclidean_distances_sparse(val, colPointer, rowIndices, n, p, triangular = False, dense_output = True,\n\tsquared = False, n_jobs = 1):\n\t""""""\n\t[Added 15/10/2018] [Edited 21/10/2018]\n\tMuch much faster than Sklearn\'s implementation. Approx not 60% faster. Probably\n\teven faster if using n_jobs = -1 (actually 73% faster). [n = 10,000 p = 1,000]\n\tUses the idea that distance(X, X) is symmetric,\tand thus algorithm runs only on \n\t1/2 triangular part. Also notice memory usage is now 60% better than Sklearn.\n\n\tIf dense_output is set to FALSE, then a TCSR Matrix (Triangular CSR Matrix) is\n\tprovided and not a CSR matrix. This has the advantage of using only 1/2n^2 - n\n\tmemory and not n^2 memory.\n\n\tOld complexity:\n\t\tX @ XT \t\t\tn^2p\n\t\trowSum(X^2)\t\tnp\t\n\t\tXXT*-2\t\t\tn^2\n\t\tXXT+X^2\t\t\t2n^2\n\t\tmaximum(XXT,0)\tn^2\n\t\t\t\t\t\tn^2p + 4n^2 + np\n\tNew complexity:\n\t\tsym X @ XT \t\tn^2p/2\n\t\trowSum(X^2)\t\tnp\t\n\t\tsym XXT*-2\t\tn^2/2\t\n\t\tsym XXT+X^2\t\tn^2\n\t\tmaximum(XXT,0)\tn^2/2\n\t\t\t\t\t\tn^2p/2 + 2n^2 + np\n\n\tSo New complexity approx= 1/2(Old complexity)\n\t""""""\n\tif dense_output:\n\t\tXXT = _XXT_sparse(val, colPointer, rowIndices, n, p, n_jobs)\n\n\t\tXXT = mult_minus2(XXT)\n\t\tS = rowSum_sparse(val, colPointer, rowIndices)\n\n\t\tXXT += S[:, newaxis]\n\t\tXXT += S #[newaxis,:]\n\t\t\n\t\tXXT = maximum0_parallel(XXT, squared) if n_jobs != 1 else maximum0(XXT, squared)\n\t\tif not triangular: \n\t\t\tXXT = reflect(XXT, n_jobs)\n\telse:\n\t\tXXT = _XXT_triangular(val, colPointer, rowIndices, n, p, n_jobs)\n\t\tS = rowSum_sparse(val, colPointer, rowIndices)\n\n\t\tXXT = euclidean_triangular_parallel(S, XXT, squared = squared) if n_jobs != 1 else \\\n\t\t\teuclidean_triangular_single(S, XXT, squared = squared)\n\n\treturn XXT\n\n'"
hyperlearn/metrics/pairwise.py,0,b'\n'
hyperlearn/sparse/base.py,0,"b'\n\nfrom numpy import uint8, uint16, uint32, uint64, float32, float64\nfrom numpy import zeros, int8, int16, int32, int64, ndim\nfrom warnings import warn as Warn\nfrom numba import njit, prange\n\n\ndef getDtype(p, size, uint = True):\n\t""""""\n\tComputes the exact best possible data type for CSR Matrix\n\tcreation.\n\t""""""\n\tp = int(1.25*p) # Just in case\n\tif uint:\n\t\tdtype = uint64\n\t\tif uint8(p) == p: dtype = uint8\n\t\telif uint16(p) == p: dtype = uint16\n\t\telif uint32(p) == p: dtype = uint32\n\t\treturn zeros(size, dtype = dtype)\n\telse:\n\t\tdtype = int64\n\t\tif int8(p) == p: dtype = int8\n\t\telif int16(p) == p: dtype = int16\n\t\telif int32(p) == p: dtype = int32\n\t\treturn zeros(size, dtype = dtype)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef determine_nnz(X, rowCount):\n\t""""""\n\tUses close to no memory at all when computing how many non\n\tzeros are in the matrix. Notice the difference with Scipy\n\tis HyperLearn does NOT use nonzero(). This reduces memory\n\tusage dramatically.\n\t""""""\n\tnnz = 0\n\tn,p = X.shape\n\t\n\tfor i in range(n):\n\t\tcurrNNZ = 0\n\t\tXi = X[i]\n\t\tfor j in range(p):\n\t\t\tif Xi[j] != 0:\n\t\t\t\tcurrNNZ += 1\n\t\tnnz += currNNZ\n\t\trowCount[i] = currNNZ\n\treturn rowCount, nnz\n\n\ndef create_csr(X, rowCount, nnz, temp):\n\t""""""\n\t[Added 10/10/2018] [Edited 13/10/2018]\n\tBefore used extra memory keeping a Boolean Matrix (np bytes) and a\n\tColIndex pointer which used p memory. Now, removed (np + p) memory usage,\n\tmeaning larger matrices can be handled.\n\n\tAlgorithm is 3 fold:\n\n\t1. Create RowIndices\n\t2. For every row in data:\n\t\t3. Store until a non 0 is seen.\n\n\tAlgorithm takes approx O(n + np) time, which is similar to Scipy\'s.\n\tThe only difference is now, parallelisation is possible, which can\n\tcut the time to approx O(n + np/c) where c = no of threads\n\t""""""\n\tn = X.shape[0]\n\tval = zeros(nnz, dtype = X.dtype)\n\trowIndices = zeros(n+1, dtype = temp.dtype)\n\tcolPointer = zeros(nnz, dtype = rowCount.dtype)\n\t\n\tp = X.shape[1]\n\t\n\tk = 0\n\tfor i in range(n):\n\t\ta = rowCount[i]\n\t\trowIndices[i] += k\n\t\tk += a\n\trowIndices[n] = nnz\n\n\tfor i in prange(n):\n\t\tXi = X[i]\n\t\tleft = rowIndices[i]\n\t\tright = rowIndices[i+1]\n\t\t\n\t\tk = 0\n\t\tfor j in range(left, right):\n\t\t\twhile Xi[k] == 0:\n\t\t\t\tk += 1\n\t\t\tval[j] = Xi[k]\n\t\t\tcolPointer[j] = k\n\t\t\tk += 1\n\t\n\treturn val, colPointer, rowIndices\ncreate_csr_cache = njit(create_csr, fastmath = True, nogil = True, cache = True)\ncreate_csr_parallel = njit(create_csr, fastmath = True, nogil = True, parallel = True)\n\n\n\ndef CreateCSR(X, n_jobs = 1):\n\t""""""\n\t[Added 10/10/2018] [Edited 13/10/2018]\n\tMuch much faster than Scipy. In fact, HyperLearn uses less memory,\n\tby noticing indices >= 0, hence unsigned ints are used.\n\n\tLikewise, parallelisation is seen possible with Numba with n_jobs.\n\tNotice, an error message will be provided if 20% of the data is only zeros.\n\tIt needs to be more than 20% zeros for CSR Matrix to shine.\n\t""""""\n\tn,p = X.shape\n\trowCount = getDtype(p, n)\n\n\trowCount, nnz = determine_nnz(X, rowCount)\n\n\tif nnz/(n*p) > 0.8:\n\t\tWarn(""Created sparse matrix has just under 20% zeros. Not a good idea to sparsify the matrix."")\n\n\ttemp = getDtype(nnz, 1)\n\n\tf = create_csr_cache if n_jobs == 1 else create_csr_parallel\n\treturn f(X, rowCount, nnz, temp)\n\n'"
hyperlearn/sparse/csr.py,0,"b'\nfrom numba import njit, prange\nfrom numpy import zeros, sum as _sum, array, hstack, searchsorted, ndim\nfrom .base import getDtype\n\n""""""\nCSR Matrix functions.\n\t1. sum [sum_A, sum_0, sum_1]\n\t2. mean [mean_A, mean_0, mean_1]\n\t3. mult [mult_A, mult_0, mult_1]\n\t4. div [div_A, div_0, div_1]\n\t5. add [add_A, add_0, add_1]\n\t6. min [min_A, min_0, min_1]\n\t7. max [max_A, max_0, max_1]\n\t8. diagonal [diagonal, diagonal_add]\n\t9. element [get_element]\n\t10. matmul [mat_vec, matT_vec, mat_mat]\n\t11. _XXT\n\t12. rowSum\n""""""\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef sum_A(val, colPointer, rowIndices, n, p):\n\tS = 0\n\tfor i in range(len(val)):\n\t\tS += val[i]\n\treturn S\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef sum_0(val, colPointer, rowIndices, n, p):\n\tS = zeros(p, dtype = val.dtype)\n\t\n\tfor i in range(len(val)):\n\t\tS[colPointer[i]] += val[i]\n\treturn S\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef sum_1(val, colPointer, rowIndices, n, p):\n\tS = np.zeros(n, dtype = val.dtype)\n\t\n\tfor i in range(n):\n\t\tleft = rowIndices[i]\n\t\tright = rowIndices[i+1]\n\t\tS[i] += _sum(val[left:right])\n\treturn S\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef mean_A(val, colPointer, rowIndices, n, p):\n\tS = sum_A(val, rowIndies)\n\treturn S/len(val)\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef mean_0(val, colPointer, rowIndices, n, p):\n\tA = zeros(p, dtype = val.dtype)\n\t\n\tnnz = len(val)\n\tfor i in range(nnz):\n\t\tj = colPointer[i]\n\t\tA[j] += val[i]\n\t\t\n\tfor i in range(p):\n\t\tif A[i] > 0:\n\t\t\tA[i] /= n\n\treturn A\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef mean_1(val, colPointer, rowIndices, n, p):\n\tA = zeros(n, dtype = val.dtype)\n\t\n\tfor i in range(n):\n\t\tleft = rowIndices[i]\n\t\tright = rowIndices[i+1]\n\t\tA[i] += _sum(val[left:right])\n\t\tif A[i] > 0:\n\t\t\tA[i] /= p\n\treturn A\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef add_A(val, rowIndices, addon, n, p, copy = True):\n\tV = val.copy() if copy else val\n\n\tfor i in range(len(val)):\n\t\tV[i] += addon\n\treturn V, addon\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef add_0(val, colPointer, addon, n, p, copy = True):\n\tV = val.copy() if copy else val\n\t\n\tfor i in range(len(val)):\n\t\tj = colPointer[i]\n\t\tV[i] += addon[j]\n\treturn V, addon\n\t\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef add_1(val, rowIndices, addon, n, p, copy = True):\n\tV = val.copy() if copy else val\n\n\tfor i in range(n):\n\t\tleft = rowIndices[i]\n\t\tright = rowIndices[i+1]\n\t\tfor j in range(left, right):\n\t\t\tV[j] += addon[i]\n\treturn V, addon\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef div_A(val, colPointer, rowIndices, divisor, n, p, copy = True):\n\tV = val.copy() if copy else val\n\n\tfor i in range(len(val)):\n\t\tV[i] /= divisor\n\treturn V\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef div_0(val, colPointer, rowIndices, divisor, n, p, copy = True):\n\tV = val.copy() if copy else val\n\t\n\tfor i in range(len(val)):\n\t\tj = colPointer[i]\n\t\tV[i] /= divisor[j]\n\treturn V\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef div_1(val, colPointer, rowIndices, divisor, n, p, copy = True):\n\tV = val.copy() if copy else val\n\n\tfor i in range(n):\n\t\tleft = rowIndices[i]\n\t\tright = rowIndices[i+1]\n\t\tfor j in range(left, right):\n\t\t\tV[j] /= divisor[i]\n\treturn V\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef mult_A(val, colPointer, rowIndices, mult, n, p, copy = True):\n\tV = val.copy() if copy else val\n\n\tfor i in range(len(val)):\n\t\tV[i] *= mult\n\treturn V\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef mult_0(val, colPointer, rowIndices, mult, n, p, copy = True):\n\tV = val.copy() if copy else val\n\t\n\tfor i in range(len(val)):\n\t\tj = colPointer[i]\n\t\tV[i] *= mult[j]\n\treturn V\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef mult_1(val, colPointer, rowIndices, mult, n, p, copy = True):\n\tV = val.copy() if copy else val\n\n\tfor i in range(n):\n\t\tleft = rowIndices[i]\n\t\tright = rowIndices[i+1]\n\t\tfor j in range(left, right):\n\t\t\tV[j] *= mult[i]\n\treturn V\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef min_A(val, colPointer, rowIndices, n, p):\n\tM = 0\n\tfor i in range(len(val)):\n\t\tv = V[i]\n\t\tif v < M:\n\t\t\tM = v\n\treturn M\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef min_0(val, colPointer, rowIndices, n, p):\n\tM = zeros(p, dtype = val.dtype)\n\t\n\tfor i in range(len(val)):\n\t\tv = V[i]\n\t\tcoli = colPointer[i]\n\t\tif v < M[coli]:\n\t\t\tM[coli] = v\n\treturn M\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef min_1(val, colPointer, rowIndices, n, p):\n\tM = zeros(n, dtype = val.dtype)\n\t\n\tfor i in range(n):\n\t\tleft = rowIndices[i]\n\t\tright = rowIndices[i+1]\n\n\t\tMi = M[i]\n\t\tfor j in val[left:right]:\n\t\t\tif j < Mi:\n\t\t\t\tMi = j\n\t\tM[i] = Mi\n\treturn M\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef max_A(val, colPointer, rowIndices, n, p):\n\tM = 0\n\tfor i in range(len(val)):\n\t\tv = V[i]\n\t\tif v > M:\n\t\t\tM = v\n\treturn M\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef max_0(val, colPointer, rowIndices, n, p):\n\tM = zeros(p, dtype = val.dtype)\n\t\n\tfor i in range(len(val)):\n\t\tv = V[i]\n\t\tcoli = colPointer[i]\n\t\tif v > M[coli]:\n\t\t\tM[coli] = v\n\treturn M\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef max_1(val, colPointer, rowIndices, n, p):\n\tM = zeros(n, dtype = val.dtype)\n\t\n\tfor i in range(n):\n\t\tleft = rowIndices[i]\n\t\tright = rowIndices[i+1]\n\n\t\tMi = M[i]\n\t\tfor j in val[left:right]:\n\t\t\tif j > Mi:\n\t\t\t\tMi = j\n\t\tM[i] = Mi\n\treturn M\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef diagonal(val, colPointer, rowIndices, n, p):\n\t""""""\n\t[Added 10/10/2018] [Edited 13/10/2018]\n\tExtracts the diagonal elements of a CSR Matrix. Note only gets\n\tsquare diagonal (not off-diagonal). HyperLearn\'s algorithm is faster\n\tthan Scipy\'s, as it uses binary search, whilst Scipy uses Linear Search.\n\t\n\td = min(n, p)\n\tHyperLearn = O(d log p)\n\tScipy = O(dp)\n\t""""""\n\tsize = min(n,p)\n\tdiag = zeros(size, dtype = val.dtype)\n\tfor i in range(size):\n\t\tleft = rowIndices[i]\n\t\tright = rowIndices[i+1]\n\t\t\n\t\tpartial = colPointer[left:right]\n\t\tk = searchsorted(partial, i)\n\t\t\n\t\t# Get only diagonal elements else 0\n\t\tif partial[k] == i:\n\t\t\tdiag[i] = val[left+k]\n\treturn diag\n\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef _diagonal_add(val, colPointer, rowIndices, n, p, minimum, addon, copy = True):\n\t""""""\n\t[Added 10/10/2018] [Edited 13/10/2018]\n\tHyperLearn\'s add diagonal to CSR Matrix is optimized and uses Binary Search.\n\tThis algorithm is executed in 4 distinct steps:\n\t\n\t1. Intialise K vector (kinda like Dijkstra\'s algorithm)\n\t\tused to log where the diagonal element is.\n\t2. Use binary search to find the diagonal element.\n\t\tScipy\'s is linear search, which is slower.\n\t3. Change Val, ColPointer vectors.\n\t4. Change RowIndices vector.\n\t\n\tSteps 1->4 also are mostly optimized in time complexity.\n\t\n\t\td = min(n, p)\n\t1. O(d)\n\t2. O(d log p)\n\t3. O(2 dp)\n\t4. O(n)\n\t\n\tThe magic is step 2, where Scipy is O(dp). Likewise, total complexity\n\tis O(d + n + dlogp + dp)\n\t""""""\n\tsize = min(n,p)\n\tA = addon.astype(val.dtype)\n\t\n\tV = val\n\tC = colPointer\n\tR = rowIndices.copy() if copy else rowIndices\n\t\n\textend = 0\n\t\n\tK = zeros(size, dtype = minimum.dtype)\n\tfor i in range(size):\n\t\tK[i] -= 1\n\n\tfor i in range(size):\n\t\tleft = R[i]\n\t\tright = R[i+1]\n\t\t\n\t\tpartial = C[left:right]\n\t\t# Get only diagonal elements else 0\n\t\tk = searchsorted(partial, i)\n\t\t\n\t\tif len(partial) != k:\n\t\t\tif partial[k] == i:\n\t\t\t\tK[i] = -k-1 # neg -1 as we don\'t want to mix\n\t\t\t\t\t\t\t# the true extra elements\n\t\t\telse:\n\t\t\t\tK[i] = k\n\t\t\t\textend += 1\n\t\telse:\n\t\t\tK[i] = k\n\t\t\textend += 1\n\n\tnewN = len(C)+extend\n\tnewC = zeros(newN, dtype = C.dtype)\n\tnewV = zeros(newN, dtype = V.dtype)\n\tadded = zeros(n, dtype = minimum.dtype)\n\tmove = 0\n\t\n\t# move = go move steps forward\n\t# goes from left --> right\n\tfor i in range(size):\n\t\ta = A[i]\n\t\tk = K[i]\n\t\tleft = R[i]\n\t\tright = R[i+1]\n\t\tl = left+move\n\t\tr = right+move\n\t\tm = l+k\n\t\tlk = left+k\n\t\t\n\t\tadded[i] = move\n\t\tif k > -1:\n\t\t\tnewC[l:m] = C[left:lk]\n\t\t\tnewC[m] = i\n\t\t\tnewC[m+1:r+1] = C[lk:right]\n\t\t\t\n\t\t\tnewV[l:m] = V[left:lk]\n\t\t\tnewV[m] = a\n\t\t\tnewV[m+1:r+1] = V[lk:right]      \n\t\t\tmove += 1\n\t\telse:\n\t\t\tnewC[l:r] = C[left:right]\n\t\t\tnewV[l:r] = V[left:right]\n\t\t\tnewV[l-k-1] += a # -1 and not + 1 since -k\n\t\t  \n\t# Update rest of matrix if n > size  \n\ti += 1\n\tif i < n:\n\t\tnewV[R[i]+move:] = V[R[i]:]\n\t\tnewC[R[i]+move:] = C[R[i]:]\n\t\tadded[i:] = move\n\t\n\t# Update rowPointer\n\tfor i in range(n):\n\t\ta = added[i]\n\t\tif i < size:\n\t\t\tR[i] += a\n\t\telse:\n\t\t\t# Handles if n > d --> just pad the rowPointer\n\t\t\tfor j in range(i, n):\n\t\t\t\tR[j] += a\n\t\t\tbreak\n\tR[n] += a # Add to end of rowPointer, since size(n+1)\n\t\n\treturn newV, newC, R\n\n\n\ndef diagonal_add(val, colPointer, rowIndices, n, p, addon, copy = True):\n\t""""""\n\tSee _diagonal_add documentation.\n\t""""""\n\tsize = min(n, p)\n\tif ndim(addon) == 0:\n\t\tA = zeros(size, dtype = val.dtype)+addon\n\telse:\n\t\tA = addon\n\tassert len(A) == size\n\tminimum = minimum = getDtype(min(n,p), size = 1, uint = False)\n\treturn _diagonal_add(val, colPointer, rowIndices, n, p, minimum, A, copy)\n\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef get_element(val, colPointer, rowIndices, n, p, i, j):\n\t""""""\n\t[Added 14/10/2018]\n\tGet A[i,j] element. HyperLearn\'s algorithm is O(logp) complexity, which is much\n\tbetter than Scipy\'s O(p) complexity. HyperLearn uses binary search to find the\n\telement.\n\t""""""\n\tassert i < n and j < p\n\tleft = rowIndices[i]\n\tright = rowIndices[i+1]\n\tselect = colPointer[left:right]\n\tsearch = searchsorted(select, j)\n\t\n\tif search < len(select):\n\t\tif select[search] == j:\n\t\t\treturn val[left+search]\n\treturn 0\n\n\n\ndef _mat_vec(val, colPointer, rowIndices, n, p, y):\n\t""""""\n\tAdded [13/10/2018]\n\tX @ y is found. Scipy & HyperLearn has similar speed. Notice, now\n\tHyperLearn can be parallelised! This reduces complexity to approx\n\tO(np/c) where c = no of threads / cores\n\t""""""\n\tZ = zeros(n, dtype = y.dtype)\n\t\n\tfor i in prange(n):\n\t\ts = 0\n\t\tfor j in range(rowIndices[i], rowIndices[i+1]):\n\t\t\ts += val[j]*y[colPointer[j]]\n\t\tZ[i] = s\n\treturn Z\nmat_vec = njit(_mat_vec, fastmath = True, nogil = True, cache = True)\nmat_vec_parallel = njit(_mat_vec, fastmath = True, nogil = True, parallel = True)\n\n\ndef _matT_vec(val, colPointer, rowIndices, n, p, y):\n\t""""""\n\tAdded [13/10/2018]\n\tX.T @ y is found. Notice how instead of converting CSR to CSC matrix, a direct\n\tX.T @ y can be found. Same complexity as mat_vec(X, y). Also, HyperLearn is\n\tparallelized, allowing for O(np/c) complexity.\n\t""""""\n\tZ = zeros(p, dtype = y.dtype)\n\n\tfor i in prange(n):\n\t\tyi = y[i]\n\t\tfor j in range(rowIndices[i], rowIndices[i+1]):\n\t\t\tZ[colPointer[j]] += val[j]*yi\n\treturn Z\nmatT_vec = njit(_matT_vec, fastmath = True, nogil = True, cache = True)\nmatT_vec_parallel = njit(_matT_vec, fastmath = True, nogil = True, parallel = True)\n\n\n\ndef _mat_mat(val, colPointer, rowIndices, n, p, X):\n\t""""""\n\tAdded [14/10/2018]\n\tA @ X is found where X is a dense matrix. Mostly the same as Scipy, albeit slightly faster.\n\tThe difference is now, HyperLearn is parallelized, which can reduce times by 1/2 or more.\n\t""""""\n\tK = X.shape[1]\n\tZ = zeros((n, K), dtype = X.dtype)\n\n\tfor i in prange(n):\n\t\tleft = rowIndices[i]\n\t\tright = rowIndices[i+1]\n\t\t\n\t\tfor k in range(K):\n\t\t\ts = 0\n\t\t\tfor j in range(left, right):\n\t\t\t\ts += val[j]*X[colPointer[j], k]\n\t\t\tZ[i, k] = s\n\treturn Z\nmat_mat = njit(_mat_mat, fastmath = True, nogil = True, cache = True)\nmat_mat_parallel = njit(_mat_mat, fastmath = True, nogil = True, parallel = True)\n\n\ndef _matT_mat(val, colPointer, rowIndices, n, p, X):\n\t""""""\n\tAdded [14/10/2018]\n\tA.T @ X is found where X is a dense matrix. Mostly the same as Scipy, albeit slightly faster.\n\tThe difference is now, HyperLearn is parallelized, which can reduce times by 1/2 or more.\n\t""""""\n\tK = X.shape[1]\n\tA = zeros((K, p), dtype = X.dtype)\n\tzero = zeros(p, dtype = X.dtype)\n\n\tfor k in prange(K):\n\t\tZ = zero.copy()\n\t\ty = X[:,k]\n\t\t\n\t\tfor i in range(n):\n\t\t\tyi = y[i]\n\t\t\tfor j in range(rowIndices[i], rowIndices[i+1]):\n\t\t\t\tZ[colPointer[j]] += val[j]*yi\n\t\tfor i in range(p):\n\t\t\tA[k, i] = Z[i]\n\treturn A.T.copy()\nmatT_mat = njit(_matT_mat, fastmath = True, nogil = True, cache = True)\nmatT_mat_parallel = njit(_matT_mat, fastmath = True, nogil = True, parallel = True)\n\n\n\n\ndef XXT_sparse(val, colPointer, rowIndices, n, p):\n\t""""""\n\tSee _XXT_sparse documentation.\n\t""""""\n\tD = zeros((n,n), dtype = val.dtype)\n\tP = zeros(p, dtype = val.dtype)\n\n\tfor k in prange(n-1):\n\t\tl = rowIndices[k]\n\t\tr = rowIndices[k+1]\n\n\t\tR = P.copy()\n\t\tb = l\n\t\tfor i in range(r-l):\n\t\t\tx = colPointer[b]\n\t\t\tR[x] = val[b]\n\t\t\tb += 1\n\t\t\n\t\tfor j in prange(k+1, n):\n\t\t\tl = rowIndices[j]\n\t\t\tr = rowIndices[j+1]\n\t\t\ts = 0\n\t\t\tc = l\n\t\t\tfor a in range(r-l):\n\t\t\t\tz = colPointer[c]\n\t\t\t\tv = R[z]\n\t\t\t\tif v != 0:\n\t\t\t\t\ts += v*val[c]\n\t\t\t\tc += 1\n\t\t\tD[j, k] = s\n\treturn D\n_XXT_sparse_single = njit(XXT_sparse, fastmath = True, nogil = True, cache = True)\n_XXT_sparse_parallel = njit(XXT_sparse, fastmath = True, nogil = True, parallel = True)\n\n\ndef _XXT(val, colPointer, rowIndices, n, p, n_jobs = 1):\n\t""""""\n\t[Added 16/10/2018]\n\tComputes X @ XT very quickly. Approx 50-60% faster than Sklearn\'s version,\n\tas it doesn\'t optimize a lot. Note, computes only lower triangular X @ XT,\n\tand disregards diagonal (set to 0)\n\t""""""\n\tXXT = _XXT_sparse_parallel(val, colPointer, rowIndices, n, p) if n_jobs != 1 else \\\n\t\t_XXT_sparse_single(val, colPointer, rowIndices, n, p)\n\treturn XXT\n\n\n\n@njit(fastmath = True, nogil = True, cache = True)\ndef rowSum(val, colPointer, rowIndices, norm = False):\n\t""""""\n\t[Added 17/10/2018]\n\tComputes rowSum**2 for sparse matrix efficiently, instead of using einsum\n\t""""""\n\tn = len(rowIndices)-1\n\tS = zeros(n, dtype = val.dtype)\n\n\tfor i in range(n):\n\t\ts = 0\n\t\tl = rowIndices[i]\n\t\tr = rowIndices[i+1]\n\t\tb = l\n\t\tfor j in range(r-l):\n\t\t\tXij = val[b]\n\t\t\ts += Xij*Xij\n\t\t\tb += 1\n\t\tS[i] = s\n\tif norm:\n\t\tS**=0.5\n\treturn S\n\n'"
hyperlearn/sparse/tcsr.py,0,"b'\nfrom numba import njit, prange\nfrom numpy import zeros\n\n""""""\nTCSR Matrix functions.\n\t1. _XXT\n""""""\n\ndef _XXT_triangular(val, colPointer, rowIndices, n, p):\n\t""""""\n\t[Added 21/10/2018]\n\tComputes XXT and stores it as a triangular sparse matrix.\n\tA triangular sparse matrix removes the colPointer and rowIndices\n\tsince the TCSR format assumes each element is in sucession. \n\n\tThis cuts memory total memory usage of the full dense matrix by 1/2\n\t[actually 1/2n^2 - n is used].\n\t""""""\n\tsize = n*(n-1) // 2 # floor division\n\t\n\tD = zeros(size, dtype = val.dtype)\n\tP = zeros(p, dtype = val.dtype)\n\n\tfor k in prange(n-1):\n\t\tl = rowIndices[k]\n\t\tr = rowIndices[k+1]\n\n\t\tA = P.copy()\n\t\tb = l\n\t\tfor i in range(r-l):\n\t\t\tx = colPointer[b]\n\t\t\tA[x] = val[b]\n\t\t\tb += 1\n\t\t\n\t\tfor j in prange(k+1, n):\n\t\t\tl = rowIndices[j]\n\t\t\tr = rowIndices[j+1]\n\t\t\ts = 0\n\t\t\tc = l\n\t\t\tfor a in range(r-l):\n\t\t\t\tz = colPointer[c]\n\t\t\t\tv = A[z]\n\t\t\t\tif v != 0:\n\t\t\t\t\ts += v*val[c]\n\t\t\t\tc += 1\n\t\t\t\t\n\t\t\t# Exact position in CSR is found via j(j-1)/2 + k\n\t\t\tD_where = j*(j-1) // 2 + k\n\t\t\tD[D_where] = s\n\t\n\treturn D\n_XXT_triangular_single = njit(_XXT_triangular, fastmath = True, nogil = True, cache = True)\n_XXT_triangular_parallel = njit(_XXT_triangular, fastmath = True, nogil = True, parallel = True)\n\n\ndef _XXT(val, colPointer, rowIndices, n, p, n_jobs = 1):\n\t""""""\n\t[Added 16/10/2018]\n\tComputes X @ XT very quickly, and stores it in a modified CSR matrix (Triangular CSR).\n\tUses 1/2n^2 - n memory, and thus much more efficient and space conversing than if\n\tusing a full CSR Matrix (memory reduced by approx 25 - 50%).\n\t""""""\n\tXXT = _XXT_triangular_parallel(val, colPointer, rowIndices, n, p) if n_jobs != 1 else \\\n\t\t_XXT_triangular_single(val, colPointer, rowIndices, n, p)\n\treturn XXT\n\n\n'"
hyperlearn_new/cython/__init__.py,0,b'\n'
hyperlearn_new/cython/setup.py,0,"b'\n# python setup.py build_ext --inplace\nfrom distutils.core import setup\nfrom Cython.Build import cythonize\nfrom numpy import get_include\nfrom Cython.Compiler import Options\nimport os\nos.environ[\'CFLAGS\'] = \'-O3 -march=native\'\nos.environ[\'CXXFLAGS\'] = \'-O3 -march=native\'\nos.environ[\'CL\'] = \'/arch:AVX /arch:AVX2 /arch:SSE2 /arch:SSE /arch:ARMv7VE /arch:VFPv4\'\n\nOptions.docstrings = True\nOptions.generate_cleanup_code = True\n\nsetup(\n    ext_modules = cythonize(""*.pyx"",\n        compiler_directives = {\n            \'language_level\':3, \n            \'boundscheck\':False, \n            \'wraparound\':False,\n            \'initializedcheck\':False, \n            \'cdivision\':True,\n            \'nonecheck\':False,\n        },\n        quiet = True,\n        force = True,\n    ),\n    include_dirs = [get_include()],\n)\n'"
hyperlearn_new/numba/__init__.py,0,b'\n'
hyperlearn_new/numba/funcs.py,0,"b'\nfrom .types import *\nimport numpy as np\nfrom ..cfuncs import uinteger\n\n###\n@jit([Tuple((M_32, A32, M_32))(M32_, bool_),Tuple((M_64, A64, M_64))(M64_, bool_),\n      Tuple((M_32, A32, M_32))(M_32, bool_),Tuple((M_64, A64, M_64))(M_64, bool_)], **nogil)\ndef svd(X, full_matrices = False): \n    return np.linalg.svd(X, full_matrices = full_matrices)\n\n###\n@jit([A32(M32_, A32), A64(M64_, A64)], **nogil)\ndef lstsq(X, y): return np.linalg.lstsq(X, y.astype(X.dtype))[0]\n\n###\n@jit([Tuple((M32, M_32))(M32_), Tuple((M64, M_64))(M64_)], **nogil)\ndef qr(X): return np.linalg.qr(X)\n\n###\n@jit([F64(A64), F64(A32)])\ndef norm(v): return np.linalg.norm(v)\n\n\n###\n@jit(**nogil)\ndef maximum(X, i): return np.maximum(X, i)\n\n###\n@jit(**nogil)\ndef minimum(X, i): return np.minimum(X, i)\n\n\n###\ndef arange(size):\n    return np.arange(size, dtype = uinteger(size))\n\n\n######################################################\n# Custom statistical functions\n# Mean, Variance\n######################################################\n\n###\n@jit([A32(M32_), A64(M64_)], **nogil)\ndef mean_1(X):\n    n, p = X.shape\n    out = np.zeros(n, dtype = X.dtype)\n    for i in range(n):\n        s = 0\n        for j in range(p):\n            s += X[i, j]\n        s /= p\n        out[i] = s\n    return out\n\n###\n@jit([A32(M32_), A64(M64_)], **nogil)\ndef mean_0(X):\n    n, p = X.shape\n    out = np.zeros(p, dtype = X.dtype)\n    for i in range(n):\n        for j in range(p):\n            out[j] += X[i, j]\n    out /= n\n    return out\n\n###\n@jit([F64(M32_), F64(M64_)], **nogil)\ndef mean_A(X):\n    n, p = X.shape\n    s = np.sum(X) / (n*p)\n    return s\n\n\n###\ndef mean(X, axis = None):\n    if axis == 0:\n        return mean_0(X)\n    elif axis == 1:\n        return mean_1(X)\n    return mean_A(X)\n\n\n###\n@jit([A32(M32_), A64(M64_)], **nogil)\ndef var_0(X):\n    mu = mean_0(X)\n    n, p = X.shape\n    variance = np.zeros(p, dtype = mu.dtype)\n\n    for i in range(n):\n        for j in range(p):\n            v = X[i, j] - mu[j]\n            v *= v\n            variance[j] += v\n    variance /= n-1     # unbiased estimator\n    return variance\n\n###\n@jit([A32(M32_), A64(M64_)], **nogil)\ndef var_1(X):\n    mu = mean_1(X)\n    n, p = X.shape\n    variance = np.zeros(n, dtype = mu.dtype)\n\n    for i in range(n):\n        _mu = mu[i]\n        var = 0\n        for j in range(p):\n            v = X[i, j] - _mu\n            v *= v\n            var += v\n        variance[i] = var\n    variance /= p-1     # unbiased estimator\n    return variance\n\n###\n@jit([F64(M32_), F64(M64_)], **nogil)\ndef var_A(X):\n    mu = mean_A(X)\n    n, p = X.shape\n\n    var = 0\n    for i in range(n):\n        for j in range(p):\n            v = X[i, j] - mu\n            v *= v\n            var += v\n    var /= n*p-1        # unbiased estimator\n    return var\n\n###\ndef var(X, axis = None):\n    if axis == 0:\n        return var_0(X)\n    elif axis == 1:\n        return var_1(X)\n    return var_A(X)\n\n###\ndef std(X, axis = None):\n    if axis == 0:\n        V = var_0(X)\n    elif axis == 1:\n        V = var_1(X)\n    else:\n        V = var_A(X)\n    return V**0.5\n\n'"
hyperlearn_new/numba/types.py,0,"b'\nfrom numba.types import float32, float64, int64, int8, bool_, uint32\nfrom numba.types import UniTuple, Tuple\nfrom numba import jit, prange\n\nnogil = {""fastmath"":True,""nogil"":True,""cache"":True,""nopython"":True}\ngil = {""fastmath"":True,""nogil"":False,""cache"":True,""nopython"":True}\nparallel = {""fastmath"":True,""nogil"":True,""cache"":True,""nopython"":True,""parallel"":True}\n\nM32 = float32[:,:]\nM64 = float64[:,:]\nA32 = float32[::1]\nA64 = float64[::1]\n\nM32_ = float32[:,::1]\nM64_ = float64[:,::1]\n\nM_32 = float32[::1,:]\nM_64 = float64[::1,:]\n\nI64 = int64\nF32 = float32\nF64 = float64\nU32 = uint32\n\n'"
hyperlearn_new/randomized/__init__.py,0,b'\n\n'
hyperlearn_new/randomized/base.py,0,"b'\nimport numpy as np\nfrom ..numba.types import *\nfrom ..numba.funcs import arange\nfrom ..random import shuffle, randbool, randint\n\n\n# Sketching methods from David Woodruff.\n\n###\ndef sketch(n, p, k = 10, method = ""left""):\n    """"""\n    Produces a CountSketch matrix which is similar in nature to\n    the Johnson\xe2\x80\x93Lindenstrauss Transform (eps Fast-JLT) as shown\n    in Sklearn. But, as shown by David Woodruff, ""Sketching as a \n    Tool for Numerical Linear Algebra"" [arxiv.org/abs/1411.4357],\n    super fast matrix multiplies can be done. Notice a difference\n    is HyperLearn\'s  K = min(n, p)/2, but david says k^2/eps is\n    needed (too memory taxing). [Added 4/12/18] [Edited 14/12/18]\n\n    Parameters\n    -----------\n    X:              Data matrix.\n    k:              (auto, int). Auto is min(n, p) / 2\n    method:         (left, right). Left is S @ X. Right X @ S.\n    \n    Returns\n    -----------\n    (Sketch Matrix S or indices)\n    """"""\n    if method == ""left"":\n        x = arange(n)\n        shuffle(x)\n        return x\n    else:\n        if k < 20:\n            sign = randbool(p)\n            position = randint(0, k, size = p)\n            return _sketch_right(k, n, p, sign, position)\n        else:\n            x = randint(0, k, size = p)\n            return x\n\n\n###\n@jit(parallel = True)\ndef sketch_multiply_left(X, S, k = 10):\n    """"""\n    Multiplies sketch matrix S onto X giving S @ X.\n    Very fast and uses little to no memory.\n    [Added 14/12/18]\n    """"""\n    n, p = X.shape\n    size = n//k\n        \n    out = np.zeros((k, p), dtype = X.dtype)\n    partial = np.zeros(p, dtype = X.dtype)\n\n    for i in prange(k-1):\n        res = partial.copy()\n        left = i*size\n        right = (i+1)*size\n        middle = int( (i+0.5)*size )\n\n        for j in range(left, middle):\n            res += X[S[j]]\n        for j in range(middle, right):\n            res -= X[S[j]]\n        out[i] = res\n    \n    i = k-1\n    left = i*size\n    middle = int( (i+0.5)*size )\n\n    for j in range(left, middle):\n        partial += X[S[j]]\n    for j in range(middle, n):\n        partial -= X[S[j]]\n    out[i] = partial\n    \n    return out\n\n\n###\n@jit(parallel = True)\ndef sketch_multiply_right(X, S, k = 10):\n    """"""\n    Multiplies sketch matrix S onto X giving X @ S.\n    Can be slow if k is small.\n    [Added 14/12/18]\n    """"""\n    n, p = X.shape\n    out = np.zeros((n, k), dtype = X.dtype)\n    \n    for i in prange(n):\n        Xi = X[i]\n        if i % 2 == 0:\n            for j in range(p):\n                out[i, S[j]] += Xi[j]\n        else:\n            for b in range(p):\n                out[i, S[j]] -= Xi[j]\n    return out\n\n\n###\n@jit\ndef _sketch_right(k, n, p, sign, position):\n    S = np.zeros((p, k), dtype = np.int8)\n    ones = np.ones(p, dtype = np.int8)\n    ones[sign] = -1\n\n    for i in range(p):\n        S[i, position[i]] = ones[i]\n    return S\n\n\n###\ndef sketch_multiply(X, S = None, k = 10, method = ""left"", n_jobs = 1):\n    """"""\n    Multiplies a sketch matrix S onto X either giving SX or XS.\n    Tries to be fast and complexity is O(np).\n    """"""\n    if S is None:\n        S = sketch(X, k, method)\n    if method == ""left"":\n        return sketch_multiply_left(X, S, k, n_jobs = n_jobs)\n    else:\n        if k < 20:\n            return X @ S\n        else:\n            return sketch_multiply_right(X, S, k, n_jobs = n_jobs)\n\n'"
hyperlearn_new/randomized/decomposition.py,0,"b'\nfrom .linalg import *\nfrom .. import linalg\n\n###\n@process(memcheck = \n    {""X"":""minimum"",""C_only"":""min_left"",""R_only"":""min_right""}, \n    fractional = False)\ndef cur(\n    X, C_only = False, R_only = False, n_components = 2, \n    solver = ""euclidean"", n_oversamples = ""klogk"", success = 0.5):\n    """"""\n    Outputs the CUR Decomposition of a general matrix. Similar\n    in spirit to SVD, but this time only uses exact columns\n    and rows of the matrix. C = columns, U = some projective\n    matrix connecting C and R, and R = rows.\n    [Added 2/12/18] [Edited 9/12/18 Added C_only and R_only]\n\n    Parameters\n    -----------\n    X:              General Matrix.\n    C_only:         Only compute C.\n    R_only:         Only compute R.\n    n_components:   How many ""row eigenvectors"" you want.\n    solver:         (euclidean, leverage, optimal) Selects columns based \n                    on separate squared norms of each property.\n                    \n                    Error bounds: (eps = 1-success)\n                    nystrom:        Nystrom method (Slightly different)\n                                    C @ pinv(C intersect R) @ R\n                    euclidean:      ||A - A*|| + eps||A||\n                    leverage:       (2 + eps)||A - A*||\n                    optimal:        (1 + eps)||A - A*||\n\n    n_oversamples:  (klogk, None, k) How many extra samples is taken.\n                    Default = k*log2(k) which guarantees (1+e)||X-X*||\n                    error.\n    success:        Probability of success. Default = 50%. Higher success\n                    rates make the algorithm run slower.\n\n    Returns\n    -----------\n    C:              Column sample\n    U:              Connection between columns and rows\n    R:              Row sample\n    """"""\n    eps = 1 - success\n    eps = 1 if eps > 1 else eps\n    eps **= 2\n    compute_all = (C_only == R_only)\n    compute_C = compute_all or C_only\n    compute_R = compute_all or R_only\n\n    n, p = X.shape\n    dtype = X.dtype\n    k = n_components\n    k_col = k if type(k) is int else _max( int(k*p), 1)\n    k_row = k if type(k) is int else _max( int(k*n), 1)\n    k = k if type(k) is int else _max( int(k *_min(n, p)), 1)\n\n    if solver == ""euclidean"":\n        # LinearTime CUR 2015 www.cs.utah.edu/~jeffp/teaching/cs7931-S15/cs7931/5-cur.pdf\n\n        if compute_C:\n            c = int(k_col / eps**2)\n            C = select(X, c, n_oversamples = None, axis = 0)\n\n        if compute_R:\n            r = int(k_row / eps)\n            r, s = select(X, r, n_oversamples = None, axis = 1, output = ""indices"")\n            R = X[r]*s\n\n        if compute_all:\n            phi = C[r]*s\n            CTC = linalg.matmul(""X.H @ X"", C)\n            inv = linalg.pinvh(CTC, reflect = False, overwrite = True)\n            U = linalg.matmul(""S @ Y.H"", inv, phi)\n\n    elif solver == ""leverage"":\n        # LeverageScore CUR 2014\n        c = int(k*np.log2(k) / eps)\n        if compute_all:\n            C, R = select(X, c, n_oversamples = None, axis = 2, solver = ""leverage"")\n\n        elif compute_C:\n            C = select(X, c, n_oversamples = None, axis = 0, solver = ""leverage"")\n\n        elif compute_R:\n            R = select(X, c, n_oversamples = None, axis = 1, solver = ""leverage"")\n\n        if compute_all:\n            U = linalg.pinvc(C) @ X @ linalg.pinvc(R)\n\n    elif solver == ""nystrom"":\n        # Nystrom Method. Microsoft 2016 ""Kernel Nystr\xc3\xb6m Method for Light Transport""\n        if compute_C:\n            c = n_components if n_components < p else p\n            c = np.random.choice(range(p), size = c, replace = False)\n            C = X[:,c]\n\n        if compute_R:\n            r = n_components if n_components < n else n\n            r = np.random.choice(range(n), size = r, replace = False)\n            R = X[r]\n\n        if compute_all:\n            U = linalg.pinv(C[r])\n\n    elif solver == ""optimal"":\n        # Optimal CUR Matrix Decompositions - David Woodruff 2014\n        # Slightly changed - uses double sampling since too taxing\n        # to compute full spectrum. (2nd sampling done via Count Sketch)\n        # random projection\n\n        k1 = int(k_col * np.log2(k_col))\n        k2 = int(k_col/eps)\n        K = k_col + k2\n        P = range(p)\n\n        col, row = select(\n            X, n_components = k, solver = ""euclidean"", output = ""statistics"", axis = 2,\n            duplicates = True)\n\n        # Get Columns from BSS sampling\n        indices1 = np.random.choice(P, size = k1, p = col)\n        indices1 = np.random.choice(indices1, size = k_col, p = proportion(col[indices1]) )\n\n        indices1, count1 = np.unique(indices1, return_counts = True)\n        scaler1 = count1 / (col[indices1] * k1)\n        scaler1 **= 0.5\n\n        # Compute error after projecting onto columns\n        C = X[:,indices1] * scaler1\n\n        # Double pass (reduce number of rows) uses Count Sketch\n        # ie Fast eps JLT Random Projection\n        print(n, p , k)\n        position, sign = sketch(n, p, k)\n        SX = sparse_sketch_multiply(k, position, sign, X)\n        SC = sparse_sketch_multiply(k, position, sign, C)\n\n        # Want X - C*inv(C)*X\n        c = SX - SC @ linalg.pinvc(SC) @ SX\n        c = proportion(col_norm(c))\n        return c, C\n\n        # Select extra columns from column residual norms\n        indices2 = np.random.choice(P, size = k2, p = c, dtype = uinteger(k2))\n        indicesP = np.hstack((indices1, indices2))\n\n        # Determine final scaling factors for C\n        indicesP, countP = np.unique(indicesP, return_counts = True)\n        scalerP = countP / (col[indicesP] * K)\n        scalerP **= 0.5\n\n        return indicesP, scalerP\n\n\n\n\n\n\n        # R = X[indicesP]\n        # RTR = linalg.pinvc(R) @ R\n\n\n\n\n\n\n\n\n\n'"
hyperlearn_new/randomized/linalg.py,0,"b'\nimport numpy as np\nfrom .. import linalg\nfrom ..numba.funcs import arange, var\nfrom ..utils import *\nfrom ..stats import corr, corr_sum\nfrom ..linalg import transpose\nfrom ..random import normal, choice\nfrom .base import *\n\n\n########################################################\n## 1. ColumnSelect\n## 2. Count Sketch (Fast JLT alternative)\n## 3. Fast Count Sketch multiply\n## 4. ColumnSelect Matrix Multiply (Nystrom Method)\n\n\n###\ndef select(\n    X, n_components = 2, solver = ""euclidean"", output = ""columns"", \n    duplicates = False, n_oversamples = 0, axis = 0, n_jobs = 1):\n    """"""\n    Selects columns from the matrix X using many solvers. Also\n    called ColumnSelect or LinearSelect, HyperLearn\'s select allows\n    randomized algorithms to select important columns.\n    [Added 30/11/18] [Edited 2/12/18 Added BSS sampling]\n    [Edited 10/12/18 Fixed BSS Sampling]\n\n    Parameters\n    -----------\n    X:              General Matrix.\n    n_components:   How many columns you want.\n    solver:         (euclidean, uniform, leverage, adaptive) Selects columns\n                    based on separate squared norms of each property.\n                    [NEW Adaptive]. Iteratively adds parts. (Most accuarate)\n\n    output:         (columns, statistics, indices) Whether to output actual\n                    columns or just indices of the selected columns.\n                    Also can choose statistics to get norms only.\n    duplicates:     If True, then leaves duplicates as is. If False,\n                    uses sqrt(count) as a scaling factor.\n    n_oversamples:  (klogk, 0) How many extra samples is taken.\n                    Default = 0. Not much difference.\n    axis:           (0, 1, 2) 0 is columns. 1 is rows. 2 means both.\n    n_jobs:         Default = 1. Whether to use >= 1 CPU.\n\n    Returns\n    -----------\n    (X*, indices) Depends on output option.\n    """"""\n    n, p = X.shape\n    dtype = X.dtype\n\n    k = n_components\n    if type(n_components) is float and n_components <= 1:\n        k = int(k * p) if axis == 0 else int(k * n)\n    else:\n        k = int(k)\n        if axis == 0: \n            if k > p: k = p\n        elif axis == 1: \n            if k > n: k = n\n    n_components = k\n\n    # If adaptive\n    adaptive = (solver == ""adaptive"")\n\n    # Oversample ratio. klogk allows (1+e)||X-X*|| error.\n    if n_oversamples == ""klogk"":\n        k = int(n_components * np.log2(n_components))\n    else:\n        k = int(n_components)\n\n    n_components, k = k, n_components\n\n    # rows and columns\n    if axis == 0:   r, c = False, True\n    elif axis == 1: r, c = True, False\n    else:           r, c = True, True\n\n    # Calculate row or column importance.\n    if solver == ""leverage"" or adaptive:\n        if axis == 2:\n            U, _, VT = svd(X, n_components = n_components, n_oversamples = 1)\n            row = row_norm(U)\n            col = col_norm(VT)\n        elif axis == 0:\n            _, V = eig(X, n_components = n_components, n_oversamples = 1)\n            col = row_norm(V)\n        else:\n            U, _, _ = svd(X, n_components = n_components, n_oversamples = 1)\n            row = row_norm(U)\n\n    elif solver == ""uniform"":\n        if r:   row = np.ones(n, dtype = dtype)\n        if c:   col = np.ones(p, dtype = dtype)\n    else:\n        if r:   row = row_norm(X)\n        if c:   col = col_norm(X)\n\n    if r:       row = proportion(row)\n    if c:       col = proportion(col)\n\n    if output == ""statistics"":\n        if axis == 2:       return col, row\n        elif axis == 0:     return col\n        else:               return row\n\n    # Make a probability drawing distribution.\n    if not adaptive:\n        if r:\n            N = arange(n)\n            indicesN = choice(N, size = k, p = row)\n\n            # Use extra sqrt(count) scaling factor for repeated columns\n            if not duplicates:\n                indicesN, countN = unique_int(indicesN, return_counts = True)\n                scalerN = countN.astype(dtype) / (row[indicesN] * k)\n            else:\n                scalerN = 1 / (row[indicesN] * k)\n            scalerN **= 0.5\n            scalerN = scalerN[:,np.newaxis]\n        if c:\n            P = arange(p)\n            indicesP = choice(P, size = k, p = col)\n\n            # Use extra sqrt(count) scaling factor for repeated columns\n            if not duplicates:\n                indicesP, countP = unique_int(indicesP, return_counts = True)\n                scalerP = countP.astype(dtype) / (col[indicesP] * k)\n            else:\n                scalerP = 1 / (col[indicesP] * k)\n            scalerP **= 0.5\n\n    # Adaptive solver from Woodruff\'s 2014 Optimal CUR Decomp paper.\n    # Changed and upgraded into incremental solver. (Approx 1+e||A-A*|| error)\n    if adaptive and c:\n        kk = int(n/2/np.log2(k+1))\n\n        # Use Woodruff\'s CountSketch matrix to reduce space complexity.\n        S = sketch(n, p, kk, method = ""left"")\n        SX = sketch_multiply_left(X, S, kk, n_jobs = n_jobs)\n\n        # Only want top eigenvector\n        _, V = eig(SX, n_components = 1, n_oversamples = 1, U_decision = None)\n\n        # Find maximum column norm -> determinstic algorithm\n        norm = proportion(row_norm(V))\n        select = norm.argmax()\n        C = np.zeros((n, k), dtype = dtype)\n        scalerP = np.zeros(k, dtype = dtype)\n        indicesP = np.zeros(k, dtype = int)\n        indicesP[0] = select\n\n        s = 1/(norm[select]**0.5)\n        scalerP[0] = s\n        C[:,0] = X[:,select]*s\n\n        seen = 1\n        for i in range(k-1):\n            I = i+1\n            # Produce S*C, which is a smaller matrix\n            SC = sketch_multiply_left(C[:,:I], S, kk, n_jobs = n_jobs)\n\n            # Find projection residual\n            # Notice left to right sometimes faster -> so check.\n            inv = linalg.pinv(SC)\n            left_to_right = linalg.dot(SC, inv, SX, message = True)\n\n            if left_to_right:\n                # Better to do CC+ by itself.\n                CinvC = SC @ inv\n                CinvC *= -1\n                CinvC.flat[::CinvC.shape[0]+1] += 1\n                norm = col_norm(CinvC @ SX)\n            else:\n                norm = col_norm(SX -  (SC @ (inv @ SX)) )\n\n            # Convert to probabilities\n            norm = proportion(norm)\n            select = norm.argmax()\n            indicesP[I] = select    \n            \n            # Have to rescale old slcaers, and update new.\n            old = seen**0.5\n            seen += 1\n            new = seen**0.5\n            s = old/new\n            C[:,:I] *= s\n            scalerP[:I] *= s\n\n            # Update C\n            s = 1/((norm[select] * seen)**0.5)\n            C[:,I] = X[:,select]*s\n            scalerP[I] = s\n\n    # Return output\n    if output == ""columns"":\n        if not adaptive:\n            if c:\n                C = X[:,indicesP] * scalerP\n            if r:\n                R = X[indicesN] * scalerN\n\n        # # Output columns if specified\n        if axis == 2:       return C, R\n        elif axis == 0:     return C\n        else:               return R\n    # Or just output indices with scalers\n    else:\n        if axis == 2:       return (indicesN, scalerN), (indicesP, scalerP)\n        elif axis == 0:     return indicesP, scalerP\n        else:               return indicesN, scalerN\n\n\n###\ndef matmul(\n    pattern, X, n_components = 0.5, solver = ""euclidean"",\n    n_oversamples = ""klogk"", axis = 1):\n    """"""\n    Mirrors hyperlearn.linalg\'s matmul functionality, but extends it by\n    using the randomized ColumnSelect algorithm. This can dramatically\n    reduce compute time, but still allows good approximate guarantees.\n    [Added 1/12/18]\n\n    Parameters\n    -----------\n    pattern:        Can include: X.H @ X | X @ X.H\n    X:              Compulsory left side matrix.\n    n_components:   (int, float). Can be a ratio of total number of\n                    columns or rows.\n    solver:         (euclidean, uniform, leverage, adaptive) Selects columns\n                    based on separate squared norms of each property.\n    n_oversamples:  (klogk, 0, k) How many extra samples is taken.\n                    Default = k*log2(k) which guarantees (1+e)||X-X*||\n                    error.\n    axis:           (0, 1). Can be 0 (columns) which reduces the total\n                    dimensionality of the data or 1 (rows) which just\n                    reduces the compute time of forming XTX or XXT.\n    Returns\n    -----------\n    out:            Special sketch matrix output according to pattern.\n    indices:        Output the indices used in the sketching matrix.\n    scaler:         Output the scaling factor by which the columns are\n                    scaled by.\n    """"""\n    dtypeX = X.dtype\n    n, p = X.shape\n    if axis == 2: axis = 1\n\n    # Use ColumnSelect to sketch the matrix:\n    indices, scaler = select(\n        X, n_components = n_components, axis = axis, n_oversamples = n_oversamples, \n        duplicates = False, solver = solver, output = ""indices"")\n\n    if axis == 0:\n        # Columns\n        A = X[:,indices] * scaler\n    else:\n        A = X[indices] * scaler\n    \n    return linalg.matmul(pattern, X = A, Y = None), indices, scaler\n\n\n########################################################\n## 1. Randomized Projection onto orthogonal bases\n## 2. SVD, eig, eigh\n## 3. CUR decomposition\n\n\n###\ndef randomized_projection(\n    X, n_components = 2, solver = \'lu\', max_iter = 4, symmetric = False):\n    """"""\n    Projects X onto some random eigenvectors, then using a special\n    variant of Orthogonal Iteration, finds the closest orthogonal\n    representation for X.\n    [Added 25/11/18] [Edited 26/11/18 Overwriting all temp variables]\n    [Edited 1/12/18 Added Eigh support]\n\n    Parameters\n    -----------\n    X:              General Matrix.\n    n_components:   How many eigenvectors you want.\n    solver:         (auto, lu, qr, None) Default is LU Decomposition\n    max_iter:       Default is 4 iterations.\n    symmetric:      If symmetric, reduces computation time.\n\n    Returns\n    -----------\n    QR Decomposition of orthogonal matrix.\n    """"""\n    n, p = X.shape\n    dtype = X.dtype\n    n_components = int(n_components)\n    if max_iter == \'auto\':\n        # From Modern Big Data Algorithms --> seems like <= 4 is enough.\n        max_iter = 6 if n_components < 0.1 * _min(n, p) else 5\n\n    # Check n_components isn\'t too large\n    if n_components > p: n_components = p\n\n    # Notice overwriting doesn\'t matter since guaranteed to converge.\n    _solver =                       lambda x: linalg.lu(x, L_only = True, overwrite = True)\n    if solver == \'qr\': _solver =    lambda x: linalg.qr(x, Q_only = True, overwrite = True)\n    elif solver is None or \\\n        solver == \'None\': _solver = lambda x: x / col_norm(x)**0.5\n\n    if symmetric:\n        # # Get normal random numbers Q~N(0,1)\n        Q = normal(0, 1, (n_components, p), dtype)\n        Q /= (row_norm(Q)**0.5)[:,np.newaxis] # Normalize columns\n        Q = Q.T\n\n        max_iter *= 1.5\n        max_iter = int(max_iter)\n        # Cause symmetric, more stable, but needs more iterations.\n\n        for __ in range(max_iter):\n            Q = linalg.matmul(""S @ Y"", X, Q)\n            Q = _solver(Q)\n        Q = linalg.matmul(""S @ Y"", X, Q)\n    else:\n        # Get normal random numbers Q~N(0,1)\n        Q = normal(0, 1, (p, n_components), dtype)\n        Q /= col_norm(Q)**0.5 # Normalize columns\n\n        for __ in range(max_iter):\n            Q = X @ Q\n            Q = _solver(Q)\n            Q = linalg.matmul(""X.H @ Y"", X, Q)\n            Q = _solver(Q)\n        Q = X @ Q\n\n    Q = linalg.qr(Q, Q_only = True, overwrite = True)\n    return Q\n\n\n###\n@process(memcheck = ""truncated"")\ndef svd(\n    X, n_components = 2, max_iter = \'auto\', solver = \'lu\', n_oversamples = 5, \n    U_decision = False, n_jobs = 1, conjugate = True):\n    """"""\n    HyperLearn\'s Fast Randomized SVD is approx 20 - 40 % faster than\n    Sklearn\'s implementation depending on n_components and max_iter.\n    [Added 27/11/18]\n\n    Parameters\n    -----------\n    X:              General Matrix.\n    n_components:   How many eigenvectors you want.\n    max_iter:       Default is \'auto\'. Can be int.\n    solver:         (auto, lu, qr, None) Default is LU Decomposition\n    n_oversamples:  Samples more components than necessary. Used for\n                    convergence purposes. More is slower, but allows\n                    better eigenvectors. Default = 5\n    U_decision:     Default = False. If True, uses max from U. If None. don\'t flip.\n    n_jobs:         Whether to use more >= 1 CPU\n    conjugate:      Whether to inplace conjugate but inplace return original.\n\n    Returns\n    -----------    \n    U:              Orthogonal Left Eigenvectors\n    S:              Descending Singular Values\n    VT:             Orthogonal Right Eigenvectors\n    """"""\n    n, p = X.shape\n    dtype = X.dtype\n    ifTranspose = p > n\n    if ifTranspose:\n        X = transpose(X, conjugate, dtype)\n\n    Q = randomized_projection(X, n_components + n_oversamples, solver, max_iter)\n\n    B = Q.T @ X\n    U, S, VT = linalg.svd(B, U_decision = None, overwrite = True)\n    del B\n    S = S[:n_components]\n    U = Q @ U[:, :n_components]\n\n    if ifTranspose:\n        U, VT = transpose(VT, True, dtype), transpose(U, True, dtype)\n        if conjugate:\n            transpose(X, True, dtype);\n        U = U[:, :n_components]\n    else:\n        VT = VT[:n_components, :]\n\n    # flip signs\n    svd_flip(U, VT, U_decision = ifTranspose, n_jobs = n_jobs)\n\n    return U, S, VT\n\n\n###\n@process(memcheck = ""same"")\ndef pinv(\n    X, alpha = None, n_components = ""auto"", max_iter = \'auto\', solver = \'SATAX\', \n    n_jobs = 1, conjugate = True, converge = True):\n    """"""\n    Returns the Pseudoinverse of the matrix X using randomizedSVD.\n    Extremely fast. If n_components = ""auto"", will get the top sqrt(p)+1\n    singular vectors.\n    [Added 30/11/18] [Edited 14/12/18 Added Newton Schulz and 2016 Gower\'s\n    Linearly Convergent Randomized Pseudoinverse - R. M. Gower and Peter \n    Richtarik. ""Linearly Convergent Randomized Iterative Methods for \n    Computing the Pseudoinverse"", arXiv:1612.06255]\n    [16/12/18 Default changed to SKETCH (Newton with Sketching)]\n\n    Parameters\n    -----------\n    X:              General matrix X.\n    alpha :         Ridge alpha regularization parameter. Default 1e-6\n    n_components:   Default = auto. Provide less to speed things up.\n    max_iter:       Default is \'auto\'. Can be int.\n    solver:         (auto, satax, newton, sketch, lu, qr, None) Default is SKETCH.\n                    SATAX is from Gower\'s 2016 paper. (lu, qr and None) use 2011\n                    Halko\'s Randomized Range Finder. Newton is Newton Schulz solver.\n                    SKETCH is Newton + sketching.\n    converge:       Default = True. If True, uses a newly discovered approach inspired\n                    from Newton Schulz (inv -= 2*inv*X*inv)\n    n_jobs:         Whether to use more >= 1 CPU\n    conjugate:      Whether to inplace conjugate but inplace return original.\n\n    Returns\n    -----------    \n    pinv(X) :       Randomized Pseudoinverse of X. Approximately allows\n                    pinv(X) @ X = I. Not exact.\n    """"""\n    dtype = X.dtype\n    n, p = X.shape\n    solver = solver.lower()\n\n    if n_components == ""auto"":\n        n_components = int(p**0.5 + 1)\n    n_components = n_components if n_components < p else p\n\n    # Gower\'s SATAX solver. Use\'s sketch and solve paradigm.\n    if solver == ""satax"":\n        if max_iter == ""auto"":\n            # No need to do a lot of iterations. But, can provide stability.\n            max_iter = 1\n\n        inv = _min(n, p) * X.T / row_norm(X)**2\n\n        # Tried just selecting via leverage scores, uniform etc. Sketch is best.\n        S = sketch(p, n, n_components, ""right"")\n        invS = sketch_multiply(inv, S, n_components, method = ""right"")\n        del S\n\n        BT = (X @ invS).T\n        C = BT @ X\n        CCT = linalg.matmul(""X @ X.H"", C)\n        invCCT = linalg.pinvh(CCT, overwrite = True)\n\n        for i in range(max_iter):\n            R = C @ inv - BT\n            inv -= linalg.dot(C.T, invCCT, R)\n\n\n    # Newton Schulz solver. Needs quite a lot of iterations\n    elif solver == ""newton"":\n        if max_iter == ""auto"":\n            max_iter = 10\n        \n        inv = X.T / (2*frobenius_norm(X))\n\n        # Check X+ (2I - XX+)\n        left_to_right = linalg.dot(inv, X, inv, message = True)\n\n        for i in range(max_iter):\n            if left_to_right:\n                diff = linalg.dot(inv, X, inv)\n                inv *= 2\n                inv -= diff\n            else:\n                Xinv = X @ inv\n                Xinv *= -1\n                Xinv.flat[::Xinv.shape[0]+1] += 2\n                inv = inv @ Xinv\n\n\n    # Newton Schulz solver with sketching. Using too many iterations\n    # causes errors to actually explode. So, 1 iteration is OK.\n    elif solver == ""sketch"":\n        k = int(n ** 0.5 + 1)\n        S = sketch(n, p, k, ""left"")\n        SX = sketch_multiply(X, S, k, method = ""left"")\n\n        inv = X.T / (2*frobenius_norm(X))\n\n        S_first = sketch(p, n, k, ""right"")\n        S_second = sketch(p, p, n_components, ""right"")\n        S_third = sketch(p, n, n_components, ""left"")\n\n        # X+S * SX\n        invS = sketch_multiply(inv, S_first, k, ""right"")\n        invX = invS @ SX\n\n        # (X+S * SX)S\n        invXS = sketch_multiply(invX, S_second, n_components, ""right"")\n\n        # (X+S * SX)S * SX+\n        Sinv = sketch_multiply(inv, S_third, n_components, ""left"")\n\n        inv *= 2\n        inv -= invXS @ Sinv\n\n\n    # Else, use Halko\'s Randomized Range Finder for SVD. Least accurate.\n    else:\n        if n_components > p/2:\n            print(f""n_components >= {n_components} will be slow. Consider full pinv or pinvc"")\n\n        U, S, VT = svd(\n                    X, U_decision = None, n_components = n_components, max_iter = max_iter,\n                    solver = solver, n_jobs = n_jobs, conjugate = conjugate)\n\n        U, _S, VT = svd_condition(U, S, VT, alpha)\n        inv = (transpose(VT, True, dtype) * _S)   @ transpose(U, True, dtype)\n\n    # Apply 1 extra iteration of Newton Schluz to allow convergence.\n    if solver != ""satax"" and converge:\n        diff = linalg.dot(inv, X, inv)\n        inv *= 2\n        inv -= diff\n    return inv\n\n\n###\n@process(memcheck = ""minimum"")\ndef eig(\n    X, n_components = 2, max_iter = \'auto\', solver = \'lu\', \n    n_oversamples = 5, conjugate = True, n_jobs = 1, U_decision = False):\n    """"""\n    HyperLearn\'s Fast Randomized Eigendecomposition is approx 20 - 40 % faster than\n    Sklearn\'s implementation depending on n_components and max_iter.\n    [Added 27/11/18] [Edited 12/12/18 Added U_decision]\n\n    Parameters\n    -----------\n    X:              General Matrix.\n    n_components:   How many eigenvectors you want.\n    max_iter:       Default is \'auto\'. Can be int.\n    solver:         (auto, lu, qr, None) Default is LU Decomposition\n    n_oversamples:  Samples more components than necessary. Used for\n                    convergence purposes. More is slower, but allows\n                    better eigenvectors. Default = 5\n    conjugate:      Whether to inplace conjugate but inplace return original.\n    n_jobs:         Whether to use more >= 1 CPU\n    U_decision:     (False, None)\n\n    Returns\n    -----------\n    W:              Eigenvalues\n    V:              Eigenvectors\n    """"""\n    n, p = X.shape\n    dtype = X.dtype\n    ifTranspose = p > n\n    if ifTranspose:\n        X = transpose(X, conjugate, dtype)\n\n    Q = randomized_projection(X, n_components + n_oversamples, solver, max_iter)\n    B = Q.T @ X\n\n\n    if ifTranspose:\n        # use SVD instead\n        V, W, _ = linalg.svd(B, U_decision = None, overwrite = True)\n        W = W[:n_components]\n        V = Q @ V[:, :n_components]\n        if conjugate:\n            transpose(X, True, dtype);\n        W **= 2\n    else:\n        W, V = linalg.eig(B, U_decision = None, overwrite = True)\n        W = W[:n_components]\n        V = V[:,:n_components]\n    del B\n\n    # Flip signs\n    svd_flip(None, V, U_decision = U_decision, n_jobs = n_jobs)\n\n    return W, V\n\n\n###\n@process(square = True, memcheck = ""minimum"")\ndef eigh(\n    X, n_components = 2, max_iter = \'auto\', solver = \'lu\', \n    n_oversamples = 5, conjugate = True, n_jobs = 1):\n    """"""\n    HyperLearn\'s Fast Randomized Hermitian Eigendecomposition uses\n    QR Orthogonal Iteration. \n    [Added 27/11/18]\n\n    Parameters\n    -----------\n    X:              General Matrix.\n    n_components:   How many eigenvectors you want.\n    max_iter:       Default is \'auto\'. Can be int.\n    solver:         (auto, lu, qr, None) Default is LU Decomposition\n    n_oversamples:  Samples more components than necessary. Used for\n                    convergence purposes. More is slower, but allows\n                    better eigenvectors. Default = 5\n    conjugate:      Whether to inplace conjugate but inplace return original.\n    n_jobs:         Whether to use more >= 1 CPU\n\n    Returns\n    -----------\n    W:              Eigenvalues\n    V:              Eigenvectors\n    """"""\n    Q = randomized_projection(\n        X, n_components + n_oversamples, solver, max_iter, symmetric = True)\n\n    B = linalg.matmul(""S @ Y"", X, Q).T\n    W, V = linalg.eig(B, U_decision = None, overwrite = True)\n    W = W[:n_components]**0.5\n\n    V = V[:,:n_components]\n    # Flip signs\n    svd_flip(None, V, U_decision = False, n_jobs = n_jobs)\n\n    return W, V\n\n\n###\n@process(memcheck = {""X"":""truncated"",""Q_only"":""truncated"",""R_only"":""truncated""})\ndef qr(\n    X, Q_only = False, R_only = False, y = None, n_components = 2, \n    solver = ""euclidean"", n_oversamples = 5):\n    """"""\n    Approximate QR Decomposition using the Gram Schmidt Process. Not very\n    accurate, but uses Euclidean Norm sampling from the ColumnSelect algo\n    to appropriately select the first columns to orthogonalise. You can\n    also set solver to ""variance"", ""correlation"" or ""euclidean"".\n    [Added 8/12/18] [Edited 12/12/18 Uses argpartititon]\n\n    Parameters\n    -----------\n    X:              General Matrix.\n    y:              Optional. Used for ""correlation"" solver.\n    n_components:   How many columns to orthogonalise. Default = 2\n    solver:         (euclidean, variance, correlation) Which method\n                    to choose columns. Default = euclidean.\n    n_oversamples:  Samples more components than necessary. Used for\n                    convergence purposes. More is slower, but allows\n                    better eigenvectors. Default = 5\n    Returns\n    -----------\n    (Q,R) or (Q) or (R) depends on option Q_only or R_only\n    """"""\n    n, p = X.shape\n    k = n_components + n_oversamples\n    if k > p: k = p\n    kk = k*-1\n\n    if solver == ""correlation"":\n        if y is None:\n            C = corr_sum( corr(X, X) )\n        else:\n            C = abs( corr(X, y) )\n        P = np.argpartition( C, k )[:k]\n    elif solver == ""variance"":\n        P = np.argpartition( var(X,0), kk)[kk:]\n    else:\n        P = np.argpartition( col_norm(X), kk)[kk:]\n\n    # Gram Schmidt process\n    Q = gram_schmidt(X, P, n, k)\n\n    if Q_only:\n        return Q.T\n\n    # X = QR -> QT X = QT Q R -> QT X = R\n    R = Q @ X\n    if R_only:\n        return R\n    return Q.T, R\n\n'"
