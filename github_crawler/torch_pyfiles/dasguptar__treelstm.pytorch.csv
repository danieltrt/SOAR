file_path,api_count,code
config.py,0,"b""import argparse\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='PyTorch TreeLSTM for Sentence Similarity on Dependency Trees')\n    # data arguments\n    parser.add_argument('--data', default='data/sick/',\n                        help='path to dataset')\n    parser.add_argument('--glove', default='data/glove/',\n                        help='directory with GLOVE embeddings')\n    parser.add_argument('--save', default='checkpoints/',\n                        help='directory to save checkpoints in')\n    parser.add_argument('--expname', type=str, default='test',\n                        help='Name to identify experiment')\n    # model arguments\n    parser.add_argument('--input_dim', default=300, type=int,\n                        help='Size of input word vector')\n    parser.add_argument('--mem_dim', default=150, type=int,\n                        help='Size of TreeLSTM cell state')\n    parser.add_argument('--hidden_dim', default=50, type=int,\n                        help='Size of classifier MLP')\n    parser.add_argument('--num_classes', default=5, type=int,\n                        help='Number of classes in dataset')\n    parser.add_argument('--freeze_embed', action='store_true',\n                        help='Freeze word embeddings')\n    # training arguments\n    parser.add_argument('--epochs', default=15, type=int,\n                        help='number of total epochs to run')\n    parser.add_argument('--batchsize', default=25, type=int,\n                        help='batchsize for optimizer updates')\n    parser.add_argument('--lr', default=0.01, type=float,\n                        metavar='LR', help='initial learning rate')\n    parser.add_argument('--wd', default=1e-4, type=float,\n                        help='weight decay (default: 1e-4)')\n    parser.add_argument('--sparse', action='store_true',\n                        help='Enable sparsity for embeddings, \\\n                              incompatible with weight decay')\n    parser.add_argument('--optim', default='adagrad',\n                        help='optimizer (default: adagrad)')\n    # miscellaneous options\n    parser.add_argument('--seed', default=123, type=int,\n                        help='random seed (default: 123)')\n    cuda_parser = parser.add_mutually_exclusive_group(required=False)\n    cuda_parser.add_argument('--cuda', dest='cuda', action='store_true')\n    cuda_parser.add_argument('--no-cuda', dest='cuda', action='store_false')\n    parser.set_defaults(cuda=True)\n\n    args = parser.parse_args()\n    return args\n"""
main.py,17,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport random\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# IMPORT CONSTANTS\nfrom treelstm import Constants\n# NEURAL NETWORK MODULES/LAYERS\nfrom treelstm import SimilarityTreeLSTM\n# DATA HANDLING CLASSES\nfrom treelstm import Vocab\n# DATASET CLASS FOR SICK DATASET\nfrom treelstm import SICKDataset\n# METRICS CLASS FOR EVALUATION\nfrom treelstm import Metrics\n# UTILITY FUNCTIONS\nfrom treelstm import utils\n# TRAIN AND TEST HELPER FUNCTIONS\nfrom treelstm import Trainer\n# CONFIG PARSER\nfrom config import parse_args\n\n\n# MAIN BLOCK\ndef main():\n    global args\n    args = parse_args()\n    # global logger\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""[%(asctime)s] %(levelname)s:%(name)s:%(message)s"")\n    # file logger\n    fh = logging.FileHandler(os.path.join(args.save, args.expname)+\'.log\', mode=\'w\')\n    fh.setLevel(logging.INFO)\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n    # console logger\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    # argument validation\n    args.cuda = args.cuda and torch.cuda.is_available()\n    device = torch.device(""cuda:0"" if args.cuda else ""cpu"")\n    if args.sparse and args.wd != 0:\n        logger.error(\'Sparsity and weight decay are incompatible, pick one!\')\n        exit()\n    logger.debug(args)\n    torch.manual_seed(args.seed)\n    random.seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed)\n        torch.backends.cudnn.benchmark = True\n    if not os.path.exists(args.save):\n        os.makedirs(args.save)\n\n    train_dir = os.path.join(args.data, \'train/\')\n    dev_dir = os.path.join(args.data, \'dev/\')\n    test_dir = os.path.join(args.data, \'test/\')\n\n    # write unique words from all token files\n    sick_vocab_file = os.path.join(args.data, \'sick.vocab\')\n    if not os.path.isfile(sick_vocab_file):\n        token_files_b = [os.path.join(split, \'b.toks\') for split in [train_dir, dev_dir, test_dir]]\n        token_files_a = [os.path.join(split, \'a.toks\') for split in [train_dir, dev_dir, test_dir]]\n        token_files = token_files_a + token_files_b\n        sick_vocab_file = os.path.join(args.data, \'sick.vocab\')\n        utils.build_vocab(token_files, sick_vocab_file)\n\n    # get vocab object from vocab file previously written\n    vocab = Vocab(filename=sick_vocab_file,\n                  data=[Constants.PAD_WORD, Constants.UNK_WORD,\n                        Constants.BOS_WORD, Constants.EOS_WORD])\n    logger.debug(\'==> SICK vocabulary size : %d \' % vocab.size())\n\n    # load SICK dataset splits\n    train_file = os.path.join(args.data, \'sick_train.pth\')\n    if os.path.isfile(train_file):\n        train_dataset = torch.load(train_file)\n    else:\n        train_dataset = SICKDataset(train_dir, vocab, args.num_classes)\n        torch.save(train_dataset, train_file)\n    logger.debug(\'==> Size of train data   : %d \' % len(train_dataset))\n    dev_file = os.path.join(args.data, \'sick_dev.pth\')\n    if os.path.isfile(dev_file):\n        dev_dataset = torch.load(dev_file)\n    else:\n        dev_dataset = SICKDataset(dev_dir, vocab, args.num_classes)\n        torch.save(dev_dataset, dev_file)\n    logger.debug(\'==> Size of dev data     : %d \' % len(dev_dataset))\n    test_file = os.path.join(args.data, \'sick_test.pth\')\n    if os.path.isfile(test_file):\n        test_dataset = torch.load(test_file)\n    else:\n        test_dataset = SICKDataset(test_dir, vocab, args.num_classes)\n        torch.save(test_dataset, test_file)\n    logger.debug(\'==> Size of test data    : %d \' % len(test_dataset))\n\n    # initialize model, criterion/loss_function, optimizer\n    model = SimilarityTreeLSTM(\n        vocab.size(),\n        args.input_dim,\n        args.mem_dim,\n        args.hidden_dim,\n        args.num_classes,\n        args.sparse,\n        args.freeze_embed)\n    criterion = nn.KLDivLoss()\n\n    # for words common to dataset vocab and GLOVE, use GLOVE vectors\n    # for other words in dataset vocab, use random normal vectors\n    emb_file = os.path.join(args.data, \'sick_embed.pth\')\n    if os.path.isfile(emb_file):\n        emb = torch.load(emb_file)\n    else:\n        # load glove embeddings and vocab\n        glove_vocab, glove_emb = utils.load_word_vectors(\n            os.path.join(args.glove, \'glove.840B.300d\'))\n        logger.debug(\'==> GLOVE vocabulary size: %d \' % glove_vocab.size())\n        emb = torch.zeros(vocab.size(), glove_emb.size(1), dtype=torch.float, device=device)\n        emb.normal_(0, 0.05)\n        # zero out the embeddings for padding and other special words if they are absent in vocab\n        for idx, item in enumerate([Constants.PAD_WORD, Constants.UNK_WORD,\n                                    Constants.BOS_WORD, Constants.EOS_WORD]):\n            emb[idx].zero_()\n        for word in vocab.labelToIdx.keys():\n            if glove_vocab.getIndex(word):\n                emb[vocab.getIndex(word)] = glove_emb[glove_vocab.getIndex(word)]\n        torch.save(emb, emb_file)\n    # plug these into embedding matrix inside model\n    model.emb.weight.data.copy_(emb)\n\n    model.to(device), criterion.to(device)\n    if args.optim == \'adam\':\n        optimizer = optim.Adam(filter(lambda p: p.requires_grad,\n                                      model.parameters()), lr=args.lr, weight_decay=args.wd)\n    elif args.optim == \'adagrad\':\n        optimizer = optim.Adagrad(filter(lambda p: p.requires_grad,\n                                         model.parameters()), lr=args.lr, weight_decay=args.wd)\n    elif args.optim == \'sgd\':\n        optimizer = optim.SGD(filter(lambda p: p.requires_grad,\n                                     model.parameters()), lr=args.lr, weight_decay=args.wd)\n    metrics = Metrics(args.num_classes)\n\n    # create trainer object for training and testing\n    trainer = Trainer(args, model, criterion, optimizer, device)\n\n    best = -float(\'inf\')\n    for epoch in range(args.epochs):\n        train_loss = trainer.train(train_dataset)\n        train_loss, train_pred = trainer.test(train_dataset)\n        dev_loss, dev_pred = trainer.test(dev_dataset)\n        test_loss, test_pred = trainer.test(test_dataset)\n\n        train_pearson = metrics.pearson(train_pred, train_dataset.labels)\n        train_mse = metrics.mse(train_pred, train_dataset.labels)\n        logger.info(\'==> Epoch {}, Train \\tLoss: {}\\tPearson: {}\\tMSE: {}\'.format(\n            epoch, train_loss, train_pearson, train_mse))\n        dev_pearson = metrics.pearson(dev_pred, dev_dataset.labels)\n        dev_mse = metrics.mse(dev_pred, dev_dataset.labels)\n        logger.info(\'==> Epoch {}, Dev \\tLoss: {}\\tPearson: {}\\tMSE: {}\'.format(\n            epoch, dev_loss, dev_pearson, dev_mse))\n        test_pearson = metrics.pearson(test_pred, test_dataset.labels)\n        test_mse = metrics.mse(test_pred, test_dataset.labels)\n        logger.info(\'==> Epoch {}, Test \\tLoss: {}\\tPearson: {}\\tMSE: {}\'.format(\n            epoch, test_loss, test_pearson, test_mse))\n\n        if best < test_pearson:\n            best = test_pearson\n            checkpoint = {\n                \'model\': trainer.model.state_dict(),\n                \'optim\': trainer.optimizer,\n                \'pearson\': test_pearson, \'mse\': test_mse,\n                \'args\': args, \'epoch\': epoch\n            }\n            logger.debug(\'==> New optimum found, checkpointing everything now...\')\n            torch.save(checkpoint, \'%s.pt\' % os.path.join(args.save, args.expname))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/download.py,0,"b'""""""\nDownloads the following:\n- Stanford parser\n- Stanford POS tagger\n- Glove vectors\n- SICK dataset (semantic relatedness task)\n""""""\n\nfrom __future__ import print_function\nimport urllib2\nimport sys\nimport os\nimport zipfile\n\n\ndef download(url, dirpath):\n    filename = url.split(\'/\')[-1]\n    filepath = os.path.join(dirpath, filename)\n    try:\n        u = urllib2.urlopen(url)\n    except Exception as e:\n        print(""URL %s failed to open"" % url)\n        raise Exception\n    try:\n        f = open(filepath, \'wb\')\n    except Exception as e:\n        print(""Cannot write %s"" % filepath)\n        raise Exception\n    try:\n        filesize = int(u.info().getheaders(""Content-Length"")[0])\n    except Exception as e:\n        print(""URL %s failed to report length"" % url)\n        raise Exception\n    print(""Downloading: %s Bytes: %s"" % (filename, filesize))\n\n    downloaded = 0\n    block_sz = 8192\n    status_width = 70\n    while True:\n        buf = u.read(block_sz)\n        if not buf:\n            print(\'\')\n            break\n        else:\n            print(\'\', end=\'\\r\')\n        downloaded += len(buf)\n        f.write(buf)\n        status = ((""[%-"" + str(status_width + 1) + ""s] %3.2f%%"") %\n                  (\'=\' * int(downloaded / filesize * status_width) + \'>\',\n                   downloaded * 100. / filesize))\n        print(status, end=\'\')\n        sys.stdout.flush()\n    f.close()\n    return filepath\n\n\ndef unzip(filepath):\n    print(""Extracting: "" + filepath)\n    dirpath = os.path.dirname(filepath)\n    with zipfile.ZipFile(filepath) as zf:\n        zf.extractall(dirpath)\n    os.remove(filepath)\n\n\ndef download_tagger(dirpath):\n    tagger_dir = \'stanford-tagger\'\n    if os.path.exists(os.path.join(dirpath, tagger_dir)):\n        print(\'Found Stanford POS Tagger - skip\')\n        return\n    url = \'http://nlp.stanford.edu/software/stanford-postagger-2015-01-29.zip\'\n    filepath = download(url, dirpath)\n    zip_dir = \'\'\n    with zipfile.ZipFile(filepath) as zf:\n        zip_dir = zf.namelist()[0]\n        zf.extractall(dirpath)\n    os.remove(filepath)\n    os.rename(os.path.join(dirpath, zip_dir), os.path.join(dirpath, tagger_dir))\n\n\ndef download_parser(dirpath):\n    parser_dir = \'stanford-parser\'\n    if os.path.exists(os.path.join(dirpath, parser_dir)):\n        print(\'Found Stanford Parser - skip\')\n        return\n    url = \'http://nlp.stanford.edu/software/stanford-parser-full-2015-01-29.zip\'\n    filepath = download(url, dirpath)\n    zip_dir = \'\'\n    with zipfile.ZipFile(filepath) as zf:\n        zip_dir = zf.namelist()[0]\n        zf.extractall(dirpath)\n    os.remove(filepath)\n    os.rename(os.path.join(dirpath, zip_dir), os.path.join(dirpath, parser_dir))\n\n\ndef download_wordvecs(dirpath):\n    if os.path.exists(dirpath):\n        print(\'Found Glove vectors - skip\')\n        return\n    else:\n        os.makedirs(dirpath)\n    url = \'http://www-nlp.stanford.edu/data/glove.840B.300d.zip\'\n    unzip(download(url, dirpath))\n\n\ndef download_sick(dirpath):\n    if os.path.exists(dirpath):\n        print(\'Found SICK dataset - skip\')\n        return\n    else:\n        os.makedirs(dirpath)\n    train_url = \'http://alt.qcri.org/semeval2014/task1/data/uploads/sick_train.zip\'\n    trial_url = \'http://alt.qcri.org/semeval2014/task1/data/uploads/sick_trial.zip\'\n    test_url = \'http://alt.qcri.org/semeval2014/task1/data/uploads/sick_test_annotated.zip\'\n    unzip(download(train_url, dirpath))\n    unzip(download(trial_url, dirpath))\n    unzip(download(test_url, dirpath))\n\n\nif __name__ == \'__main__\':\n    base_dir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n\n    # data\n    data_dir = os.path.join(base_dir, \'data\')\n    wordvec_dir = os.path.join(data_dir, \'glove\')\n    sick_dir = os.path.join(data_dir, \'sick\')\n\n    # libraries\n    lib_dir = os.path.join(base_dir, \'lib\')\n\n    # download dependencies\n    download_tagger(lib_dir)\n    download_parser(lib_dir)\n    download_wordvecs(wordvec_dir)\n    download_sick(sick_dir)\n'"
scripts/preprocess-sick.py,0,"b'""""""\nPreprocessing script for SICK data.\n\n""""""\n\nimport os\nimport glob\n\n\ndef make_dirs(dirs):\n    for d in dirs:\n        if not os.path.exists(d):\n            os.makedirs(d)\n\n\ndef dependency_parse(filepath, cp=\'\', tokenize=True):\n    print(\'\\nDependency parsing \' + filepath)\n    dirpath = os.path.dirname(filepath)\n    filepre = os.path.splitext(os.path.basename(filepath))[0]\n    tokpath = os.path.join(dirpath, filepre + \'.toks\')\n    parentpath = os.path.join(dirpath, filepre + \'.parents\')\n    relpath = os.path.join(dirpath, filepre + \'.rels\')\n    tokenize_flag = \'-tokenize - \' if tokenize else \'\'\n    cmd = (\'java -cp %s DependencyParse -tokpath %s -parentpath %s -relpath %s %s < %s\'\n           % (cp, tokpath, parentpath, relpath, tokenize_flag, filepath))\n    os.system(cmd)\n\n\ndef constituency_parse(filepath, cp=\'\', tokenize=True):\n    dirpath = os.path.dirname(filepath)\n    filepre = os.path.splitext(os.path.basename(filepath))[0]\n    tokpath = os.path.join(dirpath, filepre + \'.toks\')\n    parentpath = os.path.join(dirpath, filepre + \'.cparents\')\n    tokenize_flag = \'-tokenize - \' if tokenize else \'\'\n    cmd = (\'java -cp %s ConstituencyParse -tokpath %s -parentpath %s %s < %s\'\n           % (cp, tokpath, parentpath, tokenize_flag, filepath))\n    os.system(cmd)\n\n\ndef build_vocab(filepaths, dst_path, lowercase=True):\n    vocab = set()\n    for filepath in filepaths:\n        with open(filepath) as f:\n            for line in f:\n                if lowercase:\n                    line = line.lower()\n                vocab |= set(line.split())\n    with open(dst_path, \'w\') as f:\n        for w in sorted(vocab):\n            f.write(w + \'\\n\')\n\n\ndef split(filepath, dst_dir):\n    with open(filepath) as datafile, \\\n            open(os.path.join(dst_dir, \'a.txt\'), \'w\') as afile, \\\n            open(os.path.join(dst_dir, \'b.txt\'), \'w\') as bfile,  \\\n            open(os.path.join(dst_dir, \'id.txt\'), \'w\') as idfile, \\\n            open(os.path.join(dst_dir, \'sim.txt\'), \'w\') as simfile:\n        datafile.readline()\n        for line in datafile:\n            i, a, b, sim, ent = line.strip().split(\'\\t\')\n            idfile.write(i + \'\\n\')\n            afile.write(a + \'\\n\')\n            bfile.write(b + \'\\n\')\n            simfile.write(sim + \'\\n\')\n\n\ndef parse(dirpath, cp=\'\'):\n    dependency_parse(os.path.join(dirpath, \'a.txt\'), cp=cp, tokenize=True)\n    dependency_parse(os.path.join(dirpath, \'b.txt\'), cp=cp, tokenize=True)\n    constituency_parse(os.path.join(dirpath, \'a.txt\'), cp=cp, tokenize=True)\n    constituency_parse(os.path.join(dirpath, \'b.txt\'), cp=cp, tokenize=True)\n\n\nif __name__ == \'__main__\':\n    print(\'=\' * 80)\n    print(\'Preprocessing SICK dataset\')\n    print(\'=\' * 80)\n\n    base_dir = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))\n    data_dir = os.path.join(base_dir, \'data\')\n    sick_dir = os.path.join(data_dir, \'sick\')\n    lib_dir = os.path.join(base_dir, \'lib\')\n    train_dir = os.path.join(sick_dir, \'train\')\n    dev_dir = os.path.join(sick_dir, \'dev\')\n    test_dir = os.path.join(sick_dir, \'test\')\n    make_dirs([train_dir, dev_dir, test_dir])\n\n    # java classpath for calling Stanford parser\n    classpath = \':\'.join([\n        lib_dir,\n        os.path.join(lib_dir, \'stanford-parser/stanford-parser.jar\'),\n        os.path.join(lib_dir, \'stanford-parser/stanford-parser-3.5.1-models.jar\')])\n\n    # split into separate files\n    split(os.path.join(sick_dir, \'SICK_train.txt\'), train_dir)\n    split(os.path.join(sick_dir, \'SICK_trial.txt\'), dev_dir)\n    split(os.path.join(sick_dir, \'SICK_test_annotated.txt\'), test_dir)\n\n    # parse sentences\n    parse(train_dir, cp=classpath)\n    parse(dev_dir, cp=classpath)\n    parse(test_dir, cp=classpath)\n\n    # get vocabulary\n    build_vocab(\n        glob.glob(os.path.join(sick_dir, \'*/*.toks\')),\n        os.path.join(sick_dir, \'vocab.txt\'))\n    build_vocab(\n        glob.glob(os.path.join(sick_dir, \'*/*.toks\')),\n        os.path.join(sick_dir, \'vocab-cased.txt\'),\n        lowercase=False)\n'"
treelstm/Constants.py,0,"b""PAD = 0\nUNK = 1\nBOS = 2\nEOS = 3\n\nPAD_WORD = '<blank>'\nUNK_WORD = '<unk>'\nBOS_WORD = '<s>'\nEOS_WORD = '</s>'\n"""
treelstm/__init__.py,0,"b'from . import Constants\nfrom .dataset import SICKDataset\nfrom .metrics import Metrics\nfrom .model import SimilarityTreeLSTM\nfrom .trainer import Trainer\nfrom .tree import Tree\nfrom . import utils\nfrom .vocab import Vocab\n\n__all__ = [Constants, SICKDataset, Metrics, SimilarityTreeLSTM, Trainer, Tree, Vocab, utils]\n'"
treelstm/dataset.py,3,"b""import os\nfrom tqdm import tqdm\nfrom copy import deepcopy\n\nimport torch\nimport torch.utils.data as data\n\nfrom . import Constants\nfrom .tree import Tree\n\n\n# Dataset class for SICK dataset\nclass SICKDataset(data.Dataset):\n    def __init__(self, path, vocab, num_classes):\n        super(SICKDataset, self).__init__()\n        self.vocab = vocab\n        self.num_classes = num_classes\n\n        self.lsentences = self.read_sentences(os.path.join(path, 'a.toks'))\n        self.rsentences = self.read_sentences(os.path.join(path, 'b.toks'))\n\n        self.ltrees = self.read_trees(os.path.join(path, 'a.parents'))\n        self.rtrees = self.read_trees(os.path.join(path, 'b.parents'))\n\n        self.labels = self.read_labels(os.path.join(path, 'sim.txt'))\n\n        self.size = self.labels.size(0)\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, index):\n        ltree = deepcopy(self.ltrees[index])\n        rtree = deepcopy(self.rtrees[index])\n        lsent = deepcopy(self.lsentences[index])\n        rsent = deepcopy(self.rsentences[index])\n        label = deepcopy(self.labels[index])\n        return (ltree, lsent, rtree, rsent, label)\n\n    def read_sentences(self, filename):\n        with open(filename, 'r') as f:\n            sentences = [self.read_sentence(line) for line in tqdm(f.readlines())]\n        return sentences\n\n    def read_sentence(self, line):\n        indices = self.vocab.convertToIdx(line.split(), Constants.UNK_WORD)\n        return torch.tensor(indices, dtype=torch.long, device='cpu')\n\n    def read_trees(self, filename):\n        with open(filename, 'r') as f:\n            trees = [self.read_tree(line) for line in tqdm(f.readlines())]\n        return trees\n\n    def read_tree(self, line):\n        parents = list(map(int, line.split()))\n        trees = dict()\n        root = None\n        for i in range(1, len(parents) + 1):\n            if i - 1 not in trees.keys() and parents[i - 1] != -1:\n                idx = i\n                prev = None\n                while True:\n                    parent = parents[idx - 1]\n                    if parent == -1:\n                        break\n                    tree = Tree()\n                    if prev is not None:\n                        tree.add_child(prev)\n                    trees[idx - 1] = tree\n                    tree.idx = idx - 1\n                    if parent - 1 in trees.keys():\n                        trees[parent - 1].add_child(tree)\n                        break\n                    elif parent == 0:\n                        root = tree\n                        break\n                    else:\n                        prev = tree\n                        idx = parent\n        return root\n\n    def read_labels(self, filename):\n        with open(filename, 'r') as f:\n            labels = list(map(lambda x: float(x), f.readlines()))\n            labels = torch.tensor(labels, dtype=torch.float, device='cpu')\n        return labels\n"""
treelstm/metrics.py,2,"b'from copy import deepcopy\n\nimport torch\n\n\nclass Metrics():\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n\n    def pearson(self, predictions, labels):\n        x = deepcopy(predictions)\n        y = deepcopy(labels)\n        x = (x - x.mean()) / x.std()\n        y = (y - y.mean()) / y.std()\n        return torch.mean(torch.mul(x, y))\n\n    def mse(self, predictions, labels):\n        x = deepcopy(predictions)\n        y = deepcopy(labels)\n        return torch.mean((x - y) ** 2)\n'"
treelstm/model.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom . import Constants\n\n\n# module for childsumtreelstm\nclass ChildSumTreeLSTM(nn.Module):\n    def __init__(self, in_dim, mem_dim):\n        super(ChildSumTreeLSTM, self).__init__()\n        self.in_dim = in_dim\n        self.mem_dim = mem_dim\n        self.ioux = nn.Linear(self.in_dim, 3 * self.mem_dim)\n        self.iouh = nn.Linear(self.mem_dim, 3 * self.mem_dim)\n        self.fx = nn.Linear(self.in_dim, self.mem_dim)\n        self.fh = nn.Linear(self.mem_dim, self.mem_dim)\n\n    def node_forward(self, inputs, child_c, child_h):\n        child_h_sum = torch.sum(child_h, dim=0, keepdim=True)\n\n        iou = self.ioux(inputs) + self.iouh(child_h_sum)\n        i, o, u = torch.split(iou, iou.size(1) // 3, dim=1)\n        i, o, u = F.sigmoid(i), F.sigmoid(o), F.tanh(u)\n\n        f = F.sigmoid(\n            self.fh(child_h) +\n            self.fx(inputs).repeat(len(child_h), 1)\n        )\n        fc = torch.mul(f, child_c)\n\n        c = torch.mul(i, u) + torch.sum(fc, dim=0, keepdim=True)\n        h = torch.mul(o, F.tanh(c))\n        return c, h\n\n    def forward(self, tree, inputs):\n        for idx in range(tree.num_children):\n            self.forward(tree.children[idx], inputs)\n\n        if tree.num_children == 0:\n            child_c = inputs[0].detach().new(1, self.mem_dim).fill_(0.).requires_grad_()\n            child_h = inputs[0].detach().new(1, self.mem_dim).fill_(0.).requires_grad_()\n        else:\n            child_c, child_h = zip(* map(lambda x: x.state, tree.children))\n            child_c, child_h = torch.cat(child_c, dim=0), torch.cat(child_h, dim=0)\n\n        tree.state = self.node_forward(inputs[tree.idx], child_c, child_h)\n        return tree.state\n\n\n# module for distance-angle similarity\nclass Similarity(nn.Module):\n    def __init__(self, mem_dim, hidden_dim, num_classes):\n        super(Similarity, self).__init__()\n        self.mem_dim = mem_dim\n        self.hidden_dim = hidden_dim\n        self.num_classes = num_classes\n        self.wh = nn.Linear(2 * self.mem_dim, self.hidden_dim)\n        self.wp = nn.Linear(self.hidden_dim, self.num_classes)\n\n    def forward(self, lvec, rvec):\n        mult_dist = torch.mul(lvec, rvec)\n        abs_dist = torch.abs(torch.add(lvec, -rvec))\n        vec_dist = torch.cat((mult_dist, abs_dist), 1)\n\n        out = F.sigmoid(self.wh(vec_dist))\n        out = F.log_softmax(self.wp(out), dim=1)\n        return out\n\n\n# putting the whole model together\nclass SimilarityTreeLSTM(nn.Module):\n    def __init__(self, vocab_size, in_dim, mem_dim, hidden_dim, num_classes, sparsity, freeze):\n        super(SimilarityTreeLSTM, self).__init__()\n        self.emb = nn.Embedding(vocab_size, in_dim, padding_idx=Constants.PAD, sparse=sparsity)\n        if freeze:\n            self.emb.weight.requires_grad = False\n        self.childsumtreelstm = ChildSumTreeLSTM(in_dim, mem_dim)\n        self.similarity = Similarity(mem_dim, hidden_dim, num_classes)\n\n    def forward(self, ltree, linputs, rtree, rinputs):\n        linputs = self.emb(linputs)\n        rinputs = self.emb(rinputs)\n        lstate, lhidden = self.childsumtreelstm(ltree, linputs)\n        rstate, rhidden = self.childsumtreelstm(rtree, rinputs)\n        output = self.similarity(lstate, rstate)\n        return output\n'"
treelstm/trainer.py,5,"b""from tqdm import tqdm\n\nimport torch\n\nfrom . import utils\n\n\nclass Trainer(object):\n    def __init__(self, args, model, criterion, optimizer, device):\n        super(Trainer, self).__init__()\n        self.args = args\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.device = device\n        self.epoch = 0\n\n    # helper function for training\n    def train(self, dataset):\n        self.model.train()\n        self.optimizer.zero_grad()\n        total_loss = 0.0\n        indices = torch.randperm(len(dataset), dtype=torch.long, device='cpu')\n        for idx in tqdm(range(len(dataset)), desc='Training epoch ' + str(self.epoch + 1) + ''):\n            ltree, linput, rtree, rinput, label = dataset[indices[idx]]\n            target = utils.map_label_to_target(label, dataset.num_classes)\n            linput, rinput = linput.to(self.device), rinput.to(self.device)\n            target = target.to(self.device)\n            output = self.model(ltree, linput, rtree, rinput)\n            loss = self.criterion(output, target)\n            total_loss += loss.item()\n            loss.backward()\n            if idx % self.args.batchsize == 0 and idx > 0:\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n        self.epoch += 1\n        return total_loss / len(dataset)\n\n    # helper function for testing\n    def test(self, dataset):\n        self.model.eval()\n        with torch.no_grad():\n            total_loss = 0.0\n            predictions = torch.zeros(len(dataset), dtype=torch.float, device='cpu')\n            indices = torch.arange(1, dataset.num_classes + 1, dtype=torch.float, device='cpu')\n            for idx in tqdm(range(len(dataset)), desc='Testing epoch  ' + str(self.epoch) + ''):\n                ltree, linput, rtree, rinput, label = dataset[idx]\n                target = utils.map_label_to_target(label, dataset.num_classes)\n                linput, rinput = linput.to(self.device), rinput.to(self.device)\n                target = target.to(self.device)\n                output = self.model(ltree, linput, rtree, rinput)\n                loss = self.criterion(output, target)\n                total_loss += loss.item()\n                output = output.squeeze().to('cpu')\n                predictions[idx] = torch.dot(indices, torch.exp(output))\n        return total_loss / len(dataset), predictions\n"""
treelstm/tree.py,0,"b""# tree object from stanfordnlp/treelstm\nclass Tree(object):\n    def __init__(self):\n        self.parent = None\n        self.num_children = 0\n        self.children = list()\n\n    def add_child(self, child):\n        child.parent = self\n        self.num_children += 1\n        self.children.append(child)\n\n    def size(self):\n        if getattr(self, '_size'):\n            return self._size\n        count = 1\n        for i in range(self.num_children):\n            count += self.children[i].size()\n        self._size = count\n        return self._size\n\n    def depth(self):\n        if getattr(self, '_depth'):\n            return self._depth\n        count = 0\n        if self.num_children > 0:\n            for i in range(self.num_children):\n                child_depth = self.children[i].depth()\n                if child_depth > count:\n                    count = child_depth\n            count += 1\n        self._depth = count\n        return self._depth\n"""
treelstm/utils.py,5,"b""from __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport math\n\nimport torch\n\nfrom .vocab import Vocab\n\n\n# loading GLOVE word vectors\n# if .pth file is found, will load that\n# else will load from .txt file & save\ndef load_word_vectors(path):\n    if os.path.isfile(path + '.pth') and os.path.isfile(path + '.vocab'):\n        print('==> File found, loading to memory')\n        vectors = torch.load(path + '.pth')\n        vocab = Vocab(filename=path + '.vocab')\n        return vocab, vectors\n    # saved file not found, read from txt file\n    # and create tensors for word vectors\n    print('==> File not found, preparing, be patient')\n    count = sum(1 for line in open(path + '.txt', 'r', encoding='utf8', errors='ignore'))\n    with open(path + '.txt', 'r') as f:\n        contents = f.readline().rstrip('\\n').split(' ')\n        dim = len(contents[1:])\n    words = [None] * (count)\n    vectors = torch.zeros(count, dim, dtype=torch.float, device='cpu')\n    with open(path + '.txt', 'r', encoding='utf8', errors='ignore') as f:\n        idx = 0\n        for line in f:\n            contents = line.rstrip('\\n').split(' ')\n            words[idx] = contents[0]\n            values = list(map(float, contents[1:]))\n            vectors[idx] = torch.tensor(values, dtype=torch.float, device='cpu')\n            idx += 1\n    with open(path + '.vocab', 'w', encoding='utf8', errors='ignore') as f:\n        for word in words:\n            f.write(word + '\\n')\n    vocab = Vocab(filename=path + '.vocab')\n    torch.save(vectors, path + '.pth')\n    return vocab, vectors\n\n\n# write unique words from a set of files to a new file\ndef build_vocab(filenames, vocabfile):\n    vocab = set()\n    for filename in filenames:\n        with open(filename, 'r') as f:\n            for line in f:\n                tokens = line.rstrip('\\n').split(' ')\n                vocab |= set(tokens)\n    with open(vocabfile, 'w') as f:\n        for token in sorted(vocab):\n            f.write(token + '\\n')\n\n\n# mapping from scalar to vector\ndef map_label_to_target(label, num_classes):\n    target = torch.zeros(1, num_classes, dtype=torch.float, device='cpu')\n    ceil = int(math.ceil(label))\n    floor = int(math.floor(label))\n    if ceil == floor:\n        target[0, floor-1] = 1\n    else:\n        target[0, floor-1] = ceil - label\n        target[0, ceil-1] = label - floor\n    return target\n"""
treelstm/vocab.py,0,"b""# vocab object from harvardnlp/opennmt-py\nclass Vocab(object):\n    def __init__(self, filename=None, data=None, lower=False):\n        self.idxToLabel = {}\n        self.labelToIdx = {}\n        self.lower = lower\n\n        # Special entries will not be pruned.\n        self.special = []\n\n        if data is not None:\n            self.addSpecials(data)\n        if filename is not None:\n            self.loadFile(filename)\n\n    def size(self):\n        return len(self.idxToLabel)\n\n    # Load entries from a file.\n    def loadFile(self, filename):\n        idx = 0\n        for line in open(filename, 'r', encoding='utf8', errors='ignore'):\n            token = line.rstrip('\\n')\n            self.add(token)\n            idx += 1\n\n    def getIndex(self, key, default=None):\n        key = key.lower() if self.lower else key\n        try:\n            return self.labelToIdx[key]\n        except KeyError:\n            return default\n\n    def getLabel(self, idx, default=None):\n        try:\n            return self.idxToLabel[idx]\n        except KeyError:\n            return default\n\n    # Mark this `label` and `idx` as special\n    def addSpecial(self, label, idx=None):\n        idx = self.add(label)\n        self.special += [idx]\n\n    # Mark all labels in `labels` as specials\n    def addSpecials(self, labels):\n        for label in labels:\n            self.addSpecial(label)\n\n    # Add `label` in the dictionary. Use `idx` as its index if given.\n    def add(self, label):\n        label = label.lower() if self.lower else label\n        if label in self.labelToIdx:\n            idx = self.labelToIdx[label]\n        else:\n            idx = len(self.idxToLabel)\n            self.idxToLabel[idx] = label\n            self.labelToIdx[label] = idx\n        return idx\n\n    # Convert `labels` to indices. Use `unkWord` if not found.\n    # Optionally insert `bosWord` at the beginning and `eosWord` at the .\n    def convertToIdx(self, labels, unkWord, bosWord=None, eosWord=None):\n        vec = []\n\n        if bosWord is not None:\n            vec += [self.getIndex(bosWord)]\n\n        unk = self.getIndex(unkWord)\n        vec += [self.getIndex(label, default=unk) for label in labels]\n\n        if eosWord is not None:\n            vec += [self.getIndex(eosWord)]\n\n        return vec\n\n    # Convert `idx` to labels. If index `stop` is reached, convert it and return.\n    def convertToLabels(self, idx, stop):\n        labels = []\n\n        for i in idx:\n            labels += [self.getLabel(i)]\n            if i == stop:\n                break\n\n        return labels\n"""
