file_path,api_count,code
tbd/__init__.py,0,b''
tbd/module_net.py,9,"b'# DISTRIBUTION STATEMENT A. Approved for public release: distribution unlimited.\n#\n# This material is based upon work supported by the Assistant Secretary of Defense for Research and\n# Engineering under Air Force Contract No. FA8721-05-C-0002 and/or FA8702-15-D-0001. Any opinions,\n# findings, conclusions or recommendations expressed in this material are those of the author(s) and\n# do not necessarily reflect the views of the Assistant Secretary of Defense for Research and\n# Engineering.\n#\n# \xc2\xa9 2017 Massachusetts Institute of Technology.\n#\n# MIT Proprietary, Subject to FAR52.227-11 Patent Rights - Ownership by the contractor (May 2014)\n#\n# The software/firmware is provided to you on an As-Is basis\n#\n# Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or\n# 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are\n# defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than\n# as specifically authorized by the U.S. Government may violate any copyrights that exist in this\n# work.\n\nimport torch\nimport torch.nn as nn\n\nfrom . import modules\n\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\nclass TbDNet(nn.Module):\n    """""" The real deal. A full Transparency by Design network (TbD-net).\n\n    Extended Summary\n    ----------------\n    A :class:`TbDNet` holds neural :mod:`modules`, a stem network, and a classifier network. It\n    hooks these all together to answer a question given some scene and a program describing how to\n    arrange the neural modules.\n    """"""\n    def __init__(self,\n                 vocab,\n                 feature_dim=(512, 28, 28),\n                 module_dim=128,\n                 cls_proj_dim=512,\n                 fc_dim=1024):\n        """""" Initializes a TbDNet object.\n\n        Parameters\n        ----------\n        vocab : Dict[str, Dict[Any, Any]]\n            The vocabulary holds dictionaries that provide handles to various objects. Valid keys \n            into vocab are\n            - \'answer_idx_to_token\' whose keys are ints and values strings\n            - \'answer_token_to_idx\' whose keys are strings and values ints\n            - \'program_idx_to_token\' whose keys are ints and values strings\n            - \'program_token_to_idx\' whose keys are strings and values ints\n            These value dictionaries provide retrieval of an answer word or program token from an\n            index, or an index from a word or program token.\n\n        feature_dim : the tuple (K, R, C), optional\n            The shape of input feature tensors, excluding the batch size.\n\n        module_dim : int, optional\n            The depth of each neural module\'s convolutional blocks.\n\n        cls_proj_dim : int, optional\n            The depth to project the final feature map to before classification.\n        """"""\n        super().__init__()\n\n        # The stem takes features from ResNet (or another feature extractor) and projects down to\n        # a lower-dimensional space for sending through the TbD-net\n        self.stem = nn.Sequential(nn.Conv2d(feature_dim[0], module_dim, kernel_size=3, padding=1),\n                                  nn.ReLU(),\n                                  nn.Conv2d(module_dim, module_dim, kernel_size=3, padding=1),\n                                  nn.ReLU()\n                                 )\n\n        module_rows, module_cols = feature_dim[1], feature_dim[2]\n\n        # The classifier takes the output of the last module (which will be a Query or Equal module)\n        # and produces a distribution over answers\n        self.classifier = nn.Sequential(nn.Conv2d(module_dim, cls_proj_dim, kernel_size=1),\n                                        nn.ReLU(inplace=True),\n                                        nn.MaxPool2d(kernel_size=2, stride=2),\n                                        Flatten(),\n                                        nn.Linear(cls_proj_dim * module_rows * module_cols // 4,\n                                                  fc_dim),\n                                        nn.ReLU(inplace=True),\n                                        nn.Linear(fc_dim, 28)  # note no softmax here\n                                       )\n\n        self.function_modules = {}  # holds our modules\n        self.vocab = vocab\n        # go through the vocab and add all the modules to our model\n        for module_name in vocab[\'program_token_to_idx\']:\n            if module_name in [\'<NULL>\', \'<START>\', \'<END>\', \'<UNK>\', \'unique\']:\n                continue  # we don\'t need modules for the placeholders\n            \n            # figure out which module we want we use\n            if module_name == \'scene\':\n                # scene is just a flag that indicates the start of a new line of reasoning\n                # we set `module` to `None` because we still need the flag \'scene\' in forward()\n                module = None\n            elif module_name == \'intersect\':\n                module = modules.AndModule()\n            elif module_name == \'union\':\n                module = modules.OrModule()\n            elif \'equal\' in module_name or module_name in {\'less_than\', \'greater_than\'}:\n                module = modules.ComparisonModule(module_dim)\n            elif \'query\' in module_name or module_name in {\'exist\', \'count\'}:\n                module = modules.QueryModule(module_dim)\n            elif \'relate\' in module_name:\n                module = modules.RelateModule(module_dim)\n            elif \'same\' in module_name:\n                module = modules.SameModule(module_dim)\n            else:\n                module = modules.AttentionModule(module_dim)\n\n            # add the module to our dictionary and register its parameters so it can learn\n            self.function_modules[module_name] = module\n            self.add_module(module_name, module)\n\n        # this is used as input to the first AttentionModule in each program\n        ones = torch.ones(1, 1, module_rows, module_cols)\n        self.ones_var = ones.cuda() if torch.cuda.is_available() else ones\n        \n        self._attention_sum = 0\n\n    @property\n    def attention_sum(self):\n        \'\'\'\n        Returns\n        -------\n        attention_sum : int\n            The sum of attention masks produced during the previous forward pass, or zero if a\n            forward pass has not yet happened.\n\n        Extended Summary\n        ----------------\n        This property holds the sum of attention masks produced during a forward pass of the model.\n        It will hold the sum of all the AttentionModule, RelateModule, and SameModule outputs. This\n        can be used to regularize the output attention masks, hinting to the model that spurious\n        activations that do not correspond to objects of interest (e.g. activations in the \n        background) should be minimized. For example, a small factor multiplied by this could be\n        added to your loss function to add this type of regularization as in:\n\n            loss = xent_loss(outs, answers)\n            loss += executor.attention_sum * 2.5e-07\n            loss.backward()\n\n        where `xent_loss` is our loss function, `outs` is the output of the model, `answers` is the\n        PyTorch `Tensor` containing the answers, and `executor` is this model. The above block\n        will penalize the model\'s attention outputs multiplied by a factor of 2.5e-07 to push the\n        model to produce sensible, minimal activations.\n        \'\'\'\n        return self._attention_sum\n\n    def forward(self, feats, programs):\n        batch_size = feats.size(0)\n        assert batch_size == len(programs)\n\n        feat_input_volume = self.stem(feats)  # forward all the features through the stem at once\n\n        # We compose each module network individually since they are constructed on a per-question\n        # basis. Here we go through each program in the batch, construct a modular network based on\n        # it, and send the image forward through the modular structure. We keep the output of the\n        # last module for each program in final_module_outputs. These are needed to then compute a\n        # distribution over answers for all the questions as a batch.\n        final_module_outputs = []\n        self._attention_sum = 0\n        for n in range(batch_size):\n            feat_input = feat_input_volume[n:n+1] \n            output = feat_input\n            saved_output = None\n            for i in reversed(programs.data[n].cpu().numpy()):\n                module_type = self.vocab[\'program_idx_to_token\'][i]\n                if module_type in {\'<NULL>\', \'<START>\', \'<END>\', \'<UNK>\', \'unique\'}:\n                    continue  # the above are no-ops in our model\n                \n                module = self.function_modules[module_type]\n                if module_type == \'scene\':\n                    # store the previous output; it will be needed later\n                    # scene is just a flag, performing no computation\n                    saved_output = output\n                    output = self.ones_var\n                    continue\n                \n                if \'equal\' in module_type or module_type in {\'intersect\', \'union\', \'less_than\',\n                                                             \'greater_than\'}:\n                    output = module(output, saved_output)  # these modules take two feature maps\n                else:\n                    # these modules take extracted image features and a previous attention\n                    output = module(feat_input, output)\n\n                if any(t in module_type for t in [\'filter\', \'relate\', \'same\']):\n                    self._attention_sum += output.sum()\n                    \n            final_module_outputs.append(output)\n            \n        final_module_outputs = torch.cat(final_module_outputs, 0)\n        return self.classifier(final_module_outputs)\n\n    def forward_and_return_intermediates(self, program_var, feats_var):\n        """""" Forward program `program_var` and image features `feats_var` through the TbD-Net\n        and return an answer and intermediate outputs.\n\n        Parameters\n        ----------\n        program_var : torch.Tensor\n            The program to carry out.\n\n        feats_var : torch.Tensor\n            The image features to operate on.\n        \n        Returns\n        -------\n        Tuple[str, List[Tuple[str, numpy.ndarray]]]\n            A tuple of (answer, [(operation, attention), ...]). Note that some of the\n            intermediates will be `None`, which indicates a break in the logic chain. For\n            example, in the question:\n                ""What color is the cube to the left of the sphere and right of the cylinder?""\n            We have 3 distinct chains of reasoning. We first localize the sphere and look left. We\n            then localize the cylinder and look right. Thirdly, we look at the intersection of these\n            two, and find the cube.\n        """"""\n        intermediaries = []\n        # the logic here is the same as self.forward()\n        scene_input = self.stem(feats_var)\n        output = scene_input\n        saved_output = None\n        for i in reversed(program_var.data.cpu().numpy()[0]):\n            module_type = self.vocab[\'program_idx_to_token\'][i]\n            if module_type in {\'<NULL>\', \'<START>\', \'<END>\', \'<UNK>\', \'unique\'}:\n                continue\n\n            module = self.function_modules[module_type]\n            if module_type == \'scene\':\n                saved_output = output\n                output = self.ones_var\n                intermediaries.append(None) # indicates a break/start of a new logic chain\n                continue\n\n            if \'equal\' in module_type or module_type in {\'intersect\', \'union\', \'less_than\',\n                                                         \'greater_than\'}:\n                output = module(output, saved_output)\n            else:\n                output = module(scene_input, output)\n\n            if module_type in {\'intersect\', \'union\'}:\n                intermediaries.append(None) # this is the start of a new logic chain\n\n            if module_type in {\'intersect\', \'union\'} or any(s in module_type for s in [\'same\',\n                                                                                       \'filter\',\n                                                                                       \'relate\']):\n                intermediaries.append((module_type, output.data.cpu().numpy().squeeze()))\n\n        _, pred = self.classifier(output).max(1)\n        return (self.vocab[\'answer_idx_to_token\'][pred.item()], intermediaries)\n\n\ndef load_tbd_net(checkpoint, vocab):\n    """""" Convenience function to load a TbD-Net model from a checkpoint file.\n\n    Parameters\n    ----------\n    checkpoint : Union[pathlib.Path, str]\n        The path to the checkpoint.\n\n    vocab : Dict[str, Dict[any, any]]\n        The vocabulary file associated with the TbD-Net. For an extended description, see above.\n\n    Returns\n    -------\n    torch.nn.Module\n        The TbD-Net model.\n\n    Notes\n    -----\n    This pushes the TbD-Net model to the GPU if a GPU is available.\n    """"""\n    tbd_net = TbDNet(vocab)\n    tbd_net.load_state_dict(torch.load(str(checkpoint), map_location={\'cuda:0\': \'cpu\'}))\n    if torch.cuda.is_available():\n        tbd_net.cuda()\n    return tbd_net\n'"
tbd/modules.py,24,"b'# DISTRIBUTION STATEMENT A. Approved for public release: distribution unlimited.\n#\n# This material is based upon work supported by the Assistant Secretary of Defense for Research and\n# Engineering under Air Force Contract No. FA8721-05-C-0002 and/or FA8702-15-D-0001. Any opinions,\n# findings, conclusions or recommendations expressed in this material are those of the author(s) and\n# do not necessarily reflect the views of the Assistant Secretary of Defense for Research and\n# Engineering.\n#\n# \xc2\xa9 2017 Massachusetts Institute of Technology.\n#\n# MIT Proprietary, Subject to FAR52.227-11 Patent Rights - Ownership by the contractor (May 2014)\n#\n# The software/firmware is provided to you on an As-Is basis\n#\n# Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or\n# 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are\n# defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than\n# as specifically authorized by the U.S. Government may violate any copyrights that exist in this\n# work.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass AndModule(nn.Module):\n    """""" A neural module that (basically) performs a logical and.\n\n    Extended Summary\n    ---------------- \n    An :class:`AndModule` is a neural module that takes two input attention masks and (basically)\n    performs a set intersection. This would be used in a question like ""What color is the cube to\n    the left of the sphere and right of the yellow cylinder?"" After localizing the regions left of\n    the sphere and right of the yellow cylinder, an :class:`AndModule` would be used to find the\n    intersection of the two. Its output would then go into an :class:`AttentionModule` that finds\n    cubes.\n    """"""\n    def forward(self, attn1, attn2):\n        out = torch.min(attn1, attn2)\n        return out\n\n\nclass OrModule(nn.Module):\n    """""" A neural module that (basically) performs a logical or.\n\n    Extended Summary\n    ----------------\n    An :class:`OrModule` is a neural module that takes two input attention masks and (basically)\n    performs a set union. This would be used in a question like ""How many cubes are left of the\n    brown sphere or right of the cylinder?"" After localizing the regions left of the brown sphere\n    and right of the cylinder, an :class:`OrModule` would be used to find the union of the two. Its\n    output would then go into an :class:`AttentionModule` that finds cubes.\n    """"""\n    def forward(self, attn1, attn2):\n        out = torch.max(attn1, attn2)\n        return out\n\n\nclass AttentionModule(nn.Module):\n    """""" A neural module that takes a feature map and attention, attends to the features, and \n    produces an attention.\n\n    Extended Summary\n    ----------------\n    An :class:`AttentionModule` takes input features and an attention and produces an attention. It\n    multiplicatively combines its input feature map and attention to attend to the relevant region\n    of the feature map. It then processes the attended features via a series of convolutions and\n    produces an attention mask highlighting the objects that possess the attribute the module is\n    looking for.\n\n    For example, an :class:`AttentionModule` may be tasked with finding cubes. Given an input\n    attention of all ones, it will highlight all the cubes in the provided input features. Given an\n    attention mask highlighting all the red objects, it will produce an attention mask highlighting\n    all the red cubes.\n\n    Attributes\n    ----------\n    dim : int\n        The number of channels of each convolutional filter.\n    """"""\n    def __init__(self, dim):\n        super().__init__()\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1) \n        self.conv3 = nn.Conv2d(dim, 1, kernel_size=1, padding=0)\n        torch.nn.init.kaiming_normal_(self.conv1.weight)\n        torch.nn.init.kaiming_normal_(self.conv2.weight)\n        torch.nn.init.kaiming_normal_(self.conv3.weight)\n        self.dim = dim\n\n    def forward(self, feats, attn):\n        attended_feats = torch.mul(feats, attn.repeat(1, self.dim, 1, 1))\n        out = F.relu(self.conv1(attended_feats))\n        out = F.relu(self.conv2(out))\n        out = F.sigmoid(self.conv3(out))\n        return out\n\n\nclass QueryModule(nn.Module):\n    """""" A neural module that takes as input a feature map and an attention and produces a feature\n    map as output.\n\n    Extended Summary\n    ----------------\n    A :class:`QueryModule` takes a feature map and an attention mask as input. It attends to the\n    feature map via an elementwise multiplication with the attention mask, then processes this\n    attended feature map via a series of convolutions to extract relevant information.\n\n    For example, a :class:`QueryModule` tasked with determining the color of objects would output a\n    feature map encoding what color the attended object is. A module intended to count would output\n    a feature map encoding the number of attended objects in the scene.\n\n    Attributes\n    ----------\n    dim : int\n        The number of channels of each convolutional filter.\n    """"""\n    def __init__(self, dim):\n        super().__init__()\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        torch.nn.init.kaiming_normal_(self.conv1.weight)\n        torch.nn.init.kaiming_normal_(self.conv2.weight)\n        self.dim = dim\n\n    def forward(self, feats, attn):\n        attended_feats = torch.mul(feats, attn.repeat(1, self.dim, 1, 1))\n        out = F.relu(self.conv1(attended_feats))\n        out = F.relu(self.conv2(out))\n        return out\n\n\nclass RelateModule(nn.Module):\n    """""" A neural module that takes as input a feature map and an attention and produces an attention\n    as output.\n\n    Extended Summary\n    ----------------\n    A :class:`RelateModule` takes input features and an attention and produces an attention. It\n    multiplicatively combines the attention and the features to attend to a relevant region, then\n    uses a series of dilated convolutional filters to indicate a spatial relationship to the input\n    attended region.\n\n    Attributes\n    ----------\n    dim : int\n        The number of channels of each convolutional filter.\n    """"""\n    def __init__(self, dim):\n        super().__init__()\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1, dilation=1)  # receptive field 3\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=2, dilation=2)  # 7\n        self.conv3 = nn.Conv2d(dim, dim, kernel_size=3, padding=4, dilation=4)  # 15\n        self.conv4 = nn.Conv2d(dim, dim, kernel_size=3, padding=8, dilation=8)  # 31 -- full image\n        self.conv5 = nn.Conv2d(dim, dim, kernel_size=3, padding=1, dilation=1)\n        self.conv6 = nn.Conv2d(dim, 1, kernel_size=1, padding=0)\n        torch.nn.init.kaiming_normal_(self.conv1.weight)\n        torch.nn.init.kaiming_normal_(self.conv2.weight)\n        torch.nn.init.kaiming_normal_(self.conv3.weight)\n        torch.nn.init.kaiming_normal_(self.conv4.weight)\n        torch.nn.init.kaiming_normal_(self.conv5.weight)\n        torch.nn.init.kaiming_normal_(self.conv6.weight)\n        self.dim = dim\n\n    def forward(self, feats, attn):\n        feats = torch.mul(feats, attn.repeat(1, self.dim, 1, 1))\n        out = F.relu(self.conv1(feats))\n        out = F.relu(self.conv2(out))\n        out = F.relu(self.conv3(out))\n        out = F.relu(self.conv4(out))\n        out = F.relu(self.conv5(out))\n        out = F.sigmoid(self.conv6(out))\n        return out\n\n\nclass SameModule(nn.Module):\n    """""" A neural module that takes as input a feature map and an attention and produces an attention\n    as output.\n\n    Extended Summary\n    ----------------\n    A :class:`SameModule` takes input features and an attention and produces an attention. It\n    determines the index of the maximally-attended object, extracts the feature vector at that\n    spatial location, then performs a cross-correlation at each spatial location to determine which\n    other regions have this same property. This correlated feature map then goes through a\n    convolutional block whose output is an attention mask.\n\n    As an example, this module can be used with the CLEVR dataset to perform the `same_shape`\n    operation, which will highlight every region of an image that shares the same shape as an object\n    of interest (excluding the original object).\n\n    Attributes\n    ----------\n    dim : int\n        The number of channels in the input feature map.\n    """"""\n    def __init__(self, dim):\n        super().__init__()\n        self.conv = nn.Conv2d(dim+1, 1, kernel_size=1)\n        torch.nn.init.kaiming_normal_(self.conv.weight)\n        self.dim = dim\n\n    def forward(self, feats, attn):\n        size = attn.size()[2]\n        the_max, the_idx = F.max_pool2d(attn, size, return_indices=True)\n        attended_feats = feats.index_select(2, the_idx[0, 0, 0, 0] / size)\n        attended_feats = attended_feats.index_select(3, the_idx[0, 0, 0, 0] % size)\n        x = torch.mul(feats, attended_feats.repeat(1, 1, size, size))\n        x = torch.cat([x, attn], dim=1)\n        out = F.sigmoid(self.conv(x))\n        return out\n\n\nclass ComparisonModule(nn.Module):\n    """""" A neural module that takes as input two feature maps and produces a feature map as output.\n\n    Extended Summary\n    ----------------\n    A :class:`ComparisonModule` takes two feature maps as input and concatenates these. It then\n    processes the concatenated features and produces a feature map encoding whether the two input\n    feature maps encode the same property.\n\n    This block is useful in making integer comparisons, for example to answer the question, ``Are\n    there more red things than small spheres?\'\' It can also be used to determine whether some\n    relationship holds of two objects (e.g. they are the same shape, size, color, or material).\n\n    Attributes\n    ----------\n    dim : int\n        The number of channels of each convolutional filter.\n    """"""\n    def __init__(self, dim):\n        super().__init__()\n        self.projection = nn.Conv2d(2*dim, dim, kernel_size=1, padding=0)\n        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n        torch.nn.init.kaiming_normal_(self.conv1.weight)\n        torch.nn.init.kaiming_normal_(self.conv2.weight)\n\n    def forward(self, in1, in2):\n        out = torch.cat([in1, in2], 1)\n        out = F.relu(self.projection(out))\n        out = F.relu(self.conv1(out))\n        out = F.relu(self.conv2(out))\n        return out\n'"
utils/__init__.py,0,b''
utils/clevr.py,9,"b'# DISTRIBUTION STATEMENT A. Approved for public release: distribution unlimited.\n#\n# This material is based upon work supported by the Assistant Secretary of Defense for Research and\n# Engineering under Air Force Contract No. FA8721-05-C-0002 and/or FA8702-15-D-0001. Any opinions,\n# findings, conclusions or recommendations expressed in this material are those of the author(s) and\n# do not necessarily reflect the views of the Assistant Secretary of Defense for Research and\n# Engineering.\n#\n# \xc2\xa9 2017 Massachusetts Institute of Technology.\n#\n# MIT Proprietary, Subject to FAR52.227-11 Patent Rights - Ownership by the contractor (May 2014)\n#\n# The software/firmware is provided to you on an As-Is basis\n#\n# Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or\n# 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are\n# defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than\n# as specifically authorized by the U.S. Government may violate any copyrights that exist in this\n# work.\n\nimport numpy as np\nimport h5py\nimport warnings\nimport json\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataloader import default_collate\n\n__all__ = [\'invert_dict\', \'load_vocab\', \'clevr_collate\', \'ClevrDataLoaderH5\',\n           \'ClevrDataLoaderNumpy\', \'ClevrDataset\']\n\ndef invert_dict(d):\n    """""" Utility for swapping keys and values in a dictionary.\n    \n    Parameters\n    ----------\n    d : Dict[Any, Any]\n    \n    Returns\n    -------\n    Dict[Any, Any]\n    """"""\n    return {v: k for k, v in d.items()}\n\n\ndef load_vocab(path):\n    """""" Load the vocabulary file.\n\n    Parameters\n    ----------\n    path : Union[str, pathlib.Path]\n        Path to the vocabulary json file.\n\n    Returns\n    -------\n    vocab : Dict[str, Dict[Any, Any]]\n        The vocabulary. See the extended summary for details on the values.\n\n    Extended Summary\n    ----------------\n    The vocabulary object contains the question, program, and answer tokens and their respective\n    image indices. It is a dictionary of dictionaries. Its contents are:\n\n    - question_idx_to_token : Dict[int, str]\n        A mapping from question index to question word.\n\n    - program_idx_to_token : Dict[int, str]\n        A mapping from program index to module name/logical operation (e.g. filter_color[red]).\n\n    - answer_idx_to_token : Dict[int, str]\n        A mapping from answer index to answer word.\n\n    - question_token_to_idx : Dict[str, int]\n        A mapping from question word to index.\n\n    - program_token_to_idx : Dict[str, int]\n        A mapping from program index to module description.\n\n    - answer_token_to_idx : Dict[str, int]\n        A mapping from answer word to index.\n    """"""\n    path = str(path)  # in case we get a pathlib.Path\n        \n    with open(path, \'r\') as f:\n        vocab = json.load(f)\n        vocab[\'question_idx_to_token\'] = invert_dict(vocab[\'question_token_to_idx\'])\n        vocab[\'program_idx_to_token\'] = invert_dict(vocab[\'program_token_to_idx\'])\n        vocab[\'answer_idx_to_token\'] = invert_dict(vocab[\'answer_token_to_idx\'])\n\n    # our answers format differs from that of Johnson et al.\n    answers = [\'blue\', \'brown\', \'cyan\', \'gray\', \'green\', \'purple\', \'red\', \'yellow\',\n               \'cube\', \'cylinder\', \'sphere\',\n               \'large\', \'small\',\n               \'metal\', \'rubber\',\n               \'no\', \'yes\',\n               \'0\', \'1\', \'10\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\']\n    vocab[\'answer_idx_to_token\'] = dict(zip(range(len(answers)), answers))\n    vocab[\'answer_token_to_idx\'] = dict(zip(answers, range(len(answers))))\n    return vocab\n\n\ndef clevr_collate(batch):\n    """""" Collate a batch of data.""""""\n    transposed = list(zip(*batch))\n    question_batch = default_collate(transposed[0])\n    image_batch = transposed[1]\n    if any(img is not None for img in image_batch):\n        image_batch = default_collate(image_batch)\n    feat_batch = transposed[2]\n    if any(f is not None for f in feat_batch):\n        feat_batch = default_collate(feat_batch)\n    answer_batch = default_collate(transposed[3]) if transposed[3][0] is not None else None\n    program_seq_batch = transposed[4]\n    if transposed[4][0] is not None:\n        program_seq_batch = default_collate(transposed[4])\n    return [question_batch, image_batch, feat_batch, answer_batch, program_seq_batch]\n\n\nclass ClevrDataset(Dataset):\n    """""" Holds a handle to the CLEVR dataset.\n\n    Extended Summary\n    ----------------\n    A :class:`ClevrDataset` holds a handle to the CLEVR dataset. It loads a specified subset of the\n    questions, their image indices and extracted image features, the answer (if available), and\n    optionally the images themselves. This is best used in conjunction with a\n    :class:`ClevrDataLoaderNumpy` of a :class:`ClevrDataLoaderH5`, which handle loading the data.\n    """"""\n    def __init__(self, questions, image_indices, programs, features, answers, images=None,\n                 indices=None):\n        """""" Initialize a ClevrDataset object.\n        \n        Parameters\n        ----------\n        questions : Union[numpy.ndarray, h5py.File]\n            Object holding the questions.\n\n        image_indices : Union[numpy.ndarray, h5py.File]\n            Object holding the image indices.\n\n        programs : Union[numpy.ndarray, h5py.File]\n            Object holding the programs, or None.\n\n        features : Union[numpy.ndarray, h5py.File]\n            Object holding the extracted image features.\n\n        answers : Union[numpy.ndarray, h5py.File]\n            Object holding the answers, or None.\n\n        images : Union[numpy.ndarray, h5py.File], optional\n            Object holding the images, or None.\n\n        indices : Sequence[int], optional\n            The indices of the questions to load, or None.\n        """"""\n        assert len(questions) == len(image_indices) == len(programs) == len(answers), \\\n            \'The questions, image indices, programs, and answers are not all the same size!\'\n\n        # questions, image indices, programs, and answers are small enough to load into memory\n        self.all_questions = torch.LongTensor(np.asarray(questions))\n        self.all_image_idxs = torch.LongTensor(np.asarray(image_indices))\n        self.all_programs = torch.LongTensor(np.asarray(programs))\n        self.all_answers = torch.LongTensor(np.asarray(answers)) if answers is not None else None\n\n        # features and images are not small enough to always load\n        self.features = features\n        self.images = images\n\n        if indices is not None:\n            indices = torch.LongTensor(np.asarray(indices))\n            self.all_questions = self.all_questions[indices]\n            self.all_image_idxs = self.all_image_idxs[indices]\n            self.all_programs = self.all_programs[indices]\n            self.all_answers = self.all_answers[indices]\n\n    def __getitem__(self, index):\n        question = self.all_questions[index]\n        image_idx = self.all_image_idxs[index]\n        answer = self.all_answers[index] if self.all_answers is not None else None\n        program_seq = self.all_programs[index]\n        image = None\n        if self.images is not None:\n            image = torch.FloatTensor(np.asarray(self.image_np[image_idx]))\n        feats = torch.FloatTensor(np.asarray(self.features[image_idx]))\n\n        return (question, image, feats, answer, program_seq)\n\n    def __len__(self):\n        return len(self.all_questions)\n\n\nclass ClevrDataLoaderNumpy(DataLoader):\n    """""" Loads the CLEVR dataset from npy files.\n\n    Extended Summary\n    ----------------\n    Loads the data for, and handles construction of, a :class:`ClevrDataset`. This object can then\n    be used to iterate through batches of data for training, validation, or testing.\n    """"""\n    def __init__(self, **kwargs):\n        """""" Initialize a ClevrDataLoaderNumpy object.\n\n        Parameters\n        ----------\n        question_np : Union[pathlib.Path, str]\n            Path to the numpy file holding the questions.\n\n        feature_np : Union[pathlib.Path, str]\n            Path to the numpy file holding the extracted image features.\n\n        image_idx_np : Union[pathlib.Path, str]\n            Path to the numpy file holding the indices of each question\'s corresponding image.\n\n        program_np : Union[pathlib.Path, str]\n            Path to the numpy file holding the programs the module network should compose, or None.\n\n        answer_np : Union[pathlib.Path, str]\n            Path to the numpy file holding the answers to each question, or None.\n\n        image_np : Union[pathlib.Path, str], optional\n            Path to the numpy file holding the raw images.\n\n        shuffle : bool, optional\n            Whether to shuffle the data.\n\n        indices : Sequence[int], optional\n            The question indices to load, or None.\n        """"""\n        # The questions, image features, image indices, programs, and answers are required.\n        if \'question_np\' not in kwargs:\n            raise ValueError(\'Must give question_np\')\n        if \'feature_np\' not in kwargs:\n            raise ValueError(\'Must give feature_np\')\n        if \'image_idx_np\' not in kwargs:\n            raise ValueError(\'Must give image_idx_np\')\n        if \'program_np\' not in kwargs:\n            raise ValueError(\'Must give program_np\')\n        if \'answer_np\' not in kwargs:\n            raise ValueError(\'Must give answer_np\')\n\n        # We\'re mmapping the image features because they aren\'t small enough for everybody to load.\n        # If you have about 65 GB of memory available, feel free to remove the mmap_mode argument to\n        # load the entire dataset into memory. This will eliminate some overhead.\n        feature_np_path = str(kwargs.pop(\'feature_np\'))\n        print(\'Reading features from \', feature_np_path)\n        feature_np = np.load(feature_np_path, mmap_mode=\'r\')\n\n        # The same goes for the images. If you want to load all the images for some reason, that can\n        # be done by removing the mmap_mode argument below.\n        image_np = None\n        if \'image_np\' in kwargs:\n            image_np_path = str(kwargs.pop(\'image_np\'))\n            print(\'Reading images from \', image_np_path)\n            image_np = np.load(image_np_path, mmap_mode=\'r\')\n\n        # The question, image, program, and answer arrays are small enough to load into memory, so\n        # we directly load them here.\n        question_np_path = str(kwargs.pop(\'question_np\'))\n        print(\'Reading questions from \', question_np_path)\n        question_np = np.load(question_np_path)\n\n        image_idx_np_path = str(kwargs.pop(\'image_idx_np\'))\n        print(\'Reading image indices from\', image_idx_np_path)\n        image_idx_np = np.load(image_idx_np_path)\n\n        program_np_path = str(kwargs.pop(\'program_np\'))\n        print(\'Reading programs from\', program_np_path)\n        program_np = np.load(program_np_path)\n\n        answer_np_path = str(kwargs.pop(\'answer_np\'))\n        print(\'Reading answers from\', answer_np_path)\n        answer_np = np.load(answer_np_path) if answer_np_path is not None else None\n\n        indices = None\n        if \'indices\' in kwargs:\n            indices = kwargs.pop(\'indices\')\n\n        if \'shuffle\' not in kwargs:\n            # Be nice, and make sure the user knows they aren\'t shuffling the data\n            warnings.warn(\'\\n\\n\\tYou have not provided a \\\'shuffle\\\' argument to the data \'\n                      \'loader.\\n\\tBe aware that the default behavior is to NOT shuffle the data.\\n\')\n\n        self.dataset = ClevrDataset(question_np, image_idx_np, program_np, feature_np, answer_np,\n                                    image_np, indices)\n        kwargs[\'collate_fn\'] = clevr_collate\n        super().__init__(self.dataset, **kwargs)\n\n    def close(self):\n        # numpy handles everything here\n        return\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n\nclass ClevrDataLoaderH5(DataLoader):\n    """""" Loads the CLEVR dataset from HDF5 files.\n\n    Extended Summary\n    ----------------\n    Loads the data for, and handles construction of, a :class:`ClevrDataset`. This object can then\n    be used to iterate through batches of data for training, validation, or testing.\n    """"""\n    def __init__(self, **kwargs):\n        """""" Initialize a ClevrDataLoaderH5 object.\n\n        Parameters\n        ----------\n        question_h5 : Union[pathlib.Path, str]\n            Path to the HDF5 file holding the questions, image indices, programs, and answers.\n\n        feature_h5 : Union[pathlib.Path, str]\n            Path to the HDF5 file holding the extracted image features.\n\n        image_h5 : Union[pathlib.Path, str], optional\n            Path to the HDF5 file holding the raw images.\n\n        shuffle : bool, optional\n            Whether to shuffle the data.\n\n        indices : Sequence[int], optional\n            The question indices to load, or None.\n        """"""\n        if \'question_h5\' not in kwargs:\n            raise ValueError(\'Must give question_h5\')\n        if \'feature_h5\' not in kwargs:\n            raise ValueError(\'Must give feature_h5\')\n\n        feature_h5_path = str(kwargs.pop(\'feature_h5\'))\n        print(\'Reading features from \', feature_h5_path)\n        self.feature_h5 = h5py.File(feature_h5_path, \'r\')[\'features\']\n\n        self.image_h5 = None\n        if \'image_h5\' in kwargs:\n            image_h5_path = str(kwargs.pop(\'image_h5\'))\n            print(\'Reading images from \', image_h5_path)\n            self.image_h5 = h5py.File(image_h5_path, \'r\')[\'images\']\n\n        indices = None\n        if \'indices\' in kwargs:\n            indices = kwargs.pop(\'indices\')\n\n        if \'shuffle\' not in kwargs:\n            # be nice, and make sure the user knows they aren\'t shuffling\n            warnings.warn(\'\\n\\n\\tYou have not provided a \\\'shuffle\\\' argument to the data loader.\\n\'\n                      \'\\tBe aware that the default behavior is to NOT shuffle the data.\\n\')\n\n        question_h5_path = str(kwargs.pop(\'question_h5\'))\n        with h5py.File(question_h5_path) as question_h5:\n            questions = question_h5[\'questions\']\n            image_indices = question_h5[\'image_idxs\']\n            programs = question_h5[\'programs\']\n            answers = question_h5[\'answers\']\n            self.dataset = ClevrDataset(questions, image_indices, programs, self.feature_h5,\n                                        answers, self.image_h5, indices)\n        kwargs[\'collate_fn\'] = clevr_collate\n        super().__init__(self.dataset, **kwargs)\n\n    def close(self):\n        # Close our files to prevent leaks\n        if self.image_h5 is not None:\n            self.image_h5.close()\n        if self.feature_h5 is not None:\n            self.feature_h5.close()\n        return\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n'"
utils/download_pretrained_models.py,0,"b'# DISTRIBUTION STATEMENT A. Approved for public release: distribution unlimited.\n#\n# This material is based upon work supported by the Assistant Secretary of Defense for Research and\n# Engineering under Air Force Contract No. FA8721-05-C-0002 and/or FA8702-15-D-0001. Any opinions,\n# findings, conclusions or recommendations expressed in this material are those of the author(s) and\n# do not necessarily reflect the views of the Assistant Secretary of Defense for Research and\n# Engineering.\n#\n# \xc2\xa9 2017 Massachusetts Institute of Technology.\n#\n# MIT Proprietary, Subject to FAR52.227-11 Patent Rights - Ownership by the contractor (May 2014)\n#\n# The software/firmware is provided to you on an As-Is basis\n#\n# Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or\n# 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are\n# defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than\n# as specifically authorized by the U.S. Government may violate any copyrights that exist in this\n# work.\n\nfrom pathlib import Path\nfrom urllib.request import urlretrieve\nimport sys\nimport argparse\n\n\ndef _download_info(blocks_so_far, block_size, total_size):\n    percent = int(100*blocks_so_far*block_size / total_size)\n    print(\'Downloaded {}%\'.format(percent), end=\'\\r\')\n    if percent == 100:\n        print(\'Finished downloading\')\n\ndef download(fnames):\n    """""" Utility to download TbD-Net models.\n\n    Parameters\n    ----------\n    fnames : Union[str, Sequence]\n        The file name(s) of the models to download.\n\n    Notes\n    -----\n    Available model options (fnames) are as follows:\n      - \'clevr.pt\' : trained on CLEVR with 14x14 feature maps\n      - \'cogent-no-finetune.pt\' : the same as \'clevr.pt\' trained on CoGenT A without fine-tuning\n      - \'cogent-finetuned.pt\' : the same as above trained on CoGenT A and fine-tuned on CoGenT B\n      - \'clevr-reg.pt\' : trained on CLEVR with 14x14 feature maps using regularization\n      - \'cogent-no-finetune-reg.pt\' : the same as above trained on CoGenT A without fine-tuning\n      - \'cogent-finetuned-reg.pt\' : the same as above trained on CoGenT A and fine-tuned on B\n      - \'clevr-reg-hres.pt\' : trained on CLEVR with 28x28 feature maps and regularization\n      - \'program_generator.pt\' : the program generator; see \'generate_programs.py\'\n    """"""\n    download_path = Path(\'./models\')\n    if not download_path.exists() or not download_path.is_dir():\n        print(\'The directory \\\'models\\\' does not exist!\')\n        print(\'Please ensure you are in the top level of the visual-attention-networks repository\')\n        print(\'  and that the \\\'models\\\' directory exists\')\n        sys.exit()\n\n    server_url = \'https://github.com/davidmascharka/tbd-nets/releases/download/v1.0/\'\n    if isinstance(fnames, str): # a single file\n        fnames = [fnames]\n    for fname in fnames:\n        if (download_path / fname).exists():\n            print(\'Skipping {}: the file already exists\'.format(fname))\n            continue\n\n        print(\'Downloading {}\'.format(fname))\n        urlretrieve(server_url + fname, str((download_path/fname).absolute()), _download_info)\n    print(\'Finished\')\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    help_str = \'Which models to download. Options: original, reg, hres, all (default: hres)\' + \\\n               \' Original: no regularization. Reg: regularized with lambda = 2.5e-07.\' + \\\n               \' Hres: high-resolution feature maps with regularization. All: all\'\n    parser.add_argument(\'--models\', \'-m\', required=False, default=\'hres\',\n                        choices=[\'original\', \'reg\', \'hres\', \'all\'], help=help_str)\n    \n    args = parser.parse_args()\n\n    original_fnames = [\'clevr.pt\', \'cogent-no-finetune.pt\', \'cogent-finetuned.pt\']\n    reg_fnames = [\'clevr-reg.pt\', \'cogent-no-finetune-reg.pt\', \'cogent-finetuned-reg.pt\']\n    hres_fnames = [\'clevr-reg-hres.pt\']\n    \n    if args.models in {\'hres\', \'all\'}:\n        download(hres_fnames)\n    if args.models in {\'reg\', \'all\'}:\n        download(reg_fnames)\n    if args.models in {\'original\', \'all\'}:\n        download(original_fnames)\n'"
utils/extract_features.py,7,"b'# DISTRIBUTION STATEMENT A. Approved for public release: distribution unlimited.\n#\n# This material is based upon work supported by the Assistant Secretary of Defense for Research and\n# Engineering under Air Force Contract No. FA8721-05-C-0002 and/or FA8702-15-D-0001. Any opinions,\n# findings, conclusions or recommendations expressed in this material are those of the author(s) and\n# do not necessarily reflect the views of the Assistant Secretary of Defense for Research and\n# Engineering.\n#\n# \xc2\xa9 2017 Massachusetts Institute of Technology.\n#\n# MIT Proprietary, Subject to FAR52.227-11 Patent Rights - Ownership by the contractor (May 2014)\n#\n# The software/firmware is provided to you on an As-Is basis\n#\n# Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or\n# 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are\n# defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than\n# as specifically authorized by the U.S. Government may violate any copyrights that exist in this\n# work.\n\nimport torch\nimport numpy as np\nfrom scipy.misc import imread, imresize\nfrom torchvision.models import resnet101\n\ndef load_feature_extractor(model_stage=2):\n    """""" Load the appropriate parts of ResNet-101 for feature extraction.\n\n    Parameters\n    ----------\n    model_stage : Integral\n        The stage of ResNet-101 from which to extract features.\n        For 28x28 feature maps, this should be 2. For 14x14 feature maps, 3.\n\n    Returns\n    -------\n    torch.nn.Sequential\n        The feature extractor (ResNet-101 at `model_stage`)\n\n    Notes\n    -----\n    This function will download ResNet-101 if it is not already present through torchvision.\n    """"""\n    model = resnet101(pretrained=True)\n    layers = [model.conv1, model.bn1, model.relu, model.maxpool]\n    layers += [getattr(model, \'layer{}\'.format(i+1)) for i in range(model_stage)]\n    model = torch.nn.Sequential(*layers)\n    if torch.cuda.is_available():\n        model.cuda()\n\n    return model.eval()\n\n\ndef extract_image_feats(img_path, model):\n    """""" Extract image features from the image at `img_path` using `model`.\n\n    Parameters\n    ----------\n    img_path : Union[pathlib.Path, str]\n        The path to the image file.\n\n    model : torch.nn.Module\n        The feature extractor to use.\n\n    Returns\n    -------\n    Tuple[numpy.ndarray, torch.Tensor]\n        The image and image features extracted from `model`\n    """"""\n    # read in the image and transform it to shape (1, 3, 224, 224)\n    path = str(img_path) # to handle pathlib\n    img = imread(path, mode=\'RGB\')\n    img = imresize(img, (224, 224), interp=\'bicubic\')\n    img = img.transpose(2, 0, 1)[None]\n\n    # use ImageNet statistics to transform the data\n    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n    std = np.array([0.229, 0.224, 0.224]).reshape(1, 3, 1, 1)\n    img_tensor = torch.FloatTensor((img / 255 - mean) / std)\n\n    # push to the GPU if possible\n    if torch.cuda.is_available():\n        img_tensor = img_tensor.cuda()\n\n    return (img.squeeze().transpose(1, 2, 0), model(img_tensor))\n'"
utils/generate_programs.py,21,"b'# Copyright 2017-present, Facebook, Inc.\n# All rights reserved.\n#\n# This source code is licensed under the license available at\n# https://github.com/facebookresearch/clevr-iep/blob/master/LICENSE\n#\n# Modifications by David Mascharka to update the code for compatibility with PyTorch >0.1 lead to:\n# DISTRIBUTION STATEMENT A. Approved for public release: distribution unlimited.\n#\n# This material is based upon work supported by the Assistant Secretary of Defense for Research and\n# Engineering under Air Force Contract No. FA8721-05-C-0002 and/or FA8702-15-D-0001. Any opinions,\n# findings, conclusions or recommendations expressed in this material are those of the author(s) and\n# do not necessarily reflect the views of the Assistant Secretary of Defense for Research and\n# Engineering.\n#\n# \xc2\xa9 2017 Massachusetts Institute of Technology.\n#\n# MIT Proprietary, Subject to FAR52.227-11 Patent Rights - Ownership by the contractor (May 2014)\n#\n# The software/firmware is provided to you on an As-Is basis\n#\n# Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or\n# 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are\n# defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than\n# as specifically authorized by the U.S. Government may violate any copyrights that exist in this\n# work.\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\nimport h5py\nfrom pathlib import Path\n\n__all__ = [\'load_model\', \'generate_programs\']\n\nclass _Seq2Seq(nn.Module):\n    def __init__(self, \n        encoder_vocab_size=100,\n        decoder_vocab_size=100,\n        wordvec_dim=300,\n        hidden_dim=256,\n        rnn_num_layers=2,\n        rnn_dropout=0,\n        null_token=0,\n        start_token=1,\n        end_token=2,\n        encoder_embed=None):\n        super().__init__()\n        self.encoder_embed = nn.Embedding(encoder_vocab_size, wordvec_dim)\n        self.encoder_rnn = nn.LSTM(wordvec_dim, hidden_dim, rnn_num_layers,\n                                   dropout=rnn_dropout, batch_first=True)\n        self.decoder_embed = nn.Embedding(decoder_vocab_size, wordvec_dim)\n        self.decoder_rnn = nn.LSTM(wordvec_dim + hidden_dim, hidden_dim, rnn_num_layers,\n                                   dropout=rnn_dropout, batch_first=True)\n        self.decoder_linear = nn.Linear(hidden_dim, decoder_vocab_size)\n        self.NULL = null_token\n        self.START = start_token\n        self.END = end_token\n\n    def get_dims(self, x=None, y=None):\n        V_in = self.encoder_embed.num_embeddings\n        V_out = self.decoder_embed.num_embeddings\n        D = self.encoder_embed.embedding_dim\n        H = self.encoder_rnn.hidden_size\n        L = self.encoder_rnn.num_layers\n\n        N = x.size(0) if x is not None else None\n        N = y.size(0) if N is None and y is not None else N\n        T_in = x.size(1) if x is not None else None\n        T_out = y.size(1) if y is not None else None\n        return V_in, V_out, D, H, L, N, T_in, T_out\n\n    def before_rnn(self, x, replace=0):\n        N, T = x.size()\n        idx = torch.LongTensor(N).fill_(T - 1)\n        x_cpu = x.cpu()\n        for i in range(N):\n            for t in range(T - 1):\n                if x_cpu[i, t] != self.NULL and x_cpu[i, t + 1] == self.NULL:\n                    idx[i] = t\n                    break\n        idx = idx.type_as(x)\n        x[x == self.NULL] = replace\n        return x, idx\n\n    def encoder(self, x):\n        V_in, V_out, D, H, L, N, T_in, T_out = self.get_dims(x=x)\n        x, idx = self.before_rnn(x)\n        embed = self.encoder_embed(x)\n        h0 = torch.zeros(L, N, H).type_as(embed)\n        c0 = torch.zeros(L, N, H).type_as(embed)\n        out, _ = self.encoder_rnn(embed, (h0, c0))\n        idx = idx.view(N, 1, 1).expand(N, 1, H)\n        return out.gather(1, idx).view(N, H)\n\n    def decoder(self, encoded, y, h0=None, c0=None):\n        V_in, V_out, D, H, L, N, T_in, T_out = self.get_dims(y=y)\n\n        if T_out > 1:\n            y, _ = self.before_rnn(y)\n        y_embed = self.decoder_embed(y)\n        encoded_repeat = encoded.view(N, 1, H)\n        encoded_repeat = encoded_repeat.expand(N, T_out, H)\n        rnn_input = torch.cat([encoded_repeat, y_embed], 2)\n        if h0 is None:\n            h0 = torch.zeros(L, N, H).type_as(encoded)\n        if c0 is None:\n            c0 = torch.zeros(L, N, H).type_as(encoded)\n        rnn_output, (ht, ct) = self.decoder_rnn(rnn_input, (h0, c0))\n\n        rnn_output_2d = rnn_output.contiguous().view(N * T_out, H)\n        output_logprobs = self.decoder_linear(rnn_output_2d).view(N, T_out, V_out)\n\n        return output_logprobs, ht, ct\n\n    def reinforce_sample(self, x, max_length=30):\n        N, T = x.size(0), max_length\n        encoded = self.encoder(x)\n        y = torch.LongTensor(N, T).fill_(self.NULL)\n        done = torch.ByteTensor(N).fill_(0)\n        cur_input = x.new(N, 1).fill_(self.START)\n        h, c = None, None\n        for t in range(T):\n            # logprobs is N x 1 x V\n            logprobs, h, c = self.decoder(encoded, cur_input, h0=h, c0=c)\n            probs = F.softmax(logprobs.view(N, -1), dim=1) # Now N x V\n            _, cur_output = probs.max(1)\n            cur_output = cur_output.unsqueeze(1)\n            cur_output_data = cur_output.cpu()\n            not_done = logical_not(done)\n            y[:, t][not_done] = cur_output_data[not_done][0]\n            done = logical_or(done, cur_output_data.cpu().squeeze() == self.END)\n            cur_input = cur_output\n            if done.sum() == N:\n                break\n        return y.type_as(x)\n\ndef logical_or(x, y):\n    return (x + y).clamp_(0, 1)\n\ndef logical_not(x):\n    return x == 0\n\ndef load_program_generator(checkpoint):\n    """""" Loads the program generator model from `checkpoint`.\n\n    Parameters\n    ----------\n    checkpoint : Union[pathlib.Path, str]\n        The path to a checkpoint file.\n\n    Returns\n    -------\n    torch.nn.Module\n        The program generator model, which takes as input a question and produces a logical series\n        of operations that can be used to answer that question.\n    """"""\n    checkpoint = torch.load(str(checkpoint), map_location={\'cuda:0\': \'cpu\'})\n    kwargs = checkpoint[\'program_generator_kwargs\']\n    state = checkpoint[\'program_generator_state\']\n    program_generator = _Seq2Seq(**kwargs)\n    program_generator.load_state_dict(state)\n    return program_generator\n\ndef generate_single_program(question, program_generator, vocab, question_len=46):\n    """""" Generate a single program from a given natural-language question using the provided model.\n    \n    Parameters\n    ----------\n    question : str\n        The question to produce a program from.\n\n    program_generator : torch.nn.Module\n        The program generation model to use to produce a program.\n\n    vocab : Dict[str, Dict[any, any]]\n        The dictionary to use to convert words to indices.\n\n    Returns\n    -------\n    torch.Tensor\n        The program encoding the logical steps to perform in answering the question.\n    """"""\n    # remove punctuation from our question\n    import re\n    punc = \'!""#$%&\\\'()*+-./:<=>?@[\\\\]^_`{|}~\' # string.punctuation excluding comma and semicolon\n    punctuation_regex = re.compile(\'[{}]\'.format(re.escape(punc)))\n    question = punctuation_regex.sub(\'\', question).split()\n\n    # tell the user they can\'t use unknown words\n    question_token_to_idx = vocab[\'question_token_to_idx\']\n    if any(word not in question_token_to_idx for word in question):\n        print(\'Error: there are unknown words in the question you provided!\')\n        print(\'Unknown words:\')\n        print([word for word in question if word not in question_token_to_idx])\n        assert False\n\n    # encode the question using our vocab\n    encoded = np.zeros((1, question_len), dtype=\'int64\')\n    encoded[0, 0] = question_token_to_idx[\'<START>\']\n    encoded[0, 1:len(question)+1] = [question_token_to_idx[word] for word in question]\n    encoded[0, len(question)+1] = question_token_to_idx[\'<END>\']\n\n    question_tensor = torch.LongTensor(encoded)\n\n    # push to the GPU if we can\n    if torch.cuda.is_available():\n        program_generator.cuda()\n        question_tensor = question_tensor.cuda()\n    program_generator.eval()\n    \n    # generate a program\n    return program_generator.reinforce_sample(question_tensor)\n    \n\ndef generate_programs(h5_file, program_generator, dest_dir, batch_size):\n    """""" Generate programs from a given HDF5 file containing questions.\n\n    Parameters\n    ----------\n    h5_file : Union[pathlib.Path, str]\n        Path to hdf5 file containing the questions and image-indices.\n\n    program_generator : torch.nn.Module\n        The program generation model to use to produce programs.\n\n    dest_dir : Union[pathlib.Path, str]\n        Path to store the output program, image index, and question .npy files.\n\n    batch_size : Integral\n        How many programs to process at once.\n\n    Returns\n    -------\n    None\n    """"""\n    with h5py.File(str(h5_file)) as questions_h5:\n        questions = np.asarray(questions_h5[\'questions\'])\n        image_indices = np.asarray(questions_h5[\'image_idxs\'])\n\n        if torch.cuda.is_available():\n            dtype = torch.cuda.FloatTensor\n        else:\n            dtype = torch.FloatTensor\n\n        program_generator.type(dtype).eval()\n\n        print(\'Generating programs...\')\n        progs = []\n        for start_idx in range(0, len(questions), batch_size):\n            question_batch = questions[start_idx:start_idx+batch_size]\n            questions_var = torch.LongTensor(question_batch).type(dtype).long()\n            for question in questions_var:\n                program = program_generator.reinforce_sample(question.view(1, -1))\n                progs.append(program.cpu().numpy().squeeze())\n        progs = np.asarray(progs)\n    \n    dest = Path(dest_dir)\n    path = dest / \'programs.npy\'\n    np.save(path, progs)\n    print(\'Saved programs as {}\'.format(path.absolute()))\n    path = dest / \'image_idxs.npy\'\n    np.save(path, image_indices)\n    print(\'Saved image indices as {}\'.format(path.absolute()))\n    path = dest / \'questions.npy\'\n    np.save(path, questions)\n    print(\'Saved questions as {}\'.format(path.absolute()))\n'"
utils/h5_to_np.py,0,"b'# Provides functions for converting hdf5 files to npy files\n#\n# This script can be invoked with commandline arguments. E.g.\n# `python h5-to-np.py -q \'path/to/questions.h5\' -f \'path/to/features.h5\' `\n#\n# License: MIT. See LICENSE.txt for the full license.\n# DISTRIBUTION STATEMENT A. Approved for public release: distribution unlimited.\n#\n# This material is based upon work supported by the Assistant Secretary of Defense for Research and\n# Engineering under Air Force Contract No. FA8721-05-C-0002 and/or FA8702-15-D-0001. Any opinions,\n# findings, conclusions or recommendations expressed in this material are those of the author(s) and\n# do not necessarily reflect the views of the Assistant Secretary of Defense for Research and\n# Engineering.\n#\n# \xc2\xa9 2017 Massachusetts Institute of Technology.\n#\n# MIT Proprietary, Subject to FAR52.227-11 Patent Rights - Ownership by the contractor (May 2014)\n#\n# The software/firmware is provided to you on an As-Is basis\n#\n# Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or\n# 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are\n# defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than\n# as specifically authorized by the U.S. Government may violate any copyrights that exist in this\n# work.\n\nimport h5py\nimport numpy as np\nimport argparse\nfrom pathlib import Path\n\n__all__ = [\'questions_to_npy\', \'images_to_npy\', \'features_to_npy\']\n\ndef _verbose_save(h5_file, h5_key, dest_dir, dest_name, dtype):\n    """""" Save array stored in hdf5 dataset to a .npy file: {dest_dir}/{dest_name}.npy\n\n        Parameters\n        ----------\n        h5_file : h5py.File\n            Read-mode hdf5 file object.\n\n        h5_key : str\n            Item-retrieval key for hdf5 dataset.\n\n        dest_dir : Union[pathlib.Path, str]\n            Path to the directory in which the .npy file will be saved.\n\n        dest_name : str\n            The file will be `dest_name`.npy\n\n        dtype : numpy.dtype\n            Data type of the array being saved.\n\n        Returns\n        -------\n        None\n    """"""\n    if dest_name.endswith("".npy""):\n        dest_name = dest_name[:-4]\n    dest = Path(dest_dir)\n    print(\'Saving {} as npy...\'.format(dest_name))\n    np.save(dest / dest_name, np.asarray(h5_file[h5_key], dtype=dtype))\n    print(\'Saved {} as {}\'.format(dest_name, (dest / (dest_name + \'.npy\')).absolute()))\n\ndef questions_to_npy(hdf5_path, dest_dir=\'.\', dtype=np.int64):\n    """""" Reads from a hdf5-dataset encoded questions, image indices, and optionally,\n        the programs and image indices, and saves the arrays to .npy files.\n\n        Parameters\n        ----------\n        hdf5_path : Union[pathlib.Path, str]\n            Path to the hdf5 file to be read.\n\n        dest_dir : Union[pathlib.Path, str]\n            Path to the destination directory. Default: present working directory.\n\n        dtype : numpy.dtype\n            Data type of the array being saved. Default: np.int64.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        The output files are saved to:\n         - {dest_dir}/questions.npy\n         - {dest_dir}/image_indices.npy\n         - {dest_dir}/programs.npy\n         - {dest_dir}/answers.npy\n    """"""\n    with h5py.File(hdf5_path, mode=\'r\') as F:\n        _verbose_save(F, \'questions\', dest_dir, \'questions\', dtype)\n        _verbose_save(F, \'image_idxs\', dest_dir, \'image_indices\', dtype)\n        \n        if \'programs\' in F:\n            _verbose_save(F, \'programs\', dest_dir, \'programs\', dtype)\n    \n        if \'answers\' in F:\n            _verbose_save(F, \'answers\', dest_dir, \'answers\', dtype)\n\ndef images_to_npy(hdf5_path, dest_dir=\'.\', dtype=np.float32):\n    """""" Reads image data from a hdf5-dataset saves them to a .npy file.\n\n        Parameters\n        ----------\n        hdf5_path : Union[pathlib.Path, str]\n            Path to the hdf5 file to be read.\n\n        dest_dir : Union[pathlib.Path, str]\n            Path to the destination directory. Default: present working directory.\n\n        dtype : numpy.dtype\n            Data type of the array being saved. Default: np.float32.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        The output file is saved to:\n         - {dest_dir}/images.npy\n    """"""\n    with h5py.File(hdf5_path, mode=\'r\') as F:\n        _verbose_save(F, \'images\', dest_dir, \'images\', dtype)\n\ndef features_to_npy(hdf5_path, dest_dir=\'.\', dtype=np.float32):\n    """""" Reads feature data from a hdf5-dataset saves them to a .npy file.\n\n        Parameters\n        ----------\n        hdf5_path : Union[pathlib.Path, str]\n            Path to the hdf5 file to be read.\n\n        dest_dir : Union[pathlib.Path, str]\n            Path to the destination directory. Default: present working directory.\n\n        dtype : numpy.dtype\n            Data type of the array being saved. Default: np.float32.\n\n        Returns\n        -------\n        None\n\n        Notes\n        -----\n        The output file is saved to:\n         - {dest_dir}/features.npy""""""\n    with h5py.File(hdf5_path, mode=\'r\') as F:\n        _verbose_save(F, \'features\', dest_dir, \'features\', dtype)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--questions\', \'-q\', required=True,\n                        help=\'HDF5 file containing the questions\')\n\n    parser.add_argument(\'--features\', \'-f\', required=True,\n                        help=\'HDF5 file containing the extracted image features\')\n\n    parser.add_argument(\'--images\', \'-i\', required=False, default=None,\n                        help=\'HDF5 file containing the images\')\n\n    parser.add_argument(\'--destination\', \'-d\', required=False, default=\'.\',\n                        help=\'The directory to write the numpy files to\')\n\n    args = parser.parse_args()\n\n    questions_to_npy(args.questions, args.destination)\n    features_to_npy(args.features, args.destination)\n\n    if args.images is not None:\n        images_to_npy(args.images, args.destination)\n\n    print(\'Finished!\')\n'"
