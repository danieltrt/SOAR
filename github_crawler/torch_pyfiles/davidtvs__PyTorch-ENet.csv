file_path,api_count,code
args.py,0,"b'from argparse import ArgumentParser\n\n\ndef get_arguments():\n    """"""Defines command-line arguments, and parses them.\n\n    """"""\n    parser = ArgumentParser()\n\n    # Execution mode\n    parser.add_argument(\n        ""--mode"",\n        ""-m"",\n        choices=[\'train\', \'test\', \'full\'],\n        default=\'train\',\n        help=(""train: performs training and validation; test: tests the model ""\n              ""found in \\""--save_dir\\"" with name \\""--name\\"" on \\""--dataset\\""; ""\n              ""full: combines train and test modes. Default: train""))\n    parser.add_argument(\n        ""--resume"",\n        action=\'store_true\',\n        help=(""The model found in \\""--checkpoint_dir/--name/\\"" and filename ""\n              ""\\""--name.h5\\"" is loaded.""))\n\n    # Hyperparameters\n    parser.add_argument(\n        ""--batch-size"",\n        ""-b"",\n        type=int,\n        default=10,\n        help=""The batch size. Default: 10"")\n    parser.add_argument(\n        ""--epochs"",\n        type=int,\n        default=300,\n        help=""Number of training epochs. Default: 300"")\n    parser.add_argument(\n        ""--learning-rate"",\n        ""-lr"",\n        type=float,\n        default=5e-4,\n        help=""The learning rate. Default: 5e-4"")\n    parser.add_argument(\n        ""--lr-decay"",\n        type=float,\n        default=0.1,\n        help=""The learning rate decay factor. Default: 0.5"")\n    parser.add_argument(\n        ""--lr-decay-epochs"",\n        type=int,\n        default=100,\n        help=""The number of epochs before adjusting the learning rate. ""\n        ""Default: 100"")\n    parser.add_argument(\n        ""--weight-decay"",\n        ""-wd"",\n        type=float,\n        default=2e-4,\n        help=""L2 regularization factor. Default: 2e-4"")\n\n    # Dataset\n    parser.add_argument(\n        ""--dataset"",\n        choices=[\'camvid\', \'cityscapes\'],\n        default=\'camvid\',\n        help=""Dataset to use. Default: camvid"")\n    parser.add_argument(\n        ""--dataset-dir"",\n        type=str,\n        default=""data/CamVid"",\n        help=""Path to the root directory of the selected dataset. ""\n        ""Default: data/CamVid"")\n    parser.add_argument(\n        ""--height"",\n        type=int,\n        default=360,\n        help=""The image height. Default: 360"")\n    parser.add_argument(\n        ""--width"",\n        type=int,\n        default=480,\n        help=""The image width. Default: 480"")\n    parser.add_argument(\n        ""--weighing"",\n        choices=[\'enet\', \'mfb\', \'none\'],\n        default=\'ENet\',\n        help=""The class weighing technique to apply to the dataset. ""\n        ""Default: enet"")\n    parser.add_argument(\n        ""--with-unlabeled"",\n        dest=\'ignore_unlabeled\',\n        action=\'store_false\',\n        help=""The unlabeled class is not ignored."")\n\n    # Settings\n    parser.add_argument(\n        ""--workers"",\n        type=int,\n        default=4,\n        help=""Number of subprocesses to use for data loading. Default: 4"")\n    parser.add_argument(\n        ""--print-step"",\n        action=\'store_true\',\n        help=""Print loss every step"")\n    parser.add_argument(\n        ""--imshow-batch"",\n        action=\'store_true\',\n        help=(""Displays batch images when loading the dataset and making ""\n              ""predictions.""))\n    parser.add_argument(\n        ""--device"",\n        default=\'cuda\',\n        help=""Device on which the network will be trained. Default: cuda"")\n\n    # Storage settings\n    parser.add_argument(\n        ""--name"",\n        type=str,\n        default=\'ENet\',\n        help=""Name given to the model when saving. Default: ENet"")\n    parser.add_argument(\n        ""--save-dir"",\n        type=str,\n        default=\'save\',\n        help=""The directory where models are saved. Default: save"")\n\n    return parser.parse_args()\n'"
main.py,8,"b'import os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\n\nimport transforms as ext_transforms\nfrom models.enet import ENet\nfrom train import Train\nfrom test import Test\nfrom metric.iou import IoU\nfrom args import get_arguments\nfrom data.utils import enet_weighing, median_freq_balancing\nimport utils\n\n# Get the arguments\nargs = get_arguments()\n\ndevice = torch.device(args.device)\n\n\ndef load_dataset(dataset):\n    print(""\\nLoading dataset...\\n"")\n\n    print(""Selected dataset:"", args.dataset)\n    print(""Dataset directory:"", args.dataset_dir)\n    print(""Save directory:"", args.save_dir)\n\n    image_transform = transforms.Compose(\n        [transforms.Resize((args.height, args.width)),\n         transforms.ToTensor()])\n\n    label_transform = transforms.Compose([\n        transforms.Resize((args.height, args.width), Image.NEAREST),\n        ext_transforms.PILToLongTensor()\n    ])\n\n    # Get selected dataset\n    # Load the training set as tensors\n    train_set = dataset(\n        args.dataset_dir,\n        transform=image_transform,\n        label_transform=label_transform)\n    train_loader = data.DataLoader(\n        train_set,\n        batch_size=args.batch_size,\n        shuffle=True,\n        num_workers=args.workers)\n\n    # Load the validation set as tensors\n    val_set = dataset(\n        args.dataset_dir,\n        mode=\'val\',\n        transform=image_transform,\n        label_transform=label_transform)\n    val_loader = data.DataLoader(\n        val_set,\n        batch_size=args.batch_size,\n        shuffle=False,\n        num_workers=args.workers)\n\n    # Load the test set as tensors\n    test_set = dataset(\n        args.dataset_dir,\n        mode=\'test\',\n        transform=image_transform,\n        label_transform=label_transform)\n    test_loader = data.DataLoader(\n        test_set,\n        batch_size=args.batch_size,\n        shuffle=False,\n        num_workers=args.workers)\n\n    # Get encoding between pixel valus in label images and RGB colors\n    class_encoding = train_set.color_encoding\n\n    # Remove the road_marking class from the CamVid dataset as it\'s merged\n    # with the road class\n    if args.dataset.lower() == \'camvid\':\n        del class_encoding[\'road_marking\']\n\n    # Get number of classes to predict\n    num_classes = len(class_encoding)\n\n    # Print information for debugging\n    print(""Number of classes to predict:"", num_classes)\n    print(""Train dataset size:"", len(train_set))\n    print(""Validation dataset size:"", len(val_set))\n\n    # Get a batch of samples to display\n    if args.mode.lower() == \'test\':\n        images, labels = iter(test_loader).next()\n    else:\n        images, labels = iter(train_loader).next()\n    print(""Image size:"", images.size())\n    print(""Label size:"", labels.size())\n    print(""Class-color encoding:"", class_encoding)\n\n    # Show a batch of samples and labels\n    if args.imshow_batch:\n        print(""Close the figure window to continue..."")\n        label_to_rgb = transforms.Compose([\n            ext_transforms.LongTensorToRGBPIL(class_encoding),\n            transforms.ToTensor()\n        ])\n        color_labels = utils.batch_transform(labels, label_to_rgb)\n        utils.imshow_batch(images, color_labels)\n\n    # Get class weights from the selected weighing technique\n    print(""\\nWeighing technique:"", args.weighing)\n    print(""Computing class weights..."")\n    print(""(this can take a while depending on the dataset size)"")\n    class_weights = 0\n    if args.weighing.lower() == \'enet\':\n        class_weights = enet_weighing(train_loader, num_classes)\n    elif args.weighing.lower() == \'mfb\':\n        class_weights = median_freq_balancing(train_loader, num_classes)\n    else:\n        class_weights = None\n\n    if class_weights is not None:\n        class_weights = torch.from_numpy(class_weights).float().to(device)\n        # Set the weight of the unlabeled class to 0\n        if args.ignore_unlabeled:\n            ignore_index = list(class_encoding).index(\'unlabeled\')\n            class_weights[ignore_index] = 0\n\n    print(""Class weights:"", class_weights)\n\n    return (train_loader, val_loader,\n            test_loader), class_weights, class_encoding\n\n\ndef train(train_loader, val_loader, class_weights, class_encoding):\n    print(""\\nTraining...\\n"")\n\n    num_classes = len(class_encoding)\n\n    # Intialize ENet\n    model = ENet(num_classes).to(device)\n    # Check if the network architecture is correct\n    print(model)\n\n    # We are going to use the CrossEntropyLoss loss function as it\'s most\n    # frequentely used in classification problems with multiple classes which\n    # fits the problem. This criterion  combines LogSoftMax and NLLLoss.\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n\n    # ENet authors used Adam as the optimizer\n    optimizer = optim.Adam(\n        model.parameters(),\n        lr=args.learning_rate,\n        weight_decay=args.weight_decay)\n\n    # Learning rate decay scheduler\n    lr_updater = lr_scheduler.StepLR(optimizer, args.lr_decay_epochs,\n                                     args.lr_decay)\n\n    # Evaluation metric\n    if args.ignore_unlabeled:\n        ignore_index = list(class_encoding).index(\'unlabeled\')\n    else:\n        ignore_index = None\n    metric = IoU(num_classes, ignore_index=ignore_index)\n\n    # Optionally resume from a checkpoint\n    if args.resume:\n        model, optimizer, start_epoch, best_miou = utils.load_checkpoint(\n            model, optimizer, args.save_dir, args.name)\n        print(""Resuming from model: Start epoch = {0} ""\n              ""| Best mean IoU = {1:.4f}"".format(start_epoch, best_miou))\n    else:\n        start_epoch = 0\n        best_miou = 0\n\n    # Start Training\n    print()\n    train = Train(model, train_loader, optimizer, criterion, metric, device)\n    val = Test(model, val_loader, criterion, metric, device)\n    for epoch in range(start_epoch, args.epochs):\n        print("">>>> [Epoch: {0:d}] Training"".format(epoch))\n\n        lr_updater.step()\n        epoch_loss, (iou, miou) = train.run_epoch(args.print_step)\n\n        print("">>>> [Epoch: {0:d}] Avg. loss: {1:.4f} | Mean IoU: {2:.4f}"".\n              format(epoch, epoch_loss, miou))\n\n        if (epoch + 1) % 10 == 0 or epoch + 1 == args.epochs:\n            print("">>>> [Epoch: {0:d}] Validation"".format(epoch))\n\n            loss, (iou, miou) = val.run_epoch(args.print_step)\n\n            print("">>>> [Epoch: {0:d}] Avg. loss: {1:.4f} | Mean IoU: {2:.4f}"".\n                  format(epoch, loss, miou))\n\n            # Print per class IoU on last epoch or if best iou\n            if epoch + 1 == args.epochs or miou > best_miou:\n                for key, class_iou in zip(class_encoding.keys(), iou):\n                    print(""{0}: {1:.4f}"".format(key, class_iou))\n\n            # Save the model if it\'s the best thus far\n            if miou > best_miou:\n                print(""\\nBest model thus far. Saving...\\n"")\n                best_miou = miou\n                utils.save_checkpoint(model, optimizer, epoch + 1, best_miou,\n                                      args)\n\n    return model\n\n\ndef test(model, test_loader, class_weights, class_encoding):\n    print(""\\nTesting...\\n"")\n\n    num_classes = len(class_encoding)\n\n    # We are going to use the CrossEntropyLoss loss function as it\'s most\n    # frequentely used in classification problems with multiple classes which\n    # fits the problem. This criterion  combines LogSoftMax and NLLLoss.\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n\n    # Evaluation metric\n    if args.ignore_unlabeled:\n        ignore_index = list(class_encoding).index(\'unlabeled\')\n    else:\n        ignore_index = None\n    metric = IoU(num_classes, ignore_index=ignore_index)\n\n    # Test the trained model on the test set\n    test = Test(model, test_loader, criterion, metric, device)\n\n    print("">>>> Running test dataset"")\n\n    loss, (iou, miou) = test.run_epoch(args.print_step)\n    class_iou = dict(zip(class_encoding.keys(), iou))\n\n    print("">>>> Avg. loss: {0:.4f} | Mean IoU: {1:.4f}"".format(loss, miou))\n\n    # Print per class IoU\n    for key, class_iou in zip(class_encoding.keys(), iou):\n        print(""{0}: {1:.4f}"".format(key, class_iou))\n\n    # Show a batch of samples and labels\n    if args.imshow_batch:\n        print(""A batch of predictions from the test set..."")\n        images, _ = iter(test_loader).next()\n        predict(model, images, class_encoding)\n\n\ndef predict(model, images, class_encoding):\n    images = images.to(device)\n\n    # Make predictions!\n    model.eval()\n    with torch.no_grad():\n        predictions = model(images)\n\n    # Predictions is one-hot encoded with ""num_classes"" channels.\n    # Convert it to a single int using the indices where the maximum (1) occurs\n    _, predictions = torch.max(predictions.data, 1)\n\n    label_to_rgb = transforms.Compose([\n        ext_transforms.LongTensorToRGBPIL(class_encoding),\n        transforms.ToTensor()\n    ])\n    color_predictions = utils.batch_transform(predictions.cpu(), label_to_rgb)\n    utils.imshow_batch(images.data.cpu(), color_predictions)\n\n\n# Run only if this module is being run directly\nif __name__ == \'__main__\':\n\n    # Fail fast if the dataset directory doesn\'t exist\n    assert os.path.isdir(\n        args.dataset_dir), ""The directory \\""{0}\\"" doesn\'t exist."".format(\n            args.dataset_dir)\n\n    # Fail fast if the saving directory doesn\'t exist\n    assert os.path.isdir(\n        args.save_dir), ""The directory \\""{0}\\"" doesn\'t exist."".format(\n            args.save_dir)\n\n    # Import the requested dataset\n    if args.dataset.lower() == \'camvid\':\n        from data import CamVid as dataset\n    elif args.dataset.lower() == \'cityscapes\':\n        from data import Cityscapes as dataset\n    else:\n        # Should never happen...but just in case it does\n        raise RuntimeError(""\\""{0}\\"" is not a supported dataset."".format(\n            args.dataset))\n\n    loaders, w_class, class_encoding = load_dataset(dataset)\n    train_loader, val_loader, test_loader = loaders\n\n    if args.mode.lower() in {\'train\', \'full\'}:\n        model = train(train_loader, val_loader, w_class, class_encoding)\n\n    if args.mode.lower() in {\'test\', \'full\'}:\n        if args.mode.lower() == \'test\':\n            # Intialize a new ENet model\n            num_classes = len(class_encoding)\n            model = ENet(num_classes).to(device)\n\n        # Initialize a optimizer just so we can retrieve the model from the\n        # checkpoint\n        optimizer = optim.Adam(model.parameters())\n\n        # Load the previoulsy saved model state to the ENet model\n        model = utils.load_checkpoint(model, optimizer, args.save_dir,\n                                      args.name)[0]\n\n        if args.mode.lower() == \'test\':\n            print(model)\n\n        test(model, test_loader, w_class, class_encoding)\n'"
test.py,2,"b'import torch\n\n\nclass Test:\n    """"""Tests the ``model`` on the specified test dataset using the\n    data loader, and loss criterion.\n\n    Keyword arguments:\n    - model (``nn.Module``): the model instance to test.\n    - data_loader (``Dataloader``): Provides single or multi-process\n    iterators over the dataset.\n    - criterion (``Optimizer``): The loss criterion.\n    - metric (```Metric``): An instance specifying the metric to return.\n    - device (``torch.device``): An object representing the device on which\n    tensors are allocated.\n\n    """"""\n\n    def __init__(self, model, data_loader, criterion, metric, device):\n        self.model = model\n        self.data_loader = data_loader\n        self.criterion = criterion\n        self.metric = metric\n        self.device = device\n\n    def run_epoch(self, iteration_loss=False):\n        """"""Runs an epoch of validation.\n\n        Keyword arguments:\n        - iteration_loss (``bool``, optional): Prints loss at every step.\n\n        Returns:\n        - The epoch loss (float), and the values of the specified metrics\n\n        """"""\n        self.model.eval()\n        epoch_loss = 0.0\n        self.metric.reset()\n        for step, batch_data in enumerate(self.data_loader):\n            # Get the inputs and labels\n            inputs = batch_data[0].to(self.device)\n            labels = batch_data[1].to(self.device)\n\n            with torch.no_grad():\n                # Forward propagation\n                outputs = self.model(inputs)\n\n                # Loss computation\n                loss = self.criterion(outputs, labels)\n\n            # Keep track of loss for current epoch\n            epoch_loss += loss.item()\n\n            # Keep track of evaluation the metric\n            self.metric.add(outputs.detach(), labels.detach())\n\n            if iteration_loss:\n                print(""[Step: %d] Iteration loss: %.4f"" % (step, loss.item()))\n\n        return epoch_loss / len(self.data_loader), self.metric.value()\n'"
train.py,1,"b'class Train:\n    """"""Performs the training of ``model`` given a training dataset data\n    loader, the optimizer, and the loss criterion.\n\n    Keyword arguments:\n    - model (``nn.Module``): the model instance to train.\n    - data_loader (``Dataloader``): Provides single or multi-process\n    iterators over the dataset.\n    - optim (``Optimizer``): The optimization algorithm.\n    - criterion (``Optimizer``): The loss criterion.\n    - metric (```Metric``): An instance specifying the metric to return.\n    - device (``torch.device``): An object representing the device on which\n    tensors are allocated.\n\n    """"""\n\n    def __init__(self, model, data_loader, optim, criterion, metric, device):\n        self.model = model\n        self.data_loader = data_loader\n        self.optim = optim\n        self.criterion = criterion\n        self.metric = metric\n        self.device = device\n\n    def run_epoch(self, iteration_loss=False):\n        """"""Runs an epoch of training.\n\n        Keyword arguments:\n        - iteration_loss (``bool``, optional): Prints loss at every step.\n\n        Returns:\n        - The epoch loss (float).\n\n        """"""\n        self.model.train()\n        epoch_loss = 0.0\n        self.metric.reset()\n        for step, batch_data in enumerate(self.data_loader):\n            # Get the inputs and labels\n            inputs = batch_data[0].to(self.device)\n            labels = batch_data[1].to(self.device)\n\n            # Forward propagation\n            outputs = self.model(inputs)\n\n            # Loss computation\n            loss = self.criterion(outputs, labels)\n\n            # Backpropagation\n            self.optim.zero_grad()\n            loss.backward()\n            self.optim.step()\n\n            # Keep track of loss for current epoch\n            epoch_loss += loss.item()\n\n            # Keep track of the evaluation metric\n            self.metric.add(outputs.detach(), labels.detach())\n\n            if iteration_loss:\n                print(""[Step: %d] Iteration loss: %.4f"" % (step, loss.item()))\n\n        return epoch_loss / len(self.data_loader), self.metric.value()\n'"
transforms.py,15,"b'import torch\nimport numpy as np\nfrom PIL import Image\nfrom collections import OrderedDict\nfrom torchvision.transforms import ToPILImage\n\n\nclass PILToLongTensor(object):\n    """"""Converts a ``PIL Image`` to a ``torch.LongTensor``.\n\n    Code adapted from: http://pytorch.org/docs/master/torchvision/transforms.html?highlight=totensor\n\n    """"""\n\n    def __call__(self, pic):\n        """"""Performs the conversion from a ``PIL Image`` to a ``torch.LongTensor``.\n\n        Keyword arguments:\n        - pic (``PIL.Image``): the image to convert to ``torch.LongTensor``\n\n        Returns:\n        A ``torch.LongTensor``.\n\n        """"""\n        if not isinstance(pic, Image.Image):\n            raise TypeError(""pic should be PIL Image. Got {}"".format(\n                type(pic)))\n\n        # handle numpy array\n        if isinstance(pic, np.ndarray):\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n            # backward compatibility\n            return img.long()\n\n        # Convert PIL image to ByteTensor\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n\n        # Reshape tensor\n        nchannel = len(pic.mode)\n        img = img.view(pic.size[1], pic.size[0], nchannel)\n\n        # Convert to long and squeeze the channels\n        return img.transpose(0, 1).transpose(0,\n                                             2).contiguous().long().squeeze_()\n\n\nclass LongTensorToRGBPIL(object):\n    """"""Converts a ``torch.LongTensor`` to a ``PIL image``.\n\n    The input is a ``torch.LongTensor`` where each pixel\'s value identifies the\n    class.\n\n    Keyword arguments:\n    - rgb_encoding (``OrderedDict``): An ``OrderedDict`` that relates pixel\n    values, class names, and class colors.\n\n    """"""\n    def __init__(self, rgb_encoding):\n        self.rgb_encoding = rgb_encoding\n\n    def __call__(self, tensor):\n        """"""Performs the conversion from ``torch.LongTensor`` to a ``PIL image``\n\n        Keyword arguments:\n        - tensor (``torch.LongTensor``): the tensor to convert\n\n        Returns:\n        A ``PIL.Image``.\n\n        """"""\n        # Check if label_tensor is a LongTensor\n        if not isinstance(tensor, torch.LongTensor):\n            raise TypeError(""label_tensor should be torch.LongTensor. Got {}""\n                            .format(type(tensor)))\n        # Check if encoding is a ordered dictionary\n        if not isinstance(self.rgb_encoding, OrderedDict):\n            raise TypeError(""encoding should be an OrderedDict. Got {}"".format(\n                type(self.rgb_encoding)))\n\n        # label_tensor might be an image without a channel dimension, in this\n        # case unsqueeze it\n        if len(tensor.size()) == 2:\n            tensor.unsqueeze_(0)\n\n        color_tensor = torch.ByteTensor(3, tensor.size(1), tensor.size(2))\n\n        for index, (class_name, color) in enumerate(self.rgb_encoding.items()):\n            # Get a mask of elements equal to index\n            mask = torch.eq(tensor, index).squeeze_()\n            # Fill color_tensor with corresponding colors\n            for channel, color_value in enumerate(color):\n                color_tensor[channel].masked_fill_(mask, color_value)\n\n        return ToPILImage()(color_tensor)\n'"
utils.py,7,"b'import torch\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n\ndef batch_transform(batch, transform):\n    """"""Applies a transform to a batch of samples.\n\n    Keyword arguments:\n    - batch (): a batch os samples\n    - transform (callable): A function/transform to apply to ``batch``\n\n    """"""\n\n    # Convert the single channel label to RGB in tensor form\n    # 1. torch.unbind removes the 0-dimension of ""labels"" and returns a tuple of\n    # all slices along that dimension\n    # 2. the transform is applied to each slice\n    transf_slices = [transform(tensor) for tensor in torch.unbind(batch)]\n\n    return torch.stack(transf_slices)\n\n\ndef imshow_batch(images, labels):\n    """"""Displays two grids of images. The top grid displays ``images``\n    and the bottom grid ``labels``\n\n    Keyword arguments:\n    - images (``Tensor``): a 4D mini-batch tensor of shape\n    (B, C, H, W)\n    - labels (``Tensor``): a 4D mini-batch tensor of shape\n    (B, C, H, W)\n\n    """"""\n\n    # Make a grid with the images and labels and convert it to numpy\n    images = torchvision.utils.make_grid(images).numpy()\n    labels = torchvision.utils.make_grid(labels).numpy()\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 7))\n    ax1.imshow(np.transpose(images, (1, 2, 0)))\n    ax2.imshow(np.transpose(labels, (1, 2, 0)))\n\n    plt.show()\n\n\ndef save_checkpoint(model, optimizer, epoch, miou, args):\n    """"""Saves the model in a specified directory with a specified name.save\n\n    Keyword arguments:\n    - model (``nn.Module``): The model to save.\n    - optimizer (``torch.optim``): The optimizer state to save.\n    - epoch (``int``): The current epoch for the model.\n    - miou (``float``): The mean IoU obtained by the model.\n    - args (``ArgumentParser``): An instance of ArgumentParser which contains\n    the arguments used to train ``model``. The arguments are written to a text\n    file in ``args.save_dir`` named ""``args.name``_args.txt"".\n\n    """"""\n    name = args.name\n    save_dir = args.save_dir\n\n    assert os.path.isdir(\n        save_dir), ""The directory \\""{0}\\"" doesn\'t exist."".format(save_dir)\n\n    # Save model\n    model_path = os.path.join(save_dir, name)\n    checkpoint = {\n        \'epoch\': epoch,\n        \'miou\': miou,\n        \'state_dict\': model.state_dict(),\n        \'optimizer\': optimizer.state_dict()\n    }\n    torch.save(checkpoint, model_path)\n\n    # Save arguments\n    summary_filename = os.path.join(save_dir, name + \'_summary.txt\')\n    with open(summary_filename, \'w\') as summary_file:\n        sorted_args = sorted(vars(args))\n        summary_file.write(""ARGUMENTS\\n"")\n        for arg in sorted_args:\n            arg_str = ""{0}: {1}\\n"".format(arg, getattr(args, arg))\n            summary_file.write(arg_str)\n\n        summary_file.write(""\\nBEST VALIDATION\\n"")\n        summary_file.write(""Epoch: {0}\\n"". format(epoch))\n        summary_file.write(""Mean IoU: {0}\\n"". format(miou))\n\n\ndef load_checkpoint(model, optimizer, folder_dir, filename):\n    """"""Saves the model in a specified directory with a specified name.save\n\n    Keyword arguments:\n    - model (``nn.Module``): The stored model state is copied to this model\n    instance.\n    - optimizer (``torch.optim``): The stored optimizer state is copied to this\n    optimizer instance.\n    - folder_dir (``string``): The path to the folder where the saved model\n    state is located.\n    - filename (``string``): The model filename.\n\n    Returns:\n    The epoch, mean IoU, ``model``, and ``optimizer`` loaded from the\n    checkpoint.\n\n    """"""\n    assert os.path.isdir(\n        folder_dir), ""The directory \\""{0}\\"" doesn\'t exist."".format(folder_dir)\n\n    # Create folder to save model and information\n    model_path = os.path.join(folder_dir, filename)\n    assert os.path.isfile(\n        model_path), ""The model file \\""{0}\\"" doesn\'t exist."".format(filename)\n\n    # Load the stored model parameters to the model instance\n    checkpoint = torch.load(model_path)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n    optimizer.load_state_dict(checkpoint[\'optimizer\'])\n    epoch = checkpoint[\'epoch\']\n    miou = checkpoint[\'miou\']\n\n    return model, optimizer, epoch, miou\n'"
data/__init__.py,0,"b""from .camvid import CamVid\nfrom .cityscapes import Cityscapes\n\n__all__ = ['CamVid', 'Cityscapes']\n"""
data/camvid.py,1,"b'import os\nfrom collections import OrderedDict\nimport torch.utils.data as data\nfrom . import utils\n\n\nclass CamVid(data.Dataset):\n    """"""CamVid dataset loader where the dataset is arranged as in\n    https://github.com/alexgkendall/SegNet-Tutorial/tree/master/CamVid.\n\n\n    Keyword arguments:\n    - root_dir (``string``): Root directory path.\n    - mode (``string``): The type of dataset: \'train\' for training set, \'val\'\n    for validation set, and \'test\' for test set.\n    - transform (``callable``, optional): A function/transform that  takes in\n    an PIL image and returns a transformed version. Default: None.\n    - label_transform (``callable``, optional): A function/transform that takes\n    in the target and transforms it. Default: None.\n    - loader (``callable``, optional): A function to load an image given its\n    path. By default ``default_loader`` is used.\n\n    """"""\n    # Training dataset root folders\n    train_folder = \'train\'\n    train_lbl_folder = \'trainannot\'\n\n    # Validation dataset root folders\n    val_folder = \'val\'\n    val_lbl_folder = \'valannot\'\n\n    # Test dataset root folders\n    test_folder = \'test\'\n    test_lbl_folder = \'testannot\'\n\n    # Images extension\n    img_extension = \'.png\'\n\n    # Default encoding for pixel value, class name, and class color\n    color_encoding = OrderedDict([\n        (\'sky\', (128, 128, 128)),\n        (\'building\', (128, 0, 0)),\n        (\'pole\', (192, 192, 128)),\n        (\'road_marking\', (255, 69, 0)),\n        (\'road\', (128, 64, 128)),\n        (\'pavement\', (60, 40, 222)),\n        (\'tree\', (128, 128, 0)),\n        (\'sign_symbol\', (192, 128, 128)),\n        (\'fence\', (64, 64, 128)),\n        (\'car\', (64, 0, 128)),\n        (\'pedestrian\', (64, 64, 0)),\n        (\'bicyclist\', (0, 128, 192)),\n        (\'unlabeled\', (0, 0, 0))\n    ])\n\n    def __init__(self,\n                 root_dir,\n                 mode=\'train\',\n                 transform=None,\n                 label_transform=None,\n                 loader=utils.pil_loader):\n        self.root_dir = root_dir\n        self.mode = mode\n        self.transform = transform\n        self.label_transform = label_transform\n        self.loader = loader\n\n        if self.mode.lower() == \'train\':\n            # Get the training data and labels filepaths\n            self.train_data = utils.get_files(\n                os.path.join(root_dir, self.train_folder),\n                extension_filter=self.img_extension)\n\n            self.train_labels = utils.get_files(\n                os.path.join(root_dir, self.train_lbl_folder),\n                extension_filter=self.img_extension)\n        elif self.mode.lower() == \'val\':\n            # Get the validation data and labels filepaths\n            self.val_data = utils.get_files(\n                os.path.join(root_dir, self.val_folder),\n                extension_filter=self.img_extension)\n\n            self.val_labels = utils.get_files(\n                os.path.join(root_dir, self.val_lbl_folder),\n                extension_filter=self.img_extension)\n        elif self.mode.lower() == \'test\':\n            # Get the test data and labels filepaths\n            self.test_data = utils.get_files(\n                os.path.join(root_dir, self.test_folder),\n                extension_filter=self.img_extension)\n\n            self.test_labels = utils.get_files(\n                os.path.join(root_dir, self.test_lbl_folder),\n                extension_filter=self.img_extension)\n        else:\n            raise RuntimeError(""Unexpected dataset mode. ""\n                               ""Supported modes are: train, val and test"")\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n        - index (``int``): index of the item in the dataset\n\n        Returns:\n        A tuple of ``PIL.Image`` (image, label) where label is the ground-truth\n        of the image.\n\n        """"""\n        if self.mode.lower() == \'train\':\n            data_path, label_path = self.train_data[index], self.train_labels[\n                index]\n        elif self.mode.lower() == \'val\':\n            data_path, label_path = self.val_data[index], self.val_labels[\n                index]\n        elif self.mode.lower() == \'test\':\n            data_path, label_path = self.test_data[index], self.test_labels[\n                index]\n        else:\n            raise RuntimeError(""Unexpected dataset mode. ""\n                               ""Supported modes are: train, val and test"")\n\n        img, label = self.loader(data_path, label_path)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.label_transform is not None:\n            label = self.label_transform(label)\n\n        return img, label\n\n    def __len__(self):\n        """"""Returns the length of the dataset.""""""\n        if self.mode.lower() == \'train\':\n            return len(self.train_data)\n        elif self.mode.lower() == \'val\':\n            return len(self.val_data)\n        elif self.mode.lower() == \'test\':\n            return len(self.test_data)\n        else:\n            raise RuntimeError(""Unexpected dataset mode. ""\n                               ""Supported modes are: train, val and test"")\n'"
data/cityscapes.py,1,"b'import os\nfrom collections import OrderedDict\nimport torch.utils.data as data\nfrom . import utils\n\n\nclass Cityscapes(data.Dataset):\n    """"""Cityscapes dataset https://www.cityscapes-dataset.com/.\n\n    Keyword arguments:\n    - root_dir (``string``): Root directory path.\n    - mode (``string``): The type of dataset: \'train\' for training set, \'val\'\n    for validation set, and \'test\' for test set.\n    - transform (``callable``, optional): A function/transform that  takes in\n    an PIL image and returns a transformed version. Default: None.\n    - label_transform (``callable``, optional): A function/transform that takes\n    in the target and transforms it. Default: None.\n    - loader (``callable``, optional): A function to load an image given its\n    path. By default ``default_loader`` is used.\n\n    """"""\n    # Training dataset root folders\n    train_folder = ""leftImg8bit_trainvaltest/leftImg8bit/train""\n    train_lbl_folder = ""gtFine_trainvaltest/gtFine/train""\n\n    # Validation dataset root folders\n    val_folder = ""leftImg8bit_trainvaltest/leftImg8bit/val""\n    val_lbl_folder = ""gtFine_trainvaltest/gtFine/val""\n\n    # Test dataset root folders\n    test_folder = ""leftImg8bit_trainvaltest/leftImg8bit/test""\n    test_lbl_folder = ""gtFine_trainvaltest/gtFine/test""\n\n    # Filters to find the images\n    img_extension = \'.png\'\n    lbl_name_filter = \'labelIds\'\n\n    # The values associated with the 35 classes\n    full_classes = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n                    17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,\n                    32, 33, -1)\n    # The values above are remapped to the following\n    new_classes = (0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 3, 4, 5, 0, 0, 0, 6, 0, 7,\n                   8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 0, 17, 18, 19, 0)\n\n    # Default encoding for pixel value, class name, and class color\n    color_encoding = OrderedDict([\n            (\'unlabeled\', (0, 0, 0)),\n            (\'road\', (128, 64, 128)),\n            (\'sidewalk\', (244, 35, 232)),\n            (\'building\', (70, 70, 70)),\n            (\'wall\', (102, 102, 156)),\n            (\'fence\', (190, 153, 153)),\n            (\'pole\', (153, 153, 153)),\n            (\'traffic_light\', (250, 170, 30)),\n            (\'traffic_sign\', (220, 220, 0)),\n            (\'vegetation\', (107, 142, 35)),\n            (\'terrain\', (152, 251, 152)),\n            (\'sky\', (70, 130, 180)),\n            (\'person\', (220, 20, 60)),\n            (\'rider\', (255, 0, 0)),\n            (\'car\', (0, 0, 142)),\n            (\'truck\', (0, 0, 70)),\n            (\'bus\', (0, 60, 100)),\n            (\'train\', (0, 80, 100)),\n            (\'motorcycle\', (0, 0, 230)),\n            (\'bicycle\', (119, 11, 32))\n    ])\n\n    def __init__(self,\n                 root_dir,\n                 mode=\'train\',\n                 transform=None,\n                 label_transform=None,\n                 loader=utils.pil_loader):\n        self.root_dir = root_dir\n        self.mode = mode\n        self.transform = transform\n        self.label_transform = label_transform\n        self.loader = loader\n\n        if self.mode.lower() == \'train\':\n            # Get the training data and labels filepaths\n            self.train_data = utils.get_files(\n                os.path.join(root_dir, self.train_folder),\n                extension_filter=self.img_extension)\n\n            self.train_labels = utils.get_files(\n                os.path.join(root_dir, self.train_lbl_folder),\n                name_filter=self.lbl_name_filter,\n                extension_filter=self.img_extension)\n        elif self.mode.lower() == \'val\':\n            # Get the validation data and labels filepaths\n            self.val_data = utils.get_files(\n                os.path.join(root_dir, self.val_folder),\n                extension_filter=self.img_extension)\n\n            self.val_labels = utils.get_files(\n                os.path.join(root_dir, self.val_lbl_folder),\n                name_filter=self.lbl_name_filter,\n                extension_filter=self.img_extension)\n        elif self.mode.lower() == \'test\':\n            # Get the test data and labels filepaths\n            self.test_data = utils.get_files(\n                os.path.join(root_dir, self.test_folder),\n                extension_filter=self.img_extension)\n\n            self.test_labels = utils.get_files(\n                os.path.join(root_dir, self.test_lbl_folder),\n                name_filter=self.lbl_name_filter,\n                extension_filter=self.img_extension)\n        else:\n            raise RuntimeError(""Unexpected dataset mode. ""\n                               ""Supported modes are: train, val and test"")\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n        - index (``int``): index of the item in the dataset\n\n        Returns:\n        A tuple of ``PIL.Image`` (image, label) where label is the ground-truth\n        of the image.\n\n        """"""\n        if self.mode.lower() == \'train\':\n            data_path, label_path = self.train_data[index], self.train_labels[\n                index]\n        elif self.mode.lower() == \'val\':\n            data_path, label_path = self.val_data[index], self.val_labels[\n                index]\n        elif self.mode.lower() == \'test\':\n            data_path, label_path = self.test_data[index], self.test_labels[\n                index]\n        else:\n            raise RuntimeError(""Unexpected dataset mode. ""\n                               ""Supported modes are: train, val and test"")\n\n        img, label = self.loader(data_path, label_path)\n\n        # Remap class labels\n        label = utils.remap(label, self.full_classes, self.new_classes)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.label_transform is not None:\n            label = self.label_transform(label)\n\n        return img, label\n\n    def __len__(self):\n        """"""Returns the length of the dataset.""""""\n        if self.mode.lower() == \'train\':\n            return len(self.train_data)\n        elif self.mode.lower() == \'val\':\n            return len(self.val_data)\n        elif self.mode.lower() == \'test\':\n            return len(self.test_data)\n        else:\n            raise RuntimeError(""Unexpected dataset mode. ""\n                               ""Supported modes are: train, val and test"")\n'"
data/utils.py,0,"b'import os\nfrom PIL import Image\nimport numpy as np\n\n\ndef get_files(folder, name_filter=None, extension_filter=None):\n    """"""Helper function that returns the list of files in a specified folder\n    with a specified extension.\n\n    Keyword arguments:\n    - folder (``string``): The path to a folder.\n    - name_filter (```string``, optional): The returned files must contain\n    this substring in their filename. Default: None; files are not filtered.\n    - extension_filter (``string``, optional): The desired file extension.\n    Default: None; files are not filtered\n\n    """"""\n    if not os.path.isdir(folder):\n        raise RuntimeError(""\\""{0}\\"" is not a folder."".format(folder))\n\n    # Filename filter: if not specified don\'t filter (condition always true);\n    # otherwise, use a lambda expression to filter out files that do not\n    # contain ""name_filter""\n    if name_filter is None:\n        # This looks hackish...there is probably a better way\n        name_cond = lambda filename: True\n    else:\n        name_cond = lambda filename: name_filter in filename\n\n    # Extension filter: if not specified don\'t filter (condition always true);\n    # otherwise, use a lambda expression to filter out files whose extension\n    # is not ""extension_filter""\n    if extension_filter is None:\n        # This looks hackish...there is probably a better way\n        ext_cond = lambda filename: True\n    else:\n        ext_cond = lambda filename: filename.endswith(extension_filter)\n\n    filtered_files = []\n\n    # Explore the directory tree to get files that contain ""name_filter"" and\n    # with extension ""extension_filter""\n    for path, _, files in os.walk(folder):\n        files.sort()\n        for file in files:\n            if name_cond(file) and ext_cond(file):\n                full_path = os.path.join(path, file)\n                filtered_files.append(full_path)\n\n    return filtered_files\n\n\ndef pil_loader(data_path, label_path):\n    """"""Loads a sample and label image given their path as PIL images.\n\n    Keyword arguments:\n    - data_path (``string``): The filepath to the image.\n    - label_path (``string``): The filepath to the ground-truth image.\n\n    Returns the image and the label as PIL images.\n\n    """"""\n    data = Image.open(data_path)\n    label = Image.open(label_path)\n\n    return data, label\n\n\ndef remap(image, old_values, new_values):\n    assert isinstance(image, Image.Image) or isinstance(\n        image, np.ndarray), ""image must be of type PIL.Image or numpy.ndarray""\n    assert type(new_values) is tuple, ""new_values must be of type tuple""\n    assert type(old_values) is tuple, ""old_values must be of type tuple""\n    assert len(new_values) == len(\n        old_values), ""new_values and old_values must have the same length""\n\n    # If image is a PIL.Image convert it to a numpy array\n    if isinstance(image, Image.Image):\n        image = np.array(image)\n\n    # Replace old values by the new ones\n    tmp = np.zeros_like(image)\n    for old, new in zip(old_values, new_values):\n        # Since tmp is already initialized as zeros we can skip new values\n        # equal to 0\n        if new != 0:\n            tmp[image == old] = new\n\n    return Image.fromarray(tmp)\n\n\ndef enet_weighing(dataloader, num_classes, c=1.02):\n    """"""Computes class weights as described in the ENet paper:\n\n        w_class = 1 / (ln(c + p_class)),\n\n    where c is usually 1.02 and p_class is the propensity score of that\n    class:\n\n        propensity_score = freq_class / total_pixels.\n\n    References: https://arxiv.org/abs/1606.02147\n\n    Keyword arguments:\n    - dataloader (``data.Dataloader``): A data loader to iterate over the\n    dataset.\n    - num_classes (``int``): The number of classes.\n    - c (``int``, optional): AN additional hyper-parameter which restricts\n    the interval of values for the weights. Default: 1.02.\n\n    """"""\n    class_count = 0\n    total = 0\n    for _, label in dataloader:\n        label = label.cpu().numpy()\n\n        # Flatten label\n        flat_label = label.flatten()\n\n        # Sum up the number of pixels of each class and the total pixel\n        # counts for each label\n        class_count += np.bincount(flat_label, minlength=num_classes)\n        total += flat_label.size\n\n    # Compute propensity score and then the weights for each class\n    propensity_score = class_count / total\n    class_weights = 1 / (np.log(c + propensity_score))\n\n    return class_weights\n\n\ndef median_freq_balancing(dataloader, num_classes):\n    """"""Computes class weights using median frequency balancing as described\n    in https://arxiv.org/abs/1411.4734:\n\n        w_class = median_freq / freq_class,\n\n    where freq_class is the number of pixels of a given class divided by\n    the total number of pixels in images where that class is present, and\n    median_freq is the median of freq_class.\n\n    Keyword arguments:\n    - dataloader (``data.Dataloader``): A data loader to iterate over the\n    dataset.\n    whose weights are going to be computed.\n    - num_classes (``int``): The number of classes\n\n    """"""\n    class_count = 0\n    total = 0\n    for _, label in dataloader:\n        label = label.cpu().numpy()\n\n        # Flatten label\n        flat_label = label.flatten()\n\n        # Sum up the class frequencies\n        bincount = np.bincount(flat_label, minlength=num_classes)\n\n        # Create of mask of classes that exist in the label\n        mask = bincount > 0\n        # Multiply the mask by the pixel count. The resulting array has\n        # one element for each class. The value is either 0 (if the class\n        # does not exist in the label) or equal to the pixel count (if\n        # the class exists in the label)\n        total += mask * flat_label.size\n\n        # Sum up the number of pixels found for each class\n        class_count += bincount\n\n    # Compute the frequency and its median\n    freq = class_count / total\n    med = np.median(freq)\n\n    return med / freq\n'"
metric/__init__.py,0,"b""from .confusionmatrix import ConfusionMatrix\nfrom .iou import IoU\nfrom .metric import Metric\n\n__all__ = ['ConfusionMatrix', 'IoU', 'Metric']\n"""
metric/confusionmatrix.py,2,"b'import numpy as np\nimport torch\nfrom metric import metric\n\n\nclass ConfusionMatrix(metric.Metric):\n    """"""Constructs a confusion matrix for a multi-class classification problems.\n\n    Does not support multi-label, multi-class problems.\n\n    Keyword arguments:\n    - num_classes (int): number of classes in the classification problem.\n    - normalized (boolean, optional): Determines whether or not the confusion\n    matrix is normalized or not. Default: False.\n\n    Modified from: https://github.com/pytorch/tnt/blob/master/torchnet/meter/confusionmeter.py\n    """"""\n\n    def __init__(self, num_classes, normalized=False):\n        super().__init__()\n\n        self.conf = np.ndarray((num_classes, num_classes), dtype=np.int32)\n        self.normalized = normalized\n        self.num_classes = num_classes\n        self.reset()\n\n    def reset(self):\n        self.conf.fill(0)\n\n    def add(self, predicted, target):\n        """"""Computes the confusion matrix\n\n        The shape of the confusion matrix is K x K, where K is the number\n        of classes.\n\n        Keyword arguments:\n        - predicted (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n        predicted scores obtained from the model for N examples and K classes,\n        or an N-tensor/array of integer values between 0 and K-1.\n        - target (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n        ground-truth classes for N examples and K classes, or an N-tensor/array\n        of integer values between 0 and K-1.\n\n        """"""\n        # If target and/or predicted are tensors, convert them to numpy arrays\n        if torch.is_tensor(predicted):\n            predicted = predicted.cpu().numpy()\n        if torch.is_tensor(target):\n            target = target.cpu().numpy()\n\n        assert predicted.shape[0] == target.shape[0], \\\n            \'number of targets and predicted outputs do not match\'\n\n        if np.ndim(predicted) != 1:\n            assert predicted.shape[1] == self.num_classes, \\\n                \'number of predictions does not match size of confusion matrix\'\n            predicted = np.argmax(predicted, 1)\n        else:\n            assert (predicted.max() < self.num_classes) and (predicted.min() >= 0), \\\n                \'predicted values are not between 0 and k-1\'\n\n        if np.ndim(target) != 1:\n            assert target.shape[1] == self.num_classes, \\\n                \'Onehot target does not match size of confusion matrix\'\n            assert (target >= 0).all() and (target <= 1).all(), \\\n                \'in one-hot encoding, target values should be 0 or 1\'\n            assert (target.sum(1) == 1).all(), \\\n                \'multi-label setting is not supported\'\n            target = np.argmax(target, 1)\n        else:\n            assert (target.max() < self.num_classes) and (target.min() >= 0), \\\n                \'target values are not between 0 and k-1\'\n\n        # hack for bincounting 2 arrays together\n        x = predicted + self.num_classes * target\n        bincount_2d = np.bincount(\n            x.astype(np.int32), minlength=self.num_classes**2)\n        assert bincount_2d.size == self.num_classes**2\n        conf = bincount_2d.reshape((self.num_classes, self.num_classes))\n\n        self.conf += conf\n\n    def value(self):\n        """"""\n        Returns:\n            Confustion matrix of K rows and K columns, where rows corresponds\n            to ground-truth targets and columns corresponds to predicted\n            targets.\n        """"""\n        if self.normalized:\n            conf = self.conf.astype(np.float32)\n            return conf / conf.sum(1).clip(min=1e-12)[:, None]\n        else:\n            return self.conf\n'"
metric/iou.py,0,"b'import torch\nimport numpy as np\nfrom metric import metric\nfrom metric.confusionmatrix import ConfusionMatrix\n\n\nclass IoU(metric.Metric):\n    """"""Computes the intersection over union (IoU) per class and corresponding\n    mean (mIoU).\n\n    Intersection over union (IoU) is a common evaluation metric for semantic\n    segmentation. The predictions are first accumulated in a confusion matrix\n    and the IoU is computed from it as follows:\n\n        IoU = true_positive / (true_positive + false_positive + false_negative).\n\n    Keyword arguments:\n    - num_classes (int): number of classes in the classification problem\n    - normalized (boolean, optional): Determines whether or not the confusion\n    matrix is normalized or not. Default: False.\n    - ignore_index (int or iterable, optional): Index of the classes to ignore\n    when computing the IoU. Can be an int, or any iterable of ints.\n    """"""\n\n    def __init__(self, num_classes, normalized=False, ignore_index=None):\n        super().__init__()\n        self.conf_metric = ConfusionMatrix(num_classes, normalized)\n\n        if ignore_index is None:\n            self.ignore_index = None\n        elif isinstance(ignore_index, int):\n            self.ignore_index = (ignore_index,)\n        else:\n            try:\n                self.ignore_index = tuple(ignore_index)\n            except TypeError:\n                raise ValueError(""\'ignore_index\' must be an int or iterable"")\n\n    def reset(self):\n        self.conf_metric.reset()\n\n    def add(self, predicted, target):\n        """"""Adds the predicted and target pair to the IoU metric.\n\n        Keyword arguments:\n        - predicted (Tensor): Can be a (N, K, H, W) tensor of\n        predicted scores obtained from the model for N examples and K classes,\n        or (N, H, W) tensor of integer values between 0 and K-1.\n        - target (Tensor): Can be a (N, K, H, W) tensor of\n        target scores for N examples and K classes, or (N, H, W) tensor of\n        integer values between 0 and K-1.\n\n        """"""\n        # Dimensions check\n        assert predicted.size(0) == target.size(0), \\\n            \'number of targets and predicted outputs do not match\'\n        assert predicted.dim() == 3 or predicted.dim() == 4, \\\n            ""predictions must be of dimension (N, H, W) or (N, K, H, W)""\n        assert target.dim() == 3 or target.dim() == 4, \\\n            ""targets must be of dimension (N, H, W) or (N, K, H, W)""\n\n        # If the tensor is in categorical format convert it to integer format\n        if predicted.dim() == 4:\n            _, predicted = predicted.max(1)\n        if target.dim() == 4:\n            _, target = target.max(1)\n\n        self.conf_metric.add(predicted.view(-1), target.view(-1))\n\n    def value(self):\n        """"""Computes the IoU and mean IoU.\n\n        The mean computation ignores NaN elements of the IoU array.\n\n        Returns:\n            Tuple: (IoU, mIoU). The first output is the per class IoU,\n            for K classes it\'s numpy.ndarray with K elements. The second output,\n            is the mean IoU.\n        """"""\n        conf_matrix = self.conf_metric.value()\n        if self.ignore_index is not None:\n            conf_matrix[:, self.ignore_index] = 0\n            conf_matrix[self.ignore_index, :] = 0\n        true_positive = np.diag(conf_matrix)\n        false_positive = np.sum(conf_matrix, 0) - true_positive\n        false_negative = np.sum(conf_matrix, 1) - true_positive\n\n        # Just in case we get a division by 0, ignore/hide the error\n        with np.errstate(divide=\'ignore\', invalid=\'ignore\'):\n            iou = true_positive / (true_positive + false_positive + false_negative)\n\n        return iou, np.nanmean(iou)\n'"
metric/metric.py,0,"b'class Metric(object):\n    """"""Base class for all metrics.\n\n    From: https://github.com/pytorch/tnt/blob/master/torchnet/meter/meter.py\n    """"""\n    def reset(self):\n        pass\n\n    def add(self):\n        pass\n\n    def value(self):\n        pass\n'"
models/enet.py,4,"b'import torch.nn as nn\nimport torch\n\n\nclass InitialBlock(nn.Module):\n    """"""The initial block is composed of two branches:\n    1. a main branch which performs a regular convolution with stride 2;\n    2. an extension branch which performs max-pooling.\n\n    Doing both operations in parallel and concatenating their results\n    allows for efficient downsampling and expansion. The main branch\n    outputs 13 feature maps while the extension branch outputs 3, for a\n    total of 16 feature maps after concatenation.\n\n    Keyword arguments:\n    - in_channels (int): the number of input channels.\n    - out_channels (int): the number output channels.\n    - kernel_size (int, optional): the kernel size of the filters used in\n    the convolution layer. Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the\n    input. Default: 0.\n    - bias (bool, optional): Adds a learnable bias to the output if\n    ``True``. Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        if relu:\n            activation = nn.ReLU\n        else:\n            activation = nn.PReLU\n\n        # Main branch - As stated above the number of output channels for this\n        # branch is the total minus 3, since the remaining channels come from\n        # the extension branch\n        self.main_branch = nn.Conv2d(\n            in_channels,\n            out_channels - 3,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            bias=bias)\n\n        # Extension branch\n        self.ext_branch = nn.MaxPool2d(3, stride=2, padding=1)\n\n        # Initialize batch normalization to be used after concatenation\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_activation = activation()\n\n    def forward(self, x):\n        main = self.main_branch(x)\n        ext = self.ext_branch(x)\n\n        # Concatenate branches\n        out = torch.cat((main, ext), 1)\n\n        # Apply batch normalization\n        out = self.batch_norm(out)\n\n        return self.out_activation(out)\n\n\nclass RegularBottleneck(nn.Module):\n    """"""Regular bottlenecks are the main building block of ENet.\n    Main branch:\n    1. Shortcut connection.\n\n    Extension branch:\n    1. 1x1 convolution which decreases the number of channels by\n    ``internal_ratio``, also called a projection;\n    2. regular, dilated or asymmetric convolution;\n    3. 1x1 convolution which increases the number of channels back to\n    ``channels``, also called an expansion;\n    4. dropout as a regularizer.\n\n    Keyword arguments:\n    - channels (int): the number of input and output channels.\n    - internal_ratio (int, optional): a scale factor applied to\n    ``channels`` used to compute the number of\n    channels after the projection. eg. given ``channels`` equal to 128 and\n    internal_ratio equal to 2 the number of channels after the projection\n    is 64. Default: 4.\n    - kernel_size (int, optional): the kernel size of the filters used in\n    the convolution layer described above in item 2 of the extension\n    branch. Default: 3.\n    - padding (int, optional): zero-padding added to both sides of the\n    input. Default: 0.\n    - dilation (int, optional): spacing between kernel elements for the\n    convolution described in item 2 of the extension branch. Default: 1.\n    asymmetric (bool, optional): flags if the convolution described in\n    item 2 of the extension branch is asymmetric or not. Default: False.\n    - dropout_prob (float, optional): probability of an element to be\n    zeroed. Default: 0 (no dropout).\n    - bias (bool, optional): Adds a learnable bias to the output if\n    ``True``. Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n\n    """"""\n\n    def __init__(self,\n                 channels,\n                 internal_ratio=4,\n                 kernel_size=3,\n                 padding=0,\n                 dilation=1,\n                 asymmetric=False,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > channels:\n            raise RuntimeError(""Value out of range. Expected value in the ""\n                               ""interval [1, {0}], got internal_scale={1}.""\n                               .format(channels, internal_ratio))\n\n        internal_channels = channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU\n        else:\n            activation = nn.PReLU\n\n        # Main branch - shortcut connection\n\n        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution, and,\n        # finally, a regularizer (spatial dropout). Number of channels is constant.\n\n        # 1x1 projection convolution\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                channels,\n                internal_channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n\n        # If the convolution is asymmetric we split the main convolution in\n        # two. Eg. for a 5x5 asymmetric convolution we have two convolution:\n        # the first is 5x1 and the second is 1x5.\n        if asymmetric:\n            self.ext_conv2 = nn.Sequential(\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=(kernel_size, 1),\n                    stride=1,\n                    padding=(padding, 0),\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation(),\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=(1, kernel_size),\n                    stride=1,\n                    padding=(0, padding),\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation())\n        else:\n            self.ext_conv2 = nn.Sequential(\n                nn.Conv2d(\n                    internal_channels,\n                    internal_channels,\n                    kernel_size=kernel_size,\n                    stride=1,\n                    padding=padding,\n                    dilation=dilation,\n                    bias=bias), nn.BatchNorm2d(internal_channels), activation())\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(channels), activation())\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after adding the branches\n        self.out_activation = activation()\n\n    def forward(self, x):\n        # Main branch shortcut\n        main = x\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_activation(out)\n\n\nclass DownsamplingBottleneck(nn.Module):\n    """"""Downsampling bottlenecks further downsample the feature map size.\n\n    Main branch:\n    1. max pooling with stride 2; indices are saved to be used for\n    unpooling later.\n\n    Extension branch:\n    1. 2x2 convolution with stride 2 that decreases the number of channels\n    by ``internal_ratio``, also called a projection;\n    2. regular convolution (by default, 3x3);\n    3. 1x1 convolution which increases the number of channels to\n    ``out_channels``, also called an expansion;\n    4. dropout as a regularizer.\n\n    Keyword arguments:\n    - in_channels (int): the number of input channels.\n    - out_channels (int): the number of output channels.\n    - internal_ratio (int, optional): a scale factor applied to ``channels``\n    used to compute the number of channels after the projection. eg. given\n    ``channels`` equal to 128 and internal_ratio equal to 2 the number of\n    channels after the projection is 64. Default: 4.\n    - return_indices (bool, optional):  if ``True``, will return the max\n    indices along with the outputs. Useful when unpooling later.\n    - dropout_prob (float, optional): probability of an element to be\n    zeroed. Default: 0 (no dropout).\n    - bias (bool, optional): Adds a learnable bias to the output if\n    ``True``. Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 internal_ratio=4,\n                 return_indices=False,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Store parameters that are needed later\n        self.return_indices = return_indices\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > in_channels:\n            raise RuntimeError(""Value out of range. Expected value in the ""\n                               ""interval [1, {0}], got internal_scale={1}. ""\n                               .format(in_channels, internal_ratio))\n\n        internal_channels = in_channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU\n        else:\n            activation = nn.PReLU\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_max1 = nn.MaxPool2d(\n            2,\n            stride=2,\n            return_indices=return_indices)\n\n        # Extension branch - 2x2 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 2x2 projection convolution with stride 2\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                internal_channels,\n                kernel_size=2,\n                stride=2,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n\n        # Convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                internal_channels,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=bias), nn.BatchNorm2d(internal_channels), activation())\n\n        # 1x1 expansion convolution\n        self.ext_conv3 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels,\n                out_channels,\n                kernel_size=1,\n                stride=1,\n                bias=bias), nn.BatchNorm2d(out_channels), activation())\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_activation = activation()\n\n    def forward(self, x):\n        # Main branch shortcut\n        if self.return_indices:\n            main, max_indices = self.main_max1(x)\n        else:\n            main = self.main_max1(x)\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_conv3(ext)\n        ext = self.ext_regul(ext)\n\n        # Main branch channel padding\n        n, ch_ext, h, w = ext.size()\n        ch_main = main.size()[1]\n        padding = torch.zeros(n, ch_ext - ch_main, h, w)\n\n        # Before concatenating, check if main is on the CPU or GPU and\n        # convert padding accordingly\n        if main.is_cuda:\n            padding = padding.cuda()\n\n        # Concatenate\n        main = torch.cat((main, padding), 1)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_activation(out), max_indices\n\n\nclass UpsamplingBottleneck(nn.Module):\n    """"""The upsampling bottlenecks upsample the feature map resolution using max\n    pooling indices stored from the corresponding downsampling bottleneck.\n\n    Main branch:\n    1. 1x1 convolution with stride 1 that decreases the number of channels by\n    ``internal_ratio``, also called a projection;\n    2. max unpool layer using the max pool indices from the corresponding\n    downsampling max pool layer.\n\n    Extension branch:\n    1. 1x1 convolution with stride 1 that decreases the number of channels by\n    ``internal_ratio``, also called a projection;\n    2. transposed convolution (by default, 3x3);\n    3. 1x1 convolution which increases the number of channels to\n    ``out_channels``, also called an expansion;\n    4. dropout as a regularizer.\n\n    Keyword arguments:\n    - in_channels (int): the number of input channels.\n    - out_channels (int): the number of output channels.\n    - internal_ratio (int, optional): a scale factor applied to ``in_channels``\n     used to compute the number of channels after the projection. eg. given\n     ``in_channels`` equal to 128 and ``internal_ratio`` equal to 2 the number\n     of channels after the projection is 64. Default: 4.\n    - dropout_prob (float, optional): probability of an element to be zeroed.\n    Default: 0 (no dropout).\n    - bias (bool, optional): Adds a learnable bias to the output if ``True``.\n    Default: False.\n    - relu (bool, optional): When ``True`` ReLU is used as the activation\n    function; otherwise, PReLU is used. Default: True.\n\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 internal_ratio=4,\n                 dropout_prob=0,\n                 bias=False,\n                 relu=True):\n        super().__init__()\n\n        # Check in the internal_scale parameter is within the expected range\n        # [1, channels]\n        if internal_ratio <= 1 or internal_ratio > in_channels:\n            raise RuntimeError(""Value out of range. Expected value in the ""\n                               ""interval [1, {0}], got internal_scale={1}. ""\n                               .format(in_channels, internal_ratio))\n\n        internal_channels = in_channels // internal_ratio\n\n        if relu:\n            activation = nn.ReLU\n        else:\n            activation = nn.PReLU\n\n        # Main branch - max pooling followed by feature map (channels) padding\n        self.main_conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels))\n\n        # Remember that the stride is the same as the kernel_size, just like\n        # the max pooling layers\n        self.main_unpool1 = nn.MaxUnpool2d(kernel_size=2)\n\n        # Extension branch - 1x1 convolution, followed by a regular, dilated or\n        # asymmetric convolution, followed by another 1x1 convolution. Number\n        # of channels is doubled.\n\n        # 1x1 projection convolution with stride 1\n        self.ext_conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels, internal_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(internal_channels), activation())\n\n        # Transposed convolution\n        self.ext_tconv1 = nn.ConvTranspose2d(\n            internal_channels,\n            internal_channels,\n            kernel_size=2,\n            stride=2,\n            bias=bias)\n        self.ext_tconv1_bnorm = nn.BatchNorm2d(internal_channels)\n        self.ext_tconv1_activation = activation()\n\n        # 1x1 expansion convolution\n        self.ext_conv2 = nn.Sequential(\n            nn.Conv2d(\n                internal_channels, out_channels, kernel_size=1, bias=bias),\n            nn.BatchNorm2d(out_channels), activation())\n\n        self.ext_regul = nn.Dropout2d(p=dropout_prob)\n\n        # PReLU layer to apply after concatenating the branches\n        self.out_activation = activation()\n\n    def forward(self, x, max_indices, output_size):\n        # Main branch shortcut\n        main = self.main_conv1(x)\n        main = self.main_unpool1(\n            main, max_indices, output_size=output_size)\n\n        # Extension branch\n        ext = self.ext_conv1(x)\n        ext = self.ext_tconv1(ext, output_size=output_size)\n        ext = self.ext_tconv1_bnorm(ext)\n        ext = self.ext_tconv1_activation(ext)\n        ext = self.ext_conv2(ext)\n        ext = self.ext_regul(ext)\n\n        # Add main and extension branches\n        out = main + ext\n\n        return self.out_activation(out)\n\n\nclass ENet(nn.Module):\n    """"""Generate the ENet model.\n\n    Keyword arguments:\n    - num_classes (int): the number of classes to segment.\n    - encoder_relu (bool, optional): When ``True`` ReLU is used as the\n    activation function in the encoder blocks/layers; otherwise, PReLU\n    is used. Default: False.\n    - decoder_relu (bool, optional): When ``True`` ReLU is used as the\n    activation function in the decoder blocks/layers; otherwise, PReLU\n    is used. Default: True.\n\n    """"""\n\n    def __init__(self, num_classes, encoder_relu=False, decoder_relu=True):\n        super().__init__()\n\n        self.initial_block = InitialBlock(3, 16, relu=encoder_relu)\n\n        # Stage 1 - Encoder\n        self.downsample1_0 = DownsamplingBottleneck(\n            16,\n            64,\n            return_indices=True,\n            dropout_prob=0.01,\n            relu=encoder_relu)\n        self.regular1_1 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_2 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_3 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n        self.regular1_4 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.01, relu=encoder_relu)\n\n        # Stage 2 - Encoder\n        self.downsample2_0 = DownsamplingBottleneck(\n            64,\n            128,\n            return_indices=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.regular2_1 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_2 = RegularBottleneck(\n            128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric2_3 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            padding=2,\n            asymmetric=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated2_4 = RegularBottleneck(\n            128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular2_5 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated2_6 = RegularBottleneck(\n            128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric2_7 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            asymmetric=True,\n            padding=2,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated2_8 = RegularBottleneck(\n            128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # Stage 3 - Encoder\n        self.regular3_0 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated3_1 = RegularBottleneck(\n            128, dilation=2, padding=2, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric3_2 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            padding=2,\n            asymmetric=True,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated3_3 = RegularBottleneck(\n            128, dilation=4, padding=4, dropout_prob=0.1, relu=encoder_relu)\n        self.regular3_4 = RegularBottleneck(\n            128, padding=1, dropout_prob=0.1, relu=encoder_relu)\n        self.dilated3_5 = RegularBottleneck(\n            128, dilation=8, padding=8, dropout_prob=0.1, relu=encoder_relu)\n        self.asymmetric3_6 = RegularBottleneck(\n            128,\n            kernel_size=5,\n            asymmetric=True,\n            padding=2,\n            dropout_prob=0.1,\n            relu=encoder_relu)\n        self.dilated3_7 = RegularBottleneck(\n            128, dilation=16, padding=16, dropout_prob=0.1, relu=encoder_relu)\n\n        # Stage 4 - Decoder\n        self.upsample4_0 = UpsamplingBottleneck(\n            128, 64, dropout_prob=0.1, relu=decoder_relu)\n        self.regular4_1 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.regular4_2 = RegularBottleneck(\n            64, padding=1, dropout_prob=0.1, relu=decoder_relu)\n\n        # Stage 5 - Decoder\n        self.upsample5_0 = UpsamplingBottleneck(\n            64, 16, dropout_prob=0.1, relu=decoder_relu)\n        self.regular5_1 = RegularBottleneck(\n            16, padding=1, dropout_prob=0.1, relu=decoder_relu)\n        self.transposed_conv = nn.ConvTranspose2d(\n            16,\n            num_classes,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            bias=False)\n\n    def forward(self, x):\n        # Initial block\n        input_size = x.size()\n        x = self.initial_block(x)\n\n        # Stage 1 - Encoder\n        stage1_input_size = x.size()\n        x, max_indices1_0 = self.downsample1_0(x)\n        x = self.regular1_1(x)\n        x = self.regular1_2(x)\n        x = self.regular1_3(x)\n        x = self.regular1_4(x)\n\n        # Stage 2 - Encoder\n        stage2_input_size = x.size()\n        x, max_indices2_0 = self.downsample2_0(x)\n        x = self.regular2_1(x)\n        x = self.dilated2_2(x)\n        x = self.asymmetric2_3(x)\n        x = self.dilated2_4(x)\n        x = self.regular2_5(x)\n        x = self.dilated2_6(x)\n        x = self.asymmetric2_7(x)\n        x = self.dilated2_8(x)\n\n        # Stage 3 - Encoder\n        x = self.regular3_0(x)\n        x = self.dilated3_1(x)\n        x = self.asymmetric3_2(x)\n        x = self.dilated3_3(x)\n        x = self.regular3_4(x)\n        x = self.dilated3_5(x)\n        x = self.asymmetric3_6(x)\n        x = self.dilated3_7(x)\n\n        # Stage 4 - Decoder\n        x = self.upsample4_0(x, max_indices2_0, output_size=stage2_input_size)\n        x = self.regular4_1(x)\n        x = self.regular4_2(x)\n\n        # Stage 5 - Decoder\n        x = self.upsample5_0(x, max_indices1_0, output_size=stage1_input_size)\n        x = self.regular5_1(x)\n        x = self.transposed_conv(x, output_size=input_size)\n\n        return x\n'"
