file_path,api_count,code
setup.py,0,"b'import setuptools\nimport sys\n\n\n# install requirements for mixed precision training\nif ""amp"" in sys.argv:\n    sys.argv.remove(""amp"")\n\n    from pip import __version__ as PIP_VERSION\n\n    PIP_MAJOR, PIP_MINOR = [int(v) for v in PIP_VERSION.split(""."")[:2]]\n\n    if PIP_MAJOR <= 9:\n        raise RuntimeError(\n            ""Current version of pip is not compatible with `apex`,""\n            ""you may need to install `apex` manually.""\n        )\n    elif 10 <= PIP_MAJOR <= 19 and PIP_MINOR < 3:\n        from pip._internal import main as pipmain\n    else:\n        from pip._internal.main import main as pipmain\n\n    pipmain(\n        [\n            ""install"",\n            ""git+https://github.com/NVIDIA/apex"",\n            ""-v"",\n            ""--no-cache-dir"",\n            ""--global-option=--cpp_ext"",\n            ""--global-option=--cuda_ext"",\n        ]\n    )\n\n\nwith open(""README.md"", ""r"") as f:\n    long_description = f.read()\n\nsetuptools.setup(\n    name=""torch-lr-finder"",\n    version=""0.1.5"",\n    author=""David Silva"",\n    author_email=""davidtvs10@gmail.com"",\n    description=""Pytorch implementation of the learning rate range test"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/davidtvs/pytorch-lr-finder"",\n    packages=setuptools.find_packages(exclude=[""examples"", ""images""]),\n    classifiers=[\n        ""Programming Language :: Python"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Science/Research"",\n        ""Intended Audience :: Education"",\n        ""Topic :: Software Development"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n    ],\n    python_requires="">=3.5.9"",\n    install_requires=[""matplotlib"", ""numpy"", ""torch>=0.4.1"", ""tqdm"", ""packaging""],\n    extras_require={\n        ""tests"": [""pytest"", ""pytest-cov"", ""pytest-mock""],\n        ""dev"": [\n            ""pytest"",\n            ""pytest-cov"",\n            ""pytest-mock"",\n            ""flake8"",\n            ""black"",\n            ""pep8-naming"",\n            ""torchvision"",\n            ""ipywidgets"",\n        ],\n    },\n)\n'"
examples/cifar10_resnet.py,1,"b'import torch.nn as nn\n\n\n__all__ = [""Cifar10ResNet"", ""resnet20"", ""resnet32"", ""resnet44"", ""resnet56"", ""resnet101""]\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(\n        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False\n    )\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Cifar10ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10, ch_width=2):\n        super(Cifar10ResNet, self).__init__()\n        width = [16, 16 * ch_width, 16 * ch_width * ch_width]\n        self.inplanes = 16\n        self.conv1 = nn.Conv2d(\n            3, width[0], kernel_size=3, stride=1, padding=1, bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(width[0])\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, width[0], layers[0])\n        self.layer2 = self._make_layer(block, width[1], layers[1], stride=2)\n        self.layer3 = self._make_layer(block, width[2], layers[2], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(width[2] * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=""fan_out"", nonlinearity=""relu"")\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet20(num_classes=10, ch_width=2):\n    """"""Constructs a ResNet-20 model.\n    """"""\n    return Cifar10ResNet(\n        BasicBlock, [3, 3, 3], num_classes=num_classes, ch_width=ch_width\n    )\n\n\ndef resnet32(num_classes=10, ch_width=2):\n    """"""Constructs a ResNet-32 model.\n    """"""\n    return Cifar10ResNet(\n        BasicBlock, [5, 5, 5], num_classes=num_classes, ch_width=ch_width\n    )\n\n\ndef resnet44(num_classes=10, ch_width=2):\n    """"""Constructs a ResNet-44 model.\n    """"""\n    return Cifar10ResNet(\n        BasicBlock, [7, 7, 7], num_classes=num_classes, ch_width=ch_width\n    )\n\n\ndef resnet56(num_classes=10, ch_width=2):\n    """"""Constructs a ResNet-56 model.\n    """"""\n    return Cifar10ResNet(\n        BasicBlock, [9, 9, 9], num_classes=num_classes, ch_width=ch_width\n    )\n\n\ndef resnet101(num_classes=10, ch_width=2):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    return Cifar10ResNet(\n        BasicBlock, [18, 18, 18], num_classes=num_classes, ch_width=ch_width\n    )\n'"
tests/conftest.py,3,"b'import pytest\nimport random\nimport os\nimport numpy as np\nimport torch\n\n\nclass CustomCommandLineOption(object):\n    """"""An object for storing command line options parsed by pytest.\n\n    Since `pytest.config` global object is deprecated and removed in version\n    5.0, this class is made to work as a store of command line options for\n    those components which are not able to access them via `request.config`.\n    """"""\n\n    def __init__(self):\n        self._content = {}\n\n    def __str__(self):\n        return str(self._content)\n\n    def add(self, key, value):\n        self._content.update({key: value})\n\n    def delete(self, key):\n        del self._content[key]\n\n    def __getattr__(self, key):\n        if key in self._content:\n            return self._content[key]\n        else:\n            return super(CustomCommandLineOption, self).__getattr__(key)\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        ""--cpu_only"", action=""store_true"", help=""Forcibly run all tests on CPU.""\n    )\n\n\ndef pytest_configure(config):\n    # Bind a config object to `pytest` module instance\n    pytest.custom_cmdopt = CustomCommandLineOption()\n    pytest.custom_cmdopt.add(""cpu_only"", config.getoption(""--cpu_only""))\n\n    # Set the random seed so that the tests are reproducible between test runs and\n    # hopefully torch and numpy versions. This seed should also allow all range tests\n    # with a starting lr of 1e-5 and an ending lr of 1e-1 to run the full test without\n    # diverging\n    seed = 1\n    random.seed(seed)\n    os.environ[""PYTHONHASHSEED""] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n'"
tests/dataset.py,4,"b'import numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass XORDataset(Dataset):\n    def __init__(self, length, shape=None):\n        """"""\n        Arguments:\n            length (int): length of dataset, which equals `len(self)`.\n            shape (list, tuple, optional): shape of dataset. If it isn\'t\n                specified, it will be initialized to `(length, 8)`.\n                Default: None.\n        """"""\n        _shape = (length,) + tuple(shape) if shape else (length, 8)\n        raw = np.random.randint(0, 2, _shape)\n        self.data = torch.FloatTensor(raw)\n\n        label = np.bitwise_xor.reduce(raw, axis=1)\n        self.label = torch.tensor(label, dtype=torch.float32).unsqueeze(dim=1)\n\n    def __getitem__(self, index):\n        return self.data[index], self.label[index]\n\n    def __len__(self):\n        return len(self.data)\n\n\nclass ExtraXORDataset(XORDataset):\n    """""" A XOR dataset which is able to return extra values. """"""\n\n    def __init__(self, length, shape=None, extra_dims=1):\n        """"""\n        Arguments:\n            length (int): length of dataset, which equals `len(self)`.\n            shape (list, tuple, optional): shape of dataset. If it isn\'t\n                specified, it will be initialized to `(length, 8)`.\n                Default: None.\n            extra_dims (int, optional): dimension of extra values.\n                Default: 1.\n        """"""\n        super(ExtraXORDataset, self).__init__(length, shape=shape)\n        if extra_dims:\n            _extra_shape = (length, extra_dims)\n            self.extras = torch.randint(0, 2, _extra_shape)\n        else:\n            self.extras = None\n\n    def __getitem__(self, index):\n        if self.extras is not None:\n            retval = [self.data[index], self.label[index]]\n            retval.extend([v for v in self.extras[index]])\n            return retval\n        else:\n            return self.data[index], self.label[index]\n\n    def __len__(self):\n        return len(self.data)\n'"
tests/model.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\nclass LinearMLP(nn.Module):\n    def __init__(self, layer_dim):\n        super(LinearMLP, self).__init__()\n        io_pairs = zip(layer_dim[:-1], layer_dim[1:])\n        layers = [nn.Linear(idim, odim) for idim, odim in io_pairs]\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n'"
tests/task.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nimport pytest\n\nfrom model import LinearMLP\nfrom dataset import XORDataset, ExtraXORDataset\n\n\ndef use_cuda():\n    if pytest.custom_cmdopt.cpu_only:\n        return False\n    else:\n        return torch.cuda.is_available()\n\n\nclass TaskTemplate(type):\n    def __call__(cls, *args, **kwargs):\n        obj = type.__call__(cls, *args, **kwargs)\n        if hasattr(obj, ""__post_init__""):\n            obj.__post_init__()\n        return obj\n\n\nclass BaseTask(metaclass=TaskTemplate):\n    def __init__(self):\n        self.batch_size = -1\n        self.model = None\n        self.optimizer = None\n        self.criterion = None\n        self.device = None\n        self.train_loader = None\n        self.val_loader = None\n\n    def __post_init__(self):\n        # Check whether cuda is available or not, and we will cast `self.device`\n        # to `torch.device` here to make sure operations related to moving tensor\n        # would work fine later.\n        if not use_cuda():\n            self.device = None\n        if self.device is None:\n            return\n\n        if isinstance(self.device, str):\n            self.device = torch.device(self.device)\n        elif not isinstance(self.device, torch.device):\n            raise TypeError(""Invalid type of device."")\n\n        self.model.to(self.device)\n\n\nclass XORTask(BaseTask):\n    def __init__(self, batch_size=8, steps=100, validate=False):\n        super(XORTask, self).__init__()\n        n_total = batch_size * steps\n        dataset = XORDataset(n_total)\n        if validate:\n            n_train = int(n_total * 0.9)\n            self.train_loader = DataLoader(\n                Subset(dataset, range(n_train)),\n                batch_size=batch_size\n            )\n            self.val_loader = DataLoader(\n                Subset(dataset, range(n_train, n_total)),\n                batch_size=batch_size\n            )\n        else:\n            self.train_loader = DataLoader(dataset, batch_size=batch_size)\n            self.val_loader = None\n\n        self.batch_size = batch_size\n        self.model = LinearMLP([8, 4, 1])\n        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5)\n        self.criterion = nn.MSELoss()\n        self.device = torch.device(""cuda"")\n\n\nclass ExtraXORTask(BaseTask):\n    def __init__(self, batch_size=8, steps=100, validate=False):\n        super(ExtraXORTask, self).__init__()\n        n_total = batch_size * steps\n        dataset = ExtraXORDataset(n_total, extra_dims=2)\n        if validate:\n            n_train = int(n_total * 0.9)\n            self.train_loader = DataLoader(\n                Subset(dataset, range(n_train)),\n                batch_size=batch_size\n            )\n            self.val_loader = DataLoader(\n                Subset(dataset, range(n_train, n_total)),\n                batch_size=batch_size\n            )\n        else:\n            self.train_loader = DataLoader(dataset, batch_size=batch_size)\n            self.val_loader = None\n\n        self.batch_size = batch_size\n        self.model = LinearMLP([8, 4, 1])\n        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5)\n        self.criterion = nn.MSELoss()\n        self.device = torch.device(""cuda"")\n\n\nclass DiscriminativeLearningRateTask(BaseTask):\n    def __init__(self, batch_size=8, steps=100, validate=False):\n        super(DiscriminativeLearningRateTask, self).__init__()\n        n_total = batch_size * steps\n        dataset = XORDataset(n_total)\n        if validate:\n            n_train = int(n_total * 0.9)\n            self.train_loader = DataLoader(\n                Subset(dataset, range(n_train)),\n                batch_size=batch_size\n            )\n            self.val_loader = DataLoader(\n                Subset(dataset, range(n_train, n_total)),\n                batch_size=batch_size\n            )\n        else:\n            self.train_loader = DataLoader(dataset, batch_size=batch_size)\n            self.val_loader = None\n\n        self.batch_size = batch_size\n        self.model = LinearMLP([8, 4, 1])\n        self.optimizer = optim.SGD(\n            [\n                {""params"": self.model.net[0].parameters(), ""lr"": 1e-3},\n                {""params"": self.model.net[1].parameters(), ""lr"": 1e-5},\n            ],\n            lr=1e-5,\n            momentum=0.5,\n        )\n        self.criterion = nn.MSELoss()\n        self.device = torch.device(""cuda"")\n'"
tests/test_lr_finder.py,0,"b'import pytest\nfrom torch_lr_finder import LRFinder\n\nimport task as mod_task\n\n\ntry:\n    from apex import amp\n\n    IS_AMP_AVAILABLE = True\nexcept ImportError:\n    IS_AMP_AVAILABLE = False\n\n\ndef collect_task_classes():\n    names = [v for v in dir(mod_task) if v.endswith(""Task"") and v != ""BaseTask""]\n    attrs = [getattr(mod_task, v) for v in names]\n    classes = [v for v in attrs if issubclass(v, mod_task.BaseTask)]\n    return classes\n\n\ndef prepare_lr_finder(task, **kwargs):\n    model = task.model\n    optimizer = task.optimizer\n    criterion = task.criterion\n    config = {\n        ""device"": kwargs.get(""device"", None),\n        ""memory_cache"": kwargs.get(""memory_cache"", True),\n        ""cache_dir"": kwargs.get(""cache_dir"", None),\n    }\n    lr_finder = LRFinder(model, optimizer, criterion, **config)\n    return lr_finder\n\n\ndef get_optim_lr(optimizer):\n    return [grp[""lr""] for grp in optimizer.param_groups]\n\n\nclass TestRangeTest:\n    @pytest.mark.parametrize(""cls_task"", collect_task_classes())\n    def test_run(self, cls_task):\n        task = cls_task()\n        init_lrs = get_optim_lr(task.optimizer)\n\n        lr_finder = prepare_lr_finder(task)\n        lr_finder.range_test(task.train_loader, end_lr=0.1)\n\n        # check whether lr is actually changed\n        assert max(lr_finder.history[""lr""]) >= init_lrs[0]\n\n    @pytest.mark.parametrize(""cls_task"", collect_task_classes())\n    def test_run_with_val_loader(self, cls_task):\n        task = cls_task(validate=True)\n        init_lrs = get_optim_lr(task.optimizer)\n\n        lr_finder = prepare_lr_finder(task)\n        lr_finder.range_test(task.train_loader, val_loader=task.val_loader, end_lr=0.1)\n\n        # check whether lr is actually changed\n        assert max(lr_finder.history[""lr""]) >= init_lrs[0]\n\n\nclass TestReset:\n    @pytest.mark.parametrize(\n        ""cls_task"", [mod_task.XORTask, mod_task.DiscriminativeLearningRateTask],\n    )\n    def test_reset(self, cls_task):\n        task = cls_task()\n        init_lrs = get_optim_lr(task.optimizer)\n\n        lr_finder = prepare_lr_finder(task)\n        lr_finder.range_test(task.train_loader, val_loader=task.val_loader, end_lr=0.1)\n        lr_finder.reset()\n\n        restored_lrs = get_optim_lr(task.optimizer)\n        assert init_lrs == restored_lrs\n\n\nclass TestLRHistory:\n    def test_linear_lr_history(self):\n        task = mod_task.XORTask()\n        # prepare_lr_finder sets the starting lr to 1e-5\n        lr_finder = prepare_lr_finder(task)\n        lr_finder.range_test(\n            task.train_loader, num_iter=5, step_mode=""linear"", end_lr=5e-5\n        )\n\n        assert len(lr_finder.history[""lr""]) == 5\n        assert lr_finder.history[""lr""] == pytest.approx([1e-5, 2e-5, 3e-5, 4e-5, 5e-5])\n\n    def test_exponential_lr_history(self):\n        task = mod_task.XORTask()\n        # prepare_lr_finder sets the starting lr to 1e-5\n        lr_finder = prepare_lr_finder(task)\n        lr_finder.range_test(task.train_loader, num_iter=5, step_mode=""exp"", end_lr=0.1)\n\n        assert len(lr_finder.history[""lr""]) == 5\n        assert lr_finder.history[""lr""] == pytest.approx([1e-5, 1e-4, 1e-3, 1e-2, 0.1])\n\n\nclass TestGradientAccumulation:\n    def test_gradient_accumulation(self, mocker):\n        desired_bs, accum_steps = 32, 4\n        real_bs = desired_bs // accum_steps\n        num_iter = 10\n        task = mod_task.XORTask(batch_size=real_bs)\n\n        lr_finder = prepare_lr_finder(task)\n        spy = mocker.spy(lr_finder, ""criterion"")\n\n        lr_finder.range_test(\n            task.train_loader, num_iter=num_iter, accumulation_steps=accum_steps\n        )\n        # NOTE: We are using smaller batch size to simulate a large batch.\n        # So that the actual times of model/criterion called should be\n        # `(desired_bs/real_bs) * num_iter` == `accum_steps * num_iter`\n        assert spy.call_count == accum_steps * num_iter\n\n    @pytest.mark.skipif(\n        not (IS_AMP_AVAILABLE and mod_task.use_cuda()),\n        reason=""`apex` module and gpu is required to run this test.""\n    )\n    def test_gradient_accumulation_with_apex_amp(self, mocker):\n        desired_bs, accum_steps = 32, 4\n        real_bs = desired_bs // accum_steps\n        num_iter = 10\n        task = mod_task.XORTask(batch_size=real_bs)\n\n        # Wrap model and optimizer by `amp.initialize`. Beside, `amp` requires\n        # CUDA GPU. So we have to move model to GPU first.\n        model, optimizer, device = task.model, task.optimizer, task.device\n        model = model.to(device)\n        task.model, task.optimizer = amp.initialize(model, optimizer)\n\n        lr_finder = prepare_lr_finder(task)\n        spy = mocker.spy(amp, ""scale_loss"")\n\n        lr_finder.range_test(\n            task.train_loader, num_iter=num_iter, accumulation_steps=accum_steps\n        )\n        assert spy.call_count == accum_steps * num_iter\n\n\n@pytest.mark.skipif(\n    not (IS_AMP_AVAILABLE and mod_task.use_cuda()),\n    reason=""`apex` module and gpu is required to run these tests.""\n)\nclass TestMixedPrecision:\n    def test_mixed_precision(self, mocker):\n        batch_size = 32\n        num_iter = 10\n        task = mod_task.XORTask(batch_size=batch_size)\n\n        # Wrap model and optimizer by `amp.initialize`. Beside, `amp` requires\n        # CUDA GPU. So we have to move model to GPU first.\n        model, optimizer, device = task.model, task.optimizer, task.device\n        model = model.to(device)\n        task.model, task.optimizer = amp.initialize(model, optimizer)\n        assert hasattr(task.optimizer, ""_amp_stash"")\n\n        lr_finder = prepare_lr_finder(task)\n        spy = mocker.spy(amp, ""scale_loss"")\n\n        lr_finder.range_test(task.train_loader, num_iter=num_iter)\n        # NOTE: Here we did not perform gradient accumulation, so that call count\n        # of `amp.scale_loss` should equal to `num_iter`.\n        assert spy.call_count == num_iter\n\n\n@pytest.mark.parametrize(""num_iter"", [0, 1])\n@pytest.mark.parametrize(""scheduler"", [""exp"", ""linear""])\ndef test_scheduler_and_num_iter(num_iter, scheduler):\n    task = mod_task.XORTask()\n    # prepare_lr_finder sets the starting lr to 1e-5\n    lr_finder = prepare_lr_finder(task)\n    with pytest.raises(ValueError, match=""num_iter""):\n        lr_finder.range_test(\n            task.train_loader, num_iter=num_iter, step_mode=scheduler, end_lr=5e-5\n        )\n'"
torch_lr_finder/__init__.py,0,b'from torch_lr_finder.lr_finder import LRFinder\nfrom torch_lr_finder.lr_finder import TrainDataLoaderIter\nfrom torch_lr_finder.lr_finder import ValDataLoaderIter\n'
torch_lr_finder/lr_finder.py,17,"b'import copy\nimport os\nimport torch\nfrom tqdm.autonotebook import tqdm\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\n\nfrom packaging import version\n\nPYTORCH_VERSION = version.parse(torch.__version__)\n\ntry:\n    from apex import amp\n\n    IS_AMP_AVAILABLE = True\nexcept ImportError:\n    IS_AMP_AVAILABLE = False\n\n\nclass DataLoaderIter(object):\n    def __init__(self, data_loader):\n        self.data_loader = data_loader\n        self._iterator = iter(data_loader)\n\n    @property\n    def dataset(self):\n        return self.data_loader.dataset\n\n    def inputs_labels_from_batch(self, batch_data):\n        if not isinstance(batch_data, list) and not isinstance(batch_data, tuple):\n            raise ValueError(\n                ""Your batch type not supported: {}. Please inherit from ""\n                ""`TrainDataLoaderIter` (or `ValDataLoaderIter`) and redefine ""\n                ""`_batch_make_inputs_labels` method."".format(type(batch_data))\n            )\n\n        inputs, labels, *_ = batch_data\n\n        return inputs, labels\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        batch = next(self._iterator)\n        return self.inputs_labels_from_batch(batch)\n\n\nclass TrainDataLoaderIter(DataLoaderIter):\n    def __init__(self, data_loader, auto_reset=True):\n        super().__init__(data_loader)\n        self.auto_reset = auto_reset\n\n    def __next__(self):\n        try:\n            batch = next(self._iterator)\n            inputs, labels = self.inputs_labels_from_batch(batch)\n        except StopIteration:\n            if not self.auto_reset:\n                raise\n            self._iterator = iter(self.data_loader)\n            batch = next(self._iterator)\n            inputs, labels = self.inputs_labels_from_batch(batch)\n\n        return inputs, labels\n\n\nclass ValDataLoaderIter(DataLoaderIter):\n    pass\n\n\nclass LRFinder(object):\n    """"""Learning rate range test.\n\n    The learning rate range test increases the learning rate in a pre-training run\n    between two boundaries in a linear or exponential manner. It provides valuable\n    information on how well the network can be trained over a range of learning rates\n    and what is the optimal learning rate.\n\n    Arguments:\n        model (torch.nn.Module): wrapped model.\n        optimizer (torch.optim.Optimizer): wrapped optimizer where the defined learning\n            is assumed to be the lower boundary of the range test.\n        criterion (torch.nn.Module): wrapped loss function.\n        device (str or torch.device, optional): a string (""cpu"" or ""cuda"") with an\n            optional ordinal for the device type (e.g. ""cuda:X"", where is the ordinal).\n            Alternatively, can be an object representing the device on which the\n            computation will take place. Default: None, uses the same device as `model`.\n        memory_cache (boolean, optional): if this flag is set to True, `state_dict` of\n            model and optimizer will be cached in memory. Otherwise, they will be saved\n            to files under the `cache_dir`.\n        cache_dir (string, optional): path for storing temporary files. If no path is\n            specified, system-wide temporary directory is used. Notice that this\n            parameter will be ignored if `memory_cache` is True.\n\n    Example:\n        >>> lr_finder = LRFinder(net, optimizer, criterion, device=""cuda"")\n        >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n        >>> lr_finder.plot() # to inspect the loss-learning rate graph\n        >>> lr_finder.reset() # to reset the model and optimizer to their initial state\n\n    Reference:\n    Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n    fastai/lr_find: https://github.com/fastai/fastai\n    """"""\n\n    def __init__(\n        self,\n        model,\n        optimizer,\n        criterion,\n        device=None,\n        memory_cache=True,\n        cache_dir=None,\n    ):\n        # Check if the optimizer is already attached to a scheduler\n        self.optimizer = optimizer\n        self._check_for_scheduler()\n\n        self.model = model\n        self.criterion = criterion\n        self.history = {""lr"": [], ""loss"": []}\n        self.best_loss = None\n        self.memory_cache = memory_cache\n        self.cache_dir = cache_dir\n\n        # Save the original state of the model and optimizer so they can be restored if\n        # needed\n        self.model_device = next(self.model.parameters()).device\n        self.state_cacher = StateCacher(memory_cache, cache_dir=cache_dir)\n        self.state_cacher.store(""model"", self.model.state_dict())\n        self.state_cacher.store(""optimizer"", self.optimizer.state_dict())\n\n        # If device is None, use the same as the model\n        if device:\n            self.device = device\n        else:\n            self.device = self.model_device\n\n    def reset(self):\n        """"""Restores the model and optimizer to their initial states.""""""\n\n        self.model.load_state_dict(self.state_cacher.retrieve(""model""))\n        self.optimizer.load_state_dict(self.state_cacher.retrieve(""optimizer""))\n        self.model.to(self.model_device)\n\n    def range_test(\n        self,\n        train_loader,\n        val_loader=None,\n        start_lr=None,\n        end_lr=10,\n        num_iter=100,\n        step_mode=""exp"",\n        smooth_f=0.05,\n        diverge_th=5,\n        accumulation_steps=1,\n        non_blocking_transfer=True,\n    ):\n        """"""Performs the learning rate range test.\n\n        Arguments:\n            train_loader (`torch.utils.data.DataLoader`\n                or child of `TrainDataLoaderIter`, optional):\n                the training set data loader.\n                If your dataset (data loader) returns a tuple (inputs, labels,*) then\n                Pytorch data loader object can be provided. However, if a dataset\n                returns different outputs e.g. dicts, then you should inherit\n                from `TrainDataLoaderIter` class and redefine `inputs_labels_from_batch`\n                method so that it outputs (inputs, labels).\n            val_loader (`torch.utils.data.DataLoader`\n                or child of `ValDataLoaderIter`, optional): if `None` the range test\n                will only use the training loss. When given a data loader, the model is\n                evaluated after each iteration on that dataset and the evaluation loss\n                is used. Note that in this mode the test takes significantly longer but\n                generally produces more precise results.\n                Similarly to `train_loader`, if your dataset outputs are not standard\n                you should inherit from `ValDataLoaderIter` class and\n                redefine method `inputs_labels_from_batch` so that\n                it outputs (inputs, labels). Default: None.\n            start_lr (float, optional): the starting learning rate for the range test.\n                Default: None (uses the learning rate from the optimizer).\n            end_lr (float, optional): the maximum learning rate to test. Default: 10.\n            num_iter (int, optional): the number of iterations over which the test\n                occurs. Default: 100.\n            step_mode (str, optional): one of the available learning rate policies,\n                linear or exponential (""linear"", ""exp""). Default: ""exp"".\n            smooth_f (float, optional): the loss smoothing factor within the [0, 1[\n                interval. Disabled if set to 0, otherwise the loss is smoothed using\n                exponential smoothing. Default: 0.05.\n            diverge_th (int, optional): the test is stopped when the loss surpasses the\n                threshold:  diverge_th * best_loss. Default: 5.\n            accumulation_steps (int, optional): steps for gradient accumulation. If it\n                is 1, gradients are not accumulated. Default: 1.\n            non_blocking_transfer (bool, optional): when non_blocking_transfer is set,\n                tries to convert/move data to the device asynchronously if possible,\n                e.g., moving CPU Tensors with pinned memory to CUDA devices. Default: True.\n\n        Example (fastai approach):\n            >>> lr_finder = LRFinder(net, optimizer, criterion, device=""cuda"")\n            >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n\n        Example (Leslie Smith\'s approach):\n            >>> lr_finder = LRFinder(net, optimizer, criterion, device=""cuda"")\n            >>> lr_finder.range_test(trainloader, val_loader=val_loader, end_lr=1, num_iter=100, step_mode=""linear"")\n\n        Gradient accumulation is supported; example:\n            >>> train_data = ...    # prepared dataset\n            >>> desired_bs, real_bs = 32, 4         # batch size\n            >>> accumulation_steps = desired_bs // real_bs     # required steps for accumulation\n            >>> dataloader = torch.utils.data.DataLoader(train_data, batch_size=real_bs, shuffle=True)\n            >>> acc_lr_finder = LRFinder(net, optimizer, criterion, device=""cuda"")\n            >>> acc_lr_finder.range_test(dataloader, end_lr=10, num_iter=100, accumulation_steps=accumulation_steps)\n\n        If your DataLoader returns e.g. dict, or other non standard output, intehit from TrainDataLoaderIter,\n        redefine method `inputs_labels_from_batch` so that it outputs (inputs, lables) data:\n            >>> import torch_lr_finder\n            >>> class TrainIter(torch_lr_finder.TrainDataLoaderIter):\n            >>>     def inputs_labels_from_batch(self, batch_data):\n            >>>         return (batch_data[\'user_features\'], batch_data[\'user_history\']), batch_data[\'y_labels\']\n            >>> train_data_iter = TrainIter(train_dl)\n            >>> finder = torch_lr_finder.LRFinder(model, optimizer, partial(model._train_loss, need_one_hot=False))\n            >>> finder.range_test(train_data_iter, end_lr=10, num_iter=300, diverge_th=10)\n\n        Reference:\n        [Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](\n        https://medium.com/huggingface/ec88c3e51255)\n        [thomwolf/gradient_accumulation](https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3)\n        """"""\n\n        # Reset test results\n        self.history = {""lr"": [], ""loss"": []}\n        self.best_loss = None\n\n        # Move the model to the proper device\n        self.model.to(self.device)\n\n        # Check if the optimizer is already attached to a scheduler\n        self._check_for_scheduler()\n\n        # Set the starting learning rate\n        if start_lr:\n            self._set_learning_rate(start_lr)\n\n        # Initialize the proper learning rate policy\n        if step_mode.lower() == ""exp"":\n            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n        elif step_mode.lower() == ""linear"":\n            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n        else:\n            raise ValueError(""expected one of (exp, linear), got {}"".format(step_mode))\n\n        if smooth_f < 0 or smooth_f >= 1:\n            raise ValueError(""smooth_f is outside the range [0, 1["")\n\n        # Create an iterator to get data batch by batch\n        if isinstance(train_loader, DataLoader):\n            train_iter = TrainDataLoaderIter(train_loader)\n        elif isinstance(train_loader, TrainDataLoaderIter):\n            train_iter = train_loader\n        else:\n            raise ValueError(\n                ""`train_loader` has unsupported type: {}.""\n                ""Expected types are `torch.utils.data.DataLoader`""\n                ""or child of `TrainDataLoaderIter`."".format(type(train_loader))\n            )\n\n        if val_loader:\n            if isinstance(val_loader, DataLoader):\n                val_iter = ValDataLoaderIter(val_loader)\n            elif isinstance(val_loader, ValDataLoaderIter):\n                val_iter = val_loader\n            else:\n                raise ValueError(\n                    ""`val_loader` has unsupported type: {}.""\n                    ""Expected types are `torch.utils.data.DataLoader`""\n                    ""or child of `ValDataLoaderIter`."".format(type(val_loader))\n                )\n\n        for iteration in tqdm(range(num_iter)):\n            # Train on batch and retrieve loss\n            loss = self._train_batch(\n                train_iter,\n                accumulation_steps,\n                non_blocking_transfer=non_blocking_transfer,\n            )\n            if val_loader:\n                loss = self._validate(\n                    val_iter, non_blocking_transfer=non_blocking_transfer\n                )\n\n            # Update the learning rate\n            self.history[""lr""].append(lr_schedule.get_lr()[0])\n            lr_schedule.step()\n\n            # Track the best loss and smooth it if smooth_f is specified\n            if iteration == 0:\n                self.best_loss = loss\n            else:\n                if smooth_f > 0:\n                    loss = smooth_f * loss + (1 - smooth_f) * self.history[""loss""][-1]\n                if loss < self.best_loss:\n                    self.best_loss = loss\n\n            # Check if the loss has diverged; if it has, stop the test\n            self.history[""loss""].append(loss)\n            if loss > diverge_th * self.best_loss:\n                print(""Stopping early, the loss has diverged"")\n                break\n\n        print(""Learning rate search finished. See the graph with {finder_name}.plot()"")\n\n    def _set_learning_rate(self, new_lrs):\n        if not isinstance(new_lrs, list):\n            new_lrs = [new_lrs] * len(self.optimizer.param_groups)\n        if len(new_lrs) != len(self.optimizer.param_groups):\n            raise ValueError(\n                ""Length of `new_lrs` is not equal to the number of parameter groups ""\n                + ""in the given optimizer""\n            )\n\n        for param_group, new_lr in zip(self.optimizer.param_groups, new_lrs):\n            param_group[""lr""] = new_lr\n\n    def _check_for_scheduler(self):\n        for param_group in self.optimizer.param_groups:\n            if ""initial_lr"" in param_group:\n                raise RuntimeError(""Optimizer already has a scheduler attached to it"")\n\n    def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n        self.model.train()\n        total_loss = None  # for late initialization\n\n        self.optimizer.zero_grad()\n        for i in range(accumulation_steps):\n            inputs, labels = next(train_iter)\n            inputs, labels = self._move_to_device(\n                inputs, labels, non_blocking=non_blocking_transfer\n            )\n\n            # Forward pass\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, labels)\n\n            # Loss should be averaged in each step\n            loss /= accumulation_steps\n\n            # Backward pass\n            if IS_AMP_AVAILABLE and hasattr(self.optimizer, ""_amp_stash""):\n                # For minor performance optimization, see also:\n                # https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-iterations\n                delay_unscale = ((i + 1) % accumulation_steps) != 0\n\n                with amp.scale_loss(\n                    loss, self.optimizer, delay_unscale=delay_unscale\n                ) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            if total_loss is None:\n                total_loss = loss\n            else:\n                total_loss += loss\n\n        self.optimizer.step()\n\n        return total_loss.item()\n\n    def _move_to_device(self, inputs, labels, non_blocking=True):\n        def move(obj, device, non_blocking=True):\n            if hasattr(obj, ""to""):\n                return obj.to(device, non_blocking=non_blocking)\n            elif isinstance(obj, tuple):\n                return tuple(move(o, device, non_blocking) for o in obj)\n            elif isinstance(obj, list):\n                return [move(o, device, non_blocking) for o in obj]\n            elif isinstance(obj, dict):\n                return {k: move(o, device, non_blocking) for k, o in obj.items()}\n            else:\n                return obj\n\n        inputs = move(inputs, self.device, non_blocking=non_blocking)\n        labels = move(labels, self.device, non_blocking=non_blocking)\n        return inputs, labels\n\n    def _validate(self, val_iter, non_blocking_transfer=True):\n        # Set model to evaluation mode and disable gradient computation\n        running_loss = 0\n        self.model.eval()\n        with torch.no_grad():\n            for inputs, labels in val_iter:\n                # Move data to the correct device\n                inputs, labels = self._move_to_device(\n                    inputs, labels, non_blocking=non_blocking_transfer\n                )\n\n                if isinstance(inputs, tuple) or isinstance(inputs, list):\n                    batch_size = inputs[0].size(0)\n                else:\n                    batch_size = inputs.size(0)\n\n                # Forward pass and loss computation\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels)\n                running_loss += loss.item() * batch_size\n\n        return running_loss / len(val_iter.dataset)\n\n    def plot(self, skip_start=10, skip_end=5, log_lr=True, show_lr=None, ax=None):\n        """"""Plots the learning rate range test.\n\n        Arguments:\n            skip_start (int, optional): number of batches to trim from the start.\n                Default: 10.\n            skip_end (int, optional): number of batches to trim from the start.\n                Default: 5.\n            log_lr (bool, optional): True to plot the learning rate in a logarithmic\n                scale; otherwise, plotted in a linear scale. Default: True.\n            show_lr (float, optional): if set, adds a vertical line to visualize the\n                specified learning rate. Default: None.\n            ax (matplotlib.axes.Axes, optional): the plot is created in the specified\n                matplotlib axes object and the figure is not be shown. If `None`, then\n                the figure and axes object are created in this method and the figure is\n                shown . Default: None.\n\n        Returns:\n            The matplotlib.axes.Axes object that contains the plot.\n        """"""\n\n        if skip_start < 0:\n            raise ValueError(""skip_start cannot be negative"")\n        if skip_end < 0:\n            raise ValueError(""skip_end cannot be negative"")\n        if show_lr is not None and not isinstance(show_lr, float):\n            raise ValueError(""show_lr must be float"")\n\n        # Get the data to plot from the history dictionary. Also, handle skip_end=0\n        # properly so the behaviour is the expected\n        lrs = self.history[""lr""]\n        losses = self.history[""loss""]\n        if skip_end == 0:\n            lrs = lrs[skip_start:]\n            losses = losses[skip_start:]\n        else:\n            lrs = lrs[skip_start:-skip_end]\n            losses = losses[skip_start:-skip_end]\n\n        # Create the figure and axes object if axes was not already given\n        fig = None\n        if ax is None:\n            fig, ax = plt.subplots()\n\n        # Plot loss as a function of the learning rate\n        ax.plot(lrs, losses)\n        if log_lr:\n            ax.set_xscale(""log"")\n        ax.set_xlabel(""Learning rate"")\n        ax.set_ylabel(""Loss"")\n\n        if show_lr is not None:\n            ax.axvline(x=show_lr, color=""red"")\n\n        # Show only if the figure was created internally\n        if fig is not None:\n            plt.show()\n\n        return ax\n\n\nclass LinearLR(_LRScheduler):\n    """"""Linearly increases the learning rate between two boundaries over a number of\n    iterations.\n\n    Arguments:\n        optimizer (torch.optim.Optimizer): wrapped optimizer.\n        end_lr (float): the final learning rate.\n        num_iter (int): the number of iterations over which the test occurs.\n        last_epoch (int, optional): the index of last epoch. Default: -1.\n    """"""\n\n    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n        self.end_lr = end_lr\n\n        if num_iter <= 1:\n            raise ValueError(""`num_iter` must be larger than 1"")\n        self.num_iter = num_iter\n\n        super(LinearLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        # In earlier Pytorch versions last_epoch starts at -1, while in recent versions\n        # it starts at 0. We need to adjust the math a bit to handle this. See\n        # discussion at: https://github.com/davidtvs/pytorch-lr-finder/pull/42\n        if PYTORCH_VERSION < version.parse(""1.1.0""):\n            curr_iter = self.last_epoch + 1\n            r = curr_iter / (self.num_iter - 1)\n        else:\n            r = self.last_epoch / (self.num_iter - 1)\n\n        return [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]\n\n\nclass ExponentialLR(_LRScheduler):\n    """"""Exponentially increases the learning rate between two boundaries over a number of\n    iterations.\n\n    Arguments:\n        optimizer (torch.optim.Optimizer): wrapped optimizer.\n        end_lr (float): the final learning rate.\n        num_iter (int): the number of iterations over which the test occurs.\n        last_epoch (int, optional): the index of last epoch. Default: -1.\n    """"""\n\n    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n        self.end_lr = end_lr\n\n        if num_iter <= 1:\n            raise ValueError(""`num_iter` must be larger than 1"")\n        self.num_iter = num_iter\n\n        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        # In earlier Pytorch versions last_epoch starts at -1, while in recent versions\n        # it starts at 0. We need to adjust the math a bit to handle this. See\n        # discussion at: https://github.com/davidtvs/pytorch-lr-finder/pull/42\n        if PYTORCH_VERSION < version.parse(""1.1.0""):\n            curr_iter = self.last_epoch + 1\n            r = curr_iter / (self.num_iter - 1)\n        else:\n            r = self.last_epoch / (self.num_iter - 1)\n\n        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n\n\nclass StateCacher(object):\n    def __init__(self, in_memory, cache_dir=None):\n        self.in_memory = in_memory\n        self.cache_dir = cache_dir\n\n        if self.cache_dir is None:\n            import tempfile\n\n            self.cache_dir = tempfile.gettempdir()\n        else:\n            if not os.path.isdir(self.cache_dir):\n                raise ValueError(""Given `cache_dir` is not a valid directory."")\n\n        self.cached = {}\n\n    def store(self, key, state_dict):\n        if self.in_memory:\n            self.cached.update({key: copy.deepcopy(state_dict)})\n        else:\n            fn = os.path.join(self.cache_dir, ""state_{}_{}.pt"".format(key, id(self)))\n            self.cached.update({key: fn})\n            torch.save(state_dict, fn)\n\n    def retrieve(self, key):\n        if key not in self.cached:\n            raise KeyError(""Target {} was not cached."".format(key))\n\n        if self.in_memory:\n            return self.cached.get(key)\n        else:\n            fn = self.cached.get(key)\n            if not os.path.exists(fn):\n                raise RuntimeError(\n                    ""Failed to load state in {}. File doesn\'t exist anymore."".format(fn)\n                )\n            state_dict = torch.load(fn, map_location=lambda storage, location: storage)\n            return state_dict\n\n    def __del__(self):\n        """"""Check whether there are unused cached files existing in `cache_dir` before\n        this instance being destroyed.""""""\n\n        if self.in_memory:\n            return\n\n        for k in self.cached:\n            if os.path.exists(self.cached[k]):\n                os.remove(self.cached[k])\n'"
