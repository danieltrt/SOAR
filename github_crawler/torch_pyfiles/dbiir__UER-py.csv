file_path,api_count,code
preprocess.py,0,"b'# -*- encoding:utf-8 -*-\nfrom __future__ import absolute_import\nimport os\nimport sys\nimport torch\nimport argparse\nfrom uer.utils.vocab import Vocab\nfrom uer.utils.data import *\nfrom uer.utils.tokenizer import *\n\n\ndef main():\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    \n    # Path options.\n    parser.add_argument(""--corpus_path"", type=str, required=True,\n                        help=""Path of the corpus for pretraining."")\n    parser.add_argument(""--vocab_path"", type=str, required=True,\n                        help=""Path of the vocabulary file."")\n    parser.add_argument(""--dataset_path"", type=str, default=""dataset.pt"",\n                        help=""Path of the preprocessed dataset."")\n    \n    # Preprocess options.\n    parser.add_argument(""--tokenizer"", choices=[""bert"", ""char"", ""space""], default=""bert"",\n                        help=""Specify the tokenizer."" \n                             ""Original Google BERT uses bert tokenizer on Chinese corpus.""\n                             ""Char tokenizer segments sentences into characters.""\n                             ""Space tokenizer segments sentences into words according to space.""\n                             )\n    parser.add_argument(""--processes_num"", type=int, default=1,\n                        help=""Split the whole dataset into `processes_num` parts, ""\n                             ""and each part is fed to a single process in training step."")\n    parser.add_argument(""--target"", choices=[""bert"", ""lm"", ""cls"", ""mlm"", ""bilm""], default=""bert"",\n                        help=""The training target of the pretraining model."")\n    parser.add_argument(""--docs_buffer_size"", type=int, default=100000,\n                        help=""The buffer size of documents in memory, specific to targets that require negative sampling."")\n    parser.add_argument(""--seq_length"", type=int, default=128, help=""Sequence length of instances."")\n    parser.add_argument(""--dup_factor"", type=int, default=5,\n                        help=""Duplicate instances multiple times."")\n    parser.add_argument(""--short_seq_prob"", type=float, default=0.1,\n                        help=""Probability of truncating sequence.""\n                             ""The larger value, the higher probability of using short (truncated) sequence."")\n    parser.add_argument(""--seed"", type=int, default=7, help=""Random seed."")\n    \n    args = parser.parse_args()\n    \n    # Load vocabulary.\n    vocab = Vocab()\n    vocab.load(args.vocab_path)\n    \n    # Build tokenizer.\n    tokenizer = globals()[args.tokenizer.capitalize() + ""Tokenizer""](args)\n        \n    # Build and save dataset.\n    dataset = globals()[args.target.capitalize() + ""Dataset""](args, vocab, tokenizer)\n    dataset.build_and_save(args.processes_num)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
pretrain.py,4,"b'# -*- encoding:utf-8 -*-\nimport os\nimport json\nimport torch\nimport argparse\nimport uer.trainer as trainer\nfrom uer.utils.config import load_hyperparam\n\n\ndef main():\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    \n    # Path options.\n    parser.add_argument(""--dataset_path"", type=str, default=""dataset.pt"",\n                        help=""Path of the preprocessed dataset."")\n    parser.add_argument(""--vocab_path"", type=str, required=True,\n                        help=""Path of the vocabulary file."")\n    parser.add_argument(""--pretrained_model_path"", type=str, default=None,\n                        help=""Path of the pretrained model."")\n    parser.add_argument(""--output_model_path"", type=str, required=True,\n                        help=""Path of the output model."")\n    parser.add_argument(""--config_path"", type=str, default=None,\n                        help=""Config file of model hyper-parameters."")\n\n    # Training and saving options. \n    parser.add_argument(""--total_steps"", type=int, default=100000,\n                        help=""Total training steps."")\n    parser.add_argument(""--save_checkpoint_steps"", type=int, default=10000,\n                        help=""Specific steps to save model checkpoint."")\n    parser.add_argument(""--report_steps"", type=int, default=100,\n                        help=""Specific steps to print prompt."")\n    parser.add_argument(""--accumulation_steps"", type=int, default=1,\n                        help=""Specific steps to accumulate gradient."")\n    parser.add_argument(""--batch_size"", type=int, default=32,\n                        help=""Training batch size. The actual batch_size is [batch_size x world_size x accumulation_steps]."")\n    parser.add_argument(""--instances_buffer_size"", type=int, default=25600,\n                        help=""The buffer size of instances in memory."")\n\n    # Model options.\n    parser.add_argument(""--emb_size"", type=int, default=768, help=""Embedding dimension."")\n    parser.add_argument(""--hidden_size"", type=int, default=768,  help=""Hidden state dimension."")\n    parser.add_argument(""--feedforward_size"", type=int, default=3072, help=""Feed forward layer dimension."")\n    parser.add_argument(""--kernel_size"", type=int, default=3,  help=""Kernel size for CNN."")\n    parser.add_argument(""--block_size"", type=int, default=2,  help=""Block size for CNN."")\n    parser.add_argument(""--heads_num"", type=int, default=12, help=""The number of heads in multi-head attention."")\n    parser.add_argument(""--layers_num"", type=int, default=12, help=""The number of encoder layers."")\n    parser.add_argument(""--dropout"", type=float, default=0.1, help=""Dropout value."")\n    parser.add_argument(""--seed"", type=int, default=7,  help=""Random seed."")\n    parser.add_argument(""--embedding"", choices=[""bert"", ""word""], default=""bert"",\n                        help=""Emebdding type."")\n    parser.add_argument(""--encoder"", choices=[""bert"", ""lstm"", ""gru"", \\\n                                                   ""cnn"", ""gatedcnn"", ""attn"", \\\n                                                   ""rcnn"", ""crnn"", ""gpt"", ""bilstm""], \\\n                                                   default=""bert"", help=""Encoder type."")\n    parser.add_argument(""--bidirectional"", action=""store_true"", help=""Specific to recurrent model."")\n    parser.add_argument(""--target"", choices=[""bert"", ""lm"", ""cls"", ""mlm"", ""bilm""], default=""bert"",\n                        help=""The training target of the pretraining model."")\n    parser.add_argument(""--labels_num"", type=int, default=2, help=""Specific to classification target."")\n\n    # Optimizer options.\n    parser.add_argument(""--learning_rate"", type=float, default=2e-5, help=""Initial learning rate."")\n    parser.add_argument(""--warmup"", type=float, default=0.1, help=""Warm up value."")\n    parser.add_argument(\'--fp16\', action=\'store_true\',\n                        help=""Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit"")\n    parser.add_argument(\'--fp16_opt_level\', type=str, default=\'O1\',\n                        help=""For fp16: Apex AMP optimization level selected in [\'O0\', \'O1\', \'O2\', and \'O3\'].""\n                             ""See details at https://nvidia.github.io/apex/amp.html"")\n\n    # Subword options.\n    parser.add_argument(""--subword_type"", choices=[""none"", ""char""], default=""none"",\n                        help=""Subword feature type."")\n    parser.add_argument(""--sub_vocab_path"", type=str, default=""models/sub_vocab.txt"",\n                        help=""Path of the subword vocabulary file."")\n    parser.add_argument(""--subencoder"", choices=[""avg"", ""lstm"", ""gru"", ""cnn""], default=""avg"",\n                        help=""Subencoder type."")\n    parser.add_argument(""--sub_layers_num"", type=int, default=2, help=""The number of subencoder layers."")\n\n    # GPU options.\n    parser.add_argument(""--world_size"", type=int, default=1, help=""Total number of processes (GPUs) for training."")\n    parser.add_argument(""--gpu_ranks"", default=[], nargs=\'+\', type=int, help=""List of ranks of each process.""\n                        "" Each process has a unique integer rank whose value is in the interval [0, world_size), and runs in a single GPU."")\n    parser.add_argument(""--master_ip"", default=""tcp://localhost:12345"", type=str, help=""IP-Port of master for training."")\n    parser.add_argument(""--backend"", choices=[""nccl"", ""gloo""], default=""nccl"", type=str, help=""Distributed backend."")\n    \n    args = parser.parse_args()\n\n    # Load hyper-parameters from config file. \n    if args.config_path:\n        load_hyperparam(args)\n\n    ranks_num = len(args.gpu_ranks)\n\n    if args.world_size > 1:\n        # Multiprocessing distributed mode.\n        assert torch.cuda.is_available(), ""No available GPUs."" \n        assert ranks_num <= args.world_size, ""Started processes exceed `world_size` upper limit."" \n        assert ranks_num <= torch.cuda.device_count(), ""Started processes exceeds the available GPUs."" \n        args.dist_train = True\n        args.ranks_num = ranks_num\n        print(""Using distributed mode for training."")\n    elif args.world_size == 1 and ranks_num == 1:\n        # Single GPU mode.\n        assert torch.cuda.is_available(), ""No available GPUs."" \n        args.gpu_id = args.gpu_ranks[0]\n        assert args.gpu_id < torch.cuda.device_count(), ""Invalid specified GPU device."" \n        args.dist_train = False\n        args.single_gpu = True\n        print(""Using single GPU:%d for training."" % args.gpu_id)\n    else:\n        # CPU mode.\n        assert ranks_num == 0, ""GPUs are specified, please check the arguments.""\n        args.dist_train = False\n        args.single_gpu = False\n        print(""Using CPU mode for training."")\n\n    trainer.train_and_validate(args)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
run_classifier.py,23,"b'# -*- encoding:utf-8 -*-\n""""""\n  This script provides an exmaple to wrap UER-py for classification.\n""""""\nimport torch\nimport json\nimport random\nimport argparse\nimport collections\nimport torch.nn as nn\nfrom uer.utils.vocab import Vocab\nfrom uer.utils.constants import *\nfrom uer.utils.tokenizer import * \nfrom uer.model_builder import build_model\nfrom uer.utils.optimizers import *\nfrom uer.utils.config import load_hyperparam\nfrom uer.utils.seed import set_seed\nfrom uer.model_saver import save_model\nfrom uer.model_loader import load_model\n\n\nclass BertClassifier(nn.Module):\n    def __init__(self, args, model):\n        super(BertClassifier, self).__init__()\n        self.embedding = model.embedding\n        self.encoder = model.encoder\n        self.labels_num = args.labels_num\n        self.pooling = args.pooling\n        self.output_layer_1 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.output_layer_2 = nn.Linear(args.hidden_size, args.labels_num)\n        self.softmax = nn.LogSoftmax(dim=-1)\n        self.criterion = nn.NLLLoss()\n\n    def forward(self, src, label, mask):\n        """"""\n        Args:\n            src: [batch_size x seq_length]\n            label: [batch_size]\n            mask: [batch_size x seq_length]\n        """"""\n        # Embedding.\n        emb = self.embedding(src, mask)\n        # Encoder.\n        output = self.encoder(emb, mask)\n        # Target.\n        if self.pooling == ""mean"":\n            output = torch.mean(output, dim=1)\n        elif self.pooling == ""max"":\n            output = torch.max(output, dim=1)[0]\n        elif self.pooling == ""last"":\n            output = output[:, -1, :]\n        else:\n            output = output[:, 0, :]\n        output = torch.tanh(self.output_layer_1(output))\n        logits = self.output_layer_2(output)\n        loss = self.criterion(self.softmax(logits.view(-1, self.labels_num)), label.view(-1))\n        return loss, logits\n\n\ndef main():\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # Path options.\n    parser.add_argument(""--pretrained_model_path"", default=None, type=str,\n                        help=""Path of the pretrained model."")\n    parser.add_argument(""--output_model_path"", default=""./models/classifier_model.bin"", type=str,\n                        help=""Path of the output model."")\n    parser.add_argument(""--vocab_path"", type=str, required=True,\n                        help=""Path of the vocabulary file."")\n    parser.add_argument(""--train_path"", type=str, required=True,\n                        help=""Path of the trainset."")\n    parser.add_argument(""--dev_path"", type=str, required=True,\n                        help=""Path of the devset."") \n    parser.add_argument(""--test_path"", type=str,\n                        help=""Path of the testset."")\n    parser.add_argument(""--config_path"", default=""./models/bert_base_config.json"", type=str,\n                        help=""Path of the config file."")\n\n    # Model options.\n    parser.add_argument(""--batch_size"", type=int, default=64,\n                        help=""Batch size."")\n    parser.add_argument(""--seq_length"", type=int, default=128,\n                        help=""Sequence length."")\n    parser.add_argument(""--embedding"", choices=[""bert"", ""word""], default=""bert"",\n                        help=""Emebdding type."")\n    parser.add_argument(""--encoder"", choices=[""bert"", ""lstm"", ""gru"", \\\n                                                   ""cnn"", ""gatedcnn"", ""attn"", \\\n                                                   ""rcnn"", ""crnn"", ""gpt"", ""bilstm""], \\\n                                                   default=""bert"", help=""Encoder type."")\n    parser.add_argument(""--bidirectional"", action=""store_true"", help=""Specific to recurrent model."")\n    parser.add_argument(""--pooling"", choices=[""mean"", ""max"", ""first"", ""last""], default=""first"",\n                        help=""Pooling type."")\n\n    # Subword options.\n    parser.add_argument(""--subword_type"", choices=[""none"", ""char""], default=""none"",\n                        help=""Subword feature type."")\n    parser.add_argument(""--sub_vocab_path"", type=str, default=""models/sub_vocab.txt"",\n                        help=""Path of the subword vocabulary file."")\n    parser.add_argument(""--subencoder"", choices=[""avg"", ""lstm"", ""gru"", ""cnn""], default=""avg"",\n                        help=""Subencoder type."")\n    parser.add_argument(""--sub_layers_num"", type=int, default=2, help=""The number of subencoder layers."")\n\n    # Tokenizer options.\n    parser.add_argument(""--tokenizer"", choices=[""bert"", ""char"", ""space""], default=""bert"",\n                        help=""Specify the tokenizer."" \n                             ""Original Google BERT uses bert tokenizer on Chinese corpus.""\n                             ""Char tokenizer segments sentences into characters.""\n                             ""Space tokenizer segments sentences into words according to space.""\n                             )\n\n    # Optimizer options.\n    parser.add_argument(""--learning_rate"", type=float, default=2e-5,\n                        help=""Learning rate."")\n    parser.add_argument(""--warmup"", type=float, default=0.1,\n                        help=""Warm up value."")\n    parser.add_argument(\'--fp16\', action=\'store_true\',\n                        help=""Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit"")\n    parser.add_argument(\'--fp16_opt_level\', type=str, default=\'O1\',\n                        help=""For fp16: Apex AMP optimization level selected in [\'O0\', \'O1\', \'O2\', and \'O3\'].""\n                             ""See details at https://nvidia.github.io/apex/amp.html"")\n    \n    # Training options.\n    parser.add_argument(""--dropout"", type=float, default=0.5,\n                        help=""Dropout."")\n    parser.add_argument(""--epochs_num"", type=int, default=3,\n                        help=""Number of epochs."")\n    parser.add_argument(""--report_steps"", type=int, default=100,\n                        help=""Specific steps to print prompt."")\n    parser.add_argument(""--seed"", type=int, default=7,\n                        help=""Random seed."")\n\n    # Evaluation options.\n    parser.add_argument(""--mean_reciprocal_rank"", action=""store_true"", help=""Evaluation metrics for DBQA dataset."")\n\n    args = parser.parse_args()\n\n    # Load the hyperparameters from the config file.\n    args = load_hyperparam(args)\n\n    set_seed(args.seed)\n\n    # Count the number of labels.\n    labels_set = set()\n    columns = {}\n    with open(args.train_path, mode=""r"", encoding=""utf-8"") as f:\n        for line_id, line in enumerate(f):\n            try:\n                line = line.strip().split(""\\t"")\n                if line_id == 0:\n                    for i, column_name in enumerate(line):\n                        columns[column_name] = i\n                    continue\n                label = int(line[columns[""label""]])\n                labels_set.add(label)\n            except:\n                pass\n    args.labels_num = len(labels_set) \n\n    # Load vocabulary.\n    vocab = Vocab()\n    vocab.load(args.vocab_path)\n    args.vocab = vocab\n\n    # Build bert model.\n    # A pseudo target is added.\n    args.target = ""bert""\n    model = build_model(args)\n\n    # Load or initialize parameters.\n    if args.pretrained_model_path is not None:\n        # Initialize with pretrained model.\n        model.load_state_dict(torch.load(args.pretrained_model_path), strict=False)  \n    else:\n        # Initialize with normal distribution.\n        for n, p in list(model.named_parameters()):\n            if \'gamma\' not in n and \'beta\' not in n:\n                p.data.normal_(0, 0.02)\n    \n    # Build classification model.\n    model = BertClassifier(args, model)\n\n    # For simplicity, we use DataParallel wrapper to use multiple GPUs.\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = model.to(device)\n    \n    # Datset loader.\n    def batch_loader(batch_size, input_ids, label_ids, mask_ids):\n        instances_num = input_ids.size()[0]\n        for i in range(instances_num // batch_size):\n            input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n            label_ids_batch = label_ids[i*batch_size: (i+1)*batch_size]\n            mask_ids_batch = mask_ids[i*batch_size: (i+1)*batch_size, :]\n            yield input_ids_batch, label_ids_batch, mask_ids_batch\n        if instances_num > instances_num // batch_size * batch_size:\n            input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n            label_ids_batch = label_ids[instances_num//batch_size*batch_size:]\n            mask_ids_batch = mask_ids[instances_num//batch_size*batch_size:, :]\n            yield input_ids_batch, label_ids_batch, mask_ids_batch\n\n    # Build tokenizer.\n    tokenizer = globals()[args.tokenizer.capitalize() + ""Tokenizer""](args)\n\n    # Read dataset.\n    def read_dataset(path):\n        dataset = []\n        with open(path, mode=""r"", encoding=""utf-8"") as f:\n            for line_id, line in enumerate(f):\n                if line_id == 0:\n                    continue\n                try:\n                    line = line.strip().split(\'\\t\')\n                    if len(line) == 2:\n                        label = int(line[columns[""label""]])\n                        text = line[columns[""text_a""]]\n                        tokens = [vocab.get(t) for t in tokenizer.tokenize(text)]\n                        tokens = [CLS_ID] + tokens\n                        mask = [1] * len(tokens)\n                        if len(tokens) > args.seq_length:\n                            tokens = tokens[:args.seq_length]\n                            mask = mask[:args.seq_length]\n                        while len(tokens) < args.seq_length:\n                            tokens.append(0)\n                            mask.append(0)\n                        dataset.append((tokens, label, mask))\n                    elif len(line) == 3: # For sentence pair input.\n                        label = int(line[columns[""label""]])\n                        text_a, text_b = line[columns[""text_a""]], line[columns[""text_b""]]\n\n                        tokens_a = [vocab.get(t) for t in tokenizer.tokenize(text_a)]\n                        tokens_a = [CLS_ID] + tokens_a + [SEP_ID]\n                        tokens_b = [vocab.get(t) for t in tokenizer.tokenize(text_b)]\n                        tokens_b = tokens_b + [SEP_ID]\n\n                        tokens = tokens_a + tokens_b\n                        mask = [1] * len(tokens_a) + [2] * len(tokens_b)\n                        \n                        if len(tokens) > args.seq_length:\n                            tokens = tokens[:args.seq_length]\n                            mask = mask[:args.seq_length]\n                        while len(tokens) < args.seq_length:\n                            tokens.append(0)\n                            mask.append(0)\n                        dataset.append((tokens, label, mask))\n                    elif len(line) == 4: # For dbqa input.\n                        qid=int(line[columns[""qid""]])\n                        label = int(line[columns[""label""]])\n                        text_a, text_b = line[columns[""text_a""]], line[columns[""text_b""]]\n\n                        tokens_a = [vocab.get(t) for t in tokenizer.tokenize(text_a)]\n                        tokens_a = [CLS_ID] + tokens_a + [SEP_ID]\n                        tokens_b = [vocab.get(t) for t in tokenizer.tokenize(text_b)]\n                        tokens_b = tokens_b + [SEP_ID]\n\n                        tokens = tokens_a + tokens_b\n                        mask = [1] * len(tokens_a) + [2] * len(tokens_b)\n\n                        if len(tokens) > args.seq_length:\n                            tokens = tokens[:args.seq_length]\n                            mask = mask[:args.seq_length]\n                        while len(tokens) < args.seq_length:\n                            tokens.append(0)\n                            mask.append(0)\n                        dataset.append((tokens, label, mask, qid))\n                    else:\n                        pass\n                        \n                except:\n                    pass\n        return dataset\n\n    # Evaluation function.\n    def evaluate(args, is_test):\n        if is_test:\n            dataset = read_dataset(args.test_path)\n        else:\n            dataset = read_dataset(args.dev_path)\n\n        input_ids = torch.LongTensor([sample[0] for sample in dataset])\n        label_ids = torch.LongTensor([sample[1] for sample in dataset])\n        mask_ids = torch.LongTensor([sample[2] for sample in dataset])\n\n        batch_size = args.batch_size\n        instances_num = input_ids.size()[0]\n        if is_test:\n            print(""The number of evaluation instances: "", instances_num)\n\n        correct = 0\n        # Confusion matrix.\n        confusion = torch.zeros(args.labels_num, args.labels_num, dtype=torch.long)\n\n        model.eval()\n        \n        if not args.mean_reciprocal_rank:\n            for i, (input_ids_batch, label_ids_batch,  mask_ids_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids)):\n                input_ids_batch = input_ids_batch.to(device)\n                label_ids_batch = label_ids_batch.to(device)\n                mask_ids_batch = mask_ids_batch.to(device)\n                with torch.no_grad():\n                    loss, logits = model(input_ids_batch, label_ids_batch, mask_ids_batch)\n                logits = nn.Softmax(dim=1)(logits)\n                pred = torch.argmax(logits, dim=1)\n                gold = label_ids_batch\n                for j in range(pred.size()[0]):\n                    confusion[pred[j], gold[j]] += 1\n                correct += torch.sum(pred == gold).item()\n        \n            if is_test:\n                print(""Confusion matrix:"")\n                print(confusion)\n                print(""Report precision, recall, and f1:"")\n            for i in range(confusion.size()[0]):\n                p = confusion[i,i].item()/confusion[i,:].sum().item()\n                r = confusion[i,i].item()/confusion[:,i].sum().item()\n                f1 = 2*p*r / (p+r)\n                if is_test:\n                    print(""Label {}: {:.3f}, {:.3f}, {:.3f}"".format(i,p,r,f1))\n            print(""Acc. (Correct/Total): {:.4f} ({}/{}) "".format(correct/len(dataset), correct, len(dataset)))\n            return correct/len(dataset)\n        else:\n            for i, (input_ids_batch, label_ids_batch, mask_ids_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids)):\n                input_ids_batch = input_ids_batch.to(device)\n                label_ids_batch = label_ids_batch.to(device)\n                mask_ids_batch = mask_ids_batch.to(device)\n                with torch.no_grad():\n                    loss, logits = model(input_ids_batch, label_ids_batch, mask_ids_batch)\n                logits = nn.Softmax(dim=1)(logits)\n                if i == 0:\n                    logits_all=logits\n                if i >= 1:\n                    logits_all=torch.cat((logits_all,logits),0)\n        \n            order = -1\n            gold = []\n            for i in range(len(dataset)):\n                qid = dataset[i][3]\n                label = dataset[i][1]\n                if qid == order:\n                    j += 1\n                    if label == 1:\n                        gold.append((qid,j))\n                else:\n                    order = qid\n                    j = 0\n                    if label == 1:\n                        gold.append((qid,j))\n\n\n            label_order = []\n            order = -1\n            for i in range(len(gold)):\n                if gold[i][0] == order:\n                    templist.append(gold[i][1])\n                elif gold[i][0] != order:\n                    order=gold[i][0]\n                    if i > 0:\n                        label_order.append(templist)\n                    templist = []\n                    templist.append(gold[i][1])\n            label_order.append(templist)\n\n            order = -1\n            score_list = []\n            for i in range(len(logits_all)):\n                score = float(logits_all[i][1])\n                qid=int(dataset[i][3])\n                if qid == order:\n                    templist.append(score)\n                else:\n                    order = qid\n                    if i > 0:\n                        score_list.append(templist)\n                    templist = []\n                    templist.append(score)\n            score_list.append(templist)\n\n            rank = []\n            pred = []\n            for i in range(len(score_list)):\n                if len(label_order[i])==1:\n                    if label_order[i][0] < len(score_list[i]):\n                        true_score = score_list[i][label_order[i][0]]\n                        score_list[i].sort(reverse=True)\n                        for j in range(len(score_list[i])):\n                            if score_list[i][j] == true_score:\n                                rank.append(1 / (j + 1))\n                    else:\n                        rank.append(0)\n\n                else:\n                    true_rank = len(score_list[i])\n                    for k in range(len(label_order[i])):\n                        if label_order[i][k] < len(score_list[i]):\n                            true_score = score_list[i][label_order[i][k]]\n                            temp = sorted(score_list[i],reverse=True)\n                            for j in range(len(temp)):\n                                if temp[j] == true_score:\n                                    if j < true_rank:\n                                        true_rank = j\n                    if true_rank < len(score_list[i]):\n                        rank.append(1 / (true_rank + 1))\n                    else:\n                        rank.append(0)\n            MRR = sum(rank) / len(rank)\n            print(""Mean Reciprocal Rank: {:.4f}"".format(MRR))\n            return MRR\n\n    # Training phase.\n    print(""Start training."")\n    trainset = read_dataset(args.train_path)\n    random.shuffle(trainset)\n    instances_num = len(trainset)\n    batch_size = args.batch_size\n\n    input_ids = torch.LongTensor([example[0] for example in trainset])\n    label_ids = torch.LongTensor([example[1] for example in trainset])\n    mask_ids = torch.LongTensor([example[2] for example in trainset])\n\n    train_steps = int(instances_num * args.epochs_num / batch_size) + 1\n\n    print(""Batch size: "", batch_size)\n    print(""The number of training instances:"", instances_num)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\'bias\', \'gamma\', \'beta\']\n    optimizer_grouped_parameters = [\n                {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay_rate\': 0.01},\n                {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay_rate\': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, correct_bias=False)\n    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=train_steps*args.warmup, t_total=train_steps)\n    \n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    if torch.cuda.device_count() > 1:\n        print(""{} GPUs are available. Let\'s use them."".format(torch.cuda.device_count()))\n        model = torch.nn.DataParallel(model)\n\n    total_loss = 0.\n    result = 0.0\n    best_result = 0.0\n    \n    for epoch in range(1, args.epochs_num+1):\n        model.train()\n        for i, (input_ids_batch, label_ids_batch, mask_ids_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids)):\n            model.zero_grad()\n\n            input_ids_batch = input_ids_batch.to(device)\n            label_ids_batch = label_ids_batch.to(device)\n            mask_ids_batch = mask_ids_batch.to(device)\n\n            loss, _ = model(input_ids_batch, label_ids_batch, mask_ids_batch)\n            if torch.cuda.device_count() > 1:\n                loss = torch.mean(loss)\n            total_loss += loss.item()\n            if (i + 1) % args.report_steps == 0:\n                print(""Epoch id: {}, Training steps: {}, Avg loss: {:.3f}"".format(epoch, i+1, total_loss / args.report_steps))\n                total_loss = 0.\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            optimizer.step()\n            scheduler.step()\n        result = evaluate(args, False)\n        if result > best_result:\n            best_result = result\n            save_model(model, args.output_model_path)\n        else:\n            continue\n\n    # Evaluation phase.\n    if args.test_path is not None:\n        print(""Test set evaluation."")\n        model = load_model(model, args.output_model_path)\n        evaluate(args, True)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
run_mrc.py,20,"b'# -*- coding: utf-8 -*-\r\nfrom __future__ import print_function\r\nfrom collections import Counter, OrderedDict\r\nimport string\r\nimport re\r\nimport argparse\r\nimport json\r\nimport sys\r\nimport importlib\r\nimportlib.reload(sys)\r\nimport pdb\r\n\r\nimport torch\r\nimport random\r\nimport collections\r\nimport torch.nn as nn\r\n\r\nfrom uer.utils.vocab import Vocab\r\nfrom uer.utils.constants import *\r\nfrom uer.utils.tokenizer import * \r\nfrom uer.model_builder import build_model\r\nfrom uer.utils.optimizers import *\r\nfrom uer.utils.config import load_hyperparam\r\nfrom uer.utils.seed import set_seed\r\nfrom uer.model_saver import save_model\r\n\r\n\r\nclass BertQuestionAnswering(nn.Module):\r\n    def __init__(self, args, bert_model):\r\n        super(BertQuestionAnswering, self).__init__()\r\n        self.embedding = bert_model.embedding\r\n        self.encoder = bert_model.encoder\r\n        self.output_layer_1 = nn.Linear(args.hidden_size, args.hidden_size)\r\n        self.output_layer_2 = nn.Linear(args.hidden_size, 2)\r\n        self.softmax = nn.LogSoftmax(dim=-1)\r\n        self.criterion = nn.NLLLoss()\r\n    def forward(self, src, mask, start_positions, end_positions):\r\n        """"""\r\n        Args:\r\n            src: [batch_size x seq_length]\r\n            start_positions,end_positions: [batch_size]\r\n            mask: [batch_size x seq_length]\r\n        """"""\r\n        # Embedding.\r\n        emb = self.embedding(src, mask)\r\n        # Encoder.\r\n        output = self.encoder(emb, mask)\r\n        output = torch.tanh(self.output_layer_1(output))\r\n        logits = self.output_layer_2(output)#batch_size*seq*2\r\n        start_logits, end_logits = logits.split(1, dim=-1)\r\n        start_logits = start_logits.squeeze(-1)\r\n        end_logits = end_logits.squeeze(-1)\r\n \r\n        start_loss = self.criterion(self.softmax(start_logits), start_positions)\r\n        end_loss = self.criterion(self.softmax(end_logits), end_positions)\r\n        loss = (start_loss + end_loss) / 2\r\n        \r\n        return loss, start_logits, end_logits\r\n\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n\r\n    # Path options.\r\n    parser.add_argument(""--pretrained_model_path"", default=None, type=str,\r\n                        help=""Path of the pretrained model."")\r\n    parser.add_argument(""--output_model_path"", default=""./models/QA_model.bin"", type=str,\r\n                        help=""Path of the output model."")\r\n    parser.add_argument(""--vocab_path"", type=str, required=True,\r\n                        help=""Path of the vocabulary file."")\r\n    parser.add_argument(""--train_path"", type=str, required=True,\r\n                        help=""Path of the trainset."")\r\n    parser.add_argument(""--dev_path"", type=str, required=True,\r\n                        help=""Path of the devset."") \r\n    parser.add_argument(""--test_path"", type=str,\r\n                        help=""Path of the testset."")\r\n    parser.add_argument(""--config_path"", default=""./models/bert_base_config.json"", type=str,\r\n                        help=""Path of the config file."")\r\n\r\n    # Model options.\r\n    parser.add_argument(""--batch_size"", type=int, default=64,\r\n                        help=""Batch size."")\r\n    parser.add_argument(""--seq_length"", type=int, default=100,\r\n                        help=""Sequence length."")\r\n    parser.add_argument(""--doc_stride"", default=128, type=int,\r\n                        help=""When splitting up a long document into chunks, how much stride to take between chunks."")\r\n    parser.add_argument(""--embedding"", choices=[""bert"", ""word""], default=""bert"",\r\n                        help=""Emebdding type."")\r\n    parser.add_argument(""--encoder"", choices=[""bert"", ""lstm"", ""gru"", \\\r\n                                                   ""cnn"", ""gatedcnn"", ""attn"", \\\r\n                                                   ""rcnn"", ""crnn"", ""gpt""], \\\r\n                                                   default=""bert"", help=""Encoder type."")\r\n    parser.add_argument(""--bidirectional"", action=""store_true"", help=""Specific to recurrent model."")\r\n\r\n\r\n    # Subword options.\r\n    parser.add_argument(""--subword_type"", choices=[""none"", ""char""], default=""none"",\r\n                        help=""Subword feature type."")\r\n    parser.add_argument(""--sub_vocab_path"", type=str, default=""models/sub_vocab.txt"",\r\n                        help=""Path of the subword vocabulary file."")\r\n    parser.add_argument(""--subencoder"", choices=[""avg"", ""lstm"", ""gru"", ""cnn""], default=""avg"",\r\n                        help=""Subencoder type."")\r\n    parser.add_argument(""--sub_layers_num"", type=int, default=2, help=""The number of subencoder layers."")\r\n\r\n    # Tokenizer options.\r\n    parser.add_argument(""--tokenizer"", choices=[""bert"", ""char"", ""space""], default=""char"",\r\n                        help=""Specify the tokenizer."" \r\n                             ""Original Google BERT uses bert tokenizer on Chinese corpus.""\r\n                             ""Char tokenizer segments sentences into characters.""\r\n                             ""Space tokenizer segments sentences into words according to space.""\r\n                             )\r\n\r\n    # Optimizer options.\r\n    parser.add_argument(""--learning_rate"", type=float, default=3e-5,\r\n                        help=""Learning rate."")\r\n    parser.add_argument(""--warmup"", type=float, default=0.1,\r\n                        help=""Warm up value."")\r\n    parser.add_argument(\'--fp16\', action=\'store_true\',\r\n                        help=""Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit"")\r\n    parser.add_argument(\'--fp16_opt_level\', type=str, default=\'O1\',\r\n                        help=""For fp16: Apex AMP optimization level selected in [\'O0\', \'O1\', \'O2\', and \'O3\'].""\r\n                             ""See details at https://nvidia.github.io/apex/amp.html"")\r\n\r\n    # Training options.\r\n    parser.add_argument(""--dropout"", type=float, default=0.5,\r\n                        help=""Dropout."")\r\n    parser.add_argument(""--epochs_num"", type=int, default=3,\r\n                        help=""Number of epochs."")\r\n    parser.add_argument(""--report_steps"", type=int, default=100,\r\n                        help=""Specific steps to print prompt."")\r\n    parser.add_argument(""--seed"", type=int, default=7,\r\n                        help=""Random seed."")\r\n\r\n    args = parser.parse_args()\r\n\r\n    # Load the hyperparameters from the config file.\r\n    args = load_hyperparam(args)\r\n\r\n    set_seed(args.seed)\r\n      \r\n    # Load vocabulary.\r\n    vocab = Vocab()\r\n    vocab.load(args.vocab_path)\r\n    args.vocab = vocab\r\n\r\n    args.target = ""bert""\r\n    bert_model = build_model(args)\r\n    # Load or initialize parameters.\r\n    if args.pretrained_model_path is not None:\r\n        # Initialize with pretrained model. \r\n        bert_model.load_state_dict(torch.load(args.pretrained_model_path), strict=False)  \r\n    else:\r\n        # Initialize with normal distribution.\r\n        for n, p in list(bert_model.named_parameters()):\r\n            if \'gamma\' not in n and \'beta\' not in n:\r\n                p.data.normal_(0, 0.02)\r\n    \r\n    # Build QA model.\r\n    model = BertQuestionAnswering(args,bert_model)\r\n\r\n    # For simplicity, we use DataParallel wrapper to use multiple GPUs.\r\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\r\n    model = model.to(device)\r\n\r\n    # Dataset loader.\r\n    def batch_loader(batch_size, input_ids, mask_ids, start_positions, end_positions):\r\n        instances_num = input_ids.size()[0]\r\n        for i in range(instances_num // batch_size):\r\n            input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\r\n            mask_ids_batch = mask_ids[i*batch_size: (i+1)*batch_size, :]\r\n            start_positions_batch = start_positions[i*batch_size: (i+1)*batch_size]\r\n            end_positions_batch = end_positions[i*batch_size: (i+1)*batch_size]\r\n            yield input_ids_batch, mask_ids_batch, start_positions_batch, end_positions_batch\r\n        if instances_num > instances_num // batch_size * batch_size:\r\n            input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\r\n            mask_ids_batch = mask_ids[instances_num//batch_size*batch_size:, :]\r\n            start_positions_batch = start_positions[instances_num//batch_size*batch_size:]\r\n            end_positions_batch = end_positions[instances_num//batch_size*batch_size:]\r\n            yield input_ids_batch, mask_ids_batch, start_positions_batch, end_positions_batch\r\n\r\n    # Build tokenizer.\r\n    tokenizer = globals()[args.tokenizer.capitalize() + ""Tokenizer""](args)\r\n\r\n    # Read examples.\r\n    def read_examples(path):\r\n        examples = []\r\n        with open(path,\'r\',encoding=\'utf-8\') as fp:\r\n            all_dict = json.loads(fp.read())\r\n            v1 = all_dict[""data""]\r\n            for i in range(len(v1)):\r\n                data_dict = v1[i]\r\n                v2 = data_dict[""paragraphs""]\r\n\r\n                for j in range(len(v2)):\r\n                    para_dict = v2[j]\r\n                    context = para_dict[""context""]\r\n                    v3 = para_dict[""qas""]\r\n\r\n                    for m in range(len(v3)):\r\n                        qas_dict = v3[m]\r\n                        question = qas_dict[""question""]                                          \r\n                        question_id = qas_dict[""id""]\r\n                        v4 = qas_dict[""answers""]\r\n                        \r\n                        answers=[]\r\n                        start_positions=[]\r\n                        end_positions=[]\r\n\r\n                        for n in range(len(v4)):\r\n                            ans_dict = v4[n]\r\n                            answer = ans_dict[""text""]\r\n                            start_position = ans_dict[""answer_start""]\r\n                            end_position = start_position + len(answer)\r\n                            \r\n                            answers.append(answer)\r\n                            start_positions.append(start_position)\r\n                            end_positions.append(end_position)\r\n\r\n                        examples.append((context,question,question_id,start_positions,end_positions,answers))\r\n        \r\n        return examples\r\n\r\n\r\n    def convert_examples_to_dataset(examples, args):\r\n        dataset = []\r\n        print(""The number of questions in the dataset"",len(examples))\r\n        for i in range(len(examples)):\r\n            context = examples[i][0]\r\n            question = examples[i][1]\r\n            q_len = len(question)\r\n            question_id = examples[i][2]\r\n\r\n            start_positions_true = examples[i][3][0]#\xe5\xbe\x85\xe4\xbf\xae\xe6\x94\xb9\r\n            end_positions_true = examples[i][4][0]\r\n            \r\n            answers = examples[i][5]\r\n            max_context_length = args.seq_length - q_len - 3\r\n            # divide the context to some spans\r\n            _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\r\n                ""DocSpan"", [""start"", ""length""])\r\n            doc_spans = []\r\n            start_offset = 0\r\n            while start_offset < len(context):\r\n                length = len(context) - start_offset\r\n                if length > max_context_length:\r\n                    length = max_context_length\r\n                doc_spans.append(_DocSpan(start=start_offset, length=length))\r\n                if start_offset + length == len(context):\r\n                    break\r\n                start_offset += min(length, args.doc_stride)\r\n\r\n            for (doc_span_index, doc_span) in enumerate(doc_spans):\r\n                doc_span_start=doc_span.start\r\n                span_context = context[doc_span_start:doc_span_start+doc_span.length]         \r\n                # convert the start or end position to real position in tokens\r\n                start_positions = start_positions_true - doc_span_start + q_len + 2\r\n                end_positions = end_positions_true - doc_span_start + q_len + 2\r\n                # the answers of some question are not in the doc_span, we ignore them.\r\n                if start_positions < q_len+2 or start_positions > doc_span.length+q_len+2 or end_positions < q_len+2 or end_positions > doc_span.length+q_len+2:\r\n                    continue \r\n\r\n                tokens_a = [vocab.get(t) for t in tokenizer.tokenize(question)]\r\n                tokens_a = [CLS_ID] + tokens_a + [SEP_ID]\r\n                tokens_b = [vocab.get(t) for t in tokenizer.tokenize(span_context)]\r\n                tokens_b = tokens_b + [SEP_ID] \r\n                tokens = tokens_a + tokens_b\r\n                mask = [1] * len(tokens_a) + [2] * len(tokens_b)\r\n\r\n                while len(tokens) < args.seq_length:\r\n                    tokens.append(0)\r\n                    mask.append(0)\r\n\r\n                dataset.append((tokens,mask,start_positions,end_positions,answers,question_id,q_len,doc_span_index,doc_span_start))       \r\n        return dataset\r\n\r\n\r\n    # Evaluation function.\r\n    def evaluate(args, is_test):\r\n        # some calculation functions\r\n        def mixed_segmentation(in_str, rm_punc=False):\r\n            in_str = str(in_str).lower().strip()\r\n            segs_out = []\r\n            temp_str = """"\r\n            sp_char = [\'-\',\':\',\'_\',\'*\',\'^\',\'/\',\'\\\\\',\'~\',\'`\',\'+\',\'=\',\r\n                   \'\xef\xbc\x8c\',\'\xe3\x80\x82\',\'\xef\xbc\x9a\',\'\xef\xbc\x9f\',\'\xef\xbc\x81\',\'\xe2\x80\x9c\',\'\xe2\x80\x9d\',\'\xef\xbc\x9b\',\'\xe2\x80\x99\',\'\xe3\x80\x8a\',\'\xe3\x80\x8b\',\'\xe2\x80\xa6\xe2\x80\xa6\',\'\xc2\xb7\',\'\xe3\x80\x81\',\r\n                   \'\xe3\x80\x8c\',\'\xe3\x80\x8d\',\'\xef\xbc\x88\',\'\xef\xbc\x89\',\'\xef\xbc\x8d\',\'\xef\xbd\x9e\',\'\xe3\x80\x8e\',\'\xe3\x80\x8f\']\r\n            for char in in_str:\r\n                if rm_punc and char in sp_char:\r\n                    continue\r\n                if  re.search(r\'[\\u4e00-\\u9fa5]\', char) or char in sp_char:\r\n                    if temp_str != """":\r\n                        ss = tokenizer.tokenize(temp_str)\r\n                        segs_out.extend(ss)\r\n                        temp_str = """"\r\n                    segs_out.append(char)\r\n                else:\r\n                    temp_str += char\r\n\r\n            #handling last part\r\n            if temp_str != """":\r\n                ss = tokenizer.tokenize(temp_str)\r\n                segs_out.extend(ss)\r\n\r\n            return segs_out\r\n\r\n\r\n        # remove punctuation\r\n        def remove_punctuation(in_str):\r\n            in_str = str(in_str).lower().strip()\r\n            sp_char = [\'-\',\':\',\'_\',\'*\',\'^\',\'/\',\'\\\\\',\'~\',\'`\',\'+\',\'=\',\r\n                   \'\xef\xbc\x8c\',\'\xe3\x80\x82\',\'\xef\xbc\x9a\',\'\xef\xbc\x9f\',\'\xef\xbc\x81\',\'\xe2\x80\x9c\',\'\xe2\x80\x9d\',\'\xef\xbc\x9b\',\'\xe2\x80\x99\',\'\xe3\x80\x8a\',\'\xe3\x80\x8b\',\'\xe2\x80\xa6\xe2\x80\xa6\',\'\xc2\xb7\',\'\xe3\x80\x81\',\r\n                   \'\xe3\x80\x8c\',\'\xe3\x80\x8d\',\'\xef\xbc\x88\',\'\xef\xbc\x89\',\'\xef\xbc\x8d\',\'\xef\xbd\x9e\',\'\xe3\x80\x8e\',\'\xe3\x80\x8f\']\r\n            out_segs = []\r\n            for char in in_str:\r\n                if char in sp_char:\r\n                    continue\r\n                else:\r\n                    out_segs.append(char)\r\n            return \'\'.join(out_segs)\r\n\r\n\r\n        # find longest common string\r\n        def find_lcs(s1, s2):\r\n            m = [[0 for i in range(len(s2)+1)] for j in range(len(s1)+1)]\r\n            mmax = 0\r\n            p = 0\r\n            for i in range(len(s1)):\r\n                for j in range(len(s2)):\r\n                    if s1[i] == s2[j]:\r\n                        m[i+1][j+1] = m[i][j]+1\r\n                        if m[i+1][j+1] > mmax:\r\n                            mmax=m[i+1][j+1]\r\n                            p=i+1\r\n            return s1[p-mmax:p], mmax\r\n\r\n        def calc_f1_score(answers, prediction):\r\n            f1_scores = []     \r\n            for i in range(len(answers)):\r\n                ans = answers[i]\r\n                ans_segs = mixed_segmentation(ans, rm_punc=True)\r\n                prediction_segs = mixed_segmentation(prediction, rm_punc=True)\r\n                lcs, lcs_len = find_lcs(ans_segs, prediction_segs)\r\n                if lcs_len == 0:\r\n                    f1_scores.append(0)\r\n                else:\r\n                    precision   = 1.0*lcs_len/len(prediction_segs)\r\n                    recall      = 1.0*lcs_len/len(ans_segs)\r\n                    f1          = (2*precision*recall)/(precision+recall)\r\n                    f1_scores.append(f1)\r\n            return max(f1_scores)\r\n\r\n\r\n        def calc_em_score(answers, prediction):\r\n            em = 0\r\n            for i in range(len(answers)):\r\n                ans = answers[i]\r\n                ans_ = remove_punctuation(ans)\r\n                prediction_ = remove_punctuation(prediction)\r\n                if ans_ == prediction_:\r\n                    em = 1\r\n                    break\r\n            return em\r\n\r\n        def is_max_score(score_list):\r\n            score_max = -100\r\n            index_max = 0\r\n            best_start_prediction = 0\r\n            best_end_prediction = 0\r\n            for i in range(len(score_list)): \r\n                if score_max <= score_list[i][3]:\r\n                    score_max = score_list[i][3]\r\n                    index_max = score_list[i][0]\r\n                    best_start_prediction = score_list[i][1]\r\n                    best_end_prediction = score_list[i][2]\r\n            return index_max, best_start_prediction,best_end_prediction\r\n\r\n        if is_test:\r\n            examples = read_examples(args.test_path)\r\n            dataset = convert_examples_to_dataset(examples,args)\r\n\r\n        else:\r\n            examples = read_examples(args.dev_path)\r\n            dataset = convert_examples_to_dataset(examples,args)\r\n        \r\n        input_ids = torch.LongTensor([sample[0] for sample in dataset])\r\n        mask_ids = torch.LongTensor([sample[1] for sample in dataset])\r\n        start_positions = torch.LongTensor([sample[2] for sample in dataset])\r\n        end_positions = torch.LongTensor([sample[3] for sample in dataset])\r\n        \r\n        batch_size = args.batch_size\r\n        instances_num = input_ids.size()[0]\r\n        \r\n        if is_test:\r\n            print(""The number of evaluation instances: "", instances_num)\r\n        model.eval()\r\n        start_logits_all = []\r\n        end_logits_all = []\r\n        start_pred_all = []\r\n        end_pred_all = []\r\n        for i, (input_ids_batch, mask_ids_batch, start_positions_batch, end_positions_batch) in enumerate(batch_loader(batch_size, input_ids, mask_ids, start_positions, end_positions)):\r\n            model.zero_grad()\r\n            input_ids_batch = input_ids_batch.to(device)\r\n            mask_ids_batch = mask_ids_batch.to(device)\r\n            start_positions_batch = start_positions_batch.to(device)\r\n            end_positions_batch = end_positions_batch.to(device)\r\n                \r\n            with torch.no_grad():\r\n                loss, start_logits, end_logits = model(input_ids_batch, mask_ids_batch, start_positions_batch, end_positions_batch)\r\n                \r\n            start_logits = nn.Softmax(dim=1)(start_logits)\r\n            end_logits = nn.Softmax(dim=1)(end_logits)\r\n\r\n            start_pred = torch.argmax(start_logits, dim=1)\r\n            end_pred = torch.argmax(end_logits, dim=1)\r\n\r\n            start_pred=start_pred.cpu().numpy().tolist()\r\n            end_pred=end_pred.cpu().numpy().tolist()\r\n            \r\n            start_logits=start_logits.cpu().numpy().tolist()\r\n            end_logits=end_logits.cpu().numpy().tolist()\r\n\r\n            start_logits_max=[]\r\n            end_logits_max=[]\r\n            for j in range(len(start_pred)):\r\n                start_logits_max.append(start_logits[j][start_pred[j]])\r\n                end_logits_max.append(end_logits[j][end_pred[j]])\r\n            \r\n            start_logits_all += start_logits_max\r\n            end_logits_all += end_logits_max\r\n            start_pred_all += start_pred\r\n            end_pred_all  += end_pred\r\n        \r\n        assert len(start_pred_all)==len(dataset)\r\n        assert len(start_logits_all)==len(dataset)        \r\n\r\n        # couster by question id and chose the best answer in doc_spans\r\n        order = -1\r\n        pred_list = []\r\n        templist=[]\r\n        for i in range(len(dataset)):\r\n            qid = dataset[i][5]\r\n            q_len = dataset[i][6]\r\n            span_index =dataset[i][7]\r\n            doc_span_start = dataset[i][8]\r\n\r\n            score1 = float(start_logits_all[i])\r\n            score2 = float(end_logits_all[i])\r\n            score = (score1+score2)/2\r\n            \r\n            pre_start_pred = start_pred_all[i] + doc_span_start - q_len - 2\r\n            pre_end_pred = end_pred_all[i] + doc_span_start - q_len - 2\r\n            \r\n            if qid == order:\r\n                templist.append((span_index,pre_start_pred,pre_end_pred,score))\r\n            else:\r\n                order = qid\r\n                if i > 0:\r\n                    span_index_max, best_start_prediction,best_end_prediction = is_max_score(templist)   \r\n                    pred_list.append((span_index_max, best_start_prediction,best_end_prediction))\r\n                templist = []\r\n                templist.append((span_index,pre_start_pred,pre_end_pred,score))\r\n        span_index_max, best_start_prediction, best_end_prediction = is_max_score(templist)   \r\n        pred_list.append((span_index_max, best_start_prediction,best_end_prediction))\r\n   \r\n        assert len(pred_list) == len(examples)\r\n\r\n        #strat pred\r\n        f1 = 0\r\n        em = 0\r\n        total_count = len(examples)\r\n        skip_count = 0\r\n        for i in range(len(examples)):\r\n            question_id = examples[i][2]\r\n            answers = examples[i][5]\r\n            span_index = pred_list[i][0]\r\n            start_prediction = pred_list[i][1]\r\n            end_prediction = pred_list[i][2]\r\n            \r\n            #error prediction\r\n            if end_prediction <= start_prediction:\r\n                skip_count += 1\r\n                continue\r\n                \r\n            prediction = examples[i][0][start_prediction:end_prediction]\r\n            \r\n            f1 += calc_f1_score(answers, prediction)\r\n            em += calc_em_score(answers, prediction)\r\n        \r\n        f1_score = 100.0 * f1 / total_count\r\n        em_score = 100.0 * em / total_count\r\n        avg = (f1_score+em_score)*0.5\r\n        print(""Avg: {:.4f},F1:{:.4f},EM:{:.4f},Total:{},Skip:{}"".format(avg,f1_score,em_score,total_count,skip_count))\r\n        return avg \r\n\r\n    # Training phase\r\n    print(""Start training."")\r\n    batch_size = args.batch_size\r\n    print(""Batch size: "", batch_size)\r\n    examples = read_examples(args.train_path)\r\n    trainset = convert_examples_to_dataset(examples,args)\r\n    random.shuffle(trainset)\r\n    instances_num = len(trainset)\r\n\r\n    input_ids = torch.LongTensor([sample[0] for sample in trainset])\r\n    mask_ids = torch.LongTensor([sample[1] for sample in trainset])\r\n    start_positions = torch.LongTensor([sample[2] for sample in trainset])\r\n    end_positions = torch.LongTensor([sample[3] for sample in trainset])\r\n\r\n    train_steps = int(instances_num * args.epochs_num / batch_size) + 1\r\n   \r\n    print(""The number of training instances:"", instances_num)\r\n\r\n    param_optimizer = list(model.named_parameters())\r\n    no_decay = [\'bias\', \'gamma\', \'beta\']\r\n    optimizer_grouped_parameters = [\r\n                {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay_rate\': 0.01},\r\n                {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay_rate\': 0.0}\r\n    ]\r\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, correct_bias=False)\r\n    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=train_steps*args.warmup, t_total=train_steps)\r\n\r\n    if args.fp16:\r\n        try:\r\n            from apex import amp\r\n        except ImportError:\r\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\r\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\r\n\r\n    if torch.cuda.device_count() > 1:\r\n        print(""{} GPUs are available. Let\'s use them."".format(torch.cuda.device_count()))\r\n        model = torch.nn.DataParallel(model)\r\n    \r\n    total_loss = 0.\r\n    result = 0.0\r\n    best_result = 0.0\r\n    \r\n    for epoch in range(1, args.epochs_num+1):\r\n        model.train()\r\n        \r\n        for i, (input_ids_batch, mask_ids_batch, start_positions_batch, end_positions_batch) in enumerate(batch_loader(batch_size, input_ids, mask_ids, start_positions, end_positions)):\r\n            model.zero_grad()\r\n            input_ids_batch = input_ids_batch.to(device)\r\n            mask_ids_batch = mask_ids_batch.to(device)\r\n            start_positions_batch = start_positions_batch.to(device)\r\n            end_positions_batch = end_positions_batch.to(device)\r\n\r\n            loss, _, _ = model(input_ids_batch, mask_ids_batch, start_positions_batch, end_positions_batch)\r\n            if torch.cuda.device_count() > 1:\r\n                loss = torch.mean(loss)\r\n            total_loss += loss.item()\r\n            if (i + 1) % args.report_steps == 0:\r\n                print(""Epoch id: {}, Training steps: {}, Avg loss: {:.3f}"".format(epoch, i+1, total_loss / args.report_steps))\r\n                total_loss = 0.\r\n            \r\n            if args.fp16:\r\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\r\n                    scaled_loss.backward()\r\n            else:\r\n                loss.backward()\r\n\r\n            optimizer.step()\r\n            scheduler.step()\r\n        result = evaluate(args, False)\r\n        if result > best_result:\r\n            best_result = result\r\n            save_model(model, args.output_model_path)\r\n        else:\r\n            continue\r\n\r\n    # Evaluation phase.\r\n    if args.test_path is not None:\r\n        print(""Test set evaluation."")\r\n        model = load_model(model, args.output_model_path)\r\n        evaluate(args, True)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
run_ner.py,23,"b'# -*- encoding:utf -*-\n""""""\n  This script provides an example to wrap UER-py for NER.\n""""""\nimport random\nimport argparse\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom uer.model_builder import build_model\nfrom uer.utils.config import load_hyperparam\nfrom uer.utils.optimizers import *\nfrom uer.utils.constants import *\nfrom uer.utils.vocab import Vocab\nfrom uer.utils.seed import set_seed\nfrom uer.model_saver import save_model\nfrom uer.model_loader import load_model\n\n\nclass BertTagger(nn.Module):\n    def __init__(self, args, model):\n        super(BertTagger, self).__init__()\n        self.embedding = model.embedding\n        self.encoder = model.encoder\n        self.target = model.target\n        self.labels_num = args.labels_num\n        self.output_layer = nn.Linear(args.hidden_size, self.labels_num)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self, src, label, mask):\n        """"""\n        Args:\n            src: [batch_size x seq_length]\n            label: [batch_size x seq_length]\n            mask: [batch_size x seq_length]\n\n        Returns:\n            loss: Sequence labeling loss.\n            correct: Number of labels that are predicted correctly.\n            predict: Predicted label.\n            label: Gold label.\n        """"""\n        # Embedding.\n        emb = self.embedding(src, mask)\n        # Encoder.\n        output = self.encoder(emb, mask)\n        # Target.\n        output = self.output_layer(output)\n\n        output = output.contiguous().view(-1, self.labels_num)\n        output = self.softmax(output)\n\n        label = label.contiguous().view(-1,1)\n        label_mask = (label > 0).float().to(torch.device(label.device))\n        one_hot = torch.zeros(label_mask.size(0),  self.labels_num). \\\n                  to(torch.device(label.device)). \\\n                  scatter_(1, label, 1.0)\n\n        numerator = -torch.sum(output * one_hot, 1)\n        label_mask = label_mask.contiguous().view(-1)\n        label = label.contiguous().view(-1)\n        numerator = torch.sum(label_mask * numerator)\n        denominator = torch.sum(label_mask) + 1e-6\n        loss = numerator / denominator\n        predict = output.argmax(dim=-1)\n        correct = torch.sum(\n            label_mask * (predict.eq(label)).float()\n        )\n        \n        return loss, correct, predict, label\n\n\ndef main():\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # Path options.\n    parser.add_argument(""--pretrained_model_path"", default=None, type=str,\n                        help=""Path of the pretrained model."")\n    parser.add_argument(""--output_model_path"", default=""./models/ner_model.bin"", type=str,\n                        help=""Path of the output model."")\n    parser.add_argument(""--vocab_path"", type=str, required=True,\n                        help=""Path of the vocabulary file."")\n    parser.add_argument(""--train_path"", type=str, required=True,\n                        help=""Path of the trainset."")\n    parser.add_argument(""--dev_path"", type=str, required=True,\n                        help=""Path of the devset."")\n    parser.add_argument(""--test_path"", type=str,\n                        help=""Path of the testset."")\n    parser.add_argument(""--config_path"", default=""./models/bert_base_config.json"", type=str,\n                        help=""Path of the config file."")\n\n    # Model options.\n    parser.add_argument(""--batch_size"", type=int, default=32,\n                        help=""Batch_size."")\n    parser.add_argument(""--seq_length"", default=128, type=int,\n                        help=""Sequence length."")\n    parser.add_argument(""--embedding"", choices=[""bert"", ""word""], default=""bert"",\n                        help=""Emebdding type."")\n    parser.add_argument(""--encoder"", choices=[""bert"", ""lstm"", ""gru"", \\\n                                                   ""cnn"", ""gatedcnn"", ""attn"", \\\n                                                   ""rcnn"", ""crnn"", ""gpt"", ""bilstm""], \\\n                                                   default=""bert"", help=""Encoder type."")\n    parser.add_argument(""--bidirectional"", action=""store_true"", help=""Specific to recurrent model."")\n    \n    # Subword options.\n    parser.add_argument(""--subword_type"", choices=[""none"", ""char""], default=""none"",\n                        help=""Subword feature type."")\n    parser.add_argument(""--sub_vocab_path"", type=str, default=""models/sub_vocab.txt"",\n                        help=""Path of the subword vocabulary file."")\n    parser.add_argument(""--subencoder"", choices=[""avg"", ""lstm"", ""gru"", ""cnn""], default=""avg"",\n                        help=""Subencoder type."")\n    parser.add_argument(""--sub_layers_num"", type=int, default=2, help=""The number of subencoder layers."")\n\n    # Optimizer options.\n    parser.add_argument(""--learning_rate"", type=float, default=2e-5,\n                        help=""Learning rate."")\n    parser.add_argument(""--warmup"", type=float, default=0.1,\n                        help=""Warm up value."")\n    parser.add_argument(\'--fp16\', action=\'store_true\',\n                        help=""Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit"")\n    parser.add_argument(\'--fp16_opt_level\', type=str, default=\'O1\',\n                        help=""For fp16: Apex AMP optimization level selected in [\'O0\', \'O1\', \'O2\', and \'O3\'].""\n                             ""See details at https://nvidia.github.io/apex/amp.html"")\n\n    # Training options.\n    parser.add_argument(""--dropout"", type=float, default=0.1,\n                        help=""Dropout."")\n    parser.add_argument(""--epochs_num"", type=int, default=3,\n                        help=""Number of epochs."")\n    parser.add_argument(""--report_steps"", type=int, default=100,\n                        help=""Specific steps to print prompt."")\n    parser.add_argument(""--seed"", type=int, default=7,\n                        help=""Random seed."")\n\n    args = parser.parse_args()\n\n    # Load the hyperparameters of the config file.\n    args = load_hyperparam(args)\n\n    set_seed(args.seed)\n\n    labels_map = {""[PAD]"": 0}\n    begin_ids = []\n\n    # Find tagging labels\n    with open(args.train_path, mode=""r"", encoding=""utf-8"") as f:\n        for line_id, line in enumerate(f):\n            if line_id == 0:\n                continue\n            labels = line.strip().split(""\\t"")[1].split()\n            for l in labels:\n                if l not in labels_map:\n                    if l.startswith(""B"") or l.startswith(""S""):\n                        begin_ids.append(len(labels_map))\n                    labels_map[l] = len(labels_map)\n    \n\n    print(""Labels: "", labels_map)\n    args.labels_num = len(labels_map)\n\n    # Load vocabulary.\n    vocab = Vocab()\n    vocab.load(args.vocab_path)\n    args.vocab = vocab\n\n    # Build bert model.\n    # A pseudo target is added.\n    args.target = ""bert""\n    model = build_model(args)\n\n    # Load or initialize parameters.\n    if args.pretrained_model_path is not None:\n        # Initialize with pretrained model.\n        model.load_state_dict(torch.load(args.pretrained_model_path), strict=False)  \n    else:\n        # Initialize with normal distribution.\n        for n, p in list(model.named_parameters()):\n            if \'gamma\' not in n and \'beta\' not in n:\n                p.data.normal_(0, 0.02)\n    \n    # Build sequence labeling model.\n    model = BertTagger(args, model)\n\n    # For simplicity, we use DataParallel wrapper to use multiple GPUs.\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model = model.to(device)\n\n    # Datset loader.\n    def batch_loader(batch_size, input_ids, label_ids, mask_ids):\n        instances_num = input_ids.size()[0]\n        for i in range(instances_num // batch_size):\n            input_ids_batch = input_ids[i*batch_size: (i+1)*batch_size, :]\n            label_ids_batch = label_ids[i*batch_size: (i+1)*batch_size, :]\n            mask_ids_batch = mask_ids[i*batch_size: (i+1)*batch_size, :]\n            yield input_ids_batch, label_ids_batch, mask_ids_batch\n        if instances_num > instances_num // batch_size * batch_size:\n            input_ids_batch = input_ids[instances_num//batch_size*batch_size:, :]\n            label_ids_batch = label_ids[instances_num//batch_size*batch_size:, :]\n            mask_ids_batch = mask_ids[instances_num//batch_size*batch_size:, :]\n            yield input_ids_batch, label_ids_batch, mask_ids_batch\n\n    # Read dataset.\n    def read_dataset(path):\n        dataset = []\n        with open(path, mode=""r"", encoding=""utf-8"") as f:\n            f.readline()\n            tokens, labels = [], []\n            for line_id, line in enumerate(f):\n                tokens, labels = line.strip().split(""\\t"")\n                tokens = [vocab.get(t) for t in tokens.split("" "")]\n                labels = [labels_map[l] for l in labels.split("" "")]\n                mask = [1] * len(tokens)\n                if len(tokens) > args.seq_length:\n                    tokens = tokens[:args.seq_length]\n                    labels = labels[:args.seq_length]\n                    mask = mask[:args.seq_length]\n                while len(tokens) < args.seq_length:\n                    tokens.append(0)\n                    labels.append(0)\n                    mask.append(0)\n                dataset.append([tokens, labels, mask])\n        \n        return dataset\n\n    # Evaluation function.\n    def evaluate(args, is_test):\n        if is_test:\n            dataset = read_dataset(args.test_path)\n        else:\n            dataset = read_dataset(args.dev_path)\n\n        input_ids = torch.LongTensor([sample[0] for sample in dataset])\n        label_ids = torch.LongTensor([sample[1] for sample in dataset])\n        mask_ids = torch.LongTensor([sample[2] for sample in dataset])\n\n        instances_num = input_ids.size(0)\n        batch_size = args.batch_size\n\n        if is_test:\n            print(""Batch size: "", batch_size)\n            print(""The number of test instances:"", instances_num)\n\n    \n        correct = 0\n        gold_entities_num = 0\n        pred_entities_num = 0\n\n        confusion = torch.zeros(len(labels_map), len(labels_map), dtype=torch.long)\n\n        model.eval()\n\n        for i, (input_ids_batch, label_ids_batch, mask_ids_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids)):\n            input_ids_batch = input_ids_batch.to(device)\n            label_ids_batch = label_ids_batch.to(device)\n            mask_ids_batch = mask_ids_batch.to(device)\n            loss, _, pred, gold = model(input_ids_batch, label_ids_batch, mask_ids_batch)\n            \n            for j in range(gold.size()[0]):\n                if gold[j].item() in begin_ids:\n                    gold_entities_num += 1\n \n            for j in range(pred.size()[0]):\n                if pred[j].item() in begin_ids and gold[j].item() != labels_map[""[PAD]""]:\n                    pred_entities_num += 1\n\n            pred_entities_pos = []\n            gold_entities_pos = []\n            start, end = 0, 0\n\n            for j in range(gold.size()[0]):\n                if gold[j].item() in begin_ids:\n                    start = j\n                    for k in range(j+1, gold.size()[0]):\n                        if gold[k].item() == labels_map[""[PAD]""] or gold[k].item() == labels_map[""O""] or gold[k].item() in begin_ids:\n                            end = k - 1\n                            break\n                    else:\n                        end = gold.size()[0] - 1\n                    gold_entities_pos.append((start, end))\n            \n            for j in range(pred.size()[0]):\n                if pred[j].item() in begin_ids and gold[j].item() != labels_map[""[PAD]""]:\n                    start = j\n                    for k in range(j+1, pred.size()[0]):\n                        if pred[k].item() == labels_map[""[PAD]""] or pred[k].item() == labels_map[""O""] or pred[k].item() in begin_ids:\n                            end = k - 1\n                            break\n                    else:\n                        end = pred.size()[0] - 1\n                    pred_entities_pos.append((start, end))\n\n            for entity in pred_entities_pos:\n                if entity not in gold_entities_pos:\n                    continue\n                for j in range(entity[0], entity[1]+1):\n                    if gold[j].item() != pred[j].item():\n                        break\n                else: \n                    correct += 1\n\n        print(""Report precision, recall, and f1:"")\n        p = correct/pred_entities_num\n        r = correct/gold_entities_num\n        f1 = 2*p*r/(p+r)\n        print(""{:.3f}, {:.3f}, {:.3f}"".format(p,r,f1))\n\n        return f1\n\n    # Training phase.\n    print(""Start training."")\n    instances = read_dataset(args.train_path)\n\n    input_ids = torch.LongTensor([ins[0] for ins in instances])\n    label_ids = torch.LongTensor([ins[1] for ins in instances])\n    mask_ids = torch.LongTensor([ins[2] for ins in instances])\n\n    instances_num = input_ids.size(0)\n    batch_size = args.batch_size\n    train_steps = int(instances_num * args.epochs_num / batch_size) + 1\n\n    print(""Batch size: "", batch_size)\n    print(""The number of training instances:"", instances_num)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\'bias\', \'gamma\', \'beta\']\n    optimizer_grouped_parameters = [\n                {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay_rate\': 0.01},\n                {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay_rate\': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, correct_bias=False)\n    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=train_steps*args.warmup, t_total=train_steps)\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    if torch.cuda.device_count() > 1:\n        print(""{} GPUs are available. Let\'s use them."".format(torch.cuda.device_count()))\n        model = torch.nn.DataParallel(model)\n\n    total_loss = 0.\n    f1 = 0.0\n    best_f1 = 0.0\n\n    for epoch in range(1, args.epochs_num+1):\n        model.train()\n        for i, (input_ids_batch, label_ids_batch, mask_ids_batch) in enumerate(batch_loader(batch_size, input_ids, label_ids, mask_ids)):\n            model.zero_grad()\n\n            input_ids_batch = input_ids_batch.to(device)\n            label_ids_batch = label_ids_batch.to(device)\n            mask_ids_batch = mask_ids_batch.to(device)\n\n            loss, _, _, _ = model(input_ids_batch, label_ids_batch, mask_ids_batch)\n            if torch.cuda.device_count() > 1:\n                loss = torch.mean(loss)\n            total_loss += loss.item()\n            if (i + 1) % args.report_steps == 0:\n                print(""Epoch id: {}, Training steps: {}, Avg loss: {:.3f}"".format(epoch, i+1, total_loss / args.report_steps))\n                total_loss = 0.\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            optimizer.step()\n            scheduler.step()\n\n        f1 = evaluate(args, False)\n        if f1 > best_f1:\n            best_f1 = f1\n            save_model(model, args.output_model_path)\n        else:\n            continue\n\n    # Evaluation phase.\n    if args.test_path is not None:\n        print(""Test set evaluation."")\n        model = load_model(model, args.output_model_path)\n        evaluate(args, True)\n\n\nif __name__ == ""__main__"":\n    main()\n\n'"
scripts/average_model.py,2,"b'# -*- encoding:utf-8 -*-\nimport sys\nimport os\nimport torch\nimport argparse\n\nuer_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ""..""))\nsys.path.append(uer_dir)\n\nfrom uer.model_saver import save_model\n\n\ndef average_models(model_list_path):\n    for i, model_path in enumerate(model_list_path):\n        model = torch.load(model_path)\n        if i == 0:\n            avg_model = model\n        else:\n            for k, v in avg_model.items():\n                avg_model[k].mul_(i).add_(model[k]).div_(i+1)\n     \n    return avg_model\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description="""")\n    parser.add_argument(""--model_list_path"", nargs=""+"", required=True,\n                        help=""Path of the input model list."")\n    parser.add_argument(""--output_model_path"", required=True,\n                        help=""Path of the output model."")\n    args = parser.parse_args()\n\n    avg_model = average_models(args.model_list_path)\n    torch.save(avg_model, args.output_model_path)\n'"
scripts/build_vocab.py,0,"b'# -*- encoding:utf-8 -*-\r\n""""""\r\nBuild vocabulary with given tokenizer\r\n""""""\r\n\r\nimport sys\r\nimport os\r\nimport argparse\r\n\r\nuer_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ""..""))\r\nsys.path.append(uer_dir)\r\n\r\nfrom uer.utils.tokenizer import *\r\nfrom uer.utils.vocab import Vocab\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n\r\n    parser.add_argument(""--corpus_path"", required=True)\r\n    parser.add_argument(""--vocab_path"", required=True)\r\n    parser.add_argument(""--workers_num"", type=int, default=1, help=""The number of processes to build vocabulary."")\r\n    parser.add_argument(""--min_count"", type=int, default=1, help=""The minimum count of words retained in the vocabulary."")\r\n    # Tokenizer options.\r\n    parser.add_argument(""--tokenizer"", choices=[""char"", ""space""], default=""space"",\r\n                        help=""Specify the tokenizer."" \r\n                             ""Char tokenizer segments sentences into characters.""\r\n                             ""Space tokenizer segments sentences into words according to space.""\r\n                             )\r\n\r\n    args = parser.parse_args()\r\n\r\n    # Build tokenizer.\r\n    tokenizer = globals()[args.tokenizer.capitalize() + ""Tokenizer""](args)\r\n\r\n    # Build and save vocabulary.\r\n    vocab = Vocab()\r\n    vocab.build(args.corpus, tokenizer, args.workers_num, args.min_count)\r\n    vocab.save(args.vocab_path)\r\n'"
scripts/check_model.py,1,"b'# -*- encoding:utf -*-\nimport os\nimport sys\nimport torch\nimport argparse\n\nbert_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ""..""))\nsys.path.append(bert_dir)\n\nfrom bert.model_builder import build_model\nfrom bert.utils.vocab import Vocab\nfrom bert.utils.config import *\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(""--input_model_path"", type=str)\n    parser.add_argument(""--vocab_path"", type=str)\n    parser.add_argument(""--config_path"", type=str, default=""./model.config"")\n    args = parser.parse_args()\n    args = load_hyperparam(args)\n\n    input_model = torch.load(args.input_model_path)\n    prefix = ""module.""\n    for k, v in input_model.items():\n        if prefix in k:\n            print(""Multi-GPU version."")\n            break\n        else:\n            print(""Single-GPU version."")\n            break\n\n    print(""Check model loading operation."")\n    vocab = Vocab()\n    vocab.load(args.vocab_path)\n    test_model = build_model(args, len(vocab))\n    try:\n        test_model.load_state_dict(input_model, strict=True)\n    except RuntimeError:\n        print(""Model loading test Failed. Please check if the input model."")\n    else:\n        print(""Pass the check. Test Done."")\n'"
scripts/cloze_test.py,5,"b'# -*- encoding:utf-8 -*-\n""""""\n  This script provides an exmaple to wrap UER-py for cloze test.\n  We randomly mask some characters and use BERT to predict.\n""""""\nimport os\nimport sys\nimport torch\nimport argparse\nimport random\n\nuer_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ""..""))\nsys.path.append(uer_dir)\n\nfrom uer.utils.act_fun import gelu\nfrom uer.utils.constants import *\nfrom uer.utils.tokenizer import *\nfrom uer.layers.layer_norm import LayerNorm\nfrom uer.utils.config import load_hyperparam\nfrom uer.utils.vocab import Vocab\nfrom uer.model_builder import build_model\n\n\nclass ClozeModel(torch.nn.Module):\n    def __init__(self, args, model):\n        super(ClozeModel, self).__init__()\n        self.embedding = model.embedding\n        self.encoder = model.encoder\n        self.target = model.target\n        # Open eval mode.\n        self.eval()\n\n    def forward(self, src, seg):\n        emb = self.embedding(src, seg)\n        output = self.encoder(emb, seg)\n        output = gelu(self.target.mlm_linear_1(output))\n        output = self.target.layer_norm(output)\n        output = self.target.mlm_linear_2(output)\n        prob = torch.nn.Softmax(dim=-1)(output)\n        return prob\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    # Path options.\n    parser.add_argument(""--pretrained_model_path"", default=""models/google_model.bin"", type=str, \n                        help=""Path of the pretrained model."")\n    parser.add_argument(""--vocab_path"", type=str, required=True,\n                        help=""Path of the vocabulary file."")\n    parser.add_argument(""--input_path"", type=str, default=""datasets/cloze_input.txt"", \n                        help=""Path of the input file for cloze test. One sentence per line."")\n    parser.add_argument(""--output_path"", type=str, default=""datasets/cloze_output.txt"", \n                        help=""Path of the output file for cloze test."")\n    parser.add_argument(""--config_path"", default=""models/bert_base_config.json"", type=str,\n                        help=""Path of the config file."")\n\n    # Model options.\n    parser.add_argument(""--batch_size"", type=int, default=64,\n                        help=""Batch size."")\n    parser.add_argument(""--seq_length"", type=int, default=100,\n                        help=""Sequence length."")\n    parser.add_argument(""--embedding"", choices=[""bert"", ""word""], default=""bert"",\n                        help=""Emebdding type."")\n    parser.add_argument(""--encoder"", choices=[""bert"", ""lstm"", ""gru"", \\\n                                                   ""cnn"", ""gatedcnn"", ""attn"", \\\n                                                   ""rcnn"", ""crnn"", ""gpt""], \\\n                                                   default=""bert"", help=""Encoder type."")\n    parser.add_argument(""--bidirectional"", action=""store_true"", help=""Specific to recurrent model."")\n    parser.add_argument(""--target"", choices=[""bert"", ""mlm""], default=""bert"",\n                        help=""The training target of the pretraining model."")\n\n    # Subword options.\n    parser.add_argument(""--subword_type"", choices=[""none"", ""char""], default=""none"",\n                        help=""Subword feature type."")\n    parser.add_argument(""--sub_vocab_path"", type=str, default=""models/sub_vocab.txt"",\n                        help=""Path of the subword vocabulary file."")\n    parser.add_argument(""--subencoder_type"", choices=[""avg"", ""lstm"", ""gru"", ""cnn""], default=""avg"",\n                        help=""Subencoder type."")\n\n    # Tokenizer options.\n    parser.add_argument(""--tokenizer"", choices=[""bert"", ""char"", ""space""], default=""bert"",\n                        help=""Specify the tokenizer.""\n                             ""Original Google BERT uses bert tokenizer on Chinese corpus.""\n                             ""Char tokenizer segments sentences into characters.""\n                             ""Space tokenizer segments sentences into words according to space.""\n                             )\n\n    # Output options.\n    parser.add_argument(""--topn"", type=int, default=10,\n                        help=""Print top n nearest neighbours."")\n    \n    args = parser.parse_args()\n\n    # Load the hyperparameters from the config file.\n    args = load_hyperparam(args)\n\n    # Load Vocabulary\n    vocab = Vocab()\n    vocab.load(args.vocab_path)\n    args.vocab = vocab\n\n    # Build bert model.\n    model = build_model(args)\n\n    # Load pretrained model.\n    pretrained_model = torch.load(args.pretrained_model_path)\n    model.load_state_dict(pretrained_model, strict=False)\n\n    model = ClozeModel(args, model)\n\n    # Build tokenizer\n    tokenizer = globals()[args.tokenizer.capitalize() + ""Tokenizer""](args)\n\n    # Construct input datasets.\n    def mask_token(tokens):\n        """"""\n        Mask a random token for prediction.\n        """"""\n        start = 1\n        end = len(tokens) if len(tokens) < args.seq_length else args.seq_length\n        mask_pos = random.randint(start, end-1)\n        token = tokens[mask_pos]\n        tokens[mask_pos] = MASK_ID\n        return (tokens, mask_pos, token)\n\n    input_ids = []\n    seg_ids = []\n    mask_positions = [] # The position of the masked word.\n    label_ids = [] # The id of the masked word.\n\n    with open(args.input_path, mode=""r"", encoding=""utf-8"") as f:\n        for line in f:        \n            tokens = [vocab.get(t) for t in tokenizer.tokenize(line.strip())]\n            if len(tokens) == 0:\n                continue\n            tokens = [CLS_ID] + tokens\n            tokens, mask_pos, label = mask_token(tokens)\n\n            seg = [1] * len(tokens)\n            if len(tokens) > args.seq_length:\n                tokens = tokens[:args.seq_length]\n                seg = seg[:args.seq_length]\n            while len(tokens) < args.seq_length:\n                tokens.append(PAD_ID)\n                seg.append(PAD_ID)\n            input_ids.append(tokens)\n            seg_ids.append(seg)\n\n            mask_positions.append(mask_pos)\n            label_ids.append(label)\n\n    input_ids = torch.LongTensor(input_ids)\n    seg_ids = torch.LongTensor(seg_ids)\n\n    def batch_loader(batch_size, input_ids, seg_ids, mask_positions, label_ids):\n        instances_num = input_ids.size(0)\n        for i in range(instances_num // batch_size):\n            input_ids_batch = input_ids[i*batch_size : (i+1)*batch_size]\n            seg_ids_batch = seg_ids[i*batch_size : (i+1)*batch_size]\n            mask_positions_batch = mask_positions[i*batch_size : (i+1)*batch_size]\n            label_ids_batch = label_ids[i*batch_size : (i+1)*batch_size]\n            yield input_ids_batch, seg_ids_batch, mask_positions_batch, label_ids_batch\n\n        if instances_num > instances_num // batch_size * batch_size:\n            input_ids_batch = input_ids[instances_num//batch_size*batch_size:]\n            seg_ids_batch = seg_ids[instances_num//batch_size*batch_size:]\n            mask_positions_batch = mask_positions[instances_num//batch_size*batch_size:]\n            label_ids_batch = label_ids[instances_num//batch_size*batch_size:]\n            yield input_ids_batch, seg_ids_batch, mask_positions_batch, label_ids_batch\n\n    f_output = open(args.output_path, mode=""w"", encoding=""utf-8"")\n               \n    for i, (input_ids_batch, seg_ids_batch, mask_positions_batch, label_ids_batch) in \\\n        enumerate(batch_loader(args.batch_size, input_ids, seg_ids, mask_positions, label_ids)):\n        prob = model(input_ids_batch, seg_ids_batch)\n\n        for j, p in enumerate(mask_positions_batch):\n            topn_tokens = (-prob[j][p]).argsort()[:args.topn]\n\n            sentence = """".join([vocab.i2w[token_id] for token_id in input_ids_batch[j] if token_id != 0])\n            pred_tokens = "" "".join(vocab.i2w[token_id] for token_id in topn_tokens)\n            label_token = vocab.i2w[label_ids_batch[j]]\n            f_output.write(sentence + \'\\n\')\n            f_output.write(""Predicted answer: "" + pred_tokens + \'\\n\')\n            f_output.write(""Correct answer: "" + label_token + \'\\n\')\n            f_output.write(""\\n"")\n    \n    f_output.close()\n'"
scripts/convert_bert_from_google_to_uer.py,2,"b'# coding=utf-8\r\n""""""Convert BERT checkpoint to UER.""""""\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nimport re\r\nimport sys\r\nimport argparse\r\nimport tensorflow as tf\r\nimport torch\r\nimport numpy as np\r\nimport collections\r\nfrom tensorflow.python import pywrap_tensorflow\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n    ## Required parameters\r\n    parser.add_argument(""--layers_num"",\r\n                        type=int,\r\n                        default=""12"",\r\n                        help=""."")\r\n    parser.add_argument(""--input_model_path"",\r\n                        default=""models/bert_base_chinese/bert_model.ckpt"",\r\n                        type=str,\r\n                        help=""."")\r\n    parser.add_argument(""--output_model_path"",\r\n                        default=None,\r\n                        type=str,\r\n                        required=True,\r\n                        help=""Path to the output PyTorch model.e.g.uer_model.bin"")\r\n    parser.add_argument(""--target"",\r\n                        default=None,\r\n                        type=str,\r\n                        required=False,\r\n                        help=""convert erine to uer "")\r\n\r\n    args = parser.parse_args()\r\n    # Read data from checkpoint file\r\n    reader = pywrap_tensorflow.NewCheckpointReader(args.input_model_path)\r\n    var_to_shape_map = reader.get_variable_to_shape_map()\r\n    # Print tensor name and values\r\n    count = 0\r\n    input_model = collections.OrderedDict()\r\n    tensors_to_transopse = (\r\n        ""dense/kernel"",\r\n        ""attention/self/query"",\r\n        ""attention/self/key"",\r\n        ""attention/self/value""\r\n    )\r\n    for key in var_to_shape_map:\r\n        torch_tensor = reader.get_tensor(key)\r\n\r\n        if any([x in key for x in tensors_to_transopse]):\r\n            torch_tensor = torch_tensor.T\r\n        if key == ""bert/embeddings/token_type_embeddings"":\r\n            col_dim = torch_tensor.shape[1]\r\n            sess = tf.Session()\r\n            zeros_var = tf.Variable(tf.zeros([1, col_dim], dtype=tf.float32), name=\'zeros_var\')\r\n            sess.run(zeros_var.initializer)\r\n            torch_tensor =sess.run(tf.concat([sess.run(zeros_var),torch_tensor], 0))\r\n        input_model[key] = torch.Tensor(torch_tensor)\r\n        count += 1\r\n\r\n    output_model = collections.OrderedDict()\r\n    output_model[""embedding.word_embedding.weight""] = input_model[""bert/embeddings/word_embeddings""]\r\n    output_model[""embedding.position_embedding.weight""] = input_model[""bert/embeddings/position_embeddings""][:512]\r\n    output_model[""embedding.segment_embedding.weight""] = input_model[""bert/embeddings/token_type_embeddings""]\r\n    output_model[""embedding.layer_norm.gamma""] = input_model[""bert/embeddings/LayerNorm/gamma""]\r\n    output_model[""embedding.layer_norm.beta""] = input_model[""bert/embeddings/LayerNorm/beta""]\r\n\r\n    for i in range(args.layers_num):\r\n        output_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.0.weight""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/attention/self/query/kernel""]\r\n        output_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.0.bias""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/attention/self/query/bias""]\r\n        output_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.1.weight""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/attention/self/key/kernel""]\r\n        output_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.1.bias""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/attention/self/key/bias""]\r\n        output_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.2.weight""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/attention/self/value/kernel""]\r\n        output_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.2.bias""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/attention/self/value/bias""]\r\n        output_model[""encoder.transformer."" + str(i) + "".self_attn.final_linear.weight""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/attention/output/dense/kernel""]\r\n        output_model[""encoder.transformer."" + str(i) + "".self_attn.final_linear.bias""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/attention/output/dense/bias""]\r\n        output_model[""encoder.transformer."" + str(i) + "".layer_norm_1.gamma""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/attention/output/LayerNorm/gamma""]\r\n        output_model[""encoder.transformer."" + str(i) + "".layer_norm_1.beta""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/attention/output/LayerNorm/beta""]\r\n        output_model[""encoder.transformer."" + str(i) + "".feed_forward.linear_1.weight""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/intermediate/dense/kernel""]\r\n        output_model[""encoder.transformer."" + str(i) + "".feed_forward.linear_1.bias""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/intermediate/dense/bias""]\r\n        output_model[""encoder.transformer."" + str(i) + "".feed_forward.linear_2.weight""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/output/dense/kernel""]\r\n        output_model[""encoder.transformer."" + str(i) + "".feed_forward.linear_2.bias""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/output/dense/bias""]\r\n        output_model[""encoder.transformer."" + str(i) + "".layer_norm_2.gamma""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/output/LayerNorm/gamma""]\r\n        output_model[""encoder.transformer."" + str(i) + "".layer_norm_2.beta""] = input_model[\r\n            ""bert/encoder/layer_"" + str(i) + ""/output/LayerNorm/beta""]\r\n\r\n    output_model[""target.nsp_linear_1.weight""] = input_model[""bert/pooler/dense/kernel""]\r\n    output_model[""target.nsp_linear_1.bias""] = input_model[""bert/pooler/dense/bias""]\r\n    if args.target is None:\r\n        output_model[""target.nsp_linear_2.weight""] = input_model[""cls/seq_relationship/output_weights""]\r\n        output_model[""target.nsp_linear_2.bias""] = input_model[""cls/seq_relationship/output_bias""]\r\n        output_model[""target.mlm_linear_1.weight""] = input_model[""cls/predictions/transform/dense/kernel""]\r\n        output_model[""target.mlm_linear_1.bias""] = input_model[""cls/predictions/transform/dense/bias""]\r\n        output_model[""target.layer_norm.gamma""] = input_model[""cls/predictions/transform/LayerNorm/gamma""]\r\n        output_model[""target.layer_norm.beta""] = input_model[""cls/predictions/transform/LayerNorm/beta""]\r\n        output_model[""target.mlm_linear_2.weight""] = input_model[""bert/embeddings/word_embeddings""]\r\n        output_model[""target.mlm_linear_2.bias""] =input_model[""cls/predictions/output_bias""]\r\n\r\n    torch.save(output_model,args.output_model_path)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
scripts/convert_bert_from_huggingface_to_uer.py,3,"b'import torch\nimport argparse\nimport collections\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(""--input_model_path"", type=str, default=""pytorch_model.bin"",\n                        help=""."")\nparser.add_argument(""--output_model_path"", type=str, default=""google_model.bin"",\n                        help=""."")\nparser.add_argument(""--layers_num"", type=int, default=12, help=""."")\n\nargs = parser.parse_args()\npath = args.input_model_path\n\ninput_model = torch.load(args.input_model_path, map_location=\'cpu\')\n\noutput_model = collections.OrderedDict()\n\noutput_model[""embedding.word_embedding.weight""] = input_model[""bert.embeddings.word_embeddings.weight""]\noutput_model[""embedding.position_embedding.weight""] = input_model[""bert.embeddings.position_embeddings.weight""]\noutput_model[""embedding.segment_embedding.weight""] = torch.cat((torch.Tensor([[0]*input_model[""bert.embeddings.token_type_embeddings.weight""].size()[1]]), input_model[""bert.embeddings.token_type_embeddings.weight""]), dim=0)\noutput_model[""embedding.layer_norm.gamma""] = input_model[""bert.embeddings.LayerNorm.weight""]\noutput_model[""embedding.layer_norm.beta""] = input_model[""bert.embeddings.LayerNorm.bias""]\n\nfor i in range(args.layers_num):\n    output_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.0.weight""] = input_model[""bert.encoder.layer."" + str(i) + "".attention.self.query.weight""]\n    output_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.0.bias""] = input_model[""bert.encoder.layer."" + str(i) + "".attention.self.query.bias""]\n    output_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.1.weight""] = input_model[""bert.encoder.layer."" + str(i) + "".attention.self.key.weight""]\n    output_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.1.bias""] = input_model[""bert.encoder.layer."" + str(i) + "".attention.self.key.bias""]\n    output_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.2.weight""] = input_model[""bert.encoder.layer."" + str(i) + "".attention.self.value.weight""]\n    output_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.2.bias""] = input_model[""bert.encoder.layer."" + str(i) + "".attention.self.value.bias""]\n    output_model[""encoder.transformer."" + str(i) + "".self_attn.final_linear.weight""] = input_model[""bert.encoder.layer."" + str(i) + "".attention.output.dense.weight""]\n    output_model[""encoder.transformer."" + str(i) + "".self_attn.final_linear.bias""] = input_model[""bert.encoder.layer."" + str(i) + "".attention.output.dense.bias""]\n    output_model[""encoder.transformer."" + str(i) + "".layer_norm_1.gamma""] = input_model[""bert.encoder.layer."" + str(i) + "".attention.output.LayerNorm.weight""]\n    output_model[""encoder.transformer."" + str(i) + "".layer_norm_1.beta""] = input_model[""bert.encoder.layer."" + str(i) + "".attention.output.LayerNorm.bias""]\n    output_model[""encoder.transformer."" + str(i) + "".feed_forward.linear_1.weight""] = input_model[""bert.encoder.layer."" + str(i) + "".intermediate.dense.weight""]\n    output_model[""encoder.transformer."" + str(i) + "".feed_forward.linear_1.bias""] = input_model[""bert.encoder.layer."" + str(i) + "".intermediate.dense.bias""]\n    output_model[""encoder.transformer."" + str(i) + "".feed_forward.linear_2.weight""] = input_model[""bert.encoder.layer."" + str(i) + "".output.dense.weight""]\n    output_model[""encoder.transformer."" + str(i) + "".feed_forward.linear_2.bias""] = input_model[""bert.encoder.layer."" + str(i) + "".output.dense.bias""]\n    output_model[""encoder.transformer."" + str(i) + "".layer_norm_2.gamma""] = input_model[""bert.encoder.layer."" + str(i) + "".output.LayerNorm.weight""]\n    output_model[""encoder.transformer."" + str(i) + "".layer_norm_2.beta""] = input_model[""bert.encoder.layer."" + str(i) + "".output.LayerNorm.bias""]\n\noutput_model[""target.nsp_linear_1.weight""] = input_model[""bert.pooler.dense.weight""]\noutput_model[""target.nsp_linear_1.bias""] = input_model[""bert.pooler.dense.bias""]\noutput_model[""target.nsp_linear_2.weight""] = input_model[""cls.seq_relationship.weight""]\noutput_model[""target.nsp_linear_2.bias""] = input_model[""cls.seq_relationship.bias""]\noutput_model[""target.mlm_linear_1.weight""] = input_model[""cls.predictions.transform.dense.weight""]\noutput_model[""target.mlm_linear_1.bias""] = input_model[""cls.predictions.transform.dense.bias""]\noutput_model[""target.layer_norm.gamma""] = input_model[""cls.predictions.transform.LayerNorm.weight""]\noutput_model[""target.layer_norm.beta""] = input_model[""cls.predictions.transform.LayerNorm.bias""]\noutput_model[""target.mlm_linear_2.weight""] = input_model[""cls.predictions.decoder.weight""]\noutput_model[""target.mlm_linear_2.bias""] = input_model[""cls.predictions.bias""]\n\n\ntorch.save(output_model, args.output_model_path)\n'"
scripts/convert_bert_from_uer_to_google.py,1,"b'import os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport torch\r\nimport argparse\r\nimport collections\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n    parser.add_argument(""--layers_num"", type=int, default=""12"",\r\n                        help=""."")\r\n    parser.add_argument(""--input_model_path"", type=str, default=""models/google_model.bin"",\r\n                        help=""."")\r\n    parser.add_argument(""--output_model_path"",\r\n                        type=str,\r\n                        default=""models/bert_base_chinese.ckpt"",\r\n                        help=""."")\r\n\r\n    args = parser.parse_args()\r\n\r\n    input_model = torch.load(args.input_model_path)\r\n\r\n    session = tf.Session()\r\n    output_model = collections.OrderedDict()\r\n\r\n    output_model[""bert/embeddings/word_embeddings""] = input_model[""embedding.word_embedding.weight""]\r\n    output_model[""bert/embeddings/position_embeddings""] = input_model[""embedding.position_embedding.weight""]\r\n    output_model[""bert/embeddings/token_type_embeddings""] = input_model[""embedding.segment_embedding.weight""][1:, :]\r\n    output_model[""bert/embeddings/LayerNorm/gamma""] = input_model[""embedding.layer_norm.gamma""]\r\n    output_model[""bert/embeddings/LayerNorm/beta""] = input_model[""embedding.layer_norm.beta""]\r\n\r\n    for i in range(args.layers_num):\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/attention/self/query/kernel""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".self_attn.linear_layers.0.weight""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/attention/self/query/bias""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".self_attn.linear_layers.0.bias""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/attention/self/key/kernel""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".self_attn.linear_layers.1.weight""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/attention/self/key/bias""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".self_attn.linear_layers.1.bias""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/attention/self/value/kernel""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".self_attn.linear_layers.2.weight""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/attention/self/value/bias""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".self_attn.linear_layers.2.bias""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/attention/output/dense/kernel""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".self_attn.final_linear.weight""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/attention/output/dense/bias""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".self_attn.final_linear.bias""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/attention/output/LayerNorm/gamma""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".layer_norm_1.gamma""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/attention/output/LayerNorm/beta""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".layer_norm_1.beta""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/intermediate/dense/kernel""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".feed_forward.linear_1.weight""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/intermediate/dense/bias""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".feed_forward.linear_1.bias""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/output/dense/kernel""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".feed_forward.linear_2.weight""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/output/dense/bias""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".feed_forward.linear_2.bias""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/output/LayerNorm/gamma""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".layer_norm_2.gamma""]\r\n        output_model[""bert/encoder/layer_"" + str(i) + ""/output/LayerNorm/beta""] = input_model[\r\n            ""encoder.transformer."" + str(i) + "".layer_norm_2.beta""]\r\n\r\n    output_model[""bert/pooler/dense/kernel""] = input_model[""target.nsp_linear_1.weight""]\r\n    output_model[""bert/pooler/dense/bias""] = input_model[""target.nsp_linear_1.bias""]\r\n    output_model[""cls/seq_relationship/output_weights""] = input_model[""target.nsp_linear_2.weight""]\r\n    output_model[""cls/seq_relationship/output_bias""] = input_model[""target.nsp_linear_2.bias""]\r\n    output_model[""cls/predictions/transform/dense/kernel""] = input_model[""target.mlm_linear_1.weight""]\r\n    output_model[""cls/predictions/transform/dense/bias""] = input_model[""target.mlm_linear_1.bias""]\r\n    output_model[""cls/predictions/transform/LayerNorm/gamma""] = input_model[""target.layer_norm.gamma""]\r\n    output_model[""cls/predictions/transform/LayerNorm/beta""] = input_model[""target.layer_norm.beta""]\r\n    output_model[""cls/predictions/output_bias""] = input_model[""target.mlm_linear_2.bias""]\r\n\r\n    def assign_tf_var(tensor: np.ndarray, name: str):\r\n        tmp_var = tf.Variable(initial_value=tensor)\r\n        tf_var = tf.get_variable(dtype=tmp_var.dtype, shape=tmp_var.shape, name=name)\r\n        op = tf.assign(ref=tf_var, value=tmp_var)\r\n        session.run(tf.variables_initializer([tmp_var, tf_var]))\r\n        session.run(fetches=[op, tf_var])\r\n        return tf_var\r\n    tf_vars = []\r\n    tensors_to_transopse = (\r\n        ""dense/kernel"",\r\n        ""attention/self/query"",\r\n        ""attention/self/key"",\r\n        ""attention/self/value""\r\n    )\r\n    for k, v in output_model.items():\r\n        tf_name = k\r\n        torch_tensor = v.cpu().numpy()\r\n        if any([x in k for x in tensors_to_transopse]):\r\n            torch_tensor = torch_tensor.T\r\n        tf_tensor = assign_tf_var(tensor=torch_tensor, name=tf_name)\r\n        tf_vars.append(tf_tensor)\r\n        print(""{0}{1}initialized"".format(tf_name, "" "" * (60 - len(tf_name))))\r\n\r\n    saver = tf.train.Saver(tf_vars)\r\n    saver.save(session, args.output_model_path)\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n\r\n'"
scripts/convert_bert_from_uer_to_huggingface.py,2,"b'import torch\nimport argparse\nimport collections\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(""--input_model_path"", type=str, default=""google_model.bin"",\n                        help=""."")\nparser.add_argument(""--output_model_path"", type=str, default=""huggingface_model.bin"",\n                        help=""."")\nparser.add_argument(""--layers_num"", type=int, default=12, help=""."")\n\nargs = parser.parse_args()\npath = args.input_model_path\n\ninput_model = torch.load(args.input_model_path)\n\noutput_model = collections.OrderedDict()\n\noutput_model[""bert.embeddings.word_embeddings.weight""] = input_model[""embedding.word_embedding.weight""]\noutput_model[""bert.embeddings.position_embeddings.weight""] = input_model[""embedding.position_embedding.weight""]\noutput_model[""bert.embeddings.token_type_embeddings.weight""] = input_model[""embedding.segment_embedding.weight""][1:, :]\noutput_model[""bert.embeddings.LayerNorm.weight""] = input_model[""embedding.layer_norm.gamma""]\noutput_model[""bert.embeddings.LayerNorm.bias""] = input_model[""embedding.layer_norm.beta""]\n\nfor i in range(args.layers_num):\n    output_model[""bert.encoder.layer."" + str(i) + "".attention.self.query.weight""] = input_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.0.weight""]\n    output_model[""bert.encoder.layer."" + str(i) + "".attention.self.query.bias""] = input_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.0.bias""]\n    output_model[""bert.encoder.layer."" + str(i) + "".attention.self.key.weight""] = input_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.1.weight""]\n    output_model[""bert.encoder.layer."" + str(i) + "".attention.self.key.bias""] = input_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.1.bias""]\n    output_model[""bert.encoder.layer."" + str(i) + "".attention.self.value.weight""] = input_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.2.weight""]\n    output_model[""bert.encoder.layer."" + str(i) + "".attention.self.value.bias""] = input_model[""encoder.transformer."" + str(i) + "".self_attn.linear_layers.2.bias""]\n    output_model[""bert.encoder.layer."" + str(i) + "".attention.output.dense.weight""] = input_model[""encoder.transformer."" + str(i) + "".self_attn.final_linear.weight""]\n    output_model[""bert.encoder.layer."" + str(i) + "".attention.output.dense.bias""] = input_model[""encoder.transformer."" + str(i) + "".self_attn.final_linear.bias""]\n    output_model[""bert.encoder.layer."" + str(i) + "".attention.output.LayerNorm.weight""] = input_model[""encoder.transformer."" + str(i) + "".layer_norm_1.gamma""]\n    output_model[""bert.encoder.layer."" + str(i) + "".attention.output.LayerNorm.bias""] = input_model[""encoder.transformer."" + str(i) + "".layer_norm_1.beta""]\n    output_model[""bert.encoder.layer."" + str(i) + "".intermediate.dense.weight""] = input_model[""encoder.transformer."" + str(i) + "".feed_forward.linear_1.weight""]\n    output_model[""bert.encoder.layer."" + str(i) + "".intermediate.dense.bias""] = input_model[""encoder.transformer."" + str(i) + "".feed_forward.linear_1.bias""]\n    output_model[""bert.encoder.layer."" + str(i) + "".output.dense.weight""] = input_model[""encoder.transformer."" + str(i) + "".feed_forward.linear_2.weight""]\n    output_model[""bert.encoder.layer."" + str(i) + "".output.dense.bias""] = input_model[""encoder.transformer."" + str(i) + "".feed_forward.linear_2.bias""]\n    output_model[""bert.encoder.layer."" + str(i) + "".output.LayerNorm.weight""] = input_model[""encoder.transformer."" + str(i) + "".layer_norm_2.gamma""]\n    output_model[""bert.encoder.layer."" + str(i) + "".output.LayerNorm.bias""] = input_model[""encoder.transformer."" + str(i) + "".layer_norm_2.beta""]\n\noutput_model[""bert.pooler.dense.weight""] = input_model[""target.nsp_linear_1.weight""]\noutput_model[""bert.pooler.dense.bias""] = input_model[""target.nsp_linear_1.bias""]\noutput_model[""cls.seq_relationship.weight""] = input_model[""target.nsp_linear_2.weight""]\noutput_model[""cls.seq_relationship.bias""] = input_model[""target.nsp_linear_2.bias""]\noutput_model[""cls.predictions.transform.dense.weight""] = input_model[""target.mlm_linear_1.weight""]\noutput_model[""cls.predictions.transform.dense.bias""] = input_model[""target.mlm_linear_1.bias""]\noutput_model[""cls.predictions.transform.LayerNorm.weight""] = input_model[""target.layer_norm.gamma""]\noutput_model[""cls.predictions.transform.LayerNorm.bias""] = input_model[""target.layer_norm.beta""]\noutput_model[""cls.predictions.decoder.weight""] = input_model[""target.mlm_linear_2.weight""]\noutput_model[""cls.predictions.bias""] = input_model[""target.mlm_linear_2.bias""]\n\ntorch.save(output_model, args.output_model_path)\n'"
scripts/diff_vocab.py,0,"b'# -*- encoding:utf-8 -*-\nimport sys\nimport os\nimport argparse\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(""--vocab_1"", type=str)\n    parser.add_argument(""--vocab_2"", type=str)\n    args = parser.parse_args()\n\n    vocab_set_1 = set()\n    vocab_set_2 = set()\n\n    with open(args.vocab_1, mode=\'r\', encoding=\'utf-8\') as f:\n        for line in f:\n            try:\n                w = line.strip().split()[0]\n                vocab_set_1.add(w)\n            except:\n                pass\n    \n    with open(args.vocab_2, mode=\'r\', encoding=\'utf-8\') as f:\n        for line in f:\n            try:\n                w = line.strip().split()[0]\n                vocab_set_2.add(w)\n            except:\n                pass\n\n    print(""vocab_1: "" + args.vocab_1 + "", size: "" + str(len(vocab_set_1)) )\n    print(""vocab_2: "" + args.vocab_2 + "", size: "" + str(len(vocab_set_2)) )\n\n    print(""vocab_1 - "" + ""vocab_2 = "" + str(len(vocab_set_1 - vocab_set_2)))\n    print(""vocab_2 - "" + ""vocab_1 = "" + str(len(vocab_set_2 - vocab_set_1)))\n    print(""vocab_1 & "" + ""vocab_2 = "" + str(len(vocab_set_1 & vocab_set_2)))\n'"
scripts/dynamic_vocab_adapter.py,5,"b'# -*- encoding:utf-8 -*-\n""""""\nModify model\'s embedding and softmax layers according to the vocabulary.\n""""""\nimport sys\nimport os\nimport numpy as np\nimport torch\nimport argparse\nimport collections\n\nbert_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ""..""))\nsys.path.append(bert_dir)\n\nfrom uer.utils.vocab import Vocab\n\ndef adapter(old_model,old_vocab,new_vocab):\n    new_model = collections.OrderedDict()\n\n    embedding_key = ""embedding.word_embedding.weight""\n    softmax_key = ""target.mlm_linear_2.weight""\n    softmax_bias_key = ""target.mlm_linear_2.bias""\n\n    # Fit in parameters that would not be modified.\n    tensor_name=[]\n    for k, v in old_model.items():\n        tensor_name.append(k)\n        if k not in [embedding_key, softmax_key, softmax_bias_key]:\n            new_model[k] = v\n    bool = softmax_key in tensor_name\n    # Get word embedding, mlm, and mlm bias variables.\n    old_embedding = old_model.get(embedding_key).data.numpy()\n    if bool:\n        old_softmax = old_model.get(softmax_key).data.numpy()\n        old_softmax_bias = old_model.get(softmax_bias_key).data.numpy()\n\n    # Initialize.\n    new_embedding = np.random.normal(0, 0.02, [len(new_vocab), old_embedding.shape[1]])\n    if bool:\n        new_softmax = np.random.normal(0, 0.02, [len(new_vocab), old_softmax.shape[1]])\n        new_softmax_bias = np.random.normal(0, 0.02, [len(new_vocab)])\n\n    # Put corresponding parameters into the new model.\n    for i, w in enumerate(new_vocab.i2w):\n        if w in old_vocab.w2i:\n            old_w_index = old_vocab.w2i[w]\n            new_embedding[i] = old_embedding[old_w_index]\n            if bool:\n                new_softmax[i] = old_softmax[old_w_index]\n                new_softmax_bias[i] = old_softmax_bias[old_w_index]\n\n    new_model[embedding_key] = torch.tensor(new_embedding, dtype=torch.float32)\n    if bool:\n        new_model[softmax_key] = torch.tensor(new_softmax, dtype=torch.float32)\n        new_model[softmax_bias_key] = torch.tensor(new_softmax_bias, dtype=torch.float32)\n    return new_model\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n\n    # Input options.\n    parser.add_argument(""--old_model_path"", type=str)\n    parser.add_argument(""--old_vocab_path"", type=str)\n    parser.add_argument(""--new_vocab_path"", type=str)\n\n    # Output options.\n    parser.add_argument(""--new_model_path"", type=str)\n\n    args = parser.parse_args()\n    old_vocab = Vocab()\n    old_vocab.load(args.old_vocab_path)\n    new_vocab = Vocab()\n    new_vocab.load(args.new_vocab_path)\n\n    old_model = torch.load(args.old_model_path, map_location=""cpu"")\n\n    new_model = adapter(old_model,old_vocab,new_vocab)\n    print(""Output adapted new model."")\n    torch.save(new_model, args.new_model_path)'"
scripts/extract_embedding.py,1,"b'# -*- encoding:utf-8 -*-\nimport sys\nimport os\nimport torch\nimport argparse\n\nuer_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ""..""))\nsys.path.append(uer_dir)\n\nfrom uer.utils.vocab import Vocab\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    # Path options. \n    parser.add_argument(""--vocab_path"", help=""."")\n    parser.add_argument(""--pretrained_model_path"", help=""."")\n    parser.add_argument(""--output_word_embedding_path"", help=""."")\n\n    args = parser.parse_args()\n\n    vocab = Vocab()\n    vocab.load(args.vocab_path)\n\n    pretrained_model = torch.load(args.pretrained_model_path)\n    embedding = pretrained_model[""embedding.word_embedding.weight""]\n\n    f_out = open(args.output_word_embedding_path, mode=""w"", encoding=""utf-8"")\n\n    head=str(list(embedding.size())[0])+"" ""+str(list(embedding.size())[1])+""\\n""\n    f_out.write(head)\n\n    for i in range(len(vocab.i2w)):\n        word = vocab.i2w[i]\n        word_embedding = embedding[vocab.get(word), :]\n        word_embedding = word_embedding.cpu().numpy().tolist()\n        line = str(word)\n        for j in range(len(word_embedding)):\n            line = line + "" "" + str(word_embedding[j])\n        line += ""\\n""\n        f_out.write(line)\n'"
scripts/extract_feature.py,10,"b'# -*- encoding:utf-8 -*-\r\nimport sys\r\nimport os\r\nimport torch\r\nimport argparse\r\nimport torch.nn as nn\r\n\r\nuer_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ""..""))\r\nsys.path.append(uer_dir)\r\n\r\nfrom uer.utils.vocab import Vocab\r\nfrom uer.utils.constants import *\r\nfrom uer.utils.config import load_hyperparam\r\nfrom uer.utils.tokenizer import *\r\nfrom uer.model_builder import build_model\r\n\r\n\r\nclass SequenceEncoder(torch.nn.Module):\r\n    \r\n    def __init__(self, model):\r\n        super(SequenceEncoder, self).__init__()\r\n        self.embedding = model.embedding\r\n        self.encoder = model.encoder\r\n        # Close dropout.\r\n        self.eval()\r\n\r\n    def forward(self, src, seg):\r\n        emb = self.embedding(src, seg)\r\n        output = self.encoder(emb, seg)\r\n        return output        \r\n\r\n\r\nif __name__ == \'__main__\':\r\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n\r\n    # Path options.\r\n    parser.add_argument(""--input_path"", type=str, required=True,\r\n                        help=""Path of the input file."")\r\n    parser.add_argument(""--pretrained_model_path"", type=str, required=True,\r\n                        help=""Path of the pretrained model."")\r\n    parser.add_argument(""--vocab_path"", type=str, required=True,\r\n                        help=""Path of the vocabulary file."")\r\n    parser.add_argument(""--output_path"", required=True,\r\n                        help=""Path of the output file."")\r\n    parser.add_argument(""--config_path"", default=""models/bert_base_config.json"",\r\n                        help=""Path of the config file."")\r\n    \r\n    # Model options.\r\n    parser.add_argument(""--seq_length"", type=int, default=128, help=""Sequence length."")\r\n    parser.add_argument(""--batch_size"", type=int, default=8, help=""Batch size."")\r\n    parser.add_argument(""--embedding"", choices=[""bert"", ""word""], default=""bert"",\r\n                        help=""Emebdding type."")\r\n    parser.add_argument(""--encoder"", choices=[""bert"", ""lstm"", ""gru"", \\\r\n                                                   ""cnn"", ""gatedcnn"", ""attn"", \\\r\n                                                   ""rcnn"", ""crnn"", ""gpt""], \\\r\n                                                   default=""bert"", help=""Encoder type."")\r\n    parser.add_argument(""--bidirectional"", action=""store_true"", help=""Specific to recurrent model."")\r\n\r\n    # Subword options.\r\n    parser.add_argument(""--subword_type"", choices=[""none"", ""char""], default=""none"",\r\n                        help=""Subword feature type."")\r\n    parser.add_argument(""--sub_vocab_path"", type=str, default=""models/sub_vocab.txt"",\r\n                        help=""Path of the subword vocabulary file."")\r\n    parser.add_argument(""--subencoder"", choices=[""avg"", ""lstm"", ""gru"", ""cnn""], default=""avg"",\r\n                        help=""Subencoder type."")\r\n    parser.add_argument(""--sub_layers_num"", type=int, default=2, help=""The number of subencoder layers."")\r\n\r\n    # Tokenizer options.\r\n    parser.add_argument(""--tokenizer"", choices=[""bert"", ""char"", ""space""], default=""bert"",\r\n                        help=""Specify the tokenizer."" \r\n                             ""Original Google BERT uses bert tokenizer on Chinese corpus.""\r\n                             ""Char tokenizer segments sentences into characters.""\r\n                             ""Space tokenizer segments sentences into words according to space.""\r\n                             )\r\n\r\n    args = parser.parse_args()\r\n    args = load_hyperparam(args)\r\n\r\n    # Load vocabulary.\r\n    vocab = Vocab()\r\n    vocab.load(args.vocab_path)\r\n    args.vocab = vocab\r\n\r\n    # Build and load modeli.\r\n    # A pseudo target is added.\r\n    args.target = ""bert""\r\n    model = build_model(args)\r\n    pretrained_model = torch.load(args.pretrained_model_path)\r\n    model.load_state_dict(pretrained_model, strict=False)\r\n\r\n    seq_encoder = SequenceEncoder(model)\r\n\r\n    # For simplicity, we use DataParallel wrapper to use multiple GPUs.\r\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\r\n    if torch.cuda.device_count() > 1:\r\n        print(""{} GPUs are available. Let\'s use them."".format(torch.cuda.device_count()))\r\n        seq_encoder = nn.DataParallel(seq_encoder)\r\n\r\n    seq_encoder = seq_encoder.to(device)\r\n    \r\n    # Build tokenizer\r\n    tokenizer = globals()[args.tokenizer.capitalize() + ""Tokenizer""](args)\r\n\r\n    dataset = []\r\n    with open(args.input_path, mode=""r"", encoding=""utf-8"") as f:\r\n        for line in f:\r\n            tokens = [vocab.get(t) for t in tokenizer.tokenize(line)]\r\n            if len(tokens) == 0:\r\n                continue\r\n            tokens = [CLS_ID] + tokens\r\n            seg = [1] * len(tokens)\r\n\r\n            if len(tokens) > args.seq_length:\r\n                tokens = tokens[:args.seq_length]\r\n                seg = seg[:args.seq_length]\r\n            while len(tokens) < args.seq_length:\r\n                tokens.append(PAD_ID)\r\n                seg.append(PAD_ID)\r\n            dataset.append((tokens, seg))\r\n           \r\n    src = torch.LongTensor([e[0] for e in dataset])\r\n    seg = torch.LongTensor([e[1] for e in dataset])\r\n\r\n    def batch_loader(batch_size, src, seg):\r\n        instances_num = src.size(0)\r\n        for i in range(instances_num // batch_size):\r\n            src_batch = src[i*batch_size : (i+1)*batch_size]\r\n            seg_batch = seg[i*batch_size : (i+1)*batch_size]\r\n            yield src_batch, seg_batch\r\n        if instances_num > instances_num // batch_size * batch_size:\r\n            src_batch = src[instances_num//batch_size*batch_size:]\r\n            seg_batch = seg[instances_num//batch_size*batch_size:]\r\n            yield src_batch, seg_batch\r\n\r\n    feature_vectors = []\r\n    for i, (src_batch, seg_batch) in enumerate(batch_loader(args.batch_size, src, seg)):\r\n        src_batch = src_batch.to(device)\r\n        seg_batch = seg_batch.to(device)\r\n        output = seq_encoder(src_batch, seg_batch)\r\n        feature_vectors.append(output)\r\n\r\n    feature_vectors = torch.cat(feature_vectors, 0)\r\n    torch.save(feature_vectors,args.output_path)\r\n    print(""The number of sentences: {}"".format(feature_vectors.size(0)))\r\n'"
scripts/generate.py,11,"b'# -*- encoding:utf-8 -*-\r\n""""""\r\n  This script provides an exmaple to wrap UER-py for generate.\r\n  We randomly give the beginning of story and use GPT to generate the full of story.\r\n""""""\r\nimport sys\r\nimport os\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport argparse\r\nimport random\r\n\r\nuer_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "".."")) \r\nsys.path.append(uer_dir) \r\nfrom uer.utils.act_fun import gelu\r\nfrom uer.utils.constants import *\r\nfrom uer.utils.tokenizer import *\r\nfrom uer.layers.layer_norm import LayerNorm\r\nfrom uer.utils.config import load_hyperparam\r\nfrom uer.utils.vocab import Vocab\r\nfrom uer.model_builder import build_model\r\n\r\n\r\nclass GenerateModel(torch.nn.Module):\r\n    def __init__(self, args, model):\r\n        super(GenerateModel, self).__init__()\r\n        self.embedding = model.embedding\r\n        self.encoder = model.encoder\r\n        self.target = model.target\r\n        # Open eval mode.\r\n        self.eval()\r\n\r\n    def forward(self, src, seg):\r\n        emb = self.embedding(src, seg)\r\n        output = self.encoder(emb, seg)\r\n        output = gelu(self.target.output_layer(output))\r\n        return output\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n\r\n    # Path options.\r\n    parser.add_argument(""--pretrained_model_path"", type=str, required=True, \r\n                        help=""Path of the pretrained model."")\r\n    parser.add_argument(""--vocab_path"", default=""models/google_vocab.txt"", type=str,\r\n                        help=""Path of the vocabulary file."")\r\n    parser.add_argument(""--input_path"", type=str, required=True, \r\n                        help=""Path of the input file, containing the beginning of a story."")\r\n    parser.add_argument(""--output_path"", type=str, required=True, \r\n                        help=""Path of the output file, containing the entire story."")\r\n    parser.add_argument(""--config_path"", default=""models/bert_base_config.json"", type=str,\r\n                        help=""Path of the config file."")\r\n\r\n    # Model options.\r\n    parser.add_argument(""--seq_length"", type=int, default=128,\r\n                        help=""Sequence length."")\r\n    parser.add_argument(""--top_k"", type=int, default=0)\r\n    parser.add_argument(""--top_p"", type=float, default=0.6)\r\n    parser.add_argument(""--temperature"", type=float, default=1.0)\r\n    parser.add_argument(""--embedding"", choices=[""bert"", ""word""], default=""bert"",\r\n                        help=""Emebdding type."")\r\n    parser.add_argument(""--encoder"", choices=[""bert"", ""lstm"", ""gru"", \\\r\n                                              ""cnn"", ""gatedcnn"", ""attn"", \\\r\n                                              ""rcnn"", ""crnn"", ""gpt"", ""bilstm""], \\\r\n                                     default=""bert"", help=""Encoder type."")\r\n    parser.add_argument(""--target"", choices=[""lm""], default=""lm"",\r\n                        help=""The training target of the pretraining model."")\r\n\r\n    # Subword options.\r\n    parser.add_argument(""--subword_type"", choices=[""none"", ""char""], default=""none"",\r\n                        help=""Subword feature type."")\r\n    parser.add_argument(""--sub_vocab_path"", type=str, default=""models/sub_vocab.txt"",\r\n                        help=""Path of the subword vocabulary file."")\r\n    parser.add_argument(""--subencoder_type"", choices=[""avg"", ""lstm"", ""gru"", ""cnn""], default=""avg"",\r\n                        help=""Subencoder type."")\r\n\r\n    # Tokenizer options.\r\n    parser.add_argument(""--tokenizer"", choices=[""bert"", ""char"", ""space""], default=""bert"",\r\n                        help=""Specify the tokenizer.""\r\n                             ""Original Google BERT uses bert tokenizer on Chinese corpus.""\r\n                             ""Char tokenizer segments sentences into characters.""\r\n                             ""Space tokenizer segments sentences into words according to space.""\r\n                             )\r\n\r\n    \r\n    args = parser.parse_args()\r\n\r\n    # Load the hyperparameters from the config file.\r\n    args = load_hyperparam(args)\r\n\r\n    # Load Vocabulary\r\n    vocab = Vocab()\r\n    vocab.load(args.vocab_path)\r\n    args.vocab = vocab\r\n\r\n    # Build bert model.\r\n    model = build_model(args)\r\n\r\n    # Load pretrained model.\r\n    pretrained_model_dict = torch.load(args.pretrained_model_path)\r\n    model.load_state_dict(pretrained_model_dict, strict=False)\r\n\r\n    model = GenerateModel(args, model)\r\n\r\n    # Build tokenizer.\r\n    tokenizer = globals()[args.tokenizer.capitalize() + ""Tokenizer""](args)\r\n\r\n    def top_k_top_p_filtering(logits, top_k, top_p):\r\n        top_k = min(top_k, logits.size(-1))  # Safety check\r\n        if top_k > 0:\r\n            # Remove all tokens with a probability less than the last token of the top-k\r\n            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\r\n            logits[indices_to_remove] = -float(""Inf"")\r\n\r\n        if top_p > 0.0:\r\n            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\r\n            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\r\n\r\n            # Remove tokens with cumulative probability above the threshold\r\n            sorted_indices_to_remove = cumulative_probs > top_p\r\n            # Shift the indices to the right to keep also the first token above the threshold\r\n            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\r\n            sorted_indices_to_remove[..., 0] = 0\r\n\r\n            indices_to_remove = sorted_indices[sorted_indices_to_remove]\r\n            logits[indices_to_remove] = -float(""Inf"")\r\n        return logits\r\n\r\n    with open(args.input_path, mode=""r"", encoding=""utf-8"") as f:\r\n        line = f.readline().strip()\r\n        src = [vocab.get(t) for t in tokenizer.tokenize(line.strip())]\r\n        seg = [1] * len(src)\r\n        start_length = len(src)\r\n        if len(src) > args.seq_length:\r\n            src = src[:args.seq_length]\r\n            seg = seg[:args.seq_length]\r\n    src = [src]\r\n    seg = [seg]    \r\n    src_tensor = torch.LongTensor(src)\r\n    seg_tensor = torch.LongTensor(seg)\r\n\r\n    f_output = open(args.output_path, mode=""w"", encoding=""utf-8"")\r\n\r\n    for i in range(args.seq_length-start_length):\r\n        outputs = model(src_tensor, seg_tensor)\r\n        next_token_logits = outputs[0][-1] / args.temperature\r\n        filtered_logits = top_k_top_p_filtering(next_token_logits, args.top_k, args.top_p)\r\n        next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\r\n        \r\n        src_tensor = torch.cat([src_tensor, next_token.view(1,1)], dim=1)\r\n        seg_tensor = torch.cat([seg_tensor, torch.tensor([[1]])], dim=1)\r\n\r\n    f_output.write(line+""\\n"")\r\n    generated_sentence = """".join([vocab.i2w[token_id] for token_id in src_tensor[0]])\r\n    f_output.write(generated_sentence)\r\n    \r\n    f_output.close()\r\n'"
scripts/multi_single_convert.py,3,"b'# -*- encoding:utf-8 -*-\nimport sys\nimport torch\nimport argparse\nimport collections\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(""--input_model_path"", type=str, help=""The input model."")\n    parser.add_argument(""--output_model_path"", type=str, help=""The output model."")\n    parser.add_argument(""--delete_module_prefix"", action=""store_true"", help=""Delete \'module.\' prefix."")\n    parser.add_argument(""--add_module_prefix"", action=""store_true"", help=""Add \'module.\' prefix."")\n\n    args = parser.parse_args()\n\n    prefix = ""module.""\n    input_model = torch.load(args.input_model_path)\n    if args.delete_module_prefix:\n        output_model = collections.OrderedDict()\n        for k, v in input_model.items():\n            if prefix not in k:\n                print(""This model is already of Single-GPU version. Nothing changed."")\n                sys.exit(0)\n            else:\n                output_model[k[len(prefix):]] = v\n        print(""A Single-GPU version model is created"")\n        torch.save(output_model, args.output_model_path)\n\n        \n    if args.add_module_prefix:\n        output_model = collections.OrderedDict()\n        for k, v in input_model.items():\n            if prefix in k:\n                print(""This model is already of Multi-GPU version. Nothing changed."")\n                sys.exit(0)\n            else:\n                output_model[prefix+k] = v\n        print(""A Multi-GPU version model is created"")\n        torch.save(output_model, args.output_model_path)\n'"
scripts/topn_words_dep.py,13,"b'# -*- encoding:utf-8 -*-\nimport sys\nimport os\nimport torch\nimport codecs\nimport argparse\nimport numpy as np\n\nuer_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ""..""))\nsys.path.append(uer_dir)\n\nfrom uer.utils.vocab import Vocab\nfrom uer.utils.config import load_hyperparam\nfrom uer.layers.embeddings import BertEmbedding\nfrom uer.encoders.bert_encoder import BertEncoder\nfrom uer.utils.tokenizer import *\nfrom uer.utils.constants import *\n\n\nclass SequenceEncoder(torch.nn.Module):\n    \n    def __init__(self, args, vocab):\n        super(SequenceEncoder, self).__init__()\n        self.embedding = BertEmbedding(args, len(vocab))\n        self.encoder = BertEncoder(args)\n\n    def forward(self, src, seg):\n        emb = self.embedding(src, seg)\n        output = self.encoder(emb, seg)\n        return output    \n\n\ndef sentence_encoding(src, seg):\n    src = torch.LongTensor(src)\n    seg = torch.LongTensor(seg)\n    output = seq_encoder(src.to(device), seg.to(device))\n    output = output.cpu().data.numpy()\n    return output\n\n\ndef sentence_to_id(line):\n    src = [vocab.get(w) for w in line]\n    src = [CLS_ID] + src\n    seg = [1] * len(src)\n    if len(src) > args.seq_length:\n        src = src[:args.seq_length]\n        seg = seg[:args.seq_length]\n    while len(src) < args.seq_length:\n        src.append(PAD_ID)\n        seg.append(PAD_ID)\n    return src, seg\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    # Path options.\n    parser.add_argument(""--sent_path"", help=""."")\n    parser.add_argument(""--vocab_path"", help=""."")\n    parser.add_argument(""--cand_vocab_path"", help=""."")\n    parser.add_argument(""--pretrained_model_path"", help=""."")\n    # Model options.\n    parser.add_argument(""--seq_length"", type=int)\n    parser.add_argument(""--layers_num"", type=int, default=4)\n    parser.add_argument(""--batch_size"", type=int)\n    parser.add_argument(""--config_path"", help=""."")\n    parser.add_argument(""--topn"", type=int, default=20)\n    # Tokenizer options.\n    parser.add_argument(""--tokenizer"", choices=[""bert"", ""char"", ""space""], default=""space"",\n                        help=""Specify the tokenizer."" \n                             ""Original Google BERT uses bert tokenizer on Chinese corpus.""\n                             ""Char tokenizer segments sentences into characters.""\n                             ""Space tokenizer segments sentences into words according to space.""\n                             )\n\n    args = parser.parse_args()\n    layers_num = args.layers_num \n    args = load_hyperparam(args)\n    args.layers_num = layers_num\n\n    vocab = Vocab()\n    vocab.load(args.vocab_path)\n\n    seq_encoder = SequenceEncoder(args, vocab)    \n \n    pretrained_model = torch.load(args.pretrained_model_path)\n    seq_encoder.load_state_dict(pretrained_model, strict=False)\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    if torch.cuda.device_count() > 1:\n        print(""{} GPUs are available. Let\'s use them."".format(torch.cuda.device_count()))\n        seq_encoder = torch.nn.DataParallel(seq_encoder)\n\n    seq_encoder = seq_encoder.to(device)\n    seq_encoder.eval()\n\n    # Build tokenizer.\n    tokenizer = globals()[args.tokenizer.capitalize() + ""Tokenizer""](args)\n\n    f_sent = open(args.sent_path, mode=""r"", encoding=""utf-8"")\n\n    cand_vocab = Vocab()\n    cand_vocab.load(args.cand_vocab_path)\n    print(""length of candidate vocab: ""+str(len(cand_vocab)))\n\n    for line in f_sent:\n        # Sentence and word are splitted by ""\\t""\n        line = line.strip().split(""\\t"")\n        if len(line) != 2:\n            continue\n        target_word = line[-1]\n        print(""Original sentence: "")\n        print(line[0])\n        sent = tokenizer.tokenize(line[0])\n        print(""Target word"" + "": "" + target_word)\n\n        src, seg = sentence_to_id(sent)\n\n        target_word_id = vocab.get(target_word)\n        if target_word_id == UNK_ID:\n            print(""The candidate word is UNK in vocab."")\n            continue\n        \n        # Search the position of the word in the sentence.\n        position = -1\n        if target_word_id in src:\n            position = src.index(target_word_id)\n\n        if position < 1:\n            print(""The target word is not in the sentence."")\n            continue\n\n        index_of_word_in_line = position \n        output = sentence_encoding([src], [seg])\n        output = output.reshape([args.seq_length, -1])\n        target_embedding = output[position,:]\n\n        batch_size = args.batch_size\n        cand_word_batch = []\n        cand_embeddings = []\n        for word_id, word in enumerate(cand_vocab.i2w):\n            cand_word_batch.append(vocab.w2i.get(word))\n            if len(cand_word_batch) == batch_size or word_id == (len(cand_vocab.i2w)-1):\n                seg_batch = [seg]*len(cand_word_batch)\n                src_batch = torch.LongTensor([src]*len(cand_word_batch))\n                src_batch[:,position] = torch.LongTensor(cand_word_batch)\n                output = sentence_encoding(src_batch, seg_batch)\n                output = np.reshape(output, (len(output), args.seq_length, -1))\n                cand_embeddings.extend(output[:, position, :].tolist())\n                cand_word_batch = []\n\n        target_embedding = np.array(target_embedding).reshape(1,-1).astype(""float"")\n\n        sims = torch.nn.functional.cosine_similarity(torch.tensor(np.array(target_embedding), dtype=torch.float),\\\n                                                     torch.FloatTensor(cand_embeddings))\n           \n        sorted_id = torch.argsort(sims, descending=True)\n\n        for j in sorted_id[1: args.topn+1]:\n            print(cand_vocab.i2w[j].strip()+ ""\\t""+str(sims[j].item()))\n            \n    f_sent.close()\n'"
scripts/topn_words_indep.py,3,"b'# -*- encoding:utf-8 -*-\nimport sys\nimport os\nimport torch\nimport codecs\nimport argparse\n\nuer_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ""..""))\nsys.path.append(uer_dir)\n\nfrom uer.utils.vocab import Vocab\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    # Path options.\n    parser.add_argument(""--target_words_path"", help=""."")\n    parser.add_argument(""--vocab_path"", help=""."")\n    parser.add_argument(""--cand_vocab_path"", help=""."")\n    parser.add_argument(""--pretrained_model_path"", help=""."")\n\n    # Output path.\n    parser.add_argument(""--topn"", type=int, default=20)\n\n    args = parser.parse_args()\n\n    vocab = Vocab()\n    vocab.load(args.vocab_path)\n\n    pretrained_model = torch.load(args.pretrained_model_path)\n    embedding = pretrained_model[""embedding.word_embedding.weight""]\n\n    cand_vocab = Vocab()\n    cand_vocab.load(args.cand_vocab_path)\n    cand_vocab_id = [vocab.get(w) for w in cand_vocab.i2w]\n    cand_embedding = embedding[cand_vocab_id, :]\n\n    f_word = open(args.target_words_path, mode=""r"", encoding=""utf-8"")\n\n    for line in f_word:\n        word = line.strip().split()[0]\n        print(""Target word: "" + word)\n        target_embedding = embedding[vocab.get(word), :]\n        sims = torch.nn.functional.cosine_similarity(target_embedding.view(1, -1), cand_embedding)\n        sorted_id = torch.argsort(sims, descending=True)\n        for j in sorted_id[1: args.topn+1]:\n            print(cand_vocab.i2w[j].strip()+ ""\\t""+str(sims[j].item()))    \n        print()\n'"
uer/__init__.py,0,b''
uer/model_builder.py,0,"b'# -*- encoding:utf-8 -*-\nimport torch\nfrom uer.layers.embeddings import BertEmbedding, WordEmbedding\nfrom uer.encoders.bert_encoder import BertEncoder\nfrom uer.encoders.rnn_encoder import LstmEncoder, GruEncoder\nfrom uer.encoders.birnn_encoder import BilstmEncoder\nfrom uer.encoders.cnn_encoder import CnnEncoder, GatedcnnEncoder\nfrom uer.encoders.attn_encoder import AttnEncoder\nfrom uer.encoders.gpt_encoder import GptEncoder\nfrom uer.encoders.mixed_encoder import RcnnEncoder, CrnnEncoder\nfrom uer.targets.bert_target import BertTarget\nfrom uer.targets.lm_target import LmTarget\nfrom uer.targets.cls_target import ClsTarget\nfrom uer.targets.mlm_target import MlmTarget\nfrom uer.targets.nsp_target import NspTarget\nfrom uer.targets.s2s_target import S2sTarget\nfrom uer.targets.bilm_target import BilmTarget\nfrom uer.subencoders.avg_subencoder import AvgSubencoder\nfrom uer.subencoders.rnn_subencoder import LstmSubencoder\nfrom uer.subencoders.cnn_subencoder import CnnSubencoder\nfrom uer.models.model import Model\n\n\ndef build_model(args):\n    """"""\n    Build universial encoder representations models.\n    The combinations of different embedding, encoder, \n    and target layers yield pretrained models of different \n    properties. \n    We could select suitable one for downstream tasks.\n    """"""\n\n    if args.subword_type != ""none"":\n        subencoder = globals()[args.subencoder.capitalize() + ""Subencoder""](args, len(args.sub_vocab))\n    else:\n        subencoder = None\n\n    embedding = globals()[args.embedding.capitalize() + ""Embedding""](args, len(args.vocab))\n    encoder = globals()[args.encoder.capitalize() + ""Encoder""](args)\n    target = globals()[args.target.capitalize() + ""Target""](args, len(args.vocab))\n    model = Model(args, embedding, encoder, target, subencoder)\n\n    return model\n'"
uer/model_loader.py,2,"b'# -*- encoding:utf-8 -*-\nimport torch\n\n\ndef load_model(model, model_path):\n    if hasattr(model, ""module""):\n        model.module.load_state_dict(torch.load(model_path, map_location=\'cpu\'), strict=False)\n    else:\n        model.load_state_dict(torch.load(model_path, map_location=\'cpu\'), strict=False)\n    return model\n'"
uer/model_saver.py,2,"b'# -*- encoding:utf-8 -*-\nimport torch\n\n\ndef save_model(model, model_path):\n    if hasattr(model, ""module""):\n        torch.save(model.module.state_dict(), model_path)\n    else:\n        torch.save(model.state_dict(), model_path)\n'"
uer/trainer.py,5,"b'# -*- encoding:utf-8 -*-\nimport os\nimport sys\nimport time\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel\n\nfrom uer.model_loader import load_model\nfrom uer.model_saver import save_model\nfrom uer.model_builder import build_model\nfrom uer.utils.optimizers import *\nfrom uer.utils.data import *\nfrom uer.utils.vocab import Vocab\nfrom uer.utils.seed import set_seed\n\n\ndef train_and_validate(args):\n    set_seed(args.seed)\n\n    # Load vocabulary.\n    vocab = Vocab()\n    vocab.load(args.vocab_path)\n    args.vocab = vocab\n\n    # Build model.\n    model = build_model(args)\n\n    # Load or initialize parameters.\n    if args.pretrained_model_path is not None:\n        # Initialize with pretrained model.\n        model = load_model(model, args.pretrained_model_path) \n    else:\n        # Initialize with normal distribution.\n        for n, p in list(model.named_parameters()):\n            if \'gamma\' not in n and \'beta\' not in n:\n                p.data.normal_(0, 0.02)\n\n    if args.dist_train:\n        # Multiprocessing distributed mode.\n        mp.spawn(worker, nprocs=args.ranks_num, args=(args.gpu_ranks, args, model), daemon=False)\n    elif args.single_gpu:\n        # Single GPU mode.\n        worker(args.gpu_id, None, args, model)\n    else:\n        # CPU mode.\n        worker(None, None, args, model)\n\n\ndef worker(proc_id, gpu_ranks, args, model):\n    """"""\n    Args:\n        proc_id: The id of GPU for single GPU mode;\n                 The id of process (and GPU) for multiprocessing distributed mode.\n        gpu_ranks: List of ranks of each process.\n    """"""\n    set_seed(args.seed)\n\n    if args.dist_train:\n        rank = gpu_ranks[proc_id]\n        gpu_id = proc_id\n    elif args.single_gpu:\n        rank = None\n        gpu_id = proc_id\n    else:\n        rank = None\n        gpu_id = None\n\n    if args.dist_train:\n        train_loader = globals()[args.target.capitalize() + ""DataLoader""](args, args.dataset_path, args.batch_size, rank, args.world_size, True)\n    else:\n        train_loader = globals()[args.target.capitalize() + ""DataLoader""](args, args.dataset_path, args.batch_size, 0, 1, True)\n\n    if gpu_id is not None: \n        torch.cuda.set_device(gpu_id)\n        model.cuda(gpu_id)\n\n    # Build optimizer.\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\'bias\', \'gamma\', \'beta\']\n    optimizer_grouped_parameters = [\n        {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay_rate\': 0.01},\n        {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay_rate\': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, correct_bias=False)\n    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.total_steps*args.warmup, t_total=args.total_steps)\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n        args.amp = amp\n\n    if args.dist_train:\n        # Initialize multiprocessing distributed training environment.\n        dist.init_process_group(backend=args.backend,\n                                init_method=args.master_ip,\n                                world_size=args.world_size,\n                                rank=rank)\n        model = DistributedDataParallel(model, device_ids=[gpu_id])\n        print(""Worker %d is training ... "" % rank)\n    else:\n        print(""Worker is training ..."")\n    \n    globals().get(""train_""+args.target)(args, gpu_id, rank, train_loader, model, optimizer, scheduler)\n    \n\ndef train_bert(args, gpu_id, rank, loader, model, optimizer, scheduler):\n    model.train()\n    start_time = time.time()\n    total_loss, total_loss_mlm, total_loss_nsp = 0., 0., 0.\n    # Calculate MLM accuracy.\n    total_correct_mlm, total_denominator = 0., 0. \n    # Calculate NSP accuracy.\n    total_correct_nsp, total_instances = 0., 0.\n    steps = 1\n    total_steps = args.total_steps\n    done_tokens = 0\n    loader_iter = iter(loader)\n\n    while True:\n        if steps == total_steps + 1:\n            break\n        src, tgt_mlm, tgt_nsp, seg = next(loader_iter)\n\n        if gpu_id is not None:\n            src = src.cuda(gpu_id)\n            tgt_mlm = tgt_mlm.cuda(gpu_id)\n            tgt_nsp = tgt_nsp.cuda(gpu_id)\n            seg = seg.cuda(gpu_id)\n        \n        # Forward.\n        loss_info = model(src, (tgt_mlm, tgt_nsp), seg)\n        loss_mlm, loss_nsp, correct_mlm, correct_nsp, denominator = loss_info\n        \n         # Backward.\n        loss = loss_mlm + loss_nsp\n        total_loss += loss.item()\n        total_loss_mlm += loss_mlm.item()\n        total_loss_nsp += loss_nsp.item()\n        total_correct_mlm += correct_mlm.item()\n        total_correct_nsp += correct_nsp.item()\n        total_denominator += denominator.item()\n        total_instances += src.size(0)\n        done_tokens += src.size(0) * src.size(1)\n\n        loss = loss / args.accumulation_steps\n\n        if args.fp16:\n            with args.amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n\n        if steps % args.accumulation_steps == 0:\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n        \n        if steps % args.report_steps == 0  and \\\n            (not args.dist_train or (args.dist_train and rank == 0)):\n\n            loss = total_loss / args.report_steps\n            loss_mlm = total_loss_mlm / args.report_steps\n            loss_nsp = total_loss_nsp / args.report_steps\n\n            elapsed = time.time() - start_time\n\n            if args.dist_train:\n                done_tokens *= args.world_size\n\n            print(""| {:8d}/{:8d} steps""\n                  ""| {:8.2f} tokens/s""\n                  ""| loss {:7.2f}""\n                  ""| loss_mlm: {:3.3f}""\n                  ""| loss_nsp: {:3.3f}""\n                  ""| acc_mlm: {:3.3f}""\n                  ""| acc_nsp: {:3.3f}"".format(\n                    steps, \n                    total_steps, \n                    done_tokens / elapsed, \n                    loss, \n                    loss_mlm,\n                    loss_nsp,\n                    total_correct_mlm / total_denominator,\n                    total_correct_nsp  / total_instances))\n            \n            done_tokens = 0\n            total_loss, total_loss_mlm, total_loss_nsp = 0., 0., 0.\n            total_correct_mlm, total_denominator = 0., 0.\n            total_correct_nsp, total_instances = 0., 0.\n\n            start_time = time.time()\n\n        if steps % args.save_checkpoint_steps == 0 and \\\n                (not args.dist_train or (args.dist_train and rank == 0)):\n            save_model(model, args.output_model_path + ""-"" + str(steps))\n\n        steps += 1\n\n\ndef train_lm(args, gpu_id, rank, loader, model, optimizer, scheduler):\n    model.train()\n    start_time = time.time()\n    total_loss = 0.\n    # Calculate MLM accuracy.\n    total_correct, total_denominator = 0., 0. \n    # Calculate NSP accuracy.\n    steps = 1\n    total_steps = args.total_steps\n    loader_iter = iter(loader)\n\n    while True:\n        if steps == total_steps + 1:\n            break\n        src, tgt, seg = next(loader_iter)\n\n        if gpu_id is not None:\n            src = src.cuda(gpu_id)\n            tgt = tgt.cuda(gpu_id)\n            seg = seg.cuda(gpu_id)\n        \n        # Forward.\n        loss_info = model(src, tgt, seg)\n        loss, correct, denominator = loss_info\n        \n        # Backward.\n        total_loss += loss.item()\n        total_correct += correct.item()\n        total_denominator += denominator.item()\n\n        loss = loss / args.accumulation_steps\n\n        if args.fp16:\n            with args.amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n\n        if steps % args.accumulation_steps == 0:\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n        \n        if steps % args.report_steps == 0  and \\\n            (not args.dist_train or (args.dist_train and rank == 0)):\n\n            loss = total_loss / args.report_steps\n\n            elapsed = time.time() - start_time\n\n            done_tokens = \\\n                args.batch_size * src.size(1) * args.report_steps * args.world_size \\\n                if args.dist_train \\\n                else args.batch_size * src.size(1) * args.report_steps\n\n            print(""| {:8d}/{:8d} steps""\n                  ""| {:8.2f} tokens/s""\n                  ""| loss {:7.2f}""\n                  ""| acc: {:3.3f}"".format(\n                    steps, \n                    total_steps, \n                    done_tokens / elapsed, \n                    loss, \n                    total_correct / total_denominator))\n            \n            total_loss = 0.\n            total_correct, total_denominator = 0., 0.\n\n            start_time = time.time()\n\n        if steps % args.save_checkpoint_steps == 0 and \\\n                (not args.dist_train or (args.dist_train and rank == 0)):\n            save_model(model, args.output_model_path + ""-"" + str(steps))\n\n        steps += 1\n\n\ndef train_bilm(args, gpu_id, rank, loader, model, optimizer, scheduler):\n    model.train()\n    start_time = time.time()\n    total_loss, total_loss_forward, total_loss_backward = 0., 0., 0.\n    # Calculate BiLM accuracy.\n    total_correct_forward, total_correct_backward, total_denominator = 0., 0., 0. \n    steps = 1\n    total_steps = args.total_steps\n    loader_iter = iter(loader)\n\n    while True:\n        if steps == total_steps + 1:\n            break\n        src, tgt_forward, tgt_backward, seg = next(loader_iter)\n\n        if gpu_id is not None:\n            src = src.cuda(gpu_id)\n            tgt_forward = tgt_forward.cuda(gpu_id)\n            tgt_backward = tgt_backward.cuda(gpu_id)\n            seg = seg.cuda(gpu_id)\n        \n        # Forward.\n        loss_info = model(src, (tgt_forward, tgt_backward), seg)\n        loss_forward, loss_backward, correct_forward, correct_backward, denominator = loss_info\n        \n        # Backward.\n        loss = loss_forward + loss_backward\n        total_loss += loss.item()\n        total_loss_forward += loss_forward.item()\n        total_loss_backward += loss_backward.item()\n        total_correct_forward += correct_forward.item()\n        total_correct_backward += correct_backward.item()\n        total_denominator += denominator.item()\n\n        loss = loss / args.accumulation_steps\n\n        if args.fp16:\n            with args.amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n\n        if steps % args.accumulation_steps == 0:\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n        \n        if steps % args.report_steps == 0  and \\\n            (not args.dist_train or (args.dist_train and rank == 0)):\n\n            loss = total_loss / args.report_steps\n\n            elapsed = time.time() - start_time\n\n            done_tokens = \\\n                args.batch_size * src.size(1) * args.report_steps * args.world_size \\\n                if args.dist_train \\\n                else args.batch_size * src.size(1) * args.report_steps\n\n            print(""| {:8d}/{:8d} steps""\n                  ""| {:8.2f} tokens/s""\n                  ""| loss {:7.2f}""\n                  ""| loss_forward {:3.3f}""\n                  ""| loss_backward {:3.3f}""\n                  ""| acc_forward: {:3.3f}""\n                  ""| acc_backward: {:3.3f}"".format(\n                    steps, \n                    total_steps, \n                    done_tokens / elapsed, \n                    loss,\n                    loss_forward,\n                    loss_backward,\n                    total_correct_forward / total_denominator,\n                    total_correct_backward / total_denominator))\n            \n            total_loss, total_loss_forward, total_loss_backward = 0., 0., 0.\n            total_correct_forward, total_correct_backward, total_denominator = 0., 0., 0. \n\n            start_time = time.time()\n\n        if steps % args.save_checkpoint_steps == 0 and \\\n                (not args.dist_train or (args.dist_train and rank == 0)):\n            save_model(model, args.output_model_path + ""-"" + str(steps))\n\n        steps += 1\n\n\ndef train_cls(args, gpu_id, rank, loader, model, optimizer, scheduler):\n    model.train()\n    start_time = time.time()\n    total_loss = 0.\n    total_correct, total_instances = 0., 0.\n    steps = 1\n    total_steps = args.total_steps\n    loader_iter = iter(loader)\n\n    while True:\n        if steps == total_steps + 1:\n            break\n        src, tgt, seg = next(loader_iter)\n\n        if gpu_id is not None:\n            src = src.cuda(gpu_id)\n            tgt = tgt.cuda(gpu_id)\n            seg = seg.cuda(gpu_id)\n        \n        # Forward.\n        loss_info = model(src, tgt, seg)\n        loss, correct = loss_info\n        \n        # Backward.\n        total_loss += loss.item()\n        total_correct += correct.item()\n        total_instances += src.size(0)\n\n        loss = loss / args.accumulation_steps\n\n        if args.fp16:\n            with args.amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n\n        if steps % args.accumulation_steps == 0:\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n        \n        if steps % args.report_steps == 0  and \\\n            (not args.dist_train or (args.dist_train and rank == 0)):\n\n            loss = total_loss / args.report_steps\n\n            elapsed = time.time() - start_time\n\n            done_tokens = \\\n                args.batch_size * src.size(1) * args.report_steps * args.world_size \\\n                if args.dist_train \\\n                else args.batch_size * src.size(1) * args.report_steps\n\n            print(""| {:8d}/{:8d} steps""\n                  ""| {:8.2f} tokens/s""\n                  ""| loss {:7.2f}""\n                  ""| acc: {:3.3f}"".format(\n                    steps, \n                    total_steps, \n                    done_tokens / elapsed, \n                    loss, \n                    total_correct / total_instances))\n            \n            total_loss = 0.\n            total_correct = 0.\n            total_instances = 0.\n\n            start_time = time.time()\n\n        if steps % args.save_checkpoint_steps == 0 and \\\n                (not args.dist_train or (args.dist_train and rank == 0)):\n            save_model(model, args.output_model_path + ""-"" + str(steps))\n\n        steps += 1\n\n\ndef train_mlm(args, gpu_id, rank, loader, model, optimizer, scheduler):\n    model.train()\n    start_time = time.time()\n    total_loss, total_loss_mlm, total_loss_nsp = 0., 0., 0.\n    # Calculate MLM accuracy.\n    total_correct, total_denominator = 0., 0. \n    # Calculate NSP accuracy.\n    total_instances = 0., 0.\n    steps = 1\n    total_steps = args.total_steps\n    loader_iter = iter(loader)\n\n    while True:\n        if steps == total_steps + 1:\n            break\n        src, tgt, seg = next(loader_iter)\n\n        if gpu_id is not None:\n            src = src.cuda(gpu_id)\n            tgt = tgt.cuda(gpu_id)\n            seg = seg.cuda(gpu_id)\n        \n        # Forward.\n        loss_info = model(src, tgt, seg)\n        loss, correct, denominator = loss_info\n        \n        # Backward.\n        total_loss += loss.item()\n        total_correct += correct.item()\n        total_denominator += denominator.item()\n\n        loss = loss / args.accumulation_steps\n\n        if args.fp16:\n            with args.amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n\n        if steps % args.accumulation_steps == 0:\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n        \n        if steps % args.report_steps == 0  and \\\n            (not args.dist_train or (args.dist_train and rank == 0)):\n\n            loss = total_loss / args.report_steps\n\n            elapsed = time.time() - start_time\n\n            done_tokens = \\\n                args.batch_size * src.size(1) * args.report_steps * args.world_size \\\n                if args.dist_train \\\n                else args.batch_size * src.size(1) * args.report_steps\n\n            print(""| {:8d}/{:8d} steps""\n                  ""| {:8.2f} tokens/s""\n                  ""| loss {:7.2f}""\n                  ""| acc: {:3.3f}"".format(\n                    steps, \n                    total_steps, \n                    done_tokens / elapsed, \n                    loss, \n                    total_correct / total_denominator))\n            \n            total_loss = 0.\n            total_correct, total_denominator = 0., 0.\n\n            start_time = time.time()\n\n        if steps % args.save_checkpoint_steps == 0 and \\\n                (not args.dist_train or (args.dist_train and rank == 0)):\n            save_model(model, args.output_model_path + ""-"" + str(steps))\n\n        steps += 1\n\n\n# def train_nsp(args, gpu_id, rank, loader, model, optimizer):\n#     model.train()\n#     start_time = time.time()\n#     total_loss = 0.\n#     total_correct, total_instances = 0., 0.\n#     steps = 1\n#     total_steps = args.total_steps\n#     loader_iter = iter(loader)\n\n#     while True:\n#         if steps == total_steps + 1:\n#             break\n#         src, tgt, seg = next(loader_iter)\n\n#         if gpu_id is not None:\n#             src = src.cuda(gpu_id)\n#             tgt = tgt.cuda(gpu_id)\n#             seg = seg.cuda(gpu_id)\n        \n#         # Forward.\n#         loss_info = model(src, tgt, seg)\n#         loss, correct = loss_info\n        \n#         # Backward.\n#         total_loss += loss.item()\n#         total_correct += correct.item()\n#         total_instances += src.size(0)\n\n#         loss = loss / args.accumulation_steps\n#         loss.backward()\n\n#         if steps % args.accumulation_steps == 0:\n#             optimizer.step()\n#             model.zero_grad()\n        \n#         if steps % args.report_steps == 0  and \\\n#             (not args.dist_train or (args.dist_train and rank == 0)):\n\n#             loss = total_loss / args.report_steps\n\n#             elapsed = time.time() - start_time\n\n#             done_tokens = \\\n#                 args.batch_size * src.size(1) * args.report_steps * args.world_size \\\n#                 if args.dist_train \\\n#                 else args.batch_size * src.size(1) * args.report_steps\n\n#             print(""| {:8d}/{:8d} steps""\n#                   ""| {:8.2f} tokens/s""\n#                   ""| loss {:7.2f}""\n#                   ""| acc: {:3.3f}"".format(\n#                     steps, \n#                     total_steps, \n#                     done_tokens / elapsed, \n#                     loss, \n#                     total_correct / total_instances))\n            \n#             total_loss = 0.\n#             total_correct = 0.\n#             total_instances = 0.\n\n#             start_time = time.time()\n\n#         if steps % args.save_checkpoint_steps == 0 and \\\n#                 (not args.dist_train or (args.dist_train and rank == 0)):\n#             save_model(model, args.output_model_path + ""-"" + str(steps))\n\n#         steps += 1\n\n\n# def train_s2s(args, gpu_id, rank, loader, model, optimizer):\n#     model.train()\n#     start_time = time.time()\n#     total_loss= 0.\n#     total_correct, total_denominator = 0., 0. \n#     steps = 1\n#     total_steps = args.total_steps\n#     loader_iter = iter(loader)\n\n#     while True:\n#         if steps == total_steps + 1:\n#             break\n#         src, tgt, seg = next(loader_iter)\n\n#         if gpu_id is not None:\n#             src = src.cuda(gpu_id)\n#             tgt = tgt.cuda(gpu_id)\n#             seg = seg.cuda(gpu_id)\n        \n#         # Forward.\n#         loss_info = model(src, tgt, seg)\n#         loss, correct, denominator = loss_info\n        \n#         # Backward.\n#         total_loss += loss.item()\n#         total_correct += correct.item()\n#         total_denominator += denominator.item()\n\n#         loss = loss / args.accumulation_steps\n#         loss.backward()\n\n#         if steps % args.accumulation_steps == 0:\n#             optimizer.step()\n#             model.zero_grad()\n        \n#         if steps % args.report_steps == 0  and \\\n#             (not args.dist_train or (args.dist_train and rank == 0)):\n\n#             loss = total_loss / args.report_steps\n\n#             elapsed = time.time() - start_time\n\n#             done_tokens = \\\n#                 args.batch_size * src.size(1) * args.report_steps * args.world_size \\\n#                 if args.dist_train \\\n#                 else args.batch_size * src.size(1) * args.report_steps\n\n#             print(""| {:8d}/{:8d} steps""\n#                   ""| {:8.2f} tokens/s""\n#                   ""| loss {:7.2f}""\n#                   ""| acc: {:3.3f}"".format(\n#                     steps, \n#                     total_steps, \n#                     done_tokens / elapsed, \n#                     loss, \n#                     total_correct / total_denominator))\n            \n#             total_loss = 0.\n#             total_correct, total_denominator = 0., 0.\n\n#             start_time = time.time()\n\n#         if steps % args.save_checkpoint_steps == 0 and \\\n#                 (not args.dist_train or (args.dist_train and rank == 0)):\n#             save_model(model, args.output_model_path + ""-"" + str(steps))\n\n#         steps += 1\n'"
uer/encoders/__init__.py,0,b''
uer/encoders/attn_encoder.py,1,"b'# -*- encoding:utf-8 -*-\nimport torch.nn as nn\nfrom uer.layers.layer_norm import LayerNorm\nfrom uer.layers.position_ffn import PositionwiseFeedForward\nfrom uer.layers.multi_headed_attn import MultiHeadedAttention\n\nclass AttnEncoder(nn.Module):\n    """"""\n    BERT encoder exploits 12 or 24 transformer layers to extract features.\n    """"""\n    def __init__(self, args):\n        super(AttnEncoder, self).__init__()\n        self.layers_num = args.layers_num\n        self.self_attn = MultiHeadedAttention(\n            args.hidden_size, args.heads_num, args.dropout\n        )\n        self.self_attn = nn.ModuleList([\n            MultiHeadedAttention(\n                args.hidden_size, args.heads_num, args.dropout\n            )\n            for _ in range(self.layers_num)\n        ])\n        \n    def forward(self, emb, seg):\n        """"""\n        Args:\n            emb: [batch_size x seq_length x emb_size]\n            mask: [batch_size x 1 x seq_length x seq_length]\n\n        Returns:\n            hidden: [batch_size x seq_length x hidden_size]\n        """"""\n\n        seq_length = emb.size(1)\n        # Generate mask according to segment indicators.\n        mask = (seg > 0). \\\n                unsqueeze(1). \\\n                repeat(1, seq_length, 1). \\\n                unsqueeze(1)\n\n        mask = mask.float()\n        mask = (1.0 - mask) * -10000.0\n\n        hidden = emb\n        for i in range(self.layers_num):\n            hidden = self.self_attn[i](hidden, hidden, hidden, mask)\n            \n        return hidden\n'"
uer/encoders/bert_encoder.py,1,"b'# -*- encoding:utf-8 -*-\nimport torch.nn as nn\nfrom uer.layers.layer_norm import LayerNorm\nfrom uer.layers.position_ffn import PositionwiseFeedForward\nfrom uer.layers.multi_headed_attn import MultiHeadedAttention\nfrom uer.layers.transformer import TransformerLayer\n\n\nclass BertEncoder(nn.Module):\n    """"""\n    BERT encoder exploits 12 or 24 transformer layers to extract features.\n    """"""\n    def __init__(self, args):\n        super(BertEncoder, self).__init__()\n        self.layers_num = args.layers_num\n        self.transformer = nn.ModuleList([\n            TransformerLayer(args) for _ in range(self.layers_num)\n        ])\n        \n    def forward(self, emb, seg):\n        """"""\n        Args:\n            emb: [batch_size x seq_length x emb_size]\n            seg: [batch_size x seq_length]\n\n        Returns:\n            hidden: [batch_size x seq_length x hidden_size]\n        """"""\n\n        seq_length = emb.size(1)\n        # Generate mask according to segment indicators.\n        # mask: [batch_size x 1 x seq_length x seq_length]\n        mask = (seg > 0). \\\n                unsqueeze(1). \\\n                repeat(1, seq_length, 1). \\\n                unsqueeze(1)\n\n        mask = mask.float()\n        mask = (1.0 - mask) * -10000.0\n\n        hidden = emb\n        for i in range(self.layers_num):\n            hidden = self.transformer[i](hidden, mask)\n        return hidden\n'"
uer/encoders/birnn_encoder.py,4,"b'# -*- encoding:utf-8 -*-\nimport torch\nimport torch.nn as nn\nfrom uer.utils.misc import *\n\nclass BilstmEncoder(nn.Module):\n    def __init__(self, args):\n        super(BilstmEncoder, self).__init__()\n\n        assert args.hidden_size % 2 == 0 \n        self.hidden_size= args.hidden_size // 2\n        \n        self.layers_num = args.layers_num\n\n        self.rnn_forward = nn.LSTM(input_size=args.emb_size,\n                           hidden_size=self.hidden_size,\n                           num_layers=args.layers_num,\n                           dropout=args.dropout,\n                           batch_first=True)\n\n        self.rnn_backward = nn.LSTM(input_size=args.emb_size,\n                           hidden_size=self.hidden_size,\n                           num_layers=args.layers_num,\n                           dropout=args.dropout,\n                           batch_first=True)\n\n        self.drop = nn.Dropout(args.dropout)\n\n    def forward(self, emb, seg):\n        # Forward.\n        emb_forward = emb\n        hidden_forward = self.init_hidden(emb_forward.size(0), emb_forward.device)\n        output_forward, hidden_forward = self.rnn_forward(emb_forward, hidden_forward) \n        output_forward = self.drop(output_forward)\n\n        # Backward.\n        emb_backward = flip(emb, 1)\n        hidden_backward = self.init_hidden(emb_backward.size(0), emb_backward.device)\n        output_backward, hidden_backward = self.rnn_backward(emb_backward, hidden_backward) \n        output_backward = self.drop(output_backward)\n        output_backward = flip(output_backward, 1)\n\n        return torch.cat([output_forward, output_backward], 2)\n\n    def init_hidden(self, batch_size, device):\n        return (torch.zeros(self.layers_num, batch_size, self.hidden_size, device=device),\n                torch.zeros(self.layers_num, batch_size, self.hidden_size, device=device))\n'"
uer/encoders/cnn_encoder.py,14,"b'# -*- encoding:utf-8 -*-\nimport torch\nimport torch.nn as nn\n\n\nclass CnnEncoder(nn.Module):\n    def __init__(self, args):\n        super(CnnEncoder, self).__init__()\n        self.layers_num = args.layers_num\n        self.kernel_size = args.kernel_size\n        self.block_size = args.block_size\n        self.emb_size = args.emb_size\n        self.hidden_size = args.hidden_size\n\n        self.conv_1 = nn.Conv2d(1, args.hidden_size, (args.kernel_size, args.emb_size))\n\n        self.conv = nn.ModuleList([nn.Conv2d(args.hidden_size, args.hidden_size, (args.kernel_size, 1)) \\\n            for _ in range(args.layers_num)])\n\n    def forward(self, emb, seg):\n        batch_size, seq_len, _ = emb.size()\n        padding = torch.zeros([batch_size, self.kernel_size-1, self.emb_size]).to(emb.device)\n        emb = torch.cat([padding, emb], dim=1).unsqueeze(1) # batch_size, 1, seq_length+width-1, emb_size\n\n        hidden = self.conv_1(emb)\n\n        padding =  torch.zeros([batch_size, self.hidden_size, self.kernel_size-1, 1]).to(emb.device)\n        hidden = torch.cat([padding, hidden], dim=2)\n\n        for i, conv_i in enumerate(self.conv):\n            hidden = conv_i(hidden)\n            hidden = torch.cat([padding, hidden], dim=2)\n\n        hidden = hidden[:,:,self.kernel_size-1:,:]\n        output = hidden.transpose(1,2).contiguous().view(batch_size, seq_len, self.hidden_size)\n\n        return output\n\n\nclass GatedcnnEncoder(nn.Module):\n    def __init__(self, args):\n        super(GatedcnnEncoder, self).__init__()\n        self.layers_num = args.layers_num\n        self.kernel_size = args.kernel_size\n        self.block_size = args.block_size\n        self.emb_size = args.emb_size\n        self.hidden_size = args.hidden_size\n\n        self.conv_1 = nn.Conv2d(1, args.hidden_size, (args.kernel_size, args.emb_size))\n        self.gate_1 = nn.Conv2d(1, args.hidden_size, (args.kernel_size, args.emb_size))\n\n        self.conv = nn.ModuleList([nn.Conv2d(args.hidden_size, args.hidden_size, (args.kernel_size, 1)) \\\n            for _ in range(args.layers_num-1)])\n        self.gate = nn.ModuleList([nn.Conv2d(args.hidden_size, args.hidden_size, (args.kernel_size, 1)) \\\n            for _ in range(args.layers_num-1)])\n\n    def forward(self, emb, seg):\n        batch_size, seq_len, _ = emb.size()\n\n        res_input = torch.transpose(emb.unsqueeze(3), 1, 2)\n\n        padding = torch.zeros([batch_size, self.kernel_size-1, self.emb_size]).to(emb.device)\n        emb = torch.cat([padding, emb], dim=1).unsqueeze(1) # batch_size, 1, seq_length+width-1, emb_size\n\n        hidden = self.conv_1(emb)\n        gate = self.gate_1(emb)\n        hidden = hidden * torch.sigmoid(gate)\n\n        padding = torch.zeros([batch_size, self.hidden_size, self.kernel_size-1, 1]).to(emb.device)\n        hidden = torch.cat([padding, hidden], dim=2)\n\n        for i, (conv_i, gate_i) in enumerate(zip(self.conv, self.gate)):\n            hidden, gate = conv_i(hidden), gate_i(hidden)\n            hidden = hidden * torch.sigmoid(gate)\n            if (i + 1) % self.block_size:\n                hidden = hidden + res_input\n                res_input = hidden\n            hidden = torch.cat([padding, hidden], dim=2)\n\n        hidden = hidden[:,:,self.kernel_size-1:,:]\n        output = hidden.transpose(1,2).contiguous().view(batch_size, seq_len, self.hidden_size)\n\n        return output\n'"
uer/encoders/gpt_encoder.py,3,"b'# -*- encoding:utf-8 -*-\nimport torch\nimport torch.nn as nn\nfrom uer.layers.layer_norm import LayerNorm\nfrom uer.layers.position_ffn import PositionwiseFeedForward\nfrom uer.layers.multi_headed_attn import MultiHeadedAttention\nfrom uer.layers.transformer import TransformerLayer\n\n\nclass GptEncoder(nn.Module):\n    """"""\n    BERT encoder exploits 12 or 24 transformer layers to extract features.\n    """"""\n    def __init__(self, args):\n        super(GptEncoder, self).__init__()\n        self.layers_num = args.layers_num\n        self.transformer = nn.ModuleList([\n            TransformerLayer(args) for _ in range(self.layers_num)\n        ])\n        \n    def forward(self, emb, seg):\n        """"""\n        Args:\n            emb: [batch_size x seq_length x emb_size]\n            seg: [batch_size x seq_length]\n\n        Returns:\n            hidden: [batch_size x seq_length x hidden_size]\n        """"""\n\n        batch_size, seq_length, _ = emb.size()\n        # Generate mask according to segment indicators.\n        # mask: [batch_size x 1 x seq_length x seq_length]\n        mask = torch.ones(seq_length, seq_length, device=emb.device)\n        mask = torch.tril(mask)\n        mask = (1.0 - mask) * -10000\n        mask = mask.repeat(batch_size, 1, 1, 1)\n\n        hidden = emb\n        for i in range(self.layers_num):\n            hidden = self.transformer[i](hidden, mask)\n        return hidden\n'"
uer/encoders/mixed_encoder.py,15,"b'# -*- encoding:utf-8 -*-\nimport torch\nimport torch.nn as nn\n\n\nclass RcnnEncoder(nn.Module):\n    def __init__(self, args):\n        super(RcnnEncoder, self).__init__()\n\n        self.emb_size = args.emb_size\n        self.hidden_size= args.hidden_size\n        self.kernel_size = args.kernel_size\n        self.layers_num = args.layers_num\n\n        self.rnn = nn.LSTM(input_size=args.emb_size,\n                           hidden_size=args.hidden_size,\n                           num_layers=args.layers_num,\n                           dropout=args.dropout,\n                           batch_first=True)\n\n        self.drop = nn.Dropout(args.dropout)\n\n        self.conv_1 = nn.Conv2d(1, args.hidden_size, (args.kernel_size, args.emb_size))\n        self.conv = nn.ModuleList([nn.Conv2d(args.hidden_size, args.hidden_size, (args.kernel_size, 1)) \\\n            for _ in range(args.layers_num-1)])\n\n    def forward(self, emb, seg):\n        batch_size, seq_len, _ = emb.size()\n\n        hidden = self.init_hidden(batch_size, emb.device)\n        output, hidden = self.rnn(emb, hidden) \n        output = self.drop(output)\n\n        \n        padding = torch.zeros([batch_size, self.kernel_size-1, self.emb_size]).to(emb.device)\n        hidden = torch.cat([padding, output], dim=1).unsqueeze(1) # batch_size, 1, seq_length+width-1, emb_size\n        hidden = self.conv_1(hidden)\n        padding =  torch.zeros([batch_size, self.hidden_size, self.kernel_size-1, 1]).to(emb.device)\n        hidden = torch.cat([padding, hidden], dim=2)\n        for i, conv_i in enumerate(self.conv):\n            hidden = conv_i(hidden)\n            hidden = torch.cat([padding, hidden], dim=2)\n        hidden = hidden[:,:,self.kernel_size-1:,:]\n        output = hidden.transpose(1,2).contiguous().view(batch_size, seq_len, self.hidden_size)\n\n        return output\n\n    def init_hidden(self, batch_size, device):\n        return (torch.zeros(self.layers_num, batch_size, self.hidden_size, device=device),\n                torch.zeros(self.layers_num, batch_size, self.hidden_size, device=device))\n\n\nclass CrnnEncoder(nn.Module):\n    def __init__(self, args):\n        super(CrnnEncoder, self).__init__()\n\n        self.emb_size = args.emb_size\n        self.hidden_size= args.hidden_size\n        self.kernel_size = args.kernel_size\n        self.layers_num = args.layers_num\n\n        self.conv_1 = nn.Conv2d(1, args.hidden_size, (args.kernel_size, args.emb_size))\n        self.conv = nn.ModuleList([nn.Conv2d(args.hidden_size, args.hidden_size, (args.kernel_size, 1)) \\\n            for _ in range(args.layers_num-1)])\n\n\n        self.rnn = nn.LSTM(input_size=args.emb_size,\n                           hidden_size=args.hidden_size,\n                           num_layers=args.layers_num,\n                           dropout=args.dropout,\n                           batch_first=True)\n        self.drop = nn.Dropout(args.dropout)\n\n    def forward(self, emb, seg):\n        batch_size, seq_len, _ = emb.size()\n        padding = torch.zeros([batch_size, self.kernel_size-1, self.emb_size]).to(emb.device)\n        emb = torch.cat([padding, emb], dim=1).unsqueeze(1) # batch_size, 1, seq_length+width-1, emb_size\n        hidden = self.conv_1(emb)\n        padding =  torch.zeros([batch_size, self.hidden_size, self.kernel_size-1, 1]).to(emb.device)\n        hidden = torch.cat([padding, hidden], dim=2)\n        for i, conv_i in enumerate(self.conv):\n            hidden = conv_i(hidden)\n            hidden = torch.cat([padding, hidden], dim=2)\n        hidden = hidden[:,:,self.kernel_size-1:,:]\n        output = hidden.transpose(1,2).contiguous().view(batch_size, seq_len, self.hidden_size)\n\n        hidden = self.init_hidden(batch_size, emb.device)\n        output, hidden = self.rnn(output, hidden) \n        output = self.drop(output)\n\n        return output\n\n    def init_hidden(self, batch_size, device):\n        return (torch.zeros(self.layers_num, batch_size, self.hidden_size, device=device),\n                torch.zeros(self.layers_num, batch_size, self.hidden_size, device=device))\n\n'"
uer/encoders/rnn_encoder.py,7,"b'# -*- encoding:utf-8 -*-\nimport torch\nimport torch.nn as nn\n\n\nclass LstmEncoder(nn.Module):\n    def __init__(self, args):\n        super(LstmEncoder, self).__init__()\n\n        self.bidirectional = args.bidirectional\n        if self.bidirectional:\n            assert args.hidden_size % 2 == 0 \n            self.hidden_size= args.hidden_size // 2\n        else:\n            self.hidden_size= args.hidden_size\n        \n        self.layers_num = args.layers_num\n\n        self.rnn = nn.LSTM(input_size=args.emb_size,\n                           hidden_size=self.hidden_size,\n                           num_layers=args.layers_num,\n                           dropout=args.dropout,\n                           batch_first=True,\n                           bidirectional=self.bidirectional)\n\n        self.drop = nn.Dropout(args.dropout)\n\n    def forward(self, emb, seg):\n        hidden = self.init_hidden(emb.size(0), emb.device)\n        output, hidden = self.rnn(emb, hidden) \n        output = self.drop(output) \n        return output\n\n    def init_hidden(self, batch_size, device):\n        if self.bidirectional:\n            return (torch.zeros(self.layers_num*2, batch_size, self.hidden_size, device=device),\n                    torch.zeros(self.layers_num*2, batch_size, self.hidden_size, device=device))\n        else:\n            return (torch.zeros(self.layers_num, batch_size, self.hidden_size, device=device),\n                    torch.zeros(self.layers_num, batch_size, self.hidden_size, device=device))\n\n\nclass GruEncoder(nn.Module):\n    def __init__(self, args):\n        super(GruEncoder, self).__init__()\n\n        self.bidirectional = args.bidirectional\n        if self.bidirectional:\n            assert args.hidden_size % 2 == 0 \n            self.hidden_size= args.hidden_size // 2\n        else:\n            self.hidden_size= args.hidden_size\n\n        self.layers_num = args.layers_num\n\n        self.rnn = nn.GRU(input_size=args.emb_size,\n                           hidden_size=self.hidden_size,\n                           num_layers=args.layers_num,\n                           dropout=args.dropout,\n                           batch_first=True,\n                           bidirectional=self.bidirectional)\n\n        self.drop = nn.Dropout(args.dropout)\n\n    def forward(self, emb, seg):\n        hidden = self.init_hidden(emb.size(0), emb.device)\n        output, hidden = self.rnn(emb, hidden) \n        output = self.drop(output) \n        return output\n\n    def init_hidden(self, batch_size, device):\n        if self.bidirectional:\n            return torch.zeros(self.layers_num*2, batch_size, self.hidden_size, device=device)\n        else:\n            return torch.zeros(self.layers_num, batch_size, self.hidden_size, device=device)\n'"
uer/layers/__init__.py,0,b''
uer/layers/embeddings.py,3,"b'# -*- encoding:utf-8 -*-\nimport torch\nimport torch.nn as nn\nfrom uer.layers.layer_norm import LayerNorm\n\n\nclass BertEmbedding(nn.Module):\n    """"""\n    BERT embedding consists of three parts:\n    word embedding, position embedding, and segment embedding.\n    """"""\n    def __init__(self, args, vocab_size):\n        super(BertEmbedding, self).__init__()\n        self.dropout = nn.Dropout(args.dropout)\n        self.max_length = 512\n        self.word_embedding = nn.Embedding(vocab_size, args.emb_size)\n        self.position_embedding = nn.Embedding(self.max_length, args.emb_size)\n        self.segment_embedding = nn.Embedding(3, args.emb_size)\n        self.layer_norm = LayerNorm(args.emb_size)\n\n    def forward(self, src, seg):\n        word_emb = self.word_embedding(src)\n        pos_emb = self.position_embedding(torch.arange(0, word_emb.size(1), device=word_emb.device, \\\n                                          dtype=torch.long).unsqueeze(0).repeat(word_emb.size(0), 1))\n        seg_emb = self.segment_embedding(seg)\n\n        emb = word_emb + pos_emb + seg_emb\n        emb = self.dropout(self.layer_norm(emb))\n        return emb\n\n\nclass WordEmbedding(nn.Module):\n    """"""\n    """"""\n    def __init__(self, args, vocab_size):\n        super(WordEmbedding, self).__init__()\n        self.dropout = nn.Dropout(args.dropout)\n        self.word_embedding = nn.Embedding(vocab_size, args.emb_size)\n        self.layer_norm = LayerNorm(args.emb_size)\n\n    def forward(self, src, _):\n        emb = self.word_embedding(src)\n        emb = self.dropout(self.layer_norm(emb))\n        return emb'"
uer/layers/layer_norm.py,3,"b'# -*- encoding:utf-8 -*-\nimport torch\nimport torch.nn as nn\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.eps = eps\n        self.gamma = nn.Parameter(torch.ones(hidden_size))\n        self.beta = nn.Parameter(torch.zeros(hidden_size))\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x-mean) / (std+self.eps) + self.beta\n'"
uer/layers/multi_headed_attn.py,3,"b'# -*- encoding:utf-8 -*-\nimport math\nimport torch\nimport torch.nn as nn\n\nclass MultiHeadedAttention(nn.Module):\n    """"""\n    Each head is a self-attention operation.\n    self-attention refers to https://arxiv.org/pdf/1706.03762.pdf\n    """"""\n    def __init__(self, hidden_size, heads_num, dropout):\n        super(MultiHeadedAttention, self).__init__()\n        self.hidden_size = hidden_size\n        self.heads_num = heads_num\n        self.per_head_size = hidden_size // heads_num\n\n        self.linear_layers = nn.ModuleList([\n                nn.Linear(hidden_size, hidden_size) for _ in range(3)\n            ])\n        \n        self.dropout = nn.Dropout(dropout)\n        self.final_linear = nn.Linear(hidden_size, hidden_size)\n\n    def forward(self, key, value, query, mask):\n        """"""\n        Args:\n            key: [batch_size x seq_length x hidden_size]\n            value: [batch_size x seq_length x hidden_size]\n            query: [batch_size x seq_length x hidden_size]\n            mask: [batch_size x 1 x seq_length x seq_length]\n\n        Returns:\n            output: [batch_size x seq_length x hidden_size]\n        """"""\n        batch_size, seq_length, hidden_size = key.size()\n        heads_num = self.heads_num\n        per_head_size = self.per_head_size\n\n        def shape(x):\n            return x. \\\n                   contiguous(). \\\n                   view(batch_size, seq_length, heads_num, per_head_size). \\\n                   transpose(1, 2)\n\n        def unshape(x):\n            return x. \\\n                   transpose(1, 2). \\\n                   contiguous(). \\\n                   view(batch_size, seq_length, hidden_size)\n\n\n        query, key, value = [l(x). \\\n                             view(batch_size, -1, heads_num, per_head_size). \\\n                             transpose(1, 2) \\\n                             for l, x in zip(self.linear_layers, (query, key, value))\n                            ]\n\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        scores = scores / math.sqrt(float(per_head_size)) \n        scores = scores + mask\n        probs = nn.Softmax(dim=-1)(scores)\n        probs = self.dropout(probs)\n        output = unshape(torch.matmul(probs, value))\n        output = self.final_linear(output)\n        \n        return output\n'"
uer/layers/position_ffn.py,1,"b'# -*- encoding:utf-8 -*-\nimport torch.nn as nn\nfrom uer.utils.act_fun import gelu\n\n\nclass PositionwiseFeedForward(nn.Module):\n    """""" Feed Forward Layer """"""\n    def __init__(self, hidden_size, feedforward_size):\n        super(PositionwiseFeedForward, self).__init__()\n        self.linear_1 = nn.Linear(hidden_size, feedforward_size)\n        self.linear_2 = nn.Linear(feedforward_size, hidden_size)\n        \n    def forward(self, x):\n        inter = gelu(self.linear_1(x))\n        output = self.linear_2(inter)\n        return output\n'"
uer/layers/transformer.py,1,"b'# -*- encoding:utf-8 -*-\nimport torch.nn as nn\nfrom uer.layers.layer_norm import LayerNorm\nfrom uer.layers.position_ffn import PositionwiseFeedForward\nfrom uer.layers.multi_headed_attn import MultiHeadedAttention\n\n\nclass TransformerLayer(nn.Module):\n    """"""\n    Transformer layer mainly consists of two parts:\n    multi-headed self-attention and feed forward layer.\n    """"""\n    def __init__(self, args):\n        super(TransformerLayer, self).__init__()\n\n        # Multi-headed self-attention.\n        self.self_attn = MultiHeadedAttention(\n            args.hidden_size, args.heads_num, args.dropout\n        )\n        self.dropout_1 = nn.Dropout(args.dropout)\n        self.layer_norm_1 = LayerNorm(args.hidden_size)\n        # Feed forward layer.\n        self.feed_forward = PositionwiseFeedForward(\n            args.hidden_size, args.feedforward_size\n        )\n        self.dropout_2 = nn.Dropout(args.dropout)\n        self.layer_norm_2 = LayerNorm(args.hidden_size)\n\n    def forward(self, hidden, mask):\n        """"""\n        Args:\n            hidden: [batch_size x seq_length x emb_size]\n            mask: [batch_size x 1 x seq_length x seq_length]\n\n        Returns:\n            output: [batch_size x seq_length x hidden_size]\n        """"""\n        inter = self.dropout_1(self.self_attn(hidden, hidden, hidden, mask))\n        inter = self.layer_norm_1(inter + hidden)\n        output = self.dropout_2(self.feed_forward(inter))\n        output = self.layer_norm_2(output + inter)  \n        return output\n'"
uer/models/__init__.py,0,b''
uer/models/bert_model.py,1,"b'# -*- encoding:utf-8 -*-\nimport torch\nimport torch.nn as nn\n\nclass BertModel(nn.Module):\n    """"""\n    BertModel consists of three parts:\n        - embedding: token embedding, position embedding, segment embedding\n        - encoder: multi-layer transformer encoders\n        - target: mlm and nsp tasks\n    """"""\n    def __init__(self, args, embedding, encoder, target):\n        super(BertModel, self).__init__()\n        self.embedding = embedding\n        self.encoder = encoder\n        self.target = target\n\n    def forward(self, src, tgt_mlm, tgt_nsp, seg):\n        # [batch_size, seq_length, emb_size]\n        emb = self.embedding(src, seg) \n        seq_length = emb.size(1)\n        # Generate mask according to segment indicators.\n        mask = (seg > 0). \\\n                unsqueeze(1). \\\n                repeat(1, seq_length, 1). \\\n                unsqueeze(1)\n\n        mask = mask.float()\n        mask = (1.0 - mask) * -10000.0\n        output = self.encoder(emb, mask)            \n\n        loss_mlm, loss_nsp, correct_mlm, correct_nsp, \\\n            denominator = self.target(output, tgt_mlm, tgt_nsp)\n            \n        return loss_mlm, loss_nsp, correct_mlm, \\\n               correct_nsp, denominator\n'"
uer/models/model.py,1,"b'# -*- encoding:utf-8 -*-\nimport torch\nimport torch.nn as nn\nfrom uer.utils.constants import *\nfrom uer.utils.subword import *\n\n\nclass Model(nn.Module):\n    """"""\n    BertModel consists of three parts:\n        - embedding: token embedding, position embedding, segment embedding\n        - encoder: multi-layer transformer encoders\n        - target: mlm and nsp tasks\n    """"""\n    def __init__(self, args, embedding, encoder, target, subencoder = None):\n        super(Model, self).__init__()\n        self.embedding = embedding\n        self.encoder = encoder\n        self.target = target\n        \n        # Subencoder.\n        if subencoder is not None:\n            self.vocab, self.sub_vocab = args.vocab, args.sub_vocab\n            self.subword_type = args.subword_type\n            self.subencoder = subencoder\n        else:\n            self.subencoder = None\n\n    def forward(self, src, tgt, seg):\n        # [batch_size, seq_length, emb_size]\n\n        emb = self.embedding(src, seg) \n\n        if self.subencoder is not None:\n            sub_ids = word2sub(src, self.vocab, self.sub_vocab, self.subword_type)\n            emb = emb + self.subencoder(sub_ids).contiguous().view(*emb.size())\n\n        output = self.encoder(emb, seg)            \n\n        loss_info = self.target(output, tgt)\n            \n        return loss_info\n'"
uer/subencoders/__init__.py,0,b''
uer/subencoders/avg_subencoder.py,2,"b'import torch\nimport torch.nn as nn\nimport sys\nimport torch.nn.functional as F\n\nclass AvgSubencoder(nn.Module):\n    def __init__(self, args, vocab_size):\n        super(AvgSubencoder, self).__init__()\n        self.embedding_layer = nn.Embedding(vocab_size, args.emb_size)\n\n    def forward(self, ids):\n        emb = self.embedding_layer(ids) # batch_size, max_length, emb_size\n        output = emb.mean(1)\n        return output\n'"
uer/subencoders/cnn_subencoder.py,4,"b'import torch\nimport torch.nn as nn\nimport sys\nimport torch.nn.functional as F\n\nclass CnnSubencoder(nn.Module):\n    def __init__(self, args, vocab_size):\n        super(CnnSubencoder, self).__init__()\n        self.kernel_size = args.kernel_size\n        self.emb_size = args.emb_size\n        self.embedding_layer = nn.Embedding(vocab_size, args.emb_size)\n        self.cnn = nn.Conv2d(1, args.emb_size, (args.kernel_size, args.emb_size))\n\n    def forward(self, ids):\n        emb = self.embedding_layer(ids) # batch_size * seq_length, max_length, emb_size\n        padding = torch.zeros([emb.size(0), self.kernel_size-1, self.emb_size]).to(emb.device)\n        emb = torch.cat([padding, emb], dim=1).unsqueeze(1) # batch_size, 1, seq_length+width-1, emb_size\n        conv_output = F.relu(self.cnn(emb)).squeeze(3)\n        conv_output =F.max_pool1d(conv_output, conv_output.size(2)).squeeze(2)\n\n        return conv_output\n'"
uer/subencoders/rnn_subencoder.py,3,"b'import torch\nimport torch.nn as nn\n\n\nclass LstmSubencoder(nn.Module):\n    def __init__(self, args, vocab_size):\n        super(LstmSubencoder, self).__init__()\n        self.hidden_size= args.emb_size\n        self.layers_num = args.sub_layers_num\n\n        self.embedding_layer = nn.Embedding(vocab_size, args.emb_size)\n        self.rnn = nn.LSTM(input_size=args.emb_size,\n                           hidden_size=self.hidden_size,\n                           num_layers=self.layers_num,\n                           dropout=args.dropout,\n                           batch_first=True)\n\n    def forward(self, ids):\n        batch_size, _ = ids.size() # batch_size, max_length\n        hidden = (torch.zeros(self.layers_num, batch_size, self.hidden_size).to(ids.device),\n                  torch.zeros(self.layers_num, batch_size, self.hidden_size).to(ids.device))\n        emb = self.embedding_layer(ids)\n        output, hidden = self.rnn(emb, hidden)\n        output = output.mean(1)\n        return output\n'"
uer/targets/__init__.py,0,b''
uer/targets/bert_target.py,8,"b'# -*- encoding:utf-8 -*-\nimport math\nimport torch\nimport torch.nn as nn\nfrom uer.layers.layer_norm import LayerNorm\nfrom uer.utils.act_fun import gelu\n\n\nclass BertTarget(nn.Module):\n    """"""\n    BERT exploits masked language modeling (MLM) \n    and next sentence prediction (NSP) for pretraining.\n    """"""\n    def __init__(self, args, vocab_size):\n        super(BertTarget, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size = args.hidden_size\n\n        # MLM.\n        self.mlm_linear_1 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.layer_norm = LayerNorm(args.hidden_size)\n        self.mlm_linear_2 = nn.Linear(args.hidden_size, self.vocab_size)\n\n        # NSP.\n        self.nsp_linear_1 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.nsp_linear_2 = nn.Linear(args.hidden_size, 2)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n        self.criterion = nn.NLLLoss()\n\n    def mlm(self, memory_bank, tgt_mlm):\n        # Masked language model (MLM) with full softmax prediction.\n        output_mlm = gelu(self.mlm_linear_1(memory_bank))\n        output_mlm = self.layer_norm(output_mlm)\n        output_mlm = output_mlm.contiguous().view(-1, self.hidden_size)\n        tgt_mlm = tgt_mlm.contiguous().view(-1)\n        output_mlm = output_mlm[tgt_mlm>0,:]\n        tgt_mlm = tgt_mlm[tgt_mlm>0]\n        output_mlm = self.mlm_linear_2(output_mlm)\n        output_mlm = self.softmax(output_mlm)\n\n        one_hot = torch.zeros(output_mlm.size(0),  self.vocab_size). \\\n           to(torch.device(output_mlm.device)). \\\n           scatter_(1, tgt_mlm.contiguous().view(-1,1), 1.0)\n        numerator = -torch.sum(output_mlm * one_hot, 1)\n        denominator = torch.tensor(output_mlm.size(0) + 1e-6)\n        loss_mlm = torch.sum(numerator) / denominator\n        correct_mlm = torch.sum((output_mlm.argmax(dim=-1).eq(tgt_mlm)).float())\n        \n        return loss_mlm, correct_mlm, denominator\n\n    def forward(self, memory_bank, tgt):\n        """"""\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: tuple with tgt_mlm [batch_size x seq_length] and tgt_nsp [batch_size]\n\n        Returns:\n            loss_mlm: Masked language model loss.\n            loss_nsp: Next sentence prediction loss.\n            correct_mlm: Number of words that are predicted correctly.\n            correct_nsp: Number of sentences that are predicted correctly.\n            denominator: Number of masked words.\n        """"""\n\n        # Masked language model (MLM).\n        assert type(tgt) == tuple\n        tgt_mlm, tgt_nsp = tgt[0], tgt[1]\n        loss_mlm, correct_mlm, denominator = self.mlm(memory_bank, tgt_mlm)\n           \n        # Next sentence prediction (NSP).\n        output_nsp = torch.tanh(self.nsp_linear_1(memory_bank[:, 0, :]))\n        output_nsp = self.nsp_linear_2(output_nsp)\n        loss_nsp = self.criterion(self.softmax(output_nsp), tgt_nsp)\n        correct_nsp = self.softmax(output_nsp).argmax(dim=-1).eq(tgt_nsp).sum()\n\n        return loss_mlm, loss_nsp, correct_mlm, correct_nsp, denominator\n'"
uer/targets/bilm_target.py,15,"b'# -*- encoding:utf-8 -*-\nimport math\nimport torch\nimport torch.nn as nn\nfrom uer.layers.layer_norm import LayerNorm\nfrom uer.utils.act_fun import gelu\nfrom uer.utils.misc import *\n\n\nclass BilmTarget(nn.Module):\n    """"""\n    """"""\n    def __init__(self, args, vocab_size):\n        super(BilmTarget, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size = args.hidden_size // 2\n\n        self.softmax = nn.LogSoftmax(dim=-1)\n        self.output_layer = nn.Linear(self.hidden_size, self.vocab_size)\n\n    def forward(self, memory_bank, tgt):\n        """"""\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size x seq_length]\n\n        Returns:\n            loss: Language modeling loss.\n            correct: Number of words that are predicted correctly.\n            denominator: Number of predicted words.\n        """"""\n        \n        assert type(tgt) == tuple\n        tgt_forward, tgt_backward = tgt[0], tgt[1]\n        #tgt_backward = flip(tgt_backward, 1)\n\n        # Forward.\n        output_forward = self.output_layer(memory_bank[:,:,:self.hidden_size])        \n        output_forward = output_forward.contiguous().view(-1, self.vocab_size)\n        output_forward = self.softmax(output_forward)        \n        tgt_forward = tgt_forward.contiguous().view(-1,1)\n        label_mask_forward = (tgt_forward > 0).float().to(torch.device(output_forward.device))\n        one_hot_forward = torch.zeros(label_mask_forward.size(0),  self.vocab_size). \\\n           to(torch.device(output_forward.device)). \\\n           scatter_(1, tgt_forward, 1.0)\n        numerator_forward = -torch.sum(output_forward * one_hot_forward, 1)\n        label_mask_forward = label_mask_forward.contiguous().view(-1)\n        tgt_forward = tgt_forward.contiguous().view(-1)\n        numerator_forward = torch.sum(label_mask_forward * numerator_forward)\n        denominator_forward = torch.sum(label_mask_forward) + 1e-6\n        loss_forward = numerator_forward / denominator_forward\n        correct_forward = torch.sum(label_mask_forward * (output_forward.argmax(dim=-1).eq(tgt_forward)).float())\n\n        # Backward.\n        output_backward = self.output_layer(memory_bank[:,:,self.hidden_size:])\n        output_backward = output_backward.contiguous().view(-1, self.vocab_size)\n        output_backward = self.softmax(output_backward)\n        tgt_backward = tgt_backward.contiguous().view(-1,1)\n        label_mask_backward = (tgt_backward > 0).float().to(torch.device(output_backward.device))\n        one_hot_backward = torch.zeros(label_mask_backward.size(0),  self.vocab_size). \\\n           to(torch.device(output_backward.device)). \\\n           scatter_(1, tgt_backward, 1.0)\n        numerator_backward = -torch.sum(output_backward * one_hot_backward, 1)\n        label_mask_backward = label_mask_backward.contiguous().view(-1)\n        tgt_backward = tgt_backward.contiguous().view(-1)\n        numerator_backward = torch.sum(label_mask_backward * numerator_backward)\n        denominator_backward = torch.sum(label_mask_backward) + 1e-6\n        loss_backward = numerator_backward / denominator_backward\n        correct_backward = torch.sum(label_mask_backward * (output_backward.argmax(dim=-1).eq(tgt_backward)).float())\n\n        return loss_forward, loss_backward, correct_forward, correct_backward, denominator_backward\n'"
uer/targets/cls_target.py,2,"b'# -*- encoding:utf-8 -*-\nimport math\nimport torch\nimport torch.nn as nn\nfrom uer.layers.layer_norm import LayerNorm\nfrom uer.utils.act_fun import gelu\n\n\nclass ClsTarget(nn.Module):\n    """"""\n    """"""\n    def __init__(self, args, vocab_size):\n        super(ClsTarget, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size = args.hidden_size\n\n        self.linear_1 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.linear_2 = nn.Linear(args.hidden_size, args.labels_num)\n        self.softmax = nn.LogSoftmax(dim=-1)\n        self.criterion = nn.NLLLoss()\n\n\n    def forward(self, memory_bank, tgt):\n        """"""\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size]\n\n        Returns:\n            loss: Classification loss.\n            correct: Number of sentences that are predicted correctly.\n        """"""\n\n        output = torch.tanh(self.linear_1(memory_bank[:, 0, :]))\n        logits = self.linear_2(output)\n\n        loss = self.criterion(self.softmax(logits), tgt)\n        correct = self.softmax(logits).argmax(dim=-1).eq(tgt).sum()\n\n        return loss, correct\n'"
uer/targets/lm_target.py,8,"b'# -*- encoding:utf-8 -*-\nimport math\nimport torch\nimport torch.nn as nn\nfrom uer.layers.layer_norm import LayerNorm\nfrom uer.utils.act_fun import gelu\n\n\nclass LmTarget(nn.Module):\n    """"""\n    """"""\n    def __init__(self, args, vocab_size):\n        super(LmTarget, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size = args.hidden_size\n\n        self.softmax = nn.LogSoftmax(dim=-1)\n        self.output_layer = nn.Linear(self.hidden_size, self.vocab_size)\n\n    def forward(self, memory_bank, tgt):\n        """"""\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size x seq_length]\n\n        Returns:\n            loss: Language modeling loss.\n            correct: Number of words that are predicted correctly.\n            denominator: Number of predicted words.\n        """"""\n\n        # Language modeling (LM) with full softmax prediction.\n        output = self.output_layer(memory_bank)\n        output = output.contiguous().view(-1, self.vocab_size)\n        # Full probability distribution.\n        output = self.softmax(output)\n\n        tgt = tgt.contiguous().view(-1,1)\n        label_mask = (tgt > 0).float().to(torch.device(output.device))\n        one_hot = torch.zeros(label_mask.size(0),  self.vocab_size). \\\n           to(torch.device(output.device)). \\\n           scatter_(1, tgt, 1.0)\n\n        numerator = -torch.sum(output * one_hot, 1)\n        label_mask = label_mask.contiguous().view(-1)\n        tgt = tgt.contiguous().view(-1)\n        numerator = torch.sum(label_mask * numerator)\n        denominator = torch.sum(label_mask) + 1e-6\n        loss = numerator / denominator\n        correct = torch.sum(label_mask * (output.argmax(dim=-1).eq(tgt)).float())\n\n        return loss, correct, denominator\n'"
uer/targets/mlm_target.py,8,"b'# -*- encoding:utf-8 -*-\nimport math\nimport torch\nimport torch.nn as nn\nfrom uer.layers.layer_norm import LayerNorm\nfrom uer.utils.act_fun import gelu\n\n\nclass MlmTarget(nn.Module):\n    """"""\n    BERT exploits masked language modeling (MLM) \n    and next sentence prediction (NSP) for pretraining.\n    """"""\n    def __init__(self, args, vocab_size):\n        super(MlmTarget, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size = args.hidden_size\n\n        self.mlm_linear_1 = nn.Linear(args.hidden_size, args.hidden_size)\n        self.layer_norm = LayerNorm(args.hidden_size)\n        self.mlm_linear_2 = nn.Linear(args.hidden_size, self.vocab_size)\n\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n        self.criterion = nn.NLLLoss()\n\n    def mlm(self, memory_bank, tgt_mlm):\n        # Masked language modeling (MLM) with full softmax prediction.\n        output_mlm = gelu(self.mlm_linear_1(memory_bank))\n        output_mlm = self.layer_norm(output_mlm)\n        output_mlm = output_mlm.contiguous().view(-1, self.hidden_size)\n        tgt_mlm = tgt_mlm.contiguous().view(-1)\n        output_mlm = output_mlm[tgt_mlm>0,:]\n        tgt_mlm = tgt_mlm[tgt_mlm>0]\n        output_mlm = self.mlm_linear_2(output_mlm)\n        output_mlm = self.softmax(output_mlm)\n\n        one_hot = torch.zeros(output_mlm.size(0),  self.vocab_size). \\\n           to(torch.device(output_mlm.device)). \\\n           scatter_(1, tgt_mlm.contiguous().view(-1,1), 1.0)\n        numerator = -torch.sum(output_mlm * one_hot, 1)\n        denominator = torch.tensor(output_mlm.size(0) + 1e-6)\n        loss_mlm = torch.sum(numerator) / denominator\n        if output_mlm.size(0) == 0:\n            correct_mlm = torch.tensor(0.0)\n        else:\n            correct_mlm = torch.sum((output_mlm.argmax(dim=-1).eq(tgt_mlm)).float())\n        \n        return loss_mlm, correct_mlm, denominator\n\n    def forward(self, memory_bank, tgt):\n        """"""\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size x seq_length]\n\n        Returns:\n            loss: Masked language modeling loss.\n            correct: Number of words that are predicted correctly.\n            denominator: Number of masked words.\n        """"""\n\n        # Masked language model (MLM).\n        loss, correct, denominator = self.mlm(memory_bank, tgt)\n\n        return loss, correct, denominator\n'"
uer/targets/nsp_target.py,1,"b'# -*- encoding:utf-8 -*-\nimport math\nimport torch\nimport torch.nn as nn\nfrom uer.layers.layer_norm import LayerNorm\nfrom uer.utils.act_fun import gelu\n\n\nclass NspTarget(nn.Module):\n    """"""\n    """"""\n    def __init__(self, args, vocab_size):\n        super(NspTarget, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size = args.hidden_size\n\n        self.linear = nn.Linear(args.hidden_size, args.labels_num)\n        self.softmax = nn.LogSoftmax(dim=-1)\n        self.criterion = nn.NLLLoss()\n\n\n    def forward(self, memory_bank, tgt):\n        """"""\n        Args:\n            memory_bank: [batch_size x seq_length x hidden_size]\n            tgt: [batch_size]\n\n        Returns:\n            loss: Next sentence prediction loss.\n            correct: Number of sentences that are predicted correctly.\n        """"""\n\n        output = self.linear(memory_bank[:, 0, :])\n        loss = self.criterion(self.softmax(output), tgt)\n        correct = self.softmax(output).argmax(dim=-1).eq(tgt).sum()\n\n        return loss, correct\n'"
uer/targets/s2s_target.py,9,"b'# -*- encoding:utf-8 -*-\nimport math\nimport torch\nimport torch.nn as nn\nfrom uer.layers.layer_norm import LayerNorm\nfrom uer.utils.act_fun import gelu\n\n\nclass S2sTarget(nn.Module):\n    """"""\n    """"""\n    def __init__(self, args, vocab_size):\n        super(S2sTarget, self).__init__()\n        self.vocab_size = vocab_size\n        self.hidden_size = args.hidden_size\n\n        self.embedding_layer = nn.Embedding(vocab_size, args.emb_size)\n        self.decoder = nn.LSTM(args.emb_size, args.hidden_size, 1, batch_first=True)\n        self.output_layer = nn.Linear(self.hidden_size, self.vocab_size)\n\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self, memory_bank, tgt):\n\n        emb = self.embedding_layer(tgt[:, :]) # bactch_size, seq_length, emb_size\n        output = []\n        hidden_state = (memory_bank[:,-1,:].unsqueeze(0).contiguous(), memory_bank[:,-1,:].unsqueeze(0).contiguous())\n        for i, emb_i in enumerate(emb.split(1, dim=1)):\n            output_i, hidden_state = self.decoder(emb_i, hidden_state)\n            output.append(self.output_layer(output_i))\n\n        output = torch.cat(output, dim=1)\n\n        output = output.contiguous().view(-1, self.vocab_size)\n        output = self.softmax(output)\n\n        tgt = tgt.contiguous().view(-1,1)\n        label_mask = (tgt > 0).float().to(torch.device(output.device))\n        one_hot = torch.zeros(label_mask.size(0),  self.vocab_size). \\\n           to(torch.device(output.device)). \\\n           scatter_(1, tgt, 1.0)\n\n        numerator = -torch.sum(output * one_hot, 1)\n        label_mask = label_mask.contiguous().view(-1)\n        tgt = tgt.contiguous().view(-1)\n        numerator = torch.sum(label_mask * numerator)\n        denominator = torch.sum(label_mask) + 1e-6\n        loss = numerator / denominator\n        correct = torch.sum(label_mask * (output.argmax(dim=-1).eq(tgt)).float())\n\n        return loss, correct, denominator\n'"
uer/utils/__init__.py,0,b''
uer/utils/act_fun.py,1,b'# -*- encoding:utf-8 -*-\nimport math\nimport torch\n\ndef gelu(x):\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))'
uer/utils/config.py,0,"b'# -*- encoding:utf-8 -*-\nimport json\n\n\ndef load_hyperparam(args):\n    with open(args.config_path, mode=""r"", encoding=""utf-8"") as f:\n        param = json.load(f)\n    args.emb_size = param.get(""emb_size"", 768)\n    args.hidden_size = param.get(""hidden_size"", 768)\n    args.kernel_size = param.get(""kernel_size"", 3)\n    args.block_size = param.get(""block_size"", 2)\n    args.feedforward_size = param.get(""feedforward_size"", 3072)\n    args.heads_num = param.get(""heads_num"", 12)\n    args.layers_num = param.get(""layers_num"", 12)\n    args.dropout = param.get(""dropout"", 0.1)\n\n    return args\n'"
uer/utils/constants.py,0,"b""# -*- encoding:utf-8 -*-\nfrom __future__ import unicode_literals\n\n\n# Special token ids.\nPAD_ID = 0\nUNK_ID = 100\nCLS_ID = 101\nSEP_ID = 102\nMASK_ID = 103\n\n# Special token words.\nPAD_TOKEN = '[PAD]'\nUNK_TOKEN = '[UNK]'\nCLS_TOKEN = '[CLS]'\nSEP_TOKEN = '[SEP]'\nMASK_TOKEN = '[MASK]'\n"""
uer/utils/data.py,17,"b'# -*- encoding:utf-8 -*-\nimport os\nimport torch\nimport codecs\nimport random\nimport pickle\nfrom multiprocessing import Pool\nfrom uer.utils.constants import *\nfrom uer.utils.misc import count_lines\nfrom uer.utils.seed import set_seed\n\n\ndef mask_seq(src, vocab_size):\n    """"""\n    mask input sequence for MLM task\n    args:\n        src: a list of tokens\n        vocab_size: the vocabulary size\n    """"""\n    tgt_mlm = []\n    for (i, token) in enumerate(src):\n        if token == CLS_ID or token == SEP_ID:\n            continue\n        prob = random.random()\n        if prob < 0.15:\n            prob /= 0.15\n            if prob < 0.8:\n                src[i] = MASK_ID\n            elif prob < 0.9:\n                while True:\n                    rdi = random.randint(1, vocab_size-1)\n                    if rdi not in [CLS_ID, SEP_ID, MASK_ID]:\n                        break\n                src[i] = rdi\n            tgt_mlm.append((i, token))\n    return src, tgt_mlm\n\n\ndef merge_dataset(dataset_path, workers_num):\n        # Merge datasets.\n        f_writer = open(dataset_path, ""wb"")\n        for i in range(workers_num):\n            tmp_dataset_reader = open(""dataset-tmp-""+str(i)+"".pt"", ""rb"")\n            while True:\n                tmp_data = tmp_dataset_reader.read(2^20)\n                if tmp_data:\n                    f_writer.write(tmp_data)\n                else:\n                    break\n            tmp_dataset_reader.close()\n            os.remove(""dataset-tmp-""+str(i)+"".pt"")\n        f_writer.close()\n\n\nclass Dataset(object):\n    def __init__(self, args, vocab, tokenizer):\n        self.vocab = vocab\n        self.tokenizer = tokenizer\n        self.corpus_path = args.corpus_path\n        self.dataset_path = args.dataset_path\n        self.seq_length = args.seq_length\n        self.seed = args.seed\n\n    def build_and_save(self, workers_num):\n        """"""\n        Build dataset from the given corpus.\n        Start workers_num processes and each process deals with a part of data.\n        """"""\n        lines_num = count_lines(self.corpus_path)\n        print(""Starting %d workers for building datasets ... "" % workers_num)\n        assert(workers_num >= 1)\n        if workers_num == 1:\n            self.worker(0, 0, lines_num)\n        else:\n            pool = Pool(workers_num)\n            for i in range(workers_num):\n                start = i * lines_num // workers_num\n                end = (i+1) * lines_num // workers_num\n                pool.apply_async(func=self.worker, args=[i, start, end])\n            pool.close()\n            pool.join()\n\n        # Merge datasets.\n        merge_dataset(self.dataset_path, workers_num)\n\n    def worker(self, proc_id, start, end):\n        raise NotImplementedError()\n\n\nclass DataLoader(object):\n    def __init__(self, args, dataset_path, batch_size, proc_id, proc_num, shuffle=False):\n        self.batch_size = batch_size\n        self.instances_buffer_size = args.instances_buffer_size\n        self.proc_id = proc_id\n        self.proc_num = proc_num\n        self.shuffle = shuffle\n        self.f_read = open(dataset_path, ""rb"")\n        self.read_count = 0\n        self.start = 0\n        self.end = 0\n        self.buffer = []\n\n    def _fill_buf(self):\n        try:\n            self.buffer = []\n            while True:\n                instance = pickle.load(self.f_read)\n                self.read_count += 1\n                if (self.read_count - 1) % self.proc_num == self.proc_id:\n                    self.buffer.append(instance)\n                    if len(self.buffer) >= self.instances_buffer_size:\n                        break\n        except EOFError:\n            # Reach file end.\n            self.f_read.seek(0)\n\n        if self.shuffle:\n            random.shuffle(self.buffer)\n        self.start = 0\n        self.end = len(self.buffer)\n\n    def _empty(self):\n        return self.start >= self.end\n\n    def __del__(self):\n        self.f_read.close()\n\n\nclass BertDataset(Dataset):\n    """"""\n    Construct dataset for MLM and NSP tasks from the given corpus.\n    Each document consists of multiple sentences, \n    and each sentence occupies a single line. \n    Documents in corpus must be separated by empty lines.\n    """"""\n    def __init__(self, args, vocab, tokenizer):\n        super(BertDataset, self).__init__(args, vocab, tokenizer)\n        self.docs_buffer_size = args.docs_buffer_size\n        self.dup_factor = args.dup_factor\n        self.short_seq_prob = args.short_seq_prob\n\n    def worker(self, proc_id, start, end):\n        print(""Worker %d is building dataset ... "" % proc_id)\n        set_seed(self.seed)\n        docs_buffer = []\n        document = []\n        pos = 0\n        f_write = open(""dataset-tmp-"" + str(proc_id) + "".pt"", ""wb"")\n        with open(self.corpus_path, mode=""r"", encoding=""utf-8"") as f:\n            while pos < start:\n                try:\n                    f.readline()\n                except:\n                    continue\n                finally:\n                    pos += 1\n            while True:\n                try:\n                    line = f.readline()\n                except:\n                    continue\n                finally:\n                    pos += 1\n                if not line.strip():\n                    if len(document) >= 1:\n                        docs_buffer.append(document)\n                    document = []\n                    if len(docs_buffer) == self.docs_buffer_size:\n                        # Build instances from documents.                    \n                        instances = self.build_instances(docs_buffer)\n                        # Save instances.\n                        for instance in instances:\n                            pickle.dump(instance, f_write)\n                        # Clear buffer.\n                        docs_buffer = []\n                        instances = []\n                    continue\n                sentence = [self.vocab.get(w) for w in self.tokenizer.tokenize(line)]\n                if len(sentence) > 0:\n                    document.append(sentence)\n        \n                if pos >= end - 1:\n                    if len(docs_buffer) > 0:\n                        instances = self.build_instances(docs_buffer)\n                        for instance in instances:\n                            pickle.dump(instance, f_write)\n                    break\n        f_write.close()\n\n    def build_instances(self, all_documents):\n        instances = []\n        for _ in range(self.dup_factor):\n            for doc_index in range(len(all_documents)):\n                instances.extend(self.create_ins_from_doc(all_documents, doc_index))\n        return instances\n\n    def create_ins_from_doc(self, all_documents, document_index):\n        document = all_documents[document_index]\n        max_num_tokens = self.seq_length - 3\n        target_seq_length = max_num_tokens\n        if random.random() < self.short_seq_prob:\n            target_seq_length = random.randint(2, max_num_tokens)\n        instances = []\n        current_chunk = []\n        current_length = 0\n        i = 0\n        while i < len(document):\n            segment = document[i]\n            current_chunk.append(segment)\n            current_length += len(segment)\n            if i == len(document) - 1 or current_length >= target_seq_length:\n                if current_chunk:\n                    a_end = 1\n                    if len(current_chunk) >= 2:\n                        a_end = random.randint(1, len(current_chunk) - 1)\n\n                    tokens_a = []\n                    for j in range(a_end):\n                        tokens_a.extend(current_chunk[j])\n\n                    tokens_b = []\n                    is_random_next = 0\n\n                    # Random next\n                    if len(current_chunk) == 1 or random.random() < 0.5:\n                        is_random_next = 1\n                        target_b_length = target_seq_length - len(tokens_a)\n\n                        for _ in range(10):\n                            random_document_index = random.randint(0, len(all_documents) - 1)\n                            if random_document_index != document_index:\n                                break\n\n                        random_document = all_documents[random_document_index]\n                        random_start = random.randint(0, len(random_document) - 1)\n                        for j in range(random_start, len(random_document)):\n                            tokens_b.extend(random_document[j])\n                            if len(tokens_b) >= target_b_length:\n                                break\n\n                        num_unused_segments = len(current_chunk) - a_end\n                        i -= num_unused_segments\n\n                    # Actual next\n                    else:\n                        is_random_next = 0\n                        for j in range(a_end, len(current_chunk)):\n                            tokens_b.extend(current_chunk[j])\n\n                    self.truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n\n                    # assert len(tokens_a) >= 1\n                    # assert len(tokens_b) >= 1\n\n                    src = []\n\n                    src.append(CLS_ID)\n                    for token in tokens_a:\n                        src.append(token)\n\n                    src.append(SEP_ID)\n\n                    seg_pos = [len(src)]\n\n                    for token in tokens_b:\n                        src.append(token)\n            \n                    src.append(SEP_ID)\n\n                    seg_pos.append(len(src))\n\n                    src, tgt_mlm = mask_seq(src, len(self.vocab))\n                    \n                    while len(src) != self.seq_length:\n                        src.append(PAD_ID)\n\n                    instance = (src, tgt_mlm, is_random_next, seg_pos)\n                    instances.append(instance)\n                current_chunk = []\n                current_length = 0\n            i += 1\n        return instances\n\n    def truncate_seq_pair(self, tokens_a, tokens_b, max_num_tokens):\n        """""" truncate sequence pair to specific length """"""\n        while True:\n            total_length = len(tokens_a) + len(tokens_b)\n            if total_length <= max_num_tokens:\n                break\n                \n            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n            # assert len(trunc_tokens) >= 1\n\n            if random.random() < 0.5:\n                del trunc_tokens[0]\n            else:\n                trunc_tokens.pop()\n\n\nclass BertDataLoader(DataLoader):\n    def __iter__(self):\n        while True:\n            while self._empty():\n                self._fill_buf()\n            if self.start + self.batch_size >= self.end:\n                instances = self.buffer[self.start:]\n            else:\n                instances = self.buffer[self.start: self.start + self.batch_size]\n\n            self.start += self.batch_size\n        \n            src = []\n            tgt_mlm = []\n            is_next = []\n            seg = []\n\n            masked_words_num = 0\n            for ins in instances:\n                masked_words_num += len(ins[1])\n            if masked_words_num == 0:\n                continue\n            \n            for ins in instances:\n                src.append(ins[0])\n                tgt_mlm.append([0]*len(ins[0]))\n                for mask in ins[1]:\n                    tgt_mlm[-1][mask[0]] = mask[1]\n                is_next.append(ins[2])\n                seg.append([1]*ins[3][0] + [2]*(ins[3][1]-ins[3][0]) + [PAD_ID]*(len(ins[0])-ins[3][1]))\n\n            yield torch.LongTensor(src), \\\n                torch.LongTensor(tgt_mlm), \\\n                torch.LongTensor(is_next), \\\n                torch.LongTensor(seg)\n\n\nclass LmDataset(Dataset):\n    def worker(self, proc_id, start, end):\n        print(""Worker %d is building dataset ... "" % proc_id)\n        set_seed(self.seed)\n        pos = 0\n        f_write = open(""dataset-tmp-"" + str(proc_id) + "".pt"", ""wb"")\n        with open(self.corpus_path, mode=""r"", encoding=""utf-8"") as f:\n            while pos < start:\n                try:\n                    f.readline()\n                except:\n                    continue\n                finally:\n                    pos += 1\n            while True:\n                try:\n                    line = f.readline()\n                except:\n                    continue\n                finally:\n                    pos += 1\n\n                src = [self.vocab.get(w) for w in self.tokenizer.tokenize(line)]\n                tgt = src[1:]\n                src = src[:-1]\n                seg = [1] * len(src)\n                if len(src) >= self.seq_length:\n                    src = src[:self.seq_length]\n                    tgt = tgt[:self.seq_length]\n                    seg = seg[:self.seq_length]\n                else:\n                    while len(src) != self.seq_length:\n                        src.append(PAD_ID)\n                        tgt.append(PAD_ID)\n                        seg.append(PAD_ID)\n\n                pickle.dump((src, tgt, seg), f_write)\n\n                if pos >= end - 1:\n                    break\n\n        f_write.close()\n\n\nclass LmDataLoader(DataLoader):\n    def __iter__(self):\n        while True:\n            while self._empty():\n                self._fill_buf()\n            if self.start + self.batch_size >= self.end:\n                instances = self.buffer[self.start:]\n            else:\n                instances = self.buffer[self.start: self.start + self.batch_size]\n\n            self.start += self.batch_size\n        \n            src = []\n            tgt = []\n            seg = []\n\n            for ins in instances:\n                src.append(ins[0])\n                tgt.append(ins[1])\n                seg.append(ins[2])\n\n            yield torch.LongTensor(src), \\\n                torch.LongTensor(tgt), \\\n                torch.LongTensor(seg)\n\n\nclass BilmDataset(Dataset):\n    def worker(self, proc_id, start, end):\n        print(""Worker %d is building dataset ... "" % proc_id)\n        set_seed(self.seed)\n        pos = 0\n        f_write = open(""dataset-tmp-"" + str(proc_id) + "".pt"", ""wb"")\n        with open(self.corpus_path, mode=""r"", encoding=""utf-8"") as f:\n            while pos < start:\n                try:\n                    f.readline()\n                except:\n                    continue\n                finally:\n                    pos += 1\n            while True:\n                try:\n                    line = f.readline()\n                except:\n                    continue\n                finally:\n                    pos += 1\n\n                src = [self.vocab.get(w) for w in self.tokenizer.tokenize(line)]\n                if len(src) < 1:\n                    continue\n                tgt_forward = src[1:] + [SEP_ID]\n                tgt_backward = [CLS_ID] + src[:-1]\n                seg = [1] * len(src)\n                if len(src) >= self.seq_length:\n                    src = src[:self.seq_length]\n                    tgt_forward = tgt_forward[:self.seq_length]\n                    tgt_backward = tgt_backward[:self.seq_length]\n                    seg = seg[:self.seq_length]\n                else:\n                    while len(src) != self.seq_length:\n                        src.append(PAD_ID)\n                        tgt_forward.append(PAD_ID)\n                        tgt_backward.append(PAD_ID)\n                        seg.append(PAD_ID)\n                \n                pickle.dump((src, tgt_forward, tgt_backward, seg), f_write)\n\n                if pos >= end - 1:\n                    break\n\n        f_write.close()\n\n\nclass BilmDataLoader(DataLoader):\n    def __iter__(self):\n        while True:\n            while self._empty():\n                self._fill_buf()\n            if self.start + self.batch_size >= self.end:\n                instances = self.buffer[self.start:]\n            else:\n                instances = self.buffer[self.start: self.start + self.batch_size]\n\n            self.start += self.batch_size\n        \n            src = []\n            tgt_forward = []\n            tgt_backward = []\n            seg = []\n\n            for ins in instances:\n                src.append(ins[0])\n                tgt_forward.append(ins[1])\n                tgt_backward.append(ins[2])\n                seg.append(ins[3])\n\n            yield torch.LongTensor(src), \\\n                torch.LongTensor(tgt_forward), \\\n                torch.LongTensor(tgt_backward), \\\n                torch.LongTensor(seg)\n\n\nclass ClsDataset(Dataset):\n    def worker(self, proc_id, start, end):\n        print(""Worker %d is building dataset ... "" % proc_id)\n        set_seed(self.seed)\n        pos = 0\n        f_write = open(""dataset-tmp-"" + str(proc_id) + "".pt"", ""wb"")\n        with open(self.corpus_path, mode=""r"", encoding=""utf-8"") as f:\n            while pos < start:\n                try:\n                    f.readline()\n                except:\n                    continue\n                finally:\n                    pos += 1\n            while True:\n                try:\n                    line = f.readline()\n                except:\n                    continue\n                finally:\n                    pos += 1\n\n                line = line.strip().split(\'\\t\')\n                if len(line) == 2:\n                    label = int(line[0])\n                    text = "" "".join(line[1:])\n                    src = [self.vocab.get(t) for t in self.tokenizer.tokenize(text)]\n                    src = [CLS_ID] + src\n                    tgt = label\n                    seg = [1] * len(src)\n                    if len(src) >= self.seq_length:\n                        src = src[:self.seq_length]\n                        seg = seg[:self.seq_length]\n                    else:\n                        while len(src) != self.seq_length:\n                            src.append(PAD_ID)\n                            seg.append(PAD_ID)\n                    pickle.dump((src, tgt, seg), f_write)\n                elif len(line) == 3: # For sentence pair input.\n                    label = int(line[0])\n                    text_a, text_b = line[1], line[2]\n\n                    src_a = [self.vocab.get(t) for t in self.tokenizer.tokenize(text_a)]\n                    src_a = [CLS_ID] + tokens_a + [SEP_ID]\n                    src_b = [vocab.get(t) for t in tokenizer.tokenize(text_b)]\n                    src_b = tokens_b + [SEP_ID]\n\n                    src = src_a + src_b\n                    seg = [1] * len(src_a) + [2] * len(src_b)\n\n                    if len(src) >= self.seq_length:\n                        src = src[:self.seq_length]\n                        seg = seg[:self.seq_length]\n                    else:\n                        while len(src) != self.seq_length:\n                            src.append(PAD_ID)\n                            seg.append(PAD_ID)\n                    pickle.dump((src, tgt, seg), f_write)\n                else:\n                    pass\n\n                if pos >= end - 1:\n                    break\n\n        f_write.close()\n\n\nclass ClsDataLoader(DataLoader):\n    def __iter__(self):\n        while True:\n            while self._empty():\n                self._fill_buf()\n            if self.start + self.batch_size >= self.end:\n                instances = self.buffer[self.start:]\n            else:\n                instances = self.buffer[self.start: self.start + self.batch_size]\n\n            self.start += self.batch_size\n        \n            src = []\n            tgt = []\n            seg = []\n\n            for ins in instances:\n                src.append(ins[0])\n                tgt.append(ins[1])\n                seg.append(ins[2])\n\n            yield torch.LongTensor(src), \\\n                torch.LongTensor(tgt), \\\n                torch.LongTensor(seg)\n\n\nclass MlmDataset(Dataset):\n    def __init__(self, args, vocab, tokenizer):\n        super(MlmDataset, self).__init__(args, vocab, tokenizer)\n        self.dup_factor = args.dup_factor\n\n    def worker(self, proc_id, start, end):\n        print(""Worker %d is building dataset ... "" % proc_id)\n        set_seed(self.seed)\n        f_write = open(""dataset-tmp-"" + str(proc_id) + "".pt"", ""wb"")\n        for _ in range(self.dup_factor):\n            pos = 0\n            with open(self.corpus_path, mode=""r"", encoding=""utf-8"") as f:\n                while pos < start:\n                    try:\n                        f.readline()\n                    except:\n                        continue\n                    finally:\n                        pos += 1\n                while True:\n                    try:\n                        line = f.readline()\n                    except:\n                        continue\n                    finally:\n                        pos += 1\n\n                    src = [self.vocab.get(w) for w in self.tokenizer.tokenize(line)]\n\n                    if len(src) > self.seq_length:\n                        src = src[:self.seq_length]\n                    seg = [1] * len(src)\n\n                    src, tgt = mask_seq(src, len(self.vocab))\n\n                    while len(src) != self.seq_length:\n                        src.append(PAD_ID)\n                        seg.append(PAD_ID)\n\n                    pickle.dump((src, tgt, seg), f_write)\n\n                    if pos >= end - 1:\n                        break\n\n        f_write.close()\n\n\nclass MlmDataLoader(DataLoader):\n    def __iter__(self):\n        while True:\n            while self._empty():\n                self._fill_buf()\n            if self.start + self.batch_size >= self.end:\n                instances = self.buffer[self.start:]\n            else:\n                instances = self.buffer[self.start: self.start + self.batch_size]\n\n            self.start += self.batch_size\n\n            src = []\n            tgt = []\n            seg = []\n\n            masked_words_num = 0\n            for ins in instances:\n                masked_words_num += len(ins[1])\n            if masked_words_num == 0:\n                continue\n\n            for ins in instances:\n                src.append(ins[0])\n                seg.append(ins[2])\n                tgt.append([0]*len(ins[0]))\n                for mask in ins[1]:\n                    tgt[-1][mask[0]] = mask[1]\n\n            yield torch.LongTensor(src), \\\n                torch.LongTensor(tgt), \\\n                torch.LongTensor(seg)\n'"
uer/utils/misc.py,3,"b""# -*- encoding:utf-8 -*-\r\nimport torch\r\nimport torch.nn as nn\r\n\r\n\r\ndef count_lines(file_path):\r\n    lines_num = 0\r\n    with open(file_path, 'rb') as f:\r\n        while True:\r\n            data = f.read(2^20)\r\n            if not data:\r\n                break\r\n            lines_num += data.count(b'\\n')\r\n    return lines_num\r\n\r\n\r\ndef flip(x, dim):\r\n    indices = [slice(None)] * x.dim()\r\n    indices[dim] = torch.arange(x.size(dim) - 1, -1, -1,\r\n                                dtype=torch.long, device=x.device)\r\n    return x[tuple(indices)]\r\n"""
uer/utils/optimizers.py,6,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch optimization for BERT model.""""""\n\nimport math\n\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\nclass ConstantLRSchedule(LambdaLR):\n    """""" Constant learning rate schedule.\n    """"""\n    def __init__(self, optimizer, last_epoch=-1):\n        super(ConstantLRSchedule, self).__init__(optimizer, lambda _: 1.0, last_epoch=last_epoch)\n\n\nclass WarmupConstantSchedule(LambdaLR):\n    """""" Linear warmup and then constant.\n        Multiplies the learning rate defined in the optimizer by a dynamic variable determined by the current step.\n        Linearly increases the multiplicative variable from 0. to 1. over `warmup_steps` training steps.\n        Keeps multiplicative variable equal to 1. after warmup_steps.\n    """"""\n    def __init__(self, optimizer, warmup_steps, last_epoch=-1):\n        self.warmup_steps = warmup_steps\n        super(WarmupConstantSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n\n    def lr_lambda(self, step):\n        if step < self.warmup_steps:\n            return float(step) / float(max(1.0, self.warmup_steps))\n        return 1.\n\n\nclass WarmupLinearSchedule(LambdaLR):\n    """""" Linear warmup and then linear decay.\n        Multiplies the learning rate defined in the optimizer by a dynamic variable determined by the current step.\n        Linearly increases the multiplicative variable from 0. to 1. over `warmup_steps` training steps.\n        Linearly decreases the multiplicative variable from 1. to 0. over remaining `t_total - warmup_steps` steps.\n    """"""\n    def __init__(self, optimizer, warmup_steps, t_total, last_epoch=-1):\n        self.warmup_steps = warmup_steps\n        self.t_total = t_total\n        super(WarmupLinearSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n\n    def lr_lambda(self, step):\n        if step < self.warmup_steps:\n            return float(step) / float(max(1, self.warmup_steps))\n        return max(0.0, float(self.t_total - step) / float(max(1.0, self.t_total - self.warmup_steps)))\n\n\nclass WarmupCosineSchedule(LambdaLR):\n    """""" Linear warmup and then cosine decay.\n        Multiplies the learning rate defined in the optimizer by a dynamic variable determined by the current step.\n        Linearly increases the multiplicative variable from 0. to 1. over `warmup_steps` training steps.\n        Decreases the multiplicative variable from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.\n        If `cycles` (default=0.5) is different from default, then the multiplicative variable follows cosine function after warmup.\n    """"""\n    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n        self.warmup_steps = warmup_steps\n        self.t_total = t_total\n        self.cycles = cycles\n        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n\n    def lr_lambda(self, step):\n        if step < self.warmup_steps:\n            return float(step) / float(max(1.0, self.warmup_steps))\n        # progress after warmup\n        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n\n\nclass WarmupCosineWithHardRestartsSchedule(LambdaLR):\n    """""" Linear warmup and then cosine cycles with hard restarts.\n        Multiplies the learning rate defined in the optimizer by a dynamic variable determined by the current step.\n        Linearly increases the multiplicative variable from 0. to 1. over `warmup_steps` training steps.\n        If `cycles` (default=1.) is different from default, learning rate  follows `cycles` times a cosine decaying\n        learning rate (with hard restarts).\n    """"""\n    def __init__(self, optimizer, warmup_steps, t_total, cycles=1., last_epoch=-1):\n        self.warmup_steps = warmup_steps\n        self.t_total = t_total\n        self.cycles = cycles\n        super(WarmupCosineWithHardRestartsSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n\n    def lr_lambda(self, step):\n        if step < self.warmup_steps:\n            return float(step) / float(max(1, self.warmup_steps))\n        # progress after warmup\n        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n        if progress >= 1.0:\n            return 0.0\n        return max(0.0, 0.5 * (1. + math.cos(math.pi * ((float(self.cycles) * progress) % 1.0))))\n\n\n\nclass AdamW(Optimizer):\n    """""" Implements Adam algorithm with weight decay fix.\n    Parameters:\n        lr (float): learning rate. Default 1e-3.\n        betas (tuple of 2 floats): Adams beta parameters (b1, b2). Default: (0.9, 0.999)\n        eps (float): Adams epsilon. Default: 1e-6\n        weight_decay (float): Weight decay. Default: 0.0\n        correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\n    """"""\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n        if lr < 0.0:\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter: {} - should be in [0.0, 1.0["".format(betas[0]))\n        if not 0.0 <= betas[1]  < 1.0:\n            raise ValueError(""Invalid beta parameter: {} - should be in [0.0, 1.0["".format(betas[1]))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(eps))\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n                        correct_bias=correct_bias)\n        super(AdamW, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n                denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                step_size = group[\'lr\']\n                if group[\'correct_bias\']:  # No bias correction for Bert\n                    bias_correction1 = 1.0 - beta1 ** state[\'step\']\n                    bias_correction2 = 1.0 - beta2 ** state[\'step\']\n                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want to decay the weights in a manner that doesn\'t interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                # Add weight decay at the end (fixed version)\n                if group[\'weight_decay\'] > 0.0:\n                    p.data.add_(-group[\'lr\'] * group[\'weight_decay\'], p.data)\n\n        return loss\n\n\n# Lamb doesn\'t perform well in practice, which is also mentioned in other reports. \nclass Lamb(Optimizer):\n    """""" Implements Lamb algorithm.\n    Parameters:\n        lr (float): learning rate. Default 1e-3.\n        betas (tuple of 2 floats): Adams beta parameters (b1, b2). Default: (0.9, 0.999)\n        eps (float): Adams epsilon. Default: 1e-6\n        weight_decay (float): Weight decay. Default: 0.0\n        correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\n    """"""\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n        if lr < 0.0:\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter: {} - should be in [0.0, 1.0["".format(betas[0]))\n        if not 0.0 <= betas[1]  < 1.0:\n            raise ValueError(""Invalid beta parameter: {} - should be in [0.0, 1.0["".format(betas[1]))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(eps))\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n                        correct_bias=correct_bias)\n        super(Lamb, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n                denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                step_size = group[\'lr\']\n                if group[\'correct_bias\']:  # No bias correction for Bert\n                    bias_correction1 = 1.0 - beta1 ** state[\'step\']\n                    bias_correction2 = 1.0 - beta2 ** state[\'step\']\n                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n\n                weight_norm = p.data.pow(2).sum().sqrt()\n\n                update = exp_avg / denom\n                if group[\'weight_decay\'] > 0.0:\n                    update.add_(group[\'weight_decay\'], p.data)\n\n                update_norm = update.pow(2).sum().sqrt()\n\n                trust_ratio = 1 if weight_norm == 0 or update_norm == 0 else min(weight_norm/update_norm, 10)\n\n                p.data.add_(-step_size * trust_ratio, update)\n\n        return loss'"
uer/utils/seed.py,3,"b""import random\nimport os\nimport numpy as np\nimport torch\n\ndef set_seed(seed=7):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n"""
uer/utils/subword.py,2,"b""# -*- encoding:utf-8 -*-\nimport torch\nimport torch.nn as nn\nfrom uer.utils.constants import *\n\n\ndef word2sub(word_ids, vocab, sub_vocab, subword_type):\n    '''\n    word_ids: batch_size, seq_length\n    '''\n    batch_size, seq_length = word_ids.size()\n    device = word_ids.device\n    word_ids = word_ids.contiguous().view(-1).tolist()\n    words = [vocab.i2w[i] for i in word_ids]\n    max_length = max([len(w) for w in words])\n    sub_ids = torch.zeros((len(words), max_length), dtype=torch.long).to(device)\n    for i in range(len(words)):\n        for j, c in enumerate(words[i]):\n            sub_ids[i, j] = sub_vocab.w2i.get(c, UNK_ID)\n    return sub_ids\n"""
uer/utils/tokenizer.py,0,"b'# -*- encoding:utf-8 -*-\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nfrom uer.utils.constants import *\nfrom uer.utils.vocab import Vocab\nimport collections\nimport unicodedata\n\n\nclass Tokenizer(object):\n    \n    def __init__(self, args):\n        pass\n\n    def tokenize(self, text):\n        raise NotImplementedError\n\n\nclass CharTokenizer(Tokenizer):\n    \n    def __init__(self, args):\n        super().__init__(args)\n\n    def tokenize(self, text):\n        return list(text.strip())\n\n\nclass SpaceTokenizer(Tokenizer):\n   \n    def __init__(self, args):\n        super().__init__(args)\n\n    def tokenize(self, text):\n        """"""\n        Splitting the sentence into words according to space.\n        """"""\n        return text.strip().split("" "")\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass BertTokenizer(object):\n    """"""Runs end-to-end tokenization: punctuation splitting + wordpiece""""""\n\n    def __init__(self, args, do_lower_case=True, max_len=None, do_basic_tokenize=True,\n                 never_split=(""[UNK]"", ""[SEP]"", ""[PAD]"", ""[CLS]"", ""[MASK]"")):\n        """"""Constructs a BertTokenizer.\n        Args:\n          vocab_file: Path to a one-wordpiece-per-line vocabulary file\n          do_lower_case: Whether to lower case the input\n                         Only has an effect when do_wordpiece_only=False\n          do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n          max_len: An artificial maximum length to truncate tokenized sequences to;\n                         Effective maximum length is always the minimum of this\n                         value (if specified) and the underlying BERT model\'s\n                         sequence length.\n          never_split: List of tokens which will never be split during tokenization.\n                         Only has an effect when do_wordpiece_only=False\n        """"""\n        self.vocab = Vocab()\n        self.vocab.load(args.vocab_path, is_quiet=True)\n        self.ids_to_tokens = collections.OrderedDict(\n            [(ids, tok) for ids, tok in enumerate(self.vocab.i2w)])\n        self.do_basic_tokenize = do_basic_tokenize\n        if do_basic_tokenize:\n          self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n                                                never_split=never_split)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n        self.max_len = max_len if max_len is not None else int(1e12)\n\n    def tokenize(self, text):\n        if self.do_basic_tokenize:\n          split_tokens = []\n          for token in self.basic_tokenizer.tokenize(text):\n              for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                  split_tokens.append(sub_token)\n        else:\n          split_tokens = self.wordpiece_tokenizer.tokenize(text)\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        """"""Converts a sequence of tokens into ids using the vocab.""""""\n        ids = []\n        for token in tokens:\n            ids.append(self.vocab.w2i[token])\n        if len(ids) > self.max_len:\n            logger.warning(\n                ""Token indices sequence length is longer than the specified maximum ""\n                "" sequence length for this BERT model ({} > {}). Running this""\n                "" sequence through BERT will result in indexing errors"".format(len(ids), self.max_len)\n            )\n        return ids\n\n    def convert_ids_to_tokens(self, ids):\n        """"""Converts a sequence of ids in wordpiece tokens using the vocab.""""""\n        tokens = []\n        for i in ids:\n            tokens.append(self.ids_to_tokens[i])\n        return tokens\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self,\n                 do_lower_case=True,\n                 never_split=(""[UNK]"", ""[SEP]"", ""[PAD]"", ""[CLS]"", ""[MASK]"")):\n        """"""Constructs a BasicTokenizer.\n        Args:\n          do_lower_case: Whether to lower case the input.\n        """"""\n        self.do_lower_case = do_lower_case\n        self.never_split = never_split\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text.""""""\n        text = self._clean_text(text)\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn\'t\n        # matter since the English models were not trained on any Chinese data\n        # and generally don\'t have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case and token not in self.never_split:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text):\n        """"""Splits punctuation on a piece of text.""""""\n        if text in self.never_split:\n            return [text]\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        """"""Adds whitespace around any CJK character.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    def _is_chinese_char(self, cp):\n        """"""Checks whether CP is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenization.""""""\n\n    def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab.w2i:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n\n\n'"
uer/utils/vocab.py,0,"b'# -*- encoding:utf-8 -*-\r\nimport os\r\nimport torch\r\nfrom multiprocessing import Pool\r\nfrom uer.utils.constants import *\r\nfrom uer.utils.misc import count_lines\r\n\r\n\r\nclass Vocab(object):\r\n    """"""\r\n    """"""\r\n    def __init__(self):\r\n        self.w2i = {} \r\n        self.i2w = [] \r\n        self.w2c = {} \r\n        self.reserved_vocab_path = \\\r\n            os.path.abspath(os.path.join(os.path.dirname(__file__), ""../../models/reserved_vocab.txt""))\r\n        \r\n    def load(self, vocab_path, is_quiet=False):\r\n        with open(vocab_path, mode=""r"", encoding=""utf-8"") as reader:\r\n            for index, line in enumerate(reader):\r\n                try:\r\n                    w = line.strip().split()[0]\r\n                    self.w2i[w] = index\r\n                    self.i2w.append(w)\r\n                except:\r\n                    self.w2i[""???""+str(index)] = index\r\n                    self.i2w.append(""???""+str(index))\r\n                    if not is_quiet:\r\n                        print(""Vocabulary file line "" + str(index+1) + "" has bad format token"")\r\n            assert len(self.w2i) == len(self.i2w)\r\n        if not is_quiet:\r\n            print(""Vocabulary Size: "", len(self))\r\n\r\n    def save(self, save_path):\r\n        print(""Vocabulary Size: "", len(self))\r\n        with open(save_path, mode=""w"", encoding=""utf-8"") as writer:\r\n            for w in self.i2w:\r\n                writer.write(w + ""\\n"")\r\n        print(""Vocabulary saving done."")\r\n\r\n    def get(self, w):\r\n        return self.w2i.get(w, UNK_ID)\r\n        \r\n    def __len__(self):\r\n        return len(self.i2w)\r\n        \r\n    def worker(self, corpus_path, tokenizer, start, end):\r\n        """""" \r\n        Worker that creates vocabulary from corpus[start:end].\r\n        """"""\r\n        w2i, i2w, w2c = {}, [], {}\r\n        pos = 0\r\n        with open(corpus_path, mode=""r"", encoding=""utf-8"") as f:\r\n            while pos < start:\r\n               try:\r\n                   f.readline()\r\n               except:\r\n                   continue\r\n               finally:\r\n                   pos += 1\r\n            while True:\r\n                try:\r\n                    line = f.readline()\r\n                except:\r\n                    continue\r\n                finally:\r\n                   pos += 1\r\n\r\n                tokens = tokenizer.tokenize(line)\r\n                for t in tokens:\r\n                    if t not in w2i:\r\n                        w2i[t], w2c[t] = len(i2w), 1\r\n                        i2w.append(t)\r\n                    else:\r\n                        w2c[t] += 1\r\n                if pos >= end - 1:\r\n                    return (w2i, i2w, w2c)\r\n                            \r\n    def union(self, vocab_list):\r\n        """""" Union vocab in all workers. """"""\r\n        w2i, i2w, w2c = {}, [], {}\r\n        index = 0\r\n        for v_p in vocab_list:\r\n            w2i_p, i2w_p, w2c_p = v_p.get()\r\n            for w in i2w_p:\r\n                if w not in w2i:\r\n                    w2i[w], w2c[w] = len(i2w), w2c_p[w]\r\n                    i2w.append(w)\r\n                else:\r\n                    w2c[w] += w2c_p[w]\r\n        return (w2i, i2w, w2c)\r\n                    \r\n    def build(self, corpus_path, tokenizer, workers_num=1, min_count=1):\r\n        """""" Build vocabulary from the given corpus. """"""\r\n        print(""Start %d workers for building vocabulary..."" % workers_num)\r\n        lines_num = count_lines(corpus_path)\r\n        pool = Pool(workers_num)\r\n        vocab_list = []\r\n        for i in range(workers_num):\r\n            start = i * lines_num // workers_num\r\n            end = (i+1) * lines_num // workers_num\r\n            vocab_list.append((pool.apply_async(func=self.worker, args=[corpus_path, tokenizer, start, end])))\r\n        pool.close()\r\n        pool.join()\r\n        \r\n        # Union vocab in all workers.\r\n        w2i, i2w, w2c = self.union(vocab_list)\r\n        # Sort w2c according to word count.\r\n        sorted_w2c = sorted(w2c.items(), key=lambda item:item[1], reverse=True)\r\n\r\n        # Add special symbols and remove low frequency words.\r\n        with open(self.reserved_vocab_path, mode=""r"", encoding=""utf-8"") as reader:\r\n            self.i2w = [line.strip().split()[0] for line in reader]\r\n\r\n        for i, w in enumerate(self.i2w):\r\n            self.w2i[w] = i\r\n            self.w2c[w] = -1\r\n\r\n        for w, c in sorted_w2c:\r\n            if c < min_count:\r\n                break\r\n            if w not in self.w2i:\r\n                self.w2i[w], self.w2c[w] = len(self.i2w), c\r\n                self.i2w.append(w)\r\n                \r\n'"
