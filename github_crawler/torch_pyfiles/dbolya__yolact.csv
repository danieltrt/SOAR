file_path,api_count,code
backbone.py,5,"b'import torch\nimport torch.nn as nn\nimport pickle\n\nfrom collections import OrderedDict\n\ntry:\n    from dcn_v2 import DCN\nexcept ImportError:\n    def DCN(*args, **kwdargs):\n        raise Exception(\'DCN could not be imported. If you want to use YOLACT++ models, compile DCN. Check the README for instructions.\')\n\nclass Bottleneck(nn.Module):\n    """""" Adapted from torchvision.models.resnet """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d, dilation=1, use_dcn=False):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False, dilation=dilation)\n        self.bn1 = norm_layer(planes)\n        if use_dcn:\n            self.conv2 = DCN(planes, planes, kernel_size=3, stride=stride,\n                                padding=dilation, dilation=dilation, deformable_groups=1)\n            self.conv2.bias.data.zero_()\n            self.conv2.conv_offset_mask.weight.data.zero_()\n            self.conv2.conv_offset_mask.bias.data.zero_()\n        else:\n            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                                padding=dilation, bias=False, dilation=dilation)\n        self.bn2 = norm_layer(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False, dilation=dilation)\n        self.bn3 = norm_layer(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNetBackbone(nn.Module):\n    """""" Adapted from torchvision.models.resnet """"""\n\n    def __init__(self, layers, dcn_layers=[0, 0, 0, 0], dcn_interval=1, atrous_layers=[], block=Bottleneck, norm_layer=nn.BatchNorm2d):\n        super().__init__()\n\n        # These will be populated by _make_layer\n        self.num_base_layers = len(layers)\n        self.layers = nn.ModuleList()\n        self.channels = []\n        self.norm_layer = norm_layer\n        self.dilation = 1\n        self.atrous_layers = atrous_layers\n\n        # From torchvision.models.resnet.Resnet\n        self.inplanes = 64\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self._make_layer(block, 64, layers[0], dcn_layers=dcn_layers[0], dcn_interval=dcn_interval)\n        self._make_layer(block, 128, layers[1], stride=2, dcn_layers=dcn_layers[1], dcn_interval=dcn_interval)\n        self._make_layer(block, 256, layers[2], stride=2, dcn_layers=dcn_layers[2], dcn_interval=dcn_interval)\n        self._make_layer(block, 512, layers[3], stride=2, dcn_layers=dcn_layers[3], dcn_interval=dcn_interval)\n\n        # This contains every module that should be initialized by loading in pretrained weights.\n        # Any extra layers added onto this that won\'t be initialized by init_backbone will not be\n        # in this list. That way, Yolact::init_weights knows which backbone weights to initialize\n        # with xavier, and which ones to leave alone.\n        self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]\n        \n    \n    def _make_layer(self, block, planes, blocks, stride=1, dcn_layers=0, dcn_interval=1):\n        """""" Here one layer means a string of n Bottleneck blocks. """"""\n        downsample = None\n\n        # This is actually just to create the connection between layers, and not necessarily to\n        # downsample. Even if the second condition is met, it only downsamples when stride != 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if len(self.layers) in self.atrous_layers:\n                self.dilation += 1\n                stride = 1\n            \n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False,\n                          dilation=self.dilation),\n                self.norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        use_dcn = (dcn_layers >= blocks)\n        layers.append(block(self.inplanes, planes, stride, downsample, self.norm_layer, self.dilation, use_dcn=use_dcn))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            use_dcn = ((i+dcn_layers) >= blocks) and (i % dcn_interval == 0)\n            layers.append(block(self.inplanes, planes, norm_layer=self.norm_layer, use_dcn=use_dcn))\n        layer = nn.Sequential(*layers)\n\n        self.channels.append(planes * block.expansion)\n        self.layers.append(layer)\n\n        return layer\n\n    def forward(self, x):\n        """""" Returns a list of convouts for each layer. """"""\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        outs = []\n        for layer in self.layers:\n            x = layer(x)\n            outs.append(x)\n\n        return tuple(outs)\n\n    def init_backbone(self, path):\n        """""" Initializes the backbone weights for training. """"""\n        state_dict = torch.load(path)\n\n        # Replace layer1 -> layers.0 etc.\n        keys = list(state_dict)\n        for key in keys:\n            if key.startswith(\'layer\'):\n                idx = int(key[5])\n                new_key = \'layers.\' + str(idx-1) + key[6:]\n                state_dict[new_key] = state_dict.pop(key)\n\n        # Note: Using strict=False is berry scary. Triple check this.\n        self.load_state_dict(state_dict, strict=False)\n\n    def add_layer(self, conv_channels=1024, downsample=2, depth=1, block=Bottleneck):\n        """""" Add a downsample layer to the backbone as per what SSD does. """"""\n        self._make_layer(block, conv_channels // block.expansion, blocks=depth, stride=downsample)\n\n\n\n\nclass ResNetBackboneGN(ResNetBackbone):\n\n    def __init__(self, layers, num_groups=32):\n        super().__init__(layers, norm_layer=lambda x: nn.GroupNorm(num_groups, x))\n\n    def init_backbone(self, path):\n        """""" The path here comes from detectron. So we load it differently. """"""\n        with open(path, \'rb\') as f:\n            state_dict = pickle.load(f, encoding=\'latin1\') # From the detectron source\n            state_dict = state_dict[\'blobs\']\n        \n        our_state_dict_keys = list(self.state_dict().keys())\n        new_state_dict = {}\n    \n        gn_trans     = lambda x: (\'gn_s\' if x == \'weight\' else \'gn_b\')\n        layeridx2res = lambda x: \'res\' + str(int(x)+2)\n        block2branch = lambda x: \'branch2\' + (\'a\', \'b\', \'c\')[int(x[-1:])-1]\n\n        # Transcribe each Detectron weights name to a Yolact weights name\n        for key in our_state_dict_keys:\n            parts = key.split(\'.\')\n            transcribed_key = \'\'\n\n            if (parts[0] == \'conv1\'):\n                transcribed_key = \'conv1_w\'\n            elif (parts[0] == \'bn1\'):\n                transcribed_key = \'conv1_\' + gn_trans(parts[1])\n            elif (parts[0] == \'layers\'):\n                if int(parts[1]) >= self.num_base_layers: continue\n\n                transcribed_key = layeridx2res(parts[1])\n                transcribed_key += \'_\' + parts[2] + \'_\'\n\n                if parts[3] == \'downsample\':\n                    transcribed_key += \'branch1_\'\n                    \n                    if parts[4] == \'0\':\n                        transcribed_key += \'w\'\n                    else:\n                        transcribed_key += gn_trans(parts[5])\n                else:\n                    transcribed_key += block2branch(parts[3]) + \'_\'\n\n                    if \'conv\' in parts[3]:\n                        transcribed_key += \'w\'\n                    else:\n                        transcribed_key += gn_trans(parts[4])\n\n            new_state_dict[key] = torch.Tensor(state_dict[transcribed_key])\n        \n        # strict=False because we may have extra unitialized layers at this point\n        self.load_state_dict(new_state_dict, strict=False)\n\n\n\n\n\n\n\ndef darknetconvlayer(in_channels, out_channels, *args, **kwdargs):\n    """"""\n    Implements a conv, activation, then batch norm.\n    Arguments are passed into the conv layer.\n    """"""\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, *args, **kwdargs, bias=False),\n        nn.BatchNorm2d(out_channels),\n        # Darknet uses 0.1 here.\n        # See https://github.com/pjreddie/darknet/blob/680d3bde1924c8ee2d1c1dea54d3e56a05ca9a26/src/activations.h#L39\n        nn.LeakyReLU(0.1, inplace=True)\n    )\n\nclass DarkNetBlock(nn.Module):\n    """""" Note: channels is the lesser of the two. The output will be expansion * channels. """"""\n\n    expansion = 2\n\n    def __init__(self, in_channels, channels):\n        super().__init__()\n\n        self.conv1 = darknetconvlayer(in_channels, channels,                  kernel_size=1)\n        self.conv2 = darknetconvlayer(channels,    channels * self.expansion, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        return self.conv2(self.conv1(x)) + x\n\n\n\n\nclass DarkNetBackbone(nn.Module):\n    """"""\n    An implementation of YOLOv3\'s Darnet53 in\n    https://pjreddie.com/media/files/papers/YOLOv3.pdf\n\n    This is based off of the implementation of Resnet above.\n    """"""\n\n    def __init__(self, layers=[1, 2, 8, 8, 4], block=DarkNetBlock):\n        super().__init__()\n\n        # These will be populated by _make_layer\n        self.num_base_layers = len(layers)\n        self.layers = nn.ModuleList()\n        self.channels = []\n        \n        self._preconv = darknetconvlayer(3, 32, kernel_size=3, padding=1)\n        self.in_channels = 32\n        \n        self._make_layer(block, 32,  layers[0])\n        self._make_layer(block, 64,  layers[1])\n        self._make_layer(block, 128, layers[2])\n        self._make_layer(block, 256, layers[3])\n        self._make_layer(block, 512, layers[4])\n\n        # This contains every module that should be initialized by loading in pretrained weights.\n        # Any extra layers added onto this that won\'t be initialized by init_backbone will not be\n        # in this list. That way, Yolact::init_weights knows which backbone weights to initialize\n        # with xavier, and which ones to leave alone.\n        self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]\n    \n    def _make_layer(self, block, channels, num_blocks, stride=2):\n        """""" Here one layer means a string of n blocks. """"""\n        layer_list = []\n\n        # The downsample layer\n        layer_list.append(\n            darknetconvlayer(self.in_channels, channels * block.expansion,\n                             kernel_size=3, padding=1, stride=stride))\n\n        # Each block inputs channels and outputs channels * expansion\n        self.in_channels = channels * block.expansion\n        layer_list += [block(self.in_channels, channels) for _ in range(num_blocks)]\n\n        self.channels.append(self.in_channels)\n        self.layers.append(nn.Sequential(*layer_list))\n\n    def forward(self, x):\n        """""" Returns a list of convouts for each layer. """"""\n\n        x = self._preconv(x)\n\n        outs = []\n        for layer in self.layers:\n            x = layer(x)\n            outs.append(x)\n\n        return tuple(outs)\n\n    def add_layer(self, conv_channels=1024, stride=2, depth=1, block=DarkNetBlock):\n        """""" Add a downsample layer to the backbone as per what SSD does. """"""\n        self._make_layer(block, conv_channels // block.expansion, num_blocks=depth, stride=stride)\n    \n    def init_backbone(self, path):\n        """""" Initializes the backbone weights for training. """"""\n        # Note: Using strict=False is berry scary. Triple check this.\n        self.load_state_dict(torch.load(path), strict=False)\n\n\n\n\n\nclass VGGBackbone(nn.Module):\n    """"""\n    Args:\n        - cfg: A list of layers given as lists. Layers can be either \'M\' signifying\n                a max pooling layer, a number signifying that many feature maps in\n                a conv layer, or a tuple of \'M\' or a number and a kwdargs dict to pass\n                into the function that creates the layer (e.g. nn.MaxPool2d for \'M\').\n        - extra_args: A list of lists of arguments to pass into add_layer.\n        - norm_layers: Layers indices that need to pass through an l2norm layer.\n    """"""\n\n    def __init__(self, cfg, extra_args=[], norm_layers=[]):\n        super().__init__()\n        \n        self.channels = []\n        self.layers = nn.ModuleList()\n        self.in_channels = 3\n        self.extra_args = list(reversed(extra_args)) # So I can use it as a stack\n\n        # Keeps track of what the corresponding key will be in the state dict of the\n        # pretrained model. For instance, layers.0.2 for us is 2 for the pretrained\n        # model but layers.1.1 is 5.\n        self.total_layer_count = 0\n        self.state_dict_lookup = {}\n\n        for idx, layer_cfg in enumerate(cfg):\n            self._make_layer(layer_cfg)\n\n        self.norms = nn.ModuleList([nn.BatchNorm2d(self.channels[l]) for l in norm_layers])\n        self.norm_lookup = {l: idx for idx, l in enumerate(norm_layers)}\n\n        # These modules will be initialized by init_backbone,\n        # so don\'t overwrite their initialization later.\n        self.backbone_modules = [m for m in self.modules() if isinstance(m, nn.Conv2d)]\n\n    def _make_layer(self, cfg):\n        """"""\n        Each layer is a sequence of conv layers usually preceded by a max pooling.\n        Adapted from torchvision.models.vgg.make_layers.\n        """"""\n\n        layers = []\n\n        for v in cfg:\n            # VGG in SSD requires some special layers, so allow layers to be tuples of\n            # (<M or num_features>, kwdargs dict)\n            args = None\n            if isinstance(v, tuple):\n                args = v[1]\n                v = v[0]\n\n            # v should be either M or a number\n            if v == \'M\':\n                # Set default arguments\n                if args is None:\n                    args = {\'kernel_size\': 2, \'stride\': 2}\n\n                layers.append(nn.MaxPool2d(**args))\n            else:\n                # See the comment in __init__ for an explanation of this\n                cur_layer_idx = self.total_layer_count + len(layers)\n                self.state_dict_lookup[cur_layer_idx] = \'%d.%d\' % (len(self.layers), len(layers))\n\n                # Set default arguments\n                if args is None:\n                    args = {\'kernel_size\': 3, \'padding\': 1}\n\n                # Add the layers\n                layers.append(nn.Conv2d(self.in_channels, v, **args))\n                layers.append(nn.ReLU(inplace=True))\n                self.in_channels = v\n        \n        self.total_layer_count += len(layers)\n        self.channels.append(self.in_channels)\n        self.layers.append(nn.Sequential(*layers))\n\n    def forward(self, x):\n        """""" Returns a list of convouts for each layer. """"""\n        outs = []\n\n        for idx, layer in enumerate(self.layers):\n            x = layer(x)\n            \n            # Apply an l2norm module to the selected layers\n            # Note that this differs from the original implemenetation\n            if idx in self.norm_lookup:\n                x = self.norms[self.norm_lookup[idx]](x)\n            outs.append(x)\n        \n        return tuple(outs)\n\n    def transform_key(self, k):\n        """""" Transform e.g. features.24.bias to layers.4.1.bias """"""\n        vals = k.split(\'.\')\n        layerIdx = self.state_dict_lookup[int(vals[0])]\n        return \'layers.%s.%s\' % (layerIdx, vals[1])\n\n    def init_backbone(self, path):\n        """""" Initializes the backbone weights for training. """"""\n        state_dict = torch.load(path)\n        state_dict = OrderedDict([(self.transform_key(k), v) for k,v in state_dict.items()])\n\n        self.load_state_dict(state_dict, strict=False)\n\n    def add_layer(self, conv_channels=128, downsample=2):\n        """""" Add a downsample layer to the backbone as per what SSD does. """"""\n        if len(self.extra_args) > 0:\n            conv_channels, downsample = self.extra_args.pop()\n        \n        padding = 1 if downsample > 1 else 0\n        \n        layer = nn.Sequential(\n            nn.Conv2d(self.in_channels, conv_channels, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(conv_channels, conv_channels*2, kernel_size=3, stride=downsample, padding=padding),\n            nn.ReLU(inplace=True)\n        )\n\n        self.in_channels = conv_channels*2\n        self.channels.append(self.in_channels)\n        self.layers.append(layer)\n        \n                \n\n\ndef construct_backbone(cfg):\n    """""" Constructs a backbone given a backbone config object (see config.py). """"""\n    backbone = cfg.type(*cfg.args)\n\n    # Add downsampling layers until we reach the number we need\n    num_layers = max(cfg.selected_layers) + 1\n\n    while len(backbone.layers) < num_layers:\n        backbone.add_layer()\n\n    return backbone\n'"
eval.py,20,"b'from data import COCODetection, get_label_map, MEANS, COLORS\nfrom yolact import Yolact\nfrom utils.augmentations import BaseTransform, FastBaseTransform, Resize\nfrom utils.functions import MovingAverage, ProgressBar\nfrom layers.box_utils import jaccard, center_size, mask_iou\nfrom utils import timer\nfrom utils.functions import SavePath\nfrom layers.output_utils import postprocess, undo_image_transformation\nimport pycocotools\n\nfrom data import cfg, set_cfg, set_dataset\n\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nimport argparse\nimport time\nimport random\nimport cProfile\nimport pickle\nimport json\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom collections import OrderedDict\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\ndef parse_args(argv=None):\n    parser = argparse.ArgumentParser(\n        description=\'YOLACT COCO Evaluation\')\n    parser.add_argument(\'--trained_model\',\n                        default=\'weights/ssd300_mAP_77.43_v2.pth\', type=str,\n                        help=\'Trained state_dict file path to open. If ""interrupt"", this will open the interrupt file.\')\n    parser.add_argument(\'--top_k\', default=5, type=int,\n                        help=\'Further restrict the number of predictions to parse\')\n    parser.add_argument(\'--cuda\', default=True, type=str2bool,\n                        help=\'Use cuda to evaulate model\')\n    parser.add_argument(\'--fast_nms\', default=True, type=str2bool,\n                        help=\'Whether to use a faster, but not entirely correct version of NMS.\')\n    parser.add_argument(\'--cross_class_nms\', default=False, type=str2bool,\n                        help=\'Whether compute NMS cross-class or per-class.\')\n    parser.add_argument(\'--display_masks\', default=True, type=str2bool,\n                        help=\'Whether or not to display masks over bounding boxes\')\n    parser.add_argument(\'--display_bboxes\', default=True, type=str2bool,\n                        help=\'Whether or not to display bboxes around masks\')\n    parser.add_argument(\'--display_text\', default=True, type=str2bool,\n                        help=\'Whether or not to display text (class [score])\')\n    parser.add_argument(\'--display_scores\', default=True, type=str2bool,\n                        help=\'Whether or not to display scores in addition to classes\')\n    parser.add_argument(\'--display\', dest=\'display\', action=\'store_true\',\n                        help=\'Display qualitative results instead of quantitative ones.\')\n    parser.add_argument(\'--shuffle\', dest=\'shuffle\', action=\'store_true\',\n                        help=\'Shuffles the images when displaying them. Doesn\\\'t have much of an effect when display is off though.\')\n    parser.add_argument(\'--ap_data_file\', default=\'results/ap_data.pkl\', type=str,\n                        help=\'In quantitative mode, the file to save detections before calculating mAP.\')\n    parser.add_argument(\'--resume\', dest=\'resume\', action=\'store_true\',\n                        help=\'If display not set, this resumes mAP calculations from the ap_data_file.\')\n    parser.add_argument(\'--max_images\', default=-1, type=int,\n                        help=\'The maximum number of images from the dataset to consider. Use -1 for all.\')\n    parser.add_argument(\'--output_coco_json\', dest=\'output_coco_json\', action=\'store_true\',\n                        help=\'If display is not set, instead of processing IoU values, this just dumps detections into the coco json file.\')\n    parser.add_argument(\'--bbox_det_file\', default=\'results/bbox_detections.json\', type=str,\n                        help=\'The output file for coco bbox results if --coco_results is set.\')\n    parser.add_argument(\'--mask_det_file\', default=\'results/mask_detections.json\', type=str,\n                        help=\'The output file for coco mask results if --coco_results is set.\')\n    parser.add_argument(\'--config\', default=None,\n                        help=\'The config object to use.\')\n    parser.add_argument(\'--output_web_json\', dest=\'output_web_json\', action=\'store_true\',\n                        help=\'If display is not set, instead of processing IoU values, this dumps detections for usage with the detections viewer web thingy.\')\n    parser.add_argument(\'--web_det_path\', default=\'web/dets/\', type=str,\n                        help=\'If output_web_json is set, this is the path to dump detections into.\')\n    parser.add_argument(\'--no_bar\', dest=\'no_bar\', action=\'store_true\',\n                        help=\'Do not output the status bar. This is useful for when piping to a file.\')\n    parser.add_argument(\'--display_lincomb\', default=False, type=str2bool,\n                        help=\'If the config uses lincomb masks, output a visualization of how those masks are created.\')\n    parser.add_argument(\'--benchmark\', default=False, dest=\'benchmark\', action=\'store_true\',\n                        help=\'Equivalent to running display mode but without displaying an image.\')\n    parser.add_argument(\'--no_sort\', default=False, dest=\'no_sort\', action=\'store_true\',\n                        help=\'Do not sort images by hashed image ID.\')\n    parser.add_argument(\'--seed\', default=None, type=int,\n                        help=\'The seed to pass into random.seed. Note: this is only really for the shuffle and does not (I think) affect cuda stuff.\')\n    parser.add_argument(\'--mask_proto_debug\', default=False, dest=\'mask_proto_debug\', action=\'store_true\',\n                        help=\'Outputs stuff for scripts/compute_mask.py.\')\n    parser.add_argument(\'--no_crop\', default=False, dest=\'crop\', action=\'store_false\',\n                        help=\'Do not crop output masks with the predicted bounding box.\')\n    parser.add_argument(\'--image\', default=None, type=str,\n                        help=\'A path to an image to use for display.\')\n    parser.add_argument(\'--images\', default=None, type=str,\n                        help=\'An input folder of images and output folder to save detected images. Should be in the format input->output.\')\n    parser.add_argument(\'--video\', default=None, type=str,\n                        help=\'A path to a video to evaluate on. Passing in a number will use that index webcam.\')\n    parser.add_argument(\'--video_multiframe\', default=1, type=int,\n                        help=\'The number of frames to evaluate in parallel to make videos play at higher fps.\')\n    parser.add_argument(\'--score_threshold\', default=0, type=float,\n                        help=\'Detections with a score under this threshold will not be considered. This currently only works in display mode.\')\n    parser.add_argument(\'--dataset\', default=None, type=str,\n                        help=\'If specified, override the dataset specified in the config with this one (example: coco2017_dataset).\')\n    parser.add_argument(\'--detect\', default=False, dest=\'detect\', action=\'store_true\',\n                        help=\'Don\\\'t evauluate the mask branch at all and only do object detection. This only works for --display and --benchmark.\')\n    parser.add_argument(\'--display_fps\', default=False, dest=\'display_fps\', action=\'store_true\',\n                        help=\'When displaying / saving video, draw the FPS on the frame\')\n    parser.add_argument(\'--emulate_playback\', default=False, dest=\'emulate_playback\', action=\'store_true\',\n                        help=\'When saving a video, emulate the framerate that you\\\'d get running in real-time mode.\')\n\n    parser.set_defaults(no_bar=False, display=False, resume=False, output_coco_json=False, output_web_json=False, shuffle=False,\n                        benchmark=False, no_sort=False, no_hash=False, mask_proto_debug=False, crop=True, detect=False, display_fps=False,\n                        emulate_playback=False)\n\n    global args\n    args = parser.parse_args(argv)\n\n    if args.output_web_json:\n        args.output_coco_json = True\n    \n    if args.seed is not None:\n        random.seed(args.seed)\n\niou_thresholds = [x / 100 for x in range(50, 100, 5)]\ncoco_cats = {} # Call prep_coco_cats to fill this\ncoco_cats_inv = {}\ncolor_cache = defaultdict(lambda: {})\n\ndef prep_display(dets_out, img, h, w, undo_transform=True, class_color=False, mask_alpha=0.45, fps_str=\'\'):\n    """"""\n    Note: If undo_transform=False then im_h and im_w are allowed to be None.\n    """"""\n    if undo_transform:\n        img_numpy = undo_image_transformation(img, w, h)\n        img_gpu = torch.Tensor(img_numpy).cuda()\n    else:\n        img_gpu = img / 255.0\n        h, w, _ = img.shape\n    \n    with timer.env(\'Postprocess\'):\n        save = cfg.rescore_bbox\n        cfg.rescore_bbox = True\n        t = postprocess(dets_out, w, h, visualize_lincomb = args.display_lincomb,\n                                        crop_masks        = args.crop,\n                                        score_threshold   = args.score_threshold)\n        cfg.rescore_bbox = save\n\n    with timer.env(\'Copy\'):\n        idx = t[1].argsort(0, descending=True)[:args.top_k]\n        \n        if cfg.eval_mask_branch:\n            # Masks are drawn on the GPU, so don\'t copy\n            masks = t[3][idx]\n        classes, scores, boxes = [x[idx].cpu().numpy() for x in t[:3]]\n\n    num_dets_to_consider = min(args.top_k, classes.shape[0])\n    for j in range(num_dets_to_consider):\n        if scores[j] < args.score_threshold:\n            num_dets_to_consider = j\n            break\n\n    # Quick and dirty lambda for selecting the color for a particular index\n    # Also keeps track of a per-gpu color cache for maximum speed\n    def get_color(j, on_gpu=None):\n        global color_cache\n        color_idx = (classes[j] * 5 if class_color else j * 5) % len(COLORS)\n        \n        if on_gpu is not None and color_idx in color_cache[on_gpu]:\n            return color_cache[on_gpu][color_idx]\n        else:\n            color = COLORS[color_idx]\n            if not undo_transform:\n                # The image might come in as RGB or BRG, depending\n                color = (color[2], color[1], color[0])\n            if on_gpu is not None:\n                color = torch.Tensor(color).to(on_gpu).float() / 255.\n                color_cache[on_gpu][color_idx] = color\n            return color\n\n    # First, draw the masks on the GPU where we can do it really fast\n    # Beware: very fast but possibly unintelligible mask-drawing code ahead\n    # I wish I had access to OpenGL or Vulkan but alas, I guess Pytorch tensor operations will have to suffice\n    if args.display_masks and cfg.eval_mask_branch and num_dets_to_consider > 0:\n        # After this, mask is of size [num_dets, h, w, 1]\n        masks = masks[:num_dets_to_consider, :, :, None]\n        \n        # Prepare the RGB images for each mask given their color (size [num_dets, h, w, 1])\n        colors = torch.cat([get_color(j, on_gpu=img_gpu.device.index).view(1, 1, 1, 3) for j in range(num_dets_to_consider)], dim=0)\n        masks_color = masks.repeat(1, 1, 1, 3) * colors * mask_alpha\n\n        # This is 1 everywhere except for 1-mask_alpha where the mask is\n        inv_alph_masks = masks * (-mask_alpha) + 1\n        \n        # I did the math for this on pen and paper. This whole block should be equivalent to:\n        #    for j in range(num_dets_to_consider):\n        #        img_gpu = img_gpu * inv_alph_masks[j] + masks_color[j]\n        masks_color_summand = masks_color[0]\n        if num_dets_to_consider > 1:\n            inv_alph_cumul = inv_alph_masks[:(num_dets_to_consider-1)].cumprod(dim=0)\n            masks_color_cumul = masks_color[1:] * inv_alph_cumul\n            masks_color_summand += masks_color_cumul.sum(dim=0)\n\n        img_gpu = img_gpu * inv_alph_masks.prod(dim=0) + masks_color_summand\n    \n    if args.display_fps:\n            # Draw the box for the fps on the GPU\n        font_face = cv2.FONT_HERSHEY_DUPLEX\n        font_scale = 0.6\n        font_thickness = 1\n\n        text_w, text_h = cv2.getTextSize(fps_str, font_face, font_scale, font_thickness)[0]\n\n        img_gpu[0:text_h+8, 0:text_w+8] *= 0.6 # 1 - Box alpha\n\n\n    # Then draw the stuff that needs to be done on the cpu\n    # Note, make sure this is a uint8 tensor or opencv will not anti alias text for whatever reason\n    img_numpy = (img_gpu * 255).byte().cpu().numpy()\n\n    if args.display_fps:\n        # Draw the text on the CPU\n        text_pt = (4, text_h + 2)\n        text_color = [255, 255, 255]\n\n        cv2.putText(img_numpy, fps_str, text_pt, font_face, font_scale, text_color, font_thickness, cv2.LINE_AA)\n    \n    if num_dets_to_consider == 0:\n        return img_numpy\n\n    if args.display_text or args.display_bboxes:\n        for j in reversed(range(num_dets_to_consider)):\n            x1, y1, x2, y2 = boxes[j, :]\n            color = get_color(j)\n            score = scores[j]\n\n            if args.display_bboxes:\n                cv2.rectangle(img_numpy, (x1, y1), (x2, y2), color, 1)\n\n            if args.display_text:\n                _class = cfg.dataset.class_names[classes[j]]\n                text_str = \'%s: %.2f\' % (_class, score) if args.display_scores else _class\n\n                font_face = cv2.FONT_HERSHEY_DUPLEX\n                font_scale = 0.6\n                font_thickness = 1\n\n                text_w, text_h = cv2.getTextSize(text_str, font_face, font_scale, font_thickness)[0]\n\n                text_pt = (x1, y1 - 3)\n                text_color = [255, 255, 255]\n\n                cv2.rectangle(img_numpy, (x1, y1), (x1 + text_w, y1 - text_h - 4), color, -1)\n                cv2.putText(img_numpy, text_str, text_pt, font_face, font_scale, text_color, font_thickness, cv2.LINE_AA)\n            \n    \n    return img_numpy\n\ndef prep_benchmark(dets_out, h, w):\n    with timer.env(\'Postprocess\'):\n        t = postprocess(dets_out, w, h, crop_masks=args.crop, score_threshold=args.score_threshold)\n\n    with timer.env(\'Copy\'):\n        classes, scores, boxes, masks = [x[:args.top_k] for x in t]\n        if isinstance(scores, list):\n            box_scores = scores[0].cpu().numpy()\n            mask_scores = scores[1].cpu().numpy()\n        else:\n            scores = scores.cpu().numpy()\n        classes = classes.cpu().numpy()\n        boxes = boxes.cpu().numpy()\n        masks = masks.cpu().numpy()\n    \n    with timer.env(\'Sync\'):\n        # Just in case\n        torch.cuda.synchronize()\n\ndef prep_coco_cats():\n    """""" Prepare inverted table for category id lookup given a coco cats object. """"""\n    for coco_cat_id, transformed_cat_id_p1 in get_label_map().items():\n        transformed_cat_id = transformed_cat_id_p1 - 1\n        coco_cats[transformed_cat_id] = coco_cat_id\n        coco_cats_inv[coco_cat_id] = transformed_cat_id\n\n\ndef get_coco_cat(transformed_cat_id):\n    """""" transformed_cat_id is [0,80) as indices in cfg.dataset.class_names """"""\n    return coco_cats[transformed_cat_id]\n\ndef get_transformed_cat(coco_cat_id):\n    """""" transformed_cat_id is [0,80) as indices in cfg.dataset.class_names """"""\n    return coco_cats_inv[coco_cat_id]\n\n\nclass Detections:\n\n    def __init__(self):\n        self.bbox_data = []\n        self.mask_data = []\n\n    def add_bbox(self, image_id:int, category_id:int, bbox:list, score:float):\n        """""" Note that bbox should be a list or tuple of (x1, y1, x2, y2) """"""\n        bbox = [bbox[0], bbox[1], bbox[2]-bbox[0], bbox[3]-bbox[1]]\n\n        # Round to the nearest 10th to avoid huge file sizes, as COCO suggests\n        bbox = [round(float(x)*10)/10 for x in bbox]\n\n        self.bbox_data.append({\n            \'image_id\': int(image_id),\n            \'category_id\': get_coco_cat(int(category_id)),\n            \'bbox\': bbox,\n            \'score\': float(score)\n        })\n\n    def add_mask(self, image_id:int, category_id:int, segmentation:np.ndarray, score:float):\n        """""" The segmentation should be the full mask, the size of the image and with size [h, w]. """"""\n        rle = pycocotools.mask.encode(np.asfortranarray(segmentation.astype(np.uint8)))\n        rle[\'counts\'] = rle[\'counts\'].decode(\'ascii\') # json.dump doesn\'t like bytes strings\n\n        self.mask_data.append({\n            \'image_id\': int(image_id),\n            \'category_id\': get_coco_cat(int(category_id)),\n            \'segmentation\': rle,\n            \'score\': float(score)\n        })\n    \n    def dump(self):\n        dump_arguments = [\n            (self.bbox_data, args.bbox_det_file),\n            (self.mask_data, args.mask_det_file)\n        ]\n\n        for data, path in dump_arguments:\n            with open(path, \'w\') as f:\n                json.dump(data, f)\n    \n    def dump_web(self):\n        """""" Dumps it in the format for my web app. Warning: bad code ahead! """"""\n        config_outs = [\'preserve_aspect_ratio\', \'use_prediction_module\',\n                        \'use_yolo_regressors\', \'use_prediction_matching\',\n                        \'train_masks\']\n\n        output = {\n            \'info\' : {\n                \'Config\': {key: getattr(cfg, key) for key in config_outs},\n            }\n        }\n\n        image_ids = list(set([x[\'image_id\'] for x in self.bbox_data]))\n        image_ids.sort()\n        image_lookup = {_id: idx for idx, _id in enumerate(image_ids)}\n\n        output[\'images\'] = [{\'image_id\': image_id, \'dets\': []} for image_id in image_ids]\n\n        # These should already be sorted by score with the way prep_metrics works.\n        for bbox, mask in zip(self.bbox_data, self.mask_data):\n            image_obj = output[\'images\'][image_lookup[bbox[\'image_id\']]]\n            image_obj[\'dets\'].append({\n                \'score\': bbox[\'score\'],\n                \'bbox\': bbox[\'bbox\'],\n                \'category\': cfg.dataset.class_names[get_transformed_cat(bbox[\'category_id\'])],\n                \'mask\': mask[\'segmentation\'],\n            })\n\n        with open(os.path.join(args.web_det_path, \'%s.json\' % cfg.name), \'w\') as f:\n            json.dump(output, f)\n        \n\n        \n\ndef _mask_iou(mask1, mask2, iscrowd=False):\n    with timer.env(\'Mask IoU\'):\n        ret = mask_iou(mask1, mask2, iscrowd)\n    return ret.cpu()\n\ndef _bbox_iou(bbox1, bbox2, iscrowd=False):\n    with timer.env(\'BBox IoU\'):\n        ret = jaccard(bbox1, bbox2, iscrowd)\n    return ret.cpu()\n\ndef prep_metrics(ap_data, dets, img, gt, gt_masks, h, w, num_crowd, image_id, detections:Detections=None):\n    """""" Returns a list of APs for this image, with each element being for a class  """"""\n    if not args.output_coco_json:\n        with timer.env(\'Prepare gt\'):\n            gt_boxes = torch.Tensor(gt[:, :4])\n            gt_boxes[:, [0, 2]] *= w\n            gt_boxes[:, [1, 3]] *= h\n            gt_classes = list(gt[:, 4].astype(int))\n            gt_masks = torch.Tensor(gt_masks).view(-1, h*w)\n\n            if num_crowd > 0:\n                split = lambda x: (x[-num_crowd:], x[:-num_crowd])\n                crowd_boxes  , gt_boxes   = split(gt_boxes)\n                crowd_masks  , gt_masks   = split(gt_masks)\n                crowd_classes, gt_classes = split(gt_classes)\n\n    with timer.env(\'Postprocess\'):\n        classes, scores, boxes, masks = postprocess(dets, w, h, crop_masks=args.crop, score_threshold=args.score_threshold)\n\n        if classes.size(0) == 0:\n            return\n\n        classes = list(classes.cpu().numpy().astype(int))\n        if isinstance(scores, list):\n            box_scores = list(scores[0].cpu().numpy().astype(float))\n            mask_scores = list(scores[1].cpu().numpy().astype(float))\n        else:\n            scores = list(scores.cpu().numpy().astype(float))\n            box_scores = scores\n            mask_scores = scores\n        masks = masks.view(-1, h*w).cuda()\n        boxes = boxes.cuda()\n\n\n    if args.output_coco_json:\n        with timer.env(\'JSON Output\'):\n            boxes = boxes.cpu().numpy()\n            masks = masks.view(-1, h, w).cpu().numpy()\n            for i in range(masks.shape[0]):\n                # Make sure that the bounding box actually makes sense and a mask was produced\n                if (boxes[i, 3] - boxes[i, 1]) * (boxes[i, 2] - boxes[i, 0]) > 0:\n                    detections.add_bbox(image_id, classes[i], boxes[i,:],   box_scores[i])\n                    detections.add_mask(image_id, classes[i], masks[i,:,:], mask_scores[i])\n            return\n    \n    with timer.env(\'Eval Setup\'):\n        num_pred = len(classes)\n        num_gt   = len(gt_classes)\n\n        mask_iou_cache = _mask_iou(masks, gt_masks)\n        bbox_iou_cache = _bbox_iou(boxes.float(), gt_boxes.float())\n\n        if num_crowd > 0:\n            crowd_mask_iou_cache = _mask_iou(masks, crowd_masks, iscrowd=True)\n            crowd_bbox_iou_cache = _bbox_iou(boxes.float(), crowd_boxes.float(), iscrowd=True)\n        else:\n            crowd_mask_iou_cache = None\n            crowd_bbox_iou_cache = None\n\n        box_indices = sorted(range(num_pred), key=lambda i: -box_scores[i])\n        mask_indices = sorted(box_indices, key=lambda i: -mask_scores[i])\n\n        iou_types = [\n            (\'box\',  lambda i,j: bbox_iou_cache[i, j].item(),\n                     lambda i,j: crowd_bbox_iou_cache[i,j].item(),\n                     lambda i: box_scores[i], box_indices),\n            (\'mask\', lambda i,j: mask_iou_cache[i, j].item(),\n                     lambda i,j: crowd_mask_iou_cache[i,j].item(),\n                     lambda i: mask_scores[i], mask_indices)\n        ]\n\n    timer.start(\'Main loop\')\n    for _class in set(classes + gt_classes):\n        ap_per_iou = []\n        num_gt_for_class = sum([1 for x in gt_classes if x == _class])\n        \n        for iouIdx in range(len(iou_thresholds)):\n            iou_threshold = iou_thresholds[iouIdx]\n\n            for iou_type, iou_func, crowd_func, score_func, indices in iou_types:\n                gt_used = [False] * len(gt_classes)\n                \n                ap_obj = ap_data[iou_type][iouIdx][_class]\n                ap_obj.add_gt_positives(num_gt_for_class)\n\n                for i in indices:\n                    if classes[i] != _class:\n                        continue\n                    \n                    max_iou_found = iou_threshold\n                    max_match_idx = -1\n                    for j in range(num_gt):\n                        if gt_used[j] or gt_classes[j] != _class:\n                            continue\n                            \n                        iou = iou_func(i, j)\n\n                        if iou > max_iou_found:\n                            max_iou_found = iou\n                            max_match_idx = j\n                    \n                    if max_match_idx >= 0:\n                        gt_used[max_match_idx] = True\n                        ap_obj.push(score_func(i), True)\n                    else:\n                        # If the detection matches a crowd, we can just ignore it\n                        matched_crowd = False\n\n                        if num_crowd > 0:\n                            for j in range(len(crowd_classes)):\n                                if crowd_classes[j] != _class:\n                                    continue\n                                \n                                iou = crowd_func(i, j)\n\n                                if iou > iou_threshold:\n                                    matched_crowd = True\n                                    break\n\n                        # All this crowd code so that we can make sure that our eval code gives the\n                        # same result as COCOEval. There aren\'t even that many crowd annotations to\n                        # begin with, but accuracy is of the utmost importance.\n                        if not matched_crowd:\n                            ap_obj.push(score_func(i), False)\n    timer.stop(\'Main loop\')\n\n\nclass APDataObject:\n    """"""\n    Stores all the information necessary to calculate the AP for one IoU and one class.\n    Note: I type annotated this because why not.\n    """"""\n\n    def __init__(self):\n        self.data_points = []\n        self.num_gt_positives = 0\n\n    def push(self, score:float, is_true:bool):\n        self.data_points.append((score, is_true))\n    \n    def add_gt_positives(self, num_positives:int):\n        """""" Call this once per image. """"""\n        self.num_gt_positives += num_positives\n\n    def is_empty(self) -> bool:\n        return len(self.data_points) == 0 and self.num_gt_positives == 0\n\n    def get_ap(self) -> float:\n        """""" Warning: result not cached. """"""\n\n        if self.num_gt_positives == 0:\n            return 0\n\n        # Sort descending by score\n        self.data_points.sort(key=lambda x: -x[0])\n\n        precisions = []\n        recalls    = []\n        num_true  = 0\n        num_false = 0\n\n        # Compute the precision-recall curve. The x axis is recalls and the y axis precisions.\n        for datum in self.data_points:\n            # datum[1] is whether the detection a true or false positive\n            if datum[1]: num_true += 1\n            else: num_false += 1\n            \n            precision = num_true / (num_true + num_false)\n            recall    = num_true / self.num_gt_positives\n\n            precisions.append(precision)\n            recalls.append(recall)\n\n        # Smooth the curve by computing [max(precisions[i:]) for i in range(len(precisions))]\n        # Basically, remove any temporary dips from the curve.\n        # At least that\'s what I think, idk. COCOEval did it so I do too.\n        for i in range(len(precisions)-1, 0, -1):\n            if precisions[i] > precisions[i-1]:\n                precisions[i-1] = precisions[i]\n\n        # Compute the integral of precision(recall) d_recall from recall=0->1 using fixed-length riemann summation with 101 bars.\n        y_range = [0] * 101 # idx 0 is recall == 0.0 and idx 100 is recall == 1.00\n        x_range = np.array([x / 100 for x in range(101)])\n        recalls = np.array(recalls)\n\n        # I realize this is weird, but all it does is find the nearest precision(x) for a given x in x_range.\n        # Basically, if the closest recall we have to 0.01 is 0.009 this sets precision(0.01) = precision(0.009).\n        # I approximate the integral this way, because that\'s how COCOEval does it.\n        indices = np.searchsorted(recalls, x_range, side=\'left\')\n        for bar_idx, precision_idx in enumerate(indices):\n            if precision_idx < len(precisions):\n                y_range[bar_idx] = precisions[precision_idx]\n\n        # Finally compute the riemann sum to get our integral.\n        # avg([precision(x) for x in 0:0.01:1])\n        return sum(y_range) / len(y_range)\n\ndef badhash(x):\n    """"""\n    Just a quick and dirty hash function for doing a deterministic shuffle based on image_id.\n\n    Source:\n    https://stackoverflow.com/questions/664014/what-integer-hash-function-are-good-that-accepts-an-integer-hash-key\n    """"""\n    x = (((x >> 16) ^ x) * 0x045d9f3b) & 0xFFFFFFFF\n    x = (((x >> 16) ^ x) * 0x045d9f3b) & 0xFFFFFFFF\n    x =  ((x >> 16) ^ x) & 0xFFFFFFFF\n    return x\n\ndef evalimage(net:Yolact, path:str, save_path:str=None):\n    frame = torch.from_numpy(cv2.imread(path)).cuda().float()\n    batch = FastBaseTransform()(frame.unsqueeze(0))\n    preds = net(batch)\n\n    img_numpy = prep_display(preds, frame, None, None, undo_transform=False)\n    \n    if save_path is None:\n        img_numpy = img_numpy[:, :, (2, 1, 0)]\n\n    if save_path is None:\n        plt.imshow(img_numpy)\n        plt.title(path)\n        plt.show()\n    else:\n        cv2.imwrite(save_path, img_numpy)\n\ndef evalimages(net:Yolact, input_folder:str, output_folder:str):\n    if not os.path.exists(output_folder):\n        os.mkdir(output_folder)\n\n    print()\n    for p in Path(input_folder).glob(\'*\'): \n        path = str(p)\n        name = os.path.basename(path)\n        name = \'.\'.join(name.split(\'.\')[:-1]) + \'.png\'\n        out_path = os.path.join(output_folder, name)\n\n        evalimage(net, path, out_path)\n        print(path + \' -> \' + out_path)\n    print(\'Done.\')\n\nfrom multiprocessing.pool import ThreadPool\nfrom queue import Queue\n\nclass CustomDataParallel(torch.nn.DataParallel):\n    """""" A Custom Data Parallel class that properly gathers lists of dictionaries. """"""\n    def gather(self, outputs, output_device):\n        # Note that I don\'t actually want to convert everything to the output_device\n        return sum(outputs, [])\n\ndef evalvideo(net:Yolact, path:str, out_path:str=None):\n    # If the path is a digit, parse it as a webcam index\n    is_webcam = path.isdigit()\n    \n    # If the input image size is constant, this make things faster (hence why we can use it in a video setting).\n    cudnn.benchmark = True\n    \n    if is_webcam:\n        vid = cv2.VideoCapture(int(path))\n    else:\n        vid = cv2.VideoCapture(path)\n    \n    if not vid.isOpened():\n        print(\'Could not open video ""%s""\' % path)\n        exit(-1)\n\n    target_fps   = round(vid.get(cv2.CAP_PROP_FPS))\n    frame_width  = round(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = round(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    if is_webcam:\n        num_frames = float(\'inf\')\n    else:\n        num_frames = round(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    net = CustomDataParallel(net).cuda()\n    transform = torch.nn.DataParallel(FastBaseTransform()).cuda()\n    frame_times = MovingAverage(100)\n    fps = 0\n    frame_time_target = 1 / target_fps\n    running = True\n    fps_str = \'\'\n    vid_done = False\n    frames_displayed = 0\n\n    if out_path is not None:\n        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*""mp4v""), target_fps, (frame_width, frame_height))\n\n    def cleanup_and_exit():\n        print()\n        pool.terminate()\n        vid.release()\n        if out_path is not None:\n            out.release()\n        cv2.destroyAllWindows()\n        exit()\n\n    def get_next_frame(vid):\n        frames = []\n        for idx in range(args.video_multiframe):\n            frame = vid.read()[1]\n            if frame is None:\n                return frames\n            frames.append(frame)\n        return frames\n\n    def transform_frame(frames):\n        with torch.no_grad():\n            frames = [torch.from_numpy(frame).cuda().float() for frame in frames]\n            return frames, transform(torch.stack(frames, 0))\n\n    def eval_network(inp):\n        with torch.no_grad():\n            frames, imgs = inp\n            num_extra = 0\n            while imgs.size(0) < args.video_multiframe:\n                imgs = torch.cat([imgs, imgs[0].unsqueeze(0)], dim=0)\n                num_extra += 1\n            out = net(imgs)\n            if num_extra > 0:\n                out = out[:-num_extra]\n            return frames, out\n\n    def prep_frame(inp, fps_str):\n        with torch.no_grad():\n            frame, preds = inp\n            return prep_display(preds, frame, None, None, undo_transform=False, class_color=True, fps_str=fps_str)\n\n    frame_buffer = Queue()\n    video_fps = 0\n\n    # All this timing code to make sure that \n    def play_video():\n        try:\n            nonlocal frame_buffer, running, video_fps, is_webcam, num_frames, frames_displayed, vid_done\n\n            video_frame_times = MovingAverage(100)\n            frame_time_stabilizer = frame_time_target\n            last_time = None\n            stabilizer_step = 0.0005\n            progress_bar = ProgressBar(30, num_frames)\n\n            while running:\n                frame_time_start = time.time()\n\n                if not frame_buffer.empty():\n                    next_time = time.time()\n                    if last_time is not None:\n                        video_frame_times.add(next_time - last_time)\n                        video_fps = 1 / video_frame_times.get_avg()\n                    if out_path is None:\n                        cv2.imshow(path, frame_buffer.get())\n                    else:\n                        out.write(frame_buffer.get())\n                    frames_displayed += 1\n                    last_time = next_time\n\n                    if out_path is not None:\n                        if video_frame_times.get_avg() == 0:\n                            fps = 0\n                        else:\n                            fps = 1 / video_frame_times.get_avg()\n                        progress = frames_displayed / num_frames * 100\n                        progress_bar.set_val(frames_displayed)\n\n                        print(\'\\rProcessing Frames  %s %6d / %6d (%5.2f%%)    %5.2f fps        \'\n                            % (repr(progress_bar), frames_displayed, num_frames, progress, fps), end=\'\')\n\n                \n                # This is split because you don\'t want savevideo to require cv2 display functionality (see #197)\n                if out_path is None and cv2.waitKey(1) == 27:\n                    # Press Escape to close\n                    running = False\n                if not (frames_displayed < num_frames):\n                    running = False\n\n                if not vid_done:\n                    buffer_size = frame_buffer.qsize()\n                    if buffer_size < args.video_multiframe:\n                        frame_time_stabilizer += stabilizer_step\n                    elif buffer_size > args.video_multiframe:\n                        frame_time_stabilizer -= stabilizer_step\n                        if frame_time_stabilizer < 0:\n                            frame_time_stabilizer = 0\n\n                    new_target = frame_time_stabilizer if is_webcam else max(frame_time_stabilizer, frame_time_target)\n                else:\n                    new_target = frame_time_target\n\n                next_frame_target = max(2 * new_target - video_frame_times.get_avg(), 0)\n                target_time = frame_time_start + next_frame_target - 0.001 # Let\'s just subtract a millisecond to be safe\n                \n                if out_path is None or args.emulate_playback:\n                    # This gives more accurate timing than if sleeping the whole amount at once\n                    while time.time() < target_time:\n                        time.sleep(0.001)\n                else:\n                    # Let\'s not starve the main thread, now\n                    time.sleep(0.001)\n        except:\n            # See issue #197 for why this is necessary\n            import traceback\n            traceback.print_exc()\n\n\n    extract_frame = lambda x, i: (x[0][i] if x[1][i][\'detection\'] is None else x[0][i].to(x[1][i][\'detection\'][\'box\'].device), [x[1][i]])\n\n    # Prime the network on the first frame because I do some thread unsafe things otherwise\n    print(\'Initializing model... \', end=\'\')\n    first_batch = eval_network(transform_frame(get_next_frame(vid)))\n    print(\'Done.\')\n\n    # For each frame the sequence of functions it needs to go through to be processed (in reversed order)\n    sequence = [prep_frame, eval_network, transform_frame]\n    pool = ThreadPool(processes=len(sequence) + args.video_multiframe + 2)\n    pool.apply_async(play_video)\n    active_frames = [{\'value\': extract_frame(first_batch, i), \'idx\': 0} for i in range(len(first_batch[0]))]\n\n    print()\n    if out_path is None: print(\'Press Escape to close.\')\n    try:\n        while vid.isOpened() and running:\n            # Hard limit on frames in buffer so we don\'t run out of memory >.>\n            while frame_buffer.qsize() > 100:\n                time.sleep(0.001)\n\n            start_time = time.time()\n\n            # Start loading the next frames from the disk\n            if not vid_done:\n                next_frames = pool.apply_async(get_next_frame, args=(vid,))\n            else:\n                next_frames = None\n            \n            if not (vid_done and len(active_frames) == 0):\n                # For each frame in our active processing queue, dispatch a job\n                # for that frame using the current function in the sequence\n                for frame in active_frames:\n                    _args =  [frame[\'value\']]\n                    if frame[\'idx\'] == 0:\n                        _args.append(fps_str)\n                    frame[\'value\'] = pool.apply_async(sequence[frame[\'idx\']], args=_args)\n                \n                # For each frame whose job was the last in the sequence (i.e. for all final outputs)\n                for frame in active_frames:\n                    if frame[\'idx\'] == 0:\n                        frame_buffer.put(frame[\'value\'].get())\n\n                # Remove the finished frames from the processing queue\n                active_frames = [x for x in active_frames if x[\'idx\'] > 0]\n\n                # Finish evaluating every frame in the processing queue and advanced their position in the sequence\n                for frame in list(reversed(active_frames)):\n                    frame[\'value\'] = frame[\'value\'].get()\n                    frame[\'idx\'] -= 1\n\n                    if frame[\'idx\'] == 0:\n                        # Split this up into individual threads for prep_frame since it doesn\'t support batch size\n                        active_frames += [{\'value\': extract_frame(frame[\'value\'], i), \'idx\': 0} for i in range(1, len(frame[\'value\'][0]))]\n                        frame[\'value\'] = extract_frame(frame[\'value\'], 0)\n                \n                # Finish loading in the next frames and add them to the processing queue\n                if next_frames is not None:\n                    frames = next_frames.get()\n                    if len(frames) == 0:\n                        vid_done = True\n                    else:\n                        active_frames.append({\'value\': frames, \'idx\': len(sequence)-1})\n\n                # Compute FPS\n                frame_times.add(time.time() - start_time)\n                fps = args.video_multiframe / frame_times.get_avg()\n            else:\n                fps = 0\n            \n            fps_str = \'Processing FPS: %.2f | Video Playback FPS: %.2f | Frames in Buffer: %d\' % (fps, video_fps, frame_buffer.qsize())\n            if not args.display_fps:\n                print(\'\\r\' + fps_str + \'    \', end=\'\')\n\n    except KeyboardInterrupt:\n        print(\'\\nStopping...\')\n    \n    cleanup_and_exit()\n\ndef evaluate(net:Yolact, dataset, train_mode=False):\n    net.detect.use_fast_nms = args.fast_nms\n    net.detect.use_cross_class_nms = args.cross_class_nms\n    cfg.mask_proto_debug = args.mask_proto_debug\n\n    # TODO Currently we do not support Fast Mask Re-scroing in evalimage, evalimages, and evalvideo\n    if args.image is not None:\n        if \':\' in args.image:\n            inp, out = args.image.split(\':\')\n            evalimage(net, inp, out)\n        else:\n            evalimage(net, args.image)\n        return\n    elif args.images is not None:\n        inp, out = args.images.split(\':\')\n        evalimages(net, inp, out)\n        return\n    elif args.video is not None:\n        if \':\' in args.video:\n            inp, out = args.video.split(\':\')\n            evalvideo(net, inp, out)\n        else:\n            evalvideo(net, args.video)\n        return\n\n    frame_times = MovingAverage()\n    dataset_size = len(dataset) if args.max_images < 0 else min(args.max_images, len(dataset))\n    progress_bar = ProgressBar(30, dataset_size)\n\n    print()\n\n    if not args.display and not args.benchmark:\n        # For each class and iou, stores tuples (score, isPositive)\n        # Index ap_data[type][iouIdx][classIdx]\n        ap_data = {\n            \'box\' : [[APDataObject() for _ in cfg.dataset.class_names] for _ in iou_thresholds],\n            \'mask\': [[APDataObject() for _ in cfg.dataset.class_names] for _ in iou_thresholds]\n        }\n        detections = Detections()\n    else:\n        timer.disable(\'Load Data\')\n\n    dataset_indices = list(range(len(dataset)))\n    \n    if args.shuffle:\n        random.shuffle(dataset_indices)\n    elif not args.no_sort:\n        # Do a deterministic shuffle based on the image ids\n        #\n        # I do this because on python 3.5 dictionary key order is *random*, while in 3.6 it\'s\n        # the order of insertion. That means on python 3.6, the images come in the order they are in\n        # in the annotations file. For some reason, the first images in the annotations file are\n        # the hardest. To combat this, I use a hard-coded hash function based on the image ids\n        # to shuffle the indices we use. That way, no matter what python version or how pycocotools\n        # handles the data, we get the same result every time.\n        hashed = [badhash(x) for x in dataset.ids]\n        dataset_indices.sort(key=lambda x: hashed[x])\n\n    dataset_indices = dataset_indices[:dataset_size]\n\n    try:\n        # Main eval loop\n        for it, image_idx in enumerate(dataset_indices):\n            timer.reset()\n\n            with timer.env(\'Load Data\'):\n                img, gt, gt_masks, h, w, num_crowd = dataset.pull_item(image_idx)\n\n                # Test flag, do not upvote\n                if cfg.mask_proto_debug:\n                    with open(\'scripts/info.txt\', \'w\') as f:\n                        f.write(str(dataset.ids[image_idx]))\n                    np.save(\'scripts/gt.npy\', gt_masks)\n\n                batch = Variable(img.unsqueeze(0))\n                if args.cuda:\n                    batch = batch.cuda()\n\n            with timer.env(\'Network Extra\'):\n                preds = net(batch)\n            # Perform the meat of the operation here depending on our mode.\n            if args.display:\n                img_numpy = prep_display(preds, img, h, w)\n            elif args.benchmark:\n                prep_benchmark(preds, h, w)\n            else:\n                prep_metrics(ap_data, preds, img, gt, gt_masks, h, w, num_crowd, dataset.ids[image_idx], detections)\n            \n            # First couple of images take longer because we\'re constructing the graph.\n            # Since that\'s technically initialization, don\'t include those in the FPS calculations.\n            if it > 1:\n                frame_times.add(timer.total_time())\n            \n            if args.display:\n                if it > 1:\n                    print(\'Avg FPS: %.4f\' % (1 / frame_times.get_avg()))\n                plt.imshow(img_numpy)\n                plt.title(str(dataset.ids[image_idx]))\n                plt.show()\n            elif not args.no_bar:\n                if it > 1: fps = 1 / frame_times.get_avg()\n                else: fps = 0\n                progress = (it+1) / dataset_size * 100\n                progress_bar.set_val(it+1)\n                print(\'\\rProcessing Images  %s %6d / %6d (%5.2f%%)    %5.2f fps        \'\n                    % (repr(progress_bar), it+1, dataset_size, progress, fps), end=\'\')\n\n\n\n        if not args.display and not args.benchmark:\n            print()\n            if args.output_coco_json:\n                print(\'Dumping detections...\')\n                if args.output_web_json:\n                    detections.dump_web()\n                else:\n                    detections.dump()\n            else:\n                if not train_mode:\n                    print(\'Saving data...\')\n                    with open(args.ap_data_file, \'wb\') as f:\n                        pickle.dump(ap_data, f)\n\n                return calc_map(ap_data)\n        elif args.benchmark:\n            print()\n            print()\n            print(\'Stats for the last frame:\')\n            timer.print_stats()\n            avg_seconds = frame_times.get_avg()\n            print(\'Average: %5.2f fps, %5.2f ms\' % (1 / frame_times.get_avg(), 1000*avg_seconds))\n\n    except KeyboardInterrupt:\n        print(\'Stopping...\')\n\n\ndef calc_map(ap_data):\n    print(\'Calculating mAP...\')\n    aps = [{\'box\': [], \'mask\': []} for _ in iou_thresholds]\n\n    for _class in range(len(cfg.dataset.class_names)):\n        for iou_idx in range(len(iou_thresholds)):\n            for iou_type in (\'box\', \'mask\'):\n                ap_obj = ap_data[iou_type][iou_idx][_class]\n\n                if not ap_obj.is_empty():\n                    aps[iou_idx][iou_type].append(ap_obj.get_ap())\n\n    all_maps = {\'box\': OrderedDict(), \'mask\': OrderedDict()}\n\n    # Looking back at it, this code is really hard to read :/\n    for iou_type in (\'box\', \'mask\'):\n        all_maps[iou_type][\'all\'] = 0 # Make this first in the ordereddict\n        for i, threshold in enumerate(iou_thresholds):\n            mAP = sum(aps[i][iou_type]) / len(aps[i][iou_type]) * 100 if len(aps[i][iou_type]) > 0 else 0\n            all_maps[iou_type][int(threshold*100)] = mAP\n        all_maps[iou_type][\'all\'] = (sum(all_maps[iou_type].values()) / (len(all_maps[iou_type].values())-1))\n    \n    print_maps(all_maps)\n    \n    # Put in a prettier format so we can serialize it to json during training\n    all_maps = {k: {j: round(u, 2) for j, u in v.items()} for k, v in all_maps.items()}\n    return all_maps\n\ndef print_maps(all_maps):\n    # Warning: hacky \n    make_row = lambda vals: (\' %5s |\' * len(vals)) % tuple(vals)\n    make_sep = lambda n:  (\'-------+\' * n)\n\n    print()\n    print(make_row([\'\'] + [(\'.%d \' % x if isinstance(x, int) else x + \' \') for x in all_maps[\'box\'].keys()]))\n    print(make_sep(len(all_maps[\'box\']) + 1))\n    for iou_type in (\'box\', \'mask\'):\n        print(make_row([iou_type] + [\'%.2f\' % x if x < 100 else \'%.1f\' % x for x in all_maps[iou_type].values()]))\n    print(make_sep(len(all_maps[\'box\']) + 1))\n    print()\n\n\n\nif __name__ == \'__main__\':\n    parse_args()\n\n    if args.config is not None:\n        set_cfg(args.config)\n\n    if args.trained_model == \'interrupt\':\n        args.trained_model = SavePath.get_interrupt(\'weights/\')\n    elif args.trained_model == \'latest\':\n        args.trained_model = SavePath.get_latest(\'weights/\', cfg.name)\n\n    if args.config is None:\n        model_path = SavePath.from_str(args.trained_model)\n        # TODO: Bad practice? Probably want to do a name lookup instead.\n        args.config = model_path.model_name + \'_config\'\n        print(\'Config not specified. Parsed %s from the file name.\\n\' % args.config)\n        set_cfg(args.config)\n\n    if args.detect:\n        cfg.eval_mask_branch = False\n\n    if args.dataset is not None:\n        set_dataset(args.dataset)\n\n    with torch.no_grad():\n        if not os.path.exists(\'results\'):\n            os.makedirs(\'results\')\n\n        if args.cuda:\n            cudnn.fastest = True\n            torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n        else:\n            torch.set_default_tensor_type(\'torch.FloatTensor\')\n\n        if args.resume and not args.display:\n            with open(args.ap_data_file, \'rb\') as f:\n                ap_data = pickle.load(f)\n            calc_map(ap_data)\n            exit()\n\n        if args.image is None and args.video is None and args.images is None:\n            dataset = COCODetection(cfg.dataset.valid_images, cfg.dataset.valid_info,\n                                    transform=BaseTransform(), has_gt=cfg.dataset.has_gt)\n            prep_coco_cats()\n        else:\n            dataset = None        \n\n        print(\'Loading model...\', end=\'\')\n        net = Yolact()\n        net.load_weights(args.trained_model)\n        net.eval()\n        print(\' Done.\')\n\n        if args.cuda:\n            net = net.cuda()\n\n        evaluate(net, dataset)\n\n\n'"
run_coco_eval.py,0,"b'""""""\nRuns the coco-supplied cocoeval script to evaluate detections\noutputted by using the output_coco_json flag in eval.py.\n""""""\n\n\nimport argparse\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\n\nparser = argparse.ArgumentParser(description=\'COCO Detections Evaluator\')\nparser.add_argument(\'--bbox_det_file\', default=\'results/bbox_detections.json\', type=str)\nparser.add_argument(\'--mask_det_file\', default=\'results/mask_detections.json\', type=str)\nparser.add_argument(\'--gt_ann_file\',   default=\'data/coco/annotations/instances_val2017.json\', type=str)\nparser.add_argument(\'--eval_type\',     default=\'both\', choices=[\'bbox\', \'mask\', \'both\'], type=str)\nargs = parser.parse_args()\n\n\n\nif __name__ == \'__main__\':\n\n\teval_bbox = (args.eval_type in (\'bbox\', \'both\'))\n\teval_mask = (args.eval_type in (\'mask\', \'both\'))\n\n\tprint(\'Loading annotations...\')\n\tgt_annotations = COCO(args.gt_ann_file)\n\tif eval_bbox:\n\t\tbbox_dets = gt_annotations.loadRes(args.bbox_det_file)\n\tif eval_mask:\n\t\tmask_dets = gt_annotations.loadRes(args.mask_det_file)\n\n\tif eval_bbox:\n\t\tprint(\'\\nEvaluating BBoxes:\')\n\t\tbbox_eval = COCOeval(gt_annotations, bbox_dets, \'bbox\')\n\t\tbbox_eval.evaluate()\n\t\tbbox_eval.accumulate()\n\t\tbbox_eval.summarize()\n\t\n\tif eval_mask:\n\t\tprint(\'\\nEvaluating Masks:\')\n\t\tbbox_eval = COCOeval(gt_annotations, mask_dets, \'segm\')\n\t\tbbox_eval.evaluate()\n\t\tbbox_eval.accumulate()\n\t\tbbox_eval.summarize()\n\n\n\n'"
train.py,21,"b'from data import *\nfrom utils.augmentations import SSDAugmentation, BaseTransform\nfrom utils.functions import MovingAverage, SavePath\nfrom utils.logger import Log\nfrom utils import timer\nfrom layers.modules import MultiBoxLoss\nfrom yolact import Yolact\nimport os\nimport sys\nimport time\nimport math, random\nfrom pathlib import Path\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nimport torch.nn.init as init\nimport torch.utils.data as data\nimport numpy as np\nimport argparse\nimport datetime\n\n# Oof\nimport eval as eval_script\n\ndef str2bool(v):\n    return v.lower() in (""yes"", ""true"", ""t"", ""1"")\n\n\nparser = argparse.ArgumentParser(\n    description=\'Yolact Training Script\')\nparser.add_argument(\'--batch_size\', default=8, type=int,\n                    help=\'Batch size for training\')\nparser.add_argument(\'--resume\', default=None, type=str,\n                    help=\'Checkpoint state_dict file to resume training from. If this is ""interrupt""\'\\\n                         \', the model will resume training from the interrupt file.\')\nparser.add_argument(\'--start_iter\', default=-1, type=int,\n                    help=\'Resume training at this iter. If this is -1, the iteration will be\'\\\n                         \'determined from the file name.\')\nparser.add_argument(\'--num_workers\', default=4, type=int,\n                    help=\'Number of workers used in dataloading\')\nparser.add_argument(\'--cuda\', default=True, type=str2bool,\n                    help=\'Use CUDA to train model\')\nparser.add_argument(\'--lr\', \'--learning_rate\', default=None, type=float,\n                    help=\'Initial learning rate. Leave as None to read this from the config.\')\nparser.add_argument(\'--momentum\', default=None, type=float,\n                    help=\'Momentum for SGD. Leave as None to read this from the config.\')\nparser.add_argument(\'--decay\', \'--weight_decay\', default=None, type=float,\n                    help=\'Weight decay for SGD. Leave as None to read this from the config.\')\nparser.add_argument(\'--gamma\', default=None, type=float,\n                    help=\'For each lr step, what to multiply the lr by. Leave as None to read this from the config.\')\nparser.add_argument(\'--save_folder\', default=\'weights/\',\n                    help=\'Directory for saving checkpoint models.\')\nparser.add_argument(\'--log_folder\', default=\'logs/\',\n                    help=\'Directory for saving logs.\')\nparser.add_argument(\'--config\', default=None,\n                    help=\'The config object to use.\')\nparser.add_argument(\'--save_interval\', default=10000, type=int,\n                    help=\'The number of iterations between saving the model.\')\nparser.add_argument(\'--validation_size\', default=5000, type=int,\n                    help=\'The number of images to use for validation.\')\nparser.add_argument(\'--validation_epoch\', default=2, type=int,\n                    help=\'Output validation information every n iterations. If -1, do no validation.\')\nparser.add_argument(\'--keep_latest\', dest=\'keep_latest\', action=\'store_true\',\n                    help=\'Only keep the latest checkpoint instead of each one.\')\nparser.add_argument(\'--keep_latest_interval\', default=100000, type=int,\n                    help=\'When --keep_latest is on, don\\\'t delete the latest file at these intervals. This should be a multiple of save_interval or 0.\')\nparser.add_argument(\'--dataset\', default=None, type=str,\n                    help=\'If specified, override the dataset specified in the config with this one (example: coco2017_dataset).\')\nparser.add_argument(\'--no_log\', dest=\'log\', action=\'store_false\',\n                    help=\'Don\\\'t log per iteration information into log_folder.\')\nparser.add_argument(\'--log_gpu\', dest=\'log_gpu\', action=\'store_true\',\n                    help=\'Include GPU information in the logs. Nvidia-smi tends to be slow, so set this with caution.\')\nparser.add_argument(\'--no_interrupt\', dest=\'interrupt\', action=\'store_false\',\n                    help=\'Don\\\'t save an interrupt when KeyboardInterrupt is caught.\')\nparser.add_argument(\'--batch_alloc\', default=None, type=str,\n                    help=\'If using multiple GPUS, you can set this to be a comma separated list detailing which GPUs should get what local batch size (It should add up to your total batch size).\')\nparser.add_argument(\'--no_autoscale\', dest=\'autoscale\', action=\'store_false\',\n                    help=\'YOLACT will automatically scale the lr and the number of iterations depending on the batch size. Set this if you want to disable that.\')\n\nparser.set_defaults(keep_latest=False, log=True, log_gpu=False, interrupt=True, autoscale=True)\nargs = parser.parse_args()\n\nif args.config is not None:\n    set_cfg(args.config)\n\nif args.dataset is not None:\n    set_dataset(args.dataset)\n\nif args.autoscale and args.batch_size != 8:\n    factor = args.batch_size / 8\n    if __name__ == \'__main__\':\n        print(\'Scaling parameters by %.2f to account for a batch size of %d.\' % (factor, args.batch_size))\n\n    cfg.lr *= factor\n    cfg.max_iter //= factor\n    cfg.lr_steps = [x // factor for x in cfg.lr_steps]\n\n# Update training parameters from the config if necessary\ndef replace(name):\n    if getattr(args, name) == None: setattr(args, name, getattr(cfg, name))\nreplace(\'lr\')\nreplace(\'decay\')\nreplace(\'gamma\')\nreplace(\'momentum\')\n\n# This is managed by set_lr\ncur_lr = args.lr\n\nif torch.cuda.device_count() == 0:\n    print(\'No GPUs detected. Exiting...\')\n    exit(-1)\n\nif args.batch_size // torch.cuda.device_count() < 6:\n    if __name__ == \'__main__\':\n        print(\'Per-GPU batch size is less than the recommended limit for batch norm. Disabling batch norm.\')\n    cfg.freeze_bn = True\n\nloss_types = [\'B\', \'C\', \'M\', \'P\', \'D\', \'E\', \'S\', \'I\']\n\nif torch.cuda.is_available():\n    if args.cuda:\n        torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n    if not args.cuda:\n        print(""WARNING: It looks like you have a CUDA device, but aren\'t "" +\n              ""using CUDA.\\nRun with --cuda for optimal training speed."")\n        torch.set_default_tensor_type(\'torch.FloatTensor\')\nelse:\n    torch.set_default_tensor_type(\'torch.FloatTensor\')\n\nclass NetLoss(nn.Module):\n    """"""\n    A wrapper for running the network and computing the loss\n    This is so we can more efficiently use DataParallel.\n    """"""\n    \n    def __init__(self, net:Yolact, criterion:MultiBoxLoss):\n        super().__init__()\n\n        self.net = net\n        self.criterion = criterion\n    \n    def forward(self, images, targets, masks, num_crowds):\n        preds = self.net(images)\n        losses = self.criterion(self.net, preds, targets, masks, num_crowds)\n        return losses\n\nclass CustomDataParallel(nn.DataParallel):\n    """"""\n    This is a custom version of DataParallel that works better with our training data.\n    It should also be faster than the general case.\n    """"""\n\n    def scatter(self, inputs, kwargs, device_ids):\n        # More like scatter and data prep at the same time. The point is we prep the data in such a way\n        # that no scatter is necessary, and there\'s no need to shuffle stuff around different GPUs.\n        devices = [\'cuda:\' + str(x) for x in device_ids]\n        splits = prepare_data(inputs[0], devices, allocation=args.batch_alloc)\n\n        return [[split[device_idx] for split in splits] for device_idx in range(len(devices))], \\\n            [kwargs] * len(devices)\n\n    def gather(self, outputs, output_device):\n        out = {}\n\n        for k in outputs[0]:\n            out[k] = torch.stack([output[k].to(output_device) for output in outputs])\n        \n        return out\n\ndef train():\n    if not os.path.exists(args.save_folder):\n        os.mkdir(args.save_folder)\n\n    dataset = COCODetection(image_path=cfg.dataset.train_images,\n                            info_file=cfg.dataset.train_info,\n                            transform=SSDAugmentation(MEANS))\n    \n    if args.validation_epoch > 0:\n        setup_eval()\n        val_dataset = COCODetection(image_path=cfg.dataset.valid_images,\n                                    info_file=cfg.dataset.valid_info,\n                                    transform=BaseTransform(MEANS))\n\n    # Parallel wraps the underlying module, but when saving and loading we don\'t want that\n    yolact_net = Yolact()\n    net = yolact_net\n    net.train()\n\n    if args.log:\n        log = Log(cfg.name, args.log_folder, dict(args._get_kwargs()),\n            overwrite=(args.resume is None), log_gpu_stats=args.log_gpu)\n\n    # I don\'t use the timer during training (I use a different timing method).\n    # Apparently there\'s a race condition with multiple GPUs, so disable it just to be safe.\n    timer.disable_all()\n\n    # Both of these can set args.resume to None, so do them before the check    \n    if args.resume == \'interrupt\':\n        args.resume = SavePath.get_interrupt(args.save_folder)\n    elif args.resume == \'latest\':\n        args.resume = SavePath.get_latest(args.save_folder, cfg.name)\n\n    if args.resume is not None:\n        print(\'Resuming training, loading {}...\'.format(args.resume))\n        yolact_net.load_weights(args.resume)\n\n        if args.start_iter == -1:\n            args.start_iter = SavePath.from_str(args.resume).iteration\n    else:\n        print(\'Initializing weights...\')\n        yolact_net.init_weights(backbone_path=args.save_folder + cfg.backbone.path)\n\n    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum,\n                          weight_decay=args.decay)\n    criterion = MultiBoxLoss(num_classes=cfg.num_classes,\n                             pos_threshold=cfg.positive_iou_threshold,\n                             neg_threshold=cfg.negative_iou_threshold,\n                             negpos_ratio=cfg.ohem_negpos_ratio)\n\n    if args.batch_alloc is not None:\n        args.batch_alloc = [int(x) for x in args.batch_alloc.split(\',\')]\n        if sum(args.batch_alloc) != args.batch_size:\n            print(\'Error: Batch allocation (%s) does not sum to batch size (%s).\' % (args.batch_alloc, args.batch_size))\n            exit(-1)\n\n    net = CustomDataParallel(NetLoss(net, criterion))\n    if args.cuda:\n        net = net.cuda()\n    \n    # Initialize everything\n    if not cfg.freeze_bn: yolact_net.freeze_bn() # Freeze bn so we don\'t kill our means\n    yolact_net(torch.zeros(1, 3, cfg.max_size, cfg.max_size).cuda())\n    if not cfg.freeze_bn: yolact_net.freeze_bn(True)\n\n    # loss counters\n    loc_loss = 0\n    conf_loss = 0\n    iteration = max(args.start_iter, 0)\n    last_time = time.time()\n\n    epoch_size = len(dataset) // args.batch_size\n    num_epochs = math.ceil(cfg.max_iter / epoch_size)\n    \n    # Which learning rate adjustment step are we on? lr\' = lr * gamma ^ step_index\n    step_index = 0\n\n    data_loader = data.DataLoader(dataset, args.batch_size,\n                                  num_workers=args.num_workers,\n                                  shuffle=True, collate_fn=detection_collate,\n                                  pin_memory=True)\n    \n    \n    save_path = lambda epoch, iteration: SavePath(cfg.name, epoch, iteration).get_path(root=args.save_folder)\n    time_avg = MovingAverage()\n\n    global loss_types # Forms the print order\n    loss_avgs  = { k: MovingAverage(100) for k in loss_types }\n\n    print(\'Begin training!\')\n    print()\n    # try-except so you can use ctrl+c to save early and stop training\n    try:\n        for epoch in range(num_epochs):\n            # Resume from start_iter\n            if (epoch+1)*epoch_size < iteration:\n                continue\n            \n            for datum in data_loader:\n                # Stop if we\'ve reached an epoch if we\'re resuming from start_iter\n                if iteration == (epoch+1)*epoch_size:\n                    break\n\n                # Stop at the configured number of iterations even if mid-epoch\n                if iteration == cfg.max_iter:\n                    break\n\n                # Change a config setting if we\'ve reached the specified iteration\n                changed = False\n                for change in cfg.delayed_settings:\n                    if iteration >= change[0]:\n                        changed = True\n                        cfg.replace(change[1])\n\n                        # Reset the loss averages because things might have changed\n                        for avg in loss_avgs:\n                            avg.reset()\n                \n                # If a config setting was changed, remove it from the list so we don\'t keep checking\n                if changed:\n                    cfg.delayed_settings = [x for x in cfg.delayed_settings if x[0] > iteration]\n\n                # Warm up by linearly interpolating the learning rate from some smaller value\n                if cfg.lr_warmup_until > 0 and iteration <= cfg.lr_warmup_until:\n                    set_lr(optimizer, (args.lr - cfg.lr_warmup_init) * (iteration / cfg.lr_warmup_until) + cfg.lr_warmup_init)\n\n                # Adjust the learning rate at the given iterations, but also if we resume from past that iteration\n                while step_index < len(cfg.lr_steps) and iteration >= cfg.lr_steps[step_index]:\n                    step_index += 1\n                    set_lr(optimizer, args.lr * (args.gamma ** step_index))\n                \n                # Zero the grad to get ready to compute gradients\n                optimizer.zero_grad()\n\n                # Forward Pass + Compute loss at the same time (see CustomDataParallel and NetLoss)\n                losses = net(datum)\n                \n                losses = { k: (v).mean() for k,v in losses.items() } # Mean here because Dataparallel\n                loss = sum([losses[k] for k in losses])\n                \n                # no_inf_mean removes some components from the loss, so make sure to backward through all of it\n                # all_loss = sum([v.mean() for v in losses.values()])\n\n                # Backprop\n                loss.backward() # Do this to free up vram even if loss is not finite\n                if torch.isfinite(loss).item():\n                    optimizer.step()\n                \n                # Add the loss to the moving average for bookkeeping\n                for k in losses:\n                    loss_avgs[k].add(losses[k].item())\n\n                cur_time  = time.time()\n                elapsed   = cur_time - last_time\n                last_time = cur_time\n\n                # Exclude graph setup from the timing information\n                if iteration != args.start_iter:\n                    time_avg.add(elapsed)\n\n                if iteration % 10 == 0:\n                    eta_str = str(datetime.timedelta(seconds=(cfg.max_iter-iteration) * time_avg.get_avg())).split(\'.\')[0]\n                    \n                    total = sum([loss_avgs[k].get_avg() for k in losses])\n                    loss_labels = sum([[k, loss_avgs[k].get_avg()] for k in loss_types if k in losses], [])\n                    \n                    print((\'[%3d] %7d ||\' + (\' %s: %.3f |\' * len(losses)) + \' T: %.3f || ETA: %s || timer: %.3f\')\n                            % tuple([epoch, iteration] + loss_labels + [total, eta_str, elapsed]), flush=True)\n\n                if args.log:\n                    precision = 5\n                    loss_info = {k: round(losses[k].item(), precision) for k in losses}\n                    loss_info[\'T\'] = round(loss.item(), precision)\n\n                    if args.log_gpu:\n                        log.log_gpu_stats = (iteration % 10 == 0) # nvidia-smi is sloooow\n                        \n                    log.log(\'train\', loss=loss_info, epoch=epoch, iter=iteration,\n                        lr=round(cur_lr, 10), elapsed=elapsed)\n\n                    log.log_gpu_stats = args.log_gpu\n                \n                iteration += 1\n\n                if iteration % args.save_interval == 0 and iteration != args.start_iter:\n                    if args.keep_latest:\n                        latest = SavePath.get_latest(args.save_folder, cfg.name)\n\n                    print(\'Saving state, iter:\', iteration)\n                    yolact_net.save_weights(save_path(epoch, iteration))\n\n                    if args.keep_latest and latest is not None:\n                        if args.keep_latest_interval <= 0 or iteration % args.keep_latest_interval != args.save_interval:\n                            print(\'Deleting old save...\')\n                            os.remove(latest)\n            \n            # This is done per epoch\n            if args.validation_epoch > 0:\n                if epoch % args.validation_epoch == 0 and epoch > 0:\n                    compute_validation_map(epoch, iteration, yolact_net, val_dataset, log if args.log else None)\n        \n        # Compute validation mAP after training is finished\n        compute_validation_map(epoch, iteration, yolact_net, val_dataset, log if args.log else None)\n    except KeyboardInterrupt:\n        if args.interrupt:\n            print(\'Stopping early. Saving network...\')\n            \n            # Delete previous copy of the interrupted network so we don\'t spam the weights folder\n            SavePath.remove_interrupt(args.save_folder)\n            \n            yolact_net.save_weights(save_path(epoch, repr(iteration) + \'_interrupt\'))\n        exit()\n\n    yolact_net.save_weights(save_path(epoch, iteration))\n\n\ndef set_lr(optimizer, new_lr):\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = new_lr\n    \n    global cur_lr\n    cur_lr = new_lr\n\ndef gradinator(x):\n    x.requires_grad = False\n    return x\n\ndef prepare_data(datum, devices:list=None, allocation:list=None):\n    with torch.no_grad():\n        if devices is None:\n            devices = [\'cuda:0\'] if args.cuda else [\'cpu\']\n        if allocation is None:\n            allocation = [args.batch_size // len(devices)] * (len(devices) - 1)\n            allocation.append(args.batch_size - sum(allocation)) # The rest might need more/less\n        \n        images, (targets, masks, num_crowds) = datum\n\n        cur_idx = 0\n        for device, alloc in zip(devices, allocation):\n            for _ in range(alloc):\n                images[cur_idx]  = gradinator(images[cur_idx].to(device))\n                targets[cur_idx] = gradinator(targets[cur_idx].to(device))\n                masks[cur_idx]   = gradinator(masks[cur_idx].to(device))\n                cur_idx += 1\n\n        if cfg.preserve_aspect_ratio:\n            # Choose a random size from the batch\n            _, h, w = images[random.randint(0, len(images)-1)].size()\n\n            for idx, (image, target, mask, num_crowd) in enumerate(zip(images, targets, masks, num_crowds)):\n                images[idx], targets[idx], masks[idx], num_crowds[idx] \\\n                    = enforce_size(image, target, mask, num_crowd, w, h)\n        \n        cur_idx = 0\n        split_images, split_targets, split_masks, split_numcrowds \\\n            = [[None for alloc in allocation] for _ in range(4)]\n\n        for device_idx, alloc in enumerate(allocation):\n            split_images[device_idx]    = torch.stack(images[cur_idx:cur_idx+alloc], dim=0)\n            split_targets[device_idx]   = targets[cur_idx:cur_idx+alloc]\n            split_masks[device_idx]     = masks[cur_idx:cur_idx+alloc]\n            split_numcrowds[device_idx] = num_crowds[cur_idx:cur_idx+alloc]\n\n            cur_idx += alloc\n\n        return split_images, split_targets, split_masks, split_numcrowds\n\ndef no_inf_mean(x:torch.Tensor):\n    """"""\n    Computes the mean of a vector, throwing out all inf values.\n    If there are no non-inf values, this will return inf (i.e., just the normal mean).\n    """"""\n\n    no_inf = [a for a in x if torch.isfinite(a)]\n\n    if len(no_inf) > 0:\n        return sum(no_inf) / len(no_inf)\n    else:\n        return x.mean()\n\ndef compute_validation_loss(net, data_loader, criterion):\n    global loss_types\n\n    with torch.no_grad():\n        losses = {}\n        \n        # Don\'t switch to eval mode because we want to get losses\n        iterations = 0\n        for datum in data_loader:\n            images, targets, masks, num_crowds = prepare_data(datum)\n            out = net(images)\n\n            wrapper = ScatterWrapper(targets, masks, num_crowds)\n            _losses = criterion(out, wrapper, wrapper.make_mask())\n            \n            for k, v in _losses.items():\n                v = v.mean().item()\n                if k in losses:\n                    losses[k] += v\n                else:\n                    losses[k] = v\n\n            iterations += 1\n            if args.validation_size <= iterations * args.batch_size:\n                break\n        \n        for k in losses:\n            losses[k] /= iterations\n            \n        \n        loss_labels = sum([[k, losses[k]] for k in loss_types if k in losses], [])\n        print((\'Validation ||\' + (\' %s: %.3f |\' * len(losses)) + \')\') % tuple(loss_labels), flush=True)\n\ndef compute_validation_map(epoch, iteration, yolact_net, dataset, log:Log=None):\n    with torch.no_grad():\n        yolact_net.eval()\n        \n        start = time.time()\n        print()\n        print(""Computing validation mAP (this may take a while)..."", flush=True)\n        val_info = eval_script.evaluate(yolact_net, dataset, train_mode=True)\n        end = time.time()\n\n        if log is not None:\n            log.log(\'val\', val_info, elapsed=(end - start), epoch=epoch, iter=iteration)\n\n        yolact_net.train()\n\ndef setup_eval():\n    eval_script.parse_args([\'--no_bar\', \'--max_images=\'+str(args.validation_size)])\n\nif __name__ == \'__main__\':\n    train()\n'"
yolact.py,30,"b'import torch, torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models.resnet import Bottleneck\nimport numpy as np\nfrom itertools import product\nfrom math import sqrt\nfrom typing import List\nfrom collections import defaultdict\n\nfrom data.config import cfg, mask_type\nfrom layers import Detect\nfrom layers.interpolate import InterpolateModule\nfrom backbone import construct_backbone\n\nimport torch.backends.cudnn as cudnn\nfrom utils import timer\nfrom utils.functions import MovingAverage, make_net\n\n# This is required for Pytorch 1.0.1 on Windows to initialize Cuda on some driver versions.\n# See the bug report here: https://github.com/pytorch/pytorch/issues/17108\ntorch.cuda.current_device()\n\n# As of March 10, 2019, Pytorch DataParallel still doesn\'t support JIT Script Modules\nuse_jit = torch.cuda.device_count() <= 1\nif not use_jit:\n    print(\'Multiple GPUs detected! Turning off JIT.\')\n\nScriptModuleWrapper = torch.jit.ScriptModule if use_jit else nn.Module\nscript_method_wrapper = torch.jit.script_method if use_jit else lambda fn, _rcn=None: fn\n\n\n\nclass Concat(nn.Module):\n    def __init__(self, nets, extra_params):\n        super().__init__()\n\n        self.nets = nn.ModuleList(nets)\n        self.extra_params = extra_params\n    \n    def forward(self, x):\n        # Concat each along the channel dimension\n        return torch.cat([net(x) for net in self.nets], dim=1, **self.extra_params)\n\nprior_cache = defaultdict(lambda: None)\n\nclass PredictionModule(nn.Module):\n    """"""\n    The (c) prediction module adapted from DSSD:\n    https://arxiv.org/pdf/1701.06659.pdf\n\n    Note that this is slightly different to the module in the paper\n    because the Bottleneck block actually has a 3x3 convolution in\n    the middle instead of a 1x1 convolution. Though, I really can\'t\n    be arsed to implement it myself, and, who knows, this might be\n    better.\n\n    Args:\n        - in_channels:   The input feature size.\n        - out_channels:  The output feature size (must be a multiple of 4).\n        - aspect_ratios: A list of lists of priorbox aspect ratios (one list per scale).\n        - scales:        A list of priorbox scales relative to this layer\'s convsize.\n                         For instance: If this layer has convouts of size 30x30 for\n                                       an image of size 600x600, the \'default\' (scale\n                                       of 1) for this layer would produce bounding\n                                       boxes with an area of 20x20px. If the scale is\n                                       .5 on the other hand, this layer would consider\n                                       bounding boxes with area 10x10px, etc.\n        - parent:        If parent is a PredictionModule, this module will use all the layers\n                         from parent instead of from this module.\n    """"""\n    \n    def __init__(self, in_channels, out_channels=1024, aspect_ratios=[[1]], scales=[1], parent=None, index=0):\n        super().__init__()\n\n        self.num_classes = cfg.num_classes\n        self.mask_dim    = cfg.mask_dim # Defined by Yolact\n        self.num_priors  = sum(len(x)*len(scales) for x in aspect_ratios)\n        self.parent      = [parent] # Don\'t include this in the state dict\n        self.index       = index\n        self.num_heads   = cfg.num_heads # Defined by Yolact\n\n        if cfg.mask_proto_split_prototypes_by_head and cfg.mask_type == mask_type.lincomb:\n            self.mask_dim = self.mask_dim // self.num_heads\n\n        if cfg.mask_proto_prototypes_as_features:\n            in_channels += self.mask_dim\n        \n        if parent is None:\n            if cfg.extra_head_net is None:\n                out_channels = in_channels\n            else:\n                self.upfeature, out_channels = make_net(in_channels, cfg.extra_head_net)\n\n            if cfg.use_prediction_module:\n                self.block = Bottleneck(out_channels, out_channels // 4)\n                self.conv = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=True)\n                self.bn = nn.BatchNorm2d(out_channels)\n\n            self.bbox_layer = nn.Conv2d(out_channels, self.num_priors * 4,                **cfg.head_layer_params)\n            self.conf_layer = nn.Conv2d(out_channels, self.num_priors * self.num_classes, **cfg.head_layer_params)\n            self.mask_layer = nn.Conv2d(out_channels, self.num_priors * self.mask_dim,    **cfg.head_layer_params)\n            \n            if cfg.use_mask_scoring:\n                self.score_layer = nn.Conv2d(out_channels, self.num_priors, **cfg.head_layer_params)\n\n            if cfg.use_instance_coeff:\n                self.inst_layer = nn.Conv2d(out_channels, self.num_priors * cfg.num_instance_coeffs, **cfg.head_layer_params)\n            \n            # What is this ugly lambda doing in the middle of all this clean prediction module code?\n            def make_extra(num_layers):\n                if num_layers == 0:\n                    return lambda x: x\n                else:\n                    # Looks more complicated than it is. This just creates an array of num_layers alternating conv-relu\n                    return nn.Sequential(*sum([[\n                        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n                        nn.ReLU(inplace=True)\n                    ] for _ in range(num_layers)], []))\n\n            self.bbox_extra, self.conf_extra, self.mask_extra = [make_extra(x) for x in cfg.extra_layers]\n            \n            if cfg.mask_type == mask_type.lincomb and cfg.mask_proto_coeff_gate:\n                self.gate_layer = nn.Conv2d(out_channels, self.num_priors * self.mask_dim, kernel_size=3, padding=1)\n\n        self.aspect_ratios = aspect_ratios\n        self.scales = scales\n\n        self.priors = None\n        self.last_conv_size = None\n        self.last_img_size = None\n\n    def forward(self, x):\n        """"""\n        Args:\n            - x: The convOut from a layer in the backbone network\n                 Size: [batch_size, in_channels, conv_h, conv_w])\n\n        Returns a tuple (bbox_coords, class_confs, mask_output, prior_boxes) with sizes\n            - bbox_coords: [batch_size, conv_h*conv_w*num_priors, 4]\n            - class_confs: [batch_size, conv_h*conv_w*num_priors, num_classes]\n            - mask_output: [batch_size, conv_h*conv_w*num_priors, mask_dim]\n            - prior_boxes: [conv_h*conv_w*num_priors, 4]\n        """"""\n        # In case we want to use another module\'s layers\n        src = self if self.parent[0] is None else self.parent[0]\n        \n        conv_h = x.size(2)\n        conv_w = x.size(3)\n        \n        if cfg.extra_head_net is not None:\n            x = src.upfeature(x)\n        \n        if cfg.use_prediction_module:\n            # The two branches of PM design (c)\n            a = src.block(x)\n            \n            b = src.conv(x)\n            b = src.bn(b)\n            b = F.relu(b)\n            \n            # TODO: Possibly switch this out for a product\n            x = a + b\n\n        bbox_x = src.bbox_extra(x)\n        conf_x = src.conf_extra(x)\n        mask_x = src.mask_extra(x)\n\n        bbox = src.bbox_layer(bbox_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 4)\n        conf = src.conf_layer(conf_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.num_classes)\n        \n        if cfg.eval_mask_branch:\n            mask = src.mask_layer(mask_x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.mask_dim)\n        else:\n            mask = torch.zeros(x.size(0), bbox.size(1), self.mask_dim, device=bbox.device)\n\n        if cfg.use_mask_scoring:\n            score = src.score_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, 1)\n\n        if cfg.use_instance_coeff:\n            inst = src.inst_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, cfg.num_instance_coeffs)    \n\n        # See box_utils.decode for an explanation of this\n        if cfg.use_yolo_regressors:\n            bbox[:, :, :2] = torch.sigmoid(bbox[:, :, :2]) - 0.5\n            bbox[:, :, 0] /= conv_w\n            bbox[:, :, 1] /= conv_h\n\n        if cfg.eval_mask_branch:\n            if cfg.mask_type == mask_type.direct:\n                mask = torch.sigmoid(mask)\n            elif cfg.mask_type == mask_type.lincomb:\n                mask = cfg.mask_proto_coeff_activation(mask)\n\n                if cfg.mask_proto_coeff_gate:\n                    gate = src.gate_layer(x).permute(0, 2, 3, 1).contiguous().view(x.size(0), -1, self.mask_dim)\n                    mask = mask * torch.sigmoid(gate)\n\n        if cfg.mask_proto_split_prototypes_by_head and cfg.mask_type == mask_type.lincomb:\n            mask = F.pad(mask, (self.index * self.mask_dim, (self.num_heads - self.index - 1) * self.mask_dim), mode=\'constant\', value=0)\n        \n        priors = self.make_priors(conv_h, conv_w, x.device)\n\n        preds = { \'loc\': bbox, \'conf\': conf, \'mask\': mask, \'priors\': priors }\n\n        if cfg.use_mask_scoring:\n            preds[\'score\'] = score\n\n        if cfg.use_instance_coeff:\n            preds[\'inst\'] = inst\n        \n        return preds\n\n    def make_priors(self, conv_h, conv_w, device):\n        """""" Note that priors are [x,y,width,height] where (x,y) is the center of the box. """"""\n        global prior_cache\n        size = (conv_h, conv_w)\n\n        with timer.env(\'makepriors\'):\n            if self.last_img_size != (cfg._tmp_img_w, cfg._tmp_img_h):\n                prior_data = []\n\n                # Iteration order is important (it has to sync up with the convout)\n                for j, i in product(range(conv_h), range(conv_w)):\n                    # +0.5 because priors are in center-size notation\n                    x = (i + 0.5) / conv_w\n                    y = (j + 0.5) / conv_h\n                    \n                    for ars in self.aspect_ratios:\n                        for scale in self.scales:\n                            for ar in ars:\n                                if not cfg.backbone.preapply_sqrt:\n                                    ar = sqrt(ar)\n\n                                if cfg.backbone.use_pixel_scales:\n                                    w = scale * ar / cfg.max_size\n                                    h = scale / ar / cfg.max_size\n                                else:\n                                    w = scale * ar / conv_w\n                                    h = scale / ar / conv_h\n                                \n                                # This is for backward compatability with a bug where I made everything square by accident\n                                if cfg.backbone.use_square_anchors:\n                                    h = w\n\n                                prior_data += [x, y, w, h]\n\n                self.priors = torch.Tensor(prior_data, device=device).view(-1, 4).detach()\n                self.priors.requires_grad = False\n                self.last_img_size = (cfg._tmp_img_w, cfg._tmp_img_h)\n                self.last_conv_size = (conv_w, conv_h)\n                prior_cache[size] = None\n            elif self.priors.device != device:\n                # This whole weird situation is so that DataParalell doesn\'t copy the priors each iteration\n                if prior_cache[size] is None:\n                    prior_cache[size] = {}\n                \n                if device not in prior_cache[size]:\n                    prior_cache[size][device] = self.priors.to(device)\n\n                self.priors = prior_cache[size][device]\n        \n        return self.priors\n\nclass FPN(ScriptModuleWrapper):\n    """"""\n    Implements a general version of the FPN introduced in\n    https://arxiv.org/pdf/1612.03144.pdf\n\n    Parameters (in cfg.fpn):\n        - num_features (int): The number of output features in the fpn layers.\n        - interpolation_mode (str): The mode to pass to F.interpolate.\n        - num_downsample (int): The number of downsampled layers to add onto the selected layers.\n                                These extra layers are downsampled from the last selected layer.\n\n    Args:\n        - in_channels (list): For each conv layer you supply in the forward pass,\n                              how many features will it have?\n    """"""\n    __constants__ = [\'interpolation_mode\', \'num_downsample\', \'use_conv_downsample\', \'relu_pred_layers\',\n                     \'lat_layers\', \'pred_layers\', \'downsample_layers\', \'relu_downsample_layers\']\n\n    def __init__(self, in_channels):\n        super().__init__()\n\n        self.lat_layers  = nn.ModuleList([\n            nn.Conv2d(x, cfg.fpn.num_features, kernel_size=1)\n            for x in reversed(in_channels)\n        ])\n\n        # This is here for backwards compatability\n        padding = 1 if cfg.fpn.pad else 0\n        self.pred_layers = nn.ModuleList([\n            nn.Conv2d(cfg.fpn.num_features, cfg.fpn.num_features, kernel_size=3, padding=padding)\n            for _ in in_channels\n        ])\n\n        if cfg.fpn.use_conv_downsample:\n            self.downsample_layers = nn.ModuleList([\n                nn.Conv2d(cfg.fpn.num_features, cfg.fpn.num_features, kernel_size=3, padding=1, stride=2)\n                for _ in range(cfg.fpn.num_downsample)\n            ])\n        \n        self.interpolation_mode     = cfg.fpn.interpolation_mode\n        self.num_downsample         = cfg.fpn.num_downsample\n        self.use_conv_downsample    = cfg.fpn.use_conv_downsample\n        self.relu_downsample_layers = cfg.fpn.relu_downsample_layers\n        self.relu_pred_layers       = cfg.fpn.relu_pred_layers\n\n    @script_method_wrapper\n    def forward(self, convouts:List[torch.Tensor]):\n        """"""\n        Args:\n            - convouts (list): A list of convouts for the corresponding layers in in_channels.\n        Returns:\n            - A list of FPN convouts in the same order as x with extra downsample layers if requested.\n        """"""\n\n        out = []\n        x = torch.zeros(1, device=convouts[0].device)\n        for i in range(len(convouts)):\n            out.append(x)\n\n        # For backward compatability, the conv layers are stored in reverse but the input and output is\n        # given in the correct order. Thus, use j=-i-1 for the input and output and i for the conv layers.\n        j = len(convouts)\n        for lat_layer in self.lat_layers:\n            j -= 1\n\n            if j < len(convouts) - 1:\n                _, _, h, w = convouts[j].size()\n                x = F.interpolate(x, size=(h, w), mode=self.interpolation_mode, align_corners=False)\n            \n            x = x + lat_layer(convouts[j])\n            out[j] = x\n        \n        # This janky second loop is here because TorchScript.\n        j = len(convouts)\n        for pred_layer in self.pred_layers:\n            j -= 1\n            out[j] = pred_layer(out[j])\n\n            if self.relu_pred_layers:\n                F.relu(out[j], inplace=True)\n\n        cur_idx = len(out)\n\n        # In the original paper, this takes care of P6\n        if self.use_conv_downsample:\n            for downsample_layer in self.downsample_layers:\n                out.append(downsample_layer(out[-1]))\n        else:\n            for idx in range(self.num_downsample):\n                # Note: this is an untested alternative to out.append(out[-1][:, :, ::2, ::2]). Thanks TorchScript.\n                out.append(nn.functional.max_pool2d(out[-1], 1, stride=2))\n\n        if self.relu_downsample_layers:\n            for idx in range(len(out) - cur_idx):\n                out[idx] = F.relu(out[idx + cur_idx], inplace=False)\n\n        return out\n\nclass FastMaskIoUNet(ScriptModuleWrapper):\n\n    def __init__(self):\n        super().__init__()\n        input_channels = 1\n        last_layer = [(cfg.num_classes-1, 1, {})]\n        self.maskiou_net, _ = make_net(input_channels, cfg.maskiou_net + last_layer, include_last_relu=True)\n\n    def forward(self, x):\n        x = self.maskiou_net(x)\n        maskiou_p = F.max_pool2d(x, kernel_size=x.size()[2:]).squeeze(-1).squeeze(-1)\n\n        return maskiou_p\n\n\n\nclass Yolact(nn.Module):\n    """"""\n\n\n    \xe2\x96\x88\xe2\x96\x88\xe2\x95\x97   \xe2\x96\x88\xe2\x96\x88\xe2\x95\x97 \xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x95\x97 \xe2\x96\x88\xe2\x96\x88\xe2\x95\x97      \xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x95\x97  \xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x95\x97\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x95\x97\n    \xe2\x95\x9a\xe2\x96\x88\xe2\x96\x88\xe2\x95\x97 \xe2\x96\x88\xe2\x96\x88\xe2\x95\x94\xe2\x95\x9d\xe2\x96\x88\xe2\x96\x88\xe2\x95\x94\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x96\x88\xe2\x96\x88\xe2\x95\x97\xe2\x96\x88\xe2\x96\x88\xe2\x95\x91     \xe2\x96\x88\xe2\x96\x88\xe2\x95\x94\xe2\x95\x90\xe2\x95\x90\xe2\x96\x88\xe2\x96\x88\xe2\x95\x97\xe2\x96\x88\xe2\x96\x88\xe2\x95\x94\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x9d\xe2\x95\x9a\xe2\x95\x90\xe2\x95\x90\xe2\x96\x88\xe2\x96\x88\xe2\x95\x94\xe2\x95\x90\xe2\x95\x90\xe2\x95\x9d\n     \xe2\x95\x9a\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x95\x94\xe2\x95\x9d \xe2\x96\x88\xe2\x96\x88\xe2\x95\x91   \xe2\x96\x88\xe2\x96\x88\xe2\x95\x91\xe2\x96\x88\xe2\x96\x88\xe2\x95\x91     \xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x95\x91\xe2\x96\x88\xe2\x96\x88\xe2\x95\x91        \xe2\x96\x88\xe2\x96\x88\xe2\x95\x91   \n      \xe2\x95\x9a\xe2\x96\x88\xe2\x96\x88\xe2\x95\x94\xe2\x95\x9d  \xe2\x96\x88\xe2\x96\x88\xe2\x95\x91   \xe2\x96\x88\xe2\x96\x88\xe2\x95\x91\xe2\x96\x88\xe2\x96\x88\xe2\x95\x91     \xe2\x96\x88\xe2\x96\x88\xe2\x95\x94\xe2\x95\x90\xe2\x95\x90\xe2\x96\x88\xe2\x96\x88\xe2\x95\x91\xe2\x96\x88\xe2\x96\x88\xe2\x95\x91        \xe2\x96\x88\xe2\x96\x88\xe2\x95\x91   \n       \xe2\x96\x88\xe2\x96\x88\xe2\x95\x91   \xe2\x95\x9a\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x95\x94\xe2\x95\x9d\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x95\x97\xe2\x96\x88\xe2\x96\x88\xe2\x95\x91  \xe2\x96\x88\xe2\x96\x88\xe2\x95\x91\xe2\x95\x9a\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x95\x97   \xe2\x96\x88\xe2\x96\x88\xe2\x95\x91   \n       \xe2\x95\x9a\xe2\x95\x90\xe2\x95\x9d    \xe2\x95\x9a\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x9d \xe2\x95\x9a\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x9d\xe2\x95\x9a\xe2\x95\x90\xe2\x95\x9d  \xe2\x95\x9a\xe2\x95\x90\xe2\x95\x9d \xe2\x95\x9a\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x9d   \xe2\x95\x9a\xe2\x95\x90\xe2\x95\x9d \n\n\n    You can set the arguments by changing them in the backbone config object in config.py.\n\n    Parameters (in cfg.backbone):\n        - selected_layers: The indices of the conv layers to use for prediction.\n        - pred_scales:     A list with len(selected_layers) containing tuples of scales (see PredictionModule)\n        - pred_aspect_ratios: A list of lists of aspect ratios with len(selected_layers) (see PredictionModule)\n    """"""\n\n    def __init__(self):\n        super().__init__()\n\n        self.backbone = construct_backbone(cfg.backbone)\n\n        if cfg.freeze_bn:\n            self.freeze_bn()\n\n        # Compute mask_dim here and add it back to the config. Make sure Yolact\'s constructor is called early!\n        if cfg.mask_type == mask_type.direct:\n            cfg.mask_dim = cfg.mask_size**2\n        elif cfg.mask_type == mask_type.lincomb:\n            if cfg.mask_proto_use_grid:\n                self.grid = torch.Tensor(np.load(cfg.mask_proto_grid_file))\n                self.num_grids = self.grid.size(0)\n            else:\n                self.num_grids = 0\n\n            self.proto_src = cfg.mask_proto_src\n            \n            if self.proto_src is None: in_channels = 3\n            elif cfg.fpn is not None: in_channels = cfg.fpn.num_features\n            else: in_channels = self.backbone.channels[self.proto_src]\n            in_channels += self.num_grids\n\n            # The include_last_relu=false here is because we might want to change it to another function\n            self.proto_net, cfg.mask_dim = make_net(in_channels, cfg.mask_proto_net, include_last_relu=False)\n\n            if cfg.mask_proto_bias:\n                cfg.mask_dim += 1\n\n\n        self.selected_layers = cfg.backbone.selected_layers\n        src_channels = self.backbone.channels\n\n        if cfg.use_maskiou:\n            self.maskiou_net = FastMaskIoUNet()\n\n        if cfg.fpn is not None:\n            # Some hacky rewiring to accomodate the FPN\n            self.fpn = FPN([src_channels[i] for i in self.selected_layers])\n            self.selected_layers = list(range(len(self.selected_layers) + cfg.fpn.num_downsample))\n            src_channels = [cfg.fpn.num_features] * len(self.selected_layers)\n\n\n        self.prediction_layers = nn.ModuleList()\n        cfg.num_heads = len(self.selected_layers)\n\n        for idx, layer_idx in enumerate(self.selected_layers):\n            # If we\'re sharing prediction module weights, have every module\'s parent be the first one\n            parent = None\n            if cfg.share_prediction_module and idx > 0:\n                parent = self.prediction_layers[0]\n\n            pred = PredictionModule(src_channels[layer_idx], src_channels[layer_idx],\n                                    aspect_ratios = cfg.backbone.pred_aspect_ratios[idx],\n                                    scales        = cfg.backbone.pred_scales[idx],\n                                    parent        = parent,\n                                    index         = idx)\n            self.prediction_layers.append(pred)\n\n        # Extra parameters for the extra losses\n        if cfg.use_class_existence_loss:\n            # This comes from the smallest layer selected\n            # Also note that cfg.num_classes includes background\n            self.class_existence_fc = nn.Linear(src_channels[-1], cfg.num_classes - 1)\n        \n        if cfg.use_semantic_segmentation_loss:\n            self.semantic_seg_conv = nn.Conv2d(src_channels[0], cfg.num_classes-1, kernel_size=1)\n\n        # For use in evaluation\n        self.detect = Detect(cfg.num_classes, bkg_label=0, top_k=cfg.nms_top_k,\n            conf_thresh=cfg.nms_conf_thresh, nms_thresh=cfg.nms_thresh)\n\n    def save_weights(self, path):\n        """""" Saves the model\'s weights using compression because the file sizes were getting too big. """"""\n        torch.save(self.state_dict(), path)\n    \n    def load_weights(self, path):\n        """""" Loads weights from a compressed save file. """"""\n        state_dict = torch.load(path)\n\n        # For backward compatability, remove these (the new variable is called layers)\n        for key in list(state_dict.keys()):\n            if key.startswith(\'backbone.layer\') and not key.startswith(\'backbone.layers\'):\n                del state_dict[key]\n        \n            # Also for backward compatibility with v1.0 weights, do this check\n            if key.startswith(\'fpn.downsample_layers.\'):\n                if cfg.fpn is not None and int(key.split(\'.\')[2]) >= cfg.fpn.num_downsample:\n                    del state_dict[key]\n        self.load_state_dict(state_dict)\n\n    def init_weights(self, backbone_path):\n        """""" Initialize weights for training. """"""\n        # Initialize the backbone with the pretrained weights.\n        self.backbone.init_backbone(backbone_path)\n\n        conv_constants = getattr(nn.Conv2d(1, 1, 1), \'__constants__\')\n        \n        # Quick lambda to test if one list contains the other\n        def all_in(x, y):\n            for _x in x:\n                if _x not in y:\n                    return False\n            return True\n\n        # Initialize the rest of the conv layers with xavier\n        for name, module in self.named_modules():\n            # See issue #127 for why we need such a complicated condition if the module is a WeakScriptModuleProxy\n            # Broke in 1.3 (see issue #175), WeakScriptModuleProxy was turned into just ScriptModule.\n            # Broke in 1.4 (see issue #292), where RecursiveScriptModule is the new star of the show.\n            # Note that this might break with future pytorch updates, so let me know if it does\n            is_script_conv = False\n            if \'Script\' in type(module).__name__:\n                # 1.4 workaround: now there\'s an original_name member so just use that\n                if hasattr(module, \'original_name\'):\n                    is_script_conv = \'Conv\' in module.original_name\n                # 1.3 workaround: check if this has the same constants as a conv module\n                else:\n                    is_script_conv = (\n                        all_in(module.__dict__[\'_constants_set\'], conv_constants)\n                        and all_in(conv_constants, module.__dict__[\'_constants_set\']))\n            \n            is_conv_layer = isinstance(module, nn.Conv2d) or is_script_conv\n\n            if is_conv_layer and module not in self.backbone.backbone_modules:\n                nn.init.xavier_uniform_(module.weight.data)\n\n                if module.bias is not None:\n                    if cfg.use_focal_loss and \'conf_layer\' in name:\n                        if not cfg.use_sigmoid_focal_loss:\n                            # Initialize the last layer as in the focal loss paper.\n                            # Because we use softmax and not sigmoid, I had to derive an alternate expression\n                            # on a notecard. Define pi to be the probability of outputting a foreground detection.\n                            # Then let z = sum(exp(x)) - exp(x_0). Finally let c be the number of foreground classes.\n                            # Chugging through the math, this gives us\n                            #   x_0 = log(z * (1 - pi) / pi)    where 0 is the background class\n                            #   x_i = log(z / c)                for all i > 0\n                            # For simplicity (and because we have a degree of freedom here), set z = 1. Then we have\n                            #   x_0 =  log((1 - pi) / pi)       note: don\'t split up the log for numerical stability\n                            #   x_i = -log(c)                   for all i > 0\n                            module.bias.data[0]  = np.log((1 - cfg.focal_loss_init_pi) / cfg.focal_loss_init_pi)\n                            module.bias.data[1:] = -np.log(module.bias.size(0) - 1)\n                        else:\n                            module.bias.data[0]  = -np.log(cfg.focal_loss_init_pi / (1 - cfg.focal_loss_init_pi))\n                            module.bias.data[1:] = -np.log((1 - cfg.focal_loss_init_pi) / cfg.focal_loss_init_pi)\n                    else:\n                        module.bias.data.zero_()\n    \n    def train(self, mode=True):\n        super().train(mode)\n\n        if cfg.freeze_bn:\n            self.freeze_bn()\n\n    def freeze_bn(self, enable=False):\n        """""" Adapted from https://discuss.pytorch.org/t/how-to-train-with-frozen-batchnorm/12106/8 """"""\n        for module in self.modules():\n            if isinstance(module, nn.BatchNorm2d):\n                module.train() if enable else module.eval()\n\n                module.weight.requires_grad = enable\n                module.bias.requires_grad = enable\n    \n    def forward(self, x):\n        """""" The input should be of size [batch_size, 3, img_h, img_w] """"""\n        _, _, img_h, img_w = x.size()\n        cfg._tmp_img_h = img_h\n        cfg._tmp_img_w = img_w\n        \n        with timer.env(\'backbone\'):\n            outs = self.backbone(x)\n\n        if cfg.fpn is not None:\n            with timer.env(\'fpn\'):\n                # Use backbone.selected_layers because we overwrote self.selected_layers\n                outs = [outs[i] for i in cfg.backbone.selected_layers]\n                outs = self.fpn(outs)\n\n        proto_out = None\n        if cfg.mask_type == mask_type.lincomb and cfg.eval_mask_branch:\n            with timer.env(\'proto\'):\n                proto_x = x if self.proto_src is None else outs[self.proto_src]\n                \n                if self.num_grids > 0:\n                    grids = self.grid.repeat(proto_x.size(0), 1, 1, 1)\n                    proto_x = torch.cat([proto_x, grids], dim=1)\n\n                proto_out = self.proto_net(proto_x)\n                proto_out = cfg.mask_proto_prototype_activation(proto_out)\n\n                if cfg.mask_proto_prototypes_as_features:\n                    # Clone here because we don\'t want to permute this, though idk if contiguous makes this unnecessary\n                    proto_downsampled = proto_out.clone()\n\n                    if cfg.mask_proto_prototypes_as_features_no_grad:\n                        proto_downsampled = proto_out.detach()\n                \n                # Move the features last so the multiplication is easy\n                proto_out = proto_out.permute(0, 2, 3, 1).contiguous()\n\n                if cfg.mask_proto_bias:\n                    bias_shape = [x for x in proto_out.size()]\n                    bias_shape[-1] = 1\n                    proto_out = torch.cat([proto_out, torch.ones(*bias_shape)], -1)\n\n\n        with timer.env(\'pred_heads\'):\n            pred_outs = { \'loc\': [], \'conf\': [], \'mask\': [], \'priors\': [] }\n\n            if cfg.use_mask_scoring:\n                pred_outs[\'score\'] = []\n\n            if cfg.use_instance_coeff:\n                pred_outs[\'inst\'] = []\n            \n            for idx, pred_layer in zip(self.selected_layers, self.prediction_layers):\n                pred_x = outs[idx]\n\n                if cfg.mask_type == mask_type.lincomb and cfg.mask_proto_prototypes_as_features:\n                    # Scale the prototypes down to the current prediction layer\'s size and add it as inputs\n                    proto_downsampled = F.interpolate(proto_downsampled, size=outs[idx].size()[2:], mode=\'bilinear\', align_corners=False)\n                    pred_x = torch.cat([pred_x, proto_downsampled], dim=1)\n\n                # A hack for the way dataparallel works\n                if cfg.share_prediction_module and pred_layer is not self.prediction_layers[0]:\n                    pred_layer.parent = [self.prediction_layers[0]]\n\n                p = pred_layer(pred_x)\n                \n                for k, v in p.items():\n                    pred_outs[k].append(v)\n\n        for k, v in pred_outs.items():\n            pred_outs[k] = torch.cat(v, -2)\n\n        if proto_out is not None:\n            pred_outs[\'proto\'] = proto_out\n\n        if self.training:\n            # For the extra loss functions\n            if cfg.use_class_existence_loss:\n                pred_outs[\'classes\'] = self.class_existence_fc(outs[-1].mean(dim=(2, 3)))\n\n            if cfg.use_semantic_segmentation_loss:\n                pred_outs[\'segm\'] = self.semantic_seg_conv(outs[0])\n\n            return pred_outs\n        else:\n            if cfg.use_mask_scoring:\n                pred_outs[\'score\'] = torch.sigmoid(pred_outs[\'score\'])\n\n            if cfg.use_focal_loss:\n                if cfg.use_sigmoid_focal_loss:\n                    # Note: even though conf[0] exists, this mode doesn\'t train it so don\'t use it\n                    pred_outs[\'conf\'] = torch.sigmoid(pred_outs[\'conf\'])\n                    if cfg.use_mask_scoring:\n                        pred_outs[\'conf\'] *= pred_outs[\'score\']\n                elif cfg.use_objectness_score:\n                    # See focal_loss_sigmoid in multibox_loss.py for details\n                    objectness = torch.sigmoid(pred_outs[\'conf\'][:, :, 0])\n                    pred_outs[\'conf\'][:, :, 1:] = objectness[:, :, None] * F.softmax(pred_outs[\'conf\'][:, :, 1:], -1)\n                    pred_outs[\'conf\'][:, :, 0 ] = 1 - objectness\n                else:\n                    pred_outs[\'conf\'] = F.softmax(pred_outs[\'conf\'], -1)\n            else:\n\n                if cfg.use_objectness_score:\n                    objectness = torch.sigmoid(pred_outs[\'conf\'][:, :, 0])\n                    \n                    pred_outs[\'conf\'][:, :, 1:] = (objectness > 0.10)[..., None] \\\n                        * F.softmax(pred_outs[\'conf\'][:, :, 1:], dim=-1)\n                    \n                else:\n                    pred_outs[\'conf\'] = F.softmax(pred_outs[\'conf\'], -1)\n\n            return self.detect(pred_outs, self)\n\n\n\n\n# Some testing code\nif __name__ == \'__main__\':\n    from utils.functions import init_console\n    init_console()\n\n    # Use the first argument to set the config if you want\n    import sys\n    if len(sys.argv) > 1:\n        from data.config import set_cfg\n        set_cfg(sys.argv[1])\n\n    net = Yolact()\n    net.train()\n    net.init_weights(backbone_path=\'weights/\' + cfg.backbone.path)\n\n    # GPU\n    net = net.cuda()\n    torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n\n    x = torch.zeros((1, 3, cfg.max_size, cfg.max_size))\n    y = net(x)\n\n    for p in net.prediction_layers:\n        print(p.last_conv_size)\n\n    print()\n    for k, a in y.items():\n        print(k + \': \', a.size(), torch.sum(a))\n    exit()\n    \n    net(x)\n    # timer.disable(\'pass2\')\n    avg = MovingAverage()\n    try:\n        while True:\n            timer.reset()\n            with timer.env(\'everything else\'):\n                net(x)\n            avg.add(timer.total_time())\n            print(\'\\033[2J\') # Moves console cursor to 0,0\n            timer.print_stats()\n            print(\'Avg fps: %.2f\\tAvg ms: %.2f         \' % (1/avg.get_avg(), avg.get_avg()*1000))\n    except KeyboardInterrupt:\n        pass\n'"
data/__init__.py,0,b'from .config import *\nfrom .coco import *\n\nimport torch\nimport cv2\nimport numpy as np\n'
data/coco.py,6,"b'import os\nimport os.path as osp\nimport sys\nimport torch\nimport torch.utils.data as data\nimport torch.nn.functional as F\nimport cv2\nimport numpy as np\nfrom .config import cfg\nfrom pycocotools import mask as maskUtils\nimport random\n\ndef get_label_map():\n    if cfg.dataset.label_map is None:\n        return {x+1: x+1 for x in range(len(cfg.dataset.class_names))}\n    else:\n        return cfg.dataset.label_map \n\nclass COCOAnnotationTransform(object):\n    """"""Transforms a COCO annotation into a Tensor of bbox coords and label index\n    Initilized with a dictionary lookup of classnames to indexes\n    """"""\n    def __init__(self):\n        self.label_map = get_label_map()\n\n    def __call__(self, target, width, height):\n        """"""\n        Args:\n            target (dict): COCO target json annotation as a python dict\n            height (int): height\n            width (int): width\n        Returns:\n            a list containing lists of bounding boxes  [bbox coords, class idx]\n        """"""\n        scale = np.array([width, height, width, height])\n        res = []\n        for obj in target:\n            if \'bbox\' in obj:\n                bbox = obj[\'bbox\']\n                label_idx = obj[\'category_id\']\n                if label_idx >= 0:\n                    label_idx = self.label_map[label_idx] - 1\n                final_box = list(np.array([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]])/scale)\n                final_box.append(label_idx)\n                res += [final_box]  # [xmin, ymin, xmax, ymax, label_idx]\n            else:\n                print(""No bbox found for object "", obj)\n\n        return res\n\n\nclass COCODetection(data.Dataset):\n    """"""`MS Coco Detection <http://mscoco.org/dataset/#detections-challenge2016>`_ Dataset.\n    Args:\n        root (string): Root directory where images are downloaded to.\n        set_name (string): Name of the specific set of COCO images.\n        transform (callable, optional): A function/transform that augments the\n                                        raw images`\n        target_transform (callable, optional): A function/transform that takes\n        in the target (bbox) and transforms it.\n        prep_crowds (bool): Whether or not to prepare crowds for the evaluation step.\n    """"""\n\n    def __init__(self, image_path, info_file, transform=None,\n                 target_transform=None,\n                 dataset_name=\'MS COCO\', has_gt=True):\n        # Do this here because we have too many things named COCO\n        from pycocotools.coco import COCO\n        \n        if target_transform is None:\n            target_transform = COCOAnnotationTransform()\n\n        self.root = image_path\n        self.coco = COCO(info_file)\n        \n        self.ids = list(self.coco.imgToAnns.keys())\n        if len(self.ids) == 0 or not has_gt:\n            self.ids = list(self.coco.imgs.keys())\n        \n        self.transform = transform\n        self.target_transform = COCOAnnotationTransform()\n        \n        self.name = dataset_name\n        self.has_gt = has_gt\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: Tuple (image, (target, masks, num_crowds)).\n                   target is the object returned by ``coco.loadAnns``.\n        """"""\n        im, gt, masks, h, w, num_crowds = self.pull_item(index)\n        return im, (gt, masks, num_crowds)\n\n    def __len__(self):\n        return len(self.ids)\n\n    def pull_item(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: Tuple (image, target, masks, height, width, crowd).\n                   target is the object returned by ``coco.loadAnns``.\n            Note that if no crowd annotations exist, crowd will be None\n        """"""\n        img_id = self.ids[index]\n\n        if self.has_gt:\n            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n\n            # Target has {\'segmentation\', \'area\', iscrowd\', \'image_id\', \'bbox\', \'category_id\'}\n            target = [x for x in self.coco.loadAnns(ann_ids) if x[\'image_id\'] == img_id]\n        else:\n            target = []\n\n        # Separate out crowd annotations. These are annotations that signify a large crowd of\n        # objects of said class, where there is no annotation for each individual object. Both\n        # during testing and training, consider these crowds as neutral.\n        crowd  = [x for x in target if     (\'iscrowd\' in x and x[\'iscrowd\'])]\n        target = [x for x in target if not (\'iscrowd\' in x and x[\'iscrowd\'])]\n        num_crowds = len(crowd)\n\n        for x in crowd:\n            x[\'category_id\'] = -1\n\n        # This is so we ensure that all crowd annotations are at the end of the array\n        target += crowd\n        \n        # The split here is to have compatibility with both COCO2014 and 2017 annotations.\n        # In 2014, images have the pattern COCO_{train/val}2014_%012d.jpg, while in 2017 it\'s %012d.jpg.\n        # Our script downloads the images as %012d.jpg so convert accordingly.\n        file_name = self.coco.loadImgs(img_id)[0][\'file_name\']\n        \n        if file_name.startswith(\'COCO\'):\n            file_name = file_name.split(\'_\')[-1]\n\n        path = osp.join(self.root, file_name)\n        assert osp.exists(path), \'Image path does not exist: {}\'.format(path)\n        \n        img = cv2.imread(path)\n        height, width, _ = img.shape\n        \n        if len(target) > 0:\n            # Pool all the masks for this image into one [num_objects,height,width] matrix\n            masks = [self.coco.annToMask(obj).reshape(-1) for obj in target]\n            masks = np.vstack(masks)\n            masks = masks.reshape(-1, height, width)\n\n        if self.target_transform is not None and len(target) > 0:\n            target = self.target_transform(target, width, height)\n\n        if self.transform is not None:\n            if len(target) > 0:\n                target = np.array(target)\n                img, masks, boxes, labels = self.transform(img, masks, target[:, :4],\n                    {\'num_crowds\': num_crowds, \'labels\': target[:, 4]})\n            \n                # I stored num_crowds in labels so I didn\'t have to modify the entirety of augmentations\n                num_crowds = labels[\'num_crowds\']\n                labels     = labels[\'labels\']\n                \n                target = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n            else:\n                img, _, _, _ = self.transform(img, np.zeros((1, height, width), dtype=np.float), np.array([[0, 0, 1, 1]]),\n                    {\'num_crowds\': 0, \'labels\': np.array([0])})\n                masks = None\n                target = None\n\n        if target.shape[0] == 0:\n            print(\'Warning: Augmentation output an example with no ground truth. Resampling...\')\n            return self.pull_item(random.randint(0, len(self.ids)-1))\n\n        return torch.from_numpy(img).permute(2, 0, 1), target, masks, height, width, num_crowds\n\n    def pull_image(self, index):\n        \'\'\'Returns the original image object at index in PIL form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            cv2 img\n        \'\'\'\n        img_id = self.ids[index]\n        path = self.coco.loadImgs(img_id)[0][\'file_name\']\n        return cv2.imread(osp.join(self.root, path), cv2.IMREAD_COLOR)\n\n    def pull_anno(self, index):\n        \'\'\'Returns the original annotation of image at index\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to get annotation of\n        Return:\n            list:  [img_id, [(label, bbox coords),...]]\n                eg: (\'001718\', [(\'dog\', (96, 13, 438, 332))])\n        \'\'\'\n        img_id = self.ids[index]\n        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n        return self.coco.loadAnns(ann_ids)\n\n    def __repr__(self):\n        fmt_str = \'Dataset \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Number of datapoints: {}\\n\'.format(self.__len__())\n        fmt_str += \'    Root Location: {}\\n\'.format(self.root)\n        tmp = \'    Transforms (if any): \'\n        fmt_str += \'{0}{1}\\n\'.format(tmp, self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        tmp = \'    Target Transforms (if any): \'\n        fmt_str += \'{0}{1}\'.format(tmp, self.target_transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str\n\ndef enforce_size(img, targets, masks, num_crowds, new_w, new_h):\n    """""" Ensures that the image is the given size without distorting aspect ratio. """"""\n    with torch.no_grad():\n        _, h, w = img.size()\n\n        if h == new_h and w == new_w:\n            return img, targets, masks, num_crowds\n        \n        # Resize the image so that it fits within new_w, new_h\n        w_prime = new_w\n        h_prime = h * new_w / w\n\n        if h_prime > new_h:\n            w_prime *= new_h / h_prime\n            h_prime = new_h\n\n        w_prime = int(w_prime)\n        h_prime = int(h_prime)\n\n        # Do all the resizing\n        img = F.interpolate(img.unsqueeze(0), (h_prime, w_prime), mode=\'bilinear\', align_corners=False)\n        img.squeeze_(0)\n\n        # Act like each object is a color channel\n        masks = F.interpolate(masks.unsqueeze(0), (h_prime, w_prime), mode=\'bilinear\', align_corners=False)\n        masks.squeeze_(0)\n\n        # Scale bounding boxes (this will put them in the top left corner in the case of padding)\n        targets[:, [0, 2]] *= (w_prime / new_w)\n        targets[:, [1, 3]] *= (h_prime / new_h)\n\n        # Finally, pad everything to be the new_w, new_h\n        pad_dims = (0, new_w - w_prime, 0, new_h - h_prime)\n        img   = F.pad(  img, pad_dims, mode=\'constant\', value=0)\n        masks = F.pad(masks, pad_dims, mode=\'constant\', value=0)\n\n        return img, targets, masks, num_crowds\n        \n\n\n\ndef detection_collate(batch):\n    """"""Custom collate fn for dealing with batches of images that have a different\n    number of associated object annotations (bounding boxes).\n\n    Arguments:\n        batch: (tuple) A tuple of tensor images and (lists of annotations, masks)\n\n    Return:\n        A tuple containing:\n            1) (tensor) batch of images stacked on their 0 dim\n            2) (list<tensor>, list<tensor>, list<int>) annotations for a given image are stacked\n                on 0 dim. The output gt is a tuple of annotations and masks.\n    """"""\n    targets = []\n    imgs = []\n    masks = []\n    num_crowds = []\n\n    for sample in batch:\n        imgs.append(sample[0])\n        targets.append(torch.FloatTensor(sample[1][0]))\n        masks.append(torch.FloatTensor(sample[1][1]))\n        num_crowds.append(sample[1][2])\n\n    return imgs, (targets, masks, num_crowds)\n'"
data/config.py,4,"b'from backbone import ResNetBackbone, VGGBackbone, ResNetBackboneGN, DarkNetBackbone\nfrom math import sqrt\nimport torch\n\n# for making bounding boxes pretty\nCOLORS = ((244,  67,  54),\n          (233,  30,  99),\n          (156,  39, 176),\n          (103,  58, 183),\n          ( 63,  81, 181),\n          ( 33, 150, 243),\n          (  3, 169, 244),\n          (  0, 188, 212),\n          (  0, 150, 136),\n          ( 76, 175,  80),\n          (139, 195,  74),\n          (205, 220,  57),\n          (255, 235,  59),\n          (255, 193,   7),\n          (255, 152,   0),\n          (255,  87,  34),\n          (121,  85,  72),\n          (158, 158, 158),\n          ( 96, 125, 139))\n\n\n# These are in BGR and are for ImageNet\nMEANS = (103.94, 116.78, 123.68)\nSTD   = (57.38, 57.12, 58.40)\n\nCOCO_CLASSES = (\'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\',\n                \'train\', \'truck\', \'boat\', \'traffic light\', \'fire hydrant\',\n                \'stop sign\', \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\',\n                \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\',\n                \'backpack\', \'umbrella\', \'handbag\', \'tie\', \'suitcase\', \'frisbee\',\n                \'skis\', \'snowboard\', \'sports ball\', \'kite\', \'baseball bat\',\n                \'baseball glove\', \'skateboard\', \'surfboard\', \'tennis racket\',\n                \'bottle\', \'wine glass\', \'cup\', \'fork\', \'knife\', \'spoon\', \'bowl\',\n                \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\',\n                \'hot dog\', \'pizza\', \'donut\', \'cake\', \'chair\', \'couch\',\n                \'potted plant\', \'bed\', \'dining table\', \'toilet\', \'tv\', \'laptop\',\n                \'mouse\', \'remote\', \'keyboard\', \'cell phone\', \'microwave\', \'oven\',\n                \'toaster\', \'sink\', \'refrigerator\', \'book\', \'clock\', \'vase\',\n                \'scissors\', \'teddy bear\', \'hair drier\', \'toothbrush\')\n\nCOCO_LABEL_MAP = { 1:  1,  2:  2,  3:  3,  4:  4,  5:  5,  6:  6,  7:  7,  8:  8,\n                   9:  9, 10: 10, 11: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16,\n                  18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24,\n                  27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30, 35: 31, 36: 32,\n                  37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40,\n                  46: 41, 47: 42, 48: 43, 49: 44, 50: 45, 51: 46, 52: 47, 53: 48,\n                  54: 49, 55: 50, 56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56,\n                  62: 57, 63: 58, 64: 59, 65: 60, 67: 61, 70: 62, 72: 63, 73: 64,\n                  74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70, 80: 71, 81: 72,\n                  82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80}\n\n\n\n# ----------------------- CONFIG CLASS ----------------------- #\n\nclass Config(object):\n    """"""\n    Holds the configuration for anything you want it to.\n    To get the currently active config, call get_cfg().\n\n    To use, just do cfg.x instead of cfg[\'x\'].\n    I made this because doing cfg[\'x\'] all the time is dumb.\n    """"""\n\n    def __init__(self, config_dict):\n        for key, val in config_dict.items():\n            self.__setattr__(key, val)\n\n    def copy(self, new_config_dict={}):\n        """"""\n        Copies this config into a new config object, making\n        the changes given by new_config_dict.\n        """"""\n\n        ret = Config(vars(self))\n        \n        for key, val in new_config_dict.items():\n            ret.__setattr__(key, val)\n\n        return ret\n\n    def replace(self, new_config_dict):\n        """"""\n        Copies new_config_dict into this config object.\n        Note: new_config_dict can also be a config object.\n        """"""\n        if isinstance(new_config_dict, Config):\n            new_config_dict = vars(new_config_dict)\n\n        for key, val in new_config_dict.items():\n            self.__setattr__(key, val)\n    \n    def print(self):\n        for k, v in vars(self).items():\n            print(k, \' = \', v)\n\n\n\n\n\n# ----------------------- DATASETS ----------------------- #\n\ndataset_base = Config({\n    \'name\': \'Base Dataset\',\n\n    # Training images and annotations\n    \'train_images\': \'./data/coco/images/\',\n    \'train_info\':   \'path_to_annotation_file\',\n\n    # Validation images and annotations.\n    \'valid_images\': \'./data/coco/images/\',\n    \'valid_info\':   \'path_to_annotation_file\',\n\n    # Whether or not to load GT. If this is False, eval.py quantitative evaluation won\'t work.\n    \'has_gt\': True,\n\n    # A list of names for each of you classes.\n    \'class_names\': COCO_CLASSES,\n\n    # COCO class ids aren\'t sequential, so this is a bandage fix. If your ids aren\'t sequential,\n    # provide a map from category_id -> index in class_names + 1 (the +1 is there because it\'s 1-indexed).\n    # If not specified, this just assumes category ids start at 1 and increase sequentially.\n    \'label_map\': None\n})\n\ncoco2014_dataset = dataset_base.copy({\n    \'name\': \'COCO 2014\',\n    \n    \'train_info\': \'./data/coco/annotations/instances_train2014.json\',\n    \'valid_info\': \'./data/coco/annotations/instances_val2014.json\',\n\n    \'label_map\': COCO_LABEL_MAP\n})\n\ncoco2017_dataset = dataset_base.copy({\n    \'name\': \'COCO 2017\',\n    \n    \'train_info\': \'./data/coco/annotations/instances_train2017.json\',\n    \'valid_info\': \'./data/coco/annotations/instances_val2017.json\',\n\n    \'label_map\': COCO_LABEL_MAP\n})\n\ncoco2017_testdev_dataset = dataset_base.copy({\n    \'name\': \'COCO 2017 Test-Dev\',\n\n    \'valid_info\': \'./data/coco/annotations/image_info_test-dev2017.json\',\n    \'has_gt\': False,\n\n    \'label_map\': COCO_LABEL_MAP\n})\n\nPASCAL_CLASSES = (""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"",\n                  ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"",\n                  ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"",\n                  ""sheep"", ""sofa"", ""train"", ""tvmonitor"")\n\npascal_sbd_dataset = dataset_base.copy({\n    \'name\': \'Pascal SBD 2012\',\n\n    \'train_images\': \'./data/sbd/img\',\n    \'valid_images\': \'./data/sbd/img\',\n    \n    \'train_info\': \'./data/sbd/pascal_sbd_train.json\',\n    \'valid_info\': \'./data/sbd/pascal_sbd_val.json\',\n\n    \'class_names\': PASCAL_CLASSES,\n})\n\n\n\n\n\n# ----------------------- TRANSFORMS ----------------------- #\n\nresnet_transform = Config({\n    \'channel_order\': \'RGB\',\n    \'normalize\': True,\n    \'subtract_means\': False,\n    \'to_float\': False,\n})\n\nvgg_transform = Config({\n    # Note that though vgg is traditionally BGR,\n    # the channel order of vgg_reducedfc.pth is RGB.\n    \'channel_order\': \'RGB\',\n    \'normalize\': False,\n    \'subtract_means\': True,\n    \'to_float\': False,\n})\n\ndarknet_transform = Config({\n    \'channel_order\': \'RGB\',\n    \'normalize\': False,\n    \'subtract_means\': False,\n    \'to_float\': True,\n})\n\n\n\n\n\n# ----------------------- BACKBONES ----------------------- #\n\nbackbone_base = Config({\n    \'name\': \'Base Backbone\',\n    \'path\': \'path/to/pretrained/weights\',\n    \'type\': object,\n    \'args\': tuple(),\n    \'transform\': resnet_transform,\n\n    \'selected_layers\': list(),\n    \'pred_scales\': list(),\n    \'pred_aspect_ratios\': list(),\n\n    \'use_pixel_scales\': False,\n    \'preapply_sqrt\': True,\n    \'use_square_anchors\': False,\n})\n\nresnet101_backbone = backbone_base.copy({\n    \'name\': \'ResNet101\',\n    \'path\': \'resnet101_reducedfc.pth\',\n    \'type\': ResNetBackbone,\n    \'args\': ([3, 4, 23, 3],),\n    \'transform\': resnet_transform,\n\n    \'selected_layers\': list(range(2, 8)),\n    \'pred_scales\': [[1]]*6,\n    \'pred_aspect_ratios\': [ [[0.66685089, 1.7073535, 0.87508774, 1.16524493, 0.49059086]] ] * 6,\n})\n\nresnet101_gn_backbone = backbone_base.copy({\n    \'name\': \'ResNet101_GN\',\n    \'path\': \'R-101-GN.pkl\',\n    \'type\': ResNetBackboneGN,\n    \'args\': ([3, 4, 23, 3],),\n    \'transform\': resnet_transform,\n\n    \'selected_layers\': list(range(2, 8)),\n    \'pred_scales\': [[1]]*6,\n    \'pred_aspect_ratios\': [ [[0.66685089, 1.7073535, 0.87508774, 1.16524493, 0.49059086]] ] * 6,\n})\n\nresnet101_dcn_inter3_backbone = resnet101_backbone.copy({\n    \'name\': \'ResNet101_DCN_Interval3\',\n    \'args\': ([3, 4, 23, 3], [0, 4, 23, 3], 3),\n})\n\nresnet50_backbone = resnet101_backbone.copy({\n    \'name\': \'ResNet50\',\n    \'path\': \'resnet50-19c8e357.pth\',\n    \'type\': ResNetBackbone,\n    \'args\': ([3, 4, 6, 3],),\n    \'transform\': resnet_transform,\n})\n\nresnet50_dcnv2_backbone = resnet50_backbone.copy({\n    \'name\': \'ResNet50_DCNv2\',\n    \'args\': ([3, 4, 6, 3], [0, 4, 6, 3]),\n})\n\ndarknet53_backbone = backbone_base.copy({\n    \'name\': \'DarkNet53\',\n    \'path\': \'darknet53.pth\',\n    \'type\': DarkNetBackbone,\n    \'args\': ([1, 2, 8, 8, 4],),\n    \'transform\': darknet_transform,\n\n    \'selected_layers\': list(range(3, 9)),\n    \'pred_scales\': [[3.5, 4.95], [3.6, 4.90], [3.3, 4.02], [2.7, 3.10], [2.1, 2.37], [1.8, 1.92]],\n    \'pred_aspect_ratios\': [ [[1, sqrt(2), 1/sqrt(2), sqrt(3), 1/sqrt(3)][:n], [1]] for n in [3, 5, 5, 5, 3, 3] ],\n})\n\nvgg16_arch = [[64, 64],\n              [ \'M\', 128, 128],\n              [ \'M\', 256, 256, 256],\n              [(\'M\', {\'kernel_size\': 2, \'stride\': 2, \'ceil_mode\': True}), 512, 512, 512],\n              [ \'M\', 512, 512, 512],\n              [(\'M\',  {\'kernel_size\': 3, \'stride\':  1, \'padding\':  1}),\n               (1024, {\'kernel_size\': 3, \'padding\': 6, \'dilation\': 6}),\n               (1024, {\'kernel_size\': 1})]]\n\nvgg16_backbone = backbone_base.copy({\n    \'name\': \'VGG16\',\n    \'path\': \'vgg16_reducedfc.pth\',\n    \'type\': VGGBackbone,\n    \'args\': (vgg16_arch, [(256, 2), (128, 2), (128, 1), (128, 1)], [3]),\n    \'transform\': vgg_transform,\n\n    \'selected_layers\': [3] + list(range(5, 10)),\n    \'pred_scales\': [[5, 4]]*6,\n    \'pred_aspect_ratios\': [ [[1], [1, sqrt(2), 1/sqrt(2), sqrt(3), 1/sqrt(3)][:n]] for n in [3, 5, 5, 5, 3, 3] ],\n})\n\n\n\n\n\n# ----------------------- MASK BRANCH TYPES ----------------------- #\n\nmask_type = Config({\n    # Direct produces masks directly as the output of each pred module.\n    # This is denoted as fc-mask in the paper.\n    # Parameters: mask_size, use_gt_bboxes\n    \'direct\': 0,\n\n    # Lincomb produces coefficients as the output of each pred module then uses those coefficients\n    # to linearly combine features from a prototype network to create image-sized masks.\n    # Parameters:\n    #   - masks_to_train (int): Since we\'re producing (near) full image masks, it\'d take too much\n    #                           vram to backprop on every single mask. Thus we select only a subset.\n    #   - mask_proto_src (int): The input layer to the mask prototype generation network. This is an\n    #                           index in backbone.layers. Use to use the image itself instead.\n    #   - mask_proto_net (list<tuple>): A list of layers in the mask proto network with the last one\n    #                                   being where the masks are taken from. Each conv layer is in\n    #                                   the form (num_features, kernel_size, **kwdargs). An empty\n    #                                   list means to use the source for prototype masks. If the\n    #                                   kernel_size is negative, this creates a deconv layer instead.\n    #                                   If the kernel_size is negative and the num_features is None,\n    #                                   this creates a simple bilinear interpolation layer instead.\n    #   - mask_proto_bias (bool): Whether to include an extra coefficient that corresponds to a proto\n    #                             mask of all ones.\n    #   - mask_proto_prototype_activation (func): The activation to apply to each prototype mask.\n    #   - mask_proto_mask_activation (func): After summing the prototype masks with the predicted\n    #                                        coeffs, what activation to apply to the final mask.\n    #   - mask_proto_coeff_activation (func): The activation to apply to the mask coefficients.\n    #   - mask_proto_crop (bool): If True, crop the mask with the predicted bbox during training.\n    #   - mask_proto_crop_expand (float): If cropping, the percent to expand the cropping bbox by\n    #                                     in each direction. This is to make the model less reliant\n    #                                     on perfect bbox predictions.\n    #   - mask_proto_loss (str [l1|disj]): If not None, apply an l1 or disjunctive regularization\n    #                                      loss directly to the prototype masks.\n    #   - mask_proto_binarize_downsampled_gt (bool): Binarize GT after dowsnampling during training?\n    #   - mask_proto_normalize_mask_loss_by_sqrt_area (bool): Whether to normalize mask loss by sqrt(sum(gt))\n    #   - mask_proto_reweight_mask_loss (bool): Reweight mask loss such that background is divided by\n    #                                           #background and foreground is divided by #foreground.\n    #   - mask_proto_grid_file (str): The path to the grid file to use with the next option.\n    #                                 This should be a numpy.dump file with shape [numgrids, h, w]\n    #                                 where h and w are w.r.t. the mask_proto_src convout.\n    #   - mask_proto_use_grid (bool): Whether to add extra grid features to the proto_net input.\n    #   - mask_proto_coeff_gate (bool): Add an extra set of sigmoided coefficients that is multiplied\n    #                                   into the predicted coefficients in order to ""gate"" them.\n    #   - mask_proto_prototypes_as_features (bool): For each prediction module, downsample the prototypes\n    #                                 to the convout size of that module and supply the prototypes as input\n    #                                 in addition to the already supplied backbone features.\n    #   - mask_proto_prototypes_as_features_no_grad (bool): If the above is set, don\'t backprop gradients to\n    #                                 to the prototypes from the network head.\n    #   - mask_proto_remove_empty_masks (bool): Remove masks that are downsampled to 0 during loss calculations.\n    #   - mask_proto_reweight_coeff (float): The coefficient to multiple the forground pixels with if reweighting.\n    #   - mask_proto_coeff_diversity_loss (bool): Apply coefficient diversity loss on the coefficients so that the same\n    #                                             instance has similar coefficients.\n    #   - mask_proto_coeff_diversity_alpha (float): The weight to use for the coefficient diversity loss.\n    #   - mask_proto_normalize_emulate_roi_pooling (bool): Normalize the mask loss to emulate roi pooling\'s affect on loss.\n    #   - mask_proto_double_loss (bool): Whether to use the old loss in addition to any special new losses.\n    #   - mask_proto_double_loss_alpha (float): The alpha to weight the above loss.\n    #   - mask_proto_split_prototypes_by_head (bool): If true, this will give each prediction head its own prototypes.\n    #   - mask_proto_crop_with_pred_box (bool): Whether to crop with the predicted box or the gt box.\n    \'lincomb\': 1,\n})\n\n\n\n\n\n# ----------------------- ACTIVATION FUNCTIONS ----------------------- #\n\nactivation_func = Config({\n    \'tanh\':    torch.tanh,\n    \'sigmoid\': torch.sigmoid,\n    \'softmax\': lambda x: torch.nn.functional.softmax(x, dim=-1),\n    \'relu\':    lambda x: torch.nn.functional.relu(x, inplace=True),\n    \'none\':    lambda x: x,\n})\n\n\n\n\n\n# ----------------------- FPN DEFAULTS ----------------------- #\n\nfpn_base = Config({\n    # The number of features to have in each FPN layer\n    \'num_features\': 256,\n\n    # The upsampling mode used\n    \'interpolation_mode\': \'bilinear\',\n\n    # The number of extra layers to be produced by downsampling starting at P5\n    \'num_downsample\': 1,\n\n    # Whether to down sample with a 3x3 stride 2 conv layer instead of just a stride 2 selection\n    \'use_conv_downsample\': False,\n\n    # Whether to pad the pred layers with 1 on each side (I forgot to add this at the start)\n    # This is just here for backwards compatibility\n    \'pad\': True,\n\n    # Whether to add relu to the downsampled layers.\n    \'relu_downsample_layers\': False,\n\n    # Whether to add relu to the regular layers\n    \'relu_pred_layers\': True,\n})\n\n\n\n\n\n# ----------------------- CONFIG DEFAULTS ----------------------- #\n\ncoco_base_config = Config({\n    \'dataset\': coco2014_dataset,\n    \'num_classes\': 81, # This should include the background class\n\n    \'max_iter\': 400000,\n\n    # The maximum number of detections for evaluation\n    \'max_num_detections\': 100,\n\n    # dw\' = momentum * dw - lr * (grad + decay * w)\n    \'lr\': 1e-3,\n    \'momentum\': 0.9,\n    \'decay\': 5e-4,\n\n    # For each lr step, what to multiply the lr with\n    \'gamma\': 0.1,\n    \'lr_steps\': (280000, 360000, 400000),\n\n    # Initial learning rate to linearly warmup from (if until > 0)\n    \'lr_warmup_init\': 1e-4,\n\n    # If > 0 then increase the lr linearly from warmup_init to lr each iter for until iters\n    \'lr_warmup_until\': 500,\n\n    # The terms to scale the respective loss by\n    \'conf_alpha\': 1,\n    \'bbox_alpha\': 1.5,\n    \'mask_alpha\': 0.4 / 256 * 140 * 140, # Some funky equation. Don\'t worry about it.\n\n    # Eval.py sets this if you just want to run YOLACT as a detector\n    \'eval_mask_branch\': True,\n\n    # Top_k examples to consider for NMS\n    \'nms_top_k\': 200,\n    # Examples with confidence less than this are not considered by NMS\n    \'nms_conf_thresh\': 0.05,\n    # Boxes with IoU overlap greater than this threshold will be culled during NMS\n    \'nms_thresh\': 0.5,\n\n    # See mask_type for details.\n    \'mask_type\': mask_type.direct,\n    \'mask_size\': 16,\n    \'masks_to_train\': 100,\n    \'mask_proto_src\': None,\n    \'mask_proto_net\': [(256, 3, {}), (256, 3, {})],\n    \'mask_proto_bias\': False,\n    \'mask_proto_prototype_activation\': activation_func.relu,\n    \'mask_proto_mask_activation\': activation_func.sigmoid,\n    \'mask_proto_coeff_activation\': activation_func.tanh,\n    \'mask_proto_crop\': True,\n    \'mask_proto_crop_expand\': 0,\n    \'mask_proto_loss\': None,\n    \'mask_proto_binarize_downsampled_gt\': True,\n    \'mask_proto_normalize_mask_loss_by_sqrt_area\': False,\n    \'mask_proto_reweight_mask_loss\': False,\n    \'mask_proto_grid_file\': \'data/grid.npy\',\n    \'mask_proto_use_grid\':  False,\n    \'mask_proto_coeff_gate\': False,\n    \'mask_proto_prototypes_as_features\': False,\n    \'mask_proto_prototypes_as_features_no_grad\': False,\n    \'mask_proto_remove_empty_masks\': False,\n    \'mask_proto_reweight_coeff\': 1,\n    \'mask_proto_coeff_diversity_loss\': False,\n    \'mask_proto_coeff_diversity_alpha\': 1,\n    \'mask_proto_normalize_emulate_roi_pooling\': False,\n    \'mask_proto_double_loss\': False,\n    \'mask_proto_double_loss_alpha\': 1,\n    \'mask_proto_split_prototypes_by_head\': False,\n    \'mask_proto_crop_with_pred_box\': False,\n\n    # SSD data augmentation parameters\n    # Randomize hue, vibrance, etc.\n    \'augment_photometric_distort\': True,\n    # Have a chance to scale down the image and pad (to emulate smaller detections)\n    \'augment_expand\': True,\n    # Potentialy sample a random crop from the image and put it in a random place\n    \'augment_random_sample_crop\': True,\n    # Mirror the image with a probability of 1/2\n    \'augment_random_mirror\': True,\n    # Flip the image vertically with a probability of 1/2\n    \'augment_random_flip\': False,\n    # With uniform probability, rotate the image [0,90,180,270] degrees\n    \'augment_random_rot90\': False,\n\n    # Discard detections with width and height smaller than this (in absolute width and height)\n    \'discard_box_width\': 4 / 550,\n    \'discard_box_height\': 4 / 550,\n\n    # If using batchnorm anywhere in the backbone, freeze the batchnorm layer during training.\n    # Note: any additional batch norm layers after the backbone will not be frozen.\n    \'freeze_bn\': False,\n\n    # Set this to a config object if you want an FPN (inherit from fpn_base). See fpn_base for details.\n    \'fpn\': None,\n\n    # Use the same weights for each network head\n    \'share_prediction_module\': False,\n\n    # For hard negative mining, instead of using the negatives that are leastl confidently background,\n    # use negatives that are most confidently not background.\n    \'ohem_use_most_confident\': False,\n\n    # Use focal loss as described in https://arxiv.org/pdf/1708.02002.pdf instead of OHEM\n    \'use_focal_loss\': False,\n    \'focal_loss_alpha\': 0.25,\n    \'focal_loss_gamma\': 2,\n    \n    # The initial bias toward forground objects, as specified in the focal loss paper\n    \'focal_loss_init_pi\': 0.01,\n\n    # Keeps track of the average number of examples for each class, and weights the loss for that class accordingly.\n    \'use_class_balanced_conf\': False,\n\n    # Whether to use sigmoid focal loss instead of softmax, all else being the same.\n    \'use_sigmoid_focal_loss\': False,\n\n    # Use class[0] to be the objectness score and class[1:] to be the softmax predicted class.\n    # Note: at the moment this is only implemented if use_focal_loss is on.\n    \'use_objectness_score\': False,\n\n    # Adds a global pool + fc layer to the smallest selected layer that predicts the existence of each of the 80 classes.\n    # This branch is only evaluated during training time and is just there for multitask learning.\n    \'use_class_existence_loss\': False,\n    \'class_existence_alpha\': 1,\n\n    # Adds a 1x1 convolution directly to the biggest selected layer that predicts a semantic segmentations for each of the 80 classes.\n    # This branch is only evaluated during training time and is just there for multitask learning.\n    \'use_semantic_segmentation_loss\': False,\n    \'semantic_segmentation_alpha\': 1,\n\n    # Adds another branch to the netwok to predict Mask IoU.\n    \'use_mask_scoring\': False,\n    \'mask_scoring_alpha\': 1,\n\n    # Match gt boxes using the Box2Pix change metric instead of the standard IoU metric.\n    # Note that the threshold you set for iou_threshold should be negative with this setting on.\n    \'use_change_matching\': False,\n\n    # Uses the same network format as mask_proto_net, except this time it\'s for adding extra head layers before the final\n    # prediction in prediction modules. If this is none, no extra layers will be added.\n    \'extra_head_net\': None,\n\n    # What params should the final head layers have (the ones that predict box, confidence, and mask coeffs)\n    \'head_layer_params\': {\'kernel_size\': 3, \'padding\': 1},\n\n    # Add extra layers between the backbone and the network heads\n    # The order is (bbox, conf, mask)\n    \'extra_layers\': (0, 0, 0),\n\n    # During training, to match detections with gt, first compute the maximum gt IoU for each prior.\n    # Then, any of those priors whose maximum overlap is over the positive threshold, mark as positive.\n    # For any priors whose maximum is less than the negative iou threshold, mark them as negative.\n    # The rest are neutral and not used in calculating the loss.\n    \'positive_iou_threshold\': 0.5,\n    \'negative_iou_threshold\': 0.5,\n\n    # When using ohem, the ratio between positives and negatives (3 means 3 negatives to 1 positive)\n    \'ohem_negpos_ratio\': 3,\n\n    # If less than 1, anchors treated as a negative that have a crowd iou over this threshold with\n    # the crowd boxes will be treated as a neutral.\n    \'crowd_iou_threshold\': 1,\n\n    # This is filled in at runtime by Yolact\'s __init__, so don\'t touch it\n    \'mask_dim\': None,\n\n    # Input image size.\n    \'max_size\': 300,\n    \n    # Whether or not to do post processing on the cpu at test time\n    \'force_cpu_nms\': True,\n\n    # Whether to use mask coefficient cosine similarity nms instead of bbox iou nms\n    \'use_coeff_nms\': False,\n\n    # Whether or not to have a separate branch whose sole purpose is to act as the coefficients for coeff_diversity_loss\n    # Remember to turn on coeff_diversity_loss, or these extra coefficients won\'t do anything!\n    # To see their effect, also remember to turn on use_coeff_nms.\n    \'use_instance_coeff\': False,\n    \'num_instance_coeffs\': 64,\n\n    # Whether or not to tie the mask loss / box loss to 0\n    \'train_masks\': True,\n    \'train_boxes\': True,\n    # If enabled, the gt masks will be cropped using the gt bboxes instead of the predicted ones.\n    # This speeds up training time considerably but results in much worse mAP at test time.\n    \'use_gt_bboxes\': False,\n\n    # Whether or not to preserve aspect ratio when resizing the image.\n    # If True, this will resize all images to be max_size^2 pixels in area while keeping aspect ratio.\n    # If False, all images are resized to max_size x max_size\n    \'preserve_aspect_ratio\': False,\n\n    # Whether or not to use the prediction module (c) from DSSD\n    \'use_prediction_module\': False,\n\n    # Whether or not to use the predicted coordinate scheme from Yolo v2\n    \'use_yolo_regressors\': False,\n    \n    # For training, bboxes are considered ""positive"" if their anchors have a 0.5 IoU overlap\n    # or greater with a ground truth box. If this is true, instead of using the anchor boxes\n    # for this IoU computation, the matching function will use the predicted bbox coordinates.\n    # Don\'t turn this on if you\'re not using yolo regressors!\n    \'use_prediction_matching\': False,\n\n    # A list of settings to apply after the specified iteration. Each element of the list should look like\n    # (iteration, config_dict) where config_dict is a dictionary you\'d pass into a config object\'s init.\n    \'delayed_settings\': [],\n\n    # Use command-line arguments to set this.\n    \'no_jit\': False,\n\n    \'backbone\': None,\n    \'name\': \'base_config\',\n\n    # Fast Mask Re-scoring Network\n    # Inspried by Mask Scoring R-CNN (https://arxiv.org/abs/1903.00241)\n    # Do not crop out the mask with bbox but slide a convnet on the image-size mask,\n    # then use global pooling to get the final mask score\n    \'use_maskiou\': False,\n    \n    # Archecture for the mask iou network. A (num_classes-1, 1, {}) layer is appended to the end.\n    \'maskiou_net\': [],\n\n    # Discard predicted masks whose area is less than this\n    \'discard_mask_area\': -1,\n\n    \'maskiou_alpha\': 1.0,\n    \'rescore_mask\': False,\n    \'rescore_bbox\': False,\n    \'maskious_to_train\': -1,\n})\n\n\n\n\n\n# ----------------------- YOLACT v1.0 CONFIGS ----------------------- #\n\nyolact_base_config = coco_base_config.copy({\n    \'name\': \'yolact_base\',\n\n    # Dataset stuff\n    \'dataset\': coco2017_dataset,\n    \'num_classes\': len(coco2017_dataset.class_names) + 1,\n\n    # Image Size\n    \'max_size\': 550,\n    \n    # Training params\n    \'lr_steps\': (280000, 600000, 700000, 750000),\n    \'max_iter\': 800000,\n    \n    # Backbone Settings\n    \'backbone\': resnet101_backbone.copy({\n        \'selected_layers\': list(range(1, 4)),\n        \'use_pixel_scales\': True,\n        \'preapply_sqrt\': False,\n        \'use_square_anchors\': True, # This is for backward compatability with a bug\n\n        \'pred_aspect_ratios\': [ [[1, 1/2, 2]] ]*5,\n        \'pred_scales\': [[24], [48], [96], [192], [384]],\n    }),\n\n    # FPN Settings\n    \'fpn\': fpn_base.copy({\n        \'use_conv_downsample\': True,\n        \'num_downsample\': 2,\n    }),\n\n    # Mask Settings\n    \'mask_type\': mask_type.lincomb,\n    \'mask_alpha\': 6.125,\n    \'mask_proto_src\': 0,\n    \'mask_proto_net\': [(256, 3, {\'padding\': 1})] * 3 + [(None, -2, {}), (256, 3, {\'padding\': 1})] + [(32, 1, {})],\n    \'mask_proto_normalize_emulate_roi_pooling\': True,\n\n    # Other stuff\n    \'share_prediction_module\': True,\n    \'extra_head_net\': [(256, 3, {\'padding\': 1})],\n\n    \'positive_iou_threshold\': 0.5,\n    \'negative_iou_threshold\': 0.4,\n\n    \'crowd_iou_threshold\': 0.7,\n\n    \'use_semantic_segmentation_loss\': True,\n})\n\nyolact_im400_config = yolact_base_config.copy({\n    \'name\': \'yolact_im400\',\n\n    \'max_size\': 400,\n    \'backbone\': yolact_base_config.backbone.copy({\n        \'pred_scales\': [[int(x[0] / yolact_base_config.max_size * 400)] for x in yolact_base_config.backbone.pred_scales],\n    }),\n})\n\nyolact_im700_config = yolact_base_config.copy({\n    \'name\': \'yolact_im700\',\n\n    \'masks_to_train\': 300,\n    \'max_size\': 700,\n    \'backbone\': yolact_base_config.backbone.copy({\n        \'pred_scales\': [[int(x[0] / yolact_base_config.max_size * 700)] for x in yolact_base_config.backbone.pred_scales],\n    }),\n})\n\nyolact_darknet53_config = yolact_base_config.copy({\n    \'name\': \'yolact_darknet53\',\n\n    \'backbone\': darknet53_backbone.copy({\n        \'selected_layers\': list(range(2, 5)),\n        \n        \'pred_scales\': yolact_base_config.backbone.pred_scales,\n        \'pred_aspect_ratios\': yolact_base_config.backbone.pred_aspect_ratios,\n        \'use_pixel_scales\': True,\n        \'preapply_sqrt\': False,\n        \'use_square_anchors\': True, # This is for backward compatability with a bug\n    }),\n})\n\nyolact_resnet50_config = yolact_base_config.copy({\n    \'name\': \'yolact_resnet50\',\n\n    \'backbone\': resnet50_backbone.copy({\n        \'selected_layers\': list(range(1, 4)),\n        \n        \'pred_scales\': yolact_base_config.backbone.pred_scales,\n        \'pred_aspect_ratios\': yolact_base_config.backbone.pred_aspect_ratios,\n        \'use_pixel_scales\': True,\n        \'preapply_sqrt\': False,\n        \'use_square_anchors\': True, # This is for backward compatability with a bug\n    }),\n})\n\n\nyolact_resnet50_pascal_config = yolact_resnet50_config.copy({\n    \'name\': None, # Will default to yolact_resnet50_pascal\n    \n    # Dataset stuff\n    \'dataset\': pascal_sbd_dataset,\n    \'num_classes\': len(pascal_sbd_dataset.class_names) + 1,\n\n    \'max_iter\': 120000,\n    \'lr_steps\': (60000, 100000),\n    \n    \'backbone\': yolact_resnet50_config.backbone.copy({\n        \'pred_scales\': [[32], [64], [128], [256], [512]],\n        \'use_square_anchors\': False,\n    })\n})\n\n# ----------------------- YOLACT++ CONFIGS ----------------------- #\n\nyolact_plus_base_config = yolact_base_config.copy({\n    \'name\': \'yolact_plus_base\',\n\n    \'backbone\': resnet101_dcn_inter3_backbone.copy({\n        \'selected_layers\': list(range(1, 4)),\n        \n        \'pred_aspect_ratios\': [ [[1, 1/2, 2]] ]*5,\n        \'pred_scales\': [[i * 2 ** (j / 3.0) for j in range(3)] for i in [24, 48, 96, 192, 384]],\n        \'use_pixel_scales\': True,\n        \'preapply_sqrt\': False,\n        \'use_square_anchors\': False,\n    }),\n\n    \'use_maskiou\': True,\n    \'maskiou_net\': [(8, 3, {\'stride\': 2}), (16, 3, {\'stride\': 2}), (32, 3, {\'stride\': 2}), (64, 3, {\'stride\': 2}), (128, 3, {\'stride\': 2})],\n    \'maskiou_alpha\': 25,\n    \'rescore_bbox\': False,\n    \'rescore_mask\': True,\n\n    \'discard_mask_area\': 5*5,\n})\n\nyolact_plus_resnet50_config = yolact_plus_base_config.copy({\n    \'name\': \'yolact_plus_resnet50\',\n\n    \'backbone\': resnet50_dcnv2_backbone.copy({\n        \'selected_layers\': list(range(1, 4)),\n        \n        \'pred_aspect_ratios\': [ [[1, 1/2, 2]] ]*5,\n        \'pred_scales\': [[i * 2 ** (j / 3.0) for j in range(3)] for i in [24, 48, 96, 192, 384]],\n        \'use_pixel_scales\': True,\n        \'preapply_sqrt\': False,\n        \'use_square_anchors\': False,\n    }),\n})\n\n\n# Default config\ncfg = yolact_base_config.copy()\n\ndef set_cfg(config_name:str):\n    """""" Sets the active config. Works even if cfg is already imported! """"""\n    global cfg\n\n    # Note this is not just an eval because I\'m lazy, but also because it can\n    # be used like ssd300_config.copy({\'max_size\': 400}) for extreme fine-tuning\n    cfg.replace(eval(config_name))\n\n    if cfg.name is None:\n        cfg.name = config_name.split(\'_config\')[0]\n\ndef set_dataset(dataset_name:str):\n    """""" Sets the dataset of the current config. """"""\n    cfg.dataset = eval(dataset_name)\n    \n'"
layers/__init__.py,0,b'from .functions import *\nfrom .modules import *\n'
layers/box_utils.py,35,"b'# -*- coding: utf-8 -*-\nimport torch\nfrom utils import timer\n\nfrom data import cfg\n\n@torch.jit.script\ndef point_form(boxes):\n    """""" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n\n\n@torch.jit.script\ndef center_size(boxes):\n    """""" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat(( (boxes[:, 2:] + boxes[:, :2])/2,     # cx, cy\n                        boxes[:, 2:] - boxes[:, :2]  ), 1)  # w, h\n\n@torch.jit.script\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [n,A,4].\n      box_b: (tensor) bounding boxes, Shape: [n,B,4].\n    Return:\n      (tensor) intersection area, Shape: [n,A,B].\n    """"""\n    n = box_a.size(0)\n    A = box_a.size(1)\n    B = box_b.size(1)\n    max_xy = torch.min(box_a[:, :, 2:].unsqueeze(2).expand(n, A, B, 2),\n                       box_b[:, :, 2:].unsqueeze(1).expand(n, A, B, 2))\n    min_xy = torch.max(box_a[:, :, :2].unsqueeze(2).expand(n, A, B, 2),\n                       box_b[:, :, :2].unsqueeze(1).expand(n, A, B, 2))\n    return torch.clamp(max_xy - min_xy, min=0).prod(3)  # inter\n\n\ndef jaccard(box_a, box_b, iscrowd:bool=False):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes. If iscrowd=True, put the crowd in box_b.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    use_batch = True\n    if box_a.dim() == 2:\n        use_batch = False\n        box_a = box_a[None, ...]\n        box_b = box_b[None, ...]\n\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, :, 2]-box_a[:, :, 0]) *\n              (box_a[:, :, 3]-box_a[:, :, 1])).unsqueeze(2).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, :, 2]-box_b[:, :, 0]) *\n              (box_b[:, :, 3]-box_b[:, :, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n\n    out = inter / area_a if iscrowd else inter / union\n    return out if use_batch else out.squeeze(0)\n\ndef elemwise_box_iou(box_a, box_b):\n    """""" Does the same as above but instead of pairwise, elementwise along the inner dimension. """"""\n    max_xy = torch.min(box_a[:, 2:], box_b[:, 2:])\n    min_xy = torch.max(box_a[:, :2], box_b[:, :2])\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    inter = inter[:, 0] * inter[:, 1]\n\n    area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])\n    area_b = (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])\n\n    union = area_a + area_b - inter\n    union = torch.clamp(union, min=0.1)\n\n    # Return value is [n] for inputs [n, 4]\n    return torch.clamp(inter / union, max=1)\n\ndef mask_iou(masks_a, masks_b, iscrowd=False):\n    """"""\n    Computes the pariwise mask IoU between two sets of masks of size [a, h, w] and [b, h, w].\n    The output is of size [a, b].\n\n    Wait I thought this was ""box_utils"", why am I putting this in here?\n    """"""\n\n    masks_a = masks_a.view(masks_a.size(0), -1)\n    masks_b = masks_b.view(masks_b.size(0), -1)\n\n    intersection = masks_a @ masks_b.t()\n    area_a = masks_a.sum(dim=1).unsqueeze(1)\n    area_b = masks_b.sum(dim=1).unsqueeze(0)\n\n    return intersection / (area_a + area_b - intersection) if not iscrowd else intersection / area_a\n\ndef elemwise_mask_iou(masks_a, masks_b):\n    """""" Does the same as above but instead of pairwise, elementwise along the outer dimension. """"""\n    masks_a = masks_a.view(-1, masks_a.size(-1))\n    masks_b = masks_b.view(-1, masks_b.size(-1))\n\n    intersection = (masks_a * masks_b).sum(dim=0)\n    area_a = masks_a.sum(dim=0)\n    area_b = masks_b.sum(dim=0)\n\n    # Return value is [n] for inputs [h, w, n]\n    return torch.clamp(intersection / torch.clamp(area_a + area_b - intersection, min=0.1), max=1)\n\n\n\ndef change(gt, priors):\n    """"""\n    Compute the d_change metric proposed in Box2Pix:\n    https://lmb.informatik.uni-freiburg.de/Publications/2018/UB18/paper-box2pix.pdf\n    \n    Input should be in point form (xmin, ymin, xmax, ymax).\n\n    Output is of shape [num_gt, num_priors]\n    Note this returns -change so it can be a drop in replacement for \n    """"""\n    num_priors = priors.size(0)\n    num_gt     = gt.size(0)\n\n    gt_w = (gt[:, 2] - gt[:, 0])[:, None].expand(num_gt, num_priors)\n    gt_h = (gt[:, 3] - gt[:, 1])[:, None].expand(num_gt, num_priors)\n\n    gt_mat =     gt[:, None, :].expand(num_gt, num_priors, 4)\n    pr_mat = priors[None, :, :].expand(num_gt, num_priors, 4)\n\n    diff = gt_mat - pr_mat\n    diff[:, :, 0] /= gt_w\n    diff[:, :, 2] /= gt_w\n    diff[:, :, 1] /= gt_h\n    diff[:, :, 3] /= gt_h\n\n    return -torch.sqrt( (diff ** 2).sum(dim=2) )\n\n\n\n\ndef match(pos_thresh, neg_thresh, truths, priors, labels, crowd_boxes, loc_t, conf_t, idx_t, idx, loc_data):\n    """"""Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        pos_thresh: (float) IoU > pos_thresh ==> positive.\n        neg_thresh: (float) IoU < neg_thresh ==> negative.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        crowd_boxes: (tensor) All the crowd box annotations or None if there are none.\n        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds. Note: -1 means neutral.\n        idx_t: (tensor) Tensor to be filled w/ the index of the matched gt box for each prior.\n        idx: (int) current batch index.\n        loc_data: (tensor) The predicted bbox regression coordinates for this batch.\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n    """"""\n    decoded_priors = decode(loc_data, priors, cfg.use_yolo_regressors) if cfg.use_prediction_matching else point_form(priors)\n    \n    # Size [num_objects, num_priors]\n    overlaps = jaccard(truths, decoded_priors) if not cfg.use_change_matching else change(truths, decoded_priors)\n\n    # Size [num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0)\n\n    # We want to ensure that each gt gets used at least once so that we don\'t\n    # waste any training data. In order to do that, find the max overlap anchor\n    # with each gt, and force that anchor to use that gt.\n    for _ in range(overlaps.size(0)):\n        # Find j, the gt with the highest overlap with a prior\n        # In effect, this will loop through overlaps.size(0) in a ""smart"" order,\n        # always choosing the highest overlap first.\n        best_prior_overlap, best_prior_idx = overlaps.max(1)\n        j = best_prior_overlap.max(0)[1]\n\n        # Find i, the highest overlap anchor with this gt\n        i = best_prior_idx[j]\n\n        # Set all other overlaps with i to be -1 so that no other gt uses it\n        overlaps[:, i] = -1\n        # Set all other overlaps with j to be -1 so that this loop never uses j again\n        overlaps[j, :] = -1\n\n        # Overwrite i\'s score to be 2 so it doesn\'t get thresholded ever\n        best_truth_overlap[i] = 2\n        # Set the gt to be used for i to be j, overwriting whatever was there\n        best_truth_idx[i] = j\n\n    matches = truths[best_truth_idx]            # Shape: [num_priors,4]\n    conf = labels[best_truth_idx] + 1           # Shape: [num_priors]\n\n    conf[best_truth_overlap < pos_thresh] = -1  # label as neutral\n    conf[best_truth_overlap < neg_thresh] =  0  # label as background\n\n    # Deal with crowd annotations for COCO\n    if crowd_boxes is not None and cfg.crowd_iou_threshold < 1:\n        # Size [num_priors, num_crowds]\n        crowd_overlaps = jaccard(decoded_priors, crowd_boxes, iscrowd=True)\n        # Size [num_priors]\n        best_crowd_overlap, best_crowd_idx = crowd_overlaps.max(1)\n        # Set non-positives with crowd iou of over the threshold to be neutral.\n        conf[(conf <= 0) & (best_crowd_overlap > cfg.crowd_iou_threshold)] = -1\n\n    loc = encode(matches, priors, cfg.use_yolo_regressors)\n    loc_t[idx]  = loc    # [num_priors,4] encoded offsets to learn\n    conf_t[idx] = conf   # [num_priors] top class label for each prior\n    idx_t[idx]  = best_truth_idx # [num_priors] indices for lookup\n\n@torch.jit.script\ndef encode(matched, priors, use_yolo_regressors:bool=False):\n    """"""\n    Encode bboxes matched with each prior into the format\n    produced by the network. See decode for more details on\n    this format. Note that encode(decode(x, p), p) = x.\n    \n    Args:\n        - matched: A tensor of bboxes in point form with shape [num_priors, 4]\n        - priors:  The tensor of all priors with shape [num_priors, 4]\n    Return: A tensor with encoded relative coordinates in the format\n            outputted by the network (see decode). Size: [num_priors, 4]\n    """"""\n\n    if use_yolo_regressors:\n        # Exactly the reverse of what we did in decode\n        # In fact encode(decode(x, p), p) should be x\n        boxes = center_size(matched)\n\n        loc = torch.cat((\n            boxes[:, :2] - priors[:, :2],\n            torch.log(boxes[:, 2:] / priors[:, 2:])\n        ), 1)\n    else:\n        variances = [0.1, 0.2]\n\n        # dist b/t match center and prior\'s center\n        g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n        # encode variance\n        g_cxcy /= (variances[0] * priors[:, 2:])\n        # match wh / prior wh\n        g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n        g_wh = torch.log(g_wh) / variances[1]\n        # return target for smooth_l1_loss\n        loc = torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n        \n    return loc\n\n@torch.jit.script\ndef decode(loc, priors, use_yolo_regressors:bool=False):\n    """"""\n    Decode predicted bbox coordinates using the same scheme\n    employed by Yolov2: https://arxiv.org/pdf/1612.08242.pdf\n\n        b_x = (sigmoid(pred_x) - .5) / conv_w + prior_x\n        b_y = (sigmoid(pred_y) - .5) / conv_h + prior_y\n        b_w = prior_w * exp(loc_w)\n        b_h = prior_h * exp(loc_h)\n    \n    Note that loc is inputed as [(s(x)-.5)/conv_w, (s(y)-.5)/conv_h, w, h]\n    while priors are inputed as [x, y, w, h] where each coordinate\n    is relative to size of the image (even sigmoid(x)). We do this\n    in the network by dividing by the \'cell size\', which is just\n    the size of the convouts.\n    \n    Also note that prior_x and prior_y are center coordinates which\n    is why we have to subtract .5 from sigmoid(pred_x and pred_y).\n    \n    Args:\n        - loc:    The predicted bounding boxes of size [num_priors, 4]\n        - priors: The priorbox coords with size [num_priors, 4]\n    \n    Returns: A tensor of decoded relative coordinates in point form \n             form with size [num_priors, 4]\n    """"""\n\n    if use_yolo_regressors:\n        # Decoded boxes in center-size notation\n        boxes = torch.cat((\n            loc[:, :2] + priors[:, :2],\n            priors[:, 2:] * torch.exp(loc[:, 2:])\n        ), 1)\n\n        boxes = point_form(boxes)\n    else:\n        variances = [0.1, 0.2]\n        \n        boxes = torch.cat((\n            priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n            priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n        boxes[:, :2] -= boxes[:, 2:] / 2\n        boxes[:, 2:] += boxes[:, :2]\n    \n    return boxes\n\n\n\ndef log_sum_exp(x):\n    """"""Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    """"""\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x-x_max), 1)) + x_max\n\n\n@torch.jit.script\ndef sanitize_coordinates(_x1, _x2, img_size:int, padding:int=0, cast:bool=True):\n    """"""\n    Sanitizes the input coordinates so that x1 < x2, x1 != x2, x1 >= 0, and x2 <= image_size.\n    Also converts from relative to absolute coordinates and casts the results to long tensors.\n\n    If cast is false, the result won\'t be cast to longs.\n    Warning: this does things in-place behind the scenes so copy if necessary.\n    """"""\n    _x1 = _x1 * img_size\n    _x2 = _x2 * img_size\n    if cast:\n        _x1 = _x1.long()\n        _x2 = _x2.long()\n    x1 = torch.min(_x1, _x2)\n    x2 = torch.max(_x1, _x2)\n    x1 = torch.clamp(x1-padding, min=0)\n    x2 = torch.clamp(x2+padding, max=img_size)\n\n    return x1, x2\n\n\n@torch.jit.script\ndef crop(masks, boxes, padding:int=1):\n    """"""\n    ""Crop"" predicted masks by zeroing out everything not in the predicted bbox.\n    Vectorized by Chong (thanks Chong).\n\n    Args:\n        - masks should be a size [h, w, n] tensor of masks\n        - boxes should be a size [n, 4] tensor of bbox coords in relative point form\n    """"""\n    h, w, n = masks.size()\n    x1, x2 = sanitize_coordinates(boxes[:, 0], boxes[:, 2], w, padding, cast=False)\n    y1, y2 = sanitize_coordinates(boxes[:, 1], boxes[:, 3], h, padding, cast=False)\n\n    rows = torch.arange(w, device=masks.device, dtype=x1.dtype).view(1, -1, 1).expand(h, w, n)\n    cols = torch.arange(h, device=masks.device, dtype=x1.dtype).view(-1, 1, 1).expand(h, w, n)\n    \n    masks_left  = rows >= x1.view(1, 1, -1)\n    masks_right = rows <  x2.view(1, 1, -1)\n    masks_up    = cols >= y1.view(1, 1, -1)\n    masks_down  = cols <  y2.view(1, 1, -1)\n    \n    crop_mask = masks_left * masks_right * masks_up * masks_down\n    \n    return masks * crop_mask.float()\n\n\ndef index2d(src, idx):\n    """"""\n    Indexes a tensor by a 2d index.\n\n    In effect, this does\n        out[i, j] = src[i, idx[i, j]]\n    \n    Both src and idx should have the same size.\n    """"""\n\n    offs = torch.arange(idx.size(0), device=idx.device)[:, None].expand_as(idx)\n    idx  = idx + offs * idx.size(1)\n\n    return src.view(-1)[idx.view(-1)].view(idx.size())\n'"
layers/interpolate.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\nclass InterpolateModule(nn.Module):\n\t""""""\n\tThis is a module version of F.interpolate (rip nn.Upsampling).\n\tAny arguments you give it just get passed along for the ride.\n\t""""""\n\n\tdef __init__(self, *args, **kwdargs):\n\t\tsuper().__init__()\n\n\t\tself.args = args\n\t\tself.kwdargs = kwdargs\n\n\tdef forward(self, x):\n\t\treturn F.interpolate(x, *self.args, **self.kwdargs)\n'"
layers/output_utils.py,11,"b'"""""" Contains functions used to sanitize and prepare the output of Yolact. """"""\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport cv2\n\nfrom data import cfg, mask_type, MEANS, STD, activation_func\nfrom utils.augmentations import Resize\nfrom utils import timer\nfrom .box_utils import crop, sanitize_coordinates\n\ndef postprocess(det_output, w, h, batch_idx=0, interpolation_mode=\'bilinear\',\n                visualize_lincomb=False, crop_masks=True, score_threshold=0):\n    """"""\n    Postprocesses the output of Yolact on testing mode into a format that makes sense,\n    accounting for all the possible configuration settings.\n\n    Args:\n        - det_output: The lost of dicts that Detect outputs.\n        - w: The real with of the image.\n        - h: The real height of the image.\n        - batch_idx: If you have multiple images for this batch, the image\'s index in the batch.\n        - interpolation_mode: Can be \'nearest\' | \'area\' | \'bilinear\' (see torch.nn.functional.interpolate)\n\n    Returns 4 torch Tensors (in the following order):\n        - classes [num_det]: The class idx for each detection.\n        - scores  [num_det]: The confidence score for each detection.\n        - boxes   [num_det, 4]: The bounding box for each detection in absolute point form.\n        - masks   [num_det, h, w]: Full image masks for each detection.\n    """"""\n    \n    dets = det_output[batch_idx]\n    net = dets[\'net\']\n    dets = dets[\'detection\']\n\n    if dets is None:\n        return [torch.Tensor()] * 4 # Warning, this is 4 copies of the same thing\n\n    if score_threshold > 0:\n        keep = dets[\'score\'] > score_threshold\n\n        for k in dets:\n            if k != \'proto\':\n                dets[k] = dets[k][keep]\n        \n        if dets[\'score\'].size(0) == 0:\n            return [torch.Tensor()] * 4\n    \n    # Actually extract everything from dets now\n    classes = dets[\'class\']\n    boxes   = dets[\'box\']\n    scores  = dets[\'score\']\n    masks   = dets[\'mask\']\n\n    if cfg.mask_type == mask_type.lincomb and cfg.eval_mask_branch:\n        # At this points masks is only the coefficients\n        proto_data = dets[\'proto\']\n        \n        # Test flag, do not upvote\n        if cfg.mask_proto_debug:\n            np.save(\'scripts/proto.npy\', proto_data.cpu().numpy())\n        \n        if visualize_lincomb:\n            display_lincomb(proto_data, masks)\n\n        masks = proto_data @ masks.t()\n        masks = cfg.mask_proto_mask_activation(masks)\n\n        # Crop masks before upsampling because you know why\n        if crop_masks:\n            masks = crop(masks, boxes)\n\n        # Permute into the correct output shape [num_dets, proto_h, proto_w]\n        masks = masks.permute(2, 0, 1).contiguous()\n\n        if cfg.use_maskiou:\n            with timer.env(\'maskiou_net\'):                \n                with torch.no_grad():\n                    maskiou_p = net.maskiou_net(masks.unsqueeze(1))\n                    maskiou_p = torch.gather(maskiou_p, dim=1, index=classes.unsqueeze(1)).squeeze(1)\n                    if cfg.rescore_mask:\n                        if cfg.rescore_bbox:\n                            scores = scores * maskiou_p\n                        else:\n                            scores = [scores, scores * maskiou_p]\n\n        # Scale masks up to the full image\n        masks = F.interpolate(masks.unsqueeze(0), (h, w), mode=interpolation_mode, align_corners=False).squeeze(0)\n\n        # Binarize the masks\n        masks.gt_(0.5)\n\n    \n    boxes[:, 0], boxes[:, 2] = sanitize_coordinates(boxes[:, 0], boxes[:, 2], w, cast=False)\n    boxes[:, 1], boxes[:, 3] = sanitize_coordinates(boxes[:, 1], boxes[:, 3], h, cast=False)\n    boxes = boxes.long()\n\n    if cfg.mask_type == mask_type.direct and cfg.eval_mask_branch:\n        # Upscale masks\n        full_masks = torch.zeros(masks.size(0), h, w)\n\n        for jdx in range(masks.size(0)):\n            x1, y1, x2, y2 = boxes[jdx, :]\n\n            mask_w = x2 - x1\n            mask_h = y2 - y1\n\n            # Just in case\n            if mask_w * mask_h <= 0 or mask_w < 0:\n                continue\n            \n            mask = masks[jdx, :].view(1, 1, cfg.mask_size, cfg.mask_size)\n            mask = F.interpolate(mask, (mask_h, mask_w), mode=interpolation_mode, align_corners=False)\n            mask = mask.gt(0.5).float()\n            full_masks[jdx, y1:y2, x1:x2] = mask\n        \n        masks = full_masks\n\n    return classes, scores, boxes, masks\n\n\n    \n\n\ndef undo_image_transformation(img, w, h):\n    """"""\n    Takes a transformed image tensor and returns a numpy ndarray that is untransformed.\n    Arguments w and h are the original height and width of the image.\n    """"""\n    img_numpy = img.permute(1, 2, 0).cpu().numpy()\n    img_numpy = img_numpy[:, :, (2, 1, 0)] # To BRG\n\n    if cfg.backbone.transform.normalize:\n        img_numpy = (img_numpy * np.array(STD) + np.array(MEANS)) / 255.0\n    elif cfg.backbone.transform.subtract_means:\n        img_numpy = (img_numpy / 255.0 + np.array(MEANS) / 255.0).astype(np.float32)\n        \n    img_numpy = img_numpy[:, :, (2, 1, 0)] # To RGB\n    img_numpy = np.clip(img_numpy, 0, 1)\n\n    return cv2.resize(img_numpy, (w,h))\n\n\ndef display_lincomb(proto_data, masks):\n    out_masks = torch.matmul(proto_data, masks.t())\n    # out_masks = cfg.mask_proto_mask_activation(out_masks)\n\n    for kdx in range(1):\n        jdx = kdx + 0\n        import matplotlib.pyplot as plt\n        coeffs = masks[jdx, :].cpu().numpy()\n        idx = np.argsort(-np.abs(coeffs))\n        # plt.bar(list(range(idx.shape[0])), coeffs[idx])\n        # plt.show()\n        \n        coeffs_sort = coeffs[idx]\n        arr_h, arr_w = (4,8)\n        proto_h, proto_w, _ = proto_data.size()\n        arr_img = np.zeros([proto_h*arr_h, proto_w*arr_w])\n        arr_run = np.zeros([proto_h*arr_h, proto_w*arr_w])\n        test = torch.sum(proto_data, -1).cpu().numpy()\n\n        for y in range(arr_h):\n            for x in range(arr_w):\n                i = arr_w * y + x\n\n                if i == 0:\n                    running_total = proto_data[:, :, idx[i]].cpu().numpy() * coeffs_sort[i]\n                else:\n                    running_total += proto_data[:, :, idx[i]].cpu().numpy() * coeffs_sort[i]\n\n                running_total_nonlin = running_total\n                if cfg.mask_proto_mask_activation == activation_func.sigmoid:\n                    running_total_nonlin = (1/(1+np.exp(-running_total_nonlin)))\n\n                arr_img[y*proto_h:(y+1)*proto_h, x*proto_w:(x+1)*proto_w] = (proto_data[:, :, idx[i]] / torch.max(proto_data[:, :, idx[i]])).cpu().numpy() * coeffs_sort[i]\n                arr_run[y*proto_h:(y+1)*proto_h, x*proto_w:(x+1)*proto_w] = (running_total_nonlin > 0.5).astype(np.float)\n        plt.imshow(arr_img)\n        plt.show()\n        # plt.imshow(arr_run)\n        # plt.show()\n        # plt.imshow(test)\n        # plt.show()\n        plt.imshow(out_masks[:, :, jdx].cpu().numpy())\n        plt.show()\n'"
scripts/augment_bbox.py,0,"b'\nimport os.path as osp\nimport json, pickle\nimport sys\nfrom math import sqrt\nfrom itertools import product\nimport torch\nfrom numpy import random\n\nimport numpy as np\n\n\nmax_image_size = 550\naugment_idx = 0\ndump_file = \'weights/bboxes_aug.pkl\'\nbox_file = \'weights/bboxes.pkl\'\n\ndef augment_boxes(bboxes):\n\tbboxes_rel = []\n\tfor box in bboxes:\n\t\tbboxes_rel.append(prep_box(box))\n\tbboxes_rel = np.concatenate(bboxes_rel, axis=0)\n\n\twith open(dump_file, \'wb\') as f:\n\t\tpickle.dump(bboxes_rel, f)\n\ndef prep_box(box_list):\n\tglobal augment_idx\n\tboxes = np.array([box_list[2:]], dtype=np.float32)\n\n\t# Image width and height\n\twidth, height = box_list[:2]\n\n\t# To point form\n\tboxes[:, 2:] += boxes[:, :2]\n\n\n\t# Expand\n\tratio = random.uniform(1, 4)\n\tleft = random.uniform(0, width*ratio - width)\n\ttop = random.uniform(0, height*ratio - height)\n\n\theight *= ratio\n\twidth  *= ratio\n\n\tboxes[:, :2] += (int(left), int(top))\n\tboxes[:, 2:] += (int(left), int(top))\n\n\n\t# RandomSampleCrop\n\theight, width, boxes = random_sample_crop(height, width, boxes)\n\n\n\t# RandomMirror\n\tif random.randint(0, 2):\n\t\tboxes[:, 0::2] = width - boxes[:, 2::-2]\n\n\t\n\t# Resize\n\tboxes[:, [0, 2]] *= (max_image_size / width)\n\tboxes[:, [1, 3]] *= (max_image_size / height)\n\twidth = height = max_image_size\n\n\n\t# ToPercentCoords\n\tboxes[:, [0, 2]] /= width\n\tboxes[:, [1, 3]] /= height\n\n\tif augment_idx % 50000 == 0:\n\t\tprint(\'Current idx: %d\' % augment_idx)\n\n\taugment_idx += 1\n\n\treturn boxes\n\n\n\n\nsample_options = (\n\t# using entire original input image\n\tNone,\n\t# sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n\t(0.1, None),\n\t(0.3, None),\n\t(0.7, None),\n\t(0.9, None),\n\t# randomly sample a patch\n\t(None, None),\n)\n\ndef intersect(box_a, box_b):\n    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n    return inter[:, 0] * inter[:, 1]\n\n\ndef jaccard_numpy(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n        box_b: Single bounding box, Shape: [4]\n    Return:\n        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1]))  # [A,B]\n    area_b = ((box_b[2]-box_b[0]) *\n              (box_b[3]-box_b[1]))  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef random_sample_crop(height, width, boxes=None):\n\tglobal sample_options\n\t\n\twhile True:\n\t\t# randomly choose a mode\n\t\tmode = random.choice(sample_options)\n\t\tif mode is None:\n\t\t\treturn height, width, boxes\n\n\t\tmin_iou, max_iou = mode\n\t\tif min_iou is None:\n\t\t\tmin_iou = float(\'-inf\')\n\t\tif max_iou is None:\n\t\t\tmax_iou = float(\'inf\')\n\n\t\tfor _ in range(50):\n\t\t\tw = random.uniform(0.3 * width, width)\n\t\t\th = random.uniform(0.3 * height, height)\n\n\t\t\tif h / w < 0.5 or h / w > 2:\n\t\t\t\tcontinue\n\n\t\t\tleft = random.uniform(0, width - w)\n\t\t\ttop = random.uniform(0, height - h)\n\n\t\t\trect = np.array([int(left), int(top), int(left+w), int(top+h)])\n\t\t\toverlap = jaccard_numpy(boxes, rect)\n\t\t\tif overlap.min() < min_iou and max_iou < overlap.max():\n\t\t\t\tcontinue\n\n\t\t\tcenters = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n\n\t\t\tm1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n\t\t\tm2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n\t\t\tmask = m1 * m2\n\n\t\t\tif not mask.any():\n\t\t\t\tcontinue\n\n\t\t\tcurrent_boxes = boxes[mask, :].copy()\n\t\t\tcurrent_boxes[:, :2] = np.maximum(current_boxes[:, :2], rect[:2])\n\t\t\tcurrent_boxes[:, :2] -= rect[:2]\n\t\t\tcurrent_boxes[:, 2:] = np.minimum(current_boxes[:, 2:], rect[2:])\n\t\t\tcurrent_boxes[:, 2:] -= rect[:2]\n\n\t\t\treturn h, w, current_boxes\n\n\nif __name__ == \'__main__\':\n\t\n\twith open(box_file, \'rb\') as f:\n\t\tbboxes = pickle.load(f)\n\n\taugment_boxes(bboxes)\n'"
scripts/bbox_recall.py,11,"b'""""""\nThis script compiles all the bounding boxes in the training data and\nclusters them for each convout resolution on which they\'re used.\n\nRun this script from the Yolact root directory.\n""""""\n\nimport os.path as osp\nimport json, pickle\nimport sys\nfrom math import sqrt\nfrom itertools import product\nimport torch\nimport random\n\nimport numpy as np\n\ndump_file = \'weights/bboxes.pkl\'\naug_file  = \'weights/bboxes_aug.pkl\'\n\nuse_augmented_boxes = True\n\n\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b, iscrowd=False):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes. If iscrowd=True, put the crowd in box_b.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n\n    if iscrowd:\n        return inter / area_a\n    else:\n        return inter / union  # [A,B]\n\n# Also convert to point form\ndef to_relative(bboxes):\n    return np.concatenate((bboxes[:, 2:4] / bboxes[:, :2], (bboxes[:, 2:4] + bboxes[:, 4:]) / bboxes[:, :2]), axis=1)\n\n\ndef make_priors(conv_size, scales, aspect_ratios):\n    prior_data = []\n    conv_h = conv_size[0]\n    conv_w = conv_size[1]\n\n    # Iteration order is important (it has to sync up with the convout)\n    for j, i in product(range(conv_h), range(conv_w)):\n        x = (i + 0.5) / conv_w\n        y = (j + 0.5) / conv_h\n        \n        for scale, ars in zip(scales, aspect_ratios):\n            for ar in ars:\n                w = scale * ar / conv_w\n                h = scale / ar / conv_h\n\n                # Point form\n                prior_data += [x - w/2, y - h/2, x + w/2, y + h/2]\n    \n    return np.array(prior_data).reshape(-1, 4)\n\n# fixed_ssd_config\n# scales = [[3.5, 4.95], [3.6, 4.90], [3.3, 4.02], [2.7, 3.10], [2.1, 2.37], [2.1, 2.37], [1.8, 1.92]]\n# aspect_ratios = [ [[1, sqrt(2), 1/sqrt(2), sqrt(3), 1/sqrt(3)][:n], [1]] for n in [3, 5, 5, 5, 3, 3, 3] ]\n# conv_sizes = [(35, 35), (18, 18), (9, 9), (5, 5), (3, 3), (2, 2)]\n\nscales = [[1.68, 2.91],\n          [2.95, 2.22, 0.84],\n          [2.23, 2.17, 3.12],\n          [0.76, 1.94, 2.72],\n          [2.10, 2.65],\n          [1.80, 1.92]]\naspect_ratios = [[[0.72, 0.96], [0.68, 1.17]],\n                 [[1.28, 0.66], [0.63, 1.23], [0.89, 1.40]],\n                 [[2.05, 1.24], [0.57, 0.83], [0.61, 1.15]],\n                 [[1.00, 2.21], [0.47, 1.60], [1.44, 0.79]],\n                 [[1.00, 1.41, 0.71, 1.73, 0.58], [1.08]],\n                 [[1.00, 1.41, 0.71, 1.73, 0.58], [1.00]]]\nconv_sizes = [(35, 35), (18, 18), (9, 9), (5, 5), (3, 3), (2, 2)]\n\n# yrm33_config\n# scales = [ [5.3] ] * 5\n# aspect_ratios = [ [[1, 1/sqrt(2), sqrt(2)]] ]*5\n# conv_sizes = [(136, 136), (67, 67), (33, 33), (16, 16), (8, 8)]\n\n\nSMALL = 0\nMEDIUM = 1\nLARGE = 2\n\nif __name__ == \'__main__\':\n        \n    with open(dump_file, \'rb\') as f:\n        bboxes = pickle.load(f)\n\n    sizes = []\n    smalls = []\n    for i in range(len(bboxes)):\n        area = bboxes[i][4] * bboxes[i][5]\n        if area < 32 ** 2:\n            sizes.append(SMALL)\n            smalls.append(area)\n        elif area < 96 ** 2:\n            sizes.append(MEDIUM)\n        else:\n            sizes.append(LARGE)\n\n    # Each box is in the form [im_w, im_h, pos_x, pos_y, size_x, size_y]\n    \n    if use_augmented_boxes:\n        with open(aug_file, \'rb\') as f:\n            bboxes_rel = pickle.load(f)\n    else:\n        bboxes_rel = to_relative(np.array(bboxes))\n        \n\n    with torch.no_grad():\n        sizes = torch.Tensor(sizes)\n\n        anchors = [make_priors(cs, s, ar) for cs, s, ar in zip(conv_sizes, scales, aspect_ratios)]\n        anchors = np.concatenate(anchors, axis=0)\n        anchors = torch.Tensor(anchors).cuda()\n\n        bboxes_rel = torch.Tensor(bboxes_rel).cuda()\n        perGTAnchorMax = torch.zeros(bboxes_rel.shape[0]).cuda()\n\n        chunk_size = 1000\n        for i in range((bboxes_rel.size(0) // chunk_size) + 1):\n            start = i * chunk_size\n            end   = min((i + 1) * chunk_size, bboxes_rel.size(0))\n            \n            ious = jaccard(bboxes_rel[start:end, :], anchors)\n            maxes, maxidx = torch.max(ious, dim=1)\n\n            perGTAnchorMax[start:end] = maxes\n    \n\n        hits = (perGTAnchorMax > 0.5).float()\n\n        print(\'Total recall: %.2f\' % (torch.sum(hits) / hits.size(0) * 100))\n        print()\n\n        for i, metric in zip(range(3), (\'small\', \'medium\', \'large\')):\n            _hits = hits[sizes == i]\n            _size = (1 if _hits.size(0) == 0 else _hits.size(0))\n            print(metric + \' recall: %.2f\' % ((torch.sum(_hits) / _size) * 100))\n\n\n\n'"
scripts/cluster_bbox_sizes.py,0,"b'""""""\nThis script compiles all the bounding boxes in the training data and\nclusters them for each convout resolution on which they\'re used.\n\nRun this script from the Yolact root directory.\n""""""\n\nimport os.path as osp\nimport json, pickle\nimport sys\n\nimport numpy as np\nimport sklearn.cluster as cluster\n\ndump_file = \'weights/bboxes.pkl\'\nmax_size = 550\n\nnum_scale_clusters = 5\nnum_aspect_ratio_clusters = 3\n\ndef to_relative(bboxes):\n\treturn bboxes[:, 2:4] / bboxes[:, :2]\n\ndef process(bboxes):\n\treturn to_relative(bboxes) * max_size\n\nif __name__ == \'__main__\':\n\t\t\n\twith open(dump_file, \'rb\') as f:\n\t\tbboxes = pickle.load(f)\n\n\tbboxes = np.array(bboxes)\n\tbboxes = process(bboxes)\n\tbboxes = bboxes[(bboxes[:, 0] > 1) * (bboxes[:, 1] > 1)]\n\n\tscale  = np.sqrt(bboxes[:, 0] * bboxes[:, 1]).reshape(-1, 1)\n\n\tclusterer = cluster.KMeans(num_scale_clusters, random_state=99, n_jobs=4)\n\tassignments = clusterer.fit_predict(scale)\n\tcounts = np.bincount(assignments)\n\n\tcluster_centers = clusterer.cluster_centers_\n\n\tcenter_indices = list(range(num_scale_clusters))\n\tcenter_indices.sort(key=lambda x: cluster_centers[x, 0])\n\n\tfor idx in center_indices:\n\t\tcenter = cluster_centers[idx, 0]\n\t\tboxes_for_center = bboxes[assignments == idx]\n\t\taspect_ratios = (boxes_for_center[:,0] / boxes_for_center[:,1]).reshape(-1, 1)\n\n\t\tc = cluster.KMeans(num_aspect_ratio_clusters, random_state=idx, n_jobs=4)\n\t\tca = c.fit_predict(aspect_ratios)\n\t\tcc = np.bincount(ca)\n\n\t\tc = list(c.cluster_centers_.reshape(-1))\n\t\tcidx = list(range(num_aspect_ratio_clusters))\n\t\tcidx.sort(key=lambda x: -cc[x])\n\n\t\t# import code\n\t\t# code.interact(local=locals())\n\n\t\tprint(\'%.3f (%d) aspect ratios:\' % (center, counts[idx]))\n\t\tfor idx in cidx:\n\t\t\tprint(\'\\t%.2f (%d)\' % (c[idx], cc[idx]))\n\t\tprint()\n\t\t# exit()\n\n\n'"
scripts/compute_masks.py,7,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport torch\nimport torch.nn.functional as F\n\nCOLORS = ((255, 0, 0, 128), (0, 255, 0, 128), (0, 0, 255, 128),\n          (0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128))\n\ndef mask_iou(mask1, mask2):\n    """"""\n    Inputs inputs are matricies of size _ x N. Output is size _1 x _2.\n    Note: if iscrowd is True, then mask2 should be the crowd.\n    """"""\n    intersection = torch.matmul(mask1, mask2.t())\n    area1 = torch.sum(mask1, dim=1).view(1, -1)\n    area2 = torch.sum(mask2, dim=1).view(1, -1)\n    union = (area1.t() + area2) - intersection\n\n    return intersection / union\n\ndef paint_mask(img_numpy, mask, color):\n\th, w, _ = img_numpy.shape\n\timg_numpy = img_numpy.copy()\n\n\tmask = np.tile(mask.reshape(h, w, 1), (1, 1, 3))\n\tcolor_np = np.array(color[:3]).reshape(1, 1, 3)\n\tcolor_np = np.tile(color_np, (h, w, 1))\n\tmask_color = mask * color_np\n\n\tmask_alpha = 0.3\n\n\t# Blend image and mask\n\timage_crop = img_numpy * mask\n\timg_numpy *= (1-mask)\n\timg_numpy += image_crop * (1-mask_alpha) + mask_color * mask_alpha\n\n\treturn img_numpy\n\n# Inverse sigmoid\ndef logit(x):\n\treturn np.log(x / (1-x + 0.0001) + 0.0001)\n\ndef sigmoid(x):\n\treturn 1 / (1 + np.exp(-x))\n\nimg_fmt = \'../data/coco/images/%012d.jpg\'\nwith open(\'info.txt\', \'r\') as f:\n\timg_id = int(f.read())\n\nimg = plt.imread(img_fmt % img_id).astype(np.float32)\nh, w, _ = img.shape\n\ngt_masks    = np.load(\'gt.npy\').astype(np.float32).transpose(1, 2, 0)\nproto_masks = np.load(\'proto.npy\').astype(np.float32)\n\nproto_masks = torch.Tensor(proto_masks).permute(2, 0, 1).contiguous().unsqueeze(0)\nproto_masks = F.interpolate(proto_masks, (h, w), mode=\'bilinear\', align_corners=False).squeeze(0)\nproto_masks = proto_masks.permute(1, 2, 0).numpy()\n\n# # A x = b\nls_A = proto_masks.reshape(-1, proto_masks.shape[-1])\nls_b = gt_masks.reshape(-1, gt_masks.shape[-1])\n\n# x is size [256, num_gt]\nx = np.linalg.lstsq(ls_A, ls_b, rcond=None)[0]\n\napproximated_masks = (np.matmul(proto_masks, x) > 0.5).astype(np.float32)\n\nnum_gt = approximated_masks.shape[2]\nious = mask_iou(torch.Tensor(approximated_masks.reshape(-1, num_gt).T),\n\t\t\t\ttorch.Tensor(gt_masks.reshape(-1, num_gt).T))\n\nious = [int(ious[i, i].item() * 100) for i in range(num_gt)]\nious.sort(key=lambda x: -x)\n\nprint(ious)\n\ngt_img = img.copy()\n\nfor i in range(num_gt):\n\tgt_img = paint_mask(gt_img, gt_masks[:, :, i], COLORS[i % len(COLORS)])\n\t\nplt.imshow(gt_img / 255)\nplt.title(\'GT\')\nplt.show()\n\nfor i in range(num_gt):\n\timg = paint_mask(img, approximated_masks[:, :, i], COLORS[i % len(COLORS)])\n\nplt.imshow(img / 255)\nplt.title(\'Approximated\')\nplt.show()\n'"
scripts/convert_darknet.py,2,"b""from backbone import DarkNetBackbone\nimport h5py\nimport torch\n\nf = h5py.File('darknet53.h5', 'r')\nm = f['model_weights']\n\nyolo_keys = list(m.keys())\nyolo_keys = [x for x in yolo_keys if len(m[x].keys()) > 0]\nyolo_keys.sort()\n\nsd = DarkNetBackbone().state_dict()\n\nsd_keys = list(sd.keys())\nsd_keys.sort()\n\n# Note this won't work if there are 10 elements in some list but whatever that doesn't happen\nlayer_keys = list(set(['.'.join(x.split('.')[:-2]) for x in sd_keys]))\nlayer_keys.sort()\n\n# print([x for x in sd_keys if x.startswith(layer_keys[0])])\n\nmapping = {\n\t'.0.weight'      : ('conv2d_%d', 'kernel:0'),\n\t'.1.bias'        : ('batch_normalization_%d', 'beta:0'),\n\t'.1.weight'      : ('batch_normalization_%d', 'gamma:0'),\n\t'.1.running_var' : ('batch_normalization_%d', 'moving_variance:0'),\n\t'.1.running_mean': ('batch_normalization_%d', 'moving_mean:0'),\n\t'.1.num_batches_tracked': None,\n}\n\nfor i, layer_key in zip(range(1, len(layer_keys) + 1), layer_keys):\n\t# This is pretty inefficient but I don't care\n\tfor weight_key in [x for x in sd_keys if x.startswith(layer_key)]:\n\t\tdiff = weight_key[len(layer_key):]\n\t\t\n\t\tif mapping[diff] is not None:\n\t\t\tyolo_key = mapping[diff][0] % i\n\t\t\tsub_key  = mapping[diff][1]\n\n\t\t\tyolo_weight = torch.Tensor(m[yolo_key][yolo_key][sub_key].value)\n\t\t\tif (len(yolo_weight.size()) == 4):\n\t\t\t\tyolo_weight = yolo_weight.permute(3, 2, 0, 1).contiguous()\n\t\t\t\n\t\t\tsd[weight_key] = yolo_weight\n\ntorch.save(sd, 'weights/darknet53.pth')\n\n"""
scripts/convert_sbd.py,0,"b""import scipy.io, scipy.ndimage\nimport os.path, json\nimport pycocotools.mask\nimport numpy as np\n\ndef mask2bbox(mask):\n    rows = np.any(mask, axis=1)\n    cols = np.any(mask, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n\n    return cmin, rmin, cmax - cmin, rmax - rmin\n\n\n\ninst_path = './inst/'\nimg_path  = './img/'\nimg_name_fmt = '%s.jpg'\nann_name_fmt = '%s.mat'\n\nimage_id = 1\nann_id   = 1\n\ntypes = ['train', 'val']\n\nfor t in types:\n    with open('%s.txt' % t, 'r') as f:\n        names = f.read().strip().split('\\n')\n\n    images = []\n    annotations = []\n\n    for name in names:\n        img_name = img_name_fmt % name\n\n        ann_path = os.path.join(inst_path, ann_name_fmt % name)\n        ann = scipy.io.loadmat(ann_path)['GTinst'][0][0]\n\n        classes = [int(x[0]) for x in ann[2]]\n        seg = ann[0]\n\n        for idx in range(len(classes)):\n            mask = (seg == (idx + 1)).astype(np.float)\n\n            rle = pycocotools.mask.encode(np.asfortranarray(mask.astype(np.uint8)))\n            rle['counts'] = rle['counts'].decode('ascii')\n\n            annotations.append({\n                'id': ann_id,\n                'image_id': image_id,\n                'category_id': classes[idx],\n                'segmentation': rle,\n                'area': float(mask.sum()),\n                'bbox': [int(x) for x in mask2bbox(mask)],\n                'iscrowd': 0\n            })\n\n            ann_id += 1\n        \n        img_name = img_name_fmt % name\n        img = scipy.ndimage.imread(os.path.join(img_path, img_name))\n\n        images.append({\n            'id': image_id,\n            'width': img.shape[1],\n            'height': img.shape[0],\n            'file_name': img_name\n        })\n\n        image_id += 1\n\n    info = {\n        'year': 2012,\n        'version': 1,\n        'description': 'Pascal SBD',\n    }\n\n    categories = [{'id': x+1} for x in range(20)]\n\n    with open('pascal_sbd_%s.json' % t, 'w') as f:\n        json.dump({\n            'info': info,\n            'images': images,\n            'annotations': annotations,\n            'licenses': {},\n            'categories': categories\n        }, f)\n\n"""
scripts/make_grid.py,0,"b'import numpy as np\nimport math, random\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider, Button\n\n\nfig, ax = plt.subplots()\nplt.subplots_adjust(bottom=0.24)\nim_handle = None\n\nsave_path = \'grid.npy\'\n\ncenter_x, center_y = (0.5, 0.5)\ngrid_w, grid_h = (35, 35)\nspacing = 0\nscale = 4\nangle = 0\ngrid = None\n\nall_grids = []\nunique = False\n\n# A hack\ndisable_render = False\n\ndef render():\n\tif disable_render:\n\t\treturn\n\n\tx = np.tile(np.array(list(range(grid_w)), dtype=np.float).reshape(1, grid_w), [grid_h, 1]) - grid_w * center_x\n\ty = np.tile(np.array(list(range(grid_h)), dtype=np.float).reshape(grid_h, 1), [1, grid_w]) - grid_h * center_y\n\n\tx /= scale\n\ty /= scale\n\n\ta1 =  angle + math.pi / 3\n\ta2 = -angle + math.pi / 3\n\ta3 =  angle\n\n\tz1 = x * math.sin(a1) + y * math.cos(a1)\n\tz2 = x * math.sin(a2) - y * math.cos(a2)\n\tz3 = x * math.sin(a3) + y * math.cos(a3)\n\n\ts1 = np.square(np.sin(z1))\n\ts2 = np.square(np.sin(z2))\n\ts3 = np.square(np.sin(z3))\n\n\tline_1 = np.exp(s1 * spacing) * s1\n\tline_2 = np.exp(s2 * spacing) * s2\n\tline_3 = np.exp(s3 * spacing) * s3\n\n\tglobal grid\n\tgrid = np.clip(1 - (line_1 + line_2 + line_3) / 3, 0, 1)\n\n\tglobal im_handle\n\tif im_handle is None:\n\t\tim_handle = plt.imshow(grid)\n\telse:\n\t\tim_handle.set_data(grid)\n\tfig.canvas.draw_idle()\n\ndef update_scale(val):\n\tglobal scale\n\tscale = val\n\n\trender()\n\ndef update_angle(val):\n\tglobal angle\n\tangle = val\n\n\trender()\n\ndef update_centerx(val):\n\tglobal center_x\n\tcenter_x = val\n\n\trender()\n\ndef update_centery(val):\n\tglobal center_y\n\tcenter_y = val\n\n\trender()\n\ndef update_spacing(val):\n\tglobal spacing\n\tspacing = val\n\n\trender()\n\ndef randomize(val):\n\tglobal center_x, center_y, spacing, scale, angle, disable_render\n\n\tcenter_x, center_y = (random.uniform(0, 1), random.uniform(0, 1))\n\tspacing = random.uniform(-0.2, 2)\n\tscale = 4 * math.exp(random.uniform(-1, 1))\n\tangle = random.uniform(-math.pi, math.pi)\n\n\tdisable_render = True\n\n\tscale_slider.set_val(scale)\n\tangle_slider.set_val(angle)\n\tcentx_slider.set_val(center_x)\n\tcenty_slider.set_val(center_y)\n\tspaci_slider.set_val(spacing)\n\n\tdisable_render = False\n\n\trender()\n\ndef add(val):\n\tall_grids.append(grid)\n\n\tglobal unique\n\tif not unique:\n\t\tunique = test_uniqueness(np.stack(all_grids))\n\t\n\texport_len_text.set_text(\'Num Grids: \' + str(len(all_grids)))\n\tfig.canvas.draw_idle()\n\ndef add_randomize(val):\n\tadd(val)\n\trandomize(val)\n\ndef export(val):\n\tnp.save(save_path, np.stack(all_grids))\n\tprint(\'Saved %d grids to ""%s""\' % (len(all_grids), save_path))\n\n\tglobal unique\n\tunique = False\n\tall_grids.clear()\n\n\texport_len_text.set_text(\'Num Grids: \' + str(len(all_grids)))\n\tfig.canvas.draw_idle()\n\ndef test_uniqueness(grids):\n\t# Grids shape [ngrids, h, w]\n\tgrids = grids.reshape((-1, grid_h, grid_w))\n\n\tfor y in range(grid_h):\n\t\tfor x in range(grid_h):\n\t\t\tpixel_features = grids[:, y, x]\n\n\t\t\t# l1 distance for this pixel with every other\n\t\t\tl1_dist = np.sum(np.abs(grids - np.tile(pixel_features, grid_h*grid_w).reshape((-1, grid_h, grid_w))), axis=0)\n\n\t\t\t# Equal if l1 distance is really small. Note that this will include this pixel\n\t\t\tnum_equal = np.sum((l1_dist < 0.0001).astype(np.int32))\n\n\t\t\tif num_equal > 1:\n\t\t\t\tprint(\'Pixel at (%d, %d) has %d other pixel%s with the same representation.\' % (x, y, num_equal-1, \'\' if num_equal==2 else \'s\'))\n\t\t\t\treturn False\n\t\n\tprint(\'Each pixel has a distinct representation.\')\n\treturn True\n\n\n\nrender()\n\naxis = plt.axes([0.22, 0.19, 0.59, 0.03], facecolor=\'lightgoldenrodyellow\')\nscale_slider = Slider(axis, \'Scale\', 0.1, 20, valinit=scale, valstep=0.1)\nscale_slider.on_changed(update_scale)\n\naxis = plt.axes([0.22, 0.15, 0.59, 0.03], facecolor=\'lightgoldenrodyellow\')\nangle_slider = Slider(axis, \'Angle\', -math.pi, math.pi, valinit=angle, valstep=0.1)\nangle_slider.on_changed(update_angle)\n\naxis = plt.axes([0.22, 0.11, 0.59, 0.03], facecolor=\'lightgoldenrodyellow\')\ncentx_slider = Slider(axis, \'Center X\', 0, 1, valinit=center_x, valstep=0.05)\ncentx_slider.on_changed(update_centerx)\n\naxis = plt.axes([0.22, 0.07, 0.59, 0.03], facecolor=\'lightgoldenrodyellow\')\ncenty_slider = Slider(axis, \'Center Y\', 0, 1, valinit=center_y, valstep=0.05)\ncenty_slider.on_changed(update_centery)\n\naxis = plt.axes([0.22, 0.03, 0.59, 0.03], facecolor=\'lightgoldenrodyellow\')\nspaci_slider = Slider(axis, \'Spacing\', -1, 2, valinit=spacing, valstep=0.05)\nspaci_slider.on_changed(update_spacing)\n\naxis = plt.axes([0.8, 0.54, 0.15, 0.05], facecolor=\'lightgoldenrodyellow\')\nrando_button = Button(axis, \'Randomize\')\nrando_button.on_clicked(randomize)\n\naxis = plt.axes([0.8, 0.48, 0.15, 0.05], facecolor=\'lightgoldenrodyellow\')\naddgr_button = Button(axis, \'Add\')\naddgr_button.on_clicked(add)\n\n# Likely not a good way to do this but whatever\nexport_len_text = plt.text(0, 3, \'Num Grids: 0\')\n\naxis = plt.axes([0.8, 0.42, 0.15, 0.05], facecolor=\'lightgoldenrodyellow\')\naddra_button = Button(axis, \'Add / Rand\')\naddra_button.on_clicked(add_randomize)\n\naxis = plt.axes([0.8, 0.36, 0.15, 0.05], facecolor=\'lightgoldenrodyellow\')\nsaveg_button = Button(axis, \'Save\')\nsaveg_button.on_clicked(export)\n\n\n\nplt.show()\n'"
scripts/optimize_bboxes.py,9,"b'""""""\nInstead of clustering bbox widths and heights, this script\ndirectly optimizes average IoU across the training set given\nthe specified number of anchor boxes.\n\nRun this script from the Yolact root directory.\n""""""\n\nimport pickle\nimport random\nfrom itertools import product\nfrom math import sqrt\n\nimport numpy as np\nimport torch\nfrom scipy.optimize import minimize\n\ndump_file = \'weights/bboxes.pkl\'\naug_file  = \'weights/bboxes_aug.pkl\'\n\nuse_augmented_boxes = True\n\n\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b, iscrowd=False):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes. If iscrowd=True, put the crowd in box_b.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n\n    if iscrowd:\n        return inter / area_a\n    else:\n        return inter / union  # [A,B]\n\n# Also convert to point form\ndef to_relative(bboxes):\n    return np.concatenate((bboxes[:, 2:4] / bboxes[:, :2], (bboxes[:, 2:4] + bboxes[:, 4:]) / bboxes[:, :2]), axis=1)\n\n\ndef make_priors(conv_size, scales, aspect_ratios):\n    prior_data = []\n    conv_h = conv_size[0]\n    conv_w = conv_size[1]\n\n    # Iteration order is important (it has to sync up with the convout)\n    for j, i in product(range(conv_h), range(conv_w)):\n        x = (i + 0.5) / conv_w\n        y = (j + 0.5) / conv_h\n        \n        for scale, ars in zip(scales, aspect_ratios):\n            for ar in ars:\n                w = scale * ar / conv_w\n                h = scale / ar / conv_h\n\n                # Point form\n                prior_data += [x - w/2, y - h/2, x + w/2, y + h/2]\n    return torch.Tensor(prior_data).view(-1, 4).cuda()\n\n\n\nscales = [[1.68, 2.91], [2.95, 2.22, 0.84], [2.17, 2.22, 3.22], [0.76, 2.06, 2.81], [5.33, 2.79], [13.69]]\naspect_ratios = [[[0.72, 0.96], [0.68, 1.17]], [[1.30, 0.66], [0.63, 1.23], [0.87, 1.41]], [[1.96, 1.23], [0.58, 0.84], [0.61, 1.15]], [[19.79, 2.21], [0.47, 1.76], [1.38, 0.79]], [[4.79, 17.96], [1.04]], [[14.82]]]\nconv_sizes = [(35, 35), (18, 18), (9, 9), (5, 5), (3, 3), (2, 2)]\n\noptimize_scales = False\n\nbatch_idx = 0\n\n\ndef compute_hits(bboxes, anchors, iou_threshold=0.5):\n    ious = jaccard(bboxes, anchors)\n    perGTAnchorMax, _ = torch.max(ious, dim=1)\n    \n    return (perGTAnchorMax > iou_threshold)\n\ndef compute_recall(hits, base_hits):\n    hits = (hits | base_hits).float()\n    return torch.sum(hits) / hits.size(0)\n\n\ndef step(x, x_func, bboxes, base_hits, optim_idx):\n    # This should set the scale and aspect ratio\n    x_func(x, scales[optim_idx], aspect_ratios[optim_idx])\n\n    anchors = make_priors(conv_sizes[optim_idx], scales[optim_idx], aspect_ratios[optim_idx])\n\n    return -float(compute_recall(compute_hits(bboxes, anchors), base_hits).cpu())\n\n\ndef optimize(full_bboxes, optim_idx, batch_size=5000):\n    global batch_idx, scales, aspect_ratios, conv_sizes\n\n    start = batch_idx * batch_size\n    end   = min((batch_idx + 1) * batch_size, full_bboxes.size(0))\n\n    if batch_idx > (full_bboxes.size(0) // batch_size):\n        batch_idx = 0\n\n    bboxes = full_bboxes[start:end, :]\n\n    anchor_base = [\n        make_priors(conv_sizes[idx], scales[idx], aspect_ratios[idx])\n            for idx in range(len(conv_sizes)) if idx != optim_idx]\n    base_hits = compute_hits(bboxes, torch.cat(anchor_base, dim=0))\n    \n    \n    def set_x(x, scales, aspect_ratios):\n        if optimize_scales:\n            for i in range(len(scales)):\n                scales[i] = max(x[i], 0)\n        else:\n            k = 0\n            for i in range(len(aspect_ratios)):\n                for j in range(len(aspect_ratios[i])):\n                    aspect_ratios[i][j] = x[k]\n                    k += 1\n            \n\n    res = minimize(step, x0=scales[optim_idx] if optimize_scales else sum(aspect_ratios[optim_idx], []), method=\'Powell\',\n        args = (set_x, bboxes, base_hits, optim_idx),)\n\n\ndef pretty_str(x:list):\n    if isinstance(x, list):\n        return \'[\' + \', \'.join([pretty_str(y) for y in x]) + \']\'\n    elif isinstance(x, np.ndarray):\n        return pretty_str(list(x))\n    else:\n        return \'%.2f\' % x\n\nif __name__ == \'__main__\':\n    \n    if use_augmented_boxes:\n        with open(aug_file, \'rb\') as f:\n            bboxes = pickle.load(f)\n    else:\n        # Load widths and heights from a dump file. Obtain this with\n        # python3 scripts/save_bboxes.py\n        with open(dump_file, \'rb\') as f:\n            bboxes = pickle.load(f)\n\n            bboxes = np.array(bboxes)\n            bboxes = to_relative(bboxes)\n\n    with torch.no_grad():\n        bboxes = torch.Tensor(bboxes).cuda()\n        \n        def print_out():\n            if optimize_scales:\n                print(\'Scales: \' + pretty_str(scales))\n            else:\n                print(\'Aspect Ratios: \' + pretty_str(aspect_ratios))\n\n        for p in range(10):\n            print(\'(Sub Iteration) \', end=\'\')\n            for i in range(len(conv_sizes)):\n                print(\'%d \' % i, end=\'\', flush=True)\n                optimize(bboxes, i)\n            print(\'Done\', end=\'\\r\')\n            \n            print(\'(Iteration %d) \' % p, end=\'\')\n            print_out()\n            print()\n\n            optimize_scales = not optimize_scales\n        \n        print(\'scales = \' + pretty_str(scales))\n        print(\'aspect_ratios = \' + pretty_str(aspect_ratios))\n\n\n'"
scripts/parse_eval.py,0,"b""import re, sys, os\nimport matplotlib.pyplot as plt\nfrom matplotlib._color_data import XKCD_COLORS\n\nwith open(sys.argv[1], 'r') as f:\n\ttxt = f.read()\n\ntxt, overall = txt.split('overall performance')\n\nclass_names = []\nmAP_overall = []\nmAP_small   = []\nmAP_medium  = []\nmAP_large   = []\n\nfor class_result in txt.split('evaluate category: ')[1:]:\n\tlines = class_result.split('\\n')\n\tclass_names.append(lines[0])\n\n\tdef grabMAP(string):\n\t\treturn float(string.split('] = ')[1]) * 100\n\t\n\tmAP_overall.append(grabMAP(lines[ 7]))\n\tmAP_small  .append(grabMAP(lines[10]))\n\tmAP_medium .append(grabMAP(lines[11]))\n\tmAP_large  .append(grabMAP(lines[12]))\n\nmAP_map = {\n\t'small': mAP_small,\n\t'medium': mAP_medium,\n\t'large': mAP_large,\n}\n\nif len(sys.argv) > 2:\n\tbars = plt.bar(class_names, mAP_map[sys.argv[2]])\n\tplt.title(sys.argv[2] + ' mAP per class')\nelse:\n\tbars = plt.bar(class_names, mAP_overall)\n\tplt.title('overall mAP per class')\n\ncolors = list(XKCD_COLORS.values())\n\nfor idx, bar in enumerate(bars):\n\t# Mmm pseudorandom colors\n\tchar_sum = sum([ord(char) for char in class_names[idx]])\n\tbar.set_color(colors[char_sum % len(colors)])\n\nplt.xticks(rotation='vertical')\nplt.show()\n"""
scripts/plot_loss.py,0,"b""import re, sys, os\nimport matplotlib.pyplot as plt\n\nfrom utils.functions import MovingAverage\n\nwith open(sys.argv[1], 'r') as f:\n\tinp = f.read()\n\npatterns = {\n\t'train': re.compile(r'\\[\\s*(?P<epoch>\\d+)\\]\\s*(?P<iteration>\\d+) \\|\\| B: (?P<b>\\S+) \\| C: (?P<c>\\S+) \\| M: (?P<m>\\S+) \\|( S: (?P<s>\\S+) \\|)? T: (?P<t>\\S+)'),\n\t'val': re.compile(r'\\s*(?P<type>[a-z]+) \\|\\s*(?P<all>\\S+)')\n}\ndata = {key: [] for key in patterns}\n\nfor line in inp.split('\\n'):\n\tfor key, pattern in patterns.items():\n\t\tf = pattern.search(line)\n\t\t\n\t\tif f is not None:\n\t\t\tdatum = f.groupdict()\n\t\t\tfor k, v in datum.items():\n\t\t\t\tif v is not None:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tv = float(v)\n\t\t\t\t\texcept ValueError:\n\t\t\t\t\t\tpass\n\t\t\t\t\tdatum[k] = v\n\t\t\t\n\t\t\tif key == 'val':\n\t\t\t\tdatum = (datum, data['train'][-1])\n\t\t\tdata[key].append(datum)\n\t\t\tbreak\n\n\ndef smoother(y, interval=100):\n\tavg = MovingAverage(interval)\n\n\tfor i in range(len(y)):\n\t\tavg.append(y[i])\n\t\ty[i] = avg.get_avg()\n\t\n\treturn y\n\ndef plot_train(data):\n\tplt.title(os.path.basename(sys.argv[1]) + ' Training Loss')\n\tplt.xlabel('Iteration')\n\tplt.ylabel('Loss')\n\n\tloss_names = ['BBox Loss', 'Conf Loss', 'Mask Loss']\n\n\tx = [x['iteration'] for x in data]\n\tplt.plot(x, smoother([y['b'] for y in data]))\n\tplt.plot(x, smoother([y['c'] for y in data]))\n\tplt.plot(x, smoother([y['m'] for y in data]))\n\n\tif data[0]['s'] is not None:\n\t\tplt.plot(x, smoother([y['s'] for y in data]))\n\t\tloss_names.append('Segmentation Loss')\n\n\tplt.legend(loss_names)\n\tplt.show()\n\ndef plot_val(data):\n\tplt.title(os.path.basename(sys.argv[1]) + ' Validation mAP')\n\tplt.xlabel('Epoch')\n\tplt.ylabel('mAP')\n\n\tx = [x[1]['epoch'] for x in data if x[0]['type'] == 'box']\n\tplt.plot(x, [x[0]['all'] for x in data if x[0]['type'] == 'box'])\n\tplt.plot(x, [x[0]['all'] for x in data if x[0]['type'] == 'mask'])\n\n\tplt.legend(['BBox mAP', 'Mask mAP'])\n\tplt.show()\n\nif len(sys.argv) > 2 and sys.argv[2] == 'val':\n\tplot_val(data['val'])\nelse:\n\tplot_train(data['train'])\n"""
scripts/save_bboxes.py,0,"b'"""""" This script transforms and saves bbox coordinates into a pickle object for easy loading. """"""\n\n\nimport os.path as osp\nimport json, pickle\nimport sys\n\nimport numpy as np\n\nCOCO_ROOT = osp.join(\'.\', \'data/coco/\')\n\nannotation_file = \'instances_train2017.json\'\nannotation_path = osp.join(COCO_ROOT, \'annotations/\', annotation_file)\n\ndump_file = \'weights/bboxes.pkl\'\n\nwith open(annotation_path, \'r\') as f:\n\tannotations_json = json.load(f)\n\nannotations = annotations_json[\'annotations\']\nimages = annotations_json[\'images\']\nimages = {image[\'id\']: image for image in images}\nbboxes = []\n\nfor ann in annotations:\n\timage = images[ann[\'image_id\']]\n\tw,h = (image[\'width\'], image[\'height\'])\n\t\n\tif \'bbox\' in ann:\n\t\tbboxes.append([w, h] + ann[\'bbox\'])\n\nwith open(dump_file, \'wb\') as f:\n\tpickle.dump(bboxes, f)\n'"
scripts/unpack_statedict.py,2,"b""import torch\nimport sys, os\n\n# Usage python scripts/unpack_statedict.py path_to_pth out_folder/\n# Make sure to include that slash after your out folder, since I can't\n# be arsed to do path concatenation so I'd rather type out this comment\n\nprint('Loading state dict...')\nstate = torch.load(sys.argv[1])\n\nif not os.path.exists(sys.argv[2]):\n\tos.mkdir(sys.argv[2])\n\nprint('Saving stuff...')\nfor key, val in state.items():\n\ttorch.save(val, sys.argv[2] + key)\n"""
utils/__init__.py,0,b'from .augmentations import SSDAugmentation'
utils/augmentations.py,6,"b'import torch\nfrom torchvision import transforms\nimport cv2\nimport numpy as np\nimport types\nfrom numpy import random\nfrom math import sqrt\n\nfrom data import cfg, MEANS, STD\n\n\ndef intersect(box_a, box_b):\n    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n    return inter[:, 0] * inter[:, 1]\n\n\ndef jaccard_numpy(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n        box_b: Single bounding box, Shape: [4]\n    Return:\n        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1]))  # [A,B]\n    area_b = ((box_b[2]-box_b[0]) *\n              (box_b[3]-box_b[1]))  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\nclass Compose(object):\n    """"""Composes several augmentations together.\n    Args:\n        transforms (List[Transform]): list of transforms to compose.\n    Example:\n        >>> augmentations.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, masks=None, boxes=None, labels=None):\n        for t in self.transforms:\n            img, masks, boxes, labels = t(img, masks, boxes, labels)\n        return img, masks, boxes, labels\n\n\nclass Lambda(object):\n    """"""Applies a lambda as a transform.""""""\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, img, masks=None, boxes=None, labels=None):\n        return self.lambd(img, masks, boxes, labels)\n\n\nclass ConvertFromInts(object):\n    def __call__(self, image, masks=None, boxes=None, labels=None):\n        return image.astype(np.float32), masks, boxes, labels\n\n\n\nclass ToAbsoluteCoords(object):\n    def __call__(self, image, masks=None, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] *= width\n        boxes[:, 2] *= width\n        boxes[:, 1] *= height\n        boxes[:, 3] *= height\n\n        return image, masks, boxes, labels\n\n\nclass ToPercentCoords(object):\n    def __call__(self, image, masks=None, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] /= width\n        boxes[:, 2] /= width\n        boxes[:, 1] /= height\n        boxes[:, 3] /= height\n\n        return image, masks, boxes, labels\n\n\nclass Pad(object):\n    """"""\n    Pads the image to the input width and height, filling the\n    background with mean and putting the image in the top-left.\n\n    Note: this expects im_w <= width and im_h <= height\n    """"""\n    def __init__(self, width, height, mean=MEANS, pad_gt=True):\n        self.mean = mean\n        self.width = width\n        self.height = height\n        self.pad_gt = pad_gt\n\n    def __call__(self, image, masks, boxes=None, labels=None):\n        im_h, im_w, depth = image.shape\n\n        expand_image = np.zeros(\n            (self.height, self.width, depth),\n            dtype=image.dtype)\n        expand_image[:, :, :] = self.mean\n        expand_image[:im_h, :im_w] = image\n\n        if self.pad_gt:\n            expand_masks = np.zeros(\n                (masks.shape[0], self.height, self.width),\n                dtype=masks.dtype)\n            expand_masks[:,:im_h,:im_w] = masks\n            masks = expand_masks\n\n        return expand_image, masks, boxes, labels\n\nclass Resize(object):\n    """""" If preserve_aspect_ratio is true, this resizes to an approximate area of max_size * max_size """"""\n\n    @staticmethod\n    def calc_size_preserve_ar(img_w, img_h, max_size):\n        """""" I mathed this one out on the piece of paper. Resulting width*height = approx max_size^2 """"""\n        ratio = sqrt(img_w / img_h)\n        w = max_size * ratio\n        h = max_size / ratio\n        return int(w), int(h)\n\n    def __init__(self, resize_gt=True):\n        self.resize_gt = resize_gt\n        self.max_size = cfg.max_size\n        self.preserve_aspect_ratio = cfg.preserve_aspect_ratio\n\n    def __call__(self, image, masks, boxes, labels=None):\n        img_h, img_w, _ = image.shape\n        \n        if self.preserve_aspect_ratio:\n            width, height = Resize.calc_size_preserve_ar(img_w, img_h, self.max_size)\n        else:\n            width, height = self.max_size, self.max_size\n\n        image = cv2.resize(image, (width, height))\n\n        if self.resize_gt:\n            # Act like each object is a color channel\n            masks = masks.transpose((1, 2, 0))\n            masks = cv2.resize(masks, (width, height))\n            \n            # OpenCV resizes a (w,h,1) array to (s,s), so fix that\n            if len(masks.shape) == 2:\n                masks = np.expand_dims(masks, 0)\n            else:\n                masks = masks.transpose((2, 0, 1))\n\n            # Scale bounding boxes (which are currently absolute coordinates)\n            boxes[:, [0, 2]] *= (width  / img_w)\n            boxes[:, [1, 3]] *= (height / img_h)\n\n        # Discard boxes that are smaller than we\'d like\n        w = boxes[:, 2] - boxes[:, 0]\n        h = boxes[:, 3] - boxes[:, 1]\n\n        keep = (w > cfg.discard_box_width) * (h > cfg.discard_box_height)\n        masks = masks[keep]\n        boxes = boxes[keep]\n        labels[\'labels\'] = labels[\'labels\'][keep]\n        labels[\'num_crowds\'] = (labels[\'labels\'] < 0).sum()\n\n        return image, masks, boxes, labels\n\n\nclass RandomSaturation(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    def __call__(self, image, masks=None, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n\n        return image, masks, boxes, labels\n\n\nclass RandomHue(object):\n    def __init__(self, delta=18.0):\n        assert delta >= 0.0 and delta <= 360.0\n        self.delta = delta\n\n    def __call__(self, image, masks=None, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n        return image, masks, boxes, labels\n\n\nclass RandomLightingNoise(object):\n    def __init__(self):\n        self.perms = ((0, 1, 2), (0, 2, 1),\n                      (1, 0, 2), (1, 2, 0),\n                      (2, 0, 1), (2, 1, 0))\n\n    def __call__(self, image, masks=None, boxes=None, labels=None):\n        # Don\'t shuffle the channels please, why would you do this\n\n        # if random.randint(2):\n        #     swap = self.perms[random.randint(len(self.perms))]\n        #     shuffle = SwapChannels(swap)  # shuffle channels\n        #     image = shuffle(image)\n        return image, masks, boxes, labels\n\n\nclass ConvertColor(object):\n    def __init__(self, current=\'BGR\', transform=\'HSV\'):\n        self.transform = transform\n        self.current = current\n\n    def __call__(self, image, masks=None, boxes=None, labels=None):\n        if self.current == \'BGR\' and self.transform == \'HSV\':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        elif self.current == \'HSV\' and self.transform == \'BGR\':\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n        else:\n            raise NotImplementedError\n        return image, masks, boxes, labels\n\n\nclass RandomContrast(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    # expects float image\n    def __call__(self, image, masks=None, boxes=None, labels=None):\n        if random.randint(2):\n            alpha = random.uniform(self.lower, self.upper)\n            image *= alpha\n        return image, masks, boxes, labels\n\n\nclass RandomBrightness(object):\n    def __init__(self, delta=32):\n        assert delta >= 0.0\n        assert delta <= 255.0\n        self.delta = delta\n\n    def __call__(self, image, masks=None, boxes=None, labels=None):\n        if random.randint(2):\n            delta = random.uniform(-self.delta, self.delta)\n            image += delta\n        return image, masks, boxes, labels\n\n\nclass ToCV2Image(object):\n    def __call__(self, tensor, masks=None, boxes=None, labels=None):\n        return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), masks, boxes, labels\n\n\nclass ToTensor(object):\n    def __call__(self, cvimage, masks=None, boxes=None, labels=None):\n        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), masks, boxes, labels\n\n\nclass RandomSampleCrop(object):\n    """"""Crop\n    Arguments:\n        img (Image): the image being input during training\n        boxes (Tensor): the original bounding boxes in pt form\n        labels (Tensor): the class labels for each bbox\n        mode (float tuple): the min and max jaccard overlaps\n    Return:\n        (img, boxes, classes)\n            img (Image): the cropped image\n            boxes (Tensor): the adjusted bounding boxes in pt form\n            labels (Tensor): the class labels for each bbox\n    """"""\n    def __init__(self):\n        self.sample_options = (\n            # using entire original input image\n            None,\n            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n            (0.1, None),\n            (0.3, None),\n            (0.7, None),\n            (0.9, None),\n            # randomly sample a patch\n            (None, None),\n        )\n\n    def __call__(self, image, masks, boxes=None, labels=None):\n        height, width, _ = image.shape\n        while True:\n            # randomly choose a mode\n            mode = random.choice(self.sample_options)\n            if mode is None:\n                return image, masks, boxes, labels\n\n            min_iou, max_iou = mode\n            if min_iou is None:\n                min_iou = float(\'-inf\')\n            if max_iou is None:\n                max_iou = float(\'inf\')\n\n            # max trails (50)\n            for _ in range(50):\n                current_image = image\n\n                w = random.uniform(0.3 * width, width)\n                h = random.uniform(0.3 * height, height)\n\n                # aspect ratio constraint b/t .5 & 2\n                if h / w < 0.5 or h / w > 2:\n                    continue\n\n                left = random.uniform(width - w)\n                top = random.uniform(height - h)\n\n                # convert to integer rect x1,y1,x2,y2\n                rect = np.array([int(left), int(top), int(left+w), int(top+h)])\n\n                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n                overlap = jaccard_numpy(boxes, rect)\n\n                # This piece of code is bugged and does nothing:\n                # https://github.com/amdegroot/ssd.pytorch/issues/68\n                #\n                # However, when I fixed it with overlap.max() < min_iou,\n                # it cut the mAP in half (after 8k iterations). So it stays.\n                #\n                # is min and max overlap constraint satisfied? if not try again\n                if overlap.min() < min_iou and max_iou < overlap.max():\n                    continue\n\n                # cut the crop from the image\n                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],\n                                              :]\n\n                # keep overlap with gt box IF center in sampled patch\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n\n                # mask in all gt boxes that above and to the left of centers\n                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n\n                # mask in all gt boxes that under and to the right of centers\n                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n\n                # mask in that both m1 and m2 are true\n                mask = m1 * m2\n\n                # [0 ... 0 for num_gt and then 1 ... 1 for num_crowds]\n                num_crowds = labels[\'num_crowds\']\n                crowd_mask = np.zeros(mask.shape, dtype=np.int32)\n\n                if num_crowds > 0:\n                    crowd_mask[-num_crowds:] = 1\n\n                # have any valid boxes? try again if not\n                # Also make sure you have at least one regular gt\n                if not mask.any() or np.sum(1-crowd_mask[mask]) == 0:\n                    continue\n\n                # take only the matching gt masks\n                current_masks = masks[mask, :, :].copy()\n\n                # take only matching gt boxes\n                current_boxes = boxes[mask, :].copy()\n\n                # take only matching gt labels\n                labels[\'labels\'] = labels[\'labels\'][mask]\n                current_labels = labels\n\n                # We now might have fewer crowd annotations\n                if num_crowds > 0:\n                    labels[\'num_crowds\'] = np.sum(crowd_mask[mask])\n\n                # should we use the box left and top corner or the crop\'s\n                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n                                                  rect[:2])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, :2] -= rect[:2]\n\n                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n                                                  rect[2:])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, 2:] -= rect[:2]\n\n                # crop the current masks to the same dimensions as the image\n                current_masks = current_masks[:, rect[1]:rect[3], rect[0]:rect[2]]\n\n                return current_image, current_masks, current_boxes, current_labels\n\n\nclass Expand(object):\n    def __init__(self, mean):\n        self.mean = mean\n\n    def __call__(self, image, masks, boxes, labels):\n        if random.randint(2):\n            return image, masks, boxes, labels\n\n        height, width, depth = image.shape\n        ratio = random.uniform(1, 4)\n        left = random.uniform(0, width*ratio - width)\n        top = random.uniform(0, height*ratio - height)\n\n        expand_image = np.zeros(\n            (int(height*ratio), int(width*ratio), depth),\n            dtype=image.dtype)\n        expand_image[:, :, :] = self.mean\n        expand_image[int(top):int(top + height),\n                     int(left):int(left + width)] = image\n        image = expand_image\n\n        expand_masks = np.zeros(\n            (masks.shape[0], int(height*ratio), int(width*ratio)),\n            dtype=masks.dtype)\n        expand_masks[:,int(top):int(top + height),\n                       int(left):int(left + width)] = masks\n        masks = expand_masks\n\n        boxes = boxes.copy()\n        boxes[:, :2] += (int(left), int(top))\n        boxes[:, 2:] += (int(left), int(top))\n\n        return image, masks, boxes, labels\n\n\nclass RandomMirror(object):\n    def __call__(self, image, masks, boxes, labels):\n        _, width, _ = image.shape\n        if random.randint(2):\n            image = image[:, ::-1]\n            masks = masks[:, :, ::-1]\n            boxes = boxes.copy()\n            boxes[:, 0::2] = width - boxes[:, 2::-2]\n        return image, masks, boxes, labels\n\n\nclass RandomFlip(object):\n    def __call__(self, image, masks, boxes, labels):\n        height , _ , _ = image.shape\n        if random.randint(2):\n            image = image[::-1, :]\n            masks = masks[:, ::-1, :]\n            boxes = boxes.copy()\n            boxes[:, 1::2] = height - boxes[:, 3::-2]\n        return image, masks, boxes, labels\n\n\nclass RandomRot90(object):\n    def __call__(self, image, masks, boxes, labels):\n        old_height , old_width , _ = image.shape\n        k = random.randint(4)\n        image = np.rot90(image,k)\n        masks = np.array([np.rot90(mask,k) for mask in masks])\n        boxes = boxes.copy()\n        for _ in range(k):\n            boxes = np.array([[box[1], old_width - 1 - box[2], box[3], old_width - 1 - box[0]] for box in boxes])\n            old_width, old_height = old_height, old_width\n        return image, masks, boxes, labels\n\n\nclass SwapChannels(object):\n    """"""Transforms a tensorized image by swapping the channels in the order\n     specified in the swap tuple.\n    Args:\n        swaps (int triple): final order of channels\n            eg: (2, 1, 0)\n    """"""\n\n    def __init__(self, swaps):\n        self.swaps = swaps\n\n    def __call__(self, image):\n        """"""\n        Args:\n            image (Tensor): image tensor to be transformed\n        Return:\n            a tensor with channels swapped according to swap\n        """"""\n        # if torch.is_tensor(image):\n        #     image = image.data.cpu().numpy()\n        # else:\n        #     image = np.array(image)\n        image = image[:, :, self.swaps]\n        return image\n\n\nclass PhotometricDistort(object):\n    def __init__(self):\n        self.pd = [\n            RandomContrast(),\n            ConvertColor(transform=\'HSV\'),\n            RandomSaturation(),\n            RandomHue(),\n            ConvertColor(current=\'HSV\', transform=\'BGR\'),\n            RandomContrast()\n        ]\n        self.rand_brightness = RandomBrightness()\n        self.rand_light_noise = RandomLightingNoise()\n\n    def __call__(self, image, masks, boxes, labels):\n        im = image.copy()\n        im, masks, boxes, labels = self.rand_brightness(im, masks, boxes, labels)\n        if random.randint(2):\n            distort = Compose(self.pd[:-1])\n        else:\n            distort = Compose(self.pd[1:])\n        im, masks, boxes, labels = distort(im, masks, boxes, labels)\n        return self.rand_light_noise(im, masks, boxes, labels)\n\nclass PrepareMasks(object):\n    """"""\n    Prepares the gt masks for use_gt_bboxes by cropping with the gt box\n    and downsampling the resulting mask to mask_size, mask_size. This\n    function doesn\'t do anything if cfg.use_gt_bboxes is False.\n    """"""\n\n    def __init__(self, mask_size, use_gt_bboxes):\n        self.mask_size = mask_size\n        self.use_gt_bboxes = use_gt_bboxes\n\n    def __call__(self, image, masks, boxes, labels=None):\n        if not self.use_gt_bboxes:\n            return image, masks, boxes, labels\n        \n        height, width, _ = image.shape\n\n        new_masks = np.zeros((masks.shape[0], self.mask_size ** 2))\n\n        for i in range(len(masks)):\n            x1, y1, x2, y2 = boxes[i, :]\n            x1 *= width\n            x2 *= width\n            y1 *= height\n            y2 *= height\n            x1, y1, x2, y2 = (int(x1), int(y1), int(x2), int(y2))\n\n            # +1 So that if y1=10.6 and y2=10.9 we still have a bounding box\n            cropped_mask = masks[i, y1:(y2+1), x1:(x2+1)]\n            scaled_mask = cv2.resize(cropped_mask, (self.mask_size, self.mask_size))\n\n            new_masks[i, :] = scaled_mask.reshape(1, -1)\n        \n        # Binarize\n        new_masks[new_masks >  0.5] = 1\n        new_masks[new_masks <= 0.5] = 0\n\n        return image, new_masks, boxes, labels\n\nclass BackboneTransform(object):\n    """"""\n    Transforms a BRG image made of floats in the range [0, 255] to whatever\n    input the current backbone network needs.\n\n    transform is a transform config object (see config.py).\n    in_channel_order is probably \'BGR\' but you do you, kid.\n    """"""\n    def __init__(self, transform, mean, std, in_channel_order):\n        self.mean = np.array(mean, dtype=np.float32)\n        self.std  = np.array(std,  dtype=np.float32)\n        self.transform = transform\n\n        # Here I use ""Algorithms and Coding"" to convert string permutations to numbers\n        self.channel_map = {c: idx for idx, c in enumerate(in_channel_order)}\n        self.channel_permutation = [self.channel_map[c] for c in transform.channel_order]\n\n    def __call__(self, img, masks=None, boxes=None, labels=None):\n\n        img = img.astype(np.float32)\n\n        if self.transform.normalize:\n            img = (img - self.mean) / self.std\n        elif self.transform.subtract_means:\n            img = (img - self.mean)\n        elif self.transform.to_float:\n            img = img / 255\n\n        img = img[:, :, self.channel_permutation]\n\n        return img.astype(np.float32), masks, boxes, labels\n\n\n\n\nclass BaseTransform(object):\n    """""" Transorm to be used when evaluating. """"""\n\n    def __init__(self, mean=MEANS, std=STD):\n        self.augment = Compose([\n            ConvertFromInts(),\n            Resize(resize_gt=False),\n            BackboneTransform(cfg.backbone.transform, mean, std, \'BGR\')\n        ])\n\n    def __call__(self, img, masks=None, boxes=None, labels=None):\n        return self.augment(img, masks, boxes, labels)\n\nimport torch.nn.functional as F\n\nclass FastBaseTransform(torch.nn.Module):\n    """"""\n    Transform that does all operations on the GPU for super speed.\n    This doesn\'t suppport a lot of config settings and should only be used for production.\n    Maintain this as necessary.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n\n        self.mean = torch.Tensor(MEANS).float().cuda()[None, :, None, None]\n        self.std  = torch.Tensor( STD ).float().cuda()[None, :, None, None]\n        self.transform = cfg.backbone.transform\n\n    def forward(self, img):\n        self.mean = self.mean.to(img.device)\n        self.std  = self.std.to(img.device)\n        \n        # img assumed to be a pytorch BGR image with channel order [n, h, w, c]\n        if cfg.preserve_aspect_ratio:\n            _, h, w, _ = img.size()\n            img_size = Resize.calc_size_preserve_ar(w, h, cfg.max_size)\n            img_size = (img_size[1], img_size[0]) # Pytorch needs h, w\n        else:\n            img_size = (cfg.max_size, cfg.max_size)\n\n        img = img.permute(0, 3, 1, 2).contiguous()\n        img = F.interpolate(img, img_size, mode=\'bilinear\', align_corners=False)\n\n        if self.transform.normalize:\n            img = (img - self.mean) / self.std\n        elif self.transform.subtract_means:\n            img = (img - self.mean)\n        elif self.transform.to_float:\n            img = img / 255\n        \n        if self.transform.channel_order != \'RGB\':\n            raise NotImplementedError\n        \n        img = img[:, (2, 1, 0), :, :].contiguous()\n\n        # Return value is in channel order [n, c, h, w] and RGB\n        return img\n\ndef do_nothing(img=None, masks=None, boxes=None, labels=None):\n    return img, masks, boxes, labels\n\n\ndef enable_if(condition, obj):\n    return obj if condition else do_nothing\n\nclass SSDAugmentation(object):\n    """""" Transform to be used when training. """"""\n\n    def __init__(self, mean=MEANS, std=STD):\n        self.augment = Compose([\n            ConvertFromInts(),\n            ToAbsoluteCoords(),\n            enable_if(cfg.augment_photometric_distort, PhotometricDistort()),\n            enable_if(cfg.augment_expand, Expand(mean)),\n            enable_if(cfg.augment_random_sample_crop, RandomSampleCrop()),\n            enable_if(cfg.augment_random_mirror, RandomMirror()),\n            enable_if(cfg.augment_random_flip, RandomFlip()),\n            enable_if(cfg.augment_random_flip, RandomRot90()),\n            Resize(),\n            enable_if(not cfg.preserve_aspect_ratio, Pad(cfg.max_size, cfg.max_size, mean)),\n            ToPercentCoords(),\n            PrepareMasks(cfg.mask_size, cfg.use_gt_bboxes),\n            BackboneTransform(cfg.backbone.transform, mean, std, \'BGR\')\n        ])\n\n    def __call__(self, img, masks, boxes, labels):\n        return self.augment(img, masks, boxes, labels)\n'"
utils/functions.py,1,"b'import torch\nimport torch.nn as nn\nimport os\nimport math\nfrom collections import deque\nfrom pathlib import Path\nfrom layers.interpolate import InterpolateModule\n\nclass MovingAverage():\n    """""" Keeps an average window of the specified number of items. """"""\n\n    def __init__(self, max_window_size=1000):\n        self.max_window_size = max_window_size\n        self.reset()\n\n    def add(self, elem):\n        """""" Adds an element to the window, removing the earliest element if necessary. """"""\n        if not math.isfinite(elem):\n            print(\'Warning: Moving average ignored a value of %f\' % elem)\n            return\n        \n        self.window.append(elem)\n        self.sum += elem\n\n        if len(self.window) > self.max_window_size:\n            self.sum -= self.window.popleft()\n    \n    def append(self, elem):\n        """""" Same as add just more pythonic. """"""\n        self.add(elem)\n\n    def reset(self):\n        """""" Resets the MovingAverage to its initial state. """"""\n        self.window = deque()\n        self.sum = 0\n\n    def get_avg(self):\n        """""" Returns the average of the elements in the window. """"""\n        return self.sum / max(len(self.window), 1)\n\n    def __str__(self):\n        return str(self.get_avg())\n    \n    def __repr__(self):\n        return repr(self.get_avg())\n    \n    def __len__(self):\n        return len(self.window)\n\n\nclass ProgressBar():\n    """""" A simple progress bar that just outputs a string. """"""\n\n    def __init__(self, length, max_val):\n        self.max_val = max_val\n        self.length = length\n        self.cur_val = 0\n        \n        self.cur_num_bars = -1\n        self._update_str()\n\n    def set_val(self, new_val):\n        self.cur_val = new_val\n\n        if self.cur_val > self.max_val:\n            self.cur_val = self.max_val\n        if self.cur_val < 0:\n            self.cur_val = 0\n\n        self._update_str()\n    \n    def is_finished(self):\n        return self.cur_val == self.max_val\n\n    def _update_str(self):\n        num_bars = int(self.length * (self.cur_val / self.max_val))\n\n        if num_bars != self.cur_num_bars:\n            self.cur_num_bars = num_bars\n            self.string = \'\xe2\x96\x88\' * num_bars + \'\xe2\x96\x91\' * (self.length - num_bars)\n    \n    def __repr__(self):\n        return self.string\n    \n    def __str__(self):\n        return self.string\n\n\ndef init_console():\n    """"""\n    Initialize the console to be able to use ANSI escape characters on Windows.\n    """"""\n    if os.name == \'nt\':\n        from colorama import init\n        init()\n\n\nclass SavePath:\n    """"""\n    Why is this a class?\n    Why do I have a class for creating and parsing save paths?\n    What am I doing with my life?\n    """"""\n\n    def __init__(self, model_name:str, epoch:int, iteration:int):\n        self.model_name = model_name\n        self.epoch = epoch\n        self.iteration = iteration\n\n    def get_path(self, root:str=\'\'):\n        file_name = self.model_name + \'_\' + str(self.epoch) + \'_\' + str(self.iteration) + \'.pth\'\n        return os.path.join(root, file_name)\n\n    @staticmethod\n    def from_str(path:str):\n        file_name = os.path.basename(path)\n        \n        if file_name.endswith(\'.pth\'):\n            file_name = file_name[:-4]\n        \n        params = file_name.split(\'_\')\n\n        if file_name.endswith(\'interrupt\'):\n            params = params[:-1]\n        \n        model_name = \'_\'.join(params[:-2])\n        epoch = params[-2]\n        iteration = params[-1]\n        \n        return SavePath(model_name, int(epoch), int(iteration))\n\n    @staticmethod\n    def remove_interrupt(save_folder):\n        for p in Path(save_folder).glob(\'*_interrupt.pth\'):\n            p.unlink()\n    \n    @staticmethod\n    def get_interrupt(save_folder):\n        for p in Path(save_folder).glob(\'*_interrupt.pth\'): \n            return str(p)\n        return None\n    \n    @staticmethod\n    def get_latest(save_folder, config):\n        """""" Note: config should be config.name. """"""\n        max_iter = -1\n        max_name = None\n\n        for p in Path(save_folder).glob(config + \'_*\'):\n            path_name = str(p)\n\n            try:\n                save = SavePath.from_str(path_name)\n            except:\n                continue \n            \n            if save.model_name == config and save.iteration > max_iter:\n                max_iter = save.iteration\n                max_name = path_name\n\n        return max_name\n\ndef make_net(in_channels, conf, include_last_relu=True):\n    """"""\n    A helper function to take a config setting and turn it into a network.\n    Used by protonet and extrahead. Returns (network, out_channels)\n    """"""\n    def make_layer(layer_cfg):\n        nonlocal in_channels\n        \n        # Possible patterns:\n        # ( 256, 3, {}) -> conv\n        # ( 256,-2, {}) -> deconv\n        # (None,-2, {}) -> bilinear interpolate\n        # (\'cat\',[],{}) -> concat the subnetworks in the list\n        #\n        # You know it would have probably been simpler just to adopt a \'c\' \'d\' \'u\' naming scheme.\n        # Whatever, it\'s too late now.\n        if isinstance(layer_cfg[0], str):\n            layer_name = layer_cfg[0]\n\n            if layer_name == \'cat\':\n                nets = [make_net(in_channels, x) for x in layer_cfg[1]]\n                layer = Concat([net[0] for net in nets], layer_cfg[2])\n                num_channels = sum([net[1] for net in nets])\n        else:\n            num_channels = layer_cfg[0]\n            kernel_size = layer_cfg[1]\n\n            if kernel_size > 0:\n                layer = nn.Conv2d(in_channels, num_channels, kernel_size, **layer_cfg[2])\n            else:\n                if num_channels is None:\n                    layer = InterpolateModule(scale_factor=-kernel_size, mode=\'bilinear\', align_corners=False, **layer_cfg[2])\n                else:\n                    layer = nn.ConvTranspose2d(in_channels, num_channels, -kernel_size, **layer_cfg[2])\n        \n        in_channels = num_channels if num_channels is not None else in_channels\n\n        # Don\'t return a ReLU layer if we\'re doing an upsample. This probably doesn\'t affect anything\n        # output-wise, but there\'s no need to go through a ReLU here.\n        # Commented out for backwards compatibility with previous models\n        # if num_channels is None:\n        #     return [layer]\n        # else:\n        return [layer, nn.ReLU(inplace=True)]\n\n    # Use sum to concat together all the component layer lists\n    net = sum([make_layer(x) for x in conf], [])\n    if not include_last_relu:\n        net = net[:-1]\n\n    return nn.Sequential(*(net)), in_channels'"
utils/logger.py,0,"b'import os\nimport json\nimport time\nimport sys\n\nfrom typing import Union\nimport datetime\n\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Because Python\'s package heierarchy system sucks\nif __name__ == \'__main__\':\n    from nvinfo import gpu_info, visible_gpus, nvsmi_available\n    from functions import MovingAverage\nelse:\n    from .nvinfo import gpu_info, visible_gpus, nvsmi_available\n    from .functions import MovingAverage\n\nclass Log:\n    """"""\n    A class to log information during training per information and save it out.\n    It also can include extra debug information like GPU usage / temp automatically.\n\n    Extra args:\n     - session_data: If you have any data unique to this session, put it here.\n     - overwrite: Whether or not to overwrite a pre-existing log with this name.\n     - log_gpu_stats: Whether or not to log gpu information like temp, usage, memory.\n                      Note that this requires nvidia-smi to be present in your PATH.\n     - log_time: Also log the time in each iteration.\n    """"""\n\n    def __init__(self, log_name:str, log_dir:str=\'logs/\', session_data:dict={},\n                 overwrite:bool=False, log_gpu_stats:bool=True, log_time:bool=True):\n        \n        if log_gpu_stats and not nvsmi_available():\n            print(\'Warning: Log created with log_gpu_stats=True, but nvidia-smi \' \\\n                  \'was not found. Setting log_gpu_stats to False.\')\n            log_gpu_stats = False\n        \n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        self.log_path = os.path.join(log_dir, log_name + \'.log\')\n\n        # if os.path.exists(self.log_path) and overwrite:\n        #     os.unlink(self.log_path)\n\n        if os.path.exists(self.log_path):\n            # Log already exists, so we\'re going to add to it. Increment the session counter.\n            with open(self.log_path, \'r\') as f:\n                for last in f: pass\n\n                if len(last) > 1:\n                    self.session = json.loads(last)[\'session\'] + 1\n                else:\n                    self.session = 0\n        else:\n            self.session = 0\n\n\n        self.log_gpu_stats = log_gpu_stats\n        self.log_time = log_time\n\n        if self.log_gpu_stats:\n            self.visible_gpus = visible_gpus()\n    \n\n        self._log_session_header(session_data)\n\n\n    def _log_session_header(self, session_data:dict):\n        """"""\n        Log information that does not change between iterations here.\n        This is to cut down on the file size so you\'re not outputing this every iteration.\n        """"""\n        info = {}\n        info[\'type\'] = \'session\'\n        info[\'session\'] = self.session\n\n        info[\'data\'] = session_data\n\n        if self.log_gpu_stats:\n            keys = [\'idx\', \'name\', \'uuid\', \'pwr_cap\', \'mem_total\']\n\n            gpus = gpu_info()\n            info[\'gpus\'] = [{k: gpus[i][k] for k in keys} for i in self.visible_gpus]\n        \n        if self.log_time:\n            info[\'time\'] = time.time()\n\n        out = json.dumps(info) + \'\\n\'\n\n        with open(self.log_path, \'a\') as f:\n            f.write(out)\n\n\n    def log(self, type:str, data:dict={}, **kwdargs):\n        """"""\n        Add an iteration to the log with the specified data points.\n        Type should be the type of information this is (e.g., train, valid, etc.)\n        \n        You can either pass data points as kwdargs, or as a dictionary (or both!).\n        Values should be json-serializable.\n        """"""\n        info = {}\n        \n        info[\'type\'] = type\n        info[\'session\'] = self.session\n\n        kwdargs.update(data)\n        info[\'data\'] = kwdargs\n\n        if self.log_gpu_stats:\n            keys = [\'fan_spd\', \'temp\', \'pwr_used\', \'mem_used\', \'util\']\n            \n            gpus = gpu_info()\n            info[\'gpus\'] = [{k: gpus[i][k] for k in keys} for i in self.visible_gpus]\n        \n        if self.log_time:\n            info[\'time\'] = time.time()\n            \n        \n        out = json.dumps(info) + \'\\n\'\n\n        with open(self.log_path, \'a\') as f:\n            f.write(out)\n\n\nclass LogEntry():\n    """""" A class that allows you to navigate a dictonary using x.a.b[2].c, etc. """"""\n\n    def __init__(self, entry:Union[dict, list]):\n        self._ = entry\n\n    def __getattr__(self, name):\n        if name == \'_\':\n            return self.__dict__[\'_\']\n\n        res = self.__dict__[\'_\'][name]\n\n        if type(res) == dict or type(res) == list:\n            return LogEntry(res)\n        else:\n            return res\n    \n    def __getitem__(self, name):\n        return self.__getattr__(name)\n\n    def __len__(self):\n        return len(self.__dict__[\'_\'])\n\nclass LogVisualizer():\n\n    COLORS = [\n        \'xkcd:azure\',\n        \'xkcd:coral\',\n        \'xkcd:turquoise\',\n        \'xkcd:orchid\',\n        \'xkcd:orange\',\n        \n        \'xkcd:blue\',\n        \'xkcd:red\',\n        \'xkcd:teal\',\n        \'xkcd:magenta\',\n        \'xkcd:orangered\'\n    ]\n\n    def __init__(self):\n        self.logs = []\n        self.total_logs = []\n        self.log_names = []\n    \n    def _decode(self, query:str) -> list:\n        path, select = (query.split(\';\') + [\'\'])[:2]\n        \n        if select.strip() == \'\':\n            select = lambda x, s: True\n        else:\n            select = eval(\'lambda x, s: \' + select)\n\n        if path.strip() == \'\':\n            path = lambda x, s: x\n        else:\n            path = eval(\'lambda x, s: \' + path)\n        \n        return path, select\n\n    def _follow(self, entry:LogEntry, query:list):\n        path, select = query\n\n        try:\n            if select(entry, entry._s):\n                res = path(entry, entry._s)\n\n                if type(res) == LogEntry:\n                    return res.__dict__[\'_\']\n                else:\n                    return res\n            else:\n                return None\n        except (KeyError, IndexError):\n            return None\n\n    def _color(self, idx:int):\n        return self.COLORS[idx % len(self.COLORS)]\n\n    def sessions(self, path:str):\n        """""" Prints statistics about the sessions in the file. """"""\n\n        if not os.path.exists(path):\n            print(path + \' doesn\\\'t exist!\')\n            return\n\n        cur_session = None\n        cur_time = 0\n        last_time = 0\n        num_entries = 0\n\n        def pop_session():\n            delta = last_time - cur_time\n            time_str = str(datetime.timedelta(seconds=delta)).split(\'.\')[0]\n            print(\'Session % 3d: % 8d entries | %s elapsed\' % (cur_session, num_entries, time_str))\n\n        with open(path, \'r\') as f:\n            for line in f:\n                line = line.strip()\n                if len(line) > 0:\n                    js = json.loads(line)\n                    if js[\'type\'] == \'session\':\n                        if cur_session is not None:\n                            pop_session()\n                        cur_time = js[\'time\']\n                        cur_session = js[\'session\']\n                        num_entries = 0\n                    last_time = js[\'time\']\n                    num_entries += 1\n        \n        pop_session()\n\n    def add(self, path:str, session:Union[int,list]=None):\n        """""" Add a log file to the list of logs being considered. """"""\n\n        log = defaultdict(lambda: [])\n        total_log = []\n\n        if not os.path.exists(path):\n            print(path + \' doesn\\\'t exist!\')\n            return\n\n        session_idx = 0\n        ignoring = True\n        \n        def valid(idx):\n            if session is None:\n                return True\n            elif type(session) == int:\n                return (idx == session)\n            else:\n                return idx in session\n\n        with open(path, \'r\') as f:\n            for line in f:\n                line = line.strip()\n                if len(line) > 0:\n                    js = json.loads(line)\n                    \n                    _type = js[\'type\']\n                    if _type == \'session\':\n                        session_idx = js[\'session\']\n                        ignoring = not valid(session_idx)\n\n                    if not ignoring:\n                        ljs = LogEntry(js)\n                        if _type == \'session\':\n                            js[\'_s\'] = ljs\n                        else:\n                            js[\'_s\'] =log[\'session\'][-1]\n                        log[_type].append(ljs)\n                        total_log.append(ljs)\n        \n        name = os.path.basename(path)\n        if session is not None:\n            name += \' (Session %s)\' % session\n\n        self.logs.append(log)\n        self.total_logs.append(total_log)\n        self.log_names.append(name)\n\n    def query(self, x:Union[str, list], entry_type:str=None, x_idx:int=None, log_idx:int=None) -> list:\n        """"""\n        Given a query string (can be already decoded for faster computation), query the entire log\n        and return all values found by that query. If both log_idx and x_idx is None, this will be\n        a list of lists in the form [log_idx][result_idx]. If x_idx is not None, then the result\n        will be a list of [log_idx]. If both are not none, the return value will be a single query\n        return value. With entry_type=None, this will search the entire log.\n        """"""\n\n        if type(x) is not list:\n            x = self._decode(x)\n        \n        res = []\n\n        for idx in (range(len(self.logs)) if log_idx is None else [log_idx]):\n            candidates = []\n            log = self.total_logs[idx] if entry_type is None else self.logs[idx][entry_type]\n\n            for entry in log:\n                candidate = self._follow(entry, x)\n                if candidate is not None:\n                    candidates.append(candidate)\n            \n            if x_idx is not None:\n                candidates = candidates[x_idx]\n            res.append(candidates)\n        \n        if log_idx is not None:\n            res = res[0]\n        return res\n\n    def check(self, entry_type:str, x:str):\n        """""" Checks the log for the valid keys for this input. """"""\n        keys = set()\n        x = self._decode(x)\n\n        for log in self.logs:\n            for datum in log[entry_type]:\n                res = self._follow(datum, x)\n\n                if type(res) == dict:\n                    for key in res.keys():\n                        keys.add(key)\n                elif type(res) == list:\n                    keys.add(\'< %d\' % len(res))\n    \n        return list(keys)\n\n    def plot(self, entry_type:str, x:str, y:str, smoothness:int=0):\n        """""" Plot sequential log data. """"""\n\n        query_x = self._decode(x)\n        query_y = self._decode(y)\n\n        for idx, (log, name) in enumerate(zip(self.logs, self.log_names)):\n            log = log[entry_type]\n\n            if smoothness > 1:\n                avg = MovingAverage(smoothness)\n\n            _x = []\n            _y = []\n\n            for datum in log:\n                val_x = self._follow(datum, query_x)\n                val_y = self._follow(datum, query_y)\n\n                if val_x is not None and val_y is not None:\n                    if smoothness > 1:\n                        avg.append(val_y)\n                        val_y = avg.get_avg()\n\n                        if len(avg) < smoothness // 10:\n                            continue\n                        \n                    _x.append(val_x)\n                    _y.append(val_y)\n            \n            plt.plot(_x, _y, color=self._color(idx), label=name)\n        \n        plt.title(y.replace(\'x.\', entry_type + \'.\'))\n        plt.legend()\n        plt.grid(linestyle=\':\', linewidth=0.5)\n        plt.show()\n\n    def bar(self, entry_type:str, x:str, labels:list=None, diff:bool=False, x_idx:int=-1):\n        """""" Plot a bar chart. The result of x should be list or dictionary. """"""\n\n        query = self._decode(x)\n\n        data_points = []\n\n        for idx, (log, name) in enumerate(zip(self.logs, self.log_names)):\n            log = log[entry_type]\n\n            candidates = []\n\n            for entry in log:\n                test = self._follow(entry, query)\n\n                if type(test) == dict:\n                    candidates.append(test)\n                elif type(test) == list:\n                    candidates.append({idx: v for idx, v in enumerate(test)})\n            \n            if len(candidates) > 0:\n                data_points.append((name, candidates[x_idx]))\n        \n        if len(data_points) == 0:\n            print(\'Warning: Nothing to show in bar chart!\')\n            return\n\n        names = [x[0] for x in data_points]\n        data_points = [x[1] for x in data_points]\n\n        # Construct the labels for the data\n        if labels is not None:\n            data_labels = labels\n        else:\n            data_labels = set()\n            for datum in data_points:\n                for k in datum:\n                    data_labels.add(k)\n                \n            data_labels = list(data_labels)\n            data_labels.sort()\n        \n\n        data_values = [[(datum[k] if k in datum else None) for k in data_labels] for datum in data_points]\n\n        if diff:\n            for idx in reversed(range(len(data_values))):\n                for jdx in range(len(data_labels)):\n                    if data_values[0][jdx] is None or data_values[idx][jdx] is None:\n                        data_values[idx][jdx] = None\n                    else:\n                        data_values[idx][jdx] -= data_values[0][jdx]\n\n\n        series_labels = names\n\n        # Plot the graph now\n        num_bars = len(series_labels)\n        bar_width = 1 / (num_bars + 1)\n        \n        # Set position of bar on X axis\n        positions = [np.arange(len(data_labels))]\n        for _ in range(1, num_bars):\n            positions.append([x + bar_width for x in positions[-1]])\n        \n        # Make the plot\n        for idx, (series, data, pos) in enumerate(zip(series_labels, data_values, positions)):\n            plt.bar(pos, data, color=self._color(idx), width=bar_width, edgecolor=\'white\', label=series)\n        \n        # Add xticks on the middle of the group bars\n        plt.title(x.replace(\'x.\', entry_type + \'.\') + (\' diff\' if diff else \'\'))\n        plt.xticks([r + bar_width for r in range(len(data_labels))], data_labels)\n        \n        # Create legend & Show graphic\n        plt.legend()\n        plt.show()\n\n        \n\n    def elapsed_time(self, cond1:str=\'\', cond2:str=\'\', legible:bool=True) -> list:\n        """"""\n        Returns the elapsed time between two entries based on the given conditionals.\n        If a query isn\'t specified, the first / last entry will be used. The first query\n        uses the first value and the second query uses the last value in the results.\n\n        Setting legible to true returns human-readable results, while false returns seconds.\n        """"""\n        q1 = \'x.time; \' + cond1\n        q2 = \'x.time; \' + cond2\n\n        x1 = self.query(q1, x_idx=0)\n        x2 = self.query(q2, x_idx=-1)\n        \n        diff = (lambda x: str(datetime.timedelta(seconds=x)).split(\'.\')[0]) if legible else lambda x: x\n\n        return [diff(b - a) for a, b in zip(x1, x2)]\n\n\n\n\n\n\n\n\n\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) < 4+1:\n        print(\'Usage: python utils/logger.py <LOG_FILE> <TYPE> <X QUERY> <Y QUERY>\')\n        exit()\n    \n    vis = LogVisualizer()\n    vis.add(sys.argv[1])\n    vis.plot(sys.argv[2], sys.argv[3], sys.argv[4])\n'"
utils/nvinfo.py,1,"b'# My version of nvgpu because nvgpu didn\'t have all the information I was looking for.\nimport re\nimport subprocess\nimport shutil\nimport os\n\ndef gpu_info() -> list:\n    """"""\n    Returns a dictionary of stats mined from nvidia-smi for each gpu in a list.\n    Adapted from nvgpu: https://pypi.org/project/nvgpu/, but mine has more info.\n    """"""\n    gpus = [line for line in _run_cmd([\'nvidia-smi\', \'-L\']) if line]\n    gpu_infos = [re.match(\'GPU ([0-9]+): ([^(]+) \\(UUID: ([^)]+)\\)\', gpu).groups() for gpu in gpus]\n    gpu_infos = [dict(zip([\'idx\', \'name\', \'uuid\'], info)) for info in gpu_infos]\n    gpu_count = len(gpus)\n\n    lines = _run_cmd([\'nvidia-smi\'])\n    selected_lines = lines[7:7 + 3 * gpu_count]\n    for i in range(gpu_count):\n        mem_used, mem_total = [int(m.strip().replace(\'MiB\', \'\')) for m in\n                               selected_lines[3 * i + 1].split(\'|\')[2].strip().split(\'/\')]\n        \n        pw_tmp_info, mem_info, util_info = [x.strip() for x in selected_lines[3 * i + 1].split(\'|\')[1:-1]]\n        \n        pw_tmp_info = [x[:-1] for x in pw_tmp_info.split(\' \') if len(x) > 0]\n        fan_speed, temperature, pwr_used, pwr_cap = [int(pw_tmp_info[i]) for i in (0, 1, 3, 5)]\n        gpu_infos[i][\'fan_spd\' ] = fan_speed\n        gpu_infos[i][\'temp\'    ] = temperature\n        gpu_infos[i][\'pwr_used\'] = pwr_used\n        gpu_infos[i][\'pwr_cap\' ] = pwr_cap\n\n        mem_used, mem_total = [int(x) for x in mem_info.replace(\'MiB\', \'\').split(\' / \')]\n        gpu_infos[i][\'mem_used\' ] = mem_used\n        gpu_infos[i][\'mem_total\'] = mem_total\n\n        utilization = int(util_info.split(\' \')[0][:-1])\n        gpu_infos[i][\'util\'] = utilization\n\n        gpu_infos[i][\'idx\'] = int(gpu_infos[i][\'idx\'])\n\n    return gpu_infos\n\ndef nvsmi_available() -> bool:\n    """""" Returns whether or not nvidia-smi is present in this system\'s PATH. """"""\n    return shutil.which(\'nvidia-smi\') is not None\n\n\ndef visible_gpus() -> list:\n    """""" Returns a list of the indexes of all the gpus visible to pytorch. """"""\n\n    if \'CUDA_VISIBLE_DEVICES\' not in os.environ:\n        return list(range(len(gpu_info())))\n    else:\n        return [int(x.strip()) for x in os.environ[\'CUDA_VISIBLE_DEVICES\'].split(\',\')]\n\n\n\n\ndef _run_cmd(cmd:list) -> list:\n    """""" Runs a command and returns a list of output lines. """"""\n    output = subprocess.check_output(cmd)\n    output = output.decode(\'UTF-8\')\n    return output.split(\'\\n\')'"
utils/timer.py,0,"b'import time\nfrom collections import defaultdict\n\n_total_times = defaultdict(lambda:  0)\n_start_times = defaultdict(lambda: -1)\n_disabled_names = set()\n_timer_stack = []\n_running_timer = None\n_disable_all = False\n\ndef disable_all():\n\tglobal _disable_all\n\t_disable_all = True\n\ndef enable_all():\n\tglobal _disable_all\n\t_disable_all = False\n\ndef disable(fn_name):\n\t"""""" Disables the given function name fom being considered for the average or outputted in print_stats. """"""\n\t_disabled_names.add(fn_name)\n\ndef enable(fn_name):\n\t"""""" Enables function names disabled by disable. """"""\n\t_disabled_names.remove(fn_name)\n\ndef reset():\n\t"""""" Resets the current timer. Call this at the start of an iteration. """"""\n\tglobal _running_timer\n\t_total_times.clear()\n\t_start_times.clear()\n\t_timer_stack.clear()\n\t_running_timer = None\n\ndef start(fn_name, use_stack=True):\n\t""""""\n\tStart timing the specific function.\n\tNote: If use_stack is True, only one timer can be active at a time.\n\t      Once you stop this timer, the previous one will start again.\n\t""""""\n\tglobal _running_timer, _disable_all\n\t\n\tif _disable_all:\n\t\treturn\n\n\tif use_stack:\n\t\tif _running_timer is not None:\n\t\t\tstop(_running_timer, use_stack=False)\n\t\t\t_timer_stack.append(_running_timer)\n\t\tstart(fn_name, use_stack=False)\n\t\t_running_timer = fn_name\n\telse:\n\t\t_start_times[fn_name] = time.perf_counter()\n\ndef stop(fn_name=None, use_stack=True):\n\t""""""\n\tIf use_stack is True, this will stop the currently running timer and restore\n\tthe previous timer on the stack if that exists. Note if use_stack is True,\n\tfn_name will be ignored.\n\n\tIf use_stack is False, this will just stop timing the timer fn_name.\n\t""""""\n\tglobal _running_timer, _disable_all\n\n\tif _disable_all:\n\t\treturn\n\n\tif use_stack:\n\t\tif _running_timer is not None:\n\t\t\tstop(_running_timer, use_stack=False)\n\t\t\tif len(_timer_stack) > 0:\n\t\t\t\t_running_timer = _timer_stack.pop()\n\t\t\t\tstart(_running_timer, use_stack=False)\n\t\t\telse:\n\t\t\t\t_running_timer = None\n\t\telse:\n\t\t\tprint(\'Warning: timer stopped with no timer running!\')\n\telse:\n\t\tif _start_times[fn_name] > -1:\n\t\t\t_total_times[fn_name] += time.perf_counter() - _start_times[fn_name]\n\t\telse:\n\t\t\tprint(\'Warning: timer for %s stopped before starting!\' % fn_name)\n\n\ndef print_stats():\n\t"""""" Prints the current timing information into a table. """"""\n\tprint()\n\n\tall_fn_names = [k for k in _total_times.keys() if k not in _disabled_names]\n\n\tmax_name_width = max([len(k) for k in all_fn_names] + [4])\n\tif max_name_width % 2 == 1: max_name_width += 1\n\tformat_str = \' {:>%d} | {:>10.4f} \' % max_name_width\n\n\theader = (\' {:^%d} | {:^10} \' % max_name_width).format(\'Name\', \'Time (ms)\')\n\tprint(header)\n\n\tsep_idx = header.find(\'|\')\n\tsep_text = (\'-\' * sep_idx) + \'+\' + \'-\' * (len(header)-sep_idx-1)\n\tprint(sep_text)\n\n\tfor name in all_fn_names:\n\t\tprint(format_str.format(name, _total_times[name]*1000))\n\t\n\tprint(sep_text)\n\tprint(format_str.format(\'Total\', total_time()*1000))\n\tprint()\n\ndef total_time():\n\t"""""" Returns the total amount accumulated across all functions in seconds. """""" \n\treturn sum([elapsed_time for name, elapsed_time in _total_times.items() if name not in _disabled_names])\n\n\nclass env():\n\t""""""\n\tA class that lets you go:\n\t\twith timer.env(fn_name):\n\t\t\t# (...)\n\tThat automatically manages a timer start and stop for you.\n\t""""""\n\n\tdef __init__(self, fn_name, use_stack=True):\n\t\tself.fn_name = fn_name\n\t\tself.use_stack = use_stack\n\n\tdef __enter__(self):\n\t\tstart(self.fn_name, use_stack=self.use_stack)\n\n\tdef __exit__(self, e, ev, t):\n\t\tstop(self.fn_name, use_stack=self.use_stack)\n\n'"
web/server.py,0,"b'from http.server import SimpleHTTPRequestHandler, HTTPServer, HTTPStatus\nfrom pathlib import Path\nimport os\n\nPORT = 6337\nIMAGE_PATH = \'../data/coco/images/\'\nIMAGE_FMT  = \'%012d.jpg\'\n\nclass Handler(SimpleHTTPRequestHandler):\n\t\n\tdef do_GET(self):\n\t\tif self.path == \'/detindex\':\n\t\t\tself.send_str(\'\\n\'.join([p.name[:-5] for p in Path(\'dets/\').glob(\'*.json\')]))\n\t\telif self.path.startswith(\'/image\'):\n\t\t\t# Unsafe practices ahead!\n\t\t\tpath = self.translate_path(self.path).split(\'image\')\n\t\t\tself.send_file(os.path.join(path[0], IMAGE_PATH, IMAGE_FMT % int(path[1])))\n\t\telse:\n\t\t\tsuper().do_GET()\n\n\tdef send_str(self, string):\n\t\tself.send_response(HTTPStatus.OK)\n\t\tself.send_header(\'Content-type\', \'text/plain\')\n\t\tself.send_header(\'Content-Length\', str(len(string)))\n\t\tself.send_header(\'Last-Modified\', self.date_time_string())\n\t\tself.end_headers()\n\n\t\tself.wfile.write(string.encode())\n\n\tdef send_file(self, path):\n\t\ttry: \n\t\t\tf = open(path, \'rb\')\n\t\texcept OSError:\n\t\t\tself.send_error(HTTPStatus.NOT_FOUND, ""File not found"")\n\t\t\treturn\n\t\t\n\t\ttry:\n\t\t\tself.send_response(HTTPStatus.OK)\n\t\t\tself.send_header(""Content-type"", self.guess_type(path))\n\t\t\tfs = os.fstat(f.fileno())\n\t\t\tself.send_header(""Content-Length"", str(fs[6]))\n\t\t\tself.send_header(""Last-Modified"", self.date_time_string(fs.st_mtime))\n\t\t\tself.end_headers()\n\n\t\t\tself.copyfile(f, self.wfile)\n\t\tfinally:\n\t\t\tf.close()\n\n\tdef send_response(self, code, message=None):\n\t\tsuper().send_response(code, message)\n\n\nwith HTTPServer((\'\', PORT), Handler) as httpd:\n\tprint(\'Serving at port\', PORT)\n\ttry:\n\t\thttpd.serve_forever()\n\texcept KeyboardInterrupt:\n\t\tpass\n'"
data/scripts/mix_sets.py,0,"b'import json\nimport os\nimport sys\nfrom collections import defaultdict\n\nusage_text = """"""\nThis script creates a coco annotation file by mixing one or more existing annotation files.\n\nUsage: python data/scripts/mix_sets.py output_name [set1 range1 [set2 range2 [...]]]\n\nTo use, specify the output annotation name and any number of set + range pairs, where the sets\nare in the form instances_<set_name>.json and ranges are python-evalable ranges. The resulting\njson will be spit out as instances_<output_name>.json in the same folder as the input sets.\n\nFor instance,\n    python data/scripts/mix_sets.py trainval35k train2014 : val2014 :-5000\n\nThis will create an instance_trainval35k.json file with all images and corresponding annotations\nfrom train2014 and the first 35000 images from val2014.\n\nYou can also specify only one set:\n    python data/scripts/mix_sets.py minival5k val2014 -5000:\n\nThis will take the last 5k images from val2014 and put it in instances_minival5k.json.\n""""""\n\nannotations_path = \'data/coco/annotations/instances_%s.json\'\nfields_to_combine = (\'images\', \'annotations\')\nfields_to_steal   = (\'info\', \'categories\', \'licenses\')\n\nif __name__ == \'__main__\':\n\tif len(sys.argv) < 4 or len(sys.argv) % 2 != 0:\n\t\tprint(usage_text)\n\t\texit()\n\n\tout_name = sys.argv[1]\n\tsets = sys.argv[2:]\n\tsets = [(sets[2*i], sets[2*i+1]) for i in range(len(sets)//2)]\n\n\tout = {x: [] for x in fields_to_combine}\n\n\tfor idx, (set_name, range_str) in enumerate(sets):\n\t\tprint(\'Loading set %s...\' % set_name)\n\t\twith open(annotations_path % set_name, \'r\') as f:\n\t\t\tset_json = json.load(f)\n\n\t\t# ""Steal"" some fields that don\'t need to be combined from the first set\n\t\tif idx == 0:\n\t\t\tfor field in fields_to_steal:\n\t\t\t\tout[field] = set_json[field]\n\t\t\n\t\tprint(\'Building image index...\')\n\t\timage_idx = {x[\'id\']: x for x in set_json[\'images\']}\n\n\t\tprint(\'Collecting annotations...\')\n\t\tanns_idx = defaultdict(lambda: [])\n\n\t\tfor ann in set_json[\'annotations\']:\n\t\t\tanns_idx[ann[\'image_id\']].append(ann)\n\n\t\texport_ids = list(image_idx.keys())\n\t\texport_ids.sort()\n\t\texport_ids = eval(\'export_ids[%s]\' % range_str, {}, {\'export_ids\': export_ids})\n\n\t\tprint(\'Adding %d images...\' % len(export_ids))\n\t\tfor _id in export_ids:\n\t\t\tout[\'images\'].append(image_idx[_id])\n\t\t\tout[\'annotations\'] += anns_idx[_id]\n\n\t\tprint(\'Done.\\n\')\n\n\tprint(\'Saving result...\')\n\twith open(annotations_path % (out_name), \'w\') as out_file:\n\t\tjson.dump(out, out_file)\n'"
external/DCNv2/__init__.py,0,b''
external/DCNv2/dcn_v2.py,11,"b'#!/usr/bin/env python\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport math\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\nfrom torch.nn.modules.utils import _pair\nfrom torch.autograd.function import once_differentiable\n\nimport _ext as _backend\n\n\nclass _DCNv2(Function):\n    @staticmethod\n    def forward(ctx, input, offset, mask, weight, bias,\n                stride, padding, dilation, deformable_groups):\n        ctx.stride = _pair(stride)\n        ctx.padding = _pair(padding)\n        ctx.dilation = _pair(dilation)\n        ctx.kernel_size = _pair(weight.shape[2:4])\n        ctx.deformable_groups = deformable_groups\n        output = _backend.dcn_v2_forward(input, weight, bias,\n                                         offset, mask,\n                                         ctx.kernel_size[0], ctx.kernel_size[1],\n                                         ctx.stride[0], ctx.stride[1],\n                                         ctx.padding[0], ctx.padding[1],\n                                         ctx.dilation[0], ctx.dilation[1],\n                                         ctx.deformable_groups)\n        ctx.save_for_backward(input, offset, mask, weight, bias)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        input, offset, mask, weight, bias = ctx.saved_tensors\n        grad_input, grad_offset, grad_mask, grad_weight, grad_bias = \\\n            _backend.dcn_v2_backward(input, weight,\n                                     bias,\n                                     offset, mask,\n                                     grad_output,\n                                     ctx.kernel_size[0], ctx.kernel_size[1],\n                                     ctx.stride[0], ctx.stride[1],\n                                     ctx.padding[0], ctx.padding[1],\n                                     ctx.dilation[0], ctx.dilation[1],\n                                     ctx.deformable_groups)\n\n        return grad_input, grad_offset, grad_mask, grad_weight, grad_bias,\\\n            None, None, None, None,\n\n\ndcn_v2_conv = _DCNv2.apply\n\n\nclass DCNv2(nn.Module):\n\n    def __init__(self, in_channels, out_channels,\n                 kernel_size, stride, padding, dilation=1, deformable_groups=1):\n        super(DCNv2, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n        self.deformable_groups = deformable_groups\n\n        self.weight = nn.Parameter(torch.Tensor(\n            out_channels, in_channels, *self.kernel_size))\n        self.bias = nn.Parameter(torch.Tensor(out_channels))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        self.bias.data.zero_()\n\n    def forward(self, input, offset, mask):\n        assert 2 * self.deformable_groups * self.kernel_size[0] * self.kernel_size[1] == \\\n            offset.shape[1]\n        assert self.deformable_groups * self.kernel_size[0] * self.kernel_size[1] == \\\n            mask.shape[1]\n        return dcn_v2_conv(input, offset, mask,\n                           self.weight,\n                           self.bias,\n                           self.stride,\n                           self.padding,\n                           self.dilation,\n                           self.deformable_groups)\n\n\nclass DCN(DCNv2):\n\n    def __init__(self, in_channels, out_channels,\n                 kernel_size, stride, padding,\n                 dilation=1, deformable_groups=1):\n        super(DCN, self).__init__(in_channels, out_channels,\n                                  kernel_size, stride, padding, dilation, deformable_groups)\n\n        channels_ = self.deformable_groups * 3 * self.kernel_size[0] * self.kernel_size[1]\n        self.conv_offset_mask = nn.Conv2d(self.in_channels,\n                                          channels_,\n                                          kernel_size=self.kernel_size,\n                                          stride=self.stride,\n                                          padding=self.padding,\n                                          bias=True)\n        self.init_offset()\n\n    def init_offset(self):\n        self.conv_offset_mask.weight.data.zero_()\n        self.conv_offset_mask.bias.data.zero_()\n\n    def forward(self, input):\n        out = self.conv_offset_mask(input)\n        o1, o2, mask = torch.chunk(out, 3, dim=1)\n        offset = torch.cat((o1, o2), dim=1)\n        mask = torch.sigmoid(mask)\n        return dcn_v2_conv(input, offset, mask,\n                           self.weight, self.bias,\n                           self.stride,\n                           self.padding,\n                           self.dilation,\n                           self.deformable_groups)\n\n\n\nclass _DCNv2Pooling(Function):\n    @staticmethod\n    def forward(ctx, input, rois, offset,\n                spatial_scale,\n                pooled_size,\n                output_dim,\n                no_trans,\n                group_size=1,\n                part_size=None,\n                sample_per_part=4,\n                trans_std=.0):\n        ctx.spatial_scale = spatial_scale\n        ctx.no_trans = int(no_trans)\n        ctx.output_dim = output_dim\n        ctx.group_size = group_size\n        ctx.pooled_size = pooled_size\n        ctx.part_size = pooled_size if part_size is None else part_size\n        ctx.sample_per_part = sample_per_part\n        ctx.trans_std = trans_std\n\n        output, output_count = \\\n            _backend.dcn_v2_psroi_pooling_forward(input, rois, offset,\n                                                  ctx.no_trans, ctx.spatial_scale,\n                                                  ctx.output_dim, ctx.group_size,\n                                                  ctx.pooled_size, ctx.part_size,\n                                                  ctx.sample_per_part, ctx.trans_std)\n        ctx.save_for_backward(input, rois, offset, output_count)\n        return output\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_output):\n        input, rois, offset, output_count = ctx.saved_tensors\n        grad_input, grad_offset = \\\n            _backend.dcn_v2_psroi_pooling_backward(grad_output,\n                                                   input,\n                                                   rois,\n                                                   offset,\n                                                   output_count,\n                                                   ctx.no_trans,\n                                                   ctx.spatial_scale,\n                                                   ctx.output_dim,\n                                                   ctx.group_size,\n                                                   ctx.pooled_size,\n                                                   ctx.part_size,\n                                                   ctx.sample_per_part,\n                                                   ctx.trans_std)\n\n        return grad_input, None, grad_offset, \\\n            None, None, None, None, None, None, None, None\n\n\ndcn_v2_pooling = _DCNv2Pooling.apply\n\n\nclass DCNv2Pooling(nn.Module):\n\n    def __init__(self,\n                 spatial_scale,\n                 pooled_size,\n                 output_dim,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0):\n        super(DCNv2Pooling, self).__init__()\n        self.spatial_scale = spatial_scale\n        self.pooled_size = pooled_size\n        self.output_dim = output_dim\n        self.no_trans = no_trans\n        self.group_size = group_size\n        self.part_size = pooled_size if part_size is None else part_size\n        self.sample_per_part = sample_per_part\n        self.trans_std = trans_std\n\n    def forward(self, input, rois, offset):\n        assert input.shape[1] == self.output_dim\n        if self.no_trans:\n            offset = input.new()\n        return dcn_v2_pooling(input, rois, offset,\n                              self.spatial_scale,\n                              self.pooled_size,\n                              self.output_dim,\n                              self.no_trans,\n                              self.group_size,\n                              self.part_size,\n                              self.sample_per_part,\n                              self.trans_std)\n\n\nclass DCNPooling(DCNv2Pooling):\n\n    def __init__(self,\n                 spatial_scale,\n                 pooled_size,\n                 output_dim,\n                 no_trans,\n                 group_size=1,\n                 part_size=None,\n                 sample_per_part=4,\n                 trans_std=.0,\n                 deform_fc_dim=1024):\n        super(DCNPooling, self).__init__(spatial_scale,\n                                         pooled_size,\n                                         output_dim,\n                                         no_trans,\n                                         group_size,\n                                         part_size,\n                                         sample_per_part,\n                                         trans_std)\n\n        self.deform_fc_dim = deform_fc_dim\n\n        if not no_trans:\n            self.offset_mask_fc = nn.Sequential(\n                nn.Linear(self.pooled_size * self.pooled_size *\n                          self.output_dim, self.deform_fc_dim),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_dim, self.deform_fc_dim),\n                nn.ReLU(inplace=True),\n                nn.Linear(self.deform_fc_dim, self.pooled_size *\n                          self.pooled_size * 3)\n            )\n            self.offset_mask_fc[4].weight.data.zero_()\n            self.offset_mask_fc[4].bias.data.zero_()\n\n    def forward(self, input, rois):\n        offset = input.new()\n\n        if not self.no_trans:\n\n            # do roi_align first\n            n = rois.shape[0]\n            roi = dcn_v2_pooling(input, rois, offset,\n                                 self.spatial_scale,\n                                 self.pooled_size,\n                                 self.output_dim,\n                                 True,  # no trans\n                                 self.group_size,\n                                 self.part_size,\n                                 self.sample_per_part,\n                                 self.trans_std)\n\n            # build mask and offset\n            offset_mask = self.offset_mask_fc(roi.view(n, -1))\n            offset_mask = offset_mask.view(\n                n, 3, self.pooled_size, self.pooled_size)\n            o1, o2, mask = torch.chunk(offset_mask, 3, dim=1)\n            offset = torch.cat((o1, o2), dim=1)\n            mask = torch.sigmoid(mask)\n\n            # do pooling with offset and mask\n            return dcn_v2_pooling(input, rois, offset,\n                                  self.spatial_scale,\n                                  self.pooled_size,\n                                  self.output_dim,\n                                  self.no_trans,\n                                  self.group_size,\n                                  self.part_size,\n                                  self.sample_per_part,\n                                  self.trans_std) * mask\n        # only roi_align\n        return dcn_v2_pooling(input, rois, offset,\n                              self.spatial_scale,\n                              self.pooled_size,\n                              self.output_dim,\n                              self.no_trans,\n                              self.group_size,\n                              self.part_size,\n                              self.sample_per_part,\n                              self.trans_std)\n'"
external/DCNv2/setup.py,5,"b'#!/usr/bin/env python\n\nimport os\nimport glob\n\nimport torch\n\nfrom torch.utils.cpp_extension import CUDA_HOME\nfrom torch.utils.cpp_extension import CppExtension\nfrom torch.utils.cpp_extension import CUDAExtension\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\nrequirements = [""torch"", ""torchvision""]\n\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, ""src"")\n\n    main_file = glob.glob(os.path.join(extensions_dir, ""*.cpp""))\n    source_cpu = glob.glob(os.path.join(extensions_dir, ""cpu"", ""*.cpp""))\n    source_cuda = glob.glob(os.path.join(extensions_dir, ""cuda"", ""*.cu""))\n\n    sources = main_file + source_cpu\n    extension = CppExtension\n    extra_compile_args = {""cxx"": []}\n    define_macros = []\n\n    if torch.cuda.is_available() and CUDA_HOME is not None:\n        extension = CUDAExtension\n        sources += source_cuda\n        define_macros += [(""WITH_CUDA"", None)]\n        extra_compile_args[""nvcc""] = [\n            ""-DCUDA_HAS_FP16=1"",\n            ""-D__CUDA_NO_HALF_OPERATORS__"",\n            ""-D__CUDA_NO_HALF_CONVERSIONS__"",\n            ""-D__CUDA_NO_HALF2_OPERATORS__"",\n        ]\n    else:\n        raise NotImplementedError(\'Cuda is not available\')\n\n    sources = [os.path.join(extensions_dir, s) for s in sources]\n    include_dirs = [extensions_dir]\n    ext_modules = [\n        extension(\n            ""_ext"",\n            sources,\n            include_dirs=include_dirs,\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n    ]\n    return ext_modules\n\nsetup(\n    name=""DCNv2"",\n    version=""0.1"",\n    author=""charlesshang"",\n    url=""https://github.com/charlesshang/DCNv2"",\n    description=""deformable convolutional networks"",\n    packages=find_packages(exclude=(""configs"", ""tests"",)),\n    # install_requires=requirements,\n    ext_modules=get_extensions(),\n    cmdclass={""build_ext"": torch.utils.cpp_extension.BuildExtension},\n)\n'"
external/DCNv2/test.py,37,"b'#!/usr/bin/env python\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport time\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import gradcheck\n\nfrom dcn_v2 import dcn_v2_conv, DCNv2, DCN\nfrom dcn_v2 import dcn_v2_pooling, DCNv2Pooling, DCNPooling\n\ndeformable_groups = 1\nN, inC, inH, inW = 2, 2, 4, 4\noutC = 2\nkH, kW = 3, 3\n\n\ndef conv_identify(weight, bias):\n    weight.data.zero_()\n    bias.data.zero_()\n    o, i, h, w = weight.shape\n    y = h//2\n    x = w//2\n    for p in range(i):\n        for q in range(o):\n            if p == q:\n                weight.data[q, p, y, x] = 1.0\n\n\ndef check_zero_offset():\n    conv_offset = nn.Conv2d(inC, deformable_groups * 2 * kH * kW,\n                            kernel_size=(kH, kW),\n                            stride=(1, 1),\n                            padding=(1, 1),\n                            bias=True).cuda()\n\n    conv_mask = nn.Conv2d(inC, deformable_groups * 1 * kH * kW,\n                          kernel_size=(kH, kW),\n                          stride=(1, 1),\n                          padding=(1, 1),\n                          bias=True).cuda()\n\n    dcn_v2 = DCNv2(inC, outC, (kH, kW),\n                   stride=1, padding=1, dilation=1,\n                   deformable_groups=deformable_groups).cuda()\n\n    conv_offset.weight.data.zero_()\n    conv_offset.bias.data.zero_()\n    conv_mask.weight.data.zero_()\n    conv_mask.bias.data.zero_()\n    conv_identify(dcn_v2.weight, dcn_v2.bias)\n\n    input = torch.randn(N, inC, inH, inW).cuda()\n    offset = conv_offset(input)\n    mask = conv_mask(input)\n    mask = torch.sigmoid(mask)\n    output = dcn_v2(input, offset, mask)\n    output *= 2\n    d = (input - output).abs().max()\n    if d < 1e-10:\n        print(\'Zero offset passed\')\n    else:\n        print(\'Zero offset failed\')\n        print(input)\n        print(output)\n\ndef check_gradient_dconv():\n\n    input = torch.rand(N, inC, inH, inW).cuda() * 0.01\n    input.requires_grad = True\n\n    offset = torch.randn(N, deformable_groups * 2 * kW * kH, inH, inW).cuda() * 2\n    # offset.data.zero_()\n    # offset.data -= 0.5\n    offset.requires_grad = True\n\n    mask = torch.rand(N, deformable_groups * 1 * kW * kH, inH, inW).cuda()\n    # mask.data.zero_()\n    mask.requires_grad = True\n    mask = torch.sigmoid(mask)\n\n    weight = torch.randn(outC, inC, kH, kW).cuda()\n    weight.requires_grad = True\n\n    bias = torch.rand(outC).cuda()\n    bias.requires_grad = True\n\n    stride = 1\n    padding = 1\n    dilation = 1\n\n    print(\'check_gradient_dconv: \',\n          gradcheck(dcn_v2_conv, (input, offset, mask, weight, bias,\n                    stride, padding, dilation, deformable_groups),\n                    eps=1e-3, atol=1e-4, rtol=1e-2))\n\n\ndef check_pooling_zero_offset():\n\n    input = torch.randn(2, 16, 64, 64).cuda().zero_()\n    input[0, :, 16:26, 16:26] = 1.\n    input[1, :, 10:20, 20:30] = 2.\n    rois = torch.tensor([\n        [0, 65, 65, 103, 103],\n        [1, 81, 41, 119, 79],\n    ]).cuda().float()\n    pooling = DCNv2Pooling(spatial_scale=1.0 / 4,\n                           pooled_size=7,\n                           output_dim=16,\n                           no_trans=True,\n                           group_size=1,\n                           trans_std=0.0).cuda()\n\n    out = pooling(input, rois, input.new())\n    s = \', \'.join([\'%f\' % out[i, :, :, :].mean().item()\n                   for i in range(rois.shape[0])])\n    print(s)\n\n    dpooling = DCNv2Pooling(spatial_scale=1.0 / 4,\n                            pooled_size=7,\n                            output_dim=16,\n                            no_trans=False,\n                            group_size=1,\n                            trans_std=0.0).cuda()\n    offset = torch.randn(20, 2, 7, 7).cuda().zero_()\n    dout = dpooling(input, rois, offset)\n    s = \', \'.join([\'%f\' % dout[i, :, :, :].mean().item()\n                   for i in range(rois.shape[0])])\n    print(s)\n\n\ndef check_gradient_dpooling():\n    input = torch.randn(2, 3, 5, 5).cuda() * 0.01\n    N = 4\n    batch_inds = torch.randint(2, (N, 1)).cuda().float()\n    x = torch.rand((N, 1)).cuda().float() * 15\n    y = torch.rand((N, 1)).cuda().float() * 15\n    w = torch.rand((N, 1)).cuda().float() * 10\n    h = torch.rand((N, 1)).cuda().float() * 10\n    rois = torch.cat((batch_inds, x, y, x + w, y + h), dim=1)\n    offset = torch.randn(N, 2, 3, 3).cuda()\n    input.requires_grad = True\n    offset.requires_grad = True\n\n    spatial_scale = 1.0 / 4\n    pooled_size = 3\n    output_dim = 3\n    no_trans = 0\n    group_size = 1\n    trans_std = 0.0\n    sample_per_part = 4\n    part_size = pooled_size\n\n    print(\'check_gradient_dpooling:\',\n          gradcheck(dcn_v2_pooling, (input, rois, offset,\n                                     spatial_scale,\n                                     pooled_size,\n                                     output_dim,\n                                     no_trans,\n                                     group_size,\n                                     part_size,\n                                     sample_per_part,\n                                     trans_std),\n                    eps=1e-4))\n\n\ndef example_dconv():\n    input = torch.randn(2, 64, 128, 128).cuda()\n    # wrap all things (offset and mask) in DCN\n    dcn = DCN(64, 64, kernel_size=(3, 3), stride=1,\n              padding=1, deformable_groups=2).cuda()\n    # print(dcn.weight.shape, input.shape)\n    output = dcn(input)\n    targert = output.new(*output.size())\n    targert.data.uniform_(-0.01, 0.01)\n    error = (targert - output).mean()\n    error.backward()\n    print(output.shape)\n\n\ndef example_dpooling():\n    input = torch.randn(2, 32, 64, 64).cuda()\n    batch_inds = torch.randint(2, (20, 1)).cuda().float()\n    x = torch.randint(256, (20, 1)).cuda().float()\n    y = torch.randint(256, (20, 1)).cuda().float()\n    w = torch.randint(64, (20, 1)).cuda().float()\n    h = torch.randint(64, (20, 1)).cuda().float()\n    rois = torch.cat((batch_inds, x, y, x + w, y + h), dim=1)\n    offset = torch.randn(20, 2, 7, 7).cuda()\n    input.requires_grad = True\n    offset.requires_grad = True\n\n    # normal roi_align\n    pooling = DCNv2Pooling(spatial_scale=1.0 / 4,\n                           pooled_size=7,\n                           output_dim=32,\n                           no_trans=True,\n                           group_size=1,\n                           trans_std=0.1).cuda()\n\n    # deformable pooling\n    dpooling = DCNv2Pooling(spatial_scale=1.0 / 4,\n                            pooled_size=7,\n                            output_dim=32,\n                            no_trans=False,\n                            group_size=1,\n                            trans_std=0.1).cuda()\n\n    out = pooling(input, rois, offset)\n    dout = dpooling(input, rois, offset)\n    print(out.shape)\n    print(dout.shape)\n\n    target_out = out.new(*out.size())\n    target_out.data.uniform_(-0.01, 0.01)\n    target_dout = dout.new(*dout.size())\n    target_dout.data.uniform_(-0.01, 0.01)\n    e = (target_out - out).mean()\n    e.backward()\n    e = (target_dout - dout).mean()\n    e.backward()\n\n\ndef example_mdpooling():\n    input = torch.randn(2, 32, 64, 64).cuda()\n    input.requires_grad = True\n    batch_inds = torch.randint(2, (20, 1)).cuda().float()\n    x = torch.randint(256, (20, 1)).cuda().float()\n    y = torch.randint(256, (20, 1)).cuda().float()\n    w = torch.randint(64, (20, 1)).cuda().float()\n    h = torch.randint(64, (20, 1)).cuda().float()\n    rois = torch.cat((batch_inds, x, y, x + w, y + h), dim=1)\n\n    # mdformable pooling (V2)\n    dpooling = DCNPooling(spatial_scale=1.0 / 4,\n                          pooled_size=7,\n                          output_dim=32,\n                          no_trans=False,\n                          group_size=1,\n                          trans_std=0.1,\n                          deform_fc_dim=1024).cuda()\n\n    dout = dpooling(input, rois)\n    target = dout.new(*dout.size())\n    target.data.uniform_(-0.1, 0.1)\n    error = (target - dout).mean()\n    error.backward()\n    print(dout.shape)\n\n\nif __name__ == \'__main__\':\n\n    example_dconv()\n    example_dpooling()\n    example_mdpooling()\n\n    check_pooling_zero_offset()\n    # zero offset check\n    if inC == outC:\n        check_zero_offset()\n\n    check_gradient_dpooling()\n    check_gradient_dconv()\n    # """"""\n    # ****** Note: backward is not reentrant error may not be a serious problem,\n    # ****** since the max error is less than 1e-7,\n    # ****** Still looking for what trigger this problem\n    # """"""\n'"
layers/functions/__init__.py,0,"b""from .detection import Detect\n\n\n__all__ = ['Detect']\n"""
layers/functions/detection.py,10,"b'import torch\nimport torch.nn.functional as F\nfrom ..box_utils import decode, jaccard, index2d\nfrom utils import timer\n\nfrom data import cfg, mask_type\n\nimport numpy as np\n\n\nclass Detect(object):\n    """"""At test time, Detect is the final layer of SSD.  Decode location preds,\n    apply non-maximum suppression to location predictions based on conf\n    scores and threshold to a top_k number of output predictions for both\n    confidence score and locations, as the predicted masks.\n    """"""\n    # TODO: Refactor this whole class away. It needs to go.\n\n    def __init__(self, num_classes, bkg_label, top_k, conf_thresh, nms_thresh):\n        self.num_classes = num_classes\n        self.background_label = bkg_label\n        self.top_k = top_k\n        # Parameters used in nms.\n        self.nms_thresh = nms_thresh\n        if nms_thresh <= 0:\n            raise ValueError(\'nms_threshold must be non negative.\')\n        self.conf_thresh = conf_thresh\n        \n        self.use_cross_class_nms = False\n        self.use_fast_nms = False\n\n    def __call__(self, predictions, net):\n        """"""\n        Args:\n             loc_data: (tensor) Loc preds from loc layers\n                Shape: [batch, num_priors, 4]\n            conf_data: (tensor) Shape: Conf preds from conf layers\n                Shape: [batch, num_priors, num_classes]\n            mask_data: (tensor) Mask preds from mask layers\n                Shape: [batch, num_priors, mask_dim]\n            prior_data: (tensor) Prior boxes and variances from priorbox layers\n                Shape: [num_priors, 4]\n            proto_data: (tensor) If using mask_type.lincomb, the prototype masks\n                Shape: [batch, mask_h, mask_w, mask_dim]\n        \n        Returns:\n            output of shape (batch_size, top_k, 1 + 1 + 4 + mask_dim)\n            These outputs are in the order: class idx, confidence, bbox coords, and mask.\n\n            Note that the outputs are sorted only if cross_class_nms is False\n        """"""\n\n        loc_data   = predictions[\'loc\']\n        conf_data  = predictions[\'conf\']\n        mask_data  = predictions[\'mask\']\n        prior_data = predictions[\'priors\']\n\n        proto_data = predictions[\'proto\'] if \'proto\' in predictions else None\n        inst_data  = predictions[\'inst\']  if \'inst\'  in predictions else None\n\n        out = []\n\n        with timer.env(\'Detect\'):\n            batch_size = loc_data.size(0)\n            num_priors = prior_data.size(0)\n\n            conf_preds = conf_data.view(batch_size, num_priors, self.num_classes).transpose(2, 1).contiguous()\n\n            for batch_idx in range(batch_size):\n                decoded_boxes = decode(loc_data[batch_idx], prior_data)\n                result = self.detect(batch_idx, conf_preds, decoded_boxes, mask_data, inst_data)\n\n                if result is not None and proto_data is not None:\n                    result[\'proto\'] = proto_data[batch_idx]\n\n                out.append({\'detection\': result, \'net\': net})\n        \n        return out\n\n\n    def detect(self, batch_idx, conf_preds, decoded_boxes, mask_data, inst_data):\n        """""" Perform nms for only the max scoring class that isn\'t background (class 0) """"""\n        cur_scores = conf_preds[batch_idx, 1:, :]\n        conf_scores, _ = torch.max(cur_scores, dim=0)\n\n        keep = (conf_scores > self.conf_thresh)\n        scores = cur_scores[:, keep]\n        boxes = decoded_boxes[keep, :]\n        masks = mask_data[batch_idx, keep, :]\n\n        if inst_data is not None:\n            inst = inst_data[batch_idx, keep, :]\n    \n        if scores.size(1) == 0:\n            return None\n        \n        if self.use_fast_nms:\n            if self.use_cross_class_nms:\n                boxes, masks, classes, scores = self.cc_fast_nms(boxes, masks, scores, self.nms_thresh, self.top_k)\n            else:\n                boxes, masks, classes, scores = self.fast_nms(boxes, masks, scores, self.nms_thresh, self.top_k)\n        else:\n            boxes, masks, classes, scores = self.traditional_nms(boxes, masks, scores, self.nms_thresh, self.conf_thresh)\n\n            if self.use_cross_class_nms:\n                print(\'Warning: Cross Class Traditional NMS is not implemented.\')\n\n        return {\'box\': boxes, \'mask\': masks, \'class\': classes, \'score\': scores}\n\n\n    def cc_fast_nms(self, boxes, masks, scores, iou_threshold:float=0.5, top_k:int=200):\n        # Collapse all the classes into 1 \n        scores, classes = scores.max(dim=0)\n\n        _, idx = scores.sort(0, descending=True)\n        idx = idx[:top_k]\n\n        boxes_idx = boxes[idx]\n\n        # Compute the pairwise IoU between the boxes\n        iou = jaccard(boxes_idx, boxes_idx)\n        \n        # Zero out the lower triangle of the cosine similarity matrix and diagonal\n        iou.triu_(diagonal=1)\n\n        # Now that everything in the diagonal and below is zeroed out, if we take the max\n        # of the IoU matrix along the columns, each column will represent the maximum IoU\n        # between this element and every element with a higher score than this element.\n        iou_max, _ = torch.max(iou, dim=0)\n\n        # Now just filter out the ones greater than the threshold, i.e., only keep boxes that\n        # don\'t have a higher scoring box that would supress it in normal NMS.\n        idx_out = idx[iou_max <= iou_threshold]\n        \n        return boxes[idx_out], masks[idx_out], classes[idx_out], scores[idx_out]\n\n    def fast_nms(self, boxes, masks, scores, iou_threshold:float=0.5, top_k:int=200, second_threshold:bool=False):\n        scores, idx = scores.sort(1, descending=True)\n\n        idx = idx[:, :top_k].contiguous()\n        scores = scores[:, :top_k]\n    \n        num_classes, num_dets = idx.size()\n\n        boxes = boxes[idx.view(-1), :].view(num_classes, num_dets, 4)\n        masks = masks[idx.view(-1), :].view(num_classes, num_dets, -1)\n\n        iou = jaccard(boxes, boxes)\n        iou.triu_(diagonal=1)\n        iou_max, _ = iou.max(dim=1)\n\n        # Now just filter out the ones higher than the threshold\n        keep = (iou_max <= iou_threshold)\n\n        # We should also only keep detections over the confidence threshold, but at the cost of\n        # maxing out your detection count for every image, you can just not do that. Because we\n        # have such a minimal amount of computation per detection (matrix mulitplication only),\n        # this increase doesn\'t affect us much (+0.2 mAP for 34 -> 33 fps), so we leave it out.\n        # However, when you implement this in your method, you should do this second threshold.\n        if second_threshold:\n            keep *= (scores > self.conf_thresh)\n\n        # Assign each kept detection to its corresponding class\n        classes = torch.arange(num_classes, device=boxes.device)[:, None].expand_as(keep)\n        classes = classes[keep]\n\n        boxes = boxes[keep]\n        masks = masks[keep]\n        scores = scores[keep]\n        \n        # Only keep the top cfg.max_num_detections highest scores across all classes\n        scores, idx = scores.sort(0, descending=True)\n        idx = idx[:cfg.max_num_detections]\n        scores = scores[:cfg.max_num_detections]\n\n        classes = classes[idx]\n        boxes = boxes[idx]\n        masks = masks[idx]\n\n        return boxes, masks, classes, scores\n\n    def traditional_nms(self, boxes, masks, scores, iou_threshold=0.5, conf_thresh=0.05):\n        import pyximport\n        pyximport.install(setup_args={""include_dirs"":np.get_include()}, reload_support=True)\n\n        from utils.cython_nms import nms as cnms\n\n        num_classes = scores.size(0)\n\n        idx_lst = []\n        cls_lst = []\n        scr_lst = []\n\n        # Multiplying by max_size is necessary because of how cnms computes its area and intersections\n        boxes = boxes * cfg.max_size\n\n        for _cls in range(num_classes):\n            cls_scores = scores[_cls, :]\n            conf_mask = cls_scores > conf_thresh\n            idx = torch.arange(cls_scores.size(0), device=boxes.device)\n\n            cls_scores = cls_scores[conf_mask]\n            idx = idx[conf_mask]\n\n            if cls_scores.size(0) == 0:\n                continue\n            \n            preds = torch.cat([boxes[conf_mask], cls_scores[:, None]], dim=1).cpu().numpy()\n            keep = cnms(preds, iou_threshold)\n            keep = torch.Tensor(keep, device=boxes.device).long()\n\n            idx_lst.append(idx[keep])\n            cls_lst.append(keep * 0 + _cls)\n            scr_lst.append(cls_scores[keep])\n        \n        idx     = torch.cat(idx_lst, dim=0)\n        classes = torch.cat(cls_lst, dim=0)\n        scores  = torch.cat(scr_lst, dim=0)\n\n        scores, idx2 = scores.sort(0, descending=True)\n        idx2 = idx2[:cfg.max_num_detections]\n        scores = scores[:cfg.max_num_detections]\n\n        idx = idx[idx2]\n        classes = classes[idx2]\n\n        # Undo the multiplication above\n        return boxes[idx] / cfg.max_size, masks[idx], classes, scores\n'"
layers/modules/__init__.py,0,"b""from .multibox_loss import MultiBoxLoss\n\n__all__ = ['MultiBoxLoss']\n"""
layers/modules/multibox_loss.py,43,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom ..box_utils import match, log_sum_exp, decode, center_size, crop, elemwise_mask_iou, elemwise_box_iou\n\nfrom data import cfg, mask_type, activation_func\n\nclass MultiBoxLoss(nn.Module):\n    """"""SSD Weighted Loss Function\n    Compute Targets:\n        1) Produce Confidence Target Indices by matching  ground truth boxes\n           with (default) \'priorboxes\' that have jaccard index > threshold parameter\n           (default threshold: 0.5).\n        2) Produce localization target by \'encoding\' variance into offsets of ground\n           truth boxes and their matched  \'priorboxes\'.\n        3) Hard negative mining to filter the excessive number of negative examples\n           that comes with using a large number of default bounding boxes.\n           (default negative:positive ratio 3:1)\n    Objective Loss:\n        L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n        weighted by \xce\xb1 which is set to 1 by cross val.\n        Args:\n            c: class confidences,\n            l: predicted boxes,\n            g: ground truth boxes\n            N: number of matched default boxes\n        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n    """"""\n\n    def __init__(self, num_classes, pos_threshold, neg_threshold, negpos_ratio):\n        super(MultiBoxLoss, self).__init__()\n        self.num_classes = num_classes\n        \n        self.pos_threshold = pos_threshold\n        self.neg_threshold = neg_threshold\n        self.negpos_ratio = negpos_ratio\n\n        # If you output a proto mask with this area, your l1 loss will be l1_alpha\n        # Note that the area is relative (so 1 would be the entire image)\n        self.l1_expected_area = 20*20/70/70\n        self.l1_alpha = 0.1\n\n        if cfg.use_class_balanced_conf:\n            self.class_instances = None\n            self.total_instances = 0\n\n    def forward(self, net, predictions, targets, masks, num_crowds):\n        """"""Multibox Loss\n        Args:\n            predictions (tuple): A tuple containing loc preds, conf preds,\n            mask preds, and prior boxes from SSD net.\n                loc shape: torch.size(batch_size,num_priors,4)\n                conf shape: torch.size(batch_size,num_priors,num_classes)\n                masks shape: torch.size(batch_size,num_priors,mask_dim)\n                priors shape: torch.size(num_priors,4)\n                proto* shape: torch.size(batch_size,mask_h,mask_w,mask_dim)\n\n            targets (list<tensor>): Ground truth boxes and labels for a batch,\n                shape: [batch_size][num_objs,5] (last idx is the label).\n\n            masks (list<tensor>): Ground truth masks for each object in each image,\n                shape: [batch_size][num_objs,im_height,im_width]\n\n            num_crowds (list<int>): Number of crowd annotations per batch. The crowd\n                annotations should be the last num_crowds elements of targets and masks.\n            \n            * Only if mask_type == lincomb\n        """"""\n\n        loc_data  = predictions[\'loc\']\n        conf_data = predictions[\'conf\']\n        mask_data = predictions[\'mask\']\n        priors    = predictions[\'priors\']\n\n        if cfg.mask_type == mask_type.lincomb:\n            proto_data = predictions[\'proto\']\n\n        score_data = predictions[\'score\'] if cfg.use_mask_scoring   else None   \n        inst_data  = predictions[\'inst\']  if cfg.use_instance_coeff else None\n        \n        labels = [None] * len(targets) # Used in sem segm loss\n\n        batch_size = loc_data.size(0)\n        num_priors = priors.size(0)\n        num_classes = self.num_classes\n\n        # Match priors (default boxes) and ground truth boxes\n        # These tensors will be created with the same device as loc_data\n        loc_t = loc_data.new(batch_size, num_priors, 4)\n        gt_box_t = loc_data.new(batch_size, num_priors, 4)\n        conf_t = loc_data.new(batch_size, num_priors).long()\n        idx_t = loc_data.new(batch_size, num_priors).long()\n\n        if cfg.use_class_existence_loss:\n            class_existence_t = loc_data.new(batch_size, num_classes-1)\n\n        for idx in range(batch_size):\n            truths      = targets[idx][:, :-1].data\n            labels[idx] = targets[idx][:, -1].data.long()\n\n            if cfg.use_class_existence_loss:\n                # Construct a one-hot vector for each object and collapse it into an existence vector with max\n                # Also it\'s fine to include the crowd annotations here\n                class_existence_t[idx, :] = torch.eye(num_classes-1, device=conf_t.get_device())[labels[idx]].max(dim=0)[0]\n\n            # Split the crowd annotations because they come bundled in\n            cur_crowds = num_crowds[idx]\n            if cur_crowds > 0:\n                split = lambda x: (x[-cur_crowds:], x[:-cur_crowds])\n                crowd_boxes, truths = split(truths)\n\n                # We don\'t use the crowd labels or masks\n                _, labels[idx] = split(labels[idx])\n                _, masks[idx]  = split(masks[idx])\n            else:\n                crowd_boxes = None\n\n            \n            match(self.pos_threshold, self.neg_threshold,\n                  truths, priors.data, labels[idx], crowd_boxes,\n                  loc_t, conf_t, idx_t, idx, loc_data[idx])\n                  \n            gt_box_t[idx, :, :] = truths[idx_t[idx]]\n\n        # wrap targets\n        loc_t = Variable(loc_t, requires_grad=False)\n        conf_t = Variable(conf_t, requires_grad=False)\n        idx_t = Variable(idx_t, requires_grad=False)\n\n        pos = conf_t > 0\n        num_pos = pos.sum(dim=1, keepdim=True)\n        \n        # Shape: [batch,num_priors,4]\n        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n        \n        losses = {}\n\n        # Localization Loss (Smooth L1)\n        if cfg.train_boxes:\n            loc_p = loc_data[pos_idx].view(-1, 4)\n            loc_t = loc_t[pos_idx].view(-1, 4)\n            losses[\'B\'] = F.smooth_l1_loss(loc_p, loc_t, reduction=\'sum\') * cfg.bbox_alpha\n\n        if cfg.train_masks:\n            if cfg.mask_type == mask_type.direct:\n                if cfg.use_gt_bboxes:\n                    pos_masks = []\n                    for idx in range(batch_size):\n                        pos_masks.append(masks[idx][idx_t[idx, pos[idx]]])\n                    masks_t = torch.cat(pos_masks, 0)\n                    masks_p = mask_data[pos, :].view(-1, cfg.mask_dim)\n                    losses[\'M\'] = F.binary_cross_entropy(torch.clamp(masks_p, 0, 1), masks_t, reduction=\'sum\') * cfg.mask_alpha\n                else:\n                    losses[\'M\'] = self.direct_mask_loss(pos_idx, idx_t, loc_data, mask_data, priors, masks)\n            elif cfg.mask_type == mask_type.lincomb:\n                ret = self.lincomb_mask_loss(pos, idx_t, loc_data, mask_data, priors, proto_data, masks, gt_box_t, score_data, inst_data, labels)\n                if cfg.use_maskiou:\n                    loss, maskiou_targets = ret\n                else:\n                    loss = ret\n                losses.update(loss)\n\n                if cfg.mask_proto_loss is not None:\n                    if cfg.mask_proto_loss == \'l1\':\n                        losses[\'P\'] = torch.mean(torch.abs(proto_data)) / self.l1_expected_area * self.l1_alpha\n                    elif cfg.mask_proto_loss == \'disj\':\n                        losses[\'P\'] = -torch.mean(torch.max(F.log_softmax(proto_data, dim=-1), dim=-1)[0])\n\n        # Confidence loss\n        if cfg.use_focal_loss:\n            if cfg.use_sigmoid_focal_loss:\n                losses[\'C\'] = self.focal_conf_sigmoid_loss(conf_data, conf_t)\n            elif cfg.use_objectness_score:\n                losses[\'C\'] = self.focal_conf_objectness_loss(conf_data, conf_t)\n            else:\n                losses[\'C\'] = self.focal_conf_loss(conf_data, conf_t)\n        else:\n            if cfg.use_objectness_score:\n                losses[\'C\'] = self.conf_objectness_loss(conf_data, conf_t, batch_size, loc_p, loc_t, priors)\n            else:\n                losses[\'C\'] = self.ohem_conf_loss(conf_data, conf_t, pos, batch_size)\n\n        # Mask IoU Loss\n        if cfg.use_maskiou and maskiou_targets is not None:\n            losses[\'I\'] = self.mask_iou_loss(net, maskiou_targets)\n\n        # These losses also don\'t depend on anchors\n        if cfg.use_class_existence_loss:\n            losses[\'E\'] = self.class_existence_loss(predictions[\'classes\'], class_existence_t)\n        if cfg.use_semantic_segmentation_loss:\n            losses[\'S\'] = self.semantic_segmentation_loss(predictions[\'segm\'], masks, labels)\n\n        # Divide all losses by the number of positives.\n        # Don\'t do it for loss[P] because that doesn\'t depend on the anchors.\n        total_num_pos = num_pos.data.sum().float()\n        for k in losses:\n            if k not in (\'P\', \'E\', \'S\'):\n                losses[k] /= total_num_pos\n            else:\n                losses[k] /= batch_size\n\n        # Loss Key:\n        #  - B: Box Localization Loss\n        #  - C: Class Confidence Loss\n        #  - M: Mask Loss\n        #  - P: Prototype Loss\n        #  - D: Coefficient Diversity Loss\n        #  - E: Class Existence Loss\n        #  - S: Semantic Segmentation Loss\n        return losses\n\n    def class_existence_loss(self, class_data, class_existence_t):\n        return cfg.class_existence_alpha * F.binary_cross_entropy_with_logits(class_data, class_existence_t, reduction=\'sum\')\n\n    def semantic_segmentation_loss(self, segment_data, mask_t, class_t, interpolation_mode=\'bilinear\'):\n        # Note num_classes here is without the background class so cfg.num_classes-1\n        batch_size, num_classes, mask_h, mask_w = segment_data.size()\n        loss_s = 0\n        \n        for idx in range(batch_size):\n            cur_segment = segment_data[idx]\n            cur_class_t = class_t[idx]\n\n            with torch.no_grad():\n                downsampled_masks = F.interpolate(mask_t[idx].unsqueeze(0), (mask_h, mask_w),\n                                                  mode=interpolation_mode, align_corners=False).squeeze(0)\n                downsampled_masks = downsampled_masks.gt(0.5).float()\n                \n                # Construct Semantic Segmentation\n                segment_t = torch.zeros_like(cur_segment, requires_grad=False)\n                for obj_idx in range(downsampled_masks.size(0)):\n                    segment_t[cur_class_t[obj_idx]] = torch.max(segment_t[cur_class_t[obj_idx]], downsampled_masks[obj_idx])\n            \n            loss_s += F.binary_cross_entropy_with_logits(cur_segment, segment_t, reduction=\'sum\')\n        \n        return loss_s / mask_h / mask_w * cfg.semantic_segmentation_alpha\n\n\n    def ohem_conf_loss(self, conf_data, conf_t, pos, num):\n        # Compute max conf across batch for hard negative mining\n        batch_conf = conf_data.view(-1, self.num_classes)\n        if cfg.ohem_use_most_confident:\n            # i.e. max(softmax) along classes > 0 \n            batch_conf = F.softmax(batch_conf, dim=1)\n            loss_c, _ = batch_conf[:, 1:].max(dim=1)\n        else:\n            # i.e. -softmax(class 0 confidence)\n            loss_c = log_sum_exp(batch_conf) - batch_conf[:, 0]\n\n        # Hard Negative Mining\n        loss_c = loss_c.view(num, -1)\n        loss_c[pos]        = 0 # filter out pos boxes\n        loss_c[conf_t < 0] = 0 # filter out neutrals (conf_t = -1)\n        _, loss_idx = loss_c.sort(1, descending=True)\n        _, idx_rank = loss_idx.sort(1)\n        num_pos = pos.long().sum(1, keepdim=True)\n        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n        neg = idx_rank < num_neg.expand_as(idx_rank)\n        \n        # Just in case there aren\'t enough negatives, don\'t start using positives as negatives\n        neg[pos]        = 0\n        neg[conf_t < 0] = 0 # Filter out neutrals\n\n        # Confidence Loss Including Positive and Negative Examples\n        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n        targets_weighted = conf_t[(pos+neg).gt(0)]\n        loss_c = F.cross_entropy(conf_p, targets_weighted, reduction=\'none\')\n\n        if cfg.use_class_balanced_conf:\n            # Lazy initialization\n            if self.class_instances is None:\n                self.class_instances = torch.zeros(self.num_classes, device=targets_weighted.device)\n            \n            classes, counts = targets_weighted.unique(return_counts=True)\n            \n            for _cls, _cnt in zip(classes.cpu().numpy(), counts.cpu().numpy()):\n                self.class_instances[_cls] += _cnt\n\n            self.total_instances += targets_weighted.size(0)\n\n            weighting = 1 - (self.class_instances[targets_weighted] / self.total_instances)\n            weighting = torch.clamp(weighting, min=1/self.num_classes)\n\n            # If you do the math, the average weight of self.class_instances is this\n            avg_weight = (self.num_classes - 1) / self.num_classes\n\n            loss_c = (loss_c * weighting).sum() / avg_weight\n        else:\n            loss_c = loss_c.sum()\n        \n        return cfg.conf_alpha * loss_c\n\n    def focal_conf_loss(self, conf_data, conf_t):\n        """"""\n        Focal loss as described in https://arxiv.org/pdf/1708.02002.pdf\n        Adapted from https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py\n        Note that this uses softmax and not the original sigmoid from the paper.\n        """"""\n        conf_t = conf_t.view(-1) # [batch_size*num_priors]\n        conf_data = conf_data.view(-1, conf_data.size(-1)) # [batch_size*num_priors, num_classes]\n\n        # Ignore neutral samples (class < 0)\n        keep = (conf_t >= 0).float()\n        conf_t[conf_t < 0] = 0 # so that gather doesn\'t drum up a fuss\n\n        logpt = F.log_softmax(conf_data, dim=-1)\n        logpt = logpt.gather(1, conf_t.unsqueeze(-1))\n        logpt = logpt.view(-1)\n        pt    = logpt.exp()\n\n        # I adapted the alpha_t calculation here from\n        # https://github.com/pytorch/pytorch/blob/master/modules/detectron/softmax_focal_loss_op.cu\n        # You\'d think you want all the alphas to sum to one, but in the original implementation they\n        # just give background an alpha of 1-alpha and each forground an alpha of alpha.\n        background = (conf_t == 0).float()\n        at = (1 - cfg.focal_loss_alpha) * background + cfg.focal_loss_alpha * (1 - background)\n\n        loss = -at * (1 - pt) ** cfg.focal_loss_gamma * logpt\n\n        # See comment above for keep\n        return cfg.conf_alpha * (loss * keep).sum()\n    \n    def focal_conf_sigmoid_loss(self, conf_data, conf_t):\n        """"""\n        Focal loss but using sigmoid like the original paper.\n        Note: To make things mesh easier, the network still predicts 81 class confidences in this mode.\n              Because retinanet originally only predicts 80, we simply just don\'t use conf_data[..., 0]\n        """"""\n        num_classes = conf_data.size(-1)\n\n        conf_t = conf_t.view(-1) # [batch_size*num_priors]\n        conf_data = conf_data.view(-1, num_classes) # [batch_size*num_priors, num_classes]\n\n        # Ignore neutral samples (class < 0)\n        keep = (conf_t >= 0).float()\n        conf_t[conf_t < 0] = 0 # can\'t mask with -1, so filter that out\n\n        # Compute a one-hot embedding of conf_t\n        # From https://github.com/kuangliu/pytorch-retinanet/blob/master/utils.py\n        conf_one_t = torch.eye(num_classes, device=conf_t.get_device())[conf_t]\n        conf_pm_t  = conf_one_t * 2 - 1 # -1 if background, +1 if forground for specific class\n\n        logpt = F.logsigmoid(conf_data * conf_pm_t) # note: 1 - sigmoid(x) = sigmoid(-x)\n        pt    = logpt.exp()\n\n        at = cfg.focal_loss_alpha * conf_one_t + (1 - cfg.focal_loss_alpha) * (1 - conf_one_t)\n        at[..., 0] = 0 # Set alpha for the background class to 0 because sigmoid focal loss doesn\'t use it\n\n        loss = -at * (1 - pt) ** cfg.focal_loss_gamma * logpt\n        loss = keep * loss.sum(dim=-1)\n\n        return cfg.conf_alpha * loss.sum()\n    \n    def focal_conf_objectness_loss(self, conf_data, conf_t):\n        """"""\n        Instead of using softmax, use class[0] to be the objectness score and do sigmoid focal loss on that.\n        Then for the rest of the classes, softmax them and apply CE for only the positive examples.\n\n        If class[0] = 1 implies forground and class[0] = 0 implies background then you achieve something\n        similar during test-time to softmax by setting class[1:] = softmax(class[1:]) * class[0] and invert class[0].\n        """"""\n\n        conf_t = conf_t.view(-1) # [batch_size*num_priors]\n        conf_data = conf_data.view(-1, conf_data.size(-1)) # [batch_size*num_priors, num_classes]\n\n        # Ignore neutral samples (class < 0)\n        keep = (conf_t >= 0).float()\n        conf_t[conf_t < 0] = 0 # so that gather doesn\'t drum up a fuss\n\n        background = (conf_t == 0).float()\n        at = (1 - cfg.focal_loss_alpha) * background + cfg.focal_loss_alpha * (1 - background)\n\n        logpt = F.logsigmoid(conf_data[:, 0]) * (1 - background) + F.logsigmoid(-conf_data[:, 0]) * background\n        pt    = logpt.exp()\n\n        obj_loss = -at * (1 - pt) ** cfg.focal_loss_gamma * logpt\n\n        # All that was the objectiveness loss--now time for the class confidence loss\n        pos_mask = conf_t > 0\n        conf_data_pos = (conf_data[:, 1:])[pos_mask] # Now this has just 80 classes\n        conf_t_pos    = conf_t[pos_mask] - 1         # So subtract 1 here\n\n        class_loss = F.cross_entropy(conf_data_pos, conf_t_pos, reduction=\'sum\')\n\n        return cfg.conf_alpha * (class_loss + (obj_loss * keep).sum())\n    \n    def conf_objectness_loss(self, conf_data, conf_t, batch_size, loc_p, loc_t, priors):\n        """"""\n        Instead of using softmax, use class[0] to be p(obj) * p(IoU) as in YOLO.\n        Then for the rest of the classes, softmax them and apply CE for only the positive examples.\n        """"""\n\n        conf_t = conf_t.view(-1) # [batch_size*num_priors]\n        conf_data = conf_data.view(-1, conf_data.size(-1)) # [batch_size*num_priors, num_classes]\n\n        pos_mask = (conf_t > 0)\n        neg_mask = (conf_t == 0)\n\n        obj_data = conf_data[:, 0]\n        obj_data_pos = obj_data[pos_mask]\n        obj_data_neg = obj_data[neg_mask]\n\n        # Don\'t be confused, this is just binary cross entropy similified\n        obj_neg_loss = - F.logsigmoid(-obj_data_neg).sum()\n\n        with torch.no_grad():\n            pos_priors = priors.unsqueeze(0).expand(batch_size, -1, -1).reshape(-1, 4)[pos_mask, :]\n\n            boxes_pred = decode(loc_p, pos_priors, cfg.use_yolo_regressors)\n            boxes_targ = decode(loc_t, pos_priors, cfg.use_yolo_regressors)\n\n            iou_targets = elemwise_box_iou(boxes_pred, boxes_targ)\n\n        obj_pos_loss = - iou_targets * F.logsigmoid(obj_data_pos) - (1 - iou_targets) * F.logsigmoid(-obj_data_pos)\n        obj_pos_loss = obj_pos_loss.sum()\n\n        # All that was the objectiveness loss--now time for the class confidence loss\n        conf_data_pos = (conf_data[:, 1:])[pos_mask] # Now this has just 80 classes\n        conf_t_pos    = conf_t[pos_mask] - 1         # So subtract 1 here\n\n        class_loss = F.cross_entropy(conf_data_pos, conf_t_pos, reduction=\'sum\')\n\n        return cfg.conf_alpha * (class_loss + obj_pos_loss + obj_neg_loss)\n\n\n    def direct_mask_loss(self, pos_idx, idx_t, loc_data, mask_data, priors, masks):\n        """""" Crops the gt masks using the predicted bboxes, scales them down, and outputs the BCE loss. """"""\n        loss_m = 0\n        for idx in range(mask_data.size(0)):\n            with torch.no_grad():\n                cur_pos_idx = pos_idx[idx, :, :]\n                cur_pos_idx_squeezed = cur_pos_idx[:, 1]\n\n                # Shape: [num_priors, 4], decoded predicted bboxes\n                pos_bboxes = decode(loc_data[idx, :, :], priors.data, cfg.use_yolo_regressors)\n                pos_bboxes = pos_bboxes[cur_pos_idx].view(-1, 4).clamp(0, 1)\n                pos_lookup = idx_t[idx, cur_pos_idx_squeezed]\n\n                cur_masks = masks[idx]\n                pos_masks = cur_masks[pos_lookup, :, :]\n                \n                # Convert bboxes to absolute coordinates\n                num_pos, img_height, img_width = pos_masks.size()\n\n                # Take care of all the bad behavior that can be caused by out of bounds coordinates\n                x1, x2 = sanitize_coordinates(pos_bboxes[:, 0], pos_bboxes[:, 2], img_width)\n                y1, y2 = sanitize_coordinates(pos_bboxes[:, 1], pos_bboxes[:, 3], img_height)\n\n                # Crop each gt mask with the predicted bbox and rescale to the predicted mask size\n                # Note that each bounding box crop is a different size so I don\'t think we can vectorize this\n                scaled_masks = []\n                for jdx in range(num_pos):\n                    tmp_mask = pos_masks[jdx, y1[jdx]:y2[jdx], x1[jdx]:x2[jdx]]\n\n                    # Restore any dimensions we\'ve left out because our bbox was 1px wide\n                    while tmp_mask.dim() < 2:\n                        tmp_mask = tmp_mask.unsqueeze(0)\n\n                    new_mask = F.adaptive_avg_pool2d(tmp_mask.unsqueeze(0), cfg.mask_size)\n                    scaled_masks.append(new_mask.view(1, -1))\n\n                mask_t = torch.cat(scaled_masks, 0).gt(0.5).float() # Threshold downsampled mask\n            \n            pos_mask_data = mask_data[idx, cur_pos_idx_squeezed, :]\n            loss_m += F.binary_cross_entropy(torch.clamp(pos_mask_data, 0, 1), mask_t, reduction=\'sum\') * cfg.mask_alpha\n\n        return loss_m\n    \n\n    def coeff_diversity_loss(self, coeffs, instance_t):\n        """"""\n        coeffs     should be size [num_pos, num_coeffs]\n        instance_t should be size [num_pos] and be values from 0 to num_instances-1\n        """"""\n        num_pos = coeffs.size(0)\n        instance_t = instance_t.view(-1) # juuuust to make sure\n\n        coeffs_norm = F.normalize(coeffs, dim=1)\n        cos_sim = coeffs_norm @ coeffs_norm.t()\n\n        inst_eq = (instance_t[:, None].expand_as(cos_sim) == instance_t[None, :].expand_as(cos_sim)).float()\n\n        # Rescale to be between 0 and 1\n        cos_sim = (cos_sim + 1) / 2\n\n        # If they\'re the same instance, use cosine distance, else use cosine similarity\n        loss = (1 - cos_sim) * inst_eq + cos_sim * (1 - inst_eq)\n\n        # Only divide by num_pos once because we\'re summing over a num_pos x num_pos tensor\n        # and all the losses will be divided by num_pos at the end, so just one extra time.\n        return cfg.mask_proto_coeff_diversity_alpha * loss.sum() / num_pos\n\n\n    def lincomb_mask_loss(self, pos, idx_t, loc_data, mask_data, priors, proto_data, masks, gt_box_t, score_data, inst_data, labels, interpolation_mode=\'bilinear\'):\n        mask_h = proto_data.size(1)\n        mask_w = proto_data.size(2)\n\n        process_gt_bboxes = cfg.mask_proto_normalize_emulate_roi_pooling or cfg.mask_proto_crop\n\n        if cfg.mask_proto_remove_empty_masks:\n            # Make sure to store a copy of this because we edit it to get rid of all-zero masks\n            pos = pos.clone()\n\n        loss_m = 0\n        loss_d = 0 # Coefficient diversity loss\n\n        maskiou_t_list = []\n        maskiou_net_input_list = []\n        label_t_list = []\n\n        for idx in range(mask_data.size(0)):\n            with torch.no_grad():\n                downsampled_masks = F.interpolate(masks[idx].unsqueeze(0), (mask_h, mask_w),\n                                                  mode=interpolation_mode, align_corners=False).squeeze(0)\n                downsampled_masks = downsampled_masks.permute(1, 2, 0).contiguous()\n\n                if cfg.mask_proto_binarize_downsampled_gt:\n                    downsampled_masks = downsampled_masks.gt(0.5).float()\n\n                if cfg.mask_proto_remove_empty_masks:\n                    # Get rid of gt masks that are so small they get downsampled away\n                    very_small_masks = (downsampled_masks.sum(dim=(0,1)) <= 0.0001)\n                    for i in range(very_small_masks.size(0)):\n                        if very_small_masks[i]:\n                            pos[idx, idx_t[idx] == i] = 0\n\n                if cfg.mask_proto_reweight_mask_loss:\n                    # Ensure that the gt is binary\n                    if not cfg.mask_proto_binarize_downsampled_gt:\n                        bin_gt = downsampled_masks.gt(0.5).float()\n                    else:\n                        bin_gt = downsampled_masks\n\n                    gt_foreground_norm = bin_gt     / (torch.sum(bin_gt,   dim=(0,1), keepdim=True) + 0.0001)\n                    gt_background_norm = (1-bin_gt) / (torch.sum(1-bin_gt, dim=(0,1), keepdim=True) + 0.0001)\n\n                    mask_reweighting   = gt_foreground_norm * cfg.mask_proto_reweight_coeff + gt_background_norm\n                    mask_reweighting  *= mask_h * mask_w\n\n            cur_pos = pos[idx]\n            pos_idx_t = idx_t[idx, cur_pos]\n            \n            if process_gt_bboxes:\n                # Note: this is in point-form\n                if cfg.mask_proto_crop_with_pred_box:\n                    pos_gt_box_t = decode(loc_data[idx, :, :], priors.data, cfg.use_yolo_regressors)[cur_pos]\n                else:\n                    pos_gt_box_t = gt_box_t[idx, cur_pos]\n\n            if pos_idx_t.size(0) == 0:\n                continue\n\n            proto_masks = proto_data[idx]\n            proto_coef  = mask_data[idx, cur_pos, :]\n            if cfg.use_mask_scoring:\n                mask_scores = score_data[idx, cur_pos, :]\n\n            if cfg.mask_proto_coeff_diversity_loss:\n                if inst_data is not None:\n                    div_coeffs = inst_data[idx, cur_pos, :]\n                else:\n                    div_coeffs = proto_coef\n\n                loss_d += self.coeff_diversity_loss(div_coeffs, pos_idx_t)\n            \n            # If we have over the allowed number of masks, select a random sample\n            old_num_pos = proto_coef.size(0)\n            if old_num_pos > cfg.masks_to_train:\n                perm = torch.randperm(proto_coef.size(0))\n                select = perm[:cfg.masks_to_train]\n\n                proto_coef = proto_coef[select, :]\n                pos_idx_t  = pos_idx_t[select]\n                \n                if process_gt_bboxes:\n                    pos_gt_box_t = pos_gt_box_t[select, :]\n                if cfg.use_mask_scoring:\n                    mask_scores = mask_scores[select, :]\n\n            num_pos = proto_coef.size(0)\n            mask_t = downsampled_masks[:, :, pos_idx_t]     \n            label_t = labels[idx][pos_idx_t]     \n\n            # Size: [mask_h, mask_w, num_pos]\n            pred_masks = proto_masks @ proto_coef.t()\n            pred_masks = cfg.mask_proto_mask_activation(pred_masks)\n\n            if cfg.mask_proto_double_loss:\n                if cfg.mask_proto_mask_activation == activation_func.sigmoid:\n                    pre_loss = F.binary_cross_entropy(torch.clamp(pred_masks, 0, 1), mask_t, reduction=\'sum\')\n                else:\n                    pre_loss = F.smooth_l1_loss(pred_masks, mask_t, reduction=\'sum\')\n                \n                loss_m += cfg.mask_proto_double_loss_alpha * pre_loss\n\n            if cfg.mask_proto_crop:\n                pred_masks = crop(pred_masks, pos_gt_box_t)\n            \n            if cfg.mask_proto_mask_activation == activation_func.sigmoid:\n                pre_loss = F.binary_cross_entropy(torch.clamp(pred_masks, 0, 1), mask_t, reduction=\'none\')\n            else:\n                pre_loss = F.smooth_l1_loss(pred_masks, mask_t, reduction=\'none\')\n\n            if cfg.mask_proto_normalize_mask_loss_by_sqrt_area:\n                gt_area  = torch.sum(mask_t, dim=(0, 1), keepdim=True)\n                pre_loss = pre_loss / (torch.sqrt(gt_area) + 0.0001)\n            \n            if cfg.mask_proto_reweight_mask_loss:\n                pre_loss = pre_loss * mask_reweighting[:, :, pos_idx_t]\n                \n            if cfg.mask_proto_normalize_emulate_roi_pooling:\n                weight = mask_h * mask_w if cfg.mask_proto_crop else 1\n                pos_gt_csize = center_size(pos_gt_box_t)\n                gt_box_width  = pos_gt_csize[:, 2] * mask_w\n                gt_box_height = pos_gt_csize[:, 3] * mask_h\n                pre_loss = pre_loss.sum(dim=(0, 1)) / gt_box_width / gt_box_height * weight\n\n            # If the number of masks were limited scale the loss accordingly\n            if old_num_pos > num_pos:\n                pre_loss *= old_num_pos / num_pos\n\n            loss_m += torch.sum(pre_loss)\n\n            if cfg.use_maskiou:\n                if cfg.discard_mask_area > 0:\n                    gt_mask_area = torch.sum(mask_t, dim=(0, 1))\n                    select = gt_mask_area > cfg.discard_mask_area\n\n                    if torch.sum(select) < 1:\n                        continue\n\n                    pos_gt_box_t = pos_gt_box_t[select, :]\n                    pred_masks = pred_masks[:, :, select]\n                    mask_t = mask_t[:, :, select]\n                    label_t = label_t[select]\n\n                maskiou_net_input = pred_masks.permute(2, 0, 1).contiguous().unsqueeze(1)\n                pred_masks = pred_masks.gt(0.5).float()                \n                maskiou_t = self._mask_iou(pred_masks, mask_t)\n                \n                maskiou_net_input_list.append(maskiou_net_input)\n                maskiou_t_list.append(maskiou_t)\n                label_t_list.append(label_t)\n        \n        losses = {\'M\': loss_m * cfg.mask_alpha / mask_h / mask_w}\n        \n        if cfg.mask_proto_coeff_diversity_loss:\n            losses[\'D\'] = loss_d\n\n        if cfg.use_maskiou:\n            # discard_mask_area discarded every mask in the batch, so nothing to do here\n            if len(maskiou_t_list) == 0:\n                return losses, None\n\n            maskiou_t = torch.cat(maskiou_t_list)\n            label_t = torch.cat(label_t_list)\n            maskiou_net_input = torch.cat(maskiou_net_input_list)\n\n            num_samples = maskiou_t.size(0)\n            if cfg.maskious_to_train > 0 and num_samples > cfg.maskious_to_train:\n                perm = torch.randperm(num_samples)\n                select = perm[:cfg.masks_to_train]\n                maskiou_t = maskiou_t[select]\n                label_t = label_t[select]\n                maskiou_net_input = maskiou_net_input[select]\n\n            return losses, [maskiou_net_input, maskiou_t, label_t]\n\n        return losses\n\n    def _mask_iou(self, mask1, mask2):\n        intersection = torch.sum(mask1*mask2, dim=(0, 1))\n        area1 = torch.sum(mask1, dim=(0, 1))\n        area2 = torch.sum(mask2, dim=(0, 1))\n        union = (area1 + area2) - intersection\n        ret = intersection / union\n        return ret\n\n    def mask_iou_loss(self, net, maskiou_targets):\n        maskiou_net_input, maskiou_t, label_t = maskiou_targets\n\n        maskiou_p = net.maskiou_net(maskiou_net_input)\n\n        label_t = label_t[:, None]\n        maskiou_p = torch.gather(maskiou_p, dim=1, index=label_t).view(-1)\n\n        loss_i = F.smooth_l1_loss(maskiou_p, maskiou_t, reduction=\'sum\')\n        \n        return loss_i * cfg.maskiou_alpha\n'"
