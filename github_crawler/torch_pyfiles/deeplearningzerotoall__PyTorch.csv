file_path,api_count,code
resnet.py,7,"b'import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = conv1x1(planes, planes * self.expansion)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model'"
vgg.py,10,"b'import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\n    \'VGG\', \'vgg11\', \'vgg11_bn\', \'vgg13\', \'vgg13_bn\', \'vgg16\', \'vgg16_bn\',\n    \'vgg19_bn\', \'vgg19\',\n]\n\n\nmodel_urls = {\n    \'vgg11\': \'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n    \'vgg13\': \'https://download.pytorch.org/models/vgg13-c768596a.pth\',\n    \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n    \'vgg19\': \'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n    \'vgg11_bn\': \'https://download.pytorch.org/models/vgg11_bn-6002323d.pth\',\n    \'vgg13_bn\': \'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\',\n    \'vgg16_bn\': \'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\',\n    \'vgg19_bn\': \'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n}\n\n\nclass VGG(nn.Module):\n\n    def __init__(self, features, num_classes=1000, init_weights=True):\n        super(VGG, self).__init__()\n        self.features = features\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, num_classes),\n        )\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\ncfg = {\n    \'A\': [64, \'M\', 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'B\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'D\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n    \'E\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\'],\n}\n\n\ndef vgg11(pretrained=False, **kwargs):\n    """"""VGG 11-layer model (configuration ""A"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'A\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg11\']))\n    return model\n\n\n\ndef vgg11_bn(pretrained=False, **kwargs):\n    """"""VGG 11-layer model (configuration ""A"") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'A\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg11_bn\']))\n    return model\n\n\n\ndef vgg13(pretrained=False, **kwargs):\n    """"""VGG 13-layer model (configuration ""B"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'B\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg13\']))\n    return model\n\n\n\ndef vgg13_bn(pretrained=False, **kwargs):\n    """"""VGG 13-layer model (configuration ""B"") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'B\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg13_bn\']))\n    return model\n\n\n\ndef vgg16(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'D\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16\']))\n    return model\n\n\n\ndef vgg16_bn(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'D\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16_bn\']))\n    return model\n\n\n\ndef vgg19(pretrained=False, **kwargs):\n    """"""VGG 19-layer model (configuration ""E"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'E\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg19\']))\n    return model\n\n\n\ndef vgg19_bn(pretrained=False, **kwargs):\n    """"""VGG 19-layer model (configuration \'E\') with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(make_layers(cfg[\'E\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg19_bn\']))\n    return model'"
CNN/lab-07-4-mnist_introduction.py,10,"b""# Lab 7 Learning rate and Evaluation\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\nimport matplotlib.pyplot as plt\r\nimport random\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\nrandom.seed(777)\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\n# parameters\r\ntraining_epochs = 15\r\nbatch_size = 100\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\n# MNIST data image of shape 28 * 28 = 784\r\nlinear = torch.nn.Linear(784, 10, bias=True).to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\r\noptimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\r\n\r\nfor epoch in range(training_epochs):\r\n    avg_cost = 0\r\n    total_batch = len(data_loader)\r\n\r\n    for X, Y in data_loader:\r\n        # reshape input image into [batch_size by 784]\r\n        # label is not one-hot encoded\r\n        X = X.view(-1, 28 * 28).to(device)\r\n        Y = Y.to(device)\r\n\r\n        optimizer.zero_grad()\r\n        hypothesis = linear(X)\r\n        cost = criterion(hypothesis, Y)\r\n        cost.backward()\r\n        optimizer.step()\r\n\r\n        avg_cost += cost / total_batch\r\n\r\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\r\n\r\nprint('Learning finished')\r\n\r\n# Test the model using test sets\r\nwith torch.no_grad():\r\n    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\r\n    Y_test = mnist_test.test_labels.to(device)\r\n\r\n    prediction = linear(X_test)\r\n    correct_prediction = torch.argmax(prediction, 1) == Y_test\r\n    accuracy = correct_prediction.float().mean()\r\n    print('Accuracy:', accuracy.item())\r\n\r\n    # Get one and predict\r\n    r = random.randint(0, len(mnist_test) - 1)\r\n    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\r\n    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\r\n\r\n    print('Label: ', Y_single_data.item())\r\n    single_prediction = linear(X_single_data)\r\n    print('Prediction: ', torch.argmax(single_prediction, 1).item())\r\n\r\n    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\r\n    plt.show()\r\n"""
CNN/lab-09-1-xor.py,11,"b""# Lab 9 XOR\r\nimport torch\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\nX = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\r\nY = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\r\n\r\n# nn layers\r\nlinear = torch.nn.Linear(2, 1, bias=True)\r\nsigmoid = torch.nn.Sigmoid()\r\n\r\n# model\r\nmodel = torch.nn.Sequential(linear, sigmoid).to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.BCELoss().to(device)\r\noptimizer = torch.optim.SGD(model.parameters(), lr=1)\r\n\r\nfor step in range(10001):\r\n    optimizer.zero_grad()\r\n    hypothesis = model(X)\r\n\r\n    # cost/loss function\r\n    cost = criterion(hypothesis, Y)\r\n    cost.backward()\r\n    optimizer.step()\r\n\r\n    if step % 100 == 0:\r\n        print(step, cost.item())\r\n\r\n# Accuracy computation\r\n# True if hypothesis>0.5 else False\r\nwith torch.no_grad():\r\n    predicted = (model(X) > 0.5).float()\r\n    accuracy = (predicted == Y).float().mean()\r\n    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"""
CNN/lab-09-2-xor-nn.py,12,"b""# Lab 9 XOR\r\nimport torch\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\nX = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\r\nY = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\r\n\r\n# nn layers\r\nlinear1 = torch.nn.Linear(2, 2, bias=True)\r\nlinear2 = torch.nn.Linear(2, 1, bias=True)\r\nsigmoid = torch.nn.Sigmoid()\r\n\r\n# model\r\nmodel = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.BCELoss().to(device)\r\noptimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1\r\n\r\nfor step in range(10001):\r\n    optimizer.zero_grad()\r\n    hypothesis = model(X)\r\n\r\n    # cost/loss function\r\n    cost = criterion(hypothesis, Y)\r\n    cost.backward()\r\n    optimizer.step()\r\n\r\n    if step % 100 == 0:\r\n        print(step, cost.item())\r\n\r\n# Accuracy computation\r\n# True if hypothesis>0.5 else False\r\nwith torch.no_grad():\r\n    predicted = (model(X) > 0.5).float()\r\n    accuracy = (predicted == Y).float().mean()\r\n    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"""
CNN/lab-09-3-xor-nn-wide-deep.py,14,"b""# Lab 9 XOR\r\nimport torch\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\nX = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\r\nY = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\r\n\r\n# nn layers\r\nlinear1 = torch.nn.Linear(2, 10, bias=True)\r\nlinear2 = torch.nn.Linear(10, 10, bias=True)\r\nlinear3 = torch.nn.Linear(10, 10, bias=True)\r\nlinear4 = torch.nn.Linear(10, 1, bias=True)\r\nsigmoid = torch.nn.Sigmoid()\r\n\r\n# model\r\nmodel = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.BCELoss().to(device)\r\noptimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1\r\n\r\nfor step in range(10001):\r\n    optimizer.zero_grad()\r\n    hypothesis = model(X)\r\n\r\n    # cost/loss function\r\n    cost = criterion(hypothesis, Y)\r\n    cost.backward()\r\n    optimizer.step()\r\n\r\n    if step % 100 == 0:\r\n        print(step, cost.item())\r\n\r\n# Accuracy computation\r\n# True if hypothesis>0.5 else False\r\nwith torch.no_grad():\r\n    predicted = (model(X) > 0.5).float()\r\n    accuracy = (predicted == Y).float().mean()\r\n    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"""
CNN/lab-10-1-mnist_softmax.py,10,"b""# Lab 10 MNIST and softmax\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\nimport random\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\nrandom.seed(777)\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\n# parameters\r\nlearning_rate = 0.001\r\ntraining_epochs = 15\r\nbatch_size = 100\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\n# MNIST data image of shape 28 * 28 = 784\r\nlinear = torch.nn.Linear(784, 10, bias=True).to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\r\noptimizer = torch.optim.Adam(linear.parameters(), lr=learning_rate)\r\n\r\ntotal_batch = len(data_loader)\r\nfor epoch in range(training_epochs):\r\n    avg_cost = 0\r\n\r\n    for X, Y in data_loader:\r\n        # reshape input image into [batch_size by 784]\r\n        # label is not one-hot encoded\r\n        X = X.view(-1, 28 * 28).to(device)\r\n        Y = Y.to(device)\r\n\r\n        optimizer.zero_grad()\r\n        hypothesis = linear(X)\r\n        cost = criterion(hypothesis, Y)\r\n        cost.backward()\r\n        optimizer.step()\r\n\r\n        avg_cost += cost / total_batch\r\n\r\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\r\n\r\nprint('Learning finished')\r\n\r\n# Test the model using test sets\r\nwith torch.no_grad():\r\n    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\r\n    Y_test = mnist_test.test_labels.to(device)\r\n\r\n    prediction = linear(X_test)\r\n    correct_prediction = torch.argmax(prediction, 1) == Y_test\r\n    accuracy = correct_prediction.float().mean()\r\n    print('Accuracy:', accuracy.item())\r\n\r\n    # Get one and predict\r\n    r = random.randint(0, len(mnist_test) - 1)\r\n    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\r\n    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\r\n\r\n    print('Label: ', Y_single_data.item())\r\n    single_prediction = linear(X_single_data)\r\n    print('Prediction: ', torch.argmax(single_prediction, 1).item())"""
CNN/lab-10-2-mnist_nn.py,14,"b""# Lab 10 MNIST and softmax\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\nimport random\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\nrandom.seed(777)\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\n# parameters\r\nlearning_rate = 0.001\r\ntraining_epochs = 15\r\nbatch_size = 100\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\n# nn layers\r\nlinear1 = torch.nn.Linear(784, 256, bias=True)\r\nlinear2 = torch.nn.Linear(256, 256, bias=True)\r\nlinear3 = torch.nn.Linear(256, 10, bias=True)\r\nrelu = torch.nn.ReLU()\r\n\r\n# model\r\nmodel = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\r\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n\r\ntotal_batch = len(data_loader)\r\nfor epoch in range(training_epochs):\r\n    avg_cost = 0\r\n\r\n    for X, Y in data_loader:\r\n        # reshape input image into [batch_size by 784]\r\n        # label is not one-hot encoded\r\n        X = X.view(-1, 28 * 28).to(device)\r\n        Y = Y.to(device)\r\n\r\n        optimizer.zero_grad()\r\n        hypothesis = model(X)\r\n        cost = criterion(hypothesis, Y)\r\n        cost.backward()\r\n        optimizer.step()\r\n\r\n        avg_cost += cost / total_batch\r\n\r\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\r\n\r\nprint('Learning finished')\r\n\r\n# Test the model using test sets\r\nwith torch.no_grad():\r\n    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\r\n    Y_test = mnist_test.test_labels.to(device)\r\n\r\n    prediction = model(X_test)\r\n    correct_prediction = torch.argmax(prediction, 1) == Y_test\r\n    accuracy = correct_prediction.float().mean()\r\n    print('Accuracy:', accuracy.item())\r\n\r\n    # Get one and predict\r\n    r = random.randint(0, len(mnist_test) - 1)\r\n    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\r\n    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\r\n\r\n    print('Label: ', Y_single_data.item())\r\n    single_prediction = model(X_single_data)\r\n    print('Prediction: ', torch.argmax(single_prediction, 1).item())"""
CNN/lab-10-3-mnist_nn_xavier.py,17,"b""# Lab 10 MNIST and softmax\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\nimport random\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\nrandom.seed(777)\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\n# parameters\r\nlearning_rate = 0.001\r\ntraining_epochs = 15\r\nbatch_size = 100\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\n# nn layers\r\nlinear1 = torch.nn.Linear(784, 256, bias=True)\r\nlinear2 = torch.nn.Linear(256, 256, bias=True)\r\nlinear3 = torch.nn.Linear(256, 10, bias=True)\r\nrelu = torch.nn.ReLU()\r\n\r\n# xavier initialization\r\ntorch.nn.init.xavier_uniform_(linear1.weight)\r\ntorch.nn.init.xavier_uniform_(linear2.weight)\r\ntorch.nn.init.xavier_uniform_(linear3.weight)\r\n\r\n# model\r\nmodel = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\r\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n\r\ntotal_batch = len(data_loader)\r\nfor epoch in range(training_epochs):\r\n    avg_cost = 0\r\n\r\n    for X, Y in data_loader:\r\n        # reshape input image into [batch_size by 784]\r\n        # label is not one-hot encoded\r\n        X = X.view(-1, 28 * 28).to(device)\r\n        Y = Y.to(device)\r\n\r\n        optimizer.zero_grad()\r\n        hypothesis = model(X)\r\n        cost = criterion(hypothesis, Y)\r\n        cost.backward()\r\n        optimizer.step()\r\n\r\n        avg_cost += cost / total_batch\r\n\r\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\r\n\r\nprint('Learning finished')\r\n\r\n# Test the model using test sets\r\nwith torch.no_grad():\r\n    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\r\n    Y_test = mnist_test.test_labels.to(device)\r\n\r\n    prediction = model(X_test)\r\n    correct_prediction = torch.argmax(prediction, 1) == Y_test\r\n    accuracy = correct_prediction.float().mean()\r\n    print('Accuracy:', accuracy.item())\r\n\r\n    # Get one and predict\r\n    r = random.randint(0, len(mnist_test) - 1)\r\n    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\r\n    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\r\n\r\n    print('Label: ', Y_single_data.item())\r\n    single_prediction = model(X_single_data)\r\n    print('Prediction: ', torch.argmax(single_prediction, 1).item())"""
CNN/lab-10-4-mnist_nn_deep.py,21,"b""# Lab 10 MNIST and softmax\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\nimport random\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\nrandom.seed(777)\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\n# parameters\r\nlearning_rate = 0.001\r\ntraining_epochs = 15\r\nbatch_size = 100\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\n# nn layers\r\nlinear1 = torch.nn.Linear(784, 512, bias=True)\r\nlinear2 = torch.nn.Linear(512, 512, bias=True)\r\nlinear3 = torch.nn.Linear(512, 512, bias=True)\r\nlinear4 = torch.nn.Linear(512, 512, bias=True)\r\nlinear5 = torch.nn.Linear(512, 10, bias=True)\r\nrelu = torch.nn.ReLU()\r\n\r\n# xavier initialization\r\ntorch.nn.init.xavier_uniform_(linear1.weight)\r\ntorch.nn.init.xavier_uniform_(linear2.weight)\r\ntorch.nn.init.xavier_uniform_(linear3.weight)\r\ntorch.nn.init.xavier_uniform_(linear4.weight)\r\ntorch.nn.init.xavier_uniform_(linear5.weight)\r\n\r\n# model\r\nmodel = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\r\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n\r\ntotal_batch = len(data_loader)\r\nfor epoch in range(training_epochs):\r\n    avg_cost = 0\r\n\r\n    for X, Y in data_loader:\r\n        # reshape input image into [batch_size by 784]\r\n        # label is not one-hot encoded\r\n        X = X.view(-1, 28 * 28).to(device)\r\n        Y = Y.to(device)\r\n\r\n        optimizer.zero_grad()\r\n        hypothesis = model(X)\r\n        cost = criterion(hypothesis, Y)\r\n        cost.backward()\r\n        optimizer.step()\r\n\r\n        avg_cost += cost / total_batch\r\n\r\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\r\n\r\nprint('Learning finished')\r\n\r\n# Test the model using test sets\r\nwith torch.no_grad():\r\n    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\r\n    Y_test = mnist_test.test_labels.to(device)\r\n\r\n    prediction = model(X_test)\r\n    correct_prediction = torch.argmax(prediction, 1) == Y_test\r\n    accuracy = correct_prediction.float().mean()\r\n    print('Accuracy:', accuracy.item())\r\n\r\n    # Get one and predict\r\n    r = random.randint(0, len(mnist_test) - 1)\r\n    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\r\n    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\r\n\r\n    print('Label: ', Y_single_data.item())\r\n    single_prediction = model(X_single_data)\r\n    print('Prediction: ', torch.argmax(single_prediction, 1).item())"""
CNN/lab-10-5-mnist_nn_dropout.py,22,"b""# Lab 10 MNIST and softmax\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\nimport random\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\nrandom.seed(777)\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\n# parameters\r\nlearning_rate = 0.001\r\ntraining_epochs = 15\r\nbatch_size = 100\r\nkeep_prob = 0.7\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\n# nn layers\r\nlinear1 = torch.nn.Linear(784, 512, bias=True)\r\nlinear2 = torch.nn.Linear(512, 512, bias=True)\r\nlinear3 = torch.nn.Linear(512, 512, bias=True)\r\nlinear4 = torch.nn.Linear(512, 512, bias=True)\r\nlinear5 = torch.nn.Linear(512, 10, bias=True)\r\nrelu = torch.nn.ReLU()\r\ndropout = torch.nn.Dropout(p=1 - keep_prob)\r\n\r\n# xavier initialization\r\ntorch.nn.init.xavier_uniform_(linear1.weight)\r\ntorch.nn.init.xavier_uniform_(linear2.weight)\r\ntorch.nn.init.xavier_uniform_(linear3.weight)\r\ntorch.nn.init.xavier_uniform_(linear4.weight)\r\ntorch.nn.init.xavier_uniform_(linear5.weight)\r\n\r\n# model\r\nmodel = torch.nn.Sequential(linear1, relu, dropout,\r\n                            linear2, relu, dropout,\r\n                            linear3, relu, dropout,\r\n                            linear4, relu, dropout,\r\n                            linear5).to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\r\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n\r\ntotal_batch = len(data_loader)\r\nmodel.train()    # set the model to train mode (dropout=True)\r\nfor epoch in range(training_epochs):\r\n    avg_cost = 0\r\n\r\n    for X, Y in data_loader:\r\n        # reshape input image into [batch_size by 784]\r\n        # label is not one-hot encoded\r\n        X = X.view(-1, 28 * 28).to(device)\r\n        Y = Y.to(device)\r\n\r\n        optimizer.zero_grad()\r\n        hypothesis = model(X)\r\n        cost = criterion(hypothesis, Y)\r\n        cost.backward()\r\n        optimizer.step()\r\n\r\n        avg_cost += cost / total_batch\r\n\r\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\r\n\r\nprint('Learning finished')\r\n\r\n# Test model and check accuracy\r\nwith torch.no_grad():\r\n    model.eval()    # set the model to evaluation mode (dropout=False)\r\n\r\n    # Test the model using test sets\r\n    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\r\n    Y_test = mnist_test.test_labels.to(device)\r\n\r\n    prediction = model(X_test)\r\n    correct_prediction = torch.argmax(prediction, 1) == Y_test\r\n    accuracy = correct_prediction.float().mean()\r\n    print('Accuracy:', accuracy.item())\r\n\r\n    # Get one and predict\r\n    r = random.randint(0, len(mnist_test) - 1)\r\n    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\r\n    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\r\n\r\n    print('Label: ', Y_single_data.item())\r\n    single_prediction = model(X_single_data)\r\n    print('Prediction: ', torch.argmax(single_prediction, 1).item())"""
CNN/lab-10-6-mnist_batchnorm.py,24,"b""# Lab 10 MNIST and softmax\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\nimport matplotlib.pylab as plt\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\ntorch.manual_seed(1)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(1)\r\n\r\n# parameters\r\nlearning_rate = 0.01\r\ntraining_epochs = 10\r\nbatch_size = 32\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ntrain_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\ntest_loader = torch.utils.data.DataLoader(dataset=mnist_test,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=False,\r\n                                          drop_last=True)\r\n\r\n# nn layers\r\nlinear1 = torch.nn.Linear(784, 32, bias=True)\r\nlinear2 = torch.nn.Linear(32, 32, bias=True)\r\nlinear3 = torch.nn.Linear(32, 10, bias=True)\r\nrelu = torch.nn.ReLU()\r\nbn1 = torch.nn.BatchNorm1d(32)\r\nbn2 = torch.nn.BatchNorm1d(32)\r\n\r\nnn_linear1 = torch.nn.Linear(784, 32, bias=True)\r\nnn_linear2 = torch.nn.Linear(32, 32, bias=True)\r\nnn_linear3 = torch.nn.Linear(32, 10, bias=True)\r\n\r\n# model\r\nbn_model = torch.nn.Sequential(linear1, relu, bn1,\r\n                            linear2, relu, bn2,\r\n                            linear3).to(device)\r\nnn_model = torch.nn.Sequential(nn_linear1, relu,\r\n                               nn_linear2, relu,\r\n                               nn_linear3).to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\r\nbn_optimizer = torch.optim.Adam(bn_model.parameters(), lr=learning_rate)\r\nnn_optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)\r\n\r\n# Save Losses and Accuracies every epoch\r\n# We are going to plot them later\r\ntrain_losses = []\r\ntrain_accs = []\r\n\r\nvalid_losses = []\r\nvalid_accs = []\r\n\r\ntrain_total_batch = len(train_loader)\r\ntest_total_batch = len(test_loader)\r\nfor epoch in range(training_epochs):\r\n    bn_model.train()  # set the model to train mode\r\n\r\n    for X, Y in train_loader:\r\n        # reshape input image into [batch_size by 784]\r\n        # label is not one-hot encoded\r\n        X = X.view(-1, 28 * 28).to(device)\r\n        Y = Y.to(device)\r\n\r\n        bn_optimizer.zero_grad()\r\n        bn_prediction = bn_model(X)\r\n        bn_loss = criterion(bn_prediction, Y)\r\n        bn_loss.backward()\r\n        bn_optimizer.step()\r\n\r\n        nn_optimizer.zero_grad()\r\n        nn_prediction = nn_model(X)\r\n        nn_loss = criterion(nn_prediction, Y)\r\n        nn_loss.backward()\r\n        nn_optimizer.step()\r\n\r\n    with torch.no_grad():\r\n        bn_model.eval()     # set the model to evaluation mode\r\n\r\n        # Test the model using train sets\r\n        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\r\n        for i, (X, Y) in enumerate(train_loader):\r\n            X = X.view(-1, 28 * 28).to(device)\r\n            Y = Y.to(device)\r\n\r\n            bn_prediction = bn_model(X)\r\n            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\r\n            bn_loss += criterion(bn_prediction, Y)\r\n            bn_acc += bn_correct_prediction.float().mean()\r\n\r\n            nn_prediction = nn_model(X)\r\n            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\r\n            nn_loss += criterion(nn_prediction, Y)\r\n            nn_acc += nn_correct_prediction.float().mean()\r\n\r\n        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / train_total_batch, nn_loss / train_total_batch, bn_acc / train_total_batch, nn_acc / train_total_batch\r\n\r\n        # Save train losses/acc\r\n        train_losses.append([bn_loss, nn_loss])\r\n        train_accs.append([bn_acc, nn_acc])\r\n        print(\r\n            '[Epoch %d-TRAIN] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\r\n            (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\r\n        # Test the model using test sets\r\n        bn_loss, nn_loss, bn_acc, nn_acc = 0, 0, 0, 0\r\n        for i, (X, Y) in enumerate(test_loader):\r\n            X = X.view(-1, 28 * 28).to(device)\r\n            Y = Y.to(device)\r\n\r\n            bn_prediction = bn_model(X)\r\n            bn_correct_prediction = torch.argmax(bn_prediction, 1) == Y\r\n            bn_loss += criterion(bn_prediction, Y)\r\n            bn_acc += bn_correct_prediction.float().mean()\r\n\r\n            nn_prediction = nn_model(X)\r\n            nn_correct_prediction = torch.argmax(nn_prediction, 1) == Y\r\n            nn_loss += criterion(nn_prediction, Y)\r\n            nn_acc += nn_correct_prediction.float().mean()\r\n\r\n        bn_loss, nn_loss, bn_acc, nn_acc = bn_loss / test_total_batch, nn_loss / test_total_batch, bn_acc / test_total_batch, nn_acc / test_total_batch\r\n\r\n        # Save valid losses/acc\r\n        valid_losses.append([bn_loss, nn_loss])\r\n        valid_accs.append([bn_acc, nn_acc])\r\n        print(\r\n            '[Epoch %d-VALID] Batchnorm Loss(Acc): bn_loss:%.5f(bn_acc:%.2f) vs No Batchnorm Loss(Acc): nn_loss:%.5f(nn_acc:%.2f)' % (\r\n                (epoch + 1), bn_loss.item(), bn_acc.item(), nn_loss.item(), nn_acc.item()))\r\n        print()\r\n\r\nprint('Learning finished')\r\n\r\ndef plot_compare(loss_list: list, ylim=None, title=None) -> None:\r\n    bn = [i[0] for i in loss_list]\r\n    nn = [i[1] for i in loss_list]\r\n\r\n    plt.figure(figsize=(15, 10))\r\n    plt.plot(bn, label='With BN')\r\n    plt.plot(nn, label='Without BN')\r\n    if ylim:\r\n        plt.ylim(ylim)\r\n\r\n    if title:\r\n        plt.title(title)\r\n    plt.legend()\r\n    plt.grid('on')\r\n    plt.show()\r\n\r\nplot_compare(train_losses, title='Training Loss at Epoch')\r\nplot_compare(train_accs, [0, 1.0], title='Training Acc at Epoch')\r\nplot_compare(valid_losses, title='Validation Loss at Epoch')\r\nplot_compare(valid_accs, [0, 1.0], title='Validation Acc at Epoch')"""
CNN/lab-10-8-mnist_nn_selu(wip).py,21,"b""# Lab 10 MNIST and softmax\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\nimport random\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\nrandom.seed(777)\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\n# parameters\r\nlearning_rate = 0.001\r\ntraining_epochs = 15\r\nbatch_size = 100\r\nkeep_prob = 0.7\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\n# nn layers\r\nlinear1 = torch.nn.Linear(784, 512, bias=True)\r\nlinear2 = torch.nn.Linear(512, 512, bias=True)\r\nlinear3 = torch.nn.Linear(512, 512, bias=True)\r\nlinear4 = torch.nn.Linear(512, 512, bias=True)\r\nlinear5 = torch.nn.Linear(512, 10, bias=True)\r\nselu = torch.nn.SELU()\r\n\r\n# xavier initialization\r\ntorch.nn.init.xavier_uniform_(linear1.weight)\r\ntorch.nn.init.xavier_uniform_(linear2.weight)\r\ntorch.nn.init.xavier_uniform_(linear3.weight)\r\ntorch.nn.init.xavier_uniform_(linear4.weight)\r\ntorch.nn.init.xavier_uniform_(linear5.weight)\r\n\r\n# model\r\nmodel = torch.nn.Sequential(linear1, selu,\r\n                            linear2, selu,\r\n                            linear3, selu,\r\n                            linear4, selu,\r\n                            linear5).to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\r\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n\r\ntotal_batch = len(data_loader)\r\nmodel.train()    # set the model to train mode (dropout=True)\r\nfor epoch in range(training_epochs):\r\n    avg_cost = 0\r\n\r\n    for X, Y in data_loader:\r\n        # reshape input image into [batch_size by 784]\r\n        # label is not one-hot encoded\r\n        X = X.view(-1, 28 * 28).to(device)\r\n        Y = Y.to(device)\r\n\r\n        optimizer.zero_grad()\r\n        hypothesis = model(X)\r\n        cost = criterion(hypothesis, Y)\r\n        cost.backward()\r\n        optimizer.step()\r\n\r\n        avg_cost += cost / total_batch\r\n\r\n    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\r\n\r\nprint('Learning finished')\r\n\r\n# Test model and check accuracy\r\nwith torch.no_grad():\r\n    model.eval()    # set the model to evaluation mode (dropout=False)\r\n\r\n    # Test the model using test sets\r\n    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\r\n    Y_test = mnist_test.test_labels.to(device)\r\n\r\n    prediction = model(X_test)\r\n    correct_prediction = torch.argmax(prediction, 1) == Y_test\r\n    accuracy = correct_prediction.float().mean()\r\n    print('Accuracy:', accuracy.item())\r\n\r\n    # Get one and predict\r\n    r = random.randint(0, len(mnist_test) - 1)\r\n    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\r\n    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\r\n\r\n    print('Label: ', Y_single_data.item())\r\n    single_prediction = model(X_single_data)\r\n    print('Prediction: ', torch.argmax(single_prediction, 1).item())"""
CNN/lab-10-X1-mnist_back_prop.py,25,"b""# Lab 10 MNIST and softmax\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\n# parameters\r\nlearning_rate = 0.5\r\nbatch_size = 10\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\nw1 = torch.nn.Parameter(torch.Tensor(784, 30)).to(device)\r\nb1 = torch.nn.Parameter(torch.Tensor(30)).to(device)\r\nw2 = torch.nn.Parameter(torch.Tensor(30, 10)).to(device)\r\nb2 = torch.nn.Parameter(torch.Tensor(10)).to(device)\r\n\r\ntorch.nn.init.normal_(w1)\r\ntorch.nn.init.normal_(b1)\r\ntorch.nn.init.normal_(w2)\r\ntorch.nn.init.normal_(b2)\r\n\r\ndef sigma(x):\r\n    #  sigmoid function\r\n    return 1.0 / (1.0 + torch.exp(-x))\r\n    # return torch.div(torch.tensor(1), torch.add(torch.tensor(1.0), torch.exp(-x)))\r\n\r\n\r\ndef sigma_prime(x):\r\n    # derivative of the sigmoid function\r\n    return sigma(x) * (1 - sigma(x))\r\n\r\nX_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)[:1000]\r\nY_test = mnist_test.test_labels.to(device)[:1000]\r\ni = 0\r\nwhile not i == 10000:\r\n    for X, Y in data_loader:\r\n        i += 1\r\n\r\n        # forward\r\n        X = X.view(-1, 28 * 28).to(device)\r\n        Y = torch.zeros((batch_size, 10)).scatter_(1, Y.unsqueeze(1), 1).to(device)    # one-hot\r\n        l1 = torch.add(torch.matmul(X, w1), b1)\r\n        a1 = sigma(l1)\r\n        l2 = torch.add(torch.matmul(a1, w2), b2)\r\n        y_pred = sigma(l2)\r\n\r\n        diff = y_pred - Y\r\n\r\n        # Back prop (chain rule)\r\n        d_l2 = diff * sigma_prime(l2)\r\n        d_b2 = d_l2\r\n        d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_l2)\r\n\r\n        d_a1 = torch.matmul(d_l2, torch.transpose(w2, 0, 1))\r\n        d_l1 = d_a1 * sigma_prime(l1)\r\n        d_b1 = d_l1\r\n        d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_l1)\r\n\r\n        w1 = w1 - learning_rate * d_w1\r\n        b1 = b1 - learning_rate * torch.mean(d_b1, 0)\r\n        w2 = w2 - learning_rate * d_w2\r\n        b2 = b2 - learning_rate * torch.mean(d_b2, 0)\r\n\r\n        if i % 1000 == 0:\r\n            l1 = torch.add(torch.matmul(X_test, w1), b1)\r\n            a1 = sigma(l1)\r\n            l2 = torch.add(torch.matmul(a1, w2), b2)\r\n            y_pred = sigma(l2)\r\n            acct_mat = torch.argmax(y_pred, 1) == Y_test\r\n            acct_res = acct_mat.sum()\r\n            print(acct_res.item())\r\n\r\n        if i == 10000:\r\n            break"""
CNN/lab-11-1-mnist_cnn.py,20,"b""# Lab 11 MNIST and Convolutional Neural Network\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\nimport torch.nn.init\r\n\r\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\ndevice = 'cpu'\r\n# for reproducibility\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\n# parameters\r\nlearning_rate = 0.001\r\ntraining_epochs = 15\r\nbatch_size = 100\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\n# CNN Model (2 conv layers)\r\nclass CNN(torch.nn.Module):\r\n\r\n    def __init__(self):\r\n        super(CNN, self).__init__()\r\n        # L1 ImgIn shape=(?, 28, 28, 1)\r\n        #    Conv     -> (?, 28, 28, 32)\r\n        #    Pool     -> (?, 14, 14, 32)\r\n        self.layer1 = torch.nn.Sequential(\r\n            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\r\n            torch.nn.ReLU(),\r\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\r\n        # L2 ImgIn shape=(?, 14, 14, 32)\r\n        #    Conv      ->(?, 14, 14, 64)\r\n        #    Pool      ->(?, 7, 7, 64)\r\n        self.layer2 = torch.nn.Sequential(\r\n            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\r\n            torch.nn.ReLU(),\r\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\r\n        # Final FC 7x7x64 inputs -> 10 outputs\r\n        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\r\n        torch.nn.init.xavier_uniform_(self.fc.weight)\r\n\r\n    def forward(self, x):\r\n        out = self.layer1(x)\r\n        out = self.layer2(out)\r\n        out = out.view(out.size(0), -1)   # Flatten them for FC\r\n        out = self.fc(out)\r\n        return out\r\n\r\n\r\n# instantiate CNN model\r\nmodel = CNN().to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\r\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n\r\n# train my model\r\ntotal_batch = len(data_loader)\r\nprint('Learning started. It takes sometime.')\r\nfor epoch in range(training_epochs):\r\n    avg_cost = 0\r\n\r\n    for X, Y in data_loader:\r\n        # image is already size of (28x28), no reshape\r\n        # label is not one-hot encoded\r\n        X = X.to(device)\r\n        Y = Y.to(device)\r\n\r\n        optimizer.zero_grad()\r\n        hypothesis = model(X)\r\n        cost = criterion(hypothesis, Y)\r\n        cost.backward()\r\n        optimizer.step()\r\n\r\n        avg_cost += cost / total_batch\r\n\r\n    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\r\n\r\nprint('Learning Finished!')\r\n\r\n# Test model and check accuracy\r\nwith torch.no_grad():\r\n    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\r\n    Y_test = mnist_test.test_labels.to(device)\r\n\r\n    prediction = model(X_test)\r\n    correct_prediction = torch.argmax(prediction, 1) == Y_test\r\n    accuracy = correct_prediction.float().mean()\r\n    print('Accuracy:', accuracy.item())\r\n"""
CNN/lab-11-2-mnist_deep_cnn.py,29,"b""# Lab 11 MNIST and Deep learning CNN\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\nimport torch.nn.init\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\n# parameters\r\nlearning_rate = 0.001\r\ntraining_epochs = 15\r\nbatch_size = 100\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\n# CNN Model\r\nclass CNN(torch.nn.Module):\r\n\r\n    def __init__(self):\r\n        super(CNN, self).__init__()\r\n        self.keep_prob = 0.5\r\n        # L1 ImgIn shape=(?, 28, 28, 1)\r\n        #    Conv     -> (?, 28, 28, 32)\r\n        #    Pool     -> (?, 14, 14, 32)\r\n        self.layer1 = torch.nn.Sequential(\r\n            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\r\n            torch.nn.ReLU(),\r\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\r\n        # L2 ImgIn shape=(?, 14, 14, 32)\r\n        #    Conv      ->(?, 14, 14, 64)\r\n        #    Pool      ->(?, 7, 7, 64)\r\n        self.layer2 = torch.nn.Sequential(\r\n            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\r\n            torch.nn.ReLU(),\r\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\r\n        # L3 ImgIn shape=(?, 7, 7, 64)\r\n        #    Conv      ->(?, 7, 7, 128)\r\n        #    Pool      ->(?, 4, 4, 128)\r\n        self.layer3 = torch.nn.Sequential(\r\n            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\r\n            torch.nn.ReLU(),\r\n            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1))\r\n\r\n        # L4 FC 4x4x128 inputs -> 625 outputs\r\n        self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)\r\n        torch.nn.init.xavier_uniform_(self.fc1.weight)\r\n        self.layer4 = torch.nn.Sequential(\r\n            self.fc1,\r\n            torch.nn.ReLU(),\r\n            torch.nn.Dropout(p=1 - self.keep_prob))\r\n        # L5 Final FC 625 inputs -> 10 outputs\r\n        self.fc2 = torch.nn.Linear(625, 10, bias=True)\r\n        torch.nn.init.xavier_uniform_(self.fc2.weight)\r\n\r\n    def forward(self, x):\r\n        out = self.layer1(x)\r\n        out = self.layer2(out)\r\n        out = self.layer3(out)\r\n        out = out.view(out.size(0), -1)   # Flatten them for FC\r\n        out = self.layer4(out)\r\n        out = self.fc2(out)\r\n        return out\r\n\r\n\r\n# instantiate CNN model\r\nmodel = CNN().to(device)\r\n\r\n# define cost/loss & optimizer\r\ncriterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\r\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n\r\n# train my model\r\ntotal_batch = len(data_loader)\r\nmodel.train()    # set the model to train mode (dropout=True)\r\nprint('Learning started. It takes sometime.')\r\nfor epoch in range(training_epochs):\r\n    avg_cost = 0\r\n\r\n    for X, Y in data_loader:\r\n        # image is already size of (28x28), no reshape\r\n        # label is not one-hot encoded\r\n        X = X.to(device)\r\n        Y = Y.to(device)\r\n\r\n        optimizer.zero_grad()\r\n        hypothesis = model(X)\r\n        cost = criterion(hypothesis, Y)\r\n        cost.backward()\r\n        optimizer.step()\r\n\r\n        avg_cost += cost / total_batch\r\n\r\n    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\r\n\r\nprint('Learning Finished!')\r\n\r\n# Test model and check accuracy\r\nwith torch.no_grad():\r\n    model.eval()    # set the model to evaluation mode (dropout=False)\r\n\r\n    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\r\n    Y_test = mnist_test.test_labels.to(device)\r\n\r\n    prediction = model(X_test)\r\n    correct_prediction = torch.argmax(prediction, 1) == Y_test\r\n    accuracy = correct_prediction.float().mean()\r\n    print('Accuracy:', accuracy.item())"""
CNN/lab-11-3-mnist_cnn_class.py,29,"b""# Lab 11 MNIST and Deep learning CNN\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\nimport torch.nn.init\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\n# parameters\r\nlearning_rate = 0.001\r\ntraining_epochs = 15\r\nbatch_size = 100\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\n# CNN Model\r\nclass CNN(torch.nn.Module):\r\n\r\n    def __init__(self):\r\n        super(CNN, self).__init__()\r\n        self._build_net()\r\n\r\n    def _build_net(self):\r\n        # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\r\n        self.keep_prob = 0.5\r\n        # L1 ImgIn shape=(?, 28, 28, 1)\r\n        #    Conv     -> (?, 28, 28, 32)\r\n        #    Pool     -> (?, 14, 14, 32)\r\n        self.layer1 = torch.nn.Sequential(\r\n            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\r\n            torch.nn.ReLU(),\r\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\r\n        # L2 ImgIn shape=(?, 14, 14, 32)\r\n        #    Conv      ->(?, 14, 14, 64)\r\n        #    Pool      ->(?, 7, 7, 64)\r\n        self.layer2 = torch.nn.Sequential(\r\n            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\r\n            torch.nn.ReLU(),\r\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\r\n        # L3 ImgIn shape=(?, 7, 7, 64)\r\n        #    Conv      ->(?, 7, 7, 128)\r\n        #    Pool      ->(?, 4, 4, 128)\r\n        self.layer3 = torch.nn.Sequential(\r\n            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\r\n            torch.nn.ReLU(),\r\n            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1))\r\n        # L4 FC 4x4x128 inputs -> 625 outputs\r\n        self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)\r\n        torch.nn.init.xavier_uniform_(self.fc1.weight)\r\n        self.layer4 = torch.nn.Sequential(\r\n            self.fc1,\r\n            torch.nn.ReLU(),\r\n            torch.nn.Dropout(p=1 - self.keep_prob))\r\n        # L5 Final FC 625 inputs -> 10 outputs\r\n        self.fc2 = torch.nn.Linear(625, 10, bias=True)\r\n        torch.nn.init.xavier_uniform_(self.fc2.weight)\r\n\r\n        # define cost/loss & optimizer\r\n        self.criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\r\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\r\n\r\n    def forward(self, x):\r\n        out = self.layer1(x)\r\n        out = self.layer2(out)\r\n        out = self.layer3(out)\r\n        out = out.view(out.size(0), -1)   # Flatten them for FC\r\n        out = self.layer4(out)\r\n        out = self.fc2(out)\r\n        return out\r\n\r\n    def predict(self, x):\r\n        self.eval()\r\n        return self.forward(x)\r\n\r\n    def get_accuracy(self, x, y):\r\n        prediction = self.predict(x)\r\n        correct_prediction = torch.argmax(prediction, 1) == Y_test\r\n        self.accuracy = correct_prediction.float().mean().item()\r\n        return self.accuracy\r\n\r\n    def train_model(self, x, y):\r\n        self.train()\r\n        self.optimizer.zero_grad()\r\n        hypothesis = self.forward(x)\r\n        self.cost = self.criterion(hypothesis, y)\r\n        self.cost.backward()\r\n        self.optimizer.step()\r\n        return self.cost\r\n\r\n\r\n# instantiate CNN model\r\nmodel = CNN().to(device)\r\n\r\n# train my model\r\ntotal_batch = len(data_loader)\r\nprint('Learning started. It takes sometime.')\r\nfor epoch in range(training_epochs):\r\n    avg_cost = 0\r\n\r\n    for X, Y in data_loader:\r\n        # image is already size of (28x28), no reshape\r\n        # label is not one-hot encoded\r\n        X = X.to(device)\r\n        Y = Y.to(device)\r\n\r\n        cost = model.train_model(X, Y)\r\n\r\n        avg_cost += cost / total_batch\r\n\r\n    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\r\n\r\nprint('Learning Finished!')\r\n\r\n# Test model and check accuracy\r\nwith torch.no_grad():\r\n    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\r\n    Y_test = mnist_test.test_labels.to(device)\r\n\r\n    print('Accuracy:', model.get_accuracy(X_test, Y_test))\r\n"""
CNN/lab-11-5-mnist_cnn_ensemble.py,31,"b""# Lab 11 MNIST and Deep learning CNN\r\nimport torch\r\nimport torchvision.datasets as dsets\r\nimport torchvision.transforms as transforms\r\nimport torch.nn.init\r\nimport numpy as np\r\n\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\n# for reproducibility\r\ntorch.manual_seed(777)\r\nif device == 'cuda':\r\n    torch.cuda.manual_seed_all(777)\r\n\r\n# parameters\r\nlearning_rate = 0.001\r\ntraining_epochs = 15\r\nbatch_size = 100\r\n\r\n# MNIST dataset\r\nmnist_train = dsets.MNIST(root='MNIST_data/',\r\n                          train=True,\r\n                          transform=transforms.ToTensor(),\r\n                          download=True)\r\n\r\nmnist_test = dsets.MNIST(root='MNIST_data/',\r\n                         train=False,\r\n                         transform=transforms.ToTensor(),\r\n                         download=True)\r\n\r\n# dataset loader\r\ndata_loader = torch.utils.data.DataLoader(dataset=mnist_train,\r\n                                          batch_size=batch_size,\r\n                                          shuffle=True,\r\n                                          drop_last=True)\r\n\r\n# CNN Model\r\nclass CNN(torch.nn.Module):\r\n\r\n    def __init__(self):\r\n        super(CNN, self).__init__()\r\n        self._build_net()\r\n\r\n    def _build_net(self):\r\n        # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\r\n        self.keep_prob = 0.5\r\n        # L1 ImgIn shape=(?, 28, 28, 1)\r\n        #    Conv     -> (?, 28, 28, 32)\r\n        #    Pool     -> (?, 14, 14, 32)\r\n        self.layer1 = torch.nn.Sequential(\r\n            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\r\n            torch.nn.ReLU(),\r\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\r\n        # L2 ImgIn shape=(?, 14, 14, 32)\r\n        #    Conv      ->(?, 14, 14, 64)\r\n        #    Pool      ->(?, 7, 7, 64)\r\n        self.layer2 = torch.nn.Sequential(\r\n            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\r\n            torch.nn.ReLU(),\r\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\r\n        # L3 ImgIn shape=(?, 7, 7, 64)\r\n        #    Conv      ->(?, 7, 7, 128)\r\n        #    Pool      ->(?, 4, 4, 128)\r\n        self.layer3 = torch.nn.Sequential(\r\n            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\r\n            torch.nn.ReLU(),\r\n            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1))\r\n        # L4 FC 4x4x128 inputs -> 625 outputs\r\n        self.fc1 = torch.nn.Linear(4 * 4 * 128, 625, bias=True)\r\n        torch.nn.init.xavier_uniform_(self.fc1.weight)\r\n        self.layer4 = torch.nn.Sequential(\r\n            self.fc1,\r\n            torch.nn.ReLU(),\r\n            torch.nn.Dropout(p=1 - self.keep_prob))\r\n        # L5 Final FC 625 inputs -> 10 outputs\r\n        self.fc2 = torch.nn.Linear(625, 10, bias=True)\r\n        torch.nn.init.xavier_uniform_(self.fc2.weight)\r\n\r\n        # define cost/loss & optimizer\r\n        self.criterion = torch.nn.CrossEntropyLoss()    # Softmax is internally computed.\r\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\r\n\r\n    def forward(self, x):\r\n        out = self.layer1(x)\r\n        out = self.layer2(out)\r\n        out = self.layer3(out)\r\n        out = out.view(out.size(0), -1)   # Flatten them for FC\r\n        out = self.layer4(out)\r\n        out = self.fc2(out)\r\n        return out\r\n\r\n    def predict(self, x):\r\n        self.eval()\r\n        return self.forward(x)\r\n\r\n    def get_accuracy(self, x, y):\r\n        prediction = self.predict(x)\r\n        correct_prediction = torch.argmax(prediction, 1) == Y_test\r\n        self.accuracy = correct_prediction.float().mean()\r\n        return self.accuracy\r\n\r\n    def train_model(self, x, y):\r\n        self.train()\r\n        self.optimizer.zero_grad()\r\n        hypothesis = self.forward(x)\r\n        self.cost = self.criterion(hypothesis, y)\r\n        self.cost.backward()\r\n        self.optimizer.step()\r\n        return self.cost\r\n\r\n\r\n# instantiate CNN model\r\nmodels = []\r\nnum_models = 2\r\nfor m in range(num_models):\r\n    models.append(CNN().to(device))\r\n\r\n# train my model\r\ntotal_batch = len(data_loader)\r\nprint('Learning started. It takes sometime.')\r\nfor epoch in range(training_epochs):\r\n    avg_cost_list = np.zeros(len(models))\r\n\r\n    for X, Y in data_loader:\r\n        X = X.to(device)\r\n        Y = Y.to(device)\r\n        # image is already size of (28x28), no reshape\r\n        # label is not one-hot encoded\r\n\r\n        for m_idx, m in enumerate(models):\r\n            cost = m.train_model(X, Y)\r\n            avg_cost_list[m_idx] += cost / total_batch\r\n\r\n    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost_list.mean()))\r\n\r\nprint('Learning Finished!')\r\n\r\n# Test model and check accuracy\r\nwith torch.no_grad():\r\n    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\r\n    Y_test = mnist_test.test_labels.to(device)\r\n    predictions = torch.zeros([len(mnist_test), 10])\r\n    for m_idx, m in enumerate(models):\r\n        print(m_idx, 'Accuracy:', m.get_accuracy(X_test, Y_test))\r\n        p = m.predict(X_test)\r\n        predictions += p.cpu()\r\n\r\n    ensemble_correct_prediction = torch.argmax(predictions, 1) == Y_test.cpu()\r\n    ensemble_accuracy = ensemble_correct_prediction.float().mean()\r\n    print('Accuracy:', ensemble_accuracy.item())"""
RNN/1-basics.py,3,"b'import torch\nimport numpy as np\n\n# Random seed to make results deterministic and reproducible\ntorch.manual_seed(0)\n\n# declare dimension\ninput_size = 4\nhidden_size = 2\n\n# singleton example\n# shape : (1, 1, 4)\n# input_data_np = np.array([[[1, 0, 0, 0]]])\n\n# sequential example\n# shape : (3, 5, 4)\nh = [1, 0, 0, 0]\ne = [0, 1, 0, 0]\nl = [0, 0, 1, 0]\no = [0, 0, 0, 1]\ninput_data_np = np.array([[h, e, l, l, o], [e, o, l, l, l], [l, l, e, e, l]], dtype=np.float32)\n\n# transform as torch tensor\ninput_data = torch.Tensor(input_data_np)\n\n# declare RNN\nrnn = torch.nn.RNN(input_size, hidden_size)\n\n# check output\noutputs, _status = rnn(input_data)\nprint(outputs)\nprint(outputs.size())\n'"
RNN/2-hihello.py,6,"b'import torch\nimport torch.optim as optim\nimport numpy as np\n\n# Random seed to make results deterministic and reproducible\ntorch.manual_seed(0)\n\nchar_set = [\'h\', \'i\', \'e\', \'l\', \'o\']\n\n# hyper parameters\ninput_size = len(char_set)\nhidden_size = len(char_set)\nlearning_rate = 0.1\n\n# data setting\nx_data = [[0, 1, 0, 2, 3, 3]]\nx_one_hot = [[[1, 0, 0, 0, 0],\n              [0, 1, 0, 0, 0],\n              [1, 0, 0, 0, 0],\n              [0, 0, 1, 0, 0],\n              [0, 0, 0, 1, 0],\n              [0, 0, 0, 1, 0]]]\ny_data = [[1, 0, 2, 3, 3, 4]]\n\n# transform as torch tensor variable\nX = torch.FloatTensor(x_one_hot)\nY = torch.LongTensor(y_data)\n\n# declare RNN\nrnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)  # batch_first guarantees the order of output = (B, S, F)\n\n# loss & optimizer setting\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = optim.Adam(rnn.parameters(), learning_rate)\n\n# start training\nfor i in range(100):\n    optimizer.zero_grad()\n    outputs, _status = rnn(X)\n    loss = criterion(outputs.view(-1, input_size), Y.view(-1))\n    loss.backward()\n    optimizer.step()\n\n    result = outputs.data.numpy().argmax(axis=2)\n    result_str = \'\'.join([char_set[c] for c in np.squeeze(result)])\n    print(i, ""loss: "", loss.item(), ""prediction: "", result, ""true Y: "", y_data, ""prediction str: "", result_str)\n'"
RNN/3-charseq.py,6,"b'import torch\nimport torch.optim as optim\nimport numpy as np\n\n# Random seed to make results deterministic and reproducible\ntorch.manual_seed(0)\n\nsample = "" if you want you""\n\n# make dictionary\nchar_set = list(set(sample))\nchar_dic = {c: i for i, c in enumerate(char_set)}\n\n# hyper parameters\ndic_size = len(char_dic)\nhidden_size = len(char_dic)\nlearning_rate = 0.1\n\n# data setting\nsample_idx = [char_dic[c] for c in sample]\nx_data = [sample_idx[:-1]]\nx_one_hot = [np.eye(dic_size)[x] for x in x_data]\ny_data = [sample_idx[1:]]\n\n# transform as torch tensor variable\nX = torch.FloatTensor(x_one_hot)\nY = torch.LongTensor(y_data)\n\n# declare RNN\nrnn = torch.nn.RNN(dic_size, hidden_size, batch_first=True)\n\n# loss & optimizer setting\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = optim.Adam(rnn.parameters(), learning_rate)\n\n# start training\nfor i in range(50):\n    optimizer.zero_grad()\n    outputs, _status = rnn(X)\n    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n    loss.backward()\n    optimizer.step()\n\n    result = outputs.data.numpy().argmax(axis=2)\n    result_str = \'\'.join([char_set[c] for c in np.squeeze(result)])\n    print(i, ""loss: "", loss.item(), ""prediction: "", result, ""true Y: "", y_data, ""prediction str: "", result_str)\n'"
RNN/4-longseq.py,8,"b'import torch\nimport torch.optim as optim\nimport numpy as np\n\n# Random seed to make results deterministic and reproducible\ntorch.manual_seed(0)\n\nsentence = (""if you want to build a ship, don\'t drum up people together to ""\n            ""collect wood and don\'t assign them tasks and work, but rather ""\n            ""teach them to long for the endless immensity of the sea."")\n\n# make dictionary\nchar_set = list(set(sentence))\nchar_dic = {c: i for i, c in enumerate(char_set)}\n\n# hyper parameters\ndic_size = len(char_dic)\nhidden_size = len(char_dic)\nsequence_length = 10  # Any arbitrary number\nlearning_rate = 0.1\n\n# data setting\nx_data = []\ny_data = []\n\nfor i in range(0, len(sentence) - sequence_length):\n    x_str = sentence[i:i + sequence_length]\n    y_str = sentence[i + 1: i + sequence_length + 1]\n    print(i, x_str, \'->\', y_str)\n\n    x_data.append([char_dic[c] for c in x_str])  # x str to index\n    y_data.append([char_dic[c] for c in y_str])  # y str to index\n\nx_one_hot = [np.eye(dic_size)[x] for x in x_data]\n\n# transform as torch tensor variable\nX = torch.FloatTensor(x_one_hot)\nY = torch.LongTensor(y_data)\n\n\n# declare RNN + FC\nclass Net(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, layers):\n        super(Net, self).__init__()\n        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n\n    def forward(self, x):\n        x, _status = self.rnn(x)\n        x = self.fc(x)\n        return x\n\n\nnet = Net(dic_size, hidden_size, 2)\n\n# loss & optimizer setting\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), learning_rate)\n\n# start training\nfor i in range(100):\n    optimizer.zero_grad()\n    outputs = net(X)\n    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n    loss.backward()\n    optimizer.step()\n\n    results = outputs.argmax(dim=2)\n    predict_str = """"\n    for j, result in enumerate(results):\n        print(i, j, \'\'.join([char_set[t] for t in result]), loss.item())\n        if j == 0:\n            predict_str += \'\'.join([char_set[t] for t in result])\n        else:\n            predict_str += char_set[result[-1]]\n\n    print(predict_str)'"
RNN/5-timeseries.py,10,"b'import torch\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Random seed to make results deterministic and reproducible\ntorch.manual_seed(0)\n\n\ndef minmax_scaler(data):\n    numerator = data - np.min(data, 0)\n    denominator = np.max(data, 0) - np.min(data, 0)\n    return numerator / (denominator + 1e-7)\n\n\ndef build_dataset(time_series, seq_length):\n    dataX = []\n    dataY = []\n    for i in range(0, len(time_series) - seq_length):\n        _x = time_series[i:i + seq_length, :]\n        _y = time_series[i + seq_length, [-1]]  # Next close price\n        print(_x, ""->"", _y)\n        dataX.append(_x)\n        dataY.append(_y)\n    return np.array(dataX), np.array(dataY)\n\n\nseq_length = 7\ndata_dim = 5\nhidden_dim = 10\noutput_dim = 1\nlearning_rate = 0.01\niterations = 500\n\nxy = np.loadtxt(""data-02-stock_daily.csv"", delimiter="","")\nxy = xy[::-1]  # reverse order\n\ntrain_size = int(len(xy) * 0.7)\ntrain_set = xy[0:train_size]\ntest_set = xy[train_size - seq_length:]\n\ntrain_set = minmax_scaler(train_set)\ntest_set = minmax_scaler(test_set)\n\ntrainX, trainY = build_dataset(train_set, seq_length)\ntestX, testY = build_dataset(test_set, seq_length)\n\ntrainX_tensor = torch.FloatTensor(trainX)\ntrainY_tensor = torch.FloatTensor(trainY)\n\ntestX_tensor = torch.FloatTensor(testX)\ntestY_tensor = torch.FloatTensor(testY)\n\n\nclass Net(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, layers):\n        super(Net, self).__init__()\n        self.rnn = torch.nn.LSTM(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n        self.fc = torch.nn.Linear(hidden_dim, output_dim, bias=True)\n\n    def forward(self, x):\n        x, _status = self.rnn(x)\n        x = self.fc(x[:, -1])\n        return x\n\n\nnet = Net(data_dim, hidden_dim, output_dim, 1)\n\n# loss & optimizer setting\ncriterion = torch.nn.MSELoss()\noptimizer = optim.Adam(net.parameters(), lr=learning_rate)\n\nfor i in range(iterations):\n\n    optimizer.zero_grad()\n    outputs = net(trainX_tensor)\n    loss = criterion(outputs, trainY_tensor)\n    loss.backward()\n    optimizer.step()\n    print(i, loss.item())\n\nplt.plot(testY)\nplt.plot(net(testX_tensor).data.numpy())\nplt.legend([\'original\', \'prediction\'])\nplt.show()\n'"
RNN/6-seq2seq.py,10,"b'# main reference\n# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ntorch.manual_seed(0)\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\nraw = [""I feel hungry.\t\xeb\x82\x98\xeb\x8a\x94 \xeb\xb0\xb0\xea\xb0\x80 \xea\xb3\xa0\xed\x94\x84\xeb\x8b\xa4."",\n       ""Pytorch is very easy.\t\xed\x8c\x8c\xec\x9d\xb4\xed\x86\xa0\xec\xb9\x98\xeb\x8a\x94 \xeb\xa7\xa4\xec\x9a\xb0 \xec\x89\xbd\xeb\x8b\xa4."",\n       ""Pytorch is a framework for deep learning.\t\xed\x8c\x8c\xec\x9d\xb4\xed\x86\xa0\xec\xb9\x98\xeb\x8a\x94 \xeb\x94\xa5\xeb\x9f\xac\xeb\x8b\x9d\xec\x9d\x84 \xec\x9c\x84\xed\x95\x9c \xed\x94\x84\xeb\xa0\x88\xec\x9e\x84\xec\x9b\x8c\xed\x81\xac\xec\x9d\xb4\xeb\x8b\xa4."",\n       ""Pytorch is very clear to use.\t\xed\x8c\x8c\xec\x9d\xb4\xed\x86\xa0\xec\xb9\x98\xeb\x8a\x94 \xec\x82\xac\xec\x9a\xa9\xed\x95\x98\xea\xb8\xb0 \xeb\xa7\xa4\xec\x9a\xb0 \xec\xa7\x81\xea\xb4\x80\xec\xa0\x81\xec\x9d\xb4\xeb\x8b\xa4.""]\n\nSOS_token = 0\nEOS_token = 1\n\n\nclass Vocab:\n    def __init__(self):\n        self.vocab2index = {""<SOS>"": SOS_token, ""<EOS>"": EOS_token}\n        self.index2vocab = {SOS_token: ""<SOS>"", EOS_token: ""<EOS>""}\n        self.vocab_count = {}\n        self.n_vocab = len(self.vocab2index)\n\n    def add_vocab(self, sentence):\n        for word in sentence.split("" ""):\n            if word not in self.vocab2index:\n                self.vocab2index[word] = self.n_vocab\n                self.vocab_count[word] = 1\n                self.index2vocab[self.n_vocab] = word\n                self.n_vocab += 1\n            else:\n                self.vocab_count[word] += 1\n\n\ndef filter_pair(pair, source_max_length, target_max_length):\n    return len(pair[0].split("" "")) < source_max_length and len(pair[1].split("" "")) < target_max_length\n\n\ndef preprocess(corpus, source_max_length, target_max_length):\n    print(""reading corpus..."")\n    pairs = []\n    for line in corpus:\n        pairs.append([s for s in line.strip().lower().split(""\\t"")])\n    print(""Read {} sentence pairs"".format(len(pairs)))\n\n    pairs = [pair for pair in pairs if filter_pair(pair, source_max_length, target_max_length)]\n    print(""Trimmed to {} sentence pairs"".format(len(pairs)))\n\n    source_vocab = Vocab()\n    target_vocab = Vocab()\n\n    print(""Counting words..."")\n    for pair in pairs:\n        source_vocab.add_vocab(pair[0])\n        target_vocab.add_vocab(pair[1])\n    print(""source vocab size ="", source_vocab.n_vocab)\n    print(""target vocab size ="", target_vocab.n_vocab)\n\n    return pairs, source_vocab, target_vocab\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n\n    def forward(self, x, hidden):\n        x = self.embedding(x).view(1, 1, -1)\n        x, hidden = self.gru(x, hidden)\n        return x, hidden\n\n\nclass Decoder(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x, hidden):\n        x = self.embedding(x).view(1, 1, -1)\n        x, hidden = self.gru(x, hidden)\n        x = self.softmax(self.out(x[0]))\n        return x, hidden\n\n\ndef tensorize(vocab, sentence):\n    indexes = [vocab.vocab2index[word] for word in sentence.split("" "")]\n    indexes.append(vocab.vocab2index[""<EOS>""])\n    return torch.Tensor(indexes).long().to(device).view(-1, 1)\n\n\ndef train(pairs, source_vocab, target_vocab, encoder, decoder, n_iter, print_every=1000, learning_rate=0.01):\n    loss_total = 0\n\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n\n    training_batch = [random.choice(pairs) for _ in range(n_iter)]\n    training_source = [tensorize(source_vocab, pair[0]) for pair in training_batch]\n    training_target = [tensorize(target_vocab, pair[1]) for pair in training_batch]\n\n    criterion = nn.NLLLoss()\n\n    for i in range(1, n_iter + 1):\n        source_tensor = training_source[i - 1]\n        target_tensor = training_target[i - 1]\n\n        encoder_hidden = torch.zeros([1, 1, encoder.hidden_size]).to(device)\n\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n        source_length = source_tensor.size(0)\n        target_length = target_tensor.size(0)\n\n        loss = 0\n\n        for enc_input in range(source_length):\n            _, encoder_hidden = encoder(source_tensor[enc_input], encoder_hidden)\n\n        decoder_input = torch.Tensor([[SOS_token]]).long().to(device)\n        decoder_hidden = encoder_hidden\n\n        for di in range(target_length):\n            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # teacher forcing\n\n        loss.backward()\n\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n        loss_iter = loss.item() / target_length\n        loss_total += loss_iter\n\n        if i % print_every == 0:\n            loss_avg = loss_total / print_every\n            loss_total = 0\n            print(""[{} - {}%] loss = {:05.4f}"".format(i, i / n_iter * 100, loss_avg))\n\n\ndef evaluate(pairs, source_vocab, target_vocab, encoder, decoder, target_max_length):\n    for pair in pairs:\n        print("">"", pair[0])\n        print(""="", pair[1])\n        source_tensor = tensorize(source_vocab, pair[0])\n        source_length = source_tensor.size()[0]\n        encoder_hidden = torch.zeros([1, 1, encoder.hidden_size]).to(device)\n\n        for ei in range(source_length):\n            _, encoder_hidden = encoder(source_tensor[ei], encoder_hidden)\n\n        decoder_input = torch.Tensor([[SOS_token]], device=device).long()\n        decoder_hidden = encoder_hidden\n        decoded_words = []\n\n        for di in range(target_max_length):\n            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n            _, top_index = decoder_output.data.topk(1)\n            if top_index.item() == EOS_token:\n                decoded_words.append(""<EOS>"")\n                break\n            else:\n                decoded_words.append(target_vocab.index2vocab[top_index.item()])\n\n            decoder_input = top_index.squeeze().detach()\n\n        predict_words = decoded_words\n        predict_sentence = "" "".join(predict_words)\n        print(""<"", predict_sentence)\n        print("""")\n\n\nSOURCE_MAX_LENGTH = 10\nTARGET_MAX_LENGTH = 12\nload_pairs, load_source_vocab, load_target_vocab = preprocess(raw, SOURCE_MAX_LENGTH, TARGET_MAX_LENGTH)\nprint(random.choice(load_pairs))\n\nenc_hidden_size = 16\ndec_hidden_size = enc_hidden_size\nenc = Encoder(load_source_vocab.n_vocab, enc_hidden_size).to(device)\ndec = Decoder(dec_hidden_size, load_target_vocab.n_vocab).to(device)\n\ntrain(load_pairs, load_source_vocab, load_target_vocab, enc, dec, 5000, print_every=1000)\nevaluate(load_pairs, load_source_vocab, load_target_vocab, enc, dec, TARGET_MAX_LENGTH)\n'"
RNN/7-packed_sequence.py,4,"b""import torch\nimport numpy as np\nfrom torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence\n\n# Random word from random word generator\ndata = ['hello world',\n        'midnight',\n        'calculation',\n        'path',\n        'short circuit']\n\n# Make dictionary\nchar_set = ['<pad>'] + list(set(char for seq in data for char in seq)) # Get all characters and include pad token\nchar2idx = {char: idx for idx, char in enumerate(char_set)} # Constuct character to index dictionary\nprint('char_set:', char_set)\nprint('char_set length:', len(char_set))\n\n# Convert character to index and\n# Make list of tensors\nX = [torch.LongTensor([char2idx[char] for char in seq]) for seq in data]\n\n# Check converted result\nfor sequence in X:\n    print(sequence)\n\n# Make length tensor (will be used later in 'pack_padded_sequence' function)\nlengths = [len(seq) for seq in X]\nprint('lengths:', lengths)\n\n# Make a Tensor of shape (Batch x Maximum_Sequence_Length)\npadded_sequence = pad_sequence(X, batch_first=True) # X is now padded sequence\nprint(padded_sequence)\nprint(padded_sequence.shape)\n\n# Sort by descending lengths\nsorted_idx = sorted(range(len(lengths)), key=lengths.__getitem__, reverse=True)\nsorted_X = [X[idx] for idx in sorted_idx]\n\n# Check converted result\nfor sequence in sorted_X:\n    print(sequence)\n\npacked_sequence = pack_sequence(sorted_X)\nprint(packed_sequence)\n\n# one-hot embedding using PaddedSequence\neye = torch.eye(len(char_set)) # Identity matrix of shape (len(char_set), len(char_set))\nembedded_tensor = eye[padded_sequence]\nprint(embedded_tensor.shape) # shape: (Batch_size, max_sequence_length, number_of_input_tokens)\n\n# one-hot embedding using PackedSequence\nembedded_packed_seq = pack_sequence([eye[X[idx]] for idx in sorted_idx])\nprint(embedded_packed_seq.data.shape)\n\n# declare RNN\nrnn = torch.nn.RNN(input_size=len(char_set), hidden_size=30, batch_first=True)\n\n# Try out PaddedSequence\nrnn_output, hidden = rnn(embedded_tensor)\nprint(rnn_output.shape) # shape: (batch_size, max_seq_length, hidden_size)\nprint(hidden.shape)     # shape: (num_layers * num_directions, batch_size, hidden_size)\n\n# Try out PackedSequence\nrnn_output, hidden = rnn(embedded_packed_seq)\nprint(rnn_output.data.shape)\nprint(hidden.data.shape)\n\n# Try out pad_packed_sequence\nunpacked_sequence, seq_lengths = pad_packed_sequence(embedded_packed_seq, batch_first=True)\nprint(unpacked_sequence.shape)\nprint(seq_lengths)\n\n# Construct embedded_padded_sequence\nembedded_padded_sequence = eye[pad_sequence(sorted_X, batch_first=True)]\nprint(embedded_padded_sequence.shape)\n\n# Try out pack_padded_sequence\nsorted_lengths = sorted(lengths, reverse=True)\nnew_packed_sequence = pack_padded_sequence(embedded_padded_sequence, sorted_lengths, batch_first=True)\nprint(new_packed_sequence.data.shape)\nprint(new_packed_sequence.batch_sizes)\n\n\n"""
pyfiles/1_tensor_manipulation.py,33,"b'""""""\nCorresponds to Lab 8 of \xeb\xaa\xa8\xeb\x91\x90\xeb\xa5\xbc \xec\x9c\x84\xed\x95\x9c \xeb\x94\xa5\xeb\x9f\xac\xeb\x8b\x9d \xea\xb0\x95\xec\xa2\x8c \xec\x8b\x9c\xec\xa6\x8c 1 for TensorFlow.\n""""""\nimport numpy as np\nimport torch\n\n\nprint(\'-----------------------\')\nprint(\'NumPy Review - 1D Array\')\nprint(\'-----------------------\')\nt = np.array([0., 1., 2., 3., 4., 5., 6.])\nprint(t)\nprint(t.ndim)  # rank\nprint(t.shape) # shape\nprint(t[0], t[1], t[-1]) # Element\nprint(t[2:5], t[4:-1])   # Slicing\nprint(t[:2], t[3:])      # Slicing\n\nprint()\nprint(\'-----------------------\')\nprint(\'NumPy Review - 2D Array\')\nprint(\'-----------------------\')\nt = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\nprint(t)\nprint(t.ndim)  # rank\nprint(t.shape) # shape\n\nprint()\nprint(\'-------------------------------\')\nprint(\'PyTorch is just like NumPy - 1D\')\nprint(\'-------------------------------\')\nt = np.array([0., 1., 2., 3., 4., 5., 6.])\nft = torch.FloatTensor(t)\nprint(ft)\nprint(ft.dim())  # rank\nprint(ft.shape)  # shape\nprint(ft.size()) # shape\nprint(ft[0], ft[1], ft[-1]) # Element\nprint(ft[2:5], ft[4:-1])    # Slicing\nprint(ft[:2], ft[3:])       # Slicing\n\nprint()\nprint(\'-------------------------------\')\nprint(\'PyTorch is just like NumPy - 2D\')\nprint(\'-------------------------------\')\nt = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])\nft = torch.FloatTensor(t)\nprint(ft)\nprint(ft.dim())  # rank\nprint(ft.size()) # shape\n\nprint()\nprint(\'-----------------\')\nprint(\'Shape, Rank, Axis\')\nprint(\'-----------------\')\nt = np.array([[[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],\n               [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]]])\nft = torch.FloatTensor(t)\nprint(ft.dim())  # rank  = 4\nprint(ft.size()) # shape = (1, 2, 3, 4)\n\nprint()\nprint(\'-------------\')\nprint(\'Mul vs Matmul\')\nprint(\'-------------\')\nm1 = torch.FloatTensor([[1, 2], [3, 4]])\nm2 = torch.FloatTensor([[1], [2]])\nprint(\'Shape of Matrix 1: \', m1.shape) # 2 x 2\nprint(\'Shape of Matrix 2: \', m2.shape) # 2 x 1\nprint(m1.matmul(m2)) # 2 x 1\n\nm1 = torch.FloatTensor([[1, 2], [3, 4]])\nm2 = torch.FloatTensor([[1], [2]])\nprint(\'Shape of Matrix 1: \', m1.shape) # 2 x 2\nprint(\'Shape of Matrix 2: \', m2.shape) # 2 x 1\nprint(m1 * m2) # 2 x 2\nprint(m1.mul(m2))\n\nprint()\nprint(\'----------------------\')\nprint(\'Broadcasting [WARNING]\')\nprint(\'----------------------\')\n# Same shape\nm1 = torch.FloatTensor([[3, 3]])\nm2 = torch.FloatTensor([[2, 2]])\nprint(m1 + m2)\n\n# Vector + scalar\nm1 = torch.FloatTensor([[1, 2]])\nm2 = torch.FloatTensor([3]) # 3 -> [[3, 3]]\nprint(m1 + m2)\n\n# 2 x 1 Vector + 1 x 2 Vector\nm1 = torch.FloatTensor([[1, 2]])\nm2 = torch.FloatTensor([[3], [4]])\nprint(m1 + m2)\n\nprint()\nprint(\'---------\')\nprint(\'Mean - 1D\')\nprint(\'---------\')\nt = torch.FloatTensor([1, 2])\nprint(t.mean())\n\n# Can\'t use mean() on integers\nt = torch.LongTensor([1, 2])\ntry:\n    print(t.mean())\nexcept Exception as exc:\n    print(exc)\n\nprint()\nprint(\'---------\')\nprint(\'Mean - 2D\')\nprint(\'---------\')\nt = torch.FloatTensor([[1, 2], [3, 4]])\nprint(t)\n\nprint(t.mean())\nprint(t.mean(dim=0))\nprint(t.mean(dim=1))\nprint(t.mean(dim=-1))\n\nprint()\nprint(\'--------\')\nprint(\'Sum - 2D\')\nprint(\'--------\')\nt = torch.FloatTensor([[1, 2], [3, 4]])\nprint(t)\n\nprint(t.sum())\nprint(t.sum(dim=0))\nprint(t.sum(dim=1))\nprint(t.sum(dim=-1))\n\nprint()\nprint(\'--------------\')\nprint(\'Max and Argmax\')\nprint(\'--------------\')\nt = torch.FloatTensor([[1, 2], [3, 4]])\nprint(t)\n\nprint(t.max()) # Returns one value: max\nprint(t.max(dim=0)) # Returns two values: max and argmax\nprint(\'Max: \', t.max(dim=0)[0])\nprint(\'Argmax: \', t.max(dim=0)[1])\n\nprint(t.max(dim=1))\nprint(t.max(dim=-1))\n\nprint()\nprint(\'------\')\nprint(\'View**\')\nprint(\'------\')\nt = np.array([[[0, 1, 2],\n               [3, 4, 5]],\n\n              [[6, 7, 8],\n               [9, 10, 11]]])\nft = torch.FloatTensor(t)\nprint(ft.shape)\n\nprint(ft.view([-1, 3]))\nprint(ft.view([-1, 3]).shape)\n\nprint(ft.view([-1, 1, 3]))\nprint(ft.view([-1, 1, 3]).shape)\n\nprint()\nprint(\'---------------------\')\nprint(\'Squeeze and Unsqueeze\')\nprint(\'---------------------\')\nft = torch.FloatTensor([[0], [1], [2]])\nprint(ft)\nprint(ft.shape)\n\nprint(ft.squeeze())\nprint(ft.squeeze().shape)\n\nft = torch.Tensor([0, 1, 2])\nprint(ft.shape)\n\nprint(ft.unsqueeze(0))\nprint(ft.unsqueeze(0).shape)\n\nprint(ft.unsqueeze(1))\nprint(ft.unsqueeze(1).shape)\n\nprint(ft.unsqueeze(-1))\nprint(ft.unsqueeze(-1).shape)\n\nprint()\nprint(\'----------------------------\')\nprint(\'Scatter for One-hot encoding\')\nprint(\'----------------------------\')\nlt = torch.LongTensor([[0], [1], [2], [0]])\nprint(lt)\n\none_hot = torch.zeros(4, 3) # batch_size = 4, classes = 3\none_hot.scatter_(1, lt, 1)\nprint(one_hot)\n\nprint()\nprint(\'-------\')\nprint(\'Casting\')\nprint(\'-------\')\nlt = torch.LongTensor([1, 2, 3, 4])\nprint(lt)\nprint(lt.float())\n\nbt = torch.ByteTensor([True, False, False, True])\nprint(bt)\nprint(bt.long())\nprint(bt.float())\n\nprint()\nprint(\'--------\')\nprint(\'Stacking\')\nprint(\'--------\')\nx = torch.FloatTensor([1, 4])\ny = torch.FloatTensor([2, 5])\nz = torch.FloatTensor([3, 6])\n\nprint(torch.stack([x, y, z]))\nprint(torch.stack([x, y, z], dim=1))\n\nprint()\nprint(\'-------------------\')\nprint(\'Ones and Zeros like\')\nprint(\'-------------------\')\n\nx = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])\nprint(x)\nprint(torch.ones_like(x))\nprint(torch.zeros_like(x))\n\nprint()\nprint(\'---\')\nprint(\'Zip\')\nprint(\'---\')\n\nfor x, y in zip([1, 2, 3], [4, 5, 6]):\n    print(x, y)\n\nfor x, y, z in zip([1, 2, 3], [4, 5, 6], [7, 8, 9]):\n    print(x, y, z)\n'"
RNN/season1_refactored/RNN_intro_1.py,6,"b'import torch\nimport torch.nn as nn\n\n# Random seed to make results deterministic\ntorch.manual_seed(0)\n\n# One hot encoding for each char in \'hello\'\nh = [1, 0, 0, 0]\ne = [0, 1, 0, 0]\nl = [0, 0, 1, 0]\no = [0, 0, 0, 1]\n\n# One cell RNN input_dim (4) -> output_dim (2). sequence: 5\ncell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n\n# (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\n# num_directions is 2 when the RNN is bidirectional, otherwise, it is 1\nhidden = torch.randn(1, 1, 2)\n\n# Propagate input through RNN\n# Input: (batch, seq_len, input_size) when batch_first=True\ninputs = torch.Tensor([h, e, l, l, o])\nfor one in inputs:\n    one = one.view(1, 1, -1)\n    # Input: (batch, seq_len, input_size) when batch_first=True\n    out, hidden = cell(one, hidden)\n    print(""one input size"", one.size(), ""out size"", out.size())\n\n# We can do the whole at once\n# Propagate input through RNN\n# Input: (batch, seq_len, input_size) when batch_first=True\ninputs = inputs.view(1, 5, -1)\nout, hidden = cell(inputs, hidden)\nprint(""sequence input size"", inputs.size(), ""out size"", out.size())\n\n\n# hidden : (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\nhidden = torch.randn(1, 3, 2)\n\n# One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n# 3 batches \'hello\', \'eolll\', \'lleel\'\n# rank = (3, 5, 4)\ninputs = torch.Tensor([[h, e, l, l, o],\n                       [e, o, l, l, l],\n                       [l, l, e, e, l]])\n\n# Propagate input through RNN\n# Input: (batch, seq_len, input_size) when batch_first=True\n# B x S x I\nout, hidden = cell(inputs, hidden)\nprint(""batch input size"", inputs.size(), ""out size"", out.size())\n\n\n# One cell RNN input_dim (4) -> output_dim (2)\ncell = nn.RNN(input_size=4, hidden_size=2)\n\n# The given dimensions dim0 and dim1 are swapped.\ninputs = inputs.transpose(dim0=0, dim1=1)\n# Propagate input through RNN\n# Input: (seq_len, batch_size, input_size) when batch_first=False (default)\n# S x B x I\nout, hidden = cell(inputs, hidden)\nprint(""batch input size"", inputs.size(), ""out size"", out.size())'"
RNN/season1_refactored/RNN_intro_2.py,6,"b'# Lab 12 RNN\nimport torch\nimport torch.nn as nn\n\ntorch.manual_seed(777)  # reproducibility\n\n#            0    1    2    3    4\nidx2char = [\'h\', \'i\', \'e\', \'l\', \'o\']\n\n# Teach hihell -> ihello\nx_data = [0, 1, 0, 2, 3, 3]   # hihell\none_hot_lookup = [[1, 0, 0, 0, 0],  # 0\n                  [0, 1, 0, 0, 0],  # 1\n                  [0, 0, 1, 0, 0],  # 2\n                  [0, 0, 0, 1, 0],  # 3\n                  [0, 0, 0, 0, 1]]  # 4\n\ny_data = [1, 0, 2, 3, 3, 4]    # ihello\nx_one_hot = [one_hot_lookup[x] for x in x_data]\n\n# As we have one batch of samples, we will change them to variables only once\ninputs = torch.Tensor(x_one_hot)\nlabels = torch.LongTensor(y_data)\n\nnum_classes = 5\ninput_size = 5  # one-hot size\nhidden_size = 5  # output from the RNN. 5 to directly predict one-hot\nbatch_size = 1   # one sentence\nsequence_length = 1  # One by one\nnum_layers = 1  # one-layer rnn\n\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        super(Model, self).__init__()\n        self.rnn = nn.RNN(input_size=input_size,\n                          hidden_size=hidden_size,\n                          batch_first=True)\n\n    def forward(self, hidden, x):\n        # Reshape input (batch first)\n        x = x.view(batch_size, sequence_length, input_size)\n\n        # Propagate input through RNN\n        # Input: (batch, seq_len, input_size)\n        # hidden: (num_layers * num_directions, batch, hidden_size)\n        out, hidden = self.rnn(x, hidden)\n        return out.view(-1, num_classes), hidden\n\n    def init_hidden(self):\n        # Initialize hidden and cell states\n        # (num_layers * num_directions, batch, hidden_size)\n        return torch.zeros(num_layers, batch_size, hidden_size)\n\n\n# Instantiate RNN model\nmodel = Model()\nprint(model)\n\n# Set loss and optimizer function\n# CrossEntropyLoss = LogSoftmax + NLLLoss\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\nprint(inputs.size(), labels.size())\n# Train the model\nfor epoch in range(100):\n    optimizer.zero_grad()\n    loss = 0\n    hidden = model.init_hidden()\n\n    print(""predicted string: "", end=\'\')\n    for input, label in zip(inputs, labels):\n        output, hidden = model(hidden, input)\n        val, idx = output.max(1)\n        print(idx2char[idx.data[0]], end=\'\')\n        loss += criterion(output, label.reshape(-1))\n\n    print(f\', epoch: {epoch + 1}, loss: {loss.item():1.3f}\')\n\n    loss.backward()\n    optimizer.step()\n\nprint(""Learning finished!"")\n'"
RNN/season1_refactored/RNN_intro_3.py,7,"b'# Lab 12 RNN\nimport torch\nimport torch.nn as nn\n\ntorch.manual_seed(777)  # reproducibility\n\n\nidx2char = [\'h\', \'i\', \'e\', \'l\', \'o\']\n\n# Teach hihell -> ihello\nx_data = [[0, 1, 0, 2, 3, 3]]   # hihell\nx_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n              [0, 1, 0, 0, 0],   # i 1\n              [1, 0, 0, 0, 0],   # h 0\n              [0, 0, 1, 0, 0],   # e 2\n              [0, 0, 0, 1, 0],   # l 3\n              [0, 0, 0, 1, 0]]]  # l 3\n\ny_data = [1, 0, 2, 3, 3, 4]    # ihello\n\n# As we have one batch of samples, we will change them to variables only once\ninputs = torch.Tensor(x_one_hot)\nlabels = torch.LongTensor(y_data)\n\nnum_classes = 5\ninput_size = 5  # one-hot size\nhidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\nbatch_size = 1   # one sentence\nsequence_length = 6  # |ihello| == 6\nnum_layers = 1  # one-layer rnn\n\n\nclass RNN(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(RNN, self).__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.sequence_length = sequence_length\n\n        self.rnn = nn.RNN(input_size=self.input_size, \n                          hidden_size=self.hidden_size,\n                          batch_first=True)\n\n    def forward(self, x):\n        # Initialize hidden and cell states\n        # (num_layers * num_directions, batch, hidden_size) for batch_first=True\n        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n\n        # Propagate input through RNN\n        # Input: (batch, seq_len, input_size)\n        # h_0: (num_layers * num_directions, batch, hidden_size)\n\n        out, _ = self.rnn(x, h_0)\n        return out.view(-1, self.num_classes)\n\n\n# Instantiate RNN model\nrnn = RNN(num_classes, input_size, hidden_size, num_layers)\nprint(rnn)\n\n# Set loss and optimizer function\n# CrossEntropyLoss = LogSoftmax + NLLLoss\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(rnn.parameters(), lr=0.1)\n\n# Train the model\nfor epoch in range(100):\n    outputs = rnn(inputs)\n    optimizer.zero_grad()\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n    _, idx = outputs.max(1)\n    idx = idx.data.numpy()\n    result_str = \'\'.join(idx2char[c] for c in idx.squeeze())\n    print(f\'epoch: {epoch + 1}, loss: {loss.item():1.3f}\')\n    print(""Predicted string: "", result_str)\n\nprint(""Learning finished!"")'"
RNN/season1_refactored/RNN_intro_4.py,7,"b'# Lab 12 RNN\nimport torch\nimport torch.nn as nn\n\ntorch.manual_seed(777)  # reproducibility\n\n\nidx2char = [\'h\', \'i\', \'e\', \'l\', \'o\']\n\n# Teach hihell -> ihello\nx_data = [[0, 1, 0, 2, 3, 3]]  # hihell\ny_data = [1, 0, 2, 3, 3, 4]    # ihello\n\n# As we have one batch of samples, we will change them to variables only once\ninputs = torch.LongTensor(x_data)\nlabels = torch.LongTensor(y_data)\n\nnum_classes = 5\ninput_size = 5\nembedding_size = 10  # embedding size\nhidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\nbatch_size = 1   # one sentence\nsequence_length = 6  # |ihello| == 6\nnum_layers = 1  # one-layer rnn\n\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        super(Model, self).__init__()\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.RNN(input_size=embedding_size,\n                          hidden_size=hidden_size,\n                          batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        # Initialize hidden and cell states\n        # (num_layers * num_directions, batch, hidden_size)\n        h_0 = torch.zeros(num_layers, x.size(0), hidden_size)\n\n        emb = self.embedding(x)\n        emb = emb.view(batch_size, sequence_length, -1)\n\n        # Propagate embedding through RNN\n        # Input: (batch, seq_len, embedding_size)\n        # h_0: (num_layers * num_directions, batch, hidden_size)\n        out, _ = self.rnn(emb, h_0)\n        return self.fc(out.view(-1, num_classes))\n\n\n# Instantiate RNN model\nmodel = Model()\nprint(model)\n\n# Set loss and optimizer function\n# CrossEntropyLoss = LogSoftmax + NLLLoss\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n# Train the model\nfor epoch in range(100):\n    outputs = model(inputs)\n    optimizer.zero_grad()\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n    _, idx = outputs.max(1)\n    idx = idx.data.numpy()\n    result_str = \'\'.join(idx2char[c] for c in idx.squeeze())\n    print(f\'epoch: {epoch + 1}, loss: {loss.item():1.3f}\')\n    print(""Predicted string: "", result_str)\n\nprint(""Learning finished!"")'"
