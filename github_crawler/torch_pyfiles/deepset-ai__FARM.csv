file_path,api_count,code
run_all_experiments.py,0,"b'# coding=utf-8\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Downstream runner for all experiments in specified config files.""""""\n\nfrom pathlib import Path\nfrom farm.experiment import run_experiment, load_experiments\n\n\ndef main():\n    config_files = [\n        Path(""experiments/ner/conll2003_de_config.json""),\n        Path(""experiments/ner/conll2003_en_config.json""),\n        Path(""experiments/ner/germEval14_config.json""),\n        Path(""experiments/text_classification/germEval18Fine_config.json""),\n        Path(""experiments/text_classification/germEval18Coarse_config.json""),\n        Path(""experiments/text_classification/gnad_config.json""),\n        Path(""experiments/text_classification/cola_config.json""),\n        Path(""experiments/qa/squad20_config.json""),\n    ]\n\n    for conf_file in config_files:\n        experiments = load_experiments(conf_file)\n        for experiment in experiments:\n            run_experiment(experiment)\n\nif __name__ == ""__main__"":\n    main()\n'"
setup.py,0,"b'from io import open\n\nfrom setuptools import find_packages, setup\n\nwith open(""requirements.txt"") as f:\n    parsed_requirements = f.read().splitlines()\n# remove blank lines and comments\nparsed_requirements = [\n    x.strip()\n    for x in parsed_requirements\n    if ((x.strip()[0] != ""#"") and (len(x.strip()) > 3) and ""-e git://"" not in x)\n]\n\n\nsetup(\n    name=""farm"",\n    version=""0.4.3"",\n    author=""Malte Pietsch, Timo Moeller, Branden Chan, Tanay Soni, Huggingface Team Authors, Google AI Language Team Authors, Open AI team Authors"",\n    author_email=""malte.pietsch@deepset.ai"",\n    description=""Toolkit for finetuning and evaluating transformer based language models"",\n    long_description=open(""readme.rst"", ""r"", encoding=""utf-8"").read(),\n    long_description_content_type=""text/x-rst"",\n    keywords=""BERT NLP deep learning language-model transformer qa question-answering transfer-learning"",\n    license=""Apache"",\n    url=""https://gitlab.com/deepset-ai/ml/lm/farm"",\n    download_url=""https://github.com/deepset-ai/FARM/archive/0.4.3.tar.gz"",\n    packages=find_packages(exclude=[""*.tests"", ""*.tests.*"", ""tests.*"", ""tests""]),\n    install_requires=parsed_requirements,\n    python_requires="">=3.5.0"",\n    extras_require={\n        ""fasttext"": [""fasttext==0.9.1""],\n        ""onnx"": [""onnxruntime""],\n    },\n    tests_require=[""pytest""],\n    classifiers=[\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Programming Language :: Python :: 3"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n    ],\n)\n'"
docs/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(""..""))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = ""FARM""\ncopyright = ""2019, deepset""\nauthor = ""deepset""\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [""sphinx.ext.autodoc"", ""sphinx.ext.viewcode""]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [""_build"", ""Thumbs.db"", "".DS_Store""]\n\nautodoc_member_order = ""bysource""\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""_static""]\n\n\nhtml_logo = ""img/logo.png""\n\nhtml_context = {""css_files"": [""_static/custom.css""]}\n\n# -- Add autodocs for __init__() methods -------------------------------------\n\n\ndef skip(app, what, name, obj, would_skip, options):\n    if name == ""__init__"":\n        return False\n    return would_skip\n\n\ndef setup(app):\n    app.connect(""autodoc-skip-member"", skip)\n'"
docs/qa_formats.py,0,"b'####################################\n###### JSON (REST API) FORMAT ######\n####################################\n\n# INPUT\n\ninput = [{""questions"": [""What is X?""], ""text"":  ""Some context containing the answer""}]\n\n# OUTPUT\n\noutput= {\n    ""task"": ""qa"",\n    ""predictions"": [\n        {\n            ""question"": question,\n            ""question_id"": id,\n            ""ground_truth"": None,\n            ""answers"": answers,\n            ""no_ans_gap"": no_ans_gap # Add no_ans_gap to current no_ans_boost for switching top prediction\n        }\n    ],\n}\n\nanswer =   {""score"": score,\n              ""probability"": -1,\n              ""answer"": string,\n              ""offset_answer_start"": ans_start_ch,\n              ""offset_answer_end"": ans_end_ch,\n              ""context"": context_string,\n              ""offset_context_start"": context_start_ch,\n              ""offset_context_end"": context_end_ch,\n              ""document_id"": document_id}\n\n\n###############################\n###### SQUAD EVAL FORMAT ######\n###############################\n\n# INPUT\n\ninput = [{""qas"": [""What is X?""], ""context"":  ""Some context containing the answer""}]\n\n# OUTPUT\n\noutput = {""id"": basket_id,\n          ""preds"": [[pred_str, start_t, end_t, score, sample_idx], ...]}\n'"
examples/conversion_huggingface_models.py,0,"b'from farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.infer import Inferencer\nimport pprint\nfrom transformers.pipelines import pipeline\nimport os\nfrom pathlib import Path\n\n##############################################\n###  From Transformers -> FARM\n##############################################\ndef convert_from_transformers():\n    # CASE 1: MODEL\n    # Load model from transformers model hub (-> continue training / compare models / ...)\n    model = AdaptiveModel.convert_from_transformers(""deepset/bert-large-uncased-whole-word-masking-squad2"", device=""cpu"", task_type=""question_answering"")\n    # ... continue as in the other examples e.g. to fine-tune this QA model on your own data\n\n    # CASE 2: INFERENCER\n    # Load Inferencer from transformers, incl. model & tokenizer (-> just get predictions)\n    nlp = Inferencer.load(""deepset/bert-large-uncased-whole-word-masking-squad2"", task_type=""question_answering"")\n\n    # run predictions\n    QA_input = [{""questions"": [""Why is model conversion important?""],\n                 ""text"": ""The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.""}]\n    result = nlp.inference_from_dicts(dicts=QA_input, rest_api_schema=True)\n    pprint.pprint(result)\n\n    # save it\n    farm_model_dir = Path(""../saved_models/bert-english-qa-large"")\n    nlp.save(farm_model_dir)\n\n##############################################\n###  From FARM -> Transformers\n##############################################\ndef convert_to_transformers():\n    farm_model_dir = Path(""../saved_models/bert-english-qa-large"")\n\n    # load from FARM format\n    model = AdaptiveModel.load(farm_model_dir, device=""cpu"")\n    tokenizer = Tokenizer.load(farm_model_dir)\n\n    # convert to transformers\n    transformer_model = model.convert_to_transformers()\n\n    # save it (Note: transformers uses strings rather than Path objects)\n    model_dir = ""../saved_models/bert-large-uncased-whole-word-masking-squad2""\n    os.makedirs(model_dir, exist_ok=True)\n    transformer_model.save_pretrained(model_dir)\n    tokenizer.save_pretrained(model_dir)\n\n    # run predictions (using transformers)\n    nlp = pipeline(\'question-answering\', model=model_dir, tokenizer=model_dir)\n    res = nlp({\n        \'question\': \'Why is model conversion important?\',\n        \'context\': \'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\'\n    })\n    pprint.pprint(res)\n\n    # To upload to transformer\'s model hub run this in bash:\n    # transformers-cli upload  ../saved_models/bert-large-uncased-whole-word-masking-squad2\n\n\nif __name__ == ""__main__"":\n    convert_from_transformers()\n    convert_to_transformers()'"
examples/conversion_huggingface_models_classification.py,0,"b'from farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.data_handler.processor import Processor\n\nfrom farm.infer import Inferencer\nimport pprint\nfrom transformers.pipelines import pipeline\nfrom pathlib import Path\n\n##############################################\n###  From Transformers -> FARM\n##############################################\ndef convert_from_transformers():\n    transformers_input_name = ""deepset/bert-base-german-cased-hatespeech-GermEval18Coarse""\n    farm_output_dir = Path(""../saved_models/farm-bert-base-german-cased-hatespeech-GermEval18Coarse"")\n\n    # # CASE 1: MODEL\n    # # Load model from transformers model hub (-> continue training / compare models / ...)\n    model = AdaptiveModel.convert_from_transformers(transformers_input_name, device=""cpu"", task_type=""text_classification"")\n    # # ... continue as in the other examples e.g. to fine-tune this QA model on your own data\n    #\n    # # CASE 2: INFERENCER\n    # # Load Inferencer from transformers, incl. model & tokenizer (-> just get predictions)\n    nlp = Inferencer.load(transformers_input_name, task_type=""text_classification"")\n    #\n    # # run predictions\n    result = nlp.inference_from_dicts(dicts=[{""text"": ""Was ein schei\xc3\x9f Nazi!""}], rest_api_schema=True)\n    pprint.pprint(result)\n\n    # save it\n    nlp.save(farm_output_dir)\n\n# ##############################################\n# ###  From FARM -> Transformers\n# ##############################################\ndef convert_to_transformers():\n    farm_input_dir = Path(""../saved_models/farm-bert-base-german-cased-german-GermEval18Coarse"")\n    transformers_output_dir = ""../saved_models/bert-base-german-cased-hatespeech-GermEval18Coarse""\n    #\n    # # # load from FARM format\n    model = AdaptiveModel.load(farm_input_dir, device=""cpu"")\n    processor = Processor.load_from_dir(farm_input_dir)\n    model.connect_heads_with_processor(processor.tasks)\n\n    # convert to transformers\n    transformer_model = model.convert_to_transformers()\n\n    # save it (note: transformers use str instead of Path objects)\n    Path(transformers_output_dir).mkdir(parents=True, exist_ok=True)\n    transformer_model.save_pretrained(transformers_output_dir)\n    processor.tokenizer.save_pretrained(transformers_output_dir)\n\n    # run predictions (using transformers)\n    nlp = pipeline(\'sentiment-analysis\', model=str(transformers_output_dir), tokenizer=str(transformers_output_dir))\n    res = nlp(""Was ein schei\xc3\x9f Nazi!"")\n    pprint.pprint(res)\n\n    # # To upload to transformer\'s model hub run this in bash:\n    # # transformers-cli upload  ../saved_models/bert-large-uncased-whole-word-masking-squad2\n\nif __name__ == ""__main__"":\n    convert_from_transformers()\n    convert_to_transformers()'"
examples/doc_classification.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\ndef doc_classifcation():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_doc_classification"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    n_epochs = 1\n    batch_size = 32\n    evaluate_every = 100\n    lang_model = ""bert-base-german-cased""\n    do_lower_case = False\n    # or a local path:\n    # lang_model = Path(""../saved_models/farm-bert-base-cased"")\n    use_amp = None\n\n    device, n_gpu = initialize_device_settings(use_cuda=True, use_amp=use_amp)\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(pretrained_model_name_or_path=lang_model, do_lower_case=do_lower_case)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    # Here we load GermEval 2018 Data automaticaly if it is not available.\n    # GermEval 2018 only has train.tsv and test.tsv dataset - no dev.tsv\n\n    label_list = [""OTHER"", ""OFFENSE""]\n    metric = ""f1_macro""\n\n    processor = TextClassificationProcessor(tokenizer=tokenizer,\n                                            max_seq_len=128,\n                                            data_dir=Path(""../data/germeval18""),\n                                            label_list=label_list,\n                                            metric=metric,\n                                            label_column_name=""coarse_label""\n                                            )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a\n    #    few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(lang_model)\n    # b) and a prediction head on top that is suited for our task => Text classification\n    prediction_head = TextClassificationHead(\n        class_weights=data_silo.calculate_class_weights(task_name=""text_classification""),\n        num_labels=len(label_list))\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=3e-5,\n        device=device,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs,\n        use_amp=use_amp)\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    # 7. Let it grow\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    save_dir = Path(""saved_models/bert-german-doc-tutorial"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    # 9. Load it & harvest your fruits (Inference)\n    basic_texts = [\n        {""text"": ""Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot sei""},\n        {""text"": ""Martin M\xc3\xbcller spielt Handball in Berlin""},\n    ]\n    model = Inferencer.load(save_dir)\n    result = model.inference_from_dicts(dicts=basic_texts)\n    print(result)\n\n\nif __name__ == ""__main__"":\n    doc_classifcation()\n\n# fmt: on\n'"
examples/doc_classification_cola.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\ndef doc_classification_cola():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_cola"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    n_epochs = 5\n    batch_size = 100\n    evaluate_every = 20\n    lang_model = ""bert-base-cased""\n    do_lower_case = False\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(pretrained_model_name_or_path=lang_model, do_lower_case=do_lower_case)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    # Here we load Cola 2018 Data automaticaly if it is not available.\n    # GermEval 2018 only has train.tsv and test.tsv dataset - no dev.tsv\n\n    label_list = [""0"", ""1""]\n    metric = ""mcc""\n\n    processor = TextClassificationProcessor(tokenizer=tokenizer,\n                                            max_seq_len=64,\n                                            data_dir=Path(""../data/cola""),\n                                            dev_filename=Path(""dev.tsv""),\n                                            dev_split=None,\n                                            test_filename=None,\n                                            label_list=label_list,\n                                            metric=metric,\n                                            label_column_name=""label""\n                                            )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(lang_model)\n\n    # language_model = Roberta.load(lang_model)\n    # b) and a prediction head on top that is suited for our task => Text classification\n    prediction_head = TextClassificationHead(\n        num_labels=len(label_list),\n        class_weights=data_silo.calculate_class_weights(task_name=""text_classification""))\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        device=device,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs)\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    # 7. Let it grow\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    save_dir = Path(""saved_models/bert-doc-tutorial"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    # 9. Load it & harvest your fruits (Inference)\n    basic_texts = [\n        {""text"": ""The box contained the ball from the tree.""},\n        {""text"": ""I\'ll fix you a drink.""},\n    ]\n    model = Inferencer.load(save_dir)\n    result = model.inference_from_dicts(dicts=basic_texts)\n    print(result)\n\n\nif __name__ == ""__main__"":\n    doc_classification_cola()\n\n# fmt: on\n'"
examples/doc_classification_crossvalidation.py,0,"b'# fmt: off\nimport logging\nimport json\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo, DataSiloForCrossVal\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer, EarlyStopping\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\nfrom farm.eval import Evaluator\nfrom sklearn.metrics import matthews_corrcoef, f1_score\nfrom farm.evaluation.metrics import simple_accuracy, register_metrics\n\ndef doc_classification_crossvalidation():\n    ##########################\n    ########## Logging\n    ##########################\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n    # reduce verbosity from transformers library\n    logging.getLogger(\'transformers\').setLevel(logging.WARNING)\n\n    # ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    # for local logging instead:\n    ml_logger = MLFlowLogger(tracking_uri=""logs"")\n    # ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""DocClassification_ES_f1_1"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    xval_folds = 5\n    xval_stratified = True\n\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    n_epochs = 20\n    batch_size = 32\n    evaluate_every = 100\n    lang_model = ""bert-base-german-cased""\n    do_lower_case = False\n    use_amp = None\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=do_lower_case)\n\n    # The evaluation on the dev-set can be done with one of the predefined metrics or with a\n    # metric defined as a function from (preds, labels) to a dict that contains all the actual\n    # metrics values. The function must get registered under a string name and the string name must\n    # be used.\n    # For xval, we also store the actual predictions and labels in each result so we can\n    # calculate overall metrics over all folds later\n    def mymetrics(preds, labels):\n        acc = simple_accuracy(preds, labels).get(""acc"")\n        f1other = f1_score(y_true=labels, y_pred=preds, pos_label=""OTHER"")\n        f1offense = f1_score(y_true=labels, y_pred=preds, pos_label=""OFFENSE"")\n        f1macro = f1_score(y_true=labels, y_pred=preds, average=""macro"")\n        f1micro = f1_score(y_true=labels, y_pred=preds, average=""macro"")\n        mcc = matthews_corrcoef(labels, preds)\n        return {\n            ""acc"": acc,\n            ""f1_other"": f1other,\n            ""f1_offense"": f1offense,\n            ""f1_macro"": f1macro,\n            ""f1_micro"": f1micro,\n            ""mcc"": mcc\n        }\n    register_metrics(\'mymetrics\', mymetrics)\n    metric = \'mymetrics\'\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    # Here we load GermEval 2018 Data automaticaly if it is not available.\n    # GermEval 2018 only has train.tsv and test.tsv dataset - no dev.tsv\n\n    # The processor wants to know the possible labels ...\n    label_list = [""OTHER"", ""OFFENSE""]\n    processor = TextClassificationProcessor(tokenizer=tokenizer,\n                                            max_seq_len=64,\n                                            data_dir=Path(""../data/germeval18""),\n                                            label_list=label_list,\n                                            metric=metric,\n                                            label_column_name=""coarse_label""\n                                            )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # Load one silo for each fold in our cross-validation\n    silos = DataSiloForCrossVal.make(data_silo, n_splits=xval_folds)\n\n    # the following steps should be run for each of the folds of the cross validation, so we put them\n    # into a function\n    def train_on_split(silo_to_use, n_fold, save_dir):\n        logger.info(f""############ Crossvalidation: Fold {n_fold} ############"")\n        # Create an AdaptiveModel\n        # a) which consists of a pretrained language model as a basis\n        language_model = LanguageModel.load(lang_model)\n        # b) and a prediction head on top that is suited for our task => Text classification\n        prediction_head = TextClassificationHead(\n            class_weights=data_silo.calculate_class_weights(task_name=""text_classification""),\n            num_labels=len(label_list))\n\n        model = AdaptiveModel(\n            language_model=language_model,\n            prediction_heads=[prediction_head],\n            embeds_dropout_prob=0.2,\n            lm_output_types=[""per_sequence""],\n            device=device)\n\n        # Create an optimizer\n        model, optimizer, lr_schedule = initialize_optimizer(\n            model=model,\n            learning_rate=0.5e-5,\n            device=device,\n            n_batches=len(silo_to_use.loaders[""train""]),\n            n_epochs=n_epochs,\n            use_amp=use_amp)\n\n        # Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n        # Also create an EarlyStopping instance and pass it on to the trainer\n\n        # An early stopping instance can be used to save the model that performs best on the dev set\n        # according to some metric and stop training when no improvement is happening for some iterations.\n        # NOTE: Using a different save directory for each fold, allows us afterwards to use the\n        # nfolds best models in an ensemble!\n        save_dir = Path(str(save_dir) + f""-{n_fold}"")\n        earlystopping = EarlyStopping(\n            metric=""f1_offense"", mode=""max"",   # use the metric from our own metrics function instead of loss\n            save_dir=save_dir,  # where to save the best model\n            patience=5    # number of evaluations to wait for improvement before terminating the training\n        )\n\n        trainer = Trainer(\n            model=model,\n            optimizer=optimizer,\n            data_silo=silo_to_use,\n            epochs=n_epochs,\n            n_gpu=n_gpu,\n            lr_schedule=lr_schedule,\n            evaluate_every=evaluate_every,\n            device=device,\n            early_stopping=earlystopping,\n            evaluator_test=False)\n\n        # train it\n        trainer.train()\n\n        return trainer.model\n\n    # for each fold, run the whole training, earlystopping to get a model, then evaluate the model\n    # on the test set of each fold\n    # Remember all the results for overall metrics over all predictions of all folds and for averaging\n    allresults = []\n    all_preds = []\n    all_labels = []\n    bestfold = None\n    bestf1_offense = -1\n    save_dir = Path(""saved_models/bert-german-doc-tutorial-es"")\n    for num_fold, silo in enumerate(silos):\n        model = train_on_split(silo, num_fold, save_dir)\n\n        # do eval on test set here (and not in Trainer),\n        #  so that we can easily store the actual preds and labels for a ""global"" eval across all folds.\n        evaluator_test = Evaluator(\n            data_loader=silo.get_data_loader(""test""),\n            tasks=silo.processor.tasks,\n            device=device\n        )\n        result = evaluator_test.eval(model, return_preds_and_labels=True)\n        evaluator_test.log_results(result, ""Test"", steps=len(silo.get_data_loader(""test"")), num_fold=num_fold)\n\n        allresults.append(result)\n        all_preds.extend(result[0].get(""preds""))\n        all_labels.extend(result[0].get(""labels""))\n\n        # keep track of best fold\n        f1_offense = result[0][""f1_offense""]\n        if f1_offense > bestf1_offense:\n            bestf1_offense = f1_offense\n            bestfold = num_fold\n\n    # Save the per-fold results to json for a separate, more detailed analysis\n    with open(""doc_classification_xval.results.json"", ""wt"") as fp:\n        json.dump(allresults, fp)\n\n    # calculate overall metrics across all folds\n    xval_f1_micro = f1_score(all_labels, all_preds, labels=label_list, average=""micro"")\n    xval_f1_macro = f1_score(all_labels, all_preds, labels=label_list, average=""macro"")\n    xval_f1_offense = f1_score(all_labels, all_preds, labels=label_list, pos_label=""OFFENSE"")\n    xval_f1_other = f1_score(all_labels, all_preds, labels=label_list, pos_label=""OTHER"")\n    xval_mcc = matthews_corrcoef(all_labels, all_preds)\n\n    logger.info(""XVAL F1 MICRO:   "", xval_f1_micro)\n    logger.info(""XVAL F1 MACRO:   "", xval_f1_macro)\n    logger.info(""XVAL F1 OFFENSE: "", xval_f1_offense)\n    logger.info(""XVAL F1 OTHER:   "", xval_f1_other)\n    logger.info(""XVAL MCC:        "", xval_mcc)\n\n    # -----------------------------------------------------\n    # Just for illustration, use the best model from the best xval val for evaluation on\n    # the original (still unseen) test set.\n    logger.info(""###### Final Eval on hold out test set using best model #####"")\n    evaluator_origtest = Evaluator(\n        data_loader=data_silo.get_data_loader(""test""),\n        tasks=data_silo.processor.tasks,\n        device=device\n    )\n    # restore model from the best fold\n    lm_name = model.language_model.name\n    save_dir = Path(f""saved_models/bert-german-doc-tutorial-es-{bestfold}"")\n    model = AdaptiveModel.load(save_dir, device, lm_name=lm_name)\n    model.connect_heads_with_processor(data_silo.processor.tasks, require_labels=True)\n\n    result = evaluator_origtest.eval(model)\n    logger.info(""TEST F1 MICRO:   "", result[0][""f1_micro""])\n    logger.info(""TEST F1 MACRO:   "", result[0][""f1_macro""])\n    logger.info(""TEST F1 OFFENSE: "", result[0][""f1_offense""])\n    logger.info(""TEST F1 OTHER:   "", result[0][""f1_other""])\n    logger.info(""TEST MCC:        "", result[0][""mcc""])\n\n\nif __name__ == ""__main__"":\n    doc_classification_crossvalidation()\n\n# fmt: on\n'"
examples/doc_classification_custom_optimizer.py,2,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\ndef doc_classifcation():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_doc_classification"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    n_epochs = 1\n    batch_size = 32\n    evaluate_every = 100\n    lang_model = ""bert-base-german-cased""\n    do_lower_case = False\n    # or a local path:\n    # lang_model = Path(""../saved_models/farm-bert-base-cased"")\n    use_amp = None\n\n    #############################################\n    # CUSTOM OPTIMIZER & LR SCHEDULE\n    #############################################\n    # learning rate schedules from transformers\n    schedule_opts = {""name"": ""LinearWarmup"", ""warmup_proportion"": 0.4}\n    # schedule_opts = {""name"": ""Constant""}\n    # schedule_opts = {""name"": ""CosineWarmup"", ""warmup_proportion"": 0.4}\n    # schedule_opts = {""name"": ""CosineWarmupWithRestarts"", ""warmup_proportion"": 0.4}\n\n    # or from native pytorch (see https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html for all options)\n    # schedule_opts = {""name"": ""StepLR"", ""step_size"": 30, ""gamma"": 0.1}\n    # schedule_opts = {""name"": ""ReduceLROnPlateau"", ""mode"": \'min\', ""factor"": 0.1, ""patience"":10}\n\n    # optimizers from pytorch (see https://pytorch.org/docs/stable/optim.html for all options)\n    optimizer_opts = {""name"": ""SGD"", ""momentum"": 0.0}\n\n    # or from apex (see https://github.com/NVIDIA/apex/tree/master/apex/optimizers for all options)\n    # optimizer_opts = {""name"": ""FusedLAMB"", ""bias_correction"": True}\n\n    # or from transformers (default in FARM)\n    #optimizer_opts = {""name"": ""TransformersAdamW"", ""correct_bias"": False, ""weight_decay"": 0.01}\n    #############################################\n\n\n    device, n_gpu = initialize_device_settings(use_cuda=True, use_amp=use_amp)\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=do_lower_case)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    # Here we load GermEval 2018 Data automaticaly if it is not available.\n    # GermEval 2018 only has train.tsv and test.tsv dataset - no dev.tsv\n\n    label_list = [""OTHER"", ""OFFENSE""]\n    metric = ""f1_macro""\n\n    processor = TextClassificationProcessor(tokenizer=tokenizer,\n                                            max_seq_len=128,\n                                            data_dir=Path(""../data/germeval18""),\n                                            label_list=label_list,\n                                            metric=metric,\n                                            label_column_name=""coarse_label""\n                                            )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a\n    #    few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(lang_model)\n    # b) and a prediction head on top that is suited for our task => Text classification\n    prediction_head = TextClassificationHead(\n        class_weights=data_silo.calculate_class_weights(task_name=""text_classification""),\n        num_labels=len(label_list))\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=5e-3,\n        optimizer_opts=optimizer_opts,\n        schedule_opts=schedule_opts,\n        device=device,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs,\n        use_amp=use_amp)\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    # 7. Let it grow\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    save_dir = Path(""saved_models/bert-german-doc-tutorial"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    # 9. Load it & harvest your fruits (Inference)\n    basic_texts = [\n        {""text"": ""Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot sei""},\n        {""text"": ""Martin M\xc3\xbcller spielt Handball in Berlin""},\n    ]\n    model = Inferencer.load(save_dir)\n    result = model.inference_from_dicts(dicts=basic_texts)\n    print(result)\n\n\nif __name__ == ""__main__"":\n    doc_classifcation()\n\n# fmt: on\n'"
examples/doc_classification_fasttext_LM.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo, StreamingDataSilo\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\nfrom farm.modeling.wordembedding_utils import Fasttext_converter\n\ndef doc_classifcation():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_doc_classification_fasttext"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    n_epochs = 3\n    batch_size = 32\n    evaluate_every = 100\n    # load fasttext from a local path:\n    #fasttext_model = ""../saved_models/fasttext-german-uncased""\n    # or through s3\n    fasttext_model = ""fasttext-german-uncased""\n    do_lower_case = True\n    max_features = 10_000 # maximum number of unique words we will transform\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n\n\n    # 1. To make Fasttext work within FARM and with advanced aggregation strategies, we need a fixed vocabulary and associated Wordembeddings\n    ft_converter = Fasttext_converter(\n        pretrained_model_name_or_path=fasttext_model,\n        do_lower_case=do_lower_case,\n        data_path=Path(""../data/germeval18""),\n        train_filename=""train.tsv"",\n        output_path=Path(""../saved_models/fasttext-german-uncased-converted""),\n        language=""German"",\n        max_features=max_features)\n    # We convert the data to have fixed size vocab and embeddings\n    vocab_counts = ft_converter.convert_on_data()\n\n    # 2. Create a tokenizer\n    tokenizer = Tokenizer.load(pretrained_model_name_or_path=ft_converter.output_path, do_lower_case=do_lower_case)\n\n    # 3. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    # Here we load GermEval 2018 Data automaticaly if it is not available.\n    # GermEval 2018 only has train.tsv and test.tsv dataset - no dev.tsv\n    label_list = [""OTHER"", ""OFFENSE""]\n    metric = ""f1_macro""\n\n    processor = TextClassificationProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=128,\n        data_dir=ft_converter.data_path,\n        label_list=label_list,\n        train_filename=ft_converter.train_filename,\n        dev_split=0,\n        test_filename=""test.tsv"",\n        metric=metric,\n        label_column_name=""coarse_label""\n        )\n\n    # 4. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a\n    #    few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size,\n        max_processes=1) # multiprocessing with WordembeddingTokenizer is not optimal - so disable it\n\n    # 5. Create an AdaptiveModel\n    # a) which consists of the newly created embedding model as a basis.\n    language_model = LanguageModel.load(ft_converter.output_path)\n    # b) and a prediction head on top that is suited for our task => Text classification\n    # Since we do not have a powerful Transformer based Language Model, we need a slightly deeper NN\n    # for going the Classification\n    prediction_head = TextClassificationHead(\n        layer_dims=[300,600,len(label_list)],\n        class_weights=data_silo.calculate_class_weights(task_name=""text_classification""),\n        num_labels=len(label_list))\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n    # 6. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=3e-3,\n        device=device,\n        n_batches=len(data_silo.get_data_loader(""train"")),  #len(data_silo.loaders[""train""]),streaming: len(data_silo.get_data_loader(""train""))\n        n_epochs=n_epochs)\n\n    # 7. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    # 8. Let it grow\n    trainer.train()\n\n\nif __name__ == ""__main__"":\n    doc_classifcation()\n\n# fmt: on\n'"
examples/doc_classification_multilabel.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import MultiLabelTextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\ndef doc_classification_multilabel():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_doc_classification"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    n_epochs = 1\n    batch_size = 32\n\n    evaluate_every = 500\n    lang_model = ""bert-base-uncased""\n    do_lower_case = True\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=do_lower_case)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    # Here we load Toxic Comments Data automaticaly if it is not available.\n\n    label_list = [""toxic"",""severe_toxic"",""obscene"",""threat"",""insult"",""identity_hate""]\n    metric = ""acc""\n\n    processor = TextClassificationProcessor(tokenizer=tokenizer,\n                                            max_seq_len=128,\n                                            data_dir=Path(""../data/toxic-comments""),\n                                            label_list=label_list,\n                                            label_column_name=""label"",\n                                            metric=metric,\n                                            quote_char=\'""\',\n                                            multilabel=True,\n                                            train_filename=""train.tsv"",\n                                            dev_filename=""val.tsv"",\n                                            test_filename=None,\n                                            dev_split=0,\n                                            )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(lang_model)\n    # b) and a prediction head on top that is suited for our task => Text classification\n    prediction_head = MultiLabelTextClassificationHead(num_labels=len(label_list))\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=3e-5,\n        device=device,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs)\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    # 7. Let it grow\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    save_dir = Path(""../saved_models/bert-german-multi-doc-tutorial"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    # 9. Load it & harvest your fruits (Inference)\n    basic_texts = [\n        {""text"": ""You fucking bastards""},\n        {""text"": ""What a lovely world""},\n    ]\n    model = Inferencer.load(save_dir)\n    result = model.inference_from_dicts(dicts=basic_texts)\n    print(result)\n\n\nif __name__ == ""__main__"":\n    doc_classification_multilabel()\n\n# fmt: on\n'"
examples/doc_classification_multilabel_roberta.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import Roberta\nfrom farm.modeling.prediction_head import MultiLabelTextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\n\ndef doc_classification_multilabel_roberta():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_doc_classification"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=False)\n    n_epochs = 1\n    batch_size = 32\n\n    evaluate_every = 500\n    lang_model = ""roberta-base""\n    do_lower_case = False # roberta is a cased model\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model, do_lower_case=do_lower_case\n    )\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    # Here we load Toxic Comments Data automaticaly if it is not available.\n\n    label_list = [""toxic"",""severe_toxic"",""obscene"",""threat"",""insult"",""identity_hate""]\n    metric = ""acc""\n\n    processor = TextClassificationProcessor(tokenizer=tokenizer,\n                                            max_seq_len=128,\n                                            data_dir=Path(""../data/toxic-comments""),\n                                            label_list=label_list,\n                                            label_column_name=""label"",\n                                            metric=metric,\n                                            quote_char=\'""\',\n                                            multilabel=True,\n                                            train_filename=Path(""train.tsv""),\n                                            dev_filename=Path(""val.tsv""),\n                                            test_filename=None,\n                                            dev_split=0,\n                                            max_samples=1000\n                                            )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = Roberta.load(lang_model)\n    # b) and a prediction head on top that is suited for our task => Text classification\n    prediction_head = MultiLabelTextClassificationHead(num_labels=len(label_list))\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=3e-5,\n        device=device,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs)\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    # 7. Let it grow\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    save_dir = Path(""saved_models/bert-multi-doc-roberta"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    # 9. Load it & harvest your fruits (Inference)\n    basic_texts = [\n        {""text"": ""You fucking bastards""},\n        {""text"": ""What a lovely world""},\n    ]\n    model = Inferencer.load(save_dir)\n    result = model.run_inference(dicts=basic_texts)\n    print(result)\n\n\nif __name__ == ""__main__"":\n    doc_classification_multilabel_roberta()\n\n# fmt: on\n'"
examples/doc_classification_with_earlystopping.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer, EarlyStopping\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\nfrom sklearn.metrics import f1_score\nfrom farm.evaluation.metrics import simple_accuracy, register_metrics\n\ndef doc_classification_with_earlystopping():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    # for local logging instead:\n    # ml_logger = MLFlowLogger(tracking_uri=""logs"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""DocClassification_ES_f1_1"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    use_amp = None\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    n_epochs = 20\n    batch_size = 32\n    evaluate_every = 100\n    lang_model = ""bert-base-german-cased""\n    do_lower_case = False\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=do_lower_case)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    # Here we load GermEval 2018 Data automaticaly if it is not available.\n    # GermEval 2018 only has train.tsv and test.tsv dataset - no dev.tsv\n\n    # The processor wants to know the possible labels ...\n    label_list = [""OTHER"", ""OFFENSE""]\n\n    # The evaluation on the dev-set can be done with one of the predefined metrics or with a\n    # metric defined as a function from (preds, labels) to a dict that contains all the actual\n    # metrics values. The function must get registered under a string name and the string name must\n    # be used.\n    def mymetrics(preds, labels):\n        acc = simple_accuracy(preds, labels)\n        f1other = f1_score(y_true=labels, y_pred=preds, pos_label=""OTHER"")\n        f1offense = f1_score(y_true=labels, y_pred=preds, pos_label=""OFFENSE"")\n        f1macro = f1_score(y_true=labels, y_pred=preds, average=""macro"")\n        f1micro = f1_score(y_true=labels, y_pred=preds, average=""macro"")\n        return {""acc"": acc, ""f1_other"": f1other, ""f1_offense"": f1offense, ""f1_macro"": f1macro, ""f1_micro"": f1micro}\n    register_metrics(\'mymetrics\', mymetrics)\n    metric = \'mymetrics\'\n\n    processor = TextClassificationProcessor(tokenizer=tokenizer,\n                                            max_seq_len=64,\n                                            data_dir=Path(""../data/germeval18""),\n                                            label_list=label_list,\n                                            metric=metric,\n                                            label_column_name=""coarse_label""\n                                            )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(lang_model)\n    # b) and a prediction head on top that is suited for our task => Text classification\n    prediction_head = TextClassificationHead(num_labels=len(label_list),\n                                             class_weights=data_silo.calculate_class_weights(task_name=""text_classification""))\n\n\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.2,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=0.5e-5,\n        device=device,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs,\n        use_amp=use_amp)\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    # Also create an EarlyStopping instance and pass it on to the trainer\n\n    # An early stopping instance can be used to save the model that performs best on the dev set\n    # according to some metric and stop training when no improvement is happening for some iterations.\n    earlystopping = EarlyStopping(\n        metric=""f1_offense"", mode=""max"",   # use the metric from our own metrics function instead of loss\n        # metric=""f1_macro"", mode=""max"",  # use f1_macro from the dev evaluator of the trainer\n        # metric=""loss"", mode=""min"",   # use loss from the dev evaluator of the trainer\n        save_dir=Path(""saved_models/bert-german-doc-tutorial-es""),  # where to save the best model\n        patience=5    # number of evaluations to wait for improvement before terminating the training\n    )\n\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device,\n        early_stopping=earlystopping)\n\n    # 7. Let it grow\n    trainer.train()\n\n    # 8. Hooray! You have a model.\n    # NOTE: if early stopping is used, the best model has been stored already in the directory\n    # defined with the EarlyStopping instance\n    # The model we have at this moment is the model from the last training epoch that was carried\n    # out before early stopping terminated the training\n    save_dir = Path(""saved_models/bert-german-doc-tutorial"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    # 9. Load it & harvest your fruits (Inference)\n    basic_texts = [\n        {""text"": ""Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot sei""},\n        {""text"": ""Martin M\xc3\xbcller spielt Handball in Berlin""},\n    ]\n\n    # Load from the final epoch directory and apply\n    print(""LOADING INFERENCER FROM FINAL MODEL DURING TRAINING"")\n    model = Inferencer.load(save_dir)\n    result = model.inference_from_dicts(dicts=basic_texts)\n    print(result)\n\n    # Load from saved best model\n    print(""LOADING INFERENCER FROM BEST MODEL DURING TRAINING"")\n    model = Inferencer.load(earlystopping.save_dir)\n    result = model.inference_from_dicts(dicts=basic_texts)\n    print(""APPLICATION ON BEST MODEL"")\n    print(result)\n\n\nif __name__ == ""__main__"":\n    doc_classification_with_earlystopping()\n\n# fmt: on\n'"
examples/doc_classification_word_embedding_LM.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\nimport time\n\nfrom farm.data_handler.data_silo import DataSilo, StreamingDataSilo\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\ndef doc_classifcation():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_doc_classification_glove"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    n_epochs = 3\n    batch_size = 32\n    evaluate_every = 100\n    # load from a local path:\n    lang_model = Path(""../saved_models/glove-german-uncased"")\n    # or through s3\n    #lang_model = ""glove-german-uncased""\n    do_lower_case = True\n\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(pretrained_model_name_or_path=lang_model, do_lower_case=do_lower_case)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    # Here we load GermEval 2018 Data automaticaly if it is not available.\n    # GermEval 2018 only has train.tsv and test.tsv dataset - no dev.tsv\n    label_list = [""OTHER"", ""OFFENSE""]\n    metric = ""f1_macro""\n\n    processor = TextClassificationProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=128,\n        data_dir=Path(""../data/germeval18""),\n        label_list=label_list,\n        dev_split=0,\n        test_filename=""test.tsv"",\n        train_filename=""train.tsv"",\n        metric=metric,\n        label_column_name=""coarse_label"")\n\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size,\n        max_processes=1)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of an embedding model as a basis.\n    # Word embedding models only converts words it has seen during training to embedding vectors.\n    language_model = LanguageModel.load(lang_model)\n    # b) and a prediction head on top that is suited for our task => Text classification\n    prediction_head = TextClassificationHead(\n        layer_dims=[300,600,len(label_list)],\n        class_weights=data_silo.calculate_class_weights(task_name=""text_classification""),\n        num_labels=len(label_list))\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=3e-5,\n        device=device,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs)\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    # 7. Let it grow\n    trainer.train()\n\n\nif __name__ == ""__main__"":\n    doc_classifcation()\n\n# fmt: on\n'"
examples/doc_regression.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import RegressionProcessor\nfrom farm.experiment import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import RegressionHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\n\ndef doc_regression():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_doc_regression"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    n_epochs = 5\n    batch_size = 32\n    evaluate_every = 30\n    lang_model = ""bert-base-cased""\n    do_lower_case = False\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=do_lower_case)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    #    We do not have a sample dataset for regression yet, add your own dataset to run the example\n    processor = RegressionProcessor(tokenizer=tokenizer,\n                                    max_seq_len=128,\n                                    data_dir=Path(""../data/<YOUR-DATASET>""),\n                                    label_column_name=""label""\n                                    )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(lang_model)\n    # b) and a prediction head on top that is suited for our task => Text regression\n    prediction_head = RegressionHead()\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence_continuous""],\n        device=device)\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        device=device,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs)\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    # 7. Let it grow\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    save_dir = Path(""saved_models/bert-doc-regression-tutorial"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    # 9. Load it & harvest your fruits (Inference)\n    #    Add your own text adapted to the dataset you provide\n    basic_texts = [\n        {""text"": """"},\n        {""text"": """"},\n    ]\n    model = Inferencer.load(save_dir)\n    result = model.inference_from_dicts(dicts=basic_texts)\n\n    print(result)\n\n\nif __name__ == ""__main__"":\n    doc_regression()\n\n# fmt: on\n'"
examples/embeddings_extraction.py,0,"b'from farm.infer import Inferencer\nfrom farm.utils import set_all_seeds\nfrom pathlib import Path\n\ndef embeddings_extraction():\n    set_all_seeds(seed=42)\n    batch_size = 32\n    use_gpu = False\n    lang_model = ""bert-base-german-cased""\n    # or local path:\n    # lang_model = Path(""../saved_models/farm-bert-base-cased-squad2"")\n\n    # Input\n    basic_texts = [\n        {""text"": ""Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot ist""},\n        {""text"": ""Martin M\xc3\xbcller spielt Fussball""},\n    ]\n\n    # Load model, tokenizer and processor directly into Inferencer\n    model = Inferencer.load(lang_model, task_type=""embeddings"", gpu=use_gpu, batch_size=batch_size,\n                            extraction_strategy=""reduce_mean"", extraction_layer=-2, num_processes=0)\n\n    # Get embeddings for input text (you can vary the strategy and layer)\n    result = model.inference_from_dicts(dicts=basic_texts)\n    print(result)\n\nif __name__ == ""__main__"":\n    embeddings_extraction()\n'"
examples/embeddings_extraction_s3e_pooling.py,0,"b'import logging\nimport pickle\nfrom pathlib import Path\n\nfrom farm.data_handler.processor import InferenceProcessor\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.utils import set_all_seeds, initialize_device_settings\nfrom farm.modeling.wordembedding_utils import fit_s3e_on_corpus\n\nlogger = logging.getLogger(__name__)\n\n""""""\n    Example for generating sentence embeddings via the S3E pooling approach as described by Wang et al in the paper\n    ""Efficient Sentence Embedding via Semantic Subspace Analysis""\n    (https://arxiv.org/abs/2002.09620)\n    \n    You can use classical models like fasttext, glove or word2vec and apply S3E on top. \n    This can be a powerful benchmark for plain transformer-based embeddings.   \n\n    First, we fit the required stats on a custom corpus. This includes the derivation of token_weights depending on\n    token occurences in the corpus, creation of the semantic clusters via k-means and a couple of\n    pre-/post-processing steps to normalize the embeddings.\n    \n    Second, we feed the resulting objects into our Inferencer to extract the actual sentence embeddings for our sentences. \n""""""\n\ndef fit(language_model, corpus_path, save_dir, do_lower_case, batch_size=4, use_gpu=False):\n    # Fit S3E on a corpus\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=use_gpu, use_amp=False)\n\n    # Create a InferenceProcessor\n    tokenizer = Tokenizer.load(pretrained_model_name_or_path=language_model, do_lower_case=do_lower_case)\n    processor = InferenceProcessor(tokenizer=tokenizer, max_seq_len=128)\n\n    # Create an AdaptiveModel\n    language_model = LanguageModel.load(language_model)\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n    model, processor, s3e_stats = fit_s3e_on_corpus(processor=processor,\n                                                    model=model,\n                                                    corpus=corpus_path,\n                                                    n_clusters=10,\n                                                    pca_n_components=300,\n                                                    svd_postprocessing=True,\n                                                    min_token_occurrences=1)\n\n    # save everything to allow inference without fitting everything again\n    model.save(save_dir)\n    processor.save(save_dir)\n    with open(save_dir / ""s3e_stats.pkl"", ""wb"") as f:\n        pickle.dump(s3e_stats, f)\n\n    # Load model, tokenizer and processor directly into Inferencer\n    inferencer = Inferencer(model=model, processor=processor, task_type=""embeddings"", gpu=use_gpu,\n                       batch_size=batch_size, extraction_strategy=""s3e"", extraction_layer=-1,\n                       s3e_stats=s3e_stats)\n\n    # Input\n    basic_texts = [\n        {""text"": ""a man is walking on the street.""},\n        {""text"": ""a woman is walking on the street.""},\n    ]\n\n    # Get embeddings for input text (you can vary the strategy and layer)\n    result = inferencer.inference_from_dicts(dicts=basic_texts)\n    print(result)\n\n\ndef extract_embeddings(load_dir, use_gpu, batch_size):\n    with open(load_dir / ""s3e_stats.pkl"", ""rb"") as f:\n        s3e_stats = pickle.load(f)\n\n    # Init inferencer\n    inferencer = Inferencer.load(model_name_or_path=load_dir, task_type=""embeddings"", gpu=use_gpu,\n                       batch_size=batch_size, extraction_strategy=""s3e"", extraction_layer=-1,\n                       s3e_stats=s3e_stats)\n\n    # Input\n    basic_texts = [\n        {""text"": ""a man is walking on the street.""},\n        {""text"": ""a woman is walking on the street.""},\n    ]\n\n    # Get embeddings for input text\n    result = inferencer.inference_from_dicts(dicts=basic_texts)\n    print(result)\n\n\nif __name__ == ""__main__"":\n    lang_model = ""glove-english-uncased-6B""\n    do_lower_case = True\n    \n    # You can download this from:\n    # ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/lm_finetune_nips.tar.gz""\n    corpus_path = Path(""../data/lm_finetune_nips/train.txt"")\n\n    s3e_dir = Path(""../saved_models/fitted_s3e/"")\n\n    fit(language_model=lang_model,\n        do_lower_case=do_lower_case,\n        corpus_path=corpus_path,\n        save_dir=s3e_dir\n        )\n\n    extract_embeddings(load_dir=s3e_dir, use_gpu=False, batch_size=10)'"
examples/evaluation.py,0,"b'from farm.utils import initialize_device_settings\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.data_handler.processor import TextClassificationProcessor, SquadProcessor\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.eval import Evaluator\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom pathlib import Path\n\ndef evaluate_classification():\n    ##########################\n    ########## Settings\n    ##########################\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    lang_model = ""deepset/bert-base-german-cased-sentiment-Germeval17""\n    do_lower_case = False\n    batch_size = 100\n\n    data_dir = Path(""../data/germeval17"")\n    evaluation_filename = ""test_TIMESTAMP1.tsv""\n    label_list = [""negative"", ""neutral"", ""positive""]\n    metric = ""f1_macro""\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=do_lower_case)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    # Here we load GermEval 2017 Data automaticaly if it is not available.\n\n    processor = TextClassificationProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=384,\n        label_list=label_list,\n        metric=metric,\n        train_filename=None,\n        dev_filename=None,\n        dev_split=0,\n        test_filename=evaluation_filename,\n        data_dir=data_dir,\n    )\n\n    # 3. Create a DataSilo that loads dataset, provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # 4. Create an Evaluator\n    evaluator = Evaluator(\n        data_loader=data_silo.get_data_loader(""test""),\n        tasks=data_silo.processor.tasks,\n        device=device\n    )\n\n    # 5. Load model\n    model = AdaptiveModel.convert_from_transformers(lang_model, device=device, task_type=""text_classification"")\n    # use ""load"" if you want to use a local model that was trained with FARM\n    # model = AdaptiveModel.load(lang_model, device=device)\n    model.connect_heads_with_processor(data_silo.processor.tasks, require_labels=True)\n\n    # 6. Run the Evaluator\n    results = evaluator.eval(model)\n    f1_score = results[0][""f1_macro""]\n    print(""Macro-averaged F1-Score:"", f1_score)\n\n\ndef evaluate_question_answering():\n    ##########################\n    ########## Settings\n    ##########################\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    lang_model = ""deepset/roberta-base-squad2""\n    do_lower_case = True\n\n    data_dir = Path(""../data/squad20"")\n    evaluation_filename = ""dev-v2.0.json""\n\n    batch_size = 50\n    no_ans_boost = 0\n    recall_at = 3 # recall at n is only useful for answers inside long documents\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=do_lower_case)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    processor = SquadProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=256,\n        label_list= [""start_token"", ""end_token""],\n        metric=""squad"",\n        train_filename=None,\n        dev_filename=None,\n        dev_split=0,\n        test_filename=evaluation_filename,\n        data_dir=data_dir,\n        doc_stride=128,\n    )\n\n    # 3. Create a DataSilo that loads dataset, provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # 4. Create an Evaluator\n    evaluator = Evaluator(\n        data_loader=data_silo.get_data_loader(""test""),\n        tasks=data_silo.processor.tasks,\n        device=device\n    )\n\n    # 5. Load model\n    model = AdaptiveModel.convert_from_transformers(lang_model, device=device, task_type=""question_answering"")\n    # use ""load"" if you want to use a local model that was trained with FARM\n    #model = AdaptiveModel.load(lang_model, device=device)\n    model.prediction_heads[0].no_ans_boost = no_ans_boost\n    model.prediction_heads[0].n_best = recall_at\n    model.connect_heads_with_processor(data_silo.processor.tasks, require_labels=True)\n\n    # 6. Run the Evaluator\n    results = evaluator.eval(model)\n    f1_score = results[0][""f1""]\n    em_score = results[0][""EM""]\n    tnrecall = results[0][""top_n_recall""]\n    print(""F1-Score:"", f1_score)\n    print(""Exact Match Score:"", em_score)\n    print(f""top_{recall_at}_recall:"", tnrecall)\n\n\nif __name__ == ""__main__"":\n    #evaluate_classification()\n    evaluate_question_answering()\n'"
examples/lm_finetuning.py,0,"b'import logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import BertStyleLMProcessor\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import BertLMHead, NextSentenceHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.modeling.optimization import initialize_optimizer\n\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\n\ndef lm_finetuning():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO,\n    )\n\n    set_all_seeds(seed=42)\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(\n        experiment_name=""Public_FARM"", run_name=""Run_minimal_example_lm""\n    )\n    ##########################\n    ########## Settings\n    ##########################\n    device, n_gpu = initialize_device_settings(use_cuda=False)\n    n_epochs = 1\n    batch_size = 32\n    evaluate_every = 30\n    lang_model = ""bert-base-cased""\n    do_lower_case = False\n    next_sent_pred_style = ""bert-style""\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model, do_lower_case=do_lower_case\n    )\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    processor = BertStyleLMProcessor(\n        data_dir=Path(""../data/lm_finetune_nips""),\n        tokenizer=tokenizer,\n        max_seq_len=128,\n        max_docs=20, # We have set max_docs to 20 to speed up data processing\n        next_sent_pred_style=next_sent_pred_style\n    )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(processor=processor, batch_size=batch_size, max_multiprocessing_chunksize=20)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(lang_model)\n    # b) and *two* prediction heads on top that are suited for our task => Language Model finetuning\n    lm_prediction_head = BertLMHead.load(lang_model)\n    next_sentence_head = NextSentenceHead.load(lang_model)\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[lm_prediction_head, next_sentence_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token"", ""per_sequence""],\n        device=device,\n    )\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        device=device,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs,\n    )\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device,\n    )\n\n    # 7. Let it grow! Watch the tracked metrics live on the public mlflow server: https://public-mlflow.deepset.ai\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    save_dir = Path(""saved_models/bert-english-lm-tutorial"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n\nif __name__ == ""__main__"":\n    lm_finetuning()\n'"
examples/natural_questions.py,0,"b'# fmt: off\nimport logging\nimport pprint\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import NaturalQuestionsProcessor\nfrom farm.file_utils import fetch_archive_from_http\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.modeling.prediction_head import QuestionAnsweringHead, TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\n\ndef question_answering():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_natural_questions"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    batch_size = 24\n    n_epochs = 1\n    evaluate_every = 500\n    lang_model = ""deepset/roberta-base-squad2"" # start with a model that can already extract answers\n    do_lower_case = False # roberta is a cased model\n    train_filename = ""train_medium.jsonl""\n    dev_filename = ""dev_medium.jsonl""\n    keep_is_impossible = 0.15 # downsample negative examples after data conversion\n    downsample_context_size = 300 # reduce length of wikipedia articles to relevant part around the answer\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model, do_lower_case=do_lower_case\n    )\n\n    # Add HTML tag tokens to the tokenizer vocabulary, so they do not get split apart\n    html_tags = [\n                ""<Th>"",""</Th>"",\n                ""<Td>"",""</Td>"",\n                ""<Tr>"",""</Tr>"",\n                ""<Li>"",""</Li>"",\n                ""<P>"" ,""</P>"",\n                ""<Ul>"",""</Ul>"",\n                ""<H1>"",""</H1>"",\n                ""<H2>"",""</H2>"",\n                ""<H3>"",""</H3>"",\n                ""<H4>"",""</H4>"",\n                ""<H5>"", ""</H5>"",\n                ""<Td_colspan="",\n    ]\n    tokenizer.add_tokens(html_tags)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    processor = NaturalQuestionsProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=384,\n        train_filename=train_filename,\n        dev_filename=dev_filename,\n        keep_is_impossible=keep_is_impossible,\n        downsample_context_size=downsample_context_size,\n        data_dir=Path(""../data/natural_questions""),\n    )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(processor=processor, batch_size=batch_size, caching=True)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(lang_model,n_added_tokens=len(html_tags))\n    # b) and in case of Natural Questions we need two Prediction Heads\n    #    one for extractive Question Answering\n    qa_head = QuestionAnsweringHead()\n    #    another one for answering yes/no questions or deciding if the given text passage might contain an answer\n    classification_head = TextClassificationHead(num_labels=len(processor.answer_type_list)) # answer_type_list = [""is_impossible"", ""span"", ""yes"", ""no""]\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[qa_head, classification_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token"", ""per_sequence""],\n        device=device,\n    )\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=3e-5,\n        schedule_opts={""name"": ""LinearWarmup"", ""warmup_proportion"": 0.2},\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs,\n        device=device\n    )\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device,\n    )\n\n    # 7. Let it grow! Watch the tracked metrics live on the public mlflow server: https://public-mlflow.deepset.ai\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    save_dir = Path(""../saved_models/roberta-base-squad2-nq"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    # 9. Since training on the whole NQ corpus requires substantial compute resources we trained and uploaded a model on s3\n    fetch_archive_from_http(""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/models/roberta-base-squad2-nq.zip"", output_dir=""../saved_models/farm"")\n    QA_input = [\n        {\n            ""qas"": [""Did GameTrailers rated Twilight Princess as one of the best games ever created?""],\n            ""context"":  ""Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the greatest games ever created.""\n        }\n    ]\n\n    model = Inferencer.load(model_name_or_path=""../saved_models/farm/roberta-base-squad2-nq"", batch_size=batch_size, gpu=True)\n    result = model.inference_from_dicts(dicts=QA_input)\n\n    pprint.pprint(result)\n\n    print(f""\\nQuestion: Did GameTrailers rated Twilight Princess as one of the best games ever created?""\n          f""\\nAnswer from model: {result[0][\'predictions\'][0][\'answers\'][0][\'classification\']}"")\n\nif __name__ == ""__main__"":\n    question_answering()\n\n\n'"
examples/ner.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import NERProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import TokenClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\ndef ner():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO,\n    )\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_ner"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    n_epochs = 4\n    batch_size = 32\n    evaluate_every = 400\n    lang_model = ""bert-base-german-cased""\n    do_lower_case = False\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model, do_lower_case=do_lower_case\n    )\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    # See test/sample/ner/train-sample.txt for an example of the data format that is expected by the Processor\n    ner_labels = [""[PAD]"", ""X"", ""O"", ""B-MISC"", ""I-MISC"", ""B-PER"", ""I-PER"", ""B-ORG"", ""I-ORG"", ""B-LOC"", ""I-LOC"", ""B-OTH"", ""I-OTH""]\n\n    processor = NERProcessor(\n        tokenizer=tokenizer, max_seq_len=128, data_dir=Path(""../data/conll03-de""), delimiter="" "", metric=""seq_f1"", label_list=ner_labels\n    )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(processor=processor, batch_size=batch_size)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(lang_model)\n    # b) and a prediction head on top that is suited for our task => NER\n    prediction_head = TokenClassificationHead(num_labels=len(ner_labels))\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token""],\n        device=device,\n    )\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=1e-5,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs,\n        device=device,\n    )\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device,\n    )\n\n    # 7. Let it grow\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    save_dir = ""saved_models/bert-german-ner-tutorial""\n    model.save(save_dir)\n    processor.save(save_dir)\n\n\n    # 9. Load it & harvest your fruits (Inference)\n    basic_texts = [\n        {""text"": ""Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot sei""},\n        {""text"": ""Martin M\xc3\xbcller spielt Handball in Berlin""},\n    ]\n    model = Inferencer.load(save_dir)\n    result = model.inference_from_dicts(dicts=basic_texts)\n    print(result)\n\n\nif __name__ == ""__main__"":\n    ner()\n'"
examples/onnx_question_answering.py,0,"b'from pathlib import Path\n\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\n\n\ndef onnx_runtime_example():\n    """"""\n    This example converts a Question Answering FARM AdaptiveModel\n    to ONNX format and uses ONNX Runtime for doing Inference.\n    """"""\n\n    device = ""cpu""\n    model_name_or_path = ""deepset/bert-base-cased-squad2""\n    onnx_model_export_path = Path(""./onnx-export"")\n\n    model = AdaptiveModel.convert_from_transformers(model_name_or_path, device=device, task_type=""question_answering"")\n    model.convert_to_onnx(onnx_model_export_path)\n\n    inferencer = Inferencer.load(model_name_or_path=onnx_model_export_path)\n\n    qa_input = [\n        {\n            ""qas"": [""Who counted the game among the best ever made?""],\n            ""context"": ""Twilight Princess was released to universal critical acclaim and commercial success. ""\n            ""It received perfect scores from major publications such as 1UP.com, Computer and Video Games, ""\n            ""Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators ""\n            ""GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii ""\n            ""version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called ""\n            ""it one of the greatest games ever created."",\n        }\n    ]\n\n    results = inferencer.inference_from_dicts(qa_input)\n    print(results)\n\n\nif __name__ == ""__main__"":\n    onnx_runtime_example()\n'"
examples/passage_ranking.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import RegressionProcessor, TextPairClassificationProcessor\nfrom farm.experiment import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import RegressionHead, TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings, reformat_msmarco_train, reformat_msmarco_dev, write_msmarco_results\nfrom farm.evaluation.msmarco_passage_farm import msmarco_evaluation\n\n\ndef text_pair_classification():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_text_pair_classification"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    n_epochs = 2\n    batch_size = 64\n    evaluate_every = 500\n    lang_model = ""bert-base-cased""\n    label_list = [""0"", ""1""]\n    train_filename = ""train.tsv""\n    dev_filename = ""dev_200k.tsv""\n\n    # The source data can be found here https://github.com/microsoft/MSMARCO-Passage-Ranking\n    generate_data = False\n    data_dir = Path(""../data/msmarco_passage"")\n    predictions_raw_filename = ""predictions_raw.txt""\n    predictions_filename = ""predictions.txt""\n    train_source_filename = ""triples.train.1m.tsv""\n    qrels_filename = ""qrels.dev.tsv""\n    queries_filename = ""queries.dev.tsv""\n    passages_filename = ""collection.tsv""\n    top1000_filename = ""top1000.dev""\n\n    # 0. Preprocess and save MSMarco data in a format that can be ingested by FARM models. Only needs to be done once!\n    # The final format is a tsv file with 3 columns (text, text_b and label)\n    if generate_data:\n        reformat_msmarco_train(data_dir / train_source_filename,\n                               data_dir / train_filename)\n        reformat_msmarco_dev(data_dir / queries_filename,\n                             data_dir / passages_filename,\n                             data_dir / qrels_filename,\n                             data_dir / top1000_filename,\n                             data_dir / dev_filename)\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=False)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    #    Evaluation during training will be performed on a slice of the train set\n    #    We will be using the msmarco dev set as our final evaluation set\n    processor = TextPairClassificationProcessor(tokenizer=tokenizer,\n                                                label_list=label_list,\n                                                train_filename=train_filename,\n                                                test_filename=None,\n                                                dev_split=0.001,\n                                                max_seq_len=128,\n                                                data_dir=data_dir,\n                                                delimiter=""\\t"")\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(lang_model)\n    # b) and a prediction head on top that is suited for our task\n    prediction_head = TextClassificationHead(num_labels=len(label_list),\n                                             class_weights=data_silo.calculate_class_weights(\n                                                 task_name=""text_classification""),\n                                             )\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence_continuous""],\n        device=device)\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=1e-5,\n        device=device,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs)\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    # 7. Let it grow\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    save_dir = Path(""saved_models/passage_ranking_model"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    # 9. Load it & harvest your fruits (Inference)\n    #    Add your own text adapted to the dataset you provide\n    model = Inferencer.load(save_dir, gpu=True, max_seq_len=128, batch_size=128)\n    result = model.inference_from_file(data_dir / dev_filename)\n\n    write_msmarco_results(result, save_dir / predictions_raw_filename)\n\n    msmarco_evaluation(preds_file=save_dir / predictions_raw_filename,\n                       dev_file=data_dir / dev_filename,\n                       qrels_file=data_dir / qrels_filename,\n                       output_file=save_dir / predictions_filename)\n\n\nif __name__ == ""__main__"":\n    text_pair_classification()\n\n# fmt: on\n'"
examples/question_answering.py,0,"b'# fmt: off\nimport logging\nimport os\nimport pprint\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import SquadProcessor\nfrom farm.data_handler.utils import write_squad_predictions\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.modeling.prediction_head import QuestionAnsweringHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\ndef question_answering():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO,\n    )\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_question_answering"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    batch_size = 24\n    n_epochs = 2\n    evaluate_every = 2000\n    lang_model = ""roberta-base""\n    do_lower_case = False # roberta is a cased model\n    train_filename = ""train-v2.0.json""\n    dev_filename = ""dev-v2.0.json""\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model, do_lower_case=do_lower_case\n    )\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    label_list = [""start_token"", ""end_token""]\n    metric = ""squad""\n    processor = SquadProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=384,\n        label_list=label_list,\n        metric=metric,\n        train_filename=train_filename,\n        dev_filename=dev_filename,\n        test_filename=None,\n        data_dir=Path(""../data/squad20""),\n    )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    # NOTE: In FARM, the dev set metrics differ from test set metrics in that they are calculated on a token level instead of a word level\n    data_silo = DataSilo(processor=processor, batch_size=batch_size, distributed=False)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(lang_model)\n    # b) and a prediction head on top that is suited for our task => Question Answering\n    prediction_head = QuestionAnsweringHead()\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token""],\n        device=device,\n    )\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=3e-5,\n        schedule_opts={""name"": ""LinearWarmup"", ""warmup_proportion"": 0.2},\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs,\n        device=device\n    )\n    # 6. Feed everything to the Trainer, which keeps care of growing our model and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device,\n    )\n    # 7. Let it grow! Watch the tracked metrics live on the public mlflow server: https://public-mlflow.deepset.ai\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    save_dir = Path(""../saved_models/bert-english-qa-tutorial"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    # 9. Load it & harvest your fruits (Inference)\n    QA_input = [\n            {\n                ""qas"": [""Who counted the game among the best ever made?""],\n                ""context"":  ""Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the greatest games ever created.""\n            }]\n\n    model = Inferencer.load(save_dir, batch_size=40, gpu=True)\n    result = model.inference_from_dicts(dicts=QA_input)[0]\n\n    pprint.pprint(result)\n\n    # 10. Do Inference on whole SQuAD Dataset & write the predictions file to disk\n    filename = os.path.join(processor.data_dir,processor.dev_filename)\n    result = model.inference_from_file(file=filename, return_json=False)\n    result_squad = [x.to_squad_eval() for x in result]\n\n    write_squad_predictions(\n        predictions=result_squad,\n        predictions_filename=filename,\n        out_filename=""predictions.json""\n    )\n\n    # 11. Get final evaluation metric using the official SQuAD evaluation script\n    # To evaluate the model\'s performance on the SQuAD dev set, run the official squad eval script\n    # (farm/squad_evaluation.py) in the command line with something like the command below.\n    # This is necessary since the FARM evaluation during training is done on the token level.\n    # This script performs word level evaluation and will generate metrics that are comparable\n    # to the SQuAD leaderboard and most other frameworks:\n    #       python squad_evaluation.py path/to/squad20/dev-v2.0.json path/to/predictions.json\n\nif __name__ == ""__main__"":\n    question_answering()\n'"
examples/question_answering_crossvalidation.py,1,"b'import logging\nimport json\nimport torch\nfrom pathlib import Path\n\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.data_handler.processor import SquadProcessor\nfrom farm.data_handler.data_silo import DataSilo, DataSiloForCrossVal\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import QuestionAnsweringHead\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.train import Trainer\nfrom farm.eval import Evaluator\nfrom farm.evaluation.metrics import squad\n\n\ndef question_answering_crossvalidation():\n    ##########################\n    ########## Logging\n    ##########################\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n    # reduce verbosity from transformers library\n    logging.getLogger(\'transformers\').setLevel(logging.WARNING)\n\n    #ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    # for local logging instead:\n    ml_logger = MLFlowLogger(tracking_uri=""logs"")\n    #ml_logger.init_experiment(experiment_name=""QA_X-Validation"", run_name=""Squad_Roberta_Base"")\n\n    ##########################\n    ########## Settings\n    ##########################\n    save_per_fold_results = True\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n\n    lang_model = ""deepset/roberta-base-squad2""\n    do_lower_case = True\n\n    n_epochs = 3\n    batch_size = 24\n    learning_rate = 3e-5\n\n\n    data_dir = Path(""../data/squad20"")\n    filename = ""dev-v2.0.json""\n    xval_folds = 4\n    dev_split = 0.1\n    evaluate_every = 50\n    no_ans_boost = 0 # use large negative values to disable giving ""no answer"" option\n    recall_at = 3 # recall at n is only useful for answers inside long documents\n    use_amp = None\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=do_lower_case)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    processor = SquadProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=256,\n        label_list=[""start_token"", ""end_token""],\n        metric=""squad"",\n        train_filename=filename,\n        dev_filename=None,\n        dev_split=dev_split,\n        test_filename=None,\n        data_dir=data_dir,\n        doc_stride=128,\n    )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # Load one silo for each fold in our cross-validation\n    silos = DataSiloForCrossVal.make(data_silo, n_splits=xval_folds)\n\n    # the following steps should be run for each of the folds of the cross validation, so we put them\n    # into a function\n    def train_on_split(silo_to_use, n_fold):\n        logger.info(f""############ Crossvalidation: Fold {n_fold} ############"")\n\n        # fine-tune pre-trained question-answering model\n        model = AdaptiveModel.convert_from_transformers(lang_model, device, ""question_answering"")\n        model.connect_heads_with_processor(data_silo.processor.tasks, require_labels=True)\n        # If positive, thjs will boost ""No Answer"" as prediction.\n        # If negative, this will prevent the model from giving ""No Answer"" as prediction.\n        model.prediction_heads[0].no_ans_boost = no_ans_boost\n        # Number of predictions the model will make per Question.\n        # The multiple predictions are used for evaluating top n recall.\n        model.prediction_heads[0].n_best = recall_at\n\n        # # or train question-answering models from scratch\n        # # Create an AdaptiveModel\n        # # a) which consists of a pretrained language model as a basis\n        # language_model = LanguageModel.load(lang_model)\n        # # b) and a prediction head on top that is suited for our task => Question-answering\n        # prediction_head = QuestionAnsweringHead(no_ans_boost=no_ans_boost, n_best=recall_at)\n        # model = AdaptiveModel(\n        #    language_model=language_model,\n        #    prediction_heads=[prediction_head],\n        #    embeds_dropout_prob=0.1,\n        #    lm_output_types=[""per_token""],\n        #    device=device,)\n\n\n        # Create an optimizer\n        model, optimizer, lr_schedule = initialize_optimizer(\n            model=model,\n            learning_rate=learning_rate,\n            device=device,\n            n_batches=len(silo_to_use.loaders[""train""]),\n            n_epochs=n_epochs,\n            use_amp=use_amp)\n\n        # Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n        # Also create an EarlyStopping instance and pass it on to the trainer\n\n        trainer = Trainer(\n            model=model,\n            optimizer=optimizer,\n            data_silo=silo_to_use,\n            epochs=n_epochs,\n            n_gpu=n_gpu,\n            lr_schedule=lr_schedule,\n            evaluate_every=evaluate_every,\n            device=device,\n            evaluator_test=False)\n\n        # train it\n        trainer.train()\n\n        return trainer.model\n\n    # for each fold, run the whole training, then evaluate the model on the test set of each fold\n    # Remember all the results for overall metrics over all predictions of all folds and for averaging\n    all_results = []\n    all_preds = []\n    all_labels = []\n    all_f1 = []\n    all_em = []\n    all_topnrecall = []\n\n    for num_fold, silo in enumerate(silos):\n        model = train_on_split(silo, num_fold)\n\n        # do eval on test set here (and not in Trainer),\n        # so that we can easily store the actual preds and labels for a ""global"" eval across all folds.\n        evaluator_test = Evaluator(\n            data_loader=silo.get_data_loader(""test""),\n            tasks=silo.processor.tasks,\n            device=device\n        )\n        result = evaluator_test.eval(model, return_preds_and_labels=True)\n        evaluator_test.log_results(result, ""Test"", logging=False, steps=len(silo.get_data_loader(""test"")), num_fold=num_fold)\n\n        all_results.append(result)\n        all_preds.extend(result[0].get(""preds""))\n        all_labels.extend(result[0].get(""labels""))\n        all_f1.append(result[0][""f1""])\n        all_em.append(result[0][""EM""])\n        all_topnrecall.append(result[0][""top_n_recall""])\n\n        # emtpy cache to avoid memory leak and cuda OOM across multiple folds\n        model.cpu()\n        torch.cuda.empty_cache()\n\n    # Save the per-fold results to json for a separate, more detailed analysis\n    if save_per_fold_results:\n        def convert_numpy_dtype(obj):\n            if type(obj).__module__ == ""numpy"":\n                return obj.item()\n\n            raise TypeError(""Unknown type:"", type(obj))\n\n        with open(""qa_xval.results.json"", ""wt"") as fp:\n             json.dump(all_results, fp, default=convert_numpy_dtype)\n\n    # calculate overall metrics across all folds\n    xval_score = squad(preds=all_preds, labels=all_labels)\n\n\n    logger.info(f""Single EM-Scores:   {all_em}"")\n    logger.info(f""Single F1-Scores:   {all_f1}"")\n    logger.info(f""Single top_{recall_at}_recall Scores:   {all_topnrecall}"")\n    logger.info(f""XVAL EM:   {xval_score[\'EM\']}"")\n    logger.info(f""XVAL f1:   {xval_score[\'f1\']}"")\n    logger.info(f""XVAL top_{recall_at}_recall:   {xval_score[\'top_n_recall\']}"")\n    ml_logger.log_metrics({""XVAL EM"": xval_score[""EM""]}, 0)\n    ml_logger.log_metrics({""XVAL f1"": xval_score[""f1""]}, 0)\n    ml_logger.log_metrics({f""XVAL top_{recall_at}_recall"": xval_score[""top_n_recall""]}, 0)\nif __name__ == ""__main__"":\n    question_answering_crossvalidation()'"
examples/streaming_inference.py,0,"b'from farm.infer import Inferencer\n\n\ndef streaming_inference_example():\n    """"""\n    The FARM Inferencer has a high performance non-blocking streaming mode for large scale inference use cases. With\n    this mode, the dicts parameter can optionally be a Python generator object that yield dicts, thus avoiding loading\n    dicts in memory. The inference_from_dicts() method returns a generator that yield predictions. To use streaming,\n    set the streaming param to True and determine optimal multiprocessing_chunksize by performing speed benchmarks.\n    """"""\n\n    model_name_or_path = ""deepset/bert-base-cased-squad2""\n    inferencer = Inferencer.load(model_name_or_path=model_name_or_path, task_type=""question_answering"", num_processes=8)\n\n    dicts = sample_dicts_generator()  # it can be a list of dicts or a generator object\n    results = inferencer.inference_from_dicts(dicts, streaming=True, multiprocessing_chunksize=20)\n\n    for prediction in results:  # results is a generator object that yields predictions\n        print(prediction)\n\n\ndef sample_dicts_generator():\n    """"""\n    This is a sample dicts generator. Some exemplary use cases:\n\n    * read chunks of text from large files iteratively and generate inference predictions\n    * connect with external datasources, eg, a Elasticsearch Scroll API that reads all documents from a given index\n    * building a streaming microservice that reads from Kafka\n\n    :return: a generator that yield dicts\n    :rtype: iter\n    """"""\n    qa_input = {\n        ""qas"": [""Who counted the game among the best ever made?""],\n        ""context"": ""Twilight Princess was released to universal critical acclaim and commercial success. ""\n                   ""It received perfect scores from major publications such as 1UP.com, Computer and Video Games, ""\n                   ""Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators ""\n                   ""GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii ""\n                   ""version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called ""\n                   ""it one of the greatest games ever created."",\n    }\n\n    for i in range(100000):\n        yield qa_input\n\n\nif __name__ == ""__main__"":\n    streaming_inference_example()\n'"
examples/text_pair_classification.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import RegressionProcessor, TextPairClassificationProcessor\nfrom farm.experiment import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import RegressionHead, TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\n\ndef text_pair_classification():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ml_logger = MLFlowLogger(tracking_uri=""https://public-mlflow.deepset.ai/"")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_text_pair_classification"")\n\n    ##########################\n    ########## Settings ######\n    ##########################\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    n_epochs = 2\n    batch_size = 64\n    evaluate_every = 500\n    lang_model = ""bert-base-cased""\n    label_list = [""0"", ""1""]\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=False)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset.\n    # The TextPairClassificationProcessor expects a csv with columns called ""text\', ""text_b"" and ""label""\n    processor = TextPairClassificationProcessor(tokenizer=tokenizer,\n                                                label_list=label_list,\n                                                metric=""f1_macro"",\n                                                max_seq_len=128,\n                                                dev_filename=""dev.tsv"",\n                                                test_filename=None,\n                                                data_dir=Path(""../data/asnq_binary""),\n                                                delimiter=""\\t"")\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a few descriptive statistics of our datasets\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(lang_model)\n    # b) and a prediction head on top that is suited for our task\n    prediction_head = TextClassificationHead(num_labels=len(label_list),\n                                             class_weights=data_silo.calculate_class_weights(task_name=""text_classification"")\n                                             )\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence_continuous""],\n        device=device)\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=5e-6,\n        device=device,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs)\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    # 7. Let it grow\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    save_dir = Path(""saved_models/text_pair_classification_model"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    # 9. Load it & harvest your fruits (Inference)\n    #    Add your own text adapted to the dataset you provide\n    basic_texts = [\n        {""text"": ""how many times have real madrid won the champions league in a row"", ""text_b"": ""They have also won the competition the most times in a row, winning it five times from 1956 to 1960""},\n        {""text"": ""how many seasons of the blacklist are there on netflix"", ""text_b"": ""Retrieved March 27 , 2018 .""},\n    ]\n\n    model = Inferencer.load(save_dir)\n    result = model.inference_from_dicts(dicts=basic_texts)\n\n    print(result)\n\nif __name__ == ""__main__"":\n    text_pair_classification()\n\n# fmt: on\n'"
examples/train_from_scratch.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\nfrom transformers.tokenization_bert import BertTokenizer\n\nfrom farm.data_handler.data_silo import StreamingDataSilo\nfrom farm.data_handler.processor import BertStyleLMProcessor\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.modeling.prediction_head import BertLMHead, NextSentenceHead\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\n\ndef train_from_scratch():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO,\n    )\n\n    ml_logger = MLFlowLogger(tracking_uri="""")\n    ml_logger.init_experiment(experiment_name=""from_scratch"", run_name=""debug"")\n\n    #########################\n    ######## Settings\n    ########################\n    set_all_seeds(seed=39)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    evaluate_every = 5000\n    vocab_size = 30522\n    # dev_filename = None\n    save_dir = Path(""saved_models/train_from_scratch"")\n\n    n_epochs = 10\n    learning_rate = 1e-4\n    warmup_proportion = 0.05\n    batch_size = 16  # (probably only possible via gradient accumulation steps)\n    max_seq_len = 64\n\n    data_dir = Path(""data/lm_finetune_nips"")\n    train_filename = ""train.txt""\n    dev_filename = ""dev.txt""\n\n    # 1.Create a tokenizer\n    tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    processor = BertStyleLMProcessor(\n        data_dir=data_dir,\n        tokenizer=tokenizer, max_seq_len=max_seq_len,\n        train_filename=train_filename,\n        dev_filename=dev_filename,\n        test_filename=None,\n    )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and\n    #    calculates a few descriptive statistics of our datasets\n    stream_data_silo = StreamingDataSilo(processor=processor, batch_size=batch_size)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.from_scratch(""bert"", vocab_size)\n\n    # b) and *two* prediction heads on top that are suited for our task => Language Model finetuning\n    lm_prediction_head = BertLMHead(768, vocab_size)\n    next_sentence_head = NextSentenceHead([768, 2], task_name=""nextsentence"")\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[lm_prediction_head, next_sentence_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token"", ""per_sequence""],\n        device=device,\n    )\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=learning_rate,\n        schedule_opts={""name"": ""LinearWarmup"", ""warmup_proportion"": warmup_proportion},\n        n_batches=len(stream_data_silo.get_data_loader(""train"")),\n        n_epochs=n_epochs,\n        device=device,\n        grad_acc_steps=8,\n    )\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model and evaluates it from time to time\n    trainer = Trainer.create_or_load_checkpoint(\n        model=model,\n        optimizer=optimizer,\n        data_silo=stream_data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device,\n        grad_acc_steps=8,\n        checkpoint_root_dir=Path(""saved_models/train_from_scratch/checkpoints""),\n    )\n    # 7. Let it grow! Watch the tracked metrics live on the public mlflow server: https://public-mlflow.deepset.ai\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    model.save(save_dir)\n    processor.save(save_dir)\n\n\nif __name__ == ""__main__"":\n    train_from_scratch()\n'"
examples/train_from_scratch_with_sagemaker.py,0,"b'import json\nimport logging\nfrom pathlib import Path\n\nfrom transformers.tokenization_bert import BertTokenizer\n\nfrom farm.data_handler.data_silo import StreamingDataSilo\nfrom farm.data_handler.processor import BertStyleLMProcessor\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.modeling.prediction_head import BertLMHead, NextSentenceHead\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\n\ndef train_from_scratch(args):\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO,\n    )\n\n    ml_logger = MLFlowLogger(tracking_uri=args.get(""mlflow_tracking_uri"", ""file:/opt/ml/model/mlflow""))\n    ml_logger.init_experiment(experiment_name=""train_from_scratch"", run_name=""run"")\n\n    set_all_seeds(seed=39)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    evaluate_every = int(args[""evaluate_every""])\n\n    save_dir = Path(""/opt/ml/model"")\n    data_dir = Path(""/opt/ml/input/data/input_channel"")\n\n    # 1.Create a tokenizer\n    tokenizer = BertTokenizer(data_dir/args[""vocab_file""], do_lower_case=args[""do_lower_case""])\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a PyTorch Dataset\n    processor = BertStyleLMProcessor(\n        data_dir=data_dir,\n        tokenizer=tokenizer, max_seq_len=int(args[""max_seq_len""]),\n        train_filename=args[""train_file""],\n        dev_filename=args.get(""dev_file"", None),\n        test_filename=args.get(""test_file"", None),\n    )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and\n    #    calculates a few descriptive statistics of our datasets\n    stream_data_silo = StreamingDataSilo(processor=processor, batch_size=int(args[""batch_size""]))\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.from_scratch(""bert"", tokenizer.vocab_size)\n\n    # b) and *two* prediction heads on top that are suited for our task => Language Model finetuning\n    lm_prediction_head = BertLMHead(768, tokenizer.vocab_size)\n    next_sentence_head = NextSentenceHead([768, 2], task_name=""nextsentence"")\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[lm_prediction_head, next_sentence_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token"", ""per_sequence""],\n        device=device,\n    )\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=float(args[""learning_rate""]),\n        schedule_opts={""name"": ""LinearWarmup"", ""warmup_proportion"": float(args[""warmup_proportion""])},\n        n_batches=len(stream_data_silo.get_data_loader(""train"")),\n        n_epochs=int(args[""n_epochs""]),\n        device=device,\n        grad_acc_steps=int(args[""gradient_accumulation_steps""]),\n    )\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model and evaluates it from time to time\n    if args.get(""checkpoint_every""):\n        checkpoint_every = int(args[""checkpoint_every""])\n        checkpoint_root_dir = Path(""/opt/ml/checkpoints/training"")\n    else:\n        checkpoint_every = None\n        checkpoint_root_dir = None\n\n    trainer = Trainer.create_or_load_checkpoint(\n        model=model,\n        optimizer=optimizer,\n        data_silo=stream_data_silo,\n        epochs=int(args[""n_epochs""]),\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device,\n        grad_acc_steps=int(args[""gradient_accumulation_steps""]),\n        checkpoint_every=checkpoint_every,\n        checkpoint_root_dir=checkpoint_root_dir,\n    )\n    # 7. Let it grow! Watch the tracked metrics live on the public mlflow server: https://public-mlflow.deepset.ai\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    model.save(save_dir)\n    processor.save(save_dir)\n\n\nif __name__ == ""__main__"":\n    with open(""/opt/ml/input/config/hyperparameters.json"") as f:\n        params = json.load(f)\n    logging.info(f""Starting a train job with parameters {params}"")\n    train_from_scratch(params)\n'"
examples/wordembedding_inference.py,0,"b'# fmt: off\nimport logging\nfrom pathlib import Path\n\n\nfrom farm.data_handler.processor import InferenceProcessor\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.utils import set_all_seeds, initialize_device_settings\n\ndef embedding_extraction():\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n        datefmt=""%m/%d/%Y %H:%M:%S"",\n        level=logging.INFO)\n\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=42)\n    # load from a local path:\n    #lang_model = Path(""../saved_models/glove-german-uncased"")\n    # or through s3\n    lang_model = ""glove-german-uncased"" #only glove or word2vec or converted fasttext (fixed vocab) embeddings supported\n    do_lower_case = True\n    use_amp = None\n    device, n_gpu = initialize_device_settings(use_cuda=True, use_amp=use_amp)\n\n    # Create a InferenceProcessor\n    tokenizer = Tokenizer.load(pretrained_model_name_or_path=lang_model, do_lower_case=do_lower_case)\n    processor = InferenceProcessor(tokenizer=tokenizer, max_seq_len=128)\n\n    # Create an AdaptiveModel\n    language_model = LanguageModel.load(lang_model)\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n\n    # Create Inferencer for embedding extraction\n    inferencer = Inferencer(\n        model=model,\n        processor=processor,\n        task_type=""embeddings""\n    )\n\n\n    # Extract vectors\n    basic_texts = [\n        {""text"": ""Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot sei""},\n        {""text"": ""Martin M\xc3\xbcller spielt Handball in Berlin""},\n    ]\n\n    result = inferencer.extract_vectors(\n        dicts=basic_texts,\n        extraction_strategy=""cls_token"",\n        extraction_layer=-1\n    )\n    print(result)\n\n\nif __name__ == ""__main__"":\n    embedding_extraction()\n\n# fmt: on\n'"
farm/__init__.py,2,"b'import logging\n\nimport torch.multiprocessing as mp\n\nlogging.basicConfig(\n    format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n    datefmt=""%m/%d/%Y %H:%M:%S"",\n    level=logging.INFO,\n)\n\n# reduce verbosity from transformers library\nlogging.getLogger(\'transformers.configuration_utils\').setLevel(logging.WARNING)\n\n# https://pytorch.org/docs/stable/multiprocessing.html#sharing-strategies\nif ""file_descriptor"" in mp.get_all_sharing_strategies():\n    import resource\n\n    mp.set_sharing_strategy(""file_descriptor"")\n\n    rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n    # seting soft limit to hard limit (=rlimit[1]) minus a small amount to be safe\n    resource.setrlimit(resource.RLIMIT_NOFILE, (rlimit[1]-512, rlimit[1]))\n'"
farm/convert_tf_checkpoint_to_pytorch.py,1,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Convert BERT checkpoint.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\nimport torch\nfrom farm.modeling.bert.modeling import (\n    BertConfig,\n    BertForPreTraining,\n    load_tf_weights_in_bert,\n)\n\n\ndef convert_tf_checkpoint_to_pytorch(\n    tf_checkpoint_path, bert_config_file, pytorch_dump_path\n):\n    # Initialise PyTorch model\n    config = BertConfig.from_json_file(bert_config_file)\n    print(""Building PyTorch model from configuration: {}"".format(str(config)))\n    model = BertForPreTraining(config)\n\n    # Load weights from tf checkpoint\n    load_tf_weights_in_bert(model, tf_checkpoint_path)\n\n    # Save pytorch-model\n    print(""Save PyTorch model to {}"".format(pytorch_dump_path))\n    torch.save(model.state_dict(), pytorch_dump_path)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\n        ""--tf_checkpoint_path"",\n        default=None,\n        type=str,\n        required=True,\n        help=""Path the TensorFlow checkpoint path."",\n    )\n    parser.add_argument(\n        ""--bert_config_file"",\n        default=None,\n        type=str,\n        required=True,\n        help=""The config json file corresponding to the pre-trained BERT model. \\n""\n        ""This specifies the model architecture."",\n    )\n    parser.add_argument(\n        ""--pytorch_dump_path"",\n        default=None,\n        type=str,\n        required=True,\n        help=""Path to the output PyTorch model."",\n    )\n    args = parser.parse_args()\n    convert_tf_checkpoint_to_pytorch(\n        args.tf_checkpoint_path, args.bert_config_file, args.pytorch_dump_path\n    )\n'"
farm/eval.py,2,"b'from tqdm import tqdm\nimport torch\nimport numbers\nimport logging\nimport numpy as np\nfrom torch.utils.data import DataLoader\n\nfrom farm.evaluation.metrics import compute_metrics, compute_report_metrics\nfrom farm.utils import to_numpy\nfrom farm.utils import MLFlowLogger as MlLogger\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.visual.ascii.images import BUSH_SEP\n\nlogger = logging.getLogger(__name__)\n\n\nclass Evaluator:\n    """"""Handles evaluation of a given model over a specified dataset.""""""\n\n    def __init__(\n        self, data_loader, tasks, device, report=True\n    ):\n        """"""\n        :param data_loader: The PyTorch DataLoader that will return batches of data from the evaluation dataset\n        :type data_loader: DataLoader\n        :param label_maps:\n        :param device: The device on which the tensors should be processed. Choose from ""cpu"" and ""cuda"".\n        :param metrics: The list of metrics which need to be computed, one for each prediction head.\n        :param metrics: list\n        :param report: Whether an eval report should be generated (e.g. classification report per class).\n        :type report: bool\n        """"""\n\n        self.data_loader = data_loader\n        self.tasks = tasks\n        self.device = device\n        self.report = report\n\n    def eval(self, model, return_preds_and_labels=False):\n        """"""\n        Performs evaluation on a given model.\n\n        :param model: The model on which to perform evaluation\n        :type model: AdaptiveModel\n        :param return_preds_and_labels: Whether to add preds and labels in the returned dicts of the\n        :type return_preds_and_labels: bool\n        :return all_results: A list of dictionaries, one for each prediction head. Each dictionary contains the metrics\n                             and reports generated during evaluation.\n        :rtype all_results: list of dicts\n        """"""\n        model.eval()\n\n        # init empty lists per prediction head\n        loss_all = [0 for _ in model.prediction_heads]\n        preds_all = [[] for _ in model.prediction_heads]\n        label_all = [[] for _ in model.prediction_heads]\n        ids_all = [[] for _ in model.prediction_heads]\n        passage_start_t_all = [[] for _ in model.prediction_heads]\n\n        for step, batch in enumerate(\n            tqdm(self.data_loader, desc=""Evaluating"", mininterval=10)\n        ):\n            batch = {key: batch[key].to(self.device) for key in batch}\n\n            with torch.no_grad():\n\n                logits = model.forward(**batch)\n                losses_per_head = model.logits_to_loss_per_head(logits=logits, **batch)\n                preds = model.logits_to_preds(logits=logits, **batch)\n                labels = model.prepare_labels(**batch)\n\n            # stack results of all batches per prediction head\n            for head_num, head in enumerate(model.prediction_heads):\n                loss_all[head_num] += np.sum(to_numpy(losses_per_head[head_num]))\n                preds_all[head_num] += list(to_numpy(preds[head_num]))\n                label_all[head_num] += list(to_numpy(labels[head_num]))\n                if head.model_type == ""span_classification"":\n                    ids_all[head_num] += list(to_numpy(batch[""id""]))\n                    passage_start_t_all[head_num] += list(to_numpy(batch[""passage_start_t""]))\n\n        # Evaluate per prediction head\n        all_results = []\n        for head_num, head in enumerate(model.prediction_heads):\n            if head.model_type == ""multilabel_text_classification"":\n                # converting from string preds back to multi-hot encoding\n                from sklearn.preprocessing import MultiLabelBinarizer\n                mlb = MultiLabelBinarizer(classes=head.label_list)\n                # TODO check why .fit() should be called on predictions, rather than on labels\n                preds_all[head_num] = mlb.fit_transform(preds_all[head_num])\n                label_all[head_num] = mlb.transform(label_all[head_num])\n            if hasattr(head, \'aggregate_preds\'):\n                preds_all[head_num], label_all[head_num] = head.aggregate_preds(preds=preds_all[head_num],\n                                                                          labels=label_all[head_num],\n                                                                          passage_start_t=passage_start_t_all[head_num],\n                                                                          ids=ids_all[head_num])\n\n            result = {""loss"": loss_all[head_num] / len(self.data_loader.dataset),\n                      ""task_name"": head.task_name}\n            result.update(\n                compute_metrics(metric=head.metric, preds=preds_all[head_num], labels=label_all[head_num]\n                )\n            )\n\n            # Select type of report depending on prediction head output type\n            if self.report:\n                result[""report""] = compute_report_metrics(head, preds_all[head_num], label_all[head_num])\n\n            if return_preds_and_labels:\n                result[""preds""] = preds_all[head_num]\n                result[""labels""] = label_all[head_num]\n\n            all_results.append(result)\n\n        return all_results\n\n    @staticmethod\n    def log_results(results, dataset_name, steps, logging=True, print=True, num_fold=None):\n        # Print a header\n        header = ""\\n\\n""\n        header += BUSH_SEP + ""\\n""\n        header += ""***************************************************\\n""\n        if num_fold:\n            header += f""***** EVALUATION | FOLD: {num_fold} | {dataset_name.upper()} SET | AFTER {steps} BATCHES *****\\n""\n        else:\n            header += f""***** EVALUATION | {dataset_name.upper()} SET | AFTER {steps} BATCHES *****\\n""\n        header += ""***************************************************\\n""\n        header += BUSH_SEP + ""\\n""\n        logger.info(header)\n\n        for head_num, head in enumerate(results):\n            logger.info(""\\n _________ {} _________"".format(head[\'task_name\']))\n            for metric_name, metric_val in head.items():\n                # log with ML framework (e.g. Mlflow)\n                if logging:\n                    if not metric_name in [""preds"",""labels""] and not metric_name.startswith(""_""):\n                        if isinstance(metric_val, numbers.Number):\n                            MlLogger.log_metrics(\n                                metrics={\n                                    f""{dataset_name}_{metric_name}_{head[\'task_name\']}"": metric_val\n                                },\n                                step=steps,\n                            )\n                # print via standard python logger\n                if print:\n                    if metric_name == ""report"":\n                        if isinstance(metric_val, str) and len(metric_val) > 8000:\n                            metric_val = metric_val[:7500] + ""\\n ............................. \\n"" + metric_val[-500:]\n                        logger.info(""{}: \\n {}"".format(metric_name, metric_val))\n                    else:\n                        if not metric_name in [""preds"", ""labels""] and not metric_name.startswith(""_""):\n                            logger.info(""{}: {}"".format(metric_name, metric_val))\n'"
farm/experiment.py,0,"b'import logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.modeling.prediction_head import PredictionHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.data_handler.processor import Processor\nfrom farm.train import Trainer, EarlyStopping\nfrom farm.utils import set_all_seeds, initialize_device_settings\nfrom farm.utils import MLFlowLogger as MlLogger\nfrom farm.file_utils import read_config, unnestConfig\n\nlogger = logging.getLogger(__name__)\n\nlogging.basicConfig(\n    format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n    datefmt=""%m/%d/%Y %H:%M:%S"",\n    level=logging.INFO,\n)\n\n\ndef load_experiments(file):\n    args = read_config(file)\n    experiments = unnestConfig(args)\n    return experiments\n\n\ndef run_experiment(args):\n    logger.info(\n        ""\\n***********************************************""\n        f""\\n************* Experiment: {args.task.name} ************""\n        ""\\n************************************************""\n    )\n    ml_logger = MlLogger(tracking_uri=args.logging.mlflow_url)\n    ml_logger.init_experiment(\n        experiment_name=args.logging.mlflow_experiment,\n        run_name=args.logging.mlflow_run_name,\n        nested=args.logging.mlflow_nested,\n    )\n\n    validate_args(args)\n    distributed = bool(args.general.local_rank != -1)\n\n    # Init device and distributed settings\n    device, n_gpu = initialize_device_settings(\n        use_cuda=args.general.cuda,\n        local_rank=args.general.local_rank,\n        use_amp=args.general.use_amp,\n    )\n\n    args.parameter.batch_size = int(\n        args.parameter.batch_size // args.parameter.gradient_accumulation_steps\n    )\n\n    set_all_seeds(args.general.seed)\n\n    # Prepare Data\n    tokenizer = Tokenizer.load(\n        args.parameter.model, do_lower_case=args.parameter.lower_case\n    )\n\n    processor = Processor.load(\n        tokenizer=tokenizer,\n        max_seq_len=args.parameter.max_seq_len,\n        data_dir=Path(args.general.data_dir),\n        **args.task.toDict(),  # args is of type DotMap and needs conversion to std python dicts\n    )\n\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=args.parameter.batch_size,\n        distributed=distributed,\n    )\n\n    class_weights = None\n    if args.parameter.balance_classes:\n        task_names = list(processor.tasks.keys())\n        if len(task_names) > 1:\n            raise NotImplementedError(f""Balancing classes is currently not supported for multitask experiments. Got tasks:  {task_names} "")\n        class_weights = data_silo.calculate_class_weights(task_name=task_names[0])\n\n    model = get_adaptive_model(\n        lm_output_type=args.parameter.lm_output_type,\n        prediction_heads=args.parameter.prediction_head,\n        layer_dims=args.parameter.layer_dims,\n        model=args.parameter.model,\n        device=device,\n        class_weights=class_weights,\n        embeds_dropout_prob=args.parameter.embeds_dropout_prob,\n    )\n\n    # Init optimizer\n    optimizer_opts = args.optimizer.optimizer_opts.toDict() if args.optimizer.optimizer_opts else None\n    schedule_opts = args.optimizer.schedule_opts.toDict() if args.optimizer.schedule_opts else None\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=args.optimizer.learning_rate,\n        schedule_opts=schedule_opts,\n        optimizer_opts=optimizer_opts,\n        use_amp=args.general.use_amp,\n        n_batches=len(data_silo.loaders[""train""]),\n        grad_acc_steps=args.parameter.gradient_accumulation_steps,\n        n_epochs=args.parameter.epochs,\n        device=device\n    )\n\n    model_name = (\n        f""{model.language_model.name}-{model.language_model.language}-{args.task.name}""\n    )\n\n    # An early stopping instance can be used to save the model that performs best on the dev set\n    # according to some metric and stop training when no improvement is happening for some iterations.\n    if ""early_stopping"" in args:\n        early_stopping = EarlyStopping(\n            metric=args.task.metric,\n            mode=args.early_stopping.mode,\n            save_dir=Path(f""{args.general.output_dir}/{model_name}_early_stopping""),  # where to save the best model\n            patience=args.early_stopping.patience    # number of evaluations to wait for improvement before terminating the training\n        )\n    else:\n        early_stopping = None\n\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=args.parameter.epochs,\n        n_gpu=n_gpu,\n        grad_acc_steps=args.parameter.gradient_accumulation_steps,\n        use_amp=args.general.use_amp,\n        local_rank=args.general.local_rank,\n        lr_schedule=lr_schedule,\n        evaluate_every=args.logging.eval_every,\n        device=device,\n        early_stopping=early_stopping\n    )\n\n    model = trainer.train()\n\n    processor.save(Path(f""{args.general.output_dir}/{model_name}""))\n    model.save(Path(f""{args.general.output_dir}/{model_name}""))\n\n    ml_logger.end_run()\n\ndef get_adaptive_model(\n    lm_output_type,\n    prediction_heads,\n    layer_dims,\n    model,\n    device,\n    embeds_dropout_prob,\n    class_weights=None,\n):\n    parsed_lm_output_types = lm_output_type.split("","")\n    language_model = LanguageModel.load(model)\n\n    initialized_heads = []\n    for head_name in prediction_heads.split("",""):\n        initialized_heads.append(\n            PredictionHead.create(\n                prediction_head_name=head_name,\n                layer_dims=layer_dims,\n                class_weights=class_weights,\n            )\n        )\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=initialized_heads,\n        embeds_dropout_prob=embeds_dropout_prob,\n        lm_output_types=parsed_lm_output_types,\n        device=device,\n    )\n    return model\n\n\ndef validate_args(args):\n    if not args.task.do_train and not args.task.do_eval:\n        raise ValueError(""At least one of `do_train` or `do_eval` must be True."")\n\n    if args.parameter.gradient_accumulation_steps < 1:\n        raise ValueError(\n            ""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n                args.parameter.gradient_accumulation_steps\n            )\n        )\n\n\ndef save_model():\n    raise NotImplementedError\n\n\ndef load_model():\n    raise NotImplementedError\n'"
farm/file_utils.py,1,"b'""""""\nUtilities for working with the local dataset cache.\nThis file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\nCopyright by the AllenNLP authors.\n""""""\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport os\nimport tempfile\nimport tarfile\nimport zipfile\n\nfrom functools import wraps\nfrom hashlib import sha256\nfrom io import open\nfrom pathlib import Path\n\nimport boto3\nimport numpy as np\nimport requests\nfrom botocore.exceptions import ClientError\nfrom dotmap import DotMap\nfrom tqdm import tqdm\nfrom transformers.file_utils import cached_path\n\ntry:\n    from torch.hub import _get_torch_home\n\n    torch_cache_home = Path(_get_torch_home())\nexcept ImportError:\n    torch_cache_home = Path(os.path.expanduser(\n        os.getenv(\n            ""TORCH_HOME"", Path(os.getenv(""XDG_CACHE_HOME"", ""~/.cache"")) / ""torch""\n        )\n    ))\ndefault_cache_path = torch_cache_home / ""farm""\n\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n\ntry:\n    from pathlib import Path\n\n    FARM_CACHE = Path(os.getenv(""FARM_CACHE"", default_cache_path))\nexcept (AttributeError, ImportError):\n    FARM_CACHE = os.getenv(""FARM_CACHE"", default_cache_path)\n\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef url_to_filename(url, etag=None):\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    """"""\n    url_bytes = url.encode(""utf-8"")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(""utf-8"")\n        etag_hash = sha256(etag_bytes)\n        filename += ""."" + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename, cache_dir=None):\n    """"""\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = FARM_CACHE\n\n    cache_path = cache_dir / filename\n    if not os.path.exists(cache_path):\n        raise EnvironmentError(""file {} not found"".format(cache_path))\n\n    meta_path = cache_path + "".json""\n    if not os.path.exists(meta_path):\n        raise EnvironmentError(""file {} not found"".format(meta_path))\n\n    with open(meta_path, encoding=""utf-8"") as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[""url""]\n    etag = metadata[""etag""]\n\n    return url, etag\n\n\ndef split_s3_path(url):\n    """"""Split a full s3 path into the bucket name and path.""""""\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(""bad s3 path {}"".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove \'/\' at beginning of path.\n    if s3_path.startswith(""/""):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func):\n    """"""\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    """"""\n\n    @wraps(func)\n    def wrapper(url, *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[""Error""][""Code""]) == 404:\n                raise EnvironmentError(""file {} not found"".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url):\n    """"""Check ETag on S3 object.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url, temp_file):\n    """"""Pull a file directly from S3.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef http_get(url, temp_file, proxies=None):\n    req = requests.get(url, stream=True, proxies=proxies)\n    content_length = req.headers.get(""Content-Length"")\n    total = int(content_length) if content_length is not None else None\n    progress = tqdm(unit=""B"", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk:  # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\ndef fetch_archive_from_http(url, output_dir, proxies=None):\n    """"""\n    Fetch an archive (zip or tar.gz) from a url via http and extract content to an output directory.\n\n    :param url: http address\n    :type url: str\n    :param output_dir: local path\n    :type output_dir: str\n    :param proxies: proxies details as required by requests library\n    :type proxies: dict\n    :return: bool if anything got fetched\n    """"""\n    # verify & prepare local directory\n    path = Path(output_dir)\n    if not path.exists():\n        path.mkdir(parents=True)\n\n    is_not_empty = len(list(Path(path).rglob(""*""))) > 0\n    if is_not_empty:\n        logger.info(\n            f""Found data stored in `{output_dir}`. Delete this first if you really want to fetch new data.""\n        )\n        return False\n    else:\n        logger.info(f""Fetching from {url} to `{output_dir}`"")\n\n        # download & extract\n        with tempfile.NamedTemporaryFile() as temp_file:\n            http_get(url, temp_file, proxies=proxies)\n            temp_file.flush()\n            temp_file.seek(0)  # making tempfile accessible\n            # extract\n            if url[-4:] == "".zip"":\n                archive = zipfile.ZipFile(temp_file.name)\n                archive.extractall(output_dir)\n            elif url[-7:] == "".tar.gz"":\n                archive = tarfile.open(temp_file.name)\n                archive.extractall(output_dir)\n            # temp_file gets deleted here\n        return True\n\ndef load_from_cache(pretrained_model_name_or_path, s3_dict, **kwargs):\n    # Adjusted from HF Transformers to fit loading WordEmbeddings from deepsets s3\n    # Load from URL or cache if already cached\n    cache_dir = kwargs.pop(""cache_dir"", None)\n    force_download = kwargs.pop(""force_download"", False)\n    resume_download = kwargs.pop(""resume_download"", False)\n    proxies = kwargs.pop(""proxies"", None)\n\n    s3_file = s3_dict[pretrained_model_name_or_path]\n    try:\n        resolved_file = cached_path(\n                        s3_file,\n                        cache_dir=cache_dir,\n                        force_download=force_download,\n                        proxies=proxies,\n                        resume_download=resume_download,\n                    )\n\n        if resolved_file is None:\n            raise EnvironmentError\n\n    except EnvironmentError:\n        if pretrained_model_name_or_path in s3_dict:\n            msg = ""Couldn\'t reach server at \'{}\' to download data."".format(\n                s3_file\n            )\n        else:\n            msg = (\n                ""Model name \'{}\' was not found in model name list. ""\n                ""We assumed \'{}\' was a path, a model identifier, or url to a configuration file or ""\n                ""a directory containing such a file but couldn\'t find any such file at this path or url."".format(\n                    pretrained_model_name_or_path, s3_file,\n                )\n            )\n        raise EnvironmentError(msg)\n\n    if resolved_file == s3_file:\n        logger.info(""loading file {}"".format(s3_file))\n    else:\n        logger.info(""loading file {} from cache at {}"".format(s3_file, resolved_file))\n\n    return resolved_file\n\n\ndef read_set_from_file(filename):\n    """"""\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    """"""\n    collection = set()\n    with open(filename, ""r"", encoding=""utf-8"") as file_:\n        for line in file_:\n            collection.add(line.rstrip())\n    return collection\n\n\ndef get_file_extension(path, dot=True, lower=True):\n    ext = os.path.splitext(path)[1]\n    ext = ext if dot else ext[1:]\n    return ext.lower() if lower else ext\n\n\ndef read_config(path):\n    if path:\n        with open(path) as json_data_file:\n            conf_args = json.load(json_data_file)\n    else:\n        raise ValueError(""No config provided for classifier"")\n\n    # flatten last part of config, take either value or default as value\n    for gk, gv in conf_args.items():\n        for k, v in gv.items():\n            conf_args[gk][k] = v[""value""] if (v[""value""] is not None) else v[""default""]\n\n    # DotMap for making nested dictionary accessible through dot notation\n    args = DotMap(conf_args, _dynamic=False)\n\n    return args\n\n\ndef unnestConfig(config):\n    """"""\n    This function creates a list of config files for evaluating parameters with different values. If a config parameter\n    is of type list this list is iterated over and a config object without lists is returned. Can handle lists inside any\n    number of parameters.\n\n    Can handle nested (one level) configs\n    """"""\n    nestedKeys = []\n    nestedVals = []\n\n    for gk, gv in config.items():\n        if(gk != ""task""):\n            for k, v in gv.items():\n                if isinstance(v, list):\n                    if (\n                        k != ""layer_dims""\n                    ):  # exclude layer dims, since it is already a list\n                        nestedKeys.append([gk, k])\n                        nestedVals.append(v)\n                elif isinstance(v, dict):\n                    logger.warning(""Config too deep! Working on %s"" %(str(v)))\n\n    if len(nestedKeys) == 0:\n        unnestedConfig = [config]\n    else:\n        logger.info(\n            ""Nested config at parameters: %s""\n            % ("", "".join(""."".join(x) for x in nestedKeys))\n        )\n        unnestedConfig = []\n        mesh = np.meshgrid(\n            *nestedVals\n        )  # get all combinations, each dimension corresponds to one parameter type\n        # flatten mesh into shape: [num_parameters, num_combinations] so we can iterate in 2d over any paramter combinations\n        mesh = [x.flatten() for x in mesh]\n\n        # loop over all combinations\n        for i in range(len(mesh[0])):\n            tempconfig = config.copy()\n            for j, k in enumerate(nestedKeys):\n                if isinstance(k, str):\n                    tempconfig[k] = mesh[j][\n                        i\n                    ]  # get ith val of correct param value and overwrite original config\n                elif len(k) == 2:\n                    tempconfig[k[0]][k[1]] = mesh[j][i]  # set nested dictionary keys\n                else:\n                    logger.warning(""Config too deep! Working on %s"" %(str(k)))\n            unnestedConfig.append(tempconfig)\n\n    return unnestedConfig\n'"
farm/infer.py,6,"b'import logging\nimport multiprocessing as mp\nimport os\nfrom functools import partial\n\nimport torch\nfrom torch.utils.data.sampler import SequentialSampler\nfrom tqdm import tqdm\nfrom transformers.configuration_auto import AutoConfig\n\nfrom farm.data_handler.dataloader import NamedDataLoader\nfrom farm.data_handler.processor import Processor, InferenceProcessor, SquadProcessor, NERProcessor, TextClassificationProcessor\nfrom farm.data_handler.utils import grouper\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.modeling.adaptive_model import AdaptiveModel, BaseAdaptiveModel\nfrom farm.utils import initialize_device_settings\nfrom farm.utils import set_all_seeds, calc_chunksize, log_ascii_workers\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Inferencer:\n    """"""\n    Loads a saved AdaptiveModel/ONNXAdaptiveModel from disk and runs it in inference mode. Can be used for a\n    model with prediction head (down-stream predictions) and without (using LM as embedder).\n\n    Example usage:\n\n    .. code-block:: python\n\n       # down-stream inference\n       basic_texts = [\n           {""text"": ""Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot sei""},\n           {""text"": ""Martin M\xc3\xbcller spielt Handball in Berlin""},\n       ]\n       model = Inferencer.load(your_model_dir)\n       model.inference_from_dicts(dicts=basic_texts)\n       # LM embeddings\n       model = Inferencer.load(your_model_dir, extraction_strategy=""cls_token"", extraction_layer=-1)\n       model.inference_from_dicts(dicts=basic_texts)\n    """"""\n\n    def __init__(\n        self,\n        model,\n        processor,\n        task_type,\n        batch_size=4,\n        gpu=False,\n        name=None,\n        return_class_probs=False,\n        extraction_strategy=None,\n        extraction_layer=None,\n        s3e_stats=None,\n        num_processes=None,\n        disable_tqdm=False\n    ):\n        """"""\n        Initializes Inferencer from an AdaptiveModel and a Processor instance.\n\n        :param model: AdaptiveModel to run in inference mode\n        :type model: AdaptiveModel\n        :param processor: A dataset specific Processor object which will turn input (file or dict) into a Pytorch Dataset.\n        :type processor: Processor\n        :param task_type: Type of task the model should be used for. Currently supporting:\n                          ""embeddings"", ""question_answering"", ""text_classification"", ""ner"". More coming soon...\n        :param task_type: str\n        :param batch_size: Number of samples computed once per batch\n        :type batch_size: int\n        :param gpu: If GPU shall be used\n        :type gpu: bool\n        :param name: Name for the current Inferencer model, displayed in the REST API\n        :type name: string\n        :param return_class_probs: either return probability distribution over all labels or the prob of the associated label\n        :type return_class_probs: bool\n        :param extraction_strategy: Strategy to extract vectors. Choices: \'cls_token\' (sentence vector), \'reduce_mean\'\n                               (sentence vector), reduce_max (sentence vector), \'per_token\' (individual token vectors),\n                               \'s3e\' (sentence vector via S3E pooling, see https://arxiv.org/abs/2002.09620)\n        :type extraction_strategy: str\n        :param extraction_layer: number of layer from which the embeddings shall be extracted. Default: -1 (very last layer).\n        :type extraction_layer: int\n        :param s3e_stats: Stats of a fitted S3E model as returned by `fit_s3e_on_corpus()`\n                          (only needed for task_type=""embeddings"" and extraction_strategy = ""s3e"")\n        :type s3e_stats: dict\n        :param num_processes: the number of processes for `multiprocessing.Pool`. Set to value of 0 to disable\n                              multiprocessing. Set to None to let Inferencer use all CPU cores. If you want to\n                              debug the Language Model, you might need to disable multiprocessing!\n        :type num_processes: int\n        :param disable_tqdm: Whether to disable tqdm logging (can get very verbose in multiprocessing)\n        :type disable_tqdm: bool\n        :return: An instance of the Inferencer.\n\n        """"""\n        # Init device and distributed settings\n        device, n_gpu = initialize_device_settings(use_cuda=gpu, local_rank=-1, use_amp=None)\n\n        self.processor = processor\n        self.model = model\n        self.model.eval()\n        self.batch_size = batch_size\n        self.device = device\n        self.language = self.model.get_language()\n        self.task_type = task_type\n        self.disable_tqdm = disable_tqdm\n\n        if task_type == ""embeddings"":\n            if not extraction_layer or not extraction_strategy:\n                    logger.warning(""Using task_type=\'embeddings\', but couldn\'t find one of the args `extraction_layer` and `extraction_strategy`. ""\n                                   ""Since FARM 0.4.2, you set both when initializing the Inferencer and then call inferencer.inference_from_dicts() instead of inferencer.extract_vectors()"")\n            self.model.prediction_heads = torch.nn.ModuleList([])\n            self.model.language_model.extraction_layer = extraction_layer\n            self.model.language_model.extraction_strategy = extraction_strategy\n            self.model.language_model.s3e_stats = s3e_stats\n\n        # TODO add support for multiple prediction heads\n\n        self.name = name if name != None else f""anonymous-{self.task_type}""\n        self.return_class_probs = return_class_probs\n\n        model.connect_heads_with_processor(processor.tasks, require_labels=False)\n        set_all_seeds(42)\n\n        self._set_multiprocessing_pool(num_processes)\n\n    @classmethod\n    def load(\n        cls,\n        model_name_or_path,\n        batch_size=4,\n        gpu=False,\n        task_type=None,\n        return_class_probs=False,\n        strict=True,\n        max_seq_len=256,\n        doc_stride=128,\n        extraction_layer=None,\n        extraction_strategy=None,\n        s3e_stats=None,\n        num_processes=None,\n        disable_tqdm=False\n\n    ):\n        """"""\n        Load an Inferencer incl. all relevant components (model, tokenizer, processor ...) either by\n\n        1. specifying a public name from transformers\' model hub (https://huggingface.co/models)\n        2. or pointing to a local directory it is saved in.\n\n        :param model_name_or_path: Local directory or public name of the model to load.\n        :type model_name_or_path: str\n        :param batch_size: Number of samples computed once per batch\n        :type batch_size: int\n        :param gpu: If GPU shall be used\n        :type gpu: bool\n        :param task_type: Type of task the model should be used for. Currently supporting:\n                          ""embeddings"", ""question_answering"", ""text_classification"", ""ner"". More coming soon...\n        :param task_type: str\n        :param strict: whether to strictly enforce that the keys loaded from saved model match the ones in\n                       the PredictionHead (see torch.nn.module.load_state_dict()).\n                       Set to `False` for backwards compatibility with PHs saved with older version of FARM.\n        :type strict: bool\n        :param max_seq_len: maximum length of one text sample\n        :type max_seq_len: int\n        :param doc_stride: Only QA: When input text is longer than max_seq_len it gets split into parts, strided by doc_stride\n        :type doc_stride: int\n        :param extraction_strategy: Strategy to extract vectors. Choices: \'cls_token\' (sentence vector), \'reduce_mean\'\n                               (sentence vector), reduce_max (sentence vector), \'per_token\' (individual token vectors)\n        :type extraction_strategy: str\n        :param extraction_layer: number of layer from which the embeddings shall be extracted. Default: -1 (very last layer).\n        :type extraction_layer: int\n        :param s3e_stats: Stats of a fitted S3E model as returned by `fit_s3e_on_corpus()`\n                          (only needed for task_type=""embeddings"" and extraction_strategy = ""s3e"")\n        :type s3e_stats: dict\n        :param num_processes: the number of processes for `multiprocessing.Pool`. Set to value of 0 to disable\n                              multiprocessing. Set to None to let Inferencer use all CPU cores. If you want to\n                              debug the Language Model, you might need to disable multiprocessing!\n        :type num_processes: int\n        :param disable_tqdm: Whether to disable tqdm logging (can get very verbose in multiprocessing)\n        :type disable_tqdm: bool\n        :return: An instance of the Inferencer.\n\n        """"""\n\n        device, n_gpu = initialize_device_settings(use_cuda=gpu, local_rank=-1, use_amp=None)\n        name = os.path.basename(model_name_or_path)\n\n        # a) either from local dir\n        if os.path.exists(model_name_or_path):\n            model = BaseAdaptiveModel.load(load_dir=model_name_or_path, device=device, strict=strict)\n            if task_type == ""embeddings"":\n                processor = InferenceProcessor.load_from_dir(model_name_or_path)\n            else:\n                processor = Processor.load_from_dir(model_name_or_path)\n\n            # override processor attributes loaded from config file with inferencer params\n            processor.max_seq_len = max_seq_len\n            if hasattr(processor, ""doc_stride""):\n                processor.doc_stride = doc_stride\n\n        # b) or from remote transformers model hub\n        else:\n            logger.info(f""Could not find `{model_name_or_path}` locally. Try to download from model hub ..."")\n            if not task_type:\n                raise ValueError(""Please specify the \'task_type\' of the model you want to load from transformers. ""\n                                 ""Valid options for arg `task_type`:""\n                                 ""\'question_answering\', \'embeddings\', \'text_classification\', \'ner\'"")\n\n            model = AdaptiveModel.convert_from_transformers(model_name_or_path, device, task_type)\n            config = AutoConfig.from_pretrained(model_name_or_path)\n            tokenizer = Tokenizer.load(model_name_or_path)\n\n            # TODO infer task_type automatically from config (if possible)\n            if task_type == ""question_answering"":\n                processor = SquadProcessor(\n                    tokenizer=tokenizer,\n                    max_seq_len=max_seq_len,\n                    label_list=[""start_token"", ""end_token""],\n                    metric=""squad"",\n                    data_dir=""data"",\n                    doc_stride=doc_stride\n                )\n            elif task_type == ""embeddings"":\n                processor = InferenceProcessor(tokenizer=tokenizer, max_seq_len=max_seq_len)\n\n            elif task_type == ""text_classification"":\n                label_list = list(config.id2label[id] for id in range(len(config.id2label)))\n                processor = TextClassificationProcessor(tokenizer=tokenizer,\n                                                        max_seq_len=max_seq_len,\n                                                        data_dir=""data"",\n                                                        label_list=label_list,\n                                                        label_column_name=""label"",\n                                                        metric=""acc"",\n                                                        quote_char=\'""\',\n                                                        )\n            elif task_type == ""ner"":\n                label_list = list(config.label2id.keys())\n                processor = NERProcessor(\n                    tokenizer=tokenizer, max_seq_len=max_seq_len, data_dir=""data"", metric=""seq_f1"",\n                    label_list=label_list\n                )\n            else:\n                raise ValueError(f""`task_type` {task_type} is not supported yet. ""\n                                 f""Valid options for arg `task_type`: \'question_answering\', ""\n                                 f""\'embeddings\', \'text_classification\', \'ner\'"")\n\n        return cls(\n            model,\n            processor,\n            task_type=task_type,\n            batch_size=batch_size,\n            gpu=gpu,\n            name=name,\n            return_class_probs=return_class_probs,\n            extraction_strategy=extraction_strategy,\n            extraction_layer=extraction_layer,\n            s3e_stats=s3e_stats,\n            num_processes=num_processes,\n            disable_tqdm=disable_tqdm\n        )\n\n    def _set_multiprocessing_pool(self, num_processes):\n        """"""\n        Initialize a multiprocessing.Pool for instances of Inferencer.\n\n         :param num_processes: the number of processes for `multiprocessing.Pool`. Set to value of 0 to disable\n                               multiprocessing. Set to None to let Inferencer use all CPU cores. If you want to\n                               debug the Language Model, you might need to disable multiprocessing!\n        :type num_processes: int\n        :return:\n        """"""\n        self.process_pool = None\n        if num_processes == 0:  # disable multiprocessing\n            self.process_pool = None\n        else:\n            if num_processes is None:  # use all CPU cores\n                num_processes = mp.cpu_count() - 1\n            self.process_pool = mp.Pool(processes=num_processes)\n            logger.info(\n                f""Got ya {num_processes} parallel workers to do inference ...""\n            )\n            log_ascii_workers(n=num_processes,logger=logger)\n\n    def save(self, path):\n        self.model.save(path)\n        self.processor.save(path)\n\n    def inference_from_file(self, file, multiprocessing_chunksize=None, streaming=False, return_json=True):\n        """"""\n        Run down-stream inference on samples created from an input file.\n        The file should be in the same format as the ones used during training\n        (e.g. squad style for QA, tsv for doc classification ...) as the same Processor will be used for conversion.\n\n        :param file: path of the input file for Inference\n        :type file: str\n        :param multiprocessing_chunksize: number of dicts to put together in one chunk and feed to one process\n        :type multiprocessing_chunksize: int\n        :param streaming: return a Python generator object that yield results as they get computed, instead of\n                          blocking for all the results. To use streaming, the dicts parameter must be a generator\n                          and num_processes argument must be set. This mode can be useful to implement large scale\n                          non-blocking inference pipelines.\n        :type streaming: bool\n\n        :return: an iterator(list or generator) of predictions\n        :rtype: iter\n        """"""\n        dicts = self.processor.file_to_dicts(file)\n        preds_all = self.inference_from_dicts(\n            dicts,\n            return_json=return_json,\n            multiprocessing_chunksize=multiprocessing_chunksize,\n            streaming=streaming,\n        )\n        if streaming:\n            return preds_all\n        else:\n            return list(preds_all)\n\n    def inference_from_dicts(\n        self, dicts, return_json=True, multiprocessing_chunksize=None, streaming=False\n    ):\n        """"""\n        Runs down-stream inference on samples created from input dictionaries.\n        The format of the input `dicts` depends on the task:\n\n        * QA (SQuAD style):    [{""qas"": [""What is X?""], ""context"":  ""Some context containing the answer""}]\n        * QA (FARM style): [{""questions"": [""What is X?""], ""text"":  ""Some context containing the answer""}]\n        * Classification / NER / embeddings: [{""text"": ""Some input text""}]\n\n        Inferencer has a high performance non-blocking streaming mode for large scale inference use cases. With this\n        mode, the dicts parameter can optionally be a Python generator object that yield dicts, thus avoiding loading\n        dicts in memory. The inference_from_dicts() method returns a generator that yield predictions. To use streaming,\n        set the streaming param to True and determine optimal multiprocessing_chunksize by performing speed benchmarks.\n\n\n        :param dicts: Samples to run inference on provided as a list(or a generator object) of dicts.\n                      One dict per sample.\n        :type dicts: iter(dict)\n        :param return_json: Whether the output should be in a json appropriate format. If False, it returns the prediction\n                            object where applicable, else it returns PredObj.to_json()\n        :type return_json: bool\n        :return: dict of predictions\n        :param multiprocessing_chunksize: number of dicts to put together in one chunk and feed to one process\n                                          (only relevant if you do multiprocessing)\n        :type multiprocessing_chunksize: int\n        :param streaming: return a Python generator object that yield results as they get computed, instead of blocking\n                          for all the results. To use streaming, the dicts parameter must be a generator and\n                          num_processes argument must be set. This mode can be useful to implement large scale\n                          non-blocking inference pipelines.\n        :type streaming: bool\n\n        :return: an iterator(list or generator) of predictions\n        :rtype: iter\n        """"""\n\n        # whether to aggregate predictions across different samples (e.g. for QA on long texts)\n        aggregate_preds = False\n        if len(self.model.prediction_heads) > 0:\n            aggregate_preds = hasattr(self.model.prediction_heads[0], ""aggregate_preds"")\n\n        if self.process_pool is None:  # multiprocessing disabled (helpful for debugging or using in web frameworks)\n            predictions = self._inference_without_multiprocessing(dicts, return_json, aggregate_preds)\n            return predictions\n        else:  # use multiprocessing for inference\n            # Calculate values of multiprocessing_chunksize and num_processes if not supplied in the parameters.\n            # The calculation of the values is based on whether streaming mode is enabled. This is only for speed\n            # optimization and do not impact the results of inference.\n            if streaming:\n                if multiprocessing_chunksize is None:\n                    logger.warning(""Streaming mode is enabled for the Inferencer but multiprocessing_chunksize is not ""\n                                   ""supplied. Continuing with a default value of 20. Perform benchmarking on your data ""\n                                   ""to get the optimal chunksize."")\n                    multiprocessing_chunksize = 20\n            else:\n                if multiprocessing_chunksize is None:\n                    _chunk_size, _ = calc_chunksize(len(dicts))\n                    multiprocessing_chunksize = _chunk_size\n\n            predictions = self._inference_with_multiprocessing(\n                dicts, return_json, aggregate_preds, multiprocessing_chunksize,\n            )\n\n            # return a generator object if streaming is enabled, else, cast the generator to a list.\n            if not streaming and type(predictions) != list:\n                return list(predictions)\n            else:\n                return predictions\n\n    def _inference_without_multiprocessing(self, dicts, return_json, aggregate_preds):\n        """"""\n        Implementation of inference from dicts without using Python multiprocessing. Useful for debugging or in API\n        framework where spawning new processes could be expensive.\n\n        :param dicts: Samples to run inference on provided as a list of dicts. One dict per sample.\n        :type dicts: iter(dict)\n        :param return_json: Whether the output should be in a json appropriate format. If False, it returns the prediction\n                            object where applicable, else it returns PredObj.to_json()\n        :type return_json: bool\n        :param aggregate_preds: whether to aggregate predictions across different samples (e.g. for QA on long texts)\n        :type aggregate_preds: bool\n\n        :return: list of predictions\n        :rtype: list\n        """"""\n        dataset, tensor_names, baskets = self.processor.dataset_from_dicts(\n            dicts, indices=[i for i in range(len(dicts))], return_baskets=True\n        )\n        # TODO change format of formatted_preds in QA (list of dicts)\n        if aggregate_preds:\n            preds_all = self._get_predictions_and_aggregate(dataset, tensor_names, baskets)\n        else:\n            preds_all = self._get_predictions(dataset, tensor_names, baskets)\n\n        if return_json:\n            # TODO this try catch should be removed when all tasks return prediction objects\n            try:\n                preds_all = [x.to_json() for x in preds_all]\n            except AttributeError:\n                pass\n\n        return preds_all\n\n    def _inference_with_multiprocessing(\n        self, dicts, return_json, aggregate_preds, multiprocessing_chunksize\n    ):\n        """"""\n        Implementation of inference. This method is a generator that yields the results.\n\n        :param dicts: Samples to run inference on provided as a list of dicts or a generator object that yield dicts.\n        :type dicts: iter(dict)\n        :param return_json: Whether the output should be in a json appropriate format. If False, it returns the prediction\n                            object where applicable, else it returns PredObj.to_json()\n        :type return_json: bool\n        :param aggregate_preds: whether to aggregate predictions across different samples (e.g. for QA on long texts)\n        :type aggregate_preds: bool\n        :param multiprocessing_chunksize: number of dicts to put together in one chunk and feed to one process\n        :type multiprocessing_chunksize: int\n        :return: generator object that yield predictions\n        :rtype: iter\n        """"""\n\n        # We group the input dicts into chunks and feed each chunk to a different process\n        # in the pool, where it gets converted to a pytorch dataset\n        results = self.process_pool.imap(\n            partial(self._create_datasets_chunkwise, processor=self.processor),\n            grouper(iterable=dicts, n=multiprocessing_chunksize),\n            1,\n        )\n\n        # Once a process spits out a preprocessed chunk. we feed this dataset directly to the model.\n        # So we don\'t need to wait until all preprocessing has finished before getting first predictions.\n        for dataset, tensor_names, baskets in results:\n            # TODO change format of formatted_preds in QA (list of dicts)\n            if aggregate_preds:\n                predictions = self._get_predictions_and_aggregate(\n                    dataset, tensor_names, baskets\n                )\n            else:\n                predictions = self._get_predictions(dataset, tensor_names, baskets)\n\n            if return_json:\n                # TODO this try catch should be removed when all tasks return prediction objects\n                try:\n                    predictions = [x.to_json() for x in predictions]\n                except AttributeError:\n                    pass\n            yield from predictions\n\n    @classmethod\n    def _create_datasets_chunkwise(cls, chunk, processor):\n        """"""Convert ONE chunk of data (i.e. dictionaries) into ONE pytorch dataset.\n        This is usually executed in one of many parallel processes.\n        The resulting datasets of the processes are merged together afterwards""""""\n        dicts = [d[1] for d in chunk]\n        indices = [d[0] for d in chunk]\n        dataset, tensor_names, baskets = processor.dataset_from_dicts(dicts, indices, return_baskets=True)\n        return dataset, tensor_names, baskets\n\n    def _get_predictions(self, dataset, tensor_names, baskets):\n        """"""\n        Feed a preprocessed dataset to the model and get the actual predictions (forward pass + formatting).\n\n        :param dataset: PyTorch Dataset with samples you want to predict\n        :param tensor_names: Names of the tensors in the dataset\n        :param baskets: For each item in the dataset, we need additional information to create formatted preds.\n                        Baskets contain all relevant infos for that.\n                        Example: QA - input string to convert the predicted answer from indices back to string space\n        :param rest_api_schema: Whether input dicts use the format that complies with the FARM REST API.\n                                Currently only used for QA to switch from squad to a more useful format in production.\n                                While input is almost the same, output contains additional meta data(offset, context..)\n        :type rest_api_schema: bool\n        :return: list of predictions\n        """"""\n        samples = [s for b in baskets for s in b.samples]\n\n        data_loader = NamedDataLoader(\n            dataset=dataset, sampler=SequentialSampler(dataset), batch_size=self.batch_size, tensor_names=tensor_names\n        )\n        preds_all = []\n        for i, batch in enumerate(tqdm(data_loader, desc=f""Inferencing Samples"", unit="" Batches"", disable=self.disable_tqdm)):\n            batch = {key: batch[key].to(self.device) for key in batch}\n            batch_samples = samples[i * self.batch_size : (i + 1) * self.batch_size]\n\n            # get logits\n            with torch.no_grad():\n                logits = self.model.forward(**batch)[0]\n                preds = self.model.formatted_preds(\n                    logits=[logits],\n                    samples=batch_samples,\n                    tokenizer=self.processor.tokenizer,\n                    return_class_probs=self.return_class_probs,\n                    **batch)\n                preds_all += preds\n        return preds_all\n\n    def _get_predictions_and_aggregate(self, dataset, tensor_names, baskets):\n        """"""\n        Feed a preprocessed dataset to the model and get the actual predictions (forward pass + logits_to_preds + formatted_preds).\n\n        Difference to _get_predictions():\n         - Additional aggregation step across predictions of individual samples\n         (e.g. For QA on long texts, we extract answers from multiple passages and then aggregate them on the ""document level"")\n\n        :param dataset: PyTorch Dataset with samples you want to predict\n        :param tensor_names: Names of the tensors in the dataset\n        :param baskets: For each item in the dataset, we need additional information to create formatted preds.\n                        Baskets contain all relevant infos for that.\n                        Example: QA - input string to convert the predicted answer from indices back to string space\n        :param rest_api_schema: Whether input dicts use the format that complies with the FARM REST API.\n                                Currently only used for QA to switch from squad to a more useful format in production.\n                                While input is almost the same, output contains additional meta data(offset, context..)\n        :type rest_api_schema: bool\n        :return: list of predictions\n        """"""\n\n        data_loader = NamedDataLoader(\n            dataset=dataset, sampler=SequentialSampler(dataset), batch_size=self.batch_size, tensor_names=tensor_names\n        )\n        # TODO Sometimes this is the preds of one head, sometimes of two. We need a more advanced stacking operation\n        # TODO so that preds of the right shape are passed in to formatted_preds\n        unaggregated_preds_all = []\n\n        for i, batch in enumerate(tqdm(data_loader, desc=f""Inferencing Samples"", unit="" Batches"", disable=self.disable_tqdm)):\n\n            batch = {key: batch[key].to(self.device) for key in batch}\n\n            # get logits\n            with torch.no_grad():\n                # Aggregation works on preds, not logits. We want as much processing happening in one batch + on GPU\n                # So we transform logits to preds here as well\n                logits = self.model.forward(**batch)\n                # preds = self.model.logits_to_preds(logits, **batch)[0] (This must somehow be useful for SQuAD)\n                preds = self.model.logits_to_preds(logits, **batch)\n                unaggregated_preds_all.append(preds)\n\n        # In some use cases we want to aggregate the individual predictions.\n        # This is mostly useful, if the input text is longer than the max_seq_len that the model can process.\n        # In QA we can use this to get answers from long input texts by first getting predictions for smaller passages\n        # and then aggregating them here.\n\n        # At this point unaggregated preds has shape [n_batches][n_heads][n_samples]\n\n        # can assume that we have only complete docs i.e. all the samples of one doc are in the current chunk\n        logits = [None]\n        preds_all = self.model.formatted_preds(logits=logits, # For QA we collected preds per batch and do not want to pass logits\n                                               preds_p=unaggregated_preds_all,\n                                               baskets=baskets)\n        return preds_all\n\n    def extract_vectors(self, dicts, extraction_strategy=""cls_token"", extraction_layer=-1):\n        """"""\n        Converts a text into vector(s) using the language model only (no prediction head involved).\n\n        Example:\n            basic_texts = [{""text"": ""Some text we want to embed""}, {""text"": ""And a second one""}]\n            result = inferencer.extract_vectors(dicts=basic_texts)\n\n        :param dicts: Samples to run inference on provided as a list of dicts. One dict per sample.\n        :type dicts: [dict]\n        :param extraction_strategy: Strategy to extract vectors. Choices: \'cls_token\' (sentence vector), \'reduce_mean\'\n                               (sentence vector), reduce_max (sentence vector), \'per_token\' (individual token vectors)\n        :type extraction_strategy: str\n        :param extraction_layer: number of layer from which the embeddings shall be extracted. Default: -1 (very last layer).\n        :type extraction_layer: int\n        :return: dict of predictions\n        """"""\n\n        logger.warning(""Deprecated! Please use Inferencer.inference_from_dicts() instead."")\n        self.model.prediction_heads = torch.nn.ModuleList([])\n        self.model.language_model.extraction_layer = extraction_layer\n        self.model.language_model.extraction_strategy = extraction_strategy\n\n        return self.inference_from_dicts(dicts)\n\n\nclass FasttextInferencer:\n    def __init__(self, model, name=None):\n        self.model = model\n        self.name = name if name != None else f""anonymous-fasttext""\n        self.prediction_type = ""embedder""\n\n    @classmethod\n    def load(cls, load_dir, batch_size=4, gpu=False):\n        import fasttext\n\n        if os.path.isfile(load_dir):\n            return cls(model=fasttext.load_model(load_dir))\n        else:\n            logger.error(f""Fasttext model file does not exist at: {load_dir}"")\n\n    def extract_vectors(self, dicts, extraction_strategy=""reduce_mean""):\n        """"""\n        Converts a text into vector(s) using the language model only (no prediction head involved).\n\n        :param dicts: Samples to run inference on provided as a list of dicts. One dict per sample.\n        :type dicts: [dict]\n        :param extraction_strategy: Strategy to extract vectors. Choices: \'reduce_mean\' (mean sentence vector), \'reduce_max\' (max per embedding dim), \'CLS\'\n        :type extraction_strategy: str\n        :return: dict of predictions\n        """"""\n\n        preds_all = []\n        for d in dicts:\n            pred = {}\n            pred[""context""] = d[""text""]\n            if extraction_strategy == ""reduce_mean"":\n                pred[""vec""] = self.model.get_sentence_vector(d[""text""])\n            else:\n                raise NotImplementedError\n            preds_all.append(pred)\n\n        return preds_all\n'"
farm/inference_rest_api.py,0,"b'import json\nimport logging\nfrom pathlib import Path\n\nimport numpy as np\nfrom flask import Flask, request, make_response\nfrom flask_cors import CORS\nfrom flask_restplus import Api, Resource\n\nfrom farm.infer import Inferencer\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(\n    format=""%(asctime)s %(levelname)-8s %(message)s"",\n    level=""INFO"",\n    datefmt=""%Y-%m-%d %H:%M:%S"",\n)\n\nMODELS_DIRS = [""saved_models"", ""base_models""]\n\nmodel_paths = []\nfor model_dir in MODELS_DIRS:\n    path = Path(model_dir)\n    if path.is_dir():\n        models = [f for f in path.iterdir() if f.is_dir()]\n        model_paths.extend(models)\n\nINFERENCERS = {}\nfor idx, model_dir in enumerate(model_paths):\n    INFERENCERS[idx + 1] = Inferencer.load(str(model_dir), num_processes=0)\n\napp = Flask(__name__)\nCORS(app)\napi = Api(app, debug=True, validate=True, version=""1.0"", title=""FARM NLP APIs"")\napp.config[""JSON_SORT_KEYS""] = True\napp.config[""RESTPLUS_VALIDATE""] = True\n\n\n@api.route(""/models"")\nclass ModelListEndpoint(Resource):\n    def get(self):\n        resp = []\n\n        for idx, model in INFERENCERS.items():\n\n            #TODO UI still relies on the old prediction_type attribute, but we should switch this to inferencer.task_type\n            prediction_type = model.model.prediction_heads[0].model_type\n\n            _res = {\n                ""id"": idx,\n                ""name"": model.name,\n                ""prediction_type"": prediction_type,\n                ""language"": model.language,\n            }\n            resp.append(_res)\n\n        return resp\n\n\nclass NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        if isinstance(obj, np.float32):\n            return str(obj)\n        return json.JSONEncoder.default(self, obj)\n\n\n@api.representation(""application/json"")\ndef resp_json(data, code, headers=None):\n    resp = make_response(json.dumps(data, cls=NumpyEncoder), code)\n    resp.headers.extend(headers or {})\n    return resp\n\n\n@api.route(""/models/<int:model_id>/inference"")\nclass InferenceEndpoint(Resource):\n    def post(self, model_id):\n        model = INFERENCERS.get(model_id, None)\n        if not model:\n            return ""Model not found"", 404\n\n        dicts = request.get_json().get(""input"", None)\n        if not dicts:\n            return {}\n        results = model.inference_from_dicts(dicts=dicts, rest_api_schema=True)\n        return results[0]\n\n\nif __name__ == ""__main__"":\n    app.run(host=""0.0.0.0"")\n'"
farm/train.py,6,"b'import logging\nimport sys\nimport torch\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy\nimport shutil\nimport dill\n\nfrom farm.utils import MLFlowLogger as MlLogger\nfrom farm.utils import GracefulKiller\nfrom farm.eval import Evaluator\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.visual.ascii.images import GROWING_TREE\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.optimization import get_scheduler\n\ntry:\n    from apex import amp\n    AMP_AVAILABLE = True\nexcept ImportError:\n    AMP_AVAILABLE = False\n\nlogger = logging.getLogger(__name__)\n\n\nclass EarlyStopping:\n    """"""\n    Can be used to control early stopping with a Trainer class. Any object can be used instead which\n    implements the method check_stopping and and provides the attribute save_dir\n    """"""\n\n    def __init__(\n            self,\n            metric=""loss"",\n            save_dir=None,\n            mode=""min"",\n            patience=0,\n            min_delta=0.001,\n            min_evals=0,\n    ):\n        """"""\n        :param save_dir: the directory where to save the final best model, if None, no saving.\n        :param metric: name of dev set metric to monitor (default: loss) to get extracted from the 0th head or\n                       a function that extracts a value from the trainer dev evaluation result.\n                       NOTE: this is different from the metric to get specified for the processor which defines how\n                       to calculate one or more evaluation matric values from prediction/target sets, while this\n                       specifies the name of one particular such metric value or a method to calculate that value\n                       from the result returned from a processor metric.\n        :param mode: ""min"" or ""max""\n        :param patience: how many evaluations to wait after the best evaluation to stop\n        :param min_delta: minimum difference to a previous best value to count as an improvement.\n        :param min_evals: minimum number of evaluations to wait before using eval value\n        """"""\n\n        self.metric = metric\n        self.save_dir = save_dir\n        self.mode = mode\n        self.patience = patience\n        self.min_delta = min_delta\n        self.min_evals = min_evals\n        self.eval_values = []  # for more complex modes\n        self.n_since_best = None\n        if mode == ""min"":\n            self.best_so_far = 1.0E99\n        elif mode == ""max"":\n            self.best_so_far = -1.0E99\n        else:\n            raise Exception(""Mode must be \'min\' or \'max\'"")\n\n    def check_stopping(self, eval_result):\n        """"""\n        Provide the evaluation value for the current evaluation. Returns true if stopping should occur.\n        This will save the model, if necessary.\n\n        :param eval: the current evaluation result\n        :return: a tuple (stopprocessing, savemodel, evalvalue) indicating if processing should be stopped\n                 and if the current model should get saved and the evaluation value used.\n        """"""\n\n        if isinstance(self.metric, str):\n            eval_value = eval_result[0][self.metric]\n        else:\n            eval_value = self.metric(eval_result)\n        self.eval_values.append(float(eval_value))\n        stopprocessing, savemodel = False, False\n        if len(self.eval_values) <= self.min_evals:\n            return stopprocessing, savemodel\n        if self.mode == ""min"":\n            delta = self.best_so_far - eval_value\n        else:\n            delta = eval_value - self.best_so_far\n        if delta > self.min_delta:\n            self.best_so_far = eval_value\n            self.n_since_best = 0\n            if self.save_dir:\n                savemodel = True\n        else:\n            self.n_since_best += 1\n        if self.n_since_best > self.patience:\n            stopprocessing = True\n        return stopprocessing, savemodel, eval_value\n\n\nclass Trainer:\n    """"""Handles the main model training procedure. This includes performing evaluation on the dev set at regular\n    intervals during training as well as evaluation on the test set at the end of training.""""""\n\n    def __init__(\n        self,\n        model,\n        optimizer,\n        data_silo,\n        epochs,\n        n_gpu,\n        device,\n        lr_schedule=None,\n        evaluate_every=100,\n        eval_report=True,\n        use_amp=None,\n        grad_acc_steps=1,\n        local_rank=-1,\n        early_stopping=None,\n        log_learning_rate=False,\n        checkpoint_on_sigterm=False,\n        checkpoint_every=None,\n        checkpoint_root_dir=None,\n        checkpoints_to_keep=3,\n        from_epoch=0,\n        from_step=0,\n        global_step=0,\n        evaluator_test=True,\n    ):\n        """"""\n        :param optimizer: An optimizer object that determines the learning strategy to be used during training\n        :param data_silo: A DataSilo object that will contain the train, dev and test datasets as PyTorch DataLoaders\n        :type data_silo: DataSilo\n        :param epochs: How many times the training procedure will loop through the train dataset\n        :type epochs: int\n        :param n_gpu: The number of gpus available for training and evaluation.\n        :type n_gpu: int\n        :param device: The device on which the train, dev and test tensors should be hosted. Choose from ""cpu"" and ""cuda"".\n        :param lr_schedule: An optional scheduler object that can regulate the learning rate of the optimizer\n        :param evaluate_every: Perform dev set evaluation after this many steps of training.\n        :type evaluate_every: int\n        :param eval_report: If evaluate_every is not 0, specifies if an eval report should be generated when evaluating\n        :type eval_report: bool\n        :param use_amp: Whether to use automatic mixed precision with Apex. One of the optimization levels must be chosen.\n                        ""O1"" is recommended in almost all cases.\n        :type use_amp: str\n        :param grad_acc_steps: TODO\n        :type grad_acc_steps: int\n        :param local_rank: TODO\n        :type local_rank: int\n        :param early_stopping: an initialized EarlyStopping object to control early stopping and saving of best models.\n        :type early_stopping: EarlyStopping\n        :param log_learning_rate: Whether to log learning rate to Mlflow\n        :type log_learning_rate: bool\n        :param checkpoint_on_sigterm: save a checkpoint for the Trainer when a SIGTERM signal is sent. The checkpoint\n               can be used to resume training. It is useful in frameworks like AWS SageMaker with Spot instances where\n               a SIGTERM notifies to save the training state and subsequently the instance is terminated.\n        :type checkpoint_on_sigterm: bool\n        :param checkpoint_every: save a train checkpoint after this many steps of training.\n        :type checkpoint_every: int\n        :param checkpoint_root_dir: the Path of directory where all train checkpoints are saved. For each individual\n               checkpoint, a subdirectory with the name epoch_{epoch_num}_step_{step_num} is created.\n        :type checkpoint_root_dir: Path\n        :param checkpoints_to_keep: maximum number of train checkpoints to save.\n        :type checkpoints_to_keep: int\n        :param from_epoch: the epoch number to start the training from. In the case when training resumes from a saved\n               checkpoint, it is used to fast-forward training to the last epoch in the checkpoint.\n        :type from_epoch: int\n        :param from_step: the step number to start the training from. In the case when training resumes from a saved\n               checkpoint, it is used to fast-forward training to the last step in the checkpoint.\n        :type from_step: int\n        :param global_step: the global step number across the training epochs.\n        :type global_step: int\n        :param evaluator_test: whether to perform evaluation on the test set\n        :type evaluator_test: bool\n        """"""\n\n        self.model = model\n        self.data_silo = data_silo\n        self.epochs = int(epochs)\n        self.optimizer = optimizer\n        self.evaluate_every = evaluate_every\n        self.eval_report = eval_report\n        self.n_gpu = n_gpu\n        self.grad_acc_steps = grad_acc_steps\n        self.use_amp = use_amp\n        self.lr_schedule = lr_schedule\n        self.device = device\n        self.local_rank = local_rank\n        self.log_params()\n        self.early_stopping = early_stopping\n        self.log_learning_rate = log_learning_rate\n        self.evaluator_test = evaluator_test\n\n        if use_amp and not AMP_AVAILABLE:\n            raise ImportError(f\'Got use_amp = {use_amp}, but cannot find apex. \'\n                              \'Please install Apex if you want to make use of automatic mixed precision. \'\n                              \'https://github.com/NVIDIA/apex\')\n        self.checkpoint_on_sigterm = checkpoint_on_sigterm\n        if checkpoint_on_sigterm:\n            self.sigterm_handler = GracefulKiller()\n        else:\n            self.sigterm_handler = None\n        self.checkpoint_root_dir = checkpoint_root_dir\n        self.checkpoints_to_keep = checkpoints_to_keep\n        self.checkpoint_every = checkpoint_every\n        if self.checkpoint_every and not checkpoint_root_dir:\n            raise Exception(""checkpoint_path needs to be supplied when using checkpoint_every."")\n        if checkpoint_on_sigterm and not checkpoint_root_dir:\n            raise Exception(""checkpoint_path needs to be supplied when using checkpoint_on_sigterm."")\n\n        self.from_epoch = from_epoch\n        self.from_step = from_step\n        self.global_step = global_step\n\n    def train(self):\n        """"""\n        Perform the training procedure.\n\n        The training is visualized by a progress bar. It counts the epochs in a zero based manner.\n        For example, when you specify ``epochs=20`` it starts to count from 0 to 19.\n        """"""\n\n        # connect the prediction heads with the right output from processor\n        self.model.connect_heads_with_processor(self.data_silo.processor.tasks, require_labels=True)\n\n        # Check that the tokenizer fits the language model\n        self.model.verify_vocab_size(vocab_size=len(self.data_silo.processor.tokenizer))\n\n        logger.info(f""\\n {GROWING_TREE}"")\n        self.model.train()\n\n        do_stopping = False\n        evalnr = 0\n        loss = 0\n\n        resume_from_step = self.from_step\n\n        for epoch in range(self.from_epoch, self.epochs):\n            self.from_epoch = epoch\n            train_data_loader = self.data_silo.get_data_loader(""train"")\n            progress_bar = tqdm(train_data_loader)\n            for step, batch in enumerate(progress_bar):\n                # when resuming training from a checkpoint, we want to fast forward to the step of the checkpoint\n                if resume_from_step and step <= resume_from_step:\n                    if resume_from_step == step:\n                        resume_from_step = None\n                    continue\n\n                progress_bar.set_description(f""Train epoch {epoch}/{self.epochs-1} (Cur. train loss: {loss:.4f})"")\n\n                # Move batch of samples to device\n                batch = {key: batch[key].to(self.device) for key in batch}\n\n                # Forward pass through model\n                logits = self.model.forward(**batch)\n                per_sample_loss = self.model.logits_to_loss(logits=logits, global_step=self.global_step, **batch)\n\n                loss = self.backward_propagate(per_sample_loss, step)\n\n                # Perform  evaluation\n                if self.evaluate_every != 0 and self.global_step % self.evaluate_every == 0 and self.global_step != 0:\n                    # When using StreamingDataSilo, each evaluation creates a new instance of\n                    # dev_data_loader. In cases like training from scratch, this could cause\n                    # some variance across evaluators due to the randomness in word masking.\n                    dev_data_loader = self.data_silo.get_data_loader(""dev"")\n                    if dev_data_loader is not None:\n                        evaluator_dev = Evaluator(\n                            data_loader=dev_data_loader, tasks=self.data_silo.processor.tasks, device=self.device, report=self.eval_report\n                        )\n                        evalnr += 1\n                        result = evaluator_dev.eval(self.model)\n                        evaluator_dev.log_results(result, ""Dev"", self.global_step)\n                        if self.early_stopping:\n                            do_stopping, save_model, eval_value = self.early_stopping.check_stopping(result)\n                            if save_model:\n                                logger.info(\n                                    ""Saving current best model to {}, eval={}"".format(\n                                        self.early_stopping.save_dir, eval_value))\n                                self.model.save(self.early_stopping.save_dir)\n                                self.data_silo.processor.save(self.early_stopping.save_dir)\n                            if do_stopping:\n                                # log the stopping\n                                logger.info(""STOPPING EARLY AT EPOCH {}, STEP {}, EVALUATION {}"".format(epoch, step, evalnr))\n                if do_stopping:\n                    break\n                self.global_step += 1\n                self.from_step = step\n\n                # save the current state as a checkpoint before exiting if a SIGTERM signal is received\n                if self.sigterm_handler and self.sigterm_handler.kill_now:\n                    logger.info(""Received a SIGTERM signal. Saving the current train state as a checkpoint ..."")\n                    self._save()\n                    sys.exit(0)\n\n                # save a checkpoint and continue train\n                if self.checkpoint_every and step % self.checkpoint_every == 0:\n                    self._save()\n\n            if do_stopping:\n                break\n\n        # With early stopping we want to restore the best model\n        if self.early_stopping and self.early_stopping.save_dir:\n            logger.info(""Restoring best model so far from {}"".format(self.early_stopping.save_dir))\n            lm_name = self.model.language_model.name\n            model = AdaptiveModel.load(self.early_stopping.save_dir, self.device, lm_name=lm_name)\n            model.connect_heads_with_processor(self.data_silo.processor.tasks, require_labels=True)\n\n        # Eval on test set\n        if self.evaluator_test:\n            test_data_loader = self.data_silo.get_data_loader(""test"")\n            if test_data_loader is not None:\n                evaluator_test = Evaluator(\n                    data_loader=test_data_loader, tasks=self.data_silo.processor.tasks, device=self.device\n                )\n                result = evaluator_test.eval(self.model)\n                evaluator_test.log_results(result, ""Test"", self.global_step)\n        return self.model\n\n    def backward_propagate(self, loss, step):\n        loss = self.adjust_loss(loss)\n        if self.global_step % 10 == 1:\n            MlLogger.log_metrics(\n                {""Train_loss_total"": float(loss.detach().cpu().numpy())},\n                step=self.global_step,\n            )\n        if self.use_amp:\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n\n        if self.log_learning_rate:\n            MlLogger.log_metrics({""learning_rate"": self.lr_schedule.get_lr()[0]}, step=self.global_step)\n\n        if step % self.grad_acc_steps == 0:\n            # TODO We might wanna add gradient clipping here\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n            if self.lr_schedule:\n                self.lr_schedule.step()\n        return loss\n\n    def adjust_loss(self, loss):\n        loss = loss.mean()\n        if self.grad_acc_steps > 1:\n            loss = loss / self.grad_acc_steps\n        return loss\n\n    def log_params(self):\n        params = {""epochs"": self.epochs, ""n_gpu"": self.n_gpu, ""device"": self.device}\n        MlLogger.log_params(params)\n\n    @classmethod\n    def create_or_load_checkpoint(cls, data_silo, checkpoint_root_dir, resume_from_checkpoint=""latest"", **kwargs):\n        """"""\n        Try loading a saved Trainer checkpoint. If no checkpoint found, it creates a new instance of Trainer.\n\n        :param data_silo: A DataSilo object that will contain the train, dev and test datasets as PyTorch DataLoaders\n        :type data_silo: DataSilo\n        :param checkpoint_root_dir: Path of the directory where all train checkpoints are saved. Each individual\n               checkpoint is stored in a sub-directory under it.\n        :type checkpoint_root_dir: Path\n        :param resume_from_checkpoint: the checkpoint name to start training from, e.g., ""epoch_1_step_4532"". It\n               defaults to ""latest"", using the checkpoint with the highest train steps.\n        :type resume_from_checkpoint: str\n        """"""\n        checkpoint_to_load = None\n        if checkpoint_root_dir.exists():\n            if resume_from_checkpoint == ""latest"":\n                saved_checkpoints = cls._get_checkpoints(checkpoint_root_dir)\n                if saved_checkpoints:\n                    checkpoint_to_load = saved_checkpoints[0]  # latest checkpoint\n                else:\n                    checkpoint_to_load = None\n            else:\n                checkpoint_to_load = checkpoint_root_dir / resume_from_checkpoint\n\n        if checkpoint_to_load:\n            trainer = cls._load_checkpoint(path=checkpoint_to_load, data_silo=data_silo)\n            logging.info(f""Resuming training from the train checkpoint at {checkpoint_to_load} ..."")\n        else:\n            logging.info(f""No train checkpoints found. Starting a new training ..."")\n            trainer = Trainer(data_silo=data_silo, checkpoint_root_dir=checkpoint_root_dir, **kwargs)\n        return trainer\n\n    @classmethod\n    def _load_checkpoint(cls, path, data_silo):\n        """"""\n        Load the train checkpoint at given path.\n\n        :param path: The checkpoint path is subdirectory under checkpoint_root_dir. The individual checkpoint dirs have\n               a default naming convention of ""epoch_{epoch_num}_step_{step_num}"".\n        :type path: Path\n        :param data_silo: A DataSilo object that will contain the train, dev and test datasets as PyTorch DataLoaders\n        :type data_silo: DataSilo\n        """"""\n        if not path.exists():\n            raise Exception(f""The checkpoint path {path} does not exists."")\n\n        trainer_checkpoint = torch.load(path / ""trainer"")\n        trainer_state_dict = trainer_checkpoint[""trainer_state_dict""]\n\n        # Just setting seeds is not sufficient to have deterministic results when resuming\n        # training from a checkpoint. Additionally, the previous states of Random Number\n        # Generators also need to be restored from the saved checkpoint.\n        numpy_rng_state = trainer_checkpoint[""numpy_rng_state""]\n        numpy.random.set_state(numpy_rng_state)\n        rng_state = trainer_checkpoint[""rng_state""]\n        cuda_rng_state = trainer_checkpoint[""cuda_rng_state""]\n        torch.set_rng_state(rng_state)\n        torch.cuda.set_rng_state(cuda_rng_state)\n\n        model = trainer_checkpoint[""model""]\n\n        optimizer = trainer_checkpoint[""optimizer""]\n\n        scheduler_state_dict = trainer_checkpoint[""scheduler_state""]\n        scheduler_opts = trainer_checkpoint[""scheduler_opts""]\n        scheduler_opts[""last_epoch""] = scheduler_state_dict[""last_epoch""]\n        scheduler = get_scheduler(optimizer, scheduler_opts)\n        scheduler.load_state_dict(scheduler_state_dict)\n\n        trainer = Trainer(\n            data_silo=data_silo,\n            model=model,\n            optimizer=optimizer,\n            lr_schedule=scheduler,\n            **trainer_state_dict\n        )\n\n        logger.info(f""Loaded a train checkpoint from {path}"")\n        return trainer\n\n    @classmethod\n    def _get_checkpoints(cls, checkpoint_root_dir):\n        """"""\n        Get a list of checkpoint dirs sorted by the number of training steps.\n        """"""\n        dirs = [d for d in checkpoint_root_dir.iterdir() if d.is_dir() and d.name.startswith(""epoch"")]\n\n        checkpoints_with_epoch_and_step = []  # list of tuple(checkpoint_dir, epoch, step)\n        for d in dirs:\n            epoch, step = [int(s) for s in str(d).split(""_"") if s.isdigit()]\n            checkpoints_with_epoch_and_step.append((d, epoch, step))\n\n        sorted_checkpoints_with_epoch_and_step = sorted(checkpoints_with_epoch_and_step,\n                                                        key=lambda tup: (tup[1], tup[2]),  # sort by epoch and step\n                                                        reverse=True)\n        sorted_checkpoints = [tup[0] for tup in sorted_checkpoints_with_epoch_and_step]\n\n        return sorted_checkpoints\n\n    def _save(self):\n        """"""\n        Save a train checkpoint at the Trainer\'s checkpoint_path.\n\n        Some objects(eg, scheduler) in the Trainer are not serializable using the Pickle module. For these objects,\n        the state_dict is stored for the checkpoint, that can be used to reconstruct a similar state upon resuming\n        train from the checkpoint.\n\n        #TODO The model is currently saved as a whole serialized object. The disadvantage of this approach is that it is\n        bound to specifics Python version, FARM version, directory structures etc. A more modular and reusable approach\n        is to save using AdaptiveModel\'s save() method where the model and the state_dict are stored separately.\n\n        # TODO custom defined evaluators are not saved in the checkpoint.\n        """"""\n        logger.info(""Saving a train checkpoint ..."")\n        checkpoint_path = self.checkpoint_root_dir / ""checkpoint_in_progress""\n        checkpoint_path.mkdir(parents=True, exist_ok=True)\n\n        trainer_state_dict = self._get_state_dict()\n        self.model.save(checkpoint_path)\n        torch.save(\n            {\n                ""model"": self.model,\n                ""trainer_state_dict"": trainer_state_dict,\n                ""model_state_dict"": self.model.state_dict(),\n                ""optimizer"": self.optimizer,\n                ""scheduler_opts"": self.lr_schedule.opts,\n                ""scheduler_state"": self.lr_schedule.state_dict(),\n                ""numpy_rng_state"": numpy.random.get_state(),\n                ""rng_state"": torch.get_rng_state(),\n                ""cuda_rng_state"": torch.cuda.get_rng_state(),\n            },\n            checkpoint_path / ""trainer"",\n            pickle_module=dill,\n        )\n\n        checkpoint_name = f""epoch_{self.from_epoch}_step_{self.from_step}""\n        checkpoint_path.replace(Path(checkpoint_path.parent) / checkpoint_name)\n\n        saved_checkpoints = self._get_checkpoints(self.checkpoint_root_dir)\n        if len(saved_checkpoints) > self.checkpoints_to_keep:\n            for cp in saved_checkpoints[self.checkpoints_to_keep:]:\n                shutil.rmtree(cp)\n\n        logger.info(f""Saved a training checkpoint at {checkpoint_name}"")\n\n    def _get_state_dict(self):\n        """"""\n        Serializable state dictionary of a Trainer object\n        """"""\n        state_dict = {\n            ""evaluate_every"": self.evaluate_every,\n            ""n_gpu"": self.n_gpu,\n            ""grad_acc_steps"": self.grad_acc_steps,\n            ""device"": self.device,\n            ""local_rank"": self.local_rank,\n            ""early_stopping"": self.early_stopping,\n            ""epochs"": self.epochs,\n            ""checkpoint_on_sigterm"": self.checkpoint_on_sigterm,\n            ""checkpoint_root_dir"": self.checkpoint_root_dir,\n            ""checkpoint_every"": self.checkpoint_every,\n            ""from_epoch"": self.from_epoch,\n            ""from_step"": self.from_step,\n            ""global_step"": self.global_step,\n            ""log_learning_rate"": self.log_learning_rate,\n        }\n\n        return state_dict\n\n'"
farm/utils.py,12,"b'import hashlib\nimport json\nimport logging\nimport random\nimport os\nimport signal\nimport numpy as np\nimport torch\nfrom requests.exceptions import ConnectionError\nfrom torch import multiprocessing as mp\nimport mlflow\nfrom copy import deepcopy\nimport pandas as pd\nfrom tqdm import tqdm\n\n\nfrom farm.visual.ascii.images import WELCOME_BARN, WORKER_M, WORKER_F, WORKER_X\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef set_all_seeds(seed, deterministic_cudnn=False):\n    """"""\n    Setting multiple seeds to make runs reproducible.\n\n    Important: Enabling `deterministic_cudnn` gives you full reproducibility with CUDA,\n    but might slow down your training (see https://pytorch.org/docs/stable/notes/randomness.html#cudnn) !\n\n    :param seed:number to use as seed\n    :type seed: int\n    :param deterministic_torch: Enable for full reproducibility when using CUDA. Caution: might slow down training.\n    :type deterministic_cudnn: bool\n    :return: None\n    """"""\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\'PYTHONHASHSEED\'] = str(seed)\n    torch.cuda.manual_seed_all(seed)\n    if deterministic_cudnn:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n\ndef calc_chunksize(num_dicts, min_chunksize=4, max_chunksize=2000, max_processes=128):\n    num_cpus = min(mp.cpu_count() - 1 or 1, max_processes)  # -1 to keep a CPU core free for the main process\n    dicts_per_cpu = np.ceil(num_dicts / num_cpus)\n    # automatic adjustment of multiprocessing chunksize\n    # for small files (containing few dicts) we want small chunksize to ulitize all available cores but never less\n    # than 2, because we need it to sample another random sentence in LM finetuning\n    # for large files we want to minimize processor spawning without giving too much data to one process, so we\n    # clip it at 5k\n    multiprocessing_chunk_size = int(np.clip((np.ceil(dicts_per_cpu / 5)), a_min=min_chunksize, a_max=max_chunksize))\n    # This lets us avoid cases in lm_finetuning where a chunk only has a single doc and hence cannot pick\n    # a valid next sentence substitute from another document\n    if num_dicts != 1:\n        while num_dicts % multiprocessing_chunk_size == 1:\n            multiprocessing_chunk_size -= -1\n    dict_batches_to_process = int(num_dicts / multiprocessing_chunk_size)\n    num_processes = min(num_cpus, dict_batches_to_process) or 1\n\n    return multiprocessing_chunk_size, num_processes\n\n\ndef initialize_device_settings(use_cuda, local_rank=-1, use_amp=None):\n    if not use_cuda:\n        device = torch.device(""cpu"")\n        n_gpu = 0\n    elif local_rank == -1:\n        device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n        if not torch.cuda.is_available():\n            n_gpu = 0\n        else:\n            n_gpu = torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(local_rank)\n        device = torch.device(""cuda"", local_rank)\n        n_gpu = 1\n        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.distributed.init_process_group(backend=""nccl"")\n    logger.info(\n        ""device: {} n_gpu: {}, distributed training: {}, automatic mixed precision training: {}"".format(\n            device, n_gpu, bool(local_rank != -1), use_amp\n        )\n    )\n    return device, n_gpu\n\n\nclass BaseMLLogger:\n    """"""\n    Base class for tracking experiments.\n\n    This class can be extended to implement custom logging backends like MLFlow, Tensorboard, or Sacred.\n    """"""\n\n    def __init__(self, tracking_uri, **kwargs):\n        self.tracking_uri = tracking_uri\n        print(WELCOME_BARN)\n\n    def init_experiment(self, tracking_uri):\n        raise NotImplementedError()\n\n    @classmethod\n    def log_metrics(cls, metrics, step):\n        raise NotImplementedError()\n\n    @classmethod\n    def log_artifacts(cls, self):\n        raise NotImplementedError()\n\n    @classmethod\n    def log_params(cls, params):\n        raise NotImplementedError()\n\n\nclass MLFlowLogger(BaseMLLogger):\n    """"""\n    Logger for MLFlow experiment tracking.\n    """"""\n\n    def init_experiment(self, experiment_name, run_name=None, nested=True):\n        try:\n            mlflow.set_tracking_uri(self.tracking_uri)\n            mlflow.set_experiment(experiment_name)\n            mlflow.start_run(run_name=run_name, nested=nested)\n        except ConnectionError:\n            raise Exception(\n                f""MLFlow cannot connect to the remote server at {self.tracking_uri}.\\n""\n                f""MLFlow also supports logging runs locally to files. Set the MLFlowLogger ""\n                f""tracking_uri to an empty string to use that.""\n            )\n\n    @classmethod\n    def log_metrics(cls, metrics, step):\n        try:\n            mlflow.log_metrics(metrics, step=step)\n        except ConnectionError:\n            logger.warning(f""ConnectionError in logging metrics to MLFlow."")\n        except Exception as e:\n            logger.warning(f""Failed to log metrics: {e}"")\n\n    @classmethod\n    def log_params(cls, params):\n        try:\n            mlflow.log_params(params)\n        except ConnectionError:\n            logger.warning(""ConnectionError in logging params to MLFlow"")\n        except Exception as e:\n            logger.warning(f""Failed to log params: {e}"")\n\n    @classmethod\n    def log_artifacts(cls, dir_path, artifact_path=None):\n        try:\n            mlflow.log_artifacts(dir_path, artifact_path)\n        except ConnectionError:\n            logger.warning(f""ConnectionError in logging artifacts to MLFlow"")\n        except Exception as e:\n            logger.warning(f""Failed to log artifacts: {e}"")\n\n    @classmethod\n    def end_run(cls):\n        mlflow.end_run()\n\n\nclass TensorBoardLogger(BaseMLLogger):\n    """"""\n    PyTorch TensorBoard Logger\n    """"""\n\n    def __init__(self, **kwargs):\n        from tensorboardX import SummaryWriter\n        TensorBoardLogger.summary_writer = SummaryWriter()\n        super().__init__(**kwargs)\n\n    @classmethod\n    def log_metrics(cls, metrics, step):\n        for key, value in metrics.items():\n            TensorBoardLogger.summary_writer.add_scalar(\n                tag=key, scalar_value=value, global_step=step\n            )\n\n    @classmethod\n    def log_params(cls, params):\n        for key, value in params.items():\n            TensorBoardLogger.summary_writer.add_text(tag=key, text_string=str(value))\n\n\ndef to_numpy(container):\n    try:\n        return container.cpu().numpy()\n    except AttributeError:\n        return container\n\n\ndef convert_iob_to_simple_tags(preds, spans):\n    contains_named_entity = len([x for x in preds if ""B-"" in x]) != 0\n    simple_tags = []\n    merged_spans = []\n    open_tag = False\n    for pred, span in zip(preds, spans):\n        # no entity\n        if not (""B-"" in pred or ""I-"" in pred):\n            if open_tag:\n                # end of one tag\n                merged_spans.append(cur_span)\n                simple_tags.append(cur_tag)\n                open_tag = False\n            continue\n\n        # new span starting\n        elif ""B-"" in pred:\n            if open_tag:\n                # end of one tag\n                merged_spans.append(cur_span)\n                simple_tags.append(cur_tag)\n            cur_tag = pred.replace(""B-"", """")\n            cur_span = span\n            open_tag = True\n\n        elif ""I-"" in pred:\n            this_tag = pred.replace(""I-"", """")\n            if open_tag and this_tag == cur_tag:\n                cur_span[""end""] = span[""end""]\n            elif open_tag:\n                # end of one tag\n                merged_spans.append(cur_span)\n                simple_tags.append(cur_tag)\n                open_tag = False\n    if open_tag:\n        merged_spans.append(cur_span)\n        simple_tags.append(cur_tag)\n        open_tag = False\n    if contains_named_entity and len(simple_tags) == 0:\n        raise Exception(""Predicted Named Entities lost when converting from IOB to simple tags. Please check the format""\n                        ""of the training data adheres to either adheres to IOB2 format or is converted when ""\n                        ""read_ner_file() is called."")\n    return simple_tags, merged_spans\n\n\ndef flatten_list(nested_list):\n    """"""Flatten an arbitrarily nested list, without recursion (to avoid\n    stack overflows). Returns a new list, the original list is unchanged.\n    >> list(flatten_list([1, 2, 3, [4], [], [[[[[[[[[5]]]]]]]]]]))\n    [1, 2, 3, 4, 5]\n    >> list(flatten_list([[1, 2], 3]))\n    [1, 2, 3]\n    """"""\n    nested_list = deepcopy(nested_list)\n\n    while nested_list:\n        sublist = nested_list.pop(0)\n\n        if isinstance(sublist, list):\n            nested_list = sublist + nested_list\n        else:\n            yield sublist\n\ndef log_ascii_workers(n, logger):\n    m_worker_lines = WORKER_M.split(""\\n"")\n    f_worker_lines = WORKER_F.split(""\\n"")\n    x_worker_lines = WORKER_X.split(""\\n"")\n    all_worker_lines = []\n    for i in range(n):\n        rand = np.random.randint(low=0,high=3)\n        if(rand % 3 == 0):\n            all_worker_lines.append(f_worker_lines)\n        elif(rand % 3 == 1):\n            all_worker_lines.append(m_worker_lines)\n        else:\n            all_worker_lines.append(x_worker_lines)\n    zipped = zip(*all_worker_lines)\n    for z in zipped:\n        logger.info(""  "".join(z))\n\ndef format_log(ascii, logger):\n    ascii_lines = ascii.split(""\\n"")\n    for l in ascii_lines:\n        logger.info(l)\n\ndef get_dict_checksum(payload_dict):\n    """"""\n    Get MD5 checksum for a dict.\n    """"""\n    checksum = hashlib.md5(json.dumps(payload_dict, sort_keys=True).encode(""utf-8"")).hexdigest()\n    return checksum\n\n\nclass GracefulKiller:\n    kill_now = False\n\n    def __init__(self):\n        signal.signal(signal.SIGTERM, self.exit_gracefully)\n\n    def exit_gracefully(self, signum, frame):\n        self.kill_now = True\n\n\ndef get_dict_checksum(payload_dict):\n    """"""\n    Get MD5 checksum for a dict.\n    """"""\n    checksum = hashlib.md5(json.dumps(payload_dict, sort_keys=True).encode(""utf-8"")).hexdigest()\n    return checksum\n\ndef reformat_msmarco_train(filename, output_filename):\n    """"""\n    Given a df of structure [query, pos_passage, neg_passage], this function converts it to [query, passage, label]\n    """"""\n    print(""Reformatting MSMarco train data..."")\n    df = pd.read_csv(filename, header=None, sep=""\\t"")\n    samples = []\n    for i, row in tqdm(df.iterrows()):\n       query = row[0]\n       pos = row[1]\n       neg = row[2]\n       samples.append([query, pos, 1])\n       samples.append([query, neg, 0])\n    with open(output_filename, ""w"") as f:\n        f.write(""text\\ttext_b\\tlabel\\n"")\n        for (query, passage, label) in samples:\n            f.write(f""{query}\\t{passage}\\t{label}\\n"")\n    print(f""MSMarco train data saved at {output_filename}"")\n\ndef reformat_msmarco_dev(queries_filename, passages_filename, qrels_filename, top1000_filename, output_filename):\n    print(""Reformatting MSMarco dev data..."")\n    top1000_file = open(top1000_filename)\n    qrels_file = open(qrels_filename)\n    queries_file = open(queries_filename)\n    passages_file = open(passages_filename)\n\n    # Generate a top1000 dict\n    top1000 = dict()\n    for l in tqdm(top1000_file):\n        qid, pid, _, _ = l.split(""\\t"")\n        if qid not in top1000:\n            top1000[qid] = []\n        top1000[qid].append(pid)\n\n    # Generate a qrels dict\n    qrels = dict()\n    for l in qrels_file:\n        qid, _, pid, _ = l.split(""\\t"")\n        if qid not in qrels:\n            qrels[qid] = []\n        qrels[qid].append(pid)\n\n    # Generate a queries dict\n    queries = dict()\n    for l in queries_file:\n        qid, query = l.split(""\\t"")\n        queries[qid] = query[:-1]\n\n    # Generate a passages dict\n    passages = dict()\n    for l in tqdm(passages_file):\n        pid, passage = l.split(""\\t"")\n        passages[pid] = passage[:-1]\n\n    # Generate dict with all needed info\n    final = dict()\n    for qid in tqdm(top1000):\n        if qid not in final:\n            final[qid] = []\n        query = queries[qid]\n        curr_qrel = qrels[qid]\n        curr_top1000 = top1000[qid]\n        for ct in curr_top1000:\n            is_relevant = int(ct in curr_qrel)\n            passage = passages[ct]\n            quad = list([query, ct, passage, is_relevant])\n            final[qid].append(quad)\n\n    # Flatten the structure of final and convert to df\n    records = []\n    for k, v in tqdm(final.items()):\n        for x in v:\n            records.append([k] + x)\n    df = pd.DataFrame(records, columns=[""qid"", ""text"", ""pid"", ""text_b"", ""label""])\n    df.to_csv(output_filename, sep=""\\t"", index=None)\n    print(f""MSMarco train data saved at {output_filename}"")\n\n\ndef write_msmarco_results(results, output_filename):\n    out_file = open(output_filename, ""w"")\n    for dictionary in results:\n        for pred in dictionary[""predictions""]:\n            if pred[""label""] == ""1"":\n                score = pred[""probability""]\n            elif pred[""label""] == ""0"":\n                score = 1 - pred[""probability""]\n            out_file.write(str(score))\n            out_file.write(""\\n"")\n\ndef stack(list_of_lists):\n    n_lists_final = len(list_of_lists[0])\n    ret = [list() for _ in range(n_lists_final)]\n    for l in list_of_lists:\n        for i, x in enumerate(l):\n            ret[i] += (x)\n    return ret\n\ndef span_to_string(start_t, end_t, token_offsets, clear_text):\n\n    # If it is a no_answer prediction\n    if start_t == -1 and end_t == -1:\n        return """", 0, 0\n\n    n_tokens = len(token_offsets)\n\n    # We do this to point to the beginning of the first token after the span instead of\n    # the beginning of the last token in the span\n    end_t += 1\n\n    # Predictions sometimes land on the very final special token of the passage. But there are no\n    # special tokens on the document level. We will just interpret this as a span that stretches\n    # to the end of the document\n    end_t = min(end_t, n_tokens)\n\n    start_ch = token_offsets[start_t]\n    # i.e. pointing at the END of the last token\n    if end_t == n_tokens:\n        end_ch = len(clear_text)\n    else:\n        end_ch = token_offsets[end_t]\n    return clear_text[start_ch: end_ch].strip(), start_ch, end_ch\n'"
test/conftest.py,0,"b'import pytest\n\nfrom farm.infer import Inferencer\n\n\ndef pytest_addoption(parser):\n    """"""\n    Hook to pass pytest-fixture arguments to tests from the command line.\n    """"""\n    parser.addoption(""--use_gpu"", action=""store_true"", default=False)\n\n\ndef pytest_generate_tests(metafunc):\n    """"""\n    This method gets called for all test cases. Here, we set the arguments supplied in pytest_addoption().\n    """"""\n    option_value = metafunc.config.option.use_gpu\n    if \'use_gpu\' in metafunc.fixturenames:\n        if option_value:\n            metafunc.parametrize(""use_gpu"", [True], scope=""session"")\n        else:\n            metafunc.parametrize(""use_gpu"", [False], scope=""session"")\n\n\n@pytest.fixture(scope=""session"")\ndef adaptive_model_qa(use_gpu, num_processes):\n    """"""\n    PyTest Fixture for a Question Answering Inferencer based on PyTorch.\n    """"""\n    model = Inferencer.load(\n        ""deepset/bert-base-cased-squad2"",\n        task_type=""question_answering"",\n        batch_size=16,\n        num_processes=num_processes,\n        gpu=use_gpu,\n    )\n    return model\n'"
test/create_testdata.py,0,"b'import argparse\nimport json\nimport logging\nimport os\nimport pprint\n\nlogger = logging.getLogger(__name__)\n\ndef squad_subsample():\n    if not os.path.exists(""samples/qa""):\n        os.makedirs(""samples/qa"")\n\n    with open(\'../data/squad20/dev-v2.0.json\') as json_file:\n        data = json.load(json_file)\n\n    ss = data[""data""][0][""paragraphs""][:1]\n    sample = {}\n    sample[""data""] = [{""paragraphs"": ss}]\n    # just creating same train and dev files\n    with open(\'samples/qa/dev-sample.json\', \'w\') as outfile:\n        json.dump(sample, outfile)\n    with open(\'samples/qa/train-sample.json\', \'w\') as outfile:\n        json.dump(sample, outfile)\n\ndef germeval14_subsample():\n    if not os.path.exists(""samples/ner""):\n        os.makedirs(""samples/ner"")\n\n    with open(\'../data/germeval14/dev.txt\') as file:\n        data = file.readlines()\n\n    ss = """".join(data[:200])\n    with open(\'samples/ner/train-sample.txt\', \'w\') as outfile:\n        outfile.write(ss)\n    with open(\'samples/ner/dev-sample.txt\', \'w\') as outfile:\n        outfile.write(ss)\n\ndef germeval18_subsample():\n    if not os.path.exists(""samples/doc_class""):\n        os.makedirs(""samples/doc_class"")\n    with open(\'../data/germeval18/test.tsv\') as file:\n        data = file.readlines()\n\n    ss = """".join(data[:50])\n    with open(\'samples/doc_class/train-sample.tsv\', \'w\') as outfile:\n        outfile.write(ss)\n    with open(\'samples/doc_class/test-sample.tsv\', \'w\') as outfile:\n        outfile.write(ss)\n\nif __name__==""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--task\', type=str, default=\'\', help=""Which task to create testdata for: qa, ner, doc_class"")\n    args = parser.parse_args()\n    if(args.task == ""qa""):\n        logger.info(""Creating test data for Question Answering, please make sure the original data is already downloaded and in data/squad20"")\n        squad_subsample()\n    elif(args.task == ""ner""):\n        logger.info(\n            ""Creating test data for NER, please make sure the original data is already downloaded and in data/germeval14"")\n        germeval14_subsample()\n    elif(args.task == ""doc_class""):\n        logger.info(\n            ""Creating test data for Document Classification, please make sure the original data is already downloaded and in data/germeval18"")\n        germeval18_subsample()'"
test/test_conversion.py,0,"b'from farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.infer import Inferencer\nimport pprint\nfrom transformers.pipelines import pipeline\nfrom transformers.modeling_auto import AutoModelForQuestionAnswering\nimport os\nfrom pathlib import Path\n\nimport logging\n\ndef test_conversion_adaptive_model(caplog):\n    if caplog:\n        caplog.set_level(logging.CRITICAL)\n\n    model = AdaptiveModel.convert_from_transformers(""deepset/bert-base-cased-squad2"", device=""cpu"", task_type=""question_answering"")\n    transformer_model = model.convert_to_transformers()\n    transformer_model2 = AutoModelForQuestionAnswering.from_pretrained(""deepset/bert-base-cased-squad2"")\n    # compare weights\n    for p1, p2 in zip(transformer_model.parameters(), transformer_model2.parameters()):\n        assert(p1.data.ne(p2.data).sum() == 0)\n\n\ndef test_conversion_inferencer(caplog):\n    if caplog:\n        caplog.set_level(logging.CRITICAL)\n    # input\n    question = ""Why is model conversion important?""\n    text = ""The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.""\n\n\n    # Load from model hub\n    model = ""deepset/bert-base-cased-squad2""\n    nlp = Inferencer.load(model, task_type=""question_answering"", num_processes=0)\n\n    assert nlp.processor.tokenizer.basic_tokenizer.do_lower_case == False\n\n    QA_input = [{""questions"": [question], ""text"": text}]\n    result_farm = nlp.inference_from_dicts(dicts=QA_input)\n    answer_farm = result_farm[0][""predictions""][0][""answers""][0][""answer""]\n    assert answer_farm == \'gives freedom to the user\'\n\n    # save it\n    farm_model_dir = Path(""testsave/bert-conversion-test"")\n    nlp.save(farm_model_dir)\n\n    # load from disk in FARM format\n    model = AdaptiveModel.load(farm_model_dir, device=""cpu"")\n    tokenizer = Tokenizer.load(farm_model_dir)\n\n    # convert to transformers\n    transformer_model = model.convert_to_transformers()\n\n    # save it (Note: transformers uses strings rather than Path objects)\n    model_dir = ""testsave/bert-conversion-test-hf""\n    os.makedirs(model_dir, exist_ok=True)\n    transformer_model.save_pretrained(model_dir)\n    tokenizer.save_pretrained(model_dir)\n\n    # run predictions (using transformers)\n    nlp = pipeline(\'question-answering\', model=model_dir, tokenizer=model_dir)\n    result_transformers = nlp({\n        \'question\': question,\n        \'context\': text\n    })\n    answer_transformers = result_transformers[""answer""]\n    assert answer_farm == answer_transformers\n\nif __name__ == ""__main__"":\n    test_conversion_inferencer(None)\n    test_conversion_adaptive_model(None)\n'"
test/test_doc_classification.py,0,"b'import logging\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, initialize_device_settings\n\n\n@pytest.mark.parametrize(""data_dir_path,text_column_name"",\n                         [(""samples/doc_class"", None),\n                          (""samples/doc_class_other_text_column_name"", ""text_other"")])\ndef test_doc_classification(data_dir_path, text_column_name, caplog=None):\n    if caplog:\n        caplog.set_level(logging.CRITICAL)\n\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=False)\n    n_epochs = 1\n    batch_size = 1\n    evaluate_every = 2\n    lang_model = ""bert-base-german-cased""\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=False)\n\n    tcp_params = dict(tokenizer=tokenizer,\n                      max_seq_len=8,\n                      data_dir=Path(data_dir_path),\n                      train_filename=""train-sample.tsv"",\n                      label_list=[""OTHER"", ""OFFENSE""],\n                      metric=""f1_macro"",\n                      dev_filename=""test-sample.tsv"",\n                      test_filename=None,\n                      dev_split=0.0,\n                      label_column_name=""coarse_label"")\n\n    if text_column_name is not None:\n        tcp_params[""text_column_name""] = text_column_name\n\n    processor = TextClassificationProcessor(**tcp_params)\n\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    language_model = LanguageModel.load(lang_model)\n    prediction_head = TextClassificationHead(num_labels=2)\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        #optimizer_opts={\'name\': \'AdamW\', \'lr\': 2E-05},\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=1,\n        device=device,\n        schedule_opts=None)\n\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    trainer.train()\n\n    save_dir = Path(""testsave/doc_class"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    basic_texts = [\n        {""text"": ""Martin M\xc3\xbcller spielt Handball in Berlin.""},\n        {""text"": ""Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot sei.""}\n    ]\n\n    inf = Inferencer.load(save_dir, batch_size=2, num_processes=0)\n    result = inf.inference_from_dicts(dicts=basic_texts)\n    assert isinstance(result[0][""predictions""][0][""probability""], np.float32)\n    result2 = inf.inference_from_dicts(dicts=basic_texts, return_json=True)\n    assert result == result2\n'"
test/test_doc_classification_distilbert.py,0,"b'from pathlib import Path\nimport logging\nimport numpy as np\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import DistilBert\nfrom farm.modeling.prediction_head import TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, initialize_device_settings\n\n\ndef test_doc_classification(caplog):\n    if caplog:\n        caplog.set_level(logging.CRITICAL)\n\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=False)\n    n_epochs = 1\n    batch_size = 1\n    evaluate_every = 2\n    lang_model = ""distilbert-base-german-cased""\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=False)\n\n    processor = TextClassificationProcessor(tokenizer=tokenizer,\n                                            max_seq_len=8,\n                                            data_dir=Path(""samples/doc_class""),\n                                            train_filename=Path(""train-sample.tsv""),\n                                            label_list=[""OTHER"", ""OFFENSE""],\n                                            metric=""f1_macro"",\n                                            dev_filename=""test-sample.tsv"",\n                                            test_filename=None,\n                                            dev_split=0.0,\n                                            label_column_name=""coarse_label"")\n\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    language_model = DistilBert.load(lang_model)\n    prediction_head = TextClassificationHead(num_labels=2)\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=1,\n        device=device,\n        schedule_opts=None)\n\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    trainer.train()\n\n    save_dir = Path(""testsave/doc_class"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    basic_texts = [\n        {""text"": ""Malte liebt Berlin.""},\n        {""text"": ""Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot sei.""}\n    ]\n\n    inf = Inferencer.load(save_dir, batch_size=2, num_processes=0)\n    result = inf.inference_from_dicts(dicts=basic_texts)\n    assert isinstance(result[0][""predictions""][0][""probability""], np.float32)\n\n\nif __name__ == ""__main__"":\n    test_doc_classification(None)\n'"
test/test_doc_classification_roberta.py,0,"b'import logging\nfrom pathlib import Path\n\nimport numpy as np\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import Roberta\nfrom farm.modeling.prediction_head import TextClassificationHead\nfrom farm.modeling.tokenization import RobertaTokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, initialize_device_settings\n\ndef test_doc_classification(caplog):\n    if caplog:\n        caplog.set_level(logging.CRITICAL)\n\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=False)\n    n_epochs = 1\n    batch_size = 1\n    evaluate_every = 2\n    lang_model = ""roberta-base""\n\n    tokenizer = RobertaTokenizer.from_pretrained(\n        pretrained_model_name_or_path=lang_model)\n\n    processor = TextClassificationProcessor(tokenizer=tokenizer,\n                                            max_seq_len=8,\n                                            data_dir=Path(""samples/doc_class""),\n                                            train_filename=""train-sample.tsv"",\n                                            label_list=[""OTHER"", ""OFFENSE""],\n                                            metric=""f1_macro"",\n                                            dev_filename=""test-sample.tsv"",\n                                            test_filename=None,\n                                            dev_split=0.0,\n                                            label_column_name=""coarse_label"")\n\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    language_model = Roberta.load(lang_model)\n    prediction_head = TextClassificationHead(num_labels=2)\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device)\n\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        #optimizer_opts={\'name\': \'AdamW\', \'lr\': 2E-05},\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=1,\n        device=device,\n        schedule_opts=None)\n\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device)\n\n    trainer.train()\n\n    save_dir = Path(""testsave/doc_class_roberta"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    basic_texts = [\n        {""text"": ""Martin M\xc3\xbcller spielt Handball in Berlin.""},\n        {""text"": ""Schartau sagte dem Tagesspiegel, dass Fischer ein Idiot sei.""}\n    ]\n\n    inf = Inferencer.load(save_dir, batch_size=2, num_processes=0)\n    result = inf.inference_from_dicts(dicts=basic_texts)\n    assert isinstance(result[0][""predictions""][0][""probability""],np.float32)\n\n\nif __name__ == ""__main__"":\n    test_doc_classification(None)\n'"
test/test_doc_regression.py,0,"b'import logging\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import RegressionProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import RegressionHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, initialize_device_settings\n\n@pytest.mark.parametrize(""data_dir_path,text_column_name"",\n                         [(""samples/doc_regr"", None),\n                          (""samples/doc_regr_other_text_column_name"", ""text_other"")])\ndef test_doc_regression(data_dir_path, text_column_name, caplog=None):\n    if caplog:\n        caplog.set_level(logging.CRITICAL)\n\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=False)\n    n_epochs = 1\n    batch_size = 1\n    evaluate_every = 2\n    lang_model = ""bert-base-cased""\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=False)\n\n    rp_params = dict(tokenizer=tokenizer,\n                            max_seq_len=8,\n                            data_dir=Path(data_dir_path),\n                            train_filename=""train-sample.tsv"",\n                            dev_filename=""test-sample.tsv"",\n                            test_filename=None,\n                            label_column_name=""label"")\n\n    if text_column_name is not None:\n        rp_params[""text_column_name""] = text_column_name\n\n    processor = RegressionProcessor(**rp_params)\n\n    data_silo = DataSilo(\n        processor=processor,\n        batch_size=batch_size)\n\n    language_model = LanguageModel.load(lang_model)\n    prediction_head = RegressionHead()\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence_continuous""],\n        device=device\n    )\n\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        #optimizer_opts={\'name\': \'AdamW\', \'lr\': 2E-05},\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=1,\n        device=device,\n        schedule_opts={\'name\': \'CosineWarmup\', \'warmup_proportion\': 0.1}\n    )\n\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device\n    )\n\n    trainer.train()\n\n    save_dir = Path(""testsave/doc_regr"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    basic_texts = [\n        {""text"": ""The dress is just fabulous and it totally fits my size. The fabric is of great quality and the seams are really well hidden. I am super happy with this purchase and I am looking forward to trying some more from the same brand.""},\n        {""text"": ""it just did not fit right. The top is very thin showing everything.""},\n    ]\n\n    model = Inferencer.load(save_dir, num_processes=0)\n    result = model.inference_from_dicts(dicts=basic_texts)\n    assert isinstance(result[0][""predictions""][0][""pred""], np.float32)\n'"
test/test_inference.py,0,"b'import pytest\n\n\n@pytest.mark.parametrize(""streaming"", [True, False])\n@pytest.mark.parametrize(""multiprocessing_chunksize"", [None, 2])\n@pytest.mark.parametrize(""num_processes"", [2, 0, None], scope=""session"")\ndef test_qa_format_and_results(adaptive_model_qa, streaming, multiprocessing_chunksize):\n    qa_inputs_dicts = [\n        {\n            ""questions"": [""In what country is Normandy""],\n            ""text"": ""The Normans are an ethnic group that arose in Normandy, a northern region ""\n            ""of France, from contact between Viking settlers and indigenous Franks and Gallo-Romans"",\n        },\n        {\n            ""questions"": [""Who counted the game among the best ever made?""],\n            ""text"": ""Twilight Princess was released to universal critical acclaim and commercial success. It received ""\n            ""perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic ""\n            ""Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings ""\n            ""and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores ""\n            ""of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the ""\n            ""greatest games ever created."",\n        },\n    ]\n    ground_truths = [""France"", ""GameTrailers""]\n\n    results = adaptive_model_qa.inference_from_dicts(\n        dicts=qa_inputs_dicts,\n        multiprocessing_chunksize=multiprocessing_chunksize,\n        streaming=streaming,\n    )\n    # sample results\n    # [\n    #     {\n    #         ""task"": ""qa"",\n    #         ""predictions"": [\n    #             {\n    #                 ""question"": ""In what country is Normandy"",\n    #                 ""question_id"": ""None"",\n    #                 ""ground_truth"": None,\n    #                 ""answers"": [\n    #                     {\n    #                         ""score"": 1.1272038221359253,\n    #                         ""probability"": -1,\n    #                         ""answer"": ""France"",\n    #                         ""offset_answer_start"": 54,\n    #                         ""offset_answer_end"": 60,\n    #                         ""context"": ""The Normans gave their name to Normandy, a region in France."",\n    #                         ""offset_context_start"": 0,\n    #                         ""offset_context_end"": 60,\n    #                         ""document_id"": None,\n    #                     }\n    #                 ]\n    #             }\n    #         ],\n    #     }\n    # ]\n    predictions = list(results)[0][""predictions""]\n\n    for prediction, ground_truth, qa_input_dict in zip(\n        predictions, ground_truths, qa_inputs_dicts\n    ):\n        assert prediction[""question""] == qa_input_dict[""questions""][0]\n        answer = prediction[""answers""][0]\n        assert answer[""answer""] in answer[""context""]\n        assert answer[""answer""] == ground_truth\n        assert (\n                set(\n                    (\n                        ""answer"",\n                        ""score"",\n                        ""probability"",\n                        ""offset_answer_start"",\n                        ""offset_answer_end"",\n                        ""context"",\n                        ""offset_context_start"",\n                        ""offset_context_end"",\n                        ""document_id"",\n                        ""classification""\n                    )\n                )\n                == answer.keys()\n        )\n'"
test/test_lm_finetuning.py,6,"b'import logging\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import BertStyleLMProcessor\nfrom farm.experiment import initialize_optimizer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import BertLMHead, NextSentenceHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, initialize_device_settings\nfrom farm.infer import Inferencer\n\ndef test_lm_finetuning(caplog):\n    caplog.set_level(logging.CRITICAL)\n\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=False)\n    n_epochs = 1\n    batch_size = 1\n    evaluate_every = 2\n    lang_model = ""bert-base-cased""\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model, do_lower_case=False\n    )\n\n    processor = BertStyleLMProcessor(\n        data_dir=Path(""samples/lm_finetuning""),\n        train_filename=""train-sample.txt"",\n        test_filename=""test-sample.txt"",\n        dev_filename=None,\n        tokenizer=tokenizer,\n        max_seq_len=12,\n        next_sent_pred=True\n    )\n    data_silo = DataSilo(processor=processor, batch_size=batch_size, max_processes=1)\n\n    language_model = LanguageModel.load(lang_model)\n    lm_prediction_head = BertLMHead.load(lang_model)\n    next_sentence_head = NextSentenceHead.load(lang_model)\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[lm_prediction_head, next_sentence_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token"", ""per_sequence""],\n        device=device,\n    )\n\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        #optimizer_opts={\'name\': \'AdamW\', \'lr\': 2E-05},\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=1,\n        device=device,\n        schedule_opts={\'name\': \'CosineWarmup\', \'warmup_proportion\': 0.1})\n\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        evaluate_every=evaluate_every,\n        device=device,\n    )\n\n    trainer.train()\n\n    # LM embeddings and weight of decoder in head are shared and should therefore be equal\n    assert torch.all(\n        torch.eq(model.language_model.model.embeddings.word_embeddings.weight, model.prediction_heads[0].decoder.weight))\n\n    save_dir = Path(""testsave/lm_finetuning"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    basic_texts = [\n        {""text"": ""Farmer\'s life is great.""},\n        {""text"": ""It\'s nothing for big city kids though.""},\n    ]\n    model = Inferencer.load(save_dir, task_type=""embeddings"", num_processes=0)\n    result = model.extract_vectors(dicts=basic_texts)\n    assert result[0][""context""] == [\'Farmer\', ""\'"", \'s\', \'life\', \'is\', \'great\', \'.\']\n    assert result[0][""vec""].shape == (768,)\n    # TODO check why results vary accross runs with same seed\n    assert isinstance(result[0][""vec""][0], np.float32)\n\n\ndef test_lm_finetuning_no_next_sentence(caplog):\n    caplog.set_level(logging.CRITICAL)\n\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=False)\n    n_epochs = 1\n    batch_size = 1\n    evaluate_every = 2\n    lang_model = ""bert-base-cased""\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model, do_lower_case=False\n    )\n\n    processor = BertStyleLMProcessor(\n        data_dir=Path(""samples/lm_finetuning""),\n        train_filename=""train-sample.txt"",\n        test_filename=""test-sample.txt"",\n        dev_filename=None,\n        tokenizer=tokenizer,\n        max_seq_len=12,\n        next_sent_pred=False\n    )\n    data_silo = DataSilo(processor=processor, batch_size=batch_size, max_processes=1)\n\n    language_model = LanguageModel.load(lang_model)\n    lm_prediction_head = BertLMHead.load(lang_model)\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[lm_prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token""],\n        device=device,\n    )\n\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        #optimizer_opts={\'name\': \'AdamW\', \'lr\': 2E-05},\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=1,\n        device=device,\n        schedule_opts={\'name\': \'CosineWarmup\', \'warmup_proportion\': 0.1}\n    )\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device,\n    )\n\n    trainer.train()\n\n    # LM embeddings and weight of decoder in head are shared and should therefore be equal\n    assert torch.all(\n        torch.eq(model.language_model.model.embeddings.word_embeddings.weight, model.prediction_heads[0].decoder.weight))\n\n    save_dir = Path(""testsave/lm_finetuning_no_nsp"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    basic_texts = [\n        {""text"": ""Farmer\'s life is great.""},\n        {""text"": ""It\'s nothing for big city kids though.""},\n    ]\n    model = Inferencer.load(save_dir, task_type=""embeddings"", num_processes=0)\n    result = model.extract_vectors(dicts=basic_texts)\n    assert result[0][""context""] == [\'Farmer\', ""\'"", \'s\', \'life\', \'is\', \'great\', \'.\']\n    assert result[0][""vec""].shape == (768,)\n    # TODO check why results vary accross runs with same seed\n    assert isinstance(result[0][""vec""][0], np.float32)\n\n\ndef test_lm_finetuning_custom_vocab(caplog):\n    caplog.set_level(logging.CRITICAL)\n\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=False)\n    n_epochs = 1\n    batch_size = 1\n    evaluate_every = 2\n    lang_model = ""bert-base-cased""\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model, do_lower_case=False\n    )\n    tokenizer.add_tokens([""aaaaaaaaaaaaaaaa"", ""bbbbbbbbbbbbbbbbbbbbb"", ""ccccccccccccccccccccccc""])\n\n    processor = BertStyleLMProcessor(\n        data_dir=Path(""samples/lm_finetuning""),\n        train_filename=""train-sample.txt"",\n        test_filename=""test-sample.txt"",\n        dev_filename=None,\n        tokenizer=tokenizer,\n        max_seq_len=12,\n        next_sent_pred=True\n    )\n    data_silo = DataSilo(processor=processor, batch_size=batch_size, max_processes=1)\n\n    language_model = LanguageModel.load(lang_model, n_added_tokens=len(tokenizer.added_tokens_decoder))\n    lm_prediction_head = BertLMHead.load(lang_model, n_added_tokens=len(tokenizer.added_tokens_decoder))\n    next_sentence_head = NextSentenceHead.load(lang_model)\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[lm_prediction_head, next_sentence_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token"", ""per_sequence""],\n        device=device\n    )\n\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        #optimizer_opts={\'name\': \'AdamW\', \'lr\': 2E-05},\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=1,\n        device=device,\n        schedule_opts={\'name\': \'CosineWarmup\', \'warmup_proportion\': 0.1}\n    )\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device,\n    )\n\n    trainer.train()\n\n    # LM embeddings and weight of decoder in head are shared and should therefore be equal\n    assert torch.all(\n        torch.eq(model.language_model.model.embeddings.word_embeddings.weight, model.prediction_heads[0].decoder.weight))\n\n    save_dir = Path(""testsave/lm_finetuning"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    basic_texts = [\n        {""text"": ""Farmer\'s life is great.""},\n        {""text"": ""It\'s nothing for big city kids though.""},\n    ]\n    model = Inferencer.load(save_dir, task_type=""embeddings"", num_processes=0)\n    result = model.extract_vectors(dicts=basic_texts)\n    assert result[0][""context""] == [\'Farmer\', ""\'"", \'s\', \'life\', \'is\', \'great\', \'.\']\n    assert result[0][""vec""].shape == (768,)\n    # TODO check why results vary accross runs with same seed\n    assert isinstance(result[0][""vec""][0], np.float32)\n\nif(__name__==""__main__""):\n    test_lm_finetuning()\n'"
test/test_natural_questions.py,0,"b'import logging\nfrom pathlib import Path\nimport numpy as np\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import NaturalQuestionsProcessor\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.modeling.prediction_head import QuestionAnsweringHead, TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, initialize_device_settings\nfrom farm.infer import Inferencer\n\ndef test_nq(caplog=None):\n    if caplog:\n        caplog.set_level(logging.CRITICAL)\n\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=False)\n    batch_size = 2\n    n_epochs = 1\n    evaluate_every = 4\n    base_LM_model = ""distilbert-base-uncased""\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=base_LM_model, do_lower_case=True\n    )\n    processor = NaturalQuestionsProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=20,\n        doc_stride=10,\n        max_query_length=6,\n        train_filename=""train_sample.jsonl"",\n        dev_filename=""dev_sample.jsonl"",\n        data_dir=Path(""samples/nq"")\n    )\n\n    data_silo = DataSilo(processor=processor, batch_size=batch_size, max_processes=1)\n    language_model = LanguageModel.load(base_LM_model)\n    qa_head = QuestionAnsweringHead()\n    classification_head = TextClassificationHead(num_labels=len(processor.answer_type_list))\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[qa_head, classification_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token"", ""per_sequence""],\n        device=device,\n    )\n\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        #optimizer_opts={\'name\': \'AdamW\', \'lr\': 2E-05},\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs,\n        device=device\n    )\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device\n    )\n    trainer.train()\n    save_dir = Path(""testsave/nq"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    inferencer = Inferencer.load(save_dir, batch_size=2, gpu=False, num_processes=0)\n\n    qa_format_1 = [\n        {\n            ""questions"": [""Who counted the game among the best ever made?""],\n            ""text"": ""Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the greatest games ever created.""\n        }]\n    qa_format_2 = [{""qas"":[""Who counted the game among the best ever made?""],\n                 ""context"": ""Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the greatest games ever created."",\n                }]\n\n\n    result1 = inferencer.inference_from_dicts(dicts=qa_format_1)\n    result2 = inferencer.inference_from_dicts(dicts=qa_format_2)\n    assert result1 == result2\n\nif __name__ == ""__main__"":\n    test_nq()'"
test/test_ner.py,0,"b'from pathlib import Path\n\nimport numpy as np\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import NERProcessor\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import TokenClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, initialize_device_settings\n\nimport logging\n\n\ndef test_ner(caplog):\n    if caplog:\n        caplog.set_level(logging.CRITICAL)\n\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=False)\n    n_epochs = 3\n    batch_size = 2\n    evaluate_every = 1\n    lang_model = ""distilbert-base-german-cased""\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model, do_lower_case=False\n    )\n\n    ner_labels = [""[PAD]"", ""X"", ""O"", ""B-MISC"", ""I-MISC"", ""B-PER"", ""I-PER"", ""B-ORG"", ""I-ORG"", ""B-LOC"", ""I-LOC"", ""B-OTH"",\n                  ""I-OTH""]\n\n    processor = NERProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=8,\n        data_dir=Path(""samples/ner""),\n        train_filename=""train-sample.txt"",\n        dev_filename=""dev-sample.txt"",\n        test_filename=None,\n        delimiter="" "",\n        label_list=ner_labels,\n        metric=""seq_f1""\n    )\n\n    data_silo = DataSilo(processor=processor, batch_size=batch_size, max_processes=1)\n    language_model = LanguageModel.load(lang_model)\n    prediction_head = TokenClassificationHead(num_labels=13)\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token""],\n        device=device,\n    )\n\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        #optimizer_opts={\'name\': \'AdamW\', \'lr\': 2E-05},\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=1,\n        device=device,\n        schedule_opts={\'name\': \'LinearWarmup\', \'warmup_proportion\': 0.1}\n    )\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device,\n    )\n\n    save_dir = Path(""testsave/ner"")\n    model = trainer.train()\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    basic_texts = [\n        {""text"": ""Paris is a town in France.""},\n    ]\n    model = Inferencer.load(model_name_or_path=""dbmdz/bert-base-cased-finetuned-conll03-english"", num_processes=0, task_type=""ner"")\n    # labels arent correctly inserted from transformers\n    # They are converted to LABEL_1 ... LABEL_N\n    # For the inference result to contain predictions we need them in IOB NER format\n    model.processor.tasks[""ner""][""label_list""][-1] = ""B-LOC""\n    result = model.inference_from_dicts(dicts=basic_texts)\n\n    assert result[0][""predictions""][0][""context""] == ""Paris""\n    assert isinstance(result[0][""predictions""][0][""probability""], np.float32)\n\n\nif __name__ == ""__main__"":\n    test_ner(None)\n'"
test/test_ner_amp.py,0,"b'from pathlib import Path\nimport numpy as np\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import NERProcessor\nfrom farm.modeling.optimization import initialize_optimizer, AMP_AVAILABLE\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import TokenClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, initialize_device_settings\n\nimport logging\n\n\ndef test_ner_amp(caplog):\n    if caplog:\n        caplog.set_level(logging.CRITICAL)\n\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    n_epochs = 1\n    batch_size = 2\n    evaluate_every = 1\n    lang_model = ""bert-base-german-cased""\n    if AMP_AVAILABLE:\n        use_amp = \'O1\'\n    else:\n        use_amp = None\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model, do_lower_case=False\n    )\n\n    ner_labels = [""[PAD]"", ""X"", ""O"", ""B-MISC"", ""I-MISC"", ""B-PER"", ""I-PER"", ""B-ORG"", ""I-ORG"", ""B-LOC"", ""I-LOC"", ""B-OTH"",\n                  ""I-OTH""]\n\n    processor = NERProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=8,\n        data_dir=Path(""samples/ner""),\n        train_filename=Path(""train-sample.txt""),\n        dev_filename=Path(""dev-sample.txt""),\n        test_filename=None,\n        delimiter="" "",\n        label_list=ner_labels,\n        metric=""seq_f1""\n    )\n\n    data_silo = DataSilo(processor=processor, batch_size=batch_size, max_processes=1)\n    language_model = LanguageModel.load(lang_model)\n    prediction_head = TokenClassificationHead(num_labels=13)\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token""],\n        device=device\n    )\n\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-05,\n        schedule_opts=None,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs,\n        device=device,\n        use_amp=use_amp)\n\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device,\n    )\n\n    save_dir = Path(""testsave/ner"")\n    trainer.train()\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    basic_texts = [\n        {""text"": ""1980 kam der Crown von Toyota""},\n    ]\n    model = Inferencer.load(save_dir, num_processes=0)\n    result = model.inference_from_dicts(dicts=basic_texts)\n\n    assert result[0][""predictions""][0][""context""] == ""Crown""\n    assert isinstance(result[0][""predictions""][0][""probability""], np.float32)\n\n\nif __name__ == ""__main__"":\n    test_ner_amp(None)\n'"
test/test_processor_saving_loading.py,1,"b'import logging\nfrom pathlib import Path\n\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.utils import set_all_seeds\nimport torch\n\ndef test_processor_saving_loading(caplog):\n    if caplog is not None:\n        caplog.set_level(logging.CRITICAL)\n\n    set_all_seeds(seed=42)\n    lang_model = ""bert-base-cased""\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model, do_lower_case=False\n    )\n\n    processor = TextClassificationProcessor(tokenizer=tokenizer,\n                                            max_seq_len=128,\n                                            data_dir=Path(""samples/doc_class""),\n                                            train_filename=""train-sample.tsv"",\n                                            dev_filename=None,\n                                            test_filename=None,\n                                            label_column_name=""coarse_label"",\n                                            dev_split=0.1,\n                                            label_list=[""OTHER"", ""OFFENSE""],\n                                            metric=[""f1_macro""]\n                                            )\n    dicts = processor.file_to_dicts(file=Path(""samples/doc_class/train-sample.tsv""))\n    data, tensor_names = processor.dataset_from_dicts(dicts)\n\n    save_dir = Path(""testsave/processor"")\n    processor.save(save_dir)\n\n    processor = processor.load_from_dir(save_dir)\n    dicts = processor.file_to_dicts(file=Path(""samples/doc_class/train-sample.tsv""))\n    data_loaded, tensor_names_loaded = processor.dataset_from_dicts(dicts)\n\n    assert tensor_names == tensor_names_loaded\n    for i in range(len(data.tensors)):\n        assert torch.all(torch.eq(data.tensors[i], data_loaded.tensors[i]))\n\nif __name__ == ""__main__"":\n    test_processor_saving_loading(None)\n'"
test/test_question_answering.py,0,"b'import logging\nfrom pathlib import Path\nimport numpy as np\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import SquadProcessor\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.modeling.prediction_head import QuestionAnsweringHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, initialize_device_settings\nfrom farm.infer import Inferencer\n\ndef test_qa(caplog=None):\n    if caplog:\n        caplog.set_level(logging.CRITICAL)\n\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=False)\n    batch_size = 2\n    n_epochs = 1\n    evaluate_every = 4\n    base_LM_model = ""distilbert-base-uncased""\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=base_LM_model, do_lower_case=True\n    )\n    label_list = [""start_token"", ""end_token""]\n    processor = SquadProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=20,\n        doc_stride=10,\n        max_query_length=6,\n        train_filename=""train-sample.json"",\n        dev_filename=""dev-sample.json"",\n        test_filename=None,\n        data_dir=Path(""samples/qa""),\n        label_list=label_list,\n        metric=""squad""\n    )\n\n    data_silo = DataSilo(processor=processor, batch_size=batch_size, max_processes=1)\n    language_model = LanguageModel.load(base_LM_model)\n    prediction_head = QuestionAnsweringHead()\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token""],\n        device=device,\n    )\n\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=2e-5,\n        #optimizer_opts={\'name\': \'AdamW\', \'lr\': 2E-05},\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs,\n        device=device\n    )\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device\n    )\n    trainer.train()\n    save_dir = Path(""testsave/qa"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    inferencer = Inferencer.load(save_dir, batch_size=2, gpu=False, num_processes=0)\n\n    qa_format_1 = [\n        {\n            ""questions"": [""Who counted the game among the best ever made?""],\n            ""text"": ""Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the greatest games ever created.""\n        }]\n    qa_format_2 = [{""qas"":[""Who counted the game among the best ever made?""],\n                 ""context"": ""Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the greatest games ever created."",\n                }]\n\n\n    result1 = inferencer.inference_from_dicts(dicts=qa_format_1)\n    result2 = inferencer.inference_from_dicts(dicts=qa_format_2)\n    assert result1 == result2\n\n\ndef test_qa_onnx_inference(caplog=None):\n    if caplog:\n        caplog.set_level(logging.CRITICAL)\n\n\n    QA_input = [\n        {\n            ""questions"": [""Who counted the game among the best ever made?""],\n            ""text"": ""Twilight Princess was released to universal critical acclaim and commercial success. It received perfect scores from major publications such as 1UP.com, Computer and Video Games, Electronic Gaming Monthly, Game Informer, GamesRadar, and GameSpy. On the review aggregators GameRankings and Metacritic, Twilight Princess has average scores of 95% and 95 for the Wii version and scores of 95% and 96 for the GameCube version. GameTrailers in their review called it one of the greatest games ever created.""\n        }]\n\n    base_LM_model = ""deepset/bert-base-cased-squad2""\n\n    # Pytorch\n    inferencer = Inferencer.load(base_LM_model, batch_size=2, gpu=False, task_type=""question_answering"", num_processes=0)\n    result = inferencer.inference_from_dicts(dicts=QA_input)[0]\n\n    # ONNX\n    onnx_model_export_path = Path(""testsave/onnx-export"")\n    inferencer.model.convert_to_onnx(onnx_model_export_path)\n    inferencer = Inferencer.load(model_name_or_path=onnx_model_export_path, task_type=""question_answering"", num_processes=0)\n\n    result_onnx = inferencer.inference_from_dicts(QA_input)[0]\n\n    for (onnx, regular) in zip(result_onnx[""predictions""][0][""answers""][0].items(), result[""predictions""][0][""answers""][0].items()):\n        # keys\n        assert onnx[0] == regular[0]\n        # values\n        if type(onnx[1]) == float:\n            np.testing.assert_almost_equal(onnx[1], regular[1], decimal=4)  # score\n        else:\n            assert onnx[1] == regular[1]\n\n\nif(__name__==""__main__""):\n    test_qa()\n    test_qa_onnx_inference()\n'"
test/test_s3e_pooling.py,0,"b'import logging\nimport pickle\nfrom pathlib import Path\n\nfrom farm.data_handler.processor import InferenceProcessor\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.utils import set_all_seeds, initialize_device_settings\nfrom farm.modeling.wordembedding_utils import fit_s3e_on_corpus\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_s3e_fit():\n    # small test data\n    language_model = Path(""samples/s3e/tiny_fasttext_model"")\n    corpus_path = Path(""samples/s3e/tiny_corpus.txt"")\n    save_dir = Path(""testsave/fitted_s3e/"")\n    do_lower_case = False\n    batch_size = 2\n    use_gpu = False\n\n    # Fit S3E on a corpus\n    set_all_seeds(seed=42)\n    device, n_gpu = initialize_device_settings(use_cuda=use_gpu, use_amp=False)\n\n    # Create a InferenceProcessor\n    tokenizer = Tokenizer.load(pretrained_model_name_or_path=language_model, do_lower_case=do_lower_case)\n    processor = InferenceProcessor(tokenizer=tokenizer, max_seq_len=128)\n\n    # Create an AdaptiveModel\n    language_model = LanguageModel.load(language_model)\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[],\n        device=device)\n\n    model, processor, s3e_stats = fit_s3e_on_corpus(processor=processor,\n                                                    model=model,\n                                                    corpus=corpus_path,\n                                                    n_clusters=3,\n                                                    pca_n_components=30,\n                                                    svd_postprocessing=True,\n                                                    min_token_occurrences=1)\n\n    # save everything to allow inference without fitting everything again\n    model.save(save_dir)\n    processor.save(save_dir)\n    with open(save_dir / ""s3e_stats.pkl"", ""wb"") as f:\n        pickle.dump(s3e_stats, f)\n\n    # Load model, tokenizer and processor directly into Inferencer\n    inferencer = Inferencer(model=model, processor=processor, task_type=""embeddings"", gpu=use_gpu,\n                       batch_size=batch_size, extraction_strategy=""s3e"", extraction_layer=-1,\n                       s3e_stats=s3e_stats, num_processes=0)\n\n    # Input\n    basic_texts = [\n        {""text"": ""a man is walking on the street.""},\n        {""text"": ""a woman is walking on the street.""},\n    ]\n\n    # Get embeddings for input text (you can vary the strategy and layer)\n    result = inferencer.inference_from_dicts(dicts=basic_texts)\n    assert result[0][""context""] == [\'a\', \'man\', \'is\', \'walking\', \'on\', \'the\', \'street\', \'.\']\n    assert result[0][""vec""][0] - 0.00527727306941057 < 1e-6\n    assert result[0][""vec""][-2] + 0.21376857861379997 < 1e-6\n\n\ndef test_load_extract_s3e_embeddings():\n    load_dir = Path(""samples/s3e/fitted_s3e"")\n    use_gpu = False\n    batch_size = 2\n\n    with open(load_dir / ""s3e_stats.pkl"", ""rb"") as f:\n        s3e_stats = pickle.load(f)\n\n    # Init inferencer\n    inferencer = Inferencer.load(model_name_or_path=load_dir, task_type=""embeddings"", gpu=use_gpu,\n                       batch_size=batch_size, extraction_strategy=""s3e"", extraction_layer=-1,\n                       s3e_stats=s3e_stats, num_processes=0)\n\n    # Input\n    basic_texts = [\n        {""text"": ""a man is walking on the street.""},\n        {""text"": ""a woman is walking on the street.""},\n    ]\n\n    # Get embeddings for input text\n    result = inferencer.inference_from_dicts(dicts=basic_texts)\n    assert result[0][""context""] == [\'a\', \'man\', \'is\', \'walking\', \'on\', \'the\', \'street\', \'.\']\n    assert result[0][""vec""][0] - 0.00527727306941057 < 1e-6\n    assert result[0][""vec""][-2] + 0.21376857861379997 < 1e-6\n\n'"
test/test_tokenization.py,0,"b'import logging\nfrom farm.modeling.tokenization import Tokenizer, tokenize_with_metadata, truncate_sequences\nfrom transformers import BertTokenizer, RobertaTokenizer, XLNetTokenizer\nimport re\n\n\ndef test_basic_loading(caplog):\n    caplog.set_level(logging.CRITICAL)\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=""bert-base-cased"",\n        do_lower_case=True\n        )\n    assert type(tokenizer) == BertTokenizer\n    assert tokenizer.basic_tokenizer.do_lower_case == True\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=""xlnet-base-cased"",\n        do_lower_case=True\n        )\n    assert type(tokenizer) == XLNetTokenizer\n    assert tokenizer.do_lower_case == True\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=""roberta-base""\n        )\n    assert type(tokenizer) == RobertaTokenizer\n\n\ndef test_bert_tokenizer_all_meta(caplog):\n    caplog.set_level(logging.CRITICAL)\n\n    lang_model = ""bert-base-cased""\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=False\n        )\n\n    basic_text = ""Some Text with neverseentokens plus !215?#. and a combined-token_with/chars""\n\n    # original tokenizer from transformer repo\n    tokenized = tokenizer.tokenize(basic_text)\n    assert tokenized == [\'Some\', \'Text\', \'with\', \'never\', \'##see\', \'##nto\', \'##ken\', \'##s\', \'plus\', \'!\', \'215\', \'?\', \'#\', \'.\', \'and\', \'a\', \'combined\', \'-\', \'token\', \'_\', \'with\', \'/\', \'ch\', \'##ars\']\n\n    # ours with metadata\n    tokenized_meta = tokenize_with_metadata(text=basic_text, tokenizer=tokenizer)\n    assert tokenized_meta[""tokens""] == tokenized\n    assert tokenized_meta[""offsets""] == [0, 5, 10, 15, 20, 23, 26, 29, 31, 36, 37, 40, 41, 42, 44, 48, 50, 58, 59, 64, 65, 69, 70, 72]\n    assert tokenized_meta[""start_of_word""] == [True, True, True, True, False, False, False, False, True, True, False, False, False, False, True, True, True, False, False, False, False, False, False, False]\n\ndef test_save_load(caplog):\n    caplog.set_level(logging.CRITICAL)\n\n    lang_names = [""bert-base-cased"", ""roberta-base"", ""xlnet-base-cased""]\n    tokenizers = []\n    for lang_name in lang_names:\n        t = Tokenizer.load(lang_name, lower_case=False)\n        t.add_tokens(new_tokens=[""neverseentokens""])\n        tokenizers.append(t)\n\n    basic_text = ""Some Text with neverseentokens plus !215?#. and a combined-token_with/chars""\n\n    for tokenizer in tokenizers:\n        save_dir = f""testsave""\n        tokenizer_type = tokenizer.__class__.__name__\n        tokenizer.save_pretrained(save_dir)\n        tokenizer_loaded = Tokenizer.load(save_dir, tokenizer_class=tokenizer_type)\n        tokenized_before = tokenize_with_metadata(text=basic_text, tokenizer=tokenizer)\n        tokenized_after = tokenize_with_metadata(text=basic_text, tokenizer=tokenizer_loaded)\n        assert tokenized_before == tokenized_after\n\ndef test_truncate_sequences(caplog):\n    caplog.set_level(logging.CRITICAL)\n\n    lang_names = [""bert-base-cased"", ""roberta-base"", ""xlnet-base-cased""]\n    tokenizers = []\n    for lang_name in lang_names:\n        t = Tokenizer.load(lang_name, lower_case=False)\n        tokenizers.append(t)\n\n    # artificial sequences (could be tokens, offsets, or anything else)\n    seq_a = list(range(10))\n    seq_b = list(range(15))\n    max_seq_len = 20\n    for tokenizer in tokenizers:\n        for strategy in [""longest_first"", ""only_first"",""only_second""]:\n            trunc_a, trunc_b, overflow = truncate_sequences(seq_a=seq_a,seq_b=seq_b,tokenizer=tokenizer,\n                                                        max_seq_len=max_seq_len, truncation_strategy=strategy)\n\n            assert len(trunc_a) + len(trunc_b) + tokenizer.num_added_tokens(pair=True) == max_seq_len\n\n\ndef test_all_tokenizer_on_special_cases(caplog):\n    caplog.set_level(logging.CRITICAL)\n\n    lang_names = [""bert-base-cased"", ""roberta-base"", ""xlnet-base-cased""]\n    tokenizers = []\n    for lang_name in lang_names:\n        t = Tokenizer.load(lang_name, lower_case=False)\n        tokenizers.append(t)\n\n    texts = [\n     ""This is a sentence"",\n     ""Der entscheidende Pass"",\n    ""This      is a sentence with multiple spaces"",\n    ""\xe5\x8a\x9b\xe5\x8a\xa0\xe5\x8b\x9d\xe5\x8c\x97\xe5\x8c\xba\xe1\xb4\xb5\xe1\xb4\xba\xe1\xb5\x80\xe1\xb5\x83\xe0\xa6\x9b\xe0\xa6\x9c\xe0\xa6\x9f\xe0\xa6\xa1\xe0\xa6\xa3\xe0\xa6\xa4"",\n     ""Thiso text is included tolod makelio sure Unicodeel is handled properly:"",\n   ""This is a sentence..."",\n   ""Let\'s see all on this text and. !23# neverseenwordspossible"",\n    """"""This is a sentence.\n    With linebreak"""""",\n    """"""Sentence with multiple\n\n\n    newlines\n    """""",\n    ""and another one\\n\\n\\nwithout space"",\n    ""This is a sentence\twith tab"",\n    ""This is a sentence\t\t\twith multiple tabs"",\n    ]\n\n    for tokenizer in tokenizers:\n        for text in texts:\n            # Important: we don\'t assume to preserve whitespaces after tokenization.\n            # This means: \\t, \\n "" "" etc will all resolve to a single "" "".\n            # This doesn\'t make a difference for BERT + XLNet but it does for roBERTa\n\n            # 1. original tokenize function from transformer repo on full sentence\n            standardized_whitespace_text = \' \'.join(text.split()) # remove multiple whitespaces\n            tokenized = tokenizer.tokenize(standardized_whitespace_text)\n\n            # 2. our tokenizer with metadata on ""whitespace tokenized words""\n            tokenized_meta = tokenize_with_metadata(text=text, tokenizer=tokenizer)\n\n            # verify that tokenization on full sequence is the same as the one on ""whitespace tokenized words""\n            assert tokenized_meta[""tokens""] == tokenized, f""Failed using {tokenizer.__class__.__name__}""\n\n            # verify that offsets align back to original text\n            if text == ""\xe5\x8a\x9b\xe5\x8a\xa0\xe5\x8b\x9d\xe5\x8c\x97\xe5\x8c\xba\xe1\xb4\xb5\xe1\xb4\xba\xe1\xb5\x80\xe1\xb5\x83\xe0\xa6\x9b\xe0\xa6\x9c\xe0\xa6\x9f\xe0\xa6\xa1\xe0\xa6\xa3\xe0\xa6\xa4"":\n                # contains [UNK] that are impossible to match back to original text space\n                continue\n            for tok, offset in zip(tokenized_meta[""tokens""], tokenized_meta[""offsets""]):\n                #subword-tokens have special chars depending on model type. In order to align with original text we need to get rid of them\n                tok = re.sub(r""^(##|\xc4\xa0|\xe2\x96\x81)"", """", tok)\n                #tok = tokenizer.decode(tokenizer.convert_tokens_to_ids(tok))\n                original_tok = text[offset:offset+len(tok)]\n                assert tok == original_tok, f""Offset alignment wrong for {tokenizer.__class__.__name__} and text \'{text}\'""\n\n\ndef test_bert_custom_vocab(caplog):\n    caplog.set_level(logging.CRITICAL)\n\n    lang_model = ""bert-base-cased""\n\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model,\n        do_lower_case=False\n        )\n\n    #deprecated: tokenizer.add_custom_vocab(""samples/tokenizer/custom_vocab.txt"")\n    tokenizer.add_tokens(new_tokens=[""neverseentokens""])\n\n    basic_text = ""Some Text with neverseentokens plus !215?#. and a combined-token_with/chars""\n\n    # original tokenizer from transformer repo\n    tokenized = tokenizer.tokenize(basic_text)\n    assert tokenized == [\'Some\', \'Text\', \'with\', \'neverseentokens\', \'plus\', \'!\', \'215\', \'?\', \'#\', \'.\', \'and\', \'a\', \'combined\', \'-\', \'token\', \'_\', \'with\', \'/\', \'ch\', \'##ars\']\n\n    # ours with metadata\n    tokenized_meta = tokenize_with_metadata(text=basic_text, tokenizer=tokenizer)\n    assert tokenized_meta[""tokens""] == tokenized\n    assert tokenized_meta[""offsets""] == [0, 5, 10, 15, 31, 36, 37, 40, 41, 42, 44, 48, 50, 58, 59, 64, 65, 69, 70, 72]\n    assert tokenized_meta[""start_of_word""] == [True, True, True, True, True, True, False, False, False, False, True, True, True, False, False, False, False, False, False, False]\n\n\nif __name__ == ""__main__"":\n    test_all_tokenizer_on_special_cases()\n'"
farm/conversion/__init__.py,0,b''
farm/conversion/convert_tf_checkpoint_to_pytorch.py,1,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Convert BERT checkpoint.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\nimport torch\nfrom farm.modeling.bert.modeling import (\n    BertConfig,\n    BertForPreTraining,\n    load_tf_weights_in_bert,\n)\n\n\ndef convert_tf_checkpoint_to_pytorch(\n    tf_checkpoint_path, bert_config_file, pytorch_dump_path\n):\n    # Initialise PyTorch model\n    config = BertConfig.from_json_file(bert_config_file)\n    print(""Building PyTorch model from configuration: {}"".format(str(config)))\n    model = BertForPreTraining(config)\n\n    # Load weights from tf checkpoint\n    load_tf_weights_in_bert(model, tf_checkpoint_path)\n\n    # Save pytorch-model\n    print(""Save PyTorch model to {}"".format(pytorch_dump_path))\n    torch.save(model.state_dict(), pytorch_dump_path)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\n        ""--tf_checkpoint_path"",\n        default=None,\n        type=str,\n        required=True,\n        help=""Path the TensorFlow checkpoint path."",\n    )\n    parser.add_argument(\n        ""--bert_config_file"",\n        default=None,\n        type=str,\n        required=True,\n        help=""The config json file corresponding to the pre-trained BERT model. \\n""\n        ""This specifies the model architecture."",\n    )\n    parser.add_argument(\n        ""--pytorch_dump_path"",\n        default=None,\n        type=str,\n        required=True,\n        help=""Path to the output PyTorch model."",\n    )\n    args = parser.parse_args()\n    convert_tf_checkpoint_to_pytorch(\n        args.tf_checkpoint_path, args.bert_config_file, args.pytorch_dump_path\n    )\n'"
farm/data_handler/__init__.py,0,b''
farm/data_handler/data_silo.py,14,"b'import copy\nimport logging\nimport torch.multiprocessing as mp\nfrom contextlib import ExitStack\nfrom functools import partial\nimport random\nfrom pathlib import Path\nfrom itertools import groupby\n\nimport numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.utils.data import ConcatDataset, Dataset, Subset, IterableDataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data.sampler import RandomSampler, SequentialSampler\nimport torch\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom tqdm import tqdm\n\nfrom farm.data_handler.dataloader import NamedDataLoader\nfrom farm.data_handler.processor import Processor, BertStyleLMProcessor\nfrom farm.data_handler.utils import grouper\nfrom farm.modeling.tokenization import EmbeddingTokenizer\nfrom farm.utils import MLFlowLogger as MlLogger\nfrom farm.utils import log_ascii_workers, calc_chunksize\nfrom farm.utils import get_dict_checksum\nfrom farm.visual.ascii.images import TRACTOR_SMALL\n\n\nlogger = logging.getLogger(__name__)\n\n\n\nclass DataSilo:\n    """""" Generates and stores PyTorch DataLoader objects for the train, dev and test datasets.\n    Relies upon functionality in the processor to do the conversion of the data. Will also\n    calculate and display some statistics.\n     """"""\n\n    def __init__(\n        self,\n        processor,\n        batch_size,\n        distributed=False,\n        automatic_loading=True,\n        max_multiprocessing_chunksize=2000,\n        max_processes=128,\n        caching=False,\n        cache_path=Path(""cache/data_silo""),\n    ):\n        """"""\n        :param processor: A dataset specific Processor object which will turn input (file or dict) into a Pytorch Dataset.\n        :type processor: Processor\n        :param batch_size: The size of batch that should be returned by the DataLoaders.\n        :type batch_size: int\n        :param distributed: Set to True if the program is running in a distributed setting.\n        :type distributed: bool\n        :param automatic_loading: Set to False, if you don\'t want to automatically load data at initialization.\n        :type automatic_loading: bool\n        :param max_multiprocessing_chunksize: max possible value for chunksize as calculated by `calc_chunksize()`\n            in `farm.utils`. For certain cases like lm_finetuning, a smaller value can be set, as the default chunksize\n            values are rather large that might cause memory issues.\n        :type max_multiprocessing_chunksize: int\n        :param max_processes: the maximum number of processes to spawn in the multiprocessing.Pool used in DataSilo.\n                              It can be set to 1 to disable the use of multiprocessing ot make debugging easier.\n        :type max_processes: int\n        :param caching: save the processed datasets on disk to save time/compute if the same train data is used to run\n                        multiple experiments. Each cache has a checksum based on the train_filename of the Processor\n                        and the batch size.\n        :type caching: bool\n        :param cache_path: root dir for storing the datasets\' cache.\n        :type cache_path: Path\n        """"""\n        self.distributed = distributed\n        self.processor = processor\n        self.data = {}\n        self.batch_size = batch_size\n        self.class_weights = None\n        self.max_processes = max_processes\n        self.max_multiprocessing_chunksize = max_multiprocessing_chunksize\n        self.caching = caching\n        self.cache_path = cache_path\n        self.tensor_names = None\n\n        if len(self.processor.tasks) == 0:\n            raise Exception(""No task initialized. Try initializing the processor with a metric and a label list. ""\n                            ""Alternatively you can add a task using Processor.add_task()"")\n\n        if type(self.processor.tokenizer) == EmbeddingTokenizer:\n            if max_processes != 1:\n                logger.warning(""Multiprocessing not efficient for WordEmbedding Tokenizers. Please set max_process \\n""\n                            ""argument in DataSilo to 1."")\n\n        loaded_from_cache = False\n        if self.caching:  # Check if DataSets are present in cache\n            checksum = self._get_checksum()\n            dataset_path = self.cache_path / checksum\n\n            if dataset_path.exists():\n                self._load_dataset_from_cache(dataset_path)\n                loaded_from_cache = True\n\n        if not loaded_from_cache and automatic_loading:\n            # In most cases we want to load all data automatically, but in some cases we rather want to do this\n            # later or load from dicts instead of file (https://github.com/deepset-ai/FARM/issues/85)\n            self._load_data()\n\n    @classmethod\n    def _dataset_from_chunk(cls, chunk, processor):\n        """"""\n        Creating a dataset for a chunk (= subset) of dicts. In multiprocessing:\n          * we read in all dicts from a file\n          * split all dicts into chunks\n          * feed *one chunk* to *one process*\n          => the *one chunk*  gets converted to *one dataset* (that\'s what we do here)\n          * all datasets get collected and concatenated\n        :param chunk: Instead of only having a list of dicts here we also supply an index (ascending int) for each.\n            => [(0, dict), (1, dict) ...]\n        :type chunk: list of tuples\n        :param processor: FARM Processor (e.g. TextClassificationProcessor)\n        :return: PyTorch Dataset\n        """"""\n        dicts = [d[1] for d in chunk]\n        indices = [x[0] for x in chunk]\n        dataset = processor.dataset_from_dicts(dicts=dicts, indices=indices)\n        return dataset\n\n    def _get_dataset(self, filename, dicts=None):\n        if not filename and not dicts:\n            raise ValueError(""You must either supply `filename` or `dicts`"")\n\n        # loading dicts from file (default)\n        if dicts is None:\n            dicts = list(self.processor.file_to_dicts(filename))\n            #shuffle list of dicts here if we later want to have a random dev set splitted from train set\n            if str(self.processor.train_filename) in str(filename):\n                if not self.processor.dev_filename:\n                    if self.processor.dev_split > 0.0:\n                        random.shuffle(dicts)\n\n        num_dicts = len(dicts)\n        multiprocessing_chunk_size, num_cpus_used = calc_chunksize(\n            num_dicts=num_dicts,\n            max_processes=self.max_processes,\n            max_chunksize=self.max_multiprocessing_chunksize,\n        )\n\n        with ExitStack() as stack:\n            if self.max_processes > 1:  # use multiprocessing only when max_processes > 1\n                p = stack.enter_context(mp.Pool(processes=num_cpus_used))\n\n                logger.info(\n                    f""Got ya {num_cpus_used} parallel workers to convert {num_dicts} dictionaries ""\n                    f""to pytorch datasets (chunksize = {multiprocessing_chunk_size})...""\n                )\n                log_ascii_workers(num_cpus_used, logger)\n\n                results = p.imap(\n                    partial(self._dataset_from_chunk, processor=self.processor),\n                    grouper(dicts, multiprocessing_chunk_size),\n                    chunksize=1,\n                )\n            else:\n                logger.info(\n                    f""Multiprocessing disabled, using a single worker to convert {num_dicts}""\n                    f""dictionaries to pytorch datasets.""\n                )\n\n                results = map(partial(self._dataset_from_chunk, processor=self.processor), grouper(dicts, num_dicts))\n\n            datasets = []\n\n            desc = f""Preprocessing Dataset""\n            if filename:\n                desc += f"" {filename}""\n            with tqdm(total=len(dicts), unit=\' Dicts\', desc=desc) as pbar:\n                for dataset, tensor_names in results:\n                    datasets.append(dataset)\n                    # update progress bar (last step can have less dicts than actual chunk_size)\n                    pbar.update(min(multiprocessing_chunk_size, pbar.total-pbar.n))\n            # _dataset_from_chunk can return a None in cases where downsampling has occurred\n            datasets = [d for d in datasets if d]\n            concat_datasets = ConcatDataset(datasets)\n            return concat_datasets, tensor_names\n\n    def _load_data(self, train_dicts=None, dev_dicts=None, test_dicts=None):\n        """"""\n        Loading the train, dev and test datasets either from files (default) or from supplied dicts.\n        The processor is called to handle the full conversion from ""raw data"" to a Pytorch Dataset.\n        The resulting datasets are loaded into DataSilo.data\n\n        :param train_dicts: (Optional) dicts containing examples for training.\n        :param dev_dicts: (Optional) dicts containing examples for dev.\n        :param test_dicts: (Optional) dicts containing examples for test.\n        :return: None\n        """"""\n        logger.info(""\\nLoading data into the data silo ...""\n                    ""{}"".format(TRACTOR_SMALL))\n        # train data\n        if train_dicts:\n            # either from supplied dicts\n            logger.info(""Loading train set from supplied dicts "")\n            self.data[""train""], self.tensor_names = self._get_dataset(filename=None, dicts=train_dicts)\n        elif self.processor.train_filename:\n            # or from a file (default)\n            train_file = self.processor.data_dir / self.processor.train_filename\n            logger.info(""Loading train set from: {} "".format(train_file))\n            self.data[""train""], self.tensor_names = self._get_dataset(train_file)\n        else:\n            logger.info(""No train set is being loaded"")\n            self.data[""train""] = None\n\n        # dev data\n        if dev_dicts:\n            # either from supplied dicts\n            logger.info(""Loading train set from supplied dicts "")\n            self.data[""dev""], self.tensor_names = self._get_dataset(filename=None, dicts=dev_dicts)\n        elif self.processor.dev_filename:\n            # or from file (default)\n            dev_file = self.processor.data_dir / self.processor.dev_filename\n            logger.info(""Loading dev set from: {}"".format(dev_file))\n            self.data[""dev""], _ = self._get_dataset(dev_file)\n        elif self.processor.dev_split > 0.0:\n            # or split it apart from train set\n            logger.info(""Loading dev set as a slice of train set"")\n            self._create_dev_from_train()\n        else:\n            logger.info(""No dev set is being loaded"")\n            self.data[""dev""] = None\n\n        # test data\n        if test_dicts:\n            # either from supplied dicts\n            logger.info(""Loading train set from supplied dicts "")\n            self.data[""test""], self.tensor_names = self._get_dataset(filename=None, dicts=test_dicts)\n        elif self.processor.test_filename:\n            # or from file (default)\n            test_file = self.processor.data_dir / self.processor.test_filename\n            logger.info(""Loading test set from: {}"".format(test_file))\n            if self.tensor_names:\n                self.data[""test""], _ = self._get_dataset(test_file)\n            else:\n                self.data[""test""], self.tensor_names = self._get_dataset(test_file)\n        else:\n            logger.info(""No test set is being loaded"")\n            self.data[""test""] = None\n\n        if self.caching:\n            self._save_dataset_to_cache()\n\n        # derive stats and meta data\n        self._calculate_statistics()\n        # self.calculate_class_weights()\n\n        self._initialize_data_loaders()\n\n    def _load_dataset_from_cache(self, cache_dir):\n        """"""\n        Load serialized dataset from a cache.\n        """"""\n        logger.info(f""Loading datasets from cache at {cache_dir}"")\n        self.data[""train""] = torch.load(cache_dir / ""train_dataset"")\n\n        dev_dataset_path = cache_dir / ""dev_dataset""\n        if dev_dataset_path.exists():\n            self.data[""dev""] = torch.load(dev_dataset_path)\n        else:\n            self.data[""dev""] = None\n\n        test_dataset_path = cache_dir / ""test_dataset""\n        if test_dataset_path.exists():\n            self.data[""test""] = torch.load(test_dataset_path)\n        else:\n            self.data[""test""] = None\n\n        self.tensor_names = torch.load(cache_dir / ""tensor_names"")\n\n        # derive stats and meta data\n        self._calculate_statistics()\n        # self.calculate_class_weights()\n\n        self._initialize_data_loaders()\n\n    def _get_checksum(self):\n        """"""\n        Get checksum based on a dict to ensure validity of cached DataSilo\n        """"""\n        # keys in the dict identifies uniqueness for a given DataSilo.\n        payload_dict = {\n            ""train_filename"": str(Path(self.processor.train_filename).absolute()),\n            ""data_dir"": str(self.processor.data_dir.absolute()),\n            ""max_seq_len"": self.processor.max_seq_len,\n            ""dev_split"": self.processor.dev_split,\n            ""tasks"": self.processor.tasks\n        }\n        checksum = get_dict_checksum(payload_dict)\n        return checksum\n\n    def _save_dataset_to_cache(self):\n        """"""\n        Serialize and save dataset to a cache.\n        """"""\n        checksum = self._get_checksum()\n\n        cache_dir = self.cache_path / checksum\n        cache_dir.mkdir(parents=True, exist_ok=True)\n\n        torch.save(self.data[""train""], cache_dir / ""train_dataset"")\n\n        if self.data[""dev""]:\n            torch.save(self.data[""dev""], cache_dir / ""dev_dataset"")\n\n        if self.data[""test""]:\n            torch.save(self.data[""test""], cache_dir / ""test_dataset"")\n\n        torch.save(self.tensor_names, cache_dir / ""tensor_names"")\n        logger.info(f""Cached the datasets at {cache_dir}"")\n\n    def _initialize_data_loaders(self):\n        """""" Initializing train, dev and test data loaders for the already loaded datasets """"""\n\n        if self.processor.train_filename:\n            if self.distributed:\n                sampler_train = DistributedSampler(self.data[""train""])\n            else:\n                sampler_train = RandomSampler(self.data[""train""])\n\n            data_loader_train = NamedDataLoader(\n                dataset=self.data[""train""],\n                sampler=sampler_train,\n                batch_size=self.batch_size,\n                tensor_names=self.tensor_names,\n            )\n        else:\n            data_loader_train = None\n\n        if self.data[""dev""] is not None:\n            data_loader_dev = NamedDataLoader(\n                dataset=self.data[""dev""],\n                sampler=SequentialSampler(self.data[""dev""]),\n                batch_size=self.batch_size,\n                tensor_names=self.tensor_names,\n            )\n        else:\n            data_loader_dev = None\n\n        if self.processor.test_filename:\n            data_loader_test = NamedDataLoader(\n                dataset=self.data[""test""],\n                sampler=SequentialSampler(self.data[""test""]),\n                batch_size=self.batch_size,\n                tensor_names=self.tensor_names,\n            )\n        else:\n            data_loader_test = None\n\n        self.loaders = {\n            ""train"": data_loader_train,\n            ""dev"": data_loader_dev,\n            ""test"": data_loader_test,\n        }\n\n    def _create_dev_from_train(self):\n        """""" Split a dev set apart from the train dataset """"""\n        n_dev = int(self.processor.dev_split * len(self.data[""train""]))\n        n_train = len(self.data[""train""]) - n_dev\n\n        train_dataset, dev_dataset = self.random_split_ConcatDataset(self.data[""train""], lengths=[n_train, n_dev])\n        self.data[""train""] = train_dataset\n        if(len(dev_dataset) > 0):\n            self.data[""dev""] = dev_dataset\n        else:\n            logger.warning(""No dev set created. Please adjust the dev_split parameter."")\n\n        logger.info(\n            f""Took {len(dev_dataset)} samples out of train set to create dev set (dev split is roughly {self.processor.dev_split})""\n        )\n\n    def random_split_ConcatDataset(self, ds, lengths):\n        """"""\n        Roughly split a Concatdataset into non-overlapping new datasets of given lengths.\n        Samples inside Concatdataset should already be shuffled\n\n        :param ds: Dataset to be split\n        :type ds: Dataset\n        :param lengths: lengths of splits to be produced\n        :type lengths: list\n        """"""\n        if sum(lengths) != len(ds):\n            raise ValueError(""Sum of input lengths does not equal the length of the input dataset!"")\n\n        try:\n            idx_dataset = np.where(np.array(ds.cumulative_sizes) > lengths[0])[0][0]\n        except IndexError:\n            raise Exception(""All dataset chunks are being assigned to train set leaving no samples for dev set. ""\n                            ""Either consider increasing dev_split or setting it to 0.0\\n""\n                            f""Cumulative chunk sizes: {ds.cumulative_sizes}\\n""\n                            f""train/dev split: {lengths}"")\n\n        assert idx_dataset >= 1, ""Dev_split ratio is too large, there is no data in train set. "" \\\n                             f""Please lower dev_split = {self.processor.dev_split}""\n\n        train = ConcatDataset(ds.datasets[:idx_dataset])\n        test = ConcatDataset(ds.datasets[idx_dataset:])\n        return train, test\n\n    def _calculate_statistics(self):\n        """""" Calculate and log simple summary statistics of the datasets """"""\n\n        self.counts = {}\n\n        if self.data[""train""]:\n            self.counts[""train""] = len(self.data[""train""])\n        else:\n            self.counts[""train""] = 0\n\n        if self.data[""dev""]:\n            self.counts[""dev""] = len(self.data[""dev""])\n        else:\n            self.counts[""dev""] = 0\n\n        if self.data[""test""]:\n            self.counts[""test""] = len(self.data[""test""])\n        else:\n            self.counts[""test""] = 0\n\n        seq_lens = []\n        if self.data[""train""]:\n            for dataset in self.data[""train""].datasets:\n                train_input_numpy = dataset[:][0].numpy()\n                seq_lens.extend(np.sum(train_input_numpy != self.processor.tokenizer.pad_token_id, axis=1))\n            max_seq_len = dataset[:][0].shape[1]\n\n        self.clipped = np.mean(np.array(seq_lens) == max_seq_len) if seq_lens else 0\n        self.ave_len = np.mean(seq_lens) if seq_lens else 0\n\n        logger.info(""Examples in train: {}"".format(self.counts[""train""]))\n        logger.info(""Examples in dev  : {}"".format(self.counts[""dev""]))\n        logger.info(""Examples in test : {}"".format(self.counts[""test""]))\n        logger.info("""")\n        if self.data[""train""]:\n            logger.info(""Longest sequence length observed after clipping:     {}"".format(max(seq_lens)))\n            logger.info(""Average sequence length after clipping: {}"".format(self.ave_len))\n            logger.info(""Proportion clipped:      {}"".format(self.clipped))\n            if self.clipped > 0.5:\n                logger.info(""[Farmer\'s Tip] {}% of your samples got cut down to {} tokens. ""\n                            ""Consider increasing max_seq_len. ""\n                            ""This will lead to higher memory consumption but is likely to ""\n                            ""improve your model performance"".format(round(self.clipped * 100, 1), max_seq_len))\n\n        MlLogger.log_params(\n            {\n                ""n_samples_train"": self.counts[""train""],\n                ""n_samples_dev"": self.counts[""dev""],\n                ""n_samples_test"": self.counts[""test""],\n                ""batch_size"": self.batch_size,\n                ""ave_seq_len"": self.ave_len,\n                ""clipped"": self.clipped,\n            }\n        )\n\n    def calculate_class_weights(self, task_name, source=""train""):\n        """""" For imbalanced datasets, we can calculate class weights that can be used later in the\n        loss function of the prediction head to upweight the loss of minorities.\n\n        :param task_name: name of the task as used in the processor\n        :type task_name: str\n        """"""\n        \n        tensor_name = self.processor.tasks[task_name][""label_tensor_name""]\n        label_list = self.processor.tasks[task_name][""label_list""]\n        tensor_idx = list(self.tensor_names).index(tensor_name)\n        # we need at least ONE observation for each label to avoid division by zero in compute_class_weights.\n        observed_labels = copy.deepcopy(label_list)\n        if source == ""all"":\n            datasets = self.data.values()\n        elif source == ""train"":\n            datasets = [self.data[""train""]]\n        else:\n            raise Exception(""source argument expects one of [\\""train\\"", \\""all\\""]"")\n        for dataset in datasets:\n            if ""multilabel"" in self.processor.tasks[task_name][""task_type""]:\n                for x in dataset:\n                    observed_labels += [label_list[label_id] for label_id in (x[tensor_idx] == 1).nonzero()]\n            else:\n                observed_labels += [label_list[x[tensor_idx].item()] for x in dataset]\n\n        #TODO scale e.g. via logarithm to avoid crazy spikes for rare classes\n        class_weights = compute_class_weight(""balanced"", np.asarray(label_list), observed_labels)\n        # conversion necessary to have class weights of same type as model weights\n        class_weights = class_weights.astype(np.float32)\n        return class_weights\n\n    def get_data_loader(self, dataset_name):\n        return self.loaders[dataset_name]\n\n    def n_samples(self, dataset_name):\n        """"""\n        Returns the number of samples in a given dataset.\n\n        :param dataset_name: Choose from train, dev or test\n        :type dataset_name: str\n        """"""\n        return self.counts[dataset_name]\n\n\nclass StreamingDataSilo:\n    """"""\n    Streaming Data Silo loads and preprocesses datasets in parallel to the model training.\n\n    The samples are lazily created from the input file and batches are yielded on-the-fly when required during training.\n    This is useful if you:\n    - work with large datasets that don\'t fit in memory\n    - want to save time (by not preprocessing the entire dataset before starting training)\n\n    For optimal training performance and efficient utilization of shiny GPUs, the pipeline always keeps a few\n    pre-computed batches ready to avoid any waiting time when a batch is requested during training.\n\n    To parallelize the creation of batches, PyTorch DataLoader provide an option to use\n    multiple workers that utilize the available CPU cores and ensure enough pre-computed batches.\n    """"""\n\n    def __init__(self, processor, batch_size, dataloader_workers=8):\n        """"""\n        :param processor: A dataset specific Processor object which will turn input file into a Pytorch Dataset.\n        :type processor: Processor\n        :param batch_size: The size of batch to use for model training.\n        :type batch_size: int\n        :param dataloader_workers: number of workers for PyTorch DataLoader to create batches in parallel\n        :type dataloader_workers: int\n        """"""\n\n        self.processor = processor\n        self.batch_size = batch_size\n        self.dataloader_workers = dataloader_workers\n\n    def get_data_loader(self, dataset_name):\n        """"""\n        Returns a new instance of dataloader for the given dataset.\n\n        The dataloader lazily yields from Iterable DataSets. After a complete iteration\n        over the input data, the generators gets exhausted. So, for instance, in the \n        case of model training, a new train dataloader must be used for each train epoch.\n\n        :param dataset_name: \'train\', \'dev\', or \'test\' set.\n        :type dataset_name: str\n        """"""\n        filename = None\n        if dataset_name == ""train"":\n            filename = self.processor.train_filename\n        elif dataset_name == ""dev"":\n            if self.processor.dev_split > 0.0:\n                raise NotImplemented(\n                            ""StreamingDataSilo does not have dev_split implemented. ""\n                            ""To use dev data, supply a dev filename when creating the Processor.""\n                )\n            elif self.processor.dev_filename:\n                filename = self.processor.dev_filename\n        elif dataset_name == ""test"":\n            if self.processor.test_filename:\n                filename = self.processor.test_filename\n\n        if not filename:\n            return None\n\n        #  Batching:\n        #\n        #  The model Trainer is passed a PyTorch DataLoader instance that yields dataset batches for training.\n        #\n        #  By default, the PyTorch DataLoader prefetch (2 * num_workers) samples. However, given the higher\n        #  batch sizes(usually >64) for model training, the default prefetch is not sufficient to keep the\n        #  model Training saturated with datasets.\n        #\n        #  As a workaround, we yield batches of samples instead of yielding individual samples. The DataLoader\n        #  can then prefetch (2 * num_workers) number of batches of samples.\n        #\n        #  Since the batching is now handled within _StreamingDataSet, we disable the batching on DataLoader side\n        #  by initializing the data loader with batch_size as 1.\n\n        data_set = _StreamingDataSet(\n            processor=self.processor,\n            filepath=self.processor.data_dir / filename,\n            batch_size=self.batch_size,\n            dataloader_workers=self.dataloader_workers,\n        )\n        data_loader = NamedDataLoader(\n            dataset=data_set, batch_size=1, num_workers=self.dataloader_workers, pin_memory=True\n        )\n        return data_loader\n\n\nclass _StreamingDataSet(IterableDataset):\n    def __init__(self, processor, filepath, batch_size, dataloader_workers):\n        """"""\n        :param processor: A dataset specific Processor object which will turn input file into a Pytorch Dataset.\n        :type processor: Processor\n        :param batch_size: The size of batch that should be returned by the DataLoaders.\n        :type batch_size: int\n        :param filepath: input filename to load the dataset from\n        :type filepath: Path\n        :param dataloader_workers: number of workers for PyTorch Dataloader\n        :type dataloader_workers: int\n        """"""\n\n        self.batch_size = batch_size\n        self.processor = processor\n        self.filepath = filepath\n        self.dataloader_workers = dataloader_workers\n\n        # calculate number of samples for __len__()\n        total_lines = sum(1 for line in open(filepath, encoding=""utf-8""))\n        empty_lines = sum(1 if line == ""\\n"" else 0 for line in open(filepath, encoding=""utf-8""))\n        self.n_samples = total_lines - (2 * empty_lines)\n\n        self.file_to_dicts_generator = processor.file_to_dicts(filepath)\n\n    def __len__(self):\n        return self.n_samples\n\n    def __iter__(self):\n        #  With IterableDataset, the same __iter__ is copied over to the multiple workers of\n        #  a Dataloader. Hence, we need to configure the __iter__ to not yield duplicated data\n        #  when more than 1 workers are used.\n        #\n        #  To avoid duplicates, we need to split the input dicts between the workers.\n        #  The grouper() converts a dict generator given as input and yields only the\n        #  dicts that are to be processed by the given worker_id.\n        #\n        #  For instance, consider input as [dictA, dictB, dictC, ...], then the grouper\n        #  (with n=2) will return, [[dictA, dictB], [dictE, dictF] ...] for worker 1 and\n        #  [[dictC, dictD], [dictG, dictH] ...] for worker 2.\n\n        if self.dataloader_workers > 1:\n            worker_info = torch.utils.data.get_worker_info()\n            worker_id = worker_info.id\n            dicts = grouper(\n                self.file_to_dicts_generator, n=10, worker_id=worker_id, total_workers=self.dataloader_workers\n            )\n        else:\n            dicts = grouper(self.file_to_dicts_generator, n=10)\n\n        results = map(self._dataset_from_chunk, dicts)\n\n        batch = []\n        for datasets, tensor_names in results:\n            if not datasets:\n                continue\n            self.tensor_names = tensor_names\n            for ds in datasets:\n                batch.append(ds)\n                if len(batch) == self.batch_size:\n                    yield batch\n                    batch = []\n        if batch:\n            yield batch\n\n    def _dataset_from_chunk(self, chunk):\n        """"""\n        Creating a dataset for a chunk (= subset) of dicts.\n        :param chunk: Instead of only having a list of dicts here we also supply an index (ascending int) for each.\n            => [(0, dict), (1, dict) ...]\n        :type chunk: list of tuples\n        :return: PyTorch Dataset\n        """"""\n        dicts = [d[1] for d in chunk]\n        # need at least 2 documents to sample random sentences from\n        if len(dicts) < 2 and type(self.processor) == BertStyleLMProcessor:\n            logger.info(""Skipping a dict chunk as it contains less than 2 documents ..."")\n            return None, None\n        indices = [x[0] for x in chunk]\n        datasets, tensor_names = self.processor.dataset_from_dicts(dicts=dicts, indices=indices)\n        return datasets, tensor_names\n\n\nclass DataSiloForCrossVal:\n    """"""\n    For performing cross validation, we really want to combine all the instances from all\n    the sets or just some of the sets, then create a different data silo instance for each fold.\n    Calling DataSiloForCrossVal.make() creates a list of DataSiloForCrossVal instances - one for each fold.\n    """"""\n\n    def __init__(self, origsilo, trainset, devset, testset):\n        self.tensor_names = origsilo.tensor_names\n        self.data = {""train"": trainset, ""dev"": devset, ""test"": testset}\n        self.processor = origsilo.processor\n        self.batch_size = origsilo.batch_size\n        # should not be necessary, xval makes no sense with huge data\n        # sampler_train = DistributedSampler(self.data[""train""])\n        sampler_train = RandomSampler(trainset)\n\n        self.data_loader_train = NamedDataLoader(\n            dataset=trainset,\n            sampler=sampler_train,\n            batch_size=self.batch_size,\n            tensor_names=self.tensor_names,\n        )\n        self.data_loader_dev = NamedDataLoader(\n            dataset=devset,\n            sampler=SequentialSampler(devset),\n            batch_size=self.batch_size,\n            tensor_names=self.tensor_names,\n        )\n        self.data_loader_test = NamedDataLoader(\n            dataset=testset,\n            sampler=SequentialSampler(testset),\n            batch_size=self.batch_size,\n            tensor_names=self.tensor_names,\n        )\n        self.loaders = {\n            ""train"": self.data_loader_train,\n            ""dev"": self.data_loader_dev,\n            ""test"": self.data_loader_test,\n        }\n\n    def get_data_loader(self, which):\n        return self.loaders[which]\n\n    @classmethod\n    def make(cls, datasilo, sets=[""train"", ""dev"", ""test""], n_splits=5, shuffle=True, random_state=None,\n             stratified=True, n_neg_answers_per_question=1):\n        """"""\n        Create number of folds data-silo-like objects which can be used for training from the\n        original data silo passed on.\n\n        :param datasilo: the data silo that contains the original data\n        :type datasilo: DataSilo\n        :param sets: which sets to use to create the xval folds (strings)\n        :type sets: list\n        :param n_splits: number of folds to create\n        :type n_splits: int\n        :param shuffle: shuffle each class\' samples before splitting\n        :type shuffle: bool\n        :param random_state: random state for shuffling\n        :type random_state: int\n        :param stratified: if class stratification should be done\n        :type stratified: bool\n        :param n_neg_answers_per_question: number of negative answers per question to include for training\n        :type n_neg_answers_per_question: int\n        """"""\n\n        if ""question_answering"" in datasilo.processor.tasks:\n            return cls._make_question_answering(datasilo, sets, n_splits, shuffle, random_state, n_neg_answers_per_question)\n        else:\n            return cls._make(datasilo, sets, n_splits, shuffle, random_state, stratified)\n\n    @classmethod\n    def _make_question_answering(cls, datasilo, sets=[""train"", ""dev"", ""test""], n_splits=5, shuffle=True,\n                                 random_state=None, n_neg_answers_per_question=1):\n        """"""\n        Create number of folds data-silo-like objects which can be used for training from the\n        original data silo passed on. This function takes into account the characteristics of the\n        data for question-answering-\n\n        :param datasilo: the data silo that contains the original data\n        :type datasilo: DataSilo\n        :param sets: which sets to use to create the xval folds (strings)\n        :type sets: list\n        :param n_splits: number of folds to create\n        :type n_splits: int\n        :param shuffle: shuffle each class\' samples before splitting\n        :type shuffle: bool\n        :param random_state: random state for shuffling\n        :type random_state: int\n        :param n_neg_answers_per_question: number of negative answers per question to include for training\n        :type n_neg_answers_per_question: int\n        """"""\n        assert datasilo.tensor_names[4] == ""id"", f""Expected tensor \'id\' at index 4, found {datasilo.tensor_names[4]}""\n        assert datasilo.tensor_names[7] == ""labels"", f""Expected tensor \'labels\' at index 7, found {datasilo.tensor_names[7]}""\n\n        sets_to_concat = []\n        for setname in sets:\n            if datasilo.data[setname]:\n                sets_to_concat.extend(datasilo.data[setname])\n        all_data = ConcatDataset(sets_to_concat)\n\n        documents = []\n        keyfunc = lambda x: x[4][0]\n        all_data = sorted(all_data.datasets, key=keyfunc)\n        for key, document in groupby(all_data, key=keyfunc):\n            documents.append(list(document))\n\n        xval_split = cls._split_for_qa(documents, n_splits, shuffle, random_state)\n        silos = []\n\n        for train_set, test_set in xval_split:\n            # Each training set is further divided into actual train and dev set\n            if datasilo.processor.dev_split > 0:\n                dev_split = datasilo.processor.dev_split\n                n_dev = int(np.ceil(dev_split * len(train_set)))\n                assert n_dev > 0, f""dev split of {dev_split} is not large enough to split away a development set""\n                n_actual_train = len(train_set) - n_dev\n                actual_train_set = train_set[:n_actual_train]\n                dev_set = train_set[n_actual_train:]\n                ds_dev = [sample for document in dev_set for sample in document]\n            else:\n                ds_dev = None\n                actual_train_set = train_set\n\n            train_samples = []\n            for doc in actual_train_set:\n                keyfunc = lambda x: x[4][1]\n                doc = sorted(doc, key=keyfunc)\n                for key, question in groupby(doc, key=keyfunc):\n                    # add all available answrs to train set\n                    sample_list = list(question)\n                    neg_answer_idx = []\n                    for index, sample in enumerate(sample_list):\n                        if sample[7][0][0] or sample[7][0][1]:\n                            train_samples.append(sample)\n                        else:\n                            neg_answer_idx.append(index)\n                    # add random n_neg_answers_per_question samples to train set\n                    if len(neg_answer_idx) <= n_neg_answers_per_question:\n                        train_samples.extend([sample_list[idx] for idx in neg_answer_idx])\n                    else:\n                        neg_answer_idx = random.sample(neg_answer_idx, n_neg_answers_per_question)\n                        train_samples.extend([sample_list[idx] for idx in neg_answer_idx])\n\n            ds_train = train_samples\n            ds_test = [sample for document in test_set for sample in document]\n            silos.append(DataSiloForCrossVal(datasilo, ds_train, ds_dev, ds_test))\n        return silos\n\n    @staticmethod\n    def _make(datasilo, sets=[""train"", ""dev"", ""test""], n_splits=5, shuffle=True,\n              random_state=None, stratified=True):\n        """"""\n        Create number of folds data-silo-like objects which can be used for training from the\n        original data silo passed on.\n\n        :param datasilo: the data silo that contains the original data\n        :param sets: which sets to use to create the xval folds\n        :param n_splits: number of folds to create\n        :param shuffle: shuffle each class\' samples before splitting\n        :param random_state: random state for shuffling\n        :param stratified: if class stratification should be done\n        """"""\n        setstoconcat = [datasilo.data[setname] for setname in sets]\n        ds_all = ConcatDataset(setstoconcat)\n        idxs = list(range(len(ds_all)))\n        dev_split = datasilo.processor.dev_split\n        if stratified:\n            # get all the labels for stratification\n            ytensors = [t[3][0] for t in ds_all]\n            Y = torch.stack(ytensors)\n            xval = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n            xval_split = xval.split(idxs,Y)\n        else:\n            xval = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n            xval_split = xval.split(idxs)\n        # for each fold create a DataSilo4Xval instance, where the training set is further\n        # divided into actual train and dev set\n        silos = []\n        for train_idx, test_idx in xval_split:\n            n_dev = int(dev_split * len(train_idx))\n            n_actual_train = len(train_idx) - n_dev\n            # TODO: this split into actual train and test set could/should also be stratified, for now\n            # we just do this by taking the first/last indices from the train set (which should be\n            # shuffled by default)\n            actual_train_idx = train_idx[:n_actual_train]\n            dev_idx = train_idx[n_actual_train:]\n            # create the actual datasets\n            ds_train = Subset(ds_all, actual_train_idx)\n            ds_dev = Subset(ds_all, dev_idx)\n            ds_test = Subset(ds_all, test_idx)\n            silos.append(DataSiloForCrossVal(datasilo, ds_train, ds_dev, ds_test))\n        return silos\n\n    @staticmethod\n    def _split_for_qa(documents, n_splits=5, shuffle=True, random_state=None):\n        keyfunc = lambda x: x[4][1]\n        if shuffle:\n            random.shuffle(documents, random_state)\n\n        questions_per_doc = []\n        for doc in documents:\n            # group samples in current doc by question id\n            doc = sorted(doc, key=keyfunc)\n            questions = list(groupby(doc, key=keyfunc))\n            questions_per_doc.append(len(questions))\n\n        # split documents into n_splits splits with approximately same number of questions per split\n        questions_per_doc = np.array(questions_per_doc)\n        accumulated_questions_per_doc = questions_per_doc.cumsum()\n        questions_per_fold = accumulated_questions_per_doc[-1] // n_splits\n        accumulated_questions_per_fold = np.array(range(1, n_splits)) * questions_per_fold\n        if accumulated_questions_per_fold[0] < accumulated_questions_per_doc[0]:\n            accumulated_questions_per_fold[0] = accumulated_questions_per_doc[0] + 1\n        indices_to_split_at = np.searchsorted(accumulated_questions_per_doc, accumulated_questions_per_fold, side=""right"")\n        splits = np.split(documents, indices_to_split_at)\n\n        for split in splits:\n            assert len(split) > 0\n\n        for idx, split in enumerate(splits):\n            current_test_set = split\n            current_train_set = np.hstack(np.delete(splits, idx, axis=0))\n\n            yield current_train_set, current_test_set\n'"
farm/data_handler/dataloader.py,2,"b'from math import ceil\n\nfrom torch.utils.data import DataLoader, Dataset, Sampler\nimport torch\n\n\nclass NamedDataLoader(DataLoader):\n    """"""\n    A modified version of the PyTorch DataLoader that returns a dictionary where the key is\n    the name of the tensor and the value is the tensor itself.\n    """"""\n\n    def __init__(self, dataset, batch_size, sampler=None, tensor_names=None, num_workers=0, pin_memory=False):\n        """"""\n        :param dataset: The dataset that will be wrapped by this NamedDataLoader\n        :type dataset: Dataset\n        :param sampler: The sampler used by the NamedDataLoader to choose which samples to include in the batch\n        :type sampler: Sampler\n        :param batch_size: The size of the batch to be returned by the NamedDataLoader\n        :type batch_size: int\n        :param tensor_names: The names of the tensor, in the order that the dataset returns them in.\n        :type tensor_names: list\n        :param num_workers: number of workers to use for the DataLoader\n        :type num_workers: int\n        :param pin_memory: argument for Data Loader to use page-locked memory for faster transfer of data to GPU\n        :type pin_memory: bool\n        """"""\n\n        def collate_fn(batch):\n            """"""\n            A custom collate function that formats the batch as a dictionary where the key is\n            the name of the tensor and the value is the tensor itself\n            """"""\n\n            if type(dataset).__name__ == ""_StreamingDataSet"":\n                _tensor_names = dataset.tensor_names\n            else:\n                _tensor_names = tensor_names\n\n            if type(batch[0]) == list:\n                batch = batch[0]\n\n            assert len(batch[0]) == len(\n                _tensor_names\n            ), ""Dataset contains {} tensors while there are {} tensor names supplied: {}"".format(\n                len(batch[0]), len(_tensor_names), _tensor_names\n            )\n            lists_temp = [[] for _ in range(len(_tensor_names))]\n            ret = dict(zip(_tensor_names, lists_temp))\n\n            for example in batch:\n                for name, tensor in zip(_tensor_names, example):\n                    ret[name].append(tensor)\n\n            for key in ret:\n                ret[key] = torch.stack(ret[key])\n\n            return ret\n\n        super(NamedDataLoader, self).__init__(\n            dataset=dataset,\n            sampler=sampler,\n            batch_size=batch_size,\n            collate_fn=collate_fn,\n            pin_memory=pin_memory,\n            num_workers=num_workers,\n        )\n\n    def __len__(self):\n        if type(self.dataset).__name__ == ""_StreamingDataSet"":\n            num_samples = len(self.dataset)\n            num_batches = ceil(num_samples / self.dataset.batch_size)\n            return num_batches\n        else:\n            return super().__len__()\n\n\ndef covert_dataset_to_dataloader(dataset, sampler, batch_size):\n    """"""\n    Wraps a PyTorch Dataset with a DataLoader.\n\n    :param dataset: Dataset to be wrapped.\n    :type dataset: Dataset\n    :param sampler: PyTorch sampler used to pick samples in a batch.\n    :type sampler: Sampler\n    :param batch_size: Number of samples in the batch.\n    :return: A DataLoader that wraps the input Dataset.\n    """"""\n    sampler_initialized = sampler(dataset)\n    data_loader = DataLoader(\n        dataset, sampler=sampler_initialized, batch_size=batch_size\n    )\n    return data_loader\n'"
farm/data_handler/dataset.py,5,"b'import torch\nfrom torch.utils.data import TensorDataset\n\n\n# TODO we need the option to handle different dtypes\ndef convert_features_to_dataset(features):\n    """"""\n    Converts a list of feature dictionaries (one for each sample) into a PyTorch Dataset.\n\n    :param features: A list of dictionaries. Each dictionary corresponds to one sample. Its keys are the\n                     names of the type of feature and the keys are the features themselves.\n    :Return: a Pytorch dataset and a list of tensor names.\n    """"""\n    # features can be an empty list in cases where down sampling occurs (e.g. Natural Questions downsamples\n    # instances of is_impossible\n    if len(features) == 0:\n        return None, None\n    tensor_names = list(features[0].keys())\n    all_tensors = []\n    for t_name in tensor_names:\n        try:\n            cur_tensor = torch.tensor(\n                [sample[t_name] for sample in features], dtype=torch.long\n            )\n        except ValueError:\n            cur_tensor = torch.tensor(\n                [sample[t_name] for sample in features], dtype=torch.float32\n            )\n\n        all_tensors.append(cur_tensor)\n\n    dataset = TensorDataset(*all_tensors)\n    return dataset, tensor_names\n'"
farm/data_handler/input_features.py,0,"b'""""""\nContains functions that turn readable clear text input into dictionaries of features\n""""""\n\n\nimport logging\nimport collections\nfrom dotmap import DotMap\nimport numpy as np\n\nfrom farm.data_handler.samples import Sample\nfrom farm.data_handler.utils import (\n    expand_labels,\n    pad,\n    mask_random_words,\n    convert_id\n)\nfrom farm.modeling.tokenization import insert_at_special_tokens_pos\n\nlogger = logging.getLogger(__name__)\n\n\ndef sample_to_features_text(\n    sample, tasks, max_seq_len, tokenizer\n):\n    """"""\n    Generates a dictionary of features for a given input sample that is to be consumed by a text classification model.\n\n    :param sample: Sample object that contains human readable text and label fields from a single text classification data sample\n    :type sample: Sample\n    :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)\n    :type tasks: dict\n    :param max_seq_len: Sequences are truncated after this many tokens\n    :type max_seq_len: int\n    :param tokenizer: A tokenizer object that can turn string sentences into a list of tokens\n    :return: A list with one dictionary containing the keys ""input_ids"", ""padding_mask"" and ""segment_ids"" (also ""label_ids"" if not\n             in inference mode). The values are lists containing those features.\n    :rtype: list\n    """"""\n\n    #TODO It might be cleaner to adjust the data structure in sample.tokenized\n    # Verify if this current quickfix really works for pairs\n    tokens_a = sample.tokenized[""tokens""]\n    tokens_b = sample.tokenized.get(""tokens_b"", None)\n\n    inputs = tokenizer.encode_plus(\n        tokens_a,\n        tokens_b,\n        add_special_tokens=True,\n        max_length=max_seq_len,\n        truncation_strategy=\'do_not_truncate\',\n        return_token_type_ids=True\n    )\n\n    input_ids, segment_ids = inputs[""input_ids""], inputs[""token_type_ids""]\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    padding_mask = [1] * len(input_ids)\n\n    # Padding up to the sequence length.\n    # Normal case: adding multiple 0 to the right\n    # Special cases:\n    # a) xlnet pads on the left and uses  ""4""  for padding token_type_ids\n    if tokenizer.__class__.__name__ == ""XLNetTokenizer"":\n        pad_on_left = True\n        segment_ids = pad(segment_ids, max_seq_len, 4, pad_on_left=pad_on_left)\n    else:\n        pad_on_left = False\n        segment_ids = pad(segment_ids, max_seq_len, 0, pad_on_left=pad_on_left)\n\n    input_ids = pad(input_ids, max_seq_len, tokenizer.pad_token_id, pad_on_left=pad_on_left)\n    padding_mask = pad(padding_mask, max_seq_len, 0, pad_on_left=pad_on_left)\n\n\n    assert len(input_ids) == max_seq_len\n    assert len(padding_mask) == max_seq_len\n    assert len(segment_ids) == max_seq_len\n\n    feat_dict = {\n        ""input_ids"": input_ids,\n        ""padding_mask"": padding_mask,\n        ""segment_ids"": segment_ids,\n    }\n\n    # Add Labels for different tasks\n    for task_name, task in tasks.items():\n        try:\n            label_name = task[""label_name""]\n            label_raw = sample.clear_text[label_name]\n            label_list = task[""label_list""]\n            if task[""task_type""] == ""classification"":\n                # id of label\n                try:\n                    label_ids = [label_list.index(label_raw)]\n                except ValueError as e:\n                    raise ValueError(f\'[Task: {task_name}] Observed label {label_raw} not in defined label_list\')\n            elif task[""task_type""] == ""multilabel_classification"":\n                # multi-hot-format\n                label_ids = [0] * len(label_list)\n                for l in label_raw.split("",""):\n                    if l != """":\n                        label_ids[label_list.index(l)] = 1\n            elif task[""task_type""] == ""regression"":\n                label_ids = [float(label_raw)]\n            else:\n                raise ValueError(task[""task_type""])\n        except KeyError:\n            # For inference mode we don\'t expect labels\n            label_ids = None\n        if label_ids is not None:\n            feat_dict[task[""label_tensor_name""]] = label_ids\n    return [feat_dict]\n\n\ndef samples_to_features_ner(\n    sample,\n    tasks,\n    max_seq_len,\n    tokenizer,\n    non_initial_token=""X"",\n    **kwargs\n):\n    """"""\n    Generates a dictionary of features for a given input sample that is to be consumed by an NER model.\n\n    :param sample: Sample object that contains human readable text and label fields from a single NER data sample\n    :type sample: Sample\n    :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)\n    :type tasks: dict\n    :param max_seq_len: Sequences are truncated after this many tokens\n    :type max_seq_len: int\n    :param tokenizer: A tokenizer object that can turn string sentences into a list of tokens\n    :param non_initial_token: Token that is inserted into the label sequence in positions where there is a\n                              non-word-initial token. This is done since the default NER performs prediction\n                              only on word initial tokens\n    :return: A list with one dictionary containing the keys ""input_ids"", ""padding_mask"", ""segment_ids"", ""initial_mask""\n             (also ""label_ids"" if not in inference mode). The values are lists containing those features.\n    :rtype: list\n    """"""\n\n    tokens = sample.tokenized[""tokens""]\n    inputs = tokenizer.encode_plus(text=tokens,\n                                   text_pair=None,\n                                   add_special_tokens=True,\n                                   max_length=max_seq_len,\n                                   truncation_strategy=\'do_not_truncate\', # We\'ve already truncated our tokens before\n                                   return_special_tokens_mask=True,\n                                   return_token_type_ids=True\n                                   )\n\n    input_ids, segment_ids, special_tokens_mask = inputs[""input_ids""], inputs[""token_type_ids""], inputs[""special_tokens_mask""]\n\n    # We construct a mask to identify the first token of a word. We will later only use them for predicting entities.\n    # Special tokens don\'t count as initial tokens => we add 0 at the positions of special tokens\n    # For BERT we add a 0 in the start and end (for CLS and SEP)\n    initial_mask = [int(x) for x in sample.tokenized[""start_of_word""]]\n    initial_mask = insert_at_special_tokens_pos(initial_mask, special_tokens_mask, insert_element=0)\n    assert len(initial_mask) == len(input_ids)\n\n    for task_name, task in tasks.items():\n        try:\n            label_list = task[""label_list""]\n            label_name = task[""label_name""]\n            label_tensor_name = task[""label_tensor_name""]\n            labels_word = sample.clear_text[label_name]\n            labels_token = expand_labels(labels_word, initial_mask, non_initial_token)\n            # labels_token = add_cls_sep(labels_token, cls_token, sep_token)\n            label_ids = [label_list.index(lt) for lt in labels_token]\n        except ValueError:\n            label_ids = None\n            problematic_labels = set(labels_token).difference(set(label_list))\n            logger.warning(f""[Task: {task_name}] Could not convert labels to ids via label_list!""\n                           f""\\nWe found a problem with labels {str(problematic_labels)}"")\n        except KeyError:\n            # For inference mode we don\'t expect labels\n            label_ids = None\n            logger.warning(f""[Task: {task_name}] Could not convert labels to ids via label_list!""\n                           ""\\nIf your are running in *inference* mode: Don\'t worry!""\n                           ""\\nIf you are running in *training* mode: Verify you are supplying a proper label list to your processor and check that labels in input data are correct."")\n\n        # This mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        padding_mask = [1] * len(input_ids)\n\n        # Padding up to the sequence length.\n        # Normal case: adding multiple 0 to the right\n        # Special cases:\n        # a) xlnet pads on the left and uses  ""4"" for padding token_type_ids\n        if tokenizer.__class__.__name__ == ""XLNetTokenizer"":\n            pad_on_left = True\n            segment_ids = pad(segment_ids, max_seq_len, 4, pad_on_left=pad_on_left)\n        else:\n            pad_on_left = False\n            segment_ids = pad(segment_ids, max_seq_len, 0, pad_on_left=pad_on_left)\n\n        input_ids = pad(input_ids, max_seq_len, tokenizer.pad_token_id, pad_on_left=pad_on_left)\n        padding_mask = pad(padding_mask, max_seq_len, 0, pad_on_left=pad_on_left)\n        initial_mask = pad(initial_mask, max_seq_len, 0, pad_on_left=pad_on_left)\n        if label_ids:\n            label_ids = pad(label_ids, max_seq_len, 0, pad_on_left=pad_on_left)\n\n        feature_dict = {\n            ""input_ids"": input_ids,\n            ""padding_mask"": padding_mask,\n            ""segment_ids"": segment_ids,\n            ""initial_mask"": initial_mask,\n        }\n\n        if label_ids:\n            feature_dict[label_tensor_name] = label_ids\n\n    return [feature_dict]\n\n\ndef samples_to_features_bert_lm(sample, max_seq_len, tokenizer, next_sent_pred=True):\n    """"""\n    Convert a raw sample (pair of sentences as tokenized strings) into a proper training sample with\n    IDs, LM labels, padding_mask, CLS and SEP tokens etc.\n\n    :param sample: Sample, containing sentence input as strings and is_next label\n    :type sample: Sample\n    :param max_seq_len: Maximum length of sequence.\n    :type max_seq_len: int\n    :param tokenizer: Tokenizer\n    :return: InputFeatures, containing all inputs and labels of one sample as IDs (as used for model training)\n    """"""\n\n    if next_sent_pred:\n        tokens_a = sample.tokenized[""text_a""][""tokens""]\n        tokens_b = sample.tokenized[""text_b""][""tokens""]\n\n        # mask random words\n        tokens_a, t1_label = mask_random_words(tokens_a, tokenizer.vocab,\n                                               token_groups=sample.tokenized[""text_a""][""start_of_word""])\n\n        tokens_b, t2_label = mask_random_words(tokens_b, tokenizer.vocab,\n                                               token_groups=sample.tokenized[""text_b""][""start_of_word""])\n        # convert lm labels to ids\n        t1_label_ids = [-1 if tok == \'\' else tokenizer.convert_tokens_to_ids(tok) for tok in t1_label]\n        t2_label_ids = [-1 if tok == \'\' else tokenizer.convert_tokens_to_ids(tok) for tok in t2_label]\n        lm_label_ids = t1_label_ids + t2_label_ids\n\n        # Convert is_next_label: Note that in Bert, is_next_labelid = 0 is used for next_sentence=true!\n        if sample.clear_text[""nextsentence_label""]:\n            is_next_label_id = [0]\n        else:\n            is_next_label_id = [1]\n    else:\n        tokens_a = sample.tokenized[""text_a""][""tokens""]\n        tokens_b = None\n        tokens_a, t1_label = mask_random_words(tokens_a, tokenizer.vocab,\n                                               token_groups=sample.tokenized[""text_a""][""start_of_word""])\n        # convert lm labels to ids\n        lm_label_ids = [-1 if tok == \'\' else tokenizer.convert_tokens_to_ids(tok) for tok in t1_label]\n\n    # encode string tokens to input_ids and add special tokens\n    inputs = tokenizer.encode_plus(text=tokens_a,\n                                   text_pair=tokens_b,\n                                   add_special_tokens=True,\n                                   max_length=max_seq_len,\n                                   truncation_strategy=\'do_not_truncate\',\n                                   # We\'ve already truncated our tokens before\n                                   return_special_tokens_mask=True,\n                                   return_token_type_ids=True\n                                   )\n\n    input_ids, segment_ids, special_tokens_mask = inputs[""input_ids""], inputs[""token_type_ids""], inputs[\n        ""special_tokens_mask""]\n\n    # account for special tokens (CLS, SEP, SEP..) in lm_label_ids\n    lm_label_ids = insert_at_special_tokens_pos(lm_label_ids, special_tokens_mask, insert_element=-1)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    padding_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    # Padding up to the sequence length.\n    # Normal case: adding multiple 0 to the right\n    # Special cases:\n    # a) xlnet pads on the left and uses  ""4"" for padding token_type_ids\n    if tokenizer.__class__.__name__ == ""XLNetTokenizer"":\n        pad_on_left = True\n        segment_ids = pad(segment_ids, max_seq_len, 4, pad_on_left=pad_on_left)\n    else:\n        pad_on_left = False\n        segment_ids = pad(segment_ids, max_seq_len, 0, pad_on_left=pad_on_left)\n\n    input_ids = pad(input_ids, max_seq_len, tokenizer.pad_token_id, pad_on_left=pad_on_left)\n    padding_mask = pad(padding_mask, max_seq_len, 0, pad_on_left=pad_on_left)\n    lm_label_ids = pad(lm_label_ids, max_seq_len, -1, pad_on_left=pad_on_left)\n\n    feature_dict = {\n        ""input_ids"": input_ids,\n        ""padding_mask"": padding_mask,\n        ""segment_ids"": segment_ids,\n        ""lm_label_ids"": lm_label_ids,\n    }\n\n    if next_sent_pred:\n        feature_dict[""nextsentence_label_ids""] = is_next_label_id\n\n    assert len(input_ids) == max_seq_len\n    assert len(padding_mask) == max_seq_len\n    assert len(segment_ids) == max_seq_len\n    assert len(lm_label_ids) == max_seq_len\n\n    return [feature_dict]\n\n\ndef sample_to_features_qa(sample, tokenizer, max_seq_len, answer_type_list=None, max_answers=6):\n    """""" Prepares data for processing by the model. Supports cases where there are\n    multiple answers for the one question/document pair. max_answers is by default set to 6 since\n    that is the most number of answers in the squad2.0 dev set.""""""\n\n    # Initialize some basic variables\n    question_tokens = sample.tokenized[""question_tokens""]\n    question_start_of_word = sample.tokenized[""question_start_of_word""]\n    question_len_t = len(question_tokens)\n    passage_start_t = sample.tokenized[""passage_start_t""]\n    passage_tokens = sample.tokenized[""passage_tokens""]\n    passage_start_of_word = sample.tokenized[""passage_start_of_word""]\n    passage_len_t = len(passage_tokens)\n    answers = sample.tokenized[""answers""]\n    sample_id = convert_id(sample.id)\n\n    # Generates a numpy array of shape (max_answers, 2) where (i, 2) indexes into the start and end indices\n    # of the ith answer. The array is filled with -1 since the number of answers is often less than max_answers\n    # no answer labels are represented by (0,0)\n    labels, answer_types = generate_labels(answers,\n                                           passage_len_t,\n                                           question_len_t,\n                                           tokenizer,\n                                           answer_type_list=answer_type_list,\n                                           max_answers=max_answers)\n\n    # Generate a start of word vector for the full sequence (i.e. question + answer + special tokens).\n    # This will allow us to perform evaluation during training without clear text.\n    # Note that in the current implementation, special tokens do not count as start of word.\n    start_of_word = combine_vecs(question_start_of_word, passage_start_of_word, tokenizer, spec_tok_val=0)\n\n    # Combines question_tokens and passage_tokens (str) into a single encoded vector of token indices (int)\n    # called input_ids. This encoded vector also contains special tokens (e.g. [CLS]). It will have length =\n    # (question_len_t + passage_len_t + n_special_tokens). This may be less than max_seq_len but will not be greater\n    # than max_seq_len since truncation was already performed when the document was chunked into passages\n    # (c.f. create_samples_squad() )\n    encoded = tokenizer.encode_plus(text=sample.tokenized[""question_tokens""],\n                                    text_pair=sample.tokenized[""passage_tokens""],\n                                    add_special_tokens=True,\n                                    max_length=None,\n                                    truncation_strategy=\'only_second\',\n                                    return_token_type_ids=True,\n                                    return_tensors=None)\n    input_ids = encoded[""input_ids""]\n    segment_ids = encoded[""token_type_ids""]\n\n    # seq_2_start_t is the index of the first token in the second text sequence (e.g. passage)\n    if tokenizer.__class__.__name__ in [""RobertaTokenizer"", ""XLMRobertaTokenizer""]:\n        seq_2_start_t = get_roberta_seq_2_start(input_ids)\n    else:\n        seq_2_start_t = segment_ids.index(1)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    padding_mask = [1] * len(input_ids)\n\n    # Pad up to the sequence length. For certain models, the pad token id is not 0 (e.g. Roberta where it is 1)\n    pad_idx = tokenizer.pad_token_id\n    padding = [pad_idx] * (max_seq_len - len(input_ids))\n    zero_padding = [0] * (max_seq_len - len(input_ids))\n\n    input_ids += padding\n    padding_mask += zero_padding\n    segment_ids += zero_padding\n    start_of_word += zero_padding\n\n    # The XLM-Roberta tokenizer generates a segment_ids vector that separates the first sequence from the second.\n    # However, when this is passed in to the forward fn of the Roberta model, it throws an error since\n    # Roberta has only a single token embedding (!!!). To get around this, we want to have a segment_ids\n    # vec that is only 0s\n    if tokenizer.__class__.__name__ in [""XLMRobertaTokenizer"", ""RobertaTokenizer""]:\n        segment_ids = np.zeros_like(segment_ids)\n\n    # Todo: explain how only the first of labels will be used in train, and the full array will be used in eval\n    # TODO Offset, start of word and spec_tok_mask are not actually needed by model.forward() but are needed for model.formatted_preds()\n    # TODO passage_start_t is index of passage\'s first token  relative to document\n    # I don\'t think we actually need offsets anymore\n    feature_dict = {""input_ids"": input_ids,\n                    ""padding_mask"": padding_mask,\n                    ""segment_ids"": segment_ids,\n                    ""answer_type_ids"": answer_types,\n                    ""id"": sample_id,\n                    ""passage_start_t"": passage_start_t,\n                    ""start_of_word"": start_of_word,\n                    ""labels"": labels,\n                    ""seq_2_start_t"": seq_2_start_t}\n    return [feature_dict]\n\n\ndef generate_labels(answers, passage_len_t, question_len_t, tokenizer, max_answers, answer_type_list=None):\n    """"""\n    Creates QA label for each answer in answers. The labels are the index of the start and end token\n    relative to the passage. They are contained in an array of size (max_answers, 2).\n    -1 used to fill array since there the number of answers is often less than max_answers.\n    The index values take in to consideration the question tokens, and also special tokens such as [CLS].\n    When the answer is not fully contained in the passage, or the question\n    is impossible to answer, the start_idx and end_idx are 0 i.e. start and end are on the very first token\n    (in most models, this is the [CLS] token). Note that in our implementation NQ has 4 labels\n    [""is_impossible"", ""yes"", ""no"", ""span""] and this is what answer_type_list should look like""""""\n\n    label_idxs = np.full((max_answers, 2), fill_value=-1)\n    answer_types = np.full((max_answers), fill_value=-1)\n\n    # If there are no answers\n    if len(answers) == 0:\n        label_idxs[0, :] = 0\n        answer_types[:] = 0\n        return label_idxs, answer_types\n\n    for i, answer in enumerate(answers):\n        answer_type = answer[""answer_type""]\n        start_idx = answer[""start_t""]\n        end_idx = answer[""end_t""]\n\n        # We are going to operate on one-hot label vectors which will later be converted back to label indices.\n        # This is to take advantage of tokenizer.encode_plus() which applies model dependent special token conventions.\n        # The two label vectors (start and end) are composed of sections that correspond to the question and\n        # passage tokens. These are initialized here. The section corresponding to the question\n        # will always be composed of 0s.\n        start_vec_question = [0] * question_len_t\n        end_vec_question = [0] * question_len_t\n        start_vec_passage = [0] * passage_len_t\n        end_vec_passage = [0] * passage_len_t\n\n        # If the answer is in the current passage, populate the label vector with 1s for start and end\n        if answer_in_passage(start_idx, end_idx, passage_len_t):\n            start_vec_passage[start_idx] = 1\n            end_vec_passage[end_idx] = 1\n\n        # Combine the sections of the label vectors. The length of each of these will be:\n        # question_len_t + passage_len_t + n_special_tokens\n        start_vec = combine_vecs(start_vec_question,\n                                    start_vec_passage,\n                                    tokenizer,\n                                    spec_tok_val=0)\n        end_vec = combine_vecs(end_vec_question,\n                                  end_vec_passage,\n                                  tokenizer,\n                                  spec_tok_val=0)\n\n        start_label_present = 1 in start_vec\n        end_label_present = 1 in end_vec\n\n        # This is triggered if the answer is not in the passage or the question is_impossible\n        # In both cases, the token at idx=0 (in BERT, this is the [CLS] token) is given both the start and end label\n        if start_label_present is False and end_label_present is False:\n            start_vec[0] = 1\n            end_vec[0] = 1\n            answer_type = ""is_impossible""\n        elif start_label_present is False or end_label_present is False:\n            raise Exception(""The label vectors are lacking either a start or end label"")\n\n        # Ensure label vectors are one-hot\n        assert sum(start_vec) == 1\n        assert sum(end_vec) == 1\n\n        start_idx = start_vec.index(1)\n        end_idx = end_vec.index(1)\n\n        label_idxs[i, 0] = start_idx\n        label_idxs[i, 1] = end_idx\n\n        # Only Natural Questions trains a classification head on answer_type, SQuAD only has the QA head. answer_type_list\n        # will be None for SQuAD but something like [""is_impossible"", ""span"", ""yes"", ""no""] for Natural Questions\n        if answer_type_list:\n            answer_types[i] = answer_type_list.index(answer_type)\n\n    assert np.max(label_idxs) > -1\n\n    return label_idxs, answer_types\n\n\n\ndef combine_vecs(question_vec, passage_vec, tokenizer, spec_tok_val=-1):\n    """""" Combine a question_vec and passage_vec in a style that is appropriate to the model. Will add slots in\n    the returned vector for special tokens like [CLS] where the value is determine by spec_tok_val.""""""\n\n    # Join question_label_vec and passage_label_vec and add slots for special tokens\n    vec = tokenizer.build_inputs_with_special_tokens(token_ids_0=question_vec,\n                                                     token_ids_1=passage_vec)\n    spec_toks_mask = tokenizer.get_special_tokens_mask(token_ids_0=question_vec,\n                                                       token_ids_1=passage_vec)\n\n    # If a value in vec corresponds to a special token, it will be replaced with spec_tok_val\n    combined = [v if not special_token else spec_tok_val for v, special_token in zip(vec, spec_toks_mask)]\n\n    return combined\n\n\ndef answer_in_passage(start_idx, end_idx, passage_len):\n    if passage_len > start_idx > 0 and passage_len > end_idx > 0:\n        return True\n    return False\n\ndef get_roberta_seq_2_start(input_ids):\n    # This commit (https://github.com/huggingface/transformers/commit/dfe012ad9d6b6f0c9d30bc508b9f1e4c42280c07)from\n    # huggingface transformers now means that RobertaTokenizer.encode_plus returns only zeros in token_type_ids. Therefore, we need\n    # another way to infer the start of the second input sequence in RoBERTa. Roberta input sequences have the following\n    # format: <s> P1 </s> </s> P2 </s>\n    # <s> has index 0 and </s> has index 2. To find the beginning of the second sequence, this function first finds\n    # the index of the second </s>\n    first_backslash_s = input_ids.index(2)\n    second_backslash_s = input_ids.index(2, first_backslash_s + 1)\n    return second_backslash_s + 1\n\ndef sample_to_features_squadOLD(\n    sample, tokenizer, max_seq_len, doc_stride, max_query_length, tasks,\n):\n    sample.clear_text = DotMap(sample.clear_text, _dynamic=False)\n    is_training = sample.clear_text.is_training\n\n    unique_id = 1000000000\n    features = []\n\n    query_tokens = tokenizer.tokenize(sample.clear_text.question_text)\n\n    if len(query_tokens) > max_query_length:\n        query_tokens = query_tokens[0:max_query_length]\n\n    tok_to_orig_index = []\n    orig_to_tok_index = []\n    all_doc_tokens = []\n    for (i, token) in enumerate(sample.clear_text.doc_tokens):\n        orig_to_tok_index.append(len(all_doc_tokens))\n        sub_tokens = tokenizer.tokenize(token)\n        for sub_token in sub_tokens:\n            tok_to_orig_index.append(i)\n            all_doc_tokens.append(sub_token)\n\n    tok_start_position = None\n    tok_end_position = None\n    if is_training and sample.clear_text.is_impossible:\n        tok_start_position = -1\n        tok_end_position = -1\n    if is_training and not sample.clear_text.is_impossible:\n        tok_start_position = orig_to_tok_index[sample.clear_text.start_position]\n        if sample.clear_text.end_position < len(sample.clear_text.doc_tokens) - 1:\n            tok_end_position = orig_to_tok_index[sample.clear_text.end_position + 1] - 1\n        else:\n            tok_end_position = len(all_doc_tokens) - 1\n        (tok_start_position, tok_end_position) = _SQUAD_improve_answer_span(\n            all_doc_tokens,\n            tok_start_position,\n            tok_end_position,\n            tokenizer,\n            sample.clear_text.orig_answer_text,\n        )\n\n    # The -3 accounts for [CLS], [SEP] and [SEP]\n    max_tokens_for_doc = max_seq_len - len(query_tokens) - 3\n\n    # We can have documents that are longer than the maximum sequence length.\n    # To deal with this we do a sliding window approach, where we take chunks\n    # of the up to our max length with a stride of `doc_stride`.\n    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n        ""DocSpan"", [""start"", ""length""]\n    )\n    doc_spans = []\n    start_offset = 0\n    while start_offset < len(all_doc_tokens):\n        length = len(all_doc_tokens) - start_offset\n        if length > max_tokens_for_doc:\n            length = max_tokens_for_doc\n        doc_spans.append(_DocSpan(start=start_offset, length=length))\n        if start_offset + length == len(all_doc_tokens):\n            break\n        start_offset += min(length, doc_stride)\n\n    for (doc_span_index, doc_span) in enumerate(doc_spans):\n        tokens = []\n        segment_ids = []\n        tokens.append(""[CLS]"")\n        segment_ids.append(0)\n        for token in query_tokens:\n            tokens.append(token)\n            segment_ids.append(0)\n        tokens.append(""[SEP]"")\n        segment_ids.append(0)\n\n        for i in range(doc_span.length):\n            split_token_index = doc_span.start + i\n            tokens.append(all_doc_tokens[split_token_index])\n            segment_ids.append(1)\n        tokens.append(""[SEP]"")\n        segment_ids.append(1)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        padding_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        while len(input_ids) < max_seq_len:\n            input_ids.append(0)\n            padding_mask.append(0)\n            segment_ids.append(0)\n\n        assert len(input_ids) == max_seq_len\n        assert len(padding_mask) == max_seq_len\n        assert len(segment_ids) == max_seq_len\n\n        start_position = 0\n        end_position = 0\n        if is_training and not sample.clear_text.is_impossible:\n            # For training, if our document chunk does not contain an annotation\n            # we keep it but set the start and end position to unanswerable\n            doc_start = doc_span.start\n            doc_end = doc_span.start + doc_span.length - 1\n            out_of_span = False\n            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):\n                out_of_span = True\n            if out_of_span:\n                start_position = 0\n                end_position = 0\n            else:\n                doc_offset = len(query_tokens) + 2\n                start_position = tok_start_position - doc_start + doc_offset\n                end_position = tok_end_position - doc_start + doc_offset\n        if is_training and sample.clear_text.is_impossible:\n            start_position = 0\n            end_position = 0\n\n        inp_feat = {}\n        inp_feat[""input_ids""] = input_ids\n        inp_feat[""padding_mask""] = padding_mask  # attention_mask\n        inp_feat[""segment_ids""] = segment_ids  # token_type_ids\n        inp_feat[""start_position""] = start_position\n        inp_feat[""end_position""] = end_position\n        inp_feat[""is_impossible""] = sample.clear_text.is_impossible\n        inp_feat[""sample_id""] = sample.id\n        inp_feat[""passage_shift""] = doc_span.start\n        features.append(inp_feat)\n        unique_id += 1\n\n    return features\n\n\ndef _SQUAD_improve_answer_span(\n    doc_tokens, input_start, input_end, tokenizer, orig_answer_text\n):\n    """"""Returns tokenized answer spans that better match the annotated answer.""""""\n\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a ""better match"". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be ""(1895-1943)."". However\n    # after tokenization, our tokens will be ""( 1895 - 1943 ) ."". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose ""Japan"" as a character sub-span of\n    # the word ""Japanese"". Since our WordPiece tokenizer does not split\n    # ""Japanese"", we just use ""Japanese"" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = "" "".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = "" "".join(doc_tokens[new_start : (new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n'"
farm/data_handler/processor.py,0,"b'import abc\nimport inspect\nimport json\nimport logging\nimport os\nimport random\nfrom abc import ABC\nfrom inspect import signature\nfrom pathlib import Path\nfrom random import randint\n\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\nfrom numpy.random import random as random_float\nfrom farm.data_handler.dataset import convert_features_to_dataset\nfrom farm.data_handler.input_features import (\n    samples_to_features_ner,\n    samples_to_features_bert_lm,\n    sample_to_features_text,\n    sample_to_features_qa,\n)\nfrom farm.data_handler.samples import (\n    Sample,\n    SampleBasket,\n    create_samples_qa\n)\n\nfrom farm.data_handler.utils import (\n    read_tsv,\n    read_tsv_sentence_pair,\n    read_docs_from_txt,\n    read_ner_file,\n    read_squad_file,\n    read_jsonl,\n    is_json,\n    get_sentence_pair,\n    split_with_metadata,\n    convert_qa_input_dict,\n    get_sequence_pair,\n    join_sentences\n)\nfrom farm.modeling.tokenization import Tokenizer, tokenize_with_metadata, truncate_sequences\nfrom farm.utils import MLFlowLogger as MlLogger\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Processor(ABC):\n    """"""\n    Is used to generate PyTorch Datasets from input data. An implementation of this abstract class should be created\n    for each new data source.\n    Implement the abstract methods: file_to_dicts(), _dict_to_samples(), _sample_to_features()\n    to be compatible with your data format\n    """"""\n\n    subclasses = {}\n\n    def __init__(\n        self,\n        tokenizer,\n        max_seq_len,\n        train_filename,\n        dev_filename,\n        test_filename,\n        dev_split,\n        data_dir,\n        tasks={},\n        proxies=None\n    ):\n        """"""\n        :param tokenizer: Used to split a sentence (str) into tokens.\n        :param max_seq_len: Samples are truncated after this many tokens.\n        :type max_seq_len: int\n        :param train_filename: The name of the file containing training data.\n        :type train_filename: str\n        :param dev_filename: The name of the file containing the dev data. If None and 0.0 < dev_split < 1.0 the dev set\n                             will be a slice of the train set.\n        :type dev_filename: str or None\n        :param test_filename: The name of the file containing test data.\n        :type test_filename: str\n        :param dev_split: The proportion of the train set that will sliced. Only works if dev_filename is set to None\n        :type dev_split: float\n        :param data_dir: The directory in which the train, test and perhaps dev files can be found.\n        :type data_dir: str\n        :param tasks: Tasks for which the processor shall extract labels from the input data.\n                      Usually this includes a single, default task, e.g. text classification.\n                      In a multitask setting this includes multiple tasks, e.g. 2x text classification.\n                      The task name will be used to connect with the related PredictionHead.\n        :type tasks: dict\n        :param proxies: proxy configuration to allow downloads of remote datasets.\n                    Format as in  ""requests"" library: https://2.python-requests.org//en/latest/user/advanced/#proxies\n        :type proxies: dict\n        """"""\n\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n        self.tasks = tasks\n        self.proxies = proxies\n\n        # data sets\n        self.train_filename = train_filename\n        self.dev_filename = dev_filename\n        self.test_filename = test_filename\n        self.dev_split = dev_split\n        if data_dir:\n            self.data_dir = Path(data_dir)\n        else:\n            self.data_dir = None\n        self.baskets = []\n\n        self._log_params()\n\n    def __init_subclass__(cls, **kwargs):\n        """""" This automatically keeps track of all available subclasses.\n        Enables generic load() and load_from_dir() for all specific Processor implementation.\n        """"""\n        super().__init_subclass__(**kwargs)\n        cls.subclasses[cls.__name__] = cls\n\n    @classmethod\n    def load(\n        cls,\n        processor_name,\n        data_dir,\n        tokenizer,\n        max_seq_len,\n        train_filename,\n        dev_filename,\n        test_filename,\n        dev_split,\n        **kwargs,\n    ):\n        """"""\n        Loads the class of processor specified by processor name.\n\n        :param processor_name: The class of processor to be loaded.\n        :type processor_name: str\n        :param data_dir: Directory where data files are located.\n        :type data_dir: str\n        :param tokenizer: A tokenizer object\n        :param max_seq_len: Sequences longer than this will be truncated.\n        :type max_seq_len: int\n        :param train_filename: The name of the file containing training data.\n        :type train_filename: str\n        :param dev_filename: The name of the file containing the dev data.\n                             If None and 0.0 < dev_split < 1.0 the dev set\n                             will be a slice of the train set.\n        :type dev_filename: str or None\n        :param test_filename: The name of the file containing test data.\n        :type test_filename: str\n        :param dev_split: The proportion of the train set that will sliced.\n                          Only works if dev_filename is set to None\n        :type dev_split: float\n        :param kwargs: placeholder for passing generic parameters\n        :type kwargs: object\n        :return: An instance of the specified processor.\n        """"""\n\n        sig = signature(cls.subclasses[processor_name])\n        unused_args = {k: v for k, v in kwargs.items() if k not in sig.parameters}\n        logger.debug(\n            f""Got more parameters than needed for loading {processor_name}: {unused_args}. ""\n            f""Those won\'t be used!""\n        )\n        processor = cls.subclasses[processor_name](\n            data_dir=data_dir,\n            tokenizer=tokenizer,\n            max_seq_len=max_seq_len,\n            train_filename=train_filename,\n            dev_filename=dev_filename,\n            test_filename=test_filename,\n            dev_split=dev_split,\n            **kwargs,\n        )\n\n        return processor\n\n    @classmethod\n    def load_from_dir(cls, load_dir):\n        """"""\n         Infers the specific type of Processor from a config file (e.g. GNADProcessor) and loads an instance of it.\n\n        :param load_dir: str, directory that contains a \'processor_config.json\'\n        :return: An instance of a Processor Subclass (e.g. GNADProcessor)\n        """"""\n        # read config\n        processor_config_file = Path(load_dir) / ""processor_config.json""\n        config = json.load(open(processor_config_file))\n        config[""inference""] = True\n        # init tokenizer\n        if ""lower_case"" in config.keys():\n            logger.warning(""Loading tokenizer from deprecated FARM config. ""\n                           ""If you used `custom_vocab` or `never_split_chars`, this won\'t work anymore."")\n            tokenizer = Tokenizer.load(load_dir, tokenizer_class=config[""tokenizer""], do_lower_case=config[""lower_case""])\n        else:\n            tokenizer = Tokenizer.load(load_dir, tokenizer_class=config[""tokenizer""])\n\n        # we have to delete the tokenizer string from config, because we pass it as Object\n        del config[""tokenizer""]\n\n        processor = cls.load(tokenizer=tokenizer, processor_name=config[""processor""], **config)\n\n        for task_name, task in config[""tasks""].items():\n            processor.add_task(name=task_name,\n                               metric=task[""metric""],\n                               label_list=task[""label_list""],\n                               label_column_name=task[""label_column_name""],\n                               text_column_name=task[""text_column_name""],\n                               task_type=task[""task_type""])\n\n        if processor is None:\n            raise Exception\n\n        return processor\n\n    def save(self, save_dir):\n        """"""\n        Saves the vocabulary to file and also creates a json file containing all the\n        information needed to load the same processor.\n\n        :param save_dir: Directory where the files are to be saved\n        :type save_dir: str\n        """"""\n        os.makedirs(save_dir, exist_ok=True)\n        config = self.generate_config()\n        # save tokenizer incl. attributes\n        config[""tokenizer""] = self.tokenizer.__class__.__name__\n        self.tokenizer.save_pretrained(save_dir)\n        # save processor\n        config[""processor""] = self.__class__.__name__\n        output_config_file = Path(save_dir) / ""processor_config.json""\n        with open(output_config_file, ""w"") as file:\n            json.dump(config, file)\n\n    def generate_config(self):\n        """"""\n        Generates config file from Class and instance attributes (only for sensible config parameters).\n        """"""\n        config = {}\n        # self.__dict__ doesn\'t give parent class attributes\n        for key, value in inspect.getmembers(self):\n            if is_json(value) and key[0] != ""_"":\n                if issubclass(type(value), Path):\n                    value = str(value)\n                config[key] = value\n        return config\n\n    def add_task(self, name,  metric, label_list, label_column_name=None,\n                 label_name=None, task_type=None, text_column_name=None):\n        if type(label_list) is not list:\n            raise ValueError(f""Argument `label_list` must be of type list. Got: f{type(label_list)}"")\n\n        if label_name is None:\n            label_name = f""{name}_label""\n        label_tensor_name = label_name + ""_ids""\n        self.tasks[name] = {\n            ""label_list"": label_list,\n            ""metric"": metric,\n            ""label_tensor_name"": label_tensor_name,\n            ""label_name"": label_name,\n            ""label_column_name"": label_column_name,\n            ""text_column_name"": text_column_name,\n            ""task_type"": task_type\n        }\n\n    @abc.abstractmethod\n    def file_to_dicts(self, file: str) -> [dict]:\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def _dict_to_samples(cls, dictionary: dict, all_dicts=None) -> [Sample]:\n        raise NotImplementedError()\n\n    @abc.abstractmethod\n    def _sample_to_features(cls, sample: Sample) -> dict:\n        raise NotImplementedError()\n\n    def _init_samples_in_baskets(self):\n        for basket in self.baskets:\n            all_dicts = [b.raw for b in self.baskets]\n            try:\n                basket.samples = self._dict_to_samples(dictionary=basket.raw, all_dicts=all_dicts)\n                for num, sample in enumerate(basket.samples):\n                     sample.id = f""{basket.id}-{num}""\n            except:\n                logger.error(f""Could not create sample(s) from this dict: \\n {basket.raw}"")\n                raise\n\n    def _featurize_samples(self):\n        for basket in self.baskets:\n            for sample in basket.samples:\n                try:\n                    sample.features = self._sample_to_features(sample=sample)\n                except:\n                    logger.error(f""Could not convert this sample to features: \\n {sample}"")\n                    raise\n\n    def _create_dataset(self, keep_baskets=False):\n        features_flat = []\n        for basket in self.baskets:\n            for sample in basket.samples:\n                features_flat.extend(sample.features)\n        if not keep_baskets:\n            # free up some RAM, we don\'t need baskets from here on\n            self.baskets = None\n        dataset, tensor_names = convert_features_to_dataset(features=features_flat)\n        return dataset, tensor_names\n\n    def dataset_from_dicts(self, dicts, indices=None, rest_api_schema=False, return_baskets = False):\n        """"""\n        Contains all the functionality to turn a list of dict objects into a PyTorch Dataset and a\n        list of tensor names. This can be used for inference mode.\n\n        :param dicts: List of dictionaries where each contains the data of one input sample.\n        :type dicts: list of dicts\n        :return: a Pytorch dataset and a list of tensor names.\n        """"""\n        if rest_api_schema:\n            id_prefix = ""infer""\n        else:\n            id_prefix = ""train""\n        # We need to add the index (coming from multiprocessing chunks) to have a unique basket ID\n        if indices:\n            self.baskets = [\n                SampleBasket(raw=tr, id=f""{id_prefix}-{index}"")\n                for (tr, index) in zip(dicts, indices)\n            ]\n        else:\n            self.baskets = [\n                SampleBasket(raw=tr, id=f""{id_prefix}-{i}"")\n                for (i, tr) in enumerate(dicts)\n            ]\n        self._init_samples_in_baskets()\n        self._featurize_samples()\n        if indices:\n            if 0 in indices:\n                self._log_samples(2)\n        else:\n            self._log_samples(2)\n        if return_baskets:\n            dataset, tensor_names = self._create_dataset(keep_baskets=True)\n            return dataset, tensor_names, self.baskets\n        else:\n            dataset, tensor_names = self._create_dataset()\n            return dataset, tensor_names\n\n    def _log_samples(self, n_samples):\n        logger.info(""*** Show {} random examples ***"".format(n_samples))\n        for i in range(n_samples):\n            random_basket = random.choice(self.baskets)\n            random_sample = random.choice(random_basket.samples)\n            logger.info(random_sample)\n\n    def _log_params(self):\n        params = {\n            ""processor"": self.__class__.__name__,\n            ""tokenizer"": self.tokenizer.__class__.__name__,\n        }\n        names = [""max_seq_len"", ""dev_split""]\n        for name in names:\n            value = getattr(self, name)\n            params.update({name: str(value)})\n        MlLogger.log_params(params)\n\n\n#########################################\n# Processors for Text Classification ####\n#########################################\nclass TextClassificationProcessor(Processor):\n    """"""\n    Used to handle the text classification datasets that come in tabular format (CSV, TSV, etc.)\n    """"""\n    def __init__(\n        self,\n        tokenizer,\n        max_seq_len,\n        data_dir,\n        label_list=None,\n        metric=None,\n        train_filename=""train.tsv"",\n        dev_filename=None,\n        test_filename=""test.tsv"",\n        dev_split=0.1,\n        delimiter=""\\t"",\n        quote_char=""\'"",\n        skiprows=None,\n        label_column_name=""label"",\n        multilabel=False,\n        header=0,\n        proxies=None,\n        max_samples=None,\n        text_column_name=""text"",\n        **kwargs\n    ):\n        """"""\n        :param tokenizer: Used to split a sentence (str) into tokens.\n        :param max_seq_len: Samples are truncated after this many tokens.\n        :type max_seq_len: int\n        :param data_dir: The directory in which the train and dev files can be found.\n                         If not available the dataset will be loaded automaticaly\n                         if the last directory has the same name as a predefined dataset.\n                         These predefined datasets are defined as the keys in the dict at\n                         `farm.data_handler.utils.DOWNSTREAM_TASK_MAP <https://github.com/deepset-ai/FARM/blob/master/farm/data_handler/utils.py>`_.\n        :type data_dir: str\n        :param label_list: list of labels to predict (strings). For most cases this should be: [""start_token"", ""end_token""]\n        :type label_list: list\n        :param metric: name of metric that shall be used for evaluation, e.g. ""acc"" or ""f1_macro"".\n                 Alternatively you can also supply a custom function, that takes preds and labels as args and returns a numerical value.\n                 For using multiple metrics supply them as a list, e.g [""acc"", my_custom_metric_fn].\n        :type metric: str, function, or list\n        :param train_filename: The name of the file containing training data.\n        :type train_filename: str\n        :param dev_filename: The name of the file containing the dev data. If None and 0.0 < dev_split < 1.0 the dev set\n                             will be a slice of the train set.\n        :type dev_filename: str or None\n        :param test_filename: None\n        :type test_filename: str\n        :param dev_split: The proportion of the train set that will sliced. Only works if dev_filename is set to None\n        :type dev_split: float\n        :param delimiter: Separator used in the input tsv / csv file\n        :type delimiter: str\n        :param quote_char: Character used for quoting strings in the input tsv/ csv file\n        :type quote_char: str\n        :param skiprows: number of rows to skip in the tsvs (e.g. for multirow headers)\n        :type skiprows: int\n        :param label_column_name: name of the column in the input csv/tsv that shall be used as training labels\n        :type label_column_name: str\n        :param multilabel: set to True for multilabel classification\n        :type multilabel: bool\n        :param header: which line to use as a header in the input csv/tsv\n        :type  header: int\n        :param proxies: proxy configuration to allow downloads of remote datasets.\n                        Format as in  ""requests"" library: https://2.python-requests.org//en/latest/user/advanced/#proxies\n        :type proxies: dict\n        :param text_column_name: name of the column in the input csv/tsv that shall be used as training text\n        :type text_column_name: str\n        :param kwargs: placeholder for passing generic parameters\n        :type kwargs: object\n        """"""\n        #TODO If an arg is misspelt, e.g. metrics, it will be swallowed silently by kwargs\n\n        # Custom processor attributes\n        self.delimiter = delimiter\n        self.quote_char = quote_char\n        self.skiprows = skiprows\n        self.header = header\n        self.max_samples = max_samples\n\n        super(TextClassificationProcessor, self).__init__(\n            tokenizer=tokenizer,\n            max_seq_len=max_seq_len,\n            train_filename=train_filename,\n            dev_filename=dev_filename,\n            test_filename=test_filename,\n            dev_split=dev_split,\n            data_dir=data_dir,\n            tasks={},\n            proxies=proxies,\n\n        )\n        if metric and label_list:\n            if multilabel:\n                task_type = ""multilabel_classification""\n            else:\n                task_type = ""classification""\n            self.add_task(name=""text_classification"",\n                          metric=metric,\n                          label_list=label_list,\n                          label_column_name=label_column_name,\n                          text_column_name=text_column_name,\n                          task_type=task_type)\n        else:\n            logger.info(""Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for ""\n                        ""using the default task or add a custom task later via processor.add_task()"")\n\n    def file_to_dicts(self, file: str) -> [dict]:\n        column_mapping = {}\n        for task in self.tasks.values():\n            column_mapping[task[""label_column_name""]] = task[""label_name""]\n            column_mapping[task[""text_column_name""]] = ""text""\n        dicts = read_tsv(\n            filename=file,\n            delimiter=self.delimiter,\n            skiprows=self.skiprows,\n            quotechar=self.quote_char,\n            rename_columns=column_mapping,\n            header=self.header,\n            proxies=self.proxies,\n            max_samples=self.max_samples\n            )\n\n        return dicts\n\n    def _dict_to_samples(self, dictionary: dict, **kwargs) -> [Sample]:\n        # this tokenization also stores offsets and a start_of_word mask\n        text = dictionary[""text""]\n        tokenized = tokenize_with_metadata(text, self.tokenizer)\n        if len(tokenized[""tokens""]) == 0:\n            logger.warning(f""The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: {text}"")\n            return []\n        # truncate tokens, offsets and start_of_word to max_seq_len that can be handled by the model\n        for seq_name in tokenized.keys():\n            tokenized[seq_name], _, _ = truncate_sequences(seq_a=tokenized[seq_name], seq_b=None, tokenizer=self.tokenizer,\n                                                max_seq_len=self.max_seq_len)\n        return [Sample(id=None, clear_text=dictionary, tokenized=tokenized)]\n\n    def _sample_to_features(self, sample) -> dict:\n        features = sample_to_features_text(\n            sample=sample,\n            tasks=self.tasks,\n            max_seq_len=self.max_seq_len,\n            tokenizer=self.tokenizer,\n        )\n        return features\n\nclass TextPairClassificationProcessor(TextClassificationProcessor):\n    """"""\n    Used to handle text pair classification datasets (e.g. Answer Selection or Natural Inference) that come in\n    tsv format. The columns should be called text, text_b and label.\n    """"""\n    def __init__(self, **kwargs):\n        super(TextPairClassificationProcessor, self).__init__(**kwargs)\n\n    def file_to_dicts(self, file: str) -> [dict]:\n        column_mapping = {task[""label_column_name""]: task[""label_name""] for task in self.tasks.values()}\n        dicts = read_tsv_sentence_pair(\n            rename_columns=column_mapping,\n            filename=file,\n            delimiter=self.delimiter,\n            skiprows=self.skiprows,\n            proxies=self.proxies,\n        )\n        return dicts\n\n    def _dict_to_samples(self, dictionary: dict, **kwargs) -> [Sample]:\n        tokenized_a = tokenize_with_metadata(dictionary[""text""], self.tokenizer)\n        tokenized_b = tokenize_with_metadata(dictionary[""text_b""], self.tokenizer)\n\n        if len(tokenized_a[""tokens""]) == 0:\n            text = dictionary[""text""]\n            logger.warning(f""The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: {text}"")\n            return []\n        if len(tokenized_b[""tokens""]) == 0:\n            text_b = dictionary[""text_b""]\n            logger.warning(f""The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: {text_b}"")\n            return []\n\n        tokenized = {""tokens"": tokenized_a[""tokens""],\n                     ""tokens_b"": tokenized_b[""tokens""]}\n        tokenized[""tokens""], tokenized[""tokens_b""], _ = truncate_sequences(seq_a=tokenized[""tokens""],\n                                                                           seq_b=tokenized[""tokens_b""],\n                                                                           tokenizer=self.tokenizer,\n                                                                           max_seq_len=self.max_seq_len)\n        return [Sample(id=None, clear_text=dictionary, tokenized=tokenized)]\n\n\n#########################################\n# Processors for Basic Inference ####\n#########################################\nclass InferenceProcessor(Processor):\n    """"""\n    Generic processor used at inference time:\n    - fast\n    - no labels\n    - pure encoding of text into pytorch dataset\n    - Doesn\'t read from file, but only consumes dictionaries (e.g. coming from API requests)\n    """"""\n\n    def __init__(\n        self,\n        tokenizer,\n        max_seq_len,\n        **kwargs,\n    ):\n\n        super(InferenceProcessor, self).__init__(\n            tokenizer=tokenizer,\n            max_seq_len=max_seq_len,\n            train_filename=None,\n            dev_filename=None,\n            test_filename=None,\n            dev_split=None,\n            data_dir=None,\n            tasks={},\n        )\n\n    @classmethod\n    def load_from_dir(cls, load_dir):\n        """"""\n         Overwriting method from parent class to **always** load the InferenceProcessor instead of the specific class stored in the config.\n\n        :param load_dir: str, directory that contains a \'processor_config.json\'\n        :return: An instance of an InferenceProcessor\n        """"""\n        # read config\n        processor_config_file = Path(load_dir) / ""processor_config.json""\n        config = json.load(open(processor_config_file))\n        # init tokenizer\n        tokenizer = Tokenizer.load(load_dir, tokenizer_class=config[""tokenizer""])\n        # we have to delete the tokenizer string from config, because we pass it as Object\n        del config[""tokenizer""]\n\n        processor = cls.load(tokenizer=tokenizer, processor_name=""InferenceProcessor"", **config)\n        for task_name, task in config[""tasks""].items():\n            processor.add_task(name=task_name, metric=task[""metric""], label_list=task[""label_list""])\n\n        if processor is None:\n            raise Exception\n\n        return processor\n\n    def file_to_dicts(self, file: str) -> [dict]:\n        raise NotImplementedError\n\n    def _dict_to_samples(self, dictionary: dict, **kwargs) -> [Sample]:\n        # this tokenization also stores offsets\n        tokenized = tokenize_with_metadata(dictionary[""text""], self.tokenizer)\n        # truncate tokens, offsets and start_of_word to max_seq_len that can be handled by the model\n        for seq_name in tokenized.keys():\n            tokenized[seq_name], _, _ = truncate_sequences(seq_a=tokenized[seq_name], seq_b=None, tokenizer=self.tokenizer,\n                                                max_seq_len=self.max_seq_len)\n        return [Sample(id=None, clear_text=dictionary, tokenized=tokenized)]\n\n    def _sample_to_features(self, sample) -> dict:\n        features = sample_to_features_text(\n            sample=sample,\n            tasks=self.tasks,\n            max_seq_len=self.max_seq_len,\n            tokenizer=self.tokenizer,\n        )\n        return features\n\n#########################################\n# Processors for NER data ####\n#########################################\nclass NERProcessor(Processor):\n    """"""\n    Used to handle most NER datasets, like CoNLL or GermEval 2014\n    """"""\n\n    def __init__(\n        self,\n        tokenizer,\n        max_seq_len,\n        data_dir,\n        label_list=None,\n        metric=None,\n        train_filename=""train.txt"",\n        dev_filename=""dev.txt"",\n        test_filename=""test.txt"",\n        dev_split=0.0,\n        delimiter=""\\t"",\n        proxies=None,\n        **kwargs\n    ):\n        """"""\n        :param tokenizer: Used to split a sentence (str) into tokens.\n        :param max_seq_len: Samples are truncated after this many tokens.\n        :type max_seq_len: int\n        :param data_dir: The directory in which the train and dev files can be found.\n                         If not available the dataset will be loaded automaticaly\n                         if the last directory has the same name as a predefined dataset.\n                         These predefined datasets are defined as the keys in the dict at\n                         `farm.data_handler.utils.DOWNSTREAM_TASK_MAP <https://github.com/deepset-ai/FARM/blob/master/farm/data_handler/utils.py>`_.\n        :type data_dir: str\n        :param label_list: list of labels to predict (strings). For most cases this should be: [""start_token"", ""end_token""]\n        :type label_list: list\n        :param metric: name of metric that shall be used for evaluation, e.g. ""seq_f1"".\n                 Alternatively you can also supply a custom function, that takes preds and labels as args and returns a numerical value.\n                 For using multiple metrics supply them as a list, e.g [""seq_f1"", my_custom_metric_fn].\n        :type metric: str, function, or list\n        :param train_filename: The name of the file containing training data.\n        :type train_filename: str\n        :param dev_filename: The name of the file containing the dev data. If None and 0.0 < dev_split < 1.0 the dev set\n                             will be a slice of the train set.\n        :type dev_filename: str or None\n        :param test_filename: None\n        :type test_filename: str\n        :param dev_split: The proportion of the train set that will sliced. Only works if dev_filename is set to None\n        :type dev_split: float\n        :param delimiter: Separator used in the input tsv / csv file. German version of Conll03 uses a whitespace. GermEval 2014 is tab separated \\t\n        :type delimiter: str\n        :param proxies: proxy configuration to allow downloads of remote datasets.\n                        Format as in  ""requests"" library: https://2.python-requests.org//en/latest/user/advanced/#proxies\n        :type proxies: dict\n        :param kwargs: placeholder for passing generic parameters\n        :type kwargs: object\n        """"""\n        # Custom processor attributes\n        self.delimiter = delimiter\n\n        super(NERProcessor, self).__init__(\n            tokenizer=tokenizer,\n            max_seq_len=max_seq_len,\n            train_filename=train_filename,\n            dev_filename=dev_filename,\n            test_filename=test_filename,\n            dev_split=dev_split,\n            data_dir=data_dir,\n            tasks={},\n            proxies=proxies\n        )\n\n        if metric and label_list:\n            self.add_task(""ner"", metric, label_list)\n        else:\n            logger.info(""Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for ""\n                        ""using the default task or add a custom task later via processor.add_task()"")\n\n    def file_to_dicts(self, file: str) -> [dict]:\n        dicts = read_ner_file(filename=file, sep=self.delimiter,  proxies=self.proxies)\n        return dicts\n\n    def _dict_to_samples(self, dictionary: dict, **kwargs) -> [Sample]:\n        # this tokenization also stores offsets, which helps to map our entity tags back to original positions\n        tokenized = tokenize_with_metadata(dictionary[""text""], self.tokenizer)\n        if len(tokenized[""tokens""]) == 0:\n            text = dictionary[""text""]\n            logger.warning(f""The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: {text}"")\n            return []\n        # truncate tokens, offsets and start_of_word to max_seq_len that can be handled by the model\n        for seq_name in tokenized.keys():\n            tokenized[seq_name], _, _ = truncate_sequences(seq_a=tokenized[seq_name], seq_b=None, tokenizer=self.tokenizer,\n                                                max_seq_len=self.max_seq_len)\n        return [Sample(id=None, clear_text=dictionary, tokenized=tokenized)]\n\n    def _sample_to_features(self, sample) -> dict:\n        features = samples_to_features_ner(\n            sample=sample,\n            tasks=self.tasks,\n            max_seq_len=self.max_seq_len,\n            tokenizer=self.tokenizer,\n        )\n        return features\n\n\n#####################\n# LM Processors ####\n#####################\nclass BertStyleLMProcessor(Processor):\n    """"""\n    Prepares data for masked language model training and next sentence prediction in the style of BERT\n    """"""\n\n    def __init__(\n        self,\n        tokenizer,\n        max_seq_len,\n        data_dir,\n        train_filename=""train.txt"",\n        dev_filename=""dev.txt"",\n        test_filename=""test.txt"",\n        dev_split=0.0,\n        next_sent_pred=True,\n        next_sent_pred_style=""sentence"",\n        max_docs=None,\n        proxies=None,\n        **kwargs\n    ):\n        """"""\n        :param tokenizer: Used to split a sentence (str) into tokens.\n        :param max_seq_len: Samples are truncated after this many tokens.\n        :type max_seq_len: int\n        :param data_dir: The directory in which the train and dev files can be found.\n                         If not available the dataset will be loaded automaticaly\n                         if the last directory has the same name as a predefined dataset.\n                         These predefined datasets are defined as the keys in the dict at\n                         `farm.data_handler.utils.DOWNSTREAM_TASK_MAP <https://github.com/deepset-ai/FARM/blob/master/farm/data_handler/utils.py>`_.\n        :type data_dir: str\n        :param label_list: list of labels to predict (strings). For most cases this should be: [""start_token"", ""end_token""]\n        :type label_list: list\n        :param metric: name of metric that shall be used for evaluation, e.g. ""acc"" or ""f1_macro"".\n                 Alternatively you can also supply a custom function, that takes preds and labels as args and returns a numerical value.\n                 For using multiple metrics supply them as a list, e.g [""acc"", my_custom_metric_fn].\n        :type metric: str, function, or list\n        :param train_filename: The name of the file containing training data.\n        :type train_filename: str\n        :param dev_filename: The name of the file containing the dev data. If None and 0.0 < dev_split < 1.0 the dev set\n                             will be a slice of the train set.\n        :type dev_filename: str or None\n        :param test_filename: None\n        :type test_filename: str\n        :param dev_split: The proportion of the train set that will sliced. Only works if dev_filename is set to None\n        :type dev_split: float\n        :param next_sent_pred: Whether to use next_sentence_prediction objective or not\n        :type next_sent_pred: bool\n        :param next_sent_pred_style:\n            Two different styles for next sentence prediction available:\n                - ""sentence"":   Use of a single sentence for Sequence A and a single sentence for Sequence B\n                - ""bert-style"": Fill up all of max_seq_len tokens and split into Sequence A and B at sentence border.\n                                If there are too many tokens, Sequence B will be truncated.\n        :type next_sent_pred_style: str\n        :param max_docs: maximum number of documents to include from input dataset\n        :type max_docs: int\n        :param proxies: proxy configuration to allow downloads of remote datasets.\n                        Format as in  ""requests"" library: https://2.python-requests.org//en/latest/user/advanced/#proxies\n        :type proxies: dict\n        :param kwargs: placeholder for passing generic parameters\n        :type kwargs: object\n        """"""\n\n        self.delimiter = """"\n        self.max_docs = max_docs\n\n        super(BertStyleLMProcessor, self).__init__(\n            tokenizer=tokenizer,\n            max_seq_len=max_seq_len,\n            train_filename=train_filename,\n            dev_filename=dev_filename,\n            test_filename=test_filename,\n            dev_split=dev_split,\n            data_dir=data_dir,\n            tasks={},\n            proxies=proxies\n        )\n\n        self.next_sent_pred = next_sent_pred\n        self.next_sent_pred_style = next_sent_pred_style\n        added_tokens = self.get_added_tokens()\n        self.add_task(""lm"", ""acc"", list(self.tokenizer.vocab) + added_tokens)\n        if self.next_sent_pred:\n            self.add_task(""nextsentence"", ""acc"", [""False"", ""True""])\n\n    def get_added_tokens(self):\n        dictionary = self.tokenizer.added_tokens_encoder\n        sorted_tuples = sorted(dictionary.items(), key=lambda x: x[0])\n        return [x[1] for x in sorted_tuples]\n\n    def file_to_dicts(self, file: str) -> list:\n        dicts = read_docs_from_txt(filename=file, delimiter=self.delimiter, max_docs=self.max_docs, proxies=self.proxies)\n        return dicts\n\n    def _dict_to_samples(self, dictionary, all_dicts=None):\n        doc = dictionary[""doc""]\n\n        # next sentence prediction...\n        if self.next_sent_pred:\n            assert len(all_dicts) > 1, ""Need at least 2 documents to sample random sentences from""\n            # ...with single sentences\n            if self.next_sent_pred_style == ""sentence"":\n                samples = self._dict_to_samples_single_sentence(doc, all_dicts)\n            # ...bert style\n            elif self.next_sent_pred_style == ""bert-style"":\n                samples = self._dict_to_samples_bert_style(doc, all_dicts)\n            else:\n                raise NotImplementedError(""next_sent_pred_style has to be \'sentence\' or \'bert-style\'"")\n\n        # no next sentence prediction\n        else:\n            samples = self._dict_to_samples_no_next_sent(doc)\n\n        return samples\n\n    def _dict_to_samples_single_sentence(self, doc, all_dicts):\n        samples = []\n\n        # create one sample for each sentence in the doc (except for the very last -> ""nextSentence"" is impossible)\n        for idx in range(len(doc) - 1):\n            tokenized = {}\n            text_a, text_b, is_next_label = get_sentence_pair(doc, all_dicts, idx)\n            sample_in_clear_text = {\n                ""text_a"" : text_a,\n                ""text_b"" : text_b,\n                ""nextsentence_label"" : is_next_label,\n            }\n            # tokenize\n            tokenized[""text_a""] = tokenize_with_metadata(text_a, self.tokenizer)\n            tokenized[""text_b""] = tokenize_with_metadata(text_b, self.tokenizer)\n\n            if len(tokenized[""text_a""][""tokens""]) == 0:\n                logger.warning(\n                    f""The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: {text_a}"")\n                continue\n            if len(tokenized[""text_b""][""tokens""]) == 0:\n                logger.warning(\n                    f""The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: {text_b}"")\n                continue\n\n            # truncate to max_seq_len\n            for seq_name in [""tokens"", ""offsets"", ""start_of_word""]:\n                tokenized[""text_a""][seq_name], tokenized[""text_b""][seq_name], _ = truncate_sequences(\n                    seq_a=tokenized[""text_a""][seq_name],\n                    seq_b=tokenized[""text_b""][seq_name],\n                    tokenizer=self.tokenizer,\n                    max_seq_len=self.max_seq_len)\n\n            samples.append(Sample(id=None, clear_text=sample_in_clear_text, tokenized=tokenized))\n\n        return samples\n\n    def _dict_to_samples_bert_style(self, doc, all_dicts):\n        samples = []\n        # account for [CLS], [SEP], [SEP]\n        max_num_tokens = self.max_seq_len - 3\n\n        # tokenize\n        doc_tokenized = []\n        for sentence in doc:\n            doc_tokenized.append(tokenize_with_metadata(sentence, self.tokenizer))\n\n        current_chunk = []\n        current_chunk_clear_text = []\n        current_length = 0\n        i = 0\n        while i < len(doc_tokenized):\n            current_segment = doc_tokenized[i]\n            current_length += len(current_segment[""tokens""])\n            current_chunk.append(current_segment)\n            current_chunk_clear_text.append(doc[i])\n\n            # reached end of document or max_num_tokens\n            if (i == len(doc_tokenized) - 1) or (current_length >= max_num_tokens):\n                sequence_a, sequence_b, sample_in_clear_text, num_unused_segments = get_sequence_pair(\n                    doc,\n                    current_chunk,\n                    current_chunk_clear_text,\n                    all_dicts,\n                    self.tokenizer,\n                    max_num_tokens,\n                )\n                sequence_a = join_sentences(sequence_a)\n                sequence_b = join_sentences(sequence_b)\n\n                for seq_name in [""tokens"", ""offsets"", ""start_of_word""]:\n                    sequence_a[seq_name], sequence_b[seq_name], _ = truncate_sequences(\n                        seq_a=sequence_a[seq_name],\n                        seq_b=sequence_b[seq_name],\n                        tokenizer=self.tokenizer,\n                        max_seq_len=max_num_tokens,\n                        with_special_tokens=False,\n                        truncation_strategy=""only_second"",\n                    )\n                tokenized = {""text_a"" : sequence_a, ""text_b"" : sequence_b}\n                samples.append(Sample(id=None, clear_text=sample_in_clear_text, tokenized=tokenized))\n\n                i -= num_unused_segments\n\n                current_chunk = []\n                current_chunk_clear_text = []\n                current_length = 0\n            i += 1\n        return samples\n\n    def _dict_to_samples_no_next_sent(self, doc):\n        samples = []\n\n        for idx in range(len(doc)):\n            tokenized = {}\n            text_a = doc[idx]\n            sample_in_clear_text = {\n                ""text_a"": text_a,\n                ""text_b"": None,\n                ""nextsentence_label"": None,\n            }\n            # tokenize\n            tokenized[""text_a""] = tokenize_with_metadata(\n                text_a, self.tokenizer\n            )\n            if len(tokenized[""text_a""][""tokens""]) == 0:\n                continue\n            # truncate to max_seq_len\n            for seq_name in [""tokens"", ""offsets"", ""start_of_word""]:\n                tokenized[""text_a""][seq_name], _, _ = truncate_sequences(\n                    seq_a=tokenized[""text_a""][seq_name],\n                    seq_b=None,\n                    tokenizer=self.tokenizer,\n                    max_seq_len=self.max_seq_len,\n                )\n\n            samples.append(Sample(id=None, clear_text=sample_in_clear_text, tokenized=tokenized))\n\n        return samples\n\n    def _sample_to_features(self, sample) -> dict:\n        features = samples_to_features_bert_lm(\n            sample=sample, max_seq_len=self.max_seq_len, tokenizer=self.tokenizer,\n            next_sent_pred=self.next_sent_pred\n        )\n        return features\n\n\n#########################################\n# QA Processors ####\n#########################################\n\nclass SquadProcessor(Processor):\n    """""" Used to handle the SQuAD dataset""""""\n\n    def __init__(\n        self,\n        tokenizer,\n        max_seq_len,\n        data_dir,\n        label_list=None,\n        metric=""squad"",\n        train_filename=Path(""train-v2.0.json""),\n        dev_filename=Path(""dev-v2.0.json""),\n        test_filename=None,\n        dev_split=0,\n        doc_stride=128,\n        max_query_length=64,\n        proxies=None,\n        **kwargs\n    ):\n        """"""\n        :param tokenizer: Used to split a sentence (str) into tokens.\n        :param max_seq_len: Samples are truncated after this many tokens.\n        :type max_seq_len: int\n        :param data_dir: The directory in which the train and dev files can be found.\n                         If not available the dataset will be loaded automaticaly\n                         if the last directory has the same name as a predefined dataset.\n                         These predefined datasets are defined as the keys in the dict at\n                         `farm.data_handler.utils.DOWNSTREAM_TASK_MAP <https://github.com/deepset-ai/FARM/blob/master/farm/data_handler/utils.py>`_.\n        :type data_dir: str\n        :param label_list: list of labels to predict (strings). For most cases this should be: [""start_token"", ""end_token""]\n        :type label_list: list\n        :param metric: name of metric that shall be used for evaluation, can be ""squad"" or ""squad_top_recall""\n        :type metric: str\n        :param train_filename: The name of the file containing training data.\n        :type train_filename: str\n        :param dev_filename: The name of the file containing the dev data. If None and 0.0 < dev_split < 1.0 the dev set\n                             will be a slice of the train set.\n        :type dev_filename: str or None\n        :param test_filename: None\n        :type test_filename: str\n        :param dev_split: The proportion of the train set that will sliced. Only works if dev_filename is set to None\n        :type dev_split: float\n        :param doc_stride: When the document containing the answer is too long it gets split into part, strided by doc_stride\n        :type doc_stride: int\n        :param max_query_length: Maximum length of the question (in number of subword tokens)\n        :type max_query_length: int\n        :param kwargs: placeholder for passing generic parameters\n        :type kwargs: object\n        """"""\n\n        self.target = ""classification""\n        self.ph_output_type = ""per_token_squad""\n\n        self.doc_stride = doc_stride\n        self.max_query_length = max_query_length\n\n        super(SquadProcessor, self).__init__(\n            tokenizer=tokenizer,\n            max_seq_len=max_seq_len,\n            train_filename=train_filename,\n            dev_filename=dev_filename,\n            test_filename=test_filename,\n            dev_split=dev_split,\n            data_dir=data_dir,\n            tasks={},\n            proxies=proxies\n        )\n\n        if metric and label_list:\n            self.add_task(""question_answering"", metric, label_list)\n        else:\n            logger.info(""Initialized processor without tasks. Supply `metric` and `label_list` to the constructor for ""\n                        ""using the default task or add a custom task later via processor.add_task()"")\n\n    def dataset_from_dicts(self, dicts, indices=None, return_baskets=False):\n        """""" Overwrites the method from the base class since Question Answering processing is quite different.\n        This method allows for documents and questions to be tokenized earlier. Then SampleBaskets are initialized\n        with one document and one question. """"""\n\n        dicts = [convert_qa_input_dict(x) for x in dicts]\n        self.baskets = self._dicts_to_baskets(dicts, indices)\n        self._init_samples_in_baskets()\n        self._featurize_samples()\n        if 0 in indices:\n            self._log_samples(2)\n        # This mode is for inference where we need to keep baskets\n        if return_baskets:\n            dataset, tensor_names = self._create_dataset(keep_baskets=True)\n            return dataset, tensor_names, self.baskets\n        # This mode is for training where we can free ram by removing baskets\n        else:\n            dataset, tensor_names = self._create_dataset(keep_baskets=False)\n            return dataset, tensor_names\n\n    def _dicts_to_baskets(self, dicts, indices):\n        # Perform tokenization on documents and questions resulting in an unnested list of doc-question pairs\n        dicts_tokenized = [self.apply_tokenization(d) for d in dicts]\n\n        baskets = []\n        for index, document in zip(indices, dicts_tokenized):\n            for q_idx, raw in enumerate(document):\n                # In case of Question Answering the external ID is used for document IDs\n                basket = SampleBasket(raw=raw, id=f""{index}-{q_idx}"", external_id=raw.get(""document_id"",None))\n                baskets.append(basket)\n        return baskets\n\n\n    def apply_tokenization(self, dictionary):\n        """""" This performs tokenization on all documents and questions. The result is a list (unnested)\n        where each entry is a dictionary for one document-question pair (potentially mutliple answers). """"""\n\n        raw_baskets = []\n        dictionary = convert_qa_input_dict(dictionary)\n        document_text = dictionary[""context""]\n        document_id = dictionary.get(""document_id"",None)\n\n        document_tokenized = tokenize_with_metadata(document_text, self.tokenizer)\n        document_start_of_word = [int(x) for x in document_tokenized[""start_of_word""]]\n        questions = dictionary[""qas""]\n        for question in questions:\n            answers = []\n            # For training and dev where labelled samples are read in from a SQuAD style file\n            try:\n                squad_id = question[""id""]\n                question_text = question[""question""]\n                for answer in question[""answers""]:\n                    if answer[""text""] == """":\n                        answer_type = ""is_impossible""\n                    else:\n                        answer_type = ""span""\n                    a = {""text"": answer[""text""],\n                         ""offset"": answer[""answer_start""],\n                         ""answer_type"": answer_type}\n                    answers.append(a)\n            # For inference where samples are read in as dicts without an id or answers\n            except TypeError:\n                squad_id = None\n                question_text = question\n            question_tokenized = tokenize_with_metadata(question_text, self.tokenizer)\n            question_start_of_word = [int(x) for x in question_tokenized[""start_of_word""]]\n\n            # TODO for Squad and NQ, answer_type should be paired with the question and not the passage\n            # TODO make this change for both processors\n            if ""is_impossible"" not in question:\n                answer_type = ""span""\n            else:\n                if question[""is_impossible""]:\n                    answer_type = ""is_impossible""\n                else:\n                    answer_type = ""span""\n            raw = {""document_text"": document_text,\n                   ""document_tokens"": document_tokenized[""tokens""],\n                   ""document_offsets"": document_tokenized[""offsets""],\n                   ""document_start_of_word"": document_start_of_word,\n                   ""document_id"": document_id,\n                   ""question_text"": question_text,\n                   ""question_tokens"": question_tokenized[""tokens""],\n                   ""question_offsets"": question_tokenized[""offsets""],\n                   ""question_start_of_word"": question_start_of_word,\n                   ""answers"": answers,\n                   ""answer_type"": answer_type,\n                   ""squad_id"": squad_id}\n            raw_baskets.append(raw)\n        return raw_baskets\n\n    def file_to_dicts(self, file: str) -> [dict]:\n        nested_dicts = read_squad_file(filename=file)\n        dicts = [y for x in nested_dicts for y in x[""paragraphs""]]\n        return dicts\n\n    def _dict_to_samples(self, dictionary: dict, **kwargs) -> [Sample]:\n        n_special_tokens = self.tokenizer.num_added_tokens(pair=True)\n        samples = create_samples_qa(dictionary=dictionary,\n                                       max_query_len=self.max_query_length,\n                                       max_seq_len=self.max_seq_len,\n                                       doc_stride=self.doc_stride,\n                                       n_special_tokens=n_special_tokens)\n        return samples\n\n    def _sample_to_features(self, sample) -> dict:\n        # TODO, make this function return one set of features per sample\n        features = sample_to_features_qa(sample=sample,\n                                         tokenizer=self.tokenizer,\n                                         max_seq_len=self.max_seq_len)\n        return features\n\nclass NaturalQuestionsProcessor(Processor):\n    """""" Used to handle the Natural Question QA dataset""""""\n\n    def __init__(\n        self,\n        tokenizer,\n        max_seq_len,\n        data_dir,\n        train_filename=Path(""train-v2.0.json""),\n        dev_filename=Path(""dev-v2.0.json""),\n        test_filename=None,\n        dev_split=0,\n        doc_stride=128,\n        max_query_length=64,\n        proxies=None,\n        keep_is_impossible=0.02,\n        downsample_context_size=None,\n        inference=False,\n        **kwargs):\n        """"""\n        Deals with all the preprocessing steps needed for Natural Questions. Follows Alberti 2019 et al. (https://arxiv.org/abs/1901.08634)\n        in merging multiple disjoint short answers into the one longer label span and also by downsampling\n        samples of is_impossible during training\n\n        :param tokenizer: Used to split a sentence (str) into tokens.\n        :param max_seq_len: Samples are truncated after this many tokens.\n        :type max_seq_len: int\n        :param data_dir: The directory in which the train and dev files can be found.\n                         If not available the dataset will be loaded automaticaly\n                         if the last directory has the same name as a predefined dataset.\n                         These predefined datasets are defined as the keys in the dict at\n                         `farm.data_handler.utils.DOWNSTREAM_TASK_MAP <https://github.com/deepset-ai/FARM/blob/master/farm/data_handler/utils.py>`_.\n        :type data_dir: str\n        :param train_filename: The name of the file containing training data.\n        :type train_filename: str\n        :param dev_filename: The name of the file containing the dev data. If None and 0.0 < dev_split < 1.0 the dev set\n                             will be a slice of the train set.\n        :type dev_filename: str or None\n        :param test_filename: The name of the file containing the test data.\n        :type test_filename: str\n        :param dev_split: The proportion of the train set that will sliced. Only works if dev_filename is set to None\n        :type dev_split: float\n        :param doc_stride: When the document containing the answer is too long it gets split into parts, strided by doc_stride\n        :type doc_stride: int\n        :param max_query_length: Maximum length of the question (in number of subword tokens)\n        :type max_query_length: int\n        :param keep_is_impossible: The probability that a sample with an is_impossible label is kept\n                                    (0.0 < keep_is_impossible <= 1.0). Only works if inference is False\n        :type keep_is_impossible: float\n        :param downsample_context_size: Downsampling before any data conversion by taking a short text window of size\n                                        downsample_context_size around the long answer span. To disable set to None\n        :type downsample_context_size: int\n        :param inference: Whether we are currently using the Processsor for model inference. If True, the\n                          keep_is_impossible will be overridden and set to 1\n        :type inference: bool\n        :param kwargs: placeholder for passing generic parameters\n        :type kwargs: object\n        """"""\n        self.target = ""classification""\n        self.ph_output_type = ""per_token_squad""\n\n        # These are classification labels from Natural Questions. Note that in this implementation, we are merging\n        # the ""long_answer"" and ""short_answer"" labels into the one ""span"" label\n        self.answer_type_list = [""is_impossible"", ""span"", ""yes"", ""no""]\n\n        self.doc_stride = doc_stride\n        self.max_query_length = max_query_length\n        self.keep_is_impossible = keep_is_impossible\n        self.downsample_context_size = downsample_context_size\n        self.inference = inference\n\n        super(NaturalQuestionsProcessor, self).__init__(\n            tokenizer=tokenizer,\n            max_seq_len=max_seq_len,\n            train_filename=train_filename,\n            dev_filename=dev_filename,\n            test_filename=test_filename,\n            dev_split=dev_split,\n            data_dir=data_dir,\n            tasks={},\n            proxies=proxies\n        )\n\n        # Todo rename metric from squad to maybe QA spans or something like that\n        self.add_task(""question_answering"", ""squad"", [""start_token"", ""end_token""])\n        self.add_task(""text_classification"", ""f1_macro"", self.answer_type_list, label_name=""answer_type"")\n\n\n    def file_to_dicts(self, file: str) -> [dict]:\n        dicts = read_jsonl(file, proxies=self.proxies)\n        return dicts\n\n\n    def _dict_to_samples(self, dictionary: dict, all_dicts=None) -> [Sample]:\n        """"""\n            This method will split question-document pairs from the SampleBasket into question-passage pairs which will\n        each form one sample. The ""t"" and ""c"" in variables stand for token and character respectively. This uses many\n        methods that the SquadProcessor calls but note that the SquadProcessor overwrites Processor._dicts_to_baskets()\n        while the NaturalQuestionsProcessor does not. This was done in Squad to avoid retokenizing documents that are\n        paired with multiple questions. This is not necessary for Natural Questions since there is generally a 1 to 1\n        mapping from document to question. Input dictionaries can have either [""context"", ""qas""] (internal format) as\n        keys or [""text"", ""questions""] (api format). Both are supported.\n        """"""\n        # Turns a NQ dictionaries into a SQuAD style dictionaries\n        if not self.inference:\n            dictionary = self.prepare_dict(dictionary=dictionary)\n\n        dictionary_tokenized = self.apply_tokenization(dictionary)[0]\n        n_special_tokens = self.tokenizer.num_added_tokens(pair=True)\n        samples = create_samples_qa(dictionary_tokenized,\n                                    self.max_query_length,\n                                    self.max_seq_len,\n                                    self.doc_stride,\n                                    n_special_tokens)\n        # Downsample the number of samples with an is_impossible label. This fn will always return at least one sample\n        # so that we don\'t end up with a basket with 0 samples\n        if not self.inference:\n            samples = self.downsample(samples, self.keep_is_impossible)\n        return samples\n\n    def downsample(self, samples, keep_prob):\n        # Downsamples samples with a is_impossible label (since there is an overrepresentation of these in NQ)\n        # This method will always return at least one sample. This is done so that we don\'t end up with SampleBaskets\n        # with 0 samples\n        ret = []\n        for s in samples:\n            if self.check_is_impossible(s):\n                if random_float() > 1 - keep_prob:\n                    ret.append(s)\n            else:\n                ret.append(s)\n        if len(ret) == 0:\n            ret = [random.choice(samples)]\n        return ret\n\n    @staticmethod\n    def check_is_impossible(sample):\n        sample_tok = sample.tokenized\n        if len(sample_tok[""answers""]) == 0:\n            return True\n        first_answer = sample_tok[""answers""][0]\n        if first_answer[""start_t""] < sample_tok[""passage_start_t""]:\n            return True\n        if first_answer[""end_t""] > sample_tok[""passage_start_t""] + len(sample_tok[""passage_tokens""]):\n            return True\n        if first_answer[""answer_type""] == ""is_impossible"":\n            return True\n        else:\n            return False\n\n    def downsample_unprocessed(self, dictionary):\n        doc_text = dictionary[""document_text""]\n        doc_tokens = doc_text.split("" "")\n        annotations = dictionary.get(""annotations"",[])\n        # for simplicity we only downsample wiki pages with one long answer annotation\n        if len(annotations) == 1:\n            annotation = annotations[0]\n            # There seem to be cases where there is no answer but an annotation is given as a (-1, -1) long answer\n            if self.check_no_answer(annotation):\n                dictionary[""document_text""] = "" "".join(doc_tokens[:self.max_seq_len+randint(1,self.downsample_context_size)])\n            else:\n                # finding earliest start and latest end labels\n                long_answer_start = annotation[\'long_answer\'][\'start_token\']\n                long_answer_end = annotation[\'long_answer\'][\'end_token\']\n                short_answer_start = 1e10\n                short_answer_end = -1\n                for s in annotation[""short_answers""]:\n                    if s[""start_token""] < short_answer_start:\n                        short_answer_start = s[""start_token""]\n                    if s[""end_token""] > short_answer_end:\n                        short_answer_end = s[""end_token""]\n\n                start_threshold = min(long_answer_start,short_answer_start) - randint(1,self.downsample_context_size)\n                start_threshold = max(0, start_threshold)\n                end_threshold = max(long_answer_end,short_answer_end) + randint(1,self.downsample_context_size)\n\n                # taking subset of doc text and shift labels\n                sub_document_text = "" "".join(\n                    doc_tokens[start_threshold:end_threshold]\n                )\n                dictionary[""document_text""] = sub_document_text\n                # change of offsets happens in place (of dictionary)\n                annotation[\'long_answer\'][\'start_token\'] -= start_threshold\n                annotation[\'long_answer\'][\'end_token\'] -= start_threshold\n                for s in annotation[""short_answers""]:\n                    s[""start_token""] -= start_threshold\n                    s[""end_token""] -= start_threshold\n\n        return dictionary\n\n\n    def prepare_dict(self, dictionary):\n        """""" Casts a Natural Questions dictionary that is loaded from a jsonl file into SQuAD format so that\n        the same featurization functions can be called for both tasks. Each annotation can be one of four answer types,\n        [""yes"", ""no"", ""span"", ""is_impossible""]""""""\n\n        if self.downsample_context_size is not None:\n            dictionary = self.downsample_unprocessed(dictionary)\n\n        converted_answers = []\n        doc_text = dictionary[""document_text""]\n        _, tok_to_ch = split_with_metadata(doc_text)\n        for annotation in dictionary[""annotations""]:\n            # There seem to be cases where there is no answer but an annotation is given as a (-1, -1) long answer\n            if self.check_no_answer(annotation):\n                continue\n            sa_text, sa_start_c = self.unify_short_answers(annotation[""short_answers""], doc_text, tok_to_ch)\n            la_text, la_start_c = self.retrieve_long_answer(annotation[""long_answer""][""start_token""],\n                                                            annotation[""long_answer""][""end_token""],\n                                                            tok_to_ch,\n                                                            doc_text)\n            # Picks the span to be considered as annotation by choosing between short answer, long answer and is_impossible\n            text, start_c = self.choose_span(sa_text, sa_start_c, la_text, la_start_c)\n            converted_answers.append({""text"": text,\n                                      ""answer_start"": start_c})\n        if len(converted_answers) == 0:\n            answer_type = ""is_impossible""\n        else:\n            answer_type = dictionary[""annotations""][0][""yes_no_answer""].lower()\n            if answer_type == ""none"":\n                answer_type = ""span""\n        converted = {""id"": dictionary[""example_id""],\n                     ""context"": doc_text,\n                     ""qas"": [{""question"": dictionary[""question_text""],\n                              ""id"": dictionary[""example_id""],\n                              ""answers"": converted_answers,\n                              ""answer_type"": answer_type}]}\n        return converted\n\n    @staticmethod\n    def check_no_answer(annotation):\n        if annotation[""long_answer""][""start_token""] > -1 or annotation[""long_answer""][""end_token""] > -1:\n            return False\n        for sa in annotation[""short_answers""]:\n            if sa[""start_token""] > -1 or sa[""end_token""] > -1:\n                return False\n        else:\n            return True\n\n    def retrieve_long_answer(self, start_t, end_t, tok_to_ch, doc_text):\n        """""" Retrieves the string long answer and also its starting character index""""""\n        start_c, end_c = self.convert_tok_to_ch(start_t, end_t, tok_to_ch, doc_text)\n        text = doc_text[start_c: end_c]\n        return text, start_c\n\n    @staticmethod\n    def choose_span(sa_text, sa_start_c, la_text, la_start_c):\n        if sa_text:\n            return sa_text, sa_start_c\n        elif la_text:\n            return la_text, la_start_c\n        else:\n            return """", -1\n\n    def unify_short_answers(self, short_answers, doc_text, tok_to_ch):\n        """""" In cases where an NQ sample has multiple disjoint short answers, this fn generates the single shortest\n        span that contains all the answers""""""\n        if not short_answers:\n            return """", -1\n        short_answer_idxs = []\n        # TODO write comment explaining this\n        for short_answer in short_answers:\n            short_answer_idxs.append(short_answer[""start_token""])\n            short_answer_idxs.append(short_answer[""end_token""])\n        answer_start_t = min(short_answer_idxs)\n        answer_end_t = max(short_answer_idxs)\n        answer_start_c, answer_end_c = self.convert_tok_to_ch(answer_start_t, answer_end_t, tok_to_ch, doc_text)\n        answer_text = doc_text[answer_start_c: answer_end_c]\n        assert answer_text == "" "".join(doc_text.split()[answer_start_t: answer_end_t])\n        return answer_text, answer_start_c\n\n    @staticmethod\n    def convert_tok_to_ch(start_t, end_t, tok_to_ch, doc_text):\n        n_tokens = len(tok_to_ch)\n        if start_t == -1 and end_t == -1:\n            return -1, -1\n        start_c = tok_to_ch[start_t]\n        # when the end of the answer span is the end of the text\n        if end_t == n_tokens:\n            end_c = len(doc_text)\n        else:\n            next_word_start_c = tok_to_ch[end_t]\n            span = doc_text[:next_word_start_c].strip()\n            end_c = len(span)\n        return start_c, end_c\n\n    def apply_tokenization(self, dictionary):\n        """""" This performs tokenization on all documents and questions. The result is a list\n        where each entry is a dictionary for one document-question pair (potentially mutliple answers). This is based on\n        the apply_tokenization method of SquadProcessor but slightly modified.\n\n        TODO: See if this can be merged with SquadProcessor.apply_tokenization()""""""\n\n        raw_baskets = []\n        # Input dictionaries can have [""context"", ""qas""] (SQuAD format) as keys or\n        # [""text"", ""questions""] (FARM format). Both are supported\n        dictionary = convert_qa_input_dict(dictionary)\n        document_text = dictionary[""context""]\n        document_id = dictionary.get(""document_id"", None)\n\n        document_tokenized = tokenize_with_metadata(document_text, self.tokenizer)\n        document_start_of_word = [int(x) for x in document_tokenized[""start_of_word""]]\n        questions = dictionary[""qas""]\n        for question in questions:\n            answers = []\n            # For training and dev with labelled examples\n            try:\n                nq_id = question[""id""]\n                question_text = question[""question""]\n                for answer in question[""answers""]:\n                    a = {""text"": answer[""text""],\n                         ""offset"": answer[""answer_start""],\n                         ""answer_type"": question[""answer_type""]}\n                    answers.append(a)\n            # For inference where samples are read in without an id or answers\n            except TypeError:\n                nq_id = None\n                question_text = question\n            question_tokenized = tokenize_with_metadata(question_text, self.tokenizer)\n            question_start_of_word = [int(x) for x in question_tokenized[""start_of_word""]]\n\n            # TODO compare data format with Squad to explain what this section is doing exactly\n            # TODO suspect that this might not be right for NQ\n            if ""is_impossible"" not in question:\n                answer_type = ""span""\n            else:\n                answer_type = question[""is_impossible""]\n\n            raw = {""document_text"": document_text,\n                   ""document_tokens"": document_tokenized[""tokens""],\n                   ""document_offsets"": document_tokenized[""offsets""],\n                   ""document_start_of_word"": document_start_of_word,\n                   ""document_id"": document_id,\n                   ""question_text"": question_text,\n                   ""question_tokens"": question_tokenized[""tokens""],\n                   ""question_offsets"": question_tokenized[""offsets""],\n                   ""question_start_of_word"": question_start_of_word,\n                   ""answers"": answers,\n                   ""answer_type"": answer_type,\n                   ""nq_id"": nq_id}\n            raw_baskets.append(raw)\n        return raw_baskets\n\n    def _sample_to_features(self, sample: Sample) -> dict:\n        features = sample_to_features_qa(sample=sample,\n                                         tokenizer=self.tokenizer,\n                                         max_seq_len=self.max_seq_len,\n                                         answer_type_list=self.answer_type_list)\n        return features\n\n\nclass RegressionProcessor(Processor):\n    """"""\n    Used to handle a regression dataset in tab separated text + label\n    """"""\n    def __init__(\n        self,\n        tokenizer,\n        max_seq_len,\n        data_dir,\n        train_filename=""train.tsv"",\n        dev_filename=None,\n        test_filename=""test.tsv"",\n        dev_split=0.1,\n        delimiter=""\\t"",\n        quote_char=""\'"",\n        skiprows=None,\n        label_column_name=""label"",\n        label_name=""regression_label"",\n        scaler_mean=None,\n        scaler_scale=None,\n        proxies=None,\n        text_column_name=""text"",\n        **kwargs\n    ):\n        """"""\n        :param tokenizer: Used to split a sentence (str) into tokens.\n        :param max_seq_len: Samples are truncated after this many tokens.\n        :type max_seq_len: int\n        :param data_dir: The directory in which the train and dev files can be found.\n                         If not available the dataset will be loaded automaticaly\n                         if the last directory has the same name as a predefined dataset.\n                         These predefined datasets are defined as the keys in the dict at\n                         `farm.data_handler.utils.DOWNSTREAM_TASK_MAP <https://github.com/deepset-ai/FARM/blob/master/farm/data_handler/utils.py>`_.\n        :type data_dir: str\n        :param label_list: list of labels to predict (strings). For most cases this should be: [""start_token"", ""end_token""]\n        :type label_list: list\n        :param metric: name of metric that shall be used for evaluation, e.g. ""acc"" or ""f1_macro"".\n                 Alternatively you can also supply a custom function, that takes preds and labels as args and returns a\n                 numerical value. For using multiple metrics supply them as a list, e.g [""acc"", my_custom_metric_fn].\n        :type metric: str, function, or list\n        :param train_filename: The name of the file containing training data.\n        :type train_filename: str\n        :param dev_filename: The name of the file containing the dev data. If None and 0.0 < dev_split < 1.0 the dev set\n                             will be a slice of the train set.\n        :type dev_filename: str or None\n        :param test_filename: None\n        :type test_filename: str\n        :param dev_split: The proportion of the train set that will sliced. Only works if dev_filename is set to None\n        :type dev_split: float\n        :param delimiter: Separator used in the input tsv / csv file\n        :type delimiter: str\n        :param quote_char: Character used for quoting strings in the input tsv/ csv file\n        :type quote_char: str\n        :param skiprows: number of rows to skip in the tsvs (e.g. for multirow headers)\n        :type skiprows: int\n        :param label_column_name: name of the column in the input csv/tsv that shall be used as training labels\n        :type label_column_name: str\n        :param label_name: name for the internal label variable in FARM (only needed to adjust in rare cases)\n        :type label_name: str\n        :param scaler_mean: Value to substract from the label for normalization\n        :type scaler_mean: float\n        :param scaler_scale: Value to divide the label by for normalization\n        :type scaler_scale: float\n        :param proxies: proxy configuration to allow downloads of remote datasets.\n                        Format as in  ""requests"" library: https://2.python-requests.org//en/latest/user/advanced/#proxies\n        :type proxies: dict\n        :param text_column_name: name of the column in the input csv/tsv that shall be used as training text\n        :type text_column_name: str\n        :param kwargs: placeholder for passing generic parameters\n        :type kwargs: object\n        """"""\n\n        # Custom processor attributes\n        self.delimiter = delimiter\n        self.quote_char = quote_char\n        self.skiprows = skiprows\n\n        super(RegressionProcessor, self).__init__(\n            tokenizer=tokenizer,\n            max_seq_len=max_seq_len,\n            train_filename=train_filename,\n            dev_filename=dev_filename,\n            test_filename=test_filename,\n            dev_split=dev_split,\n            data_dir=data_dir,\n            proxies=proxies\n        )\n\n        # Note that label_list is being hijacked to store the scaling mean and scale\n        self.add_task(name=""regression"",\n                      metric=""mse"",\n                      label_list=[scaler_mean, scaler_scale],\n                      label_column_name=label_column_name,\n                      task_type=""regression"",\n                      label_name=label_name,\n                      text_column_name=text_column_name)\n\n    def file_to_dicts(self, file: str) -> [dict]:\n        column_mapping = {}\n        for task in self.tasks.values():\n            column_mapping[task[""label_column_name""]] = task[""label_name""]\n            column_mapping[task[""text_column_name""]] = ""text""\n        dicts = read_tsv(\n            rename_columns=column_mapping,\n            filename=file,\n            delimiter=self.delimiter,\n            skiprows=self.skiprows,\n            quotechar=self.quote_char,\n            proxies=self.proxies\n        )\n\n        # collect all labels and compute scaling stats\n        train_labels = []\n        for d in dicts:\n            train_labels.append(float(d[self.tasks[""regression""][""label_name""]]))\n        scaler = StandardScaler()\n        scaler.fit(np.reshape(train_labels, (-1, 1)))\n        # add to label list in regression task\n        self.tasks[""regression""][""label_list""] = [scaler.mean_.item(), scaler.scale_.item()]\n\n        return dicts\n\n    def _dict_to_samples(self, dictionary: dict, **kwargs) -> [Sample]:\n        # this tokenization also stores offsets\n        tokenized = tokenize_with_metadata(dictionary[""text""], self.tokenizer)\n        if len(tokenized[""tokens""]) == 0:\n            text = dictionary[""text""]\n            logger.warning(f""The following text could not be tokenized, likely because it contains a character that the tokenizer does not recognize: {text}"")\n            return []\n        # truncate tokens, offsets and start_of_word to max_seq_len that can be handled by the model\n        for seq_name in tokenized.keys():\n            tokenized[seq_name], _, _ = truncate_sequences(seq_a=tokenized[seq_name], seq_b=None,\n                                                           tokenizer=self.tokenizer,\n                                                           max_seq_len=self.max_seq_len)\n        # Samples don\'t have labels during Inference mode\n        if ""label"" in dictionary:\n            label = float(dictionary[""label""])\n            scaled_label = (label - self.tasks[""regression""][""label_list""][0]) / self.tasks[""regression""][""label_list""][1]\n            dictionary[""label""] = scaled_label\n        return [Sample(id=None, clear_text=dictionary, tokenized=tokenized)]\n\n    def _sample_to_features(self, sample) -> dict:\n        features = sample_to_features_text(\n            sample=sample,\n            tasks=self.tasks,\n            max_seq_len=self.max_seq_len,\n            tokenizer=self.tokenizer\n        )\n        return features\n'"
farm/data_handler/samples.py,0,"b'from transformers.tokenization_bert import whitespace_tokenize\nfrom farm.visual.ascii.images import SAMPLE\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass SampleBasket:\n    """""" An object that contains one source text and the one or more samples that will be processed. This\n    is needed for tasks like question answering where the source text can generate multiple input - label\n    pairs.""""""\n\n    def __init__(self, id: str, raw: dict, external_id=None, samples=None):\n        """"""\n        :param id: A unique identifying id. Used for identification within FARM.\n        :type id: str\n        :param external_id: Used for identification outside of FARM. E.g. if another framework wants to pass along its own id with the results.\n        :type external_id: str\n        :param raw: Contains the various data needed to form a sample. It is ideally in human readable form.\n        :type raw: dict\n        :param samples: An optional list of Samples used to populate the basket at initialization.\n        :type samples: Sample\n        """"""\n        self.id = id\n        self.external_id = external_id\n        self.raw = raw\n        self.samples = samples\n\n\nclass Sample(object):\n    """"""A single training/test sample. This should contain the input and the label. Is initialized with\n    the human readable clear_text. Over the course of data preprocessing, this object is populated\n    with tokenized and featurized versions of the data.""""""\n\n    def __init__(self, id, clear_text, tokenized=None, features=None):\n        """"""\n        :param id: The unique id of the sample\n        :type id: str\n        :param clear_text: A dictionary containing various human readable fields (e.g. text, label).\n        :type clear_text: dict\n        :param tokenized: A dictionary containing the tokenized version of clear text plus helpful meta data: offsets (start position of each token in the original text) and start_of_word (boolean if a token is the first one of a word).\n        :type tokenized: dict\n        :param features: A dictionary containing features in a vectorized format needed by the model to process this sample.\n        :type features: dict\n\n        """"""\n        self.id = id\n        self.clear_text = clear_text\n        self.features = features\n        self.tokenized = tokenized\n\n    def __str__(self):\n\n        if self.clear_text:\n            clear_text_str = ""\\n \\t"".join(\n                [k + "": "" + str(v) for k, v in self.clear_text.items()]\n            )\n            if len(clear_text_str) > 10000:\n                clear_text_str = clear_text_str[:10_000] + f""\\nTHE REST IS TOO LONG TO DISPLAY. "" \\\n                                                           f""Remaining chars :{len(clear_text_str)-10_000}""\n        else:\n            clear_text_str = ""None""\n\n        if self.features:\n            if isinstance(self.features, list):\n                features = self.features[0]\n            else:\n                features = self.features\n            feature_str = ""\\n \\t"".join([k + "": "" + str(v) for k, v in features.items()])\n        else:\n            feature_str = ""None""\n\n        if self.tokenized:\n            tokenized_str = ""\\n \\t"".join(\n                [k + "": "" + str(v) for k, v in self.tokenized.items()]\n            )\n            if len(tokenized_str) > 10000:\n                tokenized_str = tokenized_str[:10_000] + f""\\nTHE REST IS TOO LONG TO DISPLAY. "" \\\n                                                         f""Remaining chars: {len(tokenized_str)-10_000}""\n        else:\n            tokenized_str = ""None""\n        s = (\n            f""\\n{SAMPLE}\\n""\n            f""ID: {self.id}\\n""\n            f""Clear Text: \\n \\t{clear_text_str}\\n""\n            f""Tokenized: \\n \\t{tokenized_str}\\n""\n            f""Features: \\n \\t{feature_str}\\n""\n            ""_____________________________________________________""\n        )\n        return s\n\n\nclass Squad_cleartext:\n    def __init__(\n        self,\n        qas_id,\n        question_text,\n        doc_tokens,\n        orig_answer_text,\n        start_position,\n        end_position,\n        is_impossible,\n    ):\n\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.doc_tokens = doc_tokens\n        self.orig_answer_text = orig_answer_text\n        self.start_position = start_position\n        self.end_position = end_position\n        self.is_impossible = is_impossible\n\n\ndef create_sample_one_label_one_text(raw_data, text_index, label_index, basket_id):\n\n    # text = "" "".join(raw_data[text_index:])\n    text = raw_data[text_index]\n    label = raw_data[label_index]\n\n    return [Sample(id=basket_id + ""-0"", clear_text={""text"": text, ""label"": label})]\n\n\ndef create_sample_ner(split_text, label, basket_id):\n\n    text = "" "".join(split_text)\n    label = label\n\n    return [Sample(id=basket_id + ""-0"", clear_text={""text"": text, ""label"": label})]\n\n\n# TODO Remove - This has been superceded by create_samples_qa which can handle both Squad and Natural Questions\n# def create_samples_squad(dictionary, max_query_len, max_seq_len, doc_stride, n_special_tokens):\n#     """"""\n#     This method will split question-document pairs from the SampleBasket into question-passage pairs which will\n#     each form one sample. The ""t"" and ""c"" in variables stand for token and character respectively.\n#     """"""\n#\n#     # Initialize some basic variables\n#     # is_training = check_if_training(dictionary)\n#     question_tokens = dictionary[""question_tokens""][:max_query_len]\n#     question_len_t = len(question_tokens)\n#     question_offsets = dictionary[""question_offsets""]\n#     doc_tokens = dictionary[""document_tokens""]\n#     doc_offsets = dictionary[""document_offsets""]\n#     doc_text = dictionary[""document_text""]\n#     doc_start_of_word = dictionary[""document_start_of_word""]\n#     samples = []\n#\n#     # Calculate the number of tokens that can be reserved for the passage. This is calculated by considering\n#     # the max_seq_len, the number of tokens in the question and the number of special tokens that will be added\n#     # when the question and passage are joined (e.g. [CLS] and [SEP])\n#     passage_len_t = max_seq_len - question_len_t - n_special_tokens\n#\n#     # Perform chunking of document into passages. The sliding window moves in steps of doc_stride.\n#     # passage_spans is a list of dictionaries where each defines the start and end of each passage\n#     # on both token and character level\n#     passage_spans = chunk_into_passages(doc_offsets,\n#                                         doc_stride,\n#                                         passage_len_t,\n#                                         doc_text)\n#     for passage_span in passage_spans:\n#         # Unpack each variable in the dictionary. The ""_t"" and ""_c"" indicate\n#         # whether the index is on the token or character level\n#         passage_start_t = passage_span[""passage_start_t""]\n#         passage_end_t = passage_span[""passage_end_t""]\n#         passage_start_c = passage_span[""passage_start_c""]\n#         passage_end_c = passage_span[""passage_end_c""]\n#         passage_id = passage_span[""passage_id""]\n#\n#         # passage_offsets will be relative to the start of the passage (i.e. they will start at 0)\n#         # TODO: Is passage offsets actually needed? At this point, maybe we only care about token level\n#         passage_offsets = doc_offsets[passage_start_t: passage_end_t]\n#         passage_start_of_word = doc_start_of_word[passage_start_t: passage_end_t]\n#         passage_offsets = [x - passage_offsets[0] for x in passage_offsets]\n#         passage_tokens = doc_tokens[passage_start_t: passage_end_t]\n#         passage_text = dictionary[""document_text""][passage_start_c: passage_end_c]\n#\n#         # Deal with the potentially many answers (e.g. Squad dev set)\n#         answers_clear, answers_tokenized = process_answers(dictionary[""answers""],\n#                                                            doc_offsets,\n#                                                            passage_start_c,\n#                                                            passage_start_t)\n#\n#         clear_text = {""passage_text"": passage_text,\n#                       ""question_text"": dictionary[""question_text""],\n#                       ""passage_id"": passage_id,\n#                       ""answers"": answers_clear,\n#                       ""answer_type"": dictionary[""answer_type""]}\n#         tokenized = {""passage_start_t"": passage_start_t,\n#                      ""passage_tokens"": passage_tokens,\n#                      ""passage_offsets"": passage_offsets,\n#                      ""passage_start_of_word"": passage_start_of_word,\n#                      ""question_tokens"": question_tokens,\n#                      ""question_offsets"": question_offsets,\n#                      ""question_start_of_word"": dictionary[""question_start_of_word""][:max_query_len],\n#                      ""answers"": answers_tokenized}\n#         samples.append(Sample(id=passage_id,\n#                               clear_text=clear_text,\n#                               tokenized=tokenized))\n#     return samples\n\n\ndef process_answers(answers, doc_offsets, passage_start_c, passage_start_t):\n    """"""TODO Write Comment""""""\n    answers_clear = []\n    answers_tokenized = []\n    for answer in answers:\n        # This section calculates start and end relative to document\n        answer_text = answer[""text""]\n        answer_len_c = len(answer_text)\n        answer_start_c = answer[""offset""]\n        answer_end_c = answer_start_c + answer_len_c - 1\n        answer_start_t = offset_to_token_idx(doc_offsets, answer_start_c)\n        answer_end_t = offset_to_token_idx(doc_offsets, answer_end_c)\n\n        # TODO: Perform check that answer can be recovered from document?\n\n        # This section converts start and end so that they are relative to the passage\n        # TODO: Is this actually necessary on character level?\n        answer_start_c -= passage_start_c\n        answer_end_c -= passage_start_c\n        answer_start_t -= passage_start_t\n        answer_end_t -= passage_start_t\n\n        curr_answer_clear = {""text"": answer_text,\n                             ""start_c"": answer_start_c,\n                             ""end_c"": answer_end_c}\n        curr_answer_tokenized = {""start_t"": answer_start_t,\n                                 ""end_t"": answer_end_t,\n                                 ""answer_type"": answer[""answer_type""]}\n\n        answers_clear.append(curr_answer_clear)\n        answers_tokenized.append(curr_answer_tokenized)\n    return answers_clear, answers_tokenized\n\n\ndef create_samples_qa(dictionary, max_query_len, max_seq_len, doc_stride, n_special_tokens):\n    """"""\n    This method will split question-document pairs from the SampleBasket into question-passage pairs which will\n    each form one sample. The ""t"" and ""c"" in variables stand for token and character respectively.\n    """"""\n\n    # Initialize some basic variables\n    # is_training = check_if_training(dictionary)\n    question_tokens = dictionary[""question_tokens""][:max_query_len]\n    question_len_t = len(question_tokens)\n    question_offsets = dictionary[""question_offsets""]\n    doc_tokens = dictionary[""document_tokens""]\n    doc_offsets = dictionary[""document_offsets""]\n    doc_text = dictionary[""document_text""]\n    doc_start_of_word = dictionary[""document_start_of_word""]\n    samples = []\n\n    # Calculate the number of tokens that can be reserved for the passage. This is calculated by considering\n    # the max_seq_len, the number of tokens in the question and the number of special tokens that will be added\n    # when the question and passage are joined (e.g. [CLS] and [SEP])\n    passage_len_t = max_seq_len - question_len_t - n_special_tokens\n\n    # Perform chunking of document into passages. The sliding window moves in steps of doc_stride.\n    # passage_spans is a list of dictionaries where each defines the start and end of each passage\n    # on both token and character level\n    passage_spans = chunk_into_passages(doc_offsets,\n                                        doc_stride,\n                                        passage_len_t,\n                                        doc_text)\n    for passage_span in passage_spans:\n        # Unpack each variable in the dictionary. The ""_t"" and ""_c"" indicate\n        # whether the index is on the token or character level\n        passage_start_t = passage_span[""passage_start_t""]\n        passage_end_t = passage_span[""passage_end_t""]\n        passage_start_c = passage_span[""passage_start_c""]\n        passage_end_c = passage_span[""passage_end_c""]\n        passage_id = passage_span[""passage_id""]\n\n        # passage_offsets will be relative to the start of the passage (i.e. they will start at 0)\n        # TODO: Is passage offsets actually needed? At this point, maybe we only care about token level\n        passage_offsets = doc_offsets[passage_start_t: passage_end_t]\n        passage_start_of_word = doc_start_of_word[passage_start_t: passage_end_t]\n        passage_offsets = [x - passage_offsets[0] for x in passage_offsets]\n        passage_tokens = doc_tokens[passage_start_t: passage_end_t]\n        passage_text = dictionary[""document_text""][passage_start_c: passage_end_c]\n\n        # Deal with the potentially many answers (e.g. Squad or NQ dev set)\n        answers_clear, answers_tokenized = process_answers(dictionary[""answers""],\n                                                           doc_offsets,\n                                                           passage_start_c,\n                                                           passage_start_t)\n\n        clear_text = {""passage_text"": passage_text,\n                      ""question_text"": dictionary[""question_text""],\n                      ""passage_id"": passage_id,\n                      ""answers"": answers_clear}\n        tokenized = {""passage_start_t"": passage_start_t,\n                     ""passage_tokens"": passage_tokens,\n                     ""passage_offsets"": passage_offsets,\n                     ""passage_start_of_word"": passage_start_of_word,\n                     ""question_tokens"": question_tokens,\n                     ""question_offsets"": question_offsets,\n                     ""question_start_of_word"": dictionary[""question_start_of_word""][:max_query_len],\n                     ""answers"": answers_tokenized,\n                     ""document_offsets"": doc_offsets}   # So that to_doc_preds can access them\n        samples.append(Sample(id=passage_id,\n                              clear_text=clear_text,\n                              tokenized=tokenized))\n    return samples\n\n\ndef chunk_into_passages(doc_offsets,\n                        doc_stride,\n                        passage_len_t,\n                        doc_text):\n    """""" Returns a list of dictionaries which each describe the start, end and id of a passage\n    that is formed when chunking a document using a sliding window approach. """"""\n    passage_spans = []\n    passage_id = 0\n    doc_len_t = len(doc_offsets)\n    while True:\n        passage_start_t = passage_id * doc_stride\n        passage_end_t = passage_start_t + passage_len_t\n        passage_start_c = doc_offsets[passage_start_t]\n\n        # If passage_end_t points to the last token in the passage, define passage_end_c as the length of the document\n        if passage_end_t >= doc_len_t - 1:\n            passage_end_c = len(doc_text)\n\n        # Get document text up to the first token that is outside the passage. Strip of whitespace.\n        # Use the length of this text as the passage_end_c\n        else:\n            end_ch_idx = doc_offsets[passage_end_t + 1]\n            raw_passage_text = doc_text[:end_ch_idx]\n            passage_end_c = len(raw_passage_text.strip())\n\n        passage_span = {""passage_start_t"": passage_start_t,\n                        ""passage_end_t"": passage_end_t,\n                        ""passage_start_c"": passage_start_c,\n                        ""passage_end_c"": passage_end_c,\n                        ""passage_id"": passage_id}\n        passage_spans.append(passage_span)\n        passage_id += 1\n        # If the end idx is greater than or equal to the length of the passage\n        if passage_end_t >= doc_len_t:\n            break\n    return passage_spans\n\n\ndef offset_to_token_idx(token_offsets, ch_idx):\n    """""" Returns the idx of the token at the given character idx""""""\n    n_tokens = len(token_offsets)\n    for i in range(n_tokens):\n        if (i + 1 == n_tokens) or (token_offsets[i] <= ch_idx < token_offsets[i + 1]):\n            return i\n\n\ndef check_if_training(dictionary):\n    if ""is_impossible"" in dictionary:\n        return True\n    return False\n\n'"
farm/data_handler/utils.py,0,"b'import hashlib\nimport json\nimport logging\nimport os\nimport random\nimport tarfile\nimport tempfile\nimport string\nfrom itertools import islice\nfrom pathlib import Path\n\nimport pandas as pd\nfrom requests import get\nfrom tqdm import tqdm\nfrom typing import List\n\nfrom farm.file_utils import http_get\nfrom farm.modeling.tokenization import tokenize_with_metadata\n\nlogger = logging.getLogger(__name__)\n\nDOWNSTREAM_TASK_MAP = {\n    ""gnad"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/gnad.tar.gz"",\n    ""germeval14"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/germeval14.tar.gz"",\n\n    # only has train.tsv and test.tsv dataset - no dev.tsv\n    ""germeval18"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/germeval18.tar.gz"",\n\n    ""squad20"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/squad20.tar.gz"",\n    ""conll03detrain"": ""https://raw.githubusercontent.com/MaviccPRP/ger_ner_evals/master/corpora/conll2003/deu.train"",\n    ""conll03dedev"": ""https://raw.githubusercontent.com/MaviccPRP/ger_ner_evals/master/corpora/conll2003/deu.testa"", #https://www.clips.uantwerpen.be/conll2003/ner/000README says testa is dev data\n    ""conll03detest"": ""https://raw.githubusercontent.com/MaviccPRP/ger_ner_evals/master/corpora/conll2003/deu.testb"",\n    ""conll03entrain"": ""https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.train"",\n    ""conll03endev"": ""https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testa"",\n    ""conll03entest"": ""https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb"",\n    ""cord_19"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/cord_19.tar.gz"",\n    ""lm_finetune_nips"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/lm_finetune_nips.tar.gz"",\n    ""toxic-comments"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/toxic-comments.tar.gz"",\n    \'cola\': ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/cola.tar.gz"",\n    ""asnq_binary"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/asnq_binary.tar.gz"",\n    ""germeval17"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/germeval17.tar.gz"",\n    ""natural_questions"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-downstream/natural_questions.tar.gz"",\n\n}\n\ndef read_tsv(filename, rename_columns, quotechar=\'""\', delimiter=""\\t"", skiprows=None, header=0, proxies=None, max_samples=None):\n    """"""Reads a tab separated value file. Tries to download the data if filename is not found""""""\n\n    # get remote dataset if needed\n    if not (os.path.exists(filename)):\n        logger.info(f"" Couldn\'t find {filename} locally. Trying to download ..."")\n        _download_extract_downstream_data(filename, proxies=proxies)\n\n    # read file into df - but only read those cols we need\n    columns_needed = list(rename_columns.keys())\n    df = pd.read_csv(\n        filename,\n        sep=delimiter,\n        encoding=""utf-8"",\n        quotechar=quotechar,\n        dtype=str,\n        skiprows=skiprows,\n        header=header,\n        usecols=columns_needed,\n    )\n    if max_samples:\n        df = df.sample(max_samples)\n\n    # let\'s rename our target columns to the default names FARM expects:\n    # ""text"": contains the text\n    # ""text_classification_label"": contains a label for text classification\n    df.rename(columns=rename_columns, inplace=True)\n    df.fillna("""", inplace=True)\n\n    # convert df to one dict per row\n    raw_dict = df.to_dict(orient=""records"")\n    return raw_dict\n\ndef read_tsv_sentence_pair(filename, rename_columns, delimiter=""\\t"", skiprows=None, header=0, proxies=None, max_samples=None):\n    """"""Reads a tab separated value file. Tries to download the data if filename is not found""""""\n\n    # get remote dataset if needed\n    if not (os.path.exists(filename)):\n        logger.info(f"" Couldn\'t find {filename} locally. Trying to download ..."")\n        _download_extract_downstream_data(filename, proxies=proxies)\n\n    # TODO quote_char was causing trouble for the asnq dataset so it has been removed - see if there\'s a better solution\n    df = pd.read_csv(\n        filename,\n        sep=delimiter,\n        encoding=""utf-8"",\n        dtype=str,\n        skiprows=skiprows,\n        header=header\n    )\n    if max_samples:\n        df = df.sample(max_samples)\n\n    # let\'s rename our target columns to the default names FARM expects:\n    # ""text"": contains the text\n    # ""text_classification_label"": contains a label for text classification\n    columns = [""text""] + [""text_b""] + list(rename_columns.keys())\n    df = df[columns]\n    for source_column, label_name in rename_columns.items():\n        df[label_name] = df[source_column].fillna("""")\n        df.drop(columns=[source_column], inplace=True)\n    # convert df to one dict per row\n    raw_dict = df.to_dict(orient=""records"")\n    return raw_dict\n\ndef read_jsonl(file, proxies=None):\n    # get remote dataset if needed\n    if not (os.path.exists(file)):\n        logger.info(f"" Couldn\'t find {file} locally. Trying to download ..."")\n        _download_extract_downstream_data(file, proxies=proxies)\n    dicts = [json.loads(l) for l in open(file)]\n    return dicts\n\ndef read_ner_file(filename, sep=""\\t"", proxies=None):\n    """"""\n    read file\n    return format :\n    [ [\'EU\', \'B-ORG\'], [\'rejects\', \'O\'], [\'German\', \'B-MISC\'], [\'call\', \'O\'], [\'to\', \'O\'], [\'boycott\', \'O\'], [\'British\', \'B-MISC\'], [\'lamb\', \'O\'], [\'.\', \'O\'] ]\n    """"""\n    # checks for correct separator\n    if ""conll03-de"" in str(filename):\n        if sep != "" "":\n            logger.error(f""Separator {sep} for dataset German CONLL03 does not match the requirements. Setting seperator to whitespace"")\n            sep = "" ""\n    if ""germeval14"" in str(filename):\n        if sep != ""\\t"":\n            logger.error(f""Separator {sep} for dataset GermEval14 de does not match the requirements. Setting seperator to tab"")\n            sep = ""\\t""\n\n    if not (os.path.exists(filename)):\n        logger.info(f"" Couldn\'t find {filename} locally. Trying to download ..."")\n        _download_extract_downstream_data(filename, proxies)\n    if ""conll03-de"" in str(filename):\n        f = open(filename, encoding=\'cp1252\')\n    else:\n        f = open(filename, encoding=\'utf-8\')\n\n    data = []\n    sentence = []\n    label = []\n    for line in f:\n        if line.startswith(""#""):\n            continue\n        if len(line) == 0 or ""-DOCSTART-"" in line or line[0] == ""\\n"":\n            if len(sentence) > 0:\n                if ""conll03"" in str(filename):\n                    _convertIOB1_to_IOB2(label)\n                if ""germeval14"" in str(filename):\n                    label = _convert_germeval14_labels(label)\n                data.append({""text"": "" "".join(sentence), ""ner_label"": label})\n                sentence = []\n                label = []\n            continue\n        splits = line.split(sep)\n\n        # adjusting to data format in Germeval14\n        # Germeval14 has two levels of annotation. E.g. ""Univerit\xc3\xa4t Berlin"" is both ORG and LOC. We only take the first level.\n        if ""germeval14"" in str(filename):\n            sentence.append(splits[1])\n            label.append(splits[-2])\n        else:\n            sentence.append(splits[0])\n            label.append(splits[-1][:-1])\n\n    # handling end of file, adding the last sentence to data\n    if len(sentence) > 0:\n        if(label[-1] == """"):\n            logger.error(f""The last NER label: \'{splits[-1]}\'  in your dataset might have been converted incorrectly. Please insert a newline at the end of the file."")\n            label[-1] = ""O""\n\n        if ""conll03-de"" in str(filename):\n            _convertIOB1_to_IOB2(label)\n        if ""germeval14"" in str(filename):\n            label = _convert_germeval14_labels(label)\n        data.append({""text"": "" "".join(sentence), ""ner_label"": label})\n    return data\n\ndef _convert_germeval14_labels(tags: List[str]):\n    newtags = []\n    for tag in tags:\n        tag = tag.replace(""part"","""")\n        tag = tag.replace(""deriv"","""")\n        newtags.append(tag)\n    return newtags\n\n\n\ndef _convertIOB1_to_IOB2(tags: List[str]):\n    """"""\n    script taken from: https://gist.github.com/allanj/b9bd448dc9b70d71eb7c2b6dd33fe4ef\n    IOB1:  O I I B I\n    IOB2:  O B I B I\n    Check that tags have a valid IOB format.\n    Tags in IOB1 format are converted to IOB2.\n    """"""\n    for i, tag in enumerate(tags):\n        if tag == \'O\':\n            continue\n        split = tag.split(\'-\')\n        if len(split) != 2 or split[0] not in [\'I\', \'B\']:\n            return False\n        if split[0] == \'B\':\n            continue\n        elif i == 0 or tags[i - 1] == \'O\':  # conversion IOB1 to IOB2\n            tags[i] = \'B\' + tag[1:]\n        elif tags[i - 1][1:] == tag[1:]:\n            continue\n        else:  # conversion IOB1 to IOB2\n            tags[i] = \'B\' + tag[1:]\n    return True\n\n\ndef read_squad_file(filename, proxies=None):\n    """"""Read a SQuAD json file""""""\n    if not (os.path.exists(filename)):\n        logger.info(f"" Couldn\'t find {filename} locally. Trying to download ..."")\n        _download_extract_downstream_data(filename, proxies)\n    with open(filename, ""r"", encoding=""utf-8"") as reader:\n        input_data = json.load(reader)[""data""]\n    return input_data\n\ndef write_squad_predictions(predictions, out_filename, predictions_filename=None):\n    predictions_json = {}\n    for x in predictions:\n        for p in x[""predictions""]:\n            if p[""answers""][0][""answer""] is not None:\n                predictions_json[p[""question_id""]] = p[""answers""][0][""answer""]\n            else:\n                predictions_json[p[""question_id""]] = """" #convert No answer = None to format understood by the SQuAD eval script\n\n    if predictions_filename:\n        dev_labels = {}\n        temp = json.load(open(predictions_filename, ""r""))\n        for d in temp[""data""]:\n            for p in d[""paragraphs""]:\n                for q in p[""qas""]:\n                    if q[""is_impossible""]:\n                        dev_labels[q[""id""]] = ""is_impossible""\n                    else:\n                        dev_labels[q[""id""]] = q[""answers""][0][""text""]\n        not_included = set(list(dev_labels.keys())) - set(list(predictions_json.keys()))\n        if len(not_included) > 0:\n            logger.info(f""There were missing predicitons for question ids: {str(set(list(dev_labels.keys())))}"")\n        for x in not_included:\n            predictions_json[x] = """"\n\n    # os.makedirs(""model_output"", exist_ok=True)\n    # filepath = Path(""model_output"") / out_filename\n    json.dump(predictions_json, open(out_filename, ""w""))\n    logger.info(f""Written Squad predictions to: {out_filename}"")\n\ndef _get_md5checksum(fname):\n    # solution from stackoverflow: https://stackoverflow.com/a/3431838\n    hash_md5 = hashlib.md5()\n    with open(fname, ""rb"") as f:\n        for chunk in iter(lambda: f.read(4096), b""""):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()\n\ndef _download_extract_downstream_data(input_file, proxies=None):\n    # download archive to temp dir and extract to correct position\n    full_path = Path(os.path.realpath(input_file))\n    directory = full_path.parent\n    taskname = directory.stem\n    datadir = directory.parent\n    logger.info(\n        ""downloading and extracting file {} to dir {}"".format(taskname, datadir)\n    )\n    if ""conll03-"" in taskname:\n        # conll03 is copyrighted, but luckily somebody put it on github. Kudos!\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n        for dataset in [""train"", ""dev"", ""test""]:\n            if ""de"" in taskname:\n                _conll03get(dataset, directory, ""de"")\n            elif ""en"" in taskname:\n                _conll03get(dataset, directory, ""en"")\n            else:\n                logger.error(""Cannot download {}. Unknown data source."".format(taskname))\n    elif taskname not in DOWNSTREAM_TASK_MAP:\n        logger.error(""Cannot download {}. Unknown data source."".format(taskname))\n    else:\n        if os.name == ""nt"":  # make use of NamedTemporaryFile compatible with Windows\n            delete_tmp_file = False\n        else:\n            delete_tmp_file = True\n        with tempfile.NamedTemporaryFile(delete=delete_tmp_file) as temp_file:\n            http_get(DOWNSTREAM_TASK_MAP[taskname], temp_file, proxies=proxies)\n            temp_file.flush()\n            temp_file.seek(0)  # making tempfile accessible\n\n            # checking files for correctness with md5sum.\n            if(""germeval14"" in taskname):\n                if ""2c9d5337d7a25b9a4bf6f5672dd091bc"" != _get_md5checksum(temp_file.name):\n                    logger.error(f""Someone has changed the file for {taskname}. Please make sure the correct file is used and update the md5sum in farm/data_handler/utils.py"")\n            elif ""germeval18"" in taskname:\n                if ""23244fa042dcc39e844635285c455205"" != _get_md5checksum(temp_file.name):\n                    logger.error(f""Someone has changed the file for {taskname}. Please make sure the correct file is used and update the md5sum in farm/data_handler/utils.py"")\n            elif ""gnad"" in taskname:\n                if ""ef62fe3f59c1ad54cf0271d8532b8f22"" != _get_md5checksum(temp_file.name):\n                    logger.error(f""Someone has changed the file for {taskname}. Please make sure the correct file is used and update the md5sum in farm/data_handler/utils.py"")\n            elif ""germeval17"" in taskname:\n                if ""f1bf67247dcfe7c3c919b7b20b3f736e"" != _get_md5checksum(temp_file.name):\n                    logger.error(f""Someone has changed the file for {taskname}. Please make sure the correct file is used and update the md5sum in farm/data_handler/utils.py"")\n            tfile = tarfile.open(temp_file.name)\n            tfile.extractall(datadir)\n        # temp_file gets deleted here\n\n\ndef _conll03get(dataset, directory, language):\n    # open in binary mode\n    with open(directory / f""{dataset}.txt"", ""wb"") as file:\n        # get request\n        response = get(DOWNSTREAM_TASK_MAP[f""conll03{language}{dataset}""])\n        # write to file\n        file.write(response.content)\n\n    # checking files for correctness with md5sum.\n    if f""conll03{language}{dataset}"" == ""conll03detrain"":\n        if ""ae4be68b11dc94e0001568a9095eb391"" != _get_md5checksum(str(directory / f""{dataset}.txt"")):\n            logger.error(\n                f""Someone has changed the file for conll03detrain. This data was collected from an external github repository.\\n""\n                f""Please make sure the correct file is used and update the md5sum in farm/data_handler/utils.py"")\n    elif f""conll03{language}{dataset}"" == ""conll03detest"":\n        if ""b8514f44366feae8f317e767cf425f28"" != _get_md5checksum(str(directory / f""{dataset}.txt"")):\n            logger.error(\n                f""Someone has changed the file for conll03detest. This data was collected from an external github repository.\\n""\n                f""Please make sure the correct file is used and update the md5sum in farm/data_handler/utils.py"")\n    elif f""conll03{language}{dataset}"" == ""conll03entrain"":\n        if ""11a942ce9db6cc64270372825e964d26"" != _get_md5checksum(str(directory / f""{dataset}.txt"")):\n            logger.error(\n                f""Someone has changed the file for conll03entrain. This data was collected from an external github repository.\\n""\n                f""Please make sure the correct file is used and update the md5sum in farm/data_handler/utils.py"")\n\n\n\ndef read_docs_from_txt(filename, delimiter="""", encoding=""utf-8"", max_docs=None, proxies=None):\n    """"""Reads a text file with one sentence per line and a delimiter between docs (default: empty lines) .""""""\n    if not (os.path.exists(filename)):\n        _download_extract_downstream_data(filename, proxies)\n\n    doc_count = 0\n    doc = []\n    prev_doc = None\n    corpus_lines = 0\n\n    with open(filename, ""r"", encoding=encoding) as f:\n        for line_num, line in enumerate(tqdm(f, desc=""Loading Dataset"", total=corpus_lines)):\n            line = line.strip()\n            if line == delimiter:\n                if len(doc) > 0:\n                    yield {""doc"": doc}\n                    doc_count += 1\n                    prev_doc = doc\n                    doc = []\n                    if max_docs:\n                        if doc_count >= max_docs:\n                            logger.info(f""Reached number of max_docs ({max_docs}). Skipping rest of file ..."")\n                            break\n                else:\n                    logger.warning(f""Found empty document in file (line {line_num}). ""\n                                   f""Make sure that you comply with the format: ""\n                                   f""One sentence per line and exactly *one* empty line between docs. ""\n                                   f""You might have multiple subsequent empty lines."")\n            else:\n                doc.append(line)\n\n        # if last row in file is not empty, we add the last parsed doc manually to all_docs\n        if len(doc) > 0:\n            if doc_count > 0:\n                if doc != prev_doc:\n                    yield {""doc"": doc}\n                    doc_count += 1\n            else:\n                yield {""doc"": doc}\n                doc_count += 1\n\n        if doc_count < 2:\n            raise ValueError(f""Found only {doc_count} docs in {filename}). You need at least 2! \\n""\n                           f""Make sure that you comply with the format: \\n""\n                           f""-> One sentence per line and exactly *one* empty line between docs. \\n""\n                           f""You might have a single block of text without empty lines inbetween."")\n\n\ndef pad(seq, max_seq_len, pad_token, pad_on_left=False):\n    ret = seq\n    n_required_pad = max_seq_len - len(seq)\n    for _ in range(n_required_pad):\n        if pad_on_left:\n            ret.insert(0, pad_token)\n        else:\n            ret.append(pad_token)\n    return ret\n\n\ndef expand_labels(labels_word, initial_mask, non_initial_token):\n    # For inference mode\n    if not labels_word:\n        return None\n    labels_token = []\n    word_index = 0\n    for im in initial_mask:\n        if im:\n            # i.e. if token is word initial\n            labels_token.append(labels_word[word_index])\n            word_index += 1\n        else:\n            # i.e. token is not the first in the word\n            labels_token.append(non_initial_token)\n\n    assert len(labels_token) == len(initial_mask)\n    return labels_token\n\n\ndef get_sentence_pair(doc, all_baskets, idx, prob_next_sentence=0.5):\n    """"""\n    Get one sample from corpus consisting of two sentences. With prob. 50% these are two subsequent sentences\n    from one doc. With 50% the second sentence will be a random one from another doc.\n\n    :param doc: The current document\n    :param all_baskets: SampleBaskets containing multiple other docs from which we can sample the second sentence\n    if we need a random one.\n    :param idx: int, index of sample.\n    :return: (str, str, int), sentence 1, sentence 2, isNextSentence Label\n    """"""\n    sent_1, sent_2 = doc[idx], doc[idx + 1]\n\n    if random.random() > prob_next_sentence:\n        label = True\n    else:\n        sent_2 = _get_random_sentence(all_baskets, forbidden_doc=doc)\n        label = False\n\n    assert len(sent_1) > 0\n    assert len(sent_2) > 0\n    return sent_1, sent_2, label\n\n\ndef _get_random_sentence(all_baskets, forbidden_doc):\n    """"""\n    Get random line from another document for nextSentence task.\n\n    :return: str, content of one line\n    """"""\n    # Similar to original BERT tf repo: This outer loop should rarely go for more than one iteration for large\n    # corpora. However, just to be careful, we try to make sure that\n    # the random document is not the same as the document we\'re processing.\n    sentence = None\n    for _ in range(100):\n        rand_doc_idx = random.randrange(len(all_baskets))\n        rand_doc = all_baskets[rand_doc_idx][""doc""]\n\n        # check if our picked random doc is really different to our initial doc\n        if rand_doc != forbidden_doc:\n            rand_sent_idx = random.randrange(len(rand_doc))\n            sentence = rand_doc[rand_sent_idx]\n            break\n    if sentence is None:\n        raise Exception(""Failed to pick out a suitable random substitute for next sentence"")\n    return sentence\n\n\ndef get_sequence_pair(doc, chunk, chunk_clear_text, all_baskets, tokenizer, max_num_tokens, prob_next_sentence=0.5):\n    """"""\n    Get one sample from corpus consisting of two sequences. A sequence can consist of more than one sentence.\n    With prob. 50% these are two subsequent sequences from one doc. With 50% the second sequence will be a\n    random one from another document.\n\n    :param doc: The current document.\n    :type doc: [str]\n    :param chunk: List of subsequent, tokenized sentences.\n    :type chunk: [dict]\n    :param chunk_clear_text: List of subsequent sentences.\n    :type chunk_clear_text: [str]\n    :param all_baskets: SampleBaskets containing multiple other docs from which we can sample the second sequence\n    if we need a random one.\n    :type all_baskets: [dict]\n    :param tokenizer: Used to split a sentence (str) into tokens.\n    :param max_num_tokens: Samples are truncated after this many tokens.\n    :type max_num_tokens: int\n    :return: (list, list, dict, int)\n        tokenized seq a,\n        tokenized seq b,\n        sample in clear text with label,\n        number of unused sentences in chunk\n    """"""\n    sequence_a = []\n    sequence_b = []\n    sample_in_clear_text = { ""text_a"" : """", ""text_b"" : """"}\n    # determine how many segments from chunk go into sequence_a\n    len_sequence_a = 0\n    a_end = 1\n    if len(chunk) >= 2:\n        a_end = random.randrange(1, len(chunk))\n    for i in range(a_end):\n        sequence_a.append(chunk[i])\n        sample_in_clear_text[""text_a""] += f""{chunk_clear_text[i]} ""\n        len_sequence_a += len(chunk[i][""tokens""])\n    sample_in_clear_text[""text_a""].strip()\n\n    # actual next sequence\n    if (random.random() > prob_next_sentence) and (len(chunk) > 1):\n        label = True\n        for i in range(a_end, len(chunk)):\n            sequence_b.append(chunk[i])\n            sample_in_clear_text[""text_b""] += f""{chunk_clear_text[i]} ""\n        sample_in_clear_text[""text_b""].strip()\n        sample_in_clear_text[""nextsentence_label""] = True\n        num_unused_segments = 0\n    # edge case: split sequence in half\n    elif (len(chunk) == 1) and len_sequence_a >= max_num_tokens:\n        sequence_a = {}\n        sequence_b = {}\n        if int(len(chunk[0][""tokens""])/2) >= max_num_tokens:\n            boundary = int(max_num_tokens/2)\n        else:\n            boundary = int(len(chunk[0][""tokens""])/2)\n        sequence_a[""tokens""] = chunk[0][""tokens""][:boundary]\n        sequence_a[""offsets""] = chunk[0][""offsets""][:boundary]\n        sequence_a[""start_of_word""] = chunk[0][""start_of_word""][:boundary]\n        sequence_b[""tokens""] = chunk[0][""tokens""][boundary:]\n        sequence_b[""start_of_word""] = chunk[0][""start_of_word""][boundary:]\n        # get offsets for sequence_b right\n        seq_b_offset_start = chunk[0][""offsets""][boundary]\n        sequence_b[""offsets""] = [offset - seq_b_offset_start for offset in chunk[0][""offsets""][boundary:]]\n        # get clear text\n        clear_text_boundary = chunk[0][""offsets""][boundary]\n        sample_in_clear_text[""text_a""] = chunk_clear_text[0][:clear_text_boundary]\n        sample_in_clear_text[""text_b""] = chunk_clear_text[0][clear_text_boundary:]\n        sample_in_clear_text[""text_a""].strip()\n        sample_in_clear_text[""text_b""].strip()\n        sample_in_clear_text[""nextsentence_label""] = True\n        return [sequence_a], [sequence_b], sample_in_clear_text, 0\n    # random next sequence\n    else:\n        label = False\n        sequence_b_length = 0\n        target_b_length = max_num_tokens - len_sequence_a\n        random_doc = _get_random_doc(all_baskets, forbidden_doc=doc)\n\n        random_start = random.randrange(len(random_doc))\n        for i in range(random_start, len(random_doc)):\n            current_sentence_tokenized = tokenize_with_metadata(random_doc[i], tokenizer)\n            sequence_b.append(current_sentence_tokenized)\n            sample_in_clear_text[""text_b""] += f""{random_doc[i]} ""\n            sequence_b_length += len(current_sentence_tokenized[""tokens""])\n            if sequence_b_length >= target_b_length:\n                break\n\n        sample_in_clear_text[""text_b""].strip()\n        sample_in_clear_text[""nextsentence_label""] = False\n\n        # We didn\'t use all of the segments in chunk => put them back\n        num_unused_segments = len(chunk) - a_end\n\n    assert len(sequence_a) > 0\n    assert len(sequence_b) > 0\n    return sequence_a, sequence_b, sample_in_clear_text, num_unused_segments\n\n\ndef _get_random_doc(all_baskets, forbidden_doc):\n    random_doc = None\n    for _ in range(100):\n        rand_doc_idx = random.randrange(len(all_baskets))\n        random_doc = all_baskets[rand_doc_idx][""doc""]\n\n        # check if random doc is different from initial doc\n        if random_doc != forbidden_doc:\n            break\n\n    if random_doc is None:\n        raise Exception(""Failed to pick out a suitable random substitute for next sequence"")\n    return random_doc\n\n\ndef join_sentences(sequence):\n    """"""\n    Takes a list of subsequent, tokenized sentences and puts them together into one sequence.\n    :param sequence: List of tokenized sentences.\n    :type sequence: [dict]\n    :return: Tokenized sequence. (Dict with keys \'tokens\', \'offsets\' and \'start_of_word\')\n    """"""\n    sequence_joined = {\n        ""tokens"" : [],\n        ""offsets"" : [],\n        ""start_of_word"" : []\n    }\n    last_offset = 0\n    for sentence in sequence:\n        sequence_joined[""tokens""].extend(sentence[""tokens""])\n        sequence_joined[""start_of_word""].extend(sentence[""start_of_word""])\n        # get offsets right\n        current_offsets = [offset + last_offset for offset in sentence[""offsets""]]\n        sequence_joined[""offsets""].extend(current_offsets)\n        last_offset += sentence[""offsets""][-1] + 2\n\n    return sequence_joined\n\n\ndef mask_random_words(tokens, vocab, token_groups=None, max_predictions_per_seq=20, masked_lm_prob=0.15):\n    """"""\n    Masking some random tokens for Language Model task with probabilities as in the original BERT paper.\n    num_masked.\n    If token_groups is supplied, whole word masking is applied, so *all* tokens of a word are either masked or not.\n    This option was added by the BERT authors later and showed solid improvements compared to the original objective.\n    Whole Word Masking means that if we mask all of the wordpieces corresponding to an original word.\n    When a word has been split intoWordPieces, the first token does not have any marker and any subsequence\n    tokens are prefixed with ##. So whenever we see the ## token, we\n    append it to the previous set of word indexes. Note that Whole Word Masking does *not* change the training code\n    at all -- we still predict each WordPiece independently, softmaxed over the entire vocabulary.\n    This implementation is mainly a copy from the original code by Google, but includes some simplifications.\n\n    :param tokens: tokenized sentence.\n    :type tokens: [str]\n    :param vocab: vocabulary for choosing tokens for random masking.\n    :type vocab: dict\n    :param token_groups: If supplied, only whole groups of tokens get masked. This can be whole words but\n    also other types (e.g. spans). Booleans indicate the start of a group.\n    :type token_groups: [bool]\n    :param max_predictions_per_seq: maximum number of masked tokens\n    :type max_predictions_per_seq: int\n    :param masked_lm_prob: probability of masking a token\n    :type masked_lm_prob: float\n    :return: (list of str, list of int), masked tokens and related labels for LM prediction\n    """"""\n\n    #TODO make special tokens model independent\n\n    # 1. Combine tokens to one group (e.g. all subtokens of a word)\n    cand_indices = []\n    for (i, token) in enumerate(tokens):\n        if token == ""[CLS]"" or token == ""[SEP]"":\n            continue\n        if (token_groups and len(cand_indices) >= 1 and not token_groups[i]):\n            cand_indices[-1].append(i)\n        else:\n            cand_indices.append([i])\n\n    num_to_mask = min(max_predictions_per_seq,\n                      max(1, int(round(len(tokens) * masked_lm_prob))))\n\n    random.shuffle(cand_indices)\n    output_label = [\'\'] * len(tokens)\n    num_masked = 0\n    assert ""[MASK]"" not in tokens\n\n    # 2. Mask the first groups until we reach the number of tokens we wanted to mask (num_to_mask)\n    for index_set in cand_indices:\n        if num_masked >= num_to_mask:\n            break\n        # If adding a whole-word mask would exceed the maximum number of\n        # predictions, then just skip this candidate.\n        if num_masked + len(index_set) > num_to_mask:\n            continue\n\n        for index in index_set:\n            prob = random.random()\n            num_masked += 1\n            original_token = tokens[index]\n            # 80% randomly change token to mask token\n            if prob < 0.8:\n                tokens[index] = ""[MASK]""\n\n            # 10% randomly change token to random token\n            #TODO currently custom vocab is not included here\n            elif prob < 0.9:\n                tokens[index] = random.choice(list(vocab.items()))[0]\n\n            # -> rest 10% randomly keep current token\n\n            # append current token to output (we will predict these later)\n            try:\n                output_label[index] = original_token\n            except KeyError:\n                # For unknown words (should not occur with BPE vocab)\n                output_label[index] = ""[UNK]""\n                logger.warning(\n                    ""Cannot find token \'{}\' in vocab. Using [UNK] instead"".format(original_token)\n                )\n\n    return tokens, output_label\n\n\ndef is_json(x):\n    if issubclass(type(x), Path):\n        return True\n    try:\n        json.dumps(x)\n        return True\n    except:\n        return False\n\n\ndef grouper(iterable, n, worker_id=0, total_workers=1):\n    """"""\n    Split an iterable into a list of n-sized chunks. Each element in the chunk is a tuple of (index_num, element).\n\n    Example:\n\n    >>> list(grouper(\'ABCDEFG\', 3))\n    [[(0, \'A\'), (1, \'B\'), (2, \'C\')], [(3, \'D\'), (4, \'E\'), (5, \'F\')], [(6, \'G\')]]\n\n\n\n    Use with the StreamingDataSilo\n\n    When StreamingDataSilo is used with multiple PyTorch DataLoader workers, the generator\n    yielding dicts(that gets converted to datasets) is replicated across the workers.\n\n    To avoid duplicates, we split the dicts across workers by creating a new generator for\n    each worker using this method.\n\n    Input --> [dictA, dictB, dictC, dictD, dictE, ...] with total worker=3 and n=2\n\n    Output for worker 1: [(dictA, dictB), (dictG, dictH), ...]\n    Output for worker 2: [(dictC, dictD), (dictI, dictJ), ...]\n    Output for worker 3: [(dictE, dictF), (dictK, dictL), ...]\n\n    This method also adds an index number to every dict yielded similar to the grouper().\n\n    :param iterable: a generator object that yields dicts\n    :type iterable: generator\n    :param n: the dicts are grouped in n-sized chunks that gets converted to datasets\n    :type n: int\n    :param worker_id: the worker_id for the PyTorch DataLoader\n    :type worker_id: int\n    :param total_workers: total number of workers for the PyTorch DataLoader\n    :type total_workers: int\n    """"""\n    # TODO make me comprehensible :)\n    def get_iter_start_pos(gen):\n        start_pos = worker_id * n\n        for i in gen:\n            if start_pos:\n                start_pos -= 1\n                continue\n            yield i\n\n    def filter_elements_per_worker(gen):\n        x = n\n        y = (total_workers - 1) * n\n        for i in gen:\n            if x:\n                yield i\n                x -= 1\n            else:\n                if y != 1:\n                    y -= 1\n                    continue\n                else:\n                    x = n\n                    y = (total_workers - 1) * n\n\n    iterable = iter(enumerate(iterable))\n    iterable = get_iter_start_pos(iterable)\n    if total_workers > 1:\n        iterable = filter_elements_per_worker(iterable)\n\n    return iter(lambda: list(islice(iterable, n)), [])\n\ndef generate_tok_to_ch_map(text):\n    """""" Generates a mapping from token to character index when a string text is split using .split()\n    TODO e.g.""""""\n    map = [0]\n    follows_whitespace = False\n    for i, ch in enumerate(text):\n        if follows_whitespace:\n            if ch not in string.whitespace:\n                map.append(i)\n                follows_whitespace = False\n        else:\n            if ch in string.whitespace:\n                follows_whitespace = True\n    return map\n\ndef split_with_metadata(text):\n    """""""" Splits a string text by whitespace and also returns indexes which is a mapping from token index\n    to character index""""""\n    split_text = text.split()\n    indexes = generate_tok_to_ch_map(text)\n    assert len(split_text) == len(indexes)\n    return split_text, indexes\n\n\ndef convert_id(id_string):\n    """"""\n    Splits a string id into parts. If it is an id generated in the SQuAD pipeline it simple splits the id by the dashes\n    and converts the parts to ints. If it is generated by the non-SQuAD pipeline, it splits the id by the dashes and\n    converts references to ""train"" or ""infer"" into ints.\n    :param id_string:\n    :return:\n    """"""\n    ret = []\n    datasets = [""train"", ""infer""]\n    id_list = id_string.split(""-"")\n    for x in id_list:\n        if x in datasets:\n            ret.append(datasets.index(x))\n        else:\n            ret.append(int(x))\n    return ret\n\ndef convert_qa_input_dict(infer_dict):\n    """""" Input dictionaries in QA can either have [""context"", ""qas""] (internal format) as keys or\n    [""text"", ""questions""] (api format). This function converts the latter into the former""""""\n    try:\n        # Check if infer_dict is already in internal json format\n        if ""context"" in infer_dict and ""qas"" in infer_dict:\n            return infer_dict\n        # converts dicts from inference mode to data structure used in FARM\n        questions = infer_dict[""questions""]\n        text = infer_dict[""text""]\n        document_id = infer_dict.get(""document_id"", None)\n        qas = [{""question"": q,\n                ""id"": None,\n                ""answers"": [],\n                ""is_impossible"": False} for i, q in enumerate(questions)]\n        converted = {""qas"": qas,\n                     ""context"": text,\n                     ""document_id"":document_id}\n        return converted\n    except KeyError:\n        raise Exception(""Input does not have the expected format"")\n\n\n\n\n\n'"
farm/evaluation/__init__.py,0,b''
farm/evaluation/metrics.py,0,"b'import torch\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import pearsonr, spearmanr\nfrom seqeval.metrics import f1_score as ner_f1_score\nfrom seqeval.metrics import classification_report as token_classification_report\nfrom sklearn.metrics import (\n    matthews_corrcoef,\n    recall_score,\n    precision_score,\n    f1_score,\n    mean_squared_error,\n    r2_score,\n    classification_report\n)\nfrom farm.utils import flatten_list\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nregistered_metrics = {}\nregistered_reports = {}\n\ndef register_metrics(name, implementation):\n    registered_metrics[name] = implementation\n\ndef register_report(name, implementation):\n    """"""\n    Register a custom reporting function to be used during eval.\n\n    This can be useful:\n    - if you want to overwrite a report for an existing output type of prediction head (e.g. ""per_token"")\n    - if you have a new type of prediction head and want to add a custom report for it\n\n    :param name: This must match the `ph_output_type` attribute of the PredictionHead for which the report should be used.\n                 (e.g. TokenPredictionHead => `per_token`, YourCustomHead => `some_new_type`).\n    :type name: str\n    :param implementation: Function to be executed. It must take lists of `y_true` and `y_pred` as input and return a\n                           printable object (e.g. string or dict).\n                           See sklearns.metrics.classification_report for an example.\n    :type implementation: function\n    """"""\n    registered_reports[name] = implementation\n\ndef simple_accuracy(preds, labels):\n    # works also with nested lists of different lengths (needed for masked LM task)\n    if type(preds) == type(labels) == list:\n        preds = np.array(list(flatten_list(preds)))\n        labels = np.array(list(flatten_list(labels)))\n    assert type(preds) == type(labels) == np.ndarray\n    correct = preds == labels\n    return {""acc"": correct.mean()}\n\n\ndef acc_and_f1(preds, labels):\n    acc = simple_accuracy(preds, labels)\n    f1 = f1_score(y_true=labels, y_pred=preds)\n    return {""acc"": acc, ""f1"": f1, ""acc_and_f1"": (acc + f1) / 2}\n\n\ndef f1_macro(preds, labels):\n    return {""f1_macro"": f1_score(y_true=labels, y_pred=preds, average=""macro"")}\n\n\ndef pearson_and_spearman(preds, labels):\n    pearson_corr = pearsonr(preds, labels)[0]\n    spearman_corr = spearmanr(preds, labels)[0]\n    return {\n        ""pearson"": pearson_corr,\n        ""spearman"": spearman_corr,\n        ""corr"": (pearson_corr + spearman_corr) / 2,\n    }\n\ndef compute_metrics(metric, preds, labels):\n    assert len(preds) == len(labels)\n    if metric == ""mcc"":\n        return {""mcc"": matthews_corrcoef(labels, preds)}\n    elif metric == ""acc"":\n        return simple_accuracy(preds, labels)\n    elif metric == ""acc_f1"":\n        return acc_and_f1(preds, labels)\n    elif metric == ""pear_spear"":\n        return pearson_and_spearman(preds, labels)\n    # TODO this metric seems very specific for NER and doesnt work for\n    elif metric == ""seq_f1"":\n        return {""seq_f1"": ner_f1_score(labels, preds)}\n    elif metric == ""f1_macro"":\n        return f1_macro(preds, labels)\n    elif metric == ""squad"":\n        return squad(preds, labels)\n    elif metric == ""mse"":\n        return {""mse"": mean_squared_error(preds, labels)}\n    elif metric == ""r2"":\n        return {""r2"": r2_score(preds, labels)}\n    elif metric == ""top_n_recall"":\n        return {""top_n_recall"": top_n_recall(preds, labels)}\n    # elif metric == ""masked_accuracy"":\n    #     return simple_accuracy(preds, labels, ignore=-1)\n    elif metric in registered_metrics:\n        metric_func = registered_metrics[metric]\n        return metric_func(preds, labels)\n    else:\n        raise KeyError(metric)\n\n\ndef compute_report_metrics(head, preds, labels):\n    if head.ph_output_type in registered_reports:\n        report_fn = registered_reports[head.ph_output_type]\n    elif head.ph_output_type == ""per_token"":\n        report_fn = token_classification_report\n    elif head.ph_output_type == ""per_sequence"":\n        report_fn = classification_report\n    elif head.ph_output_type == ""per_token_squad"":\n        report_fn = lambda *args, **kwargs: ""Not Implemented""\n    elif head.ph_output_type == ""per_sequence_continuous"":\n        report_fn = r2_score\n    else:\n        raise AttributeError(f""No report function for head.ph_output_type \'{head.ph_output_type}\'. ""\n                             f""You can register a custom one via register_report(name=\'{head.ph_output_type}\', implementation=<your_report_function>"")\n\n    # CHANGE PARAMETERS, not all report_fn accept digits\n    if head.ph_output_type in [""per_sequence""]:\n        # supply labels as all possible combination because if ground truth labels do not cover\n        # all values in label_list (maybe dev set is small), the report will break\n        if head.model_type == ""multilabel_text_classification"":\n            # For multilabel classification, we don\'t eval with string labels here, but with multihot vectors.\n            # Therefore we need to supply all possible label ids instead of label values.\n            all_possible_labels = list(range(len(head.label_list)))\n        else:\n            all_possible_labels = head.label_list\n        return report_fn(\n            labels,\n            preds,\n            digits=4,\n            labels=all_possible_labels,\n            target_names=head.label_list\n        )\n    else:\n        return report_fn(labels, preds)\n\n\ndef squad_EM(preds, labels):\n    # TODO write comment describing function\n    n_docs = len(preds)\n    n_correct = 0\n    for doc_idx in range(n_docs):\n        span = preds[doc_idx][0][0]\n        pred_start = span.start\n        pred_end = span.end\n        curr_labels = labels[doc_idx]\n        if (pred_start, pred_end) in curr_labels:\n            n_correct += 1\n    return n_correct/n_docs\n\ndef squad_f1(preds, labels):\n    f1_scores = []\n    n_docs = len(preds)\n    for i in range(n_docs):\n        best_pred = preds[i][0]\n        best_f1 = max([squad_f1_single(best_pred, label) for label in labels[i]])\n        f1_scores.append(best_f1)\n    return np.mean(f1_scores)\n\n\ndef squad_f1_single(pred, label, pred_idx=0):\n    label_start, label_end = label\n    span = pred[pred_idx]\n    pred_start = span.start\n    pred_end = span.end\n\n    if (pred_start + pred_end == 0) or (label_start + label_end == 0):\n        if pred_start == label_start:\n            return 1.0\n        else:\n            return 0.0\n    pred_span = list(range(pred_start, pred_end + 1))\n    label_span = list(range(label_start, label_end + 1))\n    n_overlap = len([x for x in pred_span if x in label_span])\n    if n_overlap == 0:\n        return 0.0\n    precision = n_overlap / len(pred_span)\n    recall = n_overlap / len(label_span)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\ndef squad(preds, labels):\n    em = squad_EM(preds=preds, labels=labels)\n    f1 = squad_f1(preds=preds, labels=labels)\n    top_recall = top_n_recall(preds=preds, labels=labels)\n    return {""EM"": em, ""f1"": f1, ""top_n_recall"": top_recall}\n\ndef top_n_recall(preds, labels):\n    answer_in_top_n = []\n    n_questions = len(preds)\n    for i in range(n_questions):\n        f1_score = 0\n        current_preds = preds[i][0]\n        for idx, pred in enumerate(current_preds):\n            f1_score = max([squad_f1_single(current_preds, label, pred_idx=idx) for label in labels[i]])\n            if f1_score:\n                break\n        if f1_score:\n            answer_in_top_n.append(1)\n        else:\n            answer_in_top_n.append(0)\n\n    return np.mean(answer_in_top_n)\n\n\n\n'"
farm/evaluation/msmarco_passage_farm.py,0,"b'from farm.evaluation.msmarco_passage_official import compute_metrics_from_files\nimport os\nimport pandas as pd\n\n\ndef msmarco_evaluation(preds_file, dev_file, qrels_file, output_file):\n    """"""\n    Performs official msmarco passage ranking evaluation (https://github.com/microsoft/MSMARCO-Passage-Ranking)\n    on a file containing the is_relevent prediction scores. It will convert the input file (qid, pid, score)\n    into the format expected by the official eval function (compute_metrics_from_files)\n\n    :param predictions_filename: File where each line is the is_relevant prediction score\n    :param dev_filename: File in format qid, query, pid, passage, label\n    :param qrels_filename: File in the format qid, pid when is_relevant=1\n    :param output_file: File to write to in format qid, pid, rank\n\n    :return:\n    """"""\n\n    # Initialize files\n    preds_scores = [float(l) for l in open(preds_file)]\n    dev_lines = [l for i,l in enumerate(open(dev_file)) if i != 0]\n    output = open(output_file, ""w"")\n\n    # Populate a dict with all qid/pid/score triples\n    results = dict()\n    for i, (score, line) in enumerate(zip(preds_scores, dev_lines)):\n        if i == 0:\n            continue\n        qid, _, pid, _, _ = line.split(""\\t"")\n        if qid not in results:\n            results[qid] = []\n        results[qid].append((pid, score))\n\n    # ##########\n    # ### NOTE: This block is to generate a view that is interpretable when debugging\n    # ##########\n    # interpretable = dict()\n    # for i, (score, line) in enumerate(zip(preds_scores, dev_lines)):\n    #     if i == 0:\n    #         continue\n    #     _, query, _, passage, label = line.split(""\\t"")\n    #     if query not in interpretable:\n    #         interpretable[query] = []\n    #     interpretable[query].append((passage, score, label[:-1]))\n    # for query in interpretable:\n    #     sorted_scores = sorted(interpretable[query], key= lambda x: x[1], reverse=True)[:10]\n    #     results[query] = sorted_scores\n    # relevant = []\n    # for query in interpretable:\n    #     for (passage, score, label) in interpretable[query]:\n    #         if label == ""1"":\n    #             relevant.append((passage, score))\n    # rel_scores = [x[1] for x in relevant]\n    # irrelevant = []\n    # for query in interpretable:\n    #     for (passage, score, label) in interpretable[query]:\n    #         if label == ""0"":\n    #             irrelevant.append((passage, score))\n    # irrel_scores = [x[1] for x in irrelevant]\n    # print()\n\n    # Sort by scores and take top 10\n    for qid in list(results):\n        sorted_scores = sorted(results[qid], key= lambda x: x[1], reverse=True)[:10]\n        results[qid] = [(pid, i+1) for i, (pid, _)  in enumerate(sorted_scores)]\n\n    # Write to file\n    for qid in list(results):\n        for (pid, rank) in results[qid]:\n            output.write(f""{qid}\\t{pid}\\t{rank}\\n"")\n    output.close()\n\n    curr_qids = list(results)\n    df = pd.read_csv(qrels_file, sep=""\\t"", header=None)\n    df = df.loc[df[0].isin(curr_qids)]\n    df.to_csv(""tmp"", sep=""\\t"", header=None, index=None)\n\n    path_to_reference = ""tmp""\n    path_to_candidate = output_file\n    metrics = compute_metrics_from_files(path_to_reference, path_to_candidate)\n    print(\'#####################\')\n    for metric in sorted(metrics):\n        print(\'{}: {}\'.format(metric, metrics[metric]))\n    print(\'#####################\')\n    os.remove(path_to_reference)\n\n\n\n'"
farm/evaluation/msmarco_passage_official.py,0,"b'""""""\nThis module computes evaluation metrics for MSMARCO dataset on the ranking task.\nCommand line:\npython msmarco_eval_ranking.py <path_to_reference_file> <path_to_candidate_file>\n\nCreation Date : 06/12/2018\nLast Modified : 1/21/2019\nAuthors : Daniel Campos <dacamp@microsoft.com>, Rutger van Haasteren <ruvanh@microsoft.com>\n""""""\nimport sys\nimport statistics\n\nfrom collections import Counter\n\nMaxMRRRank = 10\n\n\ndef load_reference_from_stream(f):\n    """"""Load Reference reference relevant passages\n    Args:f (stream): stream to load.\n    Returns:qids_to_relevant_passageids (dict): dictionary mapping from query_id (int) to relevant passages (list of ints).\n    """"""\n    qids_to_relevant_passageids = {}\n    for l in f:\n        try:\n            l = l.strip().split(\'\\t\')\n            qid = int(l[0])\n            if qid in qids_to_relevant_passageids:\n                pass\n            else:\n                qids_to_relevant_passageids[qid] = []\n            qids_to_relevant_passageids[qid].append(int(l[2]))\n        except:\n            raise IOError(\'\\""%s\\"" is not valid format\' % l)\n    return qids_to_relevant_passageids\n\n\ndef load_reference(path_to_reference):\n    """"""Load Reference reference relevant passages\n    Args:path_to_reference (str): path to a file to load.\n    Returns:qids_to_relevant_passageids (dict): dictionary mapping from query_id (int) to relevant passages (list of ints).\n    """"""\n    with open(path_to_reference, \'r\') as f:\n        qids_to_relevant_passageids = load_reference_from_stream(f)\n    return qids_to_relevant_passageids\n\n\ndef load_candidate_from_stream(f):\n    """"""Load candidate data from a stream.\n    Args:f (stream): stream to load.\n    Returns:qid_to_ranked_candidate_passages (dict): dictionary mapping from query_id (int) to a list of 1000 passage ids(int) ranked by relevance and importance\n    """"""\n    qid_to_ranked_candidate_passages = {}\n    for l in f:\n        try:\n            l = l.strip().split(\'\\t\')\n            qid = int(l[0])\n            pid = int(l[1])\n            rank = int(l[2])\n            if qid in qid_to_ranked_candidate_passages:\n                pass\n            else:\n                # By default, all PIDs in the list of 1000 are 0. Only override those that are given\n                tmp = [0] * 1000\n                qid_to_ranked_candidate_passages[qid] = tmp\n            qid_to_ranked_candidate_passages[qid][rank - 1] = pid\n        except:\n            raise IOError(\'\\""%s\\"" is not valid format\' % l)\n    return qid_to_ranked_candidate_passages\n\n\ndef load_candidate(path_to_candidate):\n    """"""Load candidate data from a file.\n    Args:path_to_candidate (str): path to file to load.\n    Returns:qid_to_ranked_candidate_passages (dict): dictionary mapping from query_id (int) to a list of 1000 passage ids(int) ranked by relevance and importance\n    """"""\n\n    with open(path_to_candidate, \'r\') as f:\n        qid_to_ranked_candidate_passages = load_candidate_from_stream(f)\n    return qid_to_ranked_candidate_passages\n\n\ndef quality_checks_qids(qids_to_relevant_passageids, qids_to_ranked_candidate_passages):\n    """"""Perform quality checks on the dictionaries\n\n    Args:\n    p_qids_to_relevant_passageids (dict): dictionary of query-passage mapping\n        Dict as read in with load_reference or load_reference_from_stream\n    p_qids_to_ranked_candidate_passages (dict): dictionary of query-passage candidates\n    Returns:\n        bool,str: Boolean whether allowed, message to be shown in case of a problem\n    """"""\n    message = \'\'\n    allowed = True\n\n    # Create sets of the QIDs for the submitted and reference queries\n    candidate_set = set(qids_to_ranked_candidate_passages.keys())\n    ref_set = set(qids_to_relevant_passageids.keys())\n\n    # Check that we do not have multiple passages per query\n    for qid in qids_to_ranked_candidate_passages:\n        # Remove all zeros from the candidates\n        duplicate_pids = set(\n            [item for item, count in Counter(qids_to_ranked_candidate_passages[qid]).items() if count > 1])\n\n        if len(duplicate_pids - set([0])) > 0:\n            message = ""Cannot rank a passage multiple times for a single query. QID={qid}, PID={pid}"".format(\n                qid=qid, pid=list(duplicate_pids)[0])\n            allowed = False\n\n    return allowed, message\n\n\ndef compute_metrics(qids_to_relevant_passageids, qids_to_ranked_candidate_passages):\n    """"""Compute MRR metric\n    Args:\n    p_qids_to_relevant_passageids (dict): dictionary of query-passage mapping\n        Dict as read in with load_reference or load_reference_from_stream\n    p_qids_to_ranked_candidate_passages (dict): dictionary of query-passage candidates\n    Returns:\n        dict: dictionary of metrics {\'MRR\': <MRR Score>}\n    """"""\n    all_scores = {}\n    MRR = 0\n    qids_with_relevant_passages = 0\n    ranking = []\n    for qid in qids_to_ranked_candidate_passages:\n        if qid in qids_to_relevant_passageids:\n            ranking.append(0)\n            target_pid = qids_to_relevant_passageids[qid]\n            candidate_pid = qids_to_ranked_candidate_passages[qid]\n            for i in range(0, MaxMRRRank):\n                if candidate_pid[i] in target_pid:\n                    MRR += 1 / (i + 1)\n                    ranking.pop()\n                    ranking.append(i + 1)\n                    break\n    if len(ranking) == 0:\n        raise IOError(""No matching QIDs found. Are you sure you are scoring the evaluation set?"")\n\n    MRR = MRR / len(qids_to_relevant_passageids)\n    all_scores[\'MRR @10\'] = MRR\n    all_scores[\'QueriesRanked\'] = len(qids_to_ranked_candidate_passages)\n    return all_scores\n\n\ndef compute_metrics_from_files(path_to_reference, path_to_candidate, perform_checks=True):\n    """"""Compute MRR metric\n    Args:\n    p_path_to_reference_file (str): path to reference file.\n        Reference file should contain lines in the following format:\n            QUERYID\\tPASSAGEID\n            Where PASSAGEID is a relevant passage for a query. Note QUERYID can repeat on different lines with different PASSAGEIDs\n    p_path_to_candidate_file (str): path to candidate file.\n        Candidate file sould contain lines in the following format:\n            QUERYID\\tPASSAGEID1\\tRank\n            If a user wishes to use the TREC format please run the script with a -t flag at the end. If this flag is used the expected format is\n            QUERYID\\tITER\\tDOCNO\\tRANK\\tSIM\\tRUNID\n            Where the values are separated by tabs and ranked in order of relevance\n    Returns:\n        dict: dictionary of metrics {\'MRR\': <MRR Score>}\n    """"""\n\n    qids_to_relevant_passageids = load_reference(path_to_reference)\n    qids_to_ranked_candidate_passages = load_candidate(path_to_candidate)\n    if perform_checks:\n        allowed, message = quality_checks_qids(qids_to_relevant_passageids, qids_to_ranked_candidate_passages)\n        if message != \'\': print(message)\n\n    return compute_metrics(qids_to_relevant_passageids, qids_to_ranked_candidate_passages)\n\n\ndef main():\n    """"""Command line:\n    python msmarco_eval_ranking.py <path_to_reference_file> <path_to_candidate_file>\n    """"""\n\n    if len(sys.argv) == 3:\n        path_to_reference = sys.argv[1]\n        path_to_candidate = sys.argv[2]\n        metrics = compute_metrics_from_files(path_to_reference, path_to_candidate)\n        print(\'#####################\')\n        for metric in sorted(metrics):\n            print(\'{}: {}\'.format(metric, metrics[metric]))\n        print(\'#####################\')\n\n    else:\n        print(\'Usage: msmarco_eval_ranking.py <reference ranking> <candidate ranking>\')\n        exit()\n\n\nif __name__ == \'__main__\':\n    main()'"
farm/evaluation/squad_evaluation.py,0,"b'""""""\n    This is a copy of the official evaluation script for SQuAD version 2.0.\n    Modified by XLNet authors to update `find_best_threshold` scripts for SQuAD V2.0\n\nIn addition to basic functionality, we also compute additional statistics and\nplot precision-recall curves if an additional na_prob.json file is provided.\nThis file is expected to map question ID\'s to the model\'s predicted probability\nthat a question is unanswerable.\n""""""\nimport argparse\nimport collections\nimport json\nimport numpy as np\nimport os\nimport re\nimport string\nimport sys\n\nclass EVAL_OPTS():\n  def __init__(self, data_file, pred_file, out_file="""",\n               na_prob_file=""na_prob.json"", na_prob_thresh=1.0,\n               out_image_dir=None, verbose=False):\n    self.data_file = data_file\n    self.pred_file = pred_file\n    self.out_file = out_file\n    self.na_prob_file = na_prob_file\n    self.na_prob_thresh = na_prob_thresh\n    self.out_image_dir = out_image_dir\n    self.verbose = verbose\n\nOPTS = None\n\ndef parse_args():\n  parser = argparse.ArgumentParser(\'Official evaluation script for SQuAD version 2.0.\')\n  parser.add_argument(\'data_file\', metavar=\'data.json\', help=\'Input data JSON file.\')\n  parser.add_argument(\'pred_file\', metavar=\'pred.json\', help=\'Model predictions.\')\n  parser.add_argument(\'--out-file\', \'-o\', metavar=\'eval.json\',\n                      help=\'Write accuracy metrics to file (default is stdout).\')\n  parser.add_argument(\'--na-prob-file\', \'-n\', metavar=\'na_prob.json\',\n                      help=\'Model estimates of probability of no answer.\')\n  parser.add_argument(\'--na-prob-thresh\', \'-t\', type=float, default=1.0,\n                      help=\'Predict """" if no-answer probability exceeds this (default = 1.0).\')\n  parser.add_argument(\'--out-image-dir\', \'-p\', metavar=\'out_images\', default=None,\n                      help=\'Save precision-recall curves to directory.\')\n  parser.add_argument(\'--verbose\', \'-v\', action=\'store_true\')\n  if len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n  return parser.parse_args()\n\ndef make_qid_to_has_ans(dataset):\n  qid_to_has_ans = {}\n  for article in dataset:\n    for p in article[\'paragraphs\']:\n      for qa in p[\'qas\']:\n        qid_to_has_ans[qa[\'id\']] = bool(qa[\'answers\'])\n  return qid_to_has_ans\n\ndef normalize_answer(s):\n  """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n  def remove_articles(text):\n    regex = re.compile(r\'\\b(a|an|the)\\b\', re.UNICODE)\n    return re.sub(regex, \' \', text)\n  def white_space_fix(text):\n    return \' \'.join(text.split())\n  def remove_punc(text):\n    exclude = set(string.punctuation)\n    return \'\'.join(ch for ch in text if ch not in exclude)\n  def lower(text):\n    return text.lower()\n  return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n  if not s: return []\n  return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n  return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n  gold_toks = get_tokens(a_gold)\n  pred_toks = get_tokens(a_pred)\n  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n  num_same = sum(common.values())\n  if len(gold_toks) == 0 or len(pred_toks) == 0:\n    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n    return int(gold_toks == pred_toks)\n  if num_same == 0:\n    return 0\n  precision = 1.0 * num_same / len(pred_toks)\n  recall = 1.0 * num_same / len(gold_toks)\n  f1 = (2 * precision * recall) / (precision + recall)\n  return f1\n\ndef get_raw_scores_extended(dataset, preds):\n  """""" By deepset""""""\n  exact_scores = {}\n  f1_scores = {}\n  labels_and_preds = {}\n  for article in dataset:\n    for p in article[\'paragraphs\']:\n      for qa in p[\'qas\']:\n        qid = qa[\'id\']\n        gold_answers = [a[\'text\'] for a in qa[\'answers\']\n                        if normalize_answer(a[\'text\'])]\n        if not gold_answers:\n          # For unanswerable questions, only correct answer is empty string\n          gold_answers = [\'\']\n        if qid not in preds:\n          print(\'Missing prediction for %s\' % qid)\n          continue\n        a_pred = preds[qid]\n        # Take max over all gold answers\n        exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n        f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n        labels_and_preds[qid] = {""label"": list(set(gold_answers)),\n                                 ""pred"": a_pred}\n  json.dump(labels_and_preds, open(""predictions_and_labels.json"", ""w""))\n  return exact_scores, f1_scores\n\ndef get_raw_scores(dataset, preds):\n  exact_scores = {}\n  f1_scores = {}\n  for article in dataset:\n    for p in article[\'paragraphs\']:\n      for qa in p[\'qas\']:\n        qid = qa[\'id\']\n        gold_answers = [a[\'text\'] for a in qa[\'answers\']\n                        if normalize_answer(a[\'text\'])]\n        if not gold_answers:\n          # For unanswerable questions, only correct answer is empty string\n          gold_answers = [\'\']\n        if qid not in preds:\n          print(\'Missing prediction for %s\' % qid)\n          continue\n        a_pred = preds[qid]\n        # Take max over all gold answers\n        exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n        f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n  return exact_scores, f1_scores\n\ndef apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n  new_scores = {}\n  for qid, s in scores.items():\n    pred_na = na_probs[qid] > na_prob_thresh\n    if pred_na:\n      new_scores[qid] = float(not qid_to_has_ans[qid])\n    else:\n      new_scores[qid] = s\n  return new_scores\n\ndef make_eval_dict(exact_scores, f1_scores, qid_list=None):\n  if not qid_list:\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        (\'exact\', 100.0 * sum(exact_scores.values()) / total),\n        (\'f1\', 100.0 * sum(f1_scores.values()) / total),\n        (\'total\', total),\n    ])\n  else:\n    total = len(qid_list)\n    return collections.OrderedDict([\n        (\'exact\', 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n        (\'f1\', 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n        (\'total\', total),\n    ])\n\ndef merge_eval(main_eval, new_eval, prefix):\n  for k in new_eval:\n    main_eval[\'%s_%s\' % (prefix, k)] = new_eval[k]\n\ndef plot_pr_curve(precisions, recalls, out_image, title):\n  plt.step(recalls, precisions, color=\'b\', alpha=0.2, where=\'post\')\n  plt.fill_between(recalls, precisions, step=\'post\', alpha=0.2, color=\'b\')\n  plt.xlabel(\'Recall\')\n  plt.ylabel(\'Precision\')\n  plt.xlim([0.0, 1.05])\n  plt.ylim([0.0, 1.05])\n  plt.title(title)\n  plt.savefig(out_image)\n  plt.clf()\n\ndef make_precision_recall_eval(scores, na_probs, num_true_pos, qid_to_has_ans,\n                               out_image=None, title=None):\n  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n  true_pos = 0.0\n  cur_p = 1.0\n  cur_r = 0.0\n  precisions = [1.0]\n  recalls = [0.0]\n  avg_prec = 0.0\n  for i, qid in enumerate(qid_list):\n    if qid_to_has_ans[qid]:\n      true_pos += scores[qid]\n    cur_p = true_pos / float(i+1)\n    cur_r = true_pos / float(num_true_pos)\n    if i == len(qid_list) - 1 or na_probs[qid] != na_probs[qid_list[i+1]]:\n      # i.e., if we can put a threshold after this point\n      avg_prec += cur_p * (cur_r - recalls[-1])\n      precisions.append(cur_p)\n      recalls.append(cur_r)\n  if out_image:\n    plot_pr_curve(precisions, recalls, out_image, title)\n  return {\'ap\': 100.0 * avg_prec}\n\ndef run_precision_recall_analysis(main_eval, exact_raw, f1_raw, na_probs,\n                                  qid_to_has_ans, out_image_dir):\n  if out_image_dir and not os.path.exists(out_image_dir):\n    os.makedirs(out_image_dir)\n  num_true_pos = sum(1 for v in qid_to_has_ans.values() if v)\n  if num_true_pos == 0:\n    return\n  pr_exact = make_precision_recall_eval(\n      exact_raw, na_probs, num_true_pos, qid_to_has_ans,\n      out_image=os.path.join(out_image_dir, \'pr_exact.png\'),\n      title=\'Precision-Recall curve for Exact Match score\')\n  pr_f1 = make_precision_recall_eval(\n      f1_raw, na_probs, num_true_pos, qid_to_has_ans,\n      out_image=os.path.join(out_image_dir, \'pr_f1.png\'),\n      title=\'Precision-Recall curve for F1 score\')\n  oracle_scores = {k: float(v) for k, v in qid_to_has_ans.items()}\n  pr_oracle = make_precision_recall_eval(\n      oracle_scores, na_probs, num_true_pos, qid_to_has_ans,\n      out_image=os.path.join(out_image_dir, \'pr_oracle.png\'),\n      title=\'Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)\')\n  merge_eval(main_eval, pr_exact, \'pr_exact\')\n  merge_eval(main_eval, pr_f1, \'pr_f1\')\n  merge_eval(main_eval, pr_oracle, \'pr_oracle\')\n\ndef histogram_na_prob(na_probs, qid_list, image_dir, name):\n  if not qid_list:\n    return\n  x = [na_probs[k] for k in qid_list]\n  weights = np.ones_like(x) / float(len(x))\n  plt.hist(x, weights=weights, bins=20, range=(0.0, 1.0))\n  plt.xlabel(\'Model probability of no-answer\')\n  plt.ylabel(\'Proportion of dataset\')\n  plt.title(\'Histogram of no-answer probability: %s\' % name)\n  plt.savefig(os.path.join(image_dir, \'na_prob_hist_%s.png\' % name))\n  plt.clf()\n\ndef find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n  num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n  cur_score = num_no_ans\n  best_score = cur_score\n  best_thresh = 0.0\n  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n  for i, qid in enumerate(qid_list):\n    if qid not in scores: continue\n    if qid_to_has_ans[qid]:\n      diff = scores[qid]\n    else:\n      if preds[qid]:\n        diff = -1\n      else:\n        diff = 0\n    cur_score += diff\n    if cur_score > best_score:\n      best_score = cur_score\n      best_thresh = na_probs[qid]\n  return 100.0 * best_score / len(scores), best_thresh\n\ndef find_best_thresh_v2(preds, scores, na_probs, qid_to_has_ans):\n  num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n  cur_score = num_no_ans\n  best_score = cur_score\n  best_thresh = 0.0\n  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n  for i, qid in enumerate(qid_list):\n    if qid not in scores: continue\n    if qid_to_has_ans[qid]:\n      diff = scores[qid]\n    else:\n      if preds[qid]:\n        diff = -1\n      else:\n        diff = 0\n    cur_score += diff\n    if cur_score > best_score:\n      best_score = cur_score\n      best_thresh = na_probs[qid]\n\n  has_ans_score, has_ans_cnt = 0, 0\n  for qid in qid_list:\n    if not qid_to_has_ans[qid]: continue\n    has_ans_cnt += 1\n\n    if qid not in scores: continue\n    has_ans_score += scores[qid]\n\n  return 100.0 * best_score / len(scores), best_thresh, 1.0 * has_ans_score / has_ans_cnt\n\ndef find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n  best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n  best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n  main_eval[\'best_exact\'] = best_exact\n  main_eval[\'best_exact_thresh\'] = exact_thresh\n  main_eval[\'best_f1\'] = best_f1\n  main_eval[\'best_f1_thresh\'] = f1_thresh\n\ndef find_all_best_thresh_v2(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n  best_exact, exact_thresh, has_ans_exact = find_best_thresh_v2(preds, exact_raw, na_probs, qid_to_has_ans)\n  best_f1, f1_thresh, has_ans_f1 = find_best_thresh_v2(preds, f1_raw, na_probs, qid_to_has_ans)\n  main_eval[\'best_exact\'] = best_exact\n  main_eval[\'best_exact_thresh\'] = exact_thresh\n  main_eval[\'best_f1\'] = best_f1\n  main_eval[\'best_f1_thresh\'] = f1_thresh\n  main_eval[\'has_ans_exact\'] = has_ans_exact\n  main_eval[\'has_ans_f1\'] = has_ans_f1\n\ndef main(OPTS):\n  with open(OPTS.data_file) as f:\n    dataset_json = json.load(f)\n    dataset = dataset_json[\'data\']\n  with open(OPTS.pred_file) as f:\n    preds = json.load(f)\n  if OPTS.na_prob_file:\n    with open(OPTS.na_prob_file) as f:\n      na_probs = json.load(f)\n  else:\n    na_probs = {k: 0.0 for k in preds}\n  qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n  has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n  no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n  # exact_raw, f1_raw = get_raw_scores(dataset, preds)\n  exact_raw, f1_raw = get_raw_scores_extended(dataset, preds)\n  exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,\n                                        OPTS.na_prob_thresh)\n  f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans,\n                                     OPTS.na_prob_thresh)\n  out_eval = make_eval_dict(exact_thresh, f1_thresh)\n  if has_ans_qids:\n    has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n    merge_eval(out_eval, has_ans_eval, \'HasAns\')\n  if no_ans_qids:\n    no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n    merge_eval(out_eval, no_ans_eval, \'NoAns\')\n  if OPTS.na_prob_file:\n    find_all_best_thresh(out_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans)\n  if OPTS.na_prob_file and OPTS.out_image_dir:\n    run_precision_recall_analysis(out_eval, exact_raw, f1_raw, na_probs,\n                                  qid_to_has_ans, OPTS.out_image_dir)\n    histogram_na_prob(na_probs, has_ans_qids, OPTS.out_image_dir, \'hasAns\')\n    histogram_na_prob(na_probs, no_ans_qids, OPTS.out_image_dir, \'noAns\')\n  if OPTS.out_file:\n    with open(OPTS.out_file, \'w\') as f:\n      json.dump(out_eval, f)\n  else:\n    print(json.dumps(out_eval, indent=2))\n  return out_eval\n\nif __name__ == \'__main__\':\n  OPTS = parse_args()\n  if OPTS.out_image_dir:\n    import matplotlib\n    matplotlib.use(\'Agg\')\n    import matplotlib.pyplot as plt\n  main(OPTS)\n'"
farm/modeling/__init__.py,0,b''
farm/modeling/adaptive_model.py,12,"b'import copy\nimport json\nimport logging\nimport os\nfrom argparse import Namespace\nfrom pathlib import Path\n\nimport multiprocessing\nimport numpy\nimport torch\nfrom torch import nn\nfrom transformers.modeling_auto import AutoModelForQuestionAnswering, AutoModelForSequenceClassification, AutoModelForTokenClassification, AutoModelWithLMHead\n\nfrom farm.conversion.onnx_optimization.bert_model_optimization import main as optimize_onnx_model\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import SquadProcessor\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.prediction_head import PredictionHead, QuestionAnsweringHead, TokenClassificationHead, TextClassificationHead, pick_single_fn\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.utils import MLFlowLogger as MlLogger, stack\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseAdaptiveModel:\n    """"""\n    Base Class for implementing AdaptiveModel with frameworks like PyTorch and ONNX.\n    """"""\n\n    subclasses = {}\n\n    def __init_subclass__(cls, **kwargs):\n        """""" This automatically keeps track of all available subclasses.\n        Enables generic load() for all specific AdaptiveModel implementation.\n        """"""\n        super().__init_subclass__(**kwargs)\n        cls.subclasses[cls.__name__] = cls\n\n    def __init__(self, prediction_heads):\n        self.prediction_heads = prediction_heads\n\n    @classmethod\n    def load(cls, **kwargs):\n        """"""\n        Load corresponding AdaptiveModel Class(AdaptiveModel/ONNXAdaptiveModel) based on the\n        files in the load_dir.\n\n        :param kwargs: arguments to pass for loading the model.\n        :return: instance of a model\n        """"""\n        if (Path(kwargs[""load_dir""]) / ""model.onnx"").is_file():\n            model = cls.subclasses[""ONNXAdaptiveModel""].load(**kwargs)\n        else:\n            model = cls.subclasses[""AdaptiveModel""].load(**kwargs)\n        return model\n\n    def logits_to_preds(self, logits, **kwargs):\n        """"""\n        Get predictions from all prediction heads.\n\n        :param logits: logits, can vary in shape and type, depending on task\n        :type logits: object\n        :param label_maps: Maps from label encoding to label string\n        :param label_maps: dict\n        :return: A list of all predictions from all prediction heads\n        """"""\n        all_preds = []\n        # collect preds from all heads\n        for head, logits_for_head in zip(self.prediction_heads, logits):\n            preds = head.logits_to_preds(logits=logits_for_head, **kwargs)\n            all_preds.append(preds)\n        return all_preds\n\n    def formatted_preds(self, logits, **kwargs):\n        """"""\n        Format predictions for inference.\n\n        :param logits: model logits\n        :type logits: torch.tensor\n        :param kwargs: placeholder for passing generic parameters\n        :type kwargs: object\n        :return: predictions in the right format\n        """"""\n        n_heads = len(self.prediction_heads)\n\n        if n_heads == 0:\n            # just return LM output (e.g. useful for extracting embeddings at inference time)\n            preds_final = self.language_model.formatted_preds(logits=logits, **kwargs)\n\n        elif n_heads == 1:\n            preds_final = []\n            # TODO This is very specific to QA, make more general\n            try:\n                preds_p = kwargs[""preds_p""]\n                temp = [y[0] for y in preds_p]\n                preds_p_flat = [item for sublist in temp for item in sublist]\n                kwargs[""preds_p""] = preds_p_flat\n            except KeyError:\n                kwargs[""preds_p""] = None\n            head = self.prediction_heads[0]\n            logits_for_head = logits[0]\n            preds = head.formatted_preds(logits=logits_for_head, **kwargs)\n            # TODO This is very messy - we need better definition of what the output should look like\n            if type(preds) == list:\n                preds_final += preds\n            elif type(preds) == dict and ""predictions"" in preds:\n                preds_final.append(preds)\n\n        # This case is triggered by Natural Questions\n        else:\n            preds_final = [list() for _ in range(n_heads)]\n            preds = kwargs[""preds_p""]\n            preds_for_heads = stack(preds)\n            logits_for_heads = [None] * n_heads\n\n            samples = [s for b in kwargs[""baskets""] for s in b.samples]\n            kwargs[""samples""] = samples\n\n            del kwargs[""preds_p""]\n\n            for i, (head, preds_p_for_head, logits_for_head) in enumerate(zip(self.prediction_heads, preds_for_heads, logits_for_heads)):\n                preds = head.formatted_preds(logits=logits_for_head, preds_p=preds_p_for_head, **kwargs)\n                preds_final[i].append(preds)\n\n            # Look for a merge() function amongst the heads and if a single one exists, apply it to preds_final\n            merge_fn = pick_single_fn(self.prediction_heads, ""merge_formatted_preds"")\n            if merge_fn:\n                preds_final = merge_fn(preds_final)\n\n        return preds_final\n\n    def connect_heads_with_processor(self, tasks, require_labels=True):\n        """"""\n        Populates prediction head with information coming from tasks.\n\n        :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)\n        :param require_labels: If True, an error will be thrown when a task is not supplied with labels)\n        :return:\n        """"""\n\n        # Drop the next sentence prediction head if it does not appear in tasks. This is triggered by the interaction\n        # setting the argument BertStyleLMProcessor(next_sent_pred=False)\n        if ""nextsentence"" not in tasks:\n            idx = None\n            for i, ph in enumerate(self.prediction_heads):\n                if ph.task_name == ""nextsentence"":\n                    idx = i\n            if idx is not None:\n                logger.info(\n                ""Removing the NextSentenceHead since next_sent_pred is set to False in the BertStyleLMProcessor"")\n                del self.prediction_heads[i]\n\n        for head in self.prediction_heads:\n            head.label_tensor_name = tasks[head.task_name][""label_tensor_name""]\n            label_list = tasks[head.task_name][""label_list""]\n            if not label_list and require_labels:\n                raise Exception(f""The task \\\'{head.task_name}\\\' is missing a valid set of labels"")\n            label_list = tasks[head.task_name][""label_list""]\n            head.label_list = label_list\n            if ""RegressionHead"" in str(type(head)):\n                # This needs to be explicitly set because the regression label_list is being hijacked to store\n                # the scaling factor and the mean\n                num_labels = 1\n            else:\n                num_labels = len(label_list)\n            head.metric = tasks[head.task_name][""metric""]\n\n    @classmethod\n    def _get_prediction_head_files(cls, load_dir, strict=True):\n        load_dir = Path(load_dir)\n        files = os.listdir(load_dir)\n        model_files = [\n            load_dir / f\n            for f in files\n            if "".bin"" in f and ""prediction_head"" in f\n        ]\n        config_files = [\n            load_dir / f\n            for f in files\n            if ""config.json"" in f and ""prediction_head"" in f\n        ]\n        # sort them to get correct order in case of multiple prediction heads\n        model_files.sort()\n        config_files.sort()\n\n        if strict:\n            error_str = (\n                f""There is a mismatch in number of model files ({len(model_files)}) and config files ({len(config_files)}).""\n                ""This might be because the Language Model Prediction Head ""\n                ""does not currently support saving and loading""\n            )\n            assert len(model_files) == len(config_files), error_str\n        logger.info(f""Found files for loading {len(model_files)} prediction heads"")\n\n        return model_files, config_files\n\ndef loss_per_head_sum(loss_per_head, global_step=None, batch=None):\n    """"""\n    Input: loss_per_head (list of tensors), global_step (int), batch (dict)\n    Output: aggregated loss (tensor)\n    """"""\n    return sum(loss_per_head)\n\nclass AdaptiveModel(nn.Module, BaseAdaptiveModel):\n    """""" PyTorch implementation containing all the modelling needed for your NLP task. Combines a language\n    model and a prediction head. Allows for gradient flow back to the language model component.""""""\n\n    def __init__(\n        self,\n        language_model,\n        prediction_heads,\n        embeds_dropout_prob,\n        lm_output_types,\n        device,\n        loss_aggregation_fn=None,\n    ):\n        """"""\n        :param language_model: Any model that turns token ids into vector representations\n        :type language_model: LanguageModel\n        :param prediction_heads: A list of models that take embeddings and return logits for a given task\n        :type prediction_heads: list\n        :param embeds_dropout_prob: The probability that a value in the embeddings returned by the\n           language model will be zeroed.\n        :param embeds_dropout_prob: float\n        :param lm_output_types: How to extract the embeddings from the final layer of the language model. When set\n                                to ""per_token"", one embedding will be extracted per input token. If set to\n                                ""per_sequence"", a single embedding will be extracted to represent the full\n                                input sequence. Can either be a single string, or a list of strings,\n                                one for each prediction head.\n        :type lm_output_types: list or str\n        :param device: The device on which this model will operate. Either ""cpu"" or ""cuda"".\n        :param loss_aggregation_fn: Function to aggregate the loss of multiple prediction heads.\n                                    Input: loss_per_head (list of tensors), global_step (int), batch (dict)\n                                    Output: aggregated loss (tensor)\n                                    Default is a simple sum:\n                                    `lambda loss_per_head, global_step=None, batch=None: sum(tensors)`\n                                    However, you can pass more complex functions that depend on the\n                                    current step (e.g. for round-robin style multitask learning) or the actual\n                                    content of the batch (e.g. certain labels)\n                                    Note: The loss at this stage is per sample, i.e one tensor of\n                                    shape (batchsize) per prediction head.\n        :type loss_aggregation_fn: function\n        """"""\n\n        super(AdaptiveModel, self).__init__()\n        self.device = device\n        self.language_model = language_model.to(device)\n        self.lm_output_dims = language_model.get_output_dims()\n        self.prediction_heads = nn.ModuleList([ph.to(device) for ph in prediction_heads])\n        self.fit_heads_to_lm()\n        # set shared weights for LM finetuning\n        for head in self.prediction_heads:\n            if head.model_type == ""language_modelling"":\n                head.set_shared_weights(language_model.model.embeddings.word_embeddings.weight)\n        self.dropout = nn.Dropout(embeds_dropout_prob)\n        self.lm_output_types = (\n            [lm_output_types] if isinstance(lm_output_types, str) else lm_output_types\n        )\n        assert len(self.lm_output_types) == len(self.prediction_heads)\n        self.log_params()\n        # default loss aggregation function is a simple sum (without using any of the optional params)\n        if not loss_aggregation_fn:\n            loss_aggregation_fn = loss_per_head_sum\n        self.loss_aggregation_fn = loss_aggregation_fn\n\n    def fit_heads_to_lm(self):\n        """"""This iterates over each prediction head and ensures that its input dimensionality matches the output\n        dimensionality of the language model. If it doesn\'t, it is resized so it does fit""""""\n        for ph in self.prediction_heads:\n            ph.resize_input(self.lm_output_dims)\n            ph.to(self.device)\n\n    def save(self, save_dir):\n        """"""\n        Saves the language model and prediction heads. This will generate a config file\n        and model weights for each.\n\n        :param save_dir: path to save to\n        :type save_dir: Path\n        """"""\n        os.makedirs(save_dir, exist_ok=True)\n        self.language_model.save(save_dir)\n        for i, ph in enumerate(self.prediction_heads):\n            ph.save(save_dir, i)\n            # Need to save config and pipeline\n\n    @classmethod\n    def load(cls, load_dir, device, strict=True, lm_name=None, processor=None):\n        """"""\n        Loads an AdaptiveModel from a directory. The directory must contain:\n\n        * language_model.bin\n        * language_model_config.json\n        * prediction_head_X.bin  multiple PH possible\n        * prediction_head_X_config.json\n        * processor_config.json config for transforming input\n        * vocab.txt vocab file for language model, turning text to Wordpiece Tokens\n\n        :param load_dir: location where adaptive model is stored\n        :type load_dir: Path\n        :param device: to which device we want to sent the model, either cpu or cuda\n        :type device: torch.device\n        :param lm_name: the name to assign to the loaded language model\n        :type lm_name: str\n        :param strict: whether to strictly enforce that the keys loaded from saved model match the ones in\n                       the PredictionHead (see torch.nn.module.load_state_dict()).\n                       Set to `False` for backwards compatibility with PHs saved with older version of FARM.\n        :type strict: bool\n        :param processor: populates prediction head with information coming from tasks\n        :type processor: Processor\n        """"""\n\n        # Language Model\n        if lm_name:\n            language_model = LanguageModel.load(load_dir, farm_lm_name=lm_name)\n        else:\n            language_model = LanguageModel.load(load_dir)\n\n        # Prediction heads\n        _, ph_config_files = cls._get_prediction_head_files(load_dir)\n        prediction_heads = []\n        ph_output_type = []\n        for config_file in ph_config_files:\n            head = PredictionHead.load(config_file, strict=strict)\n            prediction_heads.append(head)\n            ph_output_type.append(head.ph_output_type)\n\n        model = cls(language_model, prediction_heads, 0.1, ph_output_type, device)\n        if processor:\n            model.connect_heads_with_processor(processor.tasks)\n\n        return model\n\n    def logits_to_loss_per_head(self, logits, **kwargs):\n        """"""\n        Collect losses from each prediction head.\n\n        :param logits: logits, can vary in shape and type, depending on task.\n        :type logits: object\n        :return: The per sample per prediciton head loss whose first two dimensions have length n_pred_heads, batch_size\n        """"""\n        all_losses = []\n        for head, logits_for_one_head in zip(self.prediction_heads, logits):\n            # check if PredictionHead connected to Processor\n            assert hasattr(head, ""label_tensor_name""), \\\n                (f""Label_tensor_names are missing inside the {head.task_name} Prediction Head. Did you connect the model""\n                "" with the processor through either \'model.connect_heads_with_processor(processor.tasks)\'""\n                "" or by passing the processor to the Adaptive Model?"")\n            all_losses.append(head.logits_to_loss(logits=logits_for_one_head, **kwargs))\n        return all_losses\n\n    def logits_to_loss(self, logits, global_step=None, **kwargs):\n        """"""\n        Get losses from all prediction heads & reduce to single loss *per sample*.\n\n        :param logits: logits, can vary in shape and type, depending on task\n        :type logits: object\n        :param global_step: number of current training step\n        :type global_step: int\n        :param kwargs: placeholder for passing generic parameters.\n                       Note: Contains the batch (as dict of tensors), when called from Trainer.train().\n        :type kwargs: object\n        :return loss: torch.tensor that is the per sample loss (len: batch_size)\n        """"""\n        all_losses = self.logits_to_loss_per_head(logits, **kwargs)\n        # This aggregates the loss per sample across multiple prediction heads\n        # Default is sum(), but you can configure any fn that takes [Tensor, Tensor ...] and returns [Tensor]\n        loss = self.loss_aggregation_fn(all_losses, global_step=global_step, batch=kwargs)\n        return loss\n\n    def prepare_labels(self, **kwargs):\n        """"""\n        Label conversion to original label space, per prediction head.\n\n        :param label_maps: dictionary for mapping ids to label strings\n        :type label_maps: dict[int:str]\n        :return: labels in the right format\n        """"""\n        all_labels = []\n        # for head, label_map_one_head in zip(self.prediction_heads):\n        #     labels = head.prepare_labels(label_map=label_map_one_head, **kwargs)\n        #     all_labels.append(labels)\n        for head in self.prediction_heads:\n            labels = head.prepare_labels(**kwargs)\n            all_labels.append(labels)\n        return all_labels\n\n    def forward(self, **kwargs):\n        """"""\n        Push data through the whole model and returns logits. The data will propagate through the language\n        model and each of the attached prediction heads.\n\n        :param kwargs: Holds all arguments that need to be passed to the language model and prediction head(s).\n        :return: all logits as torch.tensor or multiple tensors.\n        """"""\n\n        # Run forward pass of language model\n        sequence_output, pooled_output = self.forward_lm(**kwargs)\n\n        # Run forward pass of (multiple) prediction heads using the output from above\n        all_logits = []\n        if len(self.prediction_heads) > 0:\n            for head, lm_out in zip(self.prediction_heads, self.lm_output_types):\n                # Choose relevant vectors from LM as output and perform dropout\n                if lm_out == ""per_token"":\n                    output = self.dropout(sequence_output)\n                elif lm_out == ""per_sequence"" or lm_out == ""per_sequence_continuous"":\n                    output = self.dropout(pooled_output)\n                elif (\n                    lm_out == ""per_token_squad""\n                ):  # we need a per_token_squad because of variable metric computation later on...\n                    output = self.dropout(sequence_output)\n                else:\n                    raise ValueError(\n                        ""Unknown extraction strategy from language model: {}"".format(lm_out)\n                    )\n\n                # Do the actual forward pass of a single head\n                all_logits.append(head(output))\n        else:\n            # just return LM output (e.g. useful for extracting embeddings at inference time)\n            all_logits.append((sequence_output, pooled_output))\n\n        return all_logits\n\n    def forward_lm(self, **kwargs):\n        """"""\n        Forward pass for the language model\n\n        :param kwargs:\n        :return:\n        """"""\n\n        # Check if we have to extract from a special layer of the LM (default = last layer)\n        try:\n            extraction_layer = self.language_model.extraction_layer\n        except:\n            extraction_layer = -1\n\n        # Run forward pass of language model\n        if extraction_layer == -1:\n            sequence_output, pooled_output = self.language_model(**kwargs, output_all_encoded_layers=False)\n        else:\n            # get output from an earlier layer\n            self.language_model.enable_hidden_states_output()\n            sequence_output, pooled_output, all_hidden_states = self.language_model(**kwargs)\n            sequence_output = all_hidden_states[extraction_layer]\n            pooled_output = None #not available in earlier layers\n            self.language_model.disable_hidden_states_output()\n        return sequence_output, pooled_output\n\n    def log_params(self):\n        """"""\n        Logs paramteres to generic logger MlLogger\n        """"""\n        params = {\n            ""lm_type"": self.language_model.__class__.__name__,\n            ""lm_name"": self.language_model.name,\n            ""prediction_heads"": "","".join(\n                [head.__class__.__name__ for head in self.prediction_heads]\n            ),\n            ""lm_output_types"": "","".join(self.lm_output_types),\n        }\n        try:\n            MlLogger.log_params(params)\n        except Exception as e:\n            logger.warning(f""ML logging didn\'t work: {e}"")\n\n    def verify_vocab_size(self, vocab_size):\n        """""" Verifies that the model fits to the tokenizer vocabulary.\n        They could diverge in case of custom vocabulary added via tokenizer.add_tokens()""""""\n\n        model_vocab_len = self.language_model.model.resize_token_embeddings(new_num_tokens=None).num_embeddings\n\n        msg = f""Vocab size of tokenizer {vocab_size} doesn\'t match with model {model_vocab_len}. "" \\\n              ""If you added a custom vocabulary to the tokenizer, "" \\\n              ""make sure to supply \'n_added_tokens\' to LanguageModel.load() and BertStyleLM.load()""\n        assert vocab_size == model_vocab_len, msg\n\n        for head in self.prediction_heads:\n            if head.model_type == ""language_modelling"":\n                ph_decoder_len = head.decoder.weight.shape[0]\n                assert vocab_size == ph_decoder_len, msg\n\n    def get_language(self):\n        return self.language_model.language\n\n    def convert_to_transformers(self):\n        if len(self.prediction_heads) != 1:\n            raise ValueError(f""Currently conversion only works for models with a SINGLE prediction head. ""\n                             f""Your model has {len(self.prediction_heads)}"")\n        elif len(self.prediction_heads[0].layer_dims) != 2:\n            raise ValueError(f""Currently conversion only works for PredictionHeads that are a single layer Feed Forward NN with dimensions [LM_output_dim, number_classes].\\n""\n                             f""            Your PredictionHead has {str(self.prediction_heads[0].layer_dims)} dimensions."")\n        #TODO add more infos to config\n\n        if self.prediction_heads[0].model_type == ""span_classification"":\n            # init model\n            transformers_model = AutoModelForQuestionAnswering.from_config(self.language_model.model.config)\n            # transfer weights for language model + prediction head\n            setattr(transformers_model, transformers_model.base_model_prefix, self.language_model.model)\n            transformers_model.qa_outputs.load_state_dict(\n                self.prediction_heads[0].feed_forward.feed_forward[0].state_dict())\n\n        elif self.prediction_heads[0].model_type == ""language_modelling"":\n            # init model\n            transformers_model = AutoModelWithLMHead.from_config(self.language_model.model.config)\n            # transfer weights for language model + prediction head\n            setattr(transformers_model, transformers_model.base_model_prefix, self.language_model.model)\n            ph_state_dict = self.prediction_heads[0].state_dict()\n            ph_state_dict[""transform.dense.weight""] = ph_state_dict.pop(""dense.weight"")\n            ph_state_dict[""transform.dense.bias""] = ph_state_dict.pop(""dense.bias"")\n            ph_state_dict[""transform.LayerNorm.weight""] = ph_state_dict.pop(""LayerNorm.weight"")\n            ph_state_dict[""transform.LayerNorm.bias""] = ph_state_dict.pop(""LayerNorm.bias"")\n            transformers_model.cls.predictions.load_state_dict(ph_state_dict)\n            logger.warning(""Currently only the Masked Language Modeling component of the prediction head is converted, ""\n                           ""not the Next Sentence Prediction or Sentence Order Prediction components"")\n\n        elif self.prediction_heads[0].model_type == ""text_classification"":\n            if self.language_model.model.base_model_prefix == ""roberta"":\n                # Classification Heads in transformers have different architecture across Language Model variants\n                # The RobertaClassificationhead has components: input2dense, dropout, tanh, dense2output\n                # The tanh function cannot be mapped to current FARM style linear Feed Forward ClassificationHeads.\n                # So conversion for this type cannot work. We would need a compatible FARM RobertaClassificationHead\n                logger.error(""Conversion for Text Classification with Roberta or XLMRoberta not possible at the moment."")\n                raise NotImplementedError\n\n            # add more info to config\n            self.language_model.model.config.id2label = {id: label for id, label in enumerate(self.prediction_heads[0].label_list)}\n            self.language_model.model.config.label2id = {label: id for id, label in enumerate(self.prediction_heads[0].label_list)}\n            self.language_model.model.config.finetuning_task = ""text_classification""\n            self.language_model.model.config.language = self.language_model.language\n            self.language_model.model.config.num_labels = self.prediction_heads[0].num_labels\n\n            # init model\n            transformers_model = AutoModelForSequenceClassification.from_config(self.language_model.model.config)\n            # transfer weights for language model + prediction head\n            setattr(transformers_model, transformers_model.base_model_prefix, self.language_model.model)\n            transformers_model.classifier.load_state_dict(\n                self.prediction_heads[0].feed_forward.feed_forward[0].state_dict())\n        elif self.prediction_heads[0].model_type == ""token_classification"":\n            # add more info to config\n            self.language_model.model.config.id2label = {id: label for id, label in enumerate(self.prediction_heads[0].label_list)}\n            self.language_model.model.config.label2id = {label: id for id, label in enumerate(self.prediction_heads[0].label_list)}\n            self.language_model.model.config.finetuning_task = ""token_classification""\n            self.language_model.model.config.language = self.language_model.language\n            self.language_model.model.config.num_labels = self.prediction_heads[0].num_labels\n\n            # init model\n            transformers_model = AutoModelForTokenClassification.from_config(self.language_model.model.config)\n            # transfer weights for language model + prediction head\n            setattr(transformers_model, transformers_model.base_model_prefix, self.language_model.model)\n            transformers_model.classifier.load_state_dict(\n                self.prediction_heads[0].feed_forward.feed_forward[0].state_dict())\n        else:\n            raise NotImplementedError(f""FARM -> Transformers conversion is not supported yet for""\n                                      f"" prediction heads of type {self.prediction_heads[0].model_type}"")\n        pass\n\n        return transformers_model\n\n    @classmethod\n    def convert_from_transformers(cls, model_name_or_path, device, task_type, processor=None):\n        """"""\n        Load a (downstream) model from huggingface\'s transformers format. Use cases:\n         - continue training in FARM (e.g. take a squad QA model and fine-tune on your own data)\n         - compare models without switching frameworks\n         - use model directly for inference\n\n        :param model_name_or_path: local path of a saved model or name of a public one.\n                                              Exemplary public names:\n                                              - distilbert-base-uncased-distilled-squad\n                                              - deepset/bert-large-uncased-whole-word-masking-squad2\n\n                                              See https://huggingface.co/models for full list\n        :param device: ""cpu"" or ""cuda""\n        :param task_type: One of :\n                          - \'question_answering\'\n                          - \'text_classification\'\n                          - \'embeddings\'\n                          More tasks coming soon ...\n        :param processor: populates prediction head with information coming from tasks\n        :type processor: Processor\n        :return: AdaptiveModel\n        """"""\n        lm = LanguageModel.load(model_name_or_path)\n        #TODO Infer type of head automatically from config\n\n        if task_type == ""question_answering"":\n            ph = QuestionAnsweringHead.load(model_name_or_path)\n            adaptive_model = cls(language_model=lm, prediction_heads=[ph], embeds_dropout_prob=0.1,\n                               lm_output_types=""per_token"", device=device)\n        elif task_type == ""text_classification"":\n            if ""roberta"" in model_name_or_path:\n                # The RobertaClassificationhead has components: input2dense, dropout, tanh, dense2output\n                # The tanh function cannot be mapped to current FARM style linear Feed Forward PredictionHeads.\n                logger.error(""Conversion for Text Classification with Roberta or XLMRoberta not possible at the moment."")\n                raise NotImplementedError\n            ph = TextClassificationHead.load(model_name_or_path)\n            adaptive_model = cls(language_model=lm, prediction_heads=[ph], embeds_dropout_prob=0.1,\n                                 lm_output_types=""per_sequence"", device=device)\n        elif task_type == ""ner"":\n            ph = TokenClassificationHead.load(model_name_or_path)\n            adaptive_model = cls(language_model=lm, prediction_heads=[ph], embeds_dropout_prob=0.1,\n                               lm_output_types=""per_token"", device=device)\n        elif task_type == ""embeddings"":\n            adaptive_model = cls(language_model=lm, prediction_heads=[], embeds_dropout_prob=0.1,\n                                 lm_output_types=[""per_token"", ""per_sequence""], device=device)\n        else:\n            raise NotImplementedError(f""Huggingface\'s transformer models of type {task_type} are not supported yet"")\n\n        if processor:\n            adaptive_model.connect_heads_with_processor(processor.tasks)\n\n        return adaptive_model\n\n    def convert_to_onnx(self, output_path, opset_version=11, optimize_for=None):\n        """"""\n        Convert a PyTorch AdaptiveModel to ONNX.\n\n        The conversion is trace-based by performing a forward pass on the model with a input batch.\n\n        :param output_path: model dir to write the model and config files\n        :type output_path: Path\n        :param opset_version: ONNX opset version\n        :type opset_version: int\n        :param optimize_for: optimize the exported model for a target device. Available options\n                             are ""gpu_tensor_core"" (GPUs with tensor core like V100 or T4),\n                             ""gpu_without_tensor_core"" (most other GPUs), and ""cpu"".\n        :type optimize_for: str\n        :return:\n        """"""\n        if type(self.prediction_heads[0]) is not QuestionAnsweringHead:\n            raise NotImplementedError\n\n        tokenizer = Tokenizer.load(\n            pretrained_model_name_or_path=""deepset/bert-base-cased-squad2""\n        )\n\n        label_list = [""start_token"", ""end_token""]\n        metric = ""squad""\n        max_seq_len = 384\n        batch_size = 1\n        processor = SquadProcessor(\n            tokenizer=tokenizer,\n            max_seq_len=max_seq_len,\n            label_list=label_list,\n            metric=metric,\n            train_filename=""stub-file"",  # the data is loaded from dicts instead of file.\n            dev_filename=None,\n            test_filename=None,\n            data_dir=""stub-dir"",\n        )\n\n        data_silo = DataSilo(processor=processor, batch_size=1, distributed=False, automatic_loading=False)\n        sample_dict = [\n            {\n                ""context"": \'The Normans were the people who in the 10th and 11th centuries gave their name to Normandy, \'\n                           \'a region in France. They were descended from Norse (""Norman"" comes from ""Norseman"") raiders \'\n                           \'and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear \'\n                           \'fealty to King Charles III of West Francia.\',\n                ""qas"": [\n                    {\n                        ""question"": ""In what country is Normandy located?"",\n                        ""id"": ""56ddde6b9a695914005b9628"",\n                        ""answers"": [{""text"": ""France"", ""answer_start"": 159}],\n                        ""is_impossible"": False,\n                    }\n                ],\n            }\n        ]\n\n        data_silo._load_data(train_dicts=sample_dict)\n        data_loader = data_silo.get_data_loader(""train"")\n        data = next(iter(data_loader))\n        data = list(data.values())\n\n        inputs = {\n            \'input_ids\': data[0].to(self.device).reshape(batch_size, max_seq_len),\n            \'padding_mask\': data[1].to(self.device).reshape(batch_size, max_seq_len),\n            \'segment_ids\': data[2].to(self.device).reshape(batch_size, max_seq_len)\n        }\n\n        # The method argument passing in torch.onnx.export is different to AdaptiveModel\'s forward().\n        # To resolve that, an ONNXWrapper instance is used.\n        model = ONNXWrapper.load_from_adaptive_model(self)\n\n        if not os.path.exists(output_path):\n            os.makedirs(output_path)\n\n        with torch.no_grad():\n            symbolic_names = {0: \'batch_size\', 1: \'max_seq_len\'}\n            torch.onnx.export(model,\n                              args=tuple(inputs.values()),\n                              f=output_path / \'model.onnx\'.format(opset_version),\n                              opset_version=opset_version,\n                              do_constant_folding=True,\n                              input_names=[\'input_ids\',\n                                           \'padding_mask\',\n                                           \'segment_ids\'],\n                              output_names=[\'logits\'],\n                              dynamic_axes={\'input_ids\': symbolic_names,\n                                            \'padding_mask\': symbolic_names,\n                                            \'segment_ids\': symbolic_names,\n                                            \'logits\': symbolic_names,\n                                            })\n\n        if optimize_for:\n            optimize_args = Namespace(\n                disable_attention=False, disable_bias_gelu=False, disable_embed_layer_norm=False, opt_level=99,\n                disable_skip_layer_norm=False, disable_bias_skip_layer_norm=False, hidden_size=768, verbose=False,\n                input=\'onnx-export/model.onnx\', model_type=\'bert\', num_heads=12, output=\'onnx-export/model.onnx\'\n            )\n\n            if optimize_for == ""gpu_tensor_core"":\n                optimize_args.float16 = True\n                optimize_args.input_int32 = True\n            elif optimize_for == ""gpu_without_tensor_core"":\n                optimize_args.float16 = False\n                optimize_args.input_int32 = True\n            elif optimize_for == ""cpu"":\n                logger.info("""")\n                optimize_args.float16 = False\n                optimize_args.input_int32 = False\n            else:\n                raise NotImplementedError(f""ONNXRuntime model optimization is not available for {optimize_for}. Choose ""\n                                          f""one of \'gpu_tensor_core\'(V100 or T4), \'gpu_without_tensor_core\' or \'cpu\'."")\n\n            optimize_onnx_model(optimize_args)\n        else:\n            logger.info(""Exporting unoptimized ONNX model. To enable optimization, supply ""\n                        ""\'optimize_for\' parameter with the target device.\'"")\n\n        # PredictionHead contains functionalities like logits_to_preds() that would still be needed\n        # for Inference with ONNX models. Only the config of the PredictionHead is stored.\n        for i, ph in enumerate(self.prediction_heads):\n            ph.save_config(output_path, i)\n\n        processor.save(output_path)\n\n        onnx_model_config = {\n            ""onnx_opset_version"": opset_version,\n            ""language"": self.get_language(),\n        }\n        with open(output_path / ""model_config.json"", ""w"") as f:\n            json.dump(onnx_model_config, f)\n\n        logger.info(f""Model exported at path {output_path}"")\n\n\nclass ONNXAdaptiveModel(BaseAdaptiveModel):\n    """"""\n    Implementation of ONNX Runtime for Inference of ONNX Models.\n\n    Existing PyTorch based FARM AdaptiveModel can be converted to ONNX format using AdaptiveModel.convert_to_onnx().\n    The conversion is currently only implemented for Question Answering Models.\n\n    For inference, this class is compatible with the FARM Inferencer.\n    """"""\n    def __init__(self, onnx_session, prediction_heads, language, device):\n        if str(device) == ""cuda"" and onnxruntime.get_device() != ""GPU"":\n            raise Exception(f""Device {device} not available for Inference. For CPU, run pip install onnxruntime and""\n                            f""for GPU run pip install onnxruntime-gpu"")\n        self.onnx_session = onnx_session\n        self.prediction_heads = prediction_heads\n        self.device = device\n        self.language = language\n\n    @classmethod\n    def load(cls, load_dir, device, **kwargs):\n        import onnxruntime\n        sess_options = onnxruntime.SessionOptions()\n        # Set graph optimization level to ORT_ENABLE_EXTENDED to enable bert optimization.\n        sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n        # Use OpenMP optimizations. Only useful for CPU, has little impact for GPUs.\n        sess_options.intra_op_num_threads = multiprocessing.cpu_count()\n        onnx_session = onnxruntime.InferenceSession(str(load_dir / ""model.onnx""), sess_options)\n\n        # Prediction heads\n        _, ph_config_files = cls._get_prediction_head_files(load_dir, strict=False)\n        prediction_heads = []\n        ph_output_type = []\n        for config_file in ph_config_files:\n            # ONNX Model doesn\'t need have a separate neural network for PredictionHead. It only uses the\n            # instance methods of PredictionHead class, so, we load with the load_weights param as False.\n            head = PredictionHead.load(config_file, load_weights=False)\n            prediction_heads.append(head)\n            ph_output_type.append(head.ph_output_type)\n\n        with open(load_dir/""model_config.json"") as f:\n            model_config = json.load(f)\n            language = model_config[""language""]\n\n        return cls(onnx_session, prediction_heads, language, device)\n\n    def forward(self, **kwargs):\n        """"""\n        Perform forward pass on the model and return the logits.\n\n        :param kwargs: all arguments that needs to be passed on to the model\n        :return: all logits as torch.tensor or multiple tensors.\n        """"""\n        with torch.no_grad():\n            input_to_onnx = {\n                \'input_ids\': numpy.ascontiguousarray(kwargs[\'input_ids\'].cpu().numpy()),\n                \'padding_mask\': numpy.ascontiguousarray(kwargs[\'padding_mask\'].cpu().numpy()),\n                \'segment_ids\': numpy.ascontiguousarray(kwargs[\'segment_ids\'].cpu().numpy()),\n            }\n            res = self.onnx_session.run(None, input_to_onnx)\n            logits = [torch.from_numpy(res[0]).to(self.device)]\n\n        return logits\n\n    def eval(self):\n        """"""\n        Stub to make ONNXAdaptiveModel compatible with the PyTorch AdaptiveModel.\n        """"""\n        return True\n\n    def get_language(self):\n        """"""\n        Get the language(s) the model was trained for.\n        :return: str\n        """"""\n        return self.language\n\n\nclass ONNXWrapper(AdaptiveModel):\n    """"""\n    Wrapper Class for converting PyTorch models to ONNX.\n\n    As of torch v1.4.0, torch.onnx.export only support passing positional arguments to the forward pass of the model.\n    However, the AdaptiveModel\'s forward takes keyword arguments. This class circumvents the issue by converting\n    positional arguments to keyword arguments.\n    """"""\n    @classmethod\n    def load_from_adaptive_model(cls, adaptive_model):\n        model = copy.deepcopy(adaptive_model)\n        model.__class__ = ONNXWrapper\n        return model\n\n    def forward(self, *batch):\n        return super().forward(input_ids=batch[0], padding_mask=batch[1], segment_ids=batch[2])\n'"
farm/modeling/language_model.py,20,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors,  The HuggingFace Inc. Team and deepset Team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Acknowledgements: Many of the modeling parts here come from the great transformers repository: https://github.com/huggingface/transformers.\nThanks for the great work! """"""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport os\nimport io\nfrom pathlib import Path\nfrom collections import OrderedDict\n\nfrom dotmap import DotMap\nfrom tqdm import tqdm\nimport copy\nimport numpy as np\nimport torch\nfrom torch import nn\n\nlogger = logging.getLogger(__name__)\n\nfrom transformers.modeling_bert import BertModel, BertConfig\nfrom transformers.modeling_roberta import RobertaModel, RobertaConfig\nfrom transformers.modeling_xlnet import XLNetModel, XLNetConfig\nfrom transformers.modeling_albert import AlbertModel, AlbertConfig\nfrom transformers.modeling_xlm_roberta import XLMRobertaModel, XLMRobertaConfig\nfrom transformers.modeling_distilbert import DistilBertModel, DistilBertConfig\nfrom transformers.modeling_electra import ElectraModel, ElectraConfig\nfrom transformers.modeling_utils import SequenceSummary\nfrom transformers.tokenization_bert import load_vocab\n\nfrom farm.modeling import wordembedding_utils\nfrom farm.modeling.wordembedding_utils import s3e_pooling\n\n\n# These are the names of the attributes in various model configs which refer to the number of dimensions\n# in the output vectors\nOUTPUT_DIM_NAMES = [""dim"", ""hidden_size"", ""d_model""]\n\n\nclass LanguageModel(nn.Module):\n    """"""\n    The parent class for any kind of model that can embed language into a semantic vector space. Practically\n    speaking, these models read in tokenized sentences and return vectors that capture the meaning of sentences\n    or of tokens.\n    """"""\n\n    subclasses = {}\n\n    def __init_subclass__(cls, **kwargs):\n        """""" This automatically keeps track of all available subclasses.\n        Enables generic load() or all specific LanguageModel implementation.\n        """"""\n        super().__init_subclass__(**kwargs)\n        cls.subclasses[cls.__name__] = cls\n\n    def forward(self, input_ids, padding_mask, **kwargs):\n        raise NotImplementedError\n\n    @classmethod\n    def from_scratch(cls, model_type, vocab_size):\n        if model_type.lower() == ""bert"":\n            model = Bert\n        return model.from_scratch(vocab_size)\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path, n_added_tokens=0, language_model_class=None, **kwargs):\n        """"""\n        Load a pretrained language model either by\n\n        1. specifying its name and downloading it\n        2. or pointing to the directory it is saved in.\n\n        Available remote models:\n\n        * bert-base-uncased\n        * bert-large-uncased\n        * bert-base-cased\n        * bert-large-cased\n        * bert-base-multilingual-uncased\n        * bert-base-multilingual-cased\n        * bert-base-chinese\n        * bert-base-german-cased\n        * roberta-base\n        * roberta-large\n        * xlnet-base-cased\n        * xlnet-large-cased\n        * xlm-roberta-base\n        * xlm-roberta-large\n        * albert-base-v2\n        * albert-large-v2\n        * distilbert-base-german-cased\n        * distilbert-base-multilingual-cased\n        * google/electra-small-discriminator\n        * google/electra-base-discriminator\n        * google/electra-large-discriminator\n\n        See all supported model variations here: https://huggingface.co/models\n\n        The appropriate language model class is inferred automatically from `pretrained_model_name_or_path`\n        or can be manually supplied via `language_model_class`.\n\n        :param pretrained_model_name_or_path: The path of the saved pretrained model or its name.\n        :type pretrained_model_name_or_path: str\n        :param language_model_class: (Optional) Name of the language model class to load (e.g. `Bert`)\n        :type language_model_class: str\n\n        """"""\n        config_file = Path(pretrained_model_name_or_path) / ""language_model_config.json""\n        if os.path.exists(config_file):\n            # it\'s a local directory in FARM format\n            config = json.load(open(config_file))\n            language_model = cls.subclasses[config[""name""]].load(pretrained_model_name_or_path)\n        else:\n            if language_model_class is None:\n                # it\'s transformers format (either from model hub or local)\n                pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n                if ""xlm"" in pretrained_model_name_or_path and ""roberta"" in pretrained_model_name_or_path:\n                    language_model_class = \'XLMRoberta\'\n                elif \'roberta\' in pretrained_model_name_or_path:\n                    language_model_class = \'Roberta\'\n                elif \'albert\' in pretrained_model_name_or_path:\n                    language_model_class = \'Albert\'\n                elif \'distilbert\' in pretrained_model_name_or_path:\n                    language_model_class = \'DistilBert\'\n                elif \'bert\' in pretrained_model_name_or_path:\n                    language_model_class = \'Bert\'\n                elif \'xlnet\' in pretrained_model_name_or_path:\n                    language_model_class = \'XLNet\'\n                elif \'electra\' in pretrained_model_name_or_path:\n                    language_model_class = \'Electra\'\n                elif ""word2vec"" in pretrained_model_name_or_path.lower() or ""glove"" in pretrained_model_name_or_path.lower():\n                    language_model_class = \'WordEmbedding_LM\'\n\n            if language_model_class:\n                language_model = cls.subclasses[language_model_class].load(pretrained_model_name_or_path, **kwargs)\n            else:\n                language_model = None\n\n        if not language_model:\n            raise Exception(\n                f""Model not found for {pretrained_model_name_or_path}. Either supply the local path for a saved ""\n                f""model or one of bert/roberta/xlnet/albert/distilbert models that can be downloaded from remote. ""\n                f""Ensure that the model class name can be inferred from the directory name when loading a ""\n                f""Transformers\' model. Here\'s a list of available models: ""\n                f""https://farm.deepset.ai/api/modeling.html#farm.modeling.language_model.LanguageModel.load""\n            )\n\n        # resize embeddings in case of custom vocab\n        if n_added_tokens != 0:\n            # TODO verify for other models than BERT\n            model_emb_size = language_model.model.resize_token_embeddings(new_num_tokens=None).num_embeddings\n            vocab_size = model_emb_size + n_added_tokens\n            logger.info(\n                f""Resizing embedding layer of LM from {model_emb_size} to {vocab_size} to cope with custom vocab."")\n            language_model.model.resize_token_embeddings(vocab_size)\n            # verify\n            model_emb_size = language_model.model.resize_token_embeddings(new_num_tokens=None).num_embeddings\n            assert vocab_size == model_emb_size\n\n        return language_model\n\n    def get_output_dims(self):\n        config = self.model.config\n        for odn in OUTPUT_DIM_NAMES:\n            if odn in dir(config):\n                return getattr(config, odn)\n        else:\n            raise Exception(""Could not infer the output dimensions of the language model"")\n\n    def freeze(self, layers):\n        """""" To be implemented""""""\n        raise NotImplementedError()\n\n    def unfreeze(self):\n        """""" To be implemented""""""\n        raise NotImplementedError()\n\n    def save_config(self, save_dir):\n        save_filename = Path(save_dir) / ""language_model_config.json""\n        with open(save_filename, ""w"") as file:\n            setattr(self.model.config, ""name"", self.__class__.__name__)\n            setattr(self.model.config, ""language"", self.language)\n            string = self.model.config.to_json_string()\n            file.write(string)\n\n    def save(self, save_dir):\n        """"""\n        Save the model state_dict and its config file so that it can be loaded again.\n\n        :param save_dir: The directory in which the model should be saved.\n        :type save_dir: str\n        """"""\n        # Save Weights\n        save_name = Path(save_dir) / ""language_model.bin""\n        model_to_save = (\n            self.model.module if hasattr(self.model, ""module"") else self.model\n        )  # Only save the model it-self\n        torch.save(model_to_save.state_dict(), save_name)\n        self.save_config(save_dir)\n\n    @classmethod\n    def _get_or_infer_language_from_name(cls, language, name):\n        if language is not None:\n            return language\n        else:\n            return cls._infer_language_from_name(name)\n\n    @classmethod\n    def _infer_language_from_name(cls, name):\n        known_languages = (\n            ""german"",\n            ""english"",\n            ""chinese"",\n            ""indian"",\n            ""french"",\n            ""polish"",\n            ""spanish"",\n            ""multilingual"",\n        )\n        matches = [lang for lang in known_languages if lang in name]\n        if len(matches) == 0:\n            language = ""english""\n            logger.warning(\n                ""Could not automatically detect from language model name what language it is. \\n""\n                ""\\t We guess it\'s an *ENGLISH* model ... \\n""\n                ""\\t If not: Init the language model by supplying the \'language\' param.""\n            )\n        elif len(matches) > 1:\n            raise ValueError(\n                ""Could not automatically detect from language model name what language it is.\\n""\n                f""\\t Found multiple matches: {matches}\\n""\n                ""\\t Please init the language model by manually supplying the \'language\' as a parameter.\\n""\n            )\n        else:\n            language = matches[0]\n            logger.info(\n                f""Automatically detected language from language model name: {language}""\n            )\n\n        return language\n\n    def formatted_preds(self, logits, samples, ignore_first_token=True,\n                        padding_mask=None, input_ids=None, **kwargs):\n        """"""\n        Extracting vectors from language model (e.g. for extracting sentence embeddings).\n        Different pooling strategies and layers are available and will be determined from the object attributes\n        `extraction_layer` and `extraction_strategy`. Both should be set via the Inferencer:\n        Example:  Inferencer(extraction_strategy=\'cls_token\', extraction_layer=-1)\n\n        :param logits: Tuple of (sequence_output, pooled_output) from the language model.\n                       Sequence_output: one vector per token, pooled_output: one vector for whole sequence\n        :param samples: For each item in logits we need additional meta information to format the prediction (e.g. input text).\n                        This is created by the Processor and passed in here from the Inferencer.\n        :param ignore_first_token: Whether to include the first token for pooling operations (e.g. reduce_mean).\n                                   Many models have here a special token like [CLS] that you don\'t want to include into your average of token embeddings.\n        :param padding_mask: Mask for the padding tokens. Those will also not be included in the pooling operations to prevent a bias by the number of padding tokens.\n        :param input_ids: ids of the tokens in the vocab\n        :param kwargs: kwargs\n        :return: list of dicts containing preds, e.g. [{""context"": ""some text"", ""vec"": [-0.01, 0.5 ...]}]\n        """"""\n\n        if not hasattr(self, ""extraction_layer"") or not hasattr(self, ""extraction_strategy""):\n            raise ValueError(""`extraction_layer` or `extraction_strategy` not specified for LM. ""\n                             ""Make sure to set both, e.g. via Inferencer(extraction_strategy=\'cls_token\', extraction_layer=-1)`"")\n\n        # unpack the tuple from LM forward pass\n        sequence_output = logits[0][0]\n        pooled_output = logits[0][1]\n\n        # aggregate vectors\n        if self.extraction_strategy == ""pooled"":\n            if self.extraction_layer != -1:\n                raise ValueError(f""Pooled output only works for the last layer, but got extraction_layer = {self.extraction_layer}. Please set `extraction_layer=-1`.)"")\n            vecs = pooled_output.cpu().numpy()\n        elif self.extraction_strategy == ""per_token"":\n            vecs = sequence_output.cpu().numpy()\n        elif self.extraction_strategy == ""reduce_mean"":\n            vecs = self._pool_tokens(sequence_output, padding_mask, self.extraction_strategy, ignore_first_token=ignore_first_token)\n        elif self.extraction_strategy == ""reduce_max"":\n            vecs = self._pool_tokens(sequence_output, padding_mask, self.extraction_strategy, ignore_first_token=ignore_first_token)\n        elif self.extraction_strategy == ""cls_token"":\n            vecs = sequence_output[:, 0, :].cpu().numpy()\n        elif self.extraction_strategy == ""s3e"":\n            vecs = self._pool_tokens(sequence_output, padding_mask, self.extraction_strategy,\n                                     ignore_first_token=ignore_first_token,\n                                     input_ids=input_ids, s3e_stats=self.s3e_stats)\n        else:\n            raise NotImplementedError\n\n        preds = []\n        for vec, sample in zip(vecs, samples):\n            pred = {}\n            pred[""context""] = sample.tokenized[""tokens""]\n            pred[""vec""] = vec\n            preds.append(pred)\n        return preds\n\n    def _pool_tokens(self, sequence_output, padding_mask, strategy, ignore_first_token, input_ids=None, s3e_stats=None):\n\n        token_vecs = sequence_output.cpu().numpy()\n        # we only take the aggregated value of non-padding tokens\n        padding_mask = padding_mask.cpu().numpy()\n        ignore_mask_2d = padding_mask == 0\n        # sometimes we want to exclude the CLS token as well from our aggregation operation\n        if ignore_first_token:\n            ignore_mask_2d[:, 0] = True\n        ignore_mask_3d = np.zeros(token_vecs.shape, dtype=bool)\n        ignore_mask_3d[:, :, :] = ignore_mask_2d[:, :, np.newaxis]\n        if strategy == ""reduce_max"":\n            pooled_vecs = np.ma.array(data=token_vecs, mask=ignore_mask_3d).max(axis=1).data\n        if strategy == ""reduce_mean"":\n            pooled_vecs = np.ma.array(data=token_vecs, mask=ignore_mask_3d).mean(axis=1).data\n        if strategy == ""s3e"":\n            input_ids = input_ids.cpu().numpy()\n            pooled_vecs = s3e_pooling(token_embs=token_vecs,\n                                      token_ids=input_ids,\n                                      token_weights=s3e_stats[""token_weights""],\n                                      centroids=s3e_stats[""centroids""],\n                                      token_to_cluster=s3e_stats[""token_to_cluster""],\n                                      svd_components=s3e_stats.get(""svd_components"", None),\n                                      mask=padding_mask == 0)\n        return pooled_vecs\n\n\nclass Bert(LanguageModel):\n    """"""\n    A BERT model that wraps HuggingFace\'s implementation\n    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n    Paper: https://arxiv.org/abs/1810.04805\n\n    """"""\n\n    def __init__(self):\n        super(Bert, self).__init__()\n        self.model = None\n        self.name = ""bert""\n\n    @classmethod\n    def from_scratch(cls, vocab_size, name=""bert"", language=""en""):\n        bert = cls()\n        bert.name = name\n        bert.language = language\n        config = BertConfig(vocab_size=vocab_size)\n        bert.model = BertModel(config)\n        return bert\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n        """"""\n        Load a pretrained model by supplying\n\n        * the name of a remote model on s3 (""bert-base-cased"" ...)\n        * OR a local path of a model trained via transformers (""some_dir/huggingface_model"")\n        * OR a local path of a model trained via FARM (""some_dir/farm_model"")\n\n        :param pretrained_model_name_or_path: The path of the saved pretrained model or its name.\n        :type pretrained_model_name_or_path: str\n\n        """"""\n\n        bert = cls()\n        if ""farm_lm_name"" in kwargs:\n            bert.name = kwargs[""farm_lm_name""]\n        else:\n            bert.name = pretrained_model_name_or_path\n        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n        farm_lm_config = Path(pretrained_model_name_or_path) / ""language_model_config.json""\n        if os.path.exists(farm_lm_config):\n            # FARM style\n            bert_config = BertConfig.from_pretrained(farm_lm_config)\n            farm_lm_model = Path(pretrained_model_name_or_path) / ""language_model.bin""\n            bert.model = BertModel.from_pretrained(farm_lm_model, config=bert_config, **kwargs)\n            bert.language = bert.model.config.language\n        else:\n            # Pytorch-transformer Style\n            bert.model = BertModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n            bert.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n        return bert\n\n    def forward(\n        self,\n        input_ids,\n        segment_ids,\n        padding_mask,\n        **kwargs,\n    ):\n        """"""\n        Perform the forward pass of the BERT model.\n\n        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n        :type input_ids: torch.Tensor\n        :param segment_ids: The id of the segment. For example, in next sentence prediction, the tokens in the\n           first sentence are marked with 0 and those in the second are marked with 1.\n           It is a tensor of shape [batch_size, max_seq_len]\n        :type segment_ids: torch.Tensor\n        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n           of shape [batch_size, max_seq_len]\n        :return: Embeddings for each token in the input sequence.\n\n        """"""\n        output_tuple = self.model(\n            input_ids,\n            token_type_ids=segment_ids,\n            attention_mask=padding_mask,\n        )\n        if self.model.encoder.output_hidden_states == True:\n            sequence_output, pooled_output, all_hidden_states = output_tuple[0], output_tuple[1], output_tuple[2]\n            return sequence_output, pooled_output, all_hidden_states\n        else:\n            sequence_output, pooled_output = output_tuple[0], output_tuple[1]\n            return sequence_output, pooled_output\n\n    def enable_hidden_states_output(self):\n        self.model.encoder.output_hidden_states = True\n\n    def disable_hidden_states_output(self):\n        self.model.encoder.output_hidden_states = False\n\n\nclass Albert(LanguageModel):\n    """"""\n    An ALBERT model that wraps the HuggingFace\'s implementation\n    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n\n    """"""\n\n    def __init__(self):\n        super(Albert, self).__init__()\n        self.model = None\n        self.name = ""albert""\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n        """"""\n        Load a language model either by supplying\n\n        * the name of a remote model on s3 (""albert-base"" ...)\n        * or a local path of a model trained via transformers (""some_dir/huggingface_model"")\n        * or a local path of a model trained via FARM (""some_dir/farm_model"")\n\n        :param pretrained_model_name_or_path: name or path of a model\n        :param language: (Optional) Name of language the model was trained for (e.g. ""german"").\n                         If not supplied, FARM will try to infer it from the model name.\n        :return: Language Model\n\n        """"""\n        albert = cls()\n        if ""farm_lm_name"" in kwargs:\n            albert.name = kwargs[""farm_lm_name""]\n        else:\n            albert.name = pretrained_model_name_or_path\n        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n        farm_lm_config = Path(pretrained_model_name_or_path) / ""language_model_config.json""\n        if os.path.exists(farm_lm_config):\n            # FARM style\n            config = AlbertConfig.from_pretrained(farm_lm_config)\n            farm_lm_model = Path(pretrained_model_name_or_path) / ""language_model.bin""\n            albert.model = AlbertModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n            albert.language = albert.model.config.language\n        else:\n            # Huggingface transformer Style\n            albert.model = AlbertModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n            albert.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n        return albert\n\n    def forward(\n        self,\n        input_ids,\n        segment_ids,\n        padding_mask,\n        **kwargs,\n    ):\n        """"""\n        Perform the forward pass of the Albert model.\n\n        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n        :type input_ids: torch.Tensor\n        :param segment_ids: The id of the segment. For example, in next sentence prediction, the tokens in the\n           first sentence are marked with 0 and those in the second are marked with 1.\n           It is a tensor of shape [batch_size, max_seq_len]\n        :type segment_ids: torch.Tensor\n        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n           of shape [batch_size, max_seq_len]\n        :return: Embeddings for each token in the input sequence.\n\n        """"""\n        output_tuple = self.model(\n            input_ids,\n            token_type_ids=segment_ids,\n            attention_mask=padding_mask,\n        )\n        if self.model.encoder.output_hidden_states == True:\n            sequence_output, pooled_output, all_hidden_states = output_tuple[0], output_tuple[1], output_tuple[2]\n            return sequence_output, pooled_output, all_hidden_states\n        else:\n            sequence_output, pooled_output = output_tuple[0], output_tuple[1]\n            return sequence_output, pooled_output\n\n    def enable_hidden_states_output(self):\n        self.model.encoder.output_hidden_states = True\n\n    def disable_hidden_states_output(self):\n        self.model.encoder.output_hidden_states = False\n\n\nclass Roberta(LanguageModel):\n    """"""\n    A roberta model that wraps the HuggingFace\'s implementation\n    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n    Paper: https://arxiv.org/abs/1907.11692\n\n    """"""\n\n    def __init__(self):\n        super(Roberta, self).__init__()\n        self.model = None\n        self.name = ""roberta""\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n        """"""\n        Load a language model either by supplying\n\n        * the name of a remote model on s3 (""roberta-base"" ...)\n        * or a local path of a model trained via transformers (""some_dir/huggingface_model"")\n        * or a local path of a model trained via FARM (""some_dir/farm_model"")\n\n        :param pretrained_model_name_or_path: name or path of a model\n        :param language: (Optional) Name of language the model was trained for (e.g. ""german"").\n                         If not supplied, FARM will try to infer it from the model name.\n        :return: Language Model\n\n        """"""\n        roberta = cls()\n        if ""farm_lm_name"" in kwargs:\n            roberta.name = kwargs[""farm_lm_name""]\n        else:\n            roberta.name = pretrained_model_name_or_path\n        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n        farm_lm_config = Path(pretrained_model_name_or_path) / ""language_model_config.json""\n        if os.path.exists(farm_lm_config):\n            # FARM style\n            config = RobertaConfig.from_pretrained(farm_lm_config)\n            farm_lm_model = Path(pretrained_model_name_or_path) / ""language_model.bin""\n            roberta.model = RobertaModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n            roberta.language = roberta.model.config.language\n        else:\n            # Huggingface transformer Style\n            roberta.model = RobertaModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n            roberta.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n        return roberta\n\n    def forward(\n        self,\n        input_ids,\n        segment_ids,\n        padding_mask,\n        **kwargs,\n    ):\n        """"""\n        Perform the forward pass of the Roberta model.\n\n        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n        :type input_ids: torch.Tensor\n        :param segment_ids: The id of the segment. For example, in next sentence prediction, the tokens in the\n           first sentence are marked with 0 and those in the second are marked with 1.\n           It is a tensor of shape [batch_size, max_seq_len]\n        :type segment_ids: torch.Tensor\n        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n           of shape [batch_size, max_seq_len]\n        :return: Embeddings for each token in the input sequence.\n\n        """"""\n        output_tuple = self.model(\n            input_ids,\n            token_type_ids=segment_ids,\n            attention_mask=padding_mask,\n        )\n        if self.model.encoder.output_hidden_states == True:\n            sequence_output, pooled_output, all_hidden_states = output_tuple[0], output_tuple[1], output_tuple[2]\n            return sequence_output, pooled_output, all_hidden_states\n        else:\n            sequence_output, pooled_output = output_tuple[0], output_tuple[1]\n            return sequence_output, pooled_output\n\n    def enable_hidden_states_output(self):\n        self.model.encoder.output_hidden_states = True\n\n    def disable_hidden_states_output(self):\n        self.model.encoder.output_hidden_states = False\n\n\nclass XLMRoberta(LanguageModel):\n    """"""\n    A roberta model that wraps the HuggingFace\'s implementation\n    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n    Paper: https://arxiv.org/abs/1907.11692\n\n    """"""\n\n    def __init__(self):\n        super(XLMRoberta, self).__init__()\n        self.model = None\n        self.name = ""xlm_roberta""\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n        """"""\n        Load a language model either by supplying\n\n        * the name of a remote model on s3 (""xlm-roberta-base"" ...)\n        * or a local path of a model trained via transformers (""some_dir/huggingface_model"")\n        * or a local path of a model trained via FARM (""some_dir/farm_model"")\n\n        :param pretrained_model_name_or_path: name or path of a model\n        :param language: (Optional) Name of language the model was trained for (e.g. ""german"").\n                         If not supplied, FARM will try to infer it from the model name.\n        :return: Language Model\n\n        """"""\n        xlm_roberta = cls()\n        if ""farm_lm_name"" in kwargs:\n            xlm_roberta.name = kwargs[""farm_lm_name""]\n        else:\n            xlm_roberta.name = pretrained_model_name_or_path\n        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n        farm_lm_config = Path(pretrained_model_name_or_path) / ""language_model_config.json""\n        if os.path.exists(farm_lm_config):\n            # FARM style\n            config = XLMRobertaConfig.from_pretrained(farm_lm_config)\n            farm_lm_model = Path(pretrained_model_name_or_path) / ""language_model.bin""\n            xlm_roberta.model = XLMRobertaModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n            xlm_roberta.language = xlm_roberta.model.config.language\n        else:\n            # Huggingface transformer Style\n            xlm_roberta.model = XLMRobertaModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n            xlm_roberta.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n        return xlm_roberta\n\n    def forward(\n        self,\n        input_ids,\n        segment_ids,\n        padding_mask,\n        **kwargs,\n    ):\n        """"""\n        Perform the forward pass of the XLMRoberta model.\n\n        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n        :type input_ids: torch.Tensor\n        :param segment_ids: The id of the segment. For example, in next sentence prediction, the tokens in the\n           first sentence are marked with 0 and those in the second are marked with 1.\n           It is a tensor of shape [batch_size, max_seq_len]\n        :type segment_ids: torch.Tensor\n        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n           of shape [batch_size, max_seq_len]\n        :return: Embeddings for each token in the input sequence.\n\n        """"""\n        output_tuple = self.model(\n            input_ids,\n            token_type_ids=segment_ids,\n            attention_mask=padding_mask,\n        )\n        if self.model.encoder.output_hidden_states == True:\n            sequence_output, pooled_output, all_hidden_states = output_tuple[0], output_tuple[1], output_tuple[2]\n            return sequence_output, pooled_output, all_hidden_states\n        else:\n            sequence_output, pooled_output = output_tuple[0], output_tuple[1]\n            return sequence_output, pooled_output\n\n    def enable_hidden_states_output(self):\n        self.model.encoder.output_hidden_states = True\n\n    def disable_hidden_states_output(self):\n        self.model.encoder.output_hidden_states = False\n\n\nclass DistilBert(LanguageModel):\n    """"""\n    A DistilBERT model that wraps HuggingFace\'s implementation\n    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n\n    NOTE:\n    - DistilBert doesn\xe2\x80\x99t have token_type_ids, you don\xe2\x80\x99t need to indicate which\n    token belongs to which segment. Just separate your segments with the separation\n    token tokenizer.sep_token (or [SEP])\n    - Unlike the other BERT variants, DistilBert does not output the\n    pooled_output. An additional pooler is initialized.\n\n    """"""\n\n    def __init__(self):\n        super(DistilBert, self).__init__()\n        self.model = None\n        self.name = ""distilbert""\n        self.pooler = None\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n        """"""\n        Load a pretrained model by supplying\n\n        * the name of a remote model on s3 (""distilbert-base-german-cased"" ...)\n        * OR a local path of a model trained via transformers (""some_dir/huggingface_model"")\n        * OR a local path of a model trained via FARM (""some_dir/farm_model"")\n\n        :param pretrained_model_name_or_path: The path of the saved pretrained model or its name.\n        :type pretrained_model_name_or_path: str\n\n        """"""\n\n        distilbert = cls()\n        if ""farm_lm_name"" in kwargs:\n            distilbert.name = kwargs[""farm_lm_name""]\n        else:\n            distilbert.name = pretrained_model_name_or_path\n        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n        farm_lm_config = Path(pretrained_model_name_or_path) / ""language_model_config.json""\n        if os.path.exists(farm_lm_config):\n            # FARM style\n            config = AlbertConfig.from_pretrained(farm_lm_config)\n            farm_lm_model = Path(pretrained_model_name_or_path) / ""language_model.bin""\n            distilbert.model = DistilBertModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n            distilbert.language = distilbert.model.config.language\n        else:\n            # Pytorch-transformer Style\n            distilbert.model = DistilBertModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n            distilbert.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n        config = distilbert.model.config\n\n        # DistilBERT does not provide a pooled_output by default. Therefore, we need to initialize an extra pooler.\n        # The pooler takes the first hidden representation & feeds it to a dense layer of (hidden_dim x hidden_dim).\n        # We don\'t want a dropout in the end of the pooler, since we do that already in the adaptive model before we\n        # feed everything to the prediction head\n        config.summary_last_dropout = 0\n        config.summary_type = \'first\'\n        config.summary_activation = \'tanh\'\n        distilbert.pooler = SequenceSummary(config)\n        distilbert.pooler.apply(distilbert.model._init_weights)\n        return distilbert\n\n    def forward(\n        self,\n        input_ids,\n        padding_mask,\n        **kwargs,\n    ):\n        """"""\n        Perform the forward pass of the DistilBERT model.\n\n        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n        :type input_ids: torch.Tensor\n        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n           of shape [batch_size, max_seq_len]\n        :return: Embeddings for each token in the input sequence.\n\n        """"""\n        output_tuple = self.model(\n            input_ids,\n            attention_mask=padding_mask,\n        )\n        # We need to manually aggregate that to get a pooled output (one vec per seq)\n        pooled_output = self.pooler(output_tuple[0])\n        if self.model.config.output_hidden_states == True:\n            sequence_output, all_hidden_states = output_tuple[0], output_tuple[1]\n            return sequence_output, pooled_output\n        else:\n            sequence_output = output_tuple[0]\n            return sequence_output, pooled_output\n\n    def enable_hidden_states_output(self):\n        self.model.config.output_hidden_states = True\n\n    def disable_hidden_states_output(self):\n        self.model.config.output_hidden_states = False\n\n\nclass XLNet(LanguageModel):\n    """"""\n    A XLNet model that wraps the HuggingFace\'s implementation\n    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n    Paper: https://arxiv.org/abs/1906.08237\n    """"""\n\n    def __init__(self):\n        super(XLNet, self).__init__()\n        self.model = None\n        self.name = ""xlnet""\n        self.pooler = None\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n        """"""\n        Load a language model either by supplying\n\n        * the name of a remote model on s3 (""xlnet-base-cased"" ...)\n        * or a local path of a model trained via transformers (""some_dir/huggingface_model"")\n        * or a local path of a model trained via FARM (""some_dir/farm_model"")\n\n        :param pretrained_model_name_or_path: name or path of a model\n        :param language: (Optional) Name of language the model was trained for (e.g. ""german"").\n                         If not supplied, FARM will try to infer it from the model name.\n        :return: Language Model\n\n        """"""\n        xlnet = cls()\n        if ""farm_lm_name"" in kwargs:\n            xlnet.name = kwargs[""farm_lm_name""]\n        else:\n            xlnet.name = pretrained_model_name_or_path\n        # We need to differentiate between loading model using FARM format and Pytorch-Transformers format\n        farm_lm_config = Path(pretrained_model_name_or_path) / ""language_model_config.json""\n        if os.path.exists(farm_lm_config):\n            # FARM style\n            config = XLNetConfig.from_pretrained(farm_lm_config)\n            farm_lm_model = Path(pretrained_model_name_or_path) / ""language_model.bin""\n            xlnet.model = XLNetModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n            xlnet.language = xlnet.model.config.language\n        else:\n            # Pytorch-transformer Style\n            xlnet.model = XLNetModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n            xlnet.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n            config = xlnet.model.config\n        # XLNet does not provide a pooled_output by default. Therefore, we need to initialize an extra pooler.\n        # The pooler takes the last hidden representation & feeds it to a dense layer of (hidden_dim x hidden_dim).\n        # We don\'t want a dropout in the end of the pooler, since we do that already in the adaptive model before we\n        # feed everything to the prediction head\n        config.summary_last_dropout = 0\n        xlnet.pooler = SequenceSummary(config)\n        xlnet.pooler.apply(xlnet.model._init_weights)\n        return xlnet\n\n    def forward(\n        self,\n        input_ids,\n        segment_ids,\n        padding_mask,\n        **kwargs,\n    ):\n        """"""\n        Perform the forward pass of the XLNet model.\n\n        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n        :type input_ids: torch.Tensor\n        :param segment_ids: The id of the segment. For example, in next sentence prediction, the tokens in the\n           first sentence are marked with 0 and those in the second are marked with 1.\n           It is a tensor of shape [batch_size, max_seq_len]\n        :type segment_ids: torch.Tensor\n        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n           of shape [batch_size, max_seq_len]\n        :return: Embeddings for each token in the input sequence.\n        """"""\n\n        # Note: XLNet has a couple of special input tensors for pretraining / text generation  (perm_mask, target_mapping ...)\n        # We will need to implement them, if we wanna support LM adaptation\n\n        output_tuple = self.model(\n            input_ids,\n            token_type_ids=segment_ids,\n            attention_mask=padding_mask,\n        )\n        # XLNet also only returns the sequence_output (one vec per token)\n        # We need to manually aggregate that to get a pooled output (one vec per seq)\n        #TODO verify that this is really doing correct pooling\n        pooled_output = self.pooler(output_tuple[0])\n\n        if self.model.output_hidden_states == True:\n            sequence_output, all_hidden_states = output_tuple[0], output_tuple[1]\n            return sequence_output, pooled_output, all_hidden_states\n        else:\n            sequence_output = output_tuple[0]\n            return sequence_output, pooled_output\n\n    def enable_hidden_states_output(self):\n        self.model.output_hidden_states = True\n\n    def disable_hidden_states_output(self):\n        self.model.output_hidden_states = False\n\nclass EmbeddingConfig():\n    """"""\n    Config for Word Embeddings Models.\n    Necessary to work with Bert and other LM style functionality\n    """"""\n    def __init__(self,\n                 name=None,\n                 embeddings_filename=None,\n                 vocab_filename=None,\n                 vocab_size=None,\n                 hidden_size=None,\n                 language=None,\n                 **kwargs):\n        """"""\n        :param name: Name of config\n        :param embeddings_filename:\n        :param vocab_filename:\n        :param vocab_size:\n        :param hidden_size:\n        :param language:\n        :param kwargs:\n        """"""\n        self.name = name\n        self.embeddings_filename = embeddings_filename\n        self.vocab_filename = vocab_filename\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.language = language\n        if len(kwargs) > 0:\n            logger.info(f""Passed unused params {str(kwargs)} to the EmbeddingConfig. Might not be a problem."")\n\n    def to_dict(self):\n        """"""\n        Serializes this instance to a Python dictionary.\n\n        Returns:\n            :obj:`Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n        """"""\n        output = copy.deepcopy(self.__dict__)\n        if hasattr(self.__class__, ""model_type""):\n            output[""model_type""] = self.__class__.model_type\n        return output\n\n    def to_json_string(self):\n        """"""\n        Serializes this instance to a JSON string.\n\n        Returns:\n            :obj:`string`: String containing all the attributes that make up this configuration instance in JSON format.\n        """"""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\n\nclass EmbeddingModel():\n    """"""\n    Embedding Model that combines\n    - Embeddings\n    - Config Object\n    - Vocab\n    Necessary to work with Bert and other LM style functionality\n    """"""\n\n    def __init__(self,\n                 embedding_file,\n                 config_dict,\n                 vocab_file):\n        """"""\n\n        :param embedding_file: filename of embeddings. Usually in txt format, with the word and associated vector on each line\n        :type embedding_file: str\n        :param config_dict: dictionary containing config elements\n        :type config_dict: dict\n        :param vocab_file: filename of vocab, each line contains a word\n        :type vocab_file: str\n        """"""\n        self.config = EmbeddingConfig(**config_dict)\n        self.vocab = load_vocab(vocab_file)\n        temp = wordembedding_utils.load_embedding_vectors(embedding_file=embedding_file, vocab=self.vocab)\n        self.embeddings = torch.from_numpy(temp).float()\n        assert ""[UNK]"" in self.vocab, ""No [UNK] symbol in Wordembeddingmodel! Aborting""\n        self.unk_idx = self.vocab[""[UNK]""]\n\n    def save(self,save_dir):\n        # Save Weights\n        save_name = Path(save_dir) / self.config.embeddings_filename\n        embeddings = self.embeddings.cpu().numpy()\n        with open(save_name, ""w"") as f:\n            for w, vec in tqdm(zip(self.vocab, embeddings), desc=""Saving embeddings"", total=embeddings.shape[0]):\n                f.write(w + "" "" + "" "".join([""%.6f"" % v for v in vec]) + ""\\n"")\n        f.close()\n\n        # Save vocab\n        save_name = Path(save_dir) / self.config.vocab_filename\n        with open(save_name, ""w"") as f:\n            for w in self.vocab:\n                f.write(w + ""\\n"")\n        f.close()\n\n\n    def resize_token_embeddings(self, new_num_tokens=None):\n        # function is called as a vocab length validation inside FARM\n        # fast way of returning an object with num_embeddings attribute (needed for some checks)\n        # TODO add functionality to add words/tokens to a wordembeddingmodel after initialization\n        temp = {}\n        temp[""num_embeddings""] = len(self.vocab)\n        temp = DotMap(temp)\n        return temp\n\n\n\nclass WordEmbedding_LM(LanguageModel):\n    """"""\n    A Language Model based only on word embeddings\n    - Inside FARM, WordEmbedding Language Models must have a fixed vocabulary\n    - Each (known) word in some text input is projected to its vector representation\n    - Pooling operations can be applied for representing whole text sequences\n\n    """"""\n\n    def __init__(self):\n        super(WordEmbedding_LM, self).__init__()\n        self.model = None\n        self.name = ""WordEmbedding_LM""\n        self.pooler = None\n\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n        """"""\n        Load a language model either by supplying\n\n        * a local path of a model trained via FARM (""some_dir/farm_model"")\n        * the name of a remote model on s3\n\n        :param pretrained_model_name_or_path: name or path of a model\n        :param language: (Optional) Name of language the model was trained for (e.g. ""german"").\n                         If not supplied, FARM will try to infer it from the model name.\n        :return: Language Model\n\n        """"""\n        wordembedding_LM = cls()\n        if ""farm_lm_name"" in kwargs:\n            wordembedding_LM.name = kwargs[""farm_lm_name""]\n        else:\n            wordembedding_LM.name = pretrained_model_name_or_path\n        # We need to differentiate between loading model from local or remote\n        farm_lm_config = Path(pretrained_model_name_or_path) / ""language_model_config.json""\n        if os.path.exists(farm_lm_config):\n            # local dir\n            config = json.load(open(farm_lm_config,""r""))\n            farm_lm_model = Path(pretrained_model_name_or_path) / config[""embeddings_filename""]\n            vocab_filename = Path(pretrained_model_name_or_path) / config[""vocab_filename""]\n            wordembedding_LM.model = EmbeddingModel(embedding_file=str(farm_lm_model), config_dict=config, vocab_file=str(vocab_filename))\n            wordembedding_LM.language = config.get(""language"", None)\n        else:\n            # from remote or cache\n            config_dict, resolved_vocab_file, resolved_model_file = wordembedding_utils.load_model(pretrained_model_name_or_path, **kwargs)\n            model = EmbeddingModel(embedding_file=resolved_model_file,\n                                   config_dict=config_dict,\n                                   vocab_file=resolved_vocab_file)\n            wordembedding_LM.model = model\n            wordembedding_LM.language = model.config.language\n\n\n        # taking the mean for getting the pooled representation\n        # TODO: extend this to other pooling operations or remove\n        wordembedding_LM.pooler = lambda x: torch.mean(x, dim=0)\n        return wordembedding_LM\n\n    def save(self, save_dir):\n        """"""\n        Save the model embeddings and its config file so that it can be loaded again.\n        # TODO make embeddings trainable and save trained embeddings\n        # TODO save model weights as pytorch model bin for more efficient loading and saving\n        :param save_dir: The directory in which the model should be saved.\n        :type save_dir: str\n        """"""\n        #save model\n        self.model.save(save_dir=save_dir)\n        #save config\n        self.save_config(save_dir=save_dir)\n\n\n    def forward(self, input_ids, **kwargs,):\n        """"""\n        Perform the forward pass of the wordembedding model.\n        This is just the mapping of words to their corresponding embeddings\n        """"""\n        sequence_output = []\n        pooled_output = []\n        # TODO do not use padding items in pooled output\n        for sample in input_ids:\n            sample_embeddings = []\n            for index in sample:\n                #if index != self.model.unk_idx:\n                sample_embeddings.append(self.model.embeddings[index])\n            sample_embeddings = torch.stack(sample_embeddings)\n            sequence_output.append(sample_embeddings)\n            pooled_output.append(self.pooler(sample_embeddings))\n\n        sequence_output = torch.stack(sequence_output)\n        pooled_output = torch.stack(pooled_output)\n        m = nn.BatchNorm1d(pooled_output.shape[1])\n        # use batchnorm for more stable learning\n        # but disable it, if we have batch size of one (cannot compute batchnorm stats with only one sample)\n        if pooled_output.shape[0] > 1:\n            pooled_output = m(pooled_output)\n        return sequence_output, pooled_output\n\n    def trim_vocab(self, token_counts, processor, min_threshold):\n        """""" Remove embeddings for rare tokens in your corpus (< `min_threshold` occurrences) to reduce model size""""""\n        logger.info(f""Removing tokens with less than {min_threshold} occurrences from model vocab"")\n        new_vocab = OrderedDict()\n        valid_tok_indices = []\n        cnt = 0\n        old_num_emb = self.model.embeddings.shape[0]\n        for token, tok_idx in self.model.vocab.items():\n            if token_counts.get(token, 0) >= min_threshold or token in (""[CLS]"",""[SEP]"",""[UNK]"",""[PAD]"",""[MASK]""):\n                new_vocab[token] = cnt\n                valid_tok_indices.append(tok_idx)\n                cnt += 1\n\n        self.model.vocab = new_vocab\n        self.model.embeddings = self.model.embeddings[valid_tok_indices, :]\n\n        # update tokenizer vocab in place\n        processor.tokenizer.vocab = self.model.vocab\n        processor.tokenizer.ids_to_tokens = OrderedDict()\n        for k, v in processor.tokenizer.vocab.items():\n            processor.tokenizer.ids_to_tokens[v] = k\n\n        logger.info(f""Reduced vocab from {old_num_emb} to {self.model.embeddings.shape[0]}"")\n\n    def normalize_embeddings(self, zero_mean=True, pca_removal=False, pca_n_components=300, pca_n_top_components=10,\n                             use_mean_vec_for_special_tokens=True, n_special_tokens=5):\n        """""" Normalize word embeddings as in https://arxiv.org/pdf/1808.06305.pdf\n            (e.g. used for S3E Pooling of sentence embeddings)\n            \n        :param zero_mean: Whether to center embeddings via subtracting mean\n        :type zero_mean: bool\n        :param pca_removal: Whether to remove PCA components\n        :type pca_removal: bool\n        :param pca_n_components: Number of PCA components to use for fitting\n        :type pca_n_components: int\n        :param pca_n_top_components: Number of PCA components to remove\n        :type pca_n_top_components: int\n        :param use_mean_vec_for_special_tokens: Whether to replace embedding of special tokens with the mean embedding\n        :type use_mean_vec_for_special_tokens: bool\n        :param n_special_tokens: Number of special tokens like CLS, UNK etc. (used if `use_mean_vec_for_special_tokens`). \n                                 Note: We expect the special tokens to be the first `n_special_tokens` entries of the vocab.\n        :type n_special_tokens: int\n        :return: None\n        """"""\n\n        if zero_mean:\n            logger.info(\'Removing mean from embeddings\')\n            # self.model.embeddings[:n_special_tokens, :] = torch.zeros((n_special_tokens, 300))\n            mean_vec = torch.mean(self.model.embeddings, 0)\n            self.model.embeddings = self.model.embeddings - mean_vec\n\n            if use_mean_vec_for_special_tokens:\n                self.model.embeddings[:n_special_tokens, :] = mean_vec\n\n        if pca_removal:\n            from sklearn.decomposition import PCA\n            logger.info(\'Removing projections on top PCA components from embeddings (see https://arxiv.org/pdf/1808.06305.pdf)\')\n            pca = PCA(n_components=pca_n_components)\n            pca.fit(self.model.embeddings.cpu().numpy())\n\n            U1 = pca.components_\n            explained_variance = pca.explained_variance_\n\n            # Removing projections on top components\n            PVN_dims = pca_n_top_components\n            for emb_idx in tqdm(range(self.model.embeddings.shape[0]), desc=""Removing projections""):\n                for pca_idx, u in enumerate(U1[0:PVN_dims]):\n                    ratio = (explained_variance[pca_idx] - explained_variance[PVN_dims]) / explained_variance[pca_idx]\n                    self.model.embeddings[emb_idx] = self.model.embeddings[emb_idx] - ratio * np.dot(u.transpose(), self.model.embeddings[emb_idx]) * u\n\n\nclass Electra(LanguageModel):\n    """"""\n    ELECTRA is a new pre-training approach which trains two transformer models:\n    the generator and the discriminator. The generator replaces tokens in a sequence,\n    and is therefore trained as a masked language model. The discriminator, which is\n    the model we\'re interested in, tries to identify which tokens were replaced by\n    the generator in the sequence.\n\n    The ELECTRA model here wraps HuggingFace\'s implementation\n    (https://github.com/huggingface/transformers) to fit the LanguageModel class.\n\n    NOTE:\n    - Electra does not output the pooled_output. An additional pooler is initialized.\n\n    """"""\n\n    def __init__(self):\n        super(Electra, self).__init__()\n        self.model = None\n        self.name = ""electra""\n        self.pooler = None\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path, language=None, **kwargs):\n        """"""\n        Load a pretrained model by supplying\n\n        * the name of a remote model on s3 (""google/electra-base-discriminator"" ...)\n        * OR a local path of a model trained via transformers (""some_dir/huggingface_model"")\n        * OR a local path of a model trained via FARM (""some_dir/farm_model"")\n\n        :param pretrained_model_name_or_path: The path of the saved pretrained model or its name.\n        :type pretrained_model_name_or_path: str\n\n        """"""\n\n        electra = cls()\n        if ""farm_lm_name"" in kwargs:\n            electra.name = kwargs[""farm_lm_name""]\n        else:\n            electra.name = pretrained_model_name_or_path\n        # We need to differentiate between loading model using FARM format and Transformers format\n        farm_lm_config = Path(pretrained_model_name_or_path) / ""language_model_config.json""\n        if os.path.exists(farm_lm_config):\n            # FARM style\n            config = ElectraConfig.from_pretrained(farm_lm_config)\n            farm_lm_model = Path(pretrained_model_name_or_path) / ""language_model.bin""\n            electra.model = ElectraModel.from_pretrained(farm_lm_model, config=config, **kwargs)\n            electra.language = electra.model.config.language\n        else:\n            # Transformers Style\n            electra.model = ElectraModel.from_pretrained(str(pretrained_model_name_or_path), **kwargs)\n            electra.language = cls._get_or_infer_language_from_name(language, pretrained_model_name_or_path)\n        config = electra.model.config\n\n        # ELECTRA does not provide a pooled_output by default. Therefore, we need to initialize an extra pooler.\n        # The pooler takes the first hidden representation & feeds it to a dense layer of (hidden_dim x hidden_dim).\n        # We don\'t want a dropout in the end of the pooler, since we do that already in the adaptive model before we\n        # feed everything to the prediction head.\n        # Note: ELECTRA uses gelu as activation (BERT uses tanh instead)\n        config.summary_last_dropout = 0\n        config.summary_type = \'first\'\n        config.summary_activation = \'gelu\'\n        electra.pooler = SequenceSummary(config)\n        electra.pooler.apply(electra.model._init_weights)\n        return electra\n\n    def forward(\n        self,\n        input_ids,\n        segment_ids,\n        padding_mask,\n        **kwargs,\n    ):\n        """"""\n        Perform the forward pass of the ELECTRA model.\n\n        :param input_ids: The ids of each token in the input sequence. Is a tensor of shape [batch_size, max_seq_len]\n        :type input_ids: torch.Tensor\n        :param padding_mask: A mask that assigns a 1 to valid input tokens and 0 to padding tokens\n           of shape [batch_size, max_seq_len]\n        :return: Embeddings for each token in the input sequence.\n\n        """"""\n        output_tuple = self.model(\n            input_ids,\n            token_type_ids=segment_ids,\n            attention_mask=padding_mask,\n        )\n\n        # We need to manually aggregate that to get a pooled output (one vec per seq)\n        pooled_output = self.pooler(output_tuple[0])\n\n        if self.model.config.output_hidden_states == True:\n            sequence_output, all_hidden_states = output_tuple[0], output_tuple[1]\n            return sequence_output, pooled_output\n        else:\n            sequence_output = output_tuple[0]\n            return sequence_output, pooled_output\n\n    def enable_hidden_states_output(self):\n        self.model.config.output_hidden_states = True\n\n    def disable_hidden_states_output(self):\n        self.model.config.output_hidden_states = False\n\n'"
farm/modeling/optimization.py,11,"b'from importlib import import_module\nimport logging\nimport sys\nimport inspect\n\nimport torch\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.nn import DataParallel\n\n# Used indirectly in _get_optim() to avoid name collision with torch\'s AdamW\nfrom transformers.optimization import AdamW as TransformersAdamW\n\ntry:\n    from apex import amp\n    try:\n        from apex.parallel import convert_syncbn_model\n        APEX_PARALLEL_AVAILABLE = True\n    except AttributeError:\n        APEX_PARALLEL_AVAILABLE = False\n    AMP_AVAILABLE = True\nexcept ImportError:\n    AMP_AVAILABLE = False\n\nfrom farm.utils import MLFlowLogger as MlLogger\n\nlogger = logging.getLogger(__name__)\n\n\nclass WrappedDataParallel(DataParallel):\n    """"""\n    A way of adapting attributes of underlying class to parallel mode. See: https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html#dataparallel\n\n    Gets into recursion errors. Workaround see: https://discuss.pytorch.org/t/access-att-of-model-wrapped-within-torch-nn-dataparallel-maximum-recursion-depth-exceeded/46975\n    """"""\n\n    def __getattr__(self, name):\n        try:\n            return super().__getattr__(name)\n        except AttributeError:\n            return getattr(self.module, name)\n\n\nclass WrappedDDP(DistributedDataParallel):\n    """"""\n    A way of adapting attributes of underlying class to distributed mode. Same as in WrappedDataParallel above.\n    Even when using distributed on a single machine with multiple GPUs, apex can speed up training significantly.\n    Distributed code must be launched with ""python -m torch.distributed.launch --nproc_per_node=1 run_script.py""\n    """"""\n\n    def __getattr__(self, name):\n        try:\n            return super().__getattr__(name)\n        except AttributeError:\n            return getattr(self.module, name)\n\n\ndef initialize_optimizer(model,\n                         n_batches,\n                         n_epochs,\n                         device,\n                         learning_rate,\n                         optimizer_opts=None,\n                         schedule_opts=None,\n                         distributed=False,\n                         grad_acc_steps=1,\n                         local_rank=-1,\n                         use_amp=None):\n    """"""\n    Initializes an optimizer, a learning rate scheduler and converts the model if needed (e.g for mixed precision).\n    Per default, we use transformers\' AdamW and a linear warmup schedule with warmup ratio 0.1.\n    You can easily switch optimizer and schedule via `optimizer_opts` and `schedule_opts`.\n\n    :param model: model to optimize (e.g. trimming weights to fp16 / mixed precision)\n    :type model: AdaptiveModel\n    :param n_batches: number of batches for training\n    :type n_batches: int\n    :param n_epochs: number of epochs for training\n    :param device:\n    :param learning_rate: Learning rate\n    :type learning_rate: float\n    :param optimizer_opts: Dict to customize the optimizer. Choose any optimizer available from torch.optim, apex.optimizers or\n                           transformers.optimization by supplying the class name and the parameters for the constructor.\n                           Examples:\n                           1) AdamW from Transformers (Default):\n                           {""name"": ""TransformersAdamW"", ""correct_bias"": False, ""weight_decay"": 0.01}\n                           2) SGD from pytorch:\n                           {""name"": ""SGD"", ""momentum"": 0.0}\n                           3) FusedLAMB from apex:\n                           {""name"": ""FusedLAMB"", ""bias_correction"": True}\n    :param schedule_opts: Dict to customize the learning rate schedule.\n                          Choose any Schedule from Pytorch or Huggingface\'s Transformers by supplying the class name\n                          and the parameters needed by the constructor.\n                          Examples:\n                          1) Linear Warmup (Default):\n                          {""name"": ""LinearWarmup"",\n                          ""num_warmup_steps"": 0.1 * num_training_steps,\n                          ""num_training_steps"": num_training_steps}\n                          2) CosineWarmup:\n                          {""name"": ""CosineWarmup"",\n                          ""num_warmup_steps"": 0.1 * num_training_steps,\n                          ""num_training_steps"": num_training_steps}\n                          3) CyclicLR from pytorch:\n                          {""name"": ""CyclicLR"", ""base_lr"": 1e-5, ""max_lr"":1e-4, ""step_size_up"": 100}\n    :param distributed: Whether training on distributed machines\n    :param grad_acc_steps: Number of steps to accumulate gradients for. Helpful to mimic large batch_sizes on small machines.\n    :param local_rank: rank of the machine in a distributed setting\n    :param use_amp: Optimization level of nvidia\'s automatic mixed precision (AMP). The higher the level, the faster the model.\n                    Options:\n                    ""O0"" (Normal FP32 training)\n                    ""O1"" (Mixed Precision => Recommended)\n                    ""O2"" (Almost FP16)\n                    ""O3"" (Pure FP16).\n                    See details on: https://nvidia.github.io/apex/amp.html\n    :return: model, optimizer, scheduler\n    """"""\n\n    if use_amp and not AMP_AVAILABLE:\n        raise ImportError(f\'Got use_amp = {use_amp}, but cannot find apex. \'\n                          \'Please install Apex if you want to make use of automatic mixed precision. \'\n                          \'https://github.com/NVIDIA/apex\')\n\n    num_train_optimization_steps = calculate_optimization_steps(\n        n_batches, grad_acc_steps, n_epochs, local_rank\n    )\n    # Log params\n    MlLogger.log_params({\n         ""use_amp"": use_amp,\n         ""num_train_optimization_steps"": num_train_optimization_steps,\n        })\n\n    # Use some defaults to simplify life of inexperienced users\n    if optimizer_opts is None:\n        optimizer_opts = {""name"": ""TransformersAdamW"", ""correct_bias"": False, ""weight_decay"": 0.01}\n    optimizer_opts[""lr""] = learning_rate\n\n    if schedule_opts is None:\n        # Default schedule: Linear Warmup with 10% warmup\n        schedule_opts = {""name"": ""LinearWarmup"",\n                         ""num_warmup_steps"": 0.1 * num_train_optimization_steps,\n                         ""num_training_steps"": num_train_optimization_steps}\n\n        # schedule_opts = {""name"": ""OneCycleLR"", ""max_lr"":learning_rate, ""pct_start"": 0.1,\n        #                  ""total_steps"": num_train_optimization_steps }\n    schedule_opts[""num_training_steps""] = num_train_optimization_steps\n\n    # Get optimizer from pytorch, transformers or apex\n    optimizer = _get_optim(model, optimizer_opts)\n\n    # Get learning rate schedule\n    scheduler = get_scheduler(optimizer, schedule_opts)\n\n    # Adjust for parallel training + amp\n    model, optimizer = _optimize_model(model, device, local_rank, optimizer, distributed, use_amp)\n\n    return model, optimizer, scheduler\n\n\ndef _get_optim(model, opts):\n    """""" Get the optimizer based on dictionary with options. Options are passed to the optimizer constructor.\n\n    :param model: model to optimize\n    :param opts: config dictionary that will be passed to optimizer together with the params\n    (e.g. lr, weight_decay, correct_bias ...). no_decay\' can be given - parameters containing any of those strings\n    will have weight_decay set to 0.\n    :return: created optimizer\n    """"""\n\n    optimizer_name = opts.pop(\'name\', None)\n\n    # Logging\n    logger.info(f""Loading optimizer `{optimizer_name}`: \'{opts}\'"")\n    MlLogger.log_params(opts)\n    MlLogger.log_params({""optimizer_name"": optimizer_name})\n\n    weight_decay = opts.pop(\'weight_decay\', None)\n    no_decay = opts.pop(\'no_decay\', None)\n\n    if no_decay:\n        optimizable_parameters = [\n            {\'params\': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad],\n             **opts},\n            {\'params\': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],\n             \'weight_decay\': 0.0,\n             **opts}\n        ]\n    else:\n        optimizable_parameters = [{\'params\': [p for p in model.parameters() if p.requires_grad], **opts}]\n\n    # default weight decay is not the same for all optimizers, so we can\'t use default value\n    # only explicitly add weight decay if it\'s given\n    if weight_decay is not None:\n        optimizable_parameters[0][\'weight_decay\'] = weight_decay\n\n    # Import optimizer by checking in order: torch, transformers, apex and local imports\n    try:\n        optim_constructor = getattr(import_module(\'torch.optim\'), optimizer_name)\n    except AttributeError:\n        try:\n            optim_constructor = getattr(import_module(\'transformers.optimization\'), optimizer_name)\n        except AttributeError:\n            try:\n                optim_constructor = getattr(import_module(\'apex.optimizers\'), optimizer_name)\n            except (AttributeError, ImportError):\n                try:\n                    # Workaround to allow loading AdamW from transformers\n                    # pytorch > 1.2 has now also a AdamW (but without the option to set bias_correction = False,\n                    # which is done in the original BERT implementation)\n                    optim_constructor = getattr(sys.modules[__name__], optimizer_name)\n                except (AttributeError, ImportError):\n                    raise AttributeError(f""Optimizer \'{optimizer_name}\' not found in \'torch\', \'transformers\', \'apex\' or \'local imports"")\n\n    return optim_constructor(optimizable_parameters)\n\n\ndef get_scheduler(optimizer, opts):\n    """""" Get the scheduler based on dictionary with options. Options are passed to the scheduler constructor.\n\n    :param optimizer: optimizer whose learning rate to control\n    :param opts: dictionary of args to be passed to constructor of schedule\n    :return: created scheduler\n    """"""\n    schedule_name = opts.get(\'name\')\n    try:\n        sched_constructor = getattr(import_module(\'torch.optim.lr_scheduler\'), schedule_name)\n    except AttributeError:\n        try:\n            # The method names in transformers became quite long and unhandy.\n            # for convenience we offer usage of shorter alias (e.g. ""LinearWarmup"")\n            scheduler_translations = {""LinearWarmup"": ""get_linear_schedule_with_warmup"",\n                                      ""Constant"": ""get_constant_schedule_with_warmup"",\n                                      ""CosineWarmup"": ""get_cosine_schedule_with_warmup"",\n                                     ""CosineWarmupWithRestarts"": ""get_cosine_with_hard_restarts_schedule_with_warmup""\n            }\n            if schedule_name in scheduler_translations.keys():\n                schedule_name = scheduler_translations[schedule_name]\n            # in contrast to torch, we actually get here a method and not a class\n            sched_constructor = getattr(import_module(\'transformers.optimization\'), schedule_name)\n        except AttributeError:\n            raise AttributeError(f""Scheduler \'{schedule_name}\' not found in \'torch\' or \'transformers\'"")\n\n    logger.info(f""Using scheduler \'{schedule_name}\'"")\n\n    # get supported args of constructor\n    allowed_args = inspect.signature(sched_constructor).parameters.keys()\n\n    # convert from warmup proportion to steps if required\n    if \'num_warmup_steps\' in allowed_args and \'num_warmup_steps\' not in opts and \'warmup_proportion\' in opts:\n        opts[\'num_warmup_steps\'] = int(opts[""warmup_proportion""] * opts[""num_training_steps""])\n        MlLogger.log_params({""warmup_proportion"": opts[""warmup_proportion""]})\n\n    # only pass args that are supported by the constructor\n    constructor_opts = {k: v for k, v in opts.items() if k in allowed_args}\n\n    # Logging\n    logger.info(f""Loading schedule `{schedule_name}`: \'{constructor_opts}\'"")\n    MlLogger.log_params(constructor_opts)\n    MlLogger.log_params({""schedule_name"": schedule_name})\n\n    scheduler = sched_constructor(optimizer, **constructor_opts)\n    scheduler.opts = opts  # save the opts with the scheduler to use in load/save\n    return scheduler\n\n\ndef calculate_optimization_steps(n_batches, grad_acc_steps, n_epochs, local_rank):\n    optimization_steps = int(n_batches / grad_acc_steps) * n_epochs\n    if local_rank != -1:\n        optimization_steps = optimization_steps // torch.distributed.get_world_size()\n    return optimization_steps\n\n\ndef _optimize_model(model, device, local_rank, optimizer=None, distributed=False, use_amp=None):\n    model, optimizer = _init_amp(model, device, optimizer, use_amp)\n\n    if distributed:\n        if APEX_PARALLEL_AVAILABLE:\n            model = convert_syncbn_model(model)\n\n        n_gpu = torch.cuda.device_count() // torch.distributed.get_world_size()\n        device_ids = list(range(local_rank * n_gpu, (local_rank + 1) * n_gpu))\n        # for some models DistributedDataParallel might complain about parameters\n        # not contributing to loss. find_used_parameters remedies that.\n        #TODO check if Wrapped DDP still needed?\n        model = DistributedDataParallel(model,\n                                        device_ids=device_ids,\n                                        output_device=device_ids[0],\n                                        find_unused_parameters=True)\n\n    elif torch.cuda.device_count() > 1 and device.type == ""cuda"":\n        model = WrappedDataParallel(model)\n\n    return model, optimizer\n\n\ndef _init_amp(model, device, optimizer=None, use_amp=None):\n    model = model.to(device)\n    if use_amp and optimizer:\n        if AMP_AVAILABLE:\n            model, optimizer = amp.initialize(model, optimizer, opt_level=use_amp)\n        else:\n            logger.warning(f""Can\'t find AMP although you specificed to use amp with level {use_amp}. Will continue without AMP ..."")\n\n    return model, optimizer\n'"
farm/modeling/prediction_head.py,21,"b'import itertools\nimport json\nimport logging\nimport os\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import expit, softmax\nimport tqdm\nfrom pathlib import Path\nimport torch\nfrom transformers.modeling_bert import BertForPreTraining, BertLayerNorm, ACT2FN\nfrom transformers.modeling_auto import AutoModelForQuestionAnswering, AutoModelForTokenClassification, AutoModelForSequenceClassification\nfrom transformers.configuration_auto import AutoConfig\n\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\n\nfrom farm.data_handler.utils import is_json\nfrom farm.utils import convert_iob_to_simple_tags, span_to_string\nfrom farm.modeling.predictions import Span, DocumentPred\n\nlogger = logging.getLogger(__name__)\n\n\nclass PredictionHead(nn.Module):\n    """""" Takes word embeddings from a language model and generates logits for a given task. Can also convert logits\n    to loss and and logits to predictions. """"""\n\n    subclasses = {}\n\n    def __init_subclass__(cls, **kwargs):\n        """""" This automatically keeps track of all available subclasses.\n        Enables generic load() for all specific PredictionHead implementation.\n        """"""\n        super().__init_subclass__(**kwargs)\n        cls.subclasses[cls.__name__] = cls\n\n    @classmethod\n    def create(cls, prediction_head_name, layer_dims, class_weights=None):\n        """"""\n        Create subclass of Prediction Head.\n\n        :param prediction_head_name: Classname (exact string!) of prediction head we want to create\n        :type prediction_head_name: str\n        :param layer_dims: describing the feed forward block structure, e.g. [768,2]\n        :type layer_dims: List[Int]\n        :param class_weights: The loss weighting to be assigned to certain label classes during training.\n           Used to correct cases where there is a strong class imbalance.\n        :type class_weights: list[Float]\n        :return: Prediction Head of class prediction_head_name\n        """"""\n        # TODO make we want to make this more generic.\n        #  1. Class weights is not relevant for all heads.\n        #  2. Layer weights impose FF structure, maybe we want sth else later\n        # Solution: We could again use **kwargs\n        return cls.subclasses[prediction_head_name](\n            layer_dims=layer_dims, class_weights=class_weights\n        )\n\n    def save_config(self, save_dir, head_num=0):\n        """"""\n        Saves the config as a json file.\n\n        :param save_dir: Path to save config to\n        :type save_dir: str or Path\n        :param head_num: Which head to save\n        :type head_num: int\n        """"""\n        # updating config in case the parameters have been changed\n        self.generate_config()\n        output_config_file = Path(save_dir) / f""prediction_head_{head_num}_config.json""\n        with open(output_config_file, ""w"") as file:\n            json.dump(self.config, file)\n\n    def save(self, save_dir, head_num=0):\n        """"""\n        Saves the prediction head state dict.\n\n        :param save_dir: path to save prediction head to\n        :type save_dir: str or Path\n        :param head_num: which head to save\n        :type head_num: int\n        """"""\n        output_model_file = Path(save_dir) / f""prediction_head_{head_num}.bin""\n        torch.save(self.state_dict(), output_model_file)\n        self.save_config(save_dir, head_num)\n\n    def generate_config(self):\n        """"""\n        Generates config file from Class parameters (only for sensible config parameters).\n        """"""\n        config = {}\n        for key, value in self.__dict__.items():\n            if is_json(value) and key[0] != ""_"":\n                config[key] = value\n        config[""name""] = self.__class__.__name__\n        self.config = config\n\n    @classmethod\n    def load(cls, config_file, strict=True, load_weights=True):\n        """"""\n        Loads a Prediction Head. Infers the class of prediction head from config_file.\n\n        :param config_file: location where corresponding config is stored\n        :type config_file: str\n        :param strict: whether to strictly enforce that the keys loaded from saved model match the ones in\n                       the PredictionHead (see torch.nn.module.load_state_dict()).\n                       Set to `False` for backwards compatibility with PHs saved with older version of FARM.\n        :type strict: bool\n        :return: PredictionHead\n        :rtype: PredictionHead[T]\n        """"""\n        config = json.load(open(config_file))\n        prediction_head = cls.subclasses[config[""name""]](**config)\n        if load_weights:\n            model_file = cls._get_model_file(config_file=config_file)\n            logger.info(""Loading prediction head from {}"".format(model_file))\n            prediction_head.load_state_dict(torch.load(model_file, map_location=torch.device(""cpu"")), strict=strict)\n        return prediction_head\n\n    def logits_to_loss(self, logits, labels):\n        """"""\n        Implement this function in your special Prediction Head.\n        Should combine logits and labels with a loss fct to a per sample loss.\n\n        :param logits: logits, can vary in shape and type, depending on task\n        :type logits: object\n        :param labels: labels, can vary in shape and type, depending on task\n        :type labels: object\n        :return: per sample loss as a torch.tensor of shape [batch_size]\n        """"""\n        raise NotImplementedError()\n\n    def logits_to_preds(self, logits):\n        """"""\n        Implement this function in your special Prediction Head.\n        Should combine turn logits into predictions.\n\n        :param logits: logits, can vary in shape and type, depending on task\n        :type logits: object\n        :return: predictions as a torch.tensor of shape [batch_size]\n        """"""\n        raise NotImplementedError()\n\n    def prepare_labels(self, **kwargs):\n        """"""\n        Some prediction heads need additional label conversion.\n        E.g. NER needs word level labels turned into subword token level labels.\n\n        :param kwargs: placeholder for passing generic parameters\n        :type kwargs: object\n        :return: labels in the right format\n        :rtype: object\n        """"""\n        # TODO maybe just return **kwargs to not force people to implement this\n        raise NotImplementedError()\n\n    def resize_input(self, input_dim):\n        """""" This function compares the output dimensionality of the language model against the input dimensionality\n        of the prediction head. If there is a mismatch, the prediction head will be resized to fit.""""""\n        if ""feed_forward"" not in dir(self):\n            return\n        else:\n            old_dims = self.feed_forward.layer_dims\n            if input_dim == old_dims[0]:\n                return\n            new_dims = [input_dim] + old_dims[1:]\n            logger.info(f""Resizing input dimensions of {type(self).__name__} ({self.task_name}) ""\n                  f""from {old_dims} to {new_dims} to match language model"")\n            self.feed_forward = FeedForwardBlock(new_dims)\n            self.layer_dims[0] = input_dim\n            self.feed_forward.layer_dims[0] = input_dim\n\n    @classmethod\n    def _get_model_file(cls, config_file):\n        if ""config.json"" in str(config_file) and ""prediction_head"" in str(config_file):\n            head_num = int("""".join([char for char in os.path.basename(config_file) if char.isdigit()]))\n            model_file = Path(os.path.dirname(config_file)) / f""prediction_head_{head_num}.bin""\n        else:\n            raise ValueError(f""This doesn\'t seem to be a proper prediction_head config file: \'{config_file}\'"")\n        return model_file\n\n    def _set_name(self, name):\n        self.task_name = name\n\n\nclass RegressionHead(PredictionHead):\n    def __init__(\n        self,\n        layer_dims=[768,1],\n        task_name=""regression"",\n        **kwargs,\n    ):\n        super(RegressionHead, self).__init__()\n        # num_labels could in most cases also be automatically retrieved from the data processor\n        self.layer_dims = layer_dims\n        self.feed_forward = FeedForwardBlock(self.layer_dims)\n        # num_labels is being set to 2 since it is being hijacked to store the scaling factor and the mean\n        self.num_labels = 2\n        self.ph_output_type = ""per_sequence_continuous""\n        self.model_type = ""regression""\n        self.loss_fct = MSELoss(reduction=""none"")\n        self.task_name = task_name\n        self.generate_config()\n\n    def forward(self, x):\n        logits = self.feed_forward(x)\n        return logits\n\n    def logits_to_loss(self, logits, **kwargs):\n        label_ids = kwargs.get(self.label_tensor_name)\n        return self.loss_fct(logits, label_ids.float())\n\n    def logits_to_preds(self, logits, **kwargs):\n        preds = logits.cpu().numpy()\n        #rescale predictions to actual label distribution\n        preds = [x * self.label_list[1] + self.label_list[0] for x in preds]\n        return preds\n\n    def prepare_labels(self, **kwargs):\n        label_ids = kwargs.get(self.label_tensor_name)\n        label_ids = label_ids.cpu().numpy()\n        label_ids = [x * self.label_list[1] + self.label_list[0] for x in label_ids]\n        return label_ids\n\n    def formatted_preds(self, logits, samples, **kwargs):\n        preds = self.logits_to_preds(logits)\n        contexts = [sample.clear_text[""text""] for sample in samples]\n\n        assert len(preds) == len(contexts)\n\n        res = {""task"": ""regression"", ""predictions"": []}\n        for pred, context in zip(preds, contexts):\n            res[""predictions""].append(\n                {\n                    ""context"": f""{context}"",\n                    ""pred"": pred[0]\n                }\n            )\n        return res\n\n\nclass TextClassificationHead(PredictionHead):\n    def __init__(\n        self,\n        layer_dims=None,\n        num_labels=None,\n        class_weights=None,\n        loss_ignore_index=-100,\n        loss_reduction=""none"",\n        task_name=""text_classification"",\n        **kwargs,\n    ):\n        """"""\n        :param layer_dims: The size of the layers in the feed forward component. The feed forward will have as many layers as there are ints in this list. This param will be deprecated in future\n        :type layer_dims: list\n        :param num_labels: The numbers of labels. Use to set the size of the final layer in the feed forward component. It is recommended to only set num_labels or layer_dims, not both.\n        :type num_labels: int\n        :param class_weights:\n        :param loss_ignore_index:\n        :param loss_reduction:\n        :param task_name:\n        :param kwargs:\n        """"""\n        super(TextClassificationHead, self).__init__()\n        # num_labels could in most cases also be automatically retrieved from the data processor\n        if layer_dims:\n            self.layer_dims = layer_dims\n            logger.warning(""`layer_dims` will be deprecated in future releases"")\n        elif num_labels:\n            self.layer_dims = [768, num_labels]\n        else:\n            raise ValueError(""Please supply `num_labels` to define output dim of prediction head"")\n        self.num_labels = self.layer_dims[-1]\n        self.feed_forward = FeedForwardBlock(self.layer_dims)\n        logger.info(f""Prediction head initialized with size {self.layer_dims}"")\n        self.num_labels = self.layer_dims[-1]\n        self.ph_output_type = ""per_sequence""\n        self.model_type = ""text_classification""\n        self.task_name = task_name #used for connecting with the right output of the processor\n        self.class_weights = class_weights\n\n        if class_weights is not None:\n            logger.info(f""Using class weights for task \'{self.task_name}\': {self.class_weights}"")\n            balanced_weights = nn.Parameter(torch.tensor(class_weights), requires_grad=False)\n        else:\n            balanced_weights = None\n\n        self.loss_fct = CrossEntropyLoss(\n            weight=balanced_weights,\n            reduction=loss_reduction,\n            ignore_index=loss_ignore_index,\n        )\n\n        self.generate_config()\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path):\n        """"""\n        Load a prediction head from a saved FARM or transformers model. `pretrained_model_name_or_path`\n        can be one of the following:\n        a) Local path to a FARM prediction head config (e.g. my-bert/prediction_head_0_config.json)\n        b) Local path to a Transformers model (e.g. my-bert)\n        c) Name of a public model from https://huggingface.co/models (e.g. distilbert-base-uncased-distilled-squad)\n\n\n        :param pretrained_model_name_or_path: local path of a saved model or name of a publicly available model.\n                                              Exemplary public name:\n                                              - deepset/bert-base-german-cased-hatespeech-GermEval18Coarse\n\n                                              See https://huggingface.co/models for full list\n\n        """"""\n\n        if os.path.exists(pretrained_model_name_or_path) \\\n                and ""config.json"" in pretrained_model_name_or_path \\\n                and ""prediction_head"" in pretrained_model_name_or_path:\n            # a) FARM style\n            head = super(TextClassificationHead, cls).load(pretrained_model_name_or_path)\n        else:\n            # b) transformers style\n            # load all weights from model\n            full_model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path)\n            # init empty head\n            head = cls(layer_dims=[full_model.config.hidden_size, len(full_model.config.id2label)])\n            # transfer weights for head from full model\n            head.feed_forward.feed_forward[0].load_state_dict(full_model.classifier.state_dict())\n            del full_model\n\n        return head\n\n    def forward(self, X):\n        logits = self.feed_forward(X)\n        return logits\n\n    def logits_to_loss(self, logits, **kwargs):\n        label_ids = kwargs.get(self.label_tensor_name)\n        # In Natural Questions, each dev sample can have multiple labels\n        # For loss calculation we only use the first label\n        label_ids = label_ids.narrow(1,0,1)\n        return self.loss_fct(logits, label_ids.view(-1))\n\n    def logits_to_probs(self, logits, return_class_probs, **kwargs):\n        softmax = torch.nn.Softmax(dim=1)\n        probs = softmax(logits)\n        if return_class_probs:\n            probs = probs\n        else:\n            probs = torch.max(probs, dim=1)[0]\n        probs = probs.cpu().numpy()\n        return probs\n\n    def logits_to_preds(self, logits, **kwargs):\n        logits = logits.cpu().numpy()\n        pred_ids = logits.argmax(1)\n        preds = [self.label_list[int(x)] for x in pred_ids]\n        return preds\n\n    def prepare_labels(self, **kwargs):\n        label_ids = kwargs.get(self.label_tensor_name)\n        label_ids = label_ids.cpu().numpy()\n        # This is the standard doc classification case\n        try:\n            labels = [self.label_list[int(x)] for x in label_ids]\n        # This case is triggered in Natural Questions where each example can have multiple labels\n        except TypeError:\n            labels = [self.label_list[int(x[0])] for x in label_ids]\n        return labels\n\n    def formatted_preds(self, logits=None, preds_p=None, samples=None, return_class_probs=False, **kwargs):\n        """""" Like QuestionAnsweringHead.formatted_preds(), this fn can operate on either logits or preds_p. This\n        is needed since at inference, the order of operations is very different depending on whether we are performing\n        aggregation or not (compare Inferencer._get_predictions() vs Inferencer._get_predictions_and_aggregate())\n\n        TODO: Preds_p should be renamed to preds""""""\n\n        assert (logits is not None) or (preds_p is not None)\n\n        # When this method is used along side a QAHead at inference (e.g. Natural Questions), preds_p is the input and\n        # there is currently no good way of generating probs\n        if logits is not None:\n            preds_p = self.logits_to_preds(logits)\n            probs = self.logits_to_probs(logits, return_class_probs)\n        else:\n            probs = [None] * len(preds_p)\n\n        # TODO this block has to do with the difference in Basket and Sample structure between SQuAD and NQ\n        try:\n            contexts = [sample.clear_text[""text""] for sample in samples]\n        # This case covers Natural Questions where the sample is in a QA style\n        except KeyError:\n            contexts = [sample.clear_text[""question_text""] + "" | "" + sample.clear_text[""passage_text""] for sample in samples]\n\n        contexts_b = [sample.clear_text[""text_b""] for sample in samples if ""text_b"" in  sample.clear_text]\n        if len(contexts_b) != 0:\n            contexts = [""|"".join([a, b]) for a,b in zip(contexts, contexts_b)]\n\n        assert len(preds_p) == len(probs) == len(contexts)\n\n        res = {""task"": ""text_classification"", ""predictions"": []}\n        for pred, prob, context in zip(preds_p, probs, contexts):\n            if not return_class_probs:\n                pred_dict = {\n                    ""start"": None,\n                    ""end"": None,\n                    ""context"": f""{context}"",\n                    ""label"": f""{pred}"",\n                    ""probability"": prob,\n                }\n            else:\n                pred_dict = {\n                    ""start"": None,\n                    ""end"": None,\n                    ""context"": f""{context}"",\n                    ""label"": ""class_probabilities"",\n                    ""probability"": prob,\n                }\n\n            res[""predictions""].append(pred_dict)\n        return res\n\n\nclass MultiLabelTextClassificationHead(PredictionHead):\n    def __init__(\n        self,\n        layer_dims=None,\n        num_labels=None,\n        class_weights=None,\n        loss_reduction=""none"",\n        task_name=""text_classification"",\n        pred_threshold=0.5,\n        **kwargs,\n    ):\n        """"""\n        :param layer_dims: The size of the layers in the feed forward component. The feed forward will have as many layers as there are ints in this list. This param will be deprecated in future\n        :type layer_dims: list\n        :param num_labels: The numbers of labels. Use to set the size of the final layer in the feed forward component. It is recommended to only set num_labels or layer_dims, not both.\n        :type num_labels: int\n        :param class_weights:\n        :param loss_reduction:\n        :param task_name:\n        :param pred_threshold:\n        :param kwargs:\n        """"""\n        super(MultiLabelTextClassificationHead, self).__init__()\n        # num_labels could in most cases also be automatically retrieved from the data processor\n        if layer_dims:\n            self.layer_dims = layer_dims\n            logger.warning(""`layer_dims` will be deprecated in future releases"")\n        elif num_labels:\n            self.layer_dims = [768, num_labels]\n        else:\n            raise ValueError(""Please supply `num_labels` to define output dim of prediction head"")\n        self.num_labels = self.layer_dims[-1]\n        logger.info(f""Prediction head initialized with size {self.layer_dims}"")\n        self.feed_forward = FeedForwardBlock(self.layer_dims)\n        self.ph_output_type = ""per_sequence""\n        self.model_type = ""multilabel_text_classification""\n        self.task_name = task_name #used for connecting with the right output of the processor\n        self.class_weights = class_weights\n        self.pred_threshold = pred_threshold\n\n        if class_weights is not None:\n            logger.info(f""Using class weights for task \'{self.task_name}\': {self.class_weights}"")\n            #TODO must balanced weight really be a instance attribute?\n            self.balanced_weights = nn.Parameter(\n                torch.tensor(class_weights), requires_grad=False\n            )\n        else:\n            self.balanced_weights = None\n\n        self.loss_fct = BCEWithLogitsLoss(pos_weight=self.balanced_weights,\n                                          reduction=loss_reduction)\n\n        self.generate_config()\n\n    def forward(self, X):\n        logits = self.feed_forward(X)\n        return logits\n\n    def logits_to_loss(self, logits, **kwargs):\n        label_ids = kwargs.get(self.label_tensor_name).to(dtype=torch.float)\n        loss = self.loss_fct(logits.view(-1, self.num_labels), label_ids.view(-1, self.num_labels))\n        per_sample_loss = loss.mean(1)\n        return per_sample_loss\n\n    def logits_to_probs(self, logits, **kwargs):\n        sigmoid = torch.nn.Sigmoid()\n        probs = sigmoid(logits)\n        probs = probs.cpu().numpy()\n        return probs\n\n    def logits_to_preds(self, logits, **kwargs):\n        probs = self.logits_to_probs(logits)\n        #TODO we could potentially move this to GPU to speed it up\n        pred_ids = [np.where(row > self.pred_threshold)[0] for row in probs]\n        preds = []\n        for row in pred_ids:\n            preds.append([self.label_list[int(x)] for x in row])\n        return preds\n\n    def prepare_labels(self, **kwargs):\n        label_ids = kwargs.get(self.label_tensor_name)\n        label_ids = label_ids.cpu().numpy()\n        label_ids = [np.where(row == 1)[0] for row in label_ids]\n        labels = []\n        for row in label_ids:\n            labels.append([self.label_list[int(x)] for x in row])\n        return labels\n\n    def formatted_preds(self, logits, samples, **kwargs):\n        preds = self.logits_to_preds(logits)\n        probs = self.logits_to_probs(logits)\n        contexts = [sample.clear_text[""text""] for sample in samples]\n\n        assert len(preds) == len(probs) == len(contexts)\n\n        res = {""task"": ""text_classification"", ""predictions"": []}\n        for pred, prob, context in zip(preds, probs, contexts):\n            res[""predictions""].append(\n                {\n                    ""start"": None,\n                    ""end"": None,\n                    ""context"": f""{context}"",\n                    ""label"": f""{pred}"",\n                    ""probability"": prob,\n                }\n            )\n        return res\n\n\nclass TokenClassificationHead(PredictionHead):\n    def __init__(self,\n                 layer_dims=None,\n                 num_labels=None,\n                 task_name=""ner"",\n                 **kwargs):\n        """"""\n        :param layer_dims: The size of the layers in the feed forward component. The feed forward will have as many layers as there are ints in this list. This param will be deprecated in future\n        :type layer_dims: list\n        :param num_labels: The numbers of labels. Use to set the size of the final layer in the feed forward component. It is recommended to only set num_labels or layer_dims, not both.\n        :type num_labels: int\n        :param task_name:\n        :param kwargs:\n        """"""\n        super(TokenClassificationHead, self).__init__()\n        if layer_dims:\n            self.layer_dims = layer_dims\n            logger.warning(""`layer_dims` will be deprecated in future releases"")\n        elif num_labels:\n            self.layer_dims = [768, num_labels]\n        else:\n            raise ValueError(""Please supply `num_labels` to define output dim of prediction head"")\n        self.num_labels = self.layer_dims[-1]\n        logger.info(f""Prediction head initialized with size {self.layer_dims}"")\n        self.feed_forward = FeedForwardBlock(self.layer_dims)\n        self.num_labels = self.layer_dims[-1]\n        self.loss_fct = CrossEntropyLoss(reduction=""none"")\n        self.ph_output_type = ""per_token""\n        self.model_type = ""token_classification""\n        self.task_name = task_name\n        self.generate_config()\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path):\n        """"""\n        Load a prediction head from a saved FARM or transformers model. `pretrained_model_name_or_path`\n        can be one of the following:\n        a) Local path to a FARM prediction head config (e.g. my-bert/prediction_head_0_config.json)\n        b) Local path to a Transformers model (e.g. my-bert)\n        c) Name of a public model from https://huggingface.co/models (e.g.bert-base-cased-finetuned-conll03-english)\n\n\n        :param pretrained_model_name_or_path: local path of a saved model or name of a publicly available model.\n                                              Exemplary public names:\n                                              - bert-base-cased-finetuned-conll03-english\n\n                                              See https://huggingface.co/models for full list\n\n        """"""\n\n        if os.path.exists(pretrained_model_name_or_path) \\\n                and ""config.json"" in pretrained_model_name_or_path \\\n                and ""prediction_head"" in pretrained_model_name_or_path:\n            # a) FARM style\n            head = super(TokenClassificationHead, cls).load(pretrained_model_name_or_path)\n        else:\n            # b) transformers style\n            # load all weights from model\n            full_model = AutoModelForTokenClassification.from_pretrained(pretrained_model_name_or_path)\n            # init empty head\n            head = cls(layer_dims=[full_model.config.hidden_size, len(full_model.config.label2id)])\n            # transfer weights for head from full model\n            head.feed_forward.feed_forward[0].load_state_dict(full_model.classifier.state_dict())\n            del full_model\n        return head\n\n\n    def forward(self, X):\n        logits = self.feed_forward(X)\n        return logits\n\n    def logits_to_loss(\n        self, logits, initial_mask, padding_mask=None, **kwargs\n    ):\n        label_ids = kwargs.get(self.label_tensor_name)\n\n        # Todo: should we be applying initial mask here? Loss is currently calculated even on non initial tokens\n        active_loss = padding_mask.view(-1) == 1\n        active_logits = logits.view(-1, self.num_labels)[active_loss]\n        active_labels = label_ids.view(-1)[active_loss]\n\n        loss = self.loss_fct(\n            active_logits, active_labels\n        )  # loss is a 1 dimemnsional (active) token loss\n        return loss\n\n    def logits_to_preds(self, logits, initial_mask, **kwargs):\n        preds_word_all = []\n        preds_tokens = torch.argmax(logits, dim=2)\n        preds_token = preds_tokens.detach().cpu().numpy()\n        # used to be: padding_mask = padding_mask.detach().cpu().numpy()\n        initial_mask = initial_mask.detach().cpu().numpy()\n\n        for idx, im in enumerate(initial_mask):\n            preds_t = preds_token[idx]\n            # Get labels and predictions for just the word initial tokens\n            preds_word_id = self.initial_token_only(preds_t, initial_mask=im)\n            preds_word = [self.label_list[pwi] for pwi in preds_word_id]\n            preds_word_all.append(preds_word)\n        return preds_word_all\n\n    def logits_to_probs(self, logits, initial_mask, return_class_probs, **kwargs):\n        # get per token probs\n        softmax = torch.nn.Softmax(dim=2)\n        token_probs = softmax(logits)\n        if return_class_probs:\n            token_probs = token_probs\n        else:\n            token_probs = torch.max(token_probs, dim=2)[0]\n        token_probs = token_probs.cpu().numpy()\n\n        # convert to per word probs\n        all_probs = []\n        initial_mask = initial_mask.detach().cpu().numpy()\n        for idx, im in enumerate(initial_mask):\n            probs_t = token_probs[idx]\n            probs_words = self.initial_token_only(probs_t, initial_mask=im)\n            all_probs.append(probs_words)\n        return all_probs\n\n    def prepare_labels(self, initial_mask, **kwargs):\n        label_ids = kwargs.get(self.label_tensor_name)\n        labels_all = []\n        label_ids = label_ids.cpu().numpy()\n        for label_ids_one_sample, initial_mask_one_sample in zip(\n            label_ids, initial_mask\n        ):\n            label_ids = self.initial_token_only(\n                label_ids_one_sample, initial_mask_one_sample\n            )\n            labels = [self.label_list[l] for l in label_ids]\n            labels_all.append(labels)\n        return labels_all\n\n    @staticmethod\n    def initial_token_only(seq, initial_mask):\n        ret = []\n        for init, s in zip(initial_mask, seq):\n            if init:\n                ret.append(s)\n        return ret\n\n    def formatted_preds(self, logits, initial_mask, samples, return_class_probs=False, **kwargs):\n        preds = self.logits_to_preds(logits, initial_mask)\n        probs = self.logits_to_probs(logits, initial_mask,return_class_probs)\n\n        # align back with original input by getting the original word spans\n        spans = []\n        for sample, sample_preds in zip(samples, preds):\n            word_spans = []\n            span = None\n            for token, offset, start_of_word in zip(\n                sample.tokenized[""tokens""],\n                sample.tokenized[""offsets""],\n                sample.tokenized[""start_of_word""],\n            ):\n                if start_of_word:\n                    # previous word has ended unless it\'s the very first word\n                    if span is not None:\n                        word_spans.append(span)\n                    span = {""start"": offset, ""end"": offset + len(token)}\n                else:\n                    # expand the span to include the subword-token\n                    span[""end""] = offset + len(token.replace(""##"", """"))\n            word_spans.append(span)\n            spans.append(word_spans)\n\n        assert len(preds) == len(probs) == len(spans)\n\n        res = {""task"": ""ner"", ""predictions"": []}\n        for preds_seq, probs_seq, sample, spans_seq in zip(\n            preds, probs, samples, spans\n        ):\n            tags, spans_seq = convert_iob_to_simple_tags(preds_seq, spans_seq)\n            seq_res = []\n            for tag, prob, span in zip(tags, probs_seq, spans_seq):\n                context = sample.clear_text[""text""][span[""start""] : span[""end""]]\n                seq_res.append(\n                    {\n                        ""start"": span[""start""],\n                        ""end"": span[""end""],\n                        ""context"": f""{context}"",\n                        ""label"": f""{tag}"",\n                        ""probability"": prob,\n                    }\n                )\n            res[""predictions""].extend(seq_res)\n        return res\n\n\nclass BertLMHead(PredictionHead):\n    def __init__(self, hidden_size, vocab_size, hidden_act=""gelu"", task_name=""lm"", **kwargs):\n        super(BertLMHead, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.hidden_act = hidden_act\n        self.vocab_size = vocab_size\n        self.loss_fct = CrossEntropyLoss(reduction=""none"", ignore_index=-1)\n        self.num_labels = vocab_size  # vocab size\n        # TODO Check if weight init needed!\n        # self.apply(self.init_bert_weights)\n        self.ph_output_type = ""per_token""\n\n        self.model_type = ""language_modelling""\n        self.task_name = task_name\n        self.generate_config()\n\n        # NN Layers\n        # this is the ""transform"" module in the pytorch-transformers repo\n        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n        self.transform_act_fn = ACT2FN[self.hidden_act]\n        self.LayerNorm = BertLayerNorm(self.hidden_size, eps=1e-12)\n\n        # this is the ""decoder"" in the pytorch-transformers repo\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(hidden_size,\n                                 vocab_size,\n                                 bias=False)\n        self.bias = nn.Parameter(torch.zeros(vocab_size))\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path, n_added_tokens=0):\n        """"""\n        Load a prediction head from a saved FARM or transformers model. `pretrained_model_name_or_path`\n        can be one of the following:\n        a) Local path to a FARM prediction head config (e.g. my-bert/prediction_head_0_config.json)\n        b) Local path to a Transformers model (e.g. my-bert)\n        c) Name of a public model from https://huggingface.co/models (e.g.bert-base-cased)\n\n\n        :param pretrained_model_name_or_path: local path of a saved model or name of a publicly available model.\n                                              Exemplary public names:\n                                              - bert-base-cased\n\n                                              See https://huggingface.co/models for full list\n\n        """"""\n\n        if os.path.exists(pretrained_model_name_or_path) \\\n                and ""config.json"" in pretrained_model_name_or_path \\\n                and ""prediction_head"" in pretrained_model_name_or_path:\n            # a) FARM style\n            if n_added_tokens != 0:\n                #TODO resize prediction head decoder for custom vocab\n                raise NotImplementedError(""Custom vocab not yet supported for model loading from FARM files"")\n\n            head = super(BertLMHead, cls).load(pretrained_model_name_or_path)\n        else:\n            # b) pytorch-transformers style\n            # load weights from bert model\n            # (we might change this later to load directly from a state_dict to generalize for other language models)\n            bert_with_lm = BertForPreTraining.from_pretrained(pretrained_model_name_or_path)\n\n            # init empty head\n            vocab_size = bert_with_lm.config.vocab_size + n_added_tokens\n\n            head = cls(hidden_size=bert_with_lm.config.hidden_size,\n                       vocab_size=vocab_size,\n                       hidden_act=bert_with_lm.config.hidden_act)\n\n            # load weights\n            head.dense.load_state_dict(bert_with_lm.cls.predictions.transform.dense.state_dict())\n            head.LayerNorm.load_state_dict(bert_with_lm.cls.predictions.transform.LayerNorm.state_dict())\n\n            # Not loading weights of decoder here, since we later share weights with the embedding layer of LM\n            #head.decoder.load_state_dict(bert_with_lm.cls.predictions.decoder.state_dict())\n\n            if n_added_tokens == 0:\n                bias_params = bert_with_lm.cls.predictions.bias\n            else:\n                # Custom vocab => larger vocab => larger dims of output layer in the LM head\n                bias_params = torch.nn.Parameter(torch.cat([bert_with_lm.cls.predictions.bias,\n                                                            torch.zeros(n_added_tokens)]))\n            head.bias.data.copy_(bias_params)\n            del bert_with_lm\n            del bias_params\n\n        return head\n\n    def set_shared_weights(self, shared_embedding_weights):\n        self.decoder.weight = shared_embedding_weights\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        lm_logits = self.decoder(hidden_states) + self.bias\n        return lm_logits\n\n    def logits_to_loss(self, logits, **kwargs):\n        lm_label_ids = kwargs.get(self.label_tensor_name)\n        batch_size = lm_label_ids.shape[0]\n        masked_lm_loss = self.loss_fct(\n            logits.view(-1, self.num_labels), lm_label_ids.view(-1)\n        )\n        per_sample_loss = masked_lm_loss.view(-1, batch_size).mean(dim=0)\n        return per_sample_loss\n\n    def logits_to_preds(self, logits, **kwargs):\n        lm_label_ids = kwargs.get(self.label_tensor_name).cpu().numpy()\n        lm_preds_ids = logits.argmax(2).cpu().numpy()\n        # apply mask to get rid of predictions for non-masked tokens\n        assert lm_preds_ids.shape == lm_label_ids.shape\n        lm_preds_ids[lm_label_ids == -1] = -1\n        lm_preds_ids = lm_preds_ids.tolist()\n        preds = []\n        # we have a batch of sequences here. we need to convert for each token in each sequence.\n        for pred_ids_for_sequence in lm_preds_ids:\n            preds.append(\n                [self.label_list[int(x)] for x in pred_ids_for_sequence if int(x) != -1]\n            )\n        return preds\n\n    def prepare_labels(self, **kwargs):\n        label_ids = kwargs.get(self.label_tensor_name)\n        label_ids = label_ids.cpu().numpy().tolist()\n        labels = []\n        # we have a batch of sequences here. we need to convert for each token in each sequence.\n        for ids_for_sequence in label_ids:\n            labels.append([self.label_list[int(x)] for x in ids_for_sequence if int(x) != -1])\n        return labels\n\n\nclass NextSentenceHead(TextClassificationHead):\n    """"""\n    Almost identical to a TextClassificationHead. Only difference: we can load the weights from\n     a pretrained language model that was saved in the Transformers style (all in one model).\n    """"""\n    @classmethod\n    def load(cls, pretrained_model_name_or_path):\n        """"""\n        Load a prediction head from a saved FARM or transformers model. `pretrained_model_name_or_path`\n        can be one of the following:\n        a) Local path to a FARM prediction head config (e.g. my-bert/prediction_head_0_config.json)\n        b) Local path to a Transformers model (e.g. my-bert)\n        c) Name of a public model from https://huggingface.co/models (e.g.bert-base-cased)\n\n\n        :param pretrained_model_name_or_path: local path of a saved model or name of a publicly available model.\n                                              Exemplary public names:\n                                              - bert-base-cased\n\n                                              See https://huggingface.co/models for full list\n\n        """"""\n        if os.path.exists(pretrained_model_name_or_path) \\\n                and ""config.json"" in pretrained_model_name_or_path \\\n                and ""prediction_head"" in pretrained_model_name_or_path:\n            # a) FARM style\n            head = super(NextSentenceHead, cls).load(pretrained_model_name_or_path)\n        else:\n            # b) pytorch-transformers style\n            # load weights from bert model\n            # (we might change this later to load directly from a state_dict to generalize for other language models)\n            bert_with_lm = BertForPreTraining.from_pretrained(pretrained_model_name_or_path)\n\n            # init empty head\n            head = cls(layer_dims=[bert_with_lm.config.hidden_size, 2], loss_ignore_index=-1, task_name=""nextsentence"")\n\n            # load weights\n            head.feed_forward.feed_forward[0].load_state_dict(bert_with_lm.cls.seq_relationship.state_dict())\n            del bert_with_lm\n\n        return head\n\nclass FeedForwardBlock(nn.Module):\n    """""" A feed forward neural network of variable depth and width. """"""\n\n    def __init__(self, layer_dims, **kwargs):\n        # Todo: Consider having just one input argument\n        super(FeedForwardBlock, self).__init__()\n        self.layer_dims = layer_dims\n        # If read from config the input will be string\n        n_layers = len(layer_dims) - 1\n        layers_all = []\n        # TODO: IS this needed?\n        self.output_size = layer_dims[-1]\n\n        for i in range(n_layers):\n            size_in = layer_dims[i]\n            size_out = layer_dims[i + 1]\n            layer = nn.Linear(size_in, size_out)\n            layers_all.append(layer)\n        self.feed_forward = nn.Sequential(*layers_all)\n\n    def forward(self, X):\n        logits = self.feed_forward(X)\n        return logits\n\n\nclass QuestionAnsweringHead(PredictionHead):\n    """"""\n    A question answering head predicts the start and end of the answer on token level.\n    """"""\n\n    def __init__(self, layer_dims=[768,2],\n                 task_name=""question_answering"",\n                 no_ans_boost=0.0,\n                 context_window_size=100,\n                 n_best=5,\n                 n_best_per_sample=1,\n                 **kwargs):\n        """"""\n        :param layer_dims: dimensions of Feed Forward block, e.g. [768,2], for adjusting to BERT embedding. Output should be always 2\n        :type layer_dims: List[Int]\n        :param kwargs: placeholder for passing generic parameters\n        :type kwargs: object\n        :param no_ans_boost: How much the no_answer logit is boosted/increased.\n                             The higher the value, the more likely a ""no answer possible given the input text"" is returned by the model\n        :type no_ans_boost: float\n        :param context_window_size: The size, in characters, of the window around the answer span that is used when displaying the context around the answer.\n        :type context_window_size: int\n        :param n_best: The number of positive answer spans for each document.\n        :type n_best: int\n        :param n_best_per_sample: num candidate answer spans to consider from each passage. Each passage also returns ""no answer"" info.\n                                  This is decoupled from n_best on document level, since predictions on passage level are very similar.\n                                  It should have a low value\n        :type n_best_per_sample: int\n        """"""\n        super(QuestionAnsweringHead, self).__init__()\n        if kwargs is not None:\n            logger.warning(f""Some unused parameters are passed to the QuestionAnsweringHead. ""\n                           f""Might not be a problem. Params: {json.dumps(kwargs)}"")\n        self.layer_dims = layer_dims\n        assert self.layer_dims[-1] == 2\n        self.feed_forward = FeedForwardBlock(self.layer_dims)\n        logger.info(f""Prediction head initialized with size {self.layer_dims}"")\n        self.num_labels = self.layer_dims[-1]\n        self.ph_output_type = ""per_token_squad""\n        self.model_type = (""span_classification"")  # predicts start and end token of answer\n        self.task_name = task_name\n        self.no_ans_boost = no_ans_boost\n        self.context_window_size = context_window_size\n        self.n_best = n_best\n        self.n_best_per_sample = n_best_per_sample\n        self.generate_config()\n\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path):\n        """"""\n        Load a prediction head from a saved FARM or transformers model. `pretrained_model_name_or_path`\n        can be one of the following:\n        a) Local path to a FARM prediction head config (e.g. my-bert/prediction_head_0_config.json)\n        b) Local path to a Transformers model (e.g. my-bert)\n        c) Name of a public model from https://huggingface.co/models (e.g. distilbert-base-uncased-distilled-squad)\n\n\n        :param pretrained_model_name_or_path: local path of a saved model or name of a publicly available model.\n                                              Exemplary public names:\n                                              - distilbert-base-uncased-distilled-squad\n                                              - bert-large-uncased-whole-word-masking-finetuned-squad\n\n                                              See https://huggingface.co/models for full list\n\n        """"""\n\n        if os.path.exists(pretrained_model_name_or_path) \\\n                and ""config.json"" in pretrained_model_name_or_path \\\n                and ""prediction_head"" in pretrained_model_name_or_path:\n            # a) FARM style\n            super(QuestionAnsweringHead, cls).load(pretrained_model_name_or_path)\n        else:\n            # b) transformers style\n            # load all weights from model\n            full_qa_model = AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name_or_path)\n            # init empty head\n            head = cls(layer_dims=[full_qa_model.config.hidden_size, 2], loss_ignore_index=-1, task_name=""question_answering"")\n            # transfer weights for head from full model\n            head.feed_forward.feed_forward[0].load_state_dict(full_qa_model.qa_outputs.state_dict())\n            del full_qa_model\n\n        return head\n\n    def forward(self, X):\n        """"""\n        One forward pass through the prediction head model, starting with language model output on token level\n\n        """"""\n        logits = self.feed_forward(X)\n        return logits\n\n    def logits_to_loss(self, logits, labels, **kwargs):\n        """"""\n        Combine predictions and labels to a per sample loss.\n        """"""\n        # todo explain how we only use first answer for train\n        # labels.shape =  [batch_size, n_max_answers, 2]. n_max_answers is by default 6 since this is the\n        # most that occurs in the SQuAD dev set. The 2 in the final dimension corresponds to [start, end]\n        start_position = labels[:, 0, 0]\n        end_position = labels[:, 0, 1]\n\n        # logits is of shape [batch_size, max_seq_len, 2]. Like above, the final dimension corresponds to [start, end]\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        # Squeeze final singleton dimensions\n        if len(start_position.size()) > 1:\n            start_position = start_position.squeeze(-1)\n        if len(end_position.size()) > 1:\n            end_position = end_position.squeeze(-1)\n\n        ignored_index = start_logits.size(1)\n        start_position.clamp_(0, ignored_index)\n        end_position.clamp_(0, ignored_index)\n\n        loss_fct = CrossEntropyLoss(reduction=""none"")\n        start_loss = loss_fct(start_logits, start_position)\n        end_loss = loss_fct(end_logits, end_position)\n        per_sample_loss = (start_loss + end_loss) / 2\n        return per_sample_loss\n\n    def logits_to_preds(self, logits, padding_mask, start_of_word, seq_2_start_t, max_answer_length=1000, **kwargs):\n        """"""\n        Get the predicted index of start and end token of the answer. Note that the output is at token level\n        and not word level. Note also that these logits correspond to the tokens of a sample\n        (i.e. special tokens, question tokens, passage_tokens)\n        """"""\n\n        # Will be populated with the top-n predictions of each sample in the batch\n        # shape = batch_size x ~top_n\n        # Note that ~top_n = n   if no_answer is     within the top_n predictions\n        #           ~top_n = n+1 if no_answer is not within the top_n predictions\n        all_top_n = []\n\n        # logits is of shape [batch_size, max_seq_len, 2]. The final dimension corresponds to [start, end]\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        # Calculate a few useful variables\n        batch_size = start_logits.size()[0]\n        max_seq_len = start_logits.shape[1] # target dim\n        n_non_padding = torch.sum(padding_mask, dim=1)\n\n        # get scores for all combinations of start and end logits => candidate answers\n        start_matrix = start_logits.unsqueeze(2).expand(-1, -1, max_seq_len)\n        end_matrix = end_logits.unsqueeze(1).expand(-1, max_seq_len, -1)\n        start_end_matrix = start_matrix + end_matrix\n\n        # disqualify answers where end < start\n        # (set the lower triangular matrix to low value, excluding diagonal)\n        indices = torch.tril_indices(max_seq_len, max_seq_len, offset=-1, device=start_end_matrix.device)\n        start_end_matrix[:, indices[0][:], indices[1][:]] = -999\n\n        # disqualify answers where start=0, but end != 0\n        start_end_matrix[:, 0, 1:] = -999\n\n        # TODO continue vectorization of valid_answer_idxs\n        # # disqualify where answers < seq_2_start_t and idx != 0\n        # # disqualify where answer falls into padding\n        # # seq_2_start_t can be different when 2 different questions are handled within one batch\n        # # n_non_padding can be different on sample level, too\n        # for i in range(batch_size):\n        #     start_end_matrix[i, 1:seq_2_start_t[i], 1:seq_2_start_t[i]] = -888\n        #     start_end_matrix[i, n_non_padding[i]-1:, n_non_padding[i]-1:] = -777\n\n\n        # Sort the candidate answers by their score. Sorting happens on the flattened matrix.\n        # flat_sorted_indices.shape: (batch_size, max_seq_len^2, 1)\n        flat_scores = start_end_matrix.view(batch_size, -1)\n        flat_sorted_indices_2d = flat_scores.sort(descending=True)[1]\n        flat_sorted_indices = flat_sorted_indices_2d.unsqueeze(2)\n\n        # The returned indices are then converted back to the original dimensionality of the matrix.\n        # sorted_candidates.shape : (batch_size, max_seq_len^2, 2)\n        start_indices = flat_sorted_indices // max_seq_len\n        end_indices = flat_sorted_indices % max_seq_len\n        sorted_candidates = torch.cat((start_indices, end_indices), dim=2)\n\n        # Get the n_best candidate answers for each sample that are valid (via some heuristic checks)\n        for sample_idx in range(batch_size):\n            sample_top_n = self.get_top_candidates(sorted_candidates[sample_idx],\n                                                           start_end_matrix[sample_idx],\n                                                           n_non_padding[sample_idx].item(),\n                                                           max_answer_length,\n                                                           seq_2_start_t[sample_idx].item())\n            all_top_n.append(sample_top_n)\n\n        return all_top_n\n\n    def get_top_candidates(self, sorted_candidates, start_end_matrix,\n                           n_non_padding, max_answer_length, seq_2_start_t):\n        """""" Returns top candidate answers as a list of Span objects. Operates on a matrix of summed start and end logits.\n        This matrix corresponds to a single sample (includes special tokens, question tokens, passage tokens).\n        This method always returns a list of len n_best + 1 (it is comprised of the n_best positive answers along with the one no_answer)""""""\n\n        # Initialize some variables\n        top_candidates = []\n        n_candidates = sorted_candidates.shape[0]\n\n        # Iterate over all candidates and break when we have all our n_best candidates\n        for candidate_idx in range(n_candidates):\n            if len(top_candidates) == self.n_best_per_sample:\n                break\n            else:\n                # Retrieve candidate\'s indices\n                start_idx = sorted_candidates[candidate_idx, 0].item()\n                end_idx = sorted_candidates[candidate_idx, 1].item()\n                # Ignore no_answer scores which will be extracted later in this method\n                if start_idx == 0 and end_idx == 0:\n                    continue\n                # Check that the candidate\'s indices are valid and save them if they are\n                if self.valid_answer_idxs(start_idx, end_idx, n_non_padding, max_answer_length, seq_2_start_t):\n                    score = start_end_matrix[start_idx, end_idx].item()\n                    top_candidates.append(Span(start_idx, end_idx, score, unit=""token"", level=""passage""))\n\n        no_answer_score = start_end_matrix[0, 0].item()\n        top_candidates.append(Span(0, 0, no_answer_score, unit=""token"", pred_str="""", level=""passage""))\n\n        return top_candidates\n\n    @staticmethod\n    def valid_answer_idxs(start_idx, end_idx, n_non_padding, max_answer_length, seq_2_start_t):\n        """""" Returns True if the supplied index span is a valid prediction. The indices being provided\n        should be on sample/passage level (special tokens + question_tokens + passag_tokens)\n        and not document level""""""\n\n        # This function can seriously slow down inferencing and eval. In the future this function will be completely vectorized\n        # Continue if start or end label points to a padding token\n        if start_idx < seq_2_start_t and start_idx != 0:\n            return False\n        if end_idx < seq_2_start_t and end_idx != 0:\n            return False\n        # The -1 is to stop the idx falling on a final special token\n        # TODO: this makes the assumption that there is a special token that comes at the end of the second sequence\n        if start_idx >= n_non_padding - 1:\n            return False\n        if end_idx >= n_non_padding - 1:\n            return False\n\n        # # Check if start comes after end\n        # # Handled on matrix level by: start_end_matrix[:, indices[0][1:], indices[1][1:]] = -999\n        # if end_idx < start_idx:\n        #     return False\n\n        # # If one of the two indices is 0, the other must also be 0\n        # # Handled on matrix level by setting: start_end_matrix[:, 0, 1:] = -999\n        # if start_idx == 0 and end_idx != 0:\n        #     return False\n        # if start_idx != 0 and end_idx == 0:\n        #     return False\n\n        length = end_idx - start_idx + 1\n        if length > max_answer_length:\n            return False\n        return True\n\n    def formatted_preds(self, logits=None, preds_p=None, baskets=None, **kwargs):\n        """""" Takes a list of predictions, each corresponding to one sample, and converts them into document level\n        predictions. Leverages information in the SampleBaskets. Assumes that we are being passed predictions from\n        ALL samples in the one SampleBasket i.e. all passages of a document. Logits should be None, because we have\n        already converted the logits to predictions before calling formatted_preds\n        (see Inferencer._get_predictions_and_aggregate()).\n        """"""\n\n        # Unpack some useful variables\n        # passage_start_t is the token index of the passage relative to the document (usually a multiple of doc_stride)\n        # seq_2_start_t is the token index of the first token in passage relative to the input sequence (i.e. number of\n        # special tokens and question tokens that come before the passage tokens)\n        assert logits is None, ""Logits are not None, something is passed wrongly into formatted_preds() in infer.py""\n        assert preds_p is not None, ""No preds_p passed to formatted_preds()""\n        samples = [s for b in baskets for s in b.samples]\n        ids = [s.id.split(""-"") for s in samples]\n        passage_start_t = [s.features[0][""passage_start_t""] for s in samples]\n        seq_2_start_t = [s.features[0][""seq_2_start_t""] for s in samples]\n\n        # Aggregate passage level predictions to create document level predictions.\n        # This method assumes that all passages of each document are contained in preds_p\n        # i.e. that there are no incomplete documents. The output of this step\n        # are prediction spans\n        preds_d = self.aggregate_preds(preds_p, passage_start_t, ids, seq_2_start_t)\n\n        assert len(preds_d) == len(baskets)\n\n        # Separate top_preds list from the no_ans_gap float.\n        top_preds, no_ans_gaps = zip(*preds_d)\n\n        # Takes document level prediction spans and returns string predictions\n        doc_preds = self.to_doc_preds(top_preds, no_ans_gaps, baskets)\n\n        return doc_preds\n\n    def to_doc_preds(self, top_preds, no_ans_gaps, baskets):\n        """""" Groups Span objects together in a DocumentPred object  """"""\n        ret = []\n\n        # Iterate over each set of document level prediction\n        for pred_d, no_ans_gap, basket in zip(top_preds, no_ans_gaps, baskets):\n            # TODO the follow try catch is because of difference in Basket structure between NQ and SQuAD - resolve this!!!\n            # TODO This code is horrible - will be cleaned soon\n\n            # Unpack document offsets, clear text and squad_id\n            try:\n                token_offsets = basket.samples[0].tokenized[""document_offsets""] # NQ style\n            except KeyError:\n                token_offsets = basket.raw[""document_offsets""]                  # SQuAD style\n\n            try:\n                document_text = basket.raw[""context""]       # SQuAD style\n            except KeyError:\n                try:\n                    document_text = basket.raw[""text""] # NQ style\n                except KeyError:\n                    document_text = basket.raw[""document_text""]\n\n            try:\n                question = basket.raw[""questions""][0]  # SQuAD style\n            except KeyError:\n                try:\n                    question = basket.raw[""qas""][0]         # NQ style\n                except KeyError:\n                    question = basket.raw[""question_text""]\n\n            try:\n                question_id = basket.raw[""squad_id""]\n            except KeyError:\n                question_id = None # TODO add NQ id here\n\n            basket_id = basket.id\n\n            # Iterate over each prediction on the one document\n            full_preds = []\n            for span, basket in zip(pred_d, baskets):\n                # This should be a method of Span\n                pred_str, _, _ = span_to_string(span.start, span.end, token_offsets, document_text)\n                span.pred_str = pred_str\n                full_preds.append(span)\n            curr_doc_pred = DocumentPred(id=basket_id,\n                                         preds=full_preds,\n                                         document_text=document_text,\n                                         question=question,\n                                         no_ans_gap=no_ans_gap,\n                                         token_offsets=token_offsets,\n                                         context_window_size=self.context_window_size,\n                                         question_id=question_id)\n            ret.append(curr_doc_pred)\n        return ret\n\n    def has_no_answer_idxs(self, sample_top_n):\n        for start, end, score in sample_top_n:\n            if start == 0 and end == 0:\n                return True\n        return False\n\n    def aggregate_preds(self, preds, passage_start_t, ids, seq_2_start_t=None, labels=None):\n        """""" Aggregate passage level predictions to create document level predictions.\n        This method assumes that all passages of each document are contained in preds\n        i.e. that there are no incomplete documents. The output of this step\n        are prediction spans. No answer is represented by a (-1, -1) span on the document level """"""\n\n        # Initialize some variables\n        n_samples = len(preds)\n        all_basket_preds = {}\n        all_basket_labels = {}\n\n        # Iterate over the preds of each sample\n        for sample_idx in range(n_samples):\n            id_1, id_2, _ = ids[sample_idx]\n            basket_id = f""{id_1}-{id_2}""\n\n            # curr_passage_start_t is the token offset of the current passage\n            # It will always be a multiple of doc_stride\n            curr_passage_start_t = passage_start_t[sample_idx]\n\n            # This is to account for the fact that all model input sequences start with some special tokens\n            # and also the question tokens before passage tokens.\n            if seq_2_start_t:\n                cur_seq_2_start_t = seq_2_start_t[sample_idx]\n                curr_passage_start_t -= cur_seq_2_start_t\n\n            # Converts the passage level predictions+labels to document level predictions+labels. Note\n            # that on the passage level a no answer is (0,0) but at document level it is (-1,-1) since (0,0)\n            # would refer to the first token of the document\n            pred_d = self.pred_to_doc_idxs(preds[sample_idx], curr_passage_start_t)\n            if labels:\n                label_d = self.label_to_doc_idxs(labels[sample_idx], curr_passage_start_t)\n\n            # Initialize the basket_id as a key in the all_basket_preds and all_basket_labels dictionaries\n            if basket_id not in all_basket_preds:\n                all_basket_preds[basket_id] = []\n                all_basket_labels[basket_id] = []\n\n            # Add predictions and labels to dictionary grouped by their basket_ids\n            all_basket_preds[basket_id].append(pred_d)\n            if labels:\n                all_basket_labels[basket_id].append(label_d)\n\n        # Pick n-best predictions and remove repeated labels\n        all_basket_preds = {k: self.reduce_preds(v) for k, v in all_basket_preds.items()}\n        if labels:\n            all_basket_labels = {k: self.reduce_labels(v) for k, v in all_basket_labels.items()}\n\n        # Return aggregated predictions in order as a list of lists\n        keys = [k for k in all_basket_preds]\n        aggregated_preds = [all_basket_preds[k] for k in keys]\n        if labels:\n            labels = [all_basket_labels[k] for k in keys]\n            return aggregated_preds, labels\n        else:\n            return aggregated_preds\n\n    @staticmethod\n    def reduce_labels(labels):\n        """""" Removes repeat answers. Represents a no answer label as (-1,-1)""""""\n        positive_answers = [(start, end) for x in labels for start, end in x if not (start == -1 and end == -1)]\n        if not positive_answers:\n            return [(-1, -1)]\n        else:\n            return list(set(positive_answers))\n\n    def reduce_preds(self, preds):\n        """""" This function contains the logic for choosing the best answers from each passage. In the end, it\n        returns the n_best predictions on the document level. """"""\n\n        # Initialize variables\n        passage_no_answer = []\n        passage_best_score = []\n        no_answer_scores = []\n        n_samples = len(preds)\n\n        # Iterate over the top predictions for each sample\n        for sample_idx, sample_preds in enumerate(preds):\n            best_pred = sample_preds[0]\n            best_pred_score = best_pred.score\n            no_answer_score = self.get_no_answer_score(sample_preds) + self.no_ans_boost\n            no_answer = no_answer_score > best_pred_score\n            passage_no_answer.append(no_answer)\n            no_answer_scores.append(no_answer_score)\n            passage_best_score.append(best_pred_score)\n\n        # Get all predictions in flattened list and sort by score\n        pos_answers_flat = [\n            Span(span.start, span.end, span.score, sample_idx, n_samples, unit=""token"", level=""passage"")\n            for sample_idx, passage_preds in enumerate(preds)\n            for span in passage_preds\n            if not (span.start == -1 and span.end == -1)\n        ]\n\n        # TODO add switch for more variation in answers, e.g. if varied_ans then never return overlapping answers\n        pos_answer_dedup = self.deduplicate(pos_answers_flat)\n\n        # This is how much no_ans_boost needs to change to turn a no_answer to a positive answer (or vice versa)\n        no_ans_gap = -min([nas - pbs for nas, pbs in zip(no_answer_scores, passage_best_score)])\n\n        # ""no answer"" scores and positive answers scores are difficult to compare, because\n        # + a positive answer score is related to a specific text span\n        # - a ""no answer"" score is related to all input texts\n        # Thus we compute the ""no answer"" score relative to the best possible answer and adjust it by\n        # the most significant difference between scores.\n        # Most significant difference: change top prediction from ""no answer"" to answer (or vice versa)\n        best_overall_positive_score = max(x.score for x in pos_answer_dedup)\n        no_answer_pred = Span(-1, -1, best_overall_positive_score - no_ans_gap, None, n_samples, unit=""token"")\n\n        # Add no answer to positive answers, sort the order and return the n_best\n        n_preds = [no_answer_pred] + pos_answer_dedup\n        n_preds_sorted = sorted(n_preds, key=lambda x: x.score, reverse=True)\n        n_preds_reduced = n_preds_sorted[:self.n_best]\n        return n_preds_reduced, no_ans_gap\n\n    @staticmethod\n    def deduplicate(flat_pos_answers):\n        # Remove duplicate spans that might be twice predicted in two different passages\n        seen = {}\n        for span in flat_pos_answers:\n            if (span.start, span.end) not in seen:\n                seen[(span.start, span.end)] = span\n            else:\n                seen_score = seen[(span.start, span.end)].score\n                if span.score > seen_score:\n                    seen[(span.start, span.end)] = span\n        return [span for span in seen.values()]\n\n\n\n    ## THIS IS A SIMPLER IMPLEMENTATION OF PICKING BEST ANSWERS FOR A DOCUMENT. MATCHES THE HUGGINGFACE METHOD\n    # @staticmethod\n    # def reduce_preds(preds, n_best=5):\n    #     pos_answers = [[(start, end, score) for start, end, score in x if not (start == -1 and end == -1)] for x in preds]\n    #     pos_answer_flat = [x for y in pos_answers for x in y]\n    #     pos_answers_sorted = sorted(pos_answer_flat, key=lambda z: z[2], reverse=True)\n    #     pos_answers_filtered = pos_answers_sorted[:n_best]\n    #     top_pos_answer_score = pos_answers_filtered[0][2]\n    #\n    #     no_answer = [(start, end, score) for x in preds for start, end, score in x if (start == -1 and end == -1)]\n    #     no_answer_sorted = sorted(no_answer, key=lambda z: z[2], reverse=True)\n    #     no_answers_min = no_answer_sorted[-1]\n    #     _, _, no_answer_min_score = no_answers_min\n    #\n    #     # no answer logic\n    #     threshold = 0.\n    #     if no_answer_min_score + threshold > top_pos_answer_score:\n    #         return [no_answers_min] + pos_answers_filtered\n    #     else:\n    #         return pos_answers_filtered + [no_answers_min]\n\n    @staticmethod\n    def get_no_answer_score(preds):\n        for span in preds:\n            start = span.start\n            end = span.end\n            score = span.score\n            if start == -1 and end == -1:\n                return score\n        raise Exception\n\n    @staticmethod\n    def pred_to_doc_idxs(pred, passage_start_t):\n        """""" Converts the passage level predictions to document level predictions. Note that on the doc level we\n        don\'t have special tokens or question tokens. This means that a no answer\n        cannot be prepresented by a (0,0) span but will instead be represented by (-1, -1)""""""\n        new_pred = []\n        for span in pred:\n            start = span.start\n            end = span.end\n            score = span.score\n            if start == 0:\n                start = -1\n            else:\n                start += passage_start_t\n                assert start >= 0\n            if end == 0:\n                end = -1\n            else:\n                end += passage_start_t\n                assert start >= 0\n            new_pred.append(Span(start, end, score, level=""doc""))\n        return new_pred\n\n    @staticmethod\n    def label_to_doc_idxs(label, passage_start_t):\n        """""" Converts the passage level labels to document level labels. Note that on the doc level we\n        don\'t have special tokens or question tokens. This means that a no answer\n        cannot be prepresented by a (0,0) span but will instead be represented by (-1, -1)""""""\n        new_label = []\n        for start, end in label:\n            # If there is a valid label\n            if start > 0 or end > 0:\n                new_label.append((start + passage_start_t, end + passage_start_t))\n            # If the label is a no answer, we represent this as a (-1, -1) span\n            # since there is no CLS token on the document level\n            if start == 0 and end == 0:\n                new_label.append((-1, -1))\n        return new_label\n\n    def prepare_labels(self, labels, start_of_word, **kwargs):\n        return labels\n\n    @staticmethod\n    def merge_formatted_preds(preds_all):\n        """""" Merges results from the two prediction heads used for NQ style QA. Takes the prediction from QA head and\n        assigns it the appropriate classification label. This mapping is achieved through sample_idx.\n        preds_all should contain [QuestionAnsweringHead.formatted_preds(), TextClassificationHead()]. The first item\n        of this list should be of len=n_documents while the second item should be of len=n_passages""""""\n\n        ret = []\n\n        # This fn is used to align QA output of len=n_docs and Classification output of len=n_passages\n        def chunk(iterable, lengths):\n            assert sum(lengths) == len(iterable)\n            cumsum = list(np.cumsum(lengths))\n            cumsum = [0] + cumsum\n            spans = [(cumsum[i], cumsum[i+1]) for i in range(len(cumsum) - 1)]\n            ret = [iterable[start:end] for start, end in spans]\n            return ret\n\n        cls_preds = preds_all[1][0][""predictions""]\n        qa_preds = preds_all[0][0]\n        samples_per_doc = [doc_pred.n_samples for doc_pred in preds_all[0][0]]\n        cls_preds_grouped = chunk(cls_preds, samples_per_doc)\n\n        for qa_doc_pred, cls_preds in zip(qa_preds, cls_preds_grouped):\n            pred_spans = qa_doc_pred.preds\n            pred_spans_new = []\n            for pred_span in pred_spans:\n                sample_idx = pred_span.sample_idx\n                if sample_idx is not None:\n                    cls_pred = cls_preds[sample_idx][""label""]\n                # i.e. if is_impossible\n                else:\n                    cls_pred = None\n                pred_span.classification = cls_pred\n                pred_spans_new.append(pred_span)\n            qa_doc_pred.preds = pred_spans_new\n            ret.append(qa_doc_pred)\n        return ret\n\n\ndef pick_single_fn(heads, fn_name):\n    """""" Iterates over heads and returns a static method called fn_name\n    if and only if one head has a method of that name. If no heads have such a method, None is returned.\n    If more than one head has such a method, an Exception is thrown""""""\n    merge_fns = []\n    for h in heads:\n        merge_fns.append(getattr(h, fn_name, None))\n\n    merge_fns = [x for x in merge_fns if x is not None]\n    if len(merge_fns) == 0:\n        return None\n    elif len(merge_fns) == 1:\n        return merge_fns[0]\n    else:\n        raise Exception(f""More than one of the prediction heads have a {fn_name}() function"")'"
farm/modeling/predictions.py,0,"b'from farm.utils import span_to_string\n\nclass Span:\n    def __init__(self,\n                 start,\n                 end,\n                 score=None,\n                 sample_idx=None,\n                 n_samples=None,\n                 classification=None,\n                 unit=None,\n                 pred_str=None,\n                 id=None,\n                 level=None):\n        self.start = start\n        self.end = end\n        self.score = score\n        self.unit = unit\n        self.sample_idx = sample_idx\n        self.classification = classification\n        self.n_samples = n_samples\n        self.pred_str = pred_str\n        self.id = id\n        self.level = level\n\n    def to_list(self):\n        return [self.pred_str, self.start, self.end, self.score, self.sample_idx]\n\n    def __str__(self):\n        if self.pred_str is None:\n            pred_str = ""is_impossible""\n        else:\n            pred_str = self.pred_str\n        ret = f""answer: {pred_str}\\n"" \\\n              f""score: {self.score}""\n        return ret\n\n    def __repr__(self):\n        return str(self)\n\nclass DocumentPred:\n    """""" Contains a collection of Span predictions for one document. Used in Question Answering. Also contains all\n    attributes needed to generate the appropriate output json""""""\n    def __init__(self,\n                 id,\n                 document_text,\n                 question,\n                 preds,\n                 no_ans_gap,\n                 token_offsets,\n                 context_window_size,\n                 question_id=None):\n        self.id = id\n        self.preds = preds\n        self.n_samples = preds[0].n_samples\n        self.document_text = document_text\n        self.question = question\n        self.question_id = question_id\n        self.no_ans_gap = no_ans_gap\n        self.token_offsets = token_offsets\n        self.context_window_size = context_window_size\n\n    def __str__(self):\n        preds_str = ""\\n"".join([f""{p}"" for p in self.preds])\n        ret = f""id: {self.id}\\n"" \\\n              f""document: {self.document_text}\\n"" \\\n              f""preds:\\n{preds_str}""\n        return ret\n\n    def __repr__(self):\n        return str(self)\n\n    def to_json(self):\n        answers = self.answers_to_json()\n        ret = {\n            ""task"": ""qa"",\n            ""predictions"": [\n                {\n                    ""question"": self.question,\n                    ""question_id"": self.question_id,\n                    ""ground_truth"": None,\n                    ""answers"": answers,\n                    ""no_ans_gap"": self.no_ans_gap # Add no_ans_gap to current no_ans_boost for switching top prediction\n                }\n            ],\n        }\n        return ret\n\n    def answers_to_json(self):\n        ret = []\n\n        # iterate over the top_n predictions of the one document\n        for span in self.preds:\n            string = span.pred_str\n            start_t = span.start\n            end_t = span.end\n            score = span.score\n            classification = span.classification\n\n            _, ans_start_ch, ans_end_ch = span_to_string(start_t, end_t, self.token_offsets, self.document_text)\n            context_string, context_start_ch, context_end_ch = self.create_context(ans_start_ch, ans_end_ch, self.document_text)\n            curr = {""score"": score,\n                    ""probability"": -1,\n                    ""answer"": string,\n                    ""offset_answer_start"": ans_start_ch,\n                    ""offset_answer_end"": ans_end_ch,\n                    ""context"": context_string,\n                    ""classification"": classification,\n                    ""offset_context_start"": context_start_ch,\n                    ""offset_context_end"": context_end_ch,\n                    ""document_id"": self.id}\n            ret.append(curr)\n        return ret\n\n    def create_context(self, ans_start_ch, ans_end_ch, clear_text):\n        if ans_start_ch == 0 and ans_end_ch == 0:\n            return """", 0, 0\n        else:\n            len_text = len(clear_text)\n            midpoint = int((ans_end_ch - ans_start_ch) / 2) + ans_start_ch\n            half_window = int(self.context_window_size / 2)\n            context_start_ch = midpoint - half_window\n            context_end_ch = midpoint + half_window\n            # if we have part of the context window overlapping start or end of the passage,\n            # we\'ll trim it and use the additional chars on the other side of the answer\n            overhang_start = max(0, -context_start_ch)\n            overhang_end = max(0, context_end_ch - len_text)\n            context_start_ch -= overhang_end\n            context_start_ch = max(0, context_start_ch)\n            context_end_ch += overhang_start\n            context_end_ch = min(len_text, context_end_ch)\n        context_string = clear_text[context_start_ch: context_end_ch]\n        return context_string, context_start_ch, context_end_ch\n\n    def to_squad_eval(self):\n        preds = [x.to_list() for x in self.preds]\n        ret = {""id"": self.id,\n               ""preds"": preds}\n        return ret\n\n\n'"
farm/modeling/tokenization.py,0,"b'# coding=utf-8\n# Copyright 2018 deepset team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes.""""""\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport json\nimport logging\nimport os\nimport re\nfrom pathlib import Path\n\nimport numpy as np\nfrom transformers.tokenization_albert import AlbertTokenizer\nfrom transformers.tokenization_bert import BertTokenizer, load_vocab\nfrom transformers.tokenization_distilbert import DistilBertTokenizer\nfrom transformers.tokenization_electra import ElectraTokenizer\nfrom transformers.tokenization_roberta import RobertaTokenizer\nfrom transformers.tokenization_utils import PreTrainedTokenizer\nfrom transformers.tokenization_xlm_roberta import XLMRobertaTokenizer\nfrom transformers.tokenization_xlnet import XLNetTokenizer\n\nfrom farm.modeling.wordembedding_utils import load_from_cache, EMBEDDING_VOCAB_FILES_MAP, run_split_on_punc\n\n\nlogger = logging.getLogger(__name__)\n\n# Special characters used by the different tokenizers to indicate start of word / whitespace\nSPECIAL_TOKENIZER_CHARS = r""^(##|\xc4\xa0|\xe2\x96\x81)""\n\n\nclass Tokenizer:\n    """"""\n    Simple Wrapper for Tokenizers from the transformers package. Enables loading of different Tokenizer classes with a uniform interface.\n    """"""\n\n    @classmethod\n    def load(cls, pretrained_model_name_or_path, tokenizer_class=None, **kwargs):\n        """"""\n        Enables loading of different Tokenizer classes with a uniform interface. Either infer the class from\n        `pretrained_model_name_or_path` or define it manually via `tokenizer_class`.\n\n        :param pretrained_model_name_or_path:  The path of the saved pretrained model or its name (e.g. `bert-base-uncased`)\n        :type pretrained_model_name_or_path: str\n        :param tokenizer_class: (Optional) Name of the tokenizer class to load (e.g. `BertTokenizer`)\n        :type tokenizer_class: str\n        :param kwargs:\n        :return: Tokenizer\n        """"""\n\n        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n        # guess tokenizer type from name\n        if tokenizer_class is None:\n            if ""albert"" in pretrained_model_name_or_path.lower():\n                tokenizer_class = ""AlbertTokenizer""\n            elif ""xlm-roberta"" in pretrained_model_name_or_path.lower():\n                tokenizer_class = ""XLMRobertaTokenizer""\n            elif ""roberta"" in pretrained_model_name_or_path.lower():\n                tokenizer_class = ""RobertaTokenizer""\n            elif ""distilbert"" in pretrained_model_name_or_path.lower():\n                tokenizer_class = ""DistilBertTokenizer""\n            elif ""bert"" in pretrained_model_name_or_path.lower():\n                tokenizer_class = ""BertTokenizer""\n            elif ""xlnet"" in pretrained_model_name_or_path.lower():\n                tokenizer_class = ""XLNetTokenizer""\n            elif ""electra"" in pretrained_model_name_or_path.lower():\n                tokenizer_class = ""ElectraTokenizer""\n            elif ""word2vec"" in pretrained_model_name_or_path.lower() or \\\n                    ""glove"" in pretrained_model_name_or_path.lower() or \\\n                    ""fasttext"" in pretrained_model_name_or_path.lower():\n                tokenizer_class = ""EmbeddingTokenizer""\n            else:\n                raise ValueError(f""Could not infer tokenizer_class from name \'{pretrained_model_name_or_path}\'. Set ""\n                                 f""arg `tokenizer_class` in Tokenizer.load() to one of: AlbertTokenizer, ""\n                                 f""XLMRobertaTokenizer, RobertaTokenizer, DistilBertTokenizer, BertTokenizer, or ""\n                                 f""XLNetTokenizer."")\n            logger.info(f""Loading tokenizer of type \'{tokenizer_class}\'"")\n        # return appropriate tokenizer object\n        if tokenizer_class == ""AlbertTokenizer"":\n            ret = AlbertTokenizer.from_pretrained(pretrained_model_name_or_path, keep_accents=True,  **kwargs)\n        elif tokenizer_class == ""XLMRobertaTokenizer"":\n            ret = XLMRobertaTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif tokenizer_class == ""RobertaTokenizer"":\n            ret = RobertaTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif tokenizer_class == ""DistilBertTokenizer"":\n            ret = DistilBertTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif tokenizer_class == ""BertTokenizer"":\n            ret = BertTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif tokenizer_class == ""XLNetTokenizer"":\n            ret = XLNetTokenizer.from_pretrained(pretrained_model_name_or_path, keep_accents=True, **kwargs)\n        elif tokenizer_class == ""ElectraTokenizer"":\n            ret = ElectraTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif tokenizer_class == ""EmbeddingTokenizer"":\n            ret = EmbeddingTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        if ret is None:\n            raise Exception(""Unable to load tokenizer"")\n        else:\n            return ret\n\n\nclass EmbeddingTokenizer(PreTrainedTokenizer):\n    """"""Constructs an EmbeddingTokenizer.\n    """"""\n\n    def __init__(\n            self,\n            vocab_file,\n            do_lower_case=True,\n            unk_token=""[UNK]"",\n            sep_token=""[SEP]"",\n            pad_token=""[PAD]"",\n            cls_token=""[CLS]"",\n            mask_token=""[MASK]"",\n            **kwargs\n    ):\n        """"""\n        :param vocab_file: Path to a one-word-per-line vocabulary file\n        :type vocab_file: str\n        :param do_lower_case: Flag whether to lower case the input\n        :type do_lower_case: bool\n        """"""\n        # TODO check why EmbeddingTokenizer.tokenize gives many UNK, while tokenize_with_metadata() works fine\n\n        super().__init__(\n            unk_token=unk_token,\n            sep_token=sep_token,\n            pad_token=pad_token,\n            cls_token=cls_token,\n            mask_token=mask_token,\n            **kwargs,\n        )\n\n        if not os.path.isfile(vocab_file):\n            raise ValueError(""Can\'t find a vocabulary file at path \'{}\'."".format(vocab_file))\n        self.vocab = load_vocab(vocab_file)\n        self.unk_tok_idx = self.vocab[unk_token]\n        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n        self.do_lower_case = do_lower_case\n        self.vocab_size = len(self.vocab)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n        """"""Load the tokenizer from local path or remote.""""""\n        if pretrained_model_name_or_path in EMBEDDING_VOCAB_FILES_MAP[""vocab_file""]:\n            # Get the vocabulary from AWS S3 bucket or cache\n            resolved_vocab_file = load_from_cache(pretrained_model_name_or_path,\n                                                  EMBEDDING_VOCAB_FILES_MAP[""vocab_file""],\n                                                  **kwargs)\n        elif os.path.isdir(pretrained_model_name_or_path):\n            # Get the vocabulary from local files\n            logger.info(\n                f""Model name \'{pretrained_model_name_or_path}\' not found in model shortcut name ""\n                f""list ({\', \'.join(EMBEDDING_VOCAB_FILES_MAP[\'vocab_file\'].keys())}). ""\n                f""Assuming \'{pretrained_model_name_or_path}\' is a path to a directory containing tokenizer files."")\n\n            temp = open(str(Path(pretrained_model_name_or_path) / ""language_model_config.json""), ""r"",\n                        encoding=""utf-8"").read()\n            config_dict = json.loads(temp)\n\n            resolved_vocab_file = str(Path(pretrained_model_name_or_path) / config_dict[""vocab_filename""])\n        else:\n            logger.error(\n                f""Model name \'{pretrained_model_name_or_path}\' not found in model shortcut name ""\n                f""list ({\', \'.join(EMBEDDING_VOCAB_FILES_MAP[\'vocab_file\'].keys())}) nor as local folder "")\n            raise NotImplementedError\n\n        tokenizer = cls(vocab_file=resolved_vocab_file, **kwargs)\n        return tokenizer\n\n    def _tokenize(self, text, **kwargs):\n        if self.do_lower_case:\n            text = text.lower()\n        tokens = run_split_on_punc(text)\n        tokens = [t if t in self.vocab else self.unk_token for t in tokens]\n        return tokens\n\n    def save_pretrained(self, vocab_path):\n        """"""Save the tokenizer vocabulary to a directory or file.""""""\n        index = 0\n        if os.path.isdir(vocab_path):\n            vocab_file = os.path.join(vocab_path, ""vocab.txt"")\n        else:\n            vocab_file = vocab_path\n        with open(vocab_file, ""w"", encoding=""utf-8"") as writer:\n            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(\n                        ""Saving vocabulary to {}: vocabulary indices are not consecutive.""\n                        "" Please check that the vocabulary is not corrupted!"".format(vocab_file)\n                    )\n                    index = token_index\n                writer.write(token + ""\\n"")\n                index += 1\n        return (vocab_file,)\n\n    def _convert_token_to_id(self, token):\n        return self.vocab.get(token, self.unk_tok_idx)\n\n\ndef tokenize_with_metadata(text, tokenizer):\n    """"""\n    Performing tokenization while storing some important metadata for each token:\n\n    * offsets: (int) Character index where the token begins in the original text\n    * start_of_word: (bool) If the token is the start of a word. Particularly helpful for NER and QA tasks.\n\n    We do this by first doing whitespace tokenization and then applying the model specific tokenizer to each ""word"".\n\n    .. note::  We don\'t assume to preserve exact whitespaces in the tokens!\n               This means: tabs, new lines, multiple whitespace etc will all resolve to a single "" "".\n               This doesn\'t make a difference for BERT + XLNet but it does for RoBERTa.\n               For RoBERTa it has the positive effect of a shorter sequence length, but some information about whitespace\n               type is lost which might be helpful for certain NLP tasks ( e.g tab for tables).\n\n    :param text: Text to tokenize\n    :type text: str\n    :param tokenizer: Tokenizer (e.g. from Tokenizer.load())\n    :return: Dictionary with ""tokens"", ""offsets"" and ""start_of_word""\n    :rtype: dict\n\n    """"""\n\n    # normalize all other whitespace characters to "" ""\n    # Note: using text.split() directly would destroy the offset,\n    # since \\n\\n\\n would be treated similarly as a single \\n\n    text = re.sub(r""\\s"", "" "", text)\n    # split text into ""words"" (here: simple whitespace tokenizer).\n    words = text.split("" "")\n    word_offsets = []\n    cumulated = 0\n    for idx, word in enumerate(words):\n        word_offsets.append(cumulated)\n        cumulated += len(word) + 1  # 1 because we so far have whitespace tokenizer\n\n    # split ""words""into ""subword tokens""\n    tokens, offsets, start_of_word = _words_to_tokens(\n        words, word_offsets, tokenizer\n    )\n\n    tokenized = {""tokens"": tokens, ""offsets"": offsets, ""start_of_word"": start_of_word}\n    return tokenized\n\n\ndef _words_to_tokens(words, word_offsets, tokenizer):\n    """"""\n    Tokenize ""words"" into subword tokens while keeping track of offsets and if a token is the start of a word.\n\n    :param words: list of words.\n    :type words: list\n    :param word_offsets: Character indices where each word begins in the original text\n    :type word_offsets: list\n    :param tokenizer: Tokenizer (e.g. from Tokenizer.load())\n    :return: tokens, offsets, start_of_word\n\n    """"""\n    tokens = []\n    token_offsets = []\n    start_of_word = []\n    idx = 0\n    for w, w_off in zip(words, word_offsets):\n        idx += 1\n        if idx % 500000 == 0:\n            logger.info(idx)\n        # Get (subword) tokens of single word.\n\n        # empty / pure whitespace\n        if len(w) == 0:\n          continue\n        # For the first word of a text: we just call the regular tokenize function.\n        # For later words: we need to call it with add_prefix_space=True to get the same results with roberta / gpt2 tokenizer\n        # see discussion here. https://github.com/huggingface/transformers/issues/1196\n        elif len(tokens) == 0:\n            tokens_word = tokenizer.tokenize(w)\n        else:\n            try:\n                tokens_word = tokenizer.tokenize(w, add_prefix_space=True)\n            except TypeError:\n                tokens_word = tokenizer.tokenize(w)\n\n        # Sometimes the tokenizer returns no tokens\n        if len(tokens_word) == 0:\n            continue\n        tokens += tokens_word\n\n        # get global offset for each token in word + save marker for first tokens of a word\n        first_tok = True\n        for tok in tokens_word:\n            token_offsets.append(w_off)\n            # Depending on the tokenizer type special chars are added to distinguish tokens with preceeding\n            # whitespace (=> ""start of a word""). We need to get rid of these to calculate the original length of the token\n            orig_tok = re.sub(SPECIAL_TOKENIZER_CHARS, """", tok)\n            w_off += len(orig_tok)\n            if first_tok:\n                start_of_word.append(True)\n                first_tok = False\n            else:\n                start_of_word.append(False)\n\n    assert len(tokens) == len(token_offsets) == len(start_of_word)\n    return tokens, token_offsets, start_of_word\n\n\ndef truncate_sequences(seq_a, seq_b, tokenizer, max_seq_len, truncation_strategy=\'longest_first\',\n                       with_special_tokens=True, stride=0):\n    """"""\n    Reduces a single sequence or a pair of sequences to a maximum sequence length.\n    The sequences can contain tokens or any other elements (offsets, masks ...).\n    If `with_special_tokens` is enabled, it\'ll remove some additional tokens to have exactly enough space for later adding special tokens (CLS, SEP etc.)\n\n    Supported truncation strategies:\n\n    - longest_first: (default) Iteratively reduce the inputs sequence until the input is under max_length starting from the longest one at each token (when there is a pair of input sequences). Overflowing tokens only contains overflow from the first sequence.\n    - only_first: Only truncate the first sequence. raise an error if the first sequence is shorter or equal to than num_tokens_to_remove.\n    - only_second: Only truncate the second sequence\n    - do_not_truncate: Does not truncate (raise an error if the input sequence is longer than max_length)\n\n    :param seq_a: First sequence of tokens/offsets/...\n    :type seq_a: list\n    :param seq_b: Optional second sequence of tokens/offsets/...\n    :type seq_b: None or list\n    :param tokenizer: Tokenizer (e.g. from Tokenizer.load())\n    :param max_seq_len:\n    :type max_seq_len: int\n    :param truncation_strategy: how the sequence(s) should be truncated down. Default: ""longest_first"" (see above for other options).\n    :type truncation_strategy: str\n    :param with_special_tokens: If true, it\'ll remove some additional tokens to have exactly enough space for later adding special tokens (CLS, SEP etc.)\n    :type with_special_tokens: bool\n    :param stride: optional stride of the window during truncation\n    :type stride: int\n    :return: truncated seq_a, truncated seq_b, overflowing tokens\n\n    """"""\n    pair = bool(seq_b is not None)\n    len_a = len(seq_a)\n    len_b = len(seq_b) if pair else 0\n    num_special_tokens = tokenizer.num_added_tokens(pair=pair) if with_special_tokens else 0\n    total_len = len_a + len_b + num_special_tokens\n    overflowing_tokens = []\n\n    if max_seq_len and total_len > max_seq_len:\n        seq_a, seq_b, overflowing_tokens = tokenizer.truncate_sequences(seq_a, pair_ids=seq_b,\n                                                                        num_tokens_to_remove=total_len - max_seq_len,\n                                                                        truncation_strategy=truncation_strategy,\n                                                                        stride=stride)\n    return (seq_a, seq_b, overflowing_tokens)\n\n\ndef insert_at_special_tokens_pos(seq, special_tokens_mask, insert_element):\n    """"""\n    Adds elements to a sequence at the positions that align with special tokens.\n    This is useful for expanding label ids or masks, so that they align with corresponding tokens (incl. the special tokens)\n\n    Example:\n\n    .. code-block:: python\n\n      # Tokens:  [""CLS"", ""some"", ""words"",""SEP""]\n      >>> special_tokens_mask =  [1,0,0,1]\n      >>> lm_label_ids =  [12,200]\n      >>> insert_at_special_tokens_pos(lm_label_ids, special_tokens_mask, insert_element=-1)\n      [-1, 12, 200, -1]\n\n    :param seq: List where you want to insert new elements\n    :type seq: list\n    :param special_tokens_mask: list with ""1"" for positions of special chars\n    :type special_tokens_mask: list\n    :param insert_element: the value you want to insert\n    :return: list\n\n    """"""\n    new_seq = seq.copy()\n    special_tokens_indices = np.where(np.array(special_tokens_mask) == 1)[0]\n    for idx in special_tokens_indices:\n        new_seq.insert(idx, insert_element)\n    return new_seq\n'"
farm/modeling/wordembedding_utils.py,0,"b'from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport io\nimport json\nimport logging\nimport os\nimport unicodedata\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom transformers.tokenization_bert import BertTokenizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.cluster import KMeans\nfrom collections import Counter\n\nfrom farm.file_utils import load_from_cache\n\n\n# create dictionaries with links to wordembeddings stored on deepset s3\n# the dicts need to be used with HF transformers to use their data + modelling functionality\n# language model config\nPRETRAINED_CONFIG_ARCHIVE_MAP = {\n    ""glove-german-uncased"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-models/0.4.1/glove-german-uncased/language_model_config.json"",\n    ""glove-english-uncased-6B"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-models/0.4.1/glove-english-uncased-6B/language_model_config.json"",\n    ""glove-english-cased-840B"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-models/0.4.1/glove-english-cased-840B/language_model_config.json"",\n}\n# tokenization\nEMBEDDING_VOCAB_FILES_MAP = {}\nEMBEDDING_VOCAB_FILES_MAP[""vocab_file""] = {\n    ""glove-german-uncased"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-models/0.4.1/glove-german-uncased/vocab.txt"",\n    ""glove-english-uncased-6B"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-models/0.4.1/glove-english-uncased-6B/vocab.txt"",\n    ""glove-english-cased-840B"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-models/0.4.1/glove-english-cased-840B/vocab.txt"",\n}\nMAX_MODEL_INPU_SIZES = {\n    ""glove-german-uncased"": 10000,\n    ""glove-english-uncased-6B"": 10000,\n    ""glove-english-cased-840B"": 10000,\n}\nPRETRAINED_INIT_CONFIGURATION = {""glove-german-uncased"": {""do_lower_case"": True},\n                                 ""glove-english-uncased-6B"": {""do_lower_case"": True},\n                                 ""glove-english-cased-840B"": {""do_lower_case"": False}}\n# model\nEMBEDDING_MODEL_MAP = {\n    ""glove-german-uncased"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-models/0.4.1/glove-german-uncased/vectors.txt"",\n    ""glove-english-uncased-6B"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-models/0.4.1/glove-english-uncased-6B/vectors.txt"",\n    ""glove-english-cased-840B"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-models/0.4.1/glove-english-cased-840B/vectors.txt"",\n    ""fasttext-german-uncased"": ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-models/0.4.1/fasttext-german-uncased/language_model.bin"",\n}\n# conversion\nSPECIAL_TOKENS = [""[CLS]"", ""[SEP]"", ""[UNK]"", ""[PAD]"", ""[MASK]""]\n\nlogger = logging.getLogger(__name__)\n\n\nclass Fasttext_converter():\n    """"""\n    Class to use fasttext inside FARM by converting embeddings to format usable by preprocessing pipeline.\n    Farm needs fixed vocab and embeddings. We can construct a vocab for the data we wish to embed.\n    """"""\n\n    def __init__(self,\n                 pretrained_model_name_or_path,\n                 do_lower_case,\n                 data_path,\n                 train_filename,\n                 output_path,\n                 language=None,\n                 sep=""\\t"",\n                 text_column_name=""text"",\n                 max_inputdata_rows=None,\n                 min_vocab_count=None,\n                 max_features=None,\n                 ):\n\n        """"""\n        :param pretrained_model_name_or_path: path to local model or pointer to s3\n        :param do_lower_case: casing information, should match the vocab\n        :param data_path: path to where data is stored\n        :param train_filename:\n        :param output_path: path where the embeddings (now in word2vec format) are stored\n        :param language:\n        :param sep: seperator used in train file\n        :param text_column_name: column where the text for\n        :param max_inputdata_rows: limits the amount of rows to read from data for constructing the vocab\n        :param min_vocab_count: when constructing the vocab, words with less than min_vocab_count occurrences are ignored\n        :param max_features: maximum number of words to use in vocab\n        """"""\n        try:\n            import fasttext  # fasttext import is optional in requirements. So we just load it when needed.\n        except ModuleNotFoundError:\n            logger.error(""Could not find fasttext. Please install through \'pip install fasttext==0.9.1\'."")\n\n        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n        self.do_lower_case = do_lower_case\n        self.data_path = data_path\n        self.train_filename = train_filename\n        self.output_path = output_path\n        self.language = language\n        self.sep = sep\n        self.text_column_name = text_column_name\n        self.max_inputdata_rows = max_inputdata_rows\n        self.min_vocab_count = min_vocab_count\n        self.max_features = max_features\n\n    def convert_on_data(self, **kwargs):\n        """"""\n        Function to prepare data by\n             - computing a vocab over the data\n             - converting each vocab item to a corresponding vector\n             - saving vocab and embeddings in word2vec txt format for further processing\n        :param kwargs: placeholder for args passed to model loading, like proxy or caching settings\n        :return: vocab_counts, dict: dictionary with words and associated counts\n        """"""\n        model = self._load_model(**kwargs)\n\n        all_words = self._load_data()\n\n        temp_vocab, vocab_counts = self._create_vocab(all_words=all_words)\n\n        vocab,embeddings = self._create_embeddings(temp_vocab=temp_vocab, model=model)\n\n        self._save(vocab=vocab, embeddings=embeddings)\n\n        return vocab_counts\n\n    def _load_model(self, **kwargs):\n        # Model loading\n        farm_lm_config = Path(self.pretrained_model_name_or_path) / ""language_model_config.json""\n        if os.path.exists(farm_lm_config):\n            # from local dir\n            config = json.load(open(farm_lm_config, ""r""))\n            resolved_model_file = str(Path(self.pretrained_model_name_or_path) / config[""embeddings_filename""])\n        else:\n            # from s3 or cache\n            resolved_model_file = load_from_cache(self.pretrained_model_name_or_path, EMBEDDING_MODEL_MAP, **kwargs)\n        if os.path.isfile(resolved_model_file):\n            model = self.fasttext.load_model(resolved_model_file)\n        else:\n            logger.error(f""Could not load fasttext model at {self.pretrained_model_name_or_path}."")\n\n        return model\n\n    def _load_data(self):\n        # Data loading\n        df = pd.read_csv(str(self.data_path / self.train_filename), sep=self.sep, nrows=self.max_inputdata_rows)\n\n        if self.text_column_name not in df.columns:\n            logger.error(\n                f""Cannot find Text column name in the supplied data. Available columsn are {\', \'.join(df.columns)}."")\n        if self.max_inputdata_rows:\n            df = df.sample(n=self.max_inputdata_rows)\n        texts = df.loc[:, self.text_column_name].values\n        all_words = []\n        for t in texts:\n            if self.do_lower_case:\n                t = t.lower()\n            words = t.split("" "")\n            tokens = []\n            for w in words:\n                tokens.extend(run_split_on_punc(w))\n            all_words.extend(tokens)\n        return all_words\n\n    def _create_vocab(self, all_words):\n        # Vocab creation\n        w, c = np.unique(all_words, return_counts=True)\n        if self.min_vocab_count:\n            idx = c >= self.min_vocab_count\n            w = w[idx]\n            c = c[idx]\n        if self.max_features:\n            max_features_adjusted = self.max_features - len(SPECIAL_TOKENS)\n            if w.shape[0] > max_features_adjusted:\n                idx = np.argsort(c)[::-1]  # descending order\n                w = w[idx[:max_features_adjusted]]\n                c = c[idx[:max_features_adjusted]]\n        temp_vocab = list(w)\n        vocab_counts = dict(zip(temp_vocab, c))\n        return temp_vocab, vocab_counts\n\n    def _create_embeddings(self, temp_vocab, model):\n        # Embedding creation\n        embeddings = np.zeros((len(temp_vocab) + len(SPECIAL_TOKENS), model.get_dimension()))\n        for i, w in enumerate(temp_vocab):\n            embeddings[i + len(SPECIAL_TOKENS), :] = model.get_word_vector(w)\n        mean_embedding = np.mean(embeddings[len(SPECIAL_TOKENS):, :], axis=0)\n        for i in range(len(SPECIAL_TOKENS)):\n            embeddings[i, :] = mean_embedding\n        vocab = SPECIAL_TOKENS + temp_vocab\n        return vocab, embeddings\n\n    def _save(self, vocab, embeddings):\n        # create config\n        lm_config = {\n            ""embeddings_filename"": ""vectors.txt"",\n            ""hidden_size"": embeddings.shape[1],\n            ""language"": self.language,\n            ""name"": ""WordEmbedding_LM"",\n            ""vocab_filename"": ""vocab.txt"",\n            ""vocab_size"": embeddings.shape[0]\n        }\n        # saving\n        if not os.path.exists(self.output_path):\n            os.makedirs(self.output_path)\n        with open(self.output_path / ""language_model_config.json"", ""w"") as file:\n            file.write(json.dumps(lm_config, indent=2))\n        _save_word2vec_format(fname=str(self.output_path / lm_config[""embeddings_filename""]),\n                              fvocab=str(self.output_path / lm_config[""vocab_filename""]),\n                              vocab=vocab,\n                              vectors=embeddings)\n\n\ndef load_embedding_tokenizer(pretrained_model_name_or_path, **kwargs):\n    # if the pretrained model points to a file on deepset s3, we need to adjust transformers dictionaries\n    if pretrained_model_name_or_path in PRETRAINED_INIT_CONFIGURATION:\n        BertTokenizer.pretrained_vocab_files_map[""vocab_file""]. \\\n            update({pretrained_model_name_or_path: EMBEDDING_VOCAB_FILES_MAP[""vocab_file""].get(\n            pretrained_model_name_or_path, None)})\n        BertTokenizer.max_model_input_sizes. \\\n            update({pretrained_model_name_or_path: MAX_MODEL_INPU_SIZES.get(pretrained_model_name_or_path, None)})\n        BertTokenizer.pretrained_init_configuration. \\\n            update(\n            {pretrained_model_name_or_path: PRETRAINED_INIT_CONFIGURATION.get(pretrained_model_name_or_path, None)})\n    ret = BertTokenizer.from_pretrained(pretrained_model_name_or_path, **kwargs)\n    return ret\n\n\ndef load_model(pretrained_model_name_or_path, **kwargs):\n    # loading config\n    resolved_config_file = load_from_cache(pretrained_model_name_or_path, PRETRAINED_CONFIG_ARCHIVE_MAP, **kwargs)\n    temp = open(resolved_config_file, ""r"", encoding=""utf-8"").read()\n    config_dict = json.loads(temp)\n\n    # loading vocab\n    resolved_vocab_file = load_from_cache(pretrained_model_name_or_path, EMBEDDING_VOCAB_FILES_MAP[""vocab_file""],\n                                          **kwargs)\n\n    # loading model\n    resolved_model_file = load_from_cache(pretrained_model_name_or_path, EMBEDDING_MODEL_MAP, **kwargs)\n\n    return config_dict, resolved_vocab_file, resolved_model_file\n\n\ndef load_embedding_vectors(embedding_file, vocab):\n    f = io.open(embedding_file, \'rt\', encoding=\'utf-8\').readlines()\n\n    words_transformed = set()\n    repetitions = 0\n    embeddings_dimensionality = None\n    vectors = {}\n\n    for line in tqdm(f, desc=""Loading embeddings""):\n        line = line.strip()\n        if line:\n            word, vec = line.split(\' \', 1)\n            if (word not in words_transformed):  # omit repetitions = speed up + debug\n                try:\n                    np_vec = np.fromstring(vec, sep=\' \')\n                    if embeddings_dimensionality is None:\n                        if len(np_vec) < 4:  # word2vec includes number of vectors and its dimension as header\n                            logger.info(""Skipping header"")\n                            continue\n                        else:\n                            embeddings_dimensionality = len(np_vec)\n                    if len(np_vec) == embeddings_dimensionality:\n                        vectors[word] = np_vec\n                        words_transformed.add(word)\n                except:\n                    if logger is not None:\n                        logger.debug(""Embeddings reader: Could not convert line: {}"".format(line))\n            else:\n                repetitions += 1\n\n    embeddings = np.zeros((len(vocab), embeddings_dimensionality))\n    for i, w in enumerate(vocab):\n        current = vectors.get(w, np.zeros(embeddings_dimensionality))\n        if w not in vectors:\n            logger.warning(f""Could not load pretrained embedding for word: {w}"")\n        embeddings[i, :] = current\n    return embeddings\n\n\ndef load_word2vec_vocab(vocab_filename):\n    """"""Loads a vocabulary file into a list.""""""\n    vocab = []\n    with open(vocab_filename, ""r"", encoding=""utf-8"") as reader:\n        lines = reader.readlines()\n    for l in lines:\n        w, c = l.split("" "")\n        vocab.append(w.strip())\n    return vocab\n\n\ndef convert_WordEmbeddings(embedding_filename, vocab_filename, output_path, language=""English""):\n    """"""\n    Converts a Wordembedding model in word2vec format to a format that can be used inside FARM\n    For compatibility special tokens are added to create embeddings for [CLS], [SEP], [UNK], [PAD] and [MASK]\n    The embeddings for these special tokens are the average word embeddings of all other words\n    #TODO save model in a more efficient format\n\n    :param vector_filename: word2vec format embeddings. A txt file consisting of space separated word and n (dimension of\n    embedding) embedding values for that word\n    :type vector_filename: str\n    :param vocab_filename: a txt file with each line containing a word and its associated count\n    :type vocab_filename: str\n    :param output_path: path to store the converted model\n    :type output_path: str\n    :return:\n    """"""\n    # creating vocab\n    temp_vocab = load_word2vec_vocab(vocab_filename=vocab_filename)\n    vocab = SPECIAL_TOKENS + temp_vocab\n\n    # create embeddings\n    temp_embeddings = load_embedding_vectors(embedding_file=embedding_filename, vocab=temp_vocab)\n    mean_embedding = np.mean(temp_embeddings, axis=0)\n    embeddings = np.zeros((temp_embeddings.shape[0] + len(SPECIAL_TOKENS), temp_embeddings.shape[1]))\n    for i, tok in enumerate(SPECIAL_TOKENS):\n        embeddings[i, :] = mean_embedding\n    embeddings[len(SPECIAL_TOKENS):, :] = temp_embeddings\n\n    # create config\n    lm_config = {\n        ""embeddings_filename"": ""vectors.txt"",\n        ""hidden_size"": embeddings.shape[1],\n        ""language"": language,\n        ""name"": ""WordEmbedding_LM"",\n        ""vocab_filename"": ""vocab.txt"",\n        ""vocab_size"": embeddings.shape[0]\n    }\n\n    # saving\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n    with open(Path(output_path) / ""language_model_config.json"", ""w"") as file:\n        file.write(json.dumps(lm_config, indent=2))\n\n    _save_word2vec_format(fname=str(Path(output_path) / lm_config[""embeddings_filename""]),\n                          fvocab=str(Path(output_path) / lm_config[""vocab_filename""]),\n                          vocab=vocab,\n                          vectors=embeddings)\n\n\ndef _save_word2vec_format(fname, vocab, vectors, fvocab):\n    """"""Store the input-hidden weight matrix in the same format used by the original\n    C word2vec-tool, for compatibility.\n\n    Code taken and adjusted from gensim: https://github.com/RaRe-Technologies/gensim/blob/ec222e8e3e72608a59805040eadcf5c734a2b96c/gensim/models/utils_any2vec.py#L105\n\n    Parameters\n    ----------\n    fname : str\n        The file path used to save the vectors in.\n    vocab : list\n        The vocabulary of words.\n    vectors : numpy.array\n        The vectors to be stored.\n    fvocab : str\n        File path used to save the vocabulary.\n    """"""\n    if not (vocab or vectors):\n        raise RuntimeError(""no input"")\n    vector_size = vectors.shape[1]\n    if fvocab is not None:\n        logger.info(f""storing vocabulary in {fvocab}"")\n        with io.open(fvocab, \'w\') as vout:\n            for word in vocab:\n                vout.write(word + ""\\n"")\n    logger.info(f""storing {len(vocab)} projection weights with dimension {vector_size} into {fname}"")\n    assert (len(vocab), vector_size) == vectors.shape\n    with io.open(fname, \'w\') as fout:\n        # store in sorted order: most frequent words at the top\n        for i, word in enumerate(vocab):\n            row = vectors[i, :]\n            fout.write(f""{word} {\' \'.join(repr(val) for val in row)}\\n"")\n\n\ndef run_split_on_punc(text, never_split=None):\n    """"""Splits punctuation on a piece of text.\n    Function taken from HuggingFace: transformers.tokenization_bert.BasicTokenizer\n    """"""\n    if never_split is not None and text in never_split:\n        return [text]\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n        char = chars[i]\n        if _is_punctuation(char):\n            output.append([char])\n            start_new_word = True\n        else:\n            if start_new_word:\n                output.append([])\n            start_new_word = False\n            output[-1].append(char)\n        i += 1\n\n    return ["""".join(x) for x in output]\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n\ndef s3e_pooling(token_embs, token_ids, token_weights, centroids, token_to_cluster, mask, svd_components=None):\n    """"""\n    Pooling of word/token embeddings as described by Wang et al in their paper\n    ""Efficient Sentence Embedding via Semantic Subspace Analysis""\n    (https://arxiv.org/abs/2002.09620)\n    Adjusted their implementation from here: https://github.com/BinWang28/Sentence-Embedding-S3E\n\n    This method takes a fitted ""s3e model"" and token embeddings from a language model and returns sentence embeddings\n    using the S3E Method. The model can be fitted via `fit_s3e_on_corpus()`.\n\n    Usage: See `examples/embeddings_extraction_s3e_pooling.py`\n\n    :param token_embs: numpy array of shape (batch_size, max_seq_len, emb_dim) containing the embeddings for each token\n    :param token_ids: numpy array of shape (batch_size, max_seq_len) containing the ids for each token in the vocab\n    :param token_weights: dict with key=token_id, value= weight in corpus\n    :param centroids: numpy array of shape (n_cluster, emb_dim) that describes the centroids of our clusters in the embedding space\n    :param token_to_cluster: numpy array of shape (vocab_size, 1) where token_to_cluster[i] = cluster_id that token with id i belongs to\n    :param svd_components: Components from a truncated singular value decomposition (SVD, aka LSA) to be\n                           removed from the final sentence embeddings in a postprocessing step.\n                           SVD must be fit on representative sample of sentence embeddings first and can\n                           then be removed from all subsequent embeddings in this function.\n                           We expect the sklearn.decomposition.TruncatedSVD.fit(<your_embeddings>)._components to be passed here.\n    :return: embeddings matrix of shape (batch_size, emb_dim + (n_clusters*n_clusters+1)/2)\n    """"""\n\n    embeddings = []\n    n_clusters = centroids.shape[0]\n    emb_dim = token_embs.shape[2]\n    # n_tokens = token_embs.shape[1]\n    n_samples = token_embs.shape[0]\n    # Mask tokens that should be ignored (e.g. Padding, CLS ...)\n    token_ids[mask] = -1\n\n    # Process each sentence in the batch at a time\n    for sample_idx in range(n_samples):\n        stage_vec = [{}]\n        # 1) create a dict with key=tok_id, value = embedding\n        for tok_idx, tok_id in enumerate(token_ids[sample_idx, :]):\n            if tok_id != -1:\n                stage_vec[-1][tok_id] = token_embs[sample_idx, tok_idx]\n\n        # 2) create a second dict with key=cluster_id, val=[embeddings] (= C)\n        stage_vec.append({})\n        for k, v in stage_vec[-2].items():\n            cluster = token_to_cluster[k]\n\n            if cluster in stage_vec[-1]:\n                stage_vec[-1][cluster].append(stage_vec[-2][k] * token_weights[k])\n            else:\n                stage_vec[-1][cluster] = []\n                stage_vec[-1][cluster].append(stage_vec[-2][k] * token_weights[k])\n\n        # VLAD for each cluster\n        for k, v in stage_vec[-1].items():\n            # Centroids\n            centroid_vec = centroids[k]\n\n            # Residual\n            v = [wv - centroid_vec for wv in v]\n            stage_vec[-1][k] = np.sum(v, 0)\n\n        # Compute Sentence Embedding (weighted avg, dim = original embedding dim)\n        sentvec = []\n        vec = np.zeros((emb_dim))\n        for key, value in stage_vec[0].items():\n            # print(token_weights[key])\n            vec = vec + value * token_weights[key]\n        sentvec.append(vec / len(stage_vec[0].keys()))\n\n        # Covariance Descriptor (dim = k*(k+1)/2, with k=n_clusters)\n        matrix = np.zeros((n_clusters, emb_dim))\n        for j in range(n_clusters):\n            if j in stage_vec[-1]:\n                matrix[j, :] = stage_vec[-1][j]\n        matrix_no_mean = matrix - matrix.mean(1)[:, np.newaxis]\n        cov = matrix_no_mean.dot(matrix_no_mean.T)\n\n        # Generate Embedding\n        iu1 = np.triu_indices(cov.shape[0])\n        iu2 = np.triu_indices(cov.shape[0], 1)\n        cov[iu2] = cov[iu2] * np.sqrt(2)\n        vec = cov[iu1]\n\n        vec = vec / np.linalg.norm(vec)\n\n        sentvec.append(vec)\n\n        # Concatenate weighted avg + covariance descriptors\n        sentvec = np.concatenate(sentvec)\n\n        embeddings.append(sentvec)\n\n    embeddings = np.vstack(embeddings)\n\n    # Post processing (removal of first principal component)\n    if svd_components is not None:\n        embeddings = embeddings - embeddings.dot(svd_components.transpose()) * svd_components\n    return embeddings\n\n\ndef fit_s3e_on_corpus(processor, model, corpus, n_clusters=10,\n                      mean_removal=True, pca_removal=True,\n                      pca_n_components=300, pca_n_top_components=10,\n                      default_token_weight=1, min_token_occurrences=0,\n                      svd_postprocessing=False,\n                      use_gpu=False, batch_size=50):\n    """"""\n    Pooling of word/token embeddings as described by Wang et al in the paper\n    ""Efficient Sentence Embedding via Semantic Subspace Analysis""\n    (https://arxiv.org/abs/2002.09620)\n    Adjusted their implementation from here: https://github.com/BinWang28/Sentence-Embedding-S3E\n\n    This method fits the ""model"" on a custom corpus. This includes the derivation of token_weights depending on\n    token occurences in the corpus, creation of the semantic clusters via k-means and a couple of\n    pre-/post-processing steps to normalize the embeddings.\n\n    The resulting objects can be saved or directly passed to the Inferencer to get the actual embeddings for your sentences.\n    Note: Some operations like `mean_removal` imply changes on the AdaptiveModel or Processor. That\'s why we return them.\n\n    :param processor: FARM Processor with a Tokenizer used for reading the corpus (e.g. Inference Processor)\n    :param model: FARM AdaptiveModel with an embedding layer in the LM (currently only supporting \'WordEmbedding_LM\' as a language model)\n    :param corpus: Path to a text file or a str \n    :param n_clusters: Number of clusters for S3E. The more clusters, the higher the dimensionality of the resulting embeddings.\n    :param mean_removal: Bool, whether to remove the mean from the token embeddings (preprocessing) \n    :param pca_removal: Bool, whether to remove pca components from the token embeddings (preprocessing)\n    :param pca_n_components: int, how many PCA components to fit if `pca_removal` is enabled \n    :param pca_n_top_components: int, how many top PCA components to remove if `pca_removal` is enabled \n    :param default_token_weight: float, what weight to assign for tokens that are in vocab but not in corpus\n    :param min_token_occurrences: int, mininum number of token occurrences in the corpus for keeping it in the vocab.\n                                  Helps to shrink the model & speed it up.\n    :param svd_postprocessing: Bool, whether to remove the top truncated SVD / LSA components from the sentence embeddings (postprocessing).\n                               Note: Requires creating all sentence embeddings once for the corpus slowing down this method substantially.\n                                     Doesn\'t impact later inference speed though.\n    :param use_gpu: bool, whether to use a GPU\n    :param batch_size: int, size of batch for the inferencer (only needed when `svd_postprocessing` is enabled)\n    :return: model, processor, s3e_stats\n    """"""\n\n    from farm.infer import Inferencer\n    from farm.modeling.tokenization import tokenize_with_metadata\n\n    # Get tokens of corpus\n    if isinstance(corpus, Path):\n        logger.info(""Reading corpus for fitting S3E "")\n        with open(corpus, ""r"") as f:\n            corpus = f.read()\n    else:\n        assert type(corpus) == str, ""`corpus` must be of type str or Path()""\n\n    tokenized_corpus = tokenize_with_metadata(corpus, processor.tokenizer)[""tokens""]\n    token_counts = dict(Counter(tokenized_corpus))\n    n_tokens = sum(token_counts.values())\n\n    # Trim vocab & embeddings to most frequent tokens (only to improve speed & ram consumption)\n    model.language_model.trim_vocab(token_counts, processor, min_threshold=min_token_occurrences)\n\n    # Normalize embeddings\n    model.language_model.normalize_embeddings(zero_mean=mean_removal, pca_removal=pca_removal,\n                                              pca_n_components=pca_n_components,\n                                              pca_n_top_components=pca_n_top_components)\n    normalized_word_embs = model.language_model.model.embeddings.cpu().numpy()\n\n    # Get token weights\n    token_weights = {}\n    eps = 1e-3\n    for word, id in processor.tokenizer.vocab.items():\n        if word in token_counts:\n            token_weights[id] = eps / (eps + token_counts[word] / n_tokens)\n        else:\n            # words that are in vocab but not present in corpus get the default weight\n            token_weights[id] = default_token_weight\n\n    # Construct Cluster\n    weight_list = np.array(list(token_weights.values()))\n    logger.info(\'Creating clusters for S3E embeddings\')\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(normalized_word_embs, sample_weight=weight_list)\n\n    s3e_stats = {""token_to_cluster"": kmeans.labels_,\n                 ""centroids"": kmeans.cluster_centers_,\n                 ""token_weights"": token_weights,\n                 ""svd_components"": None}\n\n    if svd_postprocessing:\n        logger.info(\'Post processing sentence embeddings using principal component removal\')\n\n        # Input\n        sentences = [{""text"": s} for s in corpus.split(""\\n"") if len(s.strip()) > 0]\n\n        # Get embeddings\n        inferencer = Inferencer(model=model, processor=processor, task_type=""embeddings"", gpu=use_gpu,\n                                batch_size=batch_size, extraction_strategy=""s3e"", extraction_layer=-1,\n                                s3e_stats=s3e_stats)\n        result = inferencer.inference_from_dicts(dicts=sentences)\n        sentence_embeddings = [s[""vec""] for s in result]\n        sentence_embeddings = np.vstack(sentence_embeddings)\n\n        # Principal Component Removal\n        svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0)\n        svd.fit(sentence_embeddings)\n        s3e_stats[""svd_components""] = svd.components_\n\n    return model, processor, s3e_stats\n\n\nif __name__ == ""__main__"":\n    convert_WordEmbeddings(embedding_filename=""../../saved_models/glove-normal/vectors.txt"",\n                           vocab_filename=""../../saved_models/glove-normal/vocab.txt"",\n                           output_path=""../../saved_models/glove-converted"",\n                           language=""German"")\n'"
farm/visual/__init__.py,0,b''
test/benchmarks/conftest.py,0,"b'from pathlib import Path\n\nimport pytest\n\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\n\n\n@pytest.fixture(scope=""session"")\ndef onnx_adaptive_model_qa(use_gpu, num_processes):\n    model_name_or_path = ""deepset/bert-base-cased-squad2""\n    onnx_model_export_path = Path(""benchmarks/onnx-export"")\n    if not (onnx_model_export_path / ""model.onnx"").is_file():\n        model = AdaptiveModel.convert_from_transformers(\n            model_name_or_path, device=""cpu"", task_type=""question_answering""\n        )\n        model.convert_to_onnx(onnx_model_export_path)\n\n    model = Inferencer.load(\n        onnx_model_export_path, task_type=""question_answering"", batch_size=1, num_processes=num_processes, gpu=use_gpu\n    )\n\n    return model\n'"
test/benchmarks/convert_result_to_csv.py,0,"b'import json\nimport csv\n\nwith open(""result.json"") as f:\n    results = json.load(f)\n\nwith open(""result.csv"", ""w"") as f:\n    fieldnames = list(results[""benchmarks""][0][""params""].keys())\n    fieldnames.append(""time"")\n    writer = csv.DictWriter(f, fieldnames=fieldnames)\n    writer.writeheader()\n\n    for benchmark in results[""benchmarks""]:\n        writer.writerow({""time"": benchmark[""stats""][""total""], **benchmark[""params""]})\n'"
test/benchmarks/question_answering.py,2,"b'import logging\n\nimport pytest\nimport torch\n\nlogger = logging.getLogger(__name__)\n\n\n@pytest.mark.parametrize(""max_seq_len"", [128, 256, 384])\n@pytest.mark.parametrize(""batch_size"", [1, 4, 16, 64])\n@pytest.mark.parametrize(""document_size"", [10_000, 100_000])\n@pytest.mark.parametrize(""num_processes"", [0], scope=""session"")\ndef test_question_answering_pytorch(adaptive_model_qa, benchmark, max_seq_len, batch_size, use_gpu, document_size):\n    if use_gpu and not torch.cuda.is_available():\n        pytest.skip(""Skipping benchmarking on GPU as it not available."")\n\n    if not use_gpu and document_size > 10_000:\n        pytest.skip(""Document size is large for CPU"")\n\n    with open(""benchmarks/sample_file.txt"") as f:\n        context = f.read()[:document_size]\n    QA_input = [{""qas"": [""When were the first traces of Human life found in France?""], ""context"": context}]\n\n    adaptive_model_qa.batch_size = batch_size\n    adaptive_model_qa.max_seq_len = max_seq_len\n    benchmark.pedantic(\n        target=adaptive_model_qa.inference_from_dicts, args=(QA_input,), warmup_rounds=1, iterations=3,\n    )\n\n\n@pytest.mark.parametrize(""max_seq_len"", [128, 256, 384])\n@pytest.mark.parametrize(""batch_size"", [1, 4, 16, 64])\n@pytest.mark.parametrize(""document_size"", [10_000, 100_000])\n@pytest.mark.parametrize(""num_processes"", [0], scope=""session"")\ndef test_question_answering_onnx(onnx_adaptive_model_qa, benchmark, max_seq_len, batch_size, use_gpu, document_size):\n    if use_gpu and not torch.cuda.is_available():\n        pytest.skip(""Skipping benchmarking on GPU as it not available."")\n\n    if not use_gpu and document_size > 10_000:\n        pytest.skip(""Document size is large for CPU"")\n\n    with open(""benchmarks/sample_file.txt"") as f:\n        context = f.read()[:document_size]\n    QA_input = [{""qas"": [""When were the first traces of Human life found in France?""], ""context"": context}]\n\n    onnx_adaptive_model_qa.batch_size = batch_size\n    onnx_adaptive_model_qa.max_seq_len = max_seq_len\n    benchmark.pedantic(\n        target=onnx_adaptive_model_qa.inference_from_dicts, args=(QA_input,), warmup_rounds=1, iterations=3\n    )\n'"
test/benchmarks/question_answering_accuracy.py,0,"b'import logging\nimport os\nfrom pathlib import Path\nfrom time import time\n\nimport numpy as np\nfrom dotmap import DotMap\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import SquadProcessor\nfrom farm.data_handler.utils import write_squad_predictions\nfrom farm.eval import Evaluator\nfrom farm.evaluation import squad_evaluation\nfrom farm.infer import Inferencer\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.modeling.prediction_head import QuestionAnsweringHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, initialize_device_settings\n\n\ndef test_evaluation():\n    ##########################\n    ########## Settings\n    ##########################\n    lang_model = ""deepset/roberta-base-squad2""\n    do_lower_case = False\n\n    test_assertions = True\n\n    data_dir = Path(""testsave/data/squad20"")\n    evaluation_filename = ""dev-v2.0.json""\n\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n\n    # loading models and evals\n    model = AdaptiveModel.convert_from_transformers(lang_model, device=device, task_type=""question_answering"")\n    model.prediction_heads[0].no_ans_boost = 0\n    model.prediction_heads[0].n_best = 1\n\n    tokenizer = Tokenizer.load(pretrained_model_name_or_path=lang_model,do_lower_case=do_lower_case)\n    processor = SquadProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=256,\n        label_list= [""start_token"", ""end_token""],\n        metric=""squad"",\n        train_filename=None,\n        dev_filename=None,\n        dev_split=0,\n        test_filename=evaluation_filename,\n        data_dir=data_dir,\n        doc_stride=128,\n    )\n\n    starttime = time()\n\n    data_silo = DataSilo(processor=processor, batch_size=50)\n    model.connect_heads_with_processor(data_silo.processor.tasks, require_labels=True)\n    evaluator = Evaluator(data_loader=data_silo.get_data_loader(""test""), tasks=data_silo.processor.tasks, device=device)\n\n    # 1. Test FARM internal evaluation\n    results = evaluator.eval(model)\n    f1_score = results[0][""f1""]*100\n    em_score = results[0][""EM""]*100\n    tnrecall = results[0][""top_n_recall""]*100\n    elapsed = time() - starttime\n    print(results)\n    print(elapsed)\n\n    gold_EM = 77.7478\n    gold_f1 = 82.1557\n    gold_tnrecall = 84.0646 # top 1 recall\n    gold_elapsed = 70 # 4x V100\n    if test_assertions:\n        np.testing.assert_allclose(em_score, gold_EM, rtol=0.001, err_msg=f""FARM Eval changed for EM by: {em_score-gold_EM}"")\n        np.testing.assert_allclose(f1_score, gold_f1, rtol=0.001, err_msg=f""FARM Eval changed for f1 score by: {f1_score-gold_f1}"")\n        np.testing.assert_allclose(tnrecall, gold_tnrecall, rtol=0.001, err_msg=f""FARM Eval changed for top 1 recall by: {em_score-gold_EM}"")\n        np.testing.assert_allclose(elapsed, gold_elapsed, rtol=0.1, err_msg=f""FARM Eval speed changed significantly by: {elapsed - gold_elapsed} seconds"")\n\n\n    # 2. Test FARM predictions with outside eval script\n    starttime = time()\n    model = Inferencer(model=model, processor=processor, task_type=""question_answering"", batch_size=50, gpu=device.type==""cuda"")\n    filename = data_dir / evaluation_filename\n    result = model.inference_from_file(file=filename)\n\n    elapsed = time() - starttime\n\n    os.makedirs(""../testsave"", exist_ok=True)\n    write_squad_predictions(\n        predictions=result,\n        predictions_filename=filename,\n        out_filename=""testsave/predictions.json""\n    )\n    script_params = {""data_file"": filename,\n              ""pred_file"": ""testsave/predictions.json"",\n              ""na_prob_thresh"" : 1,\n              ""na_prob_file"": False,\n              ""out_file"": False}\n    results_official = squad_evaluation.main(OPTS=DotMap(script_params))\n    f1_score = results_official[""f1""]\n    em_score = results_official[""exact""]\n\n    gold_EM = 78.4890\n    gold_f1 = 81.7104\n    gold_elapsed = 66 # 4x V100\n    print(elapsed)\n    if test_assertions:\n        np.testing.assert_allclose(em_score, gold_EM, rtol=0.001,\n                                   err_msg=f""Eval with official script changed for EM by: {em_score - gold_EM}"")\n        np.testing.assert_allclose(f1_score, gold_f1, rtol=0.001,\n                                   err_msg=f""Eval with official script changed for f1 score by: {f1_score - gold_f1}"")\n        np.testing.assert_allclose(elapsed, gold_elapsed, rtol=0.1,\n                                   err_msg=f""Inference speed changed significantly by: {elapsed - gold_elapsed} seconds"")\n\n\ndef train_evaluation_single(seed=42):\n    ##########################\n    ########## Settings\n    ##########################\n    set_all_seeds(seed=seed)\n    device, n_gpu = initialize_device_settings(use_cuda=True)\n    batch_size = 32*4 # 4x V100\n    n_epochs = 2\n    evaluate_every = 2000000 # disabling dev eval\n    lang_model = ""roberta-base""\n    do_lower_case = False  # roberta is a cased model\n    train_filename = ""train-v2.0.json""\n    dev_filename = ""dev-v2.0.json""\n\n\n    # Load model and train\n    tokenizer = Tokenizer.load(\n        pretrained_model_name_or_path=lang_model, do_lower_case=do_lower_case\n    )\n    processor = SquadProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=256,\n        label_list=[""start_token"", ""end_token""],\n        metric=""squad"",\n        train_filename=train_filename,\n        dev_filename=dev_filename,\n        test_filename=None,\n        data_dir=Path(""testsave/data/squad20""),\n    )\n    data_silo = DataSilo(processor=processor, batch_size=batch_size, distributed=False)\n    language_model = LanguageModel.load(lang_model)\n    prediction_head = QuestionAnsweringHead()\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_token""],\n        device=device,\n    )\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=3e-5,\n        schedule_opts={""name"": ""LinearWarmup"", ""warmup_proportion"": 0.2},\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=n_epochs,\n        device=device\n    )\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=evaluate_every,\n        device=device,\n    )\n    starttime = time()\n    trainer.train()\n    elapsed = time() - starttime\n\n    save_dir = Path(""testsave/roberta-qa-dev"")\n    model.save(save_dir)\n    processor.save(save_dir)\n\n    # Create Evaluator\n    evaluator = Evaluator(data_loader=data_silo.get_data_loader(""dev""), tasks=data_silo.processor.tasks, device=device)\n\n    results = evaluator.eval(model)\n    f1_score = results[0][""f1""] * 100\n    em_score = results[0][""EM""] * 100\n    tnrecall = results[0][""top_n_recall""] * 100\n\n    print(results)\n    print(elapsed)\n\n\n    gold_f1 = 82.155\n    gold_EM = 77.714\n    gold_tnrecall = 97.3721 #\n    gold_elapsed = 1286.30\n    np.testing.assert_allclose(f1_score, gold_f1, rtol=0.01,\n                               err_msg=f""FARM Training changed for f1 score by: {f1_score - gold_f1}"")\n    np.testing.assert_allclose(em_score, gold_EM, rtol=0.01,\n                               err_msg=f""FARM Training changed for EM by: {em_score - gold_EM}"")\n    np.testing.assert_allclose(tnrecall, gold_tnrecall, rtol=0.01,\n                               err_msg=f""FARM Training changed for top 1 recall by: {em_score - gold_EM}"")\n    np.testing.assert_allclose(elapsed, gold_elapsed, rtol=0.1, err_msg=f""FARM Eval speed changed significantly by: {elapsed - gold_elapsed} seconds"")\n\nif __name__ == ""__main__"":\n    logging.disable(logging.WARNING)\n\n    test_evaluation()\n\n    train_evaluation_single(seed=42)'"
farm/conversion/onnx_optimization/BertOnnxModel.py,0,"b'# Source: https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/bert/BertOnnxModel.py\n#-------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation.  All rights reserved.\n# Licensed under the MIT License.\n#--------------------------------------------------------------------------\n\nimport logging\nimport sys\nimport argparse\nimport numpy as np\nfrom collections import deque\nfrom .OnnxModel import OnnxModel\n\ntry:\n    import onnx\n    from onnx import ModelProto, TensorProto, numpy_helper\nexcept:\n    pass\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass BertOptimizationOptions:\n\n    def __init__(self, model_type):\n        self.enable_attention = True\n        self.enable_skip_layer_norm = True\n        self.enable_embed_layer_norm = True\n        self.enable_bias_skip_layer_norm = True\n        self.enable_bias_gelu = True\n\n        if model_type == \'gpt2\':\n            self.enable_skip_layer_norm = False\n\n\nclass BertOnnxModel(OnnxModel):\n\n    def __init__(self, model, num_heads, hidden_size):\n        assert num_heads > 0\n        assert hidden_size % num_heads == 0\n\n        super(BertOnnxModel, self).__init__(model)\n        self.num_heads = num_heads\n        self.hidden_size = hidden_size\n\n        # A lookup table with mask input as key, and mask index output as value\n        self.mask_indice = {}\n        # A lookup table with mask input as key, and cast (to int32) output as value\n        self.mask_casted = {}\n\n        self.bert_inputs = []\n\n    def cast_input_to_int32(self, input_name):\n        cast_output = input_name + \'_int32\'\n\n        # Avoid consequent Cast nodes.\n        inputs = [input_name]\n        output_name_to_node = self.output_name_to_node()\n        if input_name in output_name_to_node:\n            parent_node = output_name_to_node[input_name]\n            if parent_node and parent_node.op_type == \'Cast\':\n                inputs = [parent_node.input[0]]\n\n        cast_node = onnx.helper.make_node(\'Cast\', inputs=inputs, outputs=[cast_output])\n        cast_node.attribute.extend([onnx.helper.make_attribute(""to"", int(TensorProto.INT32))])\n        self.add_node(cast_node)\n\n        return cast_output, cast_node\n\n    def cast_graph_input_to_int32(self, input_name):\n        graph_input = self.find_graph_input(input_name)\n        if graph_input is not None and graph_input.type.tensor_type.elem_type != TensorProto.INT32:\n            cast_output, cast_node = self.cast_input_to_int32(input_name)\n            logger.debug(f""Casted graph input {input_name} to int32"")\n            return True, cast_output\n\n        logger.debug(f""Did not cast graph input {input_name} to int32: found {graph_input is not None}"")\n        return False, input_name\n\n    def remove_cast_int32(self, input_name):\n        input_name_to_nodes = self.input_name_to_nodes()\n        nodes = input_name_to_nodes[input_name]\n        for node in nodes:\n            if node.op_type == ""Cast"":\n                is_int32 = False\n                for att in node.attribute:\n                    if att.name == \'to\' and att.i == int(TensorProto.INT32):\n                        is_int32 = True\n                        break\n                if is_int32:\n                    output_name = node.output[0]\n                    self.remove_node(node)\n                    self.replace_input_of_all_nodes(output_name, input_name)\n\n    def process_mask(self, input):\n        if input in self.mask_indice:\n            return self.mask_indice[input]\n\n        # Add cast to convert int64 to int32\n        if self.find_graph_input(input):\n            casted, input_name = self.cast_graph_input_to_int32(input)\n        else:\n            input_name, cast_node = self.cast_input_to_int32(input)\n            casted = True\n\n        if casted:\n            self.mask_casted[input] = input_name\n\n        # Add a mask processing node\n        output_name = self.create_node_name(\'mask_index\')\n        mask_index_node = onnx.helper.make_node(\'ReduceSum\',\n                                                inputs=[input_name],\n                                                outputs=[output_name],\n                                                name=self.create_node_name(\'ReduceSum\', \'MaskReduceSum\'))\n        mask_index_node.attribute.extend(\n            [onnx.helper.make_attribute(""axes"", [1]),\n             onnx.helper.make_attribute(""keepdims"", 0)])\n        self.add_node(mask_index_node)\n\n        self.mask_indice[input] = output_name\n        return output_name\n\n    def create_attention_node(self, mask_index, q_matmul, k_matmul, v_matmul, q_add, k_add, v_add, input, output):\n        q_weight = self.get_initializer(q_matmul.input[1])\n        k_weight = self.get_initializer(k_matmul.input[1])\n        v_weight = self.get_initializer(v_matmul.input[1])\n        q_bias = self.get_initializer(q_add.input[1])\n        k_bias = self.get_initializer(k_add.input[1])\n        v_bias = self.get_initializer(v_add.input[1])\n\n        qw = numpy_helper.to_array(q_weight)\n        assert qw.shape == (self.hidden_size, self.hidden_size)\n\n        kw = numpy_helper.to_array(k_weight)\n        assert kw.shape == (self.hidden_size, self.hidden_size)\n\n        vw = numpy_helper.to_array(v_weight)\n        assert vw.shape == (self.hidden_size, self.hidden_size)\n\n        qkv_weight = np.stack((qw, kw, vw), axis=-2)\n\n        qb = numpy_helper.to_array(q_bias)\n        assert qb.shape == (self.hidden_size,)\n\n        kb = numpy_helper.to_array(k_bias)\n        assert kb.shape == (self.hidden_size,)\n\n        vb = numpy_helper.to_array(v_bias)\n        assert vb.shape == (self.hidden_size,)\n\n        qkv_bias = np.stack((qb, kb, vb), axis=-2)\n\n        attention_node_name = self.create_node_name(\'Attention\')\n\n        weight = onnx.helper.make_tensor(name=attention_node_name + \'_qkv_weight\',\n                                         data_type=TensorProto.FLOAT,\n                                         dims=[self.hidden_size, 3 * self.hidden_size],\n                                         vals=qkv_weight.flatten().tolist())\n        self.add_initializer(weight)\n\n        bias = onnx.helper.make_tensor(name=attention_node_name + \'_qkv_bias\',\n                                       data_type=TensorProto.FLOAT,\n                                       dims=[3 * self.hidden_size],\n                                       vals=qkv_bias.flatten().tolist())\n        self.add_initializer(bias)\n\n        attention_node = onnx.helper.make_node(\n            \'Attention\',\n            inputs=[input, attention_node_name + \'_qkv_weight\', attention_node_name + \'_qkv_bias\', mask_index],\n            outputs=[output],\n            name=attention_node_name)\n        attention_node.domain = ""com.microsoft""\n        attention_node.attribute.extend([onnx.helper.make_attribute(""num_heads"", self.num_heads)])\n\n        self.add_node(attention_node)\n\n    def fuse_attention(self):\n        """"""\n        Fuse Attention subgraph into one Attention node.\n        """"""\n        input_name_to_nodes = self.input_name_to_nodes()\n        output_name_to_node = self.output_name_to_node()\n\n        nodes_to_remove = []\n        attention_count = 0\n\n        skip_layer_norm_nodes = self.get_nodes_by_op_type(""SkipLayerNormalization"")\n        for normalize_node in skip_layer_norm_nodes:\n            # SkipLayerNormalization has two inputs, and one of them is the\n            # root input for attention.\n            qkv_nodes = self.match_parent_path(normalize_node, [\'Add\', \'MatMul\', \'Reshape\', \'Transpose\', \'MatMul\'],\n                                               [None, 0, 0, 0, 0])\n            if qkv_nodes is None:\n                continue\n\n            other_inputs = []\n            for i, input in enumerate(normalize_node.input):\n                if input not in output_name_to_node:\n                    continue\n\n                if input == qkv_nodes[0].output[0]:\n                    continue\n                other_inputs.append(input)\n            if len(other_inputs) != 1:\n                continue\n\n            root_input = other_inputs[0]\n            children = input_name_to_nodes[root_input]\n            children_types = [child.op_type for child in children]\n            if children_types.count(\'MatMul\') != 3:\n                continue\n\n            (add_qkv, matmul_qkv, reshape_qkv, transpose_qkv, matmul_qkv) = qkv_nodes\n\n            v_nodes = self.match_parent_path(matmul_qkv, [\'Transpose\', \'Reshape\', \'Add\', \'MatMul\'], [1, 0, 0, 0])\n            if v_nodes is None:\n                logger.debug(""fuse_attention: failed to match v path"")\n                continue\n            (transpose_v, reshape_v, add_v, matmul_v) = v_nodes\n\n            qk_nodes = self.match_parent_path(matmul_qkv, [\'Softmax\', \'Add\', \'Div\', \'MatMul\'], [0, 0, 0, 0])\n            if qk_nodes is None:\n                qk_nodes = self.match_parent_path(matmul_qkv, [\'Softmax\', \'Add\', \'Mul\', \'MatMul\'], [0, 0, 0, 0])\n                if qk_nodes is None:\n                    logger.debug(""fuse_attention: failed to match qk path"")\n                    continue\n            (softmax_qk, add_qk, div_qk, matmul_qk) = qk_nodes\n\n            q_nodes = self.match_parent_path(matmul_qk, [\'Transpose\', \'Reshape\', \'Add\', \'MatMul\'], [0, 0, 0, 0])\n            if q_nodes is None:\n                logger.debug(""fuse_attention: failed to match q path"")\n                continue\n            (transpose_q, reshape_q, add_q, matmul_q) = q_nodes\n\n            k_nodes = self.match_parent_path(matmul_qk, [\'Transpose\', \'Reshape\', \'Add\', \'MatMul\'], [1, 0, 0, 0])\n            if k_nodes is None:\n                k_nodes = self.match_parent_path(matmul_qk, [\'Transpose\', \'Transpose\', \'Reshape\', \'Add\', \'MatMul\'],\n                                                 [1, 0, 0, 0, 0])\n                if k_nodes is None:\n                    logger.debug(""fuse_attention: failed to match k path"")\n                    continue\n                (transpose_k, transpose_k_2, reshape_k, add_k, matmul_k) = k_nodes\n            else:\n                (transpose_k, reshape_k, add_k, matmul_k) = k_nodes\n\n            mask_nodes = self.match_parent_path(add_qk, [\'Mul\', \'Sub\', \'Cast\', \'Unsqueeze\', \'Unsqueeze\'],\n                                                [1, 0, 1, 0, 0])\n            if mask_nodes is None:\n                logger.debug(""fuse_attention: failed to match mask path"")\n                continue\n            (mul_mask, sub_mask, cast_mask, unsqueeze_mask, unsqueeze_mask_0) = mask_nodes\n\n            if matmul_v.input[0] == root_input and matmul_q.input[0] == root_input and matmul_v.input[0] == root_input:\n                mask_index = self.process_mask(unsqueeze_mask_0.input[0])\n                self.create_attention_node(mask_index, matmul_q, matmul_k, matmul_v, add_q, add_k, add_v, root_input,\n                                           reshape_qkv.output[0])\n                nodes_to_remove.extend([reshape_qkv, transpose_qkv, matmul_qkv])\n                nodes_to_remove.extend(qk_nodes)\n                nodes_to_remove.extend(q_nodes)\n                nodes_to_remove.extend(k_nodes)\n                nodes_to_remove.extend(v_nodes)\n                nodes_to_remove.extend(mask_nodes)\n                attention_count += 1\n\n        self.remove_nodes(nodes_to_remove)\n        self.update_graph()\n        logger.info(f""Fused Attention count:{attention_count}"")\n\n    def fuse_gelu(self):\n        self.fuse_gelu_with_elf()\n        self.fuse_gelu_with_tanh()\n\n    """"""\n     Fuse Gelu with Erf into one node:\n     Pattern 1:\n                   +-------Mul(0.5)---------------------+\n                   |                                    |\n                   |                                    v\n                [root] --> Div -----> Erf  --> Add --> Mul -->\n                          (B=1.4142...)       (1)\n\n      Pattern 2:\n                   +------------------------------------+\n                   |                                    |\n                   |                                    v\n                [root] --> Div -----> Erf  --> Add --> Mul -->Mul -->\n                          (B=1.4142...)       (1)            (0.5)\n\n     Note that constant input for Add and Mul could be first or second input: like either A=0.5 or B=0.5 is fine.\n    """"""\n\n    def fuse_gelu_with_elf(self):\n        logger.debug(f""start fuse_gelu_with_elf"")\n        input_name_to_nodes = self.input_name_to_nodes()\n        output_name_to_node = self.output_name_to_node()\n\n        nodes_to_remove = []\n        nodes_to_add = []\n\n        for node in self.get_nodes_by_op_type(\'Erf\'):\n            erf_node = node\n\n            if erf_node.output[0] not in input_name_to_nodes:\n                continue\n            children = input_name_to_nodes[erf_node.output[0]]\n            if len(children) != 1 or children[0].op_type != \'Add\':\n                continue\n            add_after_erf = children[0]\n\n            if not self.has_constant_input(add_after_erf, 1):\n                continue\n\n            if add_after_erf.output[0] not in input_name_to_nodes:\n                continue\n            children = input_name_to_nodes[add_after_erf.output[0]]\n            if len(children) != 1 or children[0].op_type != \'Mul\':\n                continue\n            mul_after_erf = children[0]\n\n            div = self.match_parent(erf_node, \'Div\', 0, output_name_to_node)\n            if div is None:\n                continue\n\n            if self.find_constant_input(div, 1.4142, delta=0.001) != 1:\n                continue\n\n            subgraph_input = div.input[0]\n\n            another = 1 if mul_after_erf.input[0] == add_after_erf.output[0] else 0\n            if subgraph_input == mul_after_erf.input[another]:  # pattern 2\n                children = input_name_to_nodes[mul_after_erf.output[0]]\n                if len(children) != 1 or children[0].op_type != \'Mul\':\n                    continue\n                mul_half = children[0]\n                if not self.has_constant_input(mul_half, 0.5):\n                    continue\n                subgraph_output = mul_half.output[0]\n            else:  # pattern 1\n                mul_half = self.match_parent(mul_after_erf, \'Mul\', another, output_name_to_node)\n                if mul_half is None:\n                    continue\n\n                if not self.has_constant_input(mul_half, 0.5):\n                    continue\n\n                if subgraph_input not in mul_half.input:\n                    continue\n\n                subgraph_output = mul_after_erf.output[0]\n\n            subgraph_nodes = [div, erf_node, add_after_erf, mul_after_erf, mul_half]\n            if not self.is_safe_to_fuse_nodes(subgraph_nodes, [subgraph_output], input_name_to_nodes,\n                                              output_name_to_node):\n                continue\n\n            nodes_to_remove.extend(subgraph_nodes)\n            gelu_node = onnx.helper.make_node(\'Gelu\', inputs=[subgraph_input], outputs=[subgraph_output])\n            gelu_node.domain = ""com.microsoft""\n            nodes_to_add.append(gelu_node)\n\n        self.remove_nodes(nodes_to_remove)\n        self.add_nodes(nodes_to_add)\n        if len(nodes_to_add) > 0:\n            logger.info(f""Fused Gelu count:{len(nodes_to_add)}"")\n\n    """"""\n     Fuse Gelu with tanh into one node:\n          +---------------------------+\n          |                           |\n          |                           v\n        [root] --> Pow --> Mul -----> Add  --> Mul --> Tanh --> Add --> Mul\n          |       (Y=3)   (B=0.0447...)       (B=0.7978...)    (B=1)     ^\n          |                                                              |\n          +------> Mul(B=0.5)--------------------------------------------+\n     Note that constant input for Add and Mul could be first or second input: like either A=0.5 or B=0.5 is fine.\n    """"""\n\n    def fuse_gelu_with_tanh(self):\n        logger.debug(f""start FastGelu fusion..."")\n        input_name_to_nodes = self.input_name_to_nodes()\n        output_name_to_node = self.output_name_to_node()\n\n        nodes_to_remove = []\n        nodes_to_add = []\n\n        for node in self.get_nodes_by_op_type(\'Tanh\'):\n            tanh_node = node\n\n            if node.output[0] not in input_name_to_nodes:\n                continue\n            children = input_name_to_nodes[node.output[0]]\n            if len(children) != 1 or children[0].op_type != \'Add\':\n                continue\n            add_after_tanh = children[0]\n\n            if not self.has_constant_input(add_after_tanh, 1.0):\n                continue\n\n            if add_after_tanh.output[0] not in input_name_to_nodes:\n                continue\n            children = input_name_to_nodes[add_after_tanh.output[0]]\n            if len(children) != 1 or children[0].op_type != \'Mul\':\n                continue\n            mul_after_tanh = children[0]\n\n            mul_half = self.match_parent(mul_after_tanh, \'Mul\', None, output_name_to_node)\n            if mul_half is None:\n                continue\n\n            i = self.find_constant_input(mul_half, 0.5)\n            if i < 0:\n                continue\n\n            root_node = self.get_parent(mul_half, 0 if i == 1 else 1, output_name_to_node)\n            if root_node is None:\n                continue\n\n            mul_before_tanh = self.match_parent(tanh_node, \'Mul\', 0, output_name_to_node)\n            if mul_before_tanh is None:\n                continue\n\n            i = self.find_constant_input(mul_before_tanh, 0.7978, delta=0.0001)\n            if i < 0:\n                continue\n\n            add_before_tanh = self.match_parent(mul_before_tanh, \'Add\', 0 if i == 1 else 1, output_name_to_node)\n            if add_before_tanh is None:\n                continue\n\n            mul_after_pow = self.match_parent(add_before_tanh, \'Mul\', None, output_name_to_node, exclude=[root_node])\n            if mul_after_pow is None:\n                continue\n\n            i = self.find_constant_input(mul_after_pow, 0.0447, delta=0.0001)\n            if i < 0:\n                continue\n\n            pow = self.match_parent(mul_after_pow, \'Pow\', 0 if i == 1 else 1, output_name_to_node)\n            if pow is None:\n                continue\n\n            if not self.has_constant_input(pow, 3.0):\n                continue\n\n            if pow.input[0] != root_node.output[0]:\n                continue\n\n            subgraph_nodes = [\n                mul_after_tanh, mul_half, add_after_tanh, tanh_node, mul_before_tanh, add_before_tanh, mul_after_pow,\n                pow\n            ]\n            if not self.is_safe_to_fuse_nodes(subgraph_nodes, [mul_after_tanh.output[0]], input_name_to_nodes,\n                                              output_name_to_node):\n                continue\n\n            nodes_to_remove.extend(subgraph_nodes)\n            gelu_node = onnx.helper.make_node(\'FastGelu\',\n                                              inputs=[root_node.output[0]],\n                                              outputs=mul_after_tanh.output,\n                                              name=self.create_node_name(\'FastGelu\'))\n            gelu_node.domain = ""com.microsoft""\n            nodes_to_add.append(gelu_node)\n\n        if len(nodes_to_add) > 0:\n            logger.info(f""Fused FastGelu count: {len(nodes_to_add)}"")\n\n        self.remove_nodes(nodes_to_remove)\n        self.add_nodes(nodes_to_add)\n\n    def fuse_bias_gelu(self, is_fastgelu):\n        gelu_op_type = \'FastGelu\' if is_fastgelu else \'Gelu\'\n        bias_gelu_op_type = \'FastGelu\' if is_fastgelu else \'BiasGelu\'\n        logger.debug(f""start Bias and {gelu_op_type} fusion..."")\n\n        input_name_to_nodes = self.input_name_to_nodes()\n        output_name_to_node = self.output_name_to_node()\n        nodes_to_remove = []\n        nodes_to_add = []\n\n        # Don\'t need to fuse Gelu+Add here because ORT native code can handle it\n        for node in self.get_nodes_by_op_type(gelu_op_type):\n            if len(node.input) != 1:\n                continue\n\n            nodes = self.match_parent_path(node, [\'Add\', \'MatMul\'], [0, None])\n            if nodes is None:\n                continue\n            (add, matmul) = nodes\n\n            # bias should be one dimension\n            bias_index = -1\n            for i, input in enumerate(add.input):\n                initializer = self.get_initializer(input)\n                if initializer is None:\n                    continue\n                bias_index = i\n                bias_weight = numpy_helper.to_array(initializer)\n                break\n            if bias_weight is None:\n                continue\n            if len(bias_weight.shape) != 1:\n                continue\n\n            subgraph_nodes = [node, add]\n            if not self.is_safe_to_fuse_nodes(subgraph_nodes, [node.output[0]], input_name_to_nodes,\n                                              output_name_to_node):\n                continue\n\n            nodes_to_remove.extend(subgraph_nodes)\n            gelu_node = onnx.helper.make_node(bias_gelu_op_type,\n                                              inputs=[matmul.output[0], add.input[bias_index]],\n                                              outputs=node.output,\n                                              name=self.create_node_name(bias_gelu_op_type, gelu_op_type + ""_AddBias_""))\n            gelu_node.domain = ""com.microsoft""\n            nodes_to_add.append(gelu_node)\n\n        if len(nodes_to_add) > 0:\n            logger.info(f""Fused {bias_gelu_op_type} with Bias count:{len(nodes_to_add)}"")\n\n        self.remove_nodes(nodes_to_remove)\n        self.add_nodes(nodes_to_add)\n\n    def fuse_add_bias_skip_layer_norm(self):\n        logger.debug(f""start Bias and SkipLayerNormalization fusion..."")\n        input_name_to_nodes = self.input_name_to_nodes()\n        output_name_to_node = self.output_name_to_node()\n        nodes_to_remove = []\n        nodes_to_add = []\n\n        skip_layer_norm_nodes = self.get_nodes_by_op_type(""SkipLayerNormalization"")\n        for node in skip_layer_norm_nodes:\n            if len(node.input) != 4:\n                continue\n\n            return_indice = []\n            nodes = self.match_parent_path(node, [\'Add\', \'MatMul\'], [None, None], None, return_indice)\n            if nodes is None:\n                continue\n            assert len(return_indice) == 2\n            add_input_index = return_indice[0]\n            if add_input_index >= 2:\n                continue\n\n            (add, matmul) = nodes\n\n            # bias should be one dimension\n            bias_index = -1\n            for i, input in enumerate(add.input):\n                initializer = self.get_initializer(input)\n                if initializer is None:\n                    continue\n                bias_index = i\n                bias_weight = numpy_helper.to_array(initializer)\n                break\n            if bias_weight is None:\n                logger.debug(f""Bias weight not found"")\n                continue\n            if len(bias_weight.shape) != 1:\n                logger.debug(f""Bias weight is not 1D"")\n                continue\n\n            subgraph_nodes = [node, add]\n            if not self.is_safe_to_fuse_nodes(subgraph_nodes, [node.output[0]], input_name_to_nodes,\n                                              output_name_to_node):\n                logger.debug(f""Skip fusing SkipLayerNormalization with Bias since it is not safe"")\n                continue\n\n            nodes_to_remove.extend(subgraph_nodes)\n            new_node = onnx.helper.make_node(""SkipLayerNormalization"",\n                                             inputs=[\n                                                 node.input[1 - add_input_index], matmul.output[0], node.input[2],\n                                                 node.input[3], add.input[bias_index]\n                                             ],\n                                             outputs=node.output,\n                                             name=self.create_node_name(""SkipLayerNormalization"",\n                                                                        ""SkipLayerNorm_AddBias_""))\n            new_node.domain = ""com.microsoft""\n            nodes_to_add.append(new_node)\n\n        if len(nodes_to_add) > 0:\n            logger.info(f""Fused SkipLayerNormalization with Bias count:{len(nodes_to_add)}"")\n\n        self.remove_nodes(nodes_to_remove)\n        self.add_nodes(nodes_to_add)\n\n    def fuse_reshape(self):\n        logger.debug(f""start Reshape fusion..."")\n        nodes = self.nodes()\n        input_name_to_nodes = self.input_name_to_nodes()\n        output_name_to_node = self.output_name_to_node()\n\n        nodes_to_remove = []\n        nodes_to_add = []\n\n        for reshape_node in self.get_nodes_by_op_type(\'Reshape\'):\n            if reshape_node.input[1] not in output_name_to_node:\n                continue\n            concat_node = output_name_to_node[reshape_node.input[1]]\n            if concat_node.op_type != \'Concat\' or len(concat_node.input) < 3 or len(concat_node.input) > 4:\n                continue\n\n            path0 = self.match_parent_path(concat_node, [\'Unsqueeze\', \'Gather\', \'Shape\'], [0, 0, 0],\n                                           output_name_to_node)\n            if path0 is None:\n                continue\n            (unsqueeze_0, gather_0, shape_0) = path0\n\n            path1 = self.match_parent_path(concat_node, [\'Unsqueeze\', \'Gather\', \'Shape\'], [1, 0, 0],\n                                           output_name_to_node)\n            if path1 is None:\n                continue\n            (unsqueeze_1, gather_1, shape_1) = path1\n\n            shape = []\n            gather_value = self.get_constant_value(gather_0.input[1])\n            if gather_value == 0:\n                shape.append(0)\n\n            gather_value = self.get_constant_value(gather_1.input[1])\n            if gather_value == 1:\n                shape.append(0)\n\n            if len(shape) != 2:\n                continue\n\n            path2 = []\n            path3 = []\n            shape_nodes = [shape_0, shape_1]\n            if len(concat_node.input) == 3 and self.get_initializer(concat_node.input[2]) is None:\n                path2 = self.match_parent_path(concat_node, [\'Unsqueeze\', \'Mul\', \'Gather\', \'Shape\'], [2, 0, 0, 0],\n                                               output_name_to_node)\n                if path2 is None:\n                    path2 = self.match_parent_path(\n                        concat_node, [\'Unsqueeze\', \'Mul\', \'Squeeze\', \'Slice\', \'Shape\'], [2, 0, 0, 0, 0],\n                        output_name_to_node)  # GPT2 exported by PyTorch 1.4 with opset_version=11\n                    if path2 is None:\n                        continue\n\n                path3 = self.match_parent_path(concat_node, [\'Unsqueeze\', \'Mul\', \'Gather\', \'Shape\'], [2, 0, 1, 0],\n                                               output_name_to_node)\n                if path3 is None:\n                    path3 = self.match_parent_path(\n                        concat_node, [\'Unsqueeze\', \'Mul\', \'Squeeze\', \'Slice\', \'Shape\'], [2, 0, 1, 0, 0],\n                        output_name_to_node)  # GPT2 exported by PyTorch 1.4 with opset_version=11\n                    if path3 is None:\n                        continue\n\n                shape_nodes.extend([path2[-1], path3[-1]])\n                shape.append(-1)\n            elif (len(concat_node.input) > 2):\n                concat_2 = self.get_initializer(concat_node.input[2])\n                if concat_2 is None:\n                    continue\n                concat_value = numpy_helper.to_array(concat_2)\n                if isinstance(concat_value, list):\n                    shape.extend(concat_value)\n                else:\n                    shape.append(concat_value)\n\n            if len(concat_node.input) == 4 and self.get_initializer(concat_node.input[3]) is None:\n                if -1 in shape:\n                    continue\n\n                path2 = self.match_parent_path(concat_node, [\'Unsqueeze\', \'Div\', \'Gather\', \'Shape\'], [3, 0, 0, 0],\n                                               output_name_to_node)\n                if path2 is None:\n                    path2 = self.match_parent_path(\n                        concat_node, [\'Unsqueeze\', \'Div\', \'Squeeze\', \'Slice\', \'Shape\'], [3, 0, 0, 0, 0],\n                        output_name_to_node)  # GPT2 exported by PyTorch 1.4 with opset_version=11\n                    if path2 is None:\n                        continue\n                shape_nodes.extend([path2[-1]])\n                shape.append(-1)\n            elif (len(concat_node.input) > 3):\n                concat_3 = self.get_initializer(concat_node.input[3])\n                if concat_3 is None:\n                    continue\n\n                concat_value = numpy_helper.to_array(concat_3)\n                if isinstance(concat_value, list):\n                    shape.extend(concat_value)\n                else:\n                    shape.append(concat_value)\n\n            root_input = reshape_node.input[0]\n            same_shape_input = True\n            for shape_node in shape_nodes:\n                if shape_node.input[0] != root_input:\n                    same_shape_input = False\n\n            if not same_shape_input:\n                continue\n\n            shape_value = np.asarray(shape, dtype=np.int64)\n\n            constant_shape_name = self.create_node_name(\'Constant\', \'constant_shape\')\n            new_node = onnx.helper.make_node(\'Constant\',\n                                             inputs=[],\n                                             outputs=[constant_shape_name],\n                                             value=onnx.helper.make_tensor(name=\'const_tensor\',\n                                                                           data_type=TensorProto.INT64,\n                                                                           dims=shape_value.shape,\n                                                                           vals=shape_value))\n            reshape_node.input[1] = constant_shape_name\n            reshape_node.name = self.create_node_name(\'Reshape\', \'Reshape_Fuse\')\n            nodes_to_remove.extend([concat_node])\n            nodes_to_remove.extend(path0)\n            nodes_to_remove.extend(path1)\n            nodes_to_remove.extend(path2)\n            nodes_to_remove.extend(path3)\n            nodes_to_add.append(new_node)\n\n        logger.info(f""Fused Reshape count:{len(nodes_to_add)}"")\n\n        self.remove_nodes(nodes_to_remove)\n        self.add_nodes(nodes_to_add)\n\n    """"""\n     Embed Layer Normalization will fuse embeddings and mask processing into one node.\n     The embeddings before conversion:\n\n     (input_ids) -------->  Gather ----------+       (segment_ids)\n        |                                    |            |\n        |                                    v            v\n        +--> Shape --> Expand -> Gather---->Add         Gather\n        |                ^                   |            |\n        |                |                   v            v\n        +---(optional graph)               SkipLayerNormalization\n\n      Optional graph is used to generate position list (0, 1, ...) per batch. It can be a constant in some model.\n\n      (input_ids) --> Gather -----+           Slice\n                                  |            |\n                                  v            v\n     (segment_ids)--> Gather --->Add        Reshape\n                                  |            |\n                                  v            v\n                              SkipLayerNormalization\n    """"""\n\n    def fuse_embed_layer_without_mask(self):\n        logger.debug(f""start EmbedLayerNormalization (no mask) fusion..."")\n        nodes = self.nodes()\n        input_name_to_nodes = self.input_name_to_nodes()\n        output_name_to_node = self.output_name_to_node()\n\n        nodes_to_remove = []\n\n        # Find the first normalize node could be embedding layer.\n        normalize_node = None\n\n        skip_layer_norm_nodes = self.get_nodes_by_op_type(""SkipLayerNormalization"")\n        for node in skip_layer_norm_nodes:\n            if self.match_parent_path(node, [\'Add\', \'Gather\'], [0, 0]) is not None:\n                if self.find_first_child_by_type(node, \'Attention\', input_name_to_nodes, recursive=False) is not None:\n                    normalize_node = node\n                    break\n                # In case user disables attention fusion, check whether subgraph looks like Attention.\n                if node.output[0] not in input_name_to_nodes:\n                    continue\n                children = input_name_to_nodes[node.output[0]]\n                children_types = sorted([child.op_type for child in children])\n                if children_types == [\'MatMul\', \'MatMul\', \'MatMul\', \'SkipLayerNormalization\']:\n                    normalize_node = node\n                    break\n\n        if normalize_node is None:\n            if len(self.get_nodes_by_op_type(""EmbedLayerNormalization"")) == 0:\n                logger.info(""Failed to find embedding layer"")\n            return\n\n        # Here we assume the order of embedding is word_embedding +\n        # position_embedding + segment_embedding.\n        word_embedding_path = self.match_parent_path(normalize_node, [\'Add\', \'Gather\'], [0, 0])\n        if word_embedding_path is None:\n            logger.info(""Failed to find word embedding"")\n            return\n        add_node, word_embedding_gather = word_embedding_path\n        input_ids = word_embedding_gather.input[1]\n\n        position_embedding_path = self.match_parent_path(normalize_node, [\'Reshape\', \'Slice\'], [1, 0])\n\n        position_embedding_expand = None\n        if position_embedding_path is None:\n            position_embedding_path = self.match_parent_path(add_node, [\'Gather\', \'Expand\', \'Shape\'], [1, 1, 1])\n            if position_embedding_path is None:\n                position_embedding_path = self.match_parent_path(\n                    add_node, [\'Gather\', \'Expand\', \'Concat\', \'Unsqueeze\', \'Gather\', \'Shape\'], [1, 1, 1, 1, 0, 0])\n                if position_embedding_path is None:\n                    logger.info(""Failed to find position embedding"")\n                    return\n                position_embedding_weight_node, position_embedding_expand, _, _, _, position_embedding_shape = position_embedding_path\n            else:\n                position_embedding_weight_node, position_embedding_expand, position_embedding_shape = position_embedding_path\n\n            if not position_embedding_shape is None and position_embedding_shape.input[0] != input_ids:\n                logger.info(""position and word embedding is expected to be applied on same input"")\n                return\n        else:\n            _, position_embedding_weight_node = position_embedding_path\n\n        segment_embedding_path = self.match_parent_path(normalize_node, [\'Gather\'], [1])\n        if segment_embedding_path is None:\n            segment_embedding_path = self.match_parent_path(normalize_node, [\'Add\', \'Gather\'], [0, 1])\n            if segment_embedding_path is None:\n                logger.info(""Failed to find segment embedding"")\n                return\n            _, segment_embedding_gather = segment_embedding_path\n        else:\n            segment_embedding_gather = segment_embedding_path[0]\n\n        segment_ids = segment_embedding_gather.input[1]\n\n        if position_embedding_expand:\n            input_parent = self.get_parent(position_embedding_shape, 0, output_name_to_node)\n            subgraph_nodes = self.get_parent_subgraph_nodes(position_embedding_expand,\n                                                            [input_parent] if input_parent else [], output_name_to_node)\n            nodes_to_remove.extend(subgraph_nodes)\n\n        nodes_to_remove.extend(word_embedding_path)\n        nodes_to_remove.extend(position_embedding_path)\n        nodes_to_remove.extend(segment_embedding_path)\n\n        nodes_to_remove.extend([normalize_node])\n\n        # store inputs for further processing\n        if self.find_graph_input(input_ids):\n            self.bert_inputs = [input_ids, segment_ids] if self.find_graph_input(segment_ids) else [input_ids]\n\n        # Cast input_ids and segment_ids to int32.\n        if self.find_graph_input(input_ids):\n            casted, input_ids = self.cast_graph_input_to_int32(input_ids)\n        else:\n            input_ids, input_ids_cast_node = self.cast_input_to_int32(input_ids)\n\n        if self.find_graph_input(segment_ids):\n            casted, segment_ids = self.cast_graph_input_to_int32(segment_ids)\n        else:\n            segment_ids, segment_ids_cast_node = self.cast_input_to_int32(segment_ids)\n\n            segment_id_path = self.match_parent_path(\n                segment_ids_cast_node, [\'ConstantOfShape\', \'Concat\', \'Unsqueeze\', \'Gather\', \'Shape\', \'Cast\'],\n                [0, 0, 1, 0, 0, 0])\n            if segment_id_path and input_ids_cast_node and input_ids_cast_node.input[0] == segment_id_path[-1].input[0]:\n                logger.debug(""Simplify semgent id path..."")\n                self.add_node(\n                    onnx.helper.make_node(\'Shape\', inputs=[input_ids_cast_node.input[0]], outputs=[""input_shape""]))\n                self.add_node(\n                    onnx.helper.make_node(\'ConstantOfShape\',\n                                          inputs=[""input_shape""],\n                                          outputs=[""zeros_for_input_shape""],\n                                          value=onnx.helper.make_tensor(""value"", onnx.TensorProto.INT32, [1], [1])))\n                segment_ids = ""zeros_for_input_shape""\n\n        embed_node = onnx.helper.make_node(\n            \'EmbedLayerNormalization\',\n            inputs=[\n                input_ids,\n                segment_ids,\n                word_embedding_gather.input[0],\n                position_embedding_weight_node.input[0],\n                segment_embedding_gather.input[0],\n                normalize_node.input[2],\n                normalize_node.input[3]  # gamma and beta\n            ],\n            outputs=[""embed_output"", ""dummy_mask_index""],\n            name=""EmbedLayer"")\n\n        embed_node.domain = ""com.microsoft""\n\n        self.replace_input_of_all_nodes(normalize_node.output[0], \'embed_output\')\n\n        self.remove_nodes(nodes_to_remove)\n\n        return embed_node\n\n    def fuse_embed_layer(self):\n        embed_node = self.fuse_embed_layer_without_mask()\n        if embed_node is None:\n            logger.info(""Fused EmbedLayerNormalization count: 0"")\n            return\n\n        if len(self.mask_indice) > 1:\n            logger.info(""There are multiple mask inputs found!"")\n        elif len(self.mask_indice) != 1:\n            logger.info(""Fused EmbedLayerNormalization (no mask) count: 1"")\n        else:\n            mask_input_name = next(iter(self.mask_indice))\n            mask_output_name = self.mask_indice[mask_input_name]\n            output_name_to_node = self.output_name_to_node()\n            mask_node = output_name_to_node[mask_output_name]\n\n            nodes_to_remove = []\n            nodes_to_remove.extend([mask_node])\n\n            # store inputs for further processing\n            self.bert_inputs.append(mask_input_name)\n\n            # When mask has been casted to int32, use that casted one as input of embed layer norm.\n            if mask_input_name in self.mask_casted:\n                mask_input_name = self.mask_casted[mask_input_name]\n\n            embed_node.input.append(mask_input_name)\n            embed_node.output[1] = mask_output_name\n            logger.info(""Added mask to EmbedLayerNormalization"")\n            logger.info(""Fused EmbedLayerNormalization count: 1"")\n\n        self.add_node(embed_node)\n        self.prune_graph()\n\n    def get_bert_inputs(self, include_mask=True):\n        return self.bert_inputs if include_mask else self.bert_inputs[:2]\n\n    def get_bert_input_shape(self):\n        graph = self.graph()\n        bert_inputs = self.get_bert_inputs()\n        for input in graph.input:\n            if input.name in bert_inputs:\n                tensor_type = input.type.tensor_type\n                if (tensor_type.HasField(""shape"")):\n                    batch_size = None\n                    d = tensor_type.shape.dim[0]\n                    if (d.HasField(""dim_value"")):\n                        batch_size = d.dim_value\n                    elif (d.HasField(""dim_param"")):\n                        batch_size =  str(d.dim_param)\n\n                    sequence_length = None\n                    d = tensor_type.shape.dim[1]\n                    if (d.HasField(""dim_value"")):\n                        sequence_length = d.dim_value\n                    elif (d.HasField(""dim_param"")):\n                        sequence_length = str(d.dim_param)\n                    return batch_size, sequence_length\n\n        return None, None\n\n    def change_input_to_int32(self):\n        original_opset_version = self.model.opset_import[0].version\n        graph = self.graph()\n\n        batch_size, sequence_length = self.get_bert_input_shape()\n        new_graph_inputs = []\n\n        bert_inputs = self.get_bert_inputs()\n        for input in graph.input:\n            if input.name in bert_inputs:\n                self.remove_cast_int32(input.name)\n                input_shape = [batch_size if isinstance(batch_size, int) else 1, sequence_length if isinstance(sequence_length, int) else 128]\n                int32_input = onnx.helper.make_tensor_value_info(input.name, TensorProto.INT32, input_shape)\n                new_graph_inputs.append(int32_input)\n            else:\n                new_graph_inputs.append(input)\n\n        graph_def = onnx.helper.make_graph(graph.node,\n                                           \'int32 inputs\',\n                                           new_graph_inputs,\n                                           graph.output,\n                                           initializer=graph.initializer,\n                                           value_info=graph.value_info)\n\n        self.model = onnx.helper.make_model(graph_def, producer_name=\'bert model optimizer\')\n\n        if isinstance(batch_size, str) or isinstance(sequence_length, str):\n            self.use_dynamic_axes(batch_size if isinstance(batch_size, str) else None, sequence_length if isinstance(sequence_length, str) else None)\n\n        # restore opset version\n        self.model.opset_import[0].version = original_opset_version\n\n    def use_dynamic_axes(self, dynamic_batch_dim=\'batch_size\', dynamic_seq_len=\'max_seq_len\'):\n        """"""\n        Update input and output shape to use dynamic axes.\n        """"""\n        bert_inputs = self.get_bert_inputs()\n        dynamic_batch_inputs = {}\n        for input in self.model.graph.input:\n            for bert_input in bert_inputs:\n                if bert_input == input.name:\n                    dim_proto = input.type.tensor_type.shape.dim[0]\n                    dim_proto.dim_param = dynamic_batch_dim\n                    if dynamic_seq_len is not None:\n                        dim_proto = input.type.tensor_type.shape.dim[1]\n                        dim_proto.dim_param = dynamic_seq_len\n\n        for output in self.model.graph.output:\n            dim_proto = output.type.tensor_type.shape.dim[0]\n            dim_proto.dim_param = dynamic_batch_dim\n\n    def fuse_layer_norm(self):\n        """"""\n         Fuse Layer Normalization subgraph into one node LayerNormalization:\n              +----------------------+\n              |                      |\n              |                      v\n          [Root] --> ReduceMean -->  Sub  --> Pow --> ReduceMean --> Add --> Sqrt --> Div --> Mul --> Add\n                     (axis=2 or -1)  |      (Y=2)   (axis=2 or -1)  (E-6 or E-12 or 0)    ^\n                                     |                                               |\n                                     +-----------------------------------------------+\n\n         It also handles cases of duplicated sub nodes exported from older version of PyTorch:\n              +----------------------+\n              |                      v\n              |           +-------> Sub-----------------------------------------------+\n              |           |                                                           |\n              |           |                                                           v\n          [Root] --> ReduceMean -->  Sub  --> Pow --> ReduceMean --> Add --> Sqrt --> Div  --> Mul --> Add\n              |                      ^\n              |                      |\n              +----------------------+\n        """"""\n        logger.debug(f""start LayerNormalization fusion..."")\n\n        input_name_to_nodes = self.input_name_to_nodes()\n        output_name_to_node = self.output_name_to_node()\n\n        nodes_to_remove = []\n        skip_layernorm_nodes = []\n        layernorm_nodes = []\n        for node in self.nodes():\n            if node.op_type == \'ReduceMean\':\n                children = self.get_children(node, input_name_to_nodes)\n                if len(children) == 0 or len(children) > 2:\n                    continue\n\n                parent = self.get_parent(node, 0, output_name_to_node)\n                if parent is None:\n                    continue\n\n                if children[0].op_type != \'Sub\' or self.get_parent(children[0], 0, output_name_to_node) != parent:\n                    continue\n\n                if len(children) == 2:\n                    if children[0].op_type != \'Sub\' or self.get_parent(children[1], 0, output_name_to_node) != parent:\n                        continue\n\n                div_node = None\n                for child in children:\n                    if child.op_type == \'Sub\':\n                        div_node = self.find_first_child_by_type(child, \'Div\', input_name_to_nodes, recursive=False)\n                        if div_node is not None:\n                            break\n                if div_node is None:\n                    continue\n\n                parent_nodes = self.match_parent_path(div_node, [\'Sqrt\', \'Add\', \'ReduceMean\', \'Pow\', \'Sub\'],\n                                                      [1, 0, 0, 0, 0], output_name_to_node)\n                if parent_nodes is None:\n                    continue\n\n                sqrt_node, second_add_node, reduce_mean_node, pow_node, sub_node = parent_nodes\n                if sub_node not in children:\n                    continue\n\n                i, add_weight = self.get_constant_input(second_add_node)\n                if add_weight is None or add_weight <= 0 or add_weight > 1.0E-5:\n                    continue\n\n                if not self.find_constant_input(pow_node, 2.0) == 1:\n                    continue\n\n                mul_node = input_name_to_nodes[div_node.output[0]][0]\n                if mul_node.op_type != \'Mul\':\n                    continue\n\n                last_add_node = input_name_to_nodes[mul_node.output[0]][0]\n                if last_add_node.op_type != \'Add\':\n                    continue\n\n                subgraph_nodes = [node]\n                subgraph_nodes.extend(children)\n                subgraph_nodes.extend(\n                    [last_add_node, mul_node, div_node, sqrt_node, second_add_node, reduce_mean_node, pow_node])\n                if not self.is_safe_to_fuse_nodes(subgraph_nodes, last_add_node.output, input_name_to_nodes,\n                                                  output_name_to_node):\n                    continue\n\n                weight_input = mul_node.input[1 - self.input_index(div_node.output[0], mul_node)]\n                bias_input = last_add_node.input[1 - self.input_index(mul_node.output[0], last_add_node)]\n\n                if not self.is_constant_with_specified_dimension(weight_input, 1, ""layernorm weight""):\n                    continue\n\n                if not self.is_constant_with_specified_dimension(bias_input, 1, ""layernorm bias""):\n                    continue\n\n                nodes_to_remove.extend(subgraph_nodes)\n\n                normalize_node = onnx.helper.make_node(\'LayerNormalization\',\n                                                       inputs=[node.input[0], weight_input, bias_input],\n                                                       outputs=[last_add_node.output[0]])\n                normalize_node.attribute.extend([onnx.helper.make_attribute(""epsilon"", float(add_weight))])\n                layernorm_nodes.extend([normalize_node])\n\n        self.remove_nodes(nodes_to_remove)\n        self.add_nodes(layernorm_nodes)\n        logger.info(f""Fused LayerNormalization count: {len(layernorm_nodes)}"")\n\n    def fuse_skip_layer_norm(self):\n        """"""\n         Fuse Add + LayerNormalization into one node: SkipLayerNormalization\n        """"""\n        logger.debug(f""start SkipLayerNormaliation fusion..."")\n\n        input_name_to_nodes = self.input_name_to_nodes()\n        output_name_to_node = self.output_name_to_node()\n\n        nodes_to_remove = []\n        skip_layernorm_nodes = []\n        for node in self.nodes():\n            if node.op_type == \'LayerNormalization\':\n                add = self.get_parent(node, 0, output_name_to_node)\n                if add is None:\n                    continue\n\n                if add.op_type == \'Add\' and self.is_safe_to_fuse_nodes([add, node], node.output, input_name_to_nodes,\n                                                                       output_name_to_node):\n                    nodes_to_remove.extend([add, node])\n                    normalize_node = onnx.helper.make_node(\n                        ""SkipLayerNormalization"",\n                        inputs=[add.input[0], add.input[1], node.input[1], node.input[2]],\n                        outputs=[node.output[0]],\n                        name=self.create_node_name(""SkipLayerNormalization"", name_prefix=""SkipLayerNorm""))\n                    normalize_node.domain = ""com.microsoft""\n                    skip_layernorm_nodes.extend([normalize_node])\n\n        self.remove_nodes(nodes_to_remove)\n        self.add_nodes(skip_layernorm_nodes)\n        logger.info(f""Fused SkipLayerNormalization count: {len(skip_layernorm_nodes)}"")\n\n    def preprocess(self):\n        return\n\n    def postprocess(self):\n        self.prune_graph()\n\n    def optimize(self, options: BertOptimizationOptions = None):\n        self.fuse_layer_norm()\n\n        self.fuse_gelu()\n\n        self.preprocess()\n\n        self.fuse_reshape()\n\n        if (options is None) or options.enable_skip_layer_norm:\n            self.fuse_skip_layer_norm()\n\n        if (options is None) or options.enable_attention:\n            self.fuse_attention()\n\n        if (options is None) or options.enable_embed_layer_norm:\n            self.fuse_embed_layer()\n\n        # Post-processing like removing extra reshape nodes.\n        self.postprocess()\n\n        # Bias fusion is done after postprocess to avoid extra Reshape between bias and Gelu/FastGelu/SkipLayerNormalization\n        if (options is None) or options.enable_bias_gelu:\n            # Fuse Gelu and Add Bias before it.\n            self.fuse_bias_gelu(is_fastgelu=True)\n            self.fuse_bias_gelu(is_fastgelu=False)\n\n        if (options is None) or options.enable_bias_skip_layer_norm:\n            # Fuse SkipLayerNormalization and Add Bias before it.\n            self.fuse_add_bias_skip_layer_norm()\n\n        self.remove_unused_constant()\n\n        # Use symbolic batch dimension in input and output.\n        self.use_dynamic_axes()\n\n        logger.info(f""opset verion: {self.model.opset_import[0].version}"")\n\n    def get_fused_operator_statistics(self):\n        """"""\n        Returns node count of fused operators.\n        """"""\n        op_count = {}\n        ops = [\n            \'EmbedLayerNormalization\', \'Attention\', \'Gelu\', \'FastGelu\', \'BiasGelu\', \'LayerNormalization\',\n            \'SkipLayerNormalization\'\n        ]\n        for op in ops:\n            nodes = self.get_nodes_by_op_type(op)\n            op_count[op] = len(nodes)\n        logger.info(f""Optimized operators:{op_count}"")\n        return op_count\n\n    def is_fully_optimized(self):\n        """"""\n        Returns True when the model is fully optimized.\n        """"""\n        op_count = self.get_fused_operator_statistics()\n        embed = op_count[\'EmbedLayerNormalization\']\n        attention = op_count[\'Attention\']\n        gelu = op_count[\'Gelu\'] + op_count[\'BiasGelu\'] + op_count[\'FastGelu\']\n        layer_norm = op_count[\'LayerNormalization\'] + op_count[\'SkipLayerNormalization\']\n        is_optimized = (embed > 0) and (attention > 0) and (attention == gelu) and (layer_norm >= 2 * attention)\n        logger.info(\n            f""EmbedLayer={embed}, Attention={attention}, Gelu={gelu}, LayerNormalization={layer_norm}, Successful={is_optimized}""\n        )\n        return is_optimized\n'"
farm/conversion/onnx_optimization/OnnxModel.py,0,"b'# Source: https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/bert/OnnxModel.py\n#-------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation.  All rights reserved.\n# Licensed under the MIT License.\n#--------------------------------------------------------------------------\n\nimport logging\nimport sys\nimport argparse\nimport numpy as np\nfrom collections import deque\ntry:\n    import onnx\n    from onnx import ModelProto, TensorProto, numpy_helper\nexcept:\n    pass\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass OnnxModel:\n\n    def __init__(self, model):\n        self.model = model\n        self.node_name_counter = {}\n\n    def input_name_to_nodes(self):\n        input_name_to_nodes = {}\n        for node in self.model.graph.node:\n            for input_name in node.input:\n                if input_name not in input_name_to_nodes:\n                    input_name_to_nodes[input_name] = [node]\n                else:\n                    input_name_to_nodes[input_name].append(node)\n        return input_name_to_nodes\n\n    def output_name_to_node(self):\n        output_name_to_node = {}\n        for node in self.model.graph.node:\n            for output_name in node.output:\n                output_name_to_node[output_name] = node\n        return output_name_to_node\n\n    def nodes(self):\n        return self.model.graph.node\n\n    def graph(self):\n        return self.model.graph\n\n    def remove_node(self, node):\n        if node in self.model.graph.node:\n            self.model.graph.node.remove(node)\n\n    def remove_nodes(self, nodes_to_remove):\n        for node in nodes_to_remove:\n            self.remove_node(node)\n\n    def add_node(self, node):\n        self.model.graph.node.extend([node])\n\n    def add_nodes(self, nodes_to_add):\n        self.model.graph.node.extend(nodes_to_add)\n\n    def add_initializer(self, tensor):\n        self.model.graph.initializer.extend([tensor])\n\n    def add_input(self, input):\n        self.model.graph.input.extend([input])\n\n    @staticmethod\n    def replace_node_input(node, old_input_name, new_input_name):\n        assert isinstance(old_input_name, str) and isinstance(new_input_name, str)\n        for j in range(len(node.input)):\n            if node.input[j] == old_input_name:\n                node.input[j] = new_input_name\n\n    def replace_input_of_all_nodes(self, old_input_name, new_input_name):\n        for node in self.model.graph.node:\n            OnnxModel.replace_node_input(node, old_input_name, new_input_name)\n\n    @staticmethod\n    def replace_node_output(node, old_output_name, new_output_name):\n        assert isinstance(old_output_name, str) and isinstance(new_output_name, str)\n        for j in range(len(node.output)):\n            if node.output[j] == old_output_name:\n                node.output[j] = new_output_name\n\n    def replace_output_of_all_nodes(self, old_output_name, new_output_name):\n        for node in self.model.graph.node:\n            OnnxModel.replace_node_output(node, old_output_name, new_output_name)\n\n    def get_initializer(self, name):\n        for tensor in self.model.graph.initializer:\n            if tensor.name == name:\n                return tensor\n        return None\n\n    def get_nodes_by_op_type(self, op_type):\n        return [n for n in self.model.graph.node if n.op_type == op_type]\n\n    def get_children(self, node, input_name_to_nodes=None):\n        if (input_name_to_nodes is None):\n            input_name_to_nodes = self.input_name_to_nodes()\n\n        children = []\n        for output in node.output:\n            if output in input_name_to_nodes:\n                for node in input_name_to_nodes[output]:\n                    children.append(node)\n        return children\n\n    def get_parents(self, node, output_name_to_node=None):\n        if output_name_to_node is None:\n            output_name_to_node = self.output_name_to_node()\n\n        parents = []\n        for input in node.input:\n            if input in output_name_to_node:\n                parents.append(output_name_to_node[input])\n        return parents\n\n    def get_parent(self, node, i, output_name_to_node=None):\n        if output_name_to_node is None:\n            output_name_to_node = self.output_name_to_node()\n\n        if len(node.input) <= i:\n            return None\n\n        input = node.input[i]\n        if input not in output_name_to_node:\n            return None\n\n        return output_name_to_node[input]\n\n    def match_first_parent(self, node, parent_op_type, output_name_to_node, exclude=[]):\n        \'\'\'\n        Find parent node based on constraints on op_type.\n\n        Args:\n            node (str): current node name.\n            parent_op_type (str): constraint of parent node op_type.\n            output_name_to_node (dict): dictionary with output name as key, and node as value.\n            exclude (list): list of nodes that are excluded (not allowed to match as parent).\n\n        Returns:\n            parent: The matched parent node. None if not found.\n            index: The input index of matched parent node. None if not found.\n        \'\'\'\n        for i, input in enumerate(node.input):\n            if input in output_name_to_node:\n                parent = output_name_to_node[input]\n                if parent.op_type == parent_op_type and parent not in exclude:\n                    return parent, i\n                else:\n                    logger.debug(f""To find first {parent_op_type}, current {parent.op_type}"")\n        return None, None\n\n    def match_parent(self,\n                     node,\n                     parent_op_type,\n                     input_index=None,\n                     output_name_to_node=None,\n                     exclude=[],\n                     return_indice=None):\n        \'\'\'\n        Find parent node based on constraints on op_type and index.\n        When input_index is None, we will find the first parent node based on constraints, and return_indice will be appended the corresponding input index.\n\n        Args:\n            node (str): current node name.\n            parent_op_type (str): constraint of parent node op_type.\n            input_index (int or None): only check the parent given input index of current node.\n            output_name_to_node (dict): dictionary with output name as key, and node as value.\n            exclude (list): list of nodes that are excluded (not allowed to match as parent).\n            return_indice (list): a list to append the input index when input_index is None.\n\n        Returns:\n            parent: The matched parent node.\n        \'\'\'\n        assert node is not None\n        assert input_index is None or input_index >= 0\n\n        if output_name_to_node is None:\n            output_name_to_node = self.output_name_to_node()\n\n        if input_index is None:\n            parent, index = self.match_first_parent(node, parent_op_type, output_name_to_node, exclude)\n            if return_indice is not None:\n                return_indice.append(index)\n            return parent\n\n        if input_index >= len(node.input):\n            logger.debug(f""input_index {input_index} >= node inputs {len(node.input)}"")\n            return None\n\n        parent = self.get_parent(node, input_index, output_name_to_node)\n        if parent is not None and parent.op_type == parent_op_type and parent not in exclude:\n            return parent\n\n        if parent is not None:\n            logger.debug(f""Expect {parent_op_type}, Got {parent.op_type}"")\n\n        return None\n\n    def match_parent_path(self,\n                          node,\n                          parent_op_types,\n                          parent_input_index,\n                          output_name_to_node=None,\n                          return_indice=None):\n        \'\'\'\n        Find a sequence of input edges based on constraints on parent op_type and index.\n        When input_index is None, we will find the first parent node based on constraints, and return_indice will be appended the corresponding input index.\n\n        Args:\n            node (str): current node name.\n            parent_op_types (str): constraint of parent node op_type of each input edge.\n            parent_input_index (list): constraint of input index of each input edge. None means no constraint.\n            output_name_to_node (dict): dictionary with output name as key, and node as value.\n            return_indice (list): a list to append the input index when there is no constraint on input index of an edge.\n\n        Returns:\n            parents: a list of matched parent node.\n        \'\'\'\n        assert (len(parent_input_index) == len(parent_op_types))\n\n        if output_name_to_node is None:\n            output_name_to_node = self.output_name_to_node()\n\n        current_node = node\n        matched_parents = []\n        for i, op_type in enumerate(parent_op_types):\n            matched_parent = self.match_parent(current_node,\n                                               op_type,\n                                               parent_input_index[i],\n                                               output_name_to_node,\n                                               exclude=[],\n                                               return_indice=return_indice)\n            if matched_parent is None:\n                logger.debug(f""Failed to match index={i} parent_input_index={parent_input_index[i]} op_type={op_type}"",\n                             stack_info=True)\n                return None\n\n            matched_parents.append(matched_parent)\n            current_node = matched_parent\n\n        return matched_parents\n\n    def find_first_child_by_type(self, node, child_type, input_name_to_nodes=None, recursive=True):\n        children = self.get_children(node, input_name_to_nodes)\n        dq = deque(children)\n        while len(dq) > 0:\n            current_node = dq.pop()\n            if current_node.op_type == child_type:\n                return current_node\n\n            if recursive:\n                children = self.get_children(current_node, input_name_to_nodes)\n                for child in children:\n                    dq.appendleft(child)\n\n        return None\n\n    def find_first_parent_by_type(self, node, parent_type, output_name_to_node=None, recursive=True):\n        if output_name_to_node is None:\n            output_name_to_node = self.output_name_to_node()\n\n        parents = self.get_parents(node, output_name_to_node)\n        dq = deque(parents)\n        while len(dq) > 0:\n            current_node = dq.pop()\n            if current_node.op_type == parent_type:\n                return current_node\n\n            if recursive:\n                parents = self.get_parents(current_node, output_name_to_node)\n                for parent in parents:\n                    dq.appendleft(parent)\n\n        return None\n\n    def get_constant_value(self, output_name):\n        for node in self.get_nodes_by_op_type(\'Constant\'):\n            if node.output[0] == output_name:\n                for att in node.attribute:\n                    if att.name == \'value\':\n                        return numpy_helper.to_array(att.t)\n\n        # Fall back to intializer since constant folding might have been\n        # applied.\n        initializer = self.get_initializer(output_name)\n        if initializer is not None:\n            return numpy_helper.to_array(initializer)\n\n        return None\n\n    def get_constant_input(self, node):\n        for i, input in enumerate(node.input):\n            value = self.get_constant_value(input)\n            if value is not None:\n                return i, value\n\n        return None, None\n\n    def find_constant_input(self, node, expected_value, delta=0.000001):\n        i, value = self.get_constant_input(node)\n        if value is not None and value.size == 1 and abs(value - expected_value) < delta:\n            return i\n\n        return -1\n\n    def is_constant_with_specified_dimension(self, output_name, dimensions, description):\n        value = self.get_constant_value(output_name)\n        if value is None:\n            logger.debug(f""{description} {output_name} is not initializer."")\n            return False\n\n        if len(value.shape) != dimensions:\n            logger.debug(f""{description} {output_name} shall have {dimensions} dimensions. Got shape {value.shape}"")\n            return False\n\n        return True\n\n    def has_constant_input(self, node, expected_value, delta=0.000001):\n        return self.find_constant_input(node, expected_value, delta) >= 0\n\n    def get_children_subgraph_nodes(self, root_node, stop_nodes, input_name_to_nodes=None):\n        if input_name_to_nodes is None:\n            input_name_to_nodes = self.input_name_to_nodes()\n\n        children = input_name_to_nodes[root_node.output[0]]\n\n        unique_nodes = []\n\n        dq = deque(children)\n        while len(dq) > 0:\n            current_node = dq.pop()\n            if current_node in stop_nodes:\n                continue\n\n            if current_node not in unique_nodes:\n                unique_nodes.append(current_node)\n\n                for output in current_node.output:\n                    if output in input_name_to_nodes:\n                        children = input_name_to_nodes[output]\n                        for child in children:\n                            dq.appendleft(child)\n\n        return unique_nodes\n\n    def convert_model_float32_to_float16(self):\n        graph = self.model.graph\n        initializers = graph.initializer\n\n        for initializer in initializers:\n            if initializer.data_type == 1:\n                initializer.CopyFrom(\n                    numpy_helper.from_array(numpy_helper.to_array(initializer).astype(np.float16), initializer.name))\n\n        for node in graph.node:\n            if node.op_type == \'Constant\':\n                for att in node.attribute:\n                    if att.name == \'value\' and att.t.data_type == 1:\n                        att.CopyFrom(\n                            onnx.helper.make_attribute(\n                                ""value"", numpy_helper.from_array(numpy_helper.to_array(att.t).astype(np.float16))))\n            if node.op_type == \'Cast\':\n                for att in node.attribute:\n                    if att.name == \'to\' and att.i == 1:\n                        att.CopyFrom(onnx.helper.make_attribute(""to"", 10))\n\n        for input_value_info in graph.input:\n            if input_value_info.type.tensor_type.elem_type == TensorProto.FLOAT:\n                initializer = self.get_initializer(input_value_info.name)\n                if initializer is not None:  # for compatibility for old converter/exporter\n                    input_value_info.type.tensor_type.elem_type = 10\n                else:\n                    cast_input = input_value_info.name\n                    cast_output = input_value_info.name + \'_float16\'\n                    self.replace_input_of_all_nodes(cast_input, cast_output)\n                    cast_node = onnx.helper.make_node(\'Cast\', inputs=[cast_input], outputs=[cast_output])\n                    cast_node.attribute.extend([onnx.helper.make_attribute(""to"", int(TensorProto.FLOAT16))])\n                    self.add_node(cast_node)\n\n        for output_value_info in graph.output:\n            if output_value_info.type.tensor_type.elem_type == TensorProto.FLOAT:\n                cast_input = output_value_info.name + \'_float16\'\n                cast_output = output_value_info.name\n                self.replace_output_of_all_nodes(cast_output, cast_input)\n                cast_node = onnx.helper.make_node(\'Cast\', inputs=[cast_input], outputs=[cast_output])\n                cast_node.attribute.extend([onnx.helper.make_attribute(""to"", int(TensorProto.FLOAT))])\n                self.add_node(cast_node)\n\n    # create a new name for node\n    def create_node_name(self, op_type, name_prefix=None):\n        if op_type in self.node_name_counter:\n            self.node_name_counter[op_type] += 1\n        else:\n            self.node_name_counter[op_type] = 1\n\n        if name_prefix is not None:\n            full_name = name_prefix + str(self.node_name_counter[op_type])\n        else:\n            full_name = op_type + ""_"" + str(self.node_name_counter[op_type])\n\n        # Check whether the name is taken:\n        nodes = self.get_nodes_by_op_type(op_type)\n        for node in nodes:\n            if node.name == full_name:\n                raise Exception(""Node name already taken:"", full_name)\n\n        return full_name\n\n    def find_graph_input(self, input_name):\n        for input in self.model.graph.input:\n            if input.name == input_name:\n                return input\n        return None\n\n    def find_graph_output(self, output_name):\n        for output in self.model.graph.output:\n            if output.name == output_name:\n                return output\n        return None\n\n    def get_parent_subgraph_nodes(self, node, stop_nodes, output_name_to_node=None):\n        if output_name_to_node is None:\n            output_name_to_node = self.output_name_to_node()\n\n        unique_nodes = []\n\n        parents = self.get_parents(node, output_name_to_node)\n        dq = deque(parents)\n        while len(dq) > 0:\n            current_node = dq.pop()\n            if current_node in stop_nodes:\n                continue\n\n            if current_node not in unique_nodes:\n                unique_nodes.append(current_node)\n\n                for input in current_node.input:\n                    if input in output_name_to_node:\n                        dq.appendleft(output_name_to_node[input])\n\n        return unique_nodes\n\n    def get_graph_inputs(self, current_node, recursive=False):\n        """"""\n        Find graph inputs that linked to current node.\n        """"""\n        graph_inputs = []\n        for input in current_node.input:\n            if self.find_graph_input(input) and input not in graph_inputs:\n                graph_inputs.append(input)\n\n        if recursive:\n            parent_nodes = self.get_parent_subgraph_nodes(current_node, [])\n            for node in parent_nodes:\n                for input in node.input:\n                    if self.find_graph_input(input) and input not in graph_inputs:\n                        graph_inputs.append(input)\n        return graph_inputs\n\n    @staticmethod\n    def input_index(node_output, child_node):\n        index = 0\n        for input in child_node.input:\n            if input == node_output:\n                return index\n            index += 1\n        return -1\n\n    def remove_unused_constant(self):\n        input_name_to_nodes = self.input_name_to_nodes()\n\n        #remove unused constant\n        unused_nodes = []\n        nodes = self.nodes()\n        for node in nodes:\n            if node.op_type == ""Constant"" and node.output[0] not in input_name_to_nodes:\n                unused_nodes.append(node)\n\n        self.remove_nodes(unused_nodes)\n\n        if len(unused_nodes) > 0:\n            logger.info(f""Removed unused constant nodes: {len(unused_nodes)}"")\n\n    def prune_graph(self, outputs=None):\n        """"""\n        Prune graph to keep only required outputs. It removes unnecessary inputs and nodes.\n        Nodes are not linked (directly or indirectly) to any required output will be removed.\n\n        Args:\n            outputs (list): a list of graph outputs to retain. If it is None, all graph outputs will be kept.\n        """"""\n        if outputs is None:\n            outputs = [output.name for output in self.model.graph.output]\n\n        output_name_to_node = self.output_name_to_node()\n        all_nodes = []\n        for output in outputs:\n            if output in output_name_to_node:\n                last_node = output_name_to_node[output]\n                if last_node in all_nodes:\n                    continue\n                nodes = self.get_parent_subgraph_nodes(last_node, [])\n                all_nodes.append(last_node)\n                all_nodes.extend(nodes)\n\n        nodes_to_remove = []\n        for node in self.model.graph.node:\n            if node not in all_nodes:\n                nodes_to_remove.append(node)\n\n        self.remove_nodes(nodes_to_remove)\n\n        # remove outputs not in list\n        output_to_remove = []\n        for output in self.model.graph.output:\n            if output.name not in outputs:\n                output_to_remove.append(output)\n        for output in output_to_remove:\n            self.model.graph.output.remove(output)\n\n        # remove inputs not used by any node.\n        input_name_to_nodes = self.input_name_to_nodes()\n        input_to_remove = []\n        for input in self.model.graph.input:\n            if input.name not in input_name_to_nodes:\n                input_to_remove.append(input)\n        for input in input_to_remove:\n            self.model.graph.input.remove(input)\n\n        logger.info(""Graph pruned: {} inputs, {} outputs and {} nodes are removed"".format(\n            len(input_to_remove), len(output_to_remove), len(nodes_to_remove)))\n\n        self.update_graph()\n\n    def update_graph(self, verbose=False):\n        graph = self.model.graph\n\n        remaining_input_names = []\n        for node in graph.node:\n            if node.op_type != ""Constant"":\n                for input_name in node.input:\n                    if input_name not in remaining_input_names:\n                        remaining_input_names.append(input_name)\n        if verbose:\n            logger.debug(f""remaining input names: {remaining_input_names}"")\n\n        # remove graph input that is not used\n        inputs_to_remove = []\n        for input in graph.input:\n            if input.name not in remaining_input_names:\n                inputs_to_remove.append(input)\n        for input in inputs_to_remove:\n            graph.input.remove(input)\n\n        names_to_remove = [input.name for input in inputs_to_remove]\n        logger.debug(f""remove {len(inputs_to_remove)} unused inputs: {names_to_remove}"")\n\n        # remove weights that are not used\n        weights_to_remove = []\n        weights_to_keep = []\n        for initializer in graph.initializer:\n            if initializer.name not in remaining_input_names:\n                weights_to_remove.append(initializer)\n            else:\n                weights_to_keep.append(initializer.name)\n        for initializer in weights_to_remove:\n            graph.initializer.remove(initializer)\n\n        names_to_remove = [initializer.name for initializer in weights_to_remove]\n        logger.debug(f""remove {len(weights_to_remove)} unused initializers: {names_to_remove}"")\n        if verbose:\n            logger.debug(f""remaining initializers:{weights_to_keep}"")\n\n        self.remove_unused_constant()\n\n    def is_safe_to_fuse_nodes(self, nodes_to_remove, keep_outputs, input_name_to_nodes, output_name_to_node):\n        for node_to_remove in nodes_to_remove:\n            for output_to_remove in node_to_remove.output:\n                if output_to_remove in keep_outputs:\n                    continue\n\n                if output_to_remove in input_name_to_nodes:\n                    for impacted_node in input_name_to_nodes[output_to_remove]:\n                        if impacted_node not in nodes_to_remove:\n                            logger.debug(\n                                f""it is not safe to remove nodes since output {output_to_remove} is used by {impacted_node}""\n                            )\n                            return False\n        return True\n\n    def save_model_to_file(self, output_path):\n        logger.info(f""Output model to {output_path}"")\n\n        if output_path.endswith("".json""):\n            assert isinstance(self.model, ModelProto)\n            with open(output_path, ""w"") as out:\n                out.write(str(self.model))\n        else:\n            with open(output_path, ""wb"") as out:\n                out.write(self.model.SerializeToString())\n\n    def get_graph_inputs_excluding_initializers(self):\n        """"""\n        Returns real graph inputs (excluding initializers from older onnx model).\n        """"""\n        graph_inputs = []\n        for input in self.model.graph.input:\n            if self.get_initializer(input.name) is None:\n                graph_inputs.append(input)\n        return graph_inputs\n'"
farm/conversion/onnx_optimization/__init__.py,0,b''
farm/conversion/onnx_optimization/bert_model_optimization.py,0,"b'# Source: https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/bert/bert_model_optimization.py\n#-------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation.  All rights reserved.\n# Licensed under the MIT License.\n#--------------------------------------------------------------------------\n\n# Convert Bert ONNX model converted from TensorFlow or exported from PyTorch to use Attention, Gelu,\n# SkipLayerNormalization and EmbedLayerNormalization ops to optimize\n# performance on NVidia GPU and CPU.\n\n# For Bert model exported from PyTorch, OnnxRuntime has bert model optimization support internally.\n# You can use the option --use_onnxruntime to use model optimization from OnnxRuntime package.\n# For Bert model file like name.onnx, optimized model for GPU or CPU from OnnxRuntime will output as\n# name_ort_gpu.onnx or name_ort_cpu.onnx in the same directory.\n# This script is retained for experiment purpose. Useful senarios like the following:\n#  (1) Change model from fp32 to fp16.\n#  (2) Change input data type from int64 to int32.\n#  (3) Some model cannot be handled by OnnxRuntime, and you can modify this script to get optimized model.\n\n# This script has been tested using the following models:\n#  (1) BertForSequenceClassification as in https://github.com/huggingface/transformers/blob/master/examples/run_glue.py\n#      PyTorch 1.2 or above, and exported to Onnx using opset version 10 or 11.\n#  (2) BertForQuestionAnswering as in https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n#      PyTorch 1.2 or above, and exported to Onnx using opset version 10 or 11.\n\nimport logging\n# import coloredlogs\nimport os\nimport sys\nimport argparse\nimport numpy as np\nfrom collections import deque\nfrom .BertOnnxModel import BertOnnxModel, BertOptimizationOptions\n# from .BertOnnxModelTF import BertOnnxModelTF\n# from BertOnnxModelKeras import BertOnnxModelKeras\n# from Gpt2OnnxModel import Gpt2OnnxModel\n\ntry:\n    import onnx\n    from onnx import ModelProto, TensorProto, numpy_helper\nexcept:\n    pass\n\nlogger = logging.getLogger(\'\')\n\n# Map model type to tuple: optimizer class, export tools (pytorch, tf2onnx, keras2onnx) and whether OnnxRuntime has the optimization.\nMODEL_CLASSES = {\n    ""bert"": (BertOnnxModel, ""pytorch"", True),\n    # ""bert_tf"": (BertOnnxModelTF, ""tf2onnx"", False),\n    # ""bert_keras"": (BertOnnxModelKeras, ""keras2onnx"", False),\n    # ""gpt2"": (Gpt2OnnxModel, ""pytorch"", True)\n}\n\n\ndef optimize_by_onnxruntime(onnx_model_path, use_gpu=False, optimized_model_path=None, opt_level=99):\n    """"""\n    Use onnxruntime package to optimize model. It could support models exported by PyTorch.\n\n    Args:\n        onnx_model_path (str): th path of input onnx model.\n        use_gpu (bool): whether the optimized model is targeted to run in GPU.\n        optimized_model_path (str or None): the path of optimized model.\n\n    Returns:\n        optimized_model_path: the path of optimized model\n    """"""\n    import onnxruntime\n\n    if use_gpu and \'CUDAExecutionProvider\' not in onnxruntime.get_available_providers():\n        logger.error(""There is no gpu for onnxruntime to do optimization."")\n        return onnx_model_path\n\n    sess_options = onnxruntime.SessionOptions()\n    if opt_level == 1:\n        sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_BASIC\n    elif opt_level == 2:\n        sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n    else:\n        assert opt_level == 99\n        sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n\n    if optimized_model_path is None:\n        path_prefix = onnx_model_path[:-5]  #remove .onnx suffix\n        optimized_model_path = ""{}_ort_{}.onnx"".format(path_prefix, ""gpu"" if use_gpu else ""cpu"")\n\n    sess_options.optimized_model_filepath = optimized_model_path\n\n    if not use_gpu:\n        session = onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=[\'CPUExecutionProvider\'])\n    else:\n        session = onnxruntime.InferenceSession(onnx_model_path, sess_options)\n        assert \'CUDAExecutionProvider\' in session.get_providers()  # Make sure there is GPU\n\n    assert os.path.exists(optimized_model_path) and os.path.isfile(optimized_model_path)\n    logger.info(""Save optimized model by onnxruntime to {}"".format(optimized_model_path))\n    return optimized_model_path\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--input\', required=True, type=str, help=""input onnx model path"")\n\n    parser.add_argument(\'--output\', required=True, type=str, help=""optimized onnx model path"")\n\n    parser.add_argument(\'--model_type\',\n                        required=False,\n                        type=str.lower,\n                        default=""bert"",\n                        choices=list(MODEL_CLASSES.keys()),\n                        help=""Model type selected in the list: "" + "", "".join(MODEL_CLASSES.keys()))\n\n    parser.add_argument(\'--num_heads\',\n                        required=False,\n                        type=int,\n                        default=12,\n                        help=""number of attention heads. 12 for bert-base model and 16 for bert-large"")\n\n    parser.add_argument(\'--hidden_size\',\n                        required=False,\n                        type=int,\n                        default=768,\n                        help=""bert model hidden size. 768 for bert-base model and 1024 for bert-large"")\n\n    parser.add_argument(\'--input_int32\',\n                        required=False,\n                        action=\'store_true\',\n                        help=""Use int32 (instead of int64) tensor as input to avoid unnecessary data cast"")\n    parser.set_defaults(input_int32=False)\n\n    parser.add_argument(\n        \'--float16\',\n        required=False,\n        action=\'store_true\',\n        help=""If your target device is V100 or T4 GPU, use this to convert float32 to float16 for best performance"")\n    parser.set_defaults(float16=False)\n\n    parser.add_argument(\'--disable_attention\', required=False, action=\'store_true\', help=""disable Attention fusion"")\n    parser.set_defaults(disable_attention=False)\n\n    parser.add_argument(\'--disable_skip_layer_norm\',\n                        required=False,\n                        action=\'store_true\',\n                        help=""disable SkipLayerNormalization fusion"")\n    parser.set_defaults(disable_skip_layer_norm=False)\n\n    parser.add_argument(\'--disable_embed_layer_norm\',\n                        required=False,\n                        action=\'store_true\',\n                        help=""disable EmbedLayerNormalization fusion"")\n    parser.set_defaults(disable_embed_layer_norm=False)\n\n    parser.add_argument(\'--disable_bias_skip_layer_norm\',\n                        required=False,\n                        action=\'store_true\',\n                        help=""disable Add Bias and SkipLayerNormalization fusion"")\n    parser.set_defaults(disable_bias_skip_layer_norm=False)\n\n    parser.add_argument(\'--disable_bias_gelu\',\n                        required=False,\n                        action=\'store_true\',\n                        help=""disable Add Bias and Gelu/FastGelu fusion"")\n    parser.set_defaults(disable_bias_gelu=False)\n\n    parser.add_argument(\'--verbose\', required=False, action=\'store_true\')\n    parser.set_defaults(verbose=False)\n\n    parser.add_argument(\'--opt_level\',\n                        required=False,\n                        type=int,\n                        choices=[0, 1, 2, 99],\n                        default=99,\n                        help=""onnxruntime optimization level. 0 will disable onnxruntime."")\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef get_optimization_options(args):\n    optimization_options = BertOptimizationOptions(args.model_type)\n    if args.disable_attention:\n        optimization_options.enable_attention = False\n    if args.disable_skip_layer_norm:\n        optimization_options.enable_skip_layer_norm = False\n    if args.disable_embed_layer_norm:\n        optimization_options.enable_embed_layer_norm = False\n    if args.disable_bias_skip_layer_norm:\n        optimization_options.enable_bias_skip_layer_norm = False\n    if args.disable_bias_gelu:\n        optimization_options.enable_bias_gelu = False\n    return optimization_options\n\n\ndef optimize_model(input,\n                   model_type,\n                   num_heads,\n                   hidden_size,\n                   opt_level=99,\n                   optimization_options=None):\n    (optimizer_class, producer, run_onnxruntime) = MODEL_CLASSES[model_type]\n\n    input_model_path = input\n    if run_onnxruntime and opt_level > 0:\n        input_model_path = optimize_by_onnxruntime(input_model_path, use_gpu=False, opt_level=opt_level)\n        logger.info(""Use OnnxRuntime to optimize and save the optimized model to {}"".format(input_model_path))\n\n    model = ModelProto()\n    with open(input_model_path, ""rb"") as f:\n        model.ParseFromString(f.read())\n\n    if model.producer_name and producer != model.producer_name:\n        logger.warning(\n            f""Model producer not matched: Expect {producer},  Got {model.producer_name} {model.producer_version}. Please specify correct --model_type parameter.""\n        )\n\n    if optimization_options is None:\n        optimization_options = BertOptimizationOptions(model_type)\n\n    bert_model = optimizer_class(model, num_heads, hidden_size)\n    bert_model.optimize(optimization_options)\n\n    return bert_model\n\n\ndef setup_logger(verbose):\n    pass\n    # if verbose:\n    #     coloredlogs.install(level=\'DEBUG\', fmt=\'[%(filename)s:%(lineno)s - %(funcName)20s()] %(message)s\')\n    # else:\n    #     coloredlogs.install(fmt=\'%(funcName)20s: %(message)s\')\n\n\ndef main(args=None):\n    if not args:\n        args = parse_arguments()\n\n    setup_logger(args.verbose)\n\n    optimization_options = get_optimization_options(args)\n\n    bert_model = optimize_model(args.input, args.model_type, args.num_heads, args.hidden_size,\n                                args.opt_level, optimization_options)\n\n    if args.float16:\n        bert_model.convert_model_float32_to_float16()\n\n    if args.input_int32:\n        bert_model.change_input_to_int32()\n\n    bert_model.save_model_to_file(args.output)\n\n    if bert_model.is_fully_optimized():\n        logger.info(""The output model is fully optimized."")\n    else:\n        logger.warning(""The output model is not fully optimized. It might not be usable."")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
farm/visual/ascii/__init__.py,0,b''
farm/visual/ascii/images.py,0,"b'FLOWERS = """"""\n         \n   vVVVv                  vVVVv      \n   (___)       vVVVv      (___)       vVVVv\n    ~Y~        (___)       ~Y~        (___)\n    \\|         \\~Y~/       \\|         \\~Y~/\n   \\\\|//       \\\\|//      \\\\|//       \\\\|//\n   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ \n""""""\n\nSAMPLE = """"""\n      .--.        _____                       _      \n    .\'_\\/_\'.     / ____|                     | |     \n    \'. /\\ .\'    | (___   __ _ _ __ ___  _ __ | | ___ \n      ""||""       \\___ \\ / _` | \'_ ` _ \\| \'_ \\| |/ _ \\ \n       || /\\     ____) | (_| | | | | | | |_) | |  __/\n    /\\ ||//\\)   |_____/ \\__,_|_| |_| |_| .__/|_|\\___|\n   (/\\\\||/                             |_|           \n______\\||/___________________________________________                     \n""""""\n\nFENCE = """"""\n  _   _   _   _   _   _   _   _   _   _   _   _   _   _   _   _   _   _   _   _ \n_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_\n-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-\n | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\n_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_\n-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-| |-\n |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_| |_|  \n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n\n""""""                                                  \n\nTRACTOR_SMALL = """""" \n              ______\n               |o  |   !\n   __          |:`_|---\'-.\n  |__|______.-/ _ \\-----.|       \n (o)(o)------\'\\ _ /     ( )      \n """"""\n\n\nTRACTOR_WITH_SILO_LINE = """""" \n                                     ____\n                                    /____\\\n              ______               |      |\n               |o  |   !           |      | \n   __          |:`_|---\'-.         |      |\n  |__|______.-/ _ \\-----.|         |______|\n (o)(o)------\'\\ _ /     ( )        |      |\n """"""\n\n\nBARN_LINE = """"""\n                                     _.-^-._    .--.\n                                  .-\'   _   \'-. |__|\n                                 /     |_|     \\|  |\n                                /               \\  |\n                               /|     _____     |\\ |\n                                |    |==|==|    |  |\n|---|---|---|---|---|---|---|---|    |--|--|    |  |\n|---|---|---|---|---|---|---|---|    |==|==|    |  |\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n """"""\n\nWELCOME_BARN = """"""\n __          __  _                            _        \n \\ \\        / / | |                          | |       \n  \\ \\  /\\  / /__| | ___ ___  _ __ ___   ___  | |_ ___  \n   \\ \\/  \\/ / _ \\ |/ __/ _ \\| \'_ ` _ \\ / _ \\ | __/ _ \\ \n    \\  /\\  /  __/ | (_| (_) | | | | | |  __/ | || (_) |\n     \\/  \\/ \\___|_|\\___\\___/|_| |_| |_|\\___|  \\__\\___/ \n  ______      _____  __  __  \n |  ____/\\   |  __ \\|  \\/  |              _.-^-._    .--.\n | |__ /  \\  | |__) | \\  / |           .-\'   _   \'-. |__|\n |  __/ /\\ \\ |  _  /| |\\/| |          /     |_|     \\|  |\n | | / ____ \\| | \\ \\| |  | |         /               \\  |\n |_|/_/    \\_\\_|  \\_\\_|  |_|        /|     _____     |\\ |\n                                     |    |==|==|    |  |\n|---||---|---|---|---|---|---|---|---|    |--|--|    |  |\n|---||---|---|---|---|---|---|---|---|    |==|==|    |  |\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n """"""\n\nROOSTER = """"""\n                             _  m\n                           ,`.\\/\'>\n                           (`\\<_/`\n                             `<<\n""""""\n\n""""""\n""""""\nGROWING_TREE = """"""\n\n          &&& &&  & &&             _____                   _             \n      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n   &() &\\/&|()|/&\\/ \'%"" & ()     | | |_ | \'__/ _ \\ \\ /\\ / / | \'_ \\ / _` |\n  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n     &&     \\|||                                                   |___/\n             |||\n             |||\n             |||\n       , -=-~  .-^- _\n              `\n""""""\nPIG = """"""\n\n        .-~~~~-. |\\\\_\n     @_/        /  oo\\_\n       |    \\   \\   _("")\n        \\   /-| ||\'--\'\n         \\_\\  \\_\\\\\n\n""""""\nSMALL_PIG = """"""\n     @___,__\n     (   ^\'_]\n     //-\\\\\'\n     ^^  ^^\n""""""\nFENCE_SEP = """"""\n|---||---|---|---|---|---|---|---|\n""""""\n\nBUSH_SEP = r""""""\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^""""""\n\nWATERING_CAN = """"""\n            ______                                         \n _        ,\',----.`.                   \n\'.`-.  .-\' \'----. ||                                       \n   `.`-\'--------| ;;      \n     `.|--------|//                                        \n       \\         /         \n       \'--------\'                                       \n""""""\n\nTREE_1 = """"""\n                                                           *(*/,                         .,****  *(*/*                                          \n                               */((%*/*,.//*         .,    .* (**//(/*//(/////#/***                                    \n                             */(/%(///,(((/((,*,/#/ ,(//,,*///#/((##((*//(//(*////(.       **                          \n                        ..((,/#%#*( /(*(##/#*/#(*/#////###(,/#/*#%#(/(%#%((%(/((*%,  ,/***//*/**.                      \n                      (*//(/*/#/*/((%%((((*,*###/((/(#(%%#(%(#((///(#(%%#%#(#(#((/,.,*#/#((/%//%(/,                    \n                     .*,(###/((,((/*((/%%,(%(%%(*((((#%#(%%##(((%#(*/%%&%%(%#(*(*(##/*(#*###%#####/, **  .             \n                    .(%%##&%((((#(/#((#(*.*#%&&(#(%(*/(%(#(((%%&,/*%%(/*/(&/,**(,(/(##%*/(#&(%%%#///((*#,/,            \n                    ###%%(%%%&%#%#(#&((#(/##%#%*(#(/(//#%%#&%#%((#%/#**#(//%#(#*#**, ./(#/#*##%(#(/##*(*             \n                     /#%&%%%/%#*#%#%/%%/*(/##%((%(#%&&&**##(,(((/(*%%%%(##%(/(/,*(#*%#(/((,//%(#%#((//,            \n                 ,.  /#*%%%#(/((#/(/*(%%#/#&(##%%%####(#//*/*(%/((%####%#/%###*//,%%#%%*#%#%#%*,/,%%#&%/,,.            \n              ,*/(,/(%#&%%%/%%##%&(%%%&&%%%%%%%*/%##%(%((/#(#%###%#%##%%(#%%#/(%#/(*##(/(*#%# %//#*(/*##/              \n              (#(#(#( #/%#%&**&(/##%%%&%#(,///(((*//((%%###&&(%&%####%*%(/##//(**/#/*#%%#*//*..  ,#(/(((*/,.         \n             (#/###(##  ,%,.*%#%####%&/%###**(*,,/#%**(/###%%*,%#%/#(.#(%%#((#/**/***/((,  /./**(/,* %((#.         \n              ((&&%%(*/(*(/(%##%###%%/# (##%(#(###/,/(#(*###(((##(#/#(#(/((//(%(%%*#/*/##(,*.../(##(/*,/*,           \n             ###&(%%%####(##/(#%%&&&(/*%&&%#%%%%&&%%%%(#(#%%%%#/(##*//%#%%##((#%#/%%(/#(,/(%((#*(/               \n          /(((%*%%#%%%%%%%(/##%&%#%%%/(&&&%/##(#%%%%###%%%%#%%%%%%%#/##*.*(#&%%##%(/*%#&%%%/(####(%%#/(#*              \n       .*.*(%###%&%#%(&&(#(#&%%&&%%%#%&%%##%&%&(##%&&&%*%%(%#%###(#(#*#(.,,@/.#%###/*//#/#%%%*%%%%(#((.,             \n        /(#####%#(((*(#((#&&&&&%@%##(%%#%&@(%%%###&&%#%%%%(#(#((%##(((((,/ **&.(%#(#(*/**(#/#((,###%%%#/*              \n        (#%%#&&%##%#&%#%#%&&#%%%#%#%%#((%##%###%%%%/(#((%###//#((##(##%(%*(#*(#(%/**.#(%/*%&%%%#///.             \n           ((###%%%###%%%##(%%%%###%%%%%#%%%((#%&&&(#%&(.##/(((///%%(*%(## *(/#(%(((*(((%#%###/%///  //*         \n        (#(/%#%#%#%%(##/#%#&&%#,%%#@%##%#%#,%(*(#,%&%(#%%(%#(%&&%&%%%(%%%#%//  *%%%#//(,/##*#(%%%(//(/*//,           \n       *%#%%%%#%&%#%##*%##%&&%%%###/%%%#%/(  ,*((###%(###(/%&%*&&&%##*#%(,%%,   (/*(##%((**/#*(((##(###/*          \n          .#%%%#*%#(*/((,(/#%##&%(%#(%%&%%%%#       /*##(//#(#/*&% /&%%,, ,%%#*.   /(/(.((#((((. (%(/&((/*#.           \n            ##(  , (.   ,.%&&%/*.##&%##%#(/#/     .(%%#((#(###%& &&%*,/#(#/# ,      (,/##(//./*(/#%((*.              \n                  %/,     %##%..((%%%%%#%##%(        //(.  (%, %&&,&% %/              .##%/*,  /%#%##(,              \n                          *###*   (%%#%%%#.                 %#  &&%#.##(                  .((*(*,/##(/                 \n                                  #/%#,((((                  ## %#%###(                           ((,                  \n                                    (, (##                   ,%, &(#%(                             */                  \n                                                              %#/&(((                                                  \n                                                              .%&&///                                                  \n                                                                &%(##                                                  \n                                                                %%/%(                                                  \n                                                               (%#((,                                                  \n                                                               %&(((*                                                  \n                                                               %%(%(/                                                  \n                                                              .&%/%/(                                                  \n                                                             %##&/###                                                  \n                                                          ...,,,.,.*//,                                                """"""\n\nTREE_1_MEDIUM = """"""\n                             /./                    .*/*.,,,*/(/**                                 \n                         */#//(/(//#*,,.*,  ,*   ,//##(/(/**/***/*(/(*                             \n                     ,/* ,##/*((/(((/*(%*/#*.///**%#/#//*/##(/(/**((/  //*/////,                   \n                  (*/(/*(%(/(#(((/(*.*#%///##%&%##/(#/*#/*%%####(###, ./(/(#/#(%(*                 \n                 .,,#%(/(((((**%#(,((%%(*/#(#((#%*##(%%*,(#%#(#((*/*##/*//##((##((* ,, ,           \n                */#%%(####%#(/%%(**#%&%(%#//((%##%###(((%(/(*#//**/%#/,((,%#######/((//#.          \n                 *##&%%%%%#/###%/%/%%((#%%##((##%#(%(**(###/*(/(/*/,*(#/#/&%#%#%##/(           \n                  ##&%%*##&/##(#(((#&%%%%&%%%%(#%/(##((%((##/#((##(((%%%*(((*%( #%#//#.          \n            *//,*,#%&%(%%((&%%%%&%%##,/&%@&(#%/#%(#%#&%%(%/*%((/##((//(.*#/#####(            \n            ,#%(%.**(%*%##(%#%%#(*(((((//((%#&%&&%&%#%%%/%(####(*(//(/%%((#*,**. ((//%/*,        \n           (.(##%#. .# #%#%#%%(/%(%,*(*(*/%*////*%(/#(((*,/%%#%%(*(,,(**#. ,*/(#(* #(%*        \n           (#%%%%####/(#((%#&@&/**##%%%%#%#%%#/#(%&%###((((((#(/(*###(/#((((#/*,*(((*/             \n        . ,%%/(%%%##%#(((%%%#(%%%(%#%%%%%#%(%&%&&%####(*,#%(%##//#(%%%%(/((##&%#(/,            \n      (##(##%#%%%%%%#%#%&&%%#%%%##%%%%&&%&&&/.#%%%##(#%%(///., %(#/(/*(#%&%%%/#//*           \n       (%%##%/,*#(###/&%&%%%##%%#%&&%%%%##%%#%%#(#%#(%#(,(((/ *(&*.%/##*/*(#/%//%#%#%(*,           \n       ,/#&%%(&%#%%%(&%%%@&&%&(%%%%%*%&%#%*/%%%#%(#/#/&(##(((//,**((*#(/,/*%//%#%#((((           \n      ,*.#%#%#(/%%#/#*##%%%%%%%&(#%%#%&&%&%%/(/(/#%(##%(%/%%%%%#  *%/%%/#(*%%%%%%%#(*. ,(        \n      *(%(#*%%%%(&%%&&&%%#(%&%%&%%/* #(%#(##%%%(*&%%&%##/%  #%, */*%##/(/#&(%#%%(##,*        \n       / %####%((/((*##&%%%,##&%%%%%#.     (//((*#(**&%*%%%,. %/%*   (***((##/(, #/(,(#,         \n          %** . /    *&&%%#**##%#/##(#*     /%%,/%((*%&*&%,*#(%// ,    ,/,##/(*/(*#(##*            \n               ..    .####.##%%##%###        #,  ,&  @%&% &(/            .###*(. %#//(             \n                       .   (.%#(#%%(              (# %&%(%#*                ,*/  .((               \n                            #*(*/%(                #/ %%##                        #/               \n                                                   /#*(                                          \n                                                    %%&/#,                                         \n                                                     %%(#                                          \n                                                     &%((                                          \n                                                    .&(#(                                          \n                                                    %&(%(                                          \n                                                    &%/%(                                          \n                                                   ##%*#((                                         \n                                               ***,**.,.,.,                                        """"""\n\nTREE_1_SMALL = """"""\n                                             .    .                            \n                     .,#.,               ,(((/,.//#//.*                        \n                 .  .##(/((//**(/# */,*/#(/#/##(#(#///*.., *(/,                \n              ,***/#&*/(///( *(%/%/(%%%((((*#(%%#(**((* .(/#(/#/*,             \n              */###((%(///(**%%#(/(/(##*(/(%(*##(/*(*,###,/(%%(#*,.(/*,        \n             (%%%#(%#/%#((#&&/#(#//#%%#%##*((/((*(#  #(*%(##(#%#*/,        \n              (%%%## #&&/*((#%%#(#%%%%(/(/(((*#%%%##(*,/(#/*(*%#%(%(#((        \n         ./*  (%%%#(#%%#%&&%%,(%@(((%###&%&%(/(#/(%#//#./#((%%(,         \n         /#%(# .(&(/%(%/%(/*//(**#%%%%%(%##%#%##(((//*/(((.*(.,/#(/#*//      \n         .(%*,,*(#(#%%%#(##%(#//####*(%%(*#(#(/,##/#%((*/%((. ./*(/,%(       \n        *%%((%&%#%(#(%%%%(#%%&%#%%%&%&&&&%%%#(#(/&%%&((#%%*%*(*(%%((/          \n     #.#((###%&&%%%#%%##((#(#%#%&(%#%,#&%#%#%&((*/*%((##*/((%%#,(%/#//         \n     .%%#%,..#%(%%&%#%(##%&&%%%#%%(#%%##((#(* /&.((/%*/#%/(/#%%%(,         \n       .#%##&%#%##%%###%%%%(#%&&&&%#%%(%#(##/(#%(/*%##/#%*(/#/,(#%(/ .,      \n     %%#%%(##%/#(#%#%/,%%%%%%//%((.%%//&%(&%&(%#%.  &%#(###(%#%&%(#*#/       \n      *%%%###///,%%#%(%%&%%%#/    .(/(.((%&%#&%. .(%/  ,/*,((/,/*/#*#/       \n        *. ,..   %%%%/##%%.((%*    *#*(%##@&(&*#***    , /(///(/%#/          \n                 #/%,./%(#        .   & &&%/#(           ,(/**%%##           \n                      .#/,%*            .# #&%(                  .(            \n                                         %#(                                 \n                                          %&//                                 \n                                          %%#(                                 \n                                          %%%*                                 \n                                          &(#/                                 \n                                         (%/#(                                 \n                                        .%#(/##                               \n""""""\n\n\n\nBARN_SMALL = """"""            \n                                             .,*,   .  **                      \n                                     .,*,   ./%@@@@@@@@@#,  ,/.                \n                              ,**,   .*%@@@@@@@&&&&&&@@@@@@@&/  ,/,            \n                         */((#,,@@@@@@@&&//((/...,,,**//(%&&@@@@@@(. .**       \n                    ./((((//(( &@@&&&&%%%.####*@@@@@@@@@**//*(&&@@@@@@%, .     \n               ,//(///(((/(/(.*&&&%%%%%%%.###(*@@@%(%@@@*/(((/%%%&%&&@@%.,     \n         ./(/(//(///((/((///* &%%%%%%%%%%....,*@@@@@@@@@**(((/%%%%%%%%&@/ ,    \n       .(//////(///(//((//(( (%%%%%%%%%%%.####*@@@@@@@@@*/##(/%%%%%%%%%&%.,.   \n       /(//////(//(((////((,.&%%%%%%%%%%% ####*@@@@@@@@@*/(((/%%%%%%%%%&@/ *   \n      *(////(///(/((((((//, %%%%%%%%%%%%%*,,,.,%%%&&&@@@*.,***%%%%%%%%%%&&. ,  \n     */////////(/*//(((//( *%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%#%%%%%%%&@/.,  \n    .(//////////((//////(...............,,,,****/////(((###########%%%%%%&&* * \n    /////////(((((//((/(( %&&%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%##%%%%,., \n   ,///(((//(///((#%&@@@* &&&&%%%%%%%,                           (%%%%%%%%%.*. \n    .@@@@@@@@@@@@@@@&@&&, %%%%%%%%%%%, ,%%%%%%%%%%( /%%%%%%%%%# ,%#%%%%%%%#    \n     &&&&&&&&&&&&&&&&&&&, %%%%%%%%%%%, ,%%%%%%%%%%/ /%%%%%%%%%# ,%%%%%%%%%#    \n     &&&&&&&&&&&&&&&&&&&, %%%%%%%%%%%, ,%%%%%%%%%%( *%%%%%%%%%# ,%%%%%%%%##    \n     &%&&&&&&%&&&&&&%&&&,.%%%%%%%%%%%, ,%%%%%%%%%%( *%%%%%%%%%# ,%%%%#%####    \n     %%%%&&%%&&&&&&&&&&&,.%%%%%%%%%%%, ,%%%%%%%%%%( *%%%%%%%%%# ,%%%%#%%###    \n     %%%%&%%&&&&&&&&&&&&,.%%%%%%%%%%%,/,.......... * ..... .. ./,%%%%%%%%%#    \n     &%%%&%%&&&&&&&&&&&&..%%%%%%%%%%%,  ,%%%%%%%%/   ,#%%%%%%(  ,%%%%%%%%%#.   \n     &%%%&&&&&&&&&&&&&&&..%%%%%%%%%%%, ,%/ /%%# /%( *%# (%%/.## ,%%%%%%%%%#.   \n     &%%&&&&&&&&&&&&&&&&..%%%&%%%%%%%, ,%%%%. #%%%( *%%%% ,%%%# ,%%%%%%%%%#.   \n     &%%%%&&&&&&&&&&&&&&..%%%%&%%%%%%, ,%%* (%.*%%( *%%,,%%,,%% ,%%%%%%%%%#.   \n         ./#%&&&&&&&&&&&..%%%&%%%%%%%, ,,,%%%%%%* * , /%%%%%%/  ,%%%%%%%%%#.   \n                 .*#%&&&..%%%%%%%%%%%,(,**,,,,....             ,,##(((//***    \n                         .,...                                                            \n""""""\n\nWORKER_M = """""" 0 \n/|\\\\\n/\'\\\\\n""""""\n\nWORKER_F ="""""" 0 \n/w\\\\\n/ \\\\\n""""""\n\nWORKER_X ="""""" 0 \n/w\\\\\n/\'\\\\\n""""""'"
farm/visual/ascii/text.py,0,"b'\nFARM_BLOCKS = """"""\n .----------------.  .----------------.  .----------------.  .----------------. \n| .--------------. || .--------------. || .--------------. || .--------------. |\n| |  _________   | || |      __      | || |  _______     | || | ____    ____ | |\n| | |_   ___  |  | || |     /  \\     | || | |_   __ \\    | || ||_   \\  /   _|| |\n| |   | |_  \\_|  | || |    / /\\ \\    | || |   | |__) |   | || |  |   \\/   |  | |\n| |   |  _|      | || |   / ____ \\   | || |   |  __ /    | || |  | |\\  /| |  | |\n| |  _| |_       | || | _/ /    \\ \\_ | || |  _| |  \\ \\_  | || | _| |_\\/_| |_ | |\n| | |_____|      | || ||____|  |____|| || | |____| |___| | || ||_____||_____|| |\n| |              | || |              | || |              | || |              | |\n| \'--------------\' || \'--------------\' || \'--------------\' || \'--------------\' |\n \'----------------\'  \'----------------\'  \'----------------\'  \'----------------\' \n""""""\n\nFARM_DOOM = """"""\n  __\n / _|                    \n| |_ __ _ _ __ _ __ ___  \n|  _/ _` | \'__| \'_ ` _ \\ \n| || (_| | |  | | | | | |\n|_| \\__,_|_|  |_| |_| |_|\n""""""\n\nFARM_MODULAR = """"""\n _______  _______  ______    __   __ \n|       ||   _   ||    _ |  |  |_|  |\n|    ___||  |_|  ||   | ||  |       |\n|   |___ |       ||   |_||_ |       |\n|    ___||       ||    __  ||       |\n|   |    |   _   ||   |  | || ||_|| |\n|___|    |__| |__||___|  |_||_|   |_|\n""""""\n\nFARM_COLOSSAL = """"""\n .d888                               \nd88P""                                \n888                                  \n888888 8888b.  888d888 88888b.d88b.  \n888       ""88b 888P""   888 ""888 ""88b \n888   .d888888 888     888  888  888 \n888   888  888 888     888  888  888 \n888   ""Y888888 888     888  888  888\n""""""\n\nFARM_DIET_COLA = """"""\n   .-._.---\'                      \n  (_) /                           \n     /--..-.    ).--..  .-. .-.   \n    /   (  |   /      )/   )   )  \n .-/     `-\'-\'/      \'/   /   (   \n(_/                            `-\'\n                               """"""\n\nWELCOME = """"""\n                   .                                        \n                  /                                /        \n `)    (   .-.   / .-.  .-._..  .-. .-.   .-.  ---/---.-._. \n /  .   )./.-\'_ / (    (   )  )/   )   )./.-\'_   /   (   )  \n(_.\' `-\' (__.\'_/_.-`---\'`-\'  \'/   /   ( (__.\'   /     `-\'   \n                                       `-\'                  """"""\n\n'"
tutorials/sagemaker/source/doc_classification.py,0,"b'import argparse\nimport logging\nfrom pathlib import Path\n\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import TextClassificationProcessor\nfrom farm.modeling.adaptive_model import AdaptiveModel\nfrom farm.modeling.language_model import LanguageModel\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.modeling.prediction_head import TextClassificationHead\nfrom farm.modeling.tokenization import Tokenizer\nfrom farm.train import Trainer\nfrom farm.utils import set_all_seeds, MLFlowLogger, initialize_device_settings\n\n\ndef doc_classification(args):\n    logging.basicConfig(\n        format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"", datefmt=""%m/%d/%Y %H:%M:%S"", level=logging.INFO\n    )\n\n    ml_logger = MLFlowLogger(tracking_uri="""")\n    ml_logger.init_experiment(experiment_name=""Public_FARM"", run_name=""Run_doc_classification"")\n\n    set_all_seeds(seed=42)\n    save_dir = Path(""/opt/ml/model"")\n    use_amp = None\n\n    device, n_gpu = initialize_device_settings(use_cuda=True, use_amp=use_amp)\n\n    # 1.Create a tokenizer\n    tokenizer = Tokenizer.load(pretrained_model_name_or_path=args.base_lm_model, do_lower_case=False)\n\n    # 2. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n    # Here we load GermEval 2018 Data.\n    label_list = [""OTHER"", ""OFFENSE""]\n    metric = ""f1_macro""\n\n    processor = TextClassificationProcessor(\n        tokenizer=tokenizer,\n        max_seq_len=args.max_seq_len,\n        data_dir=Path(""../data/germeval18""),\n        label_list=label_list,\n        metric=metric,\n        label_column_name=""coarse_label"",\n    )\n\n    # 3. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them and calculates a\n    #    few descriptive statistics of our datasets\n    data_silo = DataSilo(processor=processor, batch_size=args.batch_size)\n\n    # 4. Create an AdaptiveModel\n    # a) which consists of a pretrained language model as a basis\n    language_model = LanguageModel.load(args.base_lm_model)\n    # b) and a prediction head on top that is suited for our task => Text classification\n    prediction_head = TextClassificationHead(\n        class_weights=data_silo.calculate_class_weights(task_name=""text_classification""), num_labels=len(label_list)\n    )\n\n    model = AdaptiveModel(\n        language_model=language_model,\n        prediction_heads=[prediction_head],\n        embeds_dropout_prob=0.1,\n        lm_output_types=[""per_sequence""],\n        device=device,\n    )\n\n    # 5. Create an optimizer\n    model, optimizer, lr_schedule = initialize_optimizer(\n        model=model,\n        learning_rate=3e-5,\n        device=device,\n        n_batches=len(data_silo.loaders[""train""]),\n        n_epochs=args.n_epochs,\n        use_amp=use_amp,\n    )\n\n    # 6. Feed everything to the Trainer, which keeps care of growing our model into powerful plant and evaluates it from time to time\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        data_silo=data_silo,\n        epochs=args.n_epochs,\n        n_gpu=n_gpu,\n        lr_schedule=lr_schedule,\n        evaluate_every=args.evaluate_every,\n        device=device,\n    )\n\n    # 7. Let it grow\n    trainer.train()\n\n    # 8. Hooray! You have a model. Store it:\n    model.save(save_dir)\n    processor.save(save_dir)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(""--n_epochs"", type=int, default=2, help=""number of epochs (default: 2)"")\n    parser.add_argument(""--batch_size"", type=int, default=4, help=""batch size (default: 4)"")\n    parser.add_argument(""--max_seq_len"", type=int, default=64, help=""batch size (default: 64)"")\n    parser.add_argument(\n        ""--base_lm_model"",\n        type=str,\n        default=""bert-base-uncased"",\n        help=""base language model to use (default: bert-base-uncased)"",\n    )\n    parser.add_argument(\n        ""--evaluate_every"", type=int, default=100, help=""perform evaluation every n steps (default: 100)""\n    )\n    doc_classification(parser.parse_args())\n'"
