file_path,api_count,code
setup.py,0,"b'from io import open\n\nfrom setuptools import find_packages, setup\n\nwith open(""requirements.txt"") as f:\n    parsed_requirements = f.read().splitlines()\n# remove blank lines, comments and links to specific git commits\nparsed_requirements = [\n    x.strip()\n    for x in parsed_requirements\n    if ((x.strip()[0] != ""#"") and (len(x.strip()) > 3) and ""-e git://"" not in x)\n]\n\nsetup(\n    name=""farm-haystack"",\n    version=""0.2.1"",\n    author=""Malte Pietsch, Timo Moeller, Branden Chan, Tanay Soni"",\n    author_email=""malte.pietsch@deepset.ai"",\n    description=""Neural Question Answering at Scale. Use modern transformer based models like BERT to find answers in large document collections"",\n    long_description=open(""README.rst"", ""r"", encoding=""utf-8"").read(),\n    long_description_content_type=""text/x-rst"",\n    keywords=""QA Question-Answering Reader Retriever BERT roberta albert squad mrc transfer-learning language-model transformer"",\n    license=""Apache"",\n    url=""https://github.com/deepset-ai/haystack"",\n    download_url=""https://github.com/deepset-ai/haystack/archive/0.2.1.tar.gz"",\n    packages=find_packages(exclude=[""*.tests"", ""*.tests.*"", ""tests.*"", ""tests""]),\n    install_requires=parsed_requirements,\n    python_requires="">=3.6.0"",\n    tests_require=[""pytest""],\n    classifiers=[\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Programming Language :: Python :: 3"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n    ],\n)\n'"
haystack/__init__.py,0,"b""import logging\n\nimport pandas as pd\nfrom haystack.finder import Finder\n\npd.options.display.max_colwidth = 80\n\nlogger = logging.getLogger(__name__)\n\nlogging.getLogger('farm').setLevel(logging.WARNING)\nlogging.getLogger('farm.utils').setLevel(logging.INFO)\nlogging.getLogger('farm.infer').setLevel(logging.INFO)\nlogging.getLogger('transformers').setLevel(logging.WARNING)\nlogging.getLogger('farm.eval').setLevel(logging.INFO)\n\n\n"""
haystack/finder.py,0,"b'import logging\n\nimport numpy as np\nfrom scipy.special import expit\nimport time\nfrom statistics import mean\n\nlogger = logging.getLogger(__name__)\n\n\nclass Finder:\n    """"""\n    Finder ties together instances of the Reader and Retriever class.\n\n    It provides an interface to predict top n answers for a given question.\n    """"""\n\n    def __init__(self, reader, retriever):\n        self.retriever = retriever\n        self.reader = reader\n        if self.reader is None and self.retriever is None:\n            raise AttributeError(""Finder: self.reader and self.retriever can not be both None"")\n\n    def get_answers(self, question: str, top_k_reader: int = 1, top_k_retriever: int = 10, filters: dict = None):\n        """"""\n        Get top k answers for a given question.\n\n        :param question: the question string\n        :param top_k_reader: number of answers returned by the reader\n        :param top_k_retriever: number of text units to be retrieved\n        :param filters: limit scope to documents having the given tags and their corresponding values.\n            The format for the dict is {""tag-1"": [""value-1"",""value-2""], ""tag-2"": [""value-3]"" ...}\n        :return:\n        """"""\n\n        if self.retriever is None or self.reader is None:\n            raise AttributeError(""Finder.get_answers_via_similar_questions requires self.retriever AND self.reader"")\n\n        # 1) Apply retriever(with optional filters) to get fast candidate documents\n        documents = self.retriever.retrieve(question, filters=filters, top_k=top_k_retriever)\n\n        if len(documents) == 0:\n            logger.info(""Retriever did not return any documents. Skipping reader ..."")\n            results = {""question"": question, ""answers"": []}\n            return results\n\n        # 2) Apply reader to get granular answer(s)\n        len_chars = sum([len(d.text) for d in documents])\n        logger.info(f""Reader is looking for detailed answer in {len_chars} chars ..."")\n\n        results = self.reader.predict(question=question,\n                                      documents=documents,\n                                      top_k=top_k_reader)\n\n        # Add corresponding document_name and more meta data, if an answer contains the document_id\n        for ans in results[""answers""]:\n            ans[""meta""] = {}\n            for doc in documents:\n                if doc.id == ans[""document_id""]:\n                    ans[""meta""] = doc.meta\n\n        return results\n\n    def get_answers_via_similar_questions(self, question: str, top_k_retriever: int = 10, filters: dict = None):\n        """"""\n        Get top k answers for a given question using only a retriever.\n\n        :param question: the question string\n        :param top_k_retriever: number of text units to be retrieved\n        :param filters: limit scope to documents having the given tags and their corresponding values.\n            The format for the dict is {""tag-1"": ""value-1"", ""tag-2"": ""value-2"" ...}\n        :return:\n        """"""\n\n        if self.retriever is None:\n            raise AttributeError(""Finder.get_answers_via_similar_questions requires self.retriever"")\n\n        results = {""question"": question, ""answers"": []}\n\n        # 1) Optional: reduce the search space via document tags\n        if filters:\n            logging.info(f""Apply filters: {filters}"")\n            candidate_doc_ids = self.retriever.document_store.get_document_ids_by_tags(filters)\n            logger.info(f""Got candidate IDs due to filters:  {candidate_doc_ids}"")\n\n            if len(candidate_doc_ids) == 0:\n                # We didn\'t find any doc matching the filters\n                return results\n\n        else:\n            candidate_doc_ids = None\n\n        # 2) Apply retriever to match similar questions via cosine similarity of embeddings\n        documents = self.retriever.retrieve(question, top_k=top_k_retriever, candidate_doc_ids=candidate_doc_ids)\n\n        # 3) Format response\n        for doc in documents:\n            #TODO proper calibratation of pseudo probabilities\n            cur_answer = {""question"": doc.meta[""question""], ""answer"": doc.text, ""context"": doc.text,\n                          ""score"": doc.query_score, ""offset_start"": 0, ""offset_end"": len(doc.text), ""meta"": doc.meta\n                          }\n            if self.retriever.embedding_model:\n                probability = (doc.query_score + 1) / 2\n            else:\n                probability = float(expit(np.asarray(doc.query_score / 8)))\n            cur_answer[""probability""] = probability\n            results[""answers""].append(cur_answer)\n\n        return results\n\n    def eval(self, label_index: str = ""feedback"", doc_index: str = ""eval_document"", label_origin: str = ""gold_label"",\n             top_k_retriever: int = 10, top_k_reader: int = 10):\n        """"""\n        Evaluation of the whole pipeline by first evaluating the Retriever and then evaluating the Reader on the result\n        of the Retriever.\n\n        Returns a dict containing the following metrics:\n            - ""retriever_recall"": Proportion of questions for which correct document is among retrieved documents\n            - ""retriever_map"": Mean of average precision for each question. Rewards retrievers that give relevant\n              documents a higher rank.\n            - ""reader_top1_accuracy"": Proportion of highest ranked predicted answers that overlap with corresponding correct answer\n            - ""reader_top1_accuracy_has_answer"": Proportion of highest ranked predicted answers that overlap\n                                                 with corresponding correct answer for answerable questions\n            - ""reader_top_k_accuracy"": Proportion of predicted answers that overlap with corresponding correct answer\n            - ""reader_topk_accuracy_has_answer"": Proportion of predicted answers that overlap with corresponding correct answer\n                                                 for answerable questions\n            - ""reader_top1_em"": Proportion of exact matches of highest ranked predicted answers with their corresponding\n                                correct answers\n            - ""reader_top1_em_has_answer"": Proportion of exact matches of highest ranked predicted answers with their corresponding\n                                           correct answers for answerable questions\n            - ""reader_topk_em"": Proportion of exact matches of predicted answers with their corresponding correct answers\n            - ""reader_topk_em_has_answer"": Proportion of exact matches of predicted answers with their corresponding\n                                           correct answers for answerable questions\n            - ""reader_top1_f1"": Average overlap between highest ranked predicted answers and their corresponding correct answers\n            - ""reader_top1_f1_has_answer"": Average overlap between highest ranked predicted answers and their corresponding\n                                           correct answers for answerable questions\n            - ""reader_topk_f1"": Average overlap between predicted answers and their corresponding correct answers\n            - ""reader_topk_f1_has_answer"": Average overlap between predicted answers and their corresponding correct answers\n                                           for answerable questions\n            - ""reader_top1_no_answer_accuracy"": Proportion of correct predicting unanswerable question at highest ranked prediction\n            - ""reader_topk_no_answer_accuracy"": Proportion of correct predicting unanswerable question among all predictions\n            - ""total_retrieve_time"": Time retriever needed to retrieve documents for all questions\n            - ""avg_retrieve_time"": Average time needed to retrieve documents for one question\n            - ""total_reader_time"": Time reader needed to extract answer out of retrieved documents for all questions\n                                   where the correct document is among the retrieved ones\n            - ""avg_reader_time"": Average time needed to extract answer out of retrieved documents for one question\n            - ""total_finder_time"": Total time for whole pipeline\n\n        :param label_index: Elasticsearch index where labeled questions are stored\n        :type label_index: str\n        :param doc_index: Elasticsearch index where documents that are used for evaluation are stored\n        :type doc_index: str\n        :param top_k_retriever: How many documents per question to return and pass to reader\n        :type top_k_retriever: int\n        :param top_k_reader: How many answers to return per question\n        :type top_k_reader: int\n        """"""\n        finder_start_time = time.time()\n        # extract all questions for evaluation\n        filter = {""origin"": label_origin}\n        questions = self.retriever.document_store.get_all_documents_in_index(index=label_index, filters=filter)\n\n        correct_retrievals = 0\n        summed_avg_precision_retriever = 0\n        retrieve_times = []\n\n        correct_readings_top1 = 0\n        correct_readings_topk = 0\n        correct_readings_top1_has_answer = 0\n        correct_readings_topk_has_answer = 0\n        exact_matches_top1 = 0\n        exact_matches_topk = 0\n        exact_matches_top1_has_answer = 0\n        exact_matches_topk_has_answer = 0\n        summed_f1_top1 = 0\n        summed_f1_topk = 0\n        summed_f1_top1_has_answer = 0\n        summed_f1_topk_has_answer = 0\n        correct_no_answers_top1 = 0\n        correct_no_answers_topk =  0\n        read_times = []\n\n        # retrieve documents\n        questions_with_docs = []\n        retriever_start_time = time.time()\n        for q_idx, question in enumerate(questions):\n            question_string = question[""_source""][""question""]\n            single_retrieve_start = time.time()\n            retrieved_docs = self.retriever.retrieve(question_string, top_k=top_k_retriever, index=doc_index)\n            retrieve_times.append(time.time() - single_retrieve_start)\n            for doc_idx, doc in enumerate(retrieved_docs):\n                # check if correct doc among retrieved docs\n                if doc.meta[""doc_id""] == question[""_source""][""doc_id""]:\n                    correct_retrievals += 1\n                    summed_avg_precision_retriever += 1 / (doc_idx + 1)\n                    questions_with_docs.append({\n                        ""question"": question,\n                        ""docs"": retrieved_docs,\n                        ""correct_es_doc_id"": doc.id})\n                    break\n        retriever_total_time = time.time() - retriever_start_time\n        number_of_questions = q_idx + 1\n\n        number_of_no_answer = 0\n        previous_return_no_answers = self.reader.return_no_answers\n        self.reader.return_no_answers = True\n        # extract answers\n        reader_start_time = time.time()\n        for q_idx, question in enumerate(questions_with_docs):\n            if (q_idx + 1) % 100 == 0:\n                print(f""Processed {q_idx+1} questions."")\n            question_string = question[""question""][""_source""][""question""]\n            docs = question[""docs""]\n            single_reader_start = time.time()\n            predicted_answers = self.reader.predict(question_string, docs, top_k_reader)\n            read_times.append(time.time() - single_reader_start)\n            # check if question is answerable\n            if question[""question""][""_source""][""answers""]:\n                for answer_idx, answer in enumerate(predicted_answers[""answers""]):\n                    found_answer = False\n                    found_em = False\n                    best_f1 = 0\n                    # check if correct document\n                    if answer[""document_id""] == question[""correct_es_doc_id""]:\n                        gold_spans = [(gold_answer[""answer_start""], gold_answer[""answer_start""] + len(gold_answer[""text""]) + 1)\n                                      for gold_answer in question[""question""][""_source""][""answers""]]\n                        predicted_span = (answer[""offset_start_in_doc""], answer[""offset_end_in_doc""])\n\n                        for gold_span in gold_spans:\n                            # check if overlap between gold answer and predicted answer\n                            # top-1 answer\n                            if not found_answer:\n                                if (gold_span[0] <= predicted_span[1]) and (predicted_span[0] <= gold_span[1]):\n                                    # top-1 answer\n                                    if answer_idx == 0:\n                                        correct_readings_top1 += 1\n                                        correct_readings_top1_has_answer += 1\n                                    # top-k answers\n                                    correct_readings_topk += 1\n                                    correct_readings_topk_has_answer += 1\n                                    found_answer = True\n                            # check for exact match\n                            if not found_em:\n                                if (gold_span[0] == predicted_span[0]) and (gold_span[1] == predicted_span[1]):\n                                    # top-1-answer\n                                    if answer_idx == 0:\n                                        exact_matches_top1 += 1\n                                        exact_matches_top1_has_answer += 1\n                                    # top-k answers\n                                    exact_matches_topk += 1\n                                    exact_matches_topk_has_answer += 1\n                                    found_em = True\n                            # calculate f1\n                            pred_indices = list(range(predicted_span[0], predicted_span[1] + 1))\n                            gold_indices = list(range(gold_span[0], gold_span[1] + 1))\n                            n_overlap = len([x for x in pred_indices if x in gold_indices])\n                            if pred_indices and gold_indices and n_overlap:\n                                precision = n_overlap / len(pred_indices)\n                                recall = n_overlap / len(gold_indices)\n                                current_f1 = (2 * precision * recall) / (precision + recall)\n                                # top-1 answer\n                                if answer_idx == 0:\n                                    summed_f1_top1 += current_f1\n                                    summed_f1_top1_has_answer += current_f1\n                                if current_f1 > best_f1:\n                                    best_f1 = current_f1\n                        # top-k answers: use best f1-score\n                        summed_f1_topk += best_f1\n                        summed_f1_topk_has_answer += best_f1\n\n                    if found_answer and found_em:\n                        break\n            # question not answerable\n            else:\n                number_of_no_answer += 1\n                # As question is not answerable, it is not clear how to compute average precision for this question.\n                # For now, we decided to calculate average precision based on the rank of \'no answer\'.\n                for answer_idx, answer in enumerate(predicted_answers[""answers""]):\n                    # check if \'no answer\'\n                    if answer[""answer""] is None:\n                        if answer_idx == 0:\n                            correct_no_answers_top1 += 1\n                            correct_readings_top1 += 1\n                            exact_matches_top1 += 1\n                            summed_f1_top1 += 1\n                        correct_no_answers_topk += 1\n                        correct_readings_topk += 1\n                        exact_matches_topk += 1\n                        summed_f1_topk += 1\n                        break\n        number_of_has_answer = correct_retrievals - number_of_no_answer\n\n        reader_total_time = time.time() - reader_start_time\n        finder_total_time = time.time() - finder_start_time\n\n        retriever_recall = correct_retrievals / number_of_questions\n        retriever_map = summed_avg_precision_retriever / number_of_questions\n\n        reader_top1_accuracy = correct_readings_top1 / correct_retrievals\n        reader_top1_accuracy_has_answer = correct_readings_top1_has_answer / number_of_has_answer\n        reader_top_k_accuracy = correct_readings_topk / correct_retrievals\n        reader_topk_accuracy_has_answer = correct_readings_topk_has_answer / number_of_has_answer\n        reader_top1_em = exact_matches_top1 / correct_retrievals\n        reader_top1_em_has_answer = exact_matches_top1_has_answer / number_of_has_answer\n        reader_topk_em = exact_matches_topk / correct_retrievals\n        reader_topk_em_has_answer = exact_matches_topk_has_answer / number_of_has_answer\n        reader_top1_f1 = summed_f1_top1 / correct_retrievals\n        reader_top1_f1_has_answer = summed_f1_top1_has_answer / number_of_has_answer\n        reader_topk_f1 = summed_f1_topk / correct_retrievals\n        reader_topk_f1_has_answer = summed_f1_topk_has_answer / number_of_has_answer\n        reader_top1_no_answer_accuracy = correct_no_answers_top1 / number_of_no_answer\n        reader_topk_no_answer_accuracy = correct_no_answers_topk / number_of_no_answer\n\n        self.reader.return_no_answers = previous_return_no_answers\n\n        logger.info((f""{correct_readings_topk} out of {number_of_questions} questions were correctly answered ""\n                     f""({(correct_readings_topk/number_of_questions):.2%}).""))\n        logger.info(f""{number_of_questions-correct_retrievals} questions could not be answered due to the retriever."")\n        logger.info(f""{correct_retrievals-correct_readings_topk} questions could not be answered due to the reader."")\n\n        results = {\n            ""retriever_recall"": retriever_recall,\n            ""retriever_map"": retriever_map,\n            ""reader_top1_accuracy"": reader_top1_accuracy,\n            ""reader_top1_accuracy_has_answer"": reader_top1_accuracy_has_answer,\n            ""reader_top_k_accuracy"": reader_top_k_accuracy,\n            ""reader_topk_accuracy_has_answer"": reader_topk_accuracy_has_answer,\n            ""reader_top1_em"": reader_top1_em,\n            ""reader_top1_em_has_answer"": reader_top1_em_has_answer,\n            ""reader_topk_em"": reader_topk_em,\n            ""reader_topk_em_has_answer"": reader_topk_em_has_answer,\n            ""reader_top1_f1"": reader_top1_f1,\n            ""reader_top1_f1_has_answer"": reader_top1_f1_has_answer,\n            ""reader_topk_f1"": reader_topk_f1,\n            ""reader_topk_f1_has_answer"": reader_topk_f1_has_answer,\n            ""reader_top1_no_answer_accuracy"": reader_top1_no_answer_accuracy,\n            ""reader_topk_no_answer_accuracy"": reader_topk_no_answer_accuracy,\n            ""total_retrieve_time"": retriever_total_time,\n            ""avg_retrieve_time"": mean(retrieve_times),\n            ""total_reader_time"": reader_total_time,\n            ""avg_reader_time"": mean(read_times),\n            ""total_finder_time"": finder_total_time\n        }\n\n        return results'"
haystack/utils.py,0,"b'import json\nfrom collections import defaultdict\nimport logging\nimport pprint\n\nfrom haystack.database.sql import Document\n\nlogger = logging.getLogger(__name__)\n\n\ndef print_answers(results, details=""all""):\n    answers = results[""answers""]\n    pp = pprint.PrettyPrinter(indent=4)\n    if details != ""all"":\n        if details == ""minimal"":\n            keys_to_keep = set([""answer"", ""context""])\n        elif details == ""medium"":\n            keys_to_keep = set([""answer"", ""context"", ""score""])\n        # filter the results\n        keys_to_drop = set(answers[0].keys()) - keys_to_keep\n        for a in answers:\n            for key in keys_to_drop:\n                if key in a:\n                    del a[key]\n\n        pp.pprint(answers)\n    else:\n        pp.pprint(results)\n\n\ndef convert_labels_to_squad(labels_file):\n    """"""\n    Convert the export from the labeling UI to SQuAD format for training.\n\n    :param labels_file: path for export file from the labeling tool\n    :return:\n    """"""\n    with open(labels_file) as label_file:\n        labels = json.load(label_file)\n\n    labels_grouped_by_documents = defaultdict(list)\n    for label in labels:\n        labels_grouped_by_documents[label[""document_id""]].append(label)\n\n    labels_in_squad_format = {""data"": []}\n    for document_id, labels in labels_grouped_by_documents.items():\n        qas = []\n        for label in labels:\n            doc = Document.query.get(label[""document_id""])\n\n            assert (\n                doc.text[label[""start_offset""] : label[""end_offset""]]\n                == label[""selected_text""]\n            )\n\n            qas.append(\n                {\n                    ""question"": label[""question""],\n                    ""id"": label[""id""],\n                    ""question_id"": label[""question_id""],\n                    ""answers"": [\n                        {\n                            ""text"": label[""selected_text""],\n                            ""answer_start"": label[""start_offset""],\n                            ""labeller_id"": label[""labeler_id""],\n                        }\n                    ],\n                    ""is_impossible"": False,\n                }\n            )\n\n        squad_format_label = {\n            ""paragraphs"": [\n                {""qas"": qas, ""context"": doc.text, ""document_id"": document_id}\n            ]\n        }\n\n        labels_in_squad_format[""data""].append(squad_format_label)\n\n    with open(""labels_in_squad_format.json"", ""w+"") as outfile:\n        json.dump(labels_in_squad_format, outfile)\n'"
test/conftest.py,0,"b'import tarfile\nimport time\nimport urllib.request\nfrom subprocess import Popen, PIPE, STDOUT, run\n\nimport pytest\nfrom elasticsearch import Elasticsearch\n\n\n@pytest.fixture(scope=\'session\')\ndef elasticsearch_dir(tmpdir_factory):\n    return tmpdir_factory.mktemp(\'elasticsearch\')\n\n\n@pytest.fixture(scope=""session"")\ndef elasticsearch_fixture(elasticsearch_dir):\n    # test if a ES cluster is already running. If not, download and start an ES instance locally.\n    try:\n        client = Elasticsearch(hosts=[{""host"": ""localhost""}])\n        client.info()\n    except:\n        thetarfile = ""https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.1-linux-x86_64.tar.gz""\n        ftpstream = urllib.request.urlopen(thetarfile)\n        thetarfile = tarfile.open(fileobj=ftpstream, mode=""r|gz"")\n        thetarfile.extractall(path=elasticsearch_dir)\n        es_server = Popen([elasticsearch_dir / ""elasticsearch-7.6.1/bin/elasticsearch""], stdout=PIPE, stderr=STDOUT)\n        time.sleep(40)\n\n\n@pytest.fixture(scope=""session"")\ndef xpdf_fixture():\n    verify_installation = run([""pdftotext""], shell=True)\n    if verify_installation.returncode == 127:\n        commands = """""" wget --no-check-certificate https://dl.xpdfreader.com/xpdf-tools-linux-4.02.tar.gz &&\n                       tar -xvf xpdf-tools-linux-4.02.tar.gz && sudo cp xpdf-tools-linux-4.02/bin64/pdftotext /usr/local/bin""""""\n        run([commands], shell=True)\n\n        verify_installation = run([""pdftotext -v""], shell=True)\n        if verify_installation.returncode == 127:\n            raise Exception(\n                """"""pdftotext is not installed. It is part of xpdf or poppler-utils software suite.\n                 You can download for your OS from here: https://www.xpdfreader.com/download.html.""""""\n            )\n'"
test/test_db.py,0,"b'from time import sleep\n\nfrom haystack.database.elasticsearch import ElasticsearchDocumentStore\nfrom haystack.database.sql import SQLDocumentStore\nfrom haystack.indexing.utils import convert_files_to_dicts\n\n\ndef test_sql_write_read():\n    sql_document_store = SQLDocumentStore()\n    documents = convert_files_to_dicts(dir_path=""samples/docs"")\n    sql_document_store.write_documents(documents)\n    documents = sql_document_store.get_all_documents()\n    assert len(documents) == 2\n    doc = sql_document_store.get_document_by_id(""1"")\n    assert doc.id\n    assert doc.text\n\n\ndef test_elasticsearch_write_read(elasticsearch_fixture):\n    document_store = ElasticsearchDocumentStore()\n    documents = convert_files_to_dicts(dir_path=""samples/docs"")\n    document_store.write_documents(documents)\n    sleep(2)  # wait for documents to be available for query\n    documents = document_store.get_all_documents()\n    assert len(documents) == 2\n    assert documents[0].id\n    assert documents[0].text\n'"
test/test_document.py,0,"b'from haystack.database.base import Document\n\n\ndef test_document_data_access():\n    doc = Document(id=1, text=""test"")\n    assert doc.text == ""test""\n'"
test/test_faq_retriever.py,0,"b'from haystack import Finder\n\n\ndef test_faq_retriever_in_memory_store(monkeypatch):\n    monkeypatch.setenv(""EMBEDDING_FIELD_NAME"", ""embedding"")\n\n    from haystack.database.memory import InMemoryDocumentStore\n    from haystack.retriever.elasticsearch import EmbeddingRetriever\n\n    document_store = InMemoryDocumentStore()\n\n    documents = [\n        {\'name\': \'How to test this library?\', \'text\': \'By running tox in the command line!\', \'meta\': {\'question\': \'How to test this library?\'}},\n        {\'name\': \'blah blah blah\', \'text\': \'By running tox in the command line!\', \'meta\': {\'question\': \'blah blah blah\'}},\n        {\'name\': \'blah blah blah\', \'text\': \'By running tox in the command line!\', \'meta\': {\'question\': \'blah blah blah\'}},\n        {\'name\': \'blah blah blah\', \'text\': \'By running tox in the command line!\', \'meta\': {\'question\': \'blah blah blah\'}},\n        {\'name\': \'blah blah blah\', \'text\': \'By running tox in the command line!\', \'meta\': {\'question\': \'blah blah blah\'}},\n        {\'name\': \'blah blah blah\', \'text\': \'By running tox in the command line!\', \'meta\': {\'question\': \'blah blah blah\'}},\n        {\'name\': \'blah blah blah\', \'text\': \'By running tox in the command line!\', \'meta\': {\'question\': \'blah blah blah\'}},\n        {\'name\': \'blah blah blah\', \'text\': \'By running tox in the command line!\', \'meta\': {\'question\': \'blah blah blah\'}},\n        {\'name\': \'blah blah blah\', \'text\': \'By running tox in the command line!\', \'meta\': {\'question\': \'blah blah blah\'}},\n        {\'name\': \'blah blah blah\', \'text\': \'By running tox in the command line!\', \'meta\': {\'question\': \'blah blah blah\'}},\n        {\'name\': \'blah blah blah\', \'text\': \'By running tox in the command line!\', \'meta\': {\'question\': \'blah blah blah\'}},\n    ]\n\n    retriever = EmbeddingRetriever(document_store=document_store, embedding_model=""deepset/sentence_bert"", gpu=False)\n\n    embedded = []\n    for doc in documents:\n        doc[\'embedding\'] = retriever.create_embedding([doc[\'meta\'][\'question\']])[0]\n        embedded.append(doc)\n\n    document_store.write_documents(embedded)\n\n    finder = Finder(reader=None, retriever=retriever)\n    prediction = finder.get_answers_via_similar_questions(question=""How to test this?"", top_k_retriever=1)\n\n    assert len(prediction.get(\'answers\', [])) == 1\n'"
test/test_farm_reader.py,0,"b'from haystack.reader.farm import FARMReader\n\n\ndef test_farm_reader():\n    reader = FARMReader(model_name_or_path=""deepset/bert-base-cased-squad2"", use_gpu=False)\n    assert reader is not None\n    assert isinstance(reader, FARMReader)\n'"
test/test_finder.py,0,"b'from haystack import Finder\nfrom haystack.database.sql import SQLDocumentStore\nfrom haystack.reader.transformers import TransformersReader\nfrom haystack.retriever.tfidf import TfidfRetriever\n\n\ndef test_finder_get_answers():\n    test_docs = [\n        {""name"": ""testing the finder 1"", ""text"": ""testing the finder with pyhton unit test 1"", ""meta"": {""test"": ""test""}},\n        {""name"": ""testing the finder 2"", ""text"": ""testing the finder with pyhton unit test 2"", ""meta"": {""test"": ""test""}},\n        {""name"": ""testing the finder 3"", ""text"": ""testing the finder with pyhton unit test 3"", ""meta"": {""test"": ""test""}}\n    ]\n\n    document_store = SQLDocumentStore(url=""sqlite:///qa_test.db"")\n    document_store.write_documents(test_docs)\n    retriever = TfidfRetriever(document_store=document_store)\n    reader = TransformersReader(model=""distilbert-base-uncased-distilled-squad"",\n                                tokenizer=""distilbert-base-uncased"", use_gpu=-1)\n    finder = Finder(reader, retriever)\n    prediction = finder.get_answers(question=""testing finder"", top_k_retriever=10,\n                                    top_k_reader=5)\n    assert prediction is not None\n\n\ndef test_finder_get_answers_single_result():\n    test_docs = [\n        {""name"": ""testing the finder 1"", ""text"": ""testing the finder with pyhton unit test 1""},\n        {""name"": ""testing the finder 2"", ""text"": ""testing the finder with pyhton unit test 2""},\n        {""name"": ""testing the finder 3"", ""text"": ""testing the finder with pyhton unit test 3""}\n    ]\n\n    document_store = SQLDocumentStore(url=""sqlite:///qa_test.db"")\n    document_store.write_documents(test_docs)\n    retriever = TfidfRetriever(document_store=document_store)\n    reader = TransformersReader(model=""distilbert-base-uncased-distilled-squad"",\n                                tokenizer=""distilbert-base-uncased"", use_gpu=-1)\n    finder = Finder(reader, retriever)\n    prediction = finder.get_answers(question=""testing finder"", top_k_retriever=1,\n                                    top_k_reader=1)\n    assert prediction is not None\n'"
test/test_imports.py,0,"b'def test_module_imports():\n    from haystack import Finder\n    from haystack.database.sql import SQLDocumentStore\n    from haystack.indexing.cleaning import clean_wiki_text\n    from haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http\n    from haystack.reader.farm import FARMReader\n    from haystack.reader.transformers import TransformersReader\n    from haystack.retriever.tfidf import TfidfRetriever\n    from haystack.utils import print_answers\n\n    assert Finder is not None\n    assert SQLDocumentStore is not None\n    assert clean_wiki_text is not None\n    assert convert_files_to_dicts is not None\n    assert fetch_archive_from_http is not None\n    assert FARMReader is not None\n    assert TransformersReader is not None\n    assert TfidfRetriever is not None\n    assert print_answers is not None\n'"
test/test_in_memory_store.py,0,"b'from haystack import Finder\nfrom haystack.reader.transformers import TransformersReader\nfrom haystack.retriever.tfidf import TfidfRetriever\n\n\ndef test_finder_get_answers_with_in_memory_store():\n    test_docs = [\n        {""name"": ""testing the finder 1"", ""text"": ""testing the finder with pyhton unit test 1"", \'meta\': {\'url\': \'url\'}},\n        {""name"": ""testing the finder 2"", ""text"": ""testing the finder with pyhton unit test 2"", \'meta\': {\'url\': \'url\'}},\n        {""name"": ""testing the finder 3"", ""text"": ""testing the finder with pyhton unit test 3"", \'meta\': {\'url\': \'url\'}}\n    ]\n\n    from haystack.database.memory import InMemoryDocumentStore\n    document_store = InMemoryDocumentStore()\n    document_store.write_documents(test_docs)\n\n    retriever = TfidfRetriever(document_store=document_store)\n    reader = TransformersReader(model=""distilbert-base-uncased-distilled-squad"",\n                                tokenizer=""distilbert-base-uncased"", use_gpu=-1)\n    finder = Finder(reader, retriever)\n    prediction = finder.get_answers(question=""testing finder"", top_k_retriever=10,\n                                    top_k_reader=5)\n    assert prediction is not None\n\n\ndef test_memory_store_get_by_tags():\n    test_docs = [\n        {""name"": ""testing the finder 1"", ""text"": ""testing the finder with pyhton unit test 1"", \'meta\': {\'url\': \'url\'}},\n        {""name"": ""testing the finder 2"", ""text"": ""testing the finder with pyhton unit test 2"", \'meta\': {\'url\': None}},\n        {""name"": ""testing the finder 3"", ""text"": ""testing the finder with pyhton unit test 3"", \'meta\': {\'url\': \'url\'}}\n    ]\n\n    from haystack.database.memory import InMemoryDocumentStore\n    document_store = InMemoryDocumentStore()\n    document_store.write_documents(test_docs)\n\n    docs = document_store.get_document_ids_by_tags({\'has_url\': \'false\'})\n\n    assert docs == []\n\n\ndef test_memory_store_get_by_tag_lists_union():\n    test_docs = [\n        {""name"": ""testing the finder 1"", ""text"": ""testing the finder with pyhton unit test 1"", \'meta\': {\'url\': \'url\'}, \'tags\': [{\'tag2\': [""1""]}]},\n        {""name"": ""testing the finder 2"", ""text"": ""testing the finder with pyhton unit test 2"", \'meta\': {\'url\': None}, \'tags\': [{\'tag1\': [\'1\']}]},\n        {""name"": ""testing the finder 3"", ""text"": ""testing the finder with pyhton unit test 3"", \'meta\': {\'url\': \'url\'}, \'tags\': [{\'tag2\': [""1"", ""2""]}]}\n    ]\n\n    from haystack.database.memory import InMemoryDocumentStore\n    document_store = InMemoryDocumentStore()\n    document_store.write_documents(test_docs)\n\n    docs = document_store.get_document_ids_by_tags({\'tag2\': [""1""]})\n\n    assert docs == [\n        {\'name\': \'testing the finder 1\', \'text\': \'testing the finder with pyhton unit test 1\', \'meta\': {\'url\': \'url\'}, \'tags\': [{\'tag2\': [\'1\']}]},\n        {\'name\': \'testing the finder 3\', \'text\': \'testing the finder with pyhton unit test 3\', \'meta\': {\'url\': \'url\'}, \'tags\': [{\'tag2\': [\'1\', \'2\']}]}\n    ]\n\n\ndef test_memory_store_get_by_tag_lists_non_existent_tag():\n    test_docs = [\n        {""name"": ""testing the finder 1"", ""text"": ""testing the finder with pyhton unit test 1"", \'meta\': {\'url\': \'url\'}, \'tags\': [{\'tag1\': [""1""]}]},\n    ]\n    from haystack.database.memory import InMemoryDocumentStore\n    document_store = InMemoryDocumentStore()\n    document_store.write_documents(test_docs)\n    docs = document_store.get_document_ids_by_tags({\'tag1\': [""3""]})\n    assert docs == []\n\n\ndef test_memory_store_get_by_tag_lists_disjoint():\n    test_docs = [\n        {""name"": ""testing the finder 1"", ""text"": ""testing the finder with pyhton unit test 1"", \'meta\': {\'url\': \'url\'}, \'tags\': [{\'tag1\': [""1""]}]},\n        {""name"": ""testing the finder 2"", ""text"": ""testing the finder with pyhton unit test 2"", \'meta\': {\'url\': None}, \'tags\': [{\'tag2\': [\'1\']}]},\n        {""name"": ""testing the finder 3"", ""text"": ""testing the finder with pyhton unit test 3"", \'meta\': {\'url\': \'url\'}, \'tags\': [{\'tag3\': [""1"", ""2""]}]},\n        {""name"": ""testing the finder 4"", ""text"": ""testing the finder with pyhton unit test 3"", \'meta\': {\'url\': \'url\'}, \'tags\': [{\'tag3\': [""1"", ""3""]}]}\n    ]\n\n    from haystack.database.memory import InMemoryDocumentStore\n    document_store = InMemoryDocumentStore()\n    document_store.write_documents(test_docs)\n\n    docs = document_store.get_document_ids_by_tags({\'tag3\': [""3""]})\n\n    assert docs == [{\'name\': \'testing the finder 4\', \'text\': \'testing the finder with pyhton unit test 3\', \'meta\': {\'url\': \'url\'}, \'tags\': [{\'tag3\': [\'1\', \'3\']}]}]\n'"
test/test_pdf_conversion.py,0,"b'import logging\nfrom pathlib import Path\n\nfrom haystack.indexing.file_converters.pdftotext import PDFToTextConverter\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_extract_pages(xpdf_fixture):\n    converter = PDFToTextConverter()\n    pages = converter.extract_pages(file_path=Path(""samples/pdf/sample_pdf_1.pdf""))\n    assert len(pages) == 4  # the sample PDF file has four pages.\n    assert pages[0] != """"  # the page 1 of PDF contains text.\n    assert pages[2] == """"  # the page 3 of PDF file is empty.\n\n\ndef test_table_removal(xpdf_fixture):\n    converter = PDFToTextConverter(remove_numeric_tables=True)\n    pages = converter.extract_pages(file_path=Path(""samples/pdf/sample_pdf_1.pdf""))\n\n    # assert numeric rows are removed from the table.\n    assert ""324"" not in pages[0]\n    assert ""54x growth"" not in pages[0]\n    assert ""$54.35"" not in pages[0]\n\n    # assert text is retained from the document.\n    assert ""Adobe Systems made the PDF specification available free of charge in 1993."" in pages[0]\n\n\ndef test_language_validation(xpdf_fixture, caplog):\n    converter = PDFToTextConverter(valid_languages=[""en""])\n    pages = converter.extract_pages(file_path=Path(""samples/pdf/sample_pdf_1.pdf""))\n    assert ""The language for samples/pdf/sample_pdf_1.pdf is not one of [\'en\']."" not in caplog.text\n\n    converter = PDFToTextConverter(valid_languages=[""de""])\n    pages = converter.extract_pages(file_path=Path(""samples/pdf/sample_pdf_1.pdf""))\n    assert ""The language for samples/pdf/sample_pdf_1.pdf is not one of [\'de\']."" in caplog.text\n\n\ndef test_header_footer_removal(xpdf_fixture):\n    converter = PDFToTextConverter(remove_header_footer=True)\n    converter_no_removal = PDFToTextConverter(remove_header_footer=False)\n\n    pages1 = converter.extract_pages(file_path=Path(""samples/pdf/sample_pdf_1.pdf""))  # file contains no header/footer\n    pages2 = converter_no_removal.extract_pages(file_path=Path(""samples/pdf/sample_pdf_1.pdf""))  # file contains no header/footer\n    for p1, p2 in zip(pages1, pages2):\n        assert p2 == p2\n\n    pages = converter.extract_pages(file_path=Path(""samples/pdf/sample_pdf_2.pdf""))  # file contains header and footer\n    for page in pages:\n        assert ""header"" not in page\n        assert ""footer"" not in page'"
test/test_tfidf_retriever.py,0,"b'from haystack.database.base import Document\n\n\ndef test_tfidf_retriever():\n    from haystack.retriever.tfidf import TfidfRetriever\n\n    test_docs = [\n        {""name"": ""testing the finder 1"", ""text"": ""godzilla says hello""},\n        {""name"": ""testing the finder 2"", ""text"": ""optimus prime says bye""},\n        {""name"": ""testing the finder 3"", ""text"": ""alien says arghh""}\n    ]\n\n    from haystack.database.memory import InMemoryDocumentStore\n    document_store = InMemoryDocumentStore()\n    document_store.write_documents(test_docs)\n\n    retriever = TfidfRetriever(document_store)\n    retriever.fit()\n    assert retriever.retrieve(""godzilla"", top_k=1) == [Document(id=\'0\', text=\'godzilla says hello\', external_source_id=None, question=None, query_score=None, meta={})]\n'"
tutorials/Tutorial1_Basic_QA_Pipeline.py,0,"b'# ## Task: Question Answering for Game of Thrones\n#\n# Question Answering can be used in a variety of use cases. A very common one:  Using it to navigate through complex\n# knowledge bases or long documents (""search setting"").\n#\n# A ""knowledge base"" could for example be your website, an internal wiki or a collection of financial reports.\n# In this tutorial we will work on a slightly different domain: ""Game of Thrones"".\n#\n# Let\'s see how we can use a bunch of Wikipedia articles to answer a variety of questions about the\n# marvellous seven kingdoms...\n\nimport logging\nimport subprocess\nimport time\n\nfrom haystack import Finder\nfrom haystack.database.elasticsearch import ElasticsearchDocumentStore\nfrom haystack.indexing.cleaning import clean_wiki_text\nfrom haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http\nfrom haystack.reader.farm import FARMReader\nfrom haystack.reader.transformers import TransformersReader\nfrom haystack.utils import print_answers\nfrom haystack.retriever.elasticsearch import ElasticsearchRetriever\n\n\nLAUNCH_ELASTICSEARCH = True\n\n# ## Document Store\n#\n# Haystack finds answers to queries within the documents stored in a `DocumentStore`. The current implementations of\n# `DocumentStore` include `ElasticsearchDocumentStore`, `SQLDocumentStore`, and `InMemoryDocumentStore`.\n#\n# **Here:** We recommended Elasticsearch as it comes preloaded with features like full-text queries, BM25 retrieval,\n# and vector storage for text embeddings.\n# **Alternatives:** If you are unable to setup an Elasticsearch instance, then follow the Tutorial 3\n# for using SQL/InMemory document stores.\n# **Hint**:\n# This tutorial creates a new document store instance with Wikipedia articles on Game of Thrones. However, you can\n# configure Haystack to work with your existing document stores.\n#\n# Start an Elasticsearch server\n# You can start Elasticsearch on your local machine instance using Docker. If Docker is not readily available in\n# your environment (eg., in Colab notebooks), then you can manually download and execute Elasticsearch from source.\n\nif LAUNCH_ELASTICSEARCH:\n    logging.info(""Starting Elasticsearch ..."")\n    status = subprocess.run(\n        [\'docker run -d -p 9200:9200 -e ""discovery.type=single-node"" elasticsearch:7.6.2\'], shell=True\n    )\n    if status.returncode:\n        raise Exception(""Failed to launch Elasticsearch. If you want to connect to an existing Elasticsearch instance""\n                        ""then set LAUNCH_ELASTICSEARCH in the script to False."")\n    time.sleep(15)\n\n# Connect to Elasticsearch\ndocument_store = ElasticsearchDocumentStore(host=""localhost"", username="""", password="""", index=""document"")\n\n# ## Cleaning & indexing documents\n#\n# Haystack provides a customizable cleaning and indexing pipeline for ingesting documents in Document Stores.\n#\n# In this tutorial, we download Wikipedia articles on Game of Thrones, apply a basic cleaning function, and index\n# them in Elasticsearch.\n\n\n# Let\'s first get some documents that we want to query\n# Here: 517 Wikipedia articles for Game of Thrones\ndoc_dir = ""data/article_txt_got""\ns3_url = ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip""\nfetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n\n# convert files to dicts containing documents that can be indexed to our datastore\ndicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\n# You can optionally supply a cleaning function that is applied to each doc (e.g. to remove footers)\n# It must take a str as input, and return a str.\n\n# Now, let\'s write the docs to our DB.\ndocument_store.write_documents(dicts)\n\n\n# ## Initalize Retriever, Reader,  & Finder\n#\n# ### Retriever\n#\n# Retrievers help narrowing down the scope for the Reader to smaller units of text where a given question\n# could be answered.\n#\n# They use some simple but fast algorithm.\n# **Here:** We use Elasticsearch\'s default BM25 algorithm\n# **Alternatives:**\n# - Customize the `ElasticsearchRetriever`with custom queries (e.g. boosting) and filters\n# - Use `EmbeddingRetriever` to find candidate documents based on the similarity of\n#   embeddings (e.g. created via Sentence-BERT)\n# - Use `TfidfRetriever` in combination with a SQL or InMemory Document store for simple prototyping and debugging\n\nretriever = ElasticsearchRetriever(document_store=document_store)\n\n# Alternative: An in-memory TfidfRetriever based on Pandas dataframes for building quick-prototypes\n# with SQLite document store.\n#\n# from haystack.retriever.tfidf import TfidfRetriever\n# retriever = TfidfRetriever(document_store=document_store)\n\n# ### Reader\n#\n# A Reader scans the texts returned by retrievers in detail and extracts the k best answers. They are based\n# on powerful, but slower deep learning models.\n#\n# Haystack currently supports Readers based on the frameworks FARM and Transformers.\n# With both you can either load a local model or one from Hugging Face\'s model hub (https://huggingface.co/models).\n# **Here:** a medium sized RoBERTa QA model using a Reader based on\n#           FARM (https://huggingface.co/deepset/roberta-base-squad2)\n# **Alternatives (Reader):** TransformersReader (leveraging the `pipeline` of the Transformers package)\n# **Alternatives (Models):** e.g. ""distilbert-base-uncased-distilled-squad"" (fast) or\n#                            ""deepset/bert-large-uncased-whole-word-masking-squad2"" (good accuracy)\n# **Hint:** You can adjust the model to return ""no answer possible"" with the no_ans_boost. Higher values mean\n#           the model prefers ""no answer possible""\n#\n# #### FARMReader\n\n# Load a  local model or any of the QA models on\n# Hugging Face\'s model hub (https://huggingface.co/models)\nreader = FARMReader(model_name_or_path=""deepset/roberta-base-squad2"", use_gpu=False)\n\n# #### TransformersReader\n\n# Alternative:\n# reader = TransformersReader(\n#    model=""distilbert-base-uncased-distilled-squad"", tokenizer=""distilbert-base-uncased"", use_gpu=-1)\n\n# ### Finder\n#\n# The Finder sticks together reader and retriever in a pipeline to answer our actual questions.\n\nfinder = Finder(reader, retriever)\n\n# ## Voil\xc3\xa0! Ask a question!\n# You can configure how many candidates the reader and retriever shall return\n# The higher top_k_retriever, the better (but also the slower) your answers.\nprediction = finder.get_answers(question=""Who is the father of Arya Stark?"", top_k_retriever=10, top_k_reader=5)\n\n\n# prediction = finder.get_answers(question=""Who created the Dothraki vocabulary?"", top_k_reader=5)\n# prediction = finder.get_answers(question=""Who is the sister of Sansa?"", top_k_reader=5)\n\nprint_answers(prediction, details=""minimal"")\n'"
tutorials/Tutorial2_Finetune_a_model_on_your_data.py,0,"b'# # Fine-tuning a model on your own data\n# \n# For many use cases it is sufficient to just use one of the existing public models that were trained on SQuAD or\n# other public QA datasets (e.g. ).\n# However, if you have domain-specific questions, fine-tuning your model on custom examples will very likely boost\n# your performance. While this varies by domain, we saw that ~ 2000 examples can easily increase performance by +5-20%.\n# \n# This tutorial shows you how to fine-tune a pretrained model on your own dataset.\n\nfrom haystack.reader.farm import FARMReader\n\n\n# ## Create Training Data\n# \n# There are two ways to generate training data\n# \n# 1. **Annotation**: You can use the annotation tool(https://github.com/deepset-ai/haystack#labeling-tool) to label\n#                    your data, i.e. highlighting answers to your questions in a document. The tool supports structuring\n#                   your workflow with organizations, projects, and users. The labels can be exported in SQuAD format\n#                    that is compatible for training with Haystack.\n# \n# 2. **Feedback**:   For production systems, you can collect training data from direct user feedback via Haystack\'s\n#                    REST API interface. This includes a customizable user feedback API for providing feedback on the\n#                    answer returned by the API. The API provides feedback export endpoint to obtain the feedback data\n#                    for fine-tuning your model further.\n# \n# \n# ## Fine-tune your model\n# \n# Once you have collected training data, you can fine-tune your base models.\n# We initialize a reader as a base model and fine-tune it on our own custom dataset (should be in SQuAD-like format).\n# We recommend using a base model that was trained on SQuAD or a similar QA dataset before to benefit from Transfer\n# Learning effects.\n\n#**Recommendation: Run training on a GPU. To do so change the `use_gpu` arguments below to `True`\n\nreader = FARMReader(model_name_or_path=""distilbert-base-uncased-distilled-squad"", use_gpu=False)\ntrain_data = ""data/squad20""\n# train_data = ""PATH/TO_YOUR/TRAIN_DATA"" \nreader.train(data_dir=train_data, train_filename=""dev-v2.0.json"", use_gpu=False, n_epochs=1, save_dir=""my_model"")\n\n# Saving the model happens automatically at the end of training into the `save_dir` you specified\n# However, you could also save a reader manually again via:\nreader.save(directory=""my_model"")\n\n# If you want to load it at a later point, just do:\nnew_reader = FARMReader(model_name_or_path=""my_model"")'"
tutorials/Tutorial3_Basic_QA_Pipeline_without_Elasticsearch.py,0,"b'# ## Task: Build a Question Answering pipeline without Elasticsearch\n#\n# Haystack provides alternatives to Elasticsearch for developing quick prototypes.\n#\n# You can use an `InMemoryDocumentStore` or a `SQLDocumentStore`(with SQLite) as the document store.\n#\n# If you are interested in more feature-rich Elasticsearch, then please refer to the Tutorial 1.\n\n\nfrom haystack import Finder\nfrom haystack.database.memory import InMemoryDocumentStore\nfrom haystack.database.sql import SQLDocumentStore\nfrom haystack.indexing.cleaning import clean_wiki_text\nfrom haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http\nfrom haystack.reader.farm import FARMReader\nfrom haystack.reader.transformers import TransformersReader\nfrom haystack.retriever.tfidf import TfidfRetriever\nfrom haystack.utils import print_answers\n\n\n# In-Memory Document Store\ndocument_store = InMemoryDocumentStore()\n\n# or, alternatively, SQLite Document Store\n# document_store = SQLDocumentStore(url=""sqlite:///qa.db"")\n\n\n# ## Cleaning & indexing documents\n#\n# Haystack provides a customizable cleaning and indexing pipeline for ingesting documents in Document Stores.\n#\n# In this tutorial, we download Wikipedia articles on Game of Thrones, apply a basic cleaning function, and index\n# them in Elasticsearch.\n# Let\'s first get some documents that we want to query\n# Here: 517 Wikipedia articles for Game of Thrones\ndoc_dir = ""data/article_txt_got""\ns3_url = ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/documents/wiki_gameofthrones_txt.zip""\nfetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n\n# convert files to dicts containing documents that can be indexed to our datastore\ndicts = convert_files_to_dicts(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=True)\n# You can optionally supply a cleaning function that is applied to each doc (e.g. to remove footers)\n# It must take a str as input, and return a str.\n\n# Now, let\'s write the docs to our DB.\ndocument_store.write_documents(dicts)\n\n\n# ## Initalize Retriever, Reader,  & Finder\n#\n# ### Retriever\n#\n# Retrievers help narrowing down the scope for the Reader to smaller units of text where\n# a given question could be answered.\n#\n# With InMemoryDocumentStore or SQLDocumentStore, you can use the TfidfRetriever. For more\n# retrievers, please refer to the tutorial-1.\n\n# An in-memory TfidfRetriever based on Pandas dataframes\nretriever = TfidfRetriever(document_store=document_store)\n\n# ### Reader\n#\n# A Reader scans the texts returned by retrievers in detail and extracts the k best answers. They are based\n# on powerful, but slower deep learning models.\n#\n# Haystack currently supports Readers based on the frameworks FARM and Transformers.\n# With both you can either load a local model or one from Hugging Face\'s model hub (https://huggingface.co/models).\n\n# **Here:**                   a medium sized RoBERTa QA model using a Reader based on\n#                             FARM (https://huggingface.co/deepset/roberta-base-squad2)\n# **Alternatives (Reader):**  TransformersReader (leveraging the `pipeline` of the Transformers package)\n# **Alternatives (Models):**  e.g. ""distilbert-base-uncased-distilled-squad"" (fast) or\n#                             ""deepset/bert-large-uncased-whole-word-masking-squad2"" (good accuracy)\n# **Hint:**                   You can adjust the model to return ""no answer possible"" with the no_ans_boost.\n#                             Higher values mean the model prefers ""no answer possible"".\n\n# #### FARMReader\n#\n# Load a  local model or any of the QA models on\n# Hugging Face\'s model hub (https://huggingface.co/models)\nreader = FARMReader(model_name_or_path=""deepset/roberta-base-squad2"", use_gpu=False)\n\n\n# #### TransformersReader\n# Alternative:\n# reader = TransformersReader(model=""distilbert-base-uncased-distilled-squad"", tokenizer=""distilbert-base-uncased"", use_gpu=-1)\n\n# ### Finder\n#\n# The Finder sticks together reader and retriever in a pipeline to answer our actual questions.\nfinder = Finder(reader, retriever)\n\n# ## Voil\xc3\xa0! Ask a question!\n\n# You can configure how many candidates the reader and retriever shall return\n# The higher top_k_retriever, the better (but also the slower) your answers.\nprediction = finder.get_answers(question=""Who is the father of Arya Stark?"", top_k_retriever=10, top_k_reader=5)\n# prediction = finder.get_answers(question=""Who created the Dothraki vocabulary?"", top_k_reader=5)\n# prediction = finder.get_answers(question=""Who is the sister of Sansa?"", top_k_reader=5)\n\nprint_answers(prediction, details=""minimal"")\n'"
tutorials/Tutorial4_FAQ_style_QA.py,0,"b'from haystack import Finder\nfrom haystack.database.elasticsearch import ElasticsearchDocumentStore\n\nfrom haystack.retriever.elasticsearch import EmbeddingRetriever\nfrom haystack.utils import print_answers\nimport pandas as pd\nimport requests\nimport logging\nimport subprocess\nimport time\n## ""FAQ-Style QA"": Utilizing existing FAQs for Question Answering\n\n# While *extractive Question Answering* works on pure texts and is therefore more generalizable, there\'s also a common alternative that utilizes existing FAQ data.\n#\n# Pros:\n# - Very fast at inference time\n# - Utilize existing FAQ data\n# - Quite good control over answers\n#\n# Cons:\n# - Generalizability: We can only answer questions that are similar to existing ones in FAQ\n#\n# In some use cases, a combination of extractive QA and FAQ-style can also be an interesting option.\nLAUNCH_ELASTICSEARCH=True\n\nif LAUNCH_ELASTICSEARCH:\n    logging.info(""Starting Elasticsearch ..."")\n    status = subprocess.run(\n        [\'docker run -d -p 9200:9200 -e ""discovery.type=single-node"" elasticsearch:7.6.2\'], shell=True\n    )\n    if status.returncode:\n        raise Exception(""Failed to launch Elasticsearch. If you want to connect to an existing Elasticsearch instance""\n                        ""then set LAUNCH_ELASTICSEARCH in the script to False."")\n    time.sleep(15)\n\n### Init the DocumentStore\n# In contrast to Tutorial 1 (extractive QA), we:\n#\n# * specify the name of our `text_field` in Elasticsearch that we want to return as an answer\n# * specify the name of our `embedding_field` in Elasticsearch where we\'ll store the embedding of our question and that is used later for calculating our similarity to the incoming user question\n# * set `excluded_meta_data=[""question_emb""]` so that we don\'t return the huge embedding vectors in our search results\n\ndocument_store = ElasticsearchDocumentStore(host=""localhost"", username="""", password="""",\n                                            index=""document"",\n                                            text_field=""answer"",\n                                            embedding_field=""question_emb"",\n                                            embedding_dim=768,\n                                            excluded_meta_data=[""question_emb""])\n\n### Create a Retriever using embeddings\n# Instead of retrieving via Elasticsearch\'s plain BM25, we want to use vector similarity of the questions (user question vs. FAQ ones).\n# We can use the `EmbeddingRetriever` for this purpose and specify a model that we use for the embeddings.\n#\nretriever = EmbeddingRetriever(document_store=document_store, embedding_model=""deepset/sentence_bert"", gpu=False)\n\n# Download a csv containing some FAQ data\n# Here: Some question-answer pairs related to COVID-19\ntemp = requests.get(""https://raw.githubusercontent.com/deepset-ai/COVID-QA/master/data/faqs/faq_covidbert.csv"")\nopen(\'small_faq_covid.csv\', \'wb\').write(temp.content)\n\n# Get dataframe with columns ""question"", ""answer"" and some custom metadata\ndf = pd.read_csv(""small_faq_covid.csv"")\n# Minimal cleaning\ndf.fillna(value="""", inplace=True)\ndf[""question""] = df[""question""].apply(lambda x: x.strip())\nprint(df.head())\n\n# Get embeddings for our questions from the FAQs\nquestions = list(df[""question""].values)\ndf[""question_emb""] = retriever.create_embedding(texts=questions)\n\n# Convert Dataframe to list of dicts and index them in our DocumentStore\ndocs_to_index = df.to_dict(orient=""records"")\ndocument_store.write_documents(docs_to_index)\n\n\n# Init reader & and use Finder to get answer (same as in Tutorial 1)\nfinder = Finder(reader=None, retriever=retriever)\nprediction = finder.get_answers_via_similar_questions(question=""How is the virus spreading?"", top_k_retriever=10)\nprint_answers(prediction, details=""all"")\n'"
tutorials/Tutorial5_Evaluation.py,0,"b'from haystack.database.elasticsearch import ElasticsearchDocumentStore\nfrom haystack.indexing.utils import fetch_archive_from_http\nfrom haystack.retriever.elasticsearch import ElasticsearchRetriever\nfrom haystack.reader.farm import FARMReader\nfrom haystack.finder import Finder\nfrom farm.utils import initialize_device_settings\n\nimport logging\nimport subprocess\nimport time\n\nLAUNCH_ELASTICSEARCH = False\ndevice, n_gpu = initialize_device_settings(use_cuda=True)\n\n# Start an Elasticsearch server\n# You can start Elasticsearch on your local machine instance using Docker. If Docker is not readily available in\n# your environment (eg., in Colab notebooks), then you can manually download and execute Elasticsearch from source.\nif LAUNCH_ELASTICSEARCH:\n    logging.info(""Starting Elasticsearch ..."")\n    status = subprocess.run(\n        [\'docker run -d -p 9200:9200 -e ""discovery.type=single-node"" elasticsearch:7.6.2\'], shell=True\n    )\n    if status.returncode:\n        raise Exception(""Failed to launch Elasticsearch. If you want to connect to an existing Elasticsearch instance""\n                        ""then set LAUNCH_ELASTICSEARCH in the script to False."")\n    time.sleep(30)\n\n# Download evaluation data, which is a subset of Natural Questions development set containing 50 documents\ndoc_dir = ""../data/nq""\ns3_url = ""https://s3.eu-central-1.amazonaws.com/deepset.ai-farm-qa/datasets/nq_dev_subset.json.zip""\nfetch_archive_from_http(url=s3_url, output_dir=doc_dir)\n\n# Connect to Elasticsearch\ndocument_store = ElasticsearchDocumentStore(host=""localhost"", username="""", password="""", index=""document"", create_index=False)\n# Add evaluation data to Elasticsearch database\ndocument_store.add_eval_data(""../data/nq/nq_dev_subset.json"")\n\n# Initialize Retriever\nretriever = ElasticsearchRetriever(document_store=document_store)\n\n# Initialize Reader\nreader = FARMReader(""deepset/roberta-base-squad2"")\n\n# Initialize Finder which sticks together Reader and Retriever\nfinder = Finder(reader, retriever)\n\n# Evaluate Retriever on its own\nretriever_eval_results = retriever.eval()\n## Retriever Recall is the proportion of questions for which the correct document containing the answer is\n## among the correct documents\nprint(""Retriever Recall:"", retriever_eval_results[""recall""])\n## Retriever Mean Avg Precision rewards retrievers that give relevant documents a higher rank\nprint(""Retriever Mean Avg Precision:"", retriever_eval_results[""map""])\n\n# Evaluate Reader on its own\nreader_eval_results = reader.eval(document_store=document_store, device=device)\n# Evaluation of Reader can also be done directly on a SQuAD-formatted file without passing the data to Elasticsearch\n#reader_eval_results = reader.eval_on_file(""../data/natural_questions"", ""dev_subset.json"", device=device)\n\n## Reader Top-N-Recall is the proportion of predicted answers that overlap with their corresponding correct answer\nprint(""Reader Top-N-Recall:"", reader_eval_results[""top_n_recall""])\n## Reader Exact Match is the proportion of questions where the predicted answer is exactly the same as the correct answer\nprint(""Reader Exact Match:"", reader_eval_results[""EM""])\n## Reader F1-Score is the average overlap between the predicted answers and the correct answers\nprint(""Reader F1-Score:"", reader_eval_results[""f1""])\n\n\n# Evaluate combination of Reader and Retriever through Finder\nfinder_eval_results = finder.eval()\n\nprint(""\\n___Retriever Metrics in Finder___"")\nprint(""Retriever Recall:"", finder_eval_results[""retriever_recall""])\nprint(""Retriever Mean Avg Precision:"", finder_eval_results[""retriever_map""])\n\n# Reader is only evaluated with those questions, where the correct document is among the retrieved ones\nprint(""\\n___Reader Metrics in Finder___"")\nprint(""Reader Top-1 accuracy:"", finder_eval_results[""reader_top1_accuracy""])\nprint(""Reader Top-1 accuracy (has answer):"", finder_eval_results[""reader_top1_accuracy_has_answer""])\nprint(""Reader Top-k accuracy:"", finder_eval_results[""reader_top_k_accuracy""])\nprint(""Reader Top-k accuracy (has answer):"", finder_eval_results[""reader_topk_accuracy_has_answer""])\nprint(""Reader Top-1 EM:"", finder_eval_results[""reader_top1_em""])\nprint(""Reader Top-1 EM (has answer):"", finder_eval_results[""reader_top1_em_has_answer""])\nprint(""Reader Top-k EM:"", finder_eval_results[""reader_topk_em""])\nprint(""Reader Top-k EM (has answer):"", finder_eval_results[""reader_topk_em_has_answer""])\nprint(""Reader Top-1 F1:"", finder_eval_results[""reader_top1_f1""])\nprint(""Reader Top-1 F1 (has answer):"", finder_eval_results[""reader_top1_f1_has_answer""])\nprint(""Reader Top-k F1:"", finder_eval_results[""reader_topk_f1""])\nprint(""Reader Top-k F1 (has answer):"", finder_eval_results[""reader_topk_f1_has_answer""])\nprint(""Reader Top-1 no-answer accuracy:"", finder_eval_results[""reader_top1_no_answer_accuracy""])\nprint(""Reader Top-k no-answer accuracy:"", finder_eval_results[""reader_topk_no_answer_accuracy""])\n\n# Time measurements\nprint(""\\n___Time Measurements___"")\nprint(""Total retrieve time:"", finder_eval_results[""total_retrieve_time""])\nprint(""Avg retrieve time per question:"", finder_eval_results[""avg_retrieve_time""])\nprint(""Total reader timer:"", finder_eval_results[""total_reader_time""])\nprint(""Avg read time per question:"", finder_eval_results[""avg_reader_time""])\nprint(""Total Finder time:"", finder_eval_results[""total_finder_time""])\n'"
haystack/api/__init__.py,0,b''
haystack/api/application.py,0,"b'import logging\n\nimport uvicorn\nfrom elasticapm.contrib.starlette import make_apm_client, ElasticAPM\nfrom elasticsearch import Elasticsearch\nfrom fastapi import FastAPI, HTTPException\nfrom starlette.middleware.cors import CORSMiddleware\n\nfrom haystack.api.config import DB_HOST, DB_USER, DB_PW, DB_PORT, ES_CONN_SCHEME, APM_SERVER, APM_SERVICE_NAME\nfrom haystack.api.controller.errors.http_error import http_error_handler\nfrom haystack.api.controller.router import router as api_router\n\nlogging.basicConfig(format=""%(asctime)s %(message)s"", datefmt=""%m/%d/%Y %I:%M:%S %p"")\nlogger = logging.getLogger(__name__)\nlogging.getLogger(""elasticsearch"").setLevel(logging.WARNING)\n\nelasticsearch_client = Elasticsearch(\n    hosts=[{""host"": DB_HOST, ""port"": DB_PORT}], http_auth=(DB_USER, DB_PW), scheme=ES_CONN_SCHEME, ca_certs=False, verify_certs=False\n)\n\n\ndef get_application() -> FastAPI:\n    application = FastAPI(title=""Haystack-API"", debug=True, version=""0.1"")\n\n    application.add_middleware(\n        CORSMiddleware, allow_origins=[""*""], allow_credentials=True, allow_methods=[""*""], allow_headers=[""*""],\n    )\n\n    if APM_SERVER:\n        apm_config = {""SERVICE_NAME"": APM_SERVICE_NAME, ""SERVER_URL"": APM_SERVER, ""CAPTURE_BODY"": ""all""}\n        elasticapm = make_apm_client(apm_config)\n        application.add_middleware(ElasticAPM, client=elasticapm)\n\n    application.add_exception_handler(HTTPException, http_error_handler)\n\n    application.include_router(api_router)\n\n    return application\n\n\napp = get_application()\n\nlogger.info(""Open http://127.0.0.1:8000/docs to see Swagger API Documentation."")\nlogger.info(\n    """"""\nOr just try it out directly: curl --request POST --url \'http://127.0.0.1:8000/models/1/doc-qa\' --data \'{""questions"": [""What is the capital of Germany?""]}\'\n""""""\n)\n\nif __name__ == ""__main__"":\n    uvicorn.run(app, host=""0.0.0.0"", port=8000)\n'"
haystack/api/config.py,0,"b'import ast\nimport os\n\n# FastAPI\nPROJECT_NAME = os.getenv(""PROJECT_NAME"", ""FastAPI"")\n\n# Resources / Computation\nUSE_GPU = os.getenv(""USE_GPU"", ""True"").lower() == ""true""\nMAX_PROCESSES = int(os.getenv(""MAX_PROCESSES"", 4))\nBATCHSIZE = int(os.getenv(""BATCHSIZE"", 50))\nCONCURRENT_REQUEST_PER_WORKER = int(os.getenv(""CONCURRENT_REQUEST_PER_WORKER"", 4))\n\n# DB\nDB_HOST = os.getenv(""DB_HOST"", ""localhost"")\nDB_PORT = int(os.getenv(""DB_PORT"", 9200))\nDB_USER = os.getenv(""DB_USER"", """")\nDB_PW = os.getenv(""DB_PW"", """")\nDB_INDEX = os.getenv(""DB_INDEX"", ""document"")\nDB_INDEX_FEEDBACK = os.getenv(""DB_INDEX_FEEDBACK"", ""feedback"")\nES_CONN_SCHEME = os.getenv(""ES_CONN_SCHEME"", ""http"")\nTEXT_FIELD_NAME = os.getenv(""TEXT_FIELD_NAME"", ""text"")\nSEARCH_FIELD_NAME = os.getenv(""SEARCH_FIELD_NAME"", ""text"")\nEMBEDDING_FIELD_NAME = os.getenv(""EMBEDDING_FIELD_NAME"", None)\nEMBEDDING_DIM = os.getenv(""EMBEDDING_DIM"", None)\n\n# Reader\nREADER_MODEL_PATH = os.getenv(""READER_MODEL_PATH"", None)\nCONTEXT_WINDOW_SIZE = int(os.getenv(""CONTEXT_WINDOW_SIZE"", 500))\nDEFAULT_TOP_K_READER = int(os.getenv(""DEFAULT_TOP_K_READER"", 5))\nTOP_K_PER_CANDIDATE = int(os.getenv(""TOP_K_PER_CANDIDATE"", 3))\nNO_ANS_BOOST = int(os.getenv(""NO_ANS_BOOST"", -10))\nDOC_STRIDE = int(os.getenv(""DOC_STRIDE"", 128))\nMAX_SEQ_LEN = int(os.getenv(""MAX_SEQ_LEN"", 256))\n\n# Retriever\nDEFAULT_TOP_K_RETRIEVER = int(os.getenv(""DEFAULT_TOP_K_RETRIEVER"", 10))\nEXCLUDE_META_DATA_FIELDS = os.getenv(""EXCLUDE_META_DATA_FIELDS"", None)\nif EXCLUDE_META_DATA_FIELDS:\n    EXCLUDE_META_DATA_FIELDS = ast.literal_eval(EXCLUDE_META_DATA_FIELDS)\nEMBEDDING_MODEL_PATH = os.getenv(""EMBEDDING_MODEL_PATH"", None)\n\n# Monitoring\nAPM_SERVER = os.getenv(""APM_SERVER"", None)\nAPM_SERVICE_NAME = os.getenv(""APM_SERVICE_NAME"", ""haystack-backend"")\n'"
haystack/api/elasticsearch_client.py,0,"b'from elasticsearch import Elasticsearch\n\nfrom haystack.api.config import DB_HOST, DB_USER, DB_PW, DB_PORT, ES_CONN_SCHEME\n\nelasticsearch_client = Elasticsearch(\n    hosts=[{""host"": DB_HOST, ""port"": DB_PORT}], http_auth=(DB_USER, DB_PW), scheme=ES_CONN_SCHEME, ca_certs=False, verify_certs=False\n)\n'"
haystack/database/__init__.py,0,b''
haystack/database/base.py,0,"b'from abc import abstractmethod\nfrom typing import Any, Optional, Dict\n\nfrom pydantic import BaseModel, Field\n\n\nclass BaseDocumentStore:\n    """"""\n    Base class for implementing Document Stores.\n    """"""\n\n    @abstractmethod\n    def write_documents(self, documents):\n        pass\n\n    @abstractmethod\n    def get_document_by_id(self, id):\n        pass\n\n    @abstractmethod\n    def get_document_ids_by_tags(self, tag):\n        pass\n\n    @abstractmethod\n    def get_document_count(self):\n        pass\n\n    @abstractmethod\n    def query_by_embedding(self, query_emb, top_k=10, candidate_doc_ids=None):\n        pass\n\n\nclass Document(BaseModel):\n    id: str = Field(..., description=""_id field from Elasticsearch"")\n    text: str = Field(..., description=""Text of the document"")\n    external_source_id: Optional[str] = Field(\n        None,\n        description=""id for the source file the document was created from. In the case when a large file is divided ""\n        ""across multiple Elasticsearch documents, this id can be used to reference original source file."",\n    )\n    # name: Optional[str] = Field(None, description=""Title of the document"")\n    question: Optional[str] = Field(None, description=""Question text for FAQs."")\n    query_score: Optional[float] = Field(None, description=""Elasticsearch query score for a retrieved document"")\n    meta: Optional[Dict[str, Any]] = Field(None, description="""")\n    tags: Optional[Dict[str, Any]] = Field(None, description=""Tags that allow filtering of the data"")\n'"
haystack/database/elasticsearch.py,0,"b'import json\nimport logging\nfrom string import Template\nfrom typing import Union\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import bulk, scan\n\nfrom haystack.database.base import BaseDocumentStore, Document\n\nlogger = logging.getLogger(__name__)\n\n\nclass ElasticsearchDocumentStore(BaseDocumentStore):\n    def __init__(\n        self,\n        host: str = ""localhost"",\n        port: int = 9200,\n        username: str = """",\n        password: str = """",\n        index: str = ""document"",\n        search_fields: Union[str,list] = ""text"",\n        text_field: str = ""text"",\n        name_field: str = ""name"",\n        external_source_id_field: str = ""external_source_id"",\n        embedding_field: str = None,\n        embedding_dim: str = None,\n        custom_mapping: dict = None,\n        excluded_meta_data: list = None,\n        scheme: str = ""http"",\n        ca_certs: bool = False,\n        verify_certs: bool = True,\n        create_index: bool = True\n    ):\n        """"""\n        A DocumentStore using Elasticsearch to store and query the documents for our search.\n\n            * Keeps all the logic to store and query documents from Elastic, incl. mapping of fields, adding filters or boosts to your queries, and storing embeddings\n            * You can either use an existing Elasticsearch index or create a new one via haystack\n            * Retrievers operate on top of this DocumentStore to find the relevant documents for a query\n\n        :param host: url of elasticsearch\n        :param port: port of elasticsearch\n        :param username: username\n        :param password: password\n        :param index: Name of index in elasticsearch to use. If not existing yet, we will create one.\n        :param search_fields: Name of fields used by ElasticsearchRetriever to find matches in the docs to our incoming query (using elastic\'s multi_match query), e.g. [""title"", ""full_text""]\n        :param text_field: Name of field that might contain the answer and will therefore be passed to the Reader Model (e.g. ""full_text"").\n                           If no Reader is used (e.g. in FAQ-Style QA) the plain content of this field will just be returned.\n        :param name_field: Name of field that contains the title of the the doc\n        :param external_source_id_field: If you have an external id (= non-elasticsearch) that identifies your documents, you can specify it here.\n        :param embedding_field: Name of field containing an embedding vector (Only needed when using the EmbeddingRetriever on top)\n        :param embedding_dim: Dimensionality of embedding vector (Only needed when using the EmbeddingRetriever on top)\n        :param custom_mapping: If you want to use your own custom mapping for creating a new index in Elasticsearch, you can supply it here as a dictionary.\n        :param excluded_meta_data: Name of fields in Elasticsearch that should not be returned (e.g. [field_one, field_two]).\n                                   Helpful if you have fields with long, irrelevant content that you don\'t want to display in results (e.g. embedding vectors).\n        :param scheme: \'https\' or \'http\', protocol used to connect to your elasticsearch instance\n        :param ca_certs: Root certificates for SSL\n        :param verify_certs: Whether to be strict about ca certificates\n        :param create_index: Whether to try creating a new index (If the index of that name is already existing, we will just continue in any case)\n        """"""\n        self.client = Elasticsearch(hosts=[{""host"": host, ""port"": port}], http_auth=(username, password),\n                                    scheme=scheme, ca_certs=ca_certs, verify_certs=verify_certs)\n\n        # if no custom_mapping is supplied, use the default mapping\n        if not custom_mapping:\n            custom_mapping = {\n                ""mappings"": {\n                    ""properties"": {\n                        name_field: {""type"": ""text""},\n                        text_field: {""type"": ""text""},\n                        external_source_id_field: {""type"": ""text""},\n                    }\n                }\n            }\n            if embedding_field:\n                custom_mapping[""mappings""][""properties""][embedding_field] = {""type"": ""dense_vector"",\n                                                                             ""dims"": embedding_dim}\n        # create an index if not exists\n        if create_index:\n            self.client.indices.create(index=index, ignore=400, body=custom_mapping)\n        self.index = index\n\n        # configure mappings to ES fields that will be used for querying / displaying results\n        if type(search_fields) == str:\n            search_fields = [search_fields]\n\n        #TODO we should implement a more flexible interal mapping here that simplifies the usage of additional,\n        # custom fields (e.g. meta data you want to return)\n        self.search_fields = search_fields\n        self.text_field = text_field\n        self.name_field = name_field\n        self.external_source_id_field = external_source_id_field\n        self.embedding_field = embedding_field\n        self.excluded_meta_data = excluded_meta_data\n\n    def get_document_by_id(self, id: str) -> Document:\n        query = {""query"": {""ids"": {""values"": [id]}}}\n        result = self.client.search(index=self.index, body=query)[""hits""][""hits""]\n\n        document = self._convert_es_hit_to_document(result[0]) if result else None\n        return document\n\n    def get_document_ids_by_tags(self, tags: dict) -> [str]:\n        term_queries = [{""terms"": {key: value}} for key, value in tags.items()]\n        query = {""query"": {""bool"": {""must"": term_queries}}}\n        logger.debug(f""Tag filter query: {query}"")\n        result = self.client.search(index=self.index, body=query, size=10000)[""hits""][""hits""]\n        doc_ids = []\n        for hit in result:\n            doc_ids.append(hit[""_id""])\n        return doc_ids\n\n    def write_documents(self, documents):\n        for doc in documents:\n            doc[""_op_type""] = ""create""\n            doc[""_index""] = self.index\n\n        bulk(self.client, documents, request_timeout=300)\n\n    def get_document_count(self):\n        result = self.client.count()\n        count = result[""count""]\n        return count\n\n    def get_all_documents(self):\n        result = scan(self.client, query={""query"": {""match_all"": {}}}, index=self.index)\n        documents = [self._convert_es_hit_to_document(hit) for hit in result]\n\n        return documents\n\n    def query(\n        self,\n        query: str,\n        filters: dict = None,\n        top_k: int = 10,\n        custom_query: str = None,\n        index: str = None,\n    ) -> [Document]:\n\n        if index is None:\n            index = self.index\n\n        if custom_query:  # substitute placeholder for question and filters for the custom_query template string\n            template = Template(custom_query)\n\n            substitutions = {""question"": query}  # replace all ""${question}"" placeholder(s) with query\n            # replace all filter values placeholders with a list of strings(in JSON format) for each filter\n            if filters:\n                for key, values in filters.items():\n                    values_str = json.dumps(values)\n                    substitutions[key] = values_str\n            custom_query_json = template.substitute(**substitutions)\n            body = json.loads(custom_query_json)\n        else:\n            body = {\n                ""size"": top_k,\n                ""query"": {\n                    ""bool"": {\n                        ""should"": [{""multi_match"": {""query"": query, ""type"": ""most_fields"", ""fields"": self.search_fields}}]\n                    }\n                },\n            }\n\n            if filters:\n                filter_clause = []\n                for key, values in filters.items():\n                    filter_clause.append(\n                        {\n                            ""terms"": {key: values}\n                        }\n                    )\n                body[""query""][""bool""][""filter""] = filter_clause\n\n        if self.excluded_meta_data:\n            body[""_source""] = {""excludes"": self.excluded_meta_data}\n\n        logger.debug(f""Retriever query: {body}"")\n        result = self.client.search(index=index, body=body)[""hits""][""hits""]\n\n        documents = [self._convert_es_hit_to_document(hit) for hit in result]\n        return documents\n\n    def query_by_embedding(self, query_emb, top_k=10, candidate_doc_ids=None) -> [Document]:\n        if not self.embedding_field:\n            raise RuntimeError(""Please specify arg `embedding_field` in ElasticsearchDocumentStore()"")\n        else:\n            # +1 in cosine similarity to avoid negative numbers\n            body= {\n                ""size"": top_k,\n                ""query"": {\n                    ""script_score"": {\n                        ""query"": {""match_all"": {}},\n                        ""script"": {\n                            ""source"": f""cosineSimilarity(params.query_vector,doc[\'{self.embedding_field}\']) + 1.0"",\n                            ""params"": {\n                                ""query_vector"": query_emb\n                            }\n                        }\n                    }\n                }\n            }\n\n            if candidate_doc_ids:\n                body[""query""][""script_score""][""query""] = {\n                    ""bool"": {\n                        ""should"": [{""match_all"": {}}],\n                        ""filter"": [{""terms"": {""_id"": candidate_doc_ids}}]\n                }}\n\n            if self.excluded_meta_data:\n                body[""_source""] = {""excludes"": self.excluded_meta_data}\n\n            logger.debug(f""Retriever query: {body}"")\n            result = self.client.search(index=self.index, body=body)[""hits""][""hits""]\n\n            documents = [self._convert_es_hit_to_document(hit, score_adjustment=-1) for hit in result]\n            return documents\n\n    def _convert_es_hit_to_document(self, hit, score_adjustment=0) -> Document:\n        # We put all additional data of the doc into meta_data and return it in the API\n        meta_data = {k:v for k,v in hit[""_source""].items() if k not in (self.text_field, self.external_source_id_field)}\n        meta_data[""name""] = meta_data.pop(self.name_field, None)\n\n        document = Document(\n            id=hit[""_id""],\n            text=hit[""_source""][self.text_field],\n            external_source_id=hit[""_source""].get(self.external_source_id_field),\n            meta=meta_data,\n            query_score=hit[""_score""] + score_adjustment if hit[""_score""] else None,\n        )\n        return document\n\n    def add_eval_data(self, filename: str, doc_index: str = ""eval_document"", label_index: str = ""feedback""):\n        """"""\n        Adds a SQuAD-formatted file to the DocumentStore in order to be able to perform evaluation on it.\n\n        :param filename: Name of the file containing evaluation data\n        :type filename: str\n        :param doc_index: Elasticsearch index where evaluation documents should be stored\n        :type doc_index: str\n        :param label_index: Elasticsearch index where labeled questions should be stored\n        :type label_index: str\n        """"""\n\n        eval_docs_to_index = []\n        questions_to_index = []\n\n        with open(filename, ""r"") as file:\n            data = json.load(file)\n            for document in data[""data""]:\n                for paragraph in document[""paragraphs""]:\n                    doc_to_index= {}\n                    id = hash(paragraph[""context""])\n                    for fieldname, value in paragraph.items():\n                        # write docs to doc_index\n                        if fieldname == ""context"":\n                            doc_to_index[self.text_field] = value\n                            doc_to_index[""doc_id""] = str(id)\n                            doc_to_index[""_op_type""] = ""create""\n                            doc_to_index[""_index""] = doc_index\n                        # write questions to label_index\n                        elif fieldname == ""qas"":\n                            for qa in value:\n                                question_to_index = {\n                                    ""question"": qa[""question""],\n                                    ""answers"": qa[""answers""],\n                                    ""doc_id"": str(id),\n                                    ""origin"": ""gold_label"",\n                                    ""index_name"": doc_index,\n                                    ""_op_type"": ""create"",\n                                    ""_index"": label_index\n                                }\n                                questions_to_index.append(question_to_index)\n                        # additional fields for docs\n                        else:\n                            doc_to_index[fieldname] = value\n\n                    for key, value in document.items():\n                        if key == ""title"":\n                            doc_to_index[self.name_field] = value\n                        elif key != ""paragraphs"":\n                            doc_to_index[key] = value\n\n                    eval_docs_to_index.append(doc_to_index)\n\n        bulk(self.client, eval_docs_to_index)\n        bulk(self.client, questions_to_index)\n\n    def get_all_documents_in_index(self, index, filters=None):\n        body = {\n            ""query"": {\n                ""bool"": {\n                    ""must"": {\n                        ""match_all"" : {}\n                    }\n                }\n            }\n        }\n\n        if filters:\n           body[""query""][""bool""][""filter""] = {""term"": filters}\n        result = scan(self.client, query=body, index=index)\n\n        return result'"
haystack/database/memory.py,0,"b'from haystack.database.base import BaseDocumentStore, Document\n\n\nclass InMemoryDocumentStore(BaseDocumentStore):\n    """"""\n        In-memory document store\n    """"""\n\n    def __init__(self):\n        self.docs = {}\n        self.doc_tags = {}\n\n    def write_documents(self, documents):\n        import hashlib\n\n        if documents is None:\n            return\n\n        for document in documents:\n            name = document.get(""name"", None)\n            text = document.get(""text"", None)\n\n            if name is None or text is None:\n                continue\n\n            signature = name + text\n\n            hash = hashlib.md5(signature.encode(""utf-8"")).hexdigest()\n\n            self.docs[hash] = document\n\n            tags = document.get(\'tags\', [])\n\n            self._map_tags_to_ids(hash, tags)\n\n    def _map_tags_to_ids(self, hash, tags):\n        if isinstance(tags, list):\n            for tag in tags:\n                if isinstance(tag, dict):\n                    tag_keys = tag.keys()\n                    for tag_key in tag_keys:\n                        tag_values = tag.get(tag_key, [])\n                        if tag_values:\n                            for tag_value in tag_values:\n                                comp_key = str((tag_key, tag_value))\n                                if comp_key in self.doc_tags:\n                                    self.doc_tags[comp_key].append(hash)\n                                else:\n                                    self.doc_tags[comp_key] = [hash]\n\n    def get_document_by_id(self, id):\n        return self.docs[id]\n\n    def _convert_memory_hit_to_document(self, hit, doc_id=None) -> Document:\n        document = Document(\n            id=doc_id,\n            text=hit[0].get(\'text\', None),\n            meta=hit[0].get(\'meta\', {}),\n            query_score=hit[1],\n        )\n        return document\n\n    def query_by_embedding(self, query_emb, top_k=10, candidate_doc_ids=None) -> [Document]:\n        from haystack.api import config\n        from numpy import dot\n        from numpy.linalg import norm\n\n        embedding_field_name = config.EMBEDDING_FIELD_NAME\n        if embedding_field_name is None:\n            return []\n\n        if query_emb is None:\n            return []\n\n        candidate_docs = [self._convert_memory_hit_to_document(\n            (doc, dot(query_emb, doc[embedding_field_name]) / (norm(query_emb) * norm(doc[embedding_field_name]))), doc_id=idx) for idx, doc in self.docs.items()\n        ]\n\n        return sorted(candidate_docs, key=lambda x: x.query_score, reverse=True)[0:top_k]\n\n    def get_document_ids_by_tags(self, tags):\n        """"""\n        The format for the dict is {""tag-1"": ""value-1"", ""tag-2"": ""value-2"" ...}\n        The format for the dict is {""tag-1"": [""value-1"",""value-2""], ""tag-2"": [""value-3]"" ...}\n        """"""\n        if not isinstance(tags, list):\n            tags = [tags]\n        result = self._find_ids_by_tags(tags)\n        return result\n\n    def _find_ids_by_tags(self, tags):\n        result = []\n        for tag in tags:\n            tag_keys = tag.keys()\n            for tag_key in tag_keys:\n                tag_values = tag.get(tag_key, None)\n                if tag_values:\n                    for tag_value in tag_values:\n                        comp_key = str((tag_key, tag_value))\n                        doc_ids = self.doc_tags.get(comp_key, [])\n                        for doc_id in doc_ids:\n                            result.append(self.docs.get(doc_id))\n        return result\n\n    def get_document_count(self):\n        return len(self.docs.items())\n\n    def get_all_documents(self):\n        return [Document(id=item[0], text=item[1][\'text\'], name=item[1][\'name\'], meta=item[1].get(\'meta\', {})) for item in self.docs.items()]\n'"
haystack/database/sql.py,0,"b'import json\nfrom sqlalchemy import create_engine, Column, Integer, String, DateTime, func, ForeignKey, PickleType\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship, sessionmaker\n\nfrom haystack.database.base import BaseDocumentStore, Document as DocumentSchema\n\nBase = declarative_base()\n\n\nclass ORMBase(Base):\n    __abstract__ = True\n\n    id = Column(Integer, primary_key=True)\n    created = Column(DateTime, server_default=func.now())\n    updated = Column(DateTime, server_default=func.now(), server_onupdate=func.now())\n\n\nclass Document(ORMBase):\n    __tablename__ = ""document""\n\n    name = Column(String)\n    text = Column(String)\n    meta_data = Column(PickleType)\n\n    tags = relationship(""Tag"", secondary=""document_tag"", backref=""Document"")\n\n\nclass Tag(ORMBase):\n    __tablename__ = ""tag""\n\n    name = Column(String)\n    value = Column(String)\n\n    documents = relationship(""Document"", secondary=""document_tag"", backref=""Tag"")\n\n\nclass DocumentTag(ORMBase):\n    __tablename__ = ""document_tag""\n\n    document_id = Column(Integer, ForeignKey(""document.id""), nullable=False)\n    tag_id = Column(Integer, ForeignKey(""tag.id""), nullable=False)\n\n\nclass SQLDocumentStore(BaseDocumentStore):\n    def __init__(self, url=""sqlite://""):\n        engine = create_engine(url)\n        ORMBase.metadata.create_all(engine)\n        Session = sessionmaker(bind=engine)\n        self.session = Session()\n\n    def get_document_by_id(self, id):\n        document_row = self.session.query(Document).get(id)\n        document = self._convert_sql_row_to_document(document_row)\n\n        return document\n\n    def get_all_documents(self):\n        document_rows = self.session.query(Document).all()\n        documents = []\n        for row in document_rows:\n            documents.append(self._convert_sql_row_to_document(row))\n\n        return documents\n\n    def get_document_ids_by_tags(self, tags):\n        """"""\n        Get list of document ids that have tags from the given list of tags.\n\n        :param tags: limit scope to documents having the given tags and their corresponding values.\n                     The format for the dict is {""tag-1"": ""value-1"", ""tag-2"": ""value-2"" ...}\n        """"""\n        if not tags:\n            raise Exception(""No tag supplied for filtering the documents"")\n\n        query = """"""\n                  SELECT id FROM document WHERE id in (\n                      SELECT dt.document_id\n                      FROM document_tag dt JOIN\n                          tag t\n                          ON t.id = dt.tag_id\n                      GROUP BY dt.document_id\n              """"""\n        tag_filters = []\n        for tag in tags:\n            tag_filters.append(f""SUM(CASE WHEN t.value=\'{tag}\' THEN 1 ELSE 0 END) > 0"")\n\n        final_query = f""{query} HAVING {\' AND \'.join(tag_filters)});""\n        query_results = self.session.execute(final_query)\n\n        doc_ids = [row[0] for row in query_results]\n        return doc_ids\n\n    def write_documents(self, documents):\n        for doc in documents:\n            row = Document(name=doc[""name""], text=doc[""text""], meta_data=doc.get(""meta"", {}))\n            self.session.add(row)\n        self.session.commit()\n\n    def get_document_count(self):\n        return self.session.query(Document).count()\n\n    def _convert_sql_row_to_document(self, row) -> Document:\n        document = DocumentSchema(\n            id=row.id,\n            text=row.text,\n            meta=row.meta_data,\n            tags=row.tags\n        )\n        return document\n'"
haystack/indexing/__init__.py,0,b''
haystack/indexing/cleaning.py,0,"b'import re\n\n\ndef clean_wiki_text(text):\n    # get rid of multiple new lines\n    while ""\\n\\n"" in text:\n        text = text.replace(""\\n\\n"", ""\\n"")\n\n    # remove extremely short lines\n    text = text.split(""\\n"")\n    cleaned = []\n    for l in text:\n        if len(l) > 30:\n            cleaned.append(l)\n        elif l[:2] == ""=="" and l[-2:] == ""=="":\n            cleaned.append(l)\n    text = ""\\n"".join(cleaned)\n\n    # add paragraphs (identified by wiki section title which is always in format ""==Some Title=="")\n    text = text.replace(""\\n=="", ""\\n\\n\\n=="")\n\n    # remove empty paragrahps\n    text = re.sub(r""(==.*==\\n\\n\\n)"", """", text)\n\n    return text\n'"
haystack/indexing/utils.py,0,"b'from pathlib import Path\nimport logging\nfrom farm.data_handler.utils import http_get\nimport tempfile\nimport tarfile\nimport zipfile\nfrom typing import Callable\nfrom haystack.indexing.file_converters.pdftotext import PDFToTextConverter\n\nlogger = logging.getLogger(__name__)\n\n\ndef convert_files_to_dicts(dir_path: str, clean_func: Callable = None, split_paragraphs: bool = False) -> [dict]:\n    """"""\n    Convert all files(.txt, .pdf) in the sub-directories of the given path to Python dicts that can be written to a\n    Document Store.\n\n    :param dir_path: path for the documents to be written to the database\n    :param clean_func: a custom cleaning function that gets applied to each doc (input: str, output:str)\n    :param split_paragraphs: split text in paragraphs.\n\n    :return: None\n    """"""\n\n    file_paths = [p for p in Path(dir_path).glob(""**/*"")]\n    if "".pdf"" in [p.suffix.lower() for p in file_paths]:\n        pdf_converter = PDFToTextConverter()\n    else:\n        pdf_converter = None\n\n    documents = []\n    for path in file_paths:\n        if path.suffix.lower() == "".txt"":\n            with open(path) as doc:\n                text = doc.read()\n        elif path.suffix.lower() == "".pdf"":\n            pages = pdf_converter.extract_pages(path)\n            text = ""\\n"".join(pages)\n        else:\n            raise Exception(f""Indexing of {path.suffix} files is not currently supported."")\n\n        if clean_func:\n            text = clean_func(text)\n\n        if split_paragraphs:\n            for para in text.split(""\\n\\n""):\n                if not para.strip():  # skip empty paragraphs\n                    continue\n                documents.append({""name"": path.name, ""text"": para})\n        else:\n            documents.append({""name"": path.name, ""text"": text})\n\n    return documents\n\n\ndef fetch_archive_from_http(url, output_dir, proxies=None):\n    """"""\n    Fetch an archive (zip or tar.gz) from a url via http and extract content to an output directory.\n\n    :param url: http address\n    :type url: str\n    :param output_dir: local path\n    :type output_dir: str\n    :param proxies: proxies details as required by requests library\n    :type proxies: dict\n    :return: bool if anything got fetched\n    """"""\n    # verify & prepare local directory\n    path = Path(output_dir)\n    if not path.exists():\n        path.mkdir(parents=True)\n\n    is_not_empty = len(list(Path(path).rglob(""*""))) > 0\n    if is_not_empty:\n        logger.info(\n            f""Found data stored in `{output_dir}`. Delete this first if you really want to fetch new data.""\n        )\n        return False\n    else:\n        logger.info(f""Fetching from {url} to `{output_dir}`"")\n\n        # download & extract\n        with tempfile.NamedTemporaryFile() as temp_file:\n            http_get(url, temp_file, proxies=proxies)\n            temp_file.flush()\n            temp_file.seek(0)  # making tempfile accessible\n            # extract\n            if url[-4:] == "".zip"":\n                archive = zipfile.ZipFile(temp_file.name)\n                archive.extractall(output_dir)\n            elif url[-7:] == "".tar.gz"":\n                archive = tarfile.open(temp_file.name)\n                archive.extractall(output_dir)\n            # temp_file gets deleted here\n        return True\n\n'"
haystack/reader/__init__.py,0,b''
haystack/reader/farm.py,0,"b'import logging\nfrom pathlib import Path\n\nimport numpy as np\nfrom farm.data_handler.data_silo import DataSilo\nfrom farm.data_handler.processor import SquadProcessor\nfrom farm.data_handler.dataloader import NamedDataLoader\nfrom farm.infer import Inferencer\nfrom farm.modeling.optimization import initialize_optimizer\nfrom farm.train import Trainer\nfrom farm.eval import Evaluator\nfrom farm.utils import set_all_seeds, initialize_device_settings\nfrom scipy.special import expit\n\nfrom haystack.database.base import Document\nfrom haystack.database.elasticsearch import ElasticsearchDocumentStore\n\nlogger = logging.getLogger(__name__)\n\n\nclass FARMReader:\n    """"""\n    Transformer based model for extractive Question Answering using the FARM framework (https://github.com/deepset-ai/FARM).\n    While the underlying model can vary (BERT, Roberta, DistilBERT ...) the interface remains the same.\n\n    With a FARMReader, you can:\n     - directly get predictions via predict()\n     - fine-tune the model on QA data via train()\n    """"""\n\n    def __init__(\n        self,\n        model_name_or_path,\n        context_window_size=150,\n        batch_size=50,\n        use_gpu=True,\n        no_ans_boost=None,\n        top_k_per_candidate=3,\n        top_k_per_sample=1,\n        num_processes=None,\n        max_seq_len=256,\n        doc_stride=128\n    ):\n\n        """"""\n        :param model_name_or_path: directory of a saved model or the name of a public model:\n                                   - \'bert-base-cased\'\n                                   - \'deepset/bert-base-cased-squad2\'\n                                   - \'deepset/bert-base-cased-squad2\'\n                                   - \'distilbert-base-uncased-distilled-squad\'\n                                   ....\n                                   See https://huggingface.co/models for full list of available models.\n        :param context_window_size: The size, in characters, of the window around the answer span that is used when displaying the context around the answer.\n        :param batch_size: Number of samples the model receives in one batch for inference\n                           Memory consumption is much lower in inference mode. Recommendation: increase the batch size to a value so only a single batch is used.\n        :param use_gpu: Whether to use GPU (if available)\n        :param no_ans_boost: How much the no_answer logit is boosted/increased.\n                             Possible values: None (default) = disable returning ""no answer"" predictions\n                                              Negative = lower chance of ""no answer"" being predicted\n                                              Positive = increase chance of ""no answer""\n        :param top_k_per_candidate: How many answers to extract for each candidate doc that is coming from the retriever (might be a long text).\n                                                   Note: - This is not the number of ""final answers"" you will receive\n                                                   (see `top_k` in FARMReader.predict() or Finder.get_answers() for that)\n                                                 - FARM includes no_answer in the sorted list of predictions\n        :param top_k_per_sample: How many answers to extract from each small text passage that the model can\n                                  process at once (one ""candidate doc"" is usually split into many smaller ""passages"").\n                                  You usually want a very small value here, as it slows down inference and you\n                                  don\'t gain much of quality by having multiple answers from one passage.\n                                               Note: - This is not the number of ""final answers"" you will receive\n                                               (see `top_k` in FARMReader.predict() or Finder.get_answers() for that)\n                                             - FARM includes no_answer in the sorted list of predictions\n        :param num_processes: the number of processes for `multiprocessing.Pool`. Set to value of 0 to disable\n                              multiprocessing. Set to None to let Inferencer determine optimum number. If you\n                              want to debug the Language Model, you might need to disable multiprocessing!\n        :type num_processes: int\n        :param max_seq_len: max sequence length of one input text for the model\n        :param doc_stride: length of striding window for splitting long texts (used if len(text) > max_seq_len)\n\n        """"""\n\n        if no_ans_boost is None:\n            no_ans_boost = 0\n            self.return_no_answers = False\n        else:\n            self.return_no_answers = True\n        self.top_k_per_candidate = top_k_per_candidate\n        self.inferencer = Inferencer.load(model_name_or_path, batch_size=batch_size, gpu=use_gpu,\n                                          task_type=""question_answering"", max_seq_len=max_seq_len,\n                                          doc_stride=doc_stride, num_processes=num_processes)\n        self.inferencer.model.prediction_heads[0].context_window_size = context_window_size\n        self.inferencer.model.prediction_heads[0].no_ans_boost = no_ans_boost\n        self.inferencer.model.prediction_heads[0].n_best = top_k_per_candidate + 1 # including possible no_answer\n        try:\n            self.inferencer.model.prediction_heads[0].n_best_per_sample = top_k_per_sample\n        except:\n            logger.warning(""Could not set `top_k_per_sample` in FARM. Please update FARM version."")\n        self.max_seq_len = max_seq_len\n        self.use_gpu = use_gpu\n\n    def train(self, data_dir, train_filename, dev_filename=None, test_file_name=None,\n              use_gpu=None, batch_size=10, n_epochs=2, learning_rate=1e-5,\n              max_seq_len=None, warmup_proportion=0.2, dev_split=0.1, evaluate_every=300, save_dir=None):\n        """"""\n        Fine-tune a model on a QA dataset. Options:\n        - Take a plain language model (e.g. `bert-base-cased`) and train it for QA (e.g. on SQuAD data)\n        - Take a QA model (e.g. `deepset/bert-base-cased-squad2`) and fine-tune it for your domain (e.g. using your labels collected via the haystack annotation tool)\n\n        :param data_dir: Path to directory containing your training data in SQuAD style\n        :param train_filename: filename of training data\n        :param dev_filename: filename of dev / eval data\n        :param test_file_name: filename of test data\n        :param dev_split: Instead of specifying a dev_filename you can also specify a ratio (e.g. 0.1) here\n                          that get\'s split off from training data for eval.\n        :param use_gpu: Whether to use GPU (if available)\n        :param batch_size: Number of samples the model receives in one batch for training\n        :param n_epochs: number of iterations on the whole training data set\n        :param learning_rate: learning rate of the optimizer\n        :param max_seq_len: maximum text length (in tokens). Everything longer gets cut down.\n        :param warmup_proportion: Proportion of training steps until maximum learning rate is reached.\n                                  Until that point LR is increasing linearly. After that it\'s decreasing again linearly.\n                                  Options for different schedules are available in FARM.\n        :param evaluate_every: Evaluate the model every X steps on the hold-out eval dataset\n        :param save_dir: Path to store the final model\n        :return: None\n        """"""\n\n\n        if dev_filename:\n            dev_split = None\n\n        set_all_seeds(seed=42)\n\n        # For these variables, by default, we use the value set when initializing the FARMReader.\n        # These can also be manually set when train() is called if you want a different value at train vs inference\n        if use_gpu is None:\n            use_gpu = self.use_gpu\n        if max_seq_len is None:\n            max_seq_len = self.max_seq_len\n\n        device, n_gpu = initialize_device_settings(use_cuda=use_gpu)\n\n        if not save_dir:\n            save_dir = f""../../saved_models/{self.inferencer.model.language_model.name}""\n        save_dir = Path(save_dir)\n\n        # 1. Create a DataProcessor that handles all the conversion from raw text into a pytorch Dataset\n        label_list = [""start_token"", ""end_token""]\n        metric = ""squad""\n        processor = SquadProcessor(\n            tokenizer=self.inferencer.processor.tokenizer,\n            max_seq_len=max_seq_len,\n            label_list=label_list,\n            metric=metric,\n            train_filename=train_filename,\n            dev_filename=dev_filename,\n            dev_split=dev_split,\n            test_filename=test_file_name,\n            data_dir=Path(data_dir),\n        )\n\n        # 2. Create a DataSilo that loads several datasets (train/dev/test), provides DataLoaders for them\n        # and calculates a few descriptive statistics of our datasets\n        data_silo = DataSilo(processor=processor, batch_size=batch_size, distributed=False)\n\n        # 3. Create an optimizer and pass the already initialized model\n        model, optimizer, lr_schedule = initialize_optimizer(\n            model=self.inferencer.model,\n            learning_rate=learning_rate,\n            schedule_opts={""name"": ""LinearWarmup"", ""warmup_proportion"": warmup_proportion},\n            n_batches=len(data_silo.loaders[""train""]),\n            n_epochs=n_epochs,\n            device=device\n        )\n        # 4. Feed everything to the Trainer, which keeps care of growing our model and evaluates it from time to time\n        trainer = Trainer(\n            model=model,\n            optimizer=optimizer,\n            data_silo=data_silo,\n            epochs=n_epochs,\n            n_gpu=n_gpu,\n            lr_schedule=lr_schedule,\n            evaluate_every=evaluate_every,\n            device=device,\n        )\n        # 5. Let it grow!\n        self.inferencer.model = trainer.train()\n        self.save(save_dir)\n\n    def save(self, directory):\n        logger.info(f""Saving reader model to {directory}"")\n        self.inferencer.model.save(directory)\n        self.inferencer.processor.save(directory)\n\n    def predict(self, question: str, documents: [Document], top_k: int = None):\n        """"""\n        Use loaded QA model to find answers for a question in the supplied list of Document.\n\n        Returns dictionaries containing answers sorted by (desc.) probability\n        Example:\n        {\'question\': \'Who is the father of Arya Stark?\',\n        \'answers\': [\n                     {\'answer\': \'Eddard,\',\n                     \'context\': "" She travels with her father, Eddard, to King\'s Landing when he is "",\n                     \'offset_answer_start\': 147,\n                     \'offset_answer_end\': 154,\n                     \'probability\': 0.9787139466668613,\n                     \'score\': None,\n                     \'document_id\': \'1337\'\n                     },\n                    ...\n                   ]\n        }\n\n        :param question: question string\n        :param documents: list of Document in which to search for the answer\n        :param top_k: the maximum number of answers to return\n        :return: dict containing question and answers\n        """"""\n\n        # convert input to FARM format\n        input_dicts = []\n        for doc in documents:\n            cur = {\n                ""text"": doc.text,\n                ""questions"": [question],\n                ""document_id"": doc.id\n            }\n            input_dicts.append(cur)\n\n        # get answers from QA model\n        predictions = self.inferencer.inference_from_dicts(\n            dicts=input_dicts, rest_api_schema=True, multiprocessing_chunksize=1\n        )\n        # assemble answers from all the different documents & format them.\n        # For the ""no answer"" option, we collect all no_ans_gaps and decide how likely\n        # a no answer is based on all no_ans_gaps values across all documents\n        answers = []\n        no_ans_gaps = []\n        best_score_answer = 0\n        for pred in predictions:\n            answers_per_document = []\n            no_ans_gaps.append(pred[""predictions""][0][""no_ans_gap""])\n            for a in pred[""predictions""][0][""answers""]:\n                # skip ""no answers"" here\n                if a[""answer""]:\n                    cur = {""answer"": a[""answer""],\n                           ""score"": a[""score""],\n                           ""probability"": float(expit(np.asarray([a[""score""]]) / 8)), #just a pseudo prob for now\n                           ""context"": a[""context""],\n                           ""offset_start"": a[""offset_answer_start""] - a[""offset_context_start""],\n                           ""offset_end"": a[""offset_answer_end""] - a[""offset_context_start""],\n                           ""offset_start_in_doc"": a[""offset_answer_start""],\n                           ""offset_end_in_doc"": a[""offset_answer_end""],\n                           ""document_id"": a[""document_id""]}\n                    answers_per_document.append(cur)\n\n                    if a[""score""] > best_score_answer:\n                        best_score_answer = a[""score""]\n            # only take n best candidates. Answers coming back from FARM are sorted with decreasing relevance.\n            answers += answers_per_document[:self.top_k_per_candidate]\n\n        # Calculate the score for predicting ""no answer"", relative to our best positive answer score\n        no_ans_prediction, max_no_ans_gap = self._calc_no_answer(no_ans_gaps,best_score_answer)\n        if self.return_no_answers:\n            answers.append(no_ans_prediction)\n\n        # sort answers by their `probability` and select top-k\n        answers = sorted(\n            answers, key=lambda k: k[""probability""], reverse=True\n        )\n        answers = answers[:top_k]\n        result = {""question"": question,\n                  ""no_ans_gap"": max_no_ans_gap,\n                  ""answers"": answers}\n\n        return result\n\n    def eval_on_file(self, data_dir: str, test_filename: str, device: str):\n        """"""\n        Performs evaluation on a SQuAD-formatted file.\n\n        Returns a dict containing the following metrics:\n            - ""EM"": exact match score\n            - ""f1"": F1-Score\n            - ""top_n_recall"": Proportion of predicted answers that overlap with correct answer\n\n        :param data_dir: The directory in which the test set can be found\n        :type data_dir: Path or str\n        :param test_filename: The name of the file containing the test data in SQuAD format.\n        :type test_filename: str\n        :param device: The device on which the tensors should be processed. Choose from ""cpu"" and ""cuda"".\n        :type device: str\n        """"""\n        eval_processor = SquadProcessor(\n            tokenizer=self.inferencer.processor.tokenizer,\n            max_seq_len=self.inferencer.processor.max_seq_len,\n            label_list=self.inferencer.processor.tasks[""question_answering""][""label_list""],\n            metric=self.inferencer.processor.tasks[""question_answering""][""metric""],\n            train_filename=None,\n            dev_filename=None,\n            dev_split=0,\n            test_filename=test_filename,\n            data_dir=Path(data_dir),\n        )\n\n        data_silo = DataSilo(processor=eval_processor, batch_size=self.inferencer.batch_size, distributed=False)\n        data_loader = data_silo.get_data_loader(""test"")\n\n        evaluator = Evaluator(data_loader=data_loader, tasks=eval_processor.tasks, device=device)\n\n        eval_results = evaluator.eval(self.inferencer.model)\n        results = {\n            ""EM"": eval_results[0][""EM""],\n            ""f1"": eval_results[0][""f1""],\n            ""top_n_recall"": eval_results[0][""top_n_recall""]\n        }\n        return results\n\n    def eval(self, document_store: ElasticsearchDocumentStore, device: str, label_index: str = ""feedback"",\n             doc_index: str = ""eval_document"", label_origin: str = ""gold_label""):\n        """"""\n        Performs evaluation on evaluation documents in Elasticsearch DocumentStore.\n\n        Returns a dict containing the following metrics:\n            - ""EM"": Proportion of exact matches of predicted answers with their corresponding correct answers\n            - ""f1"": Average overlap between predicted answers and their corresponding correct answers\n            - ""top_n_recall"": Proportion of predicted answers that overlap with correct answer\n\n        :param document_store: The ElasticsearchDocumentStore containing the evaluation documents\n        :type document_store: ElasticsearchDocumentStore\n        :param device: The device on which the tensors should be processed. Choose from ""cpu"" and ""cuda"".\n        :type device: str\n        :param label_index: Elasticsearch index where labeled questions are stored\n        :type label_index: str\n        :param doc_index: Elasticsearch index where documents that are used for evaluation are stored\n        :type doc_index: str\n        """"""\n\n        # extract all questions for evaluation\n        filter = {""origin"": label_origin}\n        questions = document_store.get_all_documents_in_index(index=label_index, filters=filter)\n\n        # mapping from doc_id to questions\n        doc_questions_dict = {}\n        id = 0\n        for question in questions:\n            doc_id = question[""_source""][""doc_id""]\n            if doc_id not in doc_questions_dict:\n                doc_questions_dict[doc_id] = [{\n                    ""id"": id,\n                    ""question"" : question[""_source""][""question""],\n                    ""answers"" : question[""_source""][""answers""],\n                    ""is_impossible"" : False if question[""_source""][""answers""] else True\n                }]\n            else:\n                doc_questions_dict[doc_id].append({\n                    ""id"": id,\n                    ""question"" : question[""_source""][""question""],\n                    ""answers"" : question[""_source""][""answers""],\n                    ""is_impossible"" : False if question[""_source""][""answers""] else True\n                })\n            id += 1\n\n        # extract eval documents and convert data back to SQuAD-like format\n        documents = document_store.get_all_documents_in_index(index=doc_index)\n        dicts = []\n        for document in documents:\n            doc_id = document[""_source""][""doc_id""]\n            text = document[""_source""][""text""]\n            questions = doc_questions_dict[doc_id]\n            dicts.append({""qas"" : questions, ""context"" : text})\n\n        # Create DataLoader that can be passed to the Evaluator\n        indices = range(len(dicts))\n        dataset, tensor_names = self.inferencer.processor.dataset_from_dicts(dicts, indices=indices)\n        data_loader = NamedDataLoader(dataset=dataset, batch_size=self.inferencer.batch_size, tensor_names=tensor_names)\n\n        evaluator = Evaluator(data_loader=data_loader, tasks=self.inferencer.processor.tasks, device=device)\n\n        eval_results = evaluator.eval(self.inferencer.model)\n        results = {\n            ""EM"": eval_results[0][""EM""],\n            ""f1"": eval_results[0][""f1""],\n            ""top_n_recall"": eval_results[0][""top_n_recall""]\n        }\n        return results\n\n    @staticmethod\n    def _calc_no_answer(no_ans_gaps,best_score_answer):\n        # ""no answer"" scores and positive answers scores are difficult to compare, because\n        # + a positive answer score is related to one specific document\n        # - a ""no answer"" score is related to all input documents\n        # Thus we compute the ""no answer"" score relative to the best possible answer and adjust it by\n        # the most significant difference between scores.\n        # Most significant difference: a model switching from predicting an answer to ""no answer"" (or vice versa).\n        # No_ans_gap coming from FARM mean how much no_ans_boost should change to switch predictions\n        no_ans_gaps = np.array(no_ans_gaps)\n        max_no_ans_gap = np.max(no_ans_gaps)\n        if (np.sum(no_ans_gaps < 0) == len(no_ans_gaps)):  # all passages ""no answer"" as top score\n            no_ans_score = best_score_answer - max_no_ans_gap  # max_no_ans_gap is negative, so it increases best pos score\n        else:  # case: at least one passage predicts an answer (positive no_ans_gap)\n            no_ans_score = best_score_answer - max_no_ans_gap\n\n        no_ans_prediction = {""answer"": None,\n               ""score"": no_ans_score,\n               ""probability"": float(expit(np.asarray(no_ans_score) / 8)),  # just a pseudo prob for now\n               ""context"": None,\n               ""offset_start"": 0,\n               ""offset_end"": 0,\n               ""document_id"": None}\n        return no_ans_prediction, max_no_ans_gap\n\n    def predict_on_texts(self, question: str, texts: [str], top_k=None):\n        documents = []\n        for i, text in enumerate(texts):\n            documents.append(\n                Document(\n                    id=i,\n                    text=text\n                )\n            )\n        predictions = self.predict(question, documents, top_k)\n        return predictions\n'"
haystack/reader/transformers.py,0,"b'from transformers import pipeline\n\nfrom haystack.database.base import Document\n\n\nclass TransformersReader:\n    """"""\n    Transformer based model for extractive Question Answering using the huggingface\'s transformers framework\n    (https://github.com/huggingface/transformers).\n    While the underlying model can vary (BERT, Roberta, DistilBERT ...) the interface remains the same.\n\n    With the reader, you can:\n     - directly get predictions via predict()\n    """"""\n\n    def __init__(\n        self,\n        model=""distilbert-base-uncased-distilled-squad"",\n        tokenizer=""distilbert-base-uncased"",\n        context_window_size=30,\n        #no_answer_shift=-100,\n        #batch_size=16,\n        use_gpu=0,\n        n_best_per_passage=2\n    ):\n        """"""\n        Load a QA model from Transformers.\n        Available models include:\n        - distilbert-base-uncased-distilled-squad\n        - bert-large-cased-whole-word-masking-finetuned-squad\n        - bert-large-uncased-whole-word-masking-finetuned-squad\n\n        See https://huggingface.co/models for full list of available QA models\n\n        :param model: name of the model\n        :param tokenizer: name of the tokenizer (usually the same as model)\n        :param context_window_size: num of chars (before and after the answer) to return as ""context"" for each answer.\n                            The context usually helps users to understand if the answer really makes sense.\n        :param use_gpu: < 1  -> use cpu\n                        >= 0 -> ordinal of the gpu to use\n        """"""\n        self.model = pipeline(""question-answering"", model=model, tokenizer=tokenizer, device=use_gpu)\n        self.context_window_size = context_window_size\n        self.n_best_per_passage = n_best_per_passage\n        #TODO param to modify bias for no_answer\n\n    def predict(self, question: str, documents: [Document], top_k: int = None):\n        """"""\n        Use loaded QA model to find answers for a question in the supplied list of Document.\n\n        Returns dictionaries containing answers sorted by (desc.) probability\n        Example:\n        {\'question\': \'Who is the father of Arya Stark?\',\n        \'answers\': [\n                     {\'answer\': \'Eddard,\',\n                     \'context\': "" She travels with her father, Eddard, to King\'s Landing when he is "",\n                     \'offset_answer_start\': 147,\n                     \'offset_answer_end\': 154,\n                     \'probability\': 0.9787139466668613,\n                     \'score\': None,\n                     \'document_id\': None\n                     },\n                    ...\n                   ]\n        }\n\n        :param question: question string\n        :param documents: list of Document in which to search for the answer\n        :param top_k: the maximum number of answers to return\n        :return: dict containing question and answers\n\n        """"""\n        # get top-answers for each candidate passage\n        answers = []\n        for doc in documents:\n            query = {""context"": doc.text, ""question"": question}\n            predictions = self.model(query, topk=self.n_best_per_passage)\n            # assemble and format all answers\n            for pred in predictions:\n                if pred[""answer""]:\n                    context_start = max(0, pred[""start""] - self.context_window_size)\n                    context_end = min(len(doc.text), pred[""end""] + self.context_window_size)\n                    answers.append({\n                        ""answer"": pred[""answer""],\n                        ""context"": doc.text[context_start:context_end],\n                        ""offset_answer_start"": pred[""start""],\n                        ""offset_answer_end"": pred[""end""],\n                        ""probability"": pred[""score""],\n                        ""score"": None,\n                        ""document_id"": doc.id,\n                        ""meta"": doc.meta\n                    })\n\n        # sort answers by their `probability` and select top-k\n        answers = sorted(\n            answers, key=lambda k: k[""probability""], reverse=True\n        )\n        answers = answers[:top_k]\n\n        results = {""question"": question,\n                   ""answers"": answers}\n\n        return results\n'"
haystack/retriever/__init__.py,0,b''
haystack/retriever/base.py,0,"b'from abc import ABC, abstractmethod\n\n\nclass BaseRetriever(ABC):\n    @abstractmethod\n    def retrieve(self, query, candidate_doc_ids=None, top_k=1):\n        pass\n'"
haystack/retriever/elasticsearch.py,0,"b'import logging\nfrom typing import Type\nfrom farm.infer import Inferencer\n\nfrom haystack.database.base import Document, BaseDocumentStore\nfrom haystack.retriever.base import BaseRetriever\n\nlogger = logging.getLogger(__name__)\n\n\nclass ElasticsearchRetriever(BaseRetriever):\n    def __init__(self, document_store: Type[BaseDocumentStore], custom_query: str = None):\n        """"""\n        :param document_store: an instance of a DocumentStore to retrieve documents from.\n        :param custom_query: query string as per Elasticsearch DSL with a mandatory question placeholder($question).\n\n                             Optionally, ES `filter` clause can be added where the values of `terms` are placeholders\n                             that get substituted during runtime. The placeholder(${filter_name_1}, ${filter_name_2}..)\n                             names must match with the filters dict supplied in self.retrieve().\n\n                             An example custom_query:\n                            {\n                                ""size"": 10,\n                                ""query"": {\n                                    ""bool"": {\n                                        ""should"": [{""multi_match"": {\n                                            ""query"": ""${question}"",                 // mandatory $question placeholder\n                                            ""type"": ""most_fields"",\n                                            ""fields"": [""text"", ""title""]}}],\n                                        ""filter"": [                                 // optional custom filters\n                                            {""terms"": {""year"": ""${years}""}},\n                                            {""terms"": {""quarter"": ""${quarters}""}}],\n                                    }\n                                },\n                            }\n\n                             For this custom_query, a sample retrieve() could be:\n                             self.retrieve(query=""Why did the revenue increase?"",\n                                           filters={""years"": [""2019""], ""quarters"": [""Q1"", ""Q2""]})\n        """"""\n        self.document_store = document_store\n        self.custom_query = custom_query\n\n    def retrieve(self, query: str, filters: dict = None, top_k: int = 10, index: str = None) -> [Document]:\n        if index is None:\n            index = self.document_store.index\n\n        documents = self.document_store.query(query, filters, top_k, self.custom_query, index)\n        logger.info(f""Got {len(documents)} candidates from retriever"")\n\n        return documents\n\n    def eval(self, label_index: str = ""feedback"", doc_index: str = ""eval_document"", label_origin: str = ""gold_label"",\n             top_k: int = 10) -> dict:\n        """"""\n        Performs evaluation on the Retriever.\n        Retriever is evaluated based on whether it finds the correct document given the question string and at which\n        position in the ranking of documents the correct document is.\n\n        Returns a dict containing the following metrics:\n            - ""recall"": Proportion of questions for which correct document is among retrieved documents\n            - ""mean avg precision"": Mean of average precision for each question. Rewards retrievers that give relevant\n              documents a higher rank.\n\n        :param label_index: Index/Table in DocumentStore where labeled questions are stored\n        :param doc_index: Index/Table in DocumentStore where documents that are used for evaluation are stored\n        :param top_k: How many documents to return per question\n        """"""\n\n        # extract all questions for evaluation\n        filter = {""origin"": label_origin}\n        questions = self.document_store.get_all_documents_in_index(index=label_index, filters=filter)\n\n        # calculate recall and mean-average-precision\n        correct_retrievals = 0\n        summed_avg_precision = 0\n        for q_idx, question in enumerate(questions):\n            question_string = question[""_source""][""question""]\n            retrieved_docs = self.retrieve(question_string, top_k=top_k, index=doc_index)\n            # check if correct doc in retrieved docs\n            for doc_idx, doc in enumerate(retrieved_docs):\n                if doc.meta[""doc_id""] == question[""_source""][""doc_id""]:\n                    correct_retrievals += 1\n                    summed_avg_precision += 1 / (doc_idx + 1)\n                    break\n\n        number_of_questions = q_idx + 1\n        recall = correct_retrievals / number_of_questions\n        mean_avg_precision = summed_avg_precision / number_of_questions\n\n        logger.info((f""For {correct_retrievals} out of {number_of_questions} questions ({recall:.2%}), the answer was in""\n                     f"" the top-{top_k} candidate passages selected by the retriever.""))\n\n        return {""recall"": recall, ""map"": mean_avg_precision}\n\n\nclass EmbeddingRetriever(BaseRetriever):\n    def __init__(\n        self,\n        document_store: Type[BaseDocumentStore],\n        embedding_model: str,\n        gpu: bool = True,\n        model_format: str = ""farm"",\n        pooling_strategy: str = ""reduce_mean"",\n        emb_extraction_layer: int = -1,\n    ):\n        """"""\n        TODO\n        :param document_store:\n        :param embedding_model:\n        :param gpu:\n        :param model_format:\n        """"""\n        self.document_store = document_store\n        self.model_format = model_format\n        self.embedding_model = embedding_model\n        self.pooling_strategy = pooling_strategy\n        self.emb_extraction_layer = emb_extraction_layer\n\n        logger.info(f""Init retriever using embeddings of model {embedding_model}"")\n        if model_format == ""farm"" or model_format == ""transformers"":\n            self.embedding_model = Inferencer.load(\n                embedding_model, task_type=""embeddings"", extraction_strategy=self.pooling_strategy,\n                extraction_layer=self.emb_extraction_layer, gpu=gpu, batch_size=4, max_seq_len=512, num_processes=0\n            )\n\n        elif model_format == ""sentence_transformers"":\n            from sentence_transformers import SentenceTransformer\n\n            # pretrained embedding models coming from: https://github.com/UKPLab/sentence-transformers#pretrained-models\n            # e.g. \'roberta-base-nli-stsb-mean-tokens\'\n            if gpu:\n                device = ""cuda""\n            else:\n                device = ""cpu""\n            self.embedding_model = SentenceTransformer(embedding_model, device=device)\n        else:\n            raise NotImplementedError\n\n    def retrieve(self, query: str, candidate_doc_ids: [str] = None, top_k: int = 10) -> [Document]:\n        query_emb = self.create_embedding(texts=[query])\n        documents = self.document_store.query_by_embedding(query_emb[0], top_k, candidate_doc_ids)\n\n        return documents\n\n    def create_embedding(self, texts: [str]):\n        """"""\n        Create embeddings for each text in a list of texts using the retrievers model (`self.embedding_model`)\n        :param texts: texts to embed\n        :return: list of embeddings (one per input text). Each embedding is a list of floats.\n        """"""\n\n        # for backward compatibility: cast pure str input\n        if type(texts) == str:\n            texts = [texts]\n        assert type(texts) == list, ""Expecting a list of texts, i.e. create_embeddings(texts=[\'text1\',...])""\n\n        if self.model_format == ""farm"":\n            res = self.embedding_model.inference_from_dicts(dicts=[{""text"": t} for t in texts])\n            emb = [list(r[""vec""]) for r in res] #cast from numpy\n        elif self.model_format == ""sentence_transformers"":\n            # text is single string, sentence-transformers needs a list of strings\n            res = self.embedding_model.encode(texts)  # get back list of numpy embedding vectors\n            emb = [list(r.astype(\'float64\')) for r in res] #cast from numpy\n        return emb\n'"
haystack/retriever/tfidf.py,0,"b'import logging\nfrom collections import OrderedDict, namedtuple\n\nimport pandas as pd\nfrom haystack.database.base import Document\nfrom haystack.retriever.base import BaseRetriever\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nlogger = logging.getLogger(__name__)\n\n# TODO make Paragraph generic for configurable units of text eg, pages, paragraphs, or split by a char_limit\nParagraph = namedtuple(""Paragraph"", [""paragraph_id"", ""document_id"", ""text"", ""meta""])\n\n\nclass TfidfRetriever(BaseRetriever):\n    """"""\n    Read all documents from a SQL backend.\n\n    Split documents into smaller units (eg, paragraphs or pages) to reduce the \n    computations when text is passed on to a Reader for QA.\n\n    It uses sklearn\'s TfidfVectorizer to compute a tf-idf matrix.\n    """"""\n\n    def __init__(self, document_store):\n        self.vectorizer = TfidfVectorizer(\n            lowercase=True,\n            stop_words=None,\n            token_pattern=r""(?u)\\b\\w\\w+\\b"",\n            ngram_range=(1, 1),\n        )\n\n        self.document_store = document_store\n        self.paragraphs = self._get_all_paragraphs()\n        self.df = None\n        self.fit()\n\n    def _get_all_paragraphs(self):\n        """"""\n        Split the list of documents in paragraphs\n        """"""\n        documents = self.document_store.get_all_documents()\n\n        paragraphs = []\n        p_id = 0\n        for doc in documents:\n            for p in doc.text.split(""\\n\\n""):  # TODO: this assumes paragraphs are separated by ""\\n\\n"". Can be switched to paragraph tokenizer.\n                if not p.strip():  # skip empty paragraphs\n                    continue\n                paragraphs.append(\n                    Paragraph(document_id=doc.id, paragraph_id=p_id, text=(p,), meta=doc.meta)\n                )\n                p_id += 1\n        logger.info(f""Found {len(paragraphs)} candidate paragraphs from {len(documents)} docs in DB"")\n        return paragraphs\n\n    def _calc_scores(self, query):\n        question_vector = self.vectorizer.transform([query])\n\n        scores = self.tfidf_matrix.dot(question_vector.T).toarray()\n        idx_scores = [(idx, score) for idx, score in enumerate(scores)]\n        indices_and_scores = OrderedDict(\n            sorted(idx_scores, key=(lambda tup: tup[1]), reverse=True)\n        )\n        return indices_and_scores\n\n    def retrieve(self, query, filters=None, top_k=10, verbose=True):\n        if filters:\n            raise NotImplementedError(""Filters are not implemented in TfidfRetriever."")\n\n        # get scores\n        indices_and_scores = self._calc_scores(query)\n\n        # rank paragraphs\n        df_sliced = self.df.loc[indices_and_scores.keys()]\n        df_sliced = df_sliced[:top_k]\n\n        if verbose:\n            logger.info(\n                f""Identified {df_sliced.shape[0]} candidates via retriever:\\n {df_sliced.to_string(col_space=10, index=False)}""\n            )\n\n        # get actual content for the top candidates\n        paragraphs = list(df_sliced.text.values)\n        meta_data = [{""document_id"": row[""document_id""], ""paragraph_id"": row[""paragraph_id""],  ""meta"": row.get(""meta"", {})}\n                     for idx, row in df_sliced.iterrows()]\n\n        documents = []\n        for para, meta in zip(paragraphs, meta_data):\n            documents.append(\n                Document(\n                    id=meta[""paragraph_id""],\n                    text=para,\n                    meta=meta.get(""meta"", {})\n                ))\n\n        return documents\n\n    def fit(self):\n        self.df = pd.DataFrame.from_dict(self.paragraphs)\n        self.df[""text""] = self.df[""text""].apply(lambda x: "" "".join(x))\n        self.tfidf_matrix = self.vectorizer.fit_transform(self.df[""text""])\n'"
haystack/api/controller/__init__.py,0,b''
haystack/api/controller/feedback.py,0,"b'from collections import defaultdict\nfrom typing import Optional\n\nfrom elasticsearch.helpers import scan\nfrom fastapi import APIRouter, status\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel, Field\n\nfrom haystack.api.config import (\n    DB_HOST,\n    DB_PORT,\n    DB_USER,\n    DB_PW,\n    DB_INDEX,\n    ES_CONN_SCHEME,\n    TEXT_FIELD_NAME,\n    SEARCH_FIELD_NAME,\n    EMBEDDING_DIM,\n    EMBEDDING_FIELD_NAME,\n    EXCLUDE_META_DATA_FIELDS,\n)\nfrom haystack.api.config import DB_INDEX_FEEDBACK\nfrom haystack.api.elasticsearch_client import elasticsearch_client\nfrom haystack.database.elasticsearch import ElasticsearchDocumentStore\n\nrouter = APIRouter()\n\ndocument_store = ElasticsearchDocumentStore(\n    host=DB_HOST,\n    port=DB_PORT,\n    username=DB_USER,\n    password=DB_PW,\n    index=DB_INDEX,\n    scheme=ES_CONN_SCHEME,\n    ca_certs=False,\n    verify_certs=False,\n    text_field=TEXT_FIELD_NAME,\n    search_fields=SEARCH_FIELD_NAME,\n    embedding_dim=EMBEDDING_DIM,\n    embedding_field=EMBEDDING_FIELD_NAME,\n    excluded_meta_data=EXCLUDE_META_DATA_FIELDS,\n)\n\n\nclass Feedback(BaseModel):\n    question: str = Field(..., description=""The question input by the user, i.e., the query."")\n    label: str = Field(..., description=""The Label for the feedback, eg, relevant or irrelevant."")\n    document_id: str = Field(..., description=""The document in the query result for which feedback is given."")\n    answer: Optional[str] = Field(None, description=""The answer string. Only required for doc-qa feedback."")\n    offset_start_in_doc: Optional[int] = Field(None, description=""The answer start offset in the original doc. Only required for doc-qa feedback."")\n    model_id: Optional[int] = Field(None, description=""The model used for the query."")\n\n\n@router.post(""/doc-qa-feedback"")\ndef feedback(feedback: Feedback):\n    if feedback.answer and feedback.offset_start_in_doc:\n        elasticsearch_client.index(index=DB_INDEX_FEEDBACK, body=feedback.dict())\n    else:\n        return JSONResponse(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            content=""doc-qa feedback must contain \'answer\' and \'answer_doc_start\' fields."",\n        )\n\n\n@router.post(""/faq-qa-feedback"")\ndef feedback(feedback: Feedback):\n    elasticsearch_client.index(index=DB_INDEX_FEEDBACK, body=feedback.dict())\n\n\n@router.get(""/export-doc-qa-feedback"")\ndef export_doc_qa_feedback():\n    """"""\n    SQuAD format JSON export for question/answer pairs that were marked as ""relevant"".\n\n    #TODO filter out faq-qa feedback.\n    """"""\n    relevant_feedback_query = {""query"": {""bool"": {""must"": [{""term"": {""label"": ""relevant""}}]}}}\n    result = scan(elasticsearch_client, index=DB_INDEX_FEEDBACK, query=relevant_feedback_query)\n\n    per_document_feedback = defaultdict(list)\n    for feedback in result:\n        document_id = feedback[""_source""][""document_id""]\n        per_document_feedback[document_id].append(\n            {\n                ""question"": feedback[""_source""][""question""],\n                ""id"": feedback[""_id""],\n                ""answers"": [\n                    {""text"": feedback[""_source""][""answer""], ""answer_start"": feedback[""_source""][""offset_start_in_doc""]}\n                ],\n            }\n        )\n\n    export_data = []\n    for document_id, feedback in per_document_feedback.items():\n        document = document_store.get_document_by_id(document_id)\n        context = document.text\n        export_data.append({""paragraphs"": [{""qas"": feedback}], ""context"": context})\n\n    export = {""data"": export_data}\n\n    return export\n\n\n@router.get(""/export-faq-qa-feedback"")\ndef export_faq_feedback():\n    """"""\n    Export feedback for faq-qa in JSON format.\n    """"""\n    result = scan(elasticsearch_client, index=DB_INDEX_FEEDBACK)\n\n    per_document_feedback = defaultdict(list)\n    for feedback in result:\n        document_id = feedback[""_source""][""document_id""]\n        question = feedback[""_source""][""question""]\n        feedback_id = feedback[""_id""]\n        feedback_label = feedback[""_source""][""label""]\n        per_document_feedback[document_id].append(\n            {""question"": question, ""id"": feedback_id, ""feedback_label"": feedback_label}\n        )\n\n    export_data = []\n    for document_id, feedback in per_document_feedback.items():\n        document = document_store.get_document_by_id(document_id)\n        export_data.append(\n            {""target_question"": document.question, ""target_answer"": document.text, ""queries"": feedback}\n        )\n\n    export = {""data"": export_data}\n\n    return export\n'"
haystack/api/controller/router.py,0,"b'from fastapi import APIRouter\n\nfrom haystack.api.controller import search, feedback\n\nrouter = APIRouter()\n\nrouter.include_router(search.router, tags=[""search""])\nrouter.include_router(feedback.router, tags=[""feedback""])\n'"
haystack/api/controller/search.py,0,"b'import logging\nfrom datetime import datetime\nfrom typing import List, Dict, Optional\n\nimport elasticapm\nfrom fastapi import APIRouter\nfrom fastapi import HTTPException\nfrom pydantic import BaseModel\n\nfrom haystack import Finder\nfrom haystack.api.config import DB_HOST, DB_PORT, DB_USER, DB_PW, DB_INDEX, ES_CONN_SCHEME, TEXT_FIELD_NAME, SEARCH_FIELD_NAME, \\\n    EMBEDDING_DIM, EMBEDDING_FIELD_NAME, EXCLUDE_META_DATA_FIELDS, EMBEDDING_MODEL_PATH, USE_GPU, READER_MODEL_PATH, \\\n    BATCHSIZE, CONTEXT_WINDOW_SIZE, TOP_K_PER_CANDIDATE, NO_ANS_BOOST, MAX_PROCESSES, MAX_SEQ_LEN, DOC_STRIDE, \\\n    DEFAULT_TOP_K_READER, DEFAULT_TOP_K_RETRIEVER, CONCURRENT_REQUEST_PER_WORKER\nfrom haystack.api.controller.utils import RequestLimiter\nfrom haystack.database.elasticsearch import ElasticsearchDocumentStore\nfrom haystack.reader.farm import FARMReader\nfrom haystack.retriever.elasticsearch import ElasticsearchRetriever, EmbeddingRetriever\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter()\n\n# Init global components: DocumentStore, Retriever, Reader, Finder\ndocument_store = ElasticsearchDocumentStore(\n    host=DB_HOST,\n    port=DB_PORT,\n    username=DB_USER,\n    password=DB_PW,\n    index=DB_INDEX,\n    scheme=ES_CONN_SCHEME,\n    ca_certs=False,\n    verify_certs=False,\n    text_field=TEXT_FIELD_NAME,\n    search_fields=SEARCH_FIELD_NAME,\n    embedding_dim=EMBEDDING_DIM,\n    embedding_field=EMBEDDING_FIELD_NAME,\n    excluded_meta_data=EXCLUDE_META_DATA_FIELDS,\n)\n\n\nif EMBEDDING_MODEL_PATH:\n    retriever = EmbeddingRetriever(document_store=document_store, embedding_model=EMBEDDING_MODEL_PATH, gpu=USE_GPU)\nelse:\n    retriever = ElasticsearchRetriever(document_store=document_store)\n\nif READER_MODEL_PATH:  # for extractive doc-qa\n    reader = FARMReader(\n        model_name_or_path=str(READER_MODEL_PATH),\n        batch_size=BATCHSIZE,\n        use_gpu=USE_GPU,\n        context_window_size=CONTEXT_WINDOW_SIZE,\n        top_k_per_candidate=TOP_K_PER_CANDIDATE,\n        no_ans_boost=NO_ANS_BOOST,\n        num_processes=MAX_PROCESSES,\n        max_seq_len=MAX_SEQ_LEN,\n        doc_stride=DOC_STRIDE,\n    )\nelse:\n    reader = None  # don\'t need one for pure FAQ matching\n\nFINDERS = {1: Finder(reader=reader, retriever=retriever)}\n\n\n#############################################\n# Data schema for request & response\n#############################################\nclass Question(BaseModel):\n    questions: List[str]\n    filters: Dict[str, Optional[str]] = None\n    top_k_reader: int = DEFAULT_TOP_K_READER\n    top_k_retriever: int = DEFAULT_TOP_K_RETRIEVER\n\n\nclass Answer(BaseModel):\n    answer: Optional[str]\n    question: Optional[str]\n    score: float = None\n    probability: float = None\n    context: Optional[str]\n    offset_start: int\n    offset_end: int\n    offset_start_in_doc: Optional[int]\n    offset_end_in_doc: Optional[int]\n    document_id: Optional[str] = None\n    meta: Optional[Dict[str, Optional[str]]]\n\n\nclass AnswersToIndividualQuestion(BaseModel):\n    question: str\n    answers: List[Optional[Answer]]\n\n\nclass Answers(BaseModel):\n    results: List[AnswersToIndividualQuestion]\n\n\n#############################################\n# Endpoints\n#############################################\ndoc_qa_limiter = RequestLimiter(CONCURRENT_REQUEST_PER_WORKER)\n\n@router.post(""/models/{model_id}/doc-qa"", response_model=Answers, response_model_exclude_unset=True)\ndef doc_qa(model_id: int, request: Question):\n    with doc_qa_limiter.run():\n        finder = FINDERS.get(model_id, None)\n        if not finder:\n            raise HTTPException(\n                status_code=404, detail=f""Couldn\'t get Finder with ID {model_id}. Available IDs: {list(FINDERS.keys())}""\n            )\n\n        results = []\n        for question in request.questions:\n            if request.filters:\n                # put filter values into a list and remove filters with null value\n                request.filters = {key: [value] for key, value in request.filters.items() if value is not None}\n                logger.info(f"" [{datetime.now()}] Request: {request}"")\n\n            result = finder.get_answers(\n                question=question,\n                top_k_retriever=request.top_k_retriever,\n                top_k_reader=request.top_k_reader,\n                filters=request.filters,\n            )\n            results.append(result)\n\n        elasticapm.set_custom_context({""results"": results})\n        logger.info({""request"": request.json(), ""results"": results})\n\n        return {""results"": results}\n\n\n@router.post(""/models/{model_id}/faq-qa"", response_model=Answers, response_model_exclude_unset=True)\ndef faq_qa(model_id: int, request: Question):\n    finder = FINDERS.get(model_id, None)\n    if not finder:\n        raise HTTPException(\n            status_code=404, detail=f""Couldn\'t get Finder with ID {model_id}. Available IDs: {list(FINDERS.keys())}""\n        )\n\n    results = []\n    for question in request.questions:\n        if request.filters:\n            # put filter values into a list and remove filters with null value\n            request.filters = {key: [value] for key, value in request.filters.items() if value is not None}\n            logger.info(f"" [{datetime.now()}] Request: {request}"")\n\n        result = finder.get_answers_via_similar_questions(\n            question=question, top_k_retriever=request.top_k_retriever, filters=request.filters,\n        )\n        results.append(result)\n\n    elasticapm.set_custom_context({""results"": results})\n    logger.info({""request"": request.json(), ""results"": results})\n\n    return {""results"": results}\n'"
haystack/api/controller/utils.py,0,"b'from contextlib import contextmanager\nfrom threading import Semaphore\n\nfrom fastapi import HTTPException\n\n\nclass RequestLimiter:\n    def __init__(self, limit):\n        self.semaphore = Semaphore(limit - 1)\n\n    @contextmanager\n    def run(self):\n        acquired = self.semaphore.acquire(blocking=False)\n        if not acquired:\n            raise HTTPException(status_code=503, detail=""The server is busy processing requests."")\n        try:\n            yield acquired\n        finally:\n            self.semaphore.release()\n'"
haystack/indexing/file_converters/__init__.py,0,b''
haystack/indexing/file_converters/base.py,0,"b'from abc import abstractmethod\nfrom pathlib import Path\n\n\nclass BaseConverter:\n    """"""\n    Base class for implementing file converts to transform input documents to text format for indexing in database.\n    """"""\n\n    def __init__(\n        self,\n        remove_numeric_tables: bool = None,\n        remove_header_footer: bool = None,\n        remove_whitespace: bool = None,\n        remove_empty_lines: bool = None,\n        valid_languages: [str] = None,\n    ):\n        """"""\n        :param remove_numeric_tables: This option uses heuristics to remove numeric rows from the tables.\n                                      The tabular structures in documents might be noise for the reader model if it\n                                      does not have table parsing capability for finding answers. However, tables\n                                      may also have long strings that could possible candidate for searching answers.\n                                      The rows containing strings are thus retained in this option.\n        :param remove_header_footer: use heuristic to remove footers and headers across different pages by searching\n                                     for the longest common string. This heuristic uses exact matches and therefore\n                                     works well for footers like ""Copyright 2019 by XXX"", but won\'t detect ""Page 3 of 4""\n                                     or similar.\n        :param remove_whitespace: strip whitespaces before or after each line in the text.\n        :param remove_empty_lines: remove more than two empty lines in the text.\n        :param valid_languages: validate languages from a list of languages specified in the ISO 639-1\n                                (https://en.wikipedia.org/wiki/ISO_639-1) format.\n                                This option can be used to add test for encoding errors. If the extracted text is\n                                not one of the valid languages, then it might likely be encoding error resulting\n                                in garbled text.\n        """"""\n        self.remove_numeric_tables = remove_numeric_tables\n        self.remove_header_footer = remove_header_footer\n        self.remove_whitespace = remove_whitespace\n        self.remove_empty_lines = remove_empty_lines\n        self.valid_languages = valid_languages\n\n    @abstractmethod\n    def extract_pages(self, file_path: Path) -> [str]:\n        pass\n'"
haystack/indexing/file_converters/pdftotext.py,0,"b'import logging\nimport re\nimport subprocess\nfrom functools import partial, reduce\nfrom itertools import chain\nfrom pathlib import Path\n\nimport fitz\nimport langdetect\n\nfrom haystack.indexing.file_converters.base import BaseConverter\n\nlogger = logging.getLogger(__name__)\n\n\nclass PDFToTextConverter(BaseConverter):\n    def __init__(\n        self,\n        remove_numeric_tables: bool = False,\n        remove_whitespace: bool = None,\n        remove_empty_lines: bool = None,\n        remove_header_footer: bool = None,\n        valid_languages: [str] = None,\n    ):\n        """"""\n        :param remove_numeric_tables: This option uses heuristics to remove numeric rows from the tables.\n                                      The tabular structures in documents might be noise for the reader model if it\n                                      does not have table parsing capability for finding answers. However, tables\n                                      may also have long strings that could possible candidate for searching answers.\n                                      The rows containing strings are thus retained in this option.\n        :param remove_whitespace: strip whitespaces before or after each line in the text.\n        :param remove_empty_lines: remove more than two empty lines in the text.\n        :param remove_header_footer: use heuristic to remove footers and headers across different pages by searching\n                                     for the longest common string. This heuristic uses exact matches and therefore\n                                     works well for footers like ""Copyright 2019 by XXX"", but won\'t detect ""Page 3 of 4""\n                                     or similar.\n        :param valid_languages: validate languages from a list of languages specified in the ISO 639-1\n                                (https://en.wikipedia.org/wiki/ISO_639-1) format.\n                                This option can be used to add test for encoding errors. If the extracted text is\n                                not one of the valid languages, then it might likely be encoding error resulting\n                                in garbled text.\n        """"""\n        verify_installation = subprocess.run([""pdftotext -v""], shell=True)\n        if verify_installation.returncode == 127:\n            raise Exception(\n                """"""pdftotext is not installed. It is part of xpdf or poppler-utils software suite.\n                \n                   Installation on Linux:\n                   wget --no-check-certificate https://dl.xpdfreader.com/xpdf-tools-linux-4.02.tar.gz &&\n                   tar -xvf xpdf-tools-linux-4.02.tar.gz && sudo cp xpdf-tools-linux-4.02/bin64/pdftotext /usr/local/bin\n                   \n                   Installation on MacOS:\n                   brew install xpdf\n                   \n                   You can find more details here: https://www.xpdfreader.com\n                """"""\n            )\n\n        super().__init__(\n            remove_numeric_tables=remove_numeric_tables,\n            remove_whitespace=remove_whitespace,\n            remove_empty_lines=remove_empty_lines,\n            remove_header_footer=remove_header_footer,\n            valid_languages=valid_languages,\n        )\n\n    def extract_pages(self, file_path: Path) -> [str]:\n\n        page_count = fitz.open(file_path).pageCount\n\n        pages = []\n        for page_number in range(1, page_count + 1):\n            # pdftotext tool provides an option to retain the original physical layout of a PDF page. This behaviour\n            # can be toggled by using the layout param.\n            #  layout=True\n            #      + table structures get retained better\n            #      - multi-column pages(eg, research papers) gets extracted with text from multiple columns on same line\n            #  layout=False\n            #      + keeps strings in content stream order, hence multi column layout works well\n            #      - cells of tables gets split across line\n            #\n            #  Here, as a ""safe"" default, layout is turned off.\n            page = self._extract_page(file_path, page_number, layout=False)\n            lines = page.splitlines()\n            cleaned_lines = []\n            for line in lines:\n                words = line.split()\n                digits = [word for word in words if any(i.isdigit() for i in word)]\n\n                # remove lines having > 40% of words as digits AND not ending with a period(.)\n                if self.remove_numeric_tables:\n                    if words and len(digits) / len(words) > 0.4 and not line.strip().endswith("".""):\n                        logger.debug(f""Removing line \'{line}\' from {file_path}"")\n                        continue\n\n                if self.remove_whitespace:\n                    line = line.strip()\n\n                cleaned_lines.append(line)\n\n            page = ""\\n"".join(cleaned_lines)\n\n            if self.remove_empty_lines:\n                page = re.sub(r""\\n\\n+"", ""\\n\\n"", page)\n\n            pages.append(page)\n            page_number += 1\n\n        if self.valid_languages:\n            document_text = """".join(pages)\n            if not self._validate_language(document_text):\n                logger.warning(\n                    f""The language for {file_path} is not one of {self.valid_languages}. The file may not have ""\n                    f""been decoded in the correct text format.""\n                )\n\n        if self.remove_header_footer:\n            pages, header, footer = self.find_and_remove_header_footer(\n                pages, n_chars=300, n_first_pages_to_ignore=1, n_last_pages_to_ignore=1\n            )\n            logger.info(f""Removed header \'{header}\' and footer {footer} in {file_path}"")\n\n        return pages\n\n    def _extract_page(self, file_path: Path, page_number: int, layout: bool):\n        """"""\n        Extract a page from the pdf file at file_path.\n\n        :param file_path: path of the pdf file\n        :param page_number: page number to extract(starting from 1)\n        :param layout: whether to retain the original physical layout for a page. If disabled, PDF pages are read in\n                       the content stream order.\n        """"""\n        if layout:\n            command = [""pdftotext"", ""-layout"", ""-f"", str(page_number), ""-l"", str(page_number), file_path, ""-""]\n        else:\n            command = [""pdftotext"", ""-f"", str(page_number), ""-l"", str(page_number), file_path, ""-""]\n        output_page = subprocess.run(command, capture_output=True, shell=False)\n        page = output_page.stdout.decode(errors=""ignore"")\n        return page\n\n    def _validate_language(self, text: str):\n        """"""\n        Validate if the language of the text is one of valid languages.\n        """"""\n        try:\n            lang = langdetect.detect(text)\n        except langdetect.lang_detect_exception.LangDetectException:\n            lang = None\n\n        if lang in self.valid_languages:\n            return True\n        else:\n            return False\n\n    def _ngram(self, seq: str, n: int):\n        """"""\n        Return ngram (of tokens - currently splitted by whitespace)\n        :param seq: str, string from which the ngram shall be created\n        :param n: int, n of ngram\n        :return: str, ngram as string\n        """"""\n\n        # In order to maintain the original whitespace, but still consider \\n and \\t for n-gram tokenization,\n        # we add a space here and remove it after creation of the ngrams again (see below)\n        seq = seq.replace(""\\n"", "" \\n"")\n        seq = seq.replace(""\\t"", "" \\t"")\n\n        seq = seq.split("" "")\n        ngrams = (\n            "" "".join(seq[i : i + n]).replace("" \\n"", ""\\n"").replace("" \\t"", ""\\t"") for i in range(0, len(seq) - n + 1)\n        )\n\n        return ngrams\n\n    def _allngram(self, seq: str, min_ngram: int, max_ngram: int):\n        lengths = range(min_ngram, max_ngram) if max_ngram else range(min_ngram, len(seq))\n        ngrams = map(partial(self._ngram, seq), lengths)\n        res = set(chain.from_iterable(ngrams))\n        return res\n\n    def find_longest_common_ngram(self, sequences: [str], max_ngram: int = 30, min_ngram: int = 3):\n        """"""\n        Find the longest common ngram across different text sequences (e.g. start of pages).\n        Considering all ngrams between the specified range. Helpful for finding footers, headers etc.\n\n        :param sequences: list[str], list of strings that shall be searched for common n_grams\n        :param max_ngram: int, maximum length of ngram to consider\n        :param min_ngram: minimum length of ngram to consider\n        :return: str, common string of all sections\n        """"""\n\n        seqs_ngrams = map(partial(self._allngram, min_ngram=min_ngram, max_ngram=max_ngram), sequences)\n        intersection = reduce(set.intersection, seqs_ngrams)\n\n        try:\n            longest = max(intersection, key=len)\n        except ValueError:\n            # no common sequence found\n            longest = """"\n        return longest if longest.strip() else None\n\n    def find_and_remove_header_footer(\n        self, pages: [str], n_chars: int, n_first_pages_to_ignore: int, n_last_pages_to_ignore: int\n    ):\n        """"""\n        Heuristic to find footers and headers across different pages by searching for the longest common string.\n        For headers we only search in the first n_chars characters (for footer: last n_chars).\n        Note: This heuristic uses exact matches and therefore works well for footers like ""Copyright 2019 by XXX"",\n         but won\'t detect ""Page 3 of 4"" or similar.\n\n        :param pages: list of strings, one string per page\n        :param n_chars: number of first/last characters where the header/footer shall be searched in\n        :param n_first_pages_to_ignore: number of first pages to ignore (e.g. TOCs often don\'t contain footer/header)\n        :param n_last_pages_to_ignore: number of last pages to ignore\n        :return: (cleaned pages, found_header_str, found_footer_str)\n        """"""\n\n        # header\n        start_of_pages = [p[:n_chars] for p in pages[n_first_pages_to_ignore:-n_last_pages_to_ignore]]\n        found_header = self.find_longest_common_ngram(start_of_pages)\n        if found_header:\n            pages = [page.replace(found_header, """") for page in pages]\n\n        # footer\n        end_of_pages = [p[-n_chars:] for p in pages[n_first_pages_to_ignore:-n_last_pages_to_ignore]]\n        found_footer = self.find_longest_common_ngram(end_of_pages)\n        if found_footer:\n            pages = [page.replace(found_footer, """") for page in pages]\n        return pages, found_header, found_footer\n'"
haystack/api/controller/errors/__init__.py,0,b''
haystack/api/controller/errors/http_error.py,0,"b'from fastapi import HTTPException\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse\n\n\nasync def http_error_handler(_: Request, exc: HTTPException) -> JSONResponse:\n    return JSONResponse({""errors"": [exc.detail]}, status_code=exc.status_code)\n'"
