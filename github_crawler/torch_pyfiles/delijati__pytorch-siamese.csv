file_path,api_count,code
contrastive.py,7,"b'import torch\nimport torch.nn\n\n\nclass ContrastiveLoss(torch.nn.Module):\n    """"""\n    Contrastive loss function.\n\n    Based on:\n    """"""\n\n    def __init__(self, margin=1.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def check_type_forward(self, in_types):\n        assert len(in_types) == 3\n\n        x0_type, x1_type, y_type = in_types\n        assert x0_type.size() == x1_type.shape\n        assert x1_type.size()[0] == y_type.shape[0]\n        assert x1_type.size()[0] > 0\n        assert x0_type.dim() == 2\n        assert x1_type.dim() == 2\n        assert y_type.dim() == 1\n\n    def forward(self, x0, x1, y):\n        self.check_type_forward((x0, x1, y))\n\n        # euclidian distance\n        diff = x0 - x1\n        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n        dist = torch.sqrt(dist_sq)\n\n        mdist = self.margin - dist\n        dist = torch.clamp(mdist, min=0.0)\n        loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n        loss = torch.sum(loss) / 2.0 / x0.size()[0]\n        return loss\n'"
net.py,1,"b'# -*- encoding: utf-8 -*-\nimport torch.nn as nn\n\n\nclass SiameseNetwork(nn.Module):\n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n        self.cnn1 = nn.Sequential(\n            nn.Conv2d(1, 20, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.Conv2d(20, 50, kernel_size=5),\n            nn.MaxPool2d(2, stride=2))\n\n        self.fc1 = nn.Sequential(\n            nn.Linear(50 * 4 * 4, 500),\n            nn.ReLU(inplace=True),\n            nn.Linear(500, 10),\n            nn.Linear(10, 2))\n\n    def forward_once(self, x):\n        output = self.cnn1(x)\n        output = output.view(output.size()[0], -1)\n        output = self.fc1(output)\n        return output\n\n    def forward(self, input1, input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        return output1, output2\n'"
test_contrastive.py,12,"b'import sys\nimport argparse\nimport unittest\nimport torch\nimport numpy as np\n\nfrom contrastive import ContrastiveLoss\nfrom torch.autograd import Variable, gradcheck\n\ntorch.set_default_tensor_type(\'torch.DoubleTensor\')\n\n\ndef run_tests():\n    parser = argparse.ArgumentParser(add_help=False)\n    parser.add_argument(\'--seed\', type=int, default=123)\n    args, remaining = parser.parse_known_args()\n    torch.manual_seed(args.seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(args.seed)\n    remaining = [sys.argv[0]] + remaining\n    unittest.main(argv=remaining)\n\n\nclass TestContrastive(unittest.TestCase):\n    def setUp(self):\n        self.x0 = torch.from_numpy(\n            # np.array(\n            #     [[0.39834601, 0.6656751], [-0.44211167, -0.95197892],\n            #      [0.52718359, 0.69099563], [-0.36314946, -0.07625845],\n            #      [-0.53021497, -0.67317766]],\n            #     dtype=np.float32)\n            np.random.uniform(-1, 1, (5, 2)).astype(np.float32)\n        )\n        self.x1 = torch.from_numpy(\n            # np.array(\n            #     [[0.73587674, 0.98970324], [-0.9245277, 0.93210953],\n            #      [-0.32989913, 0.36705822], [0.25636896, 0.10106555],\n            #      [-0.11412049, 0.80171216]],\n            #     dtype=np.float32)\n            np.random.uniform(-1, 1, (5, 2)).astype(np.float32)\n        )\n        self.t = torch.from_numpy(\n            # np.array(\n            #     [1, 0, 1, 1, 0], dtype=np.float32)\n            np.random.randint(0, 2, (5,)).astype(np.float32)\n        )\n        self.margin = 1\n\n    def test_contrastive_loss(self):\n        input1 = Variable(torch.randn(4, 4), requires_grad=True)\n        input2 = Variable(torch.randn(4, 4), requires_grad=True)\n        target = Variable(torch.randn(4), requires_grad=True)\n        tml = ContrastiveLoss(margin=self.margin)\n        self.assertTrue(\n            gradcheck(lambda x1, x2, t: tml.forward(x1, x2, t), (\n                input1, input2, target)))\n\n    def test_contrastive_loss_value(self):\n        x0_val = Variable(self.x0)\n        x1_val = Variable(self.x1)\n        t_val = Variable(self.t)\n        tml = ContrastiveLoss(margin=self.margin)\n        loss = tml.forward(x0_val, x1_val, t_val)\n        self.assertEqual(loss.data.numpy().shape, (1, ))\n        self.assertEqual(loss.data.numpy().dtype, np.float32)\n        loss_value = float(loss.data.numpy())\n\n        # Compute expected value\n        loss_expect = 0\n        for i in range(self.x0.size()[0]):\n            x0d, x1d, td = self.x0[i], self.x1[i], self.t[i]\n            d = torch.sum(torch.pow(x0d - x1d, 2))\n            if td == 1:  # similar pair\n                loss_expect += d\n            elif td == 0:  # dissimilar pair\n                loss_expect += max(1 - np.sqrt(d), 0)**2\n        loss_expect /= 2.0 * self.t.size()[0]\n        print(""expected %s got %s"" % (loss_expect, loss_value))\n        self.assertAlmostEqual(loss_expect, loss_value, places=5)\n\n\nif __name__ == \'__main__\':\n    run_tests()\n'"
train_mnist.py,12,"b'# -*- encoding: utf-8 -*-\nimport argparse\nimport torch\nimport torchvision.datasets as dsets\nimport random\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\n\nfrom net import SiameseNetwork\nfrom contrastive import ContrastiveLoss\nfrom torch.autograd import Variable\nfrom torchvision import transforms\n\n\nclass Dataset(object):\n\n    def __init__(self, x0, x1, label):\n        self.size = label.shape[0]\n        self.x0 = torch.from_numpy(x0)\n        self.x1 = torch.from_numpy(x1)\n        self.label = torch.from_numpy(label)\n\n    def __getitem__(self, index):\n        return (self.x0[index],\n                self.x1[index],\n                self.label[index])\n\n    def __len__(self):\n        return self.size\n\n\ndef create_pairs(data, digit_indices):\n    x0_data = []\n    x1_data = []\n    label = []\n\n    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n    for d in range(10):\n        # make n pairs with each number\n        for i in range(n):\n            # make pairs of the same class\n            # label is 1\n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n            # scale data to 0-1\n            # XXX this does ToTensor also\n            x0_data.append(data[z1] / 255.0)\n            x1_data.append(data[z2] / 255.0)\n            label.append(1)\n\n            # make pairs of different classes\n            # since the minimum value is 1, it is not the same class\n            # label is 0\n            inc = random.randrange(1, 10)\n            dn = (d + inc) % 10\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n            # scale data to 0-1\n            # XXX this does ToTensor also\n            x0_data.append(data[z1] / 255.0)\n            x1_data.append(data[z2] / 255.0)\n            label.append(0)\n\n    x0_data = np.array(x0_data, dtype=np.float32)\n    x0_data = x0_data.reshape([-1, 1, 28, 28])\n    x1_data = np.array(x1_data, dtype=np.float32)\n    x1_data = x1_data.reshape([-1, 1, 28, 28])\n    label = np.array(label, dtype=np.int32)\n    return x0_data, x1_data, label\n\n\ndef create_iterator(data, label, batchsize, shuffle=False):\n    digit_indices = [np.where(label == i)[0] for i in range(10)]\n    x0, x1, label = create_pairs(data, digit_indices)\n    ret = Dataset(x0, x1, label)\n    return ret\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--epoch\', \'-e\', type=int, default=5,\n                        help=\'Number of sweeps over the dataset to train\')\n    parser.add_argument(\'--batchsize\', \'-b\', type=int, default=128,\n                        help=\'Number of images in each mini-batch\')\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA training\')\n    parser.add_argument(\'--model\', \'-m\', default=\'\',\n                        help=\'Give a model to test\')\n    parser.add_argument(\'--train-plot\', action=\'store_true\', default=False,\n                        help=\'Plot train loss\')\n    args = parser.parse_args()\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    print(""Args: %s"" % args)\n\n    # create pair dataset iterator\n    train = dsets.MNIST(\n        root=\'../data/\',\n        train=True,\n        # transform=transforms.Compose([\n        #     transforms.ToTensor(),\n        #     transforms.Normalize((0.1307,), (0.3081,))\n        # ]),\n        download=True\n    )\n    test = dsets.MNIST(\n        root=\'../data/\',\n        train=False,\n        # XXX ToTensor scale to 0-1\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n        #     transforms.Normalize((0.1307,), (0.3081,))\n        ])\n    )\n\n    train_iter = create_iterator(\n        train.train_data.numpy(),\n        train.train_labels.numpy(),\n        args.batchsize)\n\n    # model\n    model = SiameseNetwork()\n    if args.cuda:\n        model.cuda()\n\n    learning_rate = 0.01\n    momentum = 0.9\n    # Loss and Optimizer\n    criterion = ContrastiveLoss()\n    # optimizer = torch.optim.Adam(\n    #     [p for p in model.parameters() if p.requires_grad],\n    #     lr=learning_rate\n    # )\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,\n                                momentum=momentum)\n\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\n    train_loader = torch.utils.data.DataLoader(\n        train_iter,\n        batch_size=args.batchsize, shuffle=True, **kwargs)\n\n    test_loader = torch.utils.data.DataLoader(\n        test,\n        batch_size=args.batchsize, shuffle=True, **kwargs)\n\n    def train(epoch):\n        train_loss = []\n        model.train()\n        start = time.time()\n        start_epoch = time.time()\n        for batch_idx, (x0, x1, labels) in enumerate(train_loader):\n            labels = labels.float()\n            if args.cuda:\n                x0, x1, labels = x0.cuda(), x1.cuda(), labels.cuda()\n            x0, x1, labels = Variable(x0), Variable(x1), Variable(labels)\n            output1, output2 = model(x0, x1)\n            loss = criterion(output1, output2, labels)\n            train_loss.append(loss.data[0])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            accuracy = []\n\n            for idx, logit in enumerate([output1, output2]):\n                corrects = (torch.max(logit, 1)[1].data == labels.long().data).sum()\n                accu = float(corrects) / float(labels.size()[0])\n                accuracy.append(accu)\n\n            if batch_idx % args.batchsize == 0:\n                end = time.time()\n                took = end - start\n                for idx, accu in enumerate(accuracy):\n                    print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss:{:.6f}\\tTook: {:.2f}\\tOut: {}\\tAccu: {:.2f}\'.format(\n                        epoch, batch_idx * len(labels), len(train_loader.dataset),\n                        100. * batch_idx / len(train_loader), loss.data[0],\n                        took, idx, accu * 100.))\n                start = time.time()\n        torch.save(model.state_dict(), \'./model-epoch-%s.pth\' % epoch)\n        end = time.time()\n        took = end - start_epoch\n        print(\'Train epoch: {} \\tTook:{:.2f}\'.format(epoch, took))\n        return train_loss\n\n    def test(model):\n        model.eval()\n        all = []\n        all_labels = []\n\n        for batch_idx, (x, labels) in enumerate(test_loader):\n            if args.cuda:\n                x, labels = x.cuda(), labels.cuda()\n            x, labels = Variable(x, volatile=True), Variable(labels)\n            output = model.forward_once(x)\n            all.extend(output.data.cpu().numpy().tolist())\n            all_labels.extend(labels.data.cpu().numpy().tolist())\n\n        numpy_all = np.array(all)\n        numpy_labels = np.array(all_labels)\n        return numpy_all, numpy_labels\n\n    def plot_mnist(numpy_all, numpy_labels):\n        c = [\'#ff0000\', \'#ffff00\', \'#00ff00\', \'#00ffff\', \'#0000ff\',\n             \'#ff00ff\', \'#990000\', \'#999900\', \'#009900\', \'#009999\']\n\n        for i in range(10):\n            f = numpy_all[np.where(numpy_labels == i)]\n            plt.plot(f[:, 0], f[:, 1], \'.\', c=c[i])\n        plt.legend([\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\'])\n        plt.savefig(\'result.png\')\n\n    if len(args.model) == 0:\n        train_loss = []\n        for epoch in range(1, args.epoch + 1):\n            train_loss.extend(train(epoch))\n\n        if args.train_plot:\n            plt.gca().cla()\n            plt.plot(train_loss, label=""train loss"")\n            plt.legend()\n            plt.draw()\n            plt.savefig(\'train_loss.png\')\n            plt.gca().clear()\n\n    else:\n        saved_model = torch.load(args.model)\n        model = SiameseNetwork()\n        model.load_state_dict(saved_model)\n        if args.cuda:\n            model.cuda()\n\n    numpy_all, numpy_labels = test(model)\n    plot_mnist(numpy_all, numpy_labels)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
