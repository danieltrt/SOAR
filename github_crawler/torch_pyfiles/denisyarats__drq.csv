file_path,api_count,code
drq.py,17,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\nimport math\n\nimport utils\nimport hydra\n\n\nclass Encoder(nn.Module):\n    """"""Convolutional encoder for image-based observations.""""""\n    def __init__(self, obs_shape, feature_dim):\n        super().__init__()\n\n        assert len(obs_shape) == 3\n        self.num_layers = 4\n        self.num_filters = 32\n        self.output_dim = 35\n        self.output_logits = False\n        self.feature_dim = feature_dim\n\n        self.convs = nn.ModuleList([\n            nn.Conv2d(obs_shape[0], self.num_filters, 3, stride=2),\n            nn.Conv2d(self.num_filters, self.num_filters, 3, stride=1),\n            nn.Conv2d(self.num_filters, self.num_filters, 3, stride=1),\n            nn.Conv2d(self.num_filters, self.num_filters, 3, stride=1)\n        ])\n\n        self.head = nn.Sequential(\n            nn.Linear(self.num_filters * 35 * 35, self.feature_dim),\n            nn.LayerNorm(self.feature_dim))\n\n        self.outputs = dict()\n\n    def forward_conv(self, obs):\n        obs = obs / 255.\n        self.outputs[\'obs\'] = obs\n\n        conv = torch.relu(self.convs[0](obs))\n        self.outputs[\'conv1\'] = conv\n\n        for i in range(1, self.num_layers):\n            conv = torch.relu(self.convs[i](conv))\n            self.outputs[\'conv%s\' % (i + 1)] = conv\n\n        h = conv.view(conv.size(0), -1)\n        return h\n\n    def forward(self, obs, detach=False):\n        h = self.forward_conv(obs)\n\n        if detach:\n            h = h.detach()\n\n        out = self.head(h)\n        if not self.output_logits:\n            out = torch.tanh(out)\n\n        self.outputs[\'out\'] = out\n\n        return out\n\n    def copy_conv_weights_from(self, source):\n        """"""Tie convolutional layers""""""\n        for i in range(self.num_layers):\n            utils.tie_weights(src=source.convs[i], trg=self.convs[i])\n\n    def log(self, logger, step):\n        for k, v in self.outputs.items():\n            logger.log_histogram(f\'train_encoder/{k}_hist\', v, step)\n            if len(v.shape) > 2:\n                logger.log_image(f\'train_encoder/{k}_img\', v[0], step)\n\n        for i in range(self.num_layers):\n            logger.log_param(f\'train_encoder/conv{i + 1}\', self.convs[i], step)\n\n\nclass Actor(nn.Module):\n    """"""torch.distributions implementation of an diagonal Gaussian policy.""""""\n    def __init__(self, encoder_cfg, action_shape, hidden_dim, hidden_depth,\n                 log_std_bounds):\n        super().__init__()\n\n        self.encoder = hydra.utils.instantiate(encoder_cfg)\n\n        self.log_std_bounds = log_std_bounds\n        self.trunk = utils.mlp(self.encoder.feature_dim, hidden_dim,\n                               2 * action_shape[0], hidden_depth)\n\n        self.outputs = dict()\n        self.apply(utils.weight_init)\n\n    def forward(self, obs, detach_encoder=False):\n        obs = self.encoder(obs, detach=detach_encoder)\n\n        mu, log_std = self.trunk(obs).chunk(2, dim=-1)\n\n        # constrain log_std inside [log_std_min, log_std_max]\n        log_std = torch.tanh(log_std)\n        log_std_min, log_std_max = self.log_std_bounds\n        log_std = log_std_min + 0.5 * (log_std_max - log_std_min) * (log_std +\n                                                                     1)\n        std = log_std.exp()\n\n        self.outputs[\'mu\'] = mu\n        self.outputs[\'std\'] = std\n\n        dist = utils.SquashedNormal(mu, std)\n        return dist\n\n    def log(self, logger, step):\n        for k, v in self.outputs.items():\n            logger.log_histogram(f\'train_actor/{k}_hist\', v, step)\n\n        for i, m in enumerate(self.trunk):\n            if type(m) == nn.Linear:\n                logger.log_param(f\'train_actor/fc{i}\', m, step)\n\n\nclass Critic(nn.Module):\n    """"""Critic network, employes double Q-learning.""""""\n    def __init__(self, encoder_cfg, action_shape, hidden_dim, hidden_depth):\n        super().__init__()\n\n        self.encoder = hydra.utils.instantiate(encoder_cfg)\n\n        self.Q1 = utils.mlp(self.encoder.feature_dim + action_shape[0],\n                            hidden_dim, 1, hidden_depth)\n        self.Q2 = utils.mlp(self.encoder.feature_dim + action_shape[0],\n                            hidden_dim, 1, hidden_depth)\n\n        self.outputs = dict()\n        self.apply(utils.weight_init)\n\n    def forward(self, obs, action, detach_encoder=False):\n        assert obs.size(0) == action.size(0)\n        obs = self.encoder(obs, detach=detach_encoder)\n\n        obs_action = torch.cat([obs, action], dim=-1)\n        q1 = self.Q1(obs_action)\n        q2 = self.Q2(obs_action)\n\n        self.outputs[\'q1\'] = q1\n        self.outputs[\'q2\'] = q2\n\n        return q1, q2\n\n    def log(self, logger, step):\n        self.encoder.log(logger, step)\n\n        for k, v in self.outputs.items():\n            logger.log_histogram(f\'train_critic/{k}_hist\', v, step)\n\n        assert len(self.Q1) == len(self.Q2)\n        for i, (m1, m2) in enumerate(zip(self.Q1, self.Q2)):\n            assert type(m1) == type(m2)\n            if type(m1) is nn.Linear:\n                logger.log_param(f\'train_critic/q1_fc{i}\', m1, step)\n                logger.log_param(f\'train_critic/q2_fc{i}\', m2, step)\n\n\nclass DRQAgent(object):\n    """"""Data regularized Q: actor-critic method for learning from pixels.""""""\n    def __init__(self, obs_shape, action_shape, action_range, device,\n                 encoder_cfg, critic_cfg, actor_cfg, discount,\n                 init_temperature, lr, actor_update_frequency, critic_tau,\n                 critic_target_update_frequency, batch_size):\n        self.action_range = action_range\n        self.device = device\n        self.discount = discount\n        self.critic_tau = critic_tau\n        self.actor_update_frequency = actor_update_frequency\n        self.critic_target_update_frequency = critic_target_update_frequency\n        self.batch_size = batch_size\n\n        self.actor = hydra.utils.instantiate(actor_cfg).to(self.device)\n\n        self.critic = hydra.utils.instantiate(critic_cfg).to(self.device)\n        self.critic_target = hydra.utils.instantiate(critic_cfg).to(\n            self.device)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n\n        # tie conv layers between actor and critic\n        self.actor.encoder.copy_conv_weights_from(self.critic.encoder)\n\n        self.log_alpha = torch.tensor(np.log(init_temperature)).to(device)\n        self.log_alpha.requires_grad = True\n        # set target entropy to -|A|\n        self.target_entropy = -action_shape[0]\n\n        # optimizers\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n                                                 lr=lr)\n        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=lr)\n\n        self.train()\n        self.critic_target.train()\n\n    def train(self, training=True):\n        self.training = training\n        self.actor.train(training)\n        self.critic.train(training)\n\n    @property\n    def alpha(self):\n        return self.log_alpha.exp()\n\n    def act(self, obs, sample=False):\n        obs = torch.FloatTensor(obs).to(self.device)\n        obs = obs.unsqueeze(0)\n        dist = self.actor(obs)\n        action = dist.sample() if sample else dist.mean\n        action = action.clamp(*self.action_range)\n        assert action.ndim == 2 and action.shape[0] == 1\n        return utils.to_np(action[0])\n\n    def update_critic(self, obs, obs_aug, action, reward, next_obs,\n                      next_obs_aug, not_done, logger, step):\n        with torch.no_grad():\n            dist = self.actor(next_obs)\n            next_action = dist.rsample()\n            log_prob = dist.log_prob(next_action).sum(-1, keepdim=True)\n            target_Q1, target_Q2 = self.critic_target(next_obs, next_action)\n            target_V = torch.min(target_Q1,\n                                 target_Q2) - self.alpha.detach() * log_prob\n            target_Q = reward + (not_done * self.discount * target_V)\n\n            dist_aug = self.actor(next_obs_aug)\n            next_action_aug = dist_aug.rsample()\n            log_prob_aug = dist_aug.log_prob(next_action_aug).sum(-1,\n                                                                  keepdim=True)\n            target_Q1, target_Q2 = self.critic_target(next_obs_aug,\n                                                      next_action_aug)\n            target_V = torch.min(\n                target_Q1, target_Q2) - self.alpha.detach() * log_prob_aug\n            target_Q_aug = reward + (not_done * self.discount * target_V)\n\n            target_Q = (target_Q + target_Q_aug) / 2\n\n        # get current Q estimates\n        current_Q1, current_Q2 = self.critic(obs, action)\n        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(\n            current_Q2, target_Q)\n\n        Q1_aug, Q2_aug = self.critic(obs_aug, action)\n\n        critic_loss += F.mse_loss(Q1_aug, target_Q) + F.mse_loss(\n            Q2_aug, target_Q)\n\n        logger.log(\'train_critic/loss\', critic_loss, step)\n\n        # Optimize the critic\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        self.critic.log(logger, step)\n\n    def update_actor_and_alpha(self, obs, logger, step):\n        # detach conv filters, so we don\'t update them with the actor loss\n        dist = self.actor(obs, detach_encoder=True)\n        action = dist.rsample()\n        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n        # detach conv filters, so we don\'t update them with the actor loss\n        actor_Q1, actor_Q2 = self.critic(obs, action, detach_encoder=True)\n\n        actor_Q = torch.min(actor_Q1, actor_Q2)\n\n        actor_loss = (self.alpha.detach() * log_prob - actor_Q).mean()\n\n        logger.log(\'train_actor/loss\', actor_loss, step)\n        logger.log(\'train_actor/target_entropy\', self.target_entropy, step)\n        logger.log(\'train_actor/entropy\', -log_prob.mean(), step)\n\n        # optimize the actor\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        self.actor.log(logger, step)\n\n        self.log_alpha_optimizer.zero_grad()\n        alpha_loss = (self.alpha *\n                      (-log_prob - self.target_entropy).detach()).mean()\n        logger.log(\'train_alpha/loss\', alpha_loss, step)\n        logger.log(\'train_alpha/value\', self.alpha, step)\n        alpha_loss.backward()\n        self.log_alpha_optimizer.step()\n\n    def update(self, replay_buffer, logger, step):\n        obs, action, reward, next_obs, not_done, obs_aug, next_obs_aug = replay_buffer.sample(\n            self.batch_size)\n\n        logger.log(\'train/batch_reward\', reward.mean(), step)\n\n        self.update_critic(obs, obs_aug, action, reward, next_obs,\n                           next_obs_aug, not_done, logger, step)\n\n        if step % self.actor_update_frequency == 0:\n            self.update_actor_and_alpha(obs, logger, step)\n\n        if step % self.critic_target_update_frequency == 0:\n            utils.soft_update_params(self.critic, self.critic_target,\n                                     self.critic_tau)\n'"
logger.py,3,"b'import csv\nimport json\nimport os\nimport shutil\nfrom collections import defaultdict\n\nimport numpy as np\n\nimport torch\nimport torchvision\nfrom termcolor import colored\nfrom torch.utils.tensorboard import SummaryWriter\n\nCOMMON_TRAIN_FORMAT = [(\'episode\', \'E\', \'int\'), (\'step\', \'S\', \'int\'),\n                       (\'episode_reward\', \'R\', \'float\'),\n                       (\'duration\', \'D\', \'time\')]\n\nCOMMON_EVAL_FORMAT = [(\'episode\', \'E\', \'int\'), (\'step\', \'S\', \'int\'),\n                      (\'episode_reward\', \'R\', \'float\')]\n\nAGENT_TRAIN_FORMAT = {\n    \'drq\': [(\'batch_reward\', \'BR\', \'float\'), (\'actor_loss\', \'ALOSS\', \'float\'),\n            (\'critic_loss\', \'CLOSS\', \'float\'),\n            (\'alpha_loss\', \'TLOSS\', \'float\'), (\'alpha_value\', \'TVAL\', \'float\'),\n            (\'actor_entropy\', \'AENT\', \'float\')]\n}\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self._sum = 0\n        self._count = 0\n\n    def update(self, value, n=1):\n        self._sum += value\n        self._count += n\n\n    def value(self):\n        return self._sum / max(1, self._count)\n\n\nclass MetersGroup(object):\n    def __init__(self, file_name, formating):\n        self._csv_file_name = self._prepare_file(file_name, \'csv\')\n        self._formating = formating\n        self._meters = defaultdict(AverageMeter)\n        self._csv_file = open(self._csv_file_name, \'w\')\n        self._csv_writer = None\n\n    def _prepare_file(self, prefix, suffix):\n        file_name = f\'{prefix}.{suffix}\'\n        if os.path.exists(file_name):\n            os.remove(file_name)\n        return file_name\n\n    def log(self, key, value, n=1):\n        self._meters[key].update(value, n)\n\n    def _prime_meters(self):\n        data = dict()\n        for key, meter in self._meters.items():\n            if key.startswith(\'train\'):\n                key = key[len(\'train\') + 1:]\n            else:\n                key = key[len(\'eval\') + 1:]\n            key = key.replace(\'/\', \'_\')\n            data[key] = meter.value()\n        return data\n\n    def _dump_to_csv(self, data):\n        if self._csv_writer is None:\n            self._csv_writer = csv.DictWriter(self._csv_file,\n                                              fieldnames=sorted(data.keys()),\n                                              restval=0.0)\n            self._csv_writer.writeheader()\n        self._csv_writer.writerow(data)\n        self._csv_file.flush()\n\n    def _format(self, key, value, ty):\n        if ty == \'int\':\n            value = int(value)\n            return f\'{key}: {value}\'\n        elif ty == \'float\':\n            return f\'{key}: {value:.04f}\'\n        elif ty == \'time\':\n            return f\'{key}: {value:04.1f} s\'\n        else:\n            raise f\'invalid format type: {ty}\'\n\n    def _dump_to_console(self, data, prefix):\n        prefix = colored(prefix, \'yellow\' if prefix == \'train\' else \'green\')\n        pieces = [f\'| {prefix: <14}\']\n        for key, disp_key, ty in self._formating:\n            value = data.get(key, 0)\n            pieces.append(self._format(disp_key, value, ty))\n        print(\' | \'.join(pieces))\n\n    def dump(self, step, prefix, save=True):\n        if len(self._meters) == 0:\n            return\n        if save:\n            data = self._prime_meters()\n            data[\'step\'] = step\n            self._dump_to_csv(data)\n            self._dump_to_console(data, prefix)\n        self._meters.clear()\n\n\nclass Logger(object):\n    def __init__(self,\n                 log_dir,\n                 save_tb=False,\n                 log_frequency=10000,\n                 action_repeat=1,\n                 agent=\'drq\'):\n        self._log_dir = log_dir\n        self._log_frequency = log_frequency\n        self._action_repeat = action_repeat\n        if save_tb:\n            tb_dir = os.path.join(log_dir, \'tb\')\n            if os.path.exists(tb_dir):\n                try:\n                    shutil.rmtree(tb_dir)\n                except:\n                    print(""logger.py warning: Unable to remove tb directory"")\n                    pass\n            self._sw = SummaryWriter(tb_dir)\n        else:\n            self._sw = None\n        # each agent has specific output format for training\n        assert agent in AGENT_TRAIN_FORMAT\n        train_format = COMMON_TRAIN_FORMAT + AGENT_TRAIN_FORMAT[agent]\n        self._train_mg = MetersGroup(os.path.join(log_dir, \'train\'),\n                                     formating=train_format)\n        self._eval_mg = MetersGroup(os.path.join(log_dir, \'eval\'),\n                                    formating=COMMON_EVAL_FORMAT)\n\n    def _should_log(self, step, log_frequency):\n        log_frequency = log_frequency or self._log_frequency\n        return step % log_frequency == 0\n\n    def _update_step(self, step):\n        return step * self._action_repeat\n\n    def _try_sw_log(self, key, value, step):\n        step = self._update_step(step)\n        if self._sw is not None:\n            self._sw.add_scalar(key, value, step)\n\n    def _try_sw_log_image(self, key, image, step):\n        step = self._update_step(step)\n        if self._sw is not None:\n            assert image.dim() == 3\n            grid = torchvision.utils.make_grid(image.unsqueeze(1))\n            self._sw.add_image(key, grid, step)\n\n    def _try_sw_log_video(self, key, frames, step):\n        step = self._update_step(step)\n        if self._sw is not None:\n            frames = torch.from_numpy(np.array(frames))\n            frames = frames.unsqueeze(0)\n            self._sw.add_video(key, frames, step, fps=30)\n\n    def _try_sw_log_histogram(self, key, histogram, step):\n        step = self._update_step(step)\n        if self._sw is not None:\n            self._sw.add_histogram(key, histogram, step)\n\n    def log(self, key, value, step, n=1, log_frequency=1):\n        if not self._should_log(step, log_frequency):\n            return\n        assert key.startswith(\'train\') or key.startswith(\'eval\')\n        if type(value) == torch.Tensor:\n            value = value.item()\n        self._try_sw_log(key, value / n, step)\n        mg = self._train_mg if key.startswith(\'train\') else self._eval_mg\n        mg.log(key, value, n)\n\n    def log_param(self, key, param, step, log_frequency=None):\n        if not self._should_log(step, log_frequency):\n            return\n        self.log_histogram(key + \'_w\', param.weight.data, step)\n        if hasattr(param.weight, \'grad\') and param.weight.grad is not None:\n            self.log_histogram(key + \'_w_g\', param.weight.grad.data, step)\n        if hasattr(param, \'bias\') and hasattr(param.bias, \'data\'):\n            self.log_histogram(key + \'_b\', param.bias.data, step)\n            if hasattr(param.bias, \'grad\') and param.bias.grad is not None:\n                self.log_histogram(key + \'_b_g\', param.bias.grad.data, step)\n\n    def log_image(self, key, image, step, log_frequency=None):\n        if not self._should_log(step, log_frequency):\n            return\n        assert key.startswith(\'train\') or key.startswith(\'eval\')\n        self._try_sw_log_image(key, image, step)\n\n    def log_video(self, key, frames, step, log_frequency=None):\n        if not self._should_log(step, log_frequency):\n            return\n        assert key.startswith(\'train\') or key.startswith(\'eval\')\n        self._try_sw_log_video(key, frames, step)\n\n    def log_histogram(self, key, histogram, step, log_frequency=None):\n        if not self._should_log(step, log_frequency):\n            return\n        assert key.startswith(\'train\') or key.startswith(\'eval\')\n        self._try_sw_log_histogram(key, histogram, step)\n\n    def dump(self, step, save=True, ty=None):\n        step = self._update_step(step)\n        if ty is None:\n            self._train_mg.dump(step, \'train\', save)\n            self._eval_mg.dump(step, \'eval\', save)\n        elif ty == \'eval\':\n            self._eval_mg.dump(step, \'eval\', save)\n        elif ty == \'train\':\n            self._train_mg.dump(step, \'train\', save)\n        else:\n            raise f\'invalid log type: {ty}\'\n'"
replay_buffer.py,9,"b'import numpy as np\n\nimport kornia\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport utils\n\n\nclass ReplayBuffer(object):\n    """"""Buffer to store environment transitions.""""""\n    def __init__(self, obs_shape, action_shape, capacity, image_pad, device):\n        self.capacity = capacity\n        self.device = device\n\n        self.aug_trans = nn.Sequential(\n            nn.ReplicationPad2d(image_pad),\n            kornia.augmentation.RandomCrop((obs_shape[-1], obs_shape[-1])))\n\n        self.obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n        self.next_obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n        self.actions = np.empty((capacity, *action_shape), dtype=np.float32)\n        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n        self.not_dones_no_max = np.empty((capacity, 1), dtype=np.float32)\n\n        self.idx = 0\n        self.full = False\n\n    def __len__(self):\n        return self.capacity if self.full else self.idx\n\n    def add(self, obs, action, reward, next_obs, done, done_no_max):\n        np.copyto(self.obses[self.idx], obs)\n        np.copyto(self.actions[self.idx], action)\n        np.copyto(self.rewards[self.idx], reward)\n        np.copyto(self.next_obses[self.idx], next_obs)\n        np.copyto(self.not_dones[self.idx], not done)\n        np.copyto(self.not_dones_no_max[self.idx], not done_no_max)\n\n        self.idx = (self.idx + 1) % self.capacity\n        self.full = self.full or self.idx == 0\n\n    def sample(self, batch_size):\n        idxs = np.random.randint(0,\n                                 self.capacity if self.full else self.idx,\n                                 size=batch_size)\n\n        obses = self.obses[idxs]\n        next_obses = self.next_obses[idxs]\n        obses_aug = obses.copy()\n        next_obses_aug = next_obses.copy()\n\n        obses = torch.as_tensor(obses, device=self.device).float()\n        next_obses = torch.as_tensor(next_obses, device=self.device).float()\n        obses_aug = torch.as_tensor(obses_aug, device=self.device).float()\n        next_obses_aug = torch.as_tensor(next_obses_aug,\n                                         device=self.device).float()\n        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n        not_dones_no_max = torch.as_tensor(self.not_dones_no_max[idxs],\n                                           device=self.device)\n\n        obses = self.aug_trans(obses)\n        next_obses = self.aug_trans(next_obses)\n\n        obses_aug = self.aug_trans(obses_aug)\n        next_obses_aug = self.aug_trans(next_obses_aug)\n\n        return obses, actions, rewards, next_obses, not_dones_no_max, obses_aug, next_obses_aug\n'"
train.py,4,"b'import copy\nimport math\nimport os\nimport pickle as pkl\nimport sys\nimport time\n\nimport numpy as np\n\nimport dmc2gym\nimport hydra\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport utils\nfrom logger import Logger\nfrom replay_buffer import ReplayBuffer\nfrom video import VideoRecorder\n\ntorch.backends.cudnn.benchmark = True\n\n\ndef make_env(cfg):\n    """"""Helper function to create dm_control environment""""""\n    if cfg.env == \'ball_in_cup_catch\':\n        domain_name = \'ball_in_cup\'\n        task_name = \'catch\'\n    elif cfg.env == \'point_mass_easy\':\n        domain_name = \'point_mass\'\n        task_name = \'easy\'\n    else:\n        domain_name = cfg.env.split(\'_\')[0]\n        task_name = \'_\'.join(cfg.env.split(\'_\')[1:])\n\n    # per dreamer: https://github.com/danijar/dreamer/blob/02f0210f5991c7710826ca7881f19c64a012290c/wrappers.py#L26\n    camera_id = 2 if domain_name == \'quadruped\' else 0\n\n    env = dmc2gym.make(domain_name=domain_name,\n                       task_name=task_name,\n                       seed=cfg.seed,\n                       visualize_reward=False,\n                       from_pixels=True,\n                       height=cfg.image_size,\n                       width=cfg.image_size,\n                       frame_skip=cfg.action_repeat,\n                       camera_id=camera_id)\n\n    env = utils.FrameStack(env, k=cfg.frame_stack)\n\n    env.seed(cfg.seed)\n    assert env.action_space.low.min() >= -1\n    assert env.action_space.high.max() <= 1\n\n    return env\n\n\nclass Workspace(object):\n    def __init__(self, cfg):\n        self.work_dir = os.getcwd()\n        print(f\'workspace: {self.work_dir}\')\n\n        self.cfg = cfg\n\n        self.logger = Logger(self.work_dir,\n                             save_tb=cfg.log_save_tb,\n                             log_frequency=cfg.log_frequency_step,\n                             agent=cfg.agent.name,\n                             action_repeat=cfg.action_repeat)\n\n        utils.set_seed_everywhere(cfg.seed)\n        self.device = torch.device(cfg.device)\n        self.env = make_env(cfg)\n\n        cfg.agent.params.obs_shape = self.env.observation_space.shape\n        cfg.agent.params.action_shape = self.env.action_space.shape\n        cfg.agent.params.action_range = [\n            float(self.env.action_space.low.min()),\n            float(self.env.action_space.high.max())\n        ]\n        self.agent = hydra.utils.instantiate(cfg.agent)\n\n        self.replay_buffer = ReplayBuffer(self.env.observation_space.shape,\n                                          self.env.action_space.shape,\n                                          cfg.replay_buffer_capacity,\n                                          self.cfg.image_pad, self.device)\n\n        self.video_recorder = VideoRecorder(\n            self.work_dir if cfg.save_video else None)\n        self.step = 0\n\n    def evaluate(self):\n        average_episode_reward = 0\n        for episode in range(self.cfg.num_eval_episodes):\n            obs = self.env.reset()\n            self.video_recorder.init(enabled=(episode == 0))\n            done = False\n            episode_reward = 0\n            episode_step = 0\n            while not done:\n                with utils.eval_mode(self.agent):\n                    action = self.agent.act(obs, sample=False)\n                obs, reward, done, info = self.env.step(action)\n                self.video_recorder.record(self.env)\n                episode_reward += reward\n                episode_step += 1\n\n            average_episode_reward += episode_reward\n            self.video_recorder.save(f\'{self.step}.mp4\')\n        average_episode_reward /= self.cfg.num_eval_episodes\n        self.logger.log(\'eval/episode_reward\', average_episode_reward,\n                        self.step)\n        self.logger.dump(self.step)\n\n    def run(self):\n        episode, episode_reward, episode_step, done = 0, 0, 1, True\n        start_time = time.time()\n        while self.step < self.cfg.num_train_steps:\n            if done:\n                if self.step > 0:\n                    self.logger.log(\'train/duration\',\n                                    time.time() - start_time, self.step)\n                    start_time = time.time()\n                    self.logger.dump(\n                        self.step, save=(self.step > self.cfg.num_seed_steps))\n\n                # evaluate agent periodically\n                if self.step % self.cfg.eval_frequency == 0:\n                    self.logger.log(\'eval/episode\', episode, self.step)\n                    self.evaluate()\n\n                self.logger.log(\'train/episode_reward\', episode_reward,\n                                self.step)\n\n                obs = self.env.reset()\n                done = False\n                episode_reward = 0\n                episode_step = 0\n                episode += 1\n\n                self.logger.log(\'train/episode\', episode, self.step)\n\n            # sample action for data collection\n            if self.step < self.cfg.num_seed_steps:\n                action = self.env.action_space.sample()\n            else:\n                with utils.eval_mode(self.agent):\n                    action = self.agent.act(obs, sample=True)\n\n            # run training update\n            if self.step >= self.cfg.num_seed_steps:\n                for _ in range(self.cfg.num_train_iters):\n                    self.agent.update(self.replay_buffer, self.logger,\n                                      self.step)\n\n            next_obs, reward, done, info = self.env.step(action)\n\n            # allow infinite bootstrap\n            done = float(done)\n            done_no_max = 0 if episode_step + 1 == self.env._max_episode_steps else done\n            episode_reward += reward\n\n            self.replay_buffer.add(obs, action, reward, next_obs, done,\n                                   done_no_max)\n\n            obs = next_obs\n            episode_step += 1\n            self.step += 1\n\n\n@hydra.main(config_path=\'config.yaml\', strict=True)\ndef main(cfg):\n    from train import Workspace as W\n    workspace = W(cfg)\n    workspace.run()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils.py,5,"b'import math\nimport os\nimport random\nfrom collections import deque\n\nimport numpy as np\nimport scipy.linalg as sp_la\n\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom skimage.util.shape import view_as_windows\nfrom torch import distributions as pyd\n\n\nclass eval_mode(object):\n    def __init__(self, *models):\n        self.models = models\n\n    def __enter__(self):\n        self.prev_states = []\n        for model in self.models:\n            self.prev_states.append(model.training)\n            model.train(False)\n\n    def __exit__(self, *args):\n        for model, state in zip(self.models, self.prev_states):\n            model.train(state)\n        return False\n\n\ndef soft_update_params(net, target_net, tau):\n    for param, target_param in zip(net.parameters(), target_net.parameters()):\n        target_param.data.copy_(tau * param.data +\n                                (1 - tau) * target_param.data)\n\n\ndef set_seed_everywhere(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\ndef make_dir(*path_parts):\n    dir_path = os.path.join(*path_parts)\n    try:\n        os.mkdir(dir_path)\n    except OSError:\n        pass\n    return dir_path\n\n\ndef tie_weights(src, trg):\n    assert type(src) == type(trg)\n    trg.weight = src.weight\n    trg.bias = src.bias\n\n\ndef weight_init(m):\n    """"""Custom weight init for Conv2D and Linear layers.""""""\n    if isinstance(m, nn.Linear):\n        nn.init.orthogonal_(m.weight.data)\n        if hasattr(m.bias, \'data\'):\n            m.bias.data.fill_(0.0)\n    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        gain = nn.init.calculate_gain(\'relu\')\n        nn.init.orthogonal_(m.weight.data, gain)\n        if hasattr(m.bias, \'data\'):\n            m.bias.data.fill_(0.0)\n\n\ndef mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None):\n    if hidden_depth == 0:\n        mods = [nn.Linear(input_dim, output_dim)]\n    else:\n        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n        for i in range(hidden_depth - 1):\n            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n        mods.append(nn.Linear(hidden_dim, output_dim))\n    if output_mod is not None:\n        mods.append(output_mod)\n    trunk = nn.Sequential(*mods)\n    return trunk\n\n\ndef to_np(t):\n    if t is None:\n        return None\n    elif t.nelement() == 0:\n        return np.array([])\n    else:\n        return t.cpu().detach().numpy()\n\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        gym.Wrapper.__init__(self, env)\n        self._k = k\n        self._frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=1,\n            shape=((shp[0] * k,) + shp[1:]),\n            dtype=env.observation_space.dtype)\n        self._max_episode_steps = env._max_episode_steps\n\n    def reset(self):\n        obs = self.env.reset()\n        for _ in range(self._k):\n            self._frames.append(obs)\n        return self._get_obs()\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self._frames.append(obs)\n        return self._get_obs(), reward, done, info\n\n    def _get_obs(self):\n        assert len(self._frames) == self._k\n        return np.concatenate(list(self._frames), axis=0)\n\n\nclass TanhTransform(pyd.transforms.Transform):\n    domain = pyd.constraints.real\n    codomain = pyd.constraints.interval(-1.0, 1.0)\n    bijective = True\n    sign = +1\n\n    def __init__(self, cache_size=1):\n        super().__init__(cache_size=cache_size)\n\n    @staticmethod\n    def atanh(x):\n        return 0.5 * (x.log1p() - (-x).log1p())\n\n    def __eq__(self, other):\n        return isinstance(other, TanhTransform)\n\n    def _call(self, x):\n        return x.tanh()\n\n    def _inverse(self, y):\n        # We do not clamp to the boundary here as it may degrade the performance of certain algorithms.\n        # one should use `cache_size=1` instead\n        return self.atanh(y)\n\n    def log_abs_det_jacobian(self, x, y):\n        # We use a formula that is more numerically stable, see details in the following link\n        # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n        return 2. * (math.log(2.) - x - F.softplus(-2. * x))\n\n\nclass SquashedNormal(pyd.transformed_distribution.TransformedDistribution):\n    def __init__(self, loc, scale):\n        self.loc = loc\n        self.scale = scale\n\n        self.base_dist = pyd.Normal(loc, scale)\n        transforms = [TanhTransform()]\n        super().__init__(self.base_dist, transforms)\n\n    @property\n    def mean(self):\n        mu = self.loc\n        for tr in self.transforms:\n            mu = tr(mu)\n        return mu'"
video.py,0,"b""import os\nimport sys\n\nimport imageio\nimport numpy as np\n\nimport utils\n\n\nclass VideoRecorder(object):\n    def __init__(self, root_dir, height=256, width=256, fps=10):\n        self.save_dir = utils.make_dir(root_dir, 'video') if root_dir else None\n        self.height = height\n        self.width = width\n        self.fps = fps\n        self.frames = []\n\n    def init(self, enabled=True):\n        self.frames = []\n        self.enabled = self.save_dir is not None and enabled\n\n    def record(self, env):\n        if self.enabled:\n            frame = env.render(mode='rgb_array',\n                               height=self.height,\n                               width=self.width)\n            self.frames.append(frame)\n\n    def save(self, file_name):\n        if self.enabled:\n            path = os.path.join(self.save_dir, file_name)\n            imageio.mimsave(path, self.frames, fps=self.fps)\n"""
