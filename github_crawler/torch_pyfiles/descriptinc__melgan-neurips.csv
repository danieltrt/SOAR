file_path,api_count,code
hubconf.py,0,"b'dependencies = [""torch"", ""librosa"", ""yaml""]\nfrom mel2wav import MelVocoder\n\n\ndef load_melgan(model_name=""multi_speaker""):\n    """"""\n    Exposes a MelVocoder Interface\n    Args:\n        model_name (str): Supports only 2 models, \'linda_johnson\' or \'multi_speaker\'\n    Returns:\n        object (MelVocoder):  MelVocoder class.\n            Default function (___call__) converts raw audio to mel\n            inverse function convert mel to raw audio using MelGAN\n    """"""\n\n    return MelVocoder(path=None, github=True, model_name=model_name)\n'"
mel2wav/__init__.py,0,"b'from mel2wav.interface import load_model, MelVocoder\n'"
mel2wav/dataset.py,4,"b'import torch\nimport torch.utils.data\nimport torch.nn.functional as F\n\nfrom librosa.core import load\nfrom librosa.util import normalize\n\nfrom pathlib import Path\nimport numpy as np\nimport random\n\n\ndef files_to_list(filename):\n    """"""\n    Takes a text file of filenames and makes a list of filenames\n    """"""\n    with open(filename, encoding=""utf-8"") as f:\n        files = f.readlines()\n\n    files = [f.rstrip() for f in files]\n    return files\n\n\nclass AudioDataset(torch.utils.data.Dataset):\n    """"""\n    This is the main class that calculates the spectrogram and returns the\n    spectrogram, audio pair.\n    """"""\n\n    def __init__(self, training_files, segment_length, sampling_rate, augment=True):\n        self.sampling_rate = sampling_rate\n        self.segment_length = segment_length\n        self.audio_files = files_to_list(training_files)\n        self.audio_files = [Path(training_files).parent / x for x in self.audio_files]\n        random.seed(1234)\n        random.shuffle(self.audio_files)\n        self.augment = augment\n\n    def __getitem__(self, index):\n        # Read audio\n        filename = self.audio_files[index]\n        audio, sampling_rate = self.load_wav_to_torch(filename)\n        # Take segment\n        if audio.size(0) >= self.segment_length:\n            max_audio_start = audio.size(0) - self.segment_length\n            audio_start = random.randint(0, max_audio_start)\n            audio = audio[audio_start : audio_start + self.segment_length]\n        else:\n            audio = F.pad(\n                audio, (0, self.segment_length - audio.size(0)), ""constant""\n            ).data\n\n        # audio = audio / 32768.0\n        return audio.unsqueeze(0)\n\n    def __len__(self):\n        return len(self.audio_files)\n\n    def load_wav_to_torch(self, full_path):\n        """"""\n        Loads wavdata into torch array\n        """"""\n        data, sampling_rate = load(full_path, sr=self.sampling_rate)\n        data = 0.95 * normalize(data)\n\n        if self.augment:\n            amplitude = np.random.uniform(low=0.3, high=1.0)\n            data = data * amplitude\n\n        return torch.from_numpy(data).float(), sampling_rate\n'"
mel2wav/interface.py,9,"b'from mel2wav.modules import Generator, Audio2Mel\n\nfrom pathlib import Path\nimport yaml\nimport torch\nimport os\n\n\ndef get_default_device():\n    if torch.cuda.is_available():\n        return ""cuda""\n    else:\n        return ""cpu""\n\n\ndef load_model(mel2wav_path, device=get_default_device()):\n    """"""\n    Args:\n        mel2wav_path (str or Path): path to the root folder of dumped text2mel\n        device (str or torch.device): device to load the model\n    """"""\n    root = Path(mel2wav_path)\n    with open(root / ""args.yml"", ""r"") as f:\n        args = yaml.load(f, Loader=yaml.FullLoader)\n    netG = Generator(args.n_mel_channels, args.ngf, args.n_residual_layers).to(device)\n    netG.load_state_dict(torch.load(root / ""best_netG.pt"", map_location=device))\n    return netG\n\n\nclass MelVocoder:\n    def __init__(\n        self,\n        path,\n        device=get_default_device(),\n        github=False,\n        model_name=""multi_speaker"",\n    ):\n        self.fft = Audio2Mel().to(device)\n        if github:\n            netG = Generator(80, 32, 3).to(device)\n            root = Path(os.path.dirname(__file__)).parent\n            netG.load_state_dict(\n                torch.load(root / f""models/{model_name}.pt"", map_location=device)\n            )\n            self.mel2wav = netG\n        else:\n            self.mel2wav = load_model(path, device)\n        self.device = device\n\n    def __call__(self, audio):\n        """"""\n        Performs audio to mel conversion (See Audio2Mel in mel2wav/modules.py)\n        Args:\n            audio (torch.tensor): PyTorch tensor containing audio (batch_size, timesteps)\n        Returns:\n            torch.tensor: log-mel-spectrogram computed on input audio (batch_size, 80, timesteps)\n        """"""\n        return self.fft(audio.unsqueeze(1).to(self.device))\n\n    def inverse(self, mel):\n        """"""\n        Performs mel2audio conversion\n        Args:\n            mel (torch.tensor): PyTorch tensor containing log-mel spectrograms (batch_size, 80, timesteps)\n        Returns:\n            torch.tensor:  Inverted raw audio (batch_size, timesteps)\n\n        """"""\n        with torch.no_grad():\n            return self.mel2wav(mel.to(self.device)).squeeze(1)\n'"
mel2wav/modules.py,9,"b'import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom librosa.filters import mel as librosa_mel_fn\nfrom torch.nn.utils import weight_norm\nimport numpy as np\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(""Conv"") != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find(""BatchNorm2d"") != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\n\ndef WNConv1d(*args, **kwargs):\n    return weight_norm(nn.Conv1d(*args, **kwargs))\n\n\ndef WNConvTranspose1d(*args, **kwargs):\n    return weight_norm(nn.ConvTranspose1d(*args, **kwargs))\n\n\nclass Audio2Mel(nn.Module):\n    def __init__(\n        self,\n        n_fft=1024,\n        hop_length=256,\n        win_length=1024,\n        sampling_rate=22050,\n        n_mel_channels=80,\n        mel_fmin=0.0,\n        mel_fmax=None,\n    ):\n        super().__init__()\n        ##############################################\n        # FFT Parameters                              #\n        ##############################################\n        window = torch.hann_window(win_length).float()\n        mel_basis = librosa_mel_fn(\n            sampling_rate, n_fft, n_mel_channels, mel_fmin, mel_fmax\n        )\n        mel_basis = torch.from_numpy(mel_basis).float()\n        self.register_buffer(""mel_basis"", mel_basis)\n        self.register_buffer(""window"", window)\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.sampling_rate = sampling_rate\n        self.n_mel_channels = n_mel_channels\n\n    def forward(self, audio):\n        p = (self.n_fft - self.hop_length) // 2\n        audio = F.pad(audio, (p, p), ""reflect"").squeeze(1)\n        fft = torch.stft(\n            audio,\n            n_fft=self.n_fft,\n            hop_length=self.hop_length,\n            win_length=self.win_length,\n            window=self.window,\n            center=False,\n        )\n        real_part, imag_part = fft.unbind(-1)\n        magnitude = torch.sqrt(real_part ** 2 + imag_part ** 2)\n        mel_output = torch.matmul(self.mel_basis, magnitude)\n        log_mel_spec = torch.log10(torch.clamp(mel_output, min=1e-5))\n        return log_mel_spec\n\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, dilation=1):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.LeakyReLU(0.2),\n            nn.ReflectionPad1d(dilation),\n            WNConv1d(dim, dim, kernel_size=3, dilation=dilation),\n            nn.LeakyReLU(0.2),\n            WNConv1d(dim, dim, kernel_size=1),\n        )\n        self.shortcut = WNConv1d(dim, dim, kernel_size=1)\n\n    def forward(self, x):\n        return self.shortcut(x) + self.block(x)\n\n\nclass Generator(nn.Module):\n    def __init__(self, input_size, ngf, n_residual_layers):\n        super().__init__()\n        ratios = [8, 8, 2, 2]\n        self.hop_length = np.prod(ratios)\n        mult = int(2 ** len(ratios))\n\n        model = [\n            nn.ReflectionPad1d(3),\n            WNConv1d(input_size, mult * ngf, kernel_size=7, padding=0),\n        ]\n\n        # Upsample to raw audio scale\n        for i, r in enumerate(ratios):\n            model += [\n                nn.LeakyReLU(0.2),\n                WNConvTranspose1d(\n                    mult * ngf,\n                    mult * ngf // 2,\n                    kernel_size=r * 2,\n                    stride=r,\n                    padding=r // 2 + r % 2,\n                    output_padding=r % 2,\n                ),\n            ]\n\n            for j in range(n_residual_layers):\n                model += [ResnetBlock(mult * ngf // 2, dilation=3 ** j)]\n\n            mult //= 2\n\n        model += [\n            nn.LeakyReLU(0.2),\n            nn.ReflectionPad1d(3),\n            WNConv1d(ngf, 1, kernel_size=7, padding=0),\n            nn.Tanh(),\n        ]\n\n        self.model = nn.Sequential(*model)\n        self.apply(weights_init)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, ndf, n_layers, downsampling_factor):\n        super().__init__()\n        model = nn.ModuleDict()\n\n        model[""layer_0""] = nn.Sequential(\n            nn.ReflectionPad1d(7),\n            WNConv1d(1, ndf, kernel_size=15),\n            nn.LeakyReLU(0.2, True),\n        )\n\n        nf = ndf\n        stride = downsampling_factor\n        for n in range(1, n_layers + 1):\n            nf_prev = nf\n            nf = min(nf * stride, 1024)\n\n            model[""layer_%d"" % n] = nn.Sequential(\n                WNConv1d(\n                    nf_prev,\n                    nf,\n                    kernel_size=stride * 10 + 1,\n                    stride=stride,\n                    padding=stride * 5,\n                    groups=nf_prev // 4,\n                ),\n                nn.LeakyReLU(0.2, True),\n            )\n\n        nf = min(nf * 2, 1024)\n        model[""layer_%d"" % (n_layers + 1)] = nn.Sequential(\n            WNConv1d(nf_prev, nf, kernel_size=5, stride=1, padding=2),\n            nn.LeakyReLU(0.2, True),\n        )\n\n        model[""layer_%d"" % (n_layers + 2)] = WNConv1d(\n            nf, 1, kernel_size=3, stride=1, padding=1\n        )\n\n        self.model = model\n\n    def forward(self, x):\n        results = []\n        for key, layer in self.model.items():\n            x = layer(x)\n            results.append(x)\n        return results\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, num_D, ndf, n_layers, downsampling_factor):\n        super().__init__()\n        self.model = nn.ModuleDict()\n        for i in range(num_D):\n            self.model[f""disc_{i}""] = NLayerDiscriminator(\n                ndf, n_layers, downsampling_factor\n            )\n\n        self.downsample = nn.AvgPool1d(4, stride=2, padding=1, count_include_pad=False)\n        self.apply(weights_init)\n\n    def forward(self, x):\n        results = []\n        for key, disc in self.model.items():\n            results.append(disc(x))\n            x = self.downsample(x)\n        return results\n'"
mel2wav/utils.py,1,"b'import scipy.io.wavfile\n\n\ndef save_sample(file_path, sampling_rate, audio):\n    """"""Helper function to save sample\n\n    Args:\n        file_path (str or pathlib.Path): save file path\n        sampling_rate (int): sampling rate of audio (usually 22050)\n        audio (torch.FloatTensor): torch array containing audio in [-1, 1]\n    """"""\n    audio = (audio.numpy() * 32768).astype(""int16"")\n    scipy.io.wavfile.write(file_path, sampling_rate, audio)\n'"
scripts/generate_from_folder.py,1,"b'from mel2wav import MelVocoder\n\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport argparse\nimport librosa\nimport torch\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--load_path"", type=Path, required=True)\n    parser.add_argument(""--save_path"", type=Path, required=True)\n    parser.add_argument(""--folder"", type=Path, required=True)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    vocoder = MelVocoder(args.load_path)\n\n    args.save_path.mkdir(exist_ok=True, parents=True)\n\n    for i, fname in tqdm(enumerate(args.folder.glob(""*.wav""))):\n        wavname = fname.name\n        wav, sr = librosa.core.load(fname)\n\n        mel, _ = vocoder(torch.from_numpy(wav)[None])\n        recons = vocoder.inverse(mel).squeeze().cpu().numpy()\n\n        librosa.output.write_wav(args.save_path / wavname, recons, sr=sr)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/train.py,18,"b'from mel2wav.dataset import AudioDataset\nfrom mel2wav.modules import Generator, Discriminator, Audio2Mel\nfrom mel2wav.utils import save_sample\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport yaml\nimport numpy as np\nimport time\nimport argparse\nfrom pathlib import Path\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--save_path"", required=True)\n    parser.add_argument(""--load_path"", default=None)\n\n    parser.add_argument(""--n_mel_channels"", type=int, default=80)\n    parser.add_argument(""--ngf"", type=int, default=32)\n    parser.add_argument(""--n_residual_layers"", type=int, default=3)\n\n    parser.add_argument(""--ndf"", type=int, default=16)\n    parser.add_argument(""--num_D"", type=int, default=3)\n    parser.add_argument(""--n_layers_D"", type=int, default=4)\n    parser.add_argument(""--downsamp_factor"", type=int, default=4)\n    parser.add_argument(""--lambda_feat"", type=float, default=10)\n    parser.add_argument(""--cond_disc"", action=""store_true"")\n\n    parser.add_argument(""--data_path"", default=None, type=Path)\n    parser.add_argument(""--batch_size"", type=int, default=16)\n    parser.add_argument(""--seq_len"", type=int, default=8192)\n\n    parser.add_argument(""--epochs"", type=int, default=3000)\n    parser.add_argument(""--log_interval"", type=int, default=100)\n    parser.add_argument(""--save_interval"", type=int, default=1000)\n    parser.add_argument(""--n_test_samples"", type=int, default=8)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    root = Path(args.save_path)\n    load_root = Path(args.load_path) if args.load_path else None\n    root.mkdir(parents=True, exist_ok=True)\n\n    ####################################\n    # Dump arguments and create logger #\n    ####################################\n    with open(root / ""args.yml"", ""w"") as f:\n        yaml.dump(args, f)\n    writer = SummaryWriter(str(root))\n\n    #######################\n    # Load PyTorch Models #\n    #######################\n    netG = Generator(args.n_mel_channels, args.ngf, args.n_residual_layers).cuda()\n    netD = Discriminator(\n        args.num_D, args.ndf, args.n_layers_D, args.downsamp_factor\n    ).cuda()\n    fft = Audio2Mel(n_mel_channels=args.n_mel_channels).cuda()\n\n    print(netG)\n    print(netD)\n\n    #####################\n    # Create optimizers #\n    #####################\n    optG = torch.optim.Adam(netG.parameters(), lr=1e-4, betas=(0.5, 0.9))\n    optD = torch.optim.Adam(netD.parameters(), lr=1e-4, betas=(0.5, 0.9))\n\n    if load_root and load_root.exists():\n        netG.load_state_dict(torch.load(load_root / ""netG.pt""))\n        optG.load_state_dict(torch.load(load_root / ""optG.pt""))\n        netD.load_state_dict(torch.load(load_root / ""netD.pt""))\n        optD.load_state_dict(torch.load(load_root / ""optD.pt""))\n\n    #######################\n    # Create data loaders #\n    #######################\n    train_set = AudioDataset(\n        Path(args.data_path) / ""train_files.txt"", args.seq_len, sampling_rate=22050\n    )\n    test_set = AudioDataset(\n        Path(args.data_path) / ""test_files.txt"",\n        22050 * 4,\n        sampling_rate=22050,\n        augment=False,\n    )\n\n    train_loader = DataLoader(train_set, batch_size=args.batch_size, num_workers=4)\n    test_loader = DataLoader(test_set, batch_size=1)\n\n    ##########################\n    # Dumping original audio #\n    ##########################\n    test_voc = []\n    test_audio = []\n    for i, x_t in enumerate(test_loader):\n        x_t = x_t.cuda()\n        s_t = fft(x_t).detach()\n\n        test_voc.append(s_t.cuda())\n        test_audio.append(x_t)\n\n        audio = x_t.squeeze().cpu()\n        save_sample(root / (""original_%d.wav"" % i), 22050, audio)\n        writer.add_audio(""original/sample_%d.wav"" % i, audio, 0, sample_rate=22050)\n\n        if i == args.n_test_samples - 1:\n            break\n\n    costs = []\n    start = time.time()\n\n    # enable cudnn autotuner to speed up training\n    torch.backends.cudnn.benchmark = True\n\n    best_mel_reconst = 1000000\n    steps = 0\n    for epoch in range(1, args.epochs + 1):\n        for iterno, x_t in enumerate(train_loader):\n            x_t = x_t.cuda()\n            s_t = fft(x_t).detach()\n            x_pred_t = netG(s_t.cuda())\n\n            with torch.no_grad():\n                s_pred_t = fft(x_pred_t.detach())\n                s_error = F.l1_loss(s_t, s_pred_t).item()\n\n            #######################\n            # Train Discriminator #\n            #######################\n            D_fake_det = netD(x_pred_t.cuda().detach())\n            D_real = netD(x_t.cuda())\n\n            loss_D = 0\n            for scale in D_fake_det:\n                loss_D += F.relu(1 + scale[-1]).mean()\n\n            for scale in D_real:\n                loss_D += F.relu(1 - scale[-1]).mean()\n\n            netD.zero_grad()\n            loss_D.backward()\n            optD.step()\n\n            ###################\n            # Train Generator #\n            ###################\n            D_fake = netD(x_pred_t.cuda())\n\n            loss_G = 0\n            for scale in D_fake:\n                loss_G += -scale[-1].mean()\n\n            loss_feat = 0\n            feat_weights = 4.0 / (args.n_layers_D + 1)\n            D_weights = 1.0 / args.num_D\n            wt = D_weights * feat_weights\n            for i in range(args.num_D):\n                for j in range(len(D_fake[i]) - 1):\n                    loss_feat += wt * F.l1_loss(D_fake[i][j], D_real[i][j].detach())\n\n            netG.zero_grad()\n            (loss_G + args.lambda_feat * loss_feat).backward()\n            optG.step()\n\n            ######################\n            # Update tensorboard #\n            ######################\n            costs.append([loss_D.item(), loss_G.item(), loss_feat.item(), s_error])\n\n            writer.add_scalar(""loss/discriminator"", costs[-1][0], steps)\n            writer.add_scalar(""loss/generator"", costs[-1][1], steps)\n            writer.add_scalar(""loss/feature_matching"", costs[-1][2], steps)\n            writer.add_scalar(""loss/mel_reconstruction"", costs[-1][3], steps)\n            steps += 1\n\n            if steps % args.save_interval == 0:\n                st = time.time()\n                with torch.no_grad():\n                    for i, (voc, _) in enumerate(zip(test_voc, test_audio)):\n                        pred_audio = netG(voc)\n                        pred_audio = pred_audio.squeeze().cpu()\n                        save_sample(root / (""generated_%d.wav"" % i), 22050, pred_audio)\n                        writer.add_audio(\n                            ""generated/sample_%d.wav"" % i,\n                            pred_audio,\n                            epoch,\n                            sample_rate=22050,\n                        )\n\n                torch.save(netG.state_dict(), root / ""netG.pt"")\n                torch.save(optG.state_dict(), root / ""optG.pt"")\n\n                torch.save(netD.state_dict(), root / ""netD.pt"")\n                torch.save(optD.state_dict(), root / ""optD.pt"")\n\n                if np.asarray(costs).mean(0)[-1] < best_mel_reconst:\n                    best_mel_reconst = np.asarray(costs).mean(0)[-1]\n                    torch.save(netD.state_dict(), root / ""best_netD.pt"")\n                    torch.save(netG.state_dict(), root / ""best_netG.pt"")\n\n                print(""Took %5.4fs to generate samples"" % (time.time() - st))\n                print(""-"" * 100)\n\n            if steps % args.log_interval == 0:\n                print(\n                    ""Epoch {} | Iters {} / {} | ms/batch {:5.2f} | loss {}"".format(\n                        epoch,\n                        iterno,\n                        len(train_loader),\n                        1000 * (time.time() - start) / args.log_interval,\n                        np.asarray(costs).mean(0),\n                    )\n                )\n                costs = []\n                start = time.time()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
