file_path,api_count,code
a3c_main.py,2,"b'import vizdoom\nimport argparse\nimport os\nos.environ[""OMP_NUM_THREADS""] = ""1""\nimport numpy as np\nimport torch\nimport torch.multiprocessing as mp\n\nimport env as grounding_env\nfrom models import A3C_LSTM_GA\nfrom a3c_train import train\nfrom a3c_test import test\n\nimport logging\n\nparser = argparse.ArgumentParser(description=\'Gated-Attention for Grounding\')\n\n# Environment arguments\nparser.add_argument(\'-l\', \'--max-episode-length\', type=int, default=30,\n                    help=\'maximum length of an episode (default: 30)\')\nparser.add_argument(\'-d\', \'--difficulty\', type=str, default=""hard"",\n                    help=""""""Difficulty of the environment,\n                    ""easy"", ""medium"" or ""hard"" (default: hard)"""""")\nparser.add_argument(\'--living-reward\', type=float, default=0,\n                    help=""""""Default reward at each time step (default: 0,\n                    change to -0.005 to encourage shorter paths)"""""")\nparser.add_argument(\'--frame-width\', type=int, default=300,\n                    help=\'Frame width (default: 300)\')\nparser.add_argument(\'--frame-height\', type=int, default=168,\n                    help=\'Frame height (default: 168)\')\nparser.add_argument(\'-v\', \'--visualize\', type=int, default=0,\n                    help=""""""Visualize the envrionment (default: 0,\n                    use 0 for faster training)"""""")\nparser.add_argument(\'--sleep\', type=float, default=0,\n                    help=""""""Sleep between frames for better\n                    visualization (default: 0)"""""")\nparser.add_argument(\'--scenario-path\', type=str, default=""maps/room.wad"",\n                    help=""""""Doom scenario file to load\n                    (default: maps/room.wad)"""""")\nparser.add_argument(\'--interactive\', type=int, default=0,\n                    help=""""""Interactive mode enables human to play\n                    (default: 0)"""""")\nparser.add_argument(\'--all-instr-file\', type=str,\n                    default=""data/instructions_all.json"",\n                    help=""""""All instructions file\n                    (default: data/instructions_all.json)"""""")\nparser.add_argument(\'--train-instr-file\', type=str,\n                    default=""data/instructions_train.json"",\n                    help=""""""Train instructions file\n                    (default: data/instructions_train.json)"""""")\nparser.add_argument(\'--test-instr-file\', type=str,\n                    default=""data/instructions_test.json"",\n                    help=""""""Test instructions file\n                    (default: data/instructions_test.json)"""""")\nparser.add_argument(\'--object-size-file\', type=str,\n                    default=""data/object_sizes.txt"",\n                    help=\'Object size file (default: data/object_sizes.txt)\')\n\n# A3C arguments\nparser.add_argument(\'--lr\', type=float, default=0.001, metavar=\'LR\',\n                    help=\'learning rate (default: 0.001)\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, metavar=\'G\',\n                    help=\'discount factor for rewards (default: 0.99)\')\nparser.add_argument(\'--tau\', type=float, default=1.00, metavar=\'T\',\n                    help=\'parameter for GAE (default: 1.00)\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'-n\', \'--num-processes\', type=int, default=4, metavar=\'N\',\n                    help=\'how many training processes to use (default: 4)\')\nparser.add_argument(\'--num-steps\', type=int, default=20, metavar=\'NS\',\n                    help=\'number of forward steps in A3C (default: 20)\')\nparser.add_argument(\'--load\', type=str, default=""0"",\n                    help=\'model path to load, 0 to not reload (default: 0)\')\nparser.add_argument(\'-e\', \'--evaluate\', type=int, default=0,\n                    help=""""""0:Train, 1:Evaluate MultiTask Generalization\n                    2:Evaluate Zero-shot Generalization (default: 0)"""""")\nparser.add_argument(\'--dump-location\', type=str, default=""./saved/"",\n                    help=\'path to dump models and log (default: ./saved/)\')\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n\n    if args.evaluate == 0:\n        args.use_train_instructions = 1\n        log_filename = ""train.log""\n    elif args.evaluate == 1:\n        args.use_train_instructions = 1\n        args.num_processes = 0\n        log_filename = ""test-MT.log""\n    elif args.evaluate == 2:\n        args.use_train_instructions = 0\n        args.num_processes = 0\n        log_filename = ""test-ZSL.log""\n    else:\n        assert False, ""Invalid evaluation type""\n\n    env = grounding_env.GroundingEnv(args)\n    args.input_size = len(env.word_to_idx)\n\n    # Setup logging\n    if not os.path.exists(args.dump_location):\n        os.makedirs(args.dump_location)\n    logging.basicConfig(filename=args.dump_location+log_filename,\n                        level=logging.INFO)\n\n    shared_model = A3C_LSTM_GA(args)\n\n    # Load the model\n    if (args.load != ""0""):\n        shared_model.load_state_dict(\n            torch.load(args.load, map_location=lambda storage, loc: storage))\n\n    shared_model.share_memory()\n\n    processes = []\n\n    # Start the test thread\n    p = mp.Process(target=test, args=(args.num_processes, args, shared_model))\n    p.start()\n    processes.append(p)\n\n    # Start the training thread(s)\n    for rank in range(0, args.num_processes):\n        p = mp.Process(target=train, args=(rank, args, shared_model))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n'"
a3c_test.py,12,"b'import numpy as np\nimport torch\nimport torch.nn.functional as F\nimport time\nimport logging\n\nimport env as grounding_env\nfrom models import A3C_LSTM_GA\n\nfrom torch.autograd import Variable\nfrom constants import *\n\n\ndef test(rank, args, shared_model):\n    torch.manual_seed(args.seed + rank)\n\n    env = grounding_env.GroundingEnv(args)\n    env.game_init()\n\n    model = A3C_LSTM_GA(args)\n\n    if (args.load != ""0""):\n        print(""Loading model ... ""+args.load)\n        model.load_state_dict(\n            torch.load(args.load, map_location=lambda storage, loc: storage))\n\n    model.eval()\n\n    (image, instruction), _, _, _ = env.reset()\n\n    # Print instruction while evaluating and visualizing\n    if args.evaluate != 0 and args.visualize == 1:\n        print(""Instruction: {} "".format(instruction))\n\n    # Getting indices of the words in the instruction\n    instruction_idx = []\n    for word in instruction.split("" ""):\n        instruction_idx.append(env.word_to_idx[word])\n    instruction_idx = np.array(instruction_idx)\n\n    image = torch.from_numpy(image).float()/255.0\n    instruction_idx = torch.from_numpy(instruction_idx).view(1, -1)\n\n    reward_sum = 0\n    done = True\n\n    start_time = time.time()\n\n    episode_length = 0\n    rewards_list = []\n    accuracy_list = []\n    episode_length_list = []\n    num_episode = 0\n    best_reward = 0.0\n    test_freq = 50\n    while True:\n        episode_length += 1\n        if done:\n            if (args.evaluate == 0):\n                model.load_state_dict(shared_model.state_dict())\n\n            cx = Variable(torch.zeros(1, 256), volatile=True)\n            hx = Variable(torch.zeros(1, 256), volatile=True)\n        else:\n            cx = Variable(cx.data, volatile=True)\n            hx = Variable(hx.data, volatile=True)\n\n        tx = Variable(torch.from_numpy(np.array([episode_length])).long(),\n                      volatile=True)\n\n        value, logit, (hx, cx) = model(\n                (Variable(image.unsqueeze(0), volatile=True),\n                 Variable(instruction_idx, volatile=True), (tx, hx, cx)))\n        prob = F.softmax(logit)\n        action = prob.max(1)[1].data.numpy()\n\n        (image, _), reward, done,  _ = env.step(action[0])\n\n        done = done or episode_length >= args.max_episode_length\n        reward_sum += reward\n\n        if done:\n            num_episode += 1\n            rewards_list.append(reward_sum)\n            # Print reward while evaluating and visualizing\n            if args.evaluate != 0 and args.visualize == 1:\n                print(""Total reward: {}"".format(reward_sum))\n\n            episode_length_list.append(episode_length)\n            if reward == CORRECT_OBJECT_REWARD:\n                accuracy = 1\n            else:\n                accuracy = 0\n            accuracy_list.append(accuracy)\n            if(len(rewards_list) >= test_freq):\n                print("" "".join([\n                    ""Time {},"".format(time.strftime(""%Hh %Mm %Ss"",\n                                      time.gmtime(time.time() - start_time))),\n                    ""Avg Reward {},"".format(np.mean(rewards_list)),\n                    ""Avg Accuracy {},"".format(np.mean(accuracy_list)),\n                    ""Avg Ep length {},"".format(np.mean(episode_length_list)),\n                    ""Best Reward {}"".format(best_reward)]))\n                logging.info("" "".join([\n                    ""Time {},"".format(time.strftime(""%Hh %Mm %Ss"",\n                                      time.gmtime(time.time() - start_time))),\n                    ""Avg Reward {},"".format(np.mean(rewards_list)),\n                    ""Avg Accuracy {},"".format(np.mean(accuracy_list)),\n                    ""Avg Ep length {},"".format(np.mean(episode_length_list)),\n                    ""Best Reward {}"".format(best_reward)]))\n                if np.mean(rewards_list) >= best_reward and args.evaluate == 0:\n                    torch.save(model.state_dict(),\n                               args.dump_location+""model_best"")\n                    best_reward = np.mean(rewards_list)\n\n                rewards_list = []\n                accuracy_list = []\n                episode_length_list = []\n            reward_sum = 0\n            episode_length = 0\n            (image, instruction), _, _, _ = env.reset()\n            # Print instruction while evaluating and visualizing\n            if args.evaluate != 0 and args.visualize == 1:\n                print(""Instruction: {} "".format(instruction))\n\n            # Getting indices of the words in the instruction\n            instruction_idx = []\n            for word in instruction.split("" ""):\n                instruction_idx.append(env.word_to_idx[word])\n            instruction_idx = np.array(instruction_idx)\n            instruction_idx = torch.from_numpy(instruction_idx).view(1, -1)\n\n        image = torch.from_numpy(image).float()/255.0\n'"
a3c_train.py,15,"b'import torch.optim as optim\nimport env as grounding_env\n\nfrom models import *\nfrom torch.autograd import Variable\n\nimport logging\n\n\ndef ensure_shared_grads(model, shared_model):\n    for param, shared_param in zip(model.parameters(),\n                                   shared_model.parameters()):\n        if shared_param.grad is not None:\n            return\n        shared_param._grad = param.grad\n\n\ndef train(rank, args, shared_model):\n    torch.manual_seed(args.seed + rank)\n\n    env = grounding_env.GroundingEnv(args)\n    env.game_init()\n\n    model = A3C_LSTM_GA(args)\n\n    if (args.load != ""0""):\n        print(str(rank) + "" Loading model ... ""+args.load)\n        model.load_state_dict(\n            torch.load(args.load, map_location=lambda storage, loc: storage))\n\n    model.train()\n\n    optimizer = optim.SGD(shared_model.parameters(), lr=args.lr)\n\n    p_losses = []\n    v_losses = []\n\n    (image, instruction), _, _, _ = env.reset()\n    instruction_idx = []\n    for word in instruction.split("" ""):\n        instruction_idx.append(env.word_to_idx[word])\n    instruction_idx = np.array(instruction_idx)\n\n    image = torch.from_numpy(image).float()/255.0\n    instruction_idx = torch.from_numpy(instruction_idx).view(1, -1)\n\n    done = True\n\n    episode_length = 0\n    num_iters = 0\n    while True:\n        # Sync with the shared model\n        model.load_state_dict(shared_model.state_dict())\n        if done:\n            episode_length = 0\n            cx = Variable(torch.zeros(1, 256))\n            hx = Variable(torch.zeros(1, 256))\n\n        else:\n            cx = Variable(cx.data)\n            hx = Variable(hx.data)\n\n        values = []\n        log_probs = []\n        rewards = []\n        entropies = []\n\n        for step in range(args.num_steps):\n            episode_length += 1\n            tx = Variable(torch.from_numpy(np.array([episode_length])).long())\n\n            value, logit, (hx, cx) = model((Variable(image.unsqueeze(0)),\n                                            Variable(instruction_idx),\n                                            (tx, hx, cx)))\n            prob = F.softmax(logit)\n            log_prob = F.log_softmax(logit)\n            entropy = -(log_prob * prob).sum(1)\n            entropies.append(entropy)\n\n            action = prob.multinomial().data\n            log_prob = log_prob.gather(1, Variable(action))\n\n            action = action.numpy()[0, 0]\n            (image, _), reward, done,  _ = env.step(action)\n\n            done = done or episode_length >= args.max_episode_length\n\n            if done:\n                (image, instruction), _, _, _ = env.reset()\n                instruction_idx = []\n                for word in instruction.split("" ""):\n                    instruction_idx.append(env.word_to_idx[word])\n                instruction_idx = np.array(instruction_idx)\n                instruction_idx = torch.from_numpy(\n                        instruction_idx).view(1, -1)\n\n            image = torch.from_numpy(image).float()/255.0\n\n            values.append(value)\n            log_probs.append(log_prob)\n            rewards.append(reward)\n\n            if done:\n                break\n\n        R = torch.zeros(1, 1)\n        if not done:\n            tx = Variable(torch.from_numpy(np.array([episode_length])).long())\n            value, _, _ = model((Variable(image.unsqueeze(0)),\n                                 Variable(instruction_idx), (tx, hx, cx)))\n            R = value.data\n\n        values.append(Variable(R))\n        policy_loss = 0\n        value_loss = 0\n        R = Variable(R)\n\n        gae = torch.zeros(1, 1)\n        for i in reversed(range(len(rewards))):\n            R = args.gamma * R + rewards[i]\n            advantage = R - values[i]\n            value_loss = value_loss + 0.5 * advantage.pow(2)\n\n            # Generalized Advantage Estimataion\n            delta_t = rewards[i] + args.gamma * \\\n                values[i + 1].data - values[i].data\n            gae = gae * args.gamma * args.tau + delta_t\n\n            policy_loss = policy_loss - \\\n                log_probs[i] * Variable(gae) - 0.01 * entropies[i]\n\n        optimizer.zero_grad()\n\n        p_losses.append(policy_loss.data[0, 0])\n        v_losses.append(value_loss.data[0, 0])\n\n        if(len(p_losses) > 1000):\n            num_iters += 1\n            print("" "".join([\n                  ""Training thread: {}"".format(rank),\n                  ""Num iters: {}K"".format(num_iters),\n                  ""Avg policy loss: {}"".format(np.mean(p_losses)),\n                  ""Avg value loss: {}"".format(np.mean(v_losses))]))\n            logging.info("" "".join([\n                  ""Training thread: {}"".format(rank),\n                  ""Num iters: {}K"".format(num_iters),\n                  ""Avg policy loss: {}"".format(np.mean(p_losses)),\n                  ""Avg value loss: {}"".format(np.mean(v_losses))]))\n            p_losses = []\n            v_losses = []\n\n        (policy_loss + 0.5 * value_loss).backward()\n        torch.nn.utils.clip_grad_norm(model.parameters(), 40)\n\n        ensure_shared_grads(model, shared_model)\n        optimizer.step()\n'"
constants.py,0,b'# Difference in size of the objects to be considered larger or smaller\nSIZE_THRESHOLD = 100\n\n# Maximum distance from the object to receive a reward\nREWARD_THRESHOLD_DISTANCE = 40\n\n# Rewards for reaching the correct and wrong objects\nCORRECT_OBJECT_REWARD = 1.0\nWRONG_OBJECT_REWARD = -0.2\n\n# Size of the map\nMAP_SIZE_X = 384\nMAP_SIZE_Y = 384\n\n# Map offsets in doom coordinates\nY_OFFSET = 320\nX_OFFSET = 0\n\n# Margin to avoid objects overlapping with the walls\nMARGIN = 32\n\n# Distance between y-coordinates of two objects in Easy and Medium environments\nOBJECT_Y_DIST = 64\n\n# X-coordinate of all objects in the Easy environment\nEASY_ENV_OBJECT_X = 256\n\n# Range of x coordinates of all objects in the Medium environment\nMEDIUM_ENV_OBJECT_X_MIN = 192\nMEDIUM_ENV_OBJECT_X_MAX = 352\n\n# Minimum distance between any two objects or an object and\n# the agent in the Hard environment\nHARD_ENV_OBJ_DIST_THRESHOLD = 90\n'
env.py,0,"b'from __future__ import print_function\nfrom time import sleep\n\nimport numpy as np\nimport collections\nimport codecs\nimport random\nimport json\n\nfrom utils.doom import *\nfrom utils.points import *\nfrom constants import *\n\nactions = [[True, False, False], [False, True, False], [False, False, True]]\n\nObjectLocation = collections.namedtuple(""ObjectLocation"", [""x"", ""y""])\nAgentLocation = collections.namedtuple(""AgentLocation"", [""x"", ""y"", ""theta""])\n\n\nclass GroundingEnv:\n    def __init__(self, args):\n        """"""Initializes the environment.\n\n        Args:\n          args: dictionary of parameters.\n        """"""\n        self.params = args\n\n        # Reading train and test instructions.\n        self.train_instructions = self.get_instr(self.params.train_instr_file)\n        self.test_instructions = self.get_instr(self.params.test_instr_file)\n\n        if self.params.use_train_instructions:\n            self.instructions = self.train_instructions\n        else:\n            self.instructions = self.test_instructions\n\n        self.word_to_idx = self.get_word_to_idx()\n        self.objects, self.object_dict = \\\n            self.get_all_objects(self.params.all_instr_file)\n        self.object_sizes = self.read_size_file(self.params.object_size_file)\n        self.objects_info = self.get_objects_info()\n\n    def game_init(self):\n        """"""Starts the doom game engine.""""""\n        game = DoomGame()\n        game = set_doom_configuration(game, self.params)\n        game.init()\n        self.game = game\n\n    def reset(self):\n        """"""Starts a new episode.\n\n        Returns:\n           state: A tuple of screen buffer state and instruction.\n           reward: Reward at that step.\n           is_final: Flag indicating terminal state.\n           extra_args: Dictionary of additional arguments/parameters.\n        """"""\n\n        self.game.new_episode()\n        self.time = 0\n\n        self.instruction, instruction_id = self.get_random_instruction()\n\n        # Retrieve the possible correct objects for the instruction.\n        correct_objects = self.get_target_objects(instruction_id)\n\n        # Since we fix the number of objects to 5.\n        self.correct_location = np.random.randint(5)\n\n        # Randomly select one correct object from the\n        # list of possible correct objects for the instruction.\n        correct_object_id = np.random.choice(correct_objects)\n        chosen_correct_object = [x for x in self.objects_info if\n                                 x.name == self.objects[correct_object_id]][0]\n\n        # Special code to handle \'largest\' and \'smallest\' since we need to\n        # compute sizes for those particular instructions.\n        if \'largest\' not in self.instruction \\\n                and \'smallest\' not in self.instruction:\n            object_ids = random.sample([x for x in range(len(self.objects))\n                                        if x not in correct_objects], 4)\n        else:\n            object_ids = self.get_candidate_objects_superlative_instr(\n                         chosen_correct_object)\n            object_ids = [self.object_dict[x] for x in object_ids]\n\n        assert len(object_ids) == 4\n\n        object_ids.insert(self.correct_location, correct_object_id)\n\n        # Get agent and object spawn locations.\n        agent_x_coordinate, agent_y_coordinate, \\\n            agent_orientation, object_x_coordinates, \\\n            object_y_coordinates = self.get_agent_and_object_positions()\n\n        self.object_coordinates = [ObjectLocation(x, y) for x, y in\n                                   zip(object_x_coordinates,\n                                       object_y_coordinates)]\n\n        # Spawn agent.\n        spawn_agent(self.game, agent_x_coordinate,\n                    agent_y_coordinate, agent_orientation)\n\n        # Spawn objects.\n        [spawn_object(self.game, object_id, pos_x, pos_y) for\n            object_id, pos_x, pos_y in\n            zip(object_ids, object_x_coordinates, object_y_coordinates)]\n\n        screen = self.game.get_state().screen_buffer\n        screen_buf = process_screen(screen, self.params.frame_height,\n                                    self.params.frame_width)\n\n        state = (screen_buf, self.instruction)\n        reward = self.get_reward()\n        is_final = False\n        extra_args = None\n\n        return state, reward, is_final, extra_args\n\n    def step(self, action_id):\n        """"""Executes an action in the environment to reach a new state.\n\n        Args:\n          action_id: An integer corresponding to the action.\n\n        Returns:\n           state: A tuple of screen buffer state and instruction.\n           reward: Reward at that step.\n           is_final: Flag indicating terminal state.\n           extra_args: Dictionary of additional arguments/parameters.\n        """"""\n        # Repeat the action for 5 frames.\n        if self.params.visualize:\n            # Render 5 frames for better visualization.\n            for _ in range(5):\n                self.game.make_action(actions[action_id], 1)\n                # Slowing down the game for better visualization.\n                sleep(self.params.sleep)\n        else:\n            self.game.make_action(actions[action_id], 5)\n\n        self.time += 1\n        reward = self.get_reward()\n\n        # End the episode if episode limit is reached or\n        # agent reached an object.\n        is_final = True if self.time == self.params.max_episode_length \\\n            or reward != self.params.living_reward else False\n\n        screen = self.game.get_state().screen_buffer\n        screen_buf = process_screen(\n            screen, self.params.frame_height, self.params.frame_width)\n\n        state = (screen_buf, self.instruction)\n\n        return state, reward, is_final, None\n\n    def close(self):\n        self.game.close()\n\n    def get_reward(self):\n        """"""Get the reward received by the agent in the last time step.""""""\n        # If agent reached the correct object, reward = +1.\n        # If agent reach a wrong object, reward = -0.2.\n        # Otherwise, reward = living_reward.\n        self.agent_x, self.agent_y = get_agent_location(self.game)\n        target_location = self.object_coordinates[self.correct_location]\n        dist = get_l2_distance(self.agent_x, self.agent_y,\n                               target_location.x, target_location.y)\n        if dist <= REWARD_THRESHOLD_DISTANCE:\n            reward = CORRECT_OBJECT_REWARD\n            return reward\n        else:\n            for i, object_location in enumerate(self.object_coordinates):\n                if i == self.correct_location:\n                    continue\n                dist = get_l2_distance(self.agent_x, self.agent_y,\n                                       object_location.x, object_location.y)\n                if dist <= REWARD_THRESHOLD_DISTANCE:\n                    reward = WRONG_OBJECT_REWARD\n                    return reward\n            reward = self.params.living_reward\n\n        return reward\n\n    def get_agent_and_object_positions(self):\n        """"""Get agent and object positions based on the difficulty\n        of the environment.\n        """"""\n        object_x_coordinates = []\n        object_y_coordinates = []\n\n        if self.params.difficulty == \'easy\':\n            # Agent location fixed in Easy.\n            agent_x_coordinate = 128\n            agent_y_coordinate = 512\n            agent_orientation = 0\n\n            # Candidate object locations are fixed in Easy.\n            object_x_coordinates = [EASY_ENV_OBJECT_X] * 5\n            for i in range(-2, 3):\n                object_y_coordinates.append(\n                    Y_OFFSET + MAP_SIZE_Y/2.0 + OBJECT_Y_DIST * i)\n\n        if self.params.difficulty == \'medium\':\n            # Agent location fixed in Medium.\n            agent_x_coordinate = 128\n            agent_y_coordinate = 512\n            agent_orientation = 0\n\n            # Generate 5 candidate object locations.\n            for i in range(-2, 3):\n                object_x_coordinates.append(np.random.randint(\n                    MEDIUM_ENV_OBJECT_X_MIN, MEDIUM_ENV_OBJECT_X_MAX))\n                object_y_coordinates.append(\n                    Y_OFFSET + MAP_SIZE_Y/2.0 + OBJECT_Y_DIST * i)\n\n        if self.params.difficulty == \'hard\':\n            # Generate 6 random locations: 1 for agent starting position\n            # and 5 for candidate objects.\n            random_locations = generate_points(HARD_ENV_OBJ_DIST_THRESHOLD,\n                                               MAP_SIZE_X - 2*MARGIN,\n                                               MAP_SIZE_Y - 2*MARGIN, 6)\n\n            agent_x_coordinate = random_locations[0][0] + X_OFFSET + MARGIN\n            agent_y_coordinate = random_locations[0][1] + Y_OFFSET + MARGIN\n            agent_orientation = np.random.randint(4)\n\n            for i in range(1, 6):\n                object_x_coordinates.append(\n                    random_locations[i][0] + X_OFFSET + MARGIN)\n                object_y_coordinates.append(\n                    random_locations[i][1] + Y_OFFSET + MARGIN)\n\n        return agent_x_coordinate, agent_y_coordinate, agent_orientation, \\\n            object_x_coordinates, object_y_coordinates\n\n    def get_candidate_objects_superlative_instr(self, correct_object):\n        \'\'\'\n        Get any possible combination of objects\n        and give the maximum size\n\n        SIZE_THRESHOLD refers to the size in terms of number of pixels so that\n        atleast there is minimum size difference between two objects for\n        instructions with superlative terms (largest and smallest)\n        These sizes are stored in ../data/object_sizes.txt\n        \'\'\'\n\n        instr_contains_color = False\n        # instr_contains_color is set if the instruction contains the color\n        # attribute (e.g.) ""Go to the largest green object"".\n        # instr_contains_color is True if the instruction doesn\'t contain the\n        # color attribute. (e.g.) ""Go to the smallest object""\n\n        instruction_words = self.instruction.split()\n        if len(instruction_words) == 6 and \\\n                instruction_words[-1] == \'object\':\n            instr_contains_color = True\n\n        output_objects = []\n\n        # For instructions like ""largest red object"", the incorrect object\n        # set could contain larger objects of other color\n\n        for obj in self.objects_info:\n            if instr_contains_color:\n                # first check color attribute\n                if correct_object.color != obj.color:\n                    output_objects.append(obj)\n\n            if instruction_words[3] == \'largest\':\n                if correct_object.absolute_size > \\\n                        obj.absolute_size + SIZE_THRESHOLD:\n                    output_objects.append(obj)\n\n            else:\n                if correct_object.absolute_size < \\\n                        obj.absolute_size - SIZE_THRESHOLD:\n                    output_objects.append(obj)\n\n        # shuffle the objects and select the top 4\n        # randomizing the objects combination\n        random.shuffle(output_objects)\n        return [x.name for x in output_objects[:4]]\n\n    def get_objects_info(self):\n        objects = []\n        objects_map = self.objects\n        for obj in objects_map:\n            split_word = split_object(obj)\n            candidate_object = DoomObject(*split_word)\n            candidate_object.absolute_size = self.object_sizes[obj]\n            objects.append(candidate_object)\n\n        return objects\n\n    def get_all_objects(self, filename):\n        objects = []\n        object_dict = {}\n        count = 0\n        instructions = self.get_instr(filename)\n        for instruction_data in instructions:\n            object_names = instruction_data[\'targets\']\n            for object_name in object_names:\n                if object_name not in objects:\n                    objects.append(object_name)\n                    object_dict[object_name] = count\n                    count += 1\n\n        return objects, object_dict\n\n    def get_target_objects(self, instr_id):\n        object_names = self.instructions[instr_id][\'targets\']\n        correct_objects = []\n        for object_name in object_names:\n            correct_objects.append(self.object_dict[object_name])\n\n        return correct_objects\n\n    def get_instr(self, filename):\n        with open(filename, \'rb\') as f:\n            instructions = json.load(f)\n        return instructions\n\n    def read_size_file(self, filename):\n        with codecs.open(filename, \'r\') as open_file:\n            lines = open_file.readlines()\n\n        object_sizes = {}\n        for i, line in enumerate(lines):\n            split_line = line.split(\'\\t\')\n            if split_line[1].strip() in self.objects:\n                object_sizes[split_line[1].strip()] = int(split_line[2])\n\n        return object_sizes\n\n    def get_random_instruction(self):\n        instruction_id = np.random.randint(len(self.instructions))\n        instruction = self.instructions[instruction_id][\'instruction\']\n\n        return instruction, instruction_id\n\n    def get_word_to_idx(self):\n        word_to_idx = {}\n        for instruction_data in self.train_instructions:\n            instruction = instruction_data[\'instruction\']\n            for word in instruction.split("" ""):\n                if word not in word_to_idx:\n                    word_to_idx[word] = len(word_to_idx)\n        return word_to_idx\n'"
env_test.py,0,"b'import vizdoom\nimport argparse\nimport env as grounding_env\nimport numpy as np\n\nparser = argparse.ArgumentParser(description=\'Grounding Environment Test\')\nparser.add_argument(\'-l\', \'--max-episode-length\', type=int, default=30,\n                    help=\'maximum length of an episode (default: 30)\')\nparser.add_argument(\'-d\', \'--difficulty\', type=str, default=""hard"",\n                    help=""""""Difficulty of the environment,\n                    ""easy"", ""medium"" or ""hard"" (default: hard)"""""")\nparser.add_argument(\'--living-reward\', type=float, default=0,\n                    help=""""""Default reward at each time step (default: 0,\n                    change to -0.005 to encourage shorter paths)"""""")\nparser.add_argument(\'--frame-width\', type=int, default=300,\n                    help=\'Frame width (default: 300)\')\nparser.add_argument(\'--frame-height\', type=int, default=168,\n                    help=\'Frame height (default: 168)\')\nparser.add_argument(\'-v\', \'--visualize\', type=int, default=1,\n                    help=""""""Visualize the envrionment (default: 1,\n                    change to 0 for faster training)"""""")\nparser.add_argument(\'--sleep\', type=float, default=0,\n                    help=""""""Sleep between frames for better\n                    visualization (default: 0)"""""")\nparser.add_argument(\'-t\', \'--use_train_instructions\', type=int, default=1,\n                    help=""""""0: Use test instructions, 1: Use train instructions\n                    (default: 1)"""""")\nparser.add_argument(\'--scenario-path\', type=str, default=""maps/room.wad"",\n                    help=""""""Doom scenario file to load\n                    (default: maps/room.wad)"""""")\nparser.add_argument(\'--interactive\', type=int, default=0,\n                    help=""""""Interactive mode enables human to play\n                    (default: 0)"""""")\nparser.add_argument(\'--all-instr-file\', type=str,\n                    default=""data/instructions_all.json"",\n                    help=""""""All instructions file\n                    (default: data/instructions_all.json)"""""")\nparser.add_argument(\'--train-instr-file\', type=str,\n                    default=""data/instructions_train.json"",\n                    help=""""""Train instructions file\n                    (default: data/instructions_train.json)"""""")\nparser.add_argument(\'--test-instr-file\', type=str,\n                    default=""data/instructions_test.json"",\n                    help=""""""Test instructions file\n                    (default: data/instructions_test.json)"""""")\nparser.add_argument(\'--object-size-file\', type=str,\n                    default=""data/object_sizes.txt"",\n                    help=\'Object size file (default: data/object_sizes.txt)\')\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    env = grounding_env.GroundingEnv(args)\n    env.game_init()\n\n    num_episodes = 0\n    rewards_per_episode = []\n    reward_sum = 0\n    is_final = 1\n    while num_episodes < 100:\n        if is_final:\n            (image, instruction), _, _, _ = env.reset()\n            print(""Instruction: {}"".format(instruction))\n\n        # Take a random action\n        (image, instruction), reward, is_final, _ = \\\n            env.step(np.random.randint(3))\n        reward_sum += reward\n\n        if is_final:\n            print(""Total Reward: {}"".format(reward_sum))\n            rewards_per_episode.append(reward_sum)\n            num_episodes += 1\n            reward_sum = 0\n            if num_episodes % 10 == 0:\n                print(""Avg Reward per Episode: {}"".format(\n                    np.mean(rewards_per_episode)))\n'"
models.py,8,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef normalized_columns_initializer(weights, std=1.0):\n    out = torch.randn(weights.size())\n    out *= std / torch.sqrt(out.pow(2).sum(1, keepdim=True).expand_as(out))\n    return out\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = np.prod(weight_shape[1:4])\n        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n    elif classname.find('Linear') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = weight_shape[1]\n        fan_out = weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n\n\nclass A3C_LSTM_GA(torch.nn.Module):\n\n    def __init__(self, args):\n        super(A3C_LSTM_GA, self).__init__()\n\n        # Image Processing\n        self.conv1 = nn.Conv2d(3, 128, kernel_size=8, stride=4)\n        self.conv2 = nn.Conv2d(128, 64, kernel_size=4, stride=2)\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=4, stride=2)\n\n        # Instruction Processing\n        self.gru_hidden_size = 256\n        self.input_size = args.input_size\n        self.embedding = nn.Embedding(self.input_size, 32)\n        self.gru = nn.GRU(32, self.gru_hidden_size)\n\n        # Gated-Attention layers\n        self.attn_linear = nn.Linear(self.gru_hidden_size, 64)\n\n        # Time embedding layer, helps in stabilizing value prediction\n        self.time_emb_dim = 32\n        self.time_emb_layer = nn.Embedding(\n                args.max_episode_length+1,\n                self.time_emb_dim)\n\n        # A3C-LSTM layers\n        self.linear = nn.Linear(64 * 8 * 17, 256)\n        self.lstm = nn.LSTMCell(256, 256)\n        self.critic_linear = nn.Linear(256 + self.time_emb_dim, 1)\n        self.actor_linear = nn.Linear(256 + self.time_emb_dim, 3)\n\n        # Initializing weights\n        self.apply(weights_init)\n        self.actor_linear.weight.data = normalized_columns_initializer(\n            self.actor_linear.weight.data, 0.01)\n        self.actor_linear.bias.data.fill_(0)\n        self.critic_linear.weight.data = normalized_columns_initializer(\n            self.critic_linear.weight.data, 1.0)\n        self.critic_linear.bias.data.fill_(0)\n\n        self.lstm.bias_ih.data.fill_(0)\n        self.lstm.bias_hh.data.fill_(0)\n        self.train()\n\n    def forward(self, inputs):\n        x, input_inst, (tx, hx, cx) = inputs\n\n        # Get the image representation\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x_image_rep = F.relu(self.conv3(x))\n\n        # Get the instruction representation\n        encoder_hidden = Variable(torch.zeros(1, 1, self.gru_hidden_size))\n        for i in range(input_inst.data.size(1)):\n            word_embedding = self.embedding(input_inst[0, i]).unsqueeze(0)\n            _, encoder_hidden = self.gru(word_embedding, encoder_hidden)\n        x_instr_rep = encoder_hidden.view(encoder_hidden.size(1), -1)\n\n        # Get the attention vector from the instruction representation\n        x_attention = F.sigmoid(self.attn_linear(x_instr_rep))\n\n        # Gated-Attention\n        x_attention = x_attention.unsqueeze(2).unsqueeze(3)\n        x_attention = x_attention.expand(1, 64, 8, 17)\n        assert x_image_rep.size() == x_attention.size()\n        x = x_image_rep*x_attention\n        x = x.view(x.size(0), -1)\n\n        # A3C-LSTM\n        x = F.relu(self.linear(x))\n        hx, cx = self.lstm(x, (hx, cx))\n        time_emb = self.time_emb_layer(tx)\n        x = torch.cat((hx, time_emb.view(-1, self.time_emb_dim)), 1)\n\n        return self.critic_linear(x), self.actor_linear(x), (hx, cx)\n"""
utils/__init__.py,0,b''
utils/doom.py,0,"b'from vizdoom import *\nimport re\nimport cv2\n\ndef set_doom_configuration(game, params):\n    game.set_doom_scenario_path(params.scenario_path)\n\n    if (params.visualize):\n        # Use a bigger screen size when visualizing\n        game.set_screen_resolution(ScreenResolution.RES_800X450)\n    else:\n        # Use a smaller screen size for faster simulation\n        game.set_screen_resolution(ScreenResolution.RES_400X225)\n\n    game.set_screen_format(ScreenFormat.RGB24)\n    game.set_depth_buffer_enabled(True)\n    game.set_labels_buffer_enabled(True)\n    game.set_automap_buffer_enabled(True)\n\n    # sets other rendering options\n    game.set_render_hud(False)\n    game.set_render_minimal_hud(False)  # if hud is enabled\n    game.set_render_crosshair(False)\n    game.set_render_weapon(False)\n    game.set_render_decals(False)\n    game.set_render_particles(False)\n    game.set_render_effects_sprites(False)\n    game.set_render_messages(False)\n    game.set_render_corpses(False)\n\n    game.add_available_button(Button.TURN_LEFT)\n    game.add_available_button(Button.TURN_RIGHT)\n    game.add_available_button(Button.MOVE_FORWARD)\n\n    game.set_episode_timeout(30000)\n\n    # makes episodes start after 10 tics\n    game.set_episode_start_time(10)\n\n    # makes the window appear (turned on by default)\n    game.set_window_visible(True if params.visualize else False)\n\n    if params.interactive:\n        game.set_mode(Mode.SPECTATOR)\n    else:\n        game.set_mode(Mode.PLAYER)\n\n    return game\n\n\ndef get_doom_coordinates(x, y):\n    return int(x) * 256 * 256, int(y) * 256 * 256\n\n\ndef get_world_coordinates(x):\n    return x / (256 * 256)\n\n\ndef get_agent_location(game):\n    x = get_world_coordinates(game.get_game_variable(GameVariable.USER3))\n    y = get_world_coordinates(game.get_game_variable(GameVariable.USER4))\n    return x, y\n\n\ndef spawn_object(game, object_id, x, y):\n    x_pos, y_pos = get_doom_coordinates(x, y)\n    # call spawn function twice because vizdoom objects are not spawned\n    # sometimes if spawned only once for some unknown reason\n    for _ in range(2):\n        game.send_game_command(""pukename spawn_object_by_id_and_location \\\n                                %i %i %i"" % (object_id, x_pos, y_pos))\n        pause_game(game, 1)\n\n\ndef spawn_agent(game, x, y, orientation):\n    x_pos, y_pos = get_doom_coordinates(x, y)\n    game.send_game_command(""pukename set_position %i %i %i"" %\n                           (x_pos, y_pos, orientation))\n\n\ndef pause_game(game, steps):\n    for i in range(1):\n        r = game.make_action([False, False, False])\n\n\ndef split_object(object_string):\n    split_word = re.findall(\'[A-Z][^A-Z]*\', object_string)\n    split_word.reverse()\n    return split_word\n\n\ndef get_l2_distance(x1, x2, y1, y2):\n    """"""\n    Computes the L2 distance between two points\n    """"""\n    return ((x1-y1)**2 + (x2-y2)**2)**0.5\n\n\ndef process_screen(screen, height, width):\n    """"""\n    Resize the screen.\n    """"""\n    if screen.shape != (3, height, width):\n        screen = cv2.resize(screen, (width, height),\n                            interpolation=cv2.INTER_AREA).transpose(2, 0, 1)\n    return screen\n\n\nclass DoomObject(object):\n    def __init__(self, *args):\n        self.name = \'\'.join(list(reversed(args)))\n        self.type = args[0]\n\n        try:\n            # Bug in Vizdoom, BlueArmor is actually red.\n            # I can see your expression ! ;-)\n            if self.name == \'BlueArmor\':\n                self.color = \'Red\'\n            else:\n                self.color = args[1]\n        except IndexError:\n            self.color = None\n\n        try:\n            self.relative_size = args[2]\n        except IndexError:\n            self.relative_size = None\n\n        try:\n            self.absolute_size = args[3]\n        except IndexError:\n            self.absolute_size = None\n'"
utils/points.py,0,"b'""""""\nThis is a python implementation of the poisson-disc algorithm.\nPoisson-disc code is borrowed from\nIHautal at https://github.com/IHautaI/poisson-disc\n\nPoisson-disc algorithm produces points in a grid,\nbut no closer to each other than a specified minimum distance\n\nFor more details about this algorithm :\nhttp://www.cs.ubc.ca/~rbridson/docs/bridson-siggraph07-poissondisk.pdf\n""""""\n\nimport random\nfrom math import sqrt, pi, sin, cos\nfrom itertools import product\n\n\ndef generate(grid):\n    """"""\n    build the grid for generating the points\n    """"""\n    def func(point):\n        new = [random.choice([random.uniform(-grid.r*2, 0),\n               random.uniform(0, grid.r*2)]) for _ in range(len(point))]\n        return tuple(new[i] + point[i] for i in range(len(point)))\n    return func\n\n\ndef generate_points(r, length, width, n_points, rand=None):\n    """"""\n    generate n_points over a grid of a given length and width\n    """"""\n    grid = Grid(r, length, width)\n    grid.generate = generate(grid)\n    if rand is None:\n        rand = (random.uniform(0, length), random.uniform(0, width))\n\n    for i in range(100):\n        data = grid.poisson(rand, n_points)\n        if len(data) != n_points:\n            continue\n        else:\n            return data\n\n\nclass Grid:\n    """"""\n    class for filling a rectangular prism of dimension >= 2\n    with poisson disc samples spaced at least r apart\n    and k attempts per active sample\n    override Grid.distance to change\n    distance metric used and get different forms\n    of \'discs\'\n    """"""\n    def __init__(self, r, *size):\n        self.r = r\n\n        self.size = size\n        self.dim = len(size)\n\n        self.cell_size = r/(sqrt(self.dim))\n\n        self.widths = [int(size[k]/self.cell_size)+1 for k in range(self.dim)]\n\n        nums = product(*(range(self.widths[k]) for k in range(self.dim)))\n\n        self.cells = {num: -1 for num in nums}\n        self.samples = []\n        self.active = []\n\n    def clear(self):\n        """"""\n        resets the grid\n        active points and\n        sample points\n        """"""\n        self.samples = []\n        self.active = []\n\n        for item in self.cells:\n            self.cells[item] = -1\n\n    def generate(self, point):\n        """"""\n        generates new points\n        in an annulus between\n        self.r, 2*self.r\n        """"""\n\n        rad = random.triangular(self.r, 2*self.r, .3*(2*self.r - self.r))\n        # was random.uniform(self.r, 2*self.r) but I think\n        # this may be closer to the correct distribution\n        # but easier to build\n\n        angs = [random.uniform(0, 2*pi)]\n\n        if self.dim > 2:\n            angs.extend(random.uniform(-pi/2, pi/2) for _ in range(self.dim-2))\n\n        angs[0] = 2*angs[0]\n\n        return self.convert(point, rad, angs)\n\n    def poisson(self, seed, k=30):\n        """"""\n        generates a set of poisson disc samples\n        """"""\n        self.clear()\n\n        self.samples.append(seed)\n        self.active.append(0)\n        self.update(seed, 0)\n\n        while len(self.samples) < k and self.active:\n\n            idx = random.choice(self.active)\n            point = self.samples[idx]\n            new_point = self.make_points(k, point)\n\n            if new_point:\n                self.samples.append(tuple(new_point))\n                self.active.append(len(self.samples)-1)\n                self.update(new_point, len(self.samples)-1)\n            else:\n                self.active.remove(idx)\n\n        return self.samples\n\n    def make_points(self, k, point):\n        """"""\n        uses generate to make up to\n        k new points, stopping\n        when it finds a good sample\n        using self.check\n        """"""\n        n = k\n\n        while n:\n            new_point = self.generate(point)\n            if self.check(point, new_point):\n                return new_point\n\n            n -= 1\n\n        return False\n\n    def check(self, point, new_point):\n        """"""\n        checks the neighbors of the point\n        and the new_point\n        against the new_point\n        returns True if none are closer than r\n        """"""\n        for i in range(self.dim):\n            if not (0 < new_point[i] < self.size[i] or\n               self.cellify(new_point) == -1):\n                return False\n\n        for item in self.neighbors(self.cellify(point)):\n            if self.distance(self.samples[item], new_point) < self.r**2:\n                return False\n\n        for item in self.neighbors(self.cellify(new_point)):\n            if self.distance(self.samples[item], new_point) < self.r**2:\n                return False\n\n        return True\n\n    def convert(self, point, rad, angs):\n        """"""\n        converts the random point\n        to rectangular coordinates\n        from radial coordinates centered\n        on the active point\n        """"""\n        new_point = [point[0] + rad*cos(angs[0]), point[1] + rad*sin(angs[0])]\n        if len(angs) > 1:\n            new_point.extend(point[i+1] + rad*sin(angs[i])\n                             for i in range(1, len(angs)))\n        return new_point\n\n    def cellify(self, point):\n        """"""\n        returns the cell in which the point falls\n        """"""\n        return tuple(point[i]//self.cell_size for i in range(self.dim))\n\n    def distance(self, tup1, tup2):\n        """"""\n        returns squared distance between two points\n        """"""\n        return sum((tup1[k] - tup2[k])**2 for k in range(self.dim))\n\n    def cell_distance(self, tup1, tup2):\n        """"""\n        returns true if the L1 distance is less than 2\n        for the two tuples\n        """"""\n        return sum(abs(tup1[k]-tup2[k]) for k in range(self.dim)) <= 2\n\n    def neighbors(self, cell):\n        """"""\n        finds all occupied cells within\n        a distance of the given point\n        """"""\n        return (self.cells[tup] for tup in self.cells\n                if self.cells[tup] != -1 and\n                self.cell_distance(cell, tup))\n\n    def update(self, point, index):\n        """"""\n        updates the grid with the new point\n        """"""\n        self.cells[self.cellify(point)] = index\n\n    def __str__(self):\n        return self.cells.__str__()\n'"
