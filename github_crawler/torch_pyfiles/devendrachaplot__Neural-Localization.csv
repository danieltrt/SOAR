file_path,api_count,code
a3c_main.py,3,"b'import argparse\nimport os\nos.environ[""OMP_NUM_THREADS""] = ""1""\nimport sys\nimport signal\nimport torch\nimport torch.multiprocessing as mp\n\nfrom maze2d import *\nfrom model import *\nfrom a3c_train import train\nfrom a3c_test import test\n\nimport logging\n\n\nparser = argparse.ArgumentParser(description=\'Active Neural Localization\')\n\n# Environment arguments\nparser.add_argument(\'-l\', \'--max-episode-length\', type=int,\n                    default=30, metavar=\'L\',\n                    help=\'maximum length of an episode (default: 30)\')\nparser.add_argument(\'-m\', \'--map-size\', type=int, default=7,\n                    help=\'\'\'m: Size of the maze m x m (default: 7),\n                            must be an odd natural number\'\'\')\n\n# A3C and model arguments\nparser.add_argument(\'--lr\', type=float, default=0.001, metavar=\'LR\',\n                    help=\'learning rate (default: 0.001)\')\nparser.add_argument(\'--num-iters\', type=int, default=1000000, metavar=\'NS\',\n                    help=\'\'\'number of training iterations per training thread\n                            (default: 10000000)\'\'\')\nparser.add_argument(\'--gamma\', type=float, default=0.99, metavar=\'G\',\n                    help=\'discount factor for rewards (default: 0.99)\')\nparser.add_argument(\'--tau\', type=float, default=1.00, metavar=\'T\',\n                    help=\'parameter for GAE (default: 1.00)\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'-n\', \'--num-processes\', type=int, default=8, metavar=\'N\',\n                    help=\'how many training processes to use (default: 8)\')\nparser.add_argument(\'--num-steps\', type=int, default=20, metavar=\'NS\',\n                    help=\'number of forward steps in A3C (default: 20)\')\nparser.add_argument(\'--hist-size\', type=int, default=5,\n                    help=\'action history size (default: 5)\')\nparser.add_argument(\'--load\', type=str, default=""0"",\n                    help=\'model path to load, 0 to not reload (default: 0)\')\nparser.add_argument(\'-e\', \'--evaluate\', type=int, default=0,\n                    help=\'0:Train, 1:Evaluate on test data (default: 0)\')\nparser.add_argument(\'-d\', \'--dump-location\', type=str, default=""./saved/"",\n                    help=\'path to dump models and log (default: ./saved/)\')\nparser.add_argument(\'-td\', \'--test-data\', type=str,\n                    default=""./test_data/m7_n1000.npy"",\n                    help=\'\'\'Test data filepath\n                            (default: ./test_data/m7_n1000.npy)\'\'\')\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    torch.manual_seed(args.seed)\n\n    if not os.path.exists(args.dump_location):\n        os.makedirs(args.dump_location)\n\n    logging.basicConfig(\n        filename=args.dump_location +\n        \'train.log\',\n        level=logging.INFO)\n\n    assert args.evaluate == 0 or args.num_processes == 0, \\\n        ""Can\'t train while evaluating, either n=0 or e=0""\n\n    shared_model = Localization_2D_A3C(args)\n\n    if args.load != ""0"":\n        shared_model.load_state_dict(torch.load(args.load))\n    shared_model.share_memory()\n\n    signal.signal(signal.SIGINT, signal.signal(signal.SIGINT, signal.SIG_IGN))\n    processes = []\n\n    p = mp.Process(target=test, args=(args.num_processes, args, shared_model))\n    p.start()\n    processes.append(p)\n\n    for rank in range(0, args.num_processes):\n        p = mp.Process(target=train, args=(rank, args, shared_model))\n        p.start()\n        processes.append(p)\n\n    try:\n        for p in processes:\n            p.join()\n    except KeyboardInterrupt:\n        print(""Stopping training. "" +\n              ""Best model stored at {}model_best"".format(args.dump_location))\n        for p in processes:\n            p.terminate()\n'"
a3c_test.py,8,"b'import time\nimport logging\n\nfrom maze2d import *\nfrom model import *\nfrom collections import deque\n\n\ndef test(rank, args, shared_model):\n    args.seed = args.seed + rank\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n\n    env = Maze2D(args)\n    action_hist_size = args.hist_size\n\n    model = Localization_2D_A3C(args)\n    if (args.load != ""0""):\n        print(""Loading model {}"".format(args.load))\n        model.load_state_dict(torch.load(args.load))\n    model.eval()\n\n    reward_sum = 0\n    episode_length = 0\n    rewards_list = []\n    accuracy_list = []\n    best_reward = 0.0\n    done = True\n\n    if args.evaluate != 0:\n        test_freq = env.test_mazes.shape[0]\n    else:\n        test_freq = 1000\n\n    start_time = time.time()\n\n    state, depth = env.reset()\n    state = torch.from_numpy(state).float()\n\n    while True:\n        episode_length += 1\n        if done:\n            if (args.evaluate == 0):\n                # Sync with the shared model\n                model.load_state_dict(shared_model.state_dict())\n\n            # filling action history with action 3 at the start of the episode\n            action_hist = deque(\n                [3] * action_hist_size,\n                maxlen=action_hist_size)\n            action_seq = []\n        else:\n            action_hist.append(action)\n\n        ax = Variable(torch.from_numpy(np.array(action_hist)),\n                      volatile=True)\n        dx = Variable(torch.from_numpy(np.array([depth])).long(),\n                      volatile=True)\n        tx = Variable(torch.from_numpy(np.array([episode_length])).long(),\n                      volatile=True)\n\n        value, logit = model(\n            (Variable(state.unsqueeze(0), volatile=True), (ax, dx, tx)))\n        prob = F.softmax(logit, dim=1)\n        action = prob.max(1)[1].data.numpy()[0]\n\n        state, reward, done, depth = env.step(action)\n\n        done = done or episode_length >= args.max_episode_length\n        reward_sum += reward\n\n        if done:\n            rewards_list.append(reward_sum)\n            if reward >= 1:\n                accuracy = 1\n            else:\n                accuracy = 0\n            accuracy_list.append(accuracy)\n\n            if(len(rewards_list) >= test_freq):\n                time_elapsed = time.gmtime(time.time() - start_time)\n                print("" "".join([\n                    ""Time: {0:0=2d}d"".format(time_elapsed.tm_mday-1),\n                    ""{},"".format(time.strftime(""%Hh %Mm %Ss"", time_elapsed)),\n                    ""Avg Reward: {0:.3f},"".format(np.mean(rewards_list)),\n                    ""Avg Accuracy: {0:.3f},"".format(np.mean(accuracy_list)),\n                    ""Best Reward: {0:.3f}"".format(best_reward)]))\n                logging.info("" "".join([\n                    ""Time: {0:0=2d}d"".format(time_elapsed.tm_mday-1),\n                    ""{},"".format(time.strftime(""%Hh %Mm %Ss"", time_elapsed)),\n                    ""Avg Reward: {0:.3f},"".format(np.mean(rewards_list)),\n                    ""Avg Accuracy: {0:.3f},"".format(np.mean(accuracy_list)),\n                    ""Best Reward: {0:.3f}"".format(best_reward)]))\n                if args.evaluate != 0:\n                    return\n                elif (np.mean(rewards_list) >= best_reward):\n                    torch.save(model.state_dict(),\n                               args.dump_location + ""model_best"")\n                    best_reward = np.mean(rewards_list)\n                rewards_list = []\n                accuracy_list = []\n\n            reward_sum = 0\n            episode_length = 0\n            state, depth = env.reset()\n\n        state = torch.from_numpy(state).float()\n'"
a3c_train.py,14,"b'import torch.optim as optim\nimport logging\n\nfrom maze2d import *\nfrom model import *\nfrom collections import deque\n\n\ndef ensure_shared_grads(model, shared_model):\n    for param, shared_param in zip(\n            model.parameters(), shared_model.parameters()):\n        if shared_param.grad is not None:\n            return\n        shared_param._grad = param.grad\n\n\ndef train(rank, args, shared_model):\n    args.seed = args.seed + rank\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n\n    env = Maze2D(args)\n    action_hist_size = args.hist_size\n\n    model = Localization_2D_A3C(args)\n    if (args.load != ""0""):\n        print(""Training thread: {}, Loading model {}"".format(rank, args.load))\n        model.load_state_dict(torch.load(args.load))\n    optimizer = optim.SGD(shared_model.parameters(), lr=args.lr)\n    model.train()\n\n    values = []\n    log_probs = []\n    p_losses = []\n    v_losses = []\n\n    episode_length = 0\n    num_iters = 0\n    done = True\n\n    state, depth = env.reset()\n    state = torch.from_numpy(state)\n    while num_iters < args.num_iters/1000:\n        # Sync with the shared model\n        model.load_state_dict(shared_model.state_dict())\n        if done:\n            # filling action history with action 3 at the start of the episode\n            action_hist = deque(\n                [3] * action_hist_size,\n                maxlen=action_hist_size)\n            episode_length = 0\n        else:\n            action_hist.append(action)\n\n        values = []\n        log_probs = []\n        rewards = []\n        entropies = []\n\n        for step in range(args.num_steps):\n            episode_length += 1\n            state = state.float()\n            ax = Variable(torch.from_numpy(np.array(action_hist)))\n            dx = Variable(torch.from_numpy(np.array([depth])).long())\n            tx = Variable(torch.from_numpy(np.array([episode_length])).long())\n\n            value, logit = model(\n                (Variable(state.unsqueeze(0)), (ax, dx, tx)))\n            prob = F.softmax(logit, dim=1)\n            log_prob = F.log_softmax(logit, dim=1)\n            entropy = -(log_prob * prob).sum(1)\n            entropies.append(entropy)\n\n            action = prob.multinomial().data\n\n            log_prob = log_prob.gather(1, Variable(action))\n\n            action = action.numpy()[0, 0]\n\n            state, reward, done, depth = env.step(action)\n            done = done or episode_length >= args.max_episode_length\n\n            if done:\n                episode_length = 0\n                state, depth = env.reset()\n\n            state = torch.from_numpy(state)\n            values.append(value)\n            log_probs.append(log_prob)\n            rewards.append(reward)\n\n            if done:\n                break\n\n        R = torch.zeros(1, 1)\n        state = state.float()\n        if not done:\n            action_hist.append(action)\n            ax = Variable(torch.from_numpy(np.array(action_hist)))\n            dx = Variable(torch.from_numpy(np.array([depth])).long())\n            tx = Variable(torch.from_numpy(np.array([episode_length])).long())\n            value, _ = model((Variable(state.unsqueeze(0)), (ax, dx, tx)))\n            R = value.data\n\n        values.append(Variable(R))\n        policy_loss = 0\n        value_loss = 0\n        R = Variable(R)\n        gae = torch.zeros(1, 1)\n        for i in reversed(range(len(rewards))):\n            R = args.gamma * R + rewards[i]\n            advantage = R - values[i]\n            value_loss = value_loss + 0.5 * advantage.pow(2)\n\n            # Generalized Advantage Estimataion\n            delta_t = rewards[i] + args.gamma * \\\n                values[i + 1].data - values[i].data\n            gae = gae * args.gamma * args.tau + delta_t\n\n            policy_loss = policy_loss - \\\n                log_probs[i] * Variable(gae) - 0.01 * entropies[i]\n\n        optimizer.zero_grad()\n\n        p_losses.append(policy_loss.data[0, 0])\n        v_losses.append(value_loss.data[0, 0])\n\n        if(len(p_losses) > 1000):\n            num_iters += 1\n            print("" "".join([\n                  ""Training thread: {:2d},"".format(rank),\n                  ""Num iters: {:4d}K,"".format(num_iters),\n                  ""Avg policy loss: {0:+.3f},"".format(np.mean(p_losses)),\n                  ""Avg value loss: {0:+.3f}"".format(np.mean(v_losses))]))\n            logging.info("" "".join([\n                  ""Training thread: {:2d},"".format(rank),\n                  ""Num iters: {:4d}K,"".format(num_iters),\n                  ""Avg policy loss: {0:+.3f},"".format(np.mean(p_losses)),\n                  ""Avg value loss: {0:+.3f}"".format(np.mean(v_losses))]))\n            p_losses = []\n            v_losses = []\n\n        (policy_loss + 0.5 * value_loss).backward()\n        torch.nn.utils.clip_grad_norm(model.parameters(), 40)\n\n        ensure_shared_grads(model, shared_model)\n        optimizer.step()\n\n    print(""Training thread {} completed"".format(rank))\n    logging.info(""Training thread {} completed"".format(rank))\n'"
generate_test_data.py,0,"b'import argparse\nimport os\nfrom maze2d import *\n\nimport logging\n\n\nparser = argparse.ArgumentParser(description=\'Generate test mazes\')\nparser.add_argument(\'-n\', \'--num-mazes\', type=int, default=1000,\n                    help=\'Number of mazes to generate (default: 1000)\')\nparser.add_argument(\'-m\', \'--map-size\', type=int, default=7,\n                    help=\'\'\'m: Size of the maze m x m (default: 7),\n                            must be an odd natural number\'\'\')\nparser.add_argument(\'-tdl\', \'--test-data-location\', type=str,\n                    default=""./test_data/"",\n                    help=\'Data location (default: ./test_data/)\')\nparser.add_argument(\'-tdf\', \'--test-data-filename\', type=str,\n                    default=""m7_n1000.npy"",\n                    help=\'Data location (default: m7_n1000.npy)\')\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    test_mazes = []\n\n    if not os.path.exists(args.test_data_location):\n        os.makedirs(args.test_data_location)\n\n    while len(test_mazes) < args.num_mazes:\n        map_design = generate_map(args.map_size)\n        position = np.array(get_random_position(map_design))\n        orientation = np.array([np.random.randint(4)])\n\n        maze = np.concatenate((map_design.flatten(), position, orientation))\n\n        # Make sure the maze doesn\'t exist in the test mazes already\n        if not any((maze == x).all() for x in test_mazes):\n            # Make sure map is not symmetric\n            if not (map_design == np.rot90(map_design)).all() and \\\n                    not (map_design == np.rot90(np.rot90(map_design))).all():\n                test_mazes.append(maze)\n\n    filepath = os.path.join(args.test_data_location, args.test_data_filename)\n    np.save(filepath, test_mazes)\n'"
maze2d.py,0,"b""import os\nimport numpy as np\nfrom utils.maze import *\nfrom utils.localization import *\n\n\nclass Maze2D(object):\n\n    def __init__(self, args):\n        self.args = args\n        self.test_mazes = np.load(args.test_data)\n        self.test_maze_idx = 0\n        return\n\n    def reset(self):\n\n        # Load a test maze during evaluation\n        if self.args.evaluate != 0:\n            maze_in_test_data = False\n            maze = self.test_mazes[self.test_maze_idx]\n            self.orientation = int(maze[-1])\n            self.position = (int(maze[-3]), int(maze[-2]))\n            self.map_design = maze[:-3].reshape(self.args.map_size,\n                                                self.args.map_size)\n            self.test_maze_idx += 1\n        else:\n            maze_in_test_data = True\n\n        # Generate a maze\n        while maze_in_test_data:\n            # Generate a map design\n            self.map_design = generate_map(self.args.map_size)\n\n            # Get random initial position and orientation of the agent\n            self.position = get_random_position(self.map_design)\n            self.orientation = np.random.randint(4)\n\n            maze = np.concatenate((self.map_design.flatten(),\n                                   np.array(self.position),\n                                   np.array([self.orientation])))\n\n            # Make sure the maze doesn't exist in the test mazes\n            if not any((maze == x).all() for x in self.test_mazes):\n                # Make sure map is not symmetric\n                if not (self.map_design ==\n                        np.rot90(self.map_design)).all() \\\n                    and not (self.map_design ==\n                             np.rot90(np.rot90(self.map_design))).all():\n                    maze_in_test_data = False\n\n        # Pre-compute likelihoods of all observations on the map for efficiency\n        self.likelihoods = get_all_likelihoods(self.map_design)\n\n        # Get current observation and likelihood\n        curr_depth = get_depth(self.map_design, self.position,\n                               self.orientation)\n        curr_likelihood = self.likelihoods[int(curr_depth) - 1]\n\n        # Posterior is just the likelihood as prior is uniform\n        self.posterior = curr_likelihood\n\n        # Renormalization of the posterior\n        self.posterior /= np.sum(self.posterior)\n        self.t = 0\n\n        # next state for the policy model\n        self.state = np.concatenate((self.posterior, np.expand_dims(\n                                     self.map_design, axis=0)), axis=0)\n        return self.state, int(curr_depth)\n\n    def step(self, action_id):\n        # Get the observation before taking the action\n        curr_depth = get_depth(self.map_design, self.position,\n                               self.orientation)\n\n        # Posterior from last step is the prior for this step\n        prior = self.posterior\n\n        # Transform the prior according to the action taken\n        prior = transition_function(prior, curr_depth, action_id)\n\n        # Calculate position and orientation after taking the action\n        self.position, self.orientation = get_next_state(\n            self.map_design, self.position, self.orientation, action_id)\n\n        # Get the observation and likelihood after taking the action\n        curr_depth = get_depth(\n            self.map_design, self.position, self.orientation)\n        curr_likelihood = self.likelihoods[int(curr_depth) - 1]\n\n        # Posterior = Prior * Likelihood\n        self.posterior = np.multiply(curr_likelihood, prior)\n\n        # Renormalization of the posterior\n        self.posterior /= np.sum(self.posterior)\n\n        # Calculate the reward\n        reward = self.posterior.max()\n\n        self.t += 1\n        if self.t == self.args.max_episode_length:\n            is_final = True\n        else:\n            is_final = False\n\n        # next state for the policy model\n        self.state = np.concatenate(\n            (self.posterior, np.expand_dims(\n                self.map_design, axis=0)), axis=0)\n\n        return self.state, reward, is_final, int(curr_depth)\n"""
model.py,7,"b""import math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef normalized_columns_initializer(weights, std=1.0):\n    out = torch.randn(weights.size())\n    out *= std / torch.sqrt(out.pow(2).sum(1, keepdim=True).expand_as(out))\n    return out\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = np.prod(weight_shape[1:4])\n        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n    elif classname.find('Linear') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = weight_shape[1]\n        fan_out = weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n\n\nclass Localization_2D_A3C(torch.nn.Module):\n\n    def __init__(self, args):\n        super(Localization_2D_A3C, self).__init__()\n\n        self.map_size = args.map_size\n\n        num_orientations = 4\n        num_actions = 3\n        n_policy_conv1_filters = 16\n        n_policy_conv2_filters = 16\n        size_policy_conv1_filters = 3\n        size_policy_conv2_filters = 3\n        self.action_emb_dim = 8\n        self.depth_emb_dim = 8\n        self.time_emb_dim = 8\n        self.action_hist_size = args.hist_size\n\n        conv_out_height = (((self.map_size - size_policy_conv1_filters) + 1) -\n                           size_policy_conv2_filters) + 1\n        conv_out_width = (((self.map_size - size_policy_conv1_filters) + 1) -\n                          size_policy_conv2_filters) + 1\n\n        self.policy_conv1 = nn.Conv2d(num_orientations + 1,\n                                      n_policy_conv1_filters,\n                                      size_policy_conv1_filters,\n                                      stride=1)\n        self.policy_conv2 = nn.Conv2d(n_policy_conv1_filters,\n                                      n_policy_conv2_filters,\n                                      size_policy_conv2_filters,\n                                      stride=1)\n\n        self.action_emb_layer = nn.Embedding(num_actions + 1,\n                                             self.action_emb_dim)\n        self.depth_emb_layer = nn.Embedding(args.map_size,\n                                            self.depth_emb_dim)\n        self.time_emb_layer = nn.Embedding(args.max_episode_length + 1,\n                                           self.time_emb_dim)\n\n        self.proj_layer = nn.Linear(\n            n_policy_conv2_filters * conv_out_height * conv_out_width, 256)\n        self.critic_linear = nn.Linear(\n            256 + self.action_emb_dim * self.action_hist_size +\n            self.depth_emb_dim + self.time_emb_dim, 1)\n        self.actor_linear = nn.Linear(\n            256 + self.action_emb_dim * self.action_hist_size +\n            self.depth_emb_dim + self.time_emb_dim, num_actions)\n\n        self.apply(weights_init)\n        self.actor_linear.weight.data = normalized_columns_initializer(\n            self.actor_linear.weight.data, 0.01)\n        self.actor_linear.bias.data.fill_(0)\n        self.critic_linear.weight.data = normalized_columns_initializer(\n            self.critic_linear.weight.data, 1.0)\n        self.critic_linear.bias.data.fill_(0)\n\n        self.train()\n\n    def forward(self, inputs):\n        inputs, (ax, dx, tx) = inputs\n        conv_out = F.elu(self.policy_conv1(inputs))\n        conv_out = F.elu(self.policy_conv2(conv_out))\n        conv_out = conv_out.view(conv_out.size(0), -1)\n        proj = self.proj_layer(conv_out)\n        action_emb = self.action_emb_layer(ax)\n        depth_emb = self.depth_emb_layer(dx)\n        time_emb = self.time_emb_layer(tx)\n        x = torch.cat((\n            proj,\n            action_emb.view(-1, self.action_emb_dim * self.action_hist_size),\n            depth_emb.view(-1, self.depth_emb_dim),\n            time_emb.view(-1, self.time_emb_dim)), 1)\n\n        return self.critic_linear(x), self.actor_linear(x)\n"""
utils/__init__.py,0,b''
utils/localization.py,0,"b'import os\nimport numpy as np\n\nfrom .maze import *\n\n\ndef transition_function(belief_map, depth, action):\n    (o, m, n) = belief_map.shape\n    if action == \'TURN_RIGHT\' or action == 1:\n        belief_map = np.append(belief_map,\n                               belief_map[0, :, :]\n                               ).reshape(o + 1, m, n)\n        belief_map = np.delete(belief_map, 0, axis=0)\n    elif action == ""TURN_LEFT"" or action == 0:\n        belief_map = np.insert(belief_map, 0,\n                               belief_map[-1, :, :],\n                               axis=0).reshape(o + 1, m, n)\n        belief_map = np.delete(belief_map, -1, axis=0)\n    elif action == ""MOVE_FORWARD"" or action == 2:\n        if depth != 1:\n            new_belief = np.zeros(belief_map.shape)\n            for orientation in range(belief_map.shape[0]):\n                B = belief_map[orientation]\n                Bcap = shift_belief(B, orientation)\n                new_belief[orientation, :, :] = Bcap\n            belief_map = new_belief\n    return belief_map\n\n\ndef get_all_likelihoods(map_design):\n    num_orientations = 4\n    all_likelihoods = np.zeros(\n        [map_design.shape[0] - 2, num_orientations,\n         map_design.shape[0], map_design.shape[1]])\n    for orientation in range(num_orientations):\n        for i, element in np.ndenumerate(all_likelihoods[0, orientation]):\n            depth = get_depth(map_design, i, orientation)\n            if depth > 0:\n                all_likelihoods[int(depth) - 1][orientation][i] += 1\n    return all_likelihoods\n\n\ndef shift_belief(B, orientation):\n    if orientation == 0 or orientation == ""east"":\n        Bcap = np.insert(\n            B, 0, np.zeros(\n                B.shape[1]), axis=1).reshape(\n            B.shape[0], B.shape[1] + 1)\n        Bcap = np.delete(Bcap, -1, axis=1)\n    elif orientation == 2 or orientation == ""west"":\n        Bcap = np.append(B, np.zeros([B.shape[1], 1]), axis=1).reshape(\n            B.shape[0], B.shape[1] + 1)\n        Bcap = np.delete(Bcap, 0, axis=1)\n    elif orientation == 1 or orientation == ""north"":\n        Bcap = np.append(B, np.zeros([1, B.shape[1]]), axis=0).reshape(\n            B.shape[0] + 1, B.shape[1])\n        Bcap = np.delete(Bcap, 0, axis=0)\n    elif orientation == 3 or orientation == ""south"":\n        Bcap = np.insert(\n            B, 0, np.zeros(\n                B.shape[1]), axis=0).reshape(\n            B.shape[0] + 1, B.shape[1])\n        Bcap = np.delete(Bcap, -1, axis=0)\n    else:\n        assert False, ""Invalid orientation""\n    return Bcap\n'"
utils/maze.py,0,"b'import numpy as np\nimport numpy.random as npr\nimport itertools\n\n\ndef generate_map(maze_size, decimation=0.):\n    """"""\n    Generates a maze using Kruskal\'s algorithm\n    https://en.wikipedia.org/wiki/Maze_generation_algorithm\n    """"""\n    m = (maze_size - 1)//2\n    n = (maze_size - 1)//2\n\n    maze = np.ones((maze_size, maze_size))\n    for i, j in list(itertools.product(range(m), range(n))):\n        maze[2*i+1, 2*j+1] = 0\n    m = m - 1\n    L = np.arange(n+1)\n    R = np.arange(n)\n    L[n] = n-1\n\n    while m > 0:\n        for i in range(n):\n            j = L[i+1]\n            if (i != j and npr.randint(3) != 0):\n                R[j] = R[i]\n                L[R[j]] = j\n                R[i] = i + 1\n                L[R[i]] = i\n                maze[2*(n-m)-1, 2*i+2] = 0\n            if (i != L[i] and npr.randint(3) != 0):\n                L[R[i]] = L[i]\n                R[L[i]] = R[i]\n                L[i] = i\n                R[i] = i\n            else:\n                maze[2*(n-m), 2*i+1] = 0\n        m -= 1\n\n    for i in range(n):\n        j = L[i+1]\n        if (i != j and (i == L[i] or npr.randint(3) != 0)):\n            R[j] = R[i]\n            L[R[j]] = j\n            R[i] = i+1\n            L[R[i]] = i\n            maze[2*(n-m)-1, 2*i+2] = 0\n        L[R[i]] = L[i]\n        R[L[i]] = R[i]\n        L[i] = i\n        R[i] = i\n    return maze\n\n\ndef get_depth(map_design, position, orientation):\n    m, n = map_design.shape\n    depth = 0\n    new_tuple = position\n    while(compare_tuples(new_tuple, tuple([m - 1, n - 1])) and\n            compare_tuples(tuple([0, 0]), new_tuple)):\n        if map_design[new_tuple] != 0:\n            break\n        else:\n            new_tuple = get_tuple(new_tuple, orientation)\n            depth += 1\n    return depth\n\n\ndef get_next_state(map_design, position, orientation, action):\n    m, n = map_design.shape\n    if action == \'TURN_LEFT\' or action == 0:\n        orientation = (orientation + 1) % 4\n    elif action == ""TURN_RIGHT"" or action == 1:\n        orientation = (orientation - 1) % 4\n    elif action == ""MOVE_FORWARD"" or action == 2:\n        new_tuple = get_tuple(position, orientation)\n        if compare_tuples(new_tuple, tuple([m - 1, n - 1])) and \\\n           compare_tuples(tuple([0, 0]), new_tuple) and \\\n           map_design[new_tuple] == 0:\n            position = new_tuple\n    return position, orientation\n\n\ndef get_random_position(map_design):\n    m, n = map_design.shape\n    while True:\n        index = tuple([np.random.randint(m), np.random.randint(n)])\n        if map_design[index] == 0:\n            return index\n\n\ndef get_tuple(i, orientation):\n    if orientation == 0 or orientation == ""east"":\n        new_tuple = tuple([i[0], i[1] + 1])\n    elif orientation == 2 or orientation == ""west"":\n        new_tuple = tuple([i[0], i[1] - 1])\n    elif orientation == 1 or orientation == ""north"":\n        new_tuple = tuple([i[0] - 1, i[1]])\n    elif orientation == 3 or orientation == ""south"":\n        new_tuple = tuple([i[0] + 1, i[1]])\n    else:\n        assert False, ""Invalid orientation""\n    return new_tuple\n\n\ndef compare_tuples(a, b):\n    """"""\n    Returns true if all elements of a are less than\n    or equal to b\n    """"""\n    assert len(a) == len(b), ""Unequal lengths of tuples for comparison""\n    for i in range(len(a)):\n        if a[i] > b[i]:\n            return False\n    return True\n\n\nif __name__ == \'__main__\':\n    print(generate_map(7))\n'"
