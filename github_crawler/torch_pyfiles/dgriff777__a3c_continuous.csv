file_path,api_count,code
environment.py,0,"b'from __future__ import division\nimport gym\nimport numpy as np\nfrom collections import deque\nfrom gym import spaces\n\n\ndef create_env(env_id, args):\n    env = gym.make(env_id)\n    env = frame_stack(env, args)\n    return env\n\n\nclass frame_stack(gym.Wrapper):\n    def __init__(self, env, args):\n        super(frame_stack, self).__init__(env)\n        self.stack_frames = args.stack_frames\n        self.frames = deque([], maxlen=self.stack_frames)\n        self.obs_norm = MaxMinFilter() #NormalizedEnv() alternative or can just not normalize observations as environment is already kinda normalized\n\n\n    def reset(self):\n        ob = self.env.reset()\n        ob = np.float32(ob)\n        ob = self.obs_norm(ob)\n        for _ in range(self.stack_frames):\n            self.frames.append(ob)\n        return self.observation()\n\n    def step(self, action):\n        ob, rew, done, info = self.env.step(action)\n        ob = np.float32(ob)\n        ob = self.obs_norm(ob)\n        self.frames.append(ob)\n        return self.observation(), rew, done, info\n\n    def observation(self):\n        assert len(self.frames) == self.stack_frames\n        return np.stack(self.frames, axis=0)\n\n\nclass MaxMinFilter():\n    def __init__(self):\n        self.mx_d = 3.15\n        self.mn_d = -3.15\n        self.new_maxd = 10.0\n        self.new_mind = -10.0\n\n    def __call__(self, x):\n        obs = x.clip(self.mn_d, self.mx_d)\n        new_obs = (((obs - self.mn_d) * (self.new_maxd - self.new_mind)\n                    ) / (self.mx_d - self.mn_d)) + self.new_mind\n        return new_obs\n\n\nclass NormalizedEnv():\n    def __init__(self):\n        self.state_mean = 0\n        self.state_std = 0\n        self.alpha = 0.9999\n        self.num_steps = 0\n\n    def __call__(self, observation):\n        self.num_steps += 1\n        self.state_mean = self.state_mean * self.alpha + \\\n            observation.mean() * (1 - self.alpha)\n        self.state_std = self.state_std * self.alpha + \\\n            observation.std() * (1 - self.alpha)\n\n        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n\n        return (observation - unbiased_mean) / (unbiased_std + 1e-8)\n\n'"
gym_eval.py,9,"b'from __future__ import division\nimport os\nos.environ[""OMP_NUM_THREADS""] = ""1""\nimport argparse\nimport torch\nfrom environment import create_env\nfrom utils import setup_logger\nfrom model import A3C_CONV, A3C_MLP\nfrom player_util import Agent\nfrom torch.autograd import Variable\nimport gym\nimport logging\n\n\nparser = argparse.ArgumentParser(description=\'A3C_EVAL\')\nparser.add_argument(\n    \'--env\',\n    default=\'BipedalWalker-v2\',\n    metavar=\'ENV\',\n    help=\'environment to train on (default: BipedalWalker-v2)\')\nparser.add_argument(\n    \'--num-episodes\',\n    type=int,\n    default=100,\n    metavar=\'NE\',\n    help=\'how many episodes in evaluation (default: 100)\')\nparser.add_argument(\n    \'--load-model-dir\',\n    default=\'trained_models/\',\n    metavar=\'LMD\',\n    help=\'folder to load trained models from\')\nparser.add_argument(\n    \'--log-dir\', default=\'logs/\', metavar=\'LG\', help=\'folder to save logs\')\nparser.add_argument(\n    \'--render\',\n    default=False,\n    metavar=\'R\',\n    help=\'Watch game as it being played\')\nparser.add_argument(\n    \'--render-freq\',\n    type=int,\n    default=1,\n    metavar=\'RF\',\n    help=\'Frequency to watch rendered game play\')\nparser.add_argument(\n    \'--max-episode-length\',\n    type=int,\n    default=100000,\n    metavar=\'M\',\n    help=\'maximum length of an episode (default: 100000)\')\nparser.add_argument(\n    \'--model\',\n    default=\'MLP\',\n    metavar=\'M\',\n    help=\'Model type to use\')\nparser.add_argument(\n    \'--stack-frames\',\n    type=int,\n    default=1,\n    metavar=\'SF\',\n    help=\'Choose whether to stack observations\')\nparser.add_argument(\n    \'--new-gym-eval\',\n    default=False,\n    metavar=\'NGE\',\n    help=\'Create a gym evaluation for upload\')\nparser.add_argument(\n    \'--seed\',\n    type=int,\n    default=1,\n    metavar=\'S\',\n    help=\'random seed (default: 1)\')\nparser.add_argument(\n    \'--gpu-id\',\n    type=int,\n    default=-1,\n    help=\'GPU to use [-1 CPU only] (default: -1)\')\nargs = parser.parse_args()\n\ntorch.set_default_tensor_type(\'torch.FloatTensor\')\n\nsaved_state = torch.load(\n    \'{0}{1}.dat\'.format(args.load_model_dir, args.env),\n    map_location=lambda storage, loc: storage)\n\nlog = {}\nsetup_logger(\'{}_mon_log\'.format(args.env), r\'{0}{1}_mon_log\'.format(\n    args.log_dir, args.env))\nlog[\'{}_mon_log\'.format(args.env)] = logging.getLogger(\n    \'{}_mon_log\'.format(args.env))\n\ngpu_id = args.gpu_id\n\ntorch.manual_seed(args.seed)\nif gpu_id >= 0:\n    torch.cuda.manual_seed(args.seed)\n\n\nd_args = vars(args)\nfor k in d_args.keys():\n    log[\'{}_mon_log\'.format(args.env)].info(\'{0}: {1}\'.format(k, d_args[k]))\n\nenv = create_env(""{}"".format(args.env), args)\nnum_tests = 0\nreward_total_sum = 0\nplayer = Agent(None, env, args, None)\nif args.model == \'MLP\':\n    player.model = A3C_MLP(env.observation_space.shape[0], env.action_space, args.stack_frames)\nif args.model == \'CONV\':\n    player.model = A3C_CONV(args.stack_frames, env.action_space)\n\nplayer.gpu_id = gpu_id\nif gpu_id >= 0:\n    with torch.cuda.device(gpu_id):\n        player.model = player.model.cuda()\nif args.new_gym_eval:\n    player.env = gym.wrappers.Monitor(\n        player.env, ""{}_monitor"".format(args.env), force=True)\n\nif gpu_id >= 0:\n    with torch.cuda.device(gpu_id):\n        player.model.load_state_dict(saved_state)\nelse:\n    player.model.load_state_dict(saved_state)\n\nplayer.model.eval()\nfor i_episode in range(args.num_episodes):\n    player.state = player.env.reset()\n    player.state = torch.from_numpy(player.state).float()\n    if gpu_id >= 0:\n        with torch.cuda.device(gpu_id):\n            player.state = player.state.cuda()\n    player.eps_len = 0\n    reward_sum = 0\n    while True:\n        if args.render:\n            if i_episode % args.render_freq == 0:\n                player.env.render()\n\n        player.action_test()\n        reward_sum += player.reward\n\n        if player.done:\n            num_tests += 1\n            reward_total_sum += reward_sum\n            reward_mean = reward_total_sum / num_tests\n            log[\'{}_mon_log\'.format(args.env)].info(\n                ""Episode_length, {0}, reward_sum, {1}, reward_mean, {2:.4f}"".format(player.eps_len, reward_sum, reward_mean))\n            break\n'"
main.py,4,"b'from __future__ import print_function, division\nimport os\nos.environ[""OMP_NUM_THREADS""] = ""1""\nimport argparse\nimport torch\nimport torch.multiprocessing as mp\nfrom environment import create_env\nfrom model import A3C_MLP, A3C_CONV\nfrom train import train\nfrom test import test\nfrom shared_optim import SharedRMSprop, SharedAdam\nimport time\n\n\nparser = argparse.ArgumentParser(description=\'A3C\')\nparser.add_argument(\n    \'--lr\',\n    type=float,\n    default=0.0001,\n    metavar=\'LR\',\n    help=\'learning rate (default: 0.0001)\')\nparser.add_argument(\n    \'--gamma\',\n    type=float,\n    default=0.99,\n    metavar=\'G\',\n    help=\'discount factor for rewards (default: 0.99)\')\nparser.add_argument(\n    \'--tau\',\n    type=float,\n    default=1.00,\n    metavar=\'T\',\n    help=\'parameter for GAE (default: 1.00)\')\nparser.add_argument(\n    \'--seed\',\n    type=int,\n    default=1,\n    metavar=\'S\',\n    help=\'random seed (default: 1)\')\nparser.add_argument(\n    \'--workers\',\n    type=int,\n    default=32,\n    metavar=\'W\',\n    help=\'how many training processes to use (default: 32)\')\nparser.add_argument(\n    \'--num-steps\',\n    type=int,\n    default=20,\n    metavar=\'NS\',\n    help=\'number of forward steps in A3C (default: 300)\')\nparser.add_argument(\n    \'--max-episode-length\',\n    type=int,\n    default=10000,\n    metavar=\'M\',\n    help=\'maximum length of an episode (default: 10000)\')\nparser.add_argument(\n    \'--env\',\n    default=\'BipedalWalker-v2\',\n    metavar=\'ENV\',\n    help=\'environment to train on (default: BipedalWalker-v2)\')\nparser.add_argument(\n    \'--shared-optimizer\',\n    default=True,\n    metavar=\'SO\',\n    help=\'use an optimizer without shared statistics.\')\nparser.add_argument(\n    \'--load\',\n    default=False,\n    metavar=\'L\',\n    help=\'load a trained model\')\nparser.add_argument(\n    \'--save-max\',\n    default=True,\n    metavar=\'SM\',\n    help=\'Save model on every test run high score matched or bested\')\nparser.add_argument(\n    \'--optimizer\',\n    default=\'Adam\',\n    metavar=\'OPT\',\n    help=\'shares optimizer choice of Adam or RMSprop\')\nparser.add_argument(\n    \'--load-model-dir\',\n    default=\'trained_models/\',\n    metavar=\'LMD\',\n    help=\'folder to load trained models from\')\nparser.add_argument(\n    \'--save-model-dir\',\n    default=\'trained_models/\',\n    metavar=\'SMD\',\n    help=\'folder to save trained models\')\nparser.add_argument(\n    \'--log-dir\',\n    default=\'logs/\',\n    metavar=\'LG\',\n    help=\'folder to save logs\')\nparser.add_argument(\n    \'--model\',\n    default=\'MLP\',\n    metavar=\'M\',\n    help=\'Model type to use\')\nparser.add_argument(\n    \'--stack-frames\',\n    type=int,\n    default=1,\n    metavar=\'SF\',\n    help=\'Choose number of observations to stack\')\nparser.add_argument(\n    \'--gpu-ids\',\n    type=int,\n    default=-1,\n    nargs=\'+\',\n    help=\'GPUs to use [-1 CPU only] (default: -1)\')\nparser.add_argument(\n    \'--amsgrad\',\n    default=True,\n    metavar=\'AM\',\n    help=\'Adam optimizer amsgrad parameter\')\n\n\n# Based on\n# https://github.com/pytorch/examples/tree/master/mnist_hogwild\n# Training settings\n# Implemented multiprocessing using locks but was not beneficial. Hogwild\n# training was far superior\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    torch.manual_seed(args.seed)\n    if args.gpu_ids == -1:\n        args.gpu_ids = [-1]\n    else:\n        torch.cuda.manual_seed(args.seed)\n        mp.set_start_method(\'spawn\')\n    env = create_env(args.env, args)\n    if args.model == \'MLP\':\n        shared_model = A3C_MLP(\n            env.observation_space.shape[0], env.action_space, args.stack_frames)\n    if args.model == \'CONV\':\n        shared_model = A3C_CONV(args.stack_frames, env.action_space)\n    if args.load:\n        saved_state = torch.load(\'{0}{1}.dat\'.format(\n            args.load_model_dir, args.env), map_location=lambda storage, loc: storage)\n        shared_model.load_state_dict(saved_state)\n    shared_model.share_memory()\n\n    if args.shared_optimizer:\n        if args.optimizer == \'RMSprop\':\n            optimizer = SharedRMSprop(shared_model.parameters(), lr=args.lr)\n        if args.optimizer == \'Adam\':\n            optimizer = SharedAdam(\n                shared_model.parameters(), lr=args.lr, amsgrad=args.amsgrad)\n        optimizer.share_memory()\n    else:\n        optimizer = None\n\n    processes = []\n\n    p = mp.Process(target=test, args=(args, shared_model))\n    p.start()\n    processes.append(p)\n    time.sleep(0.1)\n    for rank in range(0, args.workers):\n        p = mp.Process(target=train, args=(\n            rank, args, shared_model, optimizer))\n        p.start()\n        processes.append(p)\n        time.sleep(0.1)\n    for p in processes:\n        time.sleep(0.1)\n        p.join()\n'"
model.py,6,"b""from __future__ import division\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom utils import norm_col_init, weights_init, weights_init_mlp\n\n\nclass A3C_CONV(torch.nn.Module):\n    def __init__(self, num_inputs, action_space):\n        super(A3C_CONV, self).__init__()\n        self.conv1 = nn.Conv1d(num_inputs, 32, 3, stride=1, padding=1)\n        self.lrelu1 = nn.LeakyReLU(0.1)\n        self.conv2 = nn.Conv1d(32, 32, 3, stride=1, padding=1)\n        self.lrelu2 = nn.LeakyReLU(0.1)\n        self.conv3 = nn.Conv1d(32, 64, 2, stride=1, padding=1)\n        self.lrelu3 = nn.LeakyReLU(0.1)\n        self.conv4 = nn.Conv1d(64, 64, 1, stride=1)\n        self.lrelu4 = nn.LeakyReLU(0.1)\n\n        self.lstm = nn.LSTMCell(1600, 128)\n        num_outputs = action_space.shape[0]\n        self.critic_linear = nn.Linear(128, 1)\n        self.actor_linear = nn.Linear(128, num_outputs)\n        self.actor_linear2 = nn.Linear(128, num_outputs)\n\n        self.apply(weights_init)\n        lrelu_gain = nn.init.calculate_gain('leaky_relu')\n        self.conv1.weight.data.mul_(lrelu_gain)\n        self.conv2.weight.data.mul_(lrelu_gain)\n        self.conv3.weight.data.mul_(lrelu_gain)\n        self.conv4.weight.data.mul_(lrelu_gain)\n\n        self.actor_linear.weight.data = norm_col_init(\n            self.actor_linear.weight.data, 0.01)\n        self.actor_linear.bias.data.fill_(0)\n        self.actor_linear2.weight.data = norm_col_init(\n            self.actor_linear2.weight.data, 0.01)\n        self.actor_linear2.bias.data.fill_(0)\n        self.critic_linear.weight.data = norm_col_init(\n            self.critic_linear.weight.data, 1.0)\n        self.critic_linear.bias.data.fill_(0)\n\n        self.lstm.bias_ih.data.fill_(0)\n        self.lstm.bias_hh.data.fill_(0)\n\n        self.train()\n\n    def forward(self, inputs):\n        x, (hx, cx) = inputs\n\n        x = self.lrelu1(self.conv1(x))\n        x = self.lrelu2(self.conv2(x))\n        x = self.lrelu3(self.conv3(x))\n        x = self.lrelu4(self.conv4(x))\n\n        x = x.view(x.size(0), -1)\n        hx, cx = self.lstm(x, (hx, cx))\n        x = hx\n\n        return self.critic_linear(x), F.softsign(self.actor_linear(x)), self.actor_linear2(x), (hx, cx)\n\n\nclass A3C_MLP(torch.nn.Module):\n    def __init__(self, num_inputs, action_space, n_frames):\n        super(A3C_MLP, self).__init__()\n        self.fc1 = nn.Linear(num_inputs, 256)\n        self.lrelu1 = nn.LeakyReLU(0.1)\n        self.fc2 = nn.Linear(256, 256)\n        self.lrelu2 = nn.LeakyReLU(0.1)\n        self.fc3 = nn.Linear(256, 128)\n        self.lrelu3 = nn.LeakyReLU(0.1)\n        self.fc4 = nn.Linear(128, 128)\n        self.lrelu4 = nn.LeakyReLU(0.1)\n\n        self.m1 = n_frames * 128\n        self.lstm = nn.LSTMCell(self.m1, 128)\n        num_outputs = action_space.shape[0]\n        self.critic_linear = nn.Linear(128, 1)\n        self.actor_linear = nn.Linear(128, num_outputs)\n        self.actor_linear2 = nn.Linear(128, num_outputs)\n\n        self.apply(weights_init_mlp)\n        lrelu = nn.init.calculate_gain('leaky_relu')\n        self.fc1.weight.data.mul_(lrelu)\n        self.fc2.weight.data.mul_(lrelu)\n        self.fc3.weight.data.mul_(lrelu)\n        self.fc4.weight.data.mul_(lrelu)\n\n        self.actor_linear.weight.data = norm_col_init(\n            self.actor_linear.weight.data, 0.01)\n        self.actor_linear.bias.data.fill_(0)\n        self.actor_linear2.weight.data = norm_col_init(\n            self.actor_linear2.weight.data, 0.01)\n        self.actor_linear2.bias.data.fill_(0)\n        self.critic_linear.weight.data = norm_col_init(\n            self.critic_linear.weight.data, 1.0)\n        self.critic_linear.bias.data.fill_(0)\n\n        self.lstm.bias_ih.data.fill_(0)\n        self.lstm.bias_hh.data.fill_(0)\n\n        self.train()\n\n    def forward(self, inputs):\n        x, (hx, cx) = inputs\n\n        x = self.lrelu1(self.fc1(x))\n        x = self.lrelu2(self.fc2(x))\n        x = self.lrelu3(self.fc3(x))\n        x = self.lrelu4(self.fc4(x))\n\n        x = x.view(1, self.m1)\n        hx, cx = self.lstm(x, (hx, cx))\n        x = hx\n\n        return self.critic_linear(x), F.softsign(self.actor_linear(x)), self.actor_linear2(x), (hx, cx)\n"""
player_util.py,18,"b""from __future__ import division\nimport math\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom utils import normal  # , pi\n\n\nclass Agent(object):\n    def __init__(self, model, env, args, state):\n        self.model = model\n        self.env = env\n        self.state = state\n        self.hx = None\n        self.cx = None\n        self.eps_len = 0\n        self.args = args\n        self.values = []\n        self.log_probs = []\n        self.rewards = []\n        self.entropies = []\n        self.done = True\n        self.info = None\n        self.reward = 0\n        self.gpu_id = -1\n\n    def action_train(self):\n        if self.args.model == 'CONV':\n            self.state = self.state.unsqueeze(0)\n        value, mu, sigma, (self.hx, self.cx) = self.model(\n            (Variable(self.state), (self.hx, self.cx)))\n        mu = torch.clamp(mu, -1.0, 1.0)\n        sigma = F.softplus(sigma) + 1e-5\n        eps = torch.randn(mu.size())\n        pi = np.array([math.pi])\n        pi = torch.from_numpy(pi).float()\n        if self.gpu_id >= 0:\n            with torch.cuda.device(self.gpu_id):\n                eps = Variable(eps).cuda()\n                pi = Variable(pi).cuda()\n        else:\n            eps = Variable(eps)\n            pi = Variable(pi)\n\n        action = (mu + sigma.sqrt() * eps).data\n        act = Variable(action)\n        prob = normal(act, mu, sigma, self.gpu_id, gpu=self.gpu_id >= 0)\n        action = torch.clamp(action, -1.0, 1.0)\n        entropy = 0.5 * ((sigma * 2 * pi.expand_as(sigma)).log() + 1)\n        self.entropies.append(entropy)\n        log_prob = (prob + 1e-6).log()\n        self.log_probs.append(log_prob)\n        state, reward, self.done, self.info = self.env.step(\n            action.cpu().numpy()[0])\n        reward = max(min(float(reward), 1.0), -1.0)\n        self.state = torch.from_numpy(state).float()\n        if self.gpu_id >= 0:\n            with torch.cuda.device(self.gpu_id):\n                self.state = self.state.cuda()\n        self.eps_len += 1\n        self.done = self.done or self.eps_len >= self.args.max_episode_length\n        self.values.append(value)\n        self.rewards.append(reward)\n        return self\n\n    def action_test(self):\n        with torch.no_grad():\n            if self.done:\n                if self.gpu_id >= 0:\n                    with torch.cuda.device(self.gpu_id):\n                        self.cx = Variable(torch.zeros(\n                            1, 128).cuda())\n                        self.hx = Variable(torch.zeros(\n                            1, 128).cuda())\n                else:\n                    self.cx = Variable(torch.zeros(1, 128))\n                    self.hx = Variable(torch.zeros(1, 128))\n            else:\n                self.cx = Variable(self.cx.data)\n                self.hx = Variable(self.hx.data)\n            if self.args.model == 'CONV':\n                self.state = self.state.unsqueeze(0)\n            value, mu, sigma, (self.hx, self.cx) = self.model(\n                (Variable(self.state), (self.hx, self.cx)))\n        mu = torch.clamp(mu.data, -1.0, 1.0)\n        action = mu.cpu().numpy()[0]\n        state, self.reward, self.done, self.info = self.env.step(action)\n        self.state = torch.from_numpy(state).float()\n        if self.gpu_id >= 0:\n            with torch.cuda.device(self.gpu_id):\n                self.state = self.state.cuda()\n        self.eps_len += 1\n        self.done = self.done or self.eps_len >= self.args.max_episode_length\n        return self\n\n    def clear_actions(self):\n        self.values = []\n        self.log_probs = []\n        self.rewards = []\n        self.entropies = []\n        return self\n"""
shared_optim.py,4,"b'from __future__ import division\nimport math\nimport torch\nimport torch.optim as optim\nfrom collections import defaultdict\n\n\nclass SharedRMSprop(optim.Optimizer):\n    """"""Implements RMSprop algorithm with shared states.\n    """"""\n\n    def __init__(self,\n                 params,\n                 lr=7e-4,\n                 alpha=0.99,\n                 eps=0.1,\n                 weight_decay=0,\n                 momentum=0,\n                 centered=False):\n        defaults = defaultdict(lr=lr, alpha=alpha, eps=eps,\n                               weight_decay=weight_decay, momentum=momentum, centered=centered)\n        super(SharedRMSprop, self).__init__(params, defaults)\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'] = torch.zeros(1)\n                state[\'grad_avg\'] = p.data.new().resize_as_(p.data).zero_()\n                state[\'square_avg\'] = p.data.new().resize_as_(p.data).zero_()\n                state[\'momentum_buffer\'] = p.data.new(\n                ).resize_as_(p.data).zero_()\n\n    def share_memory(self):\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'square_avg\'].share_memory_()\n                state[\'step\'].share_memory_()\n                state[\'grad_avg\'].share_memory_()\n                state[\'momentum_buffer\'].share_memory_()\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'RMSprop does not support sparse gradients\')\n                state = self.state[p]\n\n                square_avg = state[\'square_avg\']\n                alpha = group[\'alpha\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n\n                if group[\'centered\']:\n                    grad_avg = state[\'grad_avg\']\n                    grad_avg.mul_(alpha).add_(1 - alpha, grad)\n                    avg = square_avg.addcmul(\n                        -1, grad_avg, grad_avg).sqrt().add_(group[\'eps\'])\n                else:\n                    avg = square_avg.sqrt().add_(group[\'eps\'])\n\n                if group[\'momentum\'] > 0:\n                    buf = state[\'momentum_buffer\']\n                    buf.mul_(group[\'momentum\']).addcdiv_(grad, avg)\n                    p.data.add_(-group[\'lr\'], buf)\n                else:\n                    p.data.addcdiv_(-group[\'lr\'], grad, avg)\n\n        return loss\n\n\nclass SharedAdam(optim.Optimizer):\n    """"""Implements Adam algorithm with shared states.\n    """"""\n\n    def __init__(self,\n                 params,\n                 lr=1e-3,\n                 betas=(0.9, 0.999),\n                 eps=1e-3,\n                 weight_decay=0, amsgrad=True):\n        defaults = defaultdict(lr=lr, betas=betas, eps=eps,\n                               weight_decay=weight_decay, amsgrad=amsgrad)\n        super(SharedAdam, self).__init__(params, defaults)\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'] = torch.zeros(1)\n                state[\'exp_avg\'] = p.data.new().resize_as_(p.data).zero_()\n                state[\'exp_avg_sq\'] = p.data.new().resize_as_(p.data).zero_()\n                state[\'max_exp_avg_sq\'] = p.data.new(\n                ).resize_as_(p.data).zero_()\n\n    def share_memory(self):\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'].share_memory_()\n                state[\'exp_avg\'].share_memory_()\n                state[\'exp_avg_sq\'].share_memory_()\n                state[\'max_exp_avg_sq\'].share_memory_()\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'Adam does not support sparse gradients, please consider SparseAdam instead\')\n                amsgrad = group[\'amsgrad\']\n\n                state = self.state[p]\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsgrad:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till\n                    # now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1**state[\'step\'].item()\n                bias_correction2 = 1 - beta2**state[\'step\'].item()\n                step_size = group[\'lr\'] * \\\n                    math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n        return loss\n'"
test.py,11,"b'from __future__ import division\nfrom setproctitle import setproctitle as ptitle\nimport numpy as np\nimport torch\nfrom environment import create_env\nfrom utils import setup_logger\nfrom model import A3C_CONV, A3C_MLP\nfrom player_util import Agent\nfrom torch.autograd import Variable\nimport time\nimport logging\nimport gym\n\n\ndef test(args, shared_model):\n    ptitle(\'Test Agent\')\n    gpu_id = args.gpu_ids[-1]\n    log = {}\n    setup_logger(\'{}_log\'.format(args.env),\n                 r\'{0}{1}_log\'.format(args.log_dir, args.env))\n    log[\'{}_log\'.format(args.env)] = logging.getLogger(\n        \'{}_log\'.format(args.env))\n    d_args = vars(args)\n    for k in d_args.keys():\n        log[\'{}_log\'.format(args.env)].info(\'{0}: {1}\'.format(k, d_args[k]))\n\n    torch.manual_seed(args.seed)\n    if gpu_id >= 0:\n        torch.cuda.manual_seed(args.seed)\n    env = create_env(args.env, args)\n    reward_sum = 0\n    start_time = time.time()\n    num_tests = 0\n    reward_total_sum = 0\n    player = Agent(None, env, args, None)\n    player.gpu_id = gpu_id\n    if args.model == \'MLP\':\n        player.model = A3C_MLP(\n            player.env.observation_space.shape[0], player.env.action_space, args.stack_frames)\n    if args.model == \'CONV\':\n        player.model = A3C_CONV(args.stack_frames, player.env.action_space)\n\n    player.state = player.env.reset()\n    player.state = torch.from_numpy(player.state).float()\n    if gpu_id >= 0:\n        with torch.cuda.device(gpu_id):\n            player.model = player.model.cuda()\n            player.state = player.state.cuda()\n    player.model.eval()\n    max_score = 0\n    while True:\n        if player.done:\n            if gpu_id >= 0:\n                with torch.cuda.device(gpu_id):\n                    player.model.load_state_dict(shared_model.state_dict())\n            else:\n                player.model.load_state_dict(shared_model.state_dict())\n\n        player.action_test()\n        reward_sum += player.reward\n\n        if player.done:\n            num_tests += 1\n            reward_total_sum += reward_sum\n            reward_mean = reward_total_sum / num_tests\n            log[\'{}_log\'.format(args.env)].info(\n                ""Time {0}, episode reward {1}, episode length {2}, reward mean {3:.4f}"".\n                format(\n                    time.strftime(""%Hh %Mm %Ss"",\n                                  time.gmtime(time.time() - start_time)),\n                    reward_sum, player.eps_len, reward_mean))\n\n            if args.save_max and reward_sum >= max_score:\n                max_score = reward_sum\n                if gpu_id >= 0:\n                    with torch.cuda.device(gpu_id):\n                        state_to_save = player.model.state_dict()\n                        torch.save(state_to_save, \'{0}{1}.dat\'.format(args.save_model_dir, args.env))\n                else:\n                    state_to_save = player.model.state_dict()\n                    torch.save(state_to_save, \'{0}{1}.dat\'.format(args.save_model_dir, args.env))\n\n            reward_sum = 0\n            player.eps_len = 0\n            state = player.env.reset()\n            time.sleep(60)\n            player.state = torch.from_numpy(state).float()\n            if gpu_id >= 0:\n                with torch.cuda.device(gpu_id):\n                    player.state = player.state.cuda()\n'"
train.py,20,"b""from __future__ import division\nfrom setproctitle import setproctitle as ptitle\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom environment import create_env\nfrom utils import ensure_shared_grads\nfrom model import A3C_CONV, A3C_MLP\nfrom player_util import Agent\nfrom torch.autograd import Variable\nimport gym\n\n\ndef train(rank, args, shared_model, optimizer):\n    ptitle('Training Agent: {}'.format(rank))\n    gpu_id = args.gpu_ids[rank % len(args.gpu_ids)]\n    torch.manual_seed(args.seed + rank)\n    if gpu_id >= 0:\n        torch.cuda.manual_seed(args.seed + rank)\n    env = create_env(args.env, args)\n    if optimizer is None:\n        if args.optimizer == 'RMSprop':\n            optimizer = optim.RMSprop(shared_model.parameters(), lr=args.lr)\n        if args.optimizer == 'Adam':\n            optimizer = optim.Adam(shared_model.parameters(), lr=args.lr)\n\n    env.seed(args.seed + rank)\n    player = Agent(None, env, args, None)\n    player.gpu_id = gpu_id\n    if args.model == 'MLP':\n        player.model = A3C_MLP(\n            player.env.observation_space.shape[0], player.env.action_space, args.stack_frames)\n    if args.model == 'CONV':\n        player.model = A3C_CONV(args.stack_frames, player.env.action_space)\n\n    player.state = player.env.reset()\n    player.state = torch.from_numpy(player.state).float()\n    if gpu_id >= 0:\n        with torch.cuda.device(gpu_id):\n            player.state = player.state.cuda()\n            player.model = player.model.cuda()\n    player.model.train()\n    while True:\n        if gpu_id >= 0:\n            with torch.cuda.device(gpu_id):\n                player.model.load_state_dict(shared_model.state_dict())\n        else:\n            player.model.load_state_dict(shared_model.state_dict())\n        if player.done:\n            if gpu_id >= 0:\n                with torch.cuda.device(gpu_id):\n                    player.cx = Variable(torch.zeros(1, 128).cuda())\n                    player.hx = Variable(torch.zeros(1, 128).cuda())\n            else:\n                player.cx = Variable(torch.zeros(1, 128))\n                player.hx = Variable(torch.zeros(1, 128))\n        else:\n            player.cx = Variable(player.cx.data)\n            player.hx = Variable(player.hx.data)\n            \n        for step in range(args.num_steps):\n\n            player.action_train()\n\n            if player.done:\n                break\n\n        if player.done:\n            player.eps_len = 0\n            state = player.env.reset()\n            player.state = torch.from_numpy(state).float()\n            if gpu_id >= 0:\n                with torch.cuda.device(gpu_id):\n                    player.state = player.state.cuda()\n\n        if gpu_id >= 0:\n            with torch.cuda.device(gpu_id):\n                R = torch.zeros(1, 1).cuda()\n        else:\n            R = torch.zeros(1, 1)\n        if not player.done:\n            state = player.state\n            if args.model == 'CONV':\n                state = state.unsqueeze(0)\n            value, _, _, _ = player.model(\n                (Variable(state), (player.hx, player.cx)))\n            R = value.data\n\n        player.values.append(Variable(R))\n        policy_loss = 0\n        value_loss = 0\n        R = Variable(R)\n        if gpu_id >= 0:\n            with torch.cuda.device(gpu_id):\n                gae = torch.zeros(1, 1).cuda()\n        else:\n            gae = torch.zeros(1, 1)\n        for i in reversed(range(len(player.rewards))):\n            R = args.gamma * R + player.rewards[i]\n            advantage = R - player.values[i]\n            value_loss = value_loss + 0.5 * advantage.pow(2)\n\n            # Generalized Advantage Estimataion\n  #          print(player.rewards[i])\n            delta_t = player.rewards[i] + args.gamma * \\\n                player.values[i + 1].data - player.values[i].data\n\n            gae = gae * args.gamma * args.tau + delta_t\n\n            policy_loss = policy_loss - \\\n                (player.log_probs[i].sum() * Variable(gae)) - \\\n                (0.01 * player.entropies[i].sum())\n\n        player.model.zero_grad()\n        (policy_loss + 0.5 * value_loss).backward()\n        ensure_shared_grads(player.model, shared_model, gpu=gpu_id >= 0)\n        optimizer.step()\n        player.clear_actions()\n"""
utils.py,6,"b'from __future__ import division\nimport math\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nimport json\nimport logging\n\n\ndef setup_logger(logger_name, log_file, level=logging.INFO):\n    l = logging.getLogger(logger_name)\n    formatter = logging.Formatter(\'%(asctime)s : %(message)s\')\n    fileHandler = logging.FileHandler(log_file, mode=\'w\')\n    fileHandler.setFormatter(formatter)\n    streamHandler = logging.StreamHandler()\n    streamHandler.setFormatter(formatter)\n\n    l.setLevel(level)\n    l.addHandler(fileHandler)\n    l.addHandler(streamHandler)\n\n\ndef read_config(file_path):\n    """"""Read JSON config.""""""\n    json_object = json.load(open(file_path, \'r\'))\n    return json_object\n\n\ndef norm_col_init(weights, std=1.0):\n    x = torch.randn(weights.size())\n    x *= std / torch.sqrt((x**2).sum(1, keepdim=True))\n    return x\n\n\ndef ensure_shared_grads(model, shared_model, gpu=False):\n    for param, shared_param in zip(model.parameters(), shared_model.parameters()):\n        if shared_param.grad is not None and not gpu:\n            return\n        elif not gpu:\n            shared_param._grad = param.grad\n        else:\n            shared_param._grad = param.grad.cpu()\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = np.prod(weight_shape[1:4])\n        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n    elif classname.find(\'Linear\') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = weight_shape[1]\n        fan_out = weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n\n\ndef weights_init_mlp(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Linear\') != -1:\n        m.weight.data.normal_(0, 1)\n        m.weight.data *= 1 / \\\n            torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n        if m.bias is not None:\n            m.bias.data.fill_(0)\n\n\ndef normal(x, mu, sigma, gpu_id, gpu=False):\n    pi = np.array([math.pi])\n    pi = torch.from_numpy(pi).float()\n    if gpu:\n        with torch.cuda.device(gpu_id):\n            pi = Variable(pi).cuda()\n    else:\n        pi = Variable(pi)\n    a = (-1 * (x - mu).pow(2) / (2 * sigma)).exp()\n    b = 1 / (2 * sigma * pi.expand_as(sigma)).sqrt()\n    return a * b\n'"
