file_path,api_count,code
environment.py,0,"b'from __future__ import division\nimport gym\nimport numpy as np\nfrom collections import deque\nfrom gym.spaces.box import Box\n#from skimage.color import rgb2gray\nfrom cv2 import resize\n#from skimage.transform import resize\n#from scipy.misc import imresize as resize\nimport random\n\n\ndef atari_env(env_id, env_conf, args):\n    env = gym.make(env_id)\n    if \'NoFrameskip\' in env_id:\n        assert \'NoFrameskip\' in env.spec.id\n        env._max_episode_steps = args.max_episode_length * args.skip_rate\n        env = NoopResetEnv(env, noop_max=30)\n        env = MaxAndSkipEnv(env, skip=args.skip_rate)\n    else:\n        env._max_episode_steps = args.max_episode_length\n    env = EpisodicLifeEnv(env)\n    if \'FIRE\' in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env._max_episode_steps = args.max_episode_length\n    env = AtariRescale(env, env_conf)\n    env = NormalizedEnv(env)\n    return env\n\n\ndef process_frame(frame, conf):\n    frame = frame[conf[""crop1""]:conf[""crop2""] + 160, :160]\n    frame = frame.mean(2)\n    frame = frame.astype(np.float32)\n    frame *= (1.0 / 255.0)\n    frame = resize(frame, (80, conf[""dimension2""]))\n    frame = resize(frame, (80, 80))\n    frame = np.reshape(frame, [1, 80, 80])\n    return frame\n\n\nclass AtariRescale(gym.ObservationWrapper):\n    def __init__(self, env, env_conf):\n        gym.ObservationWrapper.__init__(self, env)\n        self.observation_space = Box(0.0, 1.0, [1, 80, 80], dtype=np.uint8)\n        self.conf = env_conf\n\n    def observation(self, observation):\n        return process_frame(observation, self.conf)\n\n\nclass NormalizedEnv(gym.ObservationWrapper):\n    def __init__(self, env=None):\n        gym.ObservationWrapper.__init__(self, env)\n        self.state_mean = 0\n        self.state_std = 0\n        self.alpha = 0.9999\n        self.num_steps = 0\n\n    def observation(self, observation):\n        self.num_steps += 1\n        self.state_mean = self.state_mean * self.alpha + \\\n            observation.mean() * (1 - self.alpha)\n        self.state_std = self.state_std * self.alpha + \\\n            observation.std() * (1 - self.alpha)\n\n        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n\n        return (observation - unbiased_mean) / (unbiased_std + 1e-8)\n\n\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        """"""Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == \'NOOP\'\n\n    def reset(self, **kwargs):\n        """""" Do no-op action for a number of steps in [1, noop_max].""""""\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)  #pylint: disable=E1101\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n            # so its important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, self.was_real_done\n\n    def reset(self, **kwargs):\n        """"""Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env=None, skip=4):\n        """"""Return only every `skip`-th frame""""""\n        super(MaxAndSkipEnv, self).__init__(env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = deque(maxlen=3)\n        self._skip = skip\n\n    def step(self, action):\n        total_reward = 0.0\n        done = None\n        for _ in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            self._obs_buffer.append(obs)\n            total_reward += reward\n            if done:\n                break\n\n        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        """"""Clear past frame buffer and init. to first obs. from inner env.""""""\n        self._obs_buffer.clear()\n        obs = self.env.reset(**kwargs)\n        self._obs_buffer.append(obs)\n        return obs\n\n'"
gym_eval.py,9,"b'from __future__ import division\nimport os\nos.environ[""OMP_NUM_THREADS""] = ""1""\nimport argparse\nimport torch\nfrom environment import atari_env\nfrom utils import read_config, setup_logger\nfrom model import A3Clstm\nfrom player_util import Agent\nimport gym\nimport logging\nimport time\n#from gym.configuration import undo_logger_setup\n\n#undo_logger_setup()\nparser = argparse.ArgumentParser(description=\'A3C_EVAL\')\nparser.add_argument(\n    \'--env\',\n    default=\'Pong-v0\',\n    metavar=\'ENV\',\n    help=\'environment to train on (default: Pong-v0)\')\nparser.add_argument(\n    \'--env-config\',\n    default=\'config.json\',\n    metavar=\'EC\',\n    help=\'environment to crop and resize info (default: config.json)\')\nparser.add_argument(\n    \'--num-episodes\',\n    type=int,\n    default=100,\n    metavar=\'NE\',\n    help=\'how many episodes in evaluation (default: 100)\')\nparser.add_argument(\n    \'--load-model-dir\',\n    default=\'trained_models/\',\n    metavar=\'LMD\',\n    help=\'folder to load trained models from\')\nparser.add_argument(\n    \'--log-dir\', default=\'logs/\', metavar=\'LG\', help=\'folder to save logs\')\nparser.add_argument(\n    \'--render\',\n    default=False,\n    metavar=\'R\',\n    help=\'Watch game as it being played\')\nparser.add_argument(\n    \'--render-freq\',\n    type=int,\n    default=1,\n    metavar=\'RF\',\n    help=\'Frequency to watch rendered game play\')\nparser.add_argument(\n    \'--max-episode-length\',\n    type=int,\n    default=10000,\n    metavar=\'M\',\n    help=\'maximum length of an episode (default: 100000)\')\nparser.add_argument(\n    \'--gpu-id\',\n    type=int,\n    default=-1,\n    help=\'GPU to use [-1 CPU only] (default: -1)\')\nparser.add_argument(\n    \'--skip-rate\',\n    type=int,\n    default=4,\n    metavar=\'SR\',\n    help=\'frame skip rate (default: 4)\')\nparser.add_argument(\n    \'--seed\',\n    type=int,\n    default=1,\n    metavar=\'S\',\n    help=\'random seed (default: 1)\')\nparser.add_argument(\n    \'--new-gym-eval\',\n    default=False,\n    metavar=\'NGE\',\n    help=\'Create a gym evaluation for upload\')\nargs = parser.parse_args()\n\nsetup_json = read_config(args.env_config)\nenv_conf = setup_json[""Default""]\nfor i in setup_json.keys():\n    if i in args.env:\n        env_conf = setup_json[i]\n\ngpu_id = args.gpu_id\n\ntorch.manual_seed(args.seed)\nif gpu_id >= 0:\n    torch.cuda.manual_seed(args.seed)\n\nsaved_state = torch.load(\n    \'{0}{1}.dat\'.format(args.load_model_dir, args.env),\n    map_location=lambda storage, loc: storage)\n\nlog = {}\nsetup_logger(\'{}_mon_log\'.format(args.env), r\'{0}{1}_mon_log\'.format(\n    args.log_dir, args.env))\nlog[\'{}_mon_log\'.format(args.env)] = logging.getLogger(\'{}_mon_log\'.format(\n    args.env))\n\nd_args = vars(args)\nfor k in d_args.keys():\n    log[\'{}_mon_log\'.format(args.env)].info(\'{0}: {1}\'.format(k, d_args[k]))\n\nenv = atari_env(""{}"".format(args.env), env_conf, args)\nnum_tests = 0\nstart_time = time.time()\nreward_total_sum = 0\nplayer = Agent(None, env, args, None)\nplayer.model = A3Clstm(player.env.observation_space.shape[0],\n                       player.env.action_space)\nplayer.gpu_id = gpu_id\nif gpu_id >= 0:\n    with torch.cuda.device(gpu_id):\n        player.model = player.model.cuda()\nif args.new_gym_eval:\n    player.env = gym.wrappers.Monitor(\n        player.env, ""{}_monitor"".format(args.env), force=True)\n\nif gpu_id >= 0:\n    with torch.cuda.device(gpu_id):\n        player.model.load_state_dict(saved_state)\nelse:\n    player.model.load_state_dict(saved_state)\n\nplayer.model.eval()\nfor i_episode in range(args.num_episodes):\n    player.state = player.env.reset()\n    player.state = torch.from_numpy(player.state).float()\n    if gpu_id >= 0:\n        with torch.cuda.device(gpu_id):\n            player.state = player.state.cuda()\n    player.eps_len += 2\n    reward_sum = 0\n    while True:\n        if args.render:\n            if i_episode % args.render_freq == 0:\n                player.env.render()\n\n        player.action_test()\n        reward_sum += player.reward\n\n        if player.done and not player.info:\n            state = player.env.reset()\n            player.eps_len += 2\n            player.state = torch.from_numpy(state).float()\n            if gpu_id >= 0:\n                with torch.cuda.device(gpu_id):\n                    player.state = player.state.cuda()\n        elif player.info:\n            num_tests += 1\n            reward_total_sum += reward_sum\n            reward_mean = reward_total_sum / num_tests\n            log[\'{}_mon_log\'.format(args.env)].info(\n                ""Time {0}, episode reward {1}, episode length {2}, reward mean {3:.4f}"".\n                format(\n                    time.strftime(""%Hh %Mm %Ss"",\n                                  time.gmtime(time.time() - start_time)),\n                    reward_sum, player.eps_len, reward_mean))\n            player.eps_len = 0\n            break\n'"
main.py,4,"b'from __future__ import print_function, division\nimport os\nos.environ[""OMP_NUM_THREADS""] = ""1""\nimport argparse\nimport torch\nimport torch.multiprocessing as mp\nfrom environment import atari_env\nfrom utils import read_config\nfrom model import A3Clstm\nfrom train import train\nfrom test import test\nfrom shared_optim import SharedRMSprop, SharedAdam\n#from gym.configuration import undo_logger_setup\nimport time\n\n#undo_logger_setup()\nparser = argparse.ArgumentParser(description=\'A3C\')\nparser.add_argument(\n    \'--lr\',\n    type=float,\n    default=0.0001,\n    metavar=\'LR\',\n    help=\'learning rate (default: 0.0001)\')\nparser.add_argument(\n    \'--gamma\',\n    type=float,\n    default=0.99,\n    metavar=\'G\',\n    help=\'discount factor for rewards (default: 0.99)\')\nparser.add_argument(\n    \'--tau\',\n    type=float,\n    default=1.00,\n    metavar=\'T\',\n    help=\'parameter for GAE (default: 1.00)\')\nparser.add_argument(\n    \'--seed\',\n    type=int,\n    default=1,\n    metavar=\'S\',\n    help=\'random seed (default: 1)\')\nparser.add_argument(\n    \'--workers\',\n    type=int,\n    default=32,\n    metavar=\'W\',\n    help=\'how many training processes to use (default: 32)\')\nparser.add_argument(\n    \'--num-steps\',\n    type=int,\n    default=20,\n    metavar=\'NS\',\n    help=\'number of forward steps in A3C (default: 20)\')\nparser.add_argument(\n    \'--max-episode-length\',\n    type=int,\n    default=10000,\n    metavar=\'M\',\n    help=\'maximum length of an episode (default: 10000)\')\nparser.add_argument(\n    \'--env\',\n    default=\'Pong-v0\',\n    metavar=\'ENV\',\n    help=\'environment to train on (default: Pong-v0)\')\nparser.add_argument(\n    \'--env-config\',\n    default=\'config.json\',\n    metavar=\'EC\',\n    help=\'environment to crop and resize info (default: config.json)\')\nparser.add_argument(\n    \'--shared-optimizer\',\n    default=True,\n    metavar=\'SO\',\n    help=\'use an optimizer without shared statistics.\')\nparser.add_argument(\n    \'--load\', default=False, metavar=\'L\', help=\'load a trained model\')\nparser.add_argument(\n    \'--save-max\',\n    default=True,\n    metavar=\'SM\',\n    help=\'Save model on every test run high score matched or bested\')\nparser.add_argument(\n    \'--optimizer\',\n    default=\'Adam\',\n    metavar=\'OPT\',\n    help=\'shares optimizer choice of Adam or RMSprop\')\nparser.add_argument(\n    \'--load-model-dir\',\n    default=\'trained_models/\',\n    metavar=\'LMD\',\n    help=\'folder to load trained models from\')\nparser.add_argument(\n    \'--save-model-dir\',\n    default=\'trained_models/\',\n    metavar=\'SMD\',\n    help=\'folder to save trained models\')\nparser.add_argument(\n    \'--log-dir\', default=\'logs/\', metavar=\'LG\', help=\'folder to save logs\')\nparser.add_argument(\n    \'--gpu-ids\',\n    type=int,\n    default=-1,\n    nargs=\'+\',\n    help=\'GPUs to use [-1 CPU only] (default: -1)\')\nparser.add_argument(\n    \'--amsgrad\',\n    default=True,\n    metavar=\'AM\',\n    help=\'Adam optimizer amsgrad parameter\')\nparser.add_argument(\n    \'--skip-rate\',\n    type=int,\n    default=4,\n    metavar=\'SR\',\n    help=\'frame skip rate (default: 4)\')\n\n# Based on\n# https://github.com/pytorch/examples/tree/master/mnist_hogwild\n# Training settings\n# Implemented multiprocessing using locks but was not beneficial. Hogwild\n# training was far superior\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    torch.manual_seed(args.seed)\n    if args.gpu_ids == -1:\n        args.gpu_ids = [-1]\n    else:\n        torch.cuda.manual_seed(args.seed)\n        mp.set_start_method(\'spawn\')\n    setup_json = read_config(args.env_config)\n    env_conf = setup_json[""Default""]\n    for i in setup_json.keys():\n        if i in args.env:\n            env_conf = setup_json[i]\n    env = atari_env(args.env, env_conf, args)\n    shared_model = A3Clstm(env.observation_space.shape[0], env.action_space)\n    if args.load:\n        saved_state = torch.load(\n            \'{0}{1}.dat\'.format(args.load_model_dir, args.env),\n            map_location=lambda storage, loc: storage)\n        shared_model.load_state_dict(saved_state)\n    shared_model.share_memory()\n\n    if args.shared_optimizer:\n        if args.optimizer == \'RMSprop\':\n            optimizer = SharedRMSprop(shared_model.parameters(), lr=args.lr)\n        if args.optimizer == \'Adam\':\n            optimizer = SharedAdam(\n                shared_model.parameters(), lr=args.lr, amsgrad=args.amsgrad)\n        optimizer.share_memory()\n    else:\n        optimizer = None\n\n    processes = []\n\n    p = mp.Process(target=test, args=(args, shared_model, env_conf))\n    p.start()\n    processes.append(p)\n    time.sleep(0.1)\n    for rank in range(0, args.workers):\n        p = mp.Process(\n            target=train, args=(rank, args, shared_model, optimizer, env_conf))\n        p.start()\n        processes.append(p)\n        time.sleep(0.1)\n    for p in processes:\n        time.sleep(0.1)\n        p.join()\n'"
model.py,3,"b""from __future__ import division\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils import norm_col_init, weights_init\n\n\nclass A3Clstm(torch.nn.Module):\n    def __init__(self, num_inputs, action_space):\n        super(A3Clstm, self).__init__()\n        self.conv1 = nn.Conv2d(num_inputs, 32, 5, stride=1, padding=2)\n        self.maxp1 = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(32, 32, 5, stride=1, padding=1)\n        self.maxp2 = nn.MaxPool2d(2, 2)\n        self.conv3 = nn.Conv2d(32, 64, 4, stride=1, padding=1)\n        self.maxp3 = nn.MaxPool2d(2, 2)\n        self.conv4 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n        self.maxp4 = nn.MaxPool2d(2, 2)\n\n        self.lstm = nn.LSTMCell(1024, 512)\n        num_outputs = action_space.n\n        self.critic_linear = nn.Linear(512, 1)\n        self.actor_linear = nn.Linear(512, num_outputs)\n\n        self.apply(weights_init)\n        relu_gain = nn.init.calculate_gain('relu')\n        self.conv1.weight.data.mul_(relu_gain)\n        self.conv2.weight.data.mul_(relu_gain)\n        self.conv3.weight.data.mul_(relu_gain)\n        self.conv4.weight.data.mul_(relu_gain)\n        self.actor_linear.weight.data = norm_col_init(\n            self.actor_linear.weight.data, 0.01)\n        self.actor_linear.bias.data.fill_(0)\n        self.critic_linear.weight.data = norm_col_init(\n            self.critic_linear.weight.data, 1.0)\n        self.critic_linear.bias.data.fill_(0)\n\n        self.lstm.bias_ih.data.fill_(0)\n        self.lstm.bias_hh.data.fill_(0)\n\n        self.train()\n\n    def forward(self, inputs):\n        inputs, (hx, cx) = inputs\n        x = F.relu(self.maxp1(self.conv1(inputs)))\n        x = F.relu(self.maxp2(self.conv2(x)))\n        x = F.relu(self.maxp3(self.conv3(x)))\n        x = F.relu(self.maxp4(self.conv4(x)))\n\n        x = x.view(x.size(0), -1)\n\n        hx, cx = self.lstm(x, (hx, cx))\n\n        x = hx\n\n        return self.critic_linear(x), self.actor_linear(x), (hx, cx)\n"""
player_util.py,12,"b'from __future__ import division\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nclass Agent(object):\n    def __init__(self, model, env, args, state):\n        self.model = model\n        self.env = env\n        self.state = state\n        self.hx = None\n        self.cx = None\n        self.eps_len = 0\n        self.args = args\n        self.values = []\n        self.log_probs = []\n        self.rewards = []\n        self.entropies = []\n        self.done = True\n        self.info = None\n        self.reward = 0\n        self.gpu_id = -1\n\n    def action_train(self):\n        value, logit, (self.hx, self.cx) = self.model((Variable(\n            self.state.unsqueeze(0)), (self.hx, self.cx)))\n        prob = F.softmax(logit, dim=1)\n        log_prob = F.log_softmax(logit, dim=1)\n        entropy = -(log_prob * prob).sum(1)\n        self.entropies.append(entropy)\n        action = prob.multinomial(1).data\n        log_prob = log_prob.gather(1, Variable(action))\n        state, self.reward, self.done, self.info = self.env.step(\n            action.cpu().numpy())\n        self.state = torch.from_numpy(state).float()\n        if self.gpu_id >= 0:\n            with torch.cuda.device(self.gpu_id):\n                self.state = self.state.cuda()\n        self.reward = max(min(self.reward, 1), -1)\n        self.values.append(value)\n        self.log_probs.append(log_prob)\n        self.rewards.append(self.reward)\n        return self\n\n    def action_test(self):\n        with torch.no_grad():\n            if self.done:\n                if self.gpu_id >= 0:\n                    with torch.cuda.device(self.gpu_id):\n                        self.cx = Variable(\n                            torch.zeros(1, 512).cuda())\n                        self.hx = Variable(\n                            torch.zeros(1, 512).cuda())\n                else:\n                    self.cx = Variable(torch.zeros(1, 512))\n                    self.hx = Variable(torch.zeros(1, 512))\n            else:\n                self.cx = Variable(self.cx.data)\n                self.hx = Variable(self.hx.data)\n            value, logit, (self.hx, self.cx) = self.model((Variable(\n                self.state.unsqueeze(0)), (self.hx, self.cx)))\n        prob = F.softmax(logit, dim=1)\n        action = prob.max(1)[1].data.cpu().numpy()\n        state, self.reward, self.done, self.info = self.env.step(action[0])\n        self.state = torch.from_numpy(state).float()\n        if self.gpu_id >= 0:\n            with torch.cuda.device(self.gpu_id):\n                self.state = self.state.cuda()\n        self.eps_len += 1\n        return self\n\n    def clear_actions(self):\n        self.values = []\n        self.log_probs = []\n        self.rewards = []\n        self.entropies = []\n        return self\n'"
shared_optim.py,4,"b'from __future__ import division\nimport math\nimport torch\nimport torch.optim as optim\nfrom collections import defaultdict\n\n\nclass SharedRMSprop(optim.Optimizer):\n    """"""Implements RMSprop algorithm with shared states.\n    """"""\n\n    def __init__(self,\n                 params,\n                 lr=7e-4,\n                 alpha=0.99,\n                 eps=0.1,\n                 weight_decay=0,\n                 momentum=0,\n                 centered=False):\n        defaults = defaultdict(\n            lr=lr,\n            alpha=alpha,\n            eps=eps,\n            weight_decay=weight_decay,\n            momentum=momentum,\n            centered=centered)\n        super(SharedRMSprop, self).__init__(params, defaults)\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'] = torch.zeros(1)\n                state[\'grad_avg\'] = p.data.new().resize_as_(p.data).zero_()\n                state[\'square_avg\'] = p.data.new().resize_as_(p.data).zero_()\n                state[\'momentum_buffer\'] = p.data.new().resize_as_(\n                    p.data).zero_()\n\n    def share_memory(self):\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'square_avg\'].share_memory_()\n                state[\'step\'].share_memory_()\n                state[\'grad_avg\'].share_memory_()\n                state[\'momentum_buffer\'].share_memory_()\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'RMSprop does not support sparse gradients\')\n                state = self.state[p]\n\n                square_avg = state[\'square_avg\']\n                alpha = group[\'alpha\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n\n                if group[\'centered\']:\n                    grad_avg = state[\'grad_avg\']\n                    grad_avg.mul_(alpha).add_(1 - alpha, grad)\n                    avg = square_avg.addcmul(-1, grad_avg,\n                                             grad_avg).sqrt().add_(\n                                                 group[\'eps\'])\n                else:\n                    avg = square_avg.sqrt().add_(group[\'eps\'])\n\n                if group[\'momentum\'] > 0:\n                    buf = state[\'momentum_buffer\']\n                    buf.mul_(group[\'momentum\']).addcdiv_(grad, avg)\n                    p.data.add_(-group[\'lr\'], buf)\n                else:\n                    p.data.addcdiv_(-group[\'lr\'], grad, avg)\n\n        return loss\n\n\nclass SharedAdam(optim.Optimizer):\n    """"""Implements Adam algorithm with shared states.\n    """"""\n\n    def __init__(self,\n                 params,\n                 lr=1e-3,\n                 betas=(0.9, 0.999),\n                 eps=1e-3,\n                 weight_decay=0,\n                 amsgrad=False):\n        defaults = defaultdict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            amsgrad=amsgrad)\n        super(SharedAdam, self).__init__(params, defaults)\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'] = torch.zeros(1)\n                state[\'exp_avg\'] = p.data.new().resize_as_(p.data).zero_()\n                state[\'exp_avg_sq\'] = p.data.new().resize_as_(p.data).zero_()\n                state[\'max_exp_avg_sq\'] = p.data.new().resize_as_(\n                    p.data).zero_()\n\n    def share_memory(self):\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                state[\'step\'].share_memory_()\n                state[\'exp_avg\'].share_memory_()\n                state[\'exp_avg_sq\'].share_memory_()\n                state[\'max_exp_avg_sq\'].share_memory_()\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'Adam does not support sparse gradients, please consider SparseAdam instead\'\n                    )\n                amsgrad = group[\'amsgrad\']\n\n                state = self.state[p]\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsgrad:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till\n                    # now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1**state[\'step\'].item()\n                bias_correction2 = 1 - beta2**state[\'step\'].item()\n                step_size = group[\'lr\'] * \\\n                    math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n        return loss\n'"
test.py,13,"b'from __future__ import division\nfrom setproctitle import setproctitle as ptitle\nimport torch\nfrom environment import atari_env\nfrom utils import setup_logger\nfrom model import A3Clstm\nfrom player_util import Agent\nfrom torch.autograd import Variable\nimport time\nimport logging\n\n\ndef test(args, shared_model, env_conf):\n    ptitle(\'Test Agent\')\n    gpu_id = args.gpu_ids[-1]\n    log = {}\n    setup_logger(\'{}_log\'.format(args.env), r\'{0}{1}_log\'.format(\n        args.log_dir, args.env))\n    log[\'{}_log\'.format(args.env)] = logging.getLogger(\'{}_log\'.format(\n        args.env))\n    d_args = vars(args)\n    for k in d_args.keys():\n        log[\'{}_log\'.format(args.env)].info(\'{0}: {1}\'.format(k, d_args[k]))\n\n    torch.manual_seed(args.seed)\n    if gpu_id >= 0:\n        torch.cuda.manual_seed(args.seed)\n    env = atari_env(args.env, env_conf, args)\n    reward_sum = 0\n    start_time = time.time()\n    num_tests = 0\n    reward_total_sum = 0\n    player = Agent(None, env, args, None)\n    player.gpu_id = gpu_id\n    player.model = A3Clstm(player.env.observation_space.shape[0],\n                           player.env.action_space)\n\n    player.state = player.env.reset()\n    player.eps_len += 2\n    player.state = torch.from_numpy(player.state).float()\n    if gpu_id >= 0:\n        with torch.cuda.device(gpu_id):\n            player.model = player.model.cuda()\n            player.state = player.state.cuda()\n    flag = True\n    max_score = 0\n    while True:\n        if flag:\n            if gpu_id >= 0:\n                with torch.cuda.device(gpu_id):\n                    player.model.load_state_dict(shared_model.state_dict())\n            else:\n                player.model.load_state_dict(shared_model.state_dict())\n            player.model.eval()\n            flag = False\n\n        player.action_test()\n        reward_sum += player.reward\n\n        if player.done and not player.info:\n            state = player.env.reset()\n            player.eps_len += 2\n            player.state = torch.from_numpy(state).float()\n            if gpu_id >= 0:\n                with torch.cuda.device(gpu_id):\n                    player.state = player.state.cuda()\n        elif player.info:\n            flag = True\n            num_tests += 1\n            reward_total_sum += reward_sum\n            reward_mean = reward_total_sum / num_tests\n            log[\'{}_log\'.format(args.env)].info(\n                ""Time {0}, episode reward {1}, episode length {2}, reward mean {3:.4f}"".\n                format(\n                    time.strftime(""%Hh %Mm %Ss"",\n                                  time.gmtime(time.time() - start_time)),\n                    reward_sum, player.eps_len, reward_mean))\n\n            if args.save_max and reward_sum >= max_score:\n                max_score = reward_sum\n                if gpu_id >= 0:\n                    with torch.cuda.device(gpu_id):\n                        state_to_save = player.model.state_dict()\n                        torch.save(state_to_save, \'{0}{1}.dat\'.format(\n                            args.save_model_dir, args.env))\n                else:\n                    state_to_save = player.model.state_dict()\n                    torch.save(state_to_save, \'{0}{1}.dat\'.format(\n                        args.save_model_dir, args.env))\n\n            reward_sum = 0\n            player.eps_len = 0\n            state = player.env.reset()\n            player.eps_len += 2\n            time.sleep(10)\n            player.state = torch.from_numpy(state).float()\n            if gpu_id >= 0:\n                with torch.cuda.device(gpu_id):\n                    player.state = player.state.cuda()\n'"
train.py,18,"b""from __future__ import division\nfrom setproctitle import setproctitle as ptitle\nimport torch\nimport torch.optim as optim\nfrom environment import atari_env\nfrom utils import ensure_shared_grads\nfrom model import A3Clstm\nfrom player_util import Agent\nfrom torch.autograd import Variable\n\n\ndef train(rank, args, shared_model, optimizer, env_conf):\n    ptitle('Training Agent: {}'.format(rank))\n    gpu_id = args.gpu_ids[rank % len(args.gpu_ids)]\n    torch.manual_seed(args.seed + rank)\n    if gpu_id >= 0:\n        torch.cuda.manual_seed(args.seed + rank)\n    env = atari_env(args.env, env_conf, args)\n    if optimizer is None:\n        if args.optimizer == 'RMSprop':\n            optimizer = optim.RMSprop(shared_model.parameters(), lr=args.lr)\n        if args.optimizer == 'Adam':\n            optimizer = optim.Adam(\n                shared_model.parameters(), lr=args.lr, amsgrad=args.amsgrad)\n    env.seed(args.seed + rank)\n    player = Agent(None, env, args, None)\n    player.gpu_id = gpu_id\n    player.model = A3Clstm(player.env.observation_space.shape[0],\n                           player.env.action_space)\n\n    player.state = player.env.reset()\n    player.state = torch.from_numpy(player.state).float()\n    if gpu_id >= 0:\n        with torch.cuda.device(gpu_id):\n            player.state = player.state.cuda()\n            player.model = player.model.cuda()\n    player.model.train()\n    player.eps_len += 2\n    while True:\n        if gpu_id >= 0:\n            with torch.cuda.device(gpu_id):\n                player.model.load_state_dict(shared_model.state_dict())\n        else:\n            player.model.load_state_dict(shared_model.state_dict())\n        if player.done:\n            if gpu_id >= 0:\n                with torch.cuda.device(gpu_id):\n                    player.cx = Variable(torch.zeros(1, 512).cuda())\n                    player.hx = Variable(torch.zeros(1, 512).cuda())\n            else:\n                player.cx = Variable(torch.zeros(1, 512))\n                player.hx = Variable(torch.zeros(1, 512))\n        else:\n            player.cx = Variable(player.cx.data)\n            player.hx = Variable(player.hx.data)\n\n        for step in range(args.num_steps):\n            player.action_train()\n            if player.done:\n                break\n\n        if player.done:\n            state = player.env.reset()\n            player.state = torch.from_numpy(state).float()\n            if gpu_id >= 0:\n                with torch.cuda.device(gpu_id):\n                    player.state = player.state.cuda()\n\n        R = torch.zeros(1, 1)\n        if not player.done:\n            value, _, _ = player.model((Variable(player.state.unsqueeze(0)),\n                                        (player.hx, player.cx)))\n            R = value.data\n\n        if gpu_id >= 0:\n            with torch.cuda.device(gpu_id):\n                R = R.cuda()\n\n        player.values.append(Variable(R))\n        policy_loss = 0\n        value_loss = 0\n        gae = torch.zeros(1, 1)\n        if gpu_id >= 0:\n            with torch.cuda.device(gpu_id):\n                gae = gae.cuda()\n        R = Variable(R)\n        for i in reversed(range(len(player.rewards))):\n            R = args.gamma * R + player.rewards[i]\n            advantage = R - player.values[i]\n            value_loss = value_loss + 0.5 * advantage.pow(2)\n\n            # Generalized Advantage Estimataion\n            delta_t = player.rewards[i] + args.gamma * \\\n                player.values[i + 1].data - player.values[i].data\n\n            gae = gae * args.gamma * args.tau + delta_t\n\n            policy_loss = policy_loss - \\\n                player.log_probs[i] * \\\n                Variable(gae) - 0.01 * player.entropies[i]\n\n        player.model.zero_grad()\n        (policy_loss + 0.5 * value_loss).backward()\n        ensure_shared_grads(player.model, shared_model, gpu=gpu_id >= 0)\n        optimizer.step()\n        player.clear_actions()\n"""
utils.py,2,"b'from __future__ import division\nimport numpy as np\nimport torch\nimport json\nimport logging\n\n\ndef setup_logger(logger_name, log_file, level=logging.INFO):\n    l = logging.getLogger(logger_name)\n    formatter = logging.Formatter(\'%(asctime)s : %(message)s\')\n    fileHandler = logging.FileHandler(log_file, mode=\'w\')\n    fileHandler.setFormatter(formatter)\n    streamHandler = logging.StreamHandler()\n    streamHandler.setFormatter(formatter)\n\n    l.setLevel(level)\n    l.addHandler(fileHandler)\n    l.addHandler(streamHandler)\n\n\ndef read_config(file_path):\n    """"""Read JSON config.""""""\n    json_object = json.load(open(file_path, \'r\'))\n    return json_object\n\n\ndef norm_col_init(weights, std=1.0):\n    x = torch.randn(weights.size())\n    x *= std / torch.sqrt((x**2).sum(1, keepdim=True))\n    return x\n\n\ndef ensure_shared_grads(model, shared_model, gpu=False):\n    for param, shared_param in zip(model.parameters(),\n                                   shared_model.parameters()):\n        if shared_param.grad is not None and not gpu:\n            return\n        elif not gpu:\n            shared_param._grad = param.grad\n        else:\n            shared_param._grad = param.grad.cpu()\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = np.prod(weight_shape[1:4])\n        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n    elif classname.find(\'Linear\') != -1:\n        weight_shape = list(m.weight.data.size())\n        fan_in = weight_shape[1]\n        fan_out = weight_shape[0]\n        w_bound = np.sqrt(6. / (fan_in + fan_out))\n        m.weight.data.uniform_(-w_bound, w_bound)\n        m.bias.data.fill_(0)\n'"
