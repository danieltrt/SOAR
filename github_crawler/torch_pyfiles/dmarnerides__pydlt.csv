file_path,api_count,code
setup.py,0,"b'import os\nimport subprocess\nfrom setuptools import setup, find_packages\n\nwith open(\'.version\', \'r\') as v_file:\n    version = v_file.read().replace(\'\\n\', \'\')\n\n# Got this from here\n# http://blogs.nopcode.org/brainstorm/2013/05/20/pragmatic-python-versioning-via-setuptools-and-git-tags/\n# Fetch version from git tags, and write to version.py.\n# Also, when git is not available (PyPi package), use stored version.py.\nversion_py = os.path.join(os.path.dirname(__file__), \'dlt/version.py\')\n\ntry:\n    build = subprocess.check_output([\'git\', \'rev-parse\', \'--short\', \'HEAD\']).rstrip().decode(""utf-8"") \nexcept:\n    with open(version_py, \'r\') as fh:\n        build = open(version_py).read().strip().split(\'=\')[-1].replace(\'""\',\'\')\n\n\nfull_version = version + \'+\' + build\n\nversion_msg = ""# Do not edit this file, pipeline versioning is governed by git tags""\nwith open(version_py, \'w\') as fh:\n    fh.write(version_msg + os.linesep + ""__version__ = \\\'"" + full_version + \'\\\'\')\n\n\nreadme = open(\'README.md\').read()\n\nsetup(name=\'dlt\',\n      version=full_version,\n      description=\'Deep Learning Toolbox for PyTorch\',\n      long_description=readme,\n      url=\'https://github.com/dmarnerides/pydlt\',\n      license=\'BSD\',\n      author=\'Demetris Marnerides\',\n      author_email=\'dmarnerides@gmail.com\',\n      packages=find_packages(exclude=[\'docs\', \'tests\']),\n      entry_points={\n          \'console_scripts\': [\'dlt-plot=dlt.viz.csvplot:plot_csv\',\n                              \'dlt-dispatch=dlt.util.dispatch:dispatch\']\n      }\n)\n'"
dlt/__init__.py,0,"b""from . import config\nfrom . import util\nfrom . import viz\nfrom . import train\nfrom . import hdr\nfrom .version import __version__\n\ndef _make_log():\n    import sys\n    import logging\n    logger = logging.getLogger('dlt')\n    logger.setLevel(logging.INFO)\n    logger.propagate = False\n    sh = logging.StreamHandler()\n    sh.terminator = ''\n    sh.setFormatter(logging.Formatter('[dlt-{levelname}] {message}\\n', style='{'))\n    logger.addHandler(sh)\n\n_make_log()\n"""
docs/conf.py,4,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# PyDLT documentation build configuration file, created by\n# sphinx-quickstart on Mon Jan  8 16:59:57 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'../..\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\n\nimport sys\nfrom unittest.mock import MagicMock\n\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n            return MagicMock()\n\nMOCK_MODULES = [\n    \'torch\',\n    \'torch.optim\',\n    \'torch.autograd\',\n    \'torch.optim.lr_scheduler\',\n    \'numpy\',\n    \'numpy.ma\',\n    \'matplotlib\',\n    \'matplotlib.pyplot\',\n    \'matplotlib.animation\',\n    \'pandas\', \n    \'cv2\']\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\nsys.modules.update([(\'torch.utils.data\', Mock(Dataset=object))])\n\nimport dlt\nimport sphinx_rtd_theme\n\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\']\n\nnapoleon_use_ivar = True\nnapoleon_include_special_with_doc = True\nautodoc_docstring_signature = True\n\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'PyDLT\'\ncopyright = \'2018, Demetris Marnerides\'\nauthor = \'Demetris Marnerides\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = dlt.__version__.split(\'+\')[0]\n# The full version, including alpha/beta/rc tags.\nrelease = dlt.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# html_theme = \'alabaster\'\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': True,\n}\n\nhtml_logo = \'_static/img/new_logo.png\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\n\n\ndef setup(app):\n    app.add_stylesheet(\'https://fonts.googleapis.com/css?family=Lato\',)\n    app.add_stylesheet(\'_static/css/modified_pytorch_theme.css\')\n\n# html_static_path = [\'_static\']\n\n# html_context = {\n#     \'css_files\': [\n#         \'https://fonts.googleapis.com/css?family=Lato\',\n#         \'_static/css/modified_pytorch_theme.css\'\n#     ],\n# }\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\n# html_sidebars = {\n#     \'**\': [\n#         \'about.html\',\n#         \'navigation.html\',\n#         \'relations.html\',  # needs \'show_related\': True theme option to display\n#         \'searchbox.html\',\n#         \'donate.html\',\n#     ]\n# }\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'PyDLTdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'PyDLT.tex\', \'PyDLT Documentation\',\n     \'Demetris Marnerides\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'pydlt\', \'PyDLT Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'PyDLT\', \'PyDLT Documentation\',\n     author, \'PyDLT\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n# -- A patch that prevents Sphinx from cross-referencing ivar tags -------\n# See http://stackoverflow.com/a/41184353/3343043\n\nfrom docutils import nodes\nfrom sphinx.util.docfields import TypedField\nfrom sphinx import addnodes\n\n\ndef patched_make_field(self, types, domain, items, **kw):\n    # `kw` catches `env=None` needed for newer sphinx while maintaining\n    #  backwards compatibility when passed along further down!\n\n    # type: (List, unicode, Tuple) -> nodes.field\n    def handle_item(fieldarg, content):\n        par = nodes.paragraph()\n        par += addnodes.literal_strong(\'\', fieldarg)  # Patch: this line added\n        # par.extend(self.make_xrefs(self.rolename, domain, fieldarg,\n        #                           addnodes.literal_strong))\n        if fieldarg in types:\n            par += nodes.Text(\' (\')\n            # NOTE: using .pop() here to prevent a single type node to be\n            # inserted twice into the doctree, which leads to\n            # inconsistencies later when references are resolved\n            fieldtype = types.pop(fieldarg)\n            if len(fieldtype) == 1 and isinstance(fieldtype[0], nodes.Text):\n                typename = u\'\'.join(n.astext() for n in fieldtype)\n                typename = typename.replace(\'int\', \'python:int\')\n                typename = typename.replace(\'long\', \'python:long\')\n                typename = typename.replace(\'float\', \'python:float\')\n                typename = typename.replace(\'type\', \'python:type\')\n                par.extend(self.make_xrefs(self.typerolename, domain, typename,\n                                           addnodes.literal_emphasis, **kw))\n            else:\n                par += fieldtype\n            par += nodes.Text(\')\')\n        par += nodes.Text(\' -- \')\n        par += content\n        return par\n\n    fieldname = nodes.field_name(\'\', self.label)\n    if len(items) == 1 and self.can_collapse:\n        fieldarg, content = items[0]\n        bodynode = handle_item(fieldarg, content)\n    else:\n        bodynode = self.list_type()\n        for fieldarg, content in items:\n            bodynode += nodes.list_item(\'\', handle_item(fieldarg, content))\n    fieldbody = nodes.field_body(\'\', bodynode)\n    return nodes.field(\'\', fieldname, fieldbody)\n\nTypedField.make_field = patched_make_field'"
dlt/config/__init__.py,0,"b'from .dataset import directory_dataset, loader, torchvision_dataset\nfrom .model import model_checkpointer\nfrom .trainer import trainer_checkpointer\nfrom .optim import optimizer, scheduler, optimizer_checkpointer\nfrom .opts import print_opts, parse, add_extras, make_subsets'"
dlt/config/dataset.py,1,"b'from torch.utils.data import DataLoader\nfrom ..util import barit, DirectoryDataset, LoadedDataset\nfrom ..hdr.io import imread\nfrom .opts import fetch_opts\n\ndef _custom_get_item(self, index):\n    if self.train:\n        img, target = self.train_data[index], self.train_labels[index]\n    else:\n        img, target = self.test_data[index], self.test_labels[index]\n    if self.transform is not None:\n        img = self.transform(img)\n    if self.target_transform is not None:\n        target = self.target_transform(target)\n    return img, target\n\ndef torchvision_dataset(transform=None, target_transform=None, train=True, subset=None):\n    """"""Creates a dataset from torchvision, configured using Command Line Arguments.\n\n    Args:\n        transform (callable, optional): A function that transforms an image (default None).\n        target_transform (callable, optional): A function that transforms a label (default None).\n        train (bool, optional): Training set or validation - if applicable (default True).\n        subset (string, optional): Specifies the subset of the relevant\n            categories, if any of them was split (default, None).\n\n    Relevant Command Line Arguments:\n\n        - **dataset**: `--data`, `--torchvision_dataset`.\n\n    Note:\n        Settings are automatically acquired from a call to :func:`dlt.config.parse`\n        from the built-in ones. If :func:`dlt.config.parse` was not called in the \n        main script, this function will call it.\n\n    Warning:\n        Unlike the torchvision datasets, this function returns a dataset that\n        uses NumPy Arrays instead of a PIL Images.\n    """"""\n    opts = fetch_opts([\'dataset\'], subset)\n\n    if opts.torchvision_dataset is None:\n        if subset is not None:\n            apnd = \'_\' + subset\n        else:\n            apnd = \'\'\n        raise ValueError(\'No value given for --torchvision_dataset{0}.\'.format(apnd))\n\n    if opts.torchvision_dataset == \'mnist\':\n        from torchvision.datasets import MNIST\n        MNIST.__getitem__ = _custom_get_item\n        ret_dataset = MNIST(opts.data, train=train, download=True, transform=transform,\n                            target_transform=target_transform)\n        # Add channel dimension and make numpy for consistency\n        if train:\n            ret_dataset.train_data = ret_dataset.train_data.unsqueeze(3).numpy()\n            ret_dataset.train_labels = ret_dataset.train_labels.numpy()\n        else:\n            ret_dataset.test_data = ret_dataset.test_data.unsqueeze(3).numpy()\n            ret_dataset.test_labels = ret_dataset.test_labels.numpy()\n    elif opts.torchvision_dataset == \'fashionmnist\':\n        from torchvision.datasets import FashionMNIST\n        FashionMNIST.__getitem__ = _custom_get_item\n        ret_dataset = FashionMNIST(opts.data, train=train, download=True, transform=transform,\n                                   target_transform=target_transform)\n        if train:\n            ret_dataset.train_data = ret_dataset.train_data.unsqueeze(3).numpy()\n            ret_dataset.train_labels = ret_dataset.train_labels.numpy()\n        else:\n            ret_dataset.test_data = ret_dataset.test_data.unsqueeze(3).numpy()\n            ret_dataset.test_labels = ret_dataset.test_labels.numpy()\n    elif opts.torchvision_dataset == \'cifar10\':\n        from torchvision.datasets import CIFAR10\n        CIFAR10.__getitem__ = _custom_get_item\n        ret_dataset = CIFAR10(opts.data, train=train, download=True, transform=transform,\n                              target_transform=target_transform)\n    elif opts.torchvision_dataset == \'cifar100\':\n        from torchvision.datasets import CIFAR100\n        CIFAR100.__getitem__ = _custom_get_item\n        ret_dataset = CIFAR100(opts.data, train=train, download=True, transform=transform,\n                               target_transform=target_transform)\n    return ret_dataset\n\ndef directory_dataset(load_fn=imread, preprocess=None, subset=None):\n    """"""Creates a :class:`dlt.util.DirectoryDataset`, configured using Command Line Arguments.\n\n    Args:\n        load_fn (callable, optional): Function that loads the data files\n            (default :func:`dlt.hdr.imread`).\n        preprocess (callable, optional): A function that takes a single data\n            point from the dataset to preprocess on the fly (default None).\n        subset (string, optional): Specifies the subset of the relevant\n            categories, if any of them was split (default, None).\n\n    Relevant Command Line Arguments:\n\n        - **dataset**: `--data`, `--load_all`, `--extensions`.\n        - **dataloader**: `--num_threads`.\n\n    Note:\n        Settings are automatically acquired from a call to :func:`dlt.config.parse`\n        from the built-in ones. If :func:`dlt.config.parse` was not called in the \n        main script, this function will call it.\n\n    """"""\n    opts = fetch_opts([\'dataset\', \'dataloader\'], subset)\n    if opts.load_all:\n        dummy_set = DirectoryDataset(opts.data, extensions=opts.extensions, load_fn=load_fn)\n        dummy_loader = DataLoader(dummy_set, batch_size=1, num_workers=opts.num_threads, pin_memory=False)\n        loaded_set = [batch[0].clone().numpy() for batch in barit(dummy_loader, start=\'Loading\')]\n        ret_dataset = LoadedDataset(loaded_set, preprocess=preprocess)\n        print(\'Done loading from {0}\'.format(opts.data))\n    else:\n        ret_dataset = DirectoryDataset(opts.data, load_fn=load_fn, preprocess=preprocess, extensions=opts.extensions)\n        print(\'Created dataset from {0}\'.format(opts.data))\n    return ret_dataset\n\n\ndef loader(dataset, preprocess=None, subset=None, worker_init_fn=None):\n    """"""Creates a torch DataLoader using the dataset, configured using Command Line Arguments.\n\n    Args:\n        dataset (Dataset): A torch compatible dataset.\n        preprocess (callable, optional): A function that takes a single data\n            point from the dataset to preprocess on the fly (default None).\n        subset (string, optional): Specifies the subset of the relevant\n            categories, if any of them was split (default, None).\n\n    Relevant Command Line Arguments:\n        \n        - **dataloader**: `--batch_size`, `--num_threads`, `--pin_memory`,\n          `--shuffle`, `--drop_last`.\n\n    Note:\n        Settings are automatically acquired from a call to :func:`dlt.config.parse`\n        from the built-in ones. If :func:`dlt.config.parse` was not called in the \n        main script, this function will call it.\n\n    """"""\n    opts = fetch_opts([\'dataloader\'], subset)\n    return DataLoader(LoadedDataset(dataset,preprocess),\n                      batch_size=opts.batch_size, num_workers=opts.num_threads,\n                      pin_memory=opts.pin_memory, shuffle=opts.shuffle,\n                      drop_last=opts.drop_last, worker_init_fn=worker_init_fn)'"
dlt/config/helpers.py,0,"b""import sys\nimport logging\n\nclass CustomFileHandler(logging.FileHandler):\n    def __init__(self, *args, **kwargs):\n        super(CustomFileHandler, self).__init__(*args, **kwargs)\n    def emit(self, record):\n        try:\n            msg = self.format(record)\n            self.stream.write(msg)\n        except Exception:\n            self.handleError(record)\n\n# From \n# https://stackoverflow.com/questions/1741972/how-to-use-different-formatters-with-the-same-logging-handler-in-python\nclass DispatchingFormatter:\n    def __init__(self, formatters, default_formatter):\n        self._formatters = formatters\n        self._default_formatter = default_formatter\n\n    def format(self, record):\n        formatter = self._formatters.get(record.name, self._default_formatter)\n        return formatter.format(record)\n\n# Helper logger for outputting sys.stdout to a logfile\nclass DuplStdOut(object):\n    def __init__(self, filename):\n        self.stdout = sys.stdout\n        \n        self.logger = logging.getLogger('dlt_file_log')\n        self.logger.setLevel(logging.INFO)\n        self.logger.propagate = False\n        file_handler = CustomFileHandler(filename)\n        formatter = logging.Formatter('{message}', style='{')\n        \n        # Keep the formatting of the dlt logger\n        file_handler.setFormatter(DispatchingFormatter({\n                'dlt_file_log': formatter,\n                'dlt': logging.getLogger('dlt').handlers[0].formatter\n            },\n            formatter,\n        ))\n        self.logger.addHandler(file_handler)\n        \n        # also add the file handler to dlt logger\n        logging.getLogger('dlt').addHandler(file_handler)\n        \n        sys.stdout = self\n \n    def __del__(self):\n        if self.stdout is not None:\n            sys.stdout = self.stdout\n            self.stdout = None\n\n    def write(self, msg):\n        # if msg != os.linesep:\n        self.logger.info(msg)\n        self.stdout.write(msg)\n\n    def flush(self):\n        self.stdout.flush()\n        for h in self.logger.handlers:\n            h.flush()"""
dlt/config/model.py,0,"b'from ..util import Checkpointer, count_parameters\nfrom .opts import fetch_opts\n\n\ndef model_checkpointer(model, preprocess=None, subset=None):\n    """"""Returns the model checkpointer. Configurable using command line arguments.\n    \n    The function also loads any previous checkpoints if present.\n\n    Args:\n        model (nn.Module): The network for the checkpointer.\n        preprocess (callable, optional): Callable to change the loaded state dict before\n            assigning it to the network.\n        subset (string, optional): Specifies the subset of the relevant\n            categories, if any of them was split (default, None).\n\n    Relevant Command Line Arguments:\n\n        - **general**: `--experiment_name`, `--save_path`.\n        - **model**: `--overwrite_model_chkp`, `--timestamp_model_chkp`, `--count_model_chkp`.\n\n    Note:\n        Settings are automatically acquired from a call to :func:`dlt.config.parse`\n        from the built-in ones. If :func:`dlt.config.parse` was not called in the \n        main script, this function will call it.\n    """"""\n\n    opts = fetch_opts([\'general\', \'model\'], subset)\n    name = subset[\'model\'] if isinstance(subset, dict) else subset\n    exp_name = opts.experiment_name + \'_\' if opts.experiment_name != \'\' else \'\'\n    model_chkp = Checkpointer(\'{0}{1}weights\'.format(exp_name, \'\' if name is None else name + \'_\'),\n                                  directory=opts.save_path, overwrite=opts.overwrite_model_chkp,\n                                  timestamp=opts.timestamp_model_chkp, add_count=opts.count_model_chkp)\n    model_chkp.load(model, preprocess=preprocess)\n    print(\'{0}\\n{1}\\n{0}\\n{2}\\nParameters: {3}\\n{0}\'.format(\'-\'*80, name.capitalize() if name is not None else \'Model\', model, count_parameters(model)))\n    return model_chkp\n'"
dlt/config/optim.py,10,"b'from itertools import chain\nimport torch\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, LambdaLR\nfrom ..util import Checkpointer\nfrom .opts import fetch_opts, parse\n\ndef optimizer(model, extra_params=None, subset=None):\n    """"""Returns the optimizer for the given model.\n    \n    Args:\n        model (nn.Module): The network for the optimizer.\n        extra_params (generator, optional): Extra parameters to pass to the optimizer.\n        subset (string, optional): Specifies the subset of the relevant\n            categories, if any of them was split (default, None).\n\n    Relevant Command Line Arguments:\n\n        - **optimizer**: `--optimizer`, `--lr`, `--momentum`,\n            `--dampening`, `--beta1`, `--beta2`, `--weight_decay`.\n\n    Note:\n        Settings are automatically acquired from a call to :func:`dlt.config.parse`\n        from the built-in ones. If :func:`dlt.config.parse` was not called in the \n        main script, this function will call it.\n    """"""\n    opts = fetch_opts(categories=[\'general\', \'optimizer\'], subset=subset)\n\n    if opts.optimizer not in parse.optimizers:\n        raise ValueError(\'Optimizer {0} not available.\'.format(opts.optimizer))\n\n    grad_params = filter(lambda p: p.requires_grad, model.parameters())\n    if extra_params is not None:\n        grad_params = chain(grad_params, filter(lambda p: p.requires_grad, extra_params))\n\n    if opts.optimizer == \'adam\':\n        ret_optimizer = torch.optim.Adam(grad_params, lr=opts.lr, \n            betas=(opts.beta1, opts.beta2), weight_decay=opts.weight_decay)\n    elif opts.optimizer == \'sgd\':\n        ret_optimizer = torch.optim.SGD(grad_params, lr=opts.lr, momentum=opts.momentum, \n            dampening=opts.dampening, weight_decay=opts.weight_decay)\n    elif opts.optimizer == \'adadelta\':\n        ret_optimizer = torch.optim.Adadelta(grad_params, lr=opts.lr, rho=opts.rho, \n            eps=opts.optim_eps, weight_decay=opts.weight_decay)\n    elif opts.optimizer == \'adagrad\':\n        ret_optimizer = torch.optim.Adagrad(grad_params, lr=opts.lr, \n            lr_decay=opts.lr_decay, weight_decay=opts.weight_decay)\n    elif opts.optimizer == \'sparseadam\':\n        ret_optimizer = torch.optim.SparseAdam(grad_params, lr=opts.lr, \n            betas=(opts.beta1, opts.beta2), eps=opts.optim_eps)\n    elif opts.optimizer == \'adamax\':\n        ret_optimizer = torch.optim.Adamax(grad_params, lr=opts.lr, \n            betas=(opts.beta1, opts.beta2), eps=opts.optim_eps, weight_decay=opts.weight_decay)\n    elif opts.optimizer == \'rmsprop\':\n        ret_optimizer = torch.optim.RMSprop(grad_params, lr=opts.lr,\n            alpha=opts.alpha, eps=opts.optim_eps, weight_decay=opts.weight_decay,\n            momentum=opts.momentum, centered=opts.centered)\n\n    return ret_optimizer\n\ndef optimizer_checkpointer(optimizer, subset=None):\n    """"""Returns the optimizer checkpointer. Configurable using command line arguments.\n    \n    The function also loads any previous checkpoints if present.\n\n    Args:\n        optimizer (torch.optim.Optimizer): The optimizer.\n        subset (string, optional): Specifies the subset of the relevant\n            categories, if any of them was split (default, None).\n\n    Relevant Command Line Arguments:\n\n        - **general**: `--experiment_name`, `--save_path`.\n        - **model**: `--overwrite_optimizer_chkp`, `--timestamp_optimizer_chkp`,\n            `--count_optimizer_chkp`.\n\n    Note:\n        Settings are automatically acquired from a call to :func:`dlt.config.parse`\n        from the built-in ones. If :func:`dlt.config.parse` was not called in the \n        main script, this function will call it.\n    """"""\n\n    opts = fetch_opts([\'general\', \'optimizer\'], subset)\n    name = subset[\'optimizer\'] if isinstance(subset, dict) else subset\n    exp_name = opts.experiment_name + \'_\' if opts.experiment_name != \'\' else \'\'\n    optim_chkp = Checkpointer(\'{0}{1}optimizer\'.format(exp_name, \'\' if name is None else name + \'_\'),\n                                  directory=opts.save_path, overwrite=opts.overwrite_optimizer_chkp,\n                                  timestamp=opts.timestamp_optimizer_chkp, add_count=opts.count_optimizer_chkp)\n    optim_chkp.load(optimizer)\n    return optim_chkp\n\n\ndef scheduler(optimizer, subset=None):\n    """"""Returns a scheduler callable closure which accepts one argument.\n    \n    Configurable using command line arguments.\n    \n    Args:\n        optimizer (torch.optim.Optimizer): The optimizer for the scheduler.\n        subset (string, optional): Specifies the subset of the relevant\n            categories, if any of them was split (default, None).\n\n    Relevant Command Line Arguments:\n\n        - **scheduler**: `--lr_schedule`, `--lr_step_size`, `--lr_patience`,\n            `--lr_cooldown`, `--lr_ratio`, `--lr_min`,\n\n    Note:\n        Settings are automatically acquired from a call to :func:`dlt.config.parse`\n        from the built-in ones. If :func:`dlt.config.parse` was not called in the \n        main script, this function will call it.\n    """"""\n    opts = fetch_opts(categories=[\'scheduler\'], subset=subset)\n    if opts.lr_schedule == \'plateau\':\n        ret_scheduler = ReduceLROnPlateau(optimizer, mode=\'min\', factor=opts.lr_ratio, threshold=0.0001,\n                                    patience=opts.lr_patience, verbose=True, threshold_mode=\'rel\',\n                                    cooldown=opts.lr_cooldown, min_lr=opts.lr_min, eps=1e-08)\n    elif opts.lr_schedule == \'step\':\n        ret_scheduler = StepLR(optimizer, step_size=opts.lr_step_size, gamma=opts.lr_ratio)\n    elif opts.lr_schedule == \'none\':\n        ret_scheduler = LambdaLR(optimizer, lr_lambda=lambda x: 1)\n    \n    if opts.lr_schedule == \'plateau\':\n        def schedule_fn(metric):\n            ret_scheduler.step(metric)\n    else:\n        def schedule_fn(metric):\n            ret_scheduler.step()\n    \n    def schedule_step(metric=None):\n        current_lr = optimizer.param_groups[0][\'lr\']\n        schedule_fn(metric)\n        new_lr = optimizer.param_groups[0][\'lr\']\n        if new_lr != current_lr:\n            name = subset[\'optimizer\'] if isinstance(subset, dict) else subset\n            name = \' ({0})\'.format(name) if name else \'\'\n            print(\'Learning rate{0} changed from {1:.2e} to {2:.2e}\'.format(name, current_lr, new_lr))\n\n\n    return schedule_step'"
dlt/config/opts.py,0,"b'import os\nimport sys\nimport argparse\nimport logging\nimport subprocess\nfrom functools import partial\nfrom ..util import paths\nfrom ..util.paths import process, write_file\nfrom ..util import str2bool\nfrom .helpers import DuplStdOut\n\n# Makes an entry for the saved settings file\ndef make_entry(key, val):\n    if any([isinstance(val, x) for x in [list, tuple]]):\n        return \'--\' + key + \' \' + \' \'.join([str(x) for x in val])\n    else:\n        return \'--\' + key + \' \' + str(val)\n\ndef get_git_hash():\n    try:\n        with open(os.devnull, \'w\') as devnull:\n            git_hash = subprocess.check_output([\'git\', \'rev-parse\', \'--short\', \'HEAD\'], stderr=devnull).rstrip().decode(""utf-8"")\n        return git_hash\n    except:\n        return None\n\n\n# This is to allow commented lines in the configuration files\ndef _convert_arg_line_to_args(line):\n    for arg in line.split():\n        if not arg.strip():\n            continue\n        if arg[0] == \'#\':\n            break\n        yield arg\n\ndef print_opts(opt):\n    """"""Prints the parsed command line options.""""""\n    size = 80\n    title = \'Settings:\'\n    if hasattr(opt, \'__dict__\'):\n        opt = opt.__dict__\n    bar = \'-\'*size\n    print(\'{0}\\n  {1}\\n{0}\'.format(bar, title))\n    for k, v in opt.items():\n        print(\'  {0}: {1}\'.format(k, v))\n    print(bar)\n\ndef make_subsets(subsets):\n    """"""Splits command line argument categories into subsets.\n    \n    The subset names are appended at the end of each of the categories options\n    after an underscore. For example the dataset category can be split into \n    training and validation subsets by passing the following as `subsets`:\n    \n    .. code-block:: python3\n\n        subsets = {dataset=[\'training\', \'validation\']}\n    \n    This will cause::\n\n        - `--data` to split into `--data_training` and `--data_validation`.\n        - `--load_all` to split into `--load_all_training` and `--load_all_validation`\n        - etc ...\n\n    Args:\n        subsets (dict, optional): Dictionary containing parameter categories as\n            keys and its subsets as a list of strings (default None).\n    """"""\n    parse.subsets = subsets\n\ndef add_extras(extras):\n    """"""Adds extra options for the parser, in addition to the built-in ones.\n\n    Args:\n        extras (list or dict, optional): Extra command line arguments to parse.\n            If a list is given, a new category `extras` is added with the\n            listed arguments. If a dict is given, the keys contain the category\n            names and the elements are lists of arguments (default None).\n\n    Note:\n        The `extras` parameter must have one of the following structures:\n\n        .. code-block:: python3\n\n            # List of arguments as dicts (this will add extras to )\n            extras = [dict(flag=\'--arg1\', type=float),\n                      dict(flag=\'--arg2\', type=int),\n                      dict(flag=\'--other_arg\', type=int)]\n            # OR dictionary with category names in keys \n            # and lists of dicts for as values for each category\n            extras = {\'my_category\': [dict(flag=\'--arg1\', type=float),\n                                      dict(flag=\'--arg2\', type=int)]\n                      \'other_category\': [dict(flag=\'--other_arg\', type=int)]}\n\n        The keys accepted in the argument dictionaries are the ones used as\n        arguments in the argparse_ package `add_argument` function.\n\n        .. _argparse: https://docs.python.org/3/library/argparse.html\nDuplStdOut\n    Warning:\n        The parser takes strings as inputs, so passing \'False\' at the command\n        for a bool argument, it will be converted to *True*. Instead of using\n        type=bool, use type=dlt.util.str2bool.\n\n    """"""\n    if isinstance(extras, dict):\n        for x in extras:\n            if x in parse.param_dict.keys():\n                print(\'WARNING: Parameter category \\\'{0}\\\' overwritten\'.format(x))\n            for fl in [\'flag\', \'flags\']:\n                for extra_args in extras[x]:\n                    if fl in extra_args and isinstance(extra_args[fl], str):\n                        extra_args[fl] = [extra_args[fl]]\n\n        parse.param_dict.update(extras)\n    elif isinstance(extras, list):\n        parse.param_dict[\'extras\'] = extras\n\ndef parse(verbose=False):\n    """"""Parses Command Line Arguments using the built-in settings (and any added extra).\n\n    Args:\n        verbose (bool, optional): Print the parsed settings (default False).\n\n    Comes with built-in commonly used settings. To add extra settings use\n    :func:`add_extras`.\n\n    For a comprehensive list of available settings, their categories and\n    use of configuration files please see the :ref:`config-example` example.\n    """"""\n\n    if not hasattr(parse, \'subsets\'):\n        parse.subsets = {}\n\n    parser = argparse.ArgumentParser(fromfile_prefix_chars=\'@\')\n    parser.convert_arg_line_to_args = _convert_arg_line_to_args\n    arg = parser.add_argument\n\n    # Go through flags and make ones that are strings into lists:\n    for category, params in parse.param_dict.items():\n        for p in params:\n            for fl in [\'flag\', \'flags\']:\n                if fl in p and isinstance(p[fl], str):\n                    p[fl] = [p[fl]]\n\n    # This looks bad but at least it works.\n    # It goes through the categories and retrieves the settings from the\n    # dictionaries in the list of each category and passes them to parser.add_argument\n    # If the category needs to be subset, the \'_<subset_name>\' is added for each option\n    # for each subset name (along with an appended string at the end of the help \n    # setting if there is one. \n    for category, params in parse.param_dict.items():\n        if category in parse.subsets:\n            for subset in parse.subsets[category]:\n                for p in params:\n                    if \'help\' in p:\n                        help_str = p[\'help\'] + \' ({0})\'.format(subset)\n                    \n                    arg(*[y + \'_{0}\'.format(subset)\n                            for x in [p[fl]\n                                        for fl in [\'flag\', \'flags\']\n                                            if fl in p]\n                                for y in x],\n                         **{key: val\n                                for key, val in p.items()\n                                    if key not in [\'flag\', \'flags\', \'help\']},\n                         help = help_str if \'help\' in p else None)\n        else:\n            for p in params:\n                arg(*[y for fl in [\'flag\', \'flags\'] \n                        if fl in p for y in p[fl]], \n                    **{key: val for key, val in p.items() \n                        if key not in [\'flag\', \'flags\']})\n    opt = parser.parse_args()\n\n    # Add experiment name to path and create the directory\n    if opt.experiment_name != \'\':\n        opt.save_path = os.path.join(opt.save_path, opt.experiment_name)\n    paths.make(opt.save_path)\n\n    # Create an event log file\n    if opt.create_log:\n        logsdir = process(os.path.join(opt.save_path, \'logs\'), create=True)\n        logfile = os.path.join(logsdir, opt.log_name)\n        count = 1\n        while True:\n            if os.path.isfile(logfile):\n                logfile = os.path.join(logsdir, \'{0}.{1}\'.format(opt.log_name, count))\n                count += 1\n            else:\n                break\n        DuplStdOut(logfile)\n\n    # Manage git hashes and print message\n    git_hash = get_git_hash()\n    git_hash_file = os.path.join(opt.save_path, \'.githash\')\n    if git_hash is not None:\n        if not os.path.isfile(git_hash_file):\n            write_file(git_hash, git_hash_file)\n        else:\n            with open(git_hash_file, \'r\') as f:\n                old_git_hash = f.read()\n            if old_git_hash != git_hash:\n                hash_warn = \'Current git hash ({0}) does not match the one used to generate the save directory ({1})\\n\'\n                logging.getLogger(\'dlt\').warning(hash_warn.format(git_hash, old_git_hash))\n\n    # Print all arguments \n    if verbose:\n        print_opts(opt)\n    \n    # Save arguments to file (only once)\n    settings_file = os.path.join(opt.save_path, \'settings.cfg\')\n    if not os.path.isfile(settings_file):\n        write_file(\'\\n\'.join([ make_entry(key,val) for key, val in opt.__dict__.items()]), settings_file)\n\n    parse.opt = opt\n    return opt\n\nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n\ndef fetch_opts(categories, subset=None):\n    if not hasattr(parse, \'opt\'):\n        parse()\n    opt = parse.opt\n    if isinstance(categories, str):\n        categories = [categories]\n    ret = AttrDict()\n    for category in categories:\n        if isinstance(subset, dict):\n            subset = subset[category]\n        elif isinstance(subset, str):\n            subset = subset\n        else:\n            subset = None\n        postfix = \'_{0}\'.format(subset) if subset is not None and subset != \'\' else \'\'\n        names = parse.default_avail_params[category]\n        current = AttrDict({name: getattr(opt, name + postfix) if hasattr(opt, name + postfix) else getattr(opt, name) for name in names})\n        ret.update(current)\n    return ret\n\nparse.torchvision_datasets = [\'mnist\', \'fashionmnist\', \'cifar10\', \'cifar100\']\n\nparse.optimizers = [\'adam\', \'sgd\', \'adadelta\', \'adagrad\', \'sparseadam\', \'adamax\', \'rmsprop\']\n\nparse.param_dict = {\n    \'dlt\': [dict(flags=[\'--create_log\'], type=str2bool, default=True, help=\'Output all std_out to a log file.\'),\n            dict(flags=[\'--log_name\'], default=\'dlt.log\', help=\'Name of log_file\')],\n    \'general\': [dict(flags=[\'--experiment_name\'], default=\'\', help=\'Name of experiment\'),\n                 dict(flags=[\'--save_path\'], type=partial(process, create=True), default=\'.\', help=\'Root directory for experiments\'),\n                 dict(flags=[\'--seed\'], type=int, default=None, help=\'Seed for random number generation.\'),\n                 dict(flags=[\'--max_epochs\'], type=int, default=100000, help=\'Maximum number of epochs\')],\n    \'dataset\': [ dict(flags=[\'--data\'], type=process, default=\'.\', help=\'Data directory\' ),\n                 dict(flags=[\'--load_all\'], type=str2bool, default=False, help=\'Load data in memory\'),\n                 dict(flags=[\'--torchvision_dataset\'],type = str.lower, choices=parse.torchvision_datasets, default=None, help=\'Specific dataset to use\'),\n                 dict(flags=[\'--extensions\'], nargs=\'+\', default=[\'jpg\'], help=\'Extensions of data to load.\')],\n    \'dataloader\': [ dict(flags=[\'--num_threads\'], type=int, default=4, help=\'Number of data loading threads\'),\n                     dict(flags=[\'--batch_size\'], type=int, default=1, help=\'Batch size for loader\'),\n                     dict(flags=[\'--shuffle\'], type=str2bool, default=True, help=\'Loader shuffles data each epoch\'),\n                     dict(flags=[\'--pin_memory\'], type=str2bool, default=True, help=\'Pin tensor memory for efficient GPU loading\'),\n                     dict(flags=[\'--drop_last\'], type=str2bool, default=False, help=\'Drop last batch if its size is less than batch size\')],\n    \'model\': [ dict(flags=[\'--overwrite_model_chkp\'], type=str2bool, default=True, help=\'Overwrite model checkpoints\'),\n               dict(flags=[\'--timestamp_model_chkp\'], type=str2bool, default=False, help=\'Add timestamp to model checkpoints\'),\n               dict(flags=[\'--count_model_chkp\'], type=str2bool, default=True, help=\'Add count to model checkpoints\') ],\n    \'optimizer\': [ dict(flags=[\'--optimizer\'],type = str.lower, choices = parse.optimizers, default=\'adam\', help=\'Optimizer\'),\n                   dict(flags=[\'--overwrite_optimizer_chkp\'], type=str2bool, default=True, help=\'Overwrite optimizer checkpoints\'),\n                   dict(flags=[\'--timestamp_optimizer_chkp\'], type=str2bool, default=False, help=\'Add timestamp to optimizer checkpoints\'),\n                   dict(flags=[\'--count_optimizer_chkp\'], type=str2bool, default=True, help=\'Add count to optimizer checkpoints\'),\n                   dict(flags=[\'--lr\'], type=float, default=1e-3, help=\'Learning rate\'),\n                   dict(flags=[\'--momentum\'], type=float, default=0.9, help=\'SGD Momentum\'),\n                   dict(flags=[\'--dampening\'], type=float, default=0.0, help=\'SGD Dampening\'),\n                   dict(flags=[\'--beta1\'], type=float, default=0.9, help=\'Adam beta1 parameter\'),\n                   dict(flags=[\'--beta2\'], type=float, default=0.99, help=\'Adam beta2 parameter\'),\n                   dict(flags=[\'--rho\'], type=float, default=0.9, help=\'Adadelta rho parameter\'),\n                     dict(flags=[\'--alpha\'], type=float, default=0.99, help=\'RMSprop alpha parameter\'),\n                     dict(flags=[\'--centered\'], type=str2bool, default=False, help=\'RMSprop centered flag\'),\n                     dict(flags=[\'--lr_decay\'], type=float, default=0.0, help=\'Adagrad lr_decay\'),\n                     dict(flags=[\'--optim_eps\'], type=float, default=1e-8, help=\'Term added to denominator for numerical stability.\'),\n                     dict(flags=[\'--weight_decay\'], type=float, default=0.0, help=\'Weight decay / L2 regularization.\')],\n    \'scheduler\': [ dict(flags=[\'--lr_schedule\'], choices=[\'plateau\', \'step\', \'none\'], default=\'step\', help=\'Learning rate schedule\'),\n                     dict(flags=[\'--lr_step_size\'], type=int, default=100, help=\'Epochs per learning rate decrease (step).\'),\n                     dict(flags=[\'--lr_patience\'], type=int, default=10, help=\'Epochs of patience for metric (plateau).\'),\n                     dict(flags=[\'--lr_cooldown\'], type=int, default=0, help=\'Epochs of cooldown period after lr change (plateau).\'),\n                     dict(flags=[\'--lr_min\'], type=float, default=1e-7, help=\'Minimum learning rate (plateau)\'),\n                     dict(flags=[\'--lr_ratio\'], type=float, default=0.5, help=\'Ratio to decrease learning rate by (all)\')],\n    \'gpu\': [dict(flags=[\'--use_gpu\'], type=str2bool, default=True, help=\'Use GPU\'),\n            dict(flags=[\'--device\'], type=int, default=0, help=\'GPU device ID\'),\n            dict(flags=[\'--cudnn_benchmark\'], type=str2bool, default=True, help=\'Use cudnn benchmark mode\')],\n    \'trainer\': [ dict(flags=[\'--overwrite_trainer_chkp\'], type=str2bool, default=True, help=\'Overwrite trainer checkpoints\'),\n               dict(flags=[\'--timestamp_trainer_chkp\'], type=str2bool, default=False, help=\'Add timestamp to trainer checkpoints\'),\n               dict(flags=[\'--count_trainer_chkp\'], type=str2bool, default=True, help=\'Add count to trainer checkpoints\') ],\n}\n\nparse.default_avail_params = {category: [param[\'flags\'][0][2:] for param in params] \n                        for category, params in parse.param_dict.items()}'"
dlt/config/trainer.py,0,"b'from ..util import Checkpointer\nfrom .opts import fetch_opts\n\n\ndef trainer_checkpointer(trainer, subset=None):\n    """"""Returns the trainer checkpointer. Configurable using command line arguments.\n    \n    The function also loads any previous checkpoints if present.\n\n    Args:\n        trainer (BaseTrainer): The trainer for the checkpointer.\n        subset (string, optional): Specifies the subset of the relevant\n            categories, if any of them was split (default, None).\n\n    Relevant Command Line Arguments:\n\n        - **general**: `--experiment_name`, `--save_path`.\n        - **trainer**: `--overwrite_trainer_chkp`, `--timestamp_trainer_chkp`, `--count_trainer_chkp`.\n\n    Note:\n        Settings are automatically acquired from a call to :func:`dlt.config.parse`\n        from the built-in ones. If :func:`dlt.config.parse` was not called in the \n        main script, this function will call it.\n    """"""\n\n    opts = fetch_opts([\'general\', \'trainer\'], subset)\n    name = subset[\'trainer\'] if isinstance(subset, dict) else subset\n    exp_name = opts.experiment_name + \'_\' if opts.experiment_name != \'\' else \'\'\n    trainer_chkp = Checkpointer(\'{0}{1}trainer\'.format(exp_name, \'\' if name is None else name + \'_\'),\n                                  directory=opts.save_path, overwrite=opts.overwrite_trainer_chkp,\n                                  timestamp=opts.timestamp_trainer_chkp, add_count=opts.count_trainer_chkp)\n    trainer_chkp.load(trainer)\n    return trainer_chkp\n'"
dlt/hdr/__init__.py,0,"b'from .io import write_pfm, load_pfm, imwrite, imread, load_encoded, decode_loaded'"
dlt/hdr/io.py,0,"b'import os\nimport sys\nimport numpy as np\nimport cv2\nfrom ..util import rgb2bgr\nfrom ..util.paths import process\n\n# Accepts hwc - BGR float32 numpy array (cv style)\n# TODO: Improve multiple views support\n# TODO: Improve speed\ndef write_pfm(filename, img, scale=1):\n    """"""Writes an OpenCV image into pfm format on disk.\n\n    Args:\n        filename (str): Name of the image file. The .pfm extension is not added.\n        img (Array): Numpy Array containing the image (OpenCV view hwc-BGR)\n        scale (float): Scale factor for file. Positive for big endian,\n            otherwise little endian. The number tells the units of the samples\n            in the raster (default 1)\n    """"""\n    if img.dtype.name != \'float32\':\n        raise TypeError(\'Image dtype must be float32.\')\n\n    with open(filename, \'w\') as file:\n        file.write(\'PF\\n\' if img.shape[2] == 3 else \'Pf\\n\')\n        file.write(\'{w} {h}\\n\'.format(w=img.shape[1], h=img.shape[0]))\n\n        endian = img.dtype.byteorder\n\n        if endian == \'<\' or endian == \'=\' and sys.byteorder == \'little\':\n            scale = -scale\n\n        file.write(\'%f\\n\' % scale)\n        img = np.flip(np.flip(img, 2), 0)\n        img.tofile(file)\n\n# returns the image in hwc - BGR (cv style)\n# TODO: Improve multiple views support\n# TODO: Improve speed\ndef load_pfm(filename):\n    """"""Loads a pfm image file from disk into a Numpy Array (OpenCV view).\n\n    Supports HDR and LDR image formats.\n    \n    Args:\n        filename (str): Name of pfm image file.\n    """"""\n    filename = process(filename)\n    with open(filename, ""r"", encoding=""ISO-8859-1"") as file:\n        nc = 3 if file.readline().rstrip() == ""PF"" else 1\n        width, height = [int(x) for x in file.readline().rstrip().split()]\n        shape = (height, width, nc)\n        img = np.fromfile(file, \'{0}{1}\'.format(""<"" if float(file.readline().rstrip()) < 0 else "">"",\'f\') )\n        img = np.reshape(img, shape)\n        return np.flip(np.flip(img, 2), 0).copy()\n\ndef load_dng(filename, **kwargs):\n    """"""Loads a dng image file from disk into a float32 Numpy Array (OpenCV view).\n\n    Requires rawpy.\n\n    Args:\n        filename (str): Name of pfm image file.\n        **kwargs: Extra keyword arguments to pass to `rawpy.postprocess()`.\n    """"""\n    import rawpy\n    filename = process(filename)\n    with rawpy.imread(filename) as raw:\n        default_kwargs = dict(gamma=(1,1), no_auto_bright=True, output_bps=16)\n        default_kwargs.update(kwargs)\n        img = raw.postprocess(**default_kwargs)\n    return rgb2bgr(img,-1).astype(\'float32\')\n\n# Accepts hwc - BGR float32 numpy array (cv style)\n# TODO: Improve multiple views support\n# TODO: Improve speed\ndef imwrite(filename, img, *args, **kwargs):\n    """"""Writes an image to disk. Supports HDR and LDR image formats.\n\n    Args:\n        filename (str): Name of image file.\n        img (Array): Numpy Array containing the image (OpenCV view hwc-BGR).\n        *args: Extra arguments to pass to cv2.imwrite or write_pfm if saving a\n            .pfm image.\n        **kwargs: Extra keyword arguments to pass to cv2.imwrite or write_pfm\n            if saving a .pfm image.\n    """"""\n    ext = os.path.splitext(filename)[1]\n    if ext.lower() == \'.pfm\':\n        write_pfm(filename, img, *args, **kwargs)\n    else:\n        cv2.imwrite(filename, img, *args, **kwargs)\n\ndef imread(filename):\n    """"""Reads an image file from disk into a Numpy Array (OpenCV view).\n\n    Args:\n        filename (str): Name of pfm image file.\n    """"""\n    filename = process(filename)\n    ext = os.path.splitext(filename)[1]\n    if ext.lower() == \'.pfm\':\n        return load_pfm(filename)\n    elif ext.lower() == \'.dng\':\n        return load_dng(filename)\n    else:\n        loaded = cv2.imread(filename, flags=cv2.IMREAD_ANYDEPTH + cv2.IMREAD_COLOR)\n        if loaded is None:\n            raise IOError(\'Could not read {0}\'.format(filename))\n        else:\n            return loaded\n\ndef load_encoded(filename):\n    """"""Loads a file as a Numpy Byte (uint8) Array.\n\n    Args:\n        filename (str): Name of file.\n    """"""\n    return np.fromfile(filename, dtype=\'uint8\')\n\ndef decode_loaded(x):\n    """"""Decodes an image stored in a Numpy Byte (uint8) Array using OpenCV.\n\n    Args:\n        x: The Numpy Byte (uint8) Array.\n    """"""\n    return cv2.imdecode(x, flags=cv2.IMREAD_ANYDEPTH + cv2.IMREAD_COLOR)'"
dlt/train/__init__.py,0,b'from .basetrainer import BaseTrainer\nfrom .ganbasetrainer import GANBaseTrainer\nfrom .fishergan import FisherGANTrainer\nfrom .vanilla import VanillaTrainer\nfrom .vanillagan import VanillaGANTrainer\nfrom .wgangp import WGANGPTrainer\nfrom .wganct import WGANCTTrainer\nfrom .began import BEGANTrainer'
dlt/train/basetrainer.py,1,"b'import torch\nfrom ..util import barit\n\nclass BaseTrainer(object):\n    """"""Generic Base trainer object to inherit functionality from.""""""\n    def __init__(self):\n        self.training = True\n        self.device = \'cpu\'\n        self._models = {}\n        self._optimizers = {}\n        self._losses = {}\n        self.epoch = 1\n    \n    def cuda(self, device=0):\n        """"""Sets the trainer to GPU mode. \n        \n        If flagged, the data is cast to GPU before every iteration after being\n        retrieved from the loader.\n        """"""\n        self.device = \'cuda:{0}\'.format(device)\n\n    def cpu(self, device=0):\n        """"""Sets the trainer to CPU mode""""""\n        self.device = \'cpu:{0}\'.format(device)\n\n    def train(self):\n        """"""Sets the trainer and models to training mode""""""\n        self.training = True\n        for _, m in self._models.items():\n            m.train()\n\n    def eval(self):\n        """"""Sets the trainer and models to inference mode""""""\n        self.training = False\n        for _, m in self._models.items():\n            m.eval()\n\n    def loss_names(self, training=None):\n        """"""Returns the name(s)/key(s) of the training or validation loss(es).\n        \n        Args:\n            training (bool, optional): If provided then the training or\n                validation losses are returned if True or False respectively.\n                If not provided the current mode loss is returned.\n        """"""\n        condition = self.training if training is None else training\n        mode = \'training\' if condition else \'validation\'\n        return self._losses[mode]\n\n    def loss_names_training(self):\n        """"""Returns the name(s)/key(s) of the training loss(es).""""""\n        return self.loss_names(True)\n\n    def loss_names_validation(self):\n        """"""Returns the name(s)/key(s) of the validation loss(es).""""""\n        return self.loss_names(False)\n\n    def _to(self, data):\n        if any([isinstance(data, x) for x in [set, list, tuple]]):\n            return type(data)(self._to(x) for x in data)\n        else:\n            return data.detach().to(self.device)\n\n    def iteration(self, data):\n        raise NotImplementedError\n\n    def iterate(self, loader):\n        """"""Performs an epoch of training or validation.\n        \n        Args:\n            loader (iterable): The data loader.\n        """"""\n        \n        torch.set_grad_enabled(self.training)\n\n        for data in barit(loader, start=\'Training\' if self.training else \'Validation\'):\n            data = self._to(data)\n            yield data, self.iteration(data)\n        \n        if self.training:\n            self.epoch += 1\n\n        return\n\n    __call__ = iterate\n\n    def __getstate__(self):\n        return self.state_dict()\n\n    def __setstate__(self, state):\n        self.load_state_dict(state)\n\n    def state_dict(self):\n        """"""Returns the state of the trainer as a :class:`dict`.\n\n        It contains an entry for every variable in self.__dict__ which\n        is not the one of the models or optimizers.\n        """"""\n        return {key: value for key, value in self.__dict__.items() \n                if key not in [\'_optimizers\', \'_losses\', \'_models\']}\n\n    def load_state_dict(self, state_dict):\n        """"""Loads the trainers state.\n\n        Arguments:\n            state_dict (dict): scheduler state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        """"""\n        self.__dict__.update(state_dict)'"
dlt/train/began.py,2,"b'import math\nfrom .ganbasetrainer import GANBaseTrainer\n\nclass BEGANTrainer(GANBaseTrainer):\n    """"""Boundary Equilibrium GAN trainer. \n        \n    Args:\n        generator (nn.Module): The generator network.\n        discriminator (nn.Module): The discriminator network.\n        g_optimizer (torch.optim.Optimizer): Generator Optimizer.\n        d_optimizer (torch.optim.Optimizer): Discriminator Optimizer.\n        lambda_k (float): Learning rate of k parameter.\n        gamma (float): Diversity ratio.\n        d_iter (int): Number of discriminator steps per generator step.\n\n    Each iteration returns the mini-batch and a tuple containing:\n\n        - The generator prediction.\n        - A dictionary containing a `d_loss` (not when validating) and a \n          `g_loss` dictionary (only if a generator step is performed):\n            \n            - `d_loss contains`: `d_loss`, `real_loss`, `fake_loss`, `k`,\n              `balance`, and `measure`.\n            - `g_loss` contains: `g_loss`.\n    \n    Example:\n\n    .. code-block:: python3\n    \n        >>> trainer = dlt.train.BEGANTrainer(gen, disc, g_optim, d_optim, lambda_k, gamma)\n        >>> # Training mode\n        >>> trainer.train()\n        >>> for batch, (prediction, loss) in trainer(train_data_loader):\n        >>>     print(loss[\'d_loss\'][\'measure\'])\n    """"""\n\n    def __init__(self, generator, discriminator, g_optimizer, d_optimizer, lambda_k, gamma, d_iter=1):\n        super(BEGANTrainer, self).__init__(generator, discriminator, g_optimizer, d_optimizer, d_iter)\n        # Register losses\n        self._losses[\'training\'] = [\'d_loss\', \'real_loss\', \'fake_loss\', \'k\', \'balance\', \'measure\', \'g_loss\']\n        self._losses[\'validation\'] = [\'g_loss\']\n        self.k = 0.0\n        self.lambda_k = lambda_k\n        self.gamma = gamma\n\n\n    def d_step(self, g_input, real_input):\n        disc, gen = self._models[\'discriminator\'], self._models[\'generator\']\n        self._set_gradients(\'discriminator\', True)\n        self._set_gradients(\'generator\', False)\n\n        prediction = gen(g_input)\n\n        fake_loss = (disc(prediction) - prediction).abs().mean()\n        real_loss = (disc(real_input) - real_input).abs().mean()\n        \n        d_loss = real_loss - self.k*fake_loss\n        \n        disc.zero_grad()\n        d_loss.backward()\n        self._optimizers[\'discriminator\'].step()\n\n        balance = (self.gamma * real_loss.item() - fake_loss.item())\n        self.k = min(max(self.k + self.lambda_k*balance, 0), 1)\n        measure = real_loss.item()  + math.fabs(balance)\n\n        ret_losses = {\'d_loss\': d_loss.item(),\n                      \'real_loss\': real_loss.item(),\n                      \'fake_loss\': fake_loss.item(),\n                      \'k\': self.k, \'measure\': measure, \'balance\': balance}\n        self.d_iter_counter += 1\n        return prediction, ret_losses\n\n    def g_step(self, g_input, real_input):\n        disc, gen = self._models[\'discriminator\'], self._models[\'generator\']\n        self._set_gradients(\'discriminator\', False)\n        self._set_gradients(\'generator\', True)\n            \n        prediction = gen(g_input)\n        loss = (disc(prediction) - prediction).abs().mean()\n        \n        if self.training:\n            gen.zero_grad()\n            loss.backward()\n            self._optimizers[\'generator\'].step()\n\n        return prediction, {\'g_loss\': loss.item()}\n'"
dlt/train/fishergan.py,3,"b'import torch\nfrom .ganbasetrainer import GANBaseTrainer\n\nclass FisherGANTrainer(GANBaseTrainer):\n    """"""Fisher GAN trainer. \n    \n    Args:\n        generator (nn.Module): The generator network.\n        discriminator (nn.Module): The discriminator network.\n        g_optimizer (torch.optim.Optimizer): Generator Optimizer.\n        d_optimizer (torch.optim.Optimizer): Discriminator Optimizer.\n        rho (float): Quadratic penalty weight.\n        d_iter (int, optional): Number of discriminator steps per generator\n            step (default 1).\n\n    Each iteration returns the mini-batch and a tuple containing:\n\n        - The generator prediction.\n        - A dictionary containing a `d_loss` (not when validating) and a \n          `g_loss` dictionary (only if a generator step is performed):\n            \n            - `d_loss contains`: `ipm_enum`, `ipm_denom`, `ipm_ratio`, \n              `d_loss`, `constraint`, `epf`, `eqf`, `epf2`, `eqf2` and\n              `lagrange`.\n            - `g_loss` contains: `g_loss`.\n\n    Example:\n        >>> trainer = dlt.train.FisherGANTrainer(gen, disc, g_optim, d_optim, rho)\n        >>> # Training mode\n        >>> trainer.train()\n        >>> for batch, (prediction, loss) in trainer(train_data_loader):\n        >>>     print(loss[\'d_loss\'][\'constraint\'])\n    """"""\n    def __init__(self, generator, discriminator, g_optimizer, d_optimizer, rho, d_iter=1):\n        super(FisherGANTrainer, self).__init__(generator, discriminator, g_optimizer, \n                                                d_optimizer, d_iter)\n        # Register losses\n        self._losses[\'training\'] = [\'ipm_enum\', \'ipm_denom\', \'ipm_ratio\', \'d_loss\', \'constraint\', \n                                    \'epf\', \'eqf\', \'epf2\', \'eqf2\', \'lagrange\', \'g_loss\']\n        self._losses[\'validation\'] = [\'g_loss\']\n        self.rho = rho\n        self.alpha = torch.zeros(1, requires_grad=True)\n\n    def d_step(self, g_input, real_input):\n        disc, gen = self._models[\'discriminator\'], self._models[\'generator\']\n        self._set_gradients(\'discriminator\', True)\n        self._set_gradients(\'generator\', False)\n        self.alpha.to(g_input.device)\n    \n        prediction = gen(g_input)\n        vphi_fake = disc(prediction)\n        vphi_real = disc(real_input)\n\n        epf, eqf = vphi_real.mean(), vphi_fake.mean()\n        epf2, eqf2 = (vphi_real**2).mean(), (vphi_fake**2).mean()\n        constraint = (1- (0.5*epf2 + 0.5*eqf2))\n        d_loss = -(epf - eqf + self.alpha*constraint - self.rho/2 * constraint**2)\n        \n        disc.zero_grad()\n        d_loss.backward()\n        self._optimizers[\'discriminator\'].step()\n        self.alpha = self.alpha + self.rho * self.alpha.grad\n        self.alpha.detach().requires_grad_(True)\n\n        # IPM\n        ipm_enum = epf.item() - eqf.item()\n        ipm_denom = (0.5*epf2.item() + 0.5*eqf2.item())**0.5\n        ipm_ratio = ipm_enum/ipm_denom\n        ret_losses = {\'ipm_enum\': ipm_enum, \'ipm_denom\': ipm_denom, \n                      \'ipm_ratio\': ipm_ratio, \n                      \'d_loss\': -d_loss.item(), \n                      \'constraint\': 1 - constraint.item(),\n                      \'epf\': epf.item(),\n                      \'eqf\': eqf.item(),\n                      \'epf2\': epf2.item(),\n                      \'eqf2\': eqf2.item(),\n                      \'lagrange\': self.alpha.item()}\n        self.d_iter_counter += 1\n        return prediction, ret_losses\n\n    def g_step(self, g_input, real_input):\n        disc, gen = self._models[\'discriminator\'], self._models[\'generator\']\n        self._set_gradients(\'discriminator\', False)\n        self._set_gradients(\'generator\', True)\n\n        prediction = gen(g_input)\n        loss = - disc(prediction).mean()\n\n        if self.training:\n            gen.zero_grad()\n            loss.backward()\n            self._optimizers[\'generator\'].step()\n\n        return prediction, {\'g_loss\': loss.item()}'"
dlt/train/ganbasetrainer.py,2,"b'from .basetrainer import BaseTrainer\n\nclass GANBaseTrainer(BaseTrainer):\n    """"""Base Trainer to inherit functionality from for training Generative Adversarial Networks.\n    \n    Args:\n        generator (nn.Module): The generator network.\n        discriminator (nn.Module): The discriminator network.\n        g_optimizer (torch.optim.Optimizer): Generator Optimizer.\n        d_optimizer (torch.optim.Optimizer): Discriminator Optimizer.\n        d_iter (int): Number of discriminator steps per generator step.\n\n    Inherits from :class:`dlt.train.BaseTrainer`\n    """"""\n    def __init__(self, generator, discriminator, g_optimizer, d_optimizer, d_iter):\n        super(GANBaseTrainer, self).__init__()\n        # Register models\n        self._models[\'generator\'] = generator\n        self._models[\'discriminator\'] = discriminator\n        self._optimizers[\'generator\'] = g_optimizer\n        self._optimizers[\'discriminator\'] = d_optimizer\n\n        self.d_iter = d_iter\n        self.d_iter_counter = 0\n\n    def iteration(self, data):\n        if self.training:\n            pred, losses = self.d_step(data[0], data[1])\n            if self.d_iter_counter % self.d_iter == 0:\n                _, g_loss = self.g_step(data[0], data[1])\n                losses.update(g_loss)\n        else:\n            pred, losses = self.g_step(data[0], data[1])\n        return pred, losses\n\n    def d_step(self, g_input, real_input):\n        raise NotImplementedError\n\n    def g_step(self, g_input, real_input):\n        raise NotImplementedError\n\n    def _set_gradients(self, model, value):\n        for p in self._models[model].parameters():\n            p.requires_grad_(value)'"
dlt/train/vanilla.py,2,"b'import torch\nfrom torch.autograd import Variable\nfrom .basetrainer import BaseTrainer\n\nclass VanillaTrainer(BaseTrainer):\n    """"""Training of a network using a criterion/loss function.\n    \n    Args:\n        model (nn.Module): The network to train.\n        criterion (callable): The function to optimize.\n        optimizer (torch.optim.Optimizer): A torch Optimizer.\n\n    Each iteration returns the mini-batch and a tuple containing:\n\n        - The model prediction.\n        - A dictionary with the `training_loss` or `validation_loss`\n          (along with the partial losses, if criterion returns a dictionary).\n    \n    Example:\n        >>> trainer = dlt.train.VanillaTrainer(my_model, nn.L1Loss(), my_optimizer)\n        >>> # Training mode\n        >>> trainer.train()\n        >>> for batch, (prediction, loss) in trainer(train_data_loader):\n        >>>     print(loss[\'training_loss\'])\n        >>> # Validation mode\n        >>> trainer.eval()\n        >>> for batch, (prediction, loss) in trainer(valid_data_loader):\n        >>>     print(loss[\'validation_loss\'])\n\n    Note:\n        If the criterion returns a dict of (named) losses, then they are added\n        together to backpropagate. The total is returned along with all the\n        partial losses.\n    """"""\n    def __init__(self, model, criterion, optimizer):\n        super(VanillaTrainer, self).__init__()\n        # Register models and losses\n        self._models[\'model\'] = model\n        self._optimizers[\'optimizer\'] = optimizer\n        \n        self._losses[\'training\'] = [\'training_loss\']\n        self._losses[\'validation\'] = [\'validation_loss\']\n\n        self.criterion = criterion\n        if hasattr(criterion, \'loss_list\'):\n            for loss in criterion.loss_list:\n                self._losses[\'training\'] += [loss]\n                self._losses[\'validation\'] += [loss]\n\n\n    def iteration(self, data):\n        loss_key = \'training_loss\' if self.training else \'validation_loss\'\n        v_pred = self._models[\'model\'](data[0])\n        losses = self.criterion(v_pred, data[1])\n        if isinstance(losses, dict):\n            total_loss = 0\n            for key, val in losses.items():\n                total_loss = total_loss + val\n                losses[key] = val.item()\n            losses[loss_key] = total_loss.item()\n        else:\n            total_loss = losses\n            losses = {loss_key: total_loss.item()}\n\n        if self.training:\n            self._optimizers[\'optimizer\'].zero_grad()\n            total_loss.backward()\n            self._optimizers[\'optimizer\'].step()\n        \n        return v_pred, losses\n'"
dlt/train/vanillagan.py,7,"b'import torch\nfrom torch.autograd import Variable\nfrom .ganbasetrainer import GANBaseTrainer\n\nclass VanillaGANTrainer(GANBaseTrainer):\n    """"""Generative Adversarial Networks trainer. \n        \n    Args:\n        generator (nn.Module): The generator network.\n        discriminator (nn.Module): The discriminator network.\n        g_optimizer (torch.optim.Optimizer): Generator Optimizer.\n        d_optimizer (torch.optim.Optimizer): Discriminator Optimizer.\n        d_iter (int, optional): Number of discriminator steps per generator\n            step (default 1).\n\n    Each iteration returns the mini-batch and a tuple containing:\n\n        - The generator prediction.\n        - A dictionary containing a `d_loss` (not when validating) and a \n          `g_loss` dictionary (only if a generator step is performed):\n            \n            - `d_loss contains`: `d_loss`, `real_loss`, and `fake_loss`.\n            - `g_loss` contains: `g_loss`.\n\n    Example:\n        >>> trainer = dlt.train.VanillaGANTrainer(gen, disc, g_optim, d_optim)\n        >>> # Training mode\n        >>> trainer.train()\n        >>> for batch, (prediction, loss) in trainer(train_data_loader):\n        >>>     print(loss[\'d_loss\'][\'d_loss\'])\n\n    Warning:\n        This trainer uses BCEWithLogitsLoss, which means that the discriminator\n        must NOT have a sigmoid at the end.\n    """"""\n    def __init__(self, generator, discriminator, g_optimizer, d_optimizer, d_iter=1):\n        super(VanillaGANTrainer, self).__init__(generator, discriminator, g_optimizer, \n                                                d_optimizer, d_iter)\n        # Register losses\n        self._losses[\'training\'] = [\'d_loss\', \'fake_loss\', \'real_loss\', \'g_loss\']\n        self._losses[\'validation\'] = [\'g_loss\']\n        self.bce = torch.nn.BCEWithLogitsLoss()\n\n    def d_step(self, g_input, real_input):\n        batch_size = g_input.size(0)\n        disc, gen = self._models[\'discriminator\'], self._models[\'generator\']\n        self._set_gradients(\'discriminator\', True)\n        self._set_gradients(\'generator\', False)\n\n        prediction = gen(g_input)\n\n        real_label = torch.ones(batch_size, 1, device=prediction.device)\n        fake_label = torch.zeros(batch_size, 1, device=prediction.device)\n\n        loss_fake = self.bce(disc(prediction), fake_label)\n        loss_real = self.bce(disc(real_input), real_label)\n\n        total_loss = loss_fake + loss_real\n        \n        disc.zero_grad()\n        total_loss.backward()\n        self._optimizers[\'discriminator\'].step()\n\n        ret_losses = {\'d_loss\': total_loss.item(), \n                      \'real_loss\': loss_real.item(), \n                      \'fake_loss\': loss_fake.item()}\n        self.d_iter_counter += 1\n        return prediction, ret_losses\n\n\n    def g_step(self, g_input, real_input):\n        batch_size = g_input.size(0)\n        disc, gen = self._models[\'discriminator\'], self._models[\'generator\']\n        self._set_gradients(\'discriminator\', False)\n        self._set_gradients(\'generator\', True)\n\n        prediction = gen(g_input)\n        # fake labels are real for generator cost (optimization trick from GAN paper)\n        d_prediction = disc(prediction)\n        real_label = torch.ones(batch_size, 1, device=d_prediction.device)\n        loss = self.bce(d_prediction, real_label)\n        \n        if self.training:\n            gen.zero_grad()\n            loss.backward()\n            self._optimizers[\'generator\'].step()\n\n        return prediction, {\'g_loss\': loss.item()}\n'"
dlt/train/wganct.py,7,"b'import torch\nfrom torch import autograd\nfrom torch.autograd import Variable\nfrom .ganbasetrainer import GANBaseTrainer\n\nclass WGANCTTrainer(GANBaseTrainer):\n    """"""Wasserstein GAN Trainer with gradient penalty and correction term.\n       \n       From Improving the Improved Training of Wasserstein GANs: A Consistency\n       Term and Its Dual Effect.\n\n       https://openreview.net/forum?id=SJx9GQb0-\n    \n    Args:\n        generator (nn.Module): The generator network.\n        discriminator (nn.Module): The discriminator network.\n        g_optimizer (torch.optim.Optimizer): Generator Optimizer.\n        d_optimizer (torch.optim.Optimizer): Discriminator Optimizer.\n        lambda_gp (float): Weight of gradient penalty.\n        m_ct (float): Constant bound for consistency term.\n        lambda_ct (float): Weight of consistency term.\n        d_iter (int, optional): Number of discriminator steps per generator\n            step (default 1).\n\n    Each iteration returns the mini-batch and a tuple containing:\n\n        - The generator prediction.\n        - A dictionary containing a `d_loss` (not when validating) and a \n          `g_loss` dictionary (only if a generator step is performed):\n            \n            - `d_loss contains`: `d_loss`, `w_loss`, `gp` and `ct`.\n            - `g_loss` contains: `g_loss`.\n\n    Warning:\n\n        The discriminator forward function needs to be able to accept an optional\n        bool argument `correction_term`. When set to true, the forward function\n        must add dropout noise to the model and return a tuple containing the \n        second to last output of the discriminator along with the final output.\n\n\n    Example:\n        >>> trainer = dlt.train.WGANCTTrainer(gen, disc, g_optim, d_optim, lambda_gp, m_ct, lambda_ct)\n        >>> # Training mode\n        >>> trainer.train()\n        >>> for batch, (prediction, loss) in trainer(train_data_loader):\n        >>>     print(loss[\'d_loss\'][\'w_loss\'])\n    """"""\n    def __init__(self, generator, discriminator, g_optimizer, d_optimizer, lambda_gp, m_ct, lambda_ct, d_iter=1):\n        super(WGANCTTrainer, self).__init__(generator, discriminator, g_optimizer, \n                                                d_optimizer, d_iter)\n        # Register losses\n        self._losses[\'training\'] = [\'w_loss\', \'d_loss\', \'gp\', \'ct\', \'g_loss\']\n        self._losses[\'validation\'] = [\'g_loss\']\n        self.lambda_gp = lambda_gp\n        self.m_ct = m_ct\n        self.lambda_ct = lambda_ct\n        \n    def d_step(self, g_input, real_input):\n        disc, gen = self._models[\'discriminator\'], self._models[\'generator\']\n        self._set_gradients(\'discriminator\', True)\n        self._set_gradients(\'generator\', False)\n\n        \n        prediction = gen(g_input)\n        error_fake = disc(prediction).mean()\n        error_real = disc(Variable(real_input)).mean()\n\n        gp = self.get_gp(prediction, real_input)\n        ct = self.get_ct(real_input)\n        w_loss = error_fake - error_real\n        total_loss = w_loss + gp + ct\n\n        disc.zero_grad()\n        total_loss.backward()\n        self._optimizers[\'discriminator\'].step()\n\n        ret_losses = {\'w_loss\': w_loss.item(), \'gp\': gp.item(),\n                      \'ct\': ct.item(), \'d_loss\': total_loss.item()}\n        self.d_iter_counter += 1\n        return prediction, ret_losses\n\n    def g_step(self, g_input, real_input):\n        disc, gen = self._models[\'discriminator\'], self._models[\'generator\']\n        self._set_gradients(\'discriminator\', False)\n        self._set_gradients(\'generator\', True)\n\n        prediction = gen(Variable(g_input))\n        loss = - disc(prediction).mean()\n        \n        if self.training:    \n            gen.zero_grad()\n            loss.backward()\n            self._optimizers[\'generator\'].step()\n\n        return prediction, {\'g_loss\': loss.item()}\n\n    def get_gp(self, fake_input, real_input):\n        dimensions = [real_input.size(0)] + [1] * (real_input.ndimension() - 1)\n        alpha = torch.Tensor(*dimensions).to(real_input.device).uniform_()\n        interpolates = alpha * real_input + ((1 - alpha) * fake_input)\n        interpolates.requires_grad_(True)\n        disc_interpolates = self._models[\'discriminator\'](interpolates)\n\n        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                                  grad_outputs=torch.ones_like(disc_interpolates),\n                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n        gradients = gradients.view(gradients.size(0), -1)\n        gradient_penalty = torch.mean((1. - torch.sqrt(1e-8+torch.sum(gradients**2, dim=1)))**2)*self.lambda_gp\n        return gradient_penalty\n\n    def l2_norm(self, x):\n        return x.pow(2).view(x.size(0), -1).sum(-1).add(1e-8).sqrt()\n\n    def get_ct(self, real_input):\n        dx_dash_n2last, dx_dash = self._models[\'discriminator\'](real_input, correction_term=True)\n        dx_dashdash_n2last, dx_dashdash = self._models[\'discriminator\'](real_input, correction_term=True)\n        res = self.l2_norm(dx_dash - dx_dashdash) + 0.1 \\\n              * self.l2_norm(dx_dash_n2last - dx_dashdash_n2last) \\\n              - self.m_ct\n        return torch.nn.functional.relu(res, 0).mean()*self.lambda_ct\n'"
dlt/train/wgangp.py,6,"b'import torch\nfrom torch import autograd\nfrom torch.autograd import Variable\nfrom .ganbasetrainer import GANBaseTrainer\n\nclass WGANGPTrainer(GANBaseTrainer):\n    """"""Wasserstein GAN Trainer with gradient penalty. \n    \n    Args:\n        generator (nn.Module): The generator network.\n        discriminator (nn.Module): The discriminator network.\n        g_optimizer (torch.optim.Optimizer): Generator Optimizer.\n        d_optimizer (torch.optim.Optimizer): Discriminator Optimizer.\n        lambda_gp (float): Weight of gradient penalty.\n        d_iter (int, optional): Number of discriminator steps per generator\n            step (default 1).\n\n    Each iteration returns the mini-batch and a tuple containing:\n\n        - The generator prediction.\n        - A dictionary containing a `d_loss` (not when validating) and a \n          `g_loss` dictionary (only if a generator step is performed):\n            \n            - `d_loss contains`: `d_loss`, `w_loss`, and `gp`.\n            - `g_loss` contains: `g_loss`.\n\n    Example:\n        >>> trainer = dlt.train.WGANGPTrainer(gen, disc, g_optim, d_optim, lambda_gp)\n        >>> # Training mode\n        >>> trainer.train()\n        >>> for batch, (prediction, loss) in trainer(train_data_loader):\n        >>>     print(loss[\'d_loss\'][\'w_loss\'])\n    """"""\n    def __init__(self, generator, discriminator, g_optimizer, d_optimizer, lambda_gp, d_iter=1):\n        super(WGANGPTrainer, self).__init__(generator, discriminator, g_optimizer, \n                                                d_optimizer, d_iter)\n        # Register losses\n        self._losses[\'training\'] = [\'w_loss\', \'d_loss\', \'gp\', \'g_loss\']\n        self._losses[\'validation\'] = [\'g_loss\']\n        self.lambda_gp = lambda_gp\n                \n    def d_step(self, g_input, real_input):\n        disc, gen = self._models[\'discriminator\'], self._models[\'generator\']\n        self._set_gradients(\'discriminator\', True)\n        self._set_gradients(\'generator\', False)\n\n        \n        prediction = gen(g_input)\n        error_fake = disc(prediction).mean()\n        error_real = disc(Variable(real_input)).mean()\n\n        gp = self.get_gp(prediction, real_input)\n        w_loss = error_fake - error_real\n        total_loss = w_loss + gp\n\n        disc.zero_grad()\n        total_loss.backward()\n        self._optimizers[\'discriminator\'].step()\n\n        ret_losses = {\'w_loss\': w_loss.item(), \'gp\': gp.item(), \'d_loss\': total_loss.item()}\n        self.d_iter_counter += 1\n        return prediction, ret_losses\n\n    def g_step(self, g_input, real_input):\n        disc, gen = self._models[\'discriminator\'], self._models[\'generator\']\n        self._set_gradients(\'discriminator\', False)\n        self._set_gradients(\'generator\', True)\n\n        prediction = gen(Variable(g_input))\n        loss = - disc(prediction).mean()\n        \n        if self.training:    \n            gen.zero_grad()\n            loss.backward()\n            self._optimizers[\'generator\'].step()\n\n        return prediction, {\'g_loss\': loss.item()}\n\n    def get_gp(self, fake_input, real_input):\n        dimensions = [real_input.size(0)] + [1] * (real_input.ndimension() - 1)\n        alpha = torch.Tensor(*dimensions).to(real_input.device).uniform_()\n        interpolates = alpha * real_input + ((1 - alpha) * fake_input)\n        interpolates.requires_grad_(True)\n        disc_interpolates = self._models[\'discriminator\'](interpolates)\n\n        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                                  grad_outputs=torch.ones_like(disc_interpolates),\n                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n        gradients = gradients.view(gradients.size(0), -1)\n        gradient_penalty = torch.mean((1. - torch.sqrt(1e-8+torch.sum(gradients**2, dim=1)))**2)*self.lambda_gp\n        return gradient_penalty'"
dlt/util/__init__.py,0,"b""from .barit import barit\nfrom .checkpointer import Checkpointer\nfrom .dispatch import dispatch\nfrom .func import applier\nfrom .grid import make_grid\nfrom .layers import out_size, in_size, kernel_size, stride_size, padding_size, \\\n    dilation_size, find_layers\nfrom .logger import Logger\nfrom .meter import Averages\nfrom .misc import slide_window_, re_stride, moving_avg, moving_var, sub_avg, \\\n    sub_var, has_nan, has_inf, replace_specials_, replace_inf_, replace_nan_, \\\n    map_range, str2bool, str_is_int, is_tensor, is_cuda, is_array, \\\n    to_array, to_tensor, permute, hwc2chw, chw2hwc, channel_flip, rgb2bgr, \\\n    bgr2rgb, change_view, cv2torch, torch2cv, cv2plt, plt2cv, plt2torch, \\\n    torch2plt, replicate, _determine_view, count_parameters\nfrom . import paths\nfrom .sample import index_gauss, slice_gauss, index_uniform, slice_uniform\nfrom .slurm import slurm\nfrom .data import LoadedDataset, DirectoryDataset\nfrom .external import accuracy, compose\nfrom .imagesampler import ImageSampler\n\n__all__ = [\n    'barit', 'Checkpointer', 'dispatch', 'compose', 'applier', 'make_grid',\n    'out_size', 'in_size', 'kernel_size', 'stride_size', 'padding_size', 'dilation_size', \n    'find_layers', 'Logger', 'Averages', 'slide_window_', 're_stride', 'moving_avg', \n    'moving_var', 'sub_avg', 'sub_var', 'has_nan', 'has_inf', 'replace_specials_', 'replace_inf_',\n    'replace_nan_', 'map_range', 'str2bool', 'str_is_int', 'is_tensor',\n    'is_cuda', 'is_array', 'to_array', 'to_tensor', 'permute', 'hwc2chw', 'chw2hwc',\n    'channel_flip', 'rgb2bgr', 'bgr2rgb', 'change_view', 'cv2torch', 'torch2cv', 'cv2plt',\n    'plt2cv', 'plt2torch', 'torch2plt', 'replicate', 'paths', 'index_gauss', 'slice_gauss',\n    'index_uniform', 'slice_uniform', 'slurm', '_determine_view', 'count_parameters',\n    'LoadedDataset', 'DirectoryDataset', 'accuracy', 'ImageSampler'\n]"""
dlt/util/barit.py,0,"b'import sys\nimport time\nimport logging\n\ndef barit(iterable, start=None, end=None, time_it=True, length=20, leave=True, filler=\'=\'):\n    """"""Minimal progress bar for iterables.\n\n    Args:\n        iterable (list or tuple etc): An iterable.\n        start (str, optional): String to place infront of the progress bar (default None).\n        end (str, optional): String to add at the end of the progress bar (default None).\n        timeit (bool, optional): Print elapsed time and ETA (default True).\n        length (int, optional): Length of the progress bar not including ends (default 20).\n        leave (bool, optional): If False, it deletes the progress bar once it ends (default True).\n        filler (str, optional): Filler character for the progress bar (default \'=\'). \n\n    Example:\n        >>> for _ in dlt.util.barit(range(100), start=\'Count\'):\n        >>>     time.sleep(0.02)\n        Count: [====================] 100.0%, (10/10), Total: 0.2s, ETA: 0.0s\n  \n    Note:\n        barit can be put to silent mode using:\n\n            >>> dlt.util.silent = True\n   \n     For a progress bar with more functionality have a look at tqdm_.\n        \n        .. _tqdm: https://github.com/tqdm/tqdm\n\n    """"""\n    if barit.silent:\n        for x in iterable:\n            yield x\n    else:\n        n_iter = len(iterable)\n        start = \'\' if start is None else \'{0}: \'.format(start)\n        bar = \'[{0:\' + str(length) + \'s}]\'\n        perc = \' {0:.1f}%\'\n        counts = \', ({0}/{1})\'.format(\'{0}\', n_iter)\n        time_est = \', ETA: {0:.1f}s\' if time_it else \'\'\n        elapsed_est = \', Total: {0:.1f}s\' if time_it else \'\'\n        end = \'\' if end is None else \', {0}\'.format(end)\n\n        bar_tmpl = \'\\r\'+ start + \'{bar}{perc}{counts}{elapsed_est}{time_est}\' + end\n        start_time = time.time()\n\n        def print_bar(i, last_size):\n            elapsed = (time.time()-start_time)\n            full_bar = bar_tmpl.format(\n                bar=bar.format(filler*int(i*length/n_iter)), \n                perc=perc.format(i*100/n_iter),\n                counts=counts.format(i,n_iter),\n                elapsed_est=elapsed_est.format(elapsed),\n                time_est= time_est.format(elapsed*(n_iter-i)/i) if time_it and i>0 else \'\')\n            full_bar += \' \'*(max(last_size - len(full_bar), 0))\n            if i == n_iter:\n                sys.stdout.write(full_bar)\n                sys.stdout.flush()\n            else:\n                barit.logger.info(full_bar)\n            return len(full_bar)\n        \n        last_len = print_bar(0, 0)\n        for i_iter, x in enumerate(iterable, start=1):\n            yield x\n            last_len = print_bar(i_iter, last_len)\n\n        if leave:\n            sys.stdout.write(\'\\n\')\n            sys.stdout.flush()\n        else:\n            barit.logger.info(\'\\r\'+\' \'*last_len + \'\\r\')\n\n\nbarit.silent = False\nbarit.logger = logging.getLogger(\'dlt.barit\')\nbarit.logger.setLevel(logging.INFO)\nbarit.logger.propagate = False\nsh = logging.StreamHandler()\nsh.terminator = """"\nsh.setFormatter(logging.Formatter(\'{message}\', style=\'{\'))\nbarit.logger.addHandler(sh)'"
dlt/util/checkpointer.py,11,"b'import time\nimport os\nfrom os import path\nimport torch\nfrom .paths import process\n\nclass Checkpointer(object):\n    r""""""Checkpointer for objects using torch serialization.\n\n    Args:\n        name (str): Name of the checkpointer. This will also be used for\n            the checkpoint filename.\n        directory (str, optional): Parent directory of where the checkpoints will happen.\n            A new sub-directory called checkpoints will be created (default \'.\').\n        overwrite (bool, optional): Overwrite/remove the previous checkpoint (default True).\n        verbose (bool, optional): Print a statement when loading a checkpoint (default True).\n        timestamp (bool, optional): Add a timestamp to checkpoint filenames (default False).\n        add_count (bool, optional): Add (zero-padded) counter to checkpoint filenames (default True).\n\n    Example:\n        >>> a = {\'data\': 5}\n        >>> a_chkp = dlt.util.Checkpointer(\'a_saved\')\n        >>> a_chkp.save(a)\n        >>> b = a_chkp.load()\n        {\'data\': 5}\n\n    It automatically saves and loads the `state_dict` of objects::\n\n        >>> net = nn.Sequential(nn.Linear(1,1), nn.Sigmoid())\n        >>> net_chkp = dlt.util.Checkpointer(\'net_state_dict\')\n        >>> net_chkp.save(net)\n        >>> net_chkp.load(net)\n        Sequential(\n            (0): Linear(in_features=1, out_features=1)\n            (1): Sigmoid()\n        )\n        >>> state_dict = net_chkp.load()\n        OrderedDict([(\'0.weight\', \n        -0.2286\n        [torch.FloatTensor of size 1x1]\n        ), (\'0.bias\', \n        0.8495\n        [torch.FloatTensor of size 1]\n        )])\n\n    Warning:\n        If a model is wrapped in `nn.DataParallel` then the wrapped model.module\n        (state_dict) is saved. Thus applying `nn.DataParallel` must be done after\n        using `Checkpointer.load()`.\n\n    """"""\n\n    def __init__(self, name, directory=\'.\', overwrite=True, verbose=True, timestamp=False, add_count=True):\n        self.name = name\n        self.directory = process(directory, create=True)\n        self.directory = path.join(self.directory, \'checkpoints\')\n        self.directory = process(self.directory, create=True)\n        self.overwrite = overwrite\n        self.timestamp = timestamp\n        self.add_count = add_count\n        self.verbose = verbose\n        self.chkp = path.join(self.directory, "".{0}.chkp"".format(self.name))\n        self.counter, self.filename = torch.load(self.chkp) if path.exists(self.chkp) else (0, \'\')\n\n    def _say(self, line):\n        if self.verbose:\n            print(line)\n\n    def _make_name(self, tag=None):\n        strend = \'\'\n        if tag is not None:\n            strend += \'_\' + str(tag)\n        if self.add_count:\n            strend += ""_{:07d}"".format(self.counter)\n        if self.timestamp:\n            strend += time.strftime(""_%Y-%m-%d-%H-%M-%S"")\n        filename = ""{0}{1}.pth"".format( self.name, strend)\n        self.filename = path.join( self.directory, filename)\n        return self.filename\n\n    def _set_state(self, obj, state):\n        if hasattr(obj, \'load_state_dict\'):\n            obj.load_state_dict(state)\n            return obj\n        else:\n            return state\n\n    def _get_state(self, obj):\n        if isinstance(obj, torch.nn.DataParallel):\n            obj = obj.module\n        if hasattr(obj, \'state_dict\'):\n            return obj.state_dict()\n        else:\n            return obj\n\n    def __call__(self, obj, tag=None, *args, **kwargs):\n        """"""Same as :meth:`save`""""""\n        self.save(obj, tag=tag, *args, **kwargs)\n\n    def save(self, obj, tag=None, *args, **kwargs):\n        """"""Saves a checkpoint of an object.\n\n        Args:\n            obj: Object to save (must be serializable by torch).\n            tag (str, optional): Tag to add to saved filename (default None).\n            args: Arguments to pass to `torch.save`.\n            kwargs: Keyword arguments to pass to `torch.save`.\n\n        """"""\n        self.counter += 1\n        old_filename = self.filename\n        new_filename = self._make_name(tag)\n        if new_filename == old_filename and not self.overwrite:\n            print(\'WARNING: Overwriting file in non overwrite mode.\')\n        elif self.overwrite:\n            try:\n                os.remove(old_filename)\n            except:\n                pass\n        torch.save(self._get_state(obj), new_filename, *args, **kwargs)\n        torch.save((self.counter,new_filename) , self.chkp)\n\n    def load(self, obj=None, preprocess=None, *args, **kwargs):\n        """"""Loads a checkpoint from disk.\n\n        Args:\n            obj (optional): Needed if we load the `state_dict` of an `nn.Module`.\n            preprocess (optional): Callable to preprocess the loaded object.\n            args: Arguments to pass to `torch.load`.\n            kwargs: Keyword arguments to pass to `torch.load`.\n            \n        Returns: \n            The loaded file.\n\n        """"""\n        if self.counter > 0:\n            loaded =  torch.load(self.filename, *args, **kwargs)\n            if preprocess is not None:\n                loaded = preprocess(loaded)\n            obj = self._set_state(obj,loaded)\n            self._say(""Loaded {0} checkpoint: {1}"".format(self.name, self.filename))\n        return obj\n'"
dlt/util/data.py,3,"b'import os\nfrom torch.utils.data import Dataset\nfrom .paths import process\n\n# Helper class for pre-processing loaded datasets\nclass LoadedDataset(Dataset):\n    """"""Create a torch Dataset from data in memory with on the fly pre-processing.\n\n    Useful when to use with torch DataLoader.\n\n    Args:\n        dataset (sequence or collection): A sequence or collection of data points\n            that can be indexed.\n        preprocess (callable, optional): A function that takes a single data\n            point from the dataset to preprocess on the fly (default None).\n\n    Example:\n        >>> a = [1.0, 2.0, 3.0]\n        >>> a_dataset = dlt.util.LoadedDataset(a, lambda x: x**2)\n        >>> loader = torch.utils.data.DataLoader(a_dataset, batch_size=3)\n        >>> for val in loader:\n        >>>     print(val)\n        1\n        4\n        9\n        [torch.DoubleTensor of size 3]\n\n    """"""\n    def __init__(self, dataset, preprocess=None):\n        super(LoadedDataset, self).__init__()\n        self.dataset = dataset\n        if preprocess is None:\n            preprocess = lambda x: x\n        self.preprocess = preprocess\n    def __getitem__(self, index):\n        return self.preprocess(self.dataset[index])\n    def __len__(self):\n        return len(self.dataset)\n\nclass DirectoryDataset(Dataset):\n    """"""Creates a dataset of images (no label) recursively (no structure requirement).\n    \n    Similar to `torchvision.datasets.FolderDataset`, however there is no need for\n    a specific directory structure, or data format.\n\n    Args:\n        data_root (string): Path to root directory of data.\n        extensions (list or tuple): Extensions/ending patterns of data files.\n        loader (callable): Function that loads the data files.\n        preprocess (callable, optional): A function that takes a single data\n            point from the dataset to preprocess on the fly (default None).\n\n    """"""\n    def __init__(self, data_root, extensions, load_fn, preprocess=None):\n        super(DirectoryDataset, self).__init__()\n        data_root = process(data_root)\n        self.file_list = []\n        for root, _, fnames in sorted(os.walk(data_root)):\n            for fname in fnames:\n                if any(fname.lower().endswith(extension) for extension in extensions):\n                    self.file_list.append(os.path.join(root, fname))\n        if len(self.file_list) == 0:\n            msg = \'Could not find any files with extensions:\\n[{0}]\\nin\\n{1}\'\n            raise RuntimeError(msg.format(\', \'.join(extensions),data_root))\n\n        self.preprocess = preprocess\n        self.load_fn = load_fn\n\n    def __getitem__(self, index):\n        dpoint = self.load_fn(self.file_list[index])\n        if self.preprocess is not None:\n            dpoint = self.preprocess(dpoint)\n        return dpoint\n\n    def __len__(self):\n        return len(self.file_list)'"
dlt/util/dispatch.py,0,"b'import os\nfrom os import path\nimport argparse\nfrom .paths import process, copy_to_dir, write_file\nfrom .slurm import slurm\n\ndef read_argument_file(file_path):\n    file_path = process(file_path)\n    ret = {}\n    with open(file_path, \'r\') as f:\n        for line in f:\n            split = line.strip().split(\' \')\n            ret[split[0]] = split[1]\n    return ret\n\n\n# TODO: Keep directory structure of extras copied to directory.\ndef dispatch():\n    r""""""Creates a self contained experiment in a directory\n    \n    Also usable as a command line program `dlt-dispatch`.\n\n    Example:\n        Use with command line:\n\n        .. code-block:: console\n\n            $ dlt-dispatch test_low_lr -d ~/experiments -m main.py -e models.py data.py -c settings_low_lr.cfg\n\n    Note:\n        For information on available functionality use:\n\n        .. code-block:: console\n\n            $ dlt-dispatch --help\n    """"""\n    ### Parse arguments\n    parser = argparse.ArgumentParser(description=\'Create a self contained experiment for PyTorch.\')\n    arg = parser.add_argument\n    arg(\'name\', help=\'Name of experiment\')\n    arg(\'-d\', \'--directory\', default=\'~/experiments\', help=\'Parent directory of experiment.\')\n    arg(\'-m\', \'--main\', default=\'main.py\', help=\'Main file\')\n    arg(\'-e\', \'--extras\', nargs=\'+\', default=None, help=\'Other files to be copied (e.g. model.py)\')\n    arg(\'-c\', \'--config\', default=None, help=\'File with experiment configuration\')\n    arg(\'-s\', \'--slurm_config\', default=None, help=\'File with slurm configuration\')\n    arg(\'-p\', \'--slurm_pre_code\', default=None, help=\'File with slurm code to be ran before the main script.\')\n    opt = parser.parse_args()\n\n    directory = process(path.join(opt.directory, opt.name), create=True)\n    if not path.isfile(opt.main):\n        exit(""Could not find script "" + opt.main)\n    # Copy the script and config\n    copy_to_dir(opt.main, directory)\n    if opt.config is not None:\n        copy_to_dir(opt.config, directory)\n    # Copy extras\n    if opt.extras is not None:\n        for f in opt.extras:\n            copy_to_dir(f, directory)\n    # Create run file\n    if opt.config is not None:\n        command = ""python {0} $(< {1})\\n"".format(\n            path.basename(opt.main), path.basename(opt.config))\n    else:\n        command = ""python {0}\\n"".format(path.basename(opt.main))\n\n    run_filename = write_file(""#!/bin/bash\\n\\n"" + command, \'run.sh\', directory)\n    os.chmod(run_filename, 0o775)\n    # Create slurm\n    if opt.slurm_config is not None:\n        slurm_settings = read_argument_file(opt.slurm_config)\n        if opt.slurm_pre_code is not None:\n            command = opt.slurm_pre_code + ""\\n\\n"" + command\n        slurm_settings[""--job-name""] = opt.name\n        slurm(command, directory, directives=slurm_settings)'"
dlt/util/func.py,0,"b'\ndef applier(f):\n    """"""Returns a function that applies `f` to a collection of inputs (or just one).\n\n       Useful to use in conjuction with :func:`dlt.util.compose`\n       \n       Args:\n            f (function): Function to be applied.\n\n       Returns:\n            callable: A function that applies \'f\' to collections\n\n       Example:\n            >>> pow2 = dlt.util.applier(lambda x: x**2)\n            >>> pow2(42)\n            1764\n            >>> pow2([1, 2, 3])\n            [1, 4, 9]\n            >>> pow2({\'a\': 1, \'b\': 2, \'c\': 3})\n            {\'a\': 1, \'b\': 4, \'c\': 9}\n\n    """"""\n    def apply(objs):\n        if isinstance(objs, dict):\n            return {key: f(val)  for key, val in objs.items()}\n        elif isinstance(objs, tuple):\n            return tuple( f(x) for x in objs )\n        elif isinstance(objs, list):\n            return [f(x) for x in objs]\n        else:\n            return f(objs)\n    return apply\n'"
dlt/util/grid.py,5,"b'import torch\nimport numpy as np\nfrom .misc import _determine_view, change_view, is_array, is_tensor, \\\n                  to_array, to_tensor, map_range\n\ndef make_grid(images, view=\'torch\', color=True, size=None, inter_pad=None, fill_value=0, scale_each=False):\n    """"""Creates a single image grid from a set of images.\n\n    Args:\n        images (Tensor, Array, list or tuple): Torch Tensor(s) and/or Numpy Array(s). \n        view (str, optional): The image view e.g. \'hwc-bgr\' or \'torch\'\n            (default \'torch\').\n        color (bool, optional): Treat images as colored or not (default True).\n        size (list or tuple, optional): Grid dimensions, rows x columns. (default None).\n        inter_pad (int or list/tuple, optional): Padding separating the images (default None).\n        fill_value (int, optional): Fill value for inter-padding (default 0).\n        scale_each (bool, optional): Scale each image to [0-1] (default False).\n\n    Returns:\n        Tensor or Array: The resulting grid. If any of the inputs is an Array\n        then the result is an Array, otherwise a Tensor.\n\n    Notes:\n        - Images of **different sizes are padded** to match the largest.\n        - Works for **color** (3 channels) or **grey** (1 channel/0 channel)\n          images.\n        - Images must have the same view (e.g. chw-rgb (torch))\n        - The Tensors/Arrays can be of **any dimension >= 2**. The last 2 (grey)\n          or last 3 (color) dimensions are the images and all other dimensions\n          are stacked. E.g. a 4x5x3x256x256 (torch view) input will be treated:\n\n            - As 20 3x256x256 color images if color is True.\n            - As 60 256x256 grey images if color is False.\n        \n        - If color is False, then only the last two channels are considered\n          (as hw) thus any colored images will be split into their channels.\n        - The image list can contain both **Torch Tensors and Numpy Arrays**.\n          at the same time as long as they have the same view.\n        - If size is not given, the resulting grid will be the smallest square\n          in which all the images fit. If the images are more than the given\n          size then the default smallest square is used.\n\n    Raises:\n        TypeError: If images are not Arrays, Tensors, a list or a tuple\n        ValueError: If channels or dimensions are wrong.\n\n    """"""\n\n\n    # Determine view\n    orig_view = _determine_view(view)\n    if orig_view == \'unknown\':\n        print(\'make_grid provided with unknown view: \' + view)\n        \n    # Flag if we need to convert back to array\n    should_convert_to_array = False\n    \n    if torch.typename(images) in [\'tuple\', \'list\']:\n        # Handle tuple and list\n        # First convert to tensor\n        should_convert_to_array = any([is_array(x) for x in images])\n        images = [to_tensor(im) for im in images]\n        # Change view only if mode is color (otherwise last 2 dimensions are hw)\n        if color:\n            # Change view to torch\n            if orig_view != \'torch\':\n                images = [change_view(x, orig_view, \'torch\') for x in images]\n            # Must have more than 2 dimensions\n            if any([x.dim() <= 2 for x in images]):\n                raise ValueError(\'A provided image has less than three dimensions. \'\n                                 \'Maybe you wanted to pass color=False?\')\n            # Must have 3 channels\n            if any([x.size(-3) != 3 for x in images]):\n                raise ValueError(\'A provided image does not have 3 channels. \'\n                                 \'Maybe you wanted to pass color=False or a different view?\')\n            # Make all tensors 4d\n            images = [x.unsqueeze(0) if x.dim() == 3 else x.view(-1, *x.size()[-3:]) for x in images]\n            \n        else:\n            # Make all tensors 4d with\n            images = [x.unsqueeze(0).unsqueeze(0).view(-1, 1, *x.size()[-2:]) for x in images]\n        # Pad images to match largest\n        maxh, maxw = max([x.size(-2) for x in images]), max([x.size(-1) for x in images])\n        for i, img in enumerate(images):\n            imgh, imgw = img.size(-2), img.size(-1)\n            if (img.size(-2) < maxh) or (img.size(-1) < maxw):\n                padhl = int((maxh - imgh)/2)\n                padhr = maxh - imgh - padhl\n                padwl = int((maxw - imgw)/2)\n                padwr = maxw - imgw - padwl\n                images[i] = torch.nn.functional.pad(img, (padwl, padwr, padhl, padhr))\n        images = torch.cat(images,0)\n    elif is_tensor(images) or is_array(images):\n        should_convert_to_array = is_array(images)\n        images = to_tensor(images)\n        images = change_view(images, orig_view, \'torch\')\n        if color:\n            if images.size()[-3] != 3:\n                raise ValueError(\'A provided image does not have 3 channels. \'\n                                 \'Maybe you wanted to pass color=False or a different view?\')\n            images = images.unsqueeze(0).view(-1,3,*images.size()[-2:])\n        else:\n            images = images.unsqueeze(0).unsqueeze(0).view(-1,1,*images.size()[-2:])\n    else:\n        raise TypeError(\'make_grid can only accept tuples, lists, tensors\'\n                         \' and numpy arrays. Got {0}\'.format(torch.typename(images)))\n\n    # Scale each\n    if scale_each:\n        for i in range(images.size(0)):\n            images[i] = map_range(images[i])\n\n    ### Finally create grid\n    n_images, n_chan, im_w, im_h = images.size()[0], images.size()[1],images.size()[2], images.size()[3]    \n    # Get number of columns and rows (width and height)\n    if (size is not None and n_images > size[0] * size[1]) or size is None:\n        n_row = int(np.ceil(np.sqrt(n_images)))\n        n_col = int(np.ceil(n_images / n_row))\n    else:\n        n_col = size[0]\n        n_row = size[1]\n    \n    if inter_pad is not None:\n        if isinstance(inter_pad, int):\n            inter_pad = (inter_pad, inter_pad)\n        w_pad, h_pad = inter_pad[1], inter_pad[0]\n        total_w_padding,  total_h_padding = max(w_pad,0) * (n_col - 1), max(h_pad,0) * (n_row - 1)\n    else:\n        w_pad, h_pad = 0, 0\n        total_w_padding,  total_h_padding = 0, 0\n\n    w,h = int(im_w * n_col) + total_w_padding, int(im_h * n_row) + total_h_padding\n    grid = torch.Tensor(n_chan,w,h).type_as(images).fill_(fill_value)\n    for i in range(n_images):\n        i_row = i % n_row\n        i_col = int(i/n_row)\n        grid[:,\n             i_col*(im_w + w_pad):(i_col)*(im_w + w_pad) + im_w,\n             i_row*(im_h + h_pad):(i_row)*(im_h + h_pad) + im_h].copy_(images[i])\n\n    if should_convert_to_array:\n        grid = to_array(grid)\n    if orig_view != \'torch\':\n        grid = change_view(grid, \'torch\', orig_view)\n    return grid\n'"
dlt/util/imagesampler.py,2,"b'import os\nfrom os import path\nimport torch\nfrom ..hdr import imwrite\nfrom ..util import to_array, make_grid, change_view, applier\nfrom ..util.paths import process\nfrom ..viz import imshow\n\n\nclass ImageSampler(object):\n    """"""Saves and/or displays image samples.\n\n    Args:\n        name (str): Name of the checkpointer. This will also be used for\n                the checkpoint filename.\n        directory (str, optional): Parent directory of where the samples will be saved.\n            A new sub-directory called samples will be created (default \'.\').\n        overwrite (bool, optional): Overwrite/remove the previous checkpoint (default False).\n        view (str, optional): The image view e.g. \'hwc-bgr\' or \'torch\'\n            (default \'torch\').\n        ext (str, optional): The image format for the saved samples (default \'.jpg\').\n        color (bool, optional): Treat images as colored or not (default True).\n        size (list or tuple, optional): Grid dimensions, rows x columns. (default None).\n        inter_pad (int, optional): Padding separating the images (default None).\n        fill_value (int, optional): Fill value for inter-padding (default 0).\n        preprocess (callable, optional): Pre processing to apply to the image \n            samples (default None).\n        display (bool, optional): Display images (default False).\n        save (bool, optional): Save images to disk (default True).\n        sample_freq (int, optional): Frequency of samples (per sampler call) (default 1).\n        \n\n    """"""\n    def __init__(self, name, directory=\'.\', overwrite=False, \n                 view=\'torch\', ext=\'.jpg\', color=True, size=None,\n                 inter_pad=None, fill_value=0, preprocess=None, \n                 display=False, save=True, sample_freq=1):\n        self.name = name\n        self.directory = process(directory, create=True)\n        self.directory = path.join(self.directory, \'samples\')\n        self.directory = process(self.directory, create=True)\n        self.overwrite = overwrite\n        self.view = view\n        self.ext = ext\n        self.color = color\n        self.size = size\n        self.inter_pad = inter_pad\n        self.fill_value = fill_value\n        self.preprocess = preprocess\n        self.display_samples = display\n        self.save_samples = save\n        self.sample_freq = sample_freq\n        self.figure = None\n        self.chkp = path.join(self.directory, "".{0}.chkp"".format(self.name))\n        self.counter, self.filename = torch.load(self.chkp) if path.exists(self.chkp) else (0, \'\')\n    \n    def sample(self, imgs):\n        """"""Saves and/or displays functions depending on the configuration.\n        \n        Args:\n            imgs (Tensor, Array, list or tuple): Image samples. Will\n                automatically be put in a grid. Must be in the [0,1] range.\n        """"""\n        self.counter += 1\n        if (self.save_samples or self.display_samples) \\\n            and (self.counter % self.sample_freq == 0):\n            samples = self._get_sample(imgs)\n            if self.save_samples:\n                old_filename = self.filename\n                new_filename = self._make_name()\n                if new_filename == old_filename and not self.overwrite:\n                    print(\'WARNING: Overwriting file in non overwrite mode.\')\n                elif self.overwrite:\n                    try:\n                        os.remove(old_filename)\n                    except:\n                        pass\n                imwrite(new_filename, samples)\n                torch.save((self.counter, new_filename), self.chkp)\n            if self.display_samples:\n                self.figure = imshow(samples, interactive=True,\n                                     title=os.path.basename(self._make_name()), \n                                     figure=self.figure, view=\'cv\')\n        \n\n    def __call__(self, imgs):\n        """"""Same as :meth:`sample`""""""\n        self.sample(imgs)\n\n    def _make_name(self):\n        tmpl = \'{0}_{1:07d}{2}\'\n        fname = tmpl.format(self.name, self.counter, self.ext)\n        self.filename = path.join(self.directory, fname)\n        return self.filename\n\n    def _get_sample(self, img):\n        if self.preprocess:\n            img = self.preprocess(img)\n        img = applier(to_array)(img)\n        grid = make_grid(img, view=self.view, color=self.color, size=self.size,\n                         inter_pad=self.inter_pad, fill_value=self.fill_value)\n        grid = change_view(grid, self.view, \'cv\')\n        if self.ext not in [\'.hdr\', \'.pfm\', \'.exr\']:\n            grid = (grid*255).astype(\'uint8\')\n        return grid'"
dlt/util/layers.py,0,"b'import math\n\ndef _find_integers(down, up, include_down, include_up):\n\n    ret = []\n    if up < 0 or up < down:\n        return ret\n    if include_down and down >= 0 and float(down).is_integer():\n        ret.append(int(down))\n    for i in range(int(math.floor(down + 1)), int(math.ceil(up - 1)) + 1):\n        if i >= 0:\n            ret.append(i)\n    if include_up and float(up).is_integer():\n        ret.append(int(up))\n    return ret\n\ndef out_size(dim_in, k, s, p, d):\n    """"""Calculates the resulting size after a convolutional layer.\n    \n    Args:\n        dim_in (int): Input dimension size.\n        k (int): Kernel size.\n        s (int): Stride of convolution.\n        p (int): Padding (of input).\n        d (int): Dilation\n    """"""\n    return math.floor((dim_in + 2*p - d*(k-1) - 1)/s + 1)\n\ndef in_size(dim_out, k, s, p, d):\n    """"""Calculates the input size before a convolutional layer.\n    \n    Args:\n        dim_out (int): Output dimension size.\n        k (int): Kernel size.\n        s (int): Stride of convolution.\n        p (int): Padding (of input).\n        d (int): Dilation\n    """"""\n    down = s*(dim_out - 1) + d*(k - 1) + 1 - 2*p\n    up = s*dim_out + d*(k - 1) + 1 - 2*p\n    return _find_integers(down, up, True, False)\n\ndef kernel_size(dim_in, dim_out, s, p, d):\n    """"""Calculates the possible kernel size(s) of a convolutional layer given input and output.\n    \n    Args:\n        dim_in (int): Input dimension size.\n        dim_out (int): Output dimension size.\n        s (int): Stride of convolution.\n        p (int): Padding (of input).\n        d (int): Dilation\n    """"""\n    down = ((dim_in + 2*p - s*dim_out - 1) / d) + 1\n    up = ((dim_in + 2*p - s*(dim_out - 1) -1) / d) + 1\n    return _find_integers(down, up, False, True)\n\ndef stride_size(dim_in, dim_out, k, p, d):\n    """"""Calculates the possible stride size(s) of a convolutional layer given input and output.\n    \n    Args:\n        dim_in (int): Input dimension size.\n        dim_out (int): Output dimension size.\n        k (int): Kernel size.\n        p (int): Padding (of input).\n        d (int): Dilation\n    """"""\n    down = (dim_in + 2*p -d*(k-1) - 1) / dim_out\n    up = (dim_in + 2*p -d*(k-1) - 1) / (dim_out -1)\n    return _find_integers(down, up, False, True)\n\ndef padding_size(dim_in, dim_out, k, s, d):\n    """"""Calculates the possible padding size(s) of a convolutional layer given input and output.\n    \n    Args:\n        dim_in (int): Input dimension size.\n        dim_out (int): Output dimension size.\n        k (int): Kernel size.\n        s (int): Stride of convolution.\n        d (int): Dilation\n    """"""\n    down = (s*(dim_out - 1) + d*(k-1) - dim_in + 1)/2\n    up = (s*dim_out + d*(k-1) - dim_in + 1)/2\n    return _find_integers(down, up, True, False)\n\ndef dilation_size(dim_in, dim_out, k, s, p):\n    """"""Calculates the possible dilation size(s) of a convolutional layer given input and output.\n\n    Args:\n        dim_in (int): Input dimension size.\n        dim_out (int): Output dimension size.\n        k (int): Kernel size.\n        s (int): Stride of convolution.\n        p (int): Padding (of input).\n    """"""\n    if k <= 1:\n        print(\'Warning: Invalid kernel size {0} for dilation size prediction.\'.format(k))\n        return []\n    down = (dim_in + 2*p - s*dim_out - 1)/(k-1)\n    up = (dim_in + 2*p - s*(dim_out - 1) - 1)/(k-1)\n    return _find_integers(down, up, False, True)\n\ndef _make_list(x):\n    if isinstance(x,int):\n        return [x]\n    else:\n        return x\n\ndef find_layers(dims_in=None, dims_out=None, ks=None, ss=None, ps=None, ds=None):\n    """"""Calculates all the possible convolutional layer size(s) and parameters.\n\n    Args:\n        dim_in (list): Input dimension sizes.\n        dim_out (list): Output dimension sizes.\n        k (list): Kernel sizes.\n        s (list): Strides of convolutions.\n        p (list): Paddings (of inputs).\n    """"""\n    if isinstance(dims_in,int):\n        dims_in = [dims_in]\n    in_dims = range(16, 33) if dims_in is None else _make_list(dims_in)\n    out_dims = range(16, 33) if dims_out is None else _make_list(dims_out)\n    kernels = range(1, 5) if ks is None else _make_list(ks)\n    strides = range(1, 3) if ss is None else _make_list(ss)\n    paddings = range(1, 33) if ps is None else _make_list(ps)\n    dilations = range(1, 9) if ds is None else _make_list(ds)\n    result = []\n    for kernel in kernels:\n        for stride in strides:\n            for padding in paddings:\n                for dilation in dilations:\n                    for out_dim in out_dims:\n                        in_dim_preds = in_size(out_dim, kernel, stride, padding, dilation)\n                        for in_dim_pred in in_dim_preds:\n                            if in_dim_pred in in_dims:\n                                result.append([in_dim_pred, out_dim, kernel, stride, padding, dilation])\n\n    return result\n\n\n'"
dlt/util/logger.py,0,"b'from os import path\nimport logging\nfrom .paths import process\nfrom .misc import is_tensor\n\nclass Logger(object):\n    """"""Logs values in a csv file.\n\n    Args:\n        name (str): Filename without extension.\n        fields (list or tuple): Field names (column headers).\n        directory (str, optional): Directory to save file (default \'.\').\n        delimiter (str, optional): Delimiter for values (default \',\').\n        resume (bool, optional): If True it appends to an already existing\n            file (default True).\n\n    """"""\n    def __init__(self, name, fields, directory=""."",\n                 delimiter=\',\', resume=True):\n        self.filename = name + "".csv""\n        self.directory = process(path.join(directory), True)\n        self.file = path.join(self.directory, self.filename)\n        self.fields = fields\n        self.logger = logging.getLogger(name)\n        self.logger.setLevel(logging.INFO)\n        self.logger.propagate = False\n        # Write header\n        if not resume or not path.exists(self.file):\n            with open(self.file, \'w\') as f:\n                f.write(delimiter.join(fields)+ \'\\n\')\n\n        file_handler = logging.FileHandler(self.file)\n        # Adding underscore to avoid clashes with reserved words from logging\n        field_tmpl = delimiter.join([\'%({0})s\'.format(\'_\' + x) for x in fields])\n        file_handler.setFormatter(logging.Formatter(field_tmpl))\n        self.logger.addHandler(file_handler)\n        \n\n    def __call__(self, values):\n        """"""Same as :meth:`log`""""""\n        self.log(values)\n\n    def _create_dict(self, values):\n        ret = {\'_\' + key: \'\' for key in self.fields}\n        ret.update({\'_\' + key: val.item() if is_tensor(val) else val\n                    for key, val in values.items()})\n        return ret\n\n    def log(self, values):\n        """"""Logs a row of values.\n\n        Args:\n            values (dict): Dictionary containing the names and values.\n        """"""\n        self.logger.info(\'\', extra=self._create_dict(values))\n        '"
dlt/util/meter.py,0,"b'from collections import OrderedDict\n\nclass Average(object):\n    """"""Keeps an average of values.""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        """"""Resets the average to 0.""""""\n        self.count = 0\n        self.value = 0\n\n    def add(self, value, count=1):\n        """"""Adds a new value\n        \n        Args:\n            value: Value to be added\n            count (optional): Number of summed values that make the value given.\n                Can be used to register multiple (summed) values at once (default 1).\n        """"""\n        self.count += count\n        self.value += value*count\n\n    def get(self):\n        """"""Returns the current average""""""\n        if self.count == 0:\n            return 0\n        return self.value/self.count\n\nclass Averages(object):\n    """"""Keeps multiple named averages.\n    \n    Args:\n        names(collection): Collection of strings to be used as names for the averages.\n    """"""\n    def __init__(self, names):\n        self._names = names\n        self.avgs = OrderedDict([(name, Average()) for name in names])\n\n    def reset(self, names=None):\n        """"""Resets averages to 0.\n\n        Args:\n            names (collection, optional): Collection of the names to be reset.\n                If None is given, all the values are reset (default None). \n        """"""\n        names = names or self._names\n        for name in names:\n            self.avgs[name].reset()\n\n    def add(self, values, count=1):\n        """"""Adds new values\n\n        Args:\n            values (dict or list): Collection of values to be added. \n                Could be given as a dict or a list. Order is preserved.\n            count (int, optional): Number of summed values that make the total values given.\n                Can be used to register multiple (summed) values at once (default 1).\n        """"""\n        if isinstance(values, dict):\n            for name, value in values.items():\n                if any([isinstance(value, x) for x in [list, tuple]]):\n                    self.avgs[name].add(*value)\n                else:\n                    self.avgs[name].add(value, count)\n        else:\n            for i, value in enumerate(values):\n                if any([isinstance(value, x) for x in [list, tuple]]):\n                    self.avgs[i].add(*value)\n                else:\n                    self.avgs[i].add(value, count)\n\n    def get(self, names=None, ret_dict=False):\n        """"""Returns the current averages\n        \n        Args:\n            names (str or list, optional): Names of averages to be returned.\n            ret_dict (bool, optional): If true return the results in a dictionary,\n                otherwise a list.\n\n        Returns:\n            dict or list: The averages.\n        """"""\n        if isinstance(names, str):\n            if ret_dict:\n                return {names: self.avgs[names].get()}\n            else:\n                return self.avgs[names].get()\n        if names is None:\n            names = self._names\n        if ret_dict:\n            return {name: self.avgs[name].get() for name in names}\n        else:\n            return [self.avgs[name].get() for name in names]\n\n    def names(self):\n        """"""Returns the names of the values held.""""""\n        return self._names'"
dlt/util/misc.py,21,"b'import math\nimport torch\nfrom torch import nn\nimport numpy as np\nimport pandas as pd\n\ndef count_parameters(net):\n    """"""Counts the parameters of a given PyTorch model.""""""\n    return sum([p.numel() for p in list(net.parameters())])\n\ndef str2bool(x):\n    """"""Converts a string to boolean type.\n    \n    If the string is any of [\'no\', \'false\', \'f\', \'0\'], or any capitalization,\n    e.g. \'fAlSe\' then returns False. All other strings are True.\n\n    """"""\n    if x is None or x.lower() in [\'no\', \'false\', \'f\', \'0\']:\n        return False\n    else:\n        return True\n    \ndef str_is_int(s):\n    """"""Checks if a string can be converted to int.""""""\n    try:\n        int(s)\n        return True\n    except:\n        return False\n\ndef slide_window_(a, kernel, stride=None):\n    """"""Expands last dimension to help compute sliding windows.\n    \n    Args:\n        a (Tensor or Array): The Tensor or Array to view as a sliding window.\n        kernel (int): The size of the sliding window.\n        stride (tuple or int, optional): Strides for viewing the expanded dimension (default 1)\n\n    The new dimension is added at the end of the Tensor or Array.\n\n    Returns:\n        The expanded Tensor or Array.\n\n    Running Sum Example::\n\n        >>> a = torch.Tensor([1, 2, 3, 4, 5, 6])\n         1\n         2\n         3\n         4\n         5\n         6\n        [torch.FloatTensor of size 6]\n        >>> a_slided = dlt.util.slide_window_(a.clone(), kernel=3, stride=1)\n         1  2  3\n         2  3  4\n         3  4  5\n         4  5  6\n        [torch.FloatTensor of size 4x3]\n        >>> running_total = (a_slided*torch.Tensor([1,1,1])).sum(-1)\n          6\n          9\n         12\n         15\n        [torch.FloatTensor of size 4]\n\n    Averaging Example::\n\n        >>> a = torch.Tensor([1, 2, 3, 4, 5, 6])\n         1\n         2\n         3\n         4\n         5\n         6\n        [torch.FloatTensor of size 6]\n        >>> a_sub_slide = dlt.util.slide_window_(a.clone(), kernel=3, stride=3)\n         1  2  3\n         4  5  6\n        [torch.FloatTensor of size 2x3]\n        >>> a_sub_avg = (a_sub_slide*torch.Tensor([1,1,1])).sum(-1) / 3.0\n         2\n         5\n        [torch.FloatTensor of size 2]\n    """"""\n\n\n    if isinstance(kernel, int):\n        kernel = (kernel,)\n    if stride is None:\n        stride = tuple(1 for i in kernel)\n    elif isinstance(stride, int):\n        stride = (stride,)\n    window_dim = len(kernel)\n    if is_array(a):\n        new_shape = a.shape[:-window_dim] + tuple(int(np.floor((s - kernel[i] )/stride[i]) + 1) for i,s in enumerate(a.shape[-window_dim:])) + kernel\n        new_stride = a.strides[:-window_dim] + tuple(s*k for s,k in zip(a.strides[-window_dim:], stride)) + a.strides[-window_dim:]\n        return np.lib.stride_tricks.as_strided(a, shape=new_shape, strides=new_stride)\n    else:\n        new_shape = a.size()[:-window_dim] + tuple(int(np.floor((s - kernel[i] )/stride[i]) + 1) for i,s in enumerate(a.size()[-window_dim:])) + kernel\n        new_stride = a.stride()[:-window_dim] + tuple(s*k for s,k in zip(a.stride()[-window_dim:], stride)) + a.stride()[-window_dim:]\n        a.set_(a.storage(), storage_offset=0, size=new_shape, stride=new_stride)\n        return a\n\ndef re_stride(a, kernel, stride=None):\n    """"""Returns a re-shaped and re-strided Rensor given a kernel (uses as_strided).\n\n    Args:\n        a (Tensor): The Tensor to re-stride.\n        kernel (tuple or int): The size of the new dimension(s).\n        stride (tuple or int, optional): Strides for viewing the expanded dimension(s) (default 1)\n    """"""\n    if isinstance(kernel, int):\n        kernel = (kernel,)\n    if stride is None:\n        stride = tuple(1 for i in kernel)\n    elif isinstance(stride, int):\n        stride = (stride,)\n    window_dim = len(kernel)\n    new_shape = a.size()[:-window_dim]  + kernel + tuple(int(math.floor((s - kernel[i] )/stride[i]) + 1) for i,s in enumerate(a.size()[-window_dim:]))\n    new_stride = a.stride()[:-window_dim]  + a.stride()[-window_dim:] + tuple(s*k for s,k in zip(a.stride()[-window_dim:], stride))\n    return a.as_strided(new_shape, new_stride)\n\n\ndef replicate(x, dim=-3, nrep=3):\n    """"""Replicates Tensor/Array in a new dimension.\n\n    Args:\n        x (Tensor or Array): Tensor to replicate.\n        dim (int, optional): New dimension where replication happens.\n        nrep (int, optional): Number of replications.\n    """"""\n    if is_tensor(x):\n        return x.unsqueeze(dim).expand(*x.size()[:dim + 1],nrep,*x.size()[dim + 1:])\n    else:\n        return np.repeat(np.expand_dims(x,dim), nrep, axis=dim)\n\ndef moving_avg(x, width=5):\n    """"""Performes moving average of a one dimensional Tensor or Array\n\n    Args:\n        x (Tensor or Array): 1D Tensor or array.\n        width (int, optional): Width of the kernel.\n    """"""\n    if len(x) >= width:\n        if is_array(x):\n            return np.mean(slide_window_(x, width,1), -1)\n        else:\n            return torch.mean(slide_window_(x, width,1), -1)\n    else:\n        return x.mean()\n\ndef moving_var(x, width=5):\n    """"""Performes moving variance of a one dimensional Tensor or Array\n\n    Args:\n        x (Tensor or Array): 1D Tensor or array.\n        width (int, optional): Width of the kernel.\n    """"""\n    if len(x) >= width:\n        if is_array(x):\n            return np.var(slide_window_(x, width, 1), -1)\n        else:\n            return torch.var(slide_window_(x, width, 1), -1)\n    else:\n        return x.var()\n\ndef sub_avg(x, width=5):\n    """"""Performes averaging of a one dimensional Tensor or Array every `width` elements.\n\n    Args:\n        x (Tensor or Array): 1D Tensor or array.\n        width (int, optional): Width of the kernel.\n    """"""\n    if len(x) >= width:\n        if is_array(x):\n            return np.mean(slide_window_(x, width, width), -1)\n        else:\n            return torch.mean(slide_window_(x, width, width), -1)\n    else:\n        return x.mean()\n\ndef sub_var(x, width=5):\n    """"""Calculates variance of a one dimensional Tensor or Array every `width` elements.\n\n    Args:\n        x (Tensor or Array): 1D Tensor or array.\n        width (int, optional): Width of the kernel.\n    """"""\n    if len(x) >= width:\n        if is_array(x):\n            return np.var(slide_window_(x, width, width), -1)\n        else:\n            return torch.var(slide_window_(x, width, width), -1)\n    else:\n        return x.var()\n\ndef has(x, val):\n    """"""Checks if a Tensor/Array has a value val. Does not work with nan (use :func:`has_nan`).""""""\n    return bool((x == val).sum() != 0)\n\ndef has_nan(x):\n    """"""Checks if a Tensor/Array has NaNs.""""""\n    return bool((x != x).sum() > 0)\n\n\ndef has_inf(x):\n    """"""Checks if a Tensor/Array array has Infs.""""""\n    return has(x, float(\'inf\'))\n\ndef replace_specials_(x, val=0):\n    """"""Replaces NaNs and Infs from a Tensor/Array.\n    \n    Args:\n        x (Tensor or Array): The Tensor/Array (gets replaced in place).\n        val (int, optional): Value to replace NaNs and Infs with (default 0).\n    """"""\n    x[ (x == float(\'inf\')) | (x != x) ] = val\n    return x\n\ndef replace_inf_(x, val=0):\n    """"""Replaces Infs from a Numpy Array.\n    \n    Args:\n        x (Array): The Array (gets replaced in place).\n        val (int, optional): Value to replace Infs with (default 0).\n    """"""\n    x[x == float(\'inf\')] = val\n    return x\n\ndef replace_nan_(x, val=0):\n    """"""Replaces NaNs from a Numpy Array.\n\n    Args:\n        x (Array): The Array (gets replaced in place).\n        val (int, optional): Value to replace Infs with (default 0).\n    """"""\n    x[x != x] = val\n    return x\n\ndef map_range(x, low=0, high=1):\n    """"""Maps the range of a Numpy Array to [low, high] globally.""""""\n    if is_array(x):\n        return np.interp(x, [x.min(), x.max()], [low, high]).astype(x.dtype)\n    else:\n        xmax, xmin = x.max(), x.min()\n        if xmax - xmin == 0:\n            return torch.zeros_like(x)\n        return x.add(-xmin).div_(xmax-xmin).mul_(high-low).add_(low).clamp_(low, high)\n\n# This was added to torch in v0.3. Keeping it here too.\ndef is_tensor(x):\n    """"""Checks if input is a Tensor""""""\n    return torch.is_tensor(x)\n    \ndef is_cuda(x):\n    """"""Checks if input is a cuda Tensor.""""""\n    return torch.is_tensor(x) and x.is_cuda\n\ndef is_array(x):\n    """"""Checks if input is a numpy array or a pandas Series.""""""\n    return isinstance(x, np.ndarray) or isinstance(x, pd.Series)\n\n## Returns a numpy array version of x\ndef to_array(x):\n    """"""Converts x to a Numpy Array. Returns a copy of the data.\n    \n    Args:\n        x (Tensor or Array): Input to be converted. Can also be on the GPU.\n\n    Automatically gets the data from torch Tensors and casts GPU Tensors\n    to CPU.\n    """"""\n    if is_cuda(x):\n        x = x.cpu()\n    if is_tensor(x):\n        return x.numpy()\n    else:\n        return x.copy()\n\n## Returns a cpu tensor COPY version of x\ndef to_tensor(x):\n    """"""Converts x to a Torch Tensor (CPU). Returns a copy of the data if x is a Tensor.\n    \n    Args:\n        x (Tensor or Array): Input to be converted. Can also be on the GPU.\n\n    Automatically casts GPU Tensors to CPU.\n    """"""\n    if is_cuda(x):\n        return x.cpu()\n    if is_array(x):\n        return torch.from_numpy(x)\n    else:\n        return x.data.clone()\n\n########\n### Tensors, arrays, cuda, cpu, image views etc\n########\ndef permute(x, perm):\n    """"""Permutes the last three dimensions of the input Tensor or Array.\n\n    Args:\n        x (Tensor or Array): Input to be permuted.\n        perm (tuple or list): Permutation.\n\n    Note:\n        If the input has less than three dimensions a copy is returned.\n    """"""\n    if is_tensor(x):\n        if x.dim() < 3:\n            return x.data.clone()\n        else:     \n            s = tuple(range(0, x.dim()))\n            permutation = s[:-3] + tuple(s[-3:][i] for i in perm)\n        return x.permute(*permutation).contiguous()\n    elif is_array(x):\n        if x.ndim < 3:\n            return x.copy()\n        else:\n            s = tuple(range(0,x.ndim))\n            permutation = s[:-3] + tuple(s[-3:][i] for i in perm)\n        # Copying to get rid of negative strides\n        return np.transpose(x, permutation).copy()\n    else:\n        raise TypeError(\'Uknown type {0} encountered.\'.format(torch.typename(x)))\n\ndef hwc2chw(x):\n    """"""Permutes the last three dimensions of the hwc input to become chw.\n\n    Args:\n        x (Tensor or Array): Input to be permuted.\n    """"""\n    return permute(x, (2,0,1))\ndef chw2hwc(x):\n    """"""Permutes the last three dimensions of the chw input to become hwc.\n\n    Args:\n        x (Tensor or Array): Input to be permuted.\n    """"""\n    return permute(x, (1,2,0))\n\ndef channel_flip(x, dim=-3):\n    """"""Reverses the channel dimension.\n    \n    Args:\n        x (Tensor or Array): Input to have its channels flipped.\n        dim (int, optional): Channels dimension (default -3).\n\n    Note:\n        If the input has less than three dimensions a copy is returned.\n    """"""\n\n    if is_tensor(x):\n        dim = x.dim() + dim if dim < 0 else dim\n        if x.dim() < 3:\n            return x.data.clone()\n        return x[tuple(slice(None, None) if i != dim\n                 else torch.arange(x.size(i)-1, -1, -1).long()\n                 for i in range(x.dim()))]\n    elif is_array(x):\n        dim = x.ndim + dim if dim < 0 else dim\n        if x.ndim < 3:\n            return x.copy()\n        return np.ascontiguousarray(np.flip(x,dim))\n    else:\n        raise TypeError(\'Uknown type {0} encountered.\'.format(torch.typename(x)))\n\n# Default is dimension -3 (e.g. for bchw)\ndef rgb2bgr(x, dim=-3):\n    """"""Reverses the channel dimension. See :func:`channel_flip`""""""\n    return channel_flip(x, dim)\n    \ndef bgr2rgb(x, dim=-3):\n    """"""Reverses the channel dimension. See :func:`channel_flip`""""""\n    return channel_flip(x, dim)\n\n# Getting images from one library to the other\n# Always assuming the last three dimensions are the images\n# opencv is hwc - BGR\n# torch is chw - RGB\n# plt is hwc - RGB\nVIEW_NAMES = {\n    \'opencv\': [\'hwcbgr\', \'hwc-bgr\', \'bgrhwc\', \'bgr-hwc\', \'opencv\', \'open-cv\', \'cv\', \'cv2\'],\n    \'torch\':  [\'chwrgb\', \'chw-rgb\', \'rgbchw\', \'rgb-chw\', \'torch\', \'pytorch\'],\n    \'plt\':    [\'hwcrgb\', \'hwc-rgb\', \'rgbhwc\', \'rgb-hwc\', \'plt\', \'pyplot\', \'matplotlib\'],\n    \'other\':  [\'chwbgr\', \'chw-bgr\', \'bgrchw\', \'bgr-chw\']\n}\n\ndef _determine_view(v):\n    for view, names in VIEW_NAMES.items():\n        if v.lower() in names:\n            return view\n    return \'unknown\'\n\n# This is not elegant but at least it\'s clear and does its job\ndef change_view(x, current, new):\n    """"""Changes the view of the input. Returns a copy.\n\n    Args:\n        x (Tensor or Array): Input whose view is to be changed.\n        current (str): Current view.\n        new (str): New view.\n\n    Possible views:\n\n    ======== ==============================================================\n      View     Aliases\n    ======== ==============================================================\n     opencv   hwcbgr, hwc-bgr, bgrhwc, bgr-hwc, opencv, open-cv, cv, cv2\n    -------- --------------------------------------------------------------\n     torch    chwrgb, chw-rgb, rgbchw, rgb-chw, torch, pytorch\n    -------- --------------------------------------------------------------\n     plt      hwcrgb, hwc-rgb, rgbhwc, rgb-hwc, plt, pyplot, matplotlib\n    -------- --------------------------------------------------------------\n     other    chwbgr, chw-bgr, bgrchw, bgr-chw\n    ======== ==============================================================\n\n    Note:\n        If the input has less than three dimensions a copy is returned.    \n\n    """"""\n    curr_name, new_name = _determine_view(current), _determine_view(new)\n    if curr_name == \'unknown\':\n        raise ValueError(\'Unkown current view encountered in change_view: {0}\'.format(current))\n    if new_name == \'unknown\':\n        raise ValueError(\'Unkown new view encountered in change_view: {0}\'.format(new))\n    if new_name == curr_name:\n        if is_array(x):\n            return x.copy()\n        else:\n            return x.data.clone()\n\n    if curr_name == \'opencv\':\n        if new_name == \'torch\':\n            return bgr2rgb(hwc2chw(x), -3)\n        elif new_name == \'plt\':\n            return bgr2rgb(x, -1)\n        elif new_name == \'other\':\n            return hwc2chw(x)\n    if curr_name == \'torch\':\n        if new_name == \'opencv\':\n            return chw2hwc(rgb2bgr(x, -3))\n        elif new_name == \'plt\':\n            return chw2hwc(x)\n        elif new_name == \'other\':\n            return rgb2bgr(x, -3)\n    if curr_name == \'plt\':\n        if new_name == \'torch\':\n            return hwc2chw(x)\n        elif new_name == \'opencv\':\n            return rgb2bgr(x, -1)\n        elif new_name == \'other\':\n            return hwc2chw(rgb2bgr(x, -1))\n    if curr_name == \'other\':\n        if new_name == \'torch\':\n            return bgr2rgb(x, -3)\n        elif new_name == \'plt\':\n            return chw2hwc(rgb2bgr(x, -3))\n        elif new_name == \'opencv\':\n            return chw2hwc(x)\n\n## These functions also convert!\ndef cv2torch(x):\n    """"""Converts input to Tensor and changes view from cv (hwc-bgr) to torch (chw-rgb).\n    \n    For more detail see :func:`change_view`\n    """"""\n    return change_view(to_tensor(x), \'cv\', \'torch\')\n\ndef torch2cv(x):\n    """"""Converts input to Array and changes view from torch (chw-rgb) to cv (hwc-bgr).\n\n    For more detail see :func:`change_view`\n    """"""\n    return change_view(to_array(x), \'torch\', \'cv\')\n\ndef cv2plt(x):\n    """"""Changes view from cv (hwc-bgr) to plt (hwc-rgb).\n        \n    For more detail see :func:`change_view`\n    """"""\n    return change_view(x, \'cv\', \'plt\')\n\ndef plt2cv(x):\n    """"""Changes view from plt (hwc-rgb) to cv (hwc-bgr).\n        \n    For more detail see :func:`change_view`\n    """"""\n    return change_view(x, \'plt\', \'cv\')\n\ndef plt2torch(x):\n    """"""Converts input to Tensor and changes view from plt (hwc-rgb) to torch (chw-rgb).\n    \n    For more detail see :func:`change_view`\n    """"""\n    return change_view(to_tensor(x), \'plt\', \'torch\')\n\ndef torch2plt(x):\n    """"""Converts input to Array and changes view from torch (chw-rgb) to plt (hwc-rgb) .\n    \n    For more detail see :func:`change_view`\n    """"""\n    return change_view(to_array(x), \'torch\', \'plt\')'"
dlt/util/paths.py,0,"b'import os\nfrom os import path\nfrom shutil import copyfile\nimport errno\n\n__all__ = [\'split\', \'make\', \'copy_to_dir\', \'process\', \'write_file\']\n\ndef split(directory):\n    """"""Splits a full filename path into its directory path, name and extension\n\n    Args:\n        directory (str): Directory to split.\n\n    Returns:\n        tuple: (Directory name, filename, extension)\n    """"""\n    directory = process(directory)\n    name, ext = path.splitext(path.basename(directory))\n    return path.dirname(directory), name, ext\n\ndef make(directory):\n    """"""Make a new directory\n\n    Args:\n        directory (str): Directory to make.\n    """"""\n    try:\n        os.makedirs(directory)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise \n\ndef copy_to_dir(file, directory):\n    """"""Copies a file to a directory\n\n    Args:\n        file (str): File to copy.\n        directory (str): Directory to copy file to.\n    """"""\n    file_path = path.join(directory, path.basename(file))\n    copyfile(file, file_path)\n\ndef process(directory, create=False):\n    """"""Expands home path, finds absolute path and creates directory (if create is True).\n    \n    Args:\n        directory (str): Directory to process.\n        create (bool, optional): If True, it creates the directory.\n\n    Returns:\n        str: The processed directory.\n    """"""\n    directory = path.expanduser(directory)\n    directory = path.normpath(directory)\n    directory = path.abspath(directory)\n    if create:\n        make(directory)\n    return directory\n\ndef write_file(contents, filename, directory=""."", append=False):\n    """"""Writes contents to file.\n    \n    Args:\n        contents (str): Contents to write to file.\n        filename (str): File to write contents to.\n        directory (str, optional): Directory to put file in.\n        append (bool, optional): If True and file exists, it appends contents.\n\n    Returns:\n        str: Full path to file.\n    """"""\n    full_name = path.join(process(directory), filename)\n    mode = ""a"" if append else ""w""\n    with open(full_name, mode) as file_handle:\n        file_handle.write(contents)\n    return full_name'"
dlt/util/sample.py,0,"b'import numpy as np\n\ndef clamped_gaussian(mean, std, min_value, max_value):\n    if max_value <= min_value:\n        return mean\n    factor = 0.99\n    while True:\n        ret = np.random.normal(mean, std)\n        if ret > min_value and ret < max_value:\n            break\n        else:\n            std = std * factor\n            ret = np.random.normal(mean, std)\n        \n    return ret\n\ndef exponential_size(val):\n    return val * (np.exp(- np.random.uniform() ) ) / (np.exp(0) + 1)\n\n# Accepts hwc-bgr image\ndef index_gauss(img, precision=None, crop_size=None, random_size=True, ratio=None, seed=None):\n    """"""Returns indices (Numpy slice) of an image crop sampled spatially using a gaussian distribution.\n\n    Args:\n        img (Array): Image as a Numpy array (OpenCV view, hwc-BGR).\n        precision (list or tuple, optional): Floats representing the precision\n            of the Gaussians (default [1, 4])\n        crop_size (list or tuple, optional): Ints representing the crop size\n            (default [img_width/4, img_height/4]).\n        random_size (bool, optional): If true, randomizes the crop size with\n            a minimum of crop_size. It uses an exponential distribution such\n            that smaller crops are more likely (default True).\n        ratio (float, optional): Keep a constant crop ratio width/height (default None).\n        seed (float, optional): Set a seed for np.random.seed() (default None)\n\n    Note:\n        - If `ratio` is None then the resulting ratio can be anything.\n        - If `random_size` is False and `ratio` is not None, the largest dimension\n          dictated by the ratio is adjusted accordingly:\n                \n                - `crop_size` is (w=100, h=10) and `ratio` = 9 ==> (w=90, h=10)\n                - `crop_size` is (w=100, h=10) and `ratio` = 0.2 ==> (w=100, h=20)\n\n    """"""\n    np.random.seed(seed)\n    dims = {""w"": img.shape[1], ""h"": img.shape[0]}\n    if precision is None:\n        precision = {""w"": 1, ""h"": 4}\n    else:\n        precision = {""w"": precision[0], ""h"": precision[1]}\n    \n    if crop_size is None:\n        crop_size = {key: int(dims[key] / 4) for key in dims}\n    else:\n        crop_size = {""w"": crop_size[0], ""h"": crop_size[1]}\n\n    if ratio is not None:\n        ratio = max(ratio, 1e-4)\n        if ratio > 1:\n            if random_size:\n                crop_size[\'h\'] = int(max(crop_size[\'h\'], exponential_size(dims[\'h\']))) \n            crop_size[\'w\'] = int(np.round(crop_size[\'h\']*ratio))\n        else:\n            if random_size:\n                crop_size[\'w\'] = int(max(crop_size[\'w\'], exponential_size(dims[\'w\']))) \n            crop_size[\'h\'] = int(np.round(crop_size[\'w\']/ratio))\n    else:\n        if random_size:\n            crop_size = {key: int(max(val, exponential_size(dims[key]))) for key, val in crop_size.items()}\n\n    centers = {key: int(clamped_gaussian(dim / 2, crop_size[key] / precision[key], \n                                         min(int(crop_size[key] /2), dim), \n                                         max(int(dim - crop_size[key] / 2), 0))) \n                     for key, dim in dims.items()}\n    starts = {key: max(center - int(crop_size[key] / 2), 0)\n              for key, center in centers.items()}\n    ends = {key: start + crop_size[key] for key, start in starts.items()}\n    return np.s_[starts[""h""]:ends[""h""], starts[""w""]:ends[""w""], :]\n\n\ndef slice_gauss(img, precision=None, crop_size=None, random_size=True, ratio=None, seed=None):\n    """"""Returns a cropped sample from an image array using :func:`index_gauss`""""""\n    return img[index_gauss(img, precision, crop_size, random_size, ratio)]\n\n\n# Accepts hwc-bgr image\ndef index_uniform(img, crop_size=None, random_size=True, ratio=None, seed=None):\n    """"""Returns indices (Numpy slice) of an image crop sampled spatially using a uniform distribution.\n\n    Args:\n        img (Array): Image as a Numpy array (OpenCV view, hwc-BGR).\n        crop_size (list or tuple, optional): Ints representing the crop size\n            (default [img_width/4, img_height/4]).\n        random_size (bool, optional): If true, randomizes the crop size with\n            a minimum of crop_size. It uses an exponential distribution such\n            that smaller crops are more likely (default True).\n        ratio (float, optional): Keep a constant crop ratio width/height (default None).\n        seed (float, optional): Set a seed for np.random.seed() (default None)\n\n    Note:\n        - If `ratio` is None then the resulting ratio can be anything.\n        - If `random_size` is False and `ratio` is not None, the largest dimension\n          dictated by the ratio is adjusted accordingly:\n                \n                - `crop_size` is (w=100, h=10) and `ratio` = 9 ==> (w=90, h=10)\n                - `crop_size` is (w=100, h=10) and `ratio` = 0.2 ==> (w=100, h=20)\n\n    """"""\n    np.random.seed(seed)\n    dims = {""w"": img.shape[1], ""h"": img.shape[0]}\n    if crop_size is None:\n        crop_size = {key: int(dims[key] / 4) for key in dims}\n    if ratio is not None:\n        ratio = max(ratio, 1e-4)\n        if ratio > 1:\n            if random_size:\n                crop_size[\'h\'] = int(max(crop_size[\'h\'], exponential_size(dims[\'h\']))) \n            crop_size[\'w\'] = int(np.round(crop_size[\'h\']*ratio))\n        else:\n            if random_size:\n                crop_size[\'w\'] = int(max(crop_size[\'w\'], exponential_size(dims[\'w\']))) \n            crop_size[\'h\'] = int(np.round(crop_size[\'w\']/ratio))\n    else:\n        if random_size:\n            crop_size = {key: int(max(val, exponential_size(dims[key]))) for key, val in crop_size.items()}\n\n    centers = {key: int(np.random.uniform(int(crop_size[key]/2), int( dims[key] - crop_size[key]/2) ))\n                     for key, dim in dims.items()}\n    starts = {key: max(center - int(crop_size[key] / 2), 0)\n              for key, center in centers.items()}\n    ends = {key: start + crop_size[key] for key, start in starts.items()}\n    return np.s_[starts[""h""]:ends[""h""], starts[""w""]:ends[""w""], :]\n\ndef slice_uniform(img, crop_size=None, random_size=True, ratio=None, seed=None):\n    """"""Returns a cropped sample from an image array using :func:`index_uniform`""""""\n    return img[index_uniform(img, crop_size, random_size, ratio)]'"
dlt/util/slurm.py,0,"b'from .paths import write_file\n\nSLURM_DEFAULTS = {\n    ""--job-name"": ""job"",\n    ""--time"": ""48:00:00"",\n    ""--nodes"": 1,\n    ""--ntasks-per-node"": 1,\n    ""--mem-per-cpu"": None,\n    ""--mem"": None,\n    ""--partition"": None,\n    ""--gres"": None,\n    ""--exclude"": None,\n    ""--nodelist"": None,\n    ""--output"": None,\n    ""--mail-type"": None,\n    ""--mail-user"": None\n}\n\ndef slurm(code, directory=""."", name=""job"", directives=None):\n    """"""Creates a script for the `Slurm Scheduler`_.\n\n    .. _Slurm Scheduler: https://slurm.schedmd.com/\n\n    Args:\n        code (str): The code that is to be run from the script\n        directory (str, optional): The directory where the script is created\n            (defult \'.\').\n        name (str, optional): Script filename (default \'job\').\n        directives (dict): Set of directives to use (default None).\n\n    Available directives:\n\n        ====================== ============\n                  key             Default\n        ====================== ============\n              --job-name           job\n        ---------------------- ------------\n                --time           48:00:00\n        ---------------------- ------------\n              --nodes                1\n        ---------------------- ------------\n          --ntasks-per-node          1\n        ---------------------- ------------\n            --mem-per-cpu          None\n        ---------------------- ------------\n                --mem              None\n        ---------------------- ------------\n             --partition           None\n        ---------------------- ------------\n                --gres             None\n        ---------------------- ------------\n              --exclude            None\n        ---------------------- ------------\n             --nodelist            None\n        ---------------------- ------------\n              --output             None\n        ---------------------- ------------\n             --mail-type           None\n        ---------------------- ------------\n              --mail-user          None\n        ====================== ============\n\n""""""\n    # Create script\n    if directives is None:\n        directives = {}\n    dirs = SLURM_DEFAULTS.copy()\n    dirs.update(directives)\n    dirs_str = """"\n    for key, value in dirs.items():\n        if value is not None:\n            dirs_str -= ""#SBATCH {0}={1}\\n"".format(key, value)\n    script = ""#!/bin/bash\\n\\n{0}\\n\\n{1}\\n"".format(dirs_str, code)\n\n    # Process path\n    script_file = write_file(script, name, directory)\n    return (script, script_file)\n\n'"
dlt/viz/__init__.py,0,b'from .imshow import imshow\nfrom . import modules\nfrom .csvplot import plot_csv'
dlt/viz/csvplot.py,0,"b'import os\nimport time\nimport argparse\nfrom itertools import compress\nfrom collections import OrderedDict\nimport matplotlib\nfrom matplotlib.animation import FuncAnimation\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom ..util import str2bool, compose, str_is_int, is_array, sub_avg, moving_avg, sub_var, moving_var\n\n\ndef _read_csv(f, **kargs):\n    if os.path.isfile(f):\n        df = pd.read_csv(f, **kargs) \n    else:\n        df = pd.DataFrame()\n    return df\n\nclass DataHandler(object):\n    def __init__(self, file_lists, header, ignore_duplicates, transform, names):\n        self.header = 0 if header else None\n        self.ignore_duplicates = ignore_duplicates\n        self.transform = transform\n        self.filenames = [f[0] for f in file_lists]\n        self.orig_columns = [f[1:] if len(f)>1 else [0] for f in file_lists]\n        self.orig_columns = [ [int(c) if str_is_int(c) else c for c in j] for j in self.orig_columns ]\n        self._group_names(names)\n\n    def _group_names(self, names):\n        if names is None:\n            self.orig_names = None\n            return\n        count = 0\n        self.orig_names = []\n        for c in self.orig_columns:\n            self.orig_names.append([])\n            for _ in c:\n                if count == len(names):\n                    self.orig_names[-1].append(\'no_name\')\n                else:\n                    self.orig_names[-1].append(names[count])\n                    count += 1\n\n    #TODO: MAKE THIS MORE EFFICIENT AND LESS OBFUSCATED\n    def get(self):\n        dfs, names, columns = [], [], []\n        for i_file, fname in enumerate(self.filenames):\n            if os.path.isfile(fname):\n                dfs.append(_read_csv(fname, header=self.header))\n                columns.append([])\n                for i_col, c in enumerate(self.orig_columns[i_file]):\n                    if isinstance(c, int) and len(dfs[-1].columns) > c:\n                        columns[-1].append(dfs[-1].columns[c])\n                        if self.orig_names is not None:\n                            names.append(self.orig_names[i_file][i_col])\n                    elif c in dfs[-1]:\n                        columns[-1].append(c)\n                        if self.orig_names is not None:\n                            names.append(self.orig_names[i_file][i_col])\n        # concatenate all the dataframes into a single one\n        if not dfs:\n            return None\n        data = pd.concat([pd.DataFrame(df.loc[:,columns[i]]) for i,df in enumerate(dfs)], axis=1)\n        if not self.ignore_duplicates:\n            cols = pd.Series(data.columns)\n            for dup in data.columns.get_duplicates():\n                cols[data.columns.get_loc(dup)]=[\'{0}-dupl{1}\'.format(dup,d_idx+1) for d_idx in range(np.sum(dup == cols))]\n            data.columns = cols\n        if self.orig_names is not None:\n            data.columns = names\n        return self.transform(data) if len(data.index) > 0 else data\n\n    __call__ = get\n\n\nclass CustomAction(argparse.Action):\n    def __call__(self, parser, namespace, values, option_string=None):\n        if not \'_ordered_args\' in namespace:\n            setattr(namespace, \'_ordered_args\', [])\n        previous = namespace._ordered_args\n        previous.append(self.dest)\n        setattr(namespace, self.dest, values)\n        setattr(namespace, \'_ordered_args\', previous)\n\n\ndef _get_opts(use_args=None):\n    parser = argparse.ArgumentParser(description=\'Easily plot from csvs.\')\n    arg = parser.add_argument\n    arg(\'-f\',\'--file\', required=True, nargs=\'+\', help=\'File followed by columns (names or indices). If no column, defaults to first one (0).\', action=\'append\')\n    \n    ## Plot arguments\n    arg(\'-t\', \'--title\', nargs=\'+\', help=\'Title(s) for (sub)plot\', default=None, action=CustomAction)\n    arg(\'-x\', \'--xlabel\', nargs=\'+\', help=\'x label(s).\', default=None, action=CustomAction)\n    arg(\'-y\', \'--ylabel\', nargs=\'+\', help=\'y label(s).\', default=None, action=CustomAction)\n    arg(\'--logx\', type=str2bool, help=\'Logarithmic x.\', default=False, action=CustomAction)\n    arg(\'--logy\', type=str2bool, help=\'Logarithmic y.\', default=False, action=CustomAction)\n    arg(\'--loglog\', type=str2bool, help=\'Use log scaling on both x and y axes.\', default=False, action=CustomAction)\n    arg(\'--kind\', choices=[\'line\', \'bar\', \'barh\', \'hist\', \'box\', \'kde\', \'density\', \'area\', \'pie\', \'scatter\', \'hexbin\'], default=\'line\', action=CustomAction)\n    arg(\'--subplots\', type=str2bool, help=\'Make separate subplots for each column.\', default=False, action=CustomAction)\n    arg(\'--layout\', nargs=2, help=\'rows columns for the layout of subplots.\', default=None, action=CustomAction)\n    arg(\'--use_index\', type=str2bool, help=\'Use index as ticks for x axis.\', default=False, action=CustomAction)\n    arg(\'--sharex\', type=str2bool, help=\'In case subplots=True, share x axis.\', default=False, action=CustomAction)\n    arg(\'--sharey\', type=str2bool, help=\'In case subplots=True, share y axis.\', default=False, action=CustomAction)\n    arg(\'--figsize\', nargs=2, help=\'Figure size (width and height) in inches.\', default=None, action=CustomAction)\n    arg(\'--grid\', type=str2bool, help=\'Axis grid lines.\', default=None, action=CustomAction)\n    arg(\'--legend\', type=lambda x: x if x==\'reverse\' else str2bool(x), help=\'Place legend on axis subplots.\', default=True, action=CustomAction)\n    arg(\'--names\', nargs=\'+\', help=\'Names for each plotted series.\', default=None, action=CustomAction)\n    arg(\'--style\', nargs=\'+\', help=\'matplotlib line style per column.\', action=CustomAction)\n    arg(\'--rot\', type=int, help=\'Rotation for ticks.\', default=None, action=CustomAction)\n    arg(\'--xlim\', type=int, nargs=2, help=\'X axis limits.\', default=None, action=CustomAction)\n    arg(\'--ylim\', type=int, nargs=2, help=\'Y axis limits.\', default=None, action=CustomAction)\n    arg(\'--fontsize\', type=int, help=\'Font size for xticks and yticks.\', default=None, action=CustomAction)\n    arg(\'--colormap\', help=\'Colormap to select colors from.\', default=None, action=CustomAction)\n    arg(\'--position\', type=float, help=\'Specify relative alignments for bar plot layout. [0-1]\', default=0.5, action=CustomAction)\n    arg(\'--table\', type=str2bool, help=\'If True, draw a table.\', default=False, action=CustomAction)\n    ## Transformations\n    arg(\'-a\', \'-sa\', \'--sub_avg\', nargs=\'+\', type=int, help=\'Plot in averages\', default=None, action=CustomAction)\n    arg(\'-ma\', \'--mov_avg\', nargs=\'+\', type=int, help=\'Plot moving average\', default=None, action=CustomAction)\n    arg(\'-v\', \'-sv\', \'--sub_var\', nargs=\'+\', type=int, help=\'Plot in variances\', default=None, action=CustomAction)\n    arg(\'-mv\', \'--mov_var\', nargs=\'+\', type=int, help=\'Plot moving variances\', default=None, action=CustomAction)\n    arg(\'-sh\', \'--head\', nargs=\'+\', type=int, help=\'Select head\', default=None, action=CustomAction)\n    arg(\'-st\', \'--tail\', nargs=\'+\', type=int, help=\'Select tail\', default=None, action=CustomAction)\n    arg(\'-rh\', \'--rhead\', nargs=\'+\', type=int, help=\'Remove head\', default=None, action=CustomAction)\n    arg(\'-rt\', \'--rtail\', nargs=\'+\', type=int, help=\'Remove tail\', default=None, action=CustomAction)\n    arg(\'-lv\', \'--logarithm\', type=str2bool, help=\'Natural logarithm\', default=None, action=CustomAction)\n        ## Misc\n    arg(\'--display\', type=str2bool, help=\'Display plot.\', default=True, action=CustomAction)\n    arg(\'--header\', type=str2bool, help=\'CSV has header.\', default=True, action=CustomAction)\n    arg(\'--ignore_duplicates\', type=str2bool, help=\'Use only one of columns with the same name.\', default=False, action=CustomAction)\n    arg(\'-s\', \'--save\', help=\'Filename to save figure (Only if --refresh is not set).\', default=\'none\', action=CustomAction)\n    arg(\'-r\', \'--refresh\', type=float, help=\'Time interval (ms) to refresh figure data from csv.\', default=None, action=CustomAction)\n    \n    if use_args is not None:\n        opt = parser.parse_args(use_args)\n    else:\n        opt = parser.parse_args()\n\n    return opt\n\n\ndef plot_csv(use_args=None):\n    r""""""Plots data from csv files using pandas.\n    \n    Also usable as a command line program `dlt-plot`.\n\n    Args:\n        use_args (dict, optional): Arguments to use instead of (command line) args.\n\n    Example:\n        Use with command line:\n\n        .. code-block:: console\n\n            $ dlt-plot -f training.csv --sub_avg 500\n\n        From inside a script:\n            \n            >>> dlt.viz.plot_csv([\'--file\', \'training.csv\', \'--sub_avg\', \'500\'])\n\n    Note:\n        For information on available functionality use:\n\n        .. code-block:: console\n\n            $ dlt-plot --help\n    """"""\n    opt = _get_opts(use_args)\n    # make Namespace a dict\n    opt_dict = vars(opt)\n    # Remove size one arrays\n    for v in [\'xlabel\', \'ylabel\', \'title\', \'head\', \'tail\', \'rhead\', \'rtail\', \n              \'sub_avg\', \'mov_avg\', \'sub_var\', \'mov_var\', \'logarithm\']:\n        if opt_dict[v] is not None and len(opt_dict[v]) == 1:\n            opt_dict[v] = opt_dict[v][0]\n    \n    func_dict = OrderedDict([\n        (\'sub_avg\', (lambda x, v: sub_avg(x, v))),\n        (\'mov_avg\', (lambda x, v: moving_avg(x, v))),\n        (\'sub_var\', (lambda x, v: sub_var(x, v))),\n        (\'mov_var\', (lambda x, v: moving_var(x, v))),\n        (\'head\', (lambda x, v: x[:v])),\n        (\'tail\', (lambda x, v: x[-v:])),\n        (\'rhead\', (lambda x, v: x[v:])),\n        (\'rtail\', (lambda x, v: x[:-v])),\n        (\'logarithm\', (lambda x, v: np.log(x)))\n    ])\n    funcs = []\n    if \'_ordered_args\' in opt_dict:\n        for key in opt_dict[\'_ordered_args\']:\n            if key in func_dict and opt_dict[key] is not None:\n                if isinstance(opt_dict[key], list):\n                    func = (lambda x,i,k=key: func_dict[k](x,opt_dict[k][i]))\n                else:\n                    func = (lambda x,i,k=key: func_dict[k](x,opt_dict[k]))\n                funcs.append(lambda df, f=func: pd.DataFrame(OrderedDict([(col, pd.Series(f(df[col], i))) for i, col in enumerate(df.columns) ])))\n\n    transform = compose(funcs)\n\n    plot_args_list = [\'logx\', \'logy\', \'loglog\', \'kind\', \'subplots\', \'layout\',\n                      \'use_index\', \'sharex\', \'sharey\', \'figsize\', \'grid\', \'legend\',\n                      \'style\', \'rot\', \'xlim\', \'ylim\', \'fontsize\', \'colormap\',\n                      \'title\', \'table\']\n    plot_args = {k: v for k, v in opt_dict.items() if k in plot_args_list}\n    \n    data_handler = DataHandler(opt.file, opt.header, opt.ignore_duplicates, transform, opt.names)\n\n    def _update(frame=None):\n        data = data_handler()\n        axes.clear()\n        _set_labels()\n        if data is not None and len(data.index) > 0:\n            try:\n                return data.plot(ax = axes, **plot_args).get_lines()\n            except:\n                return axes.plot([],[])\n        else:\n            return axes.plot([],[])\n\n    def _set_labels():\n        if is_array(axes):\n            for i,a in enumerate(axes):\n                a.set_xlabel(opt_dict[\'xlabel\'][i] if isinstance(opt_dict[\'xlabel\'], list) else opt_dict[\'xlabel\'] if opt_dict[\'xlabel\'] is not None else \'x\') \n                a.set_ylabel(opt_dict[\'ylabel\'][i] if isinstance(opt_dict[\'ylabel\'], list) else opt_dict[\'ylabel\'] if opt_dict[\'ylabel\'] is not None else \'y\')\n        else:\n            axes.set_xlabel(opt_dict[\'xlabel\'] if opt_dict[\'xlabel\'] is not None else \'x\')\n            axes.set_ylabel(opt_dict[\'ylabel\'] if opt_dict[\'ylabel\'] is not None else \'y\')\n\n    figure, axes = plt.subplots(num=\'viz\')\n    _update()\n    if opt.refresh is not None and opt.refresh > 0.001:\n        anim = FuncAnimation(fig=figure, func=_update, frames=None,\n                             init_func=None, save_count=None, \n                             interval=opt.refresh*1000,\n                             repeat=False, blit=False)\n    else:\n        if opt.save != \'none\':\n            plt.savefig(opt.save, bbox_inches=\'tight\')\n\n    plt.show()'"
dlt/viz/imshow.py,0,"b'import time\nimport warnings\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom ..util import to_array, change_view, make_grid\n\n## https://stackoverflow.com/questions/22873410/how-do-i-fix-the-deprecation-warning-that-comes-with-pylab-pause\nwarnings.filterwarnings(""ignore"", "".*GUI is implemented.*"")\n\n## https://stackoverflow.com/questions/45729092/make-interactive-matplotlib-window-not-pop-to-front-on-each-update-windows-7\ndef mypause(interval):\n    backend = matplotlib.rcParams[\'backend\']\n    if backend in matplotlib.rcsetup.interactive_bk:\n        figManager = matplotlib._pylab_helpers.Gcf.get_active()\n        if figManager is not None:\n            canvas = figManager.canvas\n            if canvas.figure.stale:\n                canvas.draw()\n            canvas.start_event_loop(interval)\n            return\n\n    # No on-screen figure is active, so sleep() is all we need.\n    time.sleep(interval)\n\n_pause_min = 5e-2\n\ndef imshow(img, view=\'torch\', figure=None, pause=0, title=None, interactive=False, *args, **kwargs):\n    """"""Displays a Tensor or Array image to screen.\n\n    Args:\n        img (Tensor or Array): Image to display.\n        view (str, optional): View of image. For more details see\n            :func:`dlt.util.change_view` (default \'torch\').\n        figure (int, optional): Use selected figure (default None).\n        pause (float, optional): Number of seconds to pause execution for\n            displaying when interactive is True. If a value less than 1e-2 is\n            given then it defaults to 1e-2 (default 1e-2).\n        title (str, optional): Title for figure (default None).\n        interactive (bool, optional): If the image will be updated; uses\n            plt.ion() (default False).\n        *args (optional): Extra arguments to be passed to plt.imshow().\n        **kwargs (optional): Extra keyword arguments to be passed to plt.imshow().\n    Example:\n        >>> for video_1_frame, video_2_frame in two_videos_frames:\n        >>>     dlt.viz.imshow(video_1_frame, view=\'cv\', figure=1, interactive=True, title=\'Video 1\')\n        >>>     dlt.viz.imshow(video_2_frame, view=\'cv\', figure=2, interactive=True, title=\'Video 2\')\n\n    """"""\n    if figure is None:\n        if imshow.my_figure is None:\n            imshow.my_figure = plt.figure().number\n        figure = imshow.my_figure\n    else:\n        figure = plt.figure(figure).number\n\n    if title is not None:\n        f = plt.gcf()\n        f.canvas.set_window_title(title)\n    \n    if interactive:\n        plt.ion()\n        plt.clf()\n        pause = max(_pause_min, pause)\n    else:\n        plt.ioff()\n        \n    img = to_array(img)\n    if img.ndim not in (2,3):\n        raise ValueError(\'Images must have two or three dimensions.\')\n\n    img = change_view(img, view, \'plt\')\n    if img.shape[-1] not in (1,3,4):\n        raise ValueError(\'Invalid number of channels ({0}). \'.format(img.shape[-1])\n                         + \'Perhaps you used the wrong view?\')\n    img = img.squeeze()\n\n    plt.imshow(img, cmap=\'gray\' if img.ndim == 2 else None, *args, **kwargs)\n\n    if pause > 0:\n        mypause(pause)\n    \n    if figure not in imshow.displayed or not interactive:\n        imshow.displayed.add(figure)\n        plt.show(block=not interactive)\n    \n    return figure\n\nimshow.my_figure = None\nimshow.displayed = set()'"
dlt/viz/modules.py,3,"b'import os\nimport matplotlib.pyplot as plt\nimport cv2\nimport torch\nimport pandas as pd\nfrom ..util.paths import process\nfrom ..util import torch2cv, map_range, make_grid\n\ndef _get_tensor(x):\n    x = x[0] if torch.typename(x) in [\'tuple\', \'list\'] else x\n    return x\n\ndef save_image(img, title):\n    to_save = ((img / img.max())*255).astype(int)\n    cv2.imwrite(\'{0}.png\'.format(title), to_save)\n\n\ndef process_none(x):\n    if x is None:\n        x = []\n    elif not any((isinstance(x, y) for y in [list, tuple])):\n        x = [x]\n    return x\n\ndef _register(net, hook, modules=None, match_names=None, do_forward=True):\n    modules = process_none(modules)\n    match_names = process_none(match_names)\n    for mod_name, mod in net.named_modules():\n        name_match = any([torch.typename(modules).find(x) >= 0 for x in match_names])\n        instance_match = any([isinstance(mod, x) for x in modules])\n        if instance_match or name_match:\n            if do_forward:\n                mod.register_forward_hook(hook(mod_name))\n            else:\n                mod.register_backward_hook(hook(mod_name))\n    return net\n\ndef _hook_generator(do_input=False, do_output=True, tag=\'\', save_path=\'.\', replace=True, histogram=True, bins=100, mode=\'forward\', param_names=None):\n    save_path = process(save_path, True)\n    tensor_names = [\'input\', \'output\'] if mode in [\'forward\', \'parameters\'] else [\'grad_input\', \'grad_output\']    \n    def get_hook(module_name):\n        counter = 1\n        def hook(module, inp=None, out=None):\n            nonlocal counter, tensor_names\n            if mode == \'parameters\':\n                tensors = {x: _get_tensor(getattr(module, x)) for x in param_names}\n            else:\n                tensors = [(tensor_names[0], inp, do_input), (tensor_names[1], out, do_output)]\n                tensors = {x[0]: _get_tensor(x[1]) for x in tensors if x[2]}\n            for tensor_name, data in tensors.items():\n                if data is None:\n                    continue\n                title_end = \'\' if replace else \'-{0:06d}\'.format(counter) \n                title_end = title_end + \'-hist\' if histogram else title_end\n                title = \'{0}-{1}-{2}{3}\'.format(tag, module_name, tensor_name, title_end)\n                if histogram:\n                    img = torch2cv(data)\n                    df = pd.DataFrame(img.reshape(img.size))\n                    fig, ax = plt.subplots()\n                    df.hist(bins=bins, ax=ax)\n                    fig.savefig(os.path.join(save_path, \'{0}.png\'.format(title)))\n                    plt.close(fig)\n                else:\n                    if data.dim() > 1:\n                        img = torch2cv(make_grid(data, color=False))\n                        to_save = (map_range(img)*255).astype(int)\n                        cv2.imwrite(os.path.join(save_path, \'{0}.png\'.format(title)), to_save)\n            counter = counter+1\n        return hook\n    return get_hook\n\n\n\ndef forward_hook(net, modules=None, match_names=None, do_input=False, do_output=True, \n                       tag=\'\', save_path=\'.\', replace=True, histogram=True, bins=100):\n    """"""Registers a forward hook to a network\'s modules for vizualization of the inputs and outputs.\n\n    When net.forward() is called, the hook saves an image grid or a histogram \n    of input/output of the specified modules.\n\n    Args:\n        net (nn.Module): The network whose modules are to be visualized.\n        modules (list or tuple, optional): List of class definitions for the\n            modules where the hook is attached e.g. nn.Conv2d  (default None).\n        match_names (list or tuple, optional): List of strings. If any modules\n            contain one of the strings then the hook is attached (default None).\n        do_input (bool, optional): If True the input of the module is \n            visualized (default False).\n        do_output (bool, optional): If True the output of the module is \n            visualized (default True).\n        tag (str, optional): String tag to attach to saved images (default None).\n        save_path (str, optional): Path to save visualisation results \n            (default \'.\').\n        replace (bool, optional): If True, the images (from the same module) \n            are replaced whenever the hook is called (default True).\n        histogram (bool, optional): If True then the visualization is a\n            histrogram, otherwise it\'s an image grid.\n        bins (bool, optional): Number of bins for histogram, if `histogram` is\n            True (default 100).\n\n    Note:\n        * If modules or match_names are not provided then no hooks will be\n          attached.\n    """"""\n    hook = _hook_generator(do_input,do_output,tag,save_path,replace, histogram, bins, \'forward\')\n    _register(net, hook, modules, match_names, True)\n    return net\n\ndef backward_hook(net, modules=None, match_names=None, do_grad_input=False, do_grad_output=True, \n                       tag=\'\', save_path=\'.\', replace=True, histogram=True, bins=100):\n    """"""Registers a backward hook to a network\'s modules for vizualization of the gradients.\n\n    When net.backward() is called, the hook saves an image grid or a histogram \n    of grad_input/grad_output of the specified modules.\n\n    Args:\n        net (nn.Module): The network whose gradients are to be visualized.\n        modules (list or tuple, optional): List of class definitions for the\n            modules where the hook is attached e.g. nn.Conv2d  (default None).\n        match_names (list or tuple, optional): List of strings. If any modules\n            contain one of the strings then the hook is attached (default None).\n        do_grad_input (bool, optional): If True the grad_input of the module is \n            visualized (default False).\n        do_grad_output (bool, optional): If True the grad_output of the module \n            is visualized (default True).\n        tag (str, optional): String tag to attach to saved images (default None).\n        save_path (str, optional): Path to save visualisation results \n            (default \'.\').\n        replace (bool, optional): If True, the images (from the same module) \n            are replaced whenever the hook is called (default True).\n        histogram (bool, optional): If True then the visualization is a\n            histrogram, otherwise it\'s an image grid.\n        bins (bool, optional): Number of bins for histogram, if `histogram` is\n            True (default 100).\n    \n    Note:\n        * If modules or match_names are not provided then no hooks will be\n          attached.\n    """"""\n    hook = _hook_generator(do_grad_input,do_grad_output,tag,save_path,replace, histogram, bins, \'backward\')    \n    _register(net, hook, modules, match_names, False)\n    return net\n\ndef parameters_hook(net, modules=None, match_names=None, param_names=None,\n                    tag=\'\', save_path=\'.\', replace=True, histogram=True, bins=100):\n    """"""Registers a forward hook to a network\'s modules for vizualization of its parameters.\n\n    When net.forward() is called, the hook saves an image grid or a histogram \n    of the parameters of the specified modules.\n\n    Args:\n        net (nn.Module): The network whose parameters are to be visualized.\n        modules (list or tuple, optional): List of class definitions for the\n            modules where the hook is attached e.g. nn.Conv2d  (default None).\n        match_names (list or tuple, optional): List of strings. If any modules\n            contain one of the strings then the hook is attached (default None).\n        param_names (list or tuple, optional): List of strings. If any\n            parameters of the module contain one of the strings then they are\n            visualized (default None).\n        tag (str, optional): String tag to attach to saved images (default None).\n        save_path (str, optional): Path to save visualisation results \n            (default \'.\').\n        replace (bool, optional): If True, the images (from the same module) \n            are replaced whenever the hook is called (default True).\n        histogram (bool, optional): If True then the visualization is a\n            histrogram, otherwise it\'s an image grid.\n        bins (bool, optional): Number of bins for histogram, if `histogram` is\n            True (default 100).\n\n    Note:\n        * If modules or match_names are not provided then no hooks will be\n          attached.\n        * If param_names are not provided then no parameters will be visualized.\n    """"""\n    hook = _hook_generator(False,False,tag,save_path,replace, histogram, bins, \'parameters\', param_names)    \n    _register(net, hook, modules, match_names, True)\n    return net\n\ndef parameters(net, modules=None, match_names=None, param_names=None, tag=\'\', save_path=\'.\', histogram=True, bins=100):\n    """"""Visualizes a network\'s parameters on an image grid or histogram.\n\n    Args:\n        net (nn.Module): The network whose parameters are to be visualized.\n        modules (list or tuple, optional): List of class definitions for the\n            modules where the hook is attached e.g. nn.Conv2d  (default None).\n        match_names (list or tuple, optional): List of strings. If any modules\n            contain one of the strings then the hook is attached (default None).\n        param_names (list or tuple, optional): List of strings. If any\n            parameters of the module contain one of the strings then they are\n            visualized (default None).\n        tag (str, optional): String tag to attach to saved images (default None).\n        save_path (str, optional): Path to save visualisation results \n            (default \'.\').\n        histogram (bool, optional): If True then the visualization is a\n            histrogram, otherwise it\'s an image grid.\n        bins (bool, optional): Number of bins for histogram, if `histogram` is\n            True (default 100).\n\n    Note:\n        * If modules or match_names are not provided then no parameters will be\n          visualized.\n        * If param_names are not provided then no parameters will be visualized.\n    """"""\n    save_path = process(save_path, True)\n    modules = process_none(modules)\n    match_names = process_none(match_names)\n    for module_name, mod in net.named_modules():\n        name_match = any([torch.typename(modules).find(x) >= 0 for x in match_names])\n        instance_match = any([isinstance(mod, x) for x in modules])\n        if instance_match or name_match:\n            params = {x: _get_tensor(getattr(mod, x)) for x in param_names}\n            for tensor_name, data in params.items():\n                title = \'{0}-{1}-{2}\'.format(tag, module_name, tensor_name)\n                if data is None:\n                    continue\n                if histogram:\n                    img = torch2cv(data)\n                    df = pd.DataFrame(img.reshape(img.size))\n                    fig, ax = plt.subplots()\n                    df.hist(bins=bins, ax=ax)\n                    fig.savefig(os.path.join(save_path, \'{0}.png\'.format(title)))\n                    plt.close(fig)\n                else:\n                    if data.dim() > 1:\n                        img = torch2cv(make_grid(data, color=False))\n                        to_save = (map_range(img)*255).astype(int)\n                        cv2.imwrite(os.path.join(save_path, \'{0}.png\'.format(title)), to_save)'"
examples/gans/main.py,4,"b""import torch\nimport dlt\nfrom dlt.train import VanillaGANTrainer as GAN\nfrom dlt.train import WGANGPTrainer as WGANGP\nfrom dlt.train import WGANCTTrainer as WGANCT\nfrom dlt.train import BEGANTrainer as BEGAN\nfrom dlt.train import FisherGANTrainer as FisherGAN\nfrom models import *\n\n# Settings\ndlt.config.make_subsets({'model': ['generator', 'discriminator'],\n                         'optimizer': ['generator', 'discriminator']})\ndlt.config.add_extras([\n    dict(flag='--gan_type', choices=['vanilla', 'wgan-gp', 'wgan-ct', 'began', 'fishergan'],\n                            default='vanilla', help='Gan type'),\n    dict(flag='--num_hidden', type=int, default=64, help='Number of hidden units'),\n    dict(flag='--z_dim', type=int, default=128, help='Input noise dimensionality'),\n    dict(flag='--lambda_gp', type=float, default=10, help='Gradient penalty magnitude'),\n    dict(flag='--m_ct', type=float, default=0.001, help='Constant bound for consistency term for WGAN-CT'),\n    dict(flag='--lambda_ct', type=float, default=0.001, help='Weight of consistency term for WGAN-CT'),\n    dict(flag='--lambda_k', type=float, default=0.001, help='Learning rate for k for BEGAN'),\n    dict(flag='--gamma', type=float, default=0.5, help='Gamma for BEGAN (diversity ratio)'),\n    dict(flag='--rho', type=float, default=1e-6, help='rho for FisherGAN'),\n    dict(flag='--d_iter', type=int, default=2, help='Number of discriminator steps per generator')\n])\nopt = dlt.config.parse(verbose=True)\n\n# Configure seeds\nif opt.seed is not None:\n    torch.manual_seed(opt.seed)\n\n# Data\nsizes = {'mnist': (1, 28), 'fashionmnist': (1, 28),\n         'cifar10': (3, 32), 'cifar100': (3, 32)}\nif opt.torchvision_dataset not in sizes:\n    raise ValueError('--torchvision_dataset must be one of {0}'.format(','.join(sizes.keys())))\nsize = sizes[opt.torchvision_dataset]\ndef preprocess(datum):\n    noise = torch.Tensor(opt.z_dim).uniform_(-1, 1)\n    real_image = (dlt.util.cv2torch(datum[0]).float()/255.0) * 1.8 - 0.9\n    # By convention, the trainer accepts the first point as the generator\n    # input and the second as the real input for the discriminator\n    return noise, real_image\n\ndataset = dlt.config.torchvision_dataset()\nloader = dlt.config.loader(dataset, preprocess)\n\n# Models\ngenerator = Generator(opt.num_hidden, opt.z_dim, size[0], size[1])\ngen_chkp = dlt.config.model_checkpointer(generator, subset='generator')\n\nif opt.gan_type == 'began':\n    discriminator = DiscriminatorBEGAN(opt.num_hidden, size[0], size[1])    \nelse:\n    discriminator = Discriminator(opt.num_hidden, size[0], size[1])\ndisc_chkp = dlt.config.model_checkpointer(discriminator, subset='discriminator')\n\n# Cudafy\nif opt.use_gpu:\n    torch.cuda.set_device(opt.device)\n    torch.backends.cudnn.benchmark = opt.cudnn_benchmark\n    generator.cuda()\n    discriminator.cuda()\n\n# Optimizers\ng_optim = dlt.config.optimizer(generator, subset='generator')\ng_optim_chkp = dlt.config.optimizer_checkpointer(g_optim, subset='generator')\nd_optim = dlt.config.optimizer(discriminator, subset='discriminator')\nd_optim_chkp = dlt.config.optimizer_checkpointer(d_optim, subset='discriminator')\n\n# Trainer\nif opt.gan_type == 'wgan-gp':\n    trainer = WGANGP(generator, discriminator, g_optim, d_optim, opt.lambda_gp, opt.d_iter)\nelif opt.gan_type == 'began':\n    trainer = BEGAN(generator, discriminator, g_optim, d_optim, opt.lambda_k, opt.gamma, opt.d_iter)\nelif opt.gan_type == 'fishergan':\n    trainer = FisherGAN(generator, discriminator, g_optim, d_optim, opt.rho, opt.d_iter)\nelif opt.gan_type == 'wgan-ct':\n    trainer = WGANCT(generator, discriminator, g_optim, d_optim, opt.lambda_gp, opt.m_ct, opt.lambda_ct, opt.d_iter)\nelse:\n    trainer = GAN(generator, discriminator, g_optim, d_optim, opt.d_iter)\n\ntrainer_chkp = dlt.config.trainer_checkpointer(trainer)\n\nif opt.use_gpu:\n    trainer.cuda() # Trainers might have buffers that need to be transferred to GPU\n\n# Logging\nlog = dlt.util.Logger('training', trainer.loss_names_training(), opt.save_path)\n\n# Training loop\nfor epoch in range(trainer.epoch, opt.max_epochs):\n    tag = 'epoch-{0}'.format(epoch)\n    print('-'*79 + '\\nEpoch {0}:'.format(epoch))\n    # Set to training mode\n    trainer.train()\n    # The trainer iterator performs the optimization and gives predictions and\n    # losses at each iteration\n    for i, (batch, (prediction, losses)) in enumerate(trainer(loader)):\n        # Show progress of each iteration and log the losses\n        dlt.config.sample_images([batch[1], prediction], color=size[0] == 3,\n                                 preprocess=dlt.util.map_range, tag=tag)\n        log(losses)\n    \n    # Checkpoint everything\n    gen_chkp(generator, tag=tag)\n    disc_chkp(discriminator, tag=tag)\n    g_optim_chkp(g_optim, tag=tag)\n    d_optim_chkp(d_optim, tag=tag)\n    trainer_chkp(trainer, tag=tag)\n    """
examples/gans/models.py,0,"b'from torch import nn\n\ndef selu_init(model):\n    for m in model.modules():\n        if any([isinstance(m, x) for x in [nn.Conv2d, nn.ConvTranspose2d, nn.Linear]]):\n            nn.init.kaiming_normal_(m.weight, 1)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, val=0)\n\nclass Generator(nn.Module):\n    def __init__(self, num_hidden, z_dim, num_chan, num_pix):\n        super(Generator, self).__init__()\n        self.num_pix = num_pix\n        self.num_chan = num_chan\n        self.main = nn.Sequential(\n            nn.Linear(z_dim, num_hidden),\n            nn.SELU(),\n            nn.Linear(num_hidden, num_hidden),\n            nn.SELU(),\n            nn.Linear(num_hidden, num_chan*num_pix*num_pix),\n            nn.Tanh()\n        )\n        selu_init(self)\n\n    def forward(self, v_input):\n        return self.main(v_input).view(v_input.size(0), self.num_chan, self.num_pix, self.num_pix)\n\nclass Discriminator(nn.Module):\n    def __init__(self, num_hidden, num_chan, num_pix):\n        super(Discriminator, self).__init__()\n        self.num_pix = num_pix\n        self.num_chan = num_chan\n        self.main = nn.Sequential(\n            nn.Linear(num_chan*num_pix*num_pix, num_hidden),\n            nn.SELU(),\n            nn.Linear(num_hidden, num_hidden),\n            nn.SELU()\n        )\n        self.last_layer = nn.Linear(num_hidden, 1)\n        selu_init(self)\n\n    # The correction term is for WGAN-CT\n    def forward(self, v_input, correction_term=False):\n        if correction_term:\n            main = self.main(v_input.view(v_input.size(0), -1))\n            noisy_main = nn.functional.dropout(main, p=0.1)\n            return main, self.last_layer(noisy_main)\n        else:\n            return self.last_layer(self.main(v_input.view(v_input.size(0), -1)))\n\n# BEGAN needs an autoencoding discriminator\nclass DiscriminatorBEGAN(nn.Module):\n    def __init__(self, num_hidden, num_chan, num_pix):\n        super(DiscriminatorBEGAN, self).__init__()\n        self.num_pix = num_pix\n        self.num_chan = num_chan\n        self.main = nn.Sequential(\n            nn.Linear(num_chan*num_pix*num_pix, num_hidden), nn.SELU(),\n            nn.Linear(num_hidden, num_hidden), nn.SELU(),\n            nn.Linear(num_hidden, num_chan*num_pix*num_pix),\n        )\n        selu_init(self)\n\n    def forward(self, v_input):\n        res = self.main(v_input.view(v_input.size(0), -1))\n        return res.view(v_input.size(0), self.num_chan, self.num_pix, self.num_pix)'"
dlt/util/external/__init__.py,0,b'from .accuracy import accuracy\nfrom .compose import compose'
dlt/util/external/accuracy.py,1,"b'# Adapted from PyTorch ImageNet example\n# https://github.com/pytorch/examples/blob/master/imagenet/main.py\n\n# BSD 3-Clause License\n\n# Copyright (c) 2017, \n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport torch\n\ndef accuracy(output, target, topk=1):\n    """"""Computes the precision@k for the specified values of k.\n    \n    (From ImageNet example_)\n\n    .. _example: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n\n       Args:\n            output (Tensor): The class labels.\n            target (Tensor): The predictions from the model.\n            topk (int or collection): The specified values of k.\n\n       Returns:\n            list: The top-k accuracy values.\n    \n    """"""\n\n    if isinstance(topk, int):\n        topk = (topk, )\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n'"
dlt/util/external/compose.py,0,"b'# Adapted from pytorch torchnet \n# https://github.com/pytorch/tnt/blob/master/torchnet/transform.py\n###############################################################################\n# BSD 3-Clause License\n\n# Copyright (c) 2017- Sergey Zagoruyko,\n# Copyright (c) 2017- Sasank Chilamkurthy, \n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n###############################################################################\n\ndef compose(transforms):\n    """"""Composes list of transforms (each accept and return one item).\n       \n       (From PyTorchNet_)\n\n       .. _PyTorchNet: https://github.com/pytorch/tnt\n\n       Args:\n            transforms (list):  List of callables, each accepts \n                and returns one item.\n\n       Returns:\n            callable: The composed transforms.\n\n    """"""\n    assert isinstance(transforms, list)\n    for transform in transforms:\n        assert callable(transform), ""list of functions expected""\n\n    def composition(obj):\n        ""Composite function""\n        for transform in transforms:\n            obj = transform(obj)\n        return obj\n    return composition'"
