file_path,api_count,code
server.py,0,"b'from flask import Flask\nfrom flask import request, jsonify, abort, make_response\nfrom flask_cors import CORS\nimport nltk\nnltk.download(\'punkt\')\nfrom nltk import tokenize\nfrom typing import List\nimport argparse\nfrom summarizer import Summarizer, TransformerSummarizer\n\n\napp = Flask(__name__)\nCORS(app)\n\n\nclass Parser(object):\n\n    def __init__(self, raw_text: bytes):\n        self.all_data = str(raw_text, \'utf-8\').split(\'\\n\')\n\n    def __isint(self, v) -> bool:\n        try:\n            int(v)\n            return True\n        except:\n            return False\n\n    def __should_skip(self, v) -> bool:\n        return self.__isint(v) or v == \'\\n\' or \'-->\' in v\n\n    def __process_sentences(self, v) -> List[str]:\n        sentence = tokenize.sent_tokenize(v)\n        return sentence\n\n    def save_data(self, save_path, sentences) -> None:\n        with open(save_path, \'w\') as f:\n            for sentence in sentences:\n                f.write(""%s\\n"" % sentence)\n\n    def run(self) -> List[str]:\n        total: str = \'\'\n        for data in self.all_data:\n            if not self.__should_skip(data):\n                cleaned = data.replace(\'&gt;\', \'\').replace(\'\\n\', \'\').strip()\n                if cleaned:\n                    total += \' \' + cleaned\n        sentences = self.__process_sentences(total)\n        return sentences\n\n    def convert_to_paragraphs(self) -> str:\n        sentences: List[str] = self.run()\n        return \' \'.join([sentence.strip() for sentence in sentences]).strip()\n\n\n@app.route(\'/summarize\', methods=[\'POST\'])\ndef convert_raw_text():\n    ratio = float(request.args.get(\'ratio\', 0.2))\n    min_length = int(request.args.get(\'min_length\', 25))\n    max_length = int(request.args.get(\'max_length\', 500))\n\n    data = request.data\n    if not data:\n        abort(make_response(jsonify(message=""Request must have raw text""), 400))\n\n    parsed = Parser(data).convert_to_paragraphs()\n    summary = summarizer(parsed, ratio=ratio, min_length=min_length, max_length=max_length)\n\n    return jsonify({\n        \'summary\': summary\n    })\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'\')\n    parser.add_argument(\'-model\', dest=\'model\', default=\'bert-base-uncased\', help=\'The model to use\')\n    parser.add_argument(\'-transformer-type\',\n                        dest=\'transformer_type\', default=None,\n                        help=\'Huggingface transformer class key\')\n    parser.add_argument(\'-transformer-key\', dest=\'transformer_key\', default=None,\n                        help=\'The transformer key for huggingface. For example bert-base-uncased for Bert Class\')\n    parser.add_argument(\'-greediness\', dest=\'greediness\', help=\'\', default=0.45)\n    parser.add_argument(\'-reduce\', dest=\'reduce\', help=\'\', default=\'mean\')\n    parser.add_argument(\'-hidden\', dest=\'hidden\', help=\'\', default=-2)\n    parser.add_argument(\'-port\', dest=\'port\', help=\'\', default=5000)\n    parser.add_argument(\'-host\', dest=\'host\', help=\'\', default=\'0.0.0.0\')\n\n    args = parser.parse_args()\n\n    if args.transformer_type is not None:\n        print(f""Using Model: {args.transformer_type}"")\n        assert args.transformer_key is not None, \'Transformer Key cannot be none with the transformer type\'\n\n        summarizer = TransformerSummarizer(\n            transformer_type=args.transformer_type,\n            transformer_model_key=args.transformer_key,\n            hidden=int(args.hidden),\n            reduce_option=args.reduce\n        )\n\n    else:\n        print(f""Using Model: {args.model}"")\n\n        summarizer = Summarizer(\n            model=args.model,\n            hidden=int(args.hidden),\n            reduce_option=args.reduce\n        )\n\n    app.run(host=args.host, port=int(args.port))\n'"
setup.py,0,"b'from setuptools import setup\nfrom setuptools import find_packages\n\nsetup(name=\'bert-extractive-summarizer\',\n      version=\'0.4.2\',\n      description=\'Extractive Text Summarization with BERT\',\n      keywords = [\'bert\', \'pytorch\', \'machine learning\', \'deep learning\', \'extractive summarization\', \'summary\'],\n      long_description=open(""README.md"", ""r"", encoding=\'utf-8\').read(),\n      long_description_content_type=""text/markdown"",\n      url=\'https://github.com/dmmiller612/bert-extractive-summarizer\',\n      download_url=\'https://github.com/dmmiller612/bert-extractive-summarizer/archive/0.4.2.tar.gz\',\n      author=\'Derek Miller\',\n      author_email=\'dmmiller612@gmail.com\',\n      install_requires=[\'transformers\', \'scikit-learn\', \'spacy\'],\n      license=\'MIT\',\n      packages=find_packages(),\n      zip_safe=False)\n'"
summarize.py,0,"b'from summarizer import Summarizer\nimport argparse\n\n\ndef run():\n    parser = argparse.ArgumentParser(description=\'Process and summarize lectures\')\n    parser.add_argument(\'-path\', dest=\'path\', default=None, help=\'File path of lecture\')\n    parser.add_argument(\'-model\', dest=\'model\', default=\'bert-large-uncased\', help=\'\')\n    parser.add_argument(\'-hidden\', dest=\'hidden\', default=-2, help=\'Which hidden layer to use from Bert\')\n    parser.add_argument(\'-reduce-option\', dest=\'reduce_option\', default=\'mean\', help=\'How to reduce the hidden layer from bert\')\n    parser.add_argument(\'-greedyness\', dest=\'greedyness\', help=\'Greedyness of the NeuralCoref model\', default=0.45)\n    args = parser.parse_args()\n\n    if not args.path:\n        raise RuntimeError(""Must supply text path."")\n\n    with open(args.path) as d:\n        text_data = d.read()\n\n    model = Summarizer(\n        model=args.model,\n        hidden=args.hidden,\n        reduce_option=args.reduce_option\n    )\n\n    print(model(text_data))\n\n\nif __name__ == \'__main__\':\n    run()\n\n'"
summarizer/__init__.py,0,"b'from summarizer.model_processors import Summarizer, SingleModel, TransformerSummarizer'"
summarizer/bert_parent.py,4,"b'from transformers import *\nimport logging\nimport torch\nimport numpy as np\nfrom numpy import ndarray\nfrom typing import List\n\nlogging.basicConfig(level=logging.WARNING)\n\n\nclass BertParent(object):\n\n    """"""\n    Base handler for BERT models.\n    """"""\n\n    MODELS = {\n        \'bert-base-uncased\': (BertModel, BertTokenizer),\n        \'bert-large-uncased\': (BertModel, BertTokenizer),\n        \'xlnet-base-cased\': (XLNetModel, XLNetTokenizer),\n        \'xlm-mlm-enfr-1024\': (XLMModel, XLMTokenizer),\n        \'distilbert-base-uncased\': (DistilBertModel, DistilBertTokenizer),\n        \'albert-base-v1\': (AlbertModel, AlbertTokenizer),\n        \'albert-large-v1\': (AlbertModel, AlbertTokenizer)\n    }\n\n    def __init__(\n        self,\n        model: str,\n        custom_model: PreTrainedModel=None,\n        custom_tokenizer: PreTrainedTokenizer=None\n    ):\n        """"""\n        :param model: Model is the string path for the bert weights. If given a keyword, the s3 path will be used\n        :param custom_model: This is optional if a custom bert model is used\n        :param custom_tokenizer: Place to use custom tokenizer\n        """"""\n\n        base_model, base_tokenizer = self.MODELS.get(model, (None, None))\n\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n        if custom_model:\n            self.model = custom_model.to(self.device)\n        else:\n            self.model = base_model.from_pretrained(model, output_hidden_states=True).to(self.device)\n\n        if custom_tokenizer:\n            self.tokenizer = custom_tokenizer\n        else:\n            self.tokenizer = base_tokenizer.from_pretrained(model)\n\n        self.model.eval()\n\n    def tokenize_input(self, text: str) -> torch.tensor:\n        """"""\n        Tokenizes the text input.\n\n        :param text: Text to tokenize\n        :return: Returns a torch tensor\n        """"""\n        tokenized_text = self.tokenizer.tokenize(text)\n        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n        return torch.tensor([indexed_tokens]).to(self.device)\n\n    def extract_embeddings(\n        self,\n        text: str,\n        hidden: int=-2,\n        reduce_option: str =\'mean\'\n    ) -> torch.Tensor:\n\n        """"""\n        Extracts the embeddings for the given text\n\n        :param text: The text to extract embeddings for.\n        :param hidden: The hidden layer to use for a readout handler\n        :param squeeze: If we should squeeze the outputs (required for some layers)\n        :param reduce_option: How we should reduce the items.\n        :return: A numpy array.\n        """"""\n\n        tokens_tensor = self.tokenize_input(text)\n        pooled, hidden_states = self.model(tokens_tensor)[-2:]\n\n        if -1 > hidden > -12:\n\n            if reduce_option == \'max\':\n                pooled = hidden_states[hidden].max(dim=1)[0]\n\n            elif reduce_option == \'median\':\n                pooled = hidden_states[hidden].median(dim=1)[0]\n\n            else:\n                pooled = hidden_states[hidden].mean(dim=1)\n\n        return pooled\n\n    def create_matrix(\n        self,\n        content: List[str],\n        hidden: int=-2,\n        reduce_option: str = \'mean\'\n    ) -> ndarray:\n        """"""\n        Create matrix from the embeddings\n\n        :param content: The list of sentences\n        :param hidden: Which hidden layer to use\n        :param reduce_option: The reduce option to run.\n        :return: A numpy array matrix of the given content.\n        """"""\n\n        return np.asarray([\n            np.squeeze(self.extract_embeddings(t, hidden=hidden, reduce_option=reduce_option).data.cpu().numpy())\n            for t in content\n        ])\n\n    def __call__(\n        self,\n        content: List[str],\n        hidden: int= -2,\n        reduce_option: str = \'mean\'\n    ) -> ndarray:\n        return self.create_matrix(content, hidden, reduce_option)\n'"
summarizer/cluster_features.py,0,"b'import numpy as np\nfrom numpy import ndarray\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.decomposition import PCA\nfrom typing import List\n\n\nclass ClusterFeatures(object):\n    """"""\n    Basic handling of clustering features.\n    """"""\n\n    def __init__(\n        self,\n        features: ndarray,\n        algorithm: str = \'kmeans\',\n        pca_k: int = None,\n        random_state: int = 12345\n    ):\n        """"""\n        :param features: the embedding matrix created by bert parent\n        :param algorithm: Which clustering algorithm to use\n        :param pca_k: If you want the features to be ran through pca, this is the components number\n        :param random_state: Random state\n        """"""\n\n        if pca_k:\n            self.features = PCA(n_components=pca_k).fit_transform(features)\n        else:\n            self.features = features\n\n        self.algorithm = algorithm\n        self.pca_k = pca_k\n        self.random_state = random_state\n\n    def __get_model(self, k: int):\n        """"""\n        Retrieve clustering model\n\n        :param k: amount of clusters\n        :return: Clustering model\n\n        """"""\n\n        if self.algorithm == \'gmm\':\n            return GaussianMixture(n_components=k, random_state=self.random_state)\n        return KMeans(n_clusters=k, random_state=self.random_state)\n\n    def __get_centroids(self, model):\n        """"""\n        Retrieve centroids of model\n        :param model: Clustering model\n        :return: Centroids\n        """"""\n\n        if self.algorithm == \'gmm\':\n            return model.means_\n        return model.cluster_centers_\n\n    def __find_closest_args(self, centroids: np.ndarray):\n        """"""\n        Find the closest arguments to centroid\n        :param centroids: Centroids to find closest\n        :return: Closest arguments\n        """"""\n\n        centroid_min = 1e10\n        cur_arg = -1\n        args = {}\n        used_idx = []\n\n        for j, centroid in enumerate(centroids):\n\n            for i, feature in enumerate(self.features):\n                value = np.linalg.norm(feature - centroid)\n\n                if value < centroid_min and i not in used_idx:\n                    cur_arg = i\n                    centroid_min = value\n\n            used_idx.append(cur_arg)\n            args[j] = cur_arg\n            centroid_min = 1e10\n            cur_arg = -1\n\n        return args\n\n    def cluster(self, ratio: float = 0.1) -> List[int]:\n        """"""\n        Clusters sentences based on the ratio\n        :param ratio: Ratio to use for clustering\n        :return: Sentences index that qualify for summary\n        """"""\n\n        k = 1 if ratio * len(self.features) < 1 else int(len(self.features) * ratio)\n        model = self.__get_model(k).fit(self.features)\n        centroids = self.__get_centroids(model)\n        cluster_args = self.__find_closest_args(centroids)\n        sorted_values = sorted(cluster_args.values())\n        return sorted_values\n\n    def __call__(self, ratio: float = 0.1) -> List[int]:\n        return self.cluster(ratio)\n'"
summarizer/coreference_handler.py,0,"b'# removed previous import and related functionality since it\'s just a blank language model,\n#  while neuralcoref requires passing pretrained language model via spacy.load()\n\nimport neuralcoref\nfrom summarizer.sentence_handler import SentenceHandler\n\nimport spacy\n\n\nclass CoreferenceHandler(SentenceHandler):\n\n    def __init__(self, spacy_model: str = \'en_core_web_sm\', greedyness: float = 0.45):\n        self.nlp = spacy.load(spacy_model)\n        neuralcoref.add_to_pipe(self.nlp, greedyness=greedyness)\n\n    def process(self, body: str, min_length: int = 40, max_length: int = 600):\n        """"""\n        Processes the content sentences.\n\n        :param body: The raw string body to process\n        :param min_length: Minimum length that the sentences must be\n        :param max_length: Max length that the sentences mus fall under\n        :return: Returns a list of sentences.\n        """"""\n        doc = self.nlp(body)._.coref_resolved\n        doc = self.nlp(doc)\n        return [c.string.strip() for c in doc.sents if max_length > len(c.string.strip()) > min_length]\n'"
summarizer/model_processors.py,0,"b'from summarizer.bert_parent import BertParent\nfrom summarizer.cluster_features import ClusterFeatures\nfrom summarizer.sentence_handler import SentenceHandler\nfrom typing import List\nfrom abc import abstractmethod\nimport numpy as np\nfrom transformers import *\n\n\nclass ModelProcessor(object):\n\n    def __init__(\n        self,\n        model: str = \'bert-large-uncased\',\n        custom_model: PreTrainedModel = None,\n        custom_tokenizer: PreTrainedTokenizer = None,\n        hidden: int = -2,\n        reduce_option: str = \'mean\',\n        sentence_handler: SentenceHandler = SentenceHandler(),\n        random_state: int = 12345\n    ):\n        """"""\n        This is the parent Bert Summarizer model. New methods should implement this class\n\n        :param model: This parameter is associated with the inherit string parameters from the transformers library.\n        :param custom_model: If you have a pre-trained model, you can add the model class here.\n        :param custom_tokenizer: If you have a custom tokenizer, you can add the tokenizer here.\n        :param hidden: This signifies which layer of the BERT model you would like to use as embeddings.\n        :param reduce_option: Given the output of the bert model, this param determines how you want to reduce results.\n        :param sentence_handler: The handler to process sentences. If want to use coreference, instantiate and pass CoreferenceHandler instance\n        :param random_state: The random state to reproduce summarizations.\n        """"""\n\n        np.random.seed(random_state)\n        self.model = BertParent(model, custom_model, custom_tokenizer)\n        self.hidden = hidden\n        self.reduce_option = reduce_option\n        self.sentence_handler = sentence_handler\n        self.random_state = random_state\n\n    def process_content_sentences(self, body: str, min_length:int = 40, max_length: int = 600) -> List[str]:\n        """"""\n        Processes the content sentences with neural coreference.\n        :param body: The raw string body to process\n        :param min_length: Minimum length that the sentences must be\n        :param max_length: Max length that the sentences mus fall under\n        :return: Returns a list of sentences with coreference applied.\n        """"""\n\n        doc = self.nlp(body)._.coref_resolved\n        doc = self.nlp(doc)\n        return [c.string.strip() for c in doc.sents if max_length > len(c.string.strip()) > min_length]\n\n    @abstractmethod\n    def run_clusters(\n        self,\n        content: List[str],\n        ratio:float = 0.2,\n        algorithm: str = \'kmeans\',\n        use_first: bool = True\n    ) -> List[str]:\n        """"""\n        Classes must implement this to run the clusters.\n        """"""\n        raise NotImplementedError(""Must Implement run_clusters"")\n\n    def run(\n        self,\n        body: str,\n        ratio: float = 0.2,\n        min_length: int = 40,\n        max_length: int = 600,\n        use_first: bool = True,\n        algorithm: str =\'kmeans\'\n    ) -> str:\n        """"""\n        Preprocesses the sentences, runs the clusters to find the centroids, then combines the sentences.\n\n        :param body: The raw string body to process\n        :param ratio: Ratio of sentences to use\n        :param min_length: Minimum length of sentence candidates to utilize for the summary.\n        :param max_length: Maximum length of sentence candidates to utilize for the summary\n        :param use_first: Whether or not to use the first sentence\n        :param algorithm: Which clustering algorithm to use. (kmeans, gmm)\n        :return: A summary sentence\n        """"""\n        sentences = self.sentence_handler(body, min_length, max_length)\n\n        if sentences:\n            sentences = self.run_clusters(sentences, ratio, algorithm, use_first)\n\n        return \' \'.join(sentences)\n\n    def __call__(\n        self,\n        body: str,\n        ratio: float = 0.2,\n        min_length: int = 40,\n        max_length: int = 600,\n        use_first: bool = True,\n        algorithm: str = \'kmeans\'\n    ) -> str:\n        """"""\n        (utility that wraps around the run function)\n\n        Preprocesses the sentences, runs the clusters to find the centroids, then combines the sentences.\n\n        :param body: The raw string body to process\n        :param ratio: Ratio of sentences to use\n        :param min_length: Minimum length of sentence candidates to utilize for the summary.\n        :param max_length: Maximum length of sentence candidates to utilize for the summary\n        :param use_first: Whether or not to use the first sentence\n        :param algorithm: Which clustering algorithm to use. (kmeans, gmm)\n        :return: A summary sentence\n        """"""\n        return self.run(body, ratio, min_length, max_length, algorithm=algorithm, use_first=use_first)\n\n\nclass SingleModel(ModelProcessor):\n    """"""\n    Deprecated for naming sake.\n    """"""\n\n    def __init__(\n        self,\n        model=\'bert-large-uncased\',\n        custom_model: PreTrainedModel = None,\n        custom_tokenizer: PreTrainedTokenizer = None,\n        hidden: int=-2,\n        reduce_option: str = \'mean\',\n        sentence_handler: SentenceHandler = SentenceHandler(),\n        random_state: int=12345\n    ):\n        super(SingleModel, self).__init__(\n            model=model, custom_model=custom_model, custom_tokenizer=custom_tokenizer,\n            hidden=hidden, reduce_option=reduce_option,\n            sentence_handler=sentence_handler, random_state=random_state\n        )\n\n    def run_clusters(self, content: List[str], ratio=0.2, algorithm=\'kmeans\', use_first: bool= True) -> List[str]:\n        hidden = self.model(content, self.hidden, self.reduce_option)\n        hidden_args = ClusterFeatures(hidden, algorithm, random_state=self.random_state).cluster(ratio)\n\n        if use_first:\n            if hidden_args[0] != 0:\n                hidden_args.insert(0,0)\n\n        return [content[j] for j in hidden_args]\n\n\nclass Summarizer(SingleModel):\n\n    def __init__(\n        self,\n        model: str = \'bert-large-uncased\',\n        custom_model: PreTrainedModel = None,\n        custom_tokenizer: PreTrainedTokenizer = None,\n        hidden: int = -2,\n        reduce_option: str = \'mean\',\n        sentence_handler: SentenceHandler = SentenceHandler(),\n        random_state: int = 12345\n    ):\n        """"""\n        This is the main Bert Summarizer class.\n\n        :param model: This parameter is associated with the inherit string parameters from the transformers library.\n        :param custom_model: If you have a pre-trained model, you can add the model class here.\n        :param custom_tokenizer: If you have a custom tokenizer, you can add the tokenizer here.\n        :param hidden: This signifies which layer of the BERT model you would like to use as embeddings.\n        :param reduce_option: Given the output of the bert model, this param determines how you want to reduce results.\n        :param greedyness: associated with the neuralcoref library. Determines how greedy coref should be.\n        :param language: Which language to use for training.\n        :param random_state: The random state to reproduce summarizations.\n        """"""\n        super(Summarizer, self).__init__(\n            model, custom_model, custom_tokenizer, hidden, reduce_option, sentence_handler, random_state\n        )\n\n\nclass TransformerSummarizer(SingleModel):\n\n    MODEL_DICT = {\n        \'Bert\': (BertModel, BertTokenizer),\n        \'OpenAIGPT\': (OpenAIGPTModel, OpenAIGPTTokenizer),\n        \'GPT2\': (GPT2Model, GPT2Tokenizer),\n        \'CTRL\': (CTRLModel, CTRLTokenizer),\n        \'TransfoXL\': (TransfoXLModel, TransfoXLTokenizer),\n        \'XLNet\': (XLNetModel, XLNetTokenizer),\n        \'XLM\': (XLMModel, XLMTokenizer),\n        \'DistilBert\': (DistilBertModel, DistilBertTokenizer),\n    }\n\n    def __init__(\n        self,\n        transformer_type: str = \'Bert\',\n        transformer_model_key: str = \'bert-base-uncased\',\n        transformer_tokenizer_key: str = None,\n        hidden: int = -2,\n        reduce_option: str = \'mean\',\n        sentence_handler: SentenceHandler = SentenceHandler(),\n        random_state: int = 12345\n    ):\n\n        try:\n            self.MODEL_DICT[\'Roberta\'] = (RobertaModel, RobertaTokenizer)\n            self.MODEL_DICT[\'Albert\'] = (AlbertModel, AlbertTokenizer)\n            self.MODEL_DICT[\'Camembert\'] = (CamembertModel, CamembertTokenizer)\n        except Exception as e:\n            pass # older transformer version\n\n        model_clz, tokenizer_clz = self.MODEL_DICT[transformer_type]\n        model = model_clz.from_pretrained(transformer_model_key, output_hidden_states=True)\n\n        tokenizer = tokenizer_clz.from_pretrained(\n            transformer_tokenizer_key if transformer_tokenizer_key is not None else transformer_model_key\n        )\n\n        super().__init__(\n            None, model, tokenizer, hidden, reduce_option, sentence_handler, random_state\n        )\n'"
summarizer/sentence_handler.py,0,"b'from spacy.lang.en import English\nfrom typing import List\n\n\nclass SentenceHandler(object):\n\n    def __init__(self, language=English):\n        self.nlp = language()\n        self.nlp.add_pipe(self.nlp.create_pipe(\'sentencizer\'))\n\n    def process(self, body: str, min_length: int = 40, max_length: int = 600) -> List[str]:\n        """"""\n        Processes the content sentences.\n\n        :param body: The raw string body to process\n        :param min_length: Minimum length that the sentences must be\n        :param max_length: Max length that the sentences mus fall under\n        :return: Returns a list of sentences.\n        """"""\n        doc = self.nlp(body)\n        return [c.string.strip() for c in doc.sents if max_length > len(c.string.strip()) > min_length]\n\n    def __call__(self, body: str, min_length: int = 40, max_length: int = 600) -> List[str]:\n        return self.process(body, min_length, max_length)\n'"
tests/__init__.py,0,b''
tests/test_summary_items.py,0,"b""import pytest\nfrom summarizer import Summarizer, TransformerSummarizer\nfrom summarizer.coreference_handler import CoreferenceHandler\nfrom transformers import AlbertTokenizer, AlbertModel\n\n\n@pytest.fixture()\ndef custom_summarizer():\n    albert_model = AlbertModel.from_pretrained('albert-base-v1', output_hidden_states=True)\n    albert_tokenizer = AlbertTokenizer.from_pretrained('albert-base-v1')\n    return Summarizer(custom_model=albert_model, custom_tokenizer=albert_tokenizer)\n\n\n@pytest.fixture()\ndef albert_transformer():\n    return TransformerSummarizer('Albert', 'albert-base-v1')\n\n\n@pytest.fixture()\ndef summarizer():\n    return Summarizer('distilbert-base-uncased')\n\n\n@pytest.fixture()\ndef coreference_handler():\n    return CoreferenceHandler()\n\n\n@pytest.fixture()\ndef passage():\n    return '''\n    The Chrysler Building, the famous art deco New York skyscraper, will be sold for a small fraction of its previous sales price.\n    The deal, first reported by The Real Deal, was for $150 million, according to a source familiar with the deal.\n    Mubadala, an Abu Dhabi investment fund, purchased 90% of the building for $800 million in 2008.\n    Real estate firm Tishman Speyer had owned the other 10%.\n    The buyer is RFR Holding, a New York real estate company.\n    Officials with Tishman and RFR did not immediately respond to a request for comments.\n    It's unclear when the deal will close.\n    The building sold fairly quickly after being publicly placed on the market only two months ago.\n    The sale was handled by CBRE Group.\n    The incentive to sell the building at such a huge loss was due to the soaring rent the owners pay to Cooper Union, a New York college, for the land under the building.\n    The rent is rising from $7.75 million last year to $32.5 million this year to $41 million in 2028.\n    Meantime, rents in the building itself are not rising nearly that fast.\n    While the building is an iconic landmark in the New York skyline, it is competing against newer office towers with large floor-to-ceiling windows and all the modern amenities.\n    Still the building is among the best known in the city, even to people who have never been to New York.\n    It is famous for its triangle-shaped, vaulted windows worked into the stylized crown, along with its distinctive eagle gargoyles near the top.\n    It has been featured prominently in many films, including Men in Black 3, Spider-Man, Armageddon, Two Weeks Notice and Independence Day.\n    The previous sale took place just before the 2008 financial meltdown led to a plunge in real estate prices.\n    Still there have been a number of high profile skyscrapers purchased for top dollar in recent years, including the Waldorf Astoria hotel, which Chinese firm Anbang Insurance purchased in 2016 for nearly $2 billion, and the Willis Tower in Chicago, which was formerly known as Sears Tower, once the world's tallest.\n    Blackstone Group (BX) bought it for $1.3 billion 2015.\n    The Chrysler Building was the headquarters of the American automaker until 1953, but it was named for and owned by Chrysler chief Walter Chrysler, not the company itself.\n    Walter Chrysler had set out to build the tallest building in the world, a competition at that time with another Manhattan skyscraper under construction at 40 Wall Street at the south end of Manhattan. He kept secret the plans for the spire that would grace the top of the building, building it inside the structure and out of view of the public until 40 Wall Street was complete.\n    Once the competitor could rise no higher, the spire of the Chrysler building was raised into view, giving it the title.\n    '''\n\n\ndef test_summary_creation(summarizer, passage):\n    res = summarizer(passage, ratio=0.15, min_length=25, max_length=500)\n    assert len(res) > 10\n\ndef test_summary_larger_ratio(summarizer, passage):\n    res = summarizer(passage, ratio=0.5)\n    assert len(res) > 10\n\n\ndef test_cluster_algorithm(summarizer, passage):\n    res = summarizer(passage, algorithm='gmm')\n    assert len(res) > 10\n\n\ndef test_do_not_use_first(summarizer, passage):\n    res = summarizer(passage, ratio=0.1, use_first=False)\n    assert res is not None\n\n\ndef test_albert(custom_summarizer, passage):\n    res = custom_summarizer(passage)\n    assert len(res) > 10\n\n\ndef test_transformer_clz(albert_transformer, passage):\n    res = albert_transformer(passage)\n    assert len(res) > 10\n\n\ndef test_coreference_handler(coreference_handler):\n    orig = '''My sister has a dog. She loves him.'''\n    resolved = '''My sister has a dog. My sister loves a dog.'''\n    result = coreference_handler.process(orig, min_length=2)\n    assert ' '.join(result) == resolved\n"""
