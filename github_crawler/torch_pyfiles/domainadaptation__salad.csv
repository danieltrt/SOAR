file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\nimport setuptools\nfrom setuptools import setup\n\nwith open(""README.rst"", ""r"", encoding = ""utf8"") as fh:\n    long_description = fh.read()\n\nsetup(name=\'torch-salad\',\n      version=\'0.2.1-alpha\',\n      description=\'Semi-supervised Adaptive Learning Across Domains\',\n      long_description=long_description,\n      long_description_content_type=""text/x-rst"",\n      url=\'https://domainadaptation.org\',\n      author=\'Steffen Schneider\',\n      author_email=\'steffen.schneider@tum.de\',\n      packages=setuptools.find_packages(),\n      classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)"",\n        ""Operating System :: OS Independent"",\n        ""Development Status :: 2 - Pre-Alpha"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence""\n      ],\n)\n'"
_sphinx/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'.\'))\nsys.path.insert(0, os.path.abspath(\'../\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'salad\'\ncopyright = \'2018, Steffen Schneider\'\nauthor = \'Steffen Schneider\'\n\n# The short X.Y version\nversion = \'0.2\'\n# The full version, including alpha/beta/rc tags\nrelease = \'0.2.0-alpha\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    #\'sphinx.ext.viewcode\',\n    \'sphinx.ext.napoleon\',\n    \'m2r\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = [\'.rst\', \'.md\']\n#source_suffix = \'.rst\'\n\n#source_parsers = {\n#    \'.md\': \'recommonmark.parser.CommonMarkParser\'\n#}\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'_release\', \'Thumbs.db\', \'.DS_Store\', \'readinglist\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'sticky_navigation\' : True,\n    \'navigation_depth\' : 4\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\nhtml_sidebars = { \'**\': [\'globaltoc.html\', \'relations.html\', \'sourcelink.html\', \'searchbox.html\'] }\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'saladdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'salad.tex\', \'salad Documentation\',\n     \'Steffen Schneider\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'salad\', \'salad Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'salad\', \'salad Documentation\',\n     author, \'salad\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Extension configuration -------------------------------------------------\n'"
salad/__init__.py,0,"b'__doc__ = u"""""" Test\nsalad is a toolbox for semi-supervised adaptive learning across domains.\nIt is mainly meant for domain adaptation, semi-supervised learning and transfer learning in general,\nusing PyTorch as a backend.\n\n$$\n\\mathbb{E}_{p_t} [ \\mathcal{L}(x_t, y_t) ]\n$$\n""""""\n\nname = ""salad""\n\n__import__(\'pkg_resources\').declare_namespace(__name__)'"
salad/optim.py,2,"b'from torch.optim import Optimizer\n\nclass JointOptimizer(object):\n\n    """""" Concat multiple optimizers\n\n\n\n    Parameters\n    ----------\n    *optims: list of ``torch.optim.Optimizer``\n        Optimizers. The ``step`` and ``zero_grad`` functions will be executed in\n        the same order.\n\n    """"""\n    \n    def __init__(self, *optims):        \n        self.optims = optims\n        \n    def zero_grad(self):\n        for optim in self.optims:\n            optim.zero_grad()\n    \n    def step(self):\n        for optim in self.optims:\n            optim.step()\n\nclass _DelayedWeight (object):\n    def __init__(self, params, src_params):\n\n        self.params = list(params)\n        self.src_params = list(src_params)\n\n        for p, src_p in zip(self.params, self.src_params):\n            p.data[:] = src_p.data[:]\n\n    def step(self):\n        for p, src_p in zip(self.params, self.src_params):\n            p.data.set_(src_p.data)\n\n    def zero_grad(self):\n        pass\n\nclass WeightEMA (object):\n    """""" Exponential moving average weight optimizer for mean teacher model\n\n    Used for Self-Ensembling, code adapted from [1]_.\n\n    See Also\n    --------\n\n    ``salad.solver.SelfEnsemblingSolver``\n\n    Reference\n    ---------\n\n    .. [1] https://github.com/Britefury/self-ensemble-visual-domain-adapt\n    """"""\n    def __init__(self, params, src_params, alpha=0.999):\n\n        self.params = list(params)\n        self.src_params = list(src_params)\n        self.alpha = alpha\n\n        for p, src_p in zip(self.params, self.src_params):\n            p.data[:] = src_p.data[:]\n\n    def step(self):\n        one_minus_alpha = 1.0 - self.alpha\n        for p, src_p in zip(self.params, self.src_params):\n            p.data.mul_(self.alpha)\n            p.data.add_(src_p.data * one_minus_alpha)\n\n    def zero_grad(self):\n        pass\n'"
salad/structural.py,3,"b'"""""" Helper functions for structural learning\n""""""\n\nfrom torch import nn\nimport torch\nfrom torchvision.models import resnet\n\n## General Helper Functions\n\ndef bn2linear(bn):\n    scale, shift = get_affine(bn)\n    \n    W_ = scale.view(-1,1,1,1)\n    b_ = shift\n    \n    n_channels = W_.size()[0]\n    \n    conv = nn.Conv2d(n_channels,n_channels,kernel_size=1,groups=n_channels)\n    with torch.no_grad():\n        conv.weight.set_(W_.float())\n        conv.bias.set_(b_.float())\n    return conv\n\ndef replace_bns(module):\n    for name, layer in module._modules.items(): \n        if isinstance(layer, nn.BatchNorm2d):\n            layer_ = bn2linear(layer)\n            module._modules[name] = layer_\n\ndef reinit_bns(module):\n    """""" \n    """"""\n\n    for name, layer in module._modules.items(): \n        if isinstance(layer, nn.BatchNorm2d):\n            with torch.no_grad():\n                scale, shift = get_affine(layer)\n\n                layer_ = nn.BatchNorm2d(layer.num_features, eps=layer.eps, momentum=layer.momentum, affine=True, track_running_stats=True)\n\n                layer_.weight.set_(scale.float())\n                layer_.bias.set_(shift.float())\n\n                module._modules[name] = layer_\n            \ndef get_affine(layer):\n    mu  = layer.running_mean.double()\n    var = layer.running_var.double()\n    W   = layer.weight.double()\n    b   = layer.bias.double()\n    eps = layer.eps\n    \n    inv_std = 1./(var + eps)**.5\n    \n    scale = inv_std * W\n    shift = -mu * scale + b\n    \n    return scale, shift\n\ndef convert_conv_bn(layer, bn):\n    W = layer.weight.double()\n\n    scale, shift = get_affine(bn)\n    out, inp,_,_ = W.size()\n    W_ = (W * scale.view(-1,1,1,1))\n    b_ = scale\n\n    layer_ = nn.Conv2d(inp, out, kernel_size=layer.kernel_size, stride=layer.stride, padding = layer.padding)\n    with torch.no_grad():\n        layer_.weight.set_(W_.float())\n        layer_.bias.set_(b_.float())\n\n    return layer_\n\n## Models\n\nclass FixedBottleneck(nn.Module):\n    \n    def __init__(self, conv, downsample):\n        \n        super().__init__()\n        \n        self.conv       = conv\n        self.downsample = downsample \n        self.relu       = nn.ReLU()\n        \n    def forward(self, x):\n        \n        if self.downsample:\n            return self.relu(self.conv(x) + self.downsample(x))\n        else:\n            return self.relu(self.conv(x) + x)\n\ndef FixedResnet(backbone):\n\n    """""" ResNet Variant where each batch norm layer is replaced by a linear transformation\n    """"""\n\n    backbone.double()\n    backbone.apply(replace_bns)   \n    return backbone\n\nclass CompressedResnet(nn.Module):\n\n    """""" ResNet Variant where the batch norm statistics are merged into the transformation\n    matrices\n    """"""\n    \n    def __init__(self, backbone):\n        \n        super().__init__()\n        \n        self.preprocessing = nn.Sequential(\n            convert_conv_bn(backbone.conv1, backbone.bn1),\n            nn.ReLU(),\n            backbone.maxpool\n        )\n        \n        self.layer1 = self._convert_layer(backbone.layer1)\n        self.layer2 = self._convert_layer(backbone.layer2)\n        self.layer3 = self._convert_layer(backbone.layer3)\n        self.layer4 = self._convert_layer(backbone.layer4)\n        \n        self.avgpool = backbone.avgpool\n        self.fc = backbone.fc\n    \n    def forward(self, x):\n        \n        x = self.preprocessing(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        \n        return x\n    \n    def _convert_layer(self, layer):\n        \n        return nn.Sequential(\n            *list(self._convert_bottleneck(b) for b in layer.children())\n        )\n        \n    def _convert_bottleneck(self, layer):\n        conv1 = convert_conv_bn(layer.conv1, layer.bn1)\n        conv2 = convert_conv_bn(layer.conv2, layer.bn2)\n        conv3 = convert_conv_bn(layer.conv3, layer.bn3)\n        down  = None\n        \n        if (\'downsample\' in list(i[0] for i in layer.named_children())):\n            down  = convert_conv_bn(layer.downsample[0], layer.downsample[1])\n        \n        convs = nn.Sequential(\n            conv1,\n            nn.ReLU(),\n            conv2,\n            nn.ReLU(),\n            conv3\n        )\n        \n        bottleneck = FixedBottleneck(convs, down)\n        \n        return bottleneck\n'"
scripts/train_digits.py,1,"b'# !/bin/env python3\n# encoding: utf-8\n\n"""""" Comparison Study of Recent Unsupervised Domain Adaptation Approaches on Digit Benchmark\n""""""\n\nimport torch\nfrom torch import nn\nimport numpy as np\n\nfrom salad.datasets.da import toy\nfrom salad.utils import config\nfrom salad.datasets.da import DigitsLoader\nfrom salad.models.digits import DigitModel\n\nfrom salad.datasets.transforms import Augmentation\n\n\nclass ComparisonConfig(config.DomainAdaptConfig):\n    """""" Configuration for the comparison study\n    """"""\n\n    _algorithms = [\'adv\', \'vada\', \'dann\', \'assoc\', \'coral\', \'teach\']\n    _algo_names = [\n        ""Adversarial Domain Regularization"",\n        ""Virtual Adversarial Domain Adaptation"",\n        ""Domain Adversarial Training"",\n        ""Associative Domain Adaptation"",\n        ""Deep Correlation Alignment"",\n        ""Self-Ensembling""\n    ]\n\n    def _init(self):\n        super()._init()\n        self.add_argument(\'--seed\', default=None, type=int, help=""Random Seed"")\n        self.add_argument(\'--print\', action=\'store_true\')\n        self.add_argument(\'--null\', action=\'store_true\')\n\n        for arg, name in zip(self._algorithms, self._algo_names):\n            self.add_argument(\'--{}\'.format(arg), action=\'store_true\', help=""Train a model with {}"".format(name))\n\ndef print_experiments():\n    import itertools\n\n    datasets = [\'mnist\', \'synth\', \'svhn\']\n    algos = ComparisonConfig._algorithms\n\n    s = []\n\n    for algo, source, target in itertools.product(algos, datasets, datasets):\n\n        if source == target:\n            continue\n\n        print(\'python3 {} --{} --source {} --target {} {}\'.format(\n            __file__,\n            algo,\n            source,\n            target,\n            \'--epochs 10\'\n        ))\n\ndef experiment_setup(args):\n    """""" Set default params and construct models for various experiments\n    """"""\n\n    model = DigitModel()\n    teacher = DigitModel()\n    disc = nn.Linear(128, 1)\n\n    kwargs = {\n        \'n_epochs\': args.epochs,\n        \'multiclass\': True,\n        \'learningrate\': 3e-4,\n        \'gpu\': None if args.cpu else args.gpu,\n        \'savedir\': \'log/new14-{}-{}\'.format(args.source, args.target),\n        \'dryrun\': args.dryrun\n    }\n\n    return model, teacher, disc, kwargs\n\n\nif __name__ == \'__main__\':\n\n    from torch.utils.data import TensorDataset, DataLoader\n    from salad.datasets import DigitsLoader\n    from salad import solver\n    import sys\n\n    parser = ComparisonConfig(\'Domain Adaptation Comparision and Reproduction Study\')\n    args = parser.parse_args()\n\n    if args.print:\n        print_experiments()\n        sys.exit(0)\n\n    parser.print_config()\n\n    dataset_names = [args.source, args.target]\n\n    loader_plain   = DigitsLoader(\'/tmp/data\', dataset_names, download=True, shuffle=True, batch_size=128, normalize=True, num_workers=4)\n    loader_augment = DigitsLoader(\'/tmp/data\', dataset_names, download=True, shuffle=True, batch_size=128, num_workers=4,\n                                  normalize=True, augment={args.target: 2})\n\n    if args.adv:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.AdversarialDropoutSolver(model, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.vada:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.VADASolver(model, disc, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.dann:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.DANNSolver(model, disc, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.assoc:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.AssociativeSolver(model, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.coral:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.DeepCoralSolver(model, loader_plain, use_nullspace = args.null, **kwargs)\n        experiment.optimize()\n\n    if args.teach:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.SelfEnsemblingSolver(model, teacher, loader_augment, **kwargs)\n        experiment.optimize()\n'"
scripts/train_digits_tests.py,1,"b'# !/bin/env python3\n# encoding: utf-8\n\n"""""" Comparison Study of Recent Unsupervised Domain Adaptation Approaches on Digit Benchmark\n""""""\n\nimport torch\nfrom torch import nn\nimport numpy as np\n\nfrom salad.datasets.da import toy\nfrom salad.utils import config\nfrom salad.datasets.da import DigitsLoader\nfrom salad.models.digits import DigitModel\n\nfrom salad.datasets.transforms import Augmentation\n\n\nclass ComparisonConfig(config.DomainAdaptConfig):\n    """""" Configuration for the comparison study\n    """"""\n\n    _algorithms = [\'adv\', \'vada\', \'dann\', \'assoc\', \'coral\', \'teach\']\n\n    def _init(self):\n        super()._init()\n        self.add_argument(\'--seed\',  default=None, type=int, help=""Random Seed"")\n        self.add_argument(\'--print\', action=\'store_true\')\n        self.add_argument(\'--null\',  action=\'store_true\')\n        self.add_argument(\'--dropout\',  action=\'store_true\')\n\n        for arg in self._algorithms:\n            self.add_argument(\'--{}\'.format(arg), action=\'store_true\', help=""Enable {}"".format(arg))\n\ndef print_experiments():\n    import itertools\n\n    datasets = [\'mnist\', \'synth\', \'svhn\']\n    algos = ComparisonConfig._algorithms\n\n    s = []\n\n    for algo, source, target in itertools.product(algos, datasets, datasets):\n\n        if source == target:\n            continue\n\n        print(\'python3 {} --{} --source {} --target {} {}\'.format(\n            __file__,\n            algo,\n            source,\n            target,\n            \'--epochs 10\'\n        ))\n\ndef experiment_setup(args):\n    """""" Set default params and construct models for various experiments\n    """"""\n\n    model = DigitModel(noisy=args.dropout)\n    teacher = DigitModel(noisy=args.dropout)\n    disc = nn.Linear(128, 1)\n\n    kwargs = {\n        \'n_epochs\': args.epochs,\n        \'multiclass\': True,\n        \'learningrate\': 3e-4,\n        \'gpu\': None if args.cpu else args.gpu,\n        \'savedir\': \'log/nullspace-{}-{}\'.format(args.source, args.target),\n        \'dryrun\': args.dryrun\n    }\n\n    return model, teacher, disc, kwargs\n\nclass Duplicate():\n\n    def __init__(self, dataset, n_samples=1):\n        self.dataset = dataset\n        self.n_samples = n_samples\n\n    def __len__(self):\n\n        return len(self.dataset)\n\n    def __getitem__(self, index):\n\n        x, y = self.dataset[index]\n\n        return [x.clone() for _ in range(self.n_samples)] + [y,]\n\n\nif __name__ == \'__main__\':\n\n    from torch.utils.data import TensorDataset, DataLoader\n    from salad.datasets import DigitsLoader\n    from salad import solver\n    import sys\n\n    parser = ComparisonConfig(\'Domain Adapt Comparison\')\n    args = parser.parse_args()\n\n    if args.print:\n        print_experiments()\n        sys.exit(0)\n\n    parser.print_config()\n\n    dataset_names = [args.source, args.target]\n\n    loader_plain   = DigitsLoader(\'/tmp/data\', dataset_names, download=True, shuffle=True, batch_size=128, normalize=True, num_workers=4)\n    loader_augment = DigitsLoader(\'/tmp/data\', dataset_names, download=True, shuffle=True, batch_size=128, num_workers=4,\n                                  normalize=True, augment={args.target: 2}, augment_func = Duplicate)\n\n    if args.adv:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.AdversarialDropoutSolver(model, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.vada:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.VADASolver(model, disc, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.dann:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.DANNSolver(model, disc, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.assoc:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.AssociativeSolver(model, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.coral:\n        print(args.null)\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.DeepCoralSolver(model, loader_plain, use_nullspace = args.null, **kwargs)\n        experiment.optimize()\n\n    if args.teach:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.SelfEnsemblingSolver(model, teacher, loader_augment, **kwargs)\n        experiment.optimize()\n'"
scripts/train_noise.py,1,"b'# !/bin/env python3\n# encoding: utf-8\n\n"""""" Comparison Study of Recent Unsupervised Domain Adaptation Approaches on Digit Benchmark\n""""""\n\nimport torch\nfrom torch import nn\nimport numpy as np\n\nfrom salad.datasets.da import toy\nfrom salad.utils import config\nfrom salad.datasets.da import NoiseLoader\nfrom salad.datasets.transforms import SaltAndPepper\nfrom salad.models.digits import DigitModel\n\nfrom salad.datasets.transforms import Augmentation\n\nclass ComparisonConfig(config.DomainAdaptConfig):\n    """""" Configuration for the comparison study\n    """"""\n\n    _algorithms = [\'adv\', \'vada\', \'dann\', \'assoc\', \'coral\', \'teach\']\n\n    def _init(self):\n        super()._init()\n        self.add_argument(\'--seed\', default=None, type=int, help=""Random Seed"")\n        self.add_argument(\'--print\', action=\'store_true\')\n\n        for arg in self._algorithms:\n            self.add_argument(\'--{}\'.format(arg), action=\'store_true\', help=""Enable {}"".format(arg))\n\ndef print_experiments():\n    import itertools\n\n    datasets = [\'mnist\', \'synth\', \'svhn\']\n    algos = ComparisonConfig._algorithms\n\n    s = []\n\n    for algo, source, target in itertools.product(algos, datasets, datasets):\n\n        if source == target:\n            continue\n\n        print(\'python3 {} --{} --source {} --target {} {}\'.format(\n            __file__,\n            algo,\n            source,\n            target,\n            \'--epochs 10\'\n        ))\n\ndef experiment_setup(args):\n    """""" Set default params and construct models for various experiments\n    """"""\n\n    model = DigitModel()\n    teacher = DigitModel()\n    disc = nn.Linear(128, 1)\n\n    kwargs = {\n        \'n_epochs\': args.epochs,\n        \'multiclass\': True,\n        \'learningrate\': 3e-4,\n        \'gpu\': None if args.cpu else args.gpu,\n        \'savedir\': \'log/noise-{}-{}\'.format(args.source, args.target),\n        \'dryrun\': args.dryrun\n    }\n\n    return model, teacher, disc, kwargs\n\n\nif __name__ == \'__main__\':\n\n    from torch.utils.data import TensorDataset, DataLoader\n    from salad.datasets import DigitsLoader\n    from salad import solver\n    import sys\n\n    parser = ComparisonConfig(\'Domain Adapt Comparison\')\n    args = parser.parse_args()\n\n    if args.print:\n        print_experiments()\n        sys.exit(0)\n\n    parser.print_config()\n\n    dataset_names = [args.source, args.target]\n\n    loader_plain   = NoiseLoader(\'/tmp/data\', args.source, collate = \'stack\', noisemodels=[lambda x : x, SaltAndPepper(0.15)],batch_size = 32, shuffle = True, normalize=False)\n    loader_augment = NoiseLoader(\'/tmp/data\', args.source, collate = \'stack\', noisemodels=[lambda x : x, SaltAndPepper(0.15)],batch_size = 32,\n                                 shuffle = True, normalize=False,augment={1: 2})\n\n    if args.adv:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.AdversarialDropoutSolver(model, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.vada:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.VADASolver(model, disc, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.dann:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.DANNSolver(model, disc, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.assoc:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.AssociativeSolver(model, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.coral:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.DeepCoralSolver(model, loader_plain, **kwargs)\n        experiment.optimize()\n\n    if args.teach:\n        model, teacher, disc, kwargs = experiment_setup(args)\n        experiment = solver.SelfEnsemblingSolver(model, teacher, loader_augment, **kwargs)\n        experiment.optimize()\n'"
tests/test_datasets.py,1,"b""from salad.datasets import MNIST, USPS, SVHN, Synth, SynthSmall\nimport torch\nfrom torchvision import transforms\n\ndef test_digits():\n\n    T = transforms.ToTensor()\n\n    data = [\n        MNIST('/tmp/data', transform = T, download=True),\n        SVHN('/tmp/data',  transform = T, download=True),\n        SynthSmall('/tmp/data',  transform = T, download=True),\n        USPS('/tmp/data',  transform = T, download=True),\n    ]\n\n    for ds in data:\n\n        for x, y in ds:\n            assert isinstance(y, int) or (isinstance(y, torch.Tensor) and y.dim() == 0), (y, type(y))\n            assert x.dim() >= 3\n            break\n"""
tests/test_layers.py,3,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom salad.layers.da import AdaIN\n\n\ndef test_adain():\n\n    from_std, to_std = 5, 3\n    from_mean, to_mean = 1, 2.5\n\n    x = torch.randn(10,3,32,32) * from_std + from_mean\n    y = torch.randn(10,3,32,32) * to_std + to_mean\n\n    layer = AdaIN(3)\n\n    x_ = layer(x, y)\n\n    x_.mean(), x_.std(), y.mean(), y.std()'"
tests/test_models.py,0,"b'from salad.models import DigitModel\n\ndef test_conditional_model():\n\n    model = DigitModel(n_domains=2)\n\n    all_params_0    = set(model.parameters(0, yield_shared=True, yield_conditional=True))\n    all_params_1    = set(model.parameters(1, yield_shared=True, yield_conditional=True))\n\n    shared        = set(model.parameters(0, yield_shared=True,  yield_conditional=False))\n    conditional_0 = set(model.parameters(0, yield_shared=False, yield_conditional=True))\n    conditional_1 = set(model.parameters(1, yield_shared=False, yield_conditional=True))\n\n    print (len(conditional_0.union(conditional_1).union(shared)), len(all_params_0.union(all_params_1)))\n\n    assert conditional_0.union(conditional_1).union(shared) == all_params_0.union(all_params_1)\n    assert not (conditional_0.intersection(conditional_1))\n    assert all_params_0.union(conditional_1) == all_params_1.union(conditional_0)\n    assert all_params_0.intersection(all_params_1) == shared'"
tests/test_solver.py,0,b''
examples/association/train_association.py,3,"b'"""""" Minimal Training Script for Associative Domain Adaptation\n""""""\n\nimport sys\nimport os\nimport os.path as osp\n\nimport torch\nfrom torch import nn\n\nfrom salad import solver, datasets\nimport salad.models.digits.assoc as models\n\nfrom salad.utils import config \n\nclass AssociationConfig(config.DomainAdaptConfig):\n    def _init(self):\n        super()._init()\n        # Associative DA Hyperparams\n        self.add_argument(\'--visit\', default=0.1, type=float, help=""Visit weight"")\n        self.add_argument(\'--walker\', default=1.0, type=float, help=""Walker weight"")\n\nif __name__ == \'__main__\':\n\n    parser = AssociationConfig(\'Associative Domain Adaptation\')\n    args   = parser.parse_args()\n\n    # Network\n    if osp.exists(args.checkpoint):\n        print(""Resume from checkpoint file at {}"".format(args.checkpoint))\n        model = torch.load(args.checkpoint)\n    else:\n        model = models.SVHNmodel()\n\n    # Dataset\n    data = datasets.da.load_dataset2(path=""/tmp/data"", train=True)\n\n    train_loader = torch.utils.data.DataLoader(\n        data[args.source], batch_size=args.sourcebatch,\n        shuffle=True, num_workers=4)\n    val_loader   = torch.utils.data.DataLoader(\n        data[args.target], batch_size=args.targetbatch,\n        shuffle=True, num_workers=4)\n\n    dataset = datasets.JointLoader(train_loader, val_loader)\n\n    # Initialize the solver for this experiment\n\n    experiment = solver.AssociativeSolver(model, dataset,\n                               n_epochs=args.epochs,\n                               savedir=args.log,\n                               dryrun = args.dryrun,\n                               learningrate = args.learningrate,\n                               gpu=args.gpu if not args.cpu else None)\n\n    experiment.optimize()\n'"
examples/compare/train-mnistsvhn.py,5,"b'"""""" Comparison Study of Recent Domain Adaptation Approaches\n""""""\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom salad.datasets.da import toy\n\nclass SmallModel(nn.Module):\n    \n    def __init__(self, track_stats = True):\n        \n        super().__init__()\n        \n        self.features = nn.Sequential(\n            nn.Linear(2, 32),\n            nn.BatchNorm1d(32, track_running_stats = track_stats),\n            nn.ReLU(),\n            nn.Linear(32,64),\n            nn.BatchNorm1d(64, track_running_stats = track_stats),\n            nn.ReLU()\n        )\n        \n        self.classifier = nn.Linear(64, 2)\n        \n        self._weight_init()\n        \n    def parameters(self, d = 0):\n        return super().parameters()\n        \n    def _weight_init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n        \n    def forward(self, x, d = None):\n        \n        z = self.features(x)\n        y = self.classifier(z)\n        \n        return z, y\n\nfrom salad.utils import config \n\nclass ComparisonConfig(config.DomainAdaptConfig):\n\n    def _init(self):\n        super()._init()\n        self.add_argument(\'--seed\', default=None, type=int, help=""Random Seed"")\n        \ndef get_dataloader(Xt, yt, Xv, yv, augment = False, merge=False):\n    def collate(batch):\n\n        X = torch.cat([b[0] for b in batch], dim=0)\n        Y = torch.cat([b[1] for b in batch], dim=0)\n        D = torch.cat([torch.zeros(b[0].size(0)).long() + n for n,b in enumerate(batch)], dim=0)\n\n        return X,Y,D\n    \n    train = DataLoader(TensorDataset(Xt, yt.long()), batch_size= 256, shuffle=True)\n    if augment:\n        val   = DataLoader(TensorDataset(Xv, Xv + 0.01 * torch.randn(Xv.size()), yv.long()),\n                           batch_size= 256, shuffle=True)\n    else:\n        val   = DataLoader(TensorDataset(Xv, yv.long()),\n            batch_size= 256, shuffle=True)\n        \n    joint = JointLoader(train, val, collate_fn = collate if merge else None)\n    \n    return joint\n\ndef experiment_setup(args):\n    model   = SmallModel()\n    teacher = SmallModel()\n    disc    = nn.Linear(64, 1)\n    \n    kwargs = {\n        \'n_epochs\' : args.epochs,\n        \'multiclass\' : True,\n        \'learningrate\' : 3e-4,\n        \'gpu\' : 0,\n        \'savedir\' : args.log\n    }\n    \n    return model, teacher, disc, kwargs\n    \nif __name__ == \'__main__\':\n    from torch.utils.data import TensorDataset, DataLoader\n    from salad.datasets import JointLoader\n    from salad import solver\n\n    parser = ComparisonConfig(\'Domain Adapt Comparison\')\n    args = parser.parse_args()\n\n    Xt, yt, Xv, yv = toy.make_data(seed=args.seed)\n    loader_plain   = get_dataloader(Xt, yt, Xv, yv, augment = False)\n    loader_cross   = get_dataloader(Xt, yt, Xv, yv, augment = False, merge=True)\n    loader_augment = get_dataloader(Xt, yt, Xv, yv, augment = True)\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.CrossGradSolver(model, loader_cross, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.VADASolver(model, disc, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.DANNSolver(model, disc, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.AssociativeSolver(model, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.DeepCoralSolver(model, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.DeepLogCoralSolver(model, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.CorrDistanceSolver(model, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.SelfEnsemblingSolver(model, teacher, loader_augment, **kwargs)\n    experiment.optimize()'"
examples/compare/train_baseline.py,1,"b'"""""" Comparison Study of Recent Unsupervised Domain Adaptation Approaches on Toy Data\n""""""\n\nimport torch\nfrom torch import nn\nimport numpy as np\n\nfrom salad.datasets.da import toy\nfrom salad.utils import config \nfrom salad.datasets.da import ToyDatasetLoader\n\nclass SmallModel(nn.Module):\n    """""" Model for Toy Dataset\n    """"""\n\n    def __init__(self, track_stats = True):\n        \n        super().__init__()\n        \n        self.features = nn.Sequential(\n            nn.Linear(2, 64),\n            nn.BatchNorm1d(64, track_running_stats = track_stats),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(64,64),\n            nn.BatchNorm1d(64, track_running_stats = track_stats),\n            nn.ReLU()\n        )\n        self.classifier = nn.Linear(64, 2)\n        self._weight_init()\n        \n    def parameters(self, d = 0):\n        return super().parameters()\n        \n    def _weight_init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n        \n    def forward(self, x, d = None):\n        \n        z = self.features(x)\n        y = self.classifier(z)\n        \n        return z, y\n\nclass ComparisonConfig(config.DomainAdaptConfig):\n\n    def _init(self):\n        super()._init()\n        self.add_argument(\'--seed\', default=None, type=int, help=""Random Seed"")\n        \ndef experiment_setup(args):\n    model   = SmallModel()\n    teacher = SmallModel()\n    disc    = nn.Linear(64, 1)\n    \n    kwargs = {\n        \'n_epochs\' : args.epochs,\n        \'multiclass\' : True,\n        \'learningrate\' : 3e-4,\n        \'gpu\' : None if args.cpu else args.gpu,\n        \'savedir\' : args.log\n    }\n    \n    return model, teacher, disc, kwargs\n    \nif __name__ == \'__main__\':\n    from torch.utils.data import TensorDataset, DataLoader\n    from salad.datasets import JointLoader\n    from salad import solver\n\n    parser = ComparisonConfig(\'Domain Adapt Comparison\')\n    args = parser.parse_args()\n\n    parser.print_config()\n\n    loader_plain   = ToyDatasetLoader(augment = False, collate=\'stack\', batch_size = 256, seed=1301)\n    loader_cross   = ToyDatasetLoader(augment = False, collate=\'cat\', batch_size = 256, seed=1301)\n    loader_augment = ToyDatasetLoader(augment = True, collate=\'stack\', batch_size = 256, seed=1301)\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.BaselineDASolver(model, loader_plain, **kwargs)\n    experiment.optimize()'"
examples/compare/train_crossdomain.py,2,"b'"""""" Comparison of Cross-Domain Adaptation Approaches\n""""""\n\nimport torch\nfrom torch import nn\nimport numpy as np\n\nfrom salad.datasets.da import toy\nfrom salad.utils import config \nfrom salad.datasets.da import ToyDatasetLoader\nfrom salad.layers import concat\n\nclass MultiDomainModule(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n    def parameters_domain(self):\n        for p in self.feats_domain.parameters():\n            yield p \n        for p in self.domain.parameters():\n            yield p\n\n    def parameters_classifier(self):\n        for p in self.feats_class.parameters():\n            yield p \n        for p in self.classifier.parameters():\n            yield p\n\n    def forward_domain(self, x):\n        zd = self.feats_domain(x)\n        # zd = self.pool(zd).view(zd.size(0), zd.size(1))\n        d  = self.domain(zd)\n        return zd, d\n\nclass MultiDomainModel(MultiDomainModule):\n\n    def __init__(self, n_classes, n_domains, track_stats):\n\n        super().__init__()\n\n        self.track_stats = track_stats\n\n        self.feats_domain = self._features(2)\n        self.feats_class  = self._features(2 + 64)\n\n        self.classifier = self._classifier(n_classes)\n        self.domain     = self._classifier(n_domains)\n\n    def forward(self, x):\n        zd, d = self.forward_domain(x)\n        x_ = torch.cat([x, zd.detach()], dim = 1)\n        zy = self.feats_class(x_)\n        y = self.classifier(zy)\n\n        return d, y\n\n    def _features(self, inp):\n        return nn.Sequential(\n            nn.Linear(inp, 32),\n            nn.BatchNorm1d(32, track_running_stats = self.track_stats),\n            nn.ReLU(),\n            nn.Linear(32,64),\n            nn.BatchNorm1d(64, track_running_stats = self.track_stats),\n            nn.ReLU()\n        )\n\n    def _classifier(self, n_classes):\n        return nn.Linear(64, n_classes)\n\ndef experiment_setup(args):\n    model   = MultiDomainModel(10, 8, True)\n    \n    kwargs = {\n        \'n_epochs\'      : args.epochs,\n        \'multiclass\'    : True,\n        \'learningrate\'  : 3e-4,\n        \'gpu\'           : None if args.cpu else args.gpu,\n        \'savedir\'       : args.log\n    }\n    \n    return model, kwargs\n\nclass ComparisonConfig(config.DomainAdaptConfig):\n\n    def _init(self):\n        super()._init()\n        self.add_argument(\'--seed\', default=None, type=int, help=""Random Seed"")\n\n\nclass NoiseModel(object):\n    \n    def __init__(self, scale, bias):\n        self.scale = scale\n        self.bias = bias\n        \n    def __call__(self, x):\n        return x.dot(self.scale) + self.bias\n\n\ndef get_dataset():\n    """""" A Dataset hard to classify with a traditional ML method, but solveable by CrossGrad\n    """""" \n    shifts = [0, 1, 2, 3]\n    scales = np.array([[[ 1.13792079,  0.31566232],\n            [-0.09395214, -0.91731429]],\n\n        [[ 1.77883215,  0.13303441],\n            [ 0.84975969,  0.50246399]],\n\n        [[ 1.96448436,  0.11892903],\n            [ 0.12573305,  1.79736142]],\n\n        [[-0.10556193, -0.75719817],\n            [ 1.27046679,  1.44430405]]])\n\n    noisemodels = [NoiseModel(M, b*2) for M, b in zip(scales, shifts)]\n    domains = ToyDatasetLoader(n_domains = len(noisemodels), batch_size = 256,\n                                noisemodels=noisemodels, collate=\'cat\')\n\n    return domains\n        \nif __name__ == \'__main__\':\n\n    from torch.utils.data import TensorDataset, DataLoader\n    from salad.datasets import JointLoader\n    from salad import solver\n\n    parser = ComparisonConfig(\'Domain Adapt Comparison\')\n    args = parser.parse_args()\n\n    loader = get_dataset()\n\n    model, kwargs = experiment_setup(args)\n    experiment = solver.CrossGradSolver(model, loader, **kwargs)\n    experiment.optimize()\n\n    ## TODO: implement multi domain solver'"
examples/compare/train_refine.py,0,"b'"""""" Algorithms for unsupervised fine tuning on the target domain\n""""""\n\nif __name__ == \'__main__\':\n    pass'"
examples/compare/train_unsupervised.py,1,"b'"""""" Comparison Study of Recent Unsupervised Domain Adaptation Approaches on Toy Data\n""""""\n\nimport torch\nfrom torch import nn\nimport numpy as np\n\nfrom salad.datasets.da import toy\nfrom salad.utils import config \nfrom salad.datasets.da import ToyDatasetLoader\n\nclass SmallModel(nn.Module):\n    """""" Model for Toy Dataset\n    """"""\n\n    def __init__(self, track_stats = True):\n        \n        super().__init__()\n        \n        self.features = nn.Sequential(\n            nn.Linear(2, 64),\n            nn.BatchNorm1d(64, track_running_stats = track_stats),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            nn.Linear(64,64),\n            nn.BatchNorm1d(64, track_running_stats = track_stats),\n            nn.ReLU()\n        )\n        self.classifier = nn.Linear(64, 2)\n        self._weight_init()\n        \n    def parameters(self, d = 0):\n        return super().parameters()\n        \n    def _weight_init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n        \n    def forward(self, x, d = None):\n        \n        z = self.features(x)\n        y = self.classifier(z)\n        \n        return z, y\n\nclass ComparisonConfig(config.DomainAdaptConfig):\n\n    def _init(self):\n        super()._init()\n        self.add_argument(\'--seed\', default=None, type=int, help=""Random Seed"")\n        \ndef experiment_setup(args):\n    model   = SmallModel()\n    teacher = SmallModel()\n    disc    = nn.Linear(64, 1)\n    \n    kwargs = {\n        \'n_epochs\' : args.epochs,\n        \'multiclass\' : True,\n        \'learningrate\' : 3e-4,\n        \'gpu\' : None if args.cpu else args.gpu,\n        \'savedir\' : args.log\n    }\n    \n    return model, teacher, disc, kwargs\n    \nif __name__ == \'__main__\':\n    from torch.utils.data import TensorDataset, DataLoader\n    from salad.datasets import JointLoader\n    from salad import solver\n\n    parser = ComparisonConfig(\'Domain Adapt Comparison\')\n    args = parser.parse_args()\n\n    parser.print_config()\n\n    loader_plain   = ToyDatasetLoader(augment = False, collate=\'stack\', batch_size = 256, seed=1301)\n    loader_cross   = ToyDatasetLoader(augment = False, collate=\'cat\', batch_size = 256, seed=1301)\n    loader_augment = ToyDatasetLoader(augment = True, collate=\'stack\', batch_size = 256, seed=1301)\n\n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.BaselineDASolver(model, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.AdversarialDropoutSolver(model, loader_plain, **kwargs)\n    experiment.optimize()\n\n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.VADASolver(model, disc, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.DANNSolver(model, disc, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.AssociativeSolver(model, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.DeepCoralSolver(model, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.DeepLogCoralSolver(model, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.CorrDistanceSolver(model, loader_plain, **kwargs)\n    experiment.optimize()\n    \n    model, teacher, disc, kwargs = experiment_setup(args)\n    experiment = solver.SelfEnsemblingSolver(model, teacher, loader_augment, **kwargs)\n    experiment.optimize()'"
examples/correlation/train_adv.py,3,"b'import sys\nimport os\nimport os.path as osp\nsys.path.append(osp.dirname(__file__))\n\nimport torch\nfrom torch import nn\n\nfrom salad import solver, datasets\nfrom salad.utils import config \n\nimport salad.models.digits.adv as models\n\nif __name__ == \'__main__\':\n\n    parser = config.DomainAdaptConfig(\'Adversarial Dropout Regularization\')\n    args   = parser.parse_args()\n\n    # Network\n    if osp.exists(args.checkpoint):\n        print(""Resume from checkpoint file at {}"".format(args.checkpoint))\n        model = torch.load(args.checkpoint)\n    else:\n        model = models.AdvModel()\n\n    # Dataset\n    data = datasets.da.load_dataset2(path=""data"", train=True)\n\n    train_loader = torch.utils.data.DataLoader(\n        data[args.source], batch_size=args.sourcebatch,\n        shuffle=True, num_workers=args.njobs, drop_last = True)\n    val_loader   = torch.utils.data.DataLoader(\n        data[args.target], batch_size=args.targetbatch,\n        shuffle=True, num_workers=args.njobs, drop_last = True)\n\n    dataset = datasets.JointLoader(train_loader, val_loader)\n\n    # Initialize the solver for this experiment\n    experiment = solver.da.AdversarialDropoutSolver(model, dataset,\n                               n_epochs=args.epochs,\n                               savedir=args.log,\n                               dryrun = args.dryrun,\n                               learningrate = args.learningrate,\n                               gpu=args.gpu if not args.cpu else None)\n\n    experiment.optimize()'"
examples/correlation/train_compare.py,3,"b'import sys\nimport os\nimport os.path as osp\nsys.path.append(osp.dirname(__file__))\n\nimport torch\nfrom torch import nn\n\nfrom salad import solver, datasets\nfrom salad.utils import config \n\nimport salad.models.digits.corr as models\n\nfrom salad.layers import coral\n\nclass CorrelationAlignmentConfig(config.DomainAdaptConfig):\n\n    funcs = {\n        \'coral\'    : coral.CoralLoss,\n        \'logcoral\' : coral.LogCoralLoss,\n        \'stein\'    : coral.SteinDivergence,\n        \'jeffrey\'  : coral.JeffreyDivergence,\n        \'affine\'   : coral.AffineInvariantDivergence\n    }\n    \n    def _init(self):\n        super()._init()\n        self.add_argument(\'--dist\', default=""coral"", choices=self.funcs.keys(),\n                            help=""Distance Function for Correlation Alignment"")\n    \n    def get_func(self, key):\n        return self.funcs[key]()\n\nif __name__ == \'__main__\':\n\n    parser = CorrelationAlignmentConfig(\'Correlation Distance Comparision\')\n    args   = parser.parse_args()\n\n    # Network\n    if osp.exists(args.checkpoint):\n        print(""Resume from checkpoint file at {}"".format(args.checkpoint))\n        model = torch.load(args.checkpoint)\n    else:\n        model = models.SVHNmodel()\n\n    # Dataset\n    data = datasets.da.load_dataset2(path=""data"", train=True)\n\n    train_loader = torch.utils.data.DataLoader(\n        data[args.source], batch_size=args.sourcebatch,\n        shuffle=True, num_workers=args.njobs)\n    val_loader   = torch.utils.data.DataLoader(\n        data[args.target], batch_size=args.targetbatch,\n        shuffle=True, num_workers=args.njobs)\n\n    dataset = datasets.JointLoader(train_loader, val_loader)\n\n    dist = parser.get_func(args.dist)\n\n    # Initialize the solver for this experiment\n    experiment = solver.da.CorrelationDistanceSolver(model, dataset,\n                            corr_dist = dist,\n                            n_epochs=args.epochs,\n                            savedir=args.log,\n                            dryrun = args.dryrun,\n                            learningrate = args.learningrate,\n                            gpu=args.gpu if not args.cpu else None)\n\n    experiment.optimize()'"
examples/correlation/train_coral.py,3,"b'import sys\nimport os\nimport os.path as osp\nsys.path.append(osp.dirname(__file__))\n\nimport torch\nfrom torch import nn\n\nfrom salad import solver, datasets\nfrom salad.utils import config \n\nimport salad.models.digits.corr as models\n\nif __name__ == \'__main__\':\n\n    parser = config.DomainAdaptConfig(\'CORAL\')\n    args   = parser.parse_args()\n\n    # Network\n    if osp.exists(args.checkpoint):\n        print(""Resume from checkpoint file at {}"".format(args.checkpoint))\n        model = torch.load(args.checkpoint)\n    else:\n        model = models.SVHNmodel()\n\n    # Dataset\n    data = datasets.da.load_dataset2(path=""data"", train=True)\n\n    train_loader = torch.utils.data.DataLoader(\n        data[args.source], batch_size=args.sourcebatch,\n        shuffle=True, num_workers=args.njobs)\n    val_loader   = torch.utils.data.DataLoader(\n        data[args.target], batch_size=args.targetbatch,\n        shuffle=True, num_workers=args.njobs)\n\n    dataset = datasets.JointLoader(train_loader, val_loader)\n\n    # Initialize the solver for this experiment\n    experiment = solver.da.DeepCoralSolver(model, dataset,\n                               n_epochs=args.epochs,\n                               savedir=args.log,\n                               dryrun = args.dryrun,\n                               learningrate = args.learningrate,\n                               gpu=args.gpu if not args.cpu else None)\n\n    experiment.optimize()'"
examples/correlation/train_logcoral.py,3,"b'import sys\nimport os\nimport os.path as osp\nsys.path.append(osp.dirname(__file__))\n\nimport torch\nfrom torch import nn\n\nfrom salad import solver, datasets\nfrom salad.utils import config \n\nimport salad.models.digits.corr as models\n\nif __name__ == \'__main__\':\n\n    parser = config.DomainAdaptConfig(\'CORAL\')\n    args   = parser.parse_args()\n\n    # Network\n    if osp.exists(args.checkpoint):\n        print(""Resume from checkpoint file at {}"".format(args.checkpoint))\n        model = torch.load(args.checkpoint)\n    else:\n        model = models.SVHNmodel()\n\n    # Dataset\n    data = datasets.da.load_dataset2(path=""data"", train=True)\n\n    train_loader = torch.utils.data.DataLoader(\n        data[args.source], batch_size=args.sourcebatch,\n        shuffle=True, num_workers=args.njobs)\n    val_loader   = torch.utils.data.DataLoader(\n        data[args.target], batch_size=args.targetbatch,\n        shuffle=True, num_workers=args.njobs)\n\n    dataset = datasets.JointLoader(train_loader, val_loader)\n\n    # Initialize the solver for this experiment\n    experiment = solver.da.DeepLogCoralSolver(model, dataset,\n                               n_epochs=args.epochs,\n                               savedir=args.log,\n                               dryrun = args.dryrun,\n                               learningrate = args.learningrate,\n                               gpu=args.gpu if not args.cpu else None)\n\n    experiment.optimize()'"
examples/crossgrad/train.py,1,"b'from torchvision import transforms, datasets\n\nimport sys\n#sys.path.append(\'/gpfs01/bethge/home/sschneider/thesis/code/domainadaptation/\')\nsys.path.append(\'/home/stes/code/thesis/code/domainadaptation/\')\n\nfrom torchvision import transforms\nimport torch\nimport salad.datasets\n\nfrom salad.solver.da import CrossGradSolver, Model\nfrom salad.utils import config\n\ndef get_data(batch_size = 64, shuffle = True, num_workers = 0, train=True, test_angle = 0):\n\n    data = []\n\n    angles = list(range(0,90,15))\n    \n    print(""testing on {}"".format(angles[test_angle]))\n    if train:\n        del(angles[test_angle])\n        print(\'training on {}\'.format("","".join(str(i) for i in angles)))\n    else:\n        angles = [angles[test_angle]]\n        print(\'testing on {}\'.format("","".join(str(i) for i in angles)))   \n    \n    noisemodels = [\n        transforms.RandomRotation([i-1,i+1]) for i in angles\n    ]\n\n    for N in noisemodels:\n\n        transform = transforms.Compose([\n            N,    \n            transforms.ToTensor(),  \n                transforms.Normalize(mean=(0.43768448, 0.4437684,  0.4728041 ),\n                                    std= (0.19803017, 0.20101567, 0.19703583))\n        ])\n        mnist = datasets.MNIST(\'./data\', train=train, download=True, transform=transform)\n\n        data.append(torch.utils.data.DataLoader(\n            mnist, batch_size=batch_size,\n            shuffle=shuffle, num_workers=num_workers))\n    \n    loader = salad.datasets.JointLoader(*data, collate_fn = collate)\n    \n    return loader\n\nclass CrossGradConfig(config.DomainAdaptConfig):\n    \n    def _init(self):\n        self.add_argument(\'--testangle\', default=0, type=int, help=""Test Angle"")\n\nif __name__ == \'__main__\':\n    args = CrossGradConfig(""Cross Grad Solver"")\n\n    for angle in range(6):\n        model = Model(10, 5)\n        data  = get_data(test_angle = angle, num_workers = 4)\n        solver = CrossGradSolver(model, data, gpu=0, n_epochs = 10, savedir = args.log + \'-{}\'.format(angle))\n        solver.optimize()'"
examples/dann/train_dann.py,5,"b'"""""" Minimal Training Script for Associative Domain Adaptation\n""""""\n\nimport os\nimport os.path as osp\nimport sys\nimport argparse\n\nimport torch\n\nfrom salad import solver, datasets, models\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport os\nfrom collections import OrderedDict\n\nclass SVHN(nn.Module):\n    def __init__(self, features, n_channel, num_classes):\n        super(SVHN, self).__init__()\n        assert isinstance(features, nn.Sequential), type(features)\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Linear(n_channel, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        y = self.classifier(x)\n        return x, y\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for i, v in enumerate(cfg):\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            padding = v[1] if isinstance(v, tuple) else 1\n            out_channels = v[0] if isinstance(v, tuple) else v\n            conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(out_channels, affine=False), nn.ReLU(), nn.Dropout(0.3)]\n            else:\n                layers += [conv2d, nn.ReLU(), nn.Dropout(0.3)]\n            in_channels = out_channels\n    return nn.Sequential(*layers)\n\ndef svhn(n_channel, pretrained=None):\n    cfg = [n_channel, n_channel, \'M\', 2*n_channel, 2*n_channel, \'M\', 4*n_channel, 4*n_channel, \'M\', (8*n_channel, 0), \'M\']\n    layers = make_layers(cfg, batch_norm=True)\n    model = SVHN(layers, n_channel=8*n_channel, num_classes=10)\n    if pretrained is not None:\n        m = model_zoo.load_url(model_urls[\'svhn\'])\n        state_dict = m.state_dict() if isinstance(m, nn.Module) else m\n        assert isinstance(state_dict, (dict, OrderedDict)), type(state_dict)\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef build_parser():\n\n    parser = argparse.ArgumentParser(description=\'Associative Domain Adaptation\')\n\n    # General setup\n    parser.add_argument(\'--gpu\', default=0, help=\'Specify GPU\', type=int)\n    parser.add_argument(\'--cpu\', action=\'store_true\', help=""Use CPU Training"")\n    parser.add_argument(\'--log\', default=""./log/log2"", help=""Log directory. Will be created if non-existing"")\n    parser.add_argument(\'--epochs\', default=""1000"", help=""Number of Epochs (Full passes through the unsupervised training set)"", type=int)\n    parser.add_argument(\'--checkpoint\', default="""", help=""Checkpoint path"")\n    parser.add_argument(\'--learningrate\', default=3e-4, type=float, help=""Learning rate for Adam. Defaults to Karpathy\'s constant ;-)"")\n    parser.add_argument(\'--dryrun\', action=\'store_true\', help=""Perform a test run, without actually training a network. Usefule for debugging."")\n\n    # Domain Adaptation Args\n    parser.add_argument(\'--source\', default=""svhn"", choices=[\'mnist\', \'svhn\'], help=""Source Dataset. Choose mnist or svhn"")\n    parser.add_argument(\'--target\', default=""mnist"", choices=[\'mnist\', \'svhn\'], help=""Target Dataset. Choose mnist or svhn"")\n\n    parser.add_argument(\'--sourcebatch\', default=64, type=int, help=""Batch size of Source"")\n    parser.add_argument(\'--targetbatch\', default=64, type=int, help=""Batch size of Target"")\n\n    # Associative DA Hyperparams\n    parser.add_argument(\'--visit\', default=0.1, type=float, help=""Visit weight"")\n    parser.add_argument(\'--walker\', default=1.0, type=float, help=""Walker weight"")\n\n    return parser\n\nif __name__ == \'__main__\':\n\n    parser = build_parser()\n    args   = parser.parse_args()\n\n    # Network\n    if osp.exists(args.checkpoint):\n        print(""Resume from checkpoint file at {}"".format(args.checkpoint))\n        model = torch.load(args.checkpoint)\n    else:\n        model   = models.SVHNmodel()\n\n    # Dataset\n    data = datasets.load_dataset(path=""data"", train=True, img_size = 32)\n\n    train_loader = torch.utils.data.DataLoader(\n        data[args.source], batch_size=args.sourcebatch,\n        shuffle=True, num_workers=4)\n    val_loader   = torch.utils.data.DataLoader(\n        data[args.target], batch_size=args.targetbatch,\n        shuffle=True, num_workers=4)\n\n    dataset = datasets.JointLoader(train_loader, val_loader)\n\n    # Initialize the solver for this experiment\n    experiment = solver.DANNSolver(model, dataset,\n                               n_epochs=args.epochs,\n                               savedir=args.log,\n                               dryrun = args.dryrun,\n                               learningrate = args.learningrate,\n                               gpu=args.gpu if not args.cpu else None)\n\n\n    experiment.optimize()\n'"
examples/dirtt/train_dirtt.py,5,"b'"""""" Training script for DIRT-T and VADA models\n""""""\n\nimport os\nimport os.path as osp\nimport sys\nimport argparse\n\nimport torch\nfrom torch import nn\n\nfrom salad import solver, models, datasets\n\nimport salad.datasets.transforms\nfrom salad.datasets.transforms.ensembling import ImageAugmentation\n\nfrom torch.nn import functional as F\nfrom salad.layers import AccuracyScore\nfrom salad.utils import config\n\nimport salad.models.digits.dirtt as model\n\nclass Augmentation():\n    \n    def __init__(self, dataset, n_samples=1):\n        self.transformer = ImageAugmentation(\n            affine_std=0.1,\n            gaussian_noise_std=0.1,\n            hflip=False,\n            intens_flip=True,\n            intens_offset_range_lower=-.5, intens_offset_range_upper=.5,\n            intens_scale_range_lower=0.25, intens_scale_range_upper=1.5,\n            xlat_range=2.0\n        )\n        \n        self.dataset = dataset\n        self.n_samples = n_samples\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, index):\n        x, y = self.dataset[index]\n        X = torch.stack([x.clone() for _ in range(self.n_samples)], dim=0)\n        X = self.transformer.augment(X.numpy())\n        outp = [torch.from_numpy(x).float() for x in X] + [y,]\n        return outp\n        \nif __name__ == \'__main__\':\n\n    parser = config.DomainAdaptConfig(""DIRT-T Solver"")\n    args   = parser.parse_args()\n\n    model   = model.SVHN_MNIST_Model(n_domains=1)\n    disc    = nn.Linear(128, 1)\n\n    # Dataset\n    data = datasets.da.load_dataset2(path=""/tmp/data"", train=True)\n\n    train_loader = torch.utils.data.DataLoader(\n        Augmentation(data[args.source]), batch_size=args.sourcebatch,\n        shuffle=True, num_workers=args.njobs)\n    val_loader   = torch.utils.data.DataLoader(\n        Augmentation(data[args.target]), batch_size=args.targetbatch,\n        shuffle=True, num_workers=args.njobs)\n\n    loader = datasets.JointLoader(train_loader, val_loader)\n\n    # Initialize the solver for this experiment\n    experiment = solver.da.VADASolver(model, disc, loader,\n                               n_epochs=args.epochs,\n                               savedir=args.log,\n                               dryrun = args.dryrun,\n                               learningrate = args.learningrate,\n                               gpu=args.gpu if not args.cpu else None)\n\n    experiment.optimize()\n\n    '"
examples/dirtt/train_vada_old.py,3,"b'"""""" Minimal Training Script for Associative Domain Adaptation\n""""""\n\nimport os\nimport os.path as osp\nimport sys\nimport argparse\n\nimport torch\n\nfrom salad import solver, models, datasets\n\n\ndef build_parser():\n\n    parser = argparse.ArgumentParser(description=\'Associative Domain Adaptation\')\n\n    # General setup\n    parser.add_argument(\'--gpu\', default=0, help=\'Specify GPU\', type=int)\n    parser.add_argument(\'--cpu\', action=\'store_true\', help=""Use CPU Training"")\n    parser.add_argument(\'--log\', default=""./log/log2"", help=""Log directory. Will be created if non-existing"")\n    parser.add_argument(\'--epochs\', default=""1000"", help=""Number of Epochs (Full passes through the unsupervised training set)"", type=int)\n    parser.add_argument(\'--checkpoint\', default="""", help=""Checkpoint path"")\n    parser.add_argument(\'--learningrate\', default=3e-4, type=float, help=""Learning rate for Adam. Defaults to Karpathy\'s constant ;-)"")\n    parser.add_argument(\'--dryrun\', action=\'store_true\', help=""Perform a test run, without actually training a network. Usefule for debugging."")\n\n    # Domain Adaptation Args\n    parser.add_argument(\'--source\', default=""svhn"", choices=[\'mnist\', \'svhn\'], help=""Source Dataset. Choose mnist or svhn"")\n    parser.add_argument(\'--target\', default=""mnist"", choices=[\'mnist\', \'svhn\'], help=""Target Dataset. Choose mnist or svhn"")\n\n    parser.add_argument(\'--sourcebatch\', default=128, type=int, help=""Batch size of Source"")\n    parser.add_argument(\'--targetbatch\', default=128, type=int, help=""Batch size of Target"")\n\n    # Associative DA Hyperparams\n    parser.add_argument(\'--dirtt\', action=\'store_true\', help=\'Start DIRT-T\')\n\n    return parser\n\nif __name__ == \'__main__\':\n\n    parser = build_parser()\n    args   = parser.parse_args()\n\n    # Network\n    if osp.exists(args.checkpoint):\n        print(""Resume from checkpoint file at {}"".format(args.checkpoint))\n        model = torch.load(args.checkpoint)\n    else:\n        model   = models.FrenchModel()\n\n    # Dataset\n    data = datasets.load_dataset(path=""data"", train=True)\n\n    train_loader = torch.utils.data.DataLoader(\n        data[args.source], batch_size=args.sourcebatch,\n        shuffle=True, num_workers=4)\n    val_loader   = torch.utils.data.DataLoader(\n        data[args.target], batch_size=args.targetbatch,\n        shuffle=True, num_workers=4)\n\n    loader = datasets.JointLoader(train_loader, val_loader)\n\n    # Initialize the solver for this experiment\n\n    if args.dirtt:\n        teacher = models.FrenchModel()\n        experiment = solver.DIRTTSolver(model, teacher, val_loader,\n                                n_epochs=args.epochs,\n                                savedir=args.log,\n                                dryrun = args.dryrun,\n                                learningrate = args.learningrate,\n                                gpu=args.gpu if not args.cpu else None)\n    else:\n        experiment = solver.VADASolver(model, loader,\n                                n_epochs=args.epochs,\n                                savedir=args.log,\n                                dryrun = args.dryrun,\n                                learningrate = args.learningrate,\n                                gpu=args.gpu if not args.cpu else None)\n\n    experiment.optimize()\n'"
examples/ensembling/train_ensemble_reimplement.py,9,"b'"""""" Minimal Training Script for Associative Domain Adaptation\n""""""\n\nimport os\nimport os.path as osp\nimport sys\nimport argparse\n\nimport torch\nfrom torch import nn\n\nfrom salad import solver, models, datasets\n\nfrom augment import AffineTransformer\nfrom augment2 import ImageAugmentation\n\nfrom torch.nn import functional as F\nfrom salad.layers import AccuracyScore\n\nclass SVHN_MNIST_Model(nn.Module):\n    \n    def __init__(self, n_classes):\n        super(SVHN_MNIST_Model, self).__init__()\n        \n        self.norm = nn.InstanceNorm2d(3, affine=False,\n                momentum=0,\n                track_running_stats=False)\n        \n        self.conv1_1 = nn.Conv2d(3, 128, (3, 3), padding=1)\n        self.conv1_1_bn = nn.BatchNorm2d(128)\n        self.conv1_2 = nn.Conv2d(128, 128, (3, 3), padding=1)\n        self.conv1_2_bn = nn.BatchNorm2d(128)\n        self.conv1_3 = nn.Conv2d(128, 128, (3, 3), padding=1)\n        self.conv1_3_bn = nn.BatchNorm2d(128)\n        self.pool1 = nn.MaxPool2d((2, 2))\n        self.drop1 = nn.Dropout()\n\n        self.conv2_1 = nn.Conv2d(128, 256, (3, 3), padding=1)\n        self.conv2_1_bn = nn.BatchNorm2d(256)\n        self.conv2_2 = nn.Conv2d(256, 256, (3, 3), padding=1)\n        self.conv2_2_bn = nn.BatchNorm2d(256)\n        self.conv2_3 = nn.Conv2d(256, 256, (3, 3), padding=1)\n        self.conv2_3_bn = nn.BatchNorm2d(256)\n        self.pool2 = nn.MaxPool2d((2, 2))\n        self.drop2 = nn.Dropout()\n\n        self.conv3_1 = nn.Conv2d(256, 512, (3, 3), padding=0)\n        self.conv3_1_bn = nn.BatchNorm2d(512)\n        self.nin3_2 = nn.Conv2d(512, 256, (1, 1), padding=1)\n        self.nin3_2_bn = nn.BatchNorm2d(256)\n        self.nin3_3 = nn.Conv2d(256, 128, (1, 1), padding=1)\n        self.nin3_3_bn = nn.BatchNorm2d(128)\n\n        self.fc4 = nn.Linear(128, n_classes)\n\n    def forward(self, x):\n        x = self.norm(x)\n        \n        x = F.relu(self.conv1_1_bn(self.conv1_1(x)))\n        x = F.relu(self.conv1_2_bn(self.conv1_2(x)))\n        x = self.pool1(F.relu(self.conv1_3_bn(self.conv1_3(x))))\n        x = self.drop1(x)\n\n        x = F.relu(self.conv2_1_bn(self.conv2_1(x)))\n        x = F.relu(self.conv2_2_bn(self.conv2_2(x)))\n        x = self.pool2(F.relu(self.conv2_3_bn(self.conv2_3(x))))\n        x = self.drop2(x)\n\n        x = F.relu(self.conv3_1_bn(self.conv3_1(x)))\n        x = F.relu(self.nin3_2_bn(self.nin3_2(x)))\n        x = F.relu(self.nin3_3_bn(self.nin3_3(x)))\n\n        x = F.avg_pool2d(x, 6)\n        z = x = x.view(-1, 128)\n\n        x = self.fc4(x)\n        return z, x\n\n\ndef build_parser():\n\n    parser = argparse.ArgumentParser(description=\'Associative Domain Adaptation\')\n\n    # General setup\n    parser.add_argument(\'--gpu\', default=0,\n        help=\'Specify GPU\', type=int)\n    parser.add_argument(\'--cpu\', action=\'store_true\',\n        help=""Use CPU Training"")\n    parser.add_argument(\'--log\', default=""./log/testruns"",\n        help=""Log directory. Will be created if non-existing"")\n    parser.add_argument(\'--epochs\', default=""1000"",\n        help=""Number of Epochs (Full passes through the unsupervised training set)"", type=int)\n    parser.add_argument(\'--checkpoint\', default="""",\n        help=""Checkpoint path"")\n    parser.add_argument(\'--learningrate\', default=1e-3, type=float,\n        help=""Learning rate for Adam. Defaults to Karpathy\'s constant ;-)"")\n    parser.add_argument(\'--dryrun\', action=\'store_true\',\n        help=""Perform a test run, without actually training a network."")\n\n    # Domain Adaptation Args\n    parser.add_argument(\'--source\', default=""svhn"", choices=[\'mnist\', \'svhn\'],\n                        help=""Source Dataset. Choose mnist or svhn"")\n    parser.add_argument(\'--target\', default=""mnist"", choices=[\'mnist\', \'svhn\'],\n                        help=""Target Dataset. Choose mnist or svhn"")\n\n    parser.add_argument(\'--sourcebatch\', default=128, type=int,\n                        help=""Batch size of Source"")\n    parser.add_argument(\'--targetbatch\', default=128, type=int,\n                        help=""Batch size of Target"")\n    \n    return parser\n\nfrom salad import solver\n\nclass WeightEMA (object):\n    """"""\n    Exponential moving average weight optimizer for mean teacher model\n\n    Taken from https://github.com/Britefury/self-ensemble-visual-domain-adapt\n    """"""\n    def __init__(self, params, src_params, alpha=0.999):\n\n        self.params = list(params)\n        self.src_params = list(src_params)\n        self.alpha = alpha\n\n        for p, src_p in zip(self.params, self.src_params):\n            p.data[:] = src_p.data[:]\n\n    def step(self):\n        one_minus_alpha = 1.0 - self.alpha\n        for p, src_p in zip(self.params, self.src_params):\n            p.data.mul_(self.alpha)\n            p.data.add_(src_p.data * one_minus_alpha)\n\n    def zero_grad(self):\n        pass\n\nclass EnsemblingLoss(object):\n\n    def __init__(self, model, teacher):\n\n        self.model   = model\n        self.teacher = teacher\n\n    def __call__(self, batch):        \n        (x_stud_xs, ys), (x_stud_xt,x_teach_xt,yt) = batch\n        \n        _, stud_ys  = self.model(x_stud_xs)\n        _, stud_yt  = self.model(x_stud_xt)\n        _, teach_yt = self.teacher(x_teach_xt)\n\n        losses = {}\n        losses[\'ce\']         = (stud_ys, ys)\n        losses[\'ensemble\']   = (stud_yt, teach_yt.detach())\n        \n        losses[\'acc_s\']       = (stud_ys, ys)\n        losses[\'acc_t\']       = (stud_yt, yt)\n        losses[\'acc_teacher\'] = (teach_yt, yt)\n\n        return losses\n    \nclass WeightedCE(nn.Module):\n    \n    def __init__(self):\n        \n        super(WeightedCE, self).__init__()\n        \n        self.threshold  =  0.96837722\n        self.softmax    = nn.Softmax(dim=-1)\n        \n    def robust_binary_crossentropy(self, pred, tgt):\n        inv_tgt = -tgt + 1.0\n        inv_pred = -pred + 1.0 + 1e-6\n        return -(tgt * torch.log(pred + 1.0e-6) + inv_tgt * torch.log(inv_pred))\n        \n    def forward(self, logits, logits_target):\n        \n        p_         = self.softmax(logits)\n        p          = self.softmax(logits_target)\n        conf, _    = p.max(dim=-1)\n        mask       = torch.gt(conf, self.threshold)\n        \n        if mask.sum() > 0:\n            loss = self.robust_binary_crossentropy(p_[mask], p[mask]).mean()\n            return loss\n        \n        loss = self.robust_binary_crossentropy(p_[:1], p[:1]).mean()\n        \n        return 0 * loss\n\n    \nclass JointOptimizer(torch.optim.Optimizer):\n    \n    def __init__(self, adam, ema):\n        \n        self.adam = adam\n        self.ema  = ema\n        \n    def zero_grad(self):\n        self.adam.zero_grad()\n    \n    def step(self):\n        self.adam.step()\n        self.ema.step()\n    \nclass SelfEnsemblingSolver(solver.da.DABaseSolver):\n\n    def __init__(self, model, teacher, dataset, learningrate, *args, **kwargs):\n        super(SelfEnsemblingSolver, self).__init__(model, dataset, *args, **kwargs)\n\n        teacher_alpha = 0.99\n        \n        self.register_model(teacher, ""teacher"")\n        self.teacher = teacher\n        \n        opt_stud  = torch.optim.Adam(model.parameters(), lr=learningrate)\n        opt_teach = WeightEMA(teacher.parameters(),\n                              model.parameters(),\n                              alpha=teacher_alpha)\n        \n        optim = JointOptimizer(opt_stud, opt_teach)\n        \n\n        self.register_optimizer(optim, EnsemblingLoss(self.model, self.teacher),\n                               name=\'Joint Optimizer\')\n        self.register_loss(WeightedCE(), 3, \'ensemble\')\n        self.register_loss(AccuracyScore(), None, \'acc_teacher\')\n\n        \nclass Augmentation():\n    \n    def __init__(self, dataset, n_samples=1):\n        self.transformer = ImageAugmentation(\n            affine_std=0.1,\n            gaussian_noise_std=0.1,\n            hflip=False,\n            intens_flip=True,\n            intens_offset_range_lower=-.5, intens_offset_range_upper=.5,\n            intens_scale_range_lower=0.25, intens_scale_range_upper=1.5,\n            xlat_range=2.0\n        )\n        \n        self.dataset = dataset\n        self.n_samples = n_samples\n        \n    def __len__(self):\n        \n        return len(self.dataset)\n    \n    def __getitem__(self, index):\n        \n        x, y = self.dataset[index]\n        \n        X = torch.stack([x.clone() for _ in range(self.n_samples)], dim=0)\n        X = self.transformer.augment(X.numpy())\n        \n        outp = [torch.from_numpy(x).float() for x in X] + [y,]\n        \n        return outp\n        \nif __name__ == \'__main__\':\n\n    parser = build_parser()\n    args   = parser.parse_args()\n\n    # Network\n    model   = SVHN_MNIST_Model(10)\n    teacher = SVHN_MNIST_Model(10)\n    for param in teacher.parameters():\n        param.requires_grad_(False)\n\n    # Dataset\n    data = datasets.da.load_dataset2(path=""data"", train=True)\n\n    train_loader = torch.utils.data.DataLoader(\n        Augmentation(data[args.source]), batch_size=args.sourcebatch,\n        shuffle=True, num_workers=4)\n    val_loader   = torch.utils.data.DataLoader(\n        Augmentation(data[args.target], 2), batch_size=args.targetbatch,\n        shuffle=True, num_workers=4)\n\n    loader = datasets.JointLoader(train_loader, val_loader)\n    \n    # Initialize the solver for this experiment\n    experiment = SelfEnsemblingSolver(model, teacher, loader,\n                               n_epochs=args.epochs,\n                               savedir=args.log,\n                               dryrun = args.dryrun,\n                               learningrate = args.learningrate,\n                               gpu=args.gpu if not args.cpu else None)\n\n    experiment.optimize()\n'"
examples/ensembling/train_ensembling.py,3,"b'"""""" Minimal Training Script for Associative Domain Adaptation\n""""""\n\nimport os\nimport os.path as osp\nimport sys\nimport argparse\nimport torch\nfrom torch import nn\nfrom salad import solver, datasets, optim\nfrom torch.nn import functional as F\nfrom salad.layers import AccuracyScore\nfrom salad import solver\nfrom salad.datasets.transforms.ensembling import ImageAugmentation\nfrom salad.utils import config\nimport salad.models.digits.ensemble as models\n\nfrom salad.datasets.transforms import Augmentation\n\nif __name__ == \'__main__\':\n    parser = config.DomainAdaptConfig(""Ensembling Solver"")\n    args = parser.parse_args()\n    # Network\n\n    # Optimize the student on the supervised loss. Transfer weights to the teacher\n    # Vary the weight sharing between teacher and student and gradually let the teacher\n    # take over the dominant role\n    #\n    # student params <- CE\n    # student bn src <- CE\n    # student bn tgt <- Prox Label\n    # teacher params <- EMA\n    # teacher bn     <- student src * a + student tgt * (1 - a)\n    model = models.SVHN_MNIST_Model(n_domains=2)\n    teacher = models.SVHN_MNIST_Model(n_domains=1)\n\n    for param in teacher.parameters():\n    param.requires_grad_(False)\n    # Dataset\n    data = datasets.da.load_dataset2(path=""data"", train=True)\n    train_loader = torch.utils.data.DataLoader(\n        Augmentation(data[args.source]), batch_size=args.sourcebatch,\n        shuffle=True, num_workers=4)\n    val_loader = torch.utils.data.DataLoader(\n        Augmentation(data[args.target], 2), batch_size=args.targetbatch,\n        shuffle=True, num_workers=4)\n    loader = datasets.JointLoader(train_loader, val_loader)\n\n    # Initialize the solver for this experiment\n    experiment = solver.SelfEnsemblingSolver(model, teacher, loader,\n                                             n_epochs=args.epochs,\n                                             savedir=args.log,\n                                             dryrun=args.dryrun,\n                                             learningrate=args.learningrate,\n                                             gpu=args.gpu if not args.cpu else None)\n    experiment.optimize()\n'"
examples/ensembling/train_ensembling_noise.py,5,"b'"""""" Minimal Training Script for Associative Domain Adaptation\n""""""\n\nimport os\nimport os.path as osp\nimport sys\nimport argparse\n\nimport torch\nfrom torch import nn\n\nfrom salad import solver, datasets, optim\n\nfrom torch.nn import functional as F\nfrom salad.layers import AccuracyScore\n\nfrom salad import solver\nfrom salad.datasets.transforms.ensembling import ImageAugmentation\nfrom salad.utils import config\n\nimport salad.models.digits.ensemble as models\n\nclass Augmentation():\n    \n    def __init__(self, dataset, n_samples=1):\n        self.transformer = ImageAugmentation(\n            affine_std=0.1,\n            gaussian_noise_std=0.1,\n            hflip=False,\n            intens_flip=True,\n            intens_offset_range_lower=-.5, intens_offset_range_upper=.5,\n            intens_scale_range_lower=0.25, intens_scale_range_upper=1.5,\n            xlat_range=2.0\n        )\n        \n        self.dataset = dataset\n        self.n_samples = n_samples\n        \n    def __len__(self):\n        \n        return len(self.dataset)\n    \n    def __getitem__(self, index):\n        \n        x, y = self.dataset[index]\n        \n        X = torch.stack([x.clone() for _ in range(self.n_samples)], dim=0)\n        X = self.transformer.augment(X.numpy())\n        \n        outp = [torch.from_numpy(x).float() for x in X] + [y,]\n        \n        return outp\n        \nif __name__ == \'__main__\':\n\n    parser = config.DomainAdaptConfig(""Ensembling Solver"")\n    args   = parser.parse_args()\n\n    # Network\n    \n    # Optimize the student on the supervised loss. Transfer weights to the teacher\n    # Vary the weight sharing between teacher and student and gradually let the teacher\n    # take over the dominant role\n    #\n    # student params <- CE\n    # student bn src <- CE\n    # student bn tgt <- Prox Label\n    # teacher params <- EMA\n    # teacher bn     <- student src * a + student tgt * (1 - a)\n    model   = models.SVHN_MNIST_Model(n_domains=2)\n    teacher = models.SVHN_MNIST_Model(n_domains=1)\n    \n    for param in teacher.parameters():\n        param.requires_grad_(False)\n\n    # Dataset\n    from torchvision import transforms\n    from salad.datasets.transforms.noise import SaltAndPepper, Gaussian\n    from salad.datasets.da.digits import NoiseLoader\n\n    loader = NoiseLoader(\'/tmp/data\', \'svhn\', collate = \'stack\',\n                            noisemodels=[lambda x : x, SaltAndPepper(0.05)],\n                            batch_size = 32, shuffle = True\n\n                            )\n    source = loader.datasets[0].dataset\n    target = loader.datasets[1].dataset\n\n    train_loader = torch.utils.data.DataLoader(\n        Augmentation(source), batch_size=args.sourcebatch,\n        shuffle=True, num_workers=4)\n    val_loader   = torch.utils.data.DataLoader(\n        Augmentation(target, 2), batch_size=args.targetbatch,\n        shuffle=True, num_workers=4)\n\n    loader = datasets.JointLoader(train_loader, val_loader)\n\n    # Initialize the solver for this experiment\n    experiment = solver.SelfEnsemblingSolver(model, teacher, loader,\n                               n_epochs=args.epochs,\n                               savedir=args.log,\n                               dryrun = args.dryrun,\n                               learningrate = args.learningrate,\n                               gpu=args.gpu if not args.cpu else None)\n\n    experiment.optimize()\n\n    \n'"
salad/analysis/params.py,1,"b'param_collection = [{}, {}, {}, {}]\n\nN = 9\n\nn_models = len(fnames)\n\nfor n, fname in enumerate(fnames):\n    model = torch.load(fname)\n    \n    print(n, len(param_collection))\n\n    for i in range(N):\n\n        for j, layer in enumerate(model.conditional_layers):\n            \n            vals = get_transform(layer.layers[i])\n            \n            for val, param_dict in zip(vals, param_collection): \n                \n                val = val.data.detach().cpu().numpy()\n\n                p             = param_dict.get(j, np.zeros((10,n_models) + val.shape))\n                p[i,n]        = val\n                param_dict[j] = p\n\nP = []\nfor p in param_collection:\n    params = [p[i] for i in range(len(p))]\n    P.append(np.concatenate(params, axis=-1))\nP = np.stack(P, axis=0)[:,:9]\nP.shape\n\ndef compute_linear(params):\n\n    mu, var, gamma, beta = params\n\n    inv_std = (1e-5 + var)**(-.5)\n\n    b = beta - (mu * gamma) * inv_std \n    m = gamma * inv_std\n\n    return m, b\n\ndef get_transform(bn_layer):\n    \n    mu    = bn_layer.running_mean\n    var   = bn_layer.running_var\n    gamma = bn_layer.weight\n    beta  = bn_layer.bias\n    \n    inv_std = (1e-5 + var)**(-.5)\n    \n    b = beta - (mu * gamma) * inv_std \n    m = gamma * inv_std\n    \n    m = m.data.detach().cpu().numpy()\n    b = b.data.detach().cpu().numpy()\n    \n    return mu, var, gamma, beta'"
salad/datasets/__init__.py,2,"b'"""""" Datasets for Domain Adaptation Experiments.\n\nThis package contains datasets and tools for handling datasets.\nSimilar as in ``torchvision.datasets``, data is accessed through\nsubclasses of ``torch.utils.data.DataLoader`` and \n``torch.utils.data.Dataset``.\n\nAs one very established Domain Adaptation benchmarks, the ``digits``\npackage focusses on the small digit benchmark consisting of\n\n- MNIST\n- USPS\n- SVHN\n- SYNTH\n\nPrincipally two main methods for loading data are currently implemented.\nIn general, multiple datasets are loaded.\n\nIn **cat** mode, the dataset returns values of the form\n\n>>> for x,y,d in data_loader:\n>>>     print(x.size(), y.size(), d.size())\n\nIn **stack** mode, the dataset returns tuples (of possible different sizes):\n\n>>> for (xs,ys), (xt, yt) in data_loader:\n>>>     pass\n\n""""""\n\nfrom .da import *\nfrom .visda import *\nfrom .digits import *\nfrom .utils import *\n# from .instance import *'"
salad/datasets/utils.py,2,"b'import torch\n\ndef compute_normalization_stats(data):\n    """""" Computes Mean and Standard Deviation\n    """"""\n\n    n_samples = 0\n    running_mean = None\n    running_sd = None\n\n    for ( x, _ ), in data:\n\n        if running_mean is None:\n            running_mean = torch.zeros(x.size(1)).double()\n            running_sd   = torch.zeros(x.size(1)).double()\n\n            assert x.ndim == 4, ""compute_normalization_stats() only implemented for 4d Tensors!""\n        \n        x_ = x.transpose(1,0).contiguous().view(x.size(1), -1).double()\n        \n        running_mean += x_.sum(dim=-1)\n        running_sd   += x_.var(dim=-1, unbiased=False) * x_.size(1)\n        n_samples    += int(x_.size(1))\n        \n    mu = running_mean / n_samples\n    sd = (running_sd  / (n_samples - 1))**.5\n    \n    return mu, sd'"
salad/layers/__init__.py,0,"b'from .base import KLDivWithLogits, AccuracyScore, MeanAccuracyScore, WeightedCE\nfrom .association import *\nfrom .vat import *\nfrom .coral import *\n\nfrom .funcs import *\nfrom .noise import *'"
salad/layers/association.py,16,"b'import h5py\nimport torch\nfrom torch import nn\n\nimport torch\nimport torch.nn.functional as F\n\n\nclass WalkerLoss(nn.Module):\n\n    def forward(self, Psts, y):\n        equality_matrix = torch.eq(y.clone().view(-1,1), y).float()\n        p_target = equality_matrix / equality_matrix.sum(dim=1, keepdim=True)\n        p_target.requires_grad = False\n\n        L_walker = F.kl_div(torch.log(1e-8 + Psts), p_target, size_average=False)\n        L_walker /= p_target.size()[0]\n\n        return L_walker\n\nclass VisitLoss(nn.Module):\n\n    def forward(self, Pt):\n        p_visit = torch.ones([1, Pt.size()[1]]) / float(Pt.size()[1])\n        p_visit.requires_grad = False\n        if Pt.is_cuda: p_visit = p_visit.cuda()\n        L_visit = F.kl_div(torch.log(1e-8 + Pt), p_visit, size_average=False)\n        L_visit /= p_visit.size()[0]\n\n        return L_visit\n\nclass AssociationMatrix(nn.Module):\n\n    def __init__(self, verbose = False):\n        super(AssociationMatrix, self).__init__()\n\n        self.verbose = verbose\n\n    def forward(self, xs, xt):\n        """"""\n        xs: (Ns, K, ...)\n        xt: (Nt, K, ...)\n        """"""\n\n        # TODO not sure why clone is needed here\n        Bs = xs.size()[0]\n        Bt = xt.size()[0]\n\n        xs = xs.clone().view(Bs, -1)\n        xt = xt.clone().view(Bt, -1)\n\n        W = torch.mm(xs, xt.transpose(1,0))\n\n        # p(xt | xs) as softmax, normalize over xt axis\n        Pst = F.softmax(W, dim=1) # Ns x Nt\n        # p(xs | xt) as softmax, normalize over xs axis\n        Pts = F.softmax(W.transpose(1,0), dim=1) # Nt x Ns\n\n        # p(xs | xs)\n        Psts = Pst.mm(Pts) # Ns x Ns\n\n        # p(xt)\n        Pt = torch.mean(Pst, dim=0, keepdim=True) # Nt\n\n        return Psts, Pt\n\nclass AssociativeLoss(nn.Module):\n\n    """""" Association Loss for Domain Adaptation\n\n    Reference:\n    Associative Domain Adaptation, Hausser et al. (2017)\n    """"""\n\n    def __init__(self, walker_weight = 1., visit_weight = 1.):\n        super(AssociativeLoss, self).__init__()\n\n        self.matrix = AssociationMatrix()\n        self.walker = WalkerLoss()\n        self.visit  = VisitLoss()\n\n        self.walker_weight = walker_weight\n        self.visit_weight  = visit_weight\n\n    def forward(self, xs, xt, y):\n\n        Psts, Pt = self.matrix(xs, xt)\n        L_walker = self.walker(Psts, y)\n        L_visit  = self.visit(Pt)\n\n        return self.visit_weight*L_visit + self.walker_weight*L_walker\n\nclass OTLoss(nn.Module):\n\n    def __init__(self):\n\n        super(OTLoss).__init__(self)\n\n        self.mse_loss = nn.MSELoss()\n        self.ce_loss  = nn.CrossEntropyLoss()\n\n    def forward(self, xs, ys, xt, yt):\n\n        self.mse_loss(xs, xt)\n        self.ce_loss(ys, yt)\n\n        pass\n\nclass WassersteinLoss(nn.Module):\n\n    def __init__(self):\n\n        super(WassersteinLoss).__init__(self)\n\n        self.K = None\n\n    def forward(self, input):\n        pass\n\n    \nclass Accuracy(nn.Module):\n\n    def __init__(self):\n\n        super(Accuracy).__init__(self)\n\n    def forward(self, input):\n        pass\n    \n    \nclass AugmentationLoss(nn.Module):\n    """""" Augmentation Loss from \n    https://github.com/Britefury/self-ensemble-visual-domain-adapt\n    """"""\n    \n    def __init__(self, aug_loss_func = nn.MSELoss(), use_rampup=True):\n        pass\n    \n    def forward(self):\n        if self.use_rampup:\n            unsup_mask = None\n            conf_mask_count = None\n            unsup_mask_count = None\n        else:\n            conf_tea = torch.max(tea_out, 1)[0]\n            unsup_mask = conf_mask = torch.gt(conf_tea, confidence_thresh).float()\n            unsup_mask_count = conf_mask_count = torch.sum(conf_mask)\n\n        if loss == \'bce\':\n            aug_loss = network_architectures.robust_binary_crossentropy(stu_out, tea_out)\n        else:\n            d_aug_loss = stu_out - tea_out\n            aug_loss = d_aug_loss * d_aug_loss\n\n        aug_loss = torch.mean(aug_loss, 1)\n\n        if self.use_rampup:\n            unsup_loss = torch.mean(aug_loss) * rampup_weight_in_list[0]\n        else:\n            unsup_loss = torch.mean(aug_loss * unsup_mask)\n                \n        return unsup_loss\n    \nclass ClassBalanceLoss(nn.Module):\n    """""" Class Balance Loss from \n    https://github.com/Britefury/self-ensemble-visual-domain-adapt\n    """"""\n    \n    def forward(stu_out, tea_out):\n        # Compute per-sample average predicated probability\n        # Average over samples to get average class prediction\n        avg_cls_prob = torch.mean(stu_out, 0)\n        # Compute loss\n        equalise_cls_loss = cls_bal_fn(avg_cls_prob, float(1.0 / n_classes))\n\n        equalise_cls_loss = torch.mean(equalise_cls_loss) * n_classes\n\n        if use_rampup:\n            equalise_cls_loss = equalise_cls_loss * rampup_weight_in_list[0]\n        else:\n            if rampup == 0:\n                equalise_cls_loss = equalise_cls_loss * torch.mean(unsup_mask, 0)\n\n        return equalise_cls_loss * cls_balance\n'"
salad/layers/base.py,8,"b'import torch \nfrom torch import nn\n\nclass KLDivWithLogits(nn.Module):\n\n    def __init__(self):\n\n        super(KLDivWithLogits, self).__init__()\n\n        self.kl = nn.KLDivLoss(size_average=False, reduce=True)\n        self.logsoftmax = nn.LogSoftmax(dim = 1)\n        self.softmax = nn.Softmax(dim = 1)\n\n\n    def forward(self, x, y):\n\n        log_p = self.logsoftmax(x)\n        q     = self.softmax(y)\n\n        return self.kl(log_p, q) / x.size()[0]\n\n\nclass AccuracyScore(nn.Module):\n    \n    def forward(self, y, t):\n        \n        with torch.no_grad():\n            idc = y.detach().max(dim = -1)[1]\n            acc = torch.eq(idc, t).float()\n            acc = acc.mean()\n        \n        return acc \n    \nclass MeanAccuracyScore(nn.Module):\n    \n    def forward(self, y, t):\n\n        with torch.no_grad():\n            t = t.cpu()\n            labels = torch.unique(t)\n            \n            idc = y.detach().max(dim = -1)[1].cpu()\n            acc = torch.eq(idc, t).float()\n            \n            mean_acc = sum(acc[torch.eq(t,l)].mean() for l in labels) / len(labels)\n            \n        return mean_acc\n\nclass WeightedCE(nn.Module):\n    """""" Adapted from Self-Ensembling repository\n    """"""\n    \n    def __init__(self, confidence_threshold = 0.96837722):\n        \n        super().__init__()\n        \n        self.threshold  =  0.96837722\n        # NOTE changed to dim=1 from dim=-1\n        self.softmax    = nn.Softmax(dim=1)\n        \n    def robust_binary_crossentropy(self, pred, tgt):\n        inv_tgt = -tgt + 1.0\n        inv_pred = -pred + 1.0 + 1e-6\n        return -(tgt * torch.log(pred + 1.0e-6) + inv_tgt * torch.log(inv_pred))\n        \n    def forward(self, logits, logits_target):\n        \n        p_         = self.softmax(logits)\n        p          = self.softmax(logits_target)\n        conf, _    = p.max(dim=1)\n        mask       = torch.gt(conf, self.threshold)\n        \n        if mask.sum() > 0:\n            loss = self.robust_binary_crossentropy(p_[mask], p[mask]).mean()\n            return loss\n        \n        # TODO this is a hack. replace by sth better\n        loss = self.robust_binary_crossentropy(p_[:1], p[:1]).mean()\n        return 0 * loss'"
salad/layers/coral.py,0,"b'import torch \nfrom torch import nn\n\nfrom . import mat\n\nclass CorrelationDistance(nn.Module):\n\n    def __init__(self, distance = mat.euclid):\n        super().__init__()\n\n        self.dist = distance\n\n\n    def forward(self, xs, xt):\n        \n        Cs = mat.cov(xs) \n        Ct = mat.cov(xt)\n\n        d = self.dist(Cs, Ct)\n\n        return d\n\nclass CoralLoss(CorrelationDistance):\n    """""" Deep CORAL loss from paper: https://arxiv.org/pdf/1607.01719.pdf\n    """"""\n    def __init__(self):\n        super().__init__(mat.euclid)\n\nclass LogCoralLoss(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, xs, xt):\n        \n        d = mat.logeuclid(xs, xt)\n\n        return d\n#class LogCoralLoss(CorrelationDistance):\n#    """""" Log Coral Loss\n#    """"""\n#    def __init__(self):\n#        super().__init__(mat.logeuclid)\n\nclass SteinDivergence(CorrelationDistance):\n    """""" Log Coral Loss\n    """"""\n    def __init__(self):\n        super().__init__(mat.stein)\n\nclass JeffreyDivergence(CorrelationDistance):\n    """""" Log Coral Loss\n    """"""\n    def __init__(self):\n        super().__init__(mat.jeffrey)\n\nclass AffineInvariantDivergence(CorrelationDistance):\n    def __init__(self):\n        super().__init__(distance=mat.affineinvariant)'"
salad/layers/da.py,1,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass FeatureAwareNormalization(nn.Module):\n\n    def __init__(self):\n        pass\n\nclass AutoAlign2d(nn.BatchNorm2d):\n\n    def __init__(self):\n        pass\n\n    def forward(self):\n        pass\n\n\nclass AdaIN(nn.Module):\n    \n    def __init__(self, n_channels, eps=1e-05):\n        \n        super().__init__()\n        \n        self.n_channels = n_channels\n        self.eps = eps\n\n    def forward(self, x, y):\n        N = y.size()[0]\n        \n        sd = y.view(N, self.n_channels, -1).std(dim=-1)\n        mu  = y.view(N, self.n_channels, -1).mean(dim=-1)\n\n        x_ = F.instance_norm(x, running_mean=None, running_var=None, weight=None, #(self.eps + var)**.5,\n                                            bias=None, use_input_stats=True, momentum=0.,\n                                            eps=self.eps)\n        \n        x_ = x_ * sd.unsqueeze(-1).unsqueeze(-1) + mu.unsqueeze(-1).unsqueeze(-1)\n        \n        return x_'"
salad/layers/funcs.py,1,"b'import torch\n\ndef concat(x, z):\n\n    """""" Concat 4D tensor with expanded 2D tensor\n    """"""\n\n    _,_,h,w = x.size()\n    n,d     = z.size()\n    z_ = z.view(n,d,1,1).expand(-1,-1,h,w)\n\n    return torch.cat([x, z_], dim=1)'"
salad/layers/mat.py,28,"b'"""""" Metrics and Divergences for Correlation Matrices\n""""""\n\nimport torch\n\n#def cov(x):\n#    n, d  = x.size()\n#\n#    xm = x - x.mean(dim = 0, keepdim=True)\n#    C = 1. / (n - 1) * torch.mm(xm.transpose(1,0), x)\n#\n#    return C \n\ndef cov(x, eps = 1e-5):\n    """""" Estimate the covariance matrix \n    """"""\n    assert len(x.size()) == 2, x.size()\n    \n    N, d = x.size()\n    \n    reg = eps * x.new_tensor(torch.eye(d))\n    \n    x_ = x - x.mean(dim=0, keepdim = True)\n    return torch.einsum(\'ni,nj->ij\',(x_,x_)) / (N - 1) + reg\n\n#def logeig(M, eps = 1e-8):\n#    """""" Compute log transform on the eigenvalues of a matrix\n#    """""" \n#\n#    u,s,vh = torch.svd(M, some=True)\n#    Mlog = (u * torch.log(eps + s)).mm(vh.transpose(1,0))\n#\n#    return Mlog\n\ndef stable_logdet(A):\n    """""" Compute the logarithm of the determinant of matrix in a numerically stable way\n    """"""\n\n    G = torch.potrf(A, upper=False)\n    return 2*torch.log(torch.diag(G)).sum()\n    \n    #N = len(A)\n   # \n    #with torch.no_grad():\n    #    mu = abs(A).max()\n    \n    #scaledA = A / mu    \n    \n    #log_det_scale =  N * torch.log(mu)\n    \n    #detA    = torch.det(scaledA)\n    \n    #logdetA = torch.log(detA) + log_det_scale\n    \n    #return logdetA\n\n\ndef getdata(N,d,std):\n    x = torch.randn(N,d)\n    y = std * torch.randn(N,d)\n    \n    C = cov(x)\n    D = cov(y)\n    \n    return C, D\n\ndef apply(C, func):\n    e, v = torch.eig(C, eigenvectors=True)\n    reg = 0\n    with torch.no_grad():\n        reg = 1e-5 -  e[:,0].min()\n    e = func(e[:,0] + reg)\n    C_ = torch.einsum(\'id,d,jd->ij\', (v, e, v))\n    return C_\n\ndef logeuclid(A, B):\n\n    def log_mat(X):\n        N, d = X.size()\n        u,s,vh = torch.svd(X)\n        eigXtX = (s**2) / (N - 1)\n        logeig = torch.log(eigXtX)\n        #logX =   torch.einsum(\'id,d,jd->ij\', (vh,logeig,vh))\n        logX = vh.mm(torch.diag(logeig)).mm(vh.transpose(1,0))\n        return logX\n    \n    logA = log_mat(A)\n    logB = log_mat(B)\n\n    #logA = apply(A, torch.log)\n    #logB = apply(B, torch.log)\n    \n    return euclid(logA, logB)\n\ndef euclid(A, B):\n    \n    diff = (A - B) \n    return (diff * diff).sum() / (4 * len(A)**2)\n\ndef affineinvariant(A, B):\n    \n    Binv = torch.inverse(B + 1e-10 * A.new_tensor(torch.eye(len(A))))\n    return abs(A.mm(B)).mean()\n\ndef jeffrey(A, B):\n    \n    n = len(A)\n    \n    Ainv = torch.inverse(A)\n    Binv = torch.inverse(B)\n    \n    return 0.5 * torch.trace(Ainv.mm(B)) + 0.5 * torch.trace(Binv.mm(A)) - n\n\ndef stein(A, B):\n    \n    N = len(A)\n\n    reg = 1e-3 * A.new_tensor(torch.eye(N))\n    \n    arg_ApB = 0.5 * (A + B)  + reg\n    arg_AB  = A.mm(B) \n    \n    return stable_logdet(arg_ApB) - .5 * stable_logdet(arg_AB)\n\ndef riemann(A, B):\n    \n    B_nsqrt = apply(B, lambda x : x**(-.5))\n    \n    arg = B_nsqrt.mm(A).mm(B_nsqrt)\n    \n    return torch.log(arg - arg.min() + 1e-11).sum()\n\n'"
salad/layers/noise.py,7,"b""import torch\nfrom torch import nn\n\ndef get_permutation(n_features, p = .25):\n\n    A = torch.eye(n_features)\n\n    shuffle = torch.rand(n_features) < p\n\n    idc  = torch.arange(n_features).long()\n    perm = torch.randperm(shuffle.sum())\n\n    idc_shuffle = idc[shuffle][perm]\n\n    perm = torch.arange(n_features).long()\n    \n    perm[shuffle] = idc_shuffle\n    \n    A = torch.eye(n_features)\n    A = A[perm,:]\n    \n    return A\n    \n    #n = 0\n    #for i in idc:\n    #\n    #    if shuffle[int(i)]:\n    #        print(idc_shuffle[n])\n    #        n = n + 1\n    #    else:\n    #        print(i, 'orig')\n    \n\nclass FeatureRotation(nn.Module):\n    \n    def __init__(self, n_features, p = .25):\n        super().__init__()        \n        self.n_features = n_features\n        self.p = p\n        \n    def forward(self, x):\n        \n        if self.training:\n            W = x.new_tensor(get_permutation(self.n_features, self.p))\n            x = torch.einsum('ij,nipq->njpq', [W,x])\n        \n        return x"""
salad/layers/vat.py,3,"b'import h5py\nimport torch\nfrom torch import nn\n\nimport torch\nimport torch.nn.functional as F\n\nfrom . import KLDivWithLogits\n\ndef normalize_perturbation(d):\n    d_ = d.view(d.size()[0], -1)\n    eps = d.new_tensor(1e-12)\n    output = d / torch.sqrt(torch.max((d_**2).sum(dim = -1), eps)[0] )\n    return output\n\nclass VATLoss(nn.Module):\n\n    """""" Virtual Adversarial Training Loss function\n\n    Reference:\n    TODO\n    """"""\n\n    def __init__(self, model, radius=1):\n\n        super(VATLoss, self).__init__()\n        self.model  = model\n        self.radius = 1\n\n        self.loss_func_nll = KLDivWithLogits()\n\n    def forward(self, x, p):\n\n        x_adv    = self._pertub(x, p)\n        _, p_adv = self.model(x_adv)\n        loss     = self.loss_func_nll(p_adv, p.detach())\n\n        return loss\n\n    def _pertub(self, x, p):\n        eps = (torch.randn(size=x.size())).type(x.type())\n\n        eps = 1e-6 * normalize_perturbation(eps)\n        eps.requires_grad = True\n\n        eps_p = self.model(x + eps)[1]\n\n        loss  = self.loss_func_nll(eps_p, p.detach())\n        loss.backward()\n        eps_adv = eps.grad\n\n        eps_adv = normalize_perturbation(eps_adv)\n        x_adv = x + self.radius * eps_adv\n\n        return x_adv.detach()\n\nclass ConditionalEntropy(nn.Module):\n\n    """""" estimates the conditional cross entropy of the input\n\n    $$\n    \\frac{1}{n} \\sum_i \\sum_c p(y_i = c | x_i) \\log p(y_i = c | x_i)\n    $$\n\n    By default, will assume that samples are across the first and class probabilities\n    across the second dimension.\n    """"""\n\n    def forward(self, input):\n        p     = F.softmax(input, dim=1)\n        log_p = F.log_softmax(input, dim=1)\n\n        H = - (p * log_p).sum(dim=1).mean(dim=0)\n\n        return H'"
salad/models/__init__.py,0,b'from .gan import *\nfrom .vision.unet import *\nfrom .fan import *\nfrom .digits import *'
salad/models/base.py,0,b'import torch\nfrom torch import nn\n\nclass BaseModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n        self.features   = None\n        self.classifier = None \n\n    def forward(self):\n        pass\n\nclass ConditionalAdaptive(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n        self.features   = None\n        self.classifier = None \n\n    def forward(self):\n        pass'
salad/models/fan.py,2,"b'import torch\nimport torch.nn as nn\n\nclass ConditionalLayer(nn.Module):\n\n    def __init__(self):\n        super(ConditionalLayer, self).__init__()\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n    \nclass ConditionalBatchNorm(ConditionalLayer):\n    \n    def __init__(self, *args, n_domains = 0, bn_func = nn.BatchNorm2d, **kwargs):\n        \n        super(ConditionalBatchNorm, self).__init__()\n        \n        self.n_domains = n_domains\n        self.layers    = [bn_func(*args, **kwargs) for i in range(n_domains)]\n        \n    def _apply(self, fn): \n        super(ConditionalBatchNorm, self)._apply(fn)\n        for layer in self.layers:\n            layer._apply(fn)\n        \n    def parameters(self, d=0):\n        return self.layers[d].parameters()\n        \n    def forward(self, x, d):\n                \n        layer = self.layers[d]\n        return layer(x) \n\nclass ConditionalSequential(ConditionalLayer):\n\n    def __init__(self, *modules):\n\n        super(ConditionalSequential, self).__init__()\n        self.modulelist   = nn.ModuleList(modules)\n\n    def forward(self, x, *args):\n\n        for module in self.modulelist:\n            if isinstance(module, ConditionalLayer):\n                x = module(x, *args)\n            else:\n                x = module(x)\n        \n        return x\n\nclass FeatureAwareNorm2d(ConditionalLayer):\n    """""" Feature Aware Normalization\n    """"""\n\n    def __init__(self, in_x, in_z, norm_layer=""bn""):\n\n        super(FeatureAwareNorm2d, self).__init__()\n\n        # layers\n        self.mul_gate = nn.Sequential(\n            nn.Conv2d(in_z, in_z, 1),\n            nn.ReLU(),\n            nn.Conv2d(in_z, in_x, 1),\n            #nn.Tanh()\n        )\n        self.add_gate = nn.Sequential(\n            nn.Conv2d(in_z, in_z, 1),\n            nn.ReLU(),\n            nn.Conv2d(in_z, in_x, 1),\n        )\n\n        if norm_layer == ""bn"":\n            self.norm = nn.BatchNorm2d(in_x, momentum=0., affine=False)\n        elif norm_layer == ""in"":\n            self.norm = nn.InstanceNorm2d(in_x, momentum=0., affine=False)\n        else:\n            raise NotImplementedError()\n\n        # parameters\n        self.inp_channel = in_x\n\n    def forward(self, x, z = None):\n        if z is None:\n            x = self.norm(x)\n            return x\n        \n        gamma = self.mul_gate(z)\n        beta  = self.add_gate(z)\n        x = self.norm(x)\n        return torch.mul(x, gamma) + beta\n\n##### ---- Model Definition ---- #####\n\nclass FANModel(nn.Module):\n\n    def __init__(self):\n        super(FANModel, self).__init__()\n\n        def conv2d_3x3(inp,outp,pad=1):\n            return ConditionalSequential(\n                nn.Conv2d(inp,outp,kernel_size=3,padding=pad),\n                FeatureAwareNorm2d(outp, 128),\n                nn.ReLU()\n            )\n\n        def conv2d_1x1(inp,outp):\n            return ConditionalSequential(\n                nn.Conv2d(inp,outp,kernel_size=1,padding=0),\n                FeatureAwareNorm2d(outp, 128),\n                nn.ReLU()\n            )\n\n        def block(inp,outp):\n            return ConditionalSequential(\n                conv2d_3x3(inp,outp),\n                conv2d_3x3(outp,outp),\n                conv2d_3x3(outp,outp)\n            )\n\n        self.features = ConditionalSequential(\n            block(3,128),\n            nn.MaxPool2d(2, 2, padding=0),\n            nn.Dropout2d(p=0.5),\n            block(128,256),\n            nn.MaxPool2d(2, 2, padding=0),\n            nn.Dropout2d(p=0.5),\n            conv2d_3x3(256, 512, pad=0),\n            conv2d_1x1(512, 256),\n            conv2d_1x1(256, 128),\n            nn.AvgPool2d(6, 6, padding=0)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(128, 10)\n        )\n        \n    def forward(self, x):\n\n        phi  = self.features(x, None)\n        phi = phi.view(-1,128)\n        y = self.classifier(phi)\n\n        return phi, y\n\nclass ConditionalModel(nn.Module):\n\n    def __init__(self, n_domains):\n        super(ConditionalModel, self).__init__()\n\n        self.n_domains = n_domains\n\n        self.conditional_layers = []\n\n        def bn(n_features):\n            layer = ConditionalBatchNorm(n_features, n_domains=self.n_domains)\n            self.conditional_layers.append(layer)\n            return layer\n\n        def conv2d_3x3(inp,outp,pad=1):\n            return ConditionalSequential(\n                nn.Conv2d(inp,outp,kernel_size=3,padding=pad),\n                bn(outp),\n                nn.ReLU()\n            )\n\n        def conv2d_1x1(inp,outp):\n            return ConditionalSequential(\n                nn.Conv2d(inp,outp,kernel_size=1,padding=0),\n                bn(outp),\n                nn.ReLU()\n            )\n\n        def block(inp,outp):\n            return ConditionalSequential(\n                conv2d_3x3(inp,outp),\n                conv2d_3x3(outp,outp),\n                conv2d_3x3(outp,outp)\n            )\n\n        self.features = ConditionalSequential(\n            nn.InstanceNorm2d(3, affine=False,\n                momentum=0,\n                track_running_stats=False),\n            block(3,128),\n            nn.MaxPool2d(2, 2, padding=0),\n            nn.Dropout2d(p=0.5),\n            block(128,256),\n            nn.MaxPool2d(2, 2, padding=0),\n            nn.Dropout2d(p=0.5),\n            conv2d_3x3(256, 512, pad=0),\n            conv2d_1x1(512, 256),\n            conv2d_1x1(256, 256),\n            nn.AvgPool2d(6, 6, padding=0)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(256, 10)\n        )\n        \n    def forward(self, x, d = 0):\n\n        phi  = self.features(x, d)\n        # print(phi.size())\n        phi = phi.view(-1,256)\n        y = self.classifier(phi)\n\n        return phi, y\n\n    def conditional_params(self, d=0):\n        for module in self.conditional_layers:\n            for p in module.parameters(d):\n                yield p\n\n    def parameters(self, d=0, yield_shared=True, yield_conditional=True):\n        \n        if yield_shared:\n            for param in super(ConditionalModel, self).parameters():\n                yield param\n        \n        if yield_conditional:\n            for param in self.conditional_params(d):\n                yield param\n        '"
salad/models/gan.py,5,"b'import torch\nfrom torch import nn\n\nimport torch.nn.functional as F\n\ndef to_one_hot(y, n_dims=None):\n    y_tensor = y.long().view(-1, 1)\n    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n    #y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).type(y.type()).scatter_(1, y_tensor, 1)\n    y_one_hot = y.new(y_tensor.size()[0], n_dims).zero_().scatter_(1, y_tensor, 1)\n    y_one_hot = y_one_hot.view(*y.shape, -1)\n    return y_one_hot\n\ndef cat2d(x, *args):\n\n    w = x.size()[2]\n    h = x.size()[3]\n\n    args = torch.cat(args, dim = 1)\n    if args.dim() == 2:\n        args = args.float().unsqueeze(2).unsqueeze(3)\n\n    return torch.cat([x, args.expand([-1,-1,w,h])], dim = 1)\n\nclass ConditionalGAN(nn.Module):\n    # initializers\n    def __init__(self, d=128, n_classes = 10, n_conditions = 2, n_outputs = 3):\n        super(ConditionalGAN, self).__init__()\n        self.deconv1 = nn.ConvTranspose2d(100 + n_classes + n_conditions, d*8, 4, 1, 0)\n        self.deconv1_bn = nn.BatchNorm2d(d*8)\n        self.deconv2 = nn.ConvTranspose2d(d*8 + n_classes + n_conditions, d*4, 4, 2, 1)\n        self.deconv2_bn = nn.BatchNorm2d(d*4)\n        self.deconv3 = nn.ConvTranspose2d(d*4 + n_classes + n_conditions, d*2, 4, 2, 1)\n        self.deconv3_bn = nn.BatchNorm2d(d*2)\n        self.deconv4 = nn.ConvTranspose2d(d*2 + n_classes + n_conditions, d, 4, 2, 1)\n        self.deconv4_bn = nn.BatchNorm2d(d)\n        self.deconv5 = nn.ConvTranspose2d(d + n_classes + n_conditions, n_outputs, 4, 2, 1)\n\n        self.n_classes = n_classes\n        self.n_conditions = n_conditions\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    def forward(self, input, label, condition):\n\n        # class is a (N x n_classes) tensor, 1-hot coding\n        # condition is a (N x n_conditions) tensor, 1-hot coding\n\n        label     = to_one_hot(label, self.n_classes)\n        condition = to_one_hot(condition, self.n_conditions)\n\n        #print(input.size(), label.size(), condition.size())\n\n        x = input\n        x = cat2d(x,label,condition)\n        x = F.relu(self.deconv1_bn(self.deconv1(x)))\n\n        x = cat2d(x,label,condition)\n        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n\n        x = cat2d(x,label,condition)\n        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n\n        x = cat2d(x,label,condition)\n        x = F.relu(self.deconv4_bn(self.deconv4(x)))\n\n        x = cat2d(x,label,condition)\n        x = 3 * F.tanh(self.deconv5(x))\n\n        return x\n\nclass Discriminator(nn.Module):\n    # initializers\n    def __init__(self, d=128, n_classes=1):\n        super(Discriminator, self).__init__()\n        self.conv1 = nn.Conv2d(3, d, 4, 2, 1)\n        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n        self.conv2_bn = nn.BatchNorm2d(d*2)\n        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n        self.conv3_bn = nn.BatchNorm2d(d*4)\n        self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n        self.conv4_bn = nn.BatchNorm2d(d*8)\n        self.conv5 = nn.Conv2d(d*8, n_classes, 4, 1, 0)\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    def forward(self, input):\n        x = F.leaky_relu(self.conv1(input), 0.2)\n        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n        x = self.conv5(x)\n\n        return x\n\ndef normal_init(m, mean, std):\n    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n        m.weight.data.normal_(mean, std)\n        m.bias.data.zero_()\n'"
salad/models/resnet.py,7,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\nfrom .digits.fan import ConditionalLayer\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\ndef convert_state_dict(model, statedict):\n    \n    import re\n\n    statedict_ = {}\n    replace = re.compile(r\'conditionals\\.[0-9]+\\.\')\n\n    n_domains = 2\n\n    for key in model.state_dict().keys():\n\n        if ""conditionals"" in key:\n\n            for domain in range(n_domains):\n                tgt = replace.sub(\'\', key)\n                #print(key, tgt)\n                statedict_[key] = statedict[tgt]\n\n        else:\n            statedict_[key] = nn.Parameter(statedict[key].clone())\n            \n    return statedict_\n\nclass ConditionalSequential(nn.Sequential):\n\n    def __init__(self, *modules):\n\n        super(ConditionalSequential, self).__init__(*modules)\n\n    def forward(self, x, *args):\n\n        for module in self._modules.values():\n            \n            if isinstance(module, ConditionalLayer):\n                x = module(x, *args)\n            else:\n                x = module(x)\n        \n        return x\n\nclass ConditionalBatchNorm(ConditionalLayer):\n    \n    def __init__(self, *args, n_domains = 1, bn_func = nn.BatchNorm2d, **kwargs):\n        \n        super(ConditionalBatchNorm, self).__init__()\n        \n        self.n_domains = n_domains\n        self.conditionals    = nn.ModuleList([bn_func(*args, **kwargs) for i in range(n_domains)])\n        \n    def _apply(self, fn): \n        super(ConditionalBatchNorm, self)._apply(fn)\n        for layer in self.conditionals:\n            layer._apply(fn)\n        \n    def parameters(self, d=0):\n        return self.conditionals[d].parameters()\n        \n    def forward(self, x, d):\n                \n        layer = self.conditionals[d]\n        return layer(x) \n\nclass ConditionalParamModel(ConditionalLayer):\n    \n    def __init__(self):\n        \n        super(ConditionalParamModel, self).__init__()\n        \n        self.conditional_layers = []\n        \n    def _make_batch_norm(self, *args, **kwargs):\n\n        bn = ConditionalBatchNorm(*args, n_domains=2, bn_func=nn.BatchNorm2d,**kwargs)\n        self.conditional_layers.append(bn)\n\n        return bn\n        \n    def conditional_params(self, d=0):\n        for module in self.conditional_layers:\n            for p in module.parameters(d):\n                yield p\n\n    def parameters(self, d=0):\n        if d == 0:\n            return super(ConditionalParamModel, self).parameters()\n        else:\n            return self.conditional_params(d)\n        \n        \n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(ConditionalParamModel):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1   = self._make_batch_norm(planes)\n        self.relu  = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2   = self._make_batch_norm(planes)\n        self.downsample = downsample\n        self.stride     = stride\n\n    def forward(self, x, d = 0):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out, d)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out, d)\n\n        if self.downsample is not None:\n            residual = self.downsample(x, d)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(ConditionalParamModel):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = self._make_batch_norm(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = self._make_batch_norm(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = self._make_batch_norm(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x, d):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out, d)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out, d)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out, d)\n\n        if self.downsample is not None:\n            residual = self.downsample(x, d)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(ConditionalParamModel):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = self._make_batch_norm(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)#, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = ConditionalSequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                self._make_batch_norm(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return ConditionalSequential(*layers)\n\n    def forward(self, x, d = 0):\n        x = self.conv1(x)\n        x = self.bn1(x, d)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x, d)\n        x = self.layer2(x, d)\n        x = self.layer3(x, d)\n        x = self.layer4(x, d)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n    \n    def load_pretrained(self, statedict):\n        \n        statedict_ = convert_state_dict(self, statedict)\n        self.load_state_dict(statedict_)\n        \n        def freeze(module):\n            for param in module.parameters():\n                param.requires_grad_(False)\n\n        def enable(module):\n            if isinstance(module, ConditionalBatchNorm):\n                for param in module.conditionals.parameters():\n                    param.requires_grad_(True)\n\n        self.apply(freeze)\n        self.apply(enable)\n        \n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_pretrained(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_pretrained(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_pretrained(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_pretrained(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_pretrained(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model'"
salad/models/transfer.py,0,b'from torchvision import models\n\n# TODO'
salad/models/utils.py,3,"b'from torch import nn\nimport torch\nfrom torchvision.models import resnet\n\n## General Helper Functions\n\ndef bn2linear(bn):\n    scale, shift = get_affine(bn)\n    \n    W_ = scale.view(-1,1,1,1)\n    b_ = shift\n    \n    n_channels = W_.size()[0]\n    \n    conv = nn.Conv2d(n_channels,n_channels,kernel_size=1,groups=n_channels)\n    with torch.no_grad():\n        conv.weight.set_(W_.float())\n        conv.bias.set_(b_.float())\n    return conv\n\ndef replace_bns(module):\n    for name, layer in module._modules.items(): \n        if isinstance(layer, nn.BatchNorm2d):\n            layer_ = bn2linear(layer)\n            module._modules[name] = layer_\n\ndef reinit_bns(module):\n    """""" \n    """"""\n\n    for name, layer in module._modules.items(): \n        if isinstance(layer, nn.BatchNorm2d):\n            with torch.no_grad():\n                scale, shift = get_affine(layer)\n\n                layer_ = nn.BatchNorm2d(layer.num_features, eps=layer.eps, momentum=layer.momentum, affine=True, track_running_stats=True)\n\n                layer_.weight.set_(scale.float())\n                layer_.bias.set_(shift.float())\n\n                module._modules[name] = layer_\n            \ndef get_affine(layer):\n    mu  = layer.running_mean.double()\n    var = layer.running_var.double()\n    W   = layer.weight.double()\n    b   = layer.bias.double()\n    eps = layer.eps\n    \n    inv_std = 1./(var + eps)**.5\n    \n    scale = inv_std * W\n    shift = -mu * scale + b\n    \n    return scale, shift\n\ndef convert_conv_bn(layer, bn):\n    W = layer.weight.double()\n\n    scale, shift = get_affine(bn)\n    out, inp,_,_ = W.size()\n    W_ = (W * scale.view(-1,1,1,1))\n    b_ = scale\n\n    layer_ = nn.Conv2d(inp, out, kernel_size=layer.kernel_size, stride=layer.stride, padding = layer.padding)\n    with torch.no_grad():\n        layer_.weight.set_(W_.float())\n        layer_.bias.set_(b_.float())\n\n    return layer_\n\n## Models\n\nclass FixedBottleneck(nn.Module):\n    \n    def __init__(self, conv, downsample):\n        \n        super().__init__()\n        \n        self.conv       = conv\n        self.downsample = downsample \n        self.relu       = nn.ReLU()\n        \n    def forward(self, x):\n        \n        if self.downsample:\n            return self.relu(self.conv(x) + self.downsample(x))\n        else:\n            return self.relu(self.conv(x) + x)\n\ndef FixedResnet(backbone):\n\n    """""" ResNet Variant where each batch norm layer is replaced by a linear transformation\n    """"""\n\n    backbone.double()\n    backbone.apply(replace_bns)   \n    return backbone\n\nclass CompressedResnet(nn.Module):\n\n    """""" ResNet Variant where the batch norm statistics are merged into the transformation\n    matrices\n    """"""\n    \n    def __init__(self, backbone):\n        \n        super().__init__()\n        \n        self.preprocessing = nn.Sequential(\n            convert_conv_bn(backbone.conv1, backbone.bn1),\n            nn.ReLU(),\n            backbone.maxpool\n        )\n        \n        self.layer1 = self._convert_layer(backbone.layer1)\n        self.layer2 = self._convert_layer(backbone.layer2)\n        self.layer3 = self._convert_layer(backbone.layer3)\n        self.layer4 = self._convert_layer(backbone.layer4)\n        \n        self.avgpool = backbone.avgpool\n        self.fc = backbone.fc\n    \n    def forward(self, x):\n        \n        x = self.preprocessing(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        \n        return x\n    \n    def _convert_layer(self, layer):\n        \n        return nn.Sequential(\n            *list(self._convert_bottleneck(b) for b in layer.children())\n        )\n        \n    def _convert_bottleneck(self, layer):\n        conv1 = convert_conv_bn(layer.conv1, layer.bn1)\n        conv2 = convert_conv_bn(layer.conv2, layer.bn2)\n        conv3 = convert_conv_bn(layer.conv3, layer.bn3)\n        down  = None\n        \n        if (\'downsample\' in list(i[0] for i in layer.named_children())):\n            down  = convert_conv_bn(layer.downsample[0], layer.downsample[1])\n        \n        convs = nn.Sequential(\n            conv1,\n            nn.ReLU(),\n            conv2,\n            nn.ReLU(),\n            conv3\n        )\n        \n        bottleneck = FixedBottleneck(convs, down)\n        \n        return bottleneck\n'"
salad/solver/__init__.py,0,"b'#__import__(\'pkg_resources\').declare_namespace(__name__)\n"""""" Model optimization by stochastic gradient descent and variants \nThe general structure:\n\n- Each experiment configuration is a subclass of Solver or some derivative\n  loss functions\n- Solvers only specify how data and models are used to generate the losses\n- Similarities between deep learning experiments (checkpointing, logging, ...)\n  are implemented in the Solver class.\n\nIn general, for many experiments, it makes sense to set up a solver as a\nsubclass of a specific other solver; i.e. when the general problem is concerned\nwith classifcation, a `CrossEntropySolver` would be a natural choice.\n\nClasses where designed with the possibility of re-use in mind. The goal is to\nexploit the particular structure most deep learning experiments share.\n\n""""""\n\nfrom .base import *\nfrom .classification import *\nfrom .gan import *\nfrom .da import *'"
salad/solver/base.py,8,"b'"""""" Base classes for solvers\n\nThis module contains abstract base classes for the solvers used in ``salad``.\n""""""\n\n\nimport os, time\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\n\nimport itertools\n\nfrom .. import layers, optim\n\nclass StructuredInit(object):\n    r"""""" Structured Initialization of Solvers\n\n    Initializes the components of a solver and passes arguments.\n    Initialization is done in the following order:\n\n    - ``_init_models``\n    - ``_init_losses``\n    - ``_init_optims``\n\n    Parameters\n    ----------\n\n    kwargs : Keyword arguments\n        Pass arguments for **all** initialization functions. Keyword arguments\n        are passed through the functions in the order specified above. Unused\n        keyword arguments will be printed afterwards.\n        In general, solvers should be designed in a way that ensure that all keyword\n        argumnets are used.\n\n    .. note:\n        Don\'t instantiate or subclass this class directly.\n    """"""\n\n    def __init__(self, **kwargs):\n        # TODO more elegant solution?\n\n        self._init_models(**kwargs) \n        self._init_losses(**kwargs)\n        self._init_optims(**kwargs)\n\n        # print(\'Unused kwargs in Solver: {}\'.format(\', \'.join(kwargs.keys())))\n\n    def _init_models(self, **kwargs):\n        return kwargs\n\n    def _init_losses(self, **kwargs):\n        return kwargs\n\n    def _init_optims(self, **kwargs):\n        return kwargs\n\nclass EventBasedSolver(object):\n    r"""""" Event handling for solvers\n\n    All solvers derived from the ``EventBasedSolver`` are extended with event handlers, currently\n    for the following events:\n\n    - ``start_epoch``\n    - ``start_batch``\n    - ``finish_batch``\n    - ``stop_epoch``\n\n    .. note:\n        Don\'t instantiate or subclass this class directly.\n\n    """"""\n\n    def __init__(self):\n\n        self.start_epoch_handlers  = []\n        self.finish_epoch_handlers = []\n        self.start_batch_handlers  = []\n        self.finish_batch_handlers = []\n    \n    def _call_eventhandler(self, handler, *args, **kwargs):\n        for h in handler:\n            h(*args, **kwargs)\n \n    def start_epoch(self, *args, **kwargs):\n        self._call_eventhandler(self.start_epoch_handlers, *args, **kwargs)\n\n    def finish_epoch(self, *args, **kwargs):\n        self._call_eventhandler(self.start_epoch_handlers, *args, **kwargs)\n\n    def start_batch(self, *args, **kwargs):\n        self._call_eventhandler(self.start_batch_handlers, *args, **kwargs)\n\n    def finish_batch(self, *args, **kwargs):\n        self._call_eventhandler(self.finish_batch_handlers, *args, **kwargs)\n\nclass Solver(EventBasedSolver, StructuredInit):\n\n    """""" General gradient descent solver for deep learning experiments\n\n    This is a helper class for training of PyTorch models that makes very little assumptions\n    about the structure of a deep learning experiment.\n    Solvers are generally constructed to take one or several models (`torch.nn.Module`) and *one*\n    DataLoader instance that provides a (possibly nested) tuple of examples for training.\n\n    The ``Solver`` implements the following features:\n\n    - Logging of losses and model checkpoints\n    \n    While offering these functionality in the background, this class implementation aims at being very\n    flexible when it comes to designing any kind of deep learning experiment.\n\n    When defining your own solver class, you should first\n\n    - register models [register_model] \n    - register loss functions [register_loss]\n    - register optimizers [register_optimizer]\n\n    The abstraction goes as follows:\n    \n    - An experiment is fully characterized by its Solver class\n    - An experiment can have multiple models\n    - Parameters of the models are processed by optimizers\n    - Optimizers have a functions to derive losses\n\n    In the optimization process, the following algorithm is used:\n\n    for opt in optimizers:\n       losses = L(opt)\n       grad_losses = grad(losses)\n       opt.step(grad_losses)\n\n\n    Parameters\n    ----------\n    dataset : Dataset\n        Dataset used for training\n    n_epochs : int\n        Number of epochs (defined as full passes through the ``DataLoader``)\n    savedir  : str\n        log directory for saving model checkpoints and the loss history\n    gpu      : int\n        Number of GPU to be used. If ``None``, use CPU training instead\n    dryrun   : bool\n        Train only for the first batch. Useful for testing a new solver\n\n    Notes\n    -----\n\n    After initializing all internal dictionaries, the constructor makes calls to the\n    ``_init_models``, \n    ``_init_optims`` and\n    ``_init_losses``\n    functions. If these functions should make use of any additional keyword arguments\n    you passed in your class, make sure that you initialize them prior to calling\n    ``super().__init__`` in your constructor. \n\n    """"""\n\n    def __init__(self, dataset, n_epochs=1, savedir=""./log"",\n                 gpu=None, dryrun=False, **kwargs):\n\n        EventBasedSolver.__init__(self)\n\n        self.timestamp      = time.strftime(""%Y%m%d-%H%M%S"")\n        self.savedir        = os.path.join(savedir, str(self))\n        self.n_epochs       = n_epochs\n        self.dataset        = dataset\n        self.dryrun         = dryrun\n        self.save_frequency = 1\n\n        os.makedirs(self.savedir, exist_ok=True)\n\n        self.to_cuda = (lambda x: x.cuda(gpu)) if (gpu is not None) else lambda x : x\n\n        self.models       = []\n        self.display_loss = []\n\n        # loss functions\n        self.loss_weights = {}\n        self.loss_funcs   = {}\n        self.loss_kwargs  = {}\n\n        # models\n        self.savenames    = {}\n\n        # optimizers\n        self.optims       = []\n        self.retain_graph = {}\n        self.agg_loss     = {}\n        self.optim_name   = {}\n        self.optim_steps  = {}\n\n\n\n        StructuredInit.__init__(self, **kwargs)\n\n    def cuda(self, obj):\n        """""" Move nested iterables between CUDA or CPU\n        """"""\n        if isinstance(obj, tuple) or isinstance(obj, list):\n            obj = [self.cuda(el) for el in obj]\n        else:\n            obj = self.to_cuda(obj)\n        return obj\n\n    def register_loss(self, func, weight = 1.,\n                      name=None, display = True, \n                      override = False, **kwargs):\n        """""" Register a new loss function\n\n        Parameters\n        ----------\n\n        func : \n            pass\n        weight : float\n\n        """"""\n\n        assert name is None or isinstance(name, str)\n\n        # TODO refactor, add checks\n\n        if isinstance(weight, int):\n            weight = float(weight)\n\n        assert weight is None or isinstance(weight, float)\n        \n        # TODO assert for torch Function\n        # assert isinstance(func, nn.Module)\n        if name is None:\n            name = \'unnamed_{}\'.format(len(self.loss_funcs)+1)\n\n        if isinstance(func, nn.Module):\n            self.cuda(func)\n\n        if name in self.loss_funcs:\n            if override:\n                if name in self.display_loss:\n                    self.display_loss.remove(name)\n\n            else:\n                raise ValueError(\'Name {} for loss func {} already taken.\'.format(\n                    name, self.loss_funcs[name].__class__.__name__\n                )\n                    + \' Call register_loss with the override=True option if this was intended.\'\n                )\n\n        self.loss_funcs[name]   = func\n        if weight is not None:\n            self.loss_weights[name] = weight\n        self.loss_kwargs[name]  = kwargs\n\n        if display:\n            self.display_loss.append(name)\n\n        print(\'Registered {} as ""{}"" with weight {}\'.format(\n            func.__class__.__name__, name, weight))\n    \n    def remove_loss(self, name):\n        buffers = [\n            self.loss_funcs,\n            self.loss_weights,\n            self.loss_kwargs, \n            self.display_loss\n        ]\n\n        for b in buffers:\n            if name in b:\n                if isinstance(b, list):\n                    b.remove(name)\n                else:\n                    del(b[name])\n\n    def register_model(self, model, name=""""):\n\n        """""" Add a model to the solver\n\n        This method will also move the model directly to the correct device you specified \n        when constructin the solver.\n\n        Parameters\n        ----------\n        model     : torch.nn.Module\n            The model to be optimized. Should return a non-empty iterable when the\n            ``paramters()`` method is called\n        name      : str, optional\n            Name for the model. Useful for checkpoints when multiple models are optimized.\n        """"""\n\n        assert isinstance(model, nn.Module)\n\n        self.cuda(model)\n        self.savenames[model] = name\n\n        print(\'Registered {} as ""{}""\'.format(model.__class__.__name__, name))\n\n    def register_optimizer(self, optimizer, loss_func, retain_graph = False, name="""", n_steps = 1):\n        """""" Add an optimizer to the solver\n\n        Parameters\n        ----------\n        optimizer : Optimizer\n            A function used for updating model weights during training\n        loss_func : LossFunction\n            A function (or callable object) that, given the current batch passed by the Solver\'s data\n            loader, returns a dictionary containing either a dictionary mapping loss function names\n            to arguments, or a dictionary mapping loss function names to the loss.\n        retain_graph : bool\n            ``True`` if the computational graph should be retained after calling the loss function\n            associated to the optimizer. This is usually not needed.\n        name : str\n            Optimizer name. Useful for logging\n        n_steps : int\n            Number of consecutive steps the optimizer is exectued. Usually set to 1.\n        """"""\n\n\n        self.optims.append(optimizer)\n        self.retain_graph[optimizer] = retain_graph\n        self.agg_loss[optimizer]   = loss_func\n        self.optim_name[optimizer]   = name\n        self.optim_steps[optimizer]   = n_steps\n\n        print(\'Registered {} as ""{}""\'.format(optimizer.__class__.__name__, name))\n\n\n    def format_train_report(self, losses):\n\n        loss_str = [\'{:.3f}\'.format(float(losses[-1][key])) for key in self.display_loss]\n        return \'; \'.join(loss_str)\n\n    def format_summary_report(self, losses):\n\n        loss_str = [\'{:.3f}\'.format(losses[key].mean()) for key in self.display_loss]\n        return \'; \'.join(loss_str)\n\n    def compute_loss_dict(self, loss_args):\n        loss_dict = {}\n        for n, args in loss_args.items():\n            try:\n                if isinstance(args, tuple) or isinstance(args, list):\n                    loss_dict[n] = self.loss_funcs[n](*args)\n                elif isinstance(args, torch.Tensor):\n                    loss_dict[n] = args\n                else:\n                    raise ValueError(\'Loss args for {} have wrong type. Found {}, expected iterable or tensor\'.format(\n                        n, type(args)\n                    ))\n            except Exception as e:\n                print(\'Error in resolving: {}\'.format(n))\n                raise e\n        return loss_dict\n\n    def _loss(self, optim, batch):\n        """""" Compute loss functions for a particular model\n        """"""\n\n        # retrieve the registered loss\n        loss_args = self.agg_loss[optim](batch)\n        loss_dict = self.compute_loss_dict(loss_args)\n\n        # TODO make normalization optional?\n        keys = set(self.loss_weights.keys()).intersection(set(loss_dict.keys()))\n        if len(keys) > 0:\n            # NOTE losses are normalized by their average absolute weight.\n            # TODO make this optional in the future? Might make it harder to match hyperparamters to reference\n            # implementations\n            norm = sum( abs(self.loss_weights[k]) for k in keys)\n            loss = sum(loss_dict[k]*self.loss_weights[k]\n                       for k in keys) / norm\n        else:\n            loss = torch.tensor(0)\n\n        return loss, loss_dict \n\n    def _step(self, batch):\n        total_losses = {}\n\n        # loop through models and optimize\n        for n_optim, optimizer in enumerate(self.optims):\n            last_pass = (n_optim == len(self.optims) - 1)\n\n            for _ in range(self.optim_steps[optimizer]):\n                loss, loss_dict = self._loss(optimizer, batch)\n\n                optimizer.zero_grad()\n                if loss.requires_grad:\n                    loss.backward(retain_graph=(not last_pass) and\n                                                self.retain_graph[optimizer])\n                optimizer.step()\n\n            total_losses.update(\n                {n : float(l.data.cpu().numpy()) for n,l in loss_dict.items()}\n            )\n\n        return total_losses\n\n    def _epoch(self, epoch):\n        self.start_epoch()\n\n        for model in self.models:\n            self.cuda(model)\n\n        losses = []\n        n_batches = len(self.dataset)\n\n        if n_batches == 0:\n            raise ValueError(""No Data in DataLoader!"")\n\n        tic = time.time()\n        pbar = tqdm(enumerate(self.dataset), total=n_batches)\n        for batch_idx, batch in pbar:\n            self.start_batch()\n            try:\n                batch = self.cuda(batch)\n                total_losses = self._step(batch)\n                total_losses.update({\'epoch\' : int(epoch), \'batch\' : int(1+batch_idx)})\n                losses.append(total_losses)\n\n                pbar.set_description(self.format_train_report(losses))\n\n                if self.dryrun:\n                    return losses, True\n\n            except KeyboardInterrupt:\n                print(""Training was interrupted. Finishing training"")\n                return losses, True\n\n            self.finish_batch()\n\n        self.finish_epoch()\n\n        return losses, False\n\n    def optimize(self):\n\n        """""" Start the optimization process\n\n        Notes\n        -----\n        Prior to the optimization process, all models will be set to training\n        mode by a call to ``model.train(True)``\n\n        """"""\n\n        for model in self.models:\n            model.train(True)\n\n        losses = []\n        pbar = tqdm(range(self.n_epochs))\n        for epoch in pbar:\n            history, terminate = self._epoch(epoch)\n\n            if epoch % self.save_frequency == 0:\n                self._save_models(epoch)\n\n            if history is not None: losses += history\n            df_losses = self._save_history(epoch, losses)\n\n            # TODO do for all losses\n            epoch_losses = df_losses[df_losses[\'epoch\'] == epoch]\n            pbar.set_description(\'Average Loss: {}\'.format(\n                self.format_summary_report(epoch_losses)))\n\n            if terminate:\n                break\n\n    def _save_history(self, epoch, losses):\n        fname = \'{}-losshistory-ep{}.csv\'.format(self.timestamp, epoch)\n        fname = os.path.join(self.savedir, fname)\n        df_losses = pd.DataFrame.from_dict({i:l for i, l in enumerate(losses)}).T\n        df_losses.to_csv(fname)\n        df_losses.to_csv(os.path.join(self.savedir, \'losshistory.csv\'))\n\n        return df_losses\n\n    def _save_models(self, epoch):\n\n        fname = \'{}-checkpoint-ep{}.pth\'.format(self.timestamp, epoch)\n        fname = os.path.join(self.savedir, fname)\n        print(""Save model checkpoint after {} epochs to {}"".format(epoch, fname))\n        torch.save(self.model, fname)\n        torch.save(self.model, os.path.join(self.savedir, ""checkpoint.pth""))\n\n\n    def __str__(self):\n        return \'{}_{}\'.format(self.timestamp, self.__class__.__name__)\n'"
salad/solver/classification.py,8,"b'from .base import Solver\n\nimport os, time\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\n\nfrom salad.layers import MeanAccuracyScore\n\nimport itertools\n\nfrom .. import layers, optim\n\nclass ClassificationLoss(object):\n\n    def __init__(self, solver):\n        self.solver = solver\n\n    def __call__(self, batch):\n        losses = {}\n        x, y = batch\n\n        _ , y_ = self.solver.model(x)\n        if not self.solver.multiclass:\n            y_ = y_.squeeze()\n            y  = y.float()\n\n        losses[\'acc\'] = losses[\'mean_acc\'] = losses[\'ce\'] = (y_, y)\n\n        return losses\n\nclass MultiDomainClassificationLoss(object):\n\n    def __init__(self, solver, domain):\n\n        self.domain     = domain\n        self.solver     = solver\n\n    def __call__(self, batch):\n        losses = {}\n\n        (x, y)  = batch[self.domain]\n\n        _ , y_ = self.solver.model(x, self.domain)\n\n        if not self.solver.multiclass:\n            y_ = y_.squeeze()\n            y  = y.float()\n        \n        losses[\'CE_{}\'.format(self.domain)] = (y_, y)\n        losses[\'ACC_{}\'.format(self.domain)] = (y_, y)\n\n        return losses\n\nclass BaseClassSolver(Solver):\n\n    """""" Base Solver for classification experiments\n\n    Parameters\n    ----------\n\n    model : nn.Module\n        A model to train on a classification target\n    dataset : torch.utils.data.Dataset\n        The dataset providing training samples\n    multiclass : bool\n        If True, ``CrossEntropyLoss`` is used, ``BCEWithLogitsLoss`` otherwise.\n    """"""\n\n    def __init__(self, model, dataset, multiclass = True, *args, **kwargs):\n\n        self.model      = model\n        self.multiclass = multiclass\n        \n        super().__init__(dataset=dataset, *args, **kwargs)\n\n    def _init_losses(self, **kwargs):\n        self.register_loss(nn.CrossEntropyLoss() if self.multiclass else nn.BCEWithLogitsLoss(),\n                           weight = 1,\n                           name   = \'ce\')        \n\n    def _init_models(self, **kwargs):\n        self.register_model(self.model, ""classifier"")\n\nclass FinetuneSolver(BaseClassSolver):\n    """""" Finetune a pre-trained deep learning models\n\n    Given a model with separable feature extractor and classifier, use different learning\n    rates and regularization settings. Useful for fine-tuning pre-trained ImageNet models\n    or finetuning of saved model checkpoints\n\n    Parameters\n    ----------\n    model : nn.Module\n        Module with two separate parts\n    dataset : Dataset\n        The dataset used for training\n    """"""\n\n    def __init__(self, *args, **kwargs):\n\n        super().__init__(*args, **kwargs)\n\n    def _init_optims(self, **kwargs):\n        optim_finetune  = torch.optim.Adam(self.model.features.parameters(),\n                            lr=1e-5, amsgrad=True)\n        optim_classifier = torch.optim.Adam(self.model.classifier.parameters(),\n                            lr=1e-4, amsgrad=True)\n        optim_joint = optim.JointOptimizer(optim_finetune, optim_classifier)\n\n        self.register_optimizer(optim_joint, ClassificationLoss(self),\n                                    ""Joint Optimizer"")\n\n\nclass BCESolver(BaseClassSolver):\n\n    """""" Solver for a classification experiment\n    """"""\n\n    def __init__(self, *args, **kwargs):\n\n        super(BCESolver, self).__init__(*args, **kwargs)\n\n    def _init_optims(self, lr = 3e-4, **kwargs):\n        super()._init_optims(**kwargs)\n\n        self.register_optimizer(torch.optim.Adam(self.model.parameters(),\n                                lr=lr, amsgrad=True),\n                                ClassificationLoss(self))\n\n    def _init_losses(self, **kwargs):\n        super()._init_losses(**kwargs)\n\n        self.register_loss(layers.AccuracyScore(), None, \'acc\')\n        self.register_loss(layers.MeanAccuracyScore(), None, \'mean_acc\')\n\n\nclass MultidomainBCESolver(Solver):\n\n    def __init__(self, model, dataset, learningrate, multiclass = True,\n                 loss_weights = None, *args, **kwargs):\n\n        super(MultidomainBCESolver, self).__init__(dataset=dataset,\n                                                   *args, **kwargs)\n\n        self.model      = model\n        self.multiclass = multiclass\n        self.n_domains  = model.n_domains\n        weights         = self.cuda(torch.tensor(loss_weights).float())\n\n        self.register_model(self.model, ""domain"")\n\n        for d in range(self.n_domains):\n\n            loss_func = nn.CrossEntropyLoss(weight=weights) if multiclass else nn.BCEWithLogitsLoss()\n            self.register_loss( loss_func,\n                                weight = 1,\n                                name   = \'CE_{}\'.format(d))\n            self.register_loss( MeanAccuracyScore(),\n                    weight = None,\n                    name   = \'ACC_{}\'.format(d))\n\n            # Specify the optimizer\n            self.register_optimizer(torch.optim.Adam(self.model.parameters(d),\n                                        lr=kwargs.get(\'learningrate\', learningrate),\n                                        amsgrad=True),\n                                        MultiDomainClassificationLoss(self, domain=d)\n                                    )'"
salad/solver/gan.py,9,"b'"""""" Tools for training Generative Adversarial Networks (GANs)\n\nThe class is primarily used to train conditional networks (CGANs).\n\nNotes\n-----\n\nContributions for extensions wanted!\n""""""\n\nfrom .base import Solver\n\nimport torch\nfrom torch import nn\n\nfrom ..models import ConditionalGAN, Discriminator\n\nclass GANSolver(Solver):\n\n    # TODO: implement a more general super class\n    pass\n\n\nclass CGANLoss():\n    """""" Loss Derivation for a Conditional GAN\n    """"""\n\n    def __init__(self, solver, G, Ds, train_G):\n\n        self.solver = solver\n        self.cuda = self.solver.cuda\n\n        self.G = G\n        self.Ds = Ds\n        self.train_G = train_G\n\n    def _derive_D(self, batch):\n        losses = {}\n\n        real_, fake_, y_, c_, x_, x_gen = batch\n\n        reals = [real_, y_, c_]\n        fakes = [fake_, fake_.long(), fake_.long()]\n\n        return {\n            \'{} real\'.format(name) : (D(x_).squeeze(), real),\n            \'{} fake\'.format(name) : (D(x_gen).squeeze(), fake)\n        }\n        \n        return losses\n\n    def _derive_G(self, batch):\n        real_, y_, c_, x_gen = batch\n\n        lbl = [real_, y_, c_]\n        losses = {}\n        for y, (name, D) in zip(lbl, self.Ds):\n            losses[\'G \'+name] = (D(x_gen).squeeze(), y)\n        return losses\n\n\n    def derive_losses(self, batch):\n        \n        self.train_G = not self.train_G\n\n        (x_clean, y_clean), (x_noise, y_noise) = batch\n\n        # compose the full batches\n        x_ =     torch.cat([x_clean, x_noise], dim=0)\n        y_ = 1 + torch.cat([y_clean, y_noise], dim=0).long()\n        c_ = 1 + torch.cat([torch.zeros(x_clean.size()[0]),\n                            torch.ones (x_noise.size()[0])], dim=0).long()\n        c_ = self.cuda(c_)\n\n        mini_batch = x_.size()[0]\n\n        # TODO needs to get a pre-computed batch for efficiency\n\n        fake_ = self.cuda(torch.zeros(mini_batch))\n        real_ = self.cuda(torch.ones(mini_batch))\n\n        z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n        z_ = self.cuda(z_)\n        x_gen = self.G(z_, y_ - 1, c_ - 1)\n\n        if self.train_G:\n            batch = real_, y_, c_, x_gen\n            return self._derive_G(batch)\n        else:\n            batch = real_, fake_, y_, c_, x_, x_gen\n            return self._derive_D(batch)\n\nclass ConditionalGANSolver(Solver):\n\n    """""" Train a class conditional GAN model\n\n\n    """"""\n\n    names     = [\'D_GAN\', \'D_CL\', \'D_CON\']\n    n_classes = [1, 11, 3]\n\n    def __init__(self, model, dataset, learningrate=0.0002, *args, **kwargs):\n\n        super(ConditionalGANSolver, self).__init__(dataset=dataset, *args, **kwargs)\n\n        self.model    = model\n        self.train_G  = True\n\n        self._init_models() \n        self._init_losses()\n        self._init_optims()\n\n    def _init_losses(self, **kwargs):\n        for cl, name in zip(self.n_classes, self.names):\n            loss_func = nn.BCEWithLogitsLoss() if cl == 1 else nn.CrossEntropyLoss()\n\n            # D loss\n            self.register_loss(loss_func, weight=1, name=\'{} real\'.format(name))\n            self.register_loss(loss_func, weight=1, name=\'{} fake\'.format(name))\n            \n            # G loss\n            self.register_loss(loss_func,weight = 1, name = \'G \'+ name)\n\n    def _init_models(self, **kwargs):\n        self.register_model(self.model, \'generator\')\n        self.discriminators = []\n        for cl, name in zip(self.n_classes, self.names):\n            D = Discriminator(128, n_classes=cl)\n            D.weight_init(mean=0.0, std=0.02)\n            self.register_model(D, name)\n\n    def _init_optims(self, lr = 2e-4, beta1 = .5, beta2 = .999, **kwargs):\n        opt = torch.optim.Adam(self.model.parameters(),lr = lr, betas = (beta1, beta2))\n        self.register_optimizer(opt, CGANLoss(), name=""Generator"")\n\n        for D in self.discriminators:\n            optim = torch.optim.Adam(D.parameters(), lr = lr, betas = (beta1, beta2))\n            self.register_optim(optim, CGANLoss())\n    \n    def format_train_report(self, losses):\n                \n        if len(losses) < 2:\n            return """"\n        \n        l = dict(losses[-2])\n        l.update(losses[-1])\n        \n        return Solver.format_train_report(self, [l])'"
salad/solver/openset.py,4,"b'"""""" Routines for open set classification """"""\n\nimport torch \nfrom torch import nn\n\nfrom .da import AdversarialLoss, BaseClassSolver\nfrom .classification import BaseClassSolver\n\nclass VADAOpenset(AdversarialLoss):\n\n    def __call__(self, batch):\n        # TODO improve instead of overriding\n        \n        (src_x, src_y, src_set), (trg_x, trg_y___, trg_set___) = batch\n\n        src_e, src_p, src_t = self.G(src_x)\n        trg_e, trg_p, trg_t = self.G(trg_x)\n\n        # Compute outputs\n        real_logit   = self.D(src_e)\n        fake_logit   = self.D(trg_e)\n\n        if self.train_G:\n            return {\n            \'ce\'           : (src_p, src_y),\n            \'ce_set\'       : (src_t, src_set),\n\n            \'CL_src\'       : (real_logit, torch.zeros_like(real_logit)),\n            \'CL_tgt\'       : (fake_logit, torch.ones_like(fake_logit)),\n            \'VAT_src\'      : (src_x, src_p),\n            \'VAT_tgt\'      : (trg_x, trg_p),\n\n            \'H_tgt\'        : (trg_p,),\n            \'H_set\'        : (trg_t,),\n\n            \'acc_s\'        : (src_p, src_y),\n            \'acc_t\'        : (trg_p, trg_y___),\n            \'acc_s_set\'    : (trg_p, src_set),\n            \'acc_t_set\'    : (trg_p, trg_y___)\n            }\n        else:\n            return {\n            \'D_src\'    : (real_logit, torch.ones_like(real_logit)),\n            \'D_tgt\'    : (fake_logit, torch.zeros_like(fake_logit))\n            }\n\nclass BaseOpensetSolver(BaseClassSolver):\n\n    def __init__(self, model, dataset, multiclass = True, *args, **kwargs):\n\n        super().__init__(dataset=dataset, *args, **kwargs)\n\n        self.model      = model\n        self.multiclass = multiclass\n\n        # Specify the loss functions\n        self.register_loss(nn.CrossEntropyLoss() if multiclass else nn.BCEWithLogitsLoss(),\n                           weight = 1,\n                           name   = \'ce\')        \n\n        # Specify the optimizer\n        self.register_model(self.model, ""classifier"")'"
salad/utils/__init__.py,0,"b""__import__('pkg_resources').declare_namespace(__name__)\n\nfrom .base import *"""
salad/utils/adapt.py,4,"b'\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torchvision.models import resnet\nfrom torchvision import datasets\n\nclass ParamCollect():\n    def __init__(self, ltype):\n        self.ltype = ltype\n        self.reset()\n    \n    def reset(self):\n        self.means = []\n        self.stds  = []\n    \n    def __call__(self, x):\n        if isinstance(x, self.ltype):\n            with torch.no_grad():\n                self.means.append(x.running_mean.cpu().numpy())\n                self.stds.append(x.running_var.cpu().numpy())\n\ndef adapt_stats(model, stages, dataloader):  \n    \n    def reset(layer):\n        if isinstance(layer, nn.BatchNorm2d):\n            layer.running_mean.zero_()\n            layer.running_var.fill_(1)\n        \n    \n    def ema_fn(n):\n        \n        def apply(layer):\n            if isinstance(layer, nn.BatchNorm2d):\n                layer.momentum = float(1/(n+1))\n                \n        return apply\n    \n    with torch.no_grad():\n        for stage in tqdm.tqdm(stages, position=0):\n            stage.apply(reset)\n            stage.train(True)\n            for n, batch in tqdm.tqdm(enumerate(dataloader), total=len(dataloader)):\n                stage.apply(ema_fn(n))\n                x = batch[0].cuda()\n                stage(x).cpu()\n            stage.eval()\n            Y = []\n            for batch in tqdm.tqdm(dataloader):\n                x = batch[0].cuda()\n                Y.append(stage(x).cpu().detach())\n                #break\n            Y = torch.cat(Y, dim=0)\n            dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Y),\n                                                     shuffle=True,\n                                                     batch_size=dataloader.batch_size)\n    return x'"
salad/utils/augment.py,19,"b'import numpy as np\nimport torch\nimport torch.nn.functional as F\nimport math\nfrom torch import nn\n\nclass RandomAffines():\n\n    def __init__(self,\n                 flip_x  = 0.5,\n                 flip_y  = 0.5,\n                 shear_x = (0,0.3),\n                 shear_y = (0,0.3),\n                 scale   = (0.8,1.4),\n                 rotate  = (-math.pi/2, math.pi),\n                 dx      = (-.2,.2),\n                 dy      = (-.2,.2)\n                ):\n\n        self.__dict__.update(locals())\n\n    def identify(self, size):\n         return torch.Tensor([1, 0, 0, 0, 1, 0, 0, 0, 1]).view(-1,9).repeat(size,1).float()\n\n    def reflect(self, size, p = .5):\n\n        cx = torch.from_numpy(0 + (np.random.uniform(0,1,size = size) > self.flip_x)).float()\n        cy = torch.from_numpy(0 + (np.random.uniform(0,1,size = size) > self.flip_y)).float()\n\n        A = self.identify(size)\n        A[:,0] = cx*2-1\n        A[:,4] = cy*2-1\n\n        return A\n\n    def shear(self, size, p = .5):\n\n        cx = torch.from_numpy(np.random.uniform(*self.shear_x,size = size)).float()\n        cy = torch.from_numpy(np.random.uniform(*self.shear_y,size = size)).float()\n\n        A = self.identify(size)\n        A[:,1] = cx\n        A[:,3] = cy\n\n        return A\n\n    def scaled(self, size, p = .5):\n\n        cx = torch.from_numpy(np.random.uniform(*self.scale,size = size)).float()\n        cy = torch.from_numpy(np.random.uniform(*self.scale,size = size)).float()\n\n        A = self.identify(size)\n        A[:,0] = cx\n        A[:,4] = cy\n\n        return A\n\n    def rotated(self, size, p = .5):\n\n        theta = torch.from_numpy(np.random.uniform(*self.rotate, size))\n\n        A = self.identify(size)\n        A[:,0] =  torch.cos(theta)\n        A[:,1] =  torch.sin(theta)\n        A[:,3] = -torch.sin(theta)\n        A[:,4] =  torch.cos(theta)\n\n        return A\n\n    def shift(self, size, p = .5):\n\n        dx    = torch.from_numpy(np.random.uniform(*self.dx, size))\n        dy    = torch.from_numpy(np.random.uniform(*self.dy, size))\n\n        A = self.identify(size)\n        A[:,2] =  dx\n        A[:,5] =  dy\n\n        return A\n\n    def matmul(self, A, B):\n\n        A = A.view(-1,3,3)\n        B = B.view(-1,3,3)\n\n\n        return torch.bmm(A, B).view(-1, 9)\n\n    def compose(self, size):\n\n        order = [self.reflect,\n                 self.rotated,\n                 self.shear,\n                 self.shift,\n                 self.scaled,\n\n                ]\n\n        A = self.identify(size)\n        for cmd in order:\n            B = cmd(size)\n\n            A = self.matmul(B, A)\n\n        return A[:,:6]\n\nclass AffineTransformer(nn.Module):\n\n    def __init__(self, *args, **kwargs):\n        \n        super(AffineTransformer,self).__init__()\n\n        self.affines = RandomAffines(*args, **kwargs)\n\n    def stn(self, x, theta):\n        theta = theta.view(-1,2,3)\n        grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, grid)\n\n        return x\n\n    def affine(self, y, theta):\n        theta = theta.view(-1,2,3)\n\n        B,C,_ = y.size()\n\n        y = torch.cat([y,torch.ones(B,C,1)], dim=-1)\n        y = torch.bmm(y, theta.transpose(2,1))\n\n        return y\n\n    def invert_affine(self, M):\n\n        """"""\n        Invert matrix for an affine transformation.\n        Supports batch inputs\n\n        M : Transformation matrices of shape (... x 6)\n\n        Output: Inverse transformation matrices of shape (... x 6)\n        """"""\n\n        a,b,x,c,d,y = [M[...,i] for i in range(6)]\n\n        D = a*d - b*c\n        E = a*y - x*c\n\n        a_ = 1/a * (1 + (c*b)/D)\n        b_ = - b/D\n        x_ = E/D * b/a - x/a\n        c_ = -c / D\n        d_ = a / D\n        y_ = - E / D\n\n        M_ = torch.stack([a_,b_,x_,c_,d_,y_], dim=-1)\n\n        return M_.float()\n\n    def __call__(self, x):\n\n        n_squeeze = 0\n        while x.dim() < 4:\n            x = x.unsqueeze(0)\n            n_squeeze += 1\n\n        B,C,H,W = x.size()\n\n        a = x.new_tensor([0,0])\n        b = x.new_tensor([H,W])\n\n        rescale = lambda x, a, b : (x - a)/(b - a) * 2 - 1\n        scale   = lambda x, a, b : (x + 1)/2 * (b - a) + a\n\n        Ay = x.new_tensor(self.affines.compose(B))\n\n        ###\n\n        Ax = self.invert_affine(Ay)\n        x_ = self.stn(x, Ax)\n\n        for _ in range(n_squeeze):\n            x_ = x_.squeeze(0)\n            y_ = y_.squeeze(0)\n\n        return x_'"
salad/utils/base.py,1,"b'import numpy as np\nimport os\nimport os.path as osp\n\ndef load_or_create(init_func, path):\n    if osp.exists(path):\n        print(""Resume from checkpoint file at {}"".format(path))\n        model = torch.load(path)\n    else:\n        model = init_func()\n\n    return model\n\ndef panelize(img):\n    if img.ndim == 1:\n        raise ValueError(""Invalid dimensions for image data"" + str(img.shape))\n    if img.ndim == 2:\n        return img\n    if img.ndim == 3:\n        return panelize(img[np.newaxis, :, :])\n\n    nb = img.shape[0]\n    nb_rows = int(nb ** 0.5)\n    psize = img.shape[2]\n    nb_channel = img.shape[1]\n\n    w, h = img.shape[-2:]\n\n    img_per_row = nb // nb_rows\n    rows = []\n    for j in range(nb_rows):\n        start = j * img_per_row\n        stop = min(start + img_per_row, nb)\n        rows.append(\n            np.hstack([img[j, :, :, :].reshape(nb_channel, w, h).transpose((1, 2, 0)) for j in range(start, stop)]))\n    return np.vstack(rows)\n'"
salad/utils/config.py,0,"b'"""""" Experiment Configurations for ``salad``\n\nThis file contains classes to easily configure experiments for different solvers\navailable in ``salad``.\n""""""\n\n\nimport sys\nimport argparse\n\nclass BaseConfig(argparse.ArgumentParser):\n\n    """""" Basic configuration with arguments for most deep learning experiments\n    """"""\n    \n    def __init__(self, description, log = \'./log\'):\n        super().__init__(description=description)\n\n        self.log = log\n        self._init()\n\n    def _init(self):\n\n        self.add_argument(\'--gpu\', default=0,\n            help=\'Specify GPU\', type=int)\n        self.add_argument(\'--cpu\', action=\'store_true\',\n            help=""Use CPU Training"")\n        self.add_argument(\'--njobs\', default=4,\n            help=\'Number of processes per dataloader\', type=int)\n        self.add_argument(\'--log\', default=self.log,\n            help=""Log directory. Will be created if non-existing"")\n        self.add_argument(\'--epochs\', default=""100"",\n            help=""Number of Epochs (Full passes through the unsupervised training set)"", type=int)\n        self.add_argument(\'--checkpoint\', default="""",\n            help=""Checkpoint path"")\n        self.add_argument(\'--learningrate\', default=1e-3, type=float,\n            help=""Learning rate for Adam. Defaults to Karpathy\'s constant ;-)"")\n        self.add_argument(\'--dryrun\', action=\'store_true\',\n            help=""Perform a test run, without actually training a network."")\n\n    def print_config(self):\n        print(""Start Experiments"")\n\n\nclass DomainAdaptConfig(BaseConfig):\n    """""" Base Configuration for Unsupervised Domain Adaptation Experiments\n    """"""\n\n    def _init(self):\n        super()._init()\n\n        self.add_argument(\'--source\', default=""svhn"", choices=[\'mnist\', \'svhn\', \'usps\', \'synth\', \'synth-small\'],\n                            help=""Source Dataset. Choose mnist or svhn"")\n        self.add_argument(\'--target\', default=""mnist"", choices=[\'mnist\', \'svhn\', \'usps\', \'synth\', \'synth-small\'],\n                            help=""Target Dataset. Choose mnist or svhn"")\n\n        self.add_argument(\'--sourcebatch\', default=128, type=int,\n                            help=""Batch size of Source"")\n        self.add_argument(\'--targetbatch\', default=128, type=int,\n                            help=""Batch size of Target"")'"
salad/utils/evaluate.py,1,"b'import numpy as np\nimport tqdm\nimport torch\n\ndef _predictions(model, data_loader, domain):\n    labels = []\n    preds  = []\n    feats  = []\n\n    for x,y in tqdm.tqdm(data_loader):\n        x = x.cuda()\n        y = y.cuda()\n\n        if domain is not None:\n            f,p = model(x, domain)\n        else: \n            f,p = model(x)\n\n        labels.append(y.cpu().numpy())\n        preds.append(p.detach().cpu().numpy())\n        feats.append(f.detach().cpu().numpy())\n\n    labels = np.concatenate(labels, axis=0)\n    preds = np.concatenate(preds, axis=0)\n    feats = np.concatenate(feats, axis=0)\n    \n    return labels, preds,feats\n\ndef evaluate(checkpoints, data_loader, domain):\n\n    labels, preds, feats = [], [], []\n    \n    for i, chk in enumerate(checkpoints):\n        \n        model = torch.load(chk, map_location=lambda storage, loc: storage)\n\n        model.cuda()\n        model.eval()\n\n        l,p,f = _predictions(model, data_loader, domain)\n        labels.append(l)\n        preds.append(p)\n        feats.append(f)\n        \n    labels = np.stack(labels, axis=0)\n    preds = np.stack(preds, axis=0)\n    feats = np.stack(feats, axis=0)\n    \n    return labels, preds, feats'"
salad/utils/finetune.py,3,"b""import torch\nfrom torch import nn\n\nimport pandas as pd\nimport numpy as np\nfrom salad.solver import BaseClassSolver\nfrom salad.solver.classification import MultidomainBCESolver\nfrom salad.datasets import load_dataset\n\nclass Loss():\n    \n    def __init__(self, model):\n        self.model = model\n    \n    def __call__(self, batch):\n        \n        x,y = batch\n        _,y_   = self.model(x, 0)\n        \n        return {'ce' : (y_,y)}\n\nclass FinetuneSolver(BaseClassSolver):\n    \n    def __init__(self, *args, **kwargs):\n\n        super(FinetuneSolver, self).__init__(*args, **kwargs)\n        \n        def parameters():\n            for p in self.model.features.modulelist[-2].parameters():\n                yield p\n            for p in self.model.classifier.parameters():\n                yield p\n\n        optim = torch.optim.Adam(parameters(), lr = 3e-4)\n\n        self.register_optimizer(optim, Loss(self.model),\n                                name='adam')\n\n        \nif __name__ == '__main__':\n\n    model = torch.load('log/log2/20180725-131833_MultidomainBCESolver/20180725-131833-checkpoint-ep300.pth')\n    model.cuda()\n\n    batch_size = 64\n\n    data = load_dataset('/tmp/data', train=True)\n    loader   = torch.utils.data.DataLoader(\n        data['mnist'], batch_size=batch_size,\n        shuffle=True, num_workers=4)\n\n    solver = FinetuneSolver(model, loader, savedir='/tmp/log', gpu = 0, n_epochs=100)\n\n    solver.optimize()"""
salad/datasets/da/__init__.py,0,b'from .base import *\nfrom .toy import *\nfrom .digits import *\nfrom .office31 import *'
salad/datasets/da/base.py,7,"b'from torch.utils.data import Dataset\n\nfrom torchvision import datasets, transforms\n\nimport h5py\nimport torch\nfrom torch import nn\n\nimport torch.nn.functional as F\n\nimport torch.utils.data\n\nclass JointLoader:\n\n    def __init__(self, *datasets, collate_fn = None):\n\n        self.datasets  = datasets\n        self.iterators = [None] * len(datasets)\n        self.collate_fn = collate_fn\n\n    def __len__(self):\n\n        return min([len(d) for d in self.datasets])\n\n    def __iter__(self):\n        for i, dataset in enumerate(self.datasets):\n            self.iterators[i] = dataset.__iter__()\n        return self\n\n    def __next__(self):\n        try:\n            items = []\n            for dataset in self.iterators:\n                items.append(dataset.__next__())\n        except StopIteration:\n            raise StopIteration\n\n        if self.collate_fn is not None:\n            items = self.collate_fn(items)\n\n        return items\n\n\nclass JointDataset(torch.utils.data.Dataset):\n\n    def __init__(self, *datasets):\n        super(JointDataset, self).__init__()\n\n        self.datasets = datasets\n\n    def __len__(self):\n\n        return min([len(d) for d in self.datasets])\n\n    def __getitem__(self, index):\n\n        return [ds[index] for ds in self.datasets]\n\n\nclass AugmentationDataset(Dataset):\n\n    def __init__(self, dataset, transforms, n_samples=2):\n\n        super(AugmentationDataset, self).__init__()\n\n        self.dataset = dataset\n        self.n_samples = n_samples\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, index):\n\n        x,y = self.dataset[index]\n\n        xs = [self.transforms(x) for _ in range(self.n_samples)]\n\n        return xs + [y,]\n\ndef concat_collate(batch):\n\n    X = torch.cat([b[0] for b in batch], dim=0)\n    Y = torch.cat([b[1] for b in batch], dim=0)\n    D = torch.cat([torch.zeros(b[0].size(0)).long() + n for n,b in enumerate(batch)], dim=0)\n\n    return X,Y,D\n\nclass MultiDomainLoader(JointLoader):\n\n    """""" Wrapper around Joint Loader for multi domain training\n    """"""\n\n    def __init__(self, *args, collate = \'stack\'):#, **kwargs):\n        assert collate in [\'stack\', \'cat\']\n\n        if collate == \'stack\':\n            collate_fn = None\n        elif collate == \'cat\':\n            collate_fn = concat_collate\n        else:\n            raise NotImplementedError\n\n        super().__init__(*args, collate_fn = collate_fn) #, **kwargs)\n\n### loader functions ###\ndef load_dataset(path, train=True, img_size = 32, expand = True):\n\n    """"""\n    .. deprecated\n\n        Deprecated\n    """"""\n\n    transform = [\n            transforms.Resize(img_size),\n            transforms.ColorJitter(.1, 1, .75, 0),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.2881,0.2881,0.2881)),\n    ]\n    if expand: transform.append(transforms.Lambda(lambda x : x.expand([3,-1,-1])))\n    transform = transforms.Compose(transform)\n    mnist = datasets.MNIST(path, train=train, download=True, transform=transform)\n\n    transform = transforms.Compose([\n            transforms.Resize(img_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.188508,    0.19058265,  0.18615675))\n    ])\n    svhn = datasets.SVHN(path, split=\'train\' if train else \'test\', download=True, transform=transform)\n\n    return {\'mnist\' : mnist, \'svhn\' : svhn}\n\n\ndef load_dataset2(path, train=True, img_size = 32, expand = True):\n\n    """"""\n    .. deprecated\n\n        Deprecated\n    """"""\n\n    transform = [\n            transforms.Resize(img_size),\n            #transforms.ColorJitter(.1, 1, .75, 0),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.13089988, 0.13089988, 0.13089988),\n                                 std =(0.28928825, 0.28928825, 0.28928825)),\n    ]\n    if expand: transform.append(transforms.Lambda(lambda x : x.expand([3,-1,-1])))\n    transform = transforms.Compose(transform)\n    mnist = datasets.MNIST(path, train=train, download=True, transform=transform)\n\n    transform = transforms.Compose([\n            transforms.Resize(img_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.43768448, 0.4437684,  0.4728041 ),\n                                 std= (0.19803017, 0.20101567, 0.19703583))\n    ])\n    svhn = datasets.SVHN(path, split=\'train\' if train else \'test\', download=True, transform=transform)\n\n    return {\'mnist\' : mnist, \'svhn\' : svhn}'"
salad/datasets/da/digits.py,2,"b'"""""" Dataset loader for digit experiments\n\nDigit datasets (MNIST, USPS, SVHN, Synth Digits) are standard benchmarks for unsupervised domain adaptation.\nIn addition to access to these datasets, this module provides a collection of other datasets useful for DA\nbased on digit datasets.\n\nDatasets are collections of single datasets and are subclasses of the `MultiDomainLoader`.\n""""""\n\nfrom ..digits import MNIST, SVHN, USPS, SynthSmall, Synth\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom .base import JointLoader, MultiDomainLoader\n\nfrom ..transforms.noise import Gaussian, SaltAndPepper\nfrom ..transforms.digits import default_transforms, default_normalization\n\nimport torch\nfrom torchvision import transforms\n\nfrom salad.datasets.transforms import Augmentation\n\n\nclass DigitsLoader(MultiDomainLoader):\n    r"""""" Digits dataset\n\n    Four domains available: SVHN, MNIST, SYNTH, USPS\n\n    Parameters\n    ----------\n\n    root : str\n        Root directory where dataset is available or should be downloaded to\n    keys : list of str\n        pass\n\n    See Also\n    --------\n    ``torch.utils.data.DataLoader``\n    """"""\n\n    _constructors = {\n        \'mnist\': MNIST,\n        \'svhn\': SVHN,\n        \'synth\': Synth,\n        \'synth-small\': SynthSmall,\n        \'usps\': USPS\n    }\n\n    def __init__(self, root, keys,\n                 split=\'train\', download=True,\n                 collate=\'stack\', normalize=False,\n                 augment={}, augment_func = Augmentation, batch_size=1,\n                 **kwargs):\n\n        assert split in [\'train\', \'test\']\n\n        self.datasets = {}\n        for key in keys:\n            T = default_transforms(key)\n            if normalize:\n                print(\'Normalize data\')\n                T.transforms.append(transforms.Normalize(*default_normalization(key)))\n            func = self._constructors[key]\n\n            self.datasets[key] = func(root=root, split=split, download=download, transform=T)\n\n            if key in augment.keys():\n                self.datasets[key] = augment_func(self.datasets[key], augment[key])\n\n        if isinstance(batch_size, int):\n            batch_size = [batch_size] * len(keys)\n\n        super().__init__(*[DataLoader(self.datasets[k], batch_size=b, **kwargs) for k, b in zip(keys, batch_size)],\n                         collate=collate\n                         )\n\n\nclass AugmentationLoader(MultiDomainLoader):\n\n    _constructors = {\n        \'mnist\': MNIST,\n        \'svhn\': SVHN,\n        \'synth\': Synth,\n        \'synth-small\': SynthSmall,\n        \'usps\': USPS\n    }\n\n    def __init__(self, root, dataset_name, transforms, split=\'train\', augment={}, download=True, collate=\'cat\', **kwargs):\n\n        data = []\n        for i, T in enumerate(transforms):\n            func = self._constructors[dataset_name]\n            ds = func(root, split=split, download=download, transform=T)\n            if i in augment.keys():\n                ds = Augmentation(ds, augment[i])\n            data.append(DataLoader(ds, **kwargs))\n\n        super().__init__(*data, collate=collate)\n\n\nclass NoiseLoader(AugmentationLoader):\n\n    eps = 1.\n\n    def __init__(self, root, key,\n                 noisemodels=[], normalize=True,\n                 **kwargs):\n\n\n        self.noisemodels = []\n        for noisemodel in noisemodels:\n            T = transforms.Compose([\n                transforms.ToTensor(),\n                noisemodel,\n            ])\n \n            if normalize:\n                print(\'Normalize data\')\n                T.transforms.append(transforms.Normalize(*default_normalization(key)))\n\n            self.noisemodels.append(T)\n\n        super().__init__(root, key, self.noisemodels, **kwargs)\n\n\n\nclass RotationLoader(AugmentationLoader):\n\n    eps = 1.\n\n    def __init__(self, root, dataset_name,\n                 angles=list(range(0, 90, 15)),\n                 normalize = False,\n                 **kwargs):\n\n        self.noisemodels = []\n\n        for i in angles:\n            T = transforms.Compose([\n                transforms.RandomRotation([i-self.eps, i+self.eps]),\n                transforms.ToTensor(),\n            ])\n \n            if normalize:\n                print(\'Normalize data\')\n                T.transforms.append(transforms.Normalize(*default_normalization(key)))\n\n            self.noisemodels.append(T)\n\n        super().__init__(root, dataset_name, self.noisemodels, **kwargs)\n\n\nclass LowToHighGaussian():\n    noisemodels = [.001, .025, .05, .075, .1, .15, .2, .25, .3]\n\n\nclass HighToLowGaussian():\n    noisemodels = [.3, .25, .2, .15, .1, .075, .05, .025, .001]\n\n\nclass LowToHighSaltPepper():\n    noisemodels = [\n        Gaussian(0, .001),\n        SaltAndPepper(.0025),\n        SaltAndPepper(.01),\n        SaltAndPepper(.05),\n        SaltAndPepper(.1),\n        SaltAndPepper(.15),\n        SaltAndPepper(.2),\n        SaltAndPepper(.25),\n        SaltAndPepper(.36),\n    ]\n\n\nclass HighToLowSaltPepper():\n    noisemodels = [\n        SaltAndPepper(.36),\n        SaltAndPepper(.25),\n        SaltAndPepper(.2),\n        SaltAndPepper(.15),\n        SaltAndPepper(.1),\n        SaltAndPepper(.05),\n        SaltAndPepper(.01),\n        SaltAndPepper(.0025),\n        Gaussian(0, .001)\n    ]\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, *kwargs)\n'"
salad/datasets/da/office31.py,1,"b'from torch import nn\nimport torch\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\n\nimport os\nfrom . import MultiDomainLoader\n\nclass OfficeDataset(Dataset):\n    \n    """""" Office-31 dataset\n    \n    The Office-31 dataset consists of three domains: Amazon product images,\n    real-world photos by a DSLR, and done with a lower resolution webcam.\n\n    Notes\n    -----\n\n    Download the data from [1]_ and use the archive content as-is.\n\n    References\n    ----------\n\n        [1].. https://drive.google.com/file/d/0B4IapRTv9pJ1WGZVd1VDMmhwdlE/view\n    """"""\n    \n    names = [""Amazon"", ""DSLR"", ""Webcam""]\n\n    def __init__(self, path, transform = None, target_transform = None):\n        \n        self.amazon = ImageFolder(os.path.join(path, \'amazon/images/\'), transform=transform, target_transform=target_transform)\n        self.dslr   = ImageFolder(os.path.join(path, \'dslr/images/\'), transform=transform, target_transform=target_transform)\n        self.webcam = ImageFolder(os.path.join(path, \'webcam/images/\'), transform=transform, target_transform=target_transform)\n        \n        self.datasets = [self.amazon, self.dslr, self.webcam]\n        \n        self._check()\n        \n    @property\n    def class_to_idx(self):\n        return self.amazon.class_to_idx\n        \n    def _check(self):\n        \n        for i in self.datasets:\n            for j in self.datasets:\n                assert(i.class_to_idx == j.class_to_idx)\n    \n    def __repr__(self):\n        \n        return  ""\\n\\n"".join([""OfficeDataset"", ""Amazon""+repr(self.amazon), ""DSLR""+repr(self.dslr), ""Webcam""+repr(self.webcam)])\n        \n        \nclass OfficeDataLoader(MultiDomainLoader):\n    \n    names = [""Amazon"", ""DSLR"", ""Webcam""]\n    _keys = [""amazon"", ""dslr"", ""webcam""]\n    \n    def __init__(self, path, normalize = False, **kwargs):\n        \n        dataset = OfficeDataset(path, transform = self.get_transform(normalize = normalize))\n        super().__init__(*[DataLoader(d, **kwargs) for d in dataset.datasets], collate=""stack"")\n    \n    def get_loader(self, key):\n        \n        i = self._keys.index(key)\n        return self.datasets[i]\n        \n\n    def get_transform(self, normalize):\n\n        T = [\n            transforms.Resize(224),\n            transforms.RandomCrop(224),\n            transforms.ToTensor()\n        ]\n            \n        if normalize is True:\n            T += [transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])]\n            \n        return transforms.Compose(T)\n            \n    \n'"
salad/datasets/da/toy.py,6,"b'"""""" Toy Datasets for domain adaptation experiments\n""""""\n\n# TODO needs work\nfrom .base import MultiDomainLoader\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom sklearn.datasets import make_moons\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\ndef noise_augment(x):\n    scale = x.new_tensor(torch.from_numpy(np.eye(2) + np.random.normal(0,.01,size=(2,2))))\n    bias  = x.new_tensor(torch.from_numpy(np.random.normal(0,.1,size=(2))))\n    \n    return x.mm(scale) + bias\n\ndef make_data(n_samples = 50000, n_domains = 2, plot=False, noisemodels = None, seed = None):\n    if noisemodels is None:\n        noisemodels = []\n\n        angles = np.linspace(0,np.pi/5,n_domains)\n        for _ in range(n_domains):\n            #scale = np.eye(2) + np.random.normal(0,.1,size=(2,2))\n            a = angles[_]\n            scale = np.array([[ np.cos(a), np.sin(a)],\n                              [-np.sin(a), np.cos(a)]])\n\n            bias  = 0 #np.random.normal(0,.5,size=(1,2))\n        noisemodels.append(lambda x : x.dot(scale) + bias)\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    n_total = n_samples * n_domains\n    X, y = make_moons(n_samples=n_total, shuffle=True, noise=.1)\n    X = X.reshape(n_domains, n_samples, 2)\n    y = y.reshape(n_domains, n_samples)\n\n    for domain, noise in enumerate(noisemodels):\n        X[domain] = noise(X[domain])\n\n    Xs = torch.from_numpy(X).float()\n    ys = torch.from_numpy(y).float()\n\n    return [ (X, y) for (X, y) in zip(Xs, ys)]\n\nclass ToyDatasetLoader(MultiDomainLoader):\n    """""" Digits dataset\n\n    Four domains available: SVHN, MNIST, SYNTH, USPS\n    """""" \n    \n    def __init__(self, seed = None, augment = False,\n                 n_domains = 2, download=True, noisemodels = None,\n                 collate = \'stack\', **kwargs):\n\n\n        domains = make_data(n_domains = n_domains, seed=seed, noisemodels = noisemodels)\n\n        Xt, yt = domains[0]\n\n        loaders = []\n\n        loaders.append(DataLoader(TensorDataset(Xt, yt.long()), **kwargs))\n\n        for (Xv, yv) in domains[1:]:\n\n            if augment:\n                noise = 0.01 * torch.randn(Xv.size())\n                loaders.append( DataLoader(TensorDataset(Xv, Xv + noise, yv.long()), **kwargs) )\n            else:\n                loaders.append( DataLoader(TensorDataset(Xv, yv.long()),**kwargs) )\n            \n        super().__init__(*loaders, collate = collate)'"
salad/datasets/digits/__init__.py,0,"b'"""""" Digits datasets used in domain adaptation\n""""""\n\nfrom .svhn import SVHN\nfrom .usps import USPS\nfrom .synth import Synth, SynthSmall\nfrom .mnist import MNIST'"
salad/datasets/digits/base.py,1,"b'import gzip\nimport numpy as np\nimport os\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nfrom torchvision.datasets.utils import download_url\n\nimport torch\n\nfrom scipy.io import loadmat\n\nclass _BaseDataset(Dataset):\n\n    urls          = None\n    training_file = None\n    test_file     = None\n    \n    def __init__(self, root, split = \'train\', transform = None,\n                 label_transform = None, download=True):\n\n        super().__init__()\n        \n        self.root = root\n        self.which = split \n        \n        self.transform = transform\n        self.label_transform = label_transform\n\n        if download:\n            self.download()\n\n        self.get_data(self.which)\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        \n        x = Image.fromarray(self.images[index])\n        y = int(self.labels[index])\n        \n        if self.transform is not None:\n            x = self.transform(x)\n\n        if self.label_transform is not None:\n            y = self.label_tranform(y)\n            \n        return x, y\n\n    def get_data(self, name):\n        """"""Utility for convenient data loading.""""""\n        if name in [\'train\', \'unlabeled\']:\n            self.extract_images_labels(os.path.join(self.root, self.training_file))\n        elif name == \'test\':\n            self.extract_images_labels(os.path.join(self.root, self.test_file))\n\n    def extract_images_labels(self, filename):\n        raise NotImplementedError\n\n    def _check_exists(self):\n        return os.path.exists(os.path.join(self.root, self.training_file)) and \\\n            os.path.exists(os.path.join(self.root, self.test_file))\n\n    def download(self):\n        if self._check_exists():\n            return\n\n        os.makedirs(self.root, exist_ok = True)\n\n        for url in self.urls:\n            filename = url.rpartition(\'/\')[2]\n            file_path = os.path.join(self.root, filename)\n            download_url(url, root=self.root,\n                         filename=filename, md5=None)\n        print(\'Done!\')'"
salad/datasets/digits/mnist.py,0,"b'from torchvision.datasets import MNIST as MNISTBase\n\n\nclass MNIST(MNISTBase):\n    """""" MNIST Dataset\n    """"""\n\n    def __init__(self, root, split = \'train\', transform = None, label_transform = None, download=True):\n\n        super().__init__(root=root, train = (split == \'train\'),\n                         transform = transform,\n                         download=download)\n\n    @property\n    def images(self):\n        if self.train:\n            return self.train_data\n        else:\n            return self.test_data\n\n    @property\n    def labels(self):\n        if self.train:\n            return self.train_labels\n        else:\n            return self.test_labels\n'"
salad/datasets/digits/openset.py,2,"b'import numpy as np\nimport torch\n\nimport sys\n\nfrom torchvision import datasets, transforms\n\nfrom torch.utils.data import DataLoader\n\nclass OpenSetDataset(object):\n\n    """""" Dataset wrapper for openset classification\n\n    Works with any classification datasets that outputs a tuple (x, y) when calling\n    the `getitem` method.\n    Given two sets of label for known and unknown classes, maps unknown class labels\n    to zero.\n    """"""\n    \n    def __init__(self, dataset, known, unknown, labels=None):\n        \n        self.dataset = dataset\n        self.known = known\n        self.unknown = unknown\n        \n        self.labels = labels\n        \n        self.idx = []\n        \n        if self.labels is not None:\n            self._scan_labels()\n        else:\n            self._scan_dataset()\n        \n    def _scan_dataset(self):\n        \n        for i, (x, y) in enumerate(self.dataset):\n            if y in self.known or y in self.unknown:\n                self.idx.append(i)\n                \n    def _scan_labels(self):\n        \n        for i, y in enumerate(self.labels):\n            if y in self.known or y in self.unknown:\n                self.idx.append(i)\n                \n    def __len__(self):\n        \n        return len(self.idx)\n    \n    def __getitem__(self, index):\n        \n        index = self.idx[index]\n        \n        x,y   = self.dataset[index]\n        y     = y if y in self.known else torch.zeros_like(y)\n        \n        return x, y\n\ndef get_data(train=True, batch_size=128):\n    \n    label_common   = [0,1,2,3]\n    openset_source = [4,6,8]\n    openset_target = [5,7,9]\n\n    data1 = datasets.MNIST(\'/tmp/datasets\', train=train, download=True, transform=transforms.Compose([\n        transforms.Resize(32),\n        transforms.Grayscale(3),\n        transforms.ToTensor()\n    ]))\n    data2 = datasets.MNIST(\'/tmp/datasets\', train=train, download=True, transform=transforms.Compose([\n        transforms.Resize(32),\n        transforms.Grayscale(3),\n        transforms.ToTensor()\n    ]))\n\n    d1 = DataLoader(OpenSetDataset(data1, known=label_common, unknown=openset_source, labels=data1.train_labels if train else data1.test_labels), batch_size=batch_size, shuffle=True)\n    d2 = DataLoader(OpenSetDataset(data2, known=label_common, unknown=openset_target, labels=data2.train_labels if train else data1.test_labels), batch_size=batch_size, shuffle=True)\n    \n    return d1, d2\n\n#if __name__ == \'__main__\':\n#    \n#    d1, d2 = get_data(train=True)\n#\n#    model = FrenchModel()\n#    solver = BCESolver(model, d2, n_epochs=5, gpu=0)\n#\n#    solver.optimize()'"
salad/datasets/digits/svhn.py,0,"b'from torchvision.datasets import SVHN as SVHNBase\n\nclass SVHN(SVHNBase):\n\n    def __init__(self, *args, **kwargs):\n\n        super().__init__(*args, **kwargs)\n\n    @property\n    def images(self):\n        return self.data'"
salad/datasets/digits/synth.py,1,"b'import gzip\nimport numpy as np\nimport os\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nfrom torchvision.datasets.utils import download_url\n\nimport torch\n\nfrom scipy.io import loadmat\n\nfrom .base import _BaseDataset\n\nclass SynthSmall(_BaseDataset):\n\n    """""" Synthetic images dataset\n    """"""\n\n    num_labels  = 10\n    image_shape = [16, 16, 1]\n    \n    urls = {\n        ""https://github.com/domainadaptation/datasets/blob/master/synth/synth_train_32x32_small.mat?raw=true"", \n        ""https://github.com/domainadaptation/datasets/blob/master/synth/synth_test_32x32_small.mat?raw=true""\n    }\n    training_file = \'synth_train_32x32_small.mat?raw=true\'\n    test_file = \'synth_test_32x32.mat_small?raw=true\'\n    \n    def extract_images_labels(self, filename):\n        print(\'Extracting\', filename)\n\n        mat = loadmat(filename)\n\n        self.images = mat[\'X\'].transpose((3,0,1,2))\n        self.labels = mat[\'y\'].squeeze()\n\nclass Synth(_BaseDataset):\n    """""" Synthetic images dataset\n    """"""\n\n    num_labels  = 10\n    image_shape = [16, 16, 1]\n    \n    urls = {\n        ""https://github.com/domainadaptation/datasets/blob/master/synth/synth_train_32x32.mat?raw=true"", \n        ""https://github.com/domainadaptation/datasets/blob/master/synth/synth_test_32x32.mat?raw=true""\n    }\n    training_file = \'synth_train_32x32.mat?raw=true\'\n    test_file = \'synth_test_32x32.mat?raw=true\'\n    \n    def extract_images_labels(self, filename):\n        print(\'Extracting\', filename)\n\n        mat = loadmat(filename)\n\n        self.images = mat[\'X\'].transpose((3,0,1,2))\n        self.labels = mat[\'y\'].squeeze()'"
salad/datasets/digits/usps.py,1,"b'import gzip\nimport os\nimport numpy as np\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nfrom torchvision.datasets.utils import download_url\n\nimport torch\n\nfrom .base import _BaseDataset\n\nclass USPS(_BaseDataset):\n    """"""\n    \n    [USPS](http://statweb.stanford.edu/~tibs/ElemStatLearn/data.html) Dataset.\n\n    Args:\n        root (string): Root directory of dataset where ``processed/training.pt``\n            and  ``processed/test.pt`` exist.\n        train (bool, optional): If True, creates dataset from ``training.pt``,\n            otherwise from ``test.pt``.\n        download (bool, optional): If true, downloads the dataset from the internet and\n            puts it in root directory. If dataset is already downloaded, it is not\n            downloaded again.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n\n\n    Download USPS dataset from [1]_ or use the expliclict links [2]_ for training and [3]_\n    for testing.\n    Code for loading the dataset partly adapted from [4]_ (Apache License 2.0).\n\n    References: \n        \n        .. [1] http://statweb.stanford.edu/~tibs/ElemStatLearn/data.html\n        .. [2] Training Dataset http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.train.gz\n        .. [3] Test Dataset http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.test.gz\n        .. [4] https://github.com/haeusser/learning_by_association/blob/master/semisup/tools/usps.py\n    """"""\n\n    num_labels  = 10\n    image_shape = [16, 16, 1]\n    \n    urls = [\n        \'http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.train.gz\',\n        \'http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.test.gz\'\n    ]\n    training_file = \'zip.train.gz\'\n    test_file = \'zip.test.gz\'\n    \n    def extract_images_labels(self, filename):\n        import gzip\n\n        print(\'Extracting\', filename)\n        with gzip.open(filename, \'rb\') as f:\n            raw_data = f.read().split()\n        data = np.asarray([raw_data[start:start + 257]\n                           for start in range(0, len(raw_data), 257)],\n                          dtype=np.float32)\n        images_vec = data[:, 1:]\n        self.images = np.reshape(images_vec, (images_vec.shape[0], 16, 16))\n        self.labels = data[:, 0].astype(int)\n        self.images = ((self.images + 1)*128).astype(\'uint8\')\n'"
salad/datasets/transforms/__init__.py,0,b'from .digits import *\nfrom .ensembling import *\nfrom .noise import *'
salad/datasets/transforms/digits.py,0,"b'"""""" Standard Transformations for Digit datasets\n""""""\n\nfrom torchvision import transforms\nfrom torch import tensor\n\ndef default_normalization(key):\n\n    d = {\n    \'mnist\': (      tensor([ 0.1309,  0.1309,  0.1309]),\n                    tensor([ 0.2890,  0.2890,  0.2890])),\n    \'usps\': (       tensor([ 0.1576,  0.1576,  0.1576]),\n                    tensor([ 0.2327,  0.2327,  0.2327])),\n    \'synth\':       (tensor([ 0.4717,  0.4729,  0.4749]),\n                    tensor([ 0.3002,  0.2990,  0.3008])),\n    \'synth-small\': (tensor([ 0.4717,  0.4729,  0.4749]),\n                    tensor([ 0.3002,  0.2990,  0.3008])),\n    \'svhn\':        (tensor([ 0.4377,  0.4438,  0.4728]),\n                    tensor([ 0.1923,  0.1953,  0.1904]))\n    }\n\n    return d[key]\n\ndef default_transforms(key):\n\n    d = {\n\n        \'mnist\' : transforms.Compose([\n            transforms.Resize(32),\n            transforms.ToTensor(),\n            transforms.Lambda(lambda x : x.expand(3,-1,-1).clone())\n        ]),\n\n        \'svhn\' : transforms.Compose([\n            transforms.ToTensor(),\n        ]),\n\n        \'usps\' : transforms.Compose([\n            transforms.Resize(32),\n            transforms.ToTensor(),\n            transforms.Lambda(lambda x : x.expand(3,-1,-1).clone())\n        ]),\n\n        \'synth\' : transforms.Compose([\n            transforms.ToTensor(),\n        ]),\n\n        \'synth-small\' : transforms.Compose([\n            transforms.ToTensor(),\n        ])\n    }\n\n    return d[key]'"
salad/datasets/transforms/ensembling.py,2,"b'import numpy as np\nimport cv2\nimport torch\n\n"""""" Taken from Self-Ensembling Github Repository\n""""""\n\n\ndef identity_xf(N):\n    """"""\n    Construct N identity 2x3 transformation matrices\n    :return: array of shape (N, 2, 3)\n    """"""\n    xf = np.zeros((N, 2, 3), dtype=np.float32)\n    xf[:, 0, 0] = xf[:, 1, 1] = 1.0\n    return xf\n\n\ndef inv_nx2x2(X):\n    """"""\n    Invert the N 2x2 transformation matrices stored in X; a (N,2,2) array\n    :param X: transformation matrices to invert, (N,2,2) array\n    :return: inverse of X\n    """"""\n    rdet = 1.0 / (X[:, 0, 0] * X[:, 1, 1] - X[:, 1, 0] * X[:, 0, 1])\n    y = np.zeros_like(X)\n    y[:, 0, 0] = X[:, 1, 1] * rdet\n    y[:, 1, 1] = X[:, 0, 0] * rdet\n    y[:, 0, 1] = -X[:, 0, 1] * rdet\n    y[:, 1, 0] = -X[:, 1, 0] * rdet\n    return y\n\n\ndef inv_nx2x3(m):\n    """"""\n    Invert the N 2x3 transformation matrices stored in X; a (N,2,3) array\n    :param X: transformation matrices to invert, (N,2,3) array\n    :return: inverse of X\n    """"""\n    m2 = m[:, :, :2]\n    mx = m[:, :, 2:3]\n    m2inv = inv_nx2x2(m2)\n    mxinv = np.matmul(m2inv, -mx)\n    return np.append(m2inv, mxinv, axis=2)\n\n\ndef cat_nx2x3(a, b):\n    """"""\n    Multiply the N 2x3 transformations stored in `a` with those in `b`\n    :param a: transformation matrices, (N,2,3) array\n    :param b: transformation matrices, (N,2,3) array\n    :return: `a . b`\n    """"""\n    a2 = a[:, :, :2]\n    b2 = b[:, :, :2]\n\n    ax = a[:, :, 2:3]\n    bx = b[:, :, 2:3]\n\n    ab2 = np.matmul(a2, b2)\n    abx = ax + np.matmul(a2, bx)\n    return np.append(ab2, abx, axis=2)\n\n\ndef rotation_matrices(thetas):\n    """"""\n    Generate rotation matrices\n    :param thetas: rotation angles in radians as a (N,) array\n    :return: rotation matrices, (N,2,3) array\n    """"""\n    N = thetas.shape[0]\n    rot_xf = np.zeros((N, 2, 3), dtype=np.float32)\n    rot_xf[:, 0, 0] = rot_xf[:, 1, 1] = np.cos(thetas)\n    rot_xf[:, 1, 0] = np.sin(thetas)\n    rot_xf[:, 0, 1] = -np.sin(thetas)\n    return rot_xf\n\n\ndef centre_xf(xf, size):\n    """"""\n    Centre the transformations in `xf` around (0,0), where the current centre is assumed to be at the\n    centre of an image of shape `size`\n    :param xf: transformation matrices, (N,2,3) array\n    :param size: image size\n    :return: centred transformation matrices, (N,2,3) array\n    """"""\n    height, width = size\n\n    # centre_to_zero moves the centre of the image to (0,0)\n    centre_to_zero = np.zeros((1, 2, 3), dtype=np.float32)\n    centre_to_zero[0, 0, 0] = centre_to_zero[0, 1, 1] = 1.0\n    centre_to_zero[0, 0, 2] = -float(width) * 0.5\n    centre_to_zero[0, 1, 2] = -float(height) * 0.5\n\n    # centre_to_zero then xf\n    xf_centred = cat_nx2x3(xf, centre_to_zero)\n\n    # move (0,0) back to the centre\n    xf_centred[:, 0, 2] += float(width) * 0.5\n    xf_centred[:, 1, 2] += float(height) * 0.5\n\n    return xf_centred\n\n\nclass ImageAugmentation (object):\n    def __init__(self, hflip, xlat_range, affine_std, rot_std=0.0,\n                 intens_flip=False,\n                 intens_scale_range_lower=None, intens_scale_range_upper=None,\n                 intens_offset_range_lower=None, intens_offset_range_upper=None,\n                 scale_x_range=None, scale_y_range=None, scale_u_range=None, gaussian_noise_std=0.0,\n                 blur_range=None):\n        self.hflip = hflip\n        self.xlat_range = xlat_range\n        self.affine_std = affine_std\n        self.rot_std = rot_std\n        self.intens_scale_range_lower = intens_scale_range_lower\n        self.intens_scale_range_upper = intens_scale_range_upper\n        self.intens_offset_range_lower = intens_offset_range_lower\n        self.intens_offset_range_upper = intens_offset_range_upper\n        self.intens_flip = intens_flip\n        self.scale_x_range = scale_x_range\n        self.scale_y_range = scale_y_range\n        self.scale_u_range = scale_u_range\n        self.gaussian_noise_std = gaussian_noise_std\n        self.blur_range = blur_range\n\n    def augment(self, X):\n        X = X.copy()\n        xf = identity_xf(len(X))\n\n        if self.hflip:\n            x_hflip = np.random.binomial(1, 0.5, size=(len(X),)) * 2 - 1\n            xf[:, 0, 0] = x_hflip\n\n        if self.scale_x_range is not None and self.scale_x_range[0] is not None:\n            xf[:, 0, 0] *= np.random.uniform(low=self.scale_x_range[0],\n                                             high=self.scale_x_range[1], size=(len(X),))\n        if self.scale_y_range is not None and self.scale_y_range[0] is not None:\n            xf[:, 1, 1] *= np.random.uniform(low=self.scale_y_range[0],\n                                             high=self.scale_y_range[1], size=(len(X),))\n        if self.scale_u_range is not None and self.scale_u_range[0] is not None:\n            scale_u = np.random.uniform(\n                low=self.scale_u_range[0], high=self.scale_u_range[1], size=(len(X),))\n            xf[:, 0, 0] *= scale_u\n            xf[:, 1, 1] *= scale_u\n\n        if self.affine_std > 0.0:\n            xf[:, :, :2] += np.random.normal(scale=self.affine_std, size=(len(X), 2, 2))\n\n        if self.rot_std > 0.0:\n            thetas = np.random.normal(scale=self.rot_std, size=(len(X),))\n            rot_xf = rotation_matrices(thetas)\n            xf = cat_nx2x3(xf, rot_xf)\n\n        if self.xlat_range > 0.0:\n            xf[:, :, 2:] += np.random.uniform(low=-self.xlat_range,\n                                              high=self.xlat_range, size=(len(X), 2, 1))\n\n        if self.intens_flip:\n            col_factor = (np.random.binomial(1, 0.5, size=(\n                len(X), 1, 1, 1)) * 2 - 1).astype(np.float32)\n            X = (X * col_factor).astype(np.float32)\n\n        if self.intens_scale_range_lower is not None:\n            col_factor = np.random.uniform(low=self.intens_scale_range_lower, high=self.intens_scale_range_upper,\n                                           size=(len(X), 1, 1, 1))\n\n            X = (X * col_factor).astype(np.float32)\n\n        if self.intens_offset_range_lower is not None:\n            col_offset = np.random.uniform(low=self.intens_offset_range_lower, high=self.intens_offset_range_upper,\n                                           size=(len(X), 1, 1, 1))\n\n            X = (X + col_offset).astype(np.float32)\n\n        xf_centred = centre_xf(xf, X.shape[2:])\n        for i in range(len(X)):\n            if X.shape[1] == 1:\n                X[i, 0, :, :] = cv2.warpAffine(\n                    X[i, 0, :, :], xf_centred[i, :, :], (X.shape[3], X.shape[2]))\n            else:\n                X[i, :, :, :] = cv2.warpAffine(X[i, :, :, :].transpose(\n                    1, 2, 0), xf_centred[i, :, :], (X.shape[3], X.shape[2])).transpose(2, 0, 1)\n\n        if self.blur_range is not None and self.blur_range[0] is not None:\n            sigmas = np.random.uniform(\n                low=self.blur_range[0], high=self.blur_range[1], size=(len(X),))\n            sigmas = np.maximum(sigmas, 0.0)\n            for i in range(len(X)):\n                sigma = sigmas[i]\n                # ksize must be odd number\n                ksize = int(sigma+0.5) * 8 + 1\n                if X.shape[1] == 1:\n                    X[i, 0, :, :] = cv2.GaussianBlur(X[i, 0, :, :], (ksize, ksize), sigmaX=sigma)\n                else:\n                    X[i, :, :, :] = cv2.GaussianBlur(X[i, :, :, :].transpose(\n                        1, 2, 0), (ksize, ksize), sigmaX=sigma).transpose(2, 0, 1)\n\n        if self.gaussian_noise_std > 0.0:\n            X += np.random.normal(scale=self.gaussian_noise_std, size=X.shape).astype(np.float32)\n\n        return X\n\n    def augment_pair(self, X):\n        return self.augment(X), self.augment(X)\n\n\nclass Augmentation():\n\n    def __init__(self, dataset, n_samples=1):\n        self.transformer = ImageAugmentation(\n            affine_std=0.1,\n            gaussian_noise_std=0.1,\n            hflip=False,\n            intens_flip=True,\n            intens_offset_range_lower=-.5, intens_offset_range_upper=.5,\n            intens_scale_range_lower=0.25, intens_scale_range_upper=1.5,\n            xlat_range=2.0\n        )\n\n        self.dataset = dataset\n        self.n_samples = n_samples\n\n    def __len__(self):\n\n        return len(self.dataset)\n\n    def __getitem__(self, index):\n\n        x, y = self.dataset[index]\n\n        X = torch.stack([x.clone() for _ in range(self.n_samples)], dim=0)\n        X = self.transformer.augment(X.numpy())\n\n        outp = [torch.from_numpy(x).float() for x in X] + [y, ]\n\n        return outp\n'"
salad/datasets/transforms/noise.py,5,"b'from torchvision import datasets, transforms\n\nimport os\nimport os.path as osp\n\nimport numpy as np\n\nimport torch\n\nfrom salad import solver, models\nimport salad.datasets\n\nclass DomainConfusion():\n    \n    """""" Given x and a set of possible transforms, applies a random\n    transform on x and returns a pair (x, d)\n    """"""\n    \n    def __init__(self, transform_list, intermediate):\n        \n        self.transforms = transform_list\n        self.n_domains = len(transform_list)\n        self.intermediate = intermediate\n        \n    def __call__(self, x):\n        \n        idt = np.random.randint(self.n_domains)\n        t = self.transforms[idt]\n        x = t(x)\n        \n        for t in self.intermediate:\n            x = t(x)\n                \n        d = torch.zeros(self.n_domains)\n        d[idt] = 1\n        \n        return x, d\n    \nclass DomainLabel():\n    """""" concats a domain label to the dataset\n    """"""\n    \n    def __init__(self, domain, n_domains):\n        \n        self.domain = domain\n        self.n_domains = n_domains\n        \n    def __call__(self, x):\n        d = torch.zeros(self.n_domains)\n        d[self.domain] = 1\n        \n        return x, d\n\nclass Uniform():\n    """""" Add uniform noise\n    """"""\n    \n    def __init__(self, p=.05):\n        \n        self.prob = float(p)\n    \n    def __call__(self, x):\n        \n        N = np.random.uniform(0,1,size=x.shape)\n        X = x.numpy()\n\n        return torch.from_numpy(X + N).float()\n    \nclass Gaussian():\n    """""" Add gaussian noise\n    """"""\n    \n    def __init__(self, mu = 0., sigma = 0.1):\n        \n        self.mu    =  mu\n        self.sigma =  sigma\n    \n    def __call__(self, x):\n        \n        X = x.numpy()\n        \n        N = np.random.normal(self.mu, self.sigma, size=X.shape)\n\n        X = x.numpy()\n\n        return torch.from_numpy(X + N).float()\n    \nclass SaltAndPepper():\n    """""" Adds salt and pepper noise with probability *p* to a given image\n    or batch of images\n    """"""\n    \n    def __init__(self, p=.05):\n        \n        self.prob = float(p)\n    \n    def __call__(self, x):\n        \n        N = np.random.uniform(0,1,size=x.shape)\n\n        X = x.numpy()\n\n        X[N < self.prob]   = 1\n        X[N < self.prob/2] = 0\n\n        return torch.from_numpy(X)\n\nclass InvertContrast():\n    \n    def __call__(self, x):\n        return 1 - x\n\n    \nclass Shift():\n    \n    def __init__(self, w = 5, h = 5):\n        self.dw = w\n        self.dh = h\n    \n    def __call__(self, x):\n        x[:,self.dh:,self.dw:] = x.clone()[:,:-self.dw,:-self.dh]\n        \n        x[:,:self.dh,:] = 0\n        x[:,:,:self.dw] = 0\n\n        return x'"
salad/datasets/transforms/noisy.py,3,"b""import torch\nimport numpy as np\n\nfrom torchvision import datasets, models, transforms\n\ndef transform(x):\n\n    c,w,h = x.size()\n    add  = torch.from_numpy(np.random.normal(0,.2,size=(1,w,h))).float()\n    mult = torch.from_numpy(np.random.uniform(.3,1,size=(1,w,h))).float()\n\n    return torch.clamp(x * mult + add, 0, 1)\n\ndef noisy_transform(img_size=64):\n    transform = transforms.Compose([\n        transforms.Resize(img_size),\n        transforms.ColorJitter(.1, 1, .75, 0),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.2881,0.2881,0.2881)),\n        transforms.Lambda(lambda x : x.expand([3,-1,-1]))\n    ])\n\n    return transform\n\ndef clean_transform(img_size=64):\n    transform = transforms.Compose([\n        transforms.Resize(img_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.2881,0.2881,0.2881)),\n        transforms.Lambda(lambda x : x.expand([3,-1,-1]))\n    ])\n\n    return transform\n\n\ndef load_noisy_mnist(path, train=True):\n\n    clean = datasets.MNIST(path, train=train, download=True, transform=clean_transform())\n    noisy = datasets.MNIST(path, train=train, download=True, transform=noisy_transform())\n\n    return {'clean' : clean, 'noisy' : noisy}"""
salad/datasets/visda/__init__.py,0,b''
salad/datasets/visda/detection.py,9,"b'import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nimport numpy as np\nimport pandas as pd\nimport os.path as osp\n\nfrom PIL import Image\n\nimport torch\nfrom torch.utils.data import Dataset \n\nimport numpy as np\nimport pandas as pd\nimport os.path as osp\n\nfrom PIL import Image\n\n# from encoder import DataEncoder\n\n# from transforms import AffineAugmentation, EncoderTransform\nimport math\n\nfrom torchvision import transforms\n\n\ndef load_datalist(path):\n    \'\'\' Load COCO GT\n    \n    Adapted from\n    https://github.com/VisionLearningGroup/visda-2018-public/blob/master/detection/convert_datalist_gt_to_pkl.py\n    \'\'\'\n    \n    coco_boxes = []\n    coco_labels = []\n    fnames = []\n    for line in open(path, \'r\').readlines():\n        parts = [p.strip() for p in line.split()]\n        fnames.append(parts[0])\n\n        parts = parts[1:]\n        boxes = []\n        labs = []\n        for i in range(0, len(parts), 5):\n            x0, y0, x1, y1, cls = parts[i:i+5]\n            x0 = float(x0)\n            y0 = float(y0)\n            x1 = float(x1)\n            y1 = float(y1)\n            cls = int(cls)\n            boxes.append([x0, y0, x1, y1])\n            labs.append(cls)\n        coco_boxes.append(np.array(boxes))\n        coco_labels.append(np.array(labs))\n\n    return fnames, coco_boxes, coco_labels\n\nclass VisdaDetectionLoader(Dataset):\n    \n    id2label  = {\n        0 : ""aeroplane"",\n        1 : ""bicycle"",\n        2 : ""bus"",\n        3 : ""car"",\n        4 : ""horse"",\n        5 : ""knife"",\n        6 : ""motorcycle"",\n        7 : ""person"",\n        8 : ""plant"",\n        9 : ""skateboard"",\n        10 : ""train"",\n        11 : ""truck""\n    }\n    \n    def __init__(self, root, labels, transforms = None, joint_transforms = None):\n\n        """"""\n        root :  \n            directory with image files. labels found in the label file will be combined with this\n            directory using `os.path.join`\n        labels:\n            filename to csv file containing image filenames, bounding boxes and labels in the MS COCO\n            format\n        transforms:\n            transforms to be applied to images right after loading. Last transform should involve\n            casting the image to a pytorch tensor\n        joint_tranforms:\n            transforms jointly applied to the tuple of (image, bounding boxes, labels)\n        """"""\n        \n        super().__init__()\n        \n        self.labelfile        = labels\n        self.imgroot          = root\n        self.transforms       = transforms\n        self.joint_transforms = joint_transforms\n\n        self._load_labels()\n        \n    def __len__(self):\n\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        \n        sample  = self.samples.loc[index]\n\n        im = Image.open(osp.join(self.imgroot, sample.fname))\n        bboxes = torch.from_numpy(sample.boxes).float()\n        labels = torch.from_numpy(sample.labels).float()\n \n        if self.transforms is not None:\n            im = self.transforms(im)\n\n        if self.joint_transforms is not None:\n            im, bboxes, labels = self.joint_transforms( [im, bboxes, labels] )\n\n        return im, bboxes, labels\n\n    def _load_labels(self):\n        \n        fnames, coco_boxes, coco_labels = load_datalist(self.labelfile)\n        self.samples = pd.DataFrame(data = {\'fname\' : fnames,\n                    \'boxes\' : coco_boxes, \'labels\' : coco_labels})\n\nclass MaxTransform():\n\n    def __call__(self, args):\n\n        x,bboxes,y = args\n\n        bboxes_ = bboxes.clone()\n\n        bboxes[:,0] = torch.min(bboxes_[:,0], bboxes_[:,2])\n        bboxes[:,1] = torch.min(bboxes_[:,1], bboxes_[:,3])\n        bboxes[:,2] = torch.max(bboxes_[:,0], bboxes_[:,2])\n        bboxes[:,3] = torch.max(bboxes_[:,1], bboxes_[:,3])\n\n        return (x, bboxes, y)\n\n\nclass CoordShuffle():\n\n    def __call__(self, args):\n\n        x,bboxes,y = args\n\n        bboxes_ = bboxes.clone()\n        bboxes[:,0] = bboxes_[:,1]\n        bboxes[:,1] = bboxes_[:,0]\n        bboxes[:,2] = bboxes_[:,3]\n        bboxes[:,3] = bboxes_[:,2]\n\n        return (x, bboxes, y)\n\ndef build_dataset(batch_size, which=\'train_visda\',\n                  num_workers = None, encode = True,\n                  augment=True, shuffle = True):\n    import pandas as pd\n\n    data = {\n    \'train_visda\' : {\n        \'labels\' : \'/gpfs01/bethge/home/sschneider/data/visda-detection/visda18-detection-train.txt\',\n        \'root\'   : \'/tmp/visda-detect/png_json/\'\n    },\n    \'val_coco\' : {\n        \'labels\' : \'/gpfs01/bethge/home/sschneider/data/visda-detection/coco17-val.txt\',\n        \'root\'   : \'/tmp/visda-detect/val2017/\'\n    },\n    \'test_visda\' : {\n        \'labels\' : \'/gpfs01/bethge/home/sschneider/data/visda-detection/visda18-detection-test.txt\',\n        \'root\'   : \'/tmp/visda-detect/png_json/\'\n    }\n    }\n\n\n    datasets = pd.DataFrame(data)\n\n    T = transforms.Compose([\n        transforms.ColorJitter(.1, .8, .75, 0),\n        transforms.ToTensor(),\n        transforms.Normalize( mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225] ) \n    ]\n    )        \n\n    Tj = transforms.Compose([\n        AffineAugmentation(\n            coord_order = \'xy\',\n            flip_x  = 0.5,\n            flip_y  = 0.,\n            shear_x = (0,0.01),\n            shear_y = (0,0.01),\n            scale   = (1., 5.),\n            rotate  = (-math.pi/20, math.pi/20),\n            dx      = (-1,1),\n            dy      = (-1,1)\n        ) if augment else lambda args: args,\n        CoordShuffle(),\n        MaxTransform(),\n        EncoderTransform() if encode else lambda args: args\n    ])\n\n    dataset = VisdaDetectionLoader(**datasets[which].to_dict(),\n                                transforms=T, joint_transforms = Tj)\n    loader = DataLoader(dataset, shuffle=shuffle, batch_size=batch_size, num_workers=num_workers)\n\n    return loader\n        \ndef build_validation(batch_size, which=\'train_visda\',\n                  num_workers = None, encode = True,\n                  augment=True, shuffle = True):\n    import pandas as pd\n\n    data = {\n    \'train_visda\' : {\n        \'labels\' : \'/gpfs01/bethge/home/sschneider/data/visda-detection/visda18-detection-train.txt\',\n        \'root\'   : \'/tmp/visda-detect/png_json/\'\n    },\n    \'val_coco\' : {\n        \'labels\' : \'/gpfs01/bethge/home/sschneider/data/visda-detection/coco17-val.txt\',\n        \'root\'   : \'/tmp/visda-detect/val2017/\'\n    },\n    \'test_visda\' : {\n        \'labels\' : \'/gpfs01/bethge/home/sschneider/data/visda-detection/visda18-detection-test.txt\',\n        \'root\'   : \'/tmp/visda-detect/png_json/\'\n    }\n    }\n\n\n    datasets = pd.DataFrame(data)\n\n    T = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize( mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225] ) \n    ]\n    )        \n\n    Tj = transforms.Compose([\n        lambda args: args[0], torch.zeros(len(args[0])), torch.zeros(len(args[0]))\n    ])\n\n    dataset = VisdaDetectionLoader(**datasets[which].to_dict(),\n                                transforms=T, joint_transforms = Tj)\n    loader = DataLoader(dataset, shuffle=shuffle, batch_size=batch_size, num_workers=num_workers)\n\n    return loader\n        '"
salad/datasets/visda/openset.py,0,b''
salad/datasets/visda/utils.py,6,"b""import os\nimport os.path as osp\nimport sys\nimport argparse\n\nimport torch\nfrom torch import nn\n\nfrom salad import solver, datasets, optim\n\n# from augment import AffineTransformer\n# from augment2 import ImageAugmentation\n\nfrom torch.nn import functional as F\nfrom salad.layers import AccuracyScore, MeanAccuracyScore\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom salad import solver, optim, layers\nfrom torchvision.models import resnet\n\nimport salad\nimport salad.datasets\n\nfrom torchvision import datasets, transforms\nimport os.path as osp\n\nfrom torch.utils.data.sampler import WeightedRandomSampler\n\nclass Augmentation():\n    \n    def __init__(self, n_samples=1):\n        self.transformer = ImageAugmentation(\n            affine_std=0.1,\n            gaussian_noise_std=0.1,\n            hflip=False,\n            intens_flip=False,\n            intens_offset_range_lower=-.5, intens_offset_range_upper=.5,\n            intens_scale_range_lower=0.25, intens_scale_range_upper=1.5,\n            xlat_range=2.0\n        )\n        \n        self.n_samples = n_samples\n        \n    def __call__(self, x):\n        \n        X = torch.stack([x.clone() for _ in range(self.n_samples)], dim=0)\n        X = self.transformer.augment(X.numpy())\n        \n        outp = [torch.from_numpy(x).float() for x in X]\n\n        if len(outp) == 1:\n            return outp[0]\n                \n        return outp\n\nclass MultiTransform():\n    \n    def __init__(self, transforms, n_samples=1):\n        self.transforms = transforms\n        self.n_samples  = n_samples\n        \n    def __call__(self, x):\n\n        outp = [ self.transforms(x) for i in range(self.n_samples) ]\n        \n        if len(outp) == 1:\n            return outp[0]\n                \n        return outp\n\ndef get_class_counts(data):\n    import pandas as pd\n    import numpy as np\n    \n    lbl_counts = {l : 0 for l in data.class_to_idx.values()}\n\n    df = pd.DataFrame(data.samples, columns=['fname', 'label'])\n    counts = df.label.value_counts(sort=False).to_dict() #.values\n    \n    lbl_counts.update(counts)\n\n    return np.array([lbl_counts[i] for i in range(len(lbl_counts))])\n\ndef get_balanced_loader(data, **kwargs):\n\n    counts = get_class_counts(data)\n    weights = 1. / (1e-5 + counts)\n    weights[counts == 0] = 0.\n    weights = torch.from_numpy(weights / weights.sum()).float()\n\n    print('Class Counts', counts)\n    print('Weights', weights)\n\n    sampler = WeightedRandomSampler(weights, kwargs.get('batch_size'))\n    loader = DataLoader(data, sampler = sampler, **kwargs)\n\n    return loader\n\ndef get_unbalanced_loader(data, **kwargs):\n\n    loader = DataLoader(data, drop_last = True, **kwargs)\n\n    return loader\n\n\ndef visda_data_loader(path, batch_size, n_src = 1, n_tgt=1):\n    T = lambda i : transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomCrop(224),\n        transforms.ColorJitter(.1, .8, .75, 0),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = [0.485, 0.456, 0.406],\n                             std  = [0.229, 0.224, 0.225]),\n        Augmentation(i)\n    ])\n    train      = datasets.ImageFolder(osp.join(path, 'train/classes'), transform=T(n_src))\n    validation = datasets.ImageFolder(osp.join(path, 'validation/classes'), transform=T(n_tgt))\n\n    loader = salad.datasets.JointLoader(\n                get_balanced_loader(train, shuffle=True, batch_size=batch_size, num_workers=24),\n                get_balanced_loader(validation, shuffle=True, batch_size=batch_size, num_workers=24)\n    )\n\n    return loader\n\n#def visda_data_loader_full(path, batch_size, n_src = 1, n_tgt=1):\n#    T = lambda i : transforms.Compose([\n#        transforms.Resize(256),\n#        transforms.RandomResizedCrop(224),\n#        #transforms.RandomCrop(224),\n#        transforms.ColorJitter(.1, .8, .75, 0),\n#        transforms.ToTensor(),\n#        transforms.Normalize(mean = [0.485, 0.456, 0.406],\n#                             std  = [0.229, 0.224, 0.225]),\n#        Augmentation(i)\n#    ])\n#    train      = datasets.ImageFolder(osp.join(path, 'train'), transform=T(n_src))\n#    validation = datasets.ImageFolder(osp.join(path, 'validation'), transform=T(n_tgt))\n#\n#    loader = salad.datasets.JointLoader(\n#                get_balanced_loader(train, shuffle=True, batch_size=batch_size, num_workers=12),\n#                get_balanced_loader(validation, shuffle=True, batch_size=batch_size, num_workers=12)\n#    )\n#\n#    return loader\n\ndef visda_data_loader_pseudo(path, batch_size, n_aug = 1):\n    T = lambda i : transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomCrop(224),\n        transforms.ColorJitter(.1, .8, .75, 0),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = [0.485, 0.456, 0.406],\n                             std  = [0.229, 0.224, 0.225]),\n        Augmentation(i)\n    ])\n    data = datasets.ImageFolder(osp.join(path, 'pseudo-5'), transform=T(n_aug))\n\n    return get_balanced_loader(data, shuffle=True, batch_size=batch_size, num_workers=12)\n\ndef visda_data_loader_full(path, batch_size, n_src = 1, n_tgt=1):\n#def new_visda_data_loader(path, batch_size, n_aug = 1, which='train'):\n    T = lambda i : MultiTransform(\n                transforms.Compose([\n                transforms.RandomAffine(15, translate=(0,.1), scale=None, shear=10, resample=False, fillcolor=0),\n                transforms.Resize(224),\n                transforms.RandomResizedCrop(160, scale=(0.4, 1.0)),\n                transforms.RandomGrayscale(p=.5),\n                transforms.RandomHorizontalFlip(.5),\n                transforms.ColorJitter(.1, .5, .5, 0.1),\n                transforms.ToTensor(),\n                transforms.Normalize(mean = [0.485, 0.456, 0.406],\n                                    std  = [0.229, 0.224, 0.225]),\n            ]), n_samples = i\n        )\n\n    train      = datasets.ImageFolder(osp.join(path, 'train'), transform=T(n_src))\n    validation = datasets.ImageFolder(osp.join(path, 'validation'), transform=T(n_tgt))\n\n    loader = salad.datasets.JointLoader(\n                get_unbalanced_loader(train, shuffle=True, batch_size=batch_size, num_workers=12),\n                get_unbalanced_loader(validation, shuffle=True, batch_size=batch_size, num_workers=12)\n    )\n\n    return loader"""
salad/datasets/visda/visda.py,0,"b""import os.path as osp\n\nfrom torchvision import transforms, datasets\n\ndef load_dataset(path='./data', im_size = 224):\n    T = transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomResizedCrop(224),\n        transforms.ColorJitter(.1, .8, .75, 0),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n    ])\n    print(osp.join(path, 'train'))\n    train      = datasets.ImageFolder(osp.join(path, 'train'), transform=T)\n    validation = datasets.ImageFolder(osp.join(path,'validation'), transform=T)\n\n    return {'source' : train, 'target' : validation}"""
salad/models/audio/__init__.py,0,b''
salad/models/digits/__init__.py,0,"b'"""""" Collection of models to reproduce resuls in original publications\n""""""\n\nfrom .base import *\n'"
salad/models/digits/_legacy.py,3,"b'import h5py\nimport torch\nfrom torch import nn\n\nimport torch.nn.functional as F\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport os\nfrom collections import OrderedDict\n\nclass SVHN(nn.Module):\n    def __init__(self, features, n_channel, num_classes):\n        super(SVHN, self).__init__()\n        assert isinstance(features, nn.Sequential), type(features)\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Linear(n_channel, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        y = self.classifier(x)\n        return x, y\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for i, v in enumerate(cfg):\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            padding = v[1] if isinstance(v, tuple) else 1\n            out_channels = v[0] if isinstance(v, tuple) else v\n            conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(out_channels, affine=False), nn.ReLU(), nn.Dropout(0.3)]\n            else:\n                layers += [conv2d, nn.ReLU(), nn.Dropout(0.3)]\n            in_channels = out_channels\n    return nn.Sequential(*layers)\n\ndef svhn(n_channel, pretrained=None):\n    cfg = [n_channel, n_channel, \'M\', 2*n_channel, 2*n_channel, \'M\', 4*n_channel, 4*n_channel, \'M\', (8*n_channel, 0), \'M\']\n    layers = make_layers(cfg, batch_norm=True)\n    model = SVHN(layers, n_channel=8*n_channel, num_classes=10)\n    if pretrained is not None:\n        m = model_zoo.load_url(model_urls[\'svhn\'])\n        state_dict = m.state_dict() if isinstance(m, nn.Module) else m\n        assert isinstance(state_dict, (dict, OrderedDict)), type(state_dict)\n        model.load_state_dict(state_dict)\n    return model\n\ndef conv2d(m,n,k,act=True):\n    layers =  [nn.Conv2d(m,n,k,padding=1)]\n\n    if act: layers += [nn.ELU()]\n\n    return nn.Sequential(\n        *layers\n    )\n\nclass MNISTModel(nn.Module):\n\n    def __init__(self, n_channel):\n        super(MNISTModel, self).__init__()\n        self.conv1 = nn.Conv2d(n_channel, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        y = self.fc2(x)\n        return x, y\n\nclass SVHNmodel(nn.Module):\n\n    """"""\n    Model for application on SVHN data (32x32x3)\n    Architecture identical to https://github.com/haeusser/learning_by_association\n    """"""\n\n    def __init__(self):\n\n        super(SVHNmodel, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.InstanceNorm2d(3),\n            conv2d(3,  32, 3),\n            conv2d(32, 32, 3),\n            conv2d(32, 32, 3),\n            nn.MaxPool2d(2, 2, padding=0),\n            conv2d(32, 64, 3),\n            conv2d(64, 64, 3),\n            conv2d(64, 64, 3),\n            nn.MaxPool2d(2, 2, padding=0),\n            conv2d(64, 128, 3),\n            conv2d(128, 128, 3),\n            conv2d(128, 128, 3),\n            nn.MaxPool2d(2, 2, padding=0)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(128*4*4, 10)\n        )\n\n    def forward(self, x):\n\n        phi  = self.features(x)\n        phi_mean = phi.view(-1, 128, 16).mean(dim=-1)\n        phi = phi.view(-1,128*4*4)\n        y = self.classifier(phi)\n\n        return phi_mean, y\n\n\nclass FrenchModel(nn.Module):\n\n    """"""\n    Model used in ""Self-Ensembling for Visual Domain Adaptation""\n    by French et al.\n    """"""\n\n    def __init__(self):\n\n        super(FrenchModel, self).__init__()\n\n        def conv2d_3x3(inp,outp,pad=1):\n            return nn.Sequential(\n                nn.Conv2d(inp,outp,kernel_size=3,padding=pad),\n                nn.BatchNorm2d(outp),\n                nn.ReLU()\n            )\n\n        def conv2d_1x1(inp,outp):\n            return nn.Sequential(\n                nn.Conv2d(inp,outp,kernel_size=1,padding=0),\n                nn.BatchNorm2d(outp),\n                nn.ReLU()\n            )\n\n        def block(inp,outp):\n            return nn.Sequential(\n                conv2d_3x3(inp,outp),\n                conv2d_3x3(outp,outp),\n                conv2d_3x3(outp,outp)\n            )\n\n        self.features = nn.Sequential(\n            block(3,128),\n            nn.MaxPool2d(2, 2, padding=0),\n            nn.Dropout2d(p=0.5),\n            block(128,256),\n            nn.MaxPool2d(2, 2, padding=0),\n            nn.Dropout2d(p=0.5),\n            conv2d_3x3(256, 256, pad=0),\n            conv2d_1x1(256, 128),\n            nn.AvgPool2d(6, 6, padding=0)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n\n        phi  = self.features(x)\n        phi = phi.view(-1,128)\n        # print(x.size(), phi.size())\n        y = self.classifier(phi)\n\n        return phi, y\n\n\n'"
salad/models/digits/adv.py,2,"b'import torch\nfrom torch import nn\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n"""""" Original implementation from https://github.com/mil-tokyo/adr_da/blob/master/model/svhn2mnist.py\n""""""\n\nclass Feature(nn.Module):\n    def __init__(self):\n        super(Feature, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2)\n        self.bn3 = nn.BatchNorm2d(128)\n\n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.bn1(self.conv1(x))), stride=2, kernel_size=3, padding=1)\n        x = F.max_pool2d(F.relu(self.bn2(self.conv2(x))), stride=2, kernel_size=3, padding=1)\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = x.view(x.size(0), 8192)\n\n        return x\n\n\nclass Predictor(nn.Module):\n    def __init__(self, prob=0.5):\n        super(Predictor, self).__init__()\n        self.fc1 = nn.Linear(8192, 3072)\n        self.bn1_fc = nn.BatchNorm1d(3072)\n        self.fc2 = nn.Linear(3072, 2048)\n        self.bn2_fc = nn.BatchNorm1d(2048)\n        self.fc3 = nn.Linear(2048, 10)\n        self.bn_fc3 = nn.BatchNorm1d(10)\n        self.prob = prob\n\n    def forward(self, x):\n        x = F.relu(self.bn1_fc(self.fc1(x)))\n        x = F.dropout(x, training=self.training, p=self.prob)\n        x = F.relu(self.bn2_fc(self.fc2(x)))\n        x = F.dropout(x, training=self.training, p=self.prob)\n        x = self.fc3(x)\n        return x\n\n\nclass AdvModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.features   = Feature()\n        self.classifier = Predictor()\n\n    def forward(self, x):\n        \n        feats = self.features(x)\n        label = self.classifier(feats)\n\n        return feats, label\n\n'"
salad/models/digits/assoc.py,0,"b'import torch\nfrom torch import nn\n\nclass FrenchModel(nn.Module):\n\n    """"""\n    Model used in ""Self-Ensembling for Visual Domain Adaptation""\n    by French et al.\n    """"""\n\n    def __init__(self):\n\n        super(FrenchModel, self).__init__()\n\n        def conv2d_3x3(inp,outp,pad=1):\n            return nn.Sequential(\n                nn.Conv2d(inp,outp,kernel_size=3,padding=pad),\n                nn.BatchNorm2d(outp),\n                nn.ReLU()\n            )\n\n        def conv2d_1x1(inp,outp):\n            return nn.Sequential(\n                nn.Conv2d(inp,outp,kernel_size=1,padding=0),\n                nn.BatchNorm2d(outp),\n                nn.ReLU()\n            )\n\n        def block(inp,outp):\n            return nn.Sequential(\n                conv2d_3x3(inp,outp),\n                conv2d_3x3(outp,outp),\n                conv2d_3x3(outp,outp)\n            )\n\n        self.features = nn.Sequential(\n            block(3,128),\n            nn.MaxPool2d(2, 2, padding=0),\n            nn.Dropout2d(p=0.5),\n            block(128,256),\n            nn.MaxPool2d(2, 2, padding=0),\n            nn.Dropout2d(p=0.5),\n            conv2d_3x3(256, 512, pad=0),\n            conv2d_1x1(512, 256),\n            conv2d_1x1(256, 128),\n            nn.AvgPool2d(6, 6, padding=0)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n\n        phi  = self.features(x)\n        phi = phi.view(-1,128)\n        y = self.classifier(phi)\n\n        return phi, y\n\ndef conv2d(m,n,k,act=True):\n    layers =  [nn.Conv2d(m,n,k,padding=1)]\n\n    if act: layers += [nn.ELU()]\n\n    return nn.Sequential(\n        *layers\n    )\n\nclass SVHNmodel(nn.Module):\n\n    """"""\n    Model for application on SVHN data (32x32x3)\n    Architecture identical to https://github.com/haeusser/learning_by_association\n    """"""\n\n    def __init__(self):\n\n        super(SVHNmodel, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.InstanceNorm2d(3),\n            conv2d(3,  32, 3),\n            conv2d(32, 32, 3),\n            conv2d(32, 32, 3),\n            nn.MaxPool2d(2, 2, padding=0),\n            conv2d(32, 64, 3),\n            conv2d(64, 64, 3),\n            conv2d(64, 64, 3),\n            nn.MaxPool2d(2, 2, padding=0),\n            conv2d(64, 128, 3),\n            conv2d(128, 128, 3),\n            conv2d(128, 128, 3),\n            nn.MaxPool2d(2, 2, padding=0)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(128*4*4, 10)\n        )\n\n    def forward(self, x):\n\n        phi  = self.features(x)\n        phi_mean = phi.view(-1, 128, 16).mean(dim=-1)\n        phi = phi.view(-1,128*4*4)\n        y = self.classifier(phi)\n\n        return phi_mean, y'"
salad/models/digits/base.py,3,"b'import os\nimport os.path as osp\nimport sys\nimport argparse\n\nimport torch\nfrom torch import nn\n\nfrom ... import solver, models, datasets\n\nfrom salad.datasets.transforms.ensembling import ImageAugmentation\n\nfrom torch.nn import functional as F\nfrom salad.layers import AccuracyScore, FeatureRotation\n\nclass _ConditionalModel(nn.Module):\n\n    def __init__(self, n_domains):\n        super().__init__()\n        self.n_domains = n_domains\n\n        self.conditional_layers  = []\n\n    def conditional_params(self, d=0):\n        for module in self.conditional_layers:\n            for p in module.conditional_params(d):\n                yield p\n        \n    def parameters(self, d=0, yield_shared=True, yield_conditional=True):\n\n        if yield_shared:\n            for param in super().parameters():\n                yield param\n\n        if yield_conditional:\n            for param in self.conditional_params(d):\n                yield param\n\n    def __call__(self, x, d=0):\n        return self.forward(x, d)\n\n    def _batch_norm(self, *args, **kwargs):\n        layer = ConditionalBatchNorm(*args, n_domains=self.n_domains, **kwargs)\n        self.conditional_layers.append(layer)  \n        return layer\n\nclass ConditionalBatchNorm(_ConditionalModel):\n    \n    def __init__(self, *args, n_domains = 1, bn_func = nn.BatchNorm2d, **kwargs):\n        \n        super().__init__(n_domains)\n        self.conditional_layers    += [bn_func(*args, **kwargs) for i in range(n_domains)]\n        \n    def _apply(self, fn): \n        super()._apply(fn)\n        for layer in self.conditional_layers:\n            layer._apply(fn)\n\n    def conditional_params(self, d):\n        return self.conditional_layers[d].parameters()\n\n    def parameters(self, d=0):\n        pass\n        \n    def forward(self, x, d):\n        layer = self.conditional_layers[d]\n        return layer(x) \n\nclass NoisyDigitFeatures(_ConditionalModel):\n\n    def __init__(self, n_domains):\n\n        super().__init__(n_domains)\n\n        self.norm = nn.InstanceNorm2d(3, affine=False,\n                momentum=0,\n                track_running_stats=False)\n\n        self.conv1_1 = nn.Conv2d(3, 128, (3, 3), padding=1)\n        self.conv1_1_bn = self._batch_norm(128)\n        self.conv1_2 = nn.Conv2d(128, 128, (3, 3), padding=1)\n        self.conv1_2_bn = self._batch_norm(128)\n        self.conv1_3 = nn.Conv2d(128, 128, (3, 3), padding=1)\n        self.conv1_3_bn = self._batch_norm(128)\n        self.pool1 = nn.MaxPool2d((2, 2))\n\n        self.conv2_1 = nn.Conv2d(128, 256, (3, 3), padding=1)\n        self.conv2_1_bn = self._batch_norm(256)\n        self.conv2_2 = nn.Conv2d(256, 256, (3, 3), padding=1)\n        self.conv2_2_bn = self._batch_norm(256)\n        self.conv2_3 = nn.Conv2d(256, 256, (3, 3), padding=1)\n        self.conv2_3_bn = self._batch_norm(256)\n        self.pool2 = nn.MaxPool2d((2, 2))\n        self.drop2 = nn.Dropout()\n\n        self.conv3_1 = nn.Conv2d(256, 512, (3, 3), padding=0)\n        self.conv3_1_bn = self._batch_norm(512)\n        self.nin3_2 = nn.Conv2d(512, 256, (1, 1), padding=1)\n        self.nin3_2_bn = self._batch_norm(256)\n        self.nin3_3 = nn.Conv2d(256, 128, (1, 1), padding=1)\n        self.nin3_3_bn = self._batch_norm(128)\n\n        self.drop1_1 = nn.Dropout2d(p=0.25)\n        self.drop1_2 = nn.Dropout2d(p=0.25)\n        self.drop1_3 = nn.Dropout2d(p=0.25)\n\n        self.drop2_1 = FeatureRotation(256, p=0.25)\n        self.drop2_2 = FeatureRotation(256, p=0.25)\n        self.drop2_3 = nn.Dropout(p=0.5)\n\n        self.drop3_1 = FeatureRotation(512, p=0.5)\n        self.drop3_2 = FeatureRotation(256, p=0.5)\n        self.drop3_3 = nn.Dropout(p=0.5)\n\n    def forward(self, x, d):\n\n        x = self.norm(x)\n\n        x = F.relu(self.conv1_1_bn(self.conv1_1(x), d))\n        x = self.drop1_1(x)\n        x = F.relu(self.conv1_2_bn(self.conv1_2(x), d))\n        x = self.drop1_2(x)\n        x = self.pool1(F.relu(self.conv1_3_bn(self.conv1_3(x), d)))\n        x = self.drop1_3(x)\n\n\n        x = F.relu(self.conv2_1_bn(self.conv2_1(x), d))\n        x = self.drop2_1(x)\n        x = F.relu(self.conv2_2_bn(self.conv2_2(x), d))\n        x = self.drop2_2(x)\n        x = self.pool2(F.relu(self.conv2_3_bn(self.conv2_3(x), d)))\n        x = self.drop2_3(x)\n\n        x = F.relu(self.conv3_1_bn(self.conv3_1(x), d))\n        x = self.drop3_1(x)\n        x = F.relu(self.nin3_2_bn(self.nin3_2(x), d))\n        x = self.drop3_2(x)\n        x = F.relu(self.nin3_3_bn(self.nin3_3(x), d))\n        x = self.drop3_3(x)\n\n        x = F.avg_pool2d(x, 6)\n        x = x.view(-1, 128)\n\n        return x\n\nclass DigitFeatures(_ConditionalModel):\n\n    def __init__(self, n_domains):\n\n        super().__init__(n_domains)\n\n        self.norm = nn.InstanceNorm2d(3, affine=False,\n                momentum=0,\n                track_running_stats=False)\n\n        self.conv1_1 = nn.Conv2d(3, 128, (3, 3), padding=1)\n        self.conv1_1_bn = self._batch_norm(128)\n        self.conv1_2 = nn.Conv2d(128, 128, (3, 3), padding=1)\n        self.conv1_2_bn = self._batch_norm(128)\n        self.conv1_3 = nn.Conv2d(128, 128, (3, 3), padding=1)\n        self.conv1_3_bn = self._batch_norm(128)\n        self.pool1 = nn.MaxPool2d((2, 2))\n        self.drop1 = nn.Dropout()\n\n        self.conv2_1 = nn.Conv2d(128, 256, (3, 3), padding=1)\n        self.conv2_1_bn = self._batch_norm(256)\n        self.conv2_2 = nn.Conv2d(256, 256, (3, 3), padding=1)\n        self.conv2_2_bn = self._batch_norm(256)\n        self.conv2_3 = nn.Conv2d(256, 256, (3, 3), padding=1)\n        self.conv2_3_bn = self._batch_norm(256)\n        self.pool2 = nn.MaxPool2d((2, 2))\n        self.drop2 = nn.Dropout()\n\n        self.conv3_1 = nn.Conv2d(256, 512, (3, 3), padding=0)\n        self.conv3_1_bn = self._batch_norm(512)\n        self.nin3_2 = nn.Conv2d(512, 256, (1, 1), padding=1)\n        self.nin3_2_bn = self._batch_norm(256)\n        self.nin3_3 = nn.Conv2d(256, 128, (1, 1), padding=1)\n        self.nin3_3_bn = self._batch_norm(128)\n\n    def forward(self, x, d):\n\n        x = self.norm(x)\n\n        x = F.relu(self.conv1_1_bn(self.conv1_1(x), d))\n        x = F.relu(self.conv1_2_bn(self.conv1_2(x), d))\n        x = self.pool1(F.relu(self.conv1_3_bn(self.conv1_3(x), d)))\n        x = self.drop1(x)\n\n        x = F.relu(self.conv2_1_bn(self.conv2_1(x), d))\n        x = F.relu(self.conv2_2_bn(self.conv2_2(x), d))\n        x = self.pool2(F.relu(self.conv2_3_bn(self.conv2_3(x), d)))\n        x = self.drop2(x)\n\n        x = F.relu(self.conv3_1_bn(self.conv3_1(x), d))\n        x = F.relu(self.nin3_2_bn(self.nin3_2(x), d))\n        x = F.relu(self.nin3_3_bn(self.nin3_3(x), d))\n\n        x = F.avg_pool2d(x, 6)\n        x = x.view(-1, 128)\n\n        return x\n\nclass NullspaceDigitFeatures(DigitFeatures):\n\n     def __init__(self, linear):\n        super().__init__()\n        self.W = linear.weight\n\n     def _compute_nullspace(self):\n        with torch.no_grad():\n            u,s,vh = torch.svd(self.W)\n            N = vh.mm(vh.transpose(1,0))\n        return N\n\n     def forward(self, x, d):\n        x = super().forward(x)\n        N = self._compute_nullspace()\n        x_ = x.mm(N)\n        return x_\n\nclass DigitModel(_ConditionalModel):\n    \n    def __init__(self, n_classes=10, n_domains=2, noisy=False, nullspace = False):\n        super().__init__(n_domains)\n        \n        self.n_domains = n_domains\n        if noisy:\n            self.features   = NoisyDigitFeatures(n_domains = n_domains)\n        elif nullspace:\n            self.features = NullspaceDigitFeatures(self.classifier)\n        else:\n            self.features   = DigitFeatures(n_domains = n_domains)\n\n        self.classifier = nn.Linear(128, n_classes)\n\n        self.conditional_layers.append(self.features)  \n    \n    def forward(self, x, d=0):\n        z = self.features(x)\n        y = self.classifier(z)\n\n        return z, y\n\n'"
salad/models/digits/corr.py,0,"b'import torch\nfrom torch import nn\n\nclass FrenchModel(nn.Module):\n\n    """"""\n    Model used in ""Self-Ensembling for Visual Domain Adaptation""\n    by French et al.\n    """"""\n\n    def __init__(self):\n\n        super(FrenchModel, self).__init__()\n\n        def conv2d_3x3(inp,outp,pad=1):\n            return nn.Sequential(\n                nn.Conv2d(inp,outp,kernel_size=3,padding=pad),\n                nn.BatchNorm2d(outp),\n                nn.ReLU()\n            )\n\n        def conv2d_1x1(inp,outp):\n            return nn.Sequential(\n                nn.Conv2d(inp,outp,kernel_size=1,padding=0),\n                nn.BatchNorm2d(outp),\n                nn.ReLU()\n            )\n\n        def block(inp,outp):\n            return nn.Sequential(\n                conv2d_3x3(inp,outp),\n                conv2d_3x3(outp,outp),\n                conv2d_3x3(outp,outp)\n            )\n\n        self.features = nn.Sequential(\n            block(3,128),\n            nn.MaxPool2d(2, 2, padding=0),\n            nn.Dropout2d(p=0.5),\n            block(128,256),\n            nn.MaxPool2d(2, 2, padding=0),\n            nn.Dropout2d(p=0.5),\n            conv2d_3x3(256, 512, pad=0),\n            conv2d_1x1(512, 256),\n            conv2d_1x1(256, 128),\n            nn.AvgPool2d(6, 6, padding=0)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n\n        phi  = self.features(x)\n        phi = phi.view(-1,128)\n        y = self.classifier(phi)\n\n        return phi, y\n\ndef conv2d(m,n,k,act=True):\n    layers =  [nn.Conv2d(m,n,k,padding=1)]\n\n    if act: layers += [nn.ELU()]\n\n    return nn.Sequential(\n        *layers\n    )\n\nclass SVHNmodel(nn.Module):\n\n    """"""\n    Model for application on SVHN data (32x32x3)\n    Architecture identical to https://github.com/haeusser/learning_by_association\n    """"""\n\n    def __init__(self):\n\n        super(SVHNmodel, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.InstanceNorm2d(3),\n            conv2d(3,  32, 3),\n            conv2d(32, 32, 3),\n            conv2d(32, 32, 3),\n            nn.MaxPool2d(2, 2, padding=0),\n            conv2d(32, 64, 3),\n            conv2d(64, 64, 3),\n            conv2d(64, 64, 3),\n            nn.MaxPool2d(2, 2, padding=0),\n            conv2d(64, 128, 3),\n            conv2d(128, 128, 3),\n            conv2d(128, 128, 3),\n            nn.AvgPool2d(8, 8, padding=0)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n\n        phi  = self.features(x)\n        phi = phi.view(-1,128)\n        y = self.classifier(phi)\n\n        return phi, y'"
salad/models/digits/dirtt.py,1,"b'import os\nimport os.path as osp\nimport sys\nimport argparse\n\nimport torch\nfrom torch import nn\n\nfrom salad import solver, models, datasets\n\nfrom salad.datasets.transforms.ensembling import ImageAugmentation\n\nfrom torch.nn import functional as F\nfrom salad.layers import AccuracyScore\n\nclass ConditionalBatchNorm(nn.Module):\n    \n    def __init__(self, *args, n_domains = 1, bn_func = nn.BatchNorm2d, **kwargs):\n        \n        super(ConditionalBatchNorm, self).__init__()\n        \n        self.n_domains = n_domains\n        self.layers    = [bn_func(*args, **kwargs) for i in range(n_domains)]\n        \n    def _apply(self, fn): \n        super(ConditionalBatchNorm, self)._apply(fn)\n        for layer in self.layers:\n            layer._apply(fn)\n        \n    def parameters(self, d=0):\n        return self.layers[d].parameters()\n        \n    def forward(self, x, d):\n                \n        layer = self.layers[d]\n        return layer(x) \n\nclass SVHN_MNIST_Model(nn.Module):\n    \n    def __init__(self, n_classes=10, n_domains=2):\n        super(SVHN_MNIST_Model, self).__init__()\n        \n        self.conditional_layers = []\n        self.n_domains = n_domains\n        \n        self.norm = nn.InstanceNorm2d(3, affine=False,\n                momentum=0,\n                track_running_stats=False)\n        \n        self.conv1_1 = nn.Conv2d(3, 128, (3, 3), padding=1)\n        self.conv1_1_bn = self._batch_norm(128)\n        self.conv1_2 = nn.Conv2d(128, 128, (3, 3), padding=1)\n        self.conv1_2_bn = self._batch_norm(128)\n        self.conv1_3 = nn.Conv2d(128, 128, (3, 3), padding=1)\n        self.conv1_3_bn = self._batch_norm(128)\n        self.pool1 = nn.MaxPool2d((2, 2))\n        self.drop1 = nn.Dropout()\n\n        self.conv2_1 = nn.Conv2d(128, 256, (3, 3), padding=1)\n        self.conv2_1_bn = self._batch_norm(256)\n        self.conv2_2 = nn.Conv2d(256, 256, (3, 3), padding=1)\n        self.conv2_2_bn = self._batch_norm(256)\n        self.conv2_3 = nn.Conv2d(256, 256, (3, 3), padding=1)\n        self.conv2_3_bn = self._batch_norm(256)\n        self.pool2 = nn.MaxPool2d((2, 2))\n        self.drop2 = nn.Dropout()\n\n        self.conv3_1 = nn.Conv2d(256, 512, (3, 3), padding=0)\n        self.conv3_1_bn = self._batch_norm(512)\n        self.nin3_2 = nn.Conv2d(512, 256, (1, 1), padding=1)\n        self.nin3_2_bn = self._batch_norm(256)\n        self.nin3_3 = nn.Conv2d(256, 128, (1, 1), padding=1)\n        self.nin3_3_bn = self._batch_norm(128)\n\n        self.fc4 = nn.Linear(128, n_classes)\n        \n    def _batch_norm(self, *args, **kwargs):\n        \n        layer = ConditionalBatchNorm(*args, n_domains=self.n_domains, **kwargs)\n        self.conditional_layers.append(layer)  \n        return layer\n    \n    def __call__(self, x, d=0):\n        \n        return self.forward(x, d)\n        \n    \n    def forward(self, x, d=0):\n        x = self.norm(x)\n        \n        x = F.relu(self.conv1_1_bn(self.conv1_1(x), d))\n        x = F.relu(self.conv1_2_bn(self.conv1_2(x), d))\n        x = self.pool1(F.relu(self.conv1_3_bn(self.conv1_3(x), d)))\n        x = self.drop1(x)\n\n        x = F.relu(self.conv2_1_bn(self.conv2_1(x), d))\n        x = F.relu(self.conv2_2_bn(self.conv2_2(x), d))\n        x = self.pool2(F.relu(self.conv2_3_bn(self.conv2_3(x), d)))\n        x = self.drop2(x)\n\n        x = F.relu(self.conv3_1_bn(self.conv3_1(x), d))\n        x = F.relu(self.nin3_2_bn(self.nin3_2(x), d))\n        x = F.relu(self.nin3_3_bn(self.nin3_3(x), d))\n\n        x = F.avg_pool2d(x, 6)\n        z = x = x.view(-1, 128)\n\n        x = self.fc4(x)\n        return z, x\n    \n    def conditional_params(self, d=0):\n        for module in self.conditional_layers:\n            for p in module.parameters(d):\n                yield p\n\n    def parameters(self, d=0, yield_shared=True, yield_conditional=True):\n\n        if yield_shared:\n            for param in super(SVHN_MNIST_Model, self).parameters():\n                yield param\n\n        if yield_conditional:\n            for param in self.conditional_params(d):\n                yield param\n\n'"
salad/models/digits/ensemble.py,3,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass ConditionalBatchNorm(nn.Module):\n    \n    def __init__(self, *args, n_domains = 1, bn_func = nn.BatchNorm2d, **kwargs):\n        \n        super(ConditionalBatchNorm, self).__init__()\n        \n        self.n_domains = n_domains\n        self.layers    = [bn_func(*args, affine=False, **kwargs) for i in range(n_domains)]\n\n        self.bias  = nn.Parameter(torch.zeros(args[0]).view(1,-1,1,1))\n        self.scale = nn.Parameter(torch.ones(args[0]).view(1,-1,1,1))\n        \n    def _apply(self, fn): \n        super(ConditionalBatchNorm, self)._apply(fn)\n        for layer in self.layers:\n            layer._apply(fn)\n        \n    def parameters(self, d=0):\n        return super().parameters()\n        \n    def forward(self, x, d):\n                \n        layer = self.layers[d]\n        return layer(x) * self.scale + self.bias \n\nclass SVHN_MNIST_Model(nn.Module):\n    \n    def __init__(self, n_classes=10, n_domains=2):\n        super(SVHN_MNIST_Model, self).__init__()\n        \n        self.conditional_layers = []\n        self.n_domains = n_domains\n        \n        self.norm = nn.InstanceNorm2d(3, affine=False,\n                momentum=0,\n                track_running_stats=False)\n        \n        self.conv1_1 = nn.Conv2d(3, 128, (3, 3), padding=1)\n        self.conv1_1_bn = self._batch_norm(128)\n        self.conv1_2 = nn.Conv2d(128, 128, (3, 3), padding=1)\n        self.conv1_2_bn = self._batch_norm(128)\n        self.conv1_3 = nn.Conv2d(128, 128, (3, 3), padding=1)\n        self.conv1_3_bn = self._batch_norm(128)\n        self.pool1 = nn.MaxPool2d((2, 2))\n        self.drop1 = nn.Dropout()\n\n        self.conv2_1 = nn.Conv2d(128, 256, (3, 3), padding=1)\n        self.conv2_1_bn = self._batch_norm(256)\n        self.conv2_2 = nn.Conv2d(256, 256, (3, 3), padding=1)\n        self.conv2_2_bn = self._batch_norm(256)\n        self.conv2_3 = nn.Conv2d(256, 256, (3, 3), padding=1)\n        self.conv2_3_bn = self._batch_norm(256)\n        self.pool2 = nn.MaxPool2d((2, 2))\n        self.drop2 = nn.Dropout()\n\n        self.conv3_1 = nn.Conv2d(256, 512, (3, 3), padding=0)\n        self.conv3_1_bn = self._batch_norm(512)\n        self.nin3_2 = nn.Conv2d(512, 256, (1, 1), padding=1)\n        self.nin3_2_bn = self._batch_norm(256)\n        self.nin3_3 = nn.Conv2d(256, 128, (1, 1), padding=1)\n        self.nin3_3_bn = self._batch_norm(128)\n\n        self.fc4 = nn.Linear(128, n_classes)\n        \n    def _batch_norm(self, *args, **kwargs):\n        \n        layer = ConditionalBatchNorm(*args, n_domains=self.n_domains, **kwargs)\n        self.conditional_layers.append(layer)  \n        return layer\n    \n    def __call__(self, x, d=0):\n        \n        return self.forward(x, d)\n        \n    \n    def forward(self, x, d=0):\n        x = self.norm(x)\n        \n        x = F.relu(self.conv1_1_bn(self.conv1_1(x), d))\n        x = F.relu(self.conv1_2_bn(self.conv1_2(x), d))\n        x = self.pool1(F.relu(self.conv1_3_bn(self.conv1_3(x), d)))\n        x = self.drop1(x)\n\n        x = F.relu(self.conv2_1_bn(self.conv2_1(x), d))\n        x = F.relu(self.conv2_2_bn(self.conv2_2(x), d))\n        x = self.pool2(F.relu(self.conv2_3_bn(self.conv2_3(x), d)))\n        x = self.drop2(x)\n\n        x = F.relu(self.conv3_1_bn(self.conv3_1(x), d))\n        x = F.relu(self.nin3_2_bn(self.nin3_2(x), d))\n        x = F.relu(self.nin3_3_bn(self.nin3_3(x), d))\n\n        x = F.avg_pool2d(x, 6)\n        z = x = x.view(-1, 128)\n\n        x = self.fc4(x)\n        return z, x\n    \n    def conditional_params(self, d=0):\n        for module in self.conditional_layers:\n            for p in module.parameters(d):\n                yield p\n\n    def parameters(self, d=0, yield_shared=True, yield_conditional=True):\n\n        if yield_shared:\n            for param in super(SVHN_MNIST_Model, self).parameters():\n                yield param\n\n        if yield_conditional:\n            for param in self.conditional_params(d):\n                yield param'"
salad/models/vision/__init__.py,0,b''
salad/models/vision/unet.py,4,"b'"""""" Models for image registration\n""""""\n\n__author__ = ""Steffen Schneider""\n__email__  = ""steffen.schneider@tum.de""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass double_conv(nn.Module):\n    \'\'\'(conv => BN => ReLU) * 2\'\'\'\n    def __init__(self, in_ch, out_ch):\n        super(double_conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_ch, out_ch, 3, padding=0),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(out_ch, out_ch, 3, padding=0),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nclass inconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(inconv, self).__init__()\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass down(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(down, self).__init__()\n        self.mpconv = nn.Sequential(\n            nn.MaxPool2d(2),\n            double_conv(in_ch, out_ch)\n        )\n\n    def forward(self, x):\n        x = self.mpconv(x)\n        return x\n\n\nclass up(nn.Module):\n    def __init__(self, in_ch, out_ch, bilinear=True):\n        super(up, self).__init__()\n\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode=""bilinear"", align_corners=True)\n        else:\n            self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        diffX = x1.size()[2] - x2.size()[2]\n        diffY = x1.size()[3] - x2.size()[3]\n        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n                        diffY // 2, int(diffY / 2)))\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        return x\n\n\nclass outconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(outconv, self).__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, channels=[16, 16, 32, 32]):\n\n        super(UNet, self).__init__()\n        \n        self.channels = channels\n\n        self.inc = inconv(n_channels, self.channels[0])\n        \n        layers_down = []\n        layers_up   = []\n        for src, tgt in zip(channels, self.channels[1:]):\n            layers_down.append(down(src, tgt))\n            layers_up.insert(0, up(tgt+src, src))\n            \n        self.down = nn.ModuleList(layers_down)\n        self.up   = nn.ModuleList(layers_up)\n    \n        self.outc = outconv(self.channels[0], n_classes)\n\n    def get_n_params(self):\n        \n        return sum(p.view(-1).size()[0] for p in self.parameters())\n        \n    def forward(self, x):\n        h = []\n        \n        h.append(self.inc(x))\n        \n        for down in self.down:\n            h.append(down(h[-1]))\n\n        # print(\'\\n\'.join([str(i.size()) for i in h]))\n    \n        x = h.pop()\n        for up in self.up:\n            y = h.pop()\n            x = up(x, y)\n        x = self.outc(x)\n        return x\n'"
salad/solver/da/__init__.py,0,"b'"""""" Domain Adaptation solvers\n""""""\n\nfrom .association import *\nfrom .base import *\nfrom .coral import *\nfrom .dann import *\nfrom .dirtt import *\nfrom .ensembling import *\nfrom .crossgrad import *\nfrom .advdrop import *'"
salad/solver/da/advdrop.py,6,"b'import salad\n\nimport torch\nfrom torch import nn\n\nfrom salad.solver.da import DABaseSolver\nfrom salad.layers import KLDivWithLogits\nfrom salad.optim import JointOptimizer\n\ndef pack(*args):\n    return torch.cat(args, 0)\n\ndef unpack(arg, n_tensors):\n\n    shape = arg.size()\n    return list(arg.view(n_tensors, shape[0] // n_tensors, *shape[1:]) )\n\n\nclass SymmetricKL(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.kl = KLDivWithLogits()\n\n    def forward(self, x, y):\n        return .5 * (self.kl(x,y.detach()) + self.kl(y, x.detach()))\n\nclass AdversarialDropoutLoss():\n    """""" Loss Derivation for Adversarial Dropout Regularization\n\n    See also\n    --------\n\n    salad.solver.AdversarialDropoutSolver\n    """"""\n\n    def __init__(self, model, step = 1):\n\n        self.model = model\n        self.step  = step\n\n\n    def step1(self, batch):\n\n        (xs, ys), (xt, yt) = batch\n\n        x = pack(xs, xt)\n        _, p = self.model(x)\n        ps, _ = unpack(p, 2)\n\n        return {\n            ""ce""    : (ps, ys),\n            ""acc_s"" : (ps, ys)\n        }\n\n    def step2(self, batch):\n\n        (xs, ys), (xt, yt) = batch\n\n        with torch.no_grad():\n            x = pack(xs, xt)\n            z  = self.model.features(x)\n            zs, zt = unpack(z, 2)\n\n        ps  = self.model.classifier(zs)\n        pt1 = self.model.classifier(zt)\n        pt2 = self.model.classifier(zt)\n    \n        return {\n            ""ce_C""      : (ps,  ys),\n            ""adv_C""   : (pt1, pt2),\n            ""acc_t""   : (pt1,  yt)\n        }\n\n    def step3(self, batch):\n\n        (xt, yt) = batch[1]\n        \n        zt1 = self.model.features(xt)\n        zt2 = self.model.features(xt)\n\n        #with torch.no_grad():\n        pt1 = self.model.classifier(zt1)\n        pt2 = self.model.classifier(zt2)\n\n        return {""adv_G"" : (pt1, pt2)}\n\n    def __call__(self, batch):\n       if self.step == 1: \n           return self.step1(batch)\n       elif self.step == 2: \n           return self.step2(batch)\n       elif self.step == 3:\n           return self.step3(batch)\n\n\nclass AdversarialDropoutSolver(DABaseSolver):\n    r"""""" Implementation of ""Adversarial Dropout Regularization""\n\n    Adversarial Dropout Regulariation [1]_ estimates uncertainties about the classification process\n    by sampling different models using dropout.\n    On the source domain, a standard cross entropy loss is employed.\n    On the target domain, two predictions are sampled from the model.\n\n    Both network parts are jointly trained on the source domain using the standard cross entropy loss,\n\n    ..math::\n\n        \\min_{C, G} - \\sum_k p^s_k \\log y^s_k\n\n    The classifier part of the network is trained to maximize the symmetric KL distance between\n    two predictions. This distance is one option for measuring uncertainty in a network. In other\n    words, the classifier aims at maximizing uncertainty given two noisy estimates of the current\n    feature vector.\n\n    ..math::\n\n        \\min_{C} - \\sum_k p^s_k \\log y^s_k + \\frac{p^t_k - q^t_k}{2} \\log \\frac{p^t_k}{q^t_k} \n\n\n    In contrast, the feature extrator aims at minimizing the uncertainty between two noisy samples\n    given a fixed target classifier.\n\n    ..math::\n\n        \\min_{G} \\frac{p^t_k - q^t_k}{2} \\log \\frac{p^t_k}{q^t_k}\n\n\n    References\n    ----------\n    \n    [1] Adversarial Dropout Regularization, Saito et al., ICLR 2018\n\n    """"""\n\n    def __init__(self, model, dataset, **kwargs):\n        super().__init__(model, dataset, **kwargs)\n\n    def _init_optims(self, lr_GC=0.0002, lr_C=0.0002, lr_G=0.0002, **kwargs):\n        super()._init_optims(**kwargs)\n\n        G_params   = list(self.model.features.parameters())\n        C_params  = list(self.model.classifier.parameters())\n        GC_params = G_params + C_params\n\n        # NOTE: Changing from three to two optimizers (As in the reference implementation)\n        # made a huge difference!\n\n        #opt_GC = torch.optim.Adam(GC_params, lr=lr_GC, weight_decay = 0.00005)\n        opt_G  = torch.optim.Adam(G_params, lr=lr_G, weight_decay = 0.0005)\n        opt_C  = torch.optim.Adam(C_params, lr=lr_C, weight_decay = 0.0005)\n        opt_GC = JointOptimizer(opt_C, opt_G)\n\n        self.register_optimizer(opt_GC, AdversarialDropoutLoss(self.model, step=1))\n        self.register_optimizer(opt_C,  AdversarialDropoutLoss(self.model, step=2))\n        self.register_optimizer(opt_G,  AdversarialDropoutLoss(self.model, step=3), n_steps = 4)\n\n    def _init_models(self, **kwargs):\n        super()._init_models(**kwargs)\n\n    def _init_losses(self, **kwargs):\n        super()._init_losses(**kwargs)\n\n        self.register_loss(nn.CrossEntropyLoss(), 1., ""ce_C"")\n        self.register_loss(SymmetricKL(), -1., ""adv_C"")\n        self.register_loss(SymmetricKL(), 1., ""adv_G"")\n'"
salad/solver/da/advdrop_refactor.py,5,"b'import salad\n\nimport torch\nfrom torch import nn\n\nfrom salad.solver.da import DABaseSolver\nfrom salad.layers import KLDivWithLogits\n\nclass SymmetricKL(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.kl = KLDivWithLogits()\n\n    def forward(self, x, y):\n        return .5 * (self.kl(x,y.detach()) + self.kl(y, x.detach()))\n\nclass AdversarialDropoutLoss():\n\n    def __init__(self, model, step):\n\n        self.model = model\n        self.step = step\n\n\n    def step1(self, batch):\n\n        (xs, ys), (xt, yt) = batch\n\n        zs = self.model.features(xs)\n        ps = self.model.classifier(xs)\n\n        return {\n            ""ce""    : (ps, ys),\n            ""acc_s"" : (ps, ys)\n        }\n\n    def step2(self, batch):\n\n        xs, ys = batch[0]\n\n        _, ps = self.model(xs)\n\n        return {\n            ""ce""    : (ps, ys),\n            ""acc_s"" : (ps, ys)\n        }\n\n    def step2(self, batch):\n\n        (xs, ys), (xt, yt) = batch\n\n        with torch.no_grad():\n            zs  = self.model.features(xs)\n            zt1 = self.model.features(xt)\n            zt2 = self.model.features(xt)\n\n        ps  = self.model.classifier(zs.detach())\n        pt1 = self.model.classifier(zt1.detach())\n        pt2 = self.model.classifier(zt2.detach())\n    \n        return {\n            ""ce""      : (ps,  ys),\n            ""adv_C""   : (pt1, pt2),\n            ""acc_t""   : (pt1,  yt)\n        }\n\n    def step3(self, batch):\n\n        (xt, yt) = batch[1]\n        \n        zt1 = self.model.features(xt)\n        zt2 = self.model.features(xt)\n\n        #with torch.no_grad():\n        pt1 = self.model.classifier(zt1)\n        pt2 = self.model.classifier(zt2)\n\n        return {""adv_G"" : (pt1, pt2)}\n\n    def __call__(self, batch):\n       if self.step == 1: \n           return self.step1(batch)\n       elif self.step == 2: \n           return self.step2(batch)\n       elif self.step == 3:\n           return self.step3(batch)\n\n\nclass AdversarialDropoutSolver(DABaseSolver):\n    r"""""" Implementation of ""Adversarial Dropout Regularization""\n\n    Adversarial Dropout Regulariation [1] estimates uncertainties about the classification process\n    by sampling different models using dropout.\n    On the source domain, a standard cross entropy loss is employed.\n    On the target domain, two predictions are sampled from the model.\n\n    Both network parts are jointly trained on the source domain using the standard cross entropy loss,\n\n    $$\n    \\min_{C, G} - \\sum_k p^s_k \\log y^s_k\n    $$\n\n    The classifier part of the network is trained to maximize the symmetric KL distance between\n    two predictions. This distance is one option for measuring uncertainty in a network. In other\n    words, the classifier aims at maximizing uncertainty given two noisy estimates of the current\n    feature vector.\n\n    $$\n    \\min_{C} - \\sum_k p^s_k \\log y^s_k + \\frac{p^t_k - q^t_k}{2} \\log \\frac{p^t_k}{q^t_k} \n    $$\n\n\n    In contrast, the feature extrator aims at minimizing the uncertainty between two noisy samples\n    given a fixed target classifier.\n\n    $$\n    \\min_{G} \\frac{p^t_k - q^t_k}{2} \\log \\frac{p^t_k}{q^t_k}\n    $$\n\n\n    References\n    ----------\n    \n    [1] Adversarial Dropout Regularization, Saito et al., ICLR 2018\n\n    """"""\n\n    def __init__(self, model, dataset, **kwargs):\n        super().__init__(model, dataset, **kwargs)\n\n    def _init_optims(self, lr_GC=3e-4, lr_C=3e-4, lr_G=3e-4, **kwargs):\n        super()._init_optims(**kwargs)\n\n        G_params  = list(self.model.features.parameters())\n        C_params  = list(self.model.classifier.parameters())\n        GC_params = G_params + C_params\n\n        opt_GC = torch.optim.Adam(GC_params, lr=lr_GC)\n        opt_G  = torch.optim.Adam(G_params, lr=lr_G)\n        opt_C  = torch.optim.Adam(C_params, lr=lr_C)\n\n        self.register_optimizer(opt_GC, AdversarialDropoutLoss(self.model, step=1))\n        self.register_optimizer(opt_C,  AdversarialDropoutLoss(self.model, step=2))\n        self.register_optimizer(opt_G,  AdversarialDropoutLoss(self.model, step=3))\n\n    def _init_models(self, **kwargs):\n        super()._init_models(**kwargs)\n\n    def _init_losses(self, **kwargs):\n        super()._init_losses(**kwargs)\n\n        self.register_loss(SymmetricKL(), 1, ""adv_C"")\n        self.register_loss(SymmetricKL(), 1, ""adv_G"")\n'"
salad/solver/da/association.py,4,"b'"""""" Associative Domain Adaptation\n\n[Hausser et al., CVPR 2017](#)\n""""""\n\n\n__author__ = ""Steffen Schneider""\n__email__  = ""steffen.schneider@tum.de""\n\nimport os, time\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\n\nfrom .. import Solver, BaseClassSolver\nfrom .base import DABaseSolver\nfrom ... import layers, optim\n\nimport itertools\n\nclass AssociationLoss(object):\n    """""" Loss function for associative domain adaptation\n\n    Given a model, derive a function that computes arguments for the association loss.\n    """"""\n\n    def __init__(self, model):\n\n        self.model = model\n\n    def __call__(self, batch):\n\n        (xs, ys), (xt, yt) = batch\n\n        phi_s, yp   = self.model(xs)\n        phi_t, ypt  = self.model(xt)\n\n        yp  = yp.clone()\n        ypt = ypt.clone()\n\n        losses = {}\n        losses[\'ce\']      = (yp, ys)\n        losses[\'assoc\']   = (phi_s, phi_t, ys)\n        losses[\'acc_s\']   = (yp, ys)\n        losses[\'acc_t\']   = (ypt, yt)\n\n        return losses\n\nclass AssociativeSolver(DABaseSolver):\n\n    r"""""" Implementation of ""Associative Domain Adaptation""\n\n    Associative Domain Adaptation [1] leverages a random walk based on feature similarity as a distance between source and \n    target feature correlations.\n    The algorithm is based on two loss functions that are added to the standard cross entropy loss on the source domain.\n\n    Given features for source and target domain, a kernel function is used to measure similiarity between both domains.\n    The original implementation uses the scalar product between feature vectors, scaled by an exponential,\n\n    .. math::\n        \n        K_{ij} = k(x^s_i, x^t_j) = \\exp(\\langle x^s_i, x^t_j \\rangle)\n\n    This kernel is then used to compute transition probabilities\n\n    .. math::\n        p(x^t_j | x^s_i) = \\frac{K_{ij}}{\\sum_{l} K_{lj}}\n\n    and \n\n    .. math::\n        \n        p(x^s_k | x^t_j) = \\frac{K_{jk}}{\\sum_{l} K_{kl}}\n\n    to compute the roundtrip\n\n    .. math::\n        \n        p(x^s_k | x^s_i) =  \\sum_{j} p(x^s_k | x^t_j) p(x^t_j | x^s_i)\n\n    It is then required that\n\n    1. `WalkerLoss` The roundtrip ends at a sample with the same class label, i.e., $y^s_i = y^s_k$\n    2. `VisitLoss`  Each target sample is visited with a certain probability \n\n    As one possible modification, different kernel functions could be used to measure similarity between\n    the domains.\n    With this solver, it is advised to use large sample sizes for the target domain and ensure that a\n    sufficient number of source samples is available for each batch.\n\n    TODO: Possibly in the solver class, implement a functionality to aggregate batches to avoid memory issues.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        A pytorch model to be trained by association\n\n    dataset : StackedDataset\n        A dataset suitable for an unsupervised solver\n    \n    learningrate : int\n        TODO\n\n    References\n    ----------\n\n    [1] Associative Domain Adaptation, H\xc3\xa4usser et al., CVPR 2017, https://arxiv.org/abs/1708.00938 \n    """"""\n\n    def __init__(self, model, dataset, learningrate,\n                    walker_weight = 1.,\n                    visit_weight = .1,\n                    *args, **kwargs):\n\n        super(AssociativeSolver, self).__init__(model, dataset,*args, **kwargs)\n\n    def _init_losses(self, walker_weight = 1., visit_weight = .1, **kwargs):\n        super()._init_losses(**kwargs)\n        loss = layers.AssociativeLoss(walker_weight=walker_weight,\n                                      visit_weight=visit_weight)\n        self.register_loss(loss, 1, \'assoc\')\n\n    def _init_optims(self, lr=3e-4, **kwargs):\n        super()._init_optims(**kwargs)\n        self.register_optimizer(torch.optim.Adam(self.model.parameters(),\n                                lr=lr),\n                                AssociationLoss(self.model))'"
salad/solver/da/base.py,4,"b'"""""" Solver classes for domain adaptation experiments \n""""""\n\n__author__ = ""Steffen Schneider""\n__email__  = ""steffen.schneider@tum.de""\n\nimport os, time\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\n\nfrom .. import Solver, BaseClassSolver\nfrom ... import layers, optim\n\nimport itertools\n\nclass DABaseSolver(BaseClassSolver):\n    \n    r"""""" Base Class for Unsupervised Domain Adaptation Approaches\n\n    Unsupervised DA assumes the presence of a single source domain :math:`\\mathcal S`\n    along with a target domain :math:`\\mathcal T` known at training time.\n    Given a labeled sample of points drawn from :math:`\\mathcal S`, :math:`\\{x^s_i, y^s_i\\}_{i}^{N_s}`,\n    and an unlabeled sample drawn from :math:`\\mathcal T`, :math:`\\{x^t_i\\}_{i}^{N_t}`, unsupervised\n    adaptation aims at minimizing the \n    \n    .. math::\n        \\min_\\theta \\mathcal{R}^l_{\\mathcal S} (\\theta) +  \\lambda \\mathcal{R}^u_{\\mathcal {S \\times T}} (\\theta), \n\n    leveraging an unsupervised risk term :math:`\\mathcal{R}^u_{\\mathcal {S \\times T}} (\\theta)` that depends on\n    feature representations :math:`f_\\theta(x^s,s)` and :math:`f_\\theta(x^t,t)`, \n    classifier labels :math:`h_\\theta(x^s,s), h_\\theta(x^t,t)` as well as source labels :math:`y^s`.\n    The full model :math:`h = g \\circ f` is a composition of a feature extractor :math:`f` and classifier :math:`g`, both of which\n    can possibly depend on the domain label :math:`s` or :math:`t` for domain-specific computations.\n\n    Notes\n    -----\n    This solver adds two accuracies with keys ``acc_s`` and ``acc_t`` for the source and target domain, respectively.\n    Make sure to include derivation of these accuracy in your loss computation.\n\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(DABaseSolver, self).__init__(*args, **kwargs)\n    \n    def _init_losses(self, **kwargs):\n        super()._init_losses(**kwargs)\n\n        self.register_loss(layers.AccuracyScore(), name = \'acc_s\', weight = None)\n        self.register_loss(layers.AccuracyScore(), name = \'acc_t\', weight = None)\n\n\nclass DGBaseSolver(BaseClassSolver):\n\n    r"""""" Base Class for Domain Generalization Approaches\n    \n    Domain generalization assumes the presence of multiple source domains alongside\n    a target domain unknown at training time.\n    Following \\cite{Shankar2018}, this setting requires a dataset of training examples\n    :math:`\\{x_i, y_i, d_i\\}_{i}^{N}` with class and domain labels.\n    Importantly, the domains present at training time should reflect the kind of variability\n    that can be expected during inference.\n    The ERM problem is then approached as\n\n    .. math::\n\n        \\min_\\theta \\sum_d \\mathcal{R}^l_{\\mathcal S_d} (\\theta) \n        = \\sum_d \\lambda_d \\mathbb{E}_{x,y \\sim \\mathcal S_d }[\\ell ( f_\\theta(x), h_\\theta(x), y, d) ].\n\n    In contrast to the unsupervised setting, samples are now presented in a single batch\n    comprised of inputs :math:`x`, labels :math:`y` and domains :math:`d`.\n    In a addition to a feature extractor :math:`f_\\theta` and classifier :math:`g_\\theta`, models should\n    also provide a feature extractor :math:`f^d_\\theta` to derive domain features along with a domain\n    classifier :math:`g^d_\\theta`, with possibly shared parameters.\n\n    In contrast to unsupervised DA, this training setting leverages information from multiple labeled\n    source domains with the goal of generalizing well on data from a previously unseen domain during\n    test time.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n\nclass DATeacher(Solver):\n    \n    """""" Base Class for Unsupervised Domain Adaptation Approaches using a teacher model\n    """"""\n\n    def __init__(self, model, teacher, dataset, *args, **kwargs):\n        super().__init__(model, dataset, *args, **kwargs)\n        self.teacher = teacher\n        \n    def _init_models(self, **kwargs):\n        super()._init_models(**kwargs)\n        self.register_model(self.teacher, \'teacher\')\n\nclass DABaselineLoss(object):\n\n    def __init__(self, solver):\n        self.solver = solver\n\n    def _predict(self, x, y):\n\n        _ , y_ = self.solver.model(x)\n        if not self.solver.multiclass:\n            y_ = y_.squeeze()\n            y  = y.float()\n\n        return y_, y\n\n    def __call__(self, batch):\n        losses = {}\n        (x, y) = batch[0]\n\n        losses[\'acc_s\'] = losses[\'ce\'] = self._predict(x,y)\n\n        with torch.no_grad():\n            x,y = batch[1]\n            losses[\'acc_t\'] =  self._predict(x,y)\n\n        return losses\n\nclass BaselineDASolver(DABaseSolver):\n    """""" A domain adaptation solver that actually does not run any adaptation algorithm\n\n    This is useful to establish baseline results for the case of no adaptation, for measurement\n    of the domain shift between datasets.\n    """"""\n\n    def _init_optims(self, lr = 3e-4, **kwargs):\n        super()._init_optims(**kwargs)\n\n        self.register_optimizer(torch.optim.Adam(self.model.parameters(),\n                                lr=lr, amsgrad=True),\n                                DABaselineLoss(self))'"
salad/solver/da/coral.py,6,"b'"""""" Losses for Correlatin Alignment \n\nDeep CORAL: Correlation Alignment for Deep Domain Adaptation\nPaper: https://arxiv.org/pdf/1607.01719.pdf\n\nMinimal Entropy Correlation Alignment for Unsupervised Domain Adaptation\nPaper: https://openreview.net/pdf?id=rJWechg0Z\n""""""\n\nimport torch \nfrom torch import nn\n\nfrom .base import DABaseSolver\nfrom ...layers import CoralLoss, LogCoralLoss, CorrelationDistance\n\nclass CentroidLoss(object):\n\n    def __init__(self, model):\n        self.model = model\n        self.n_classes = 10\n\n    def __call__(self, batch):\n        (src_x, src_y), (trg_x, trg_y___) = batch\n\n        src_e, src_p = self.model(src_x)\n        trg_e, trg_p = self.model(trg_x)\n        \n        with torch.no_grad():\n            _, trg_y = trg_p.max(dim = 1)\n\n        centroid_loss = []\n        for i in range(self.n_classes):\n            src_idc = torch.eq(src_y, i)\n            trg_idc = torch.eq(trg_y, i)\n            src_mu = src_e[src_idc].mean(axis=0)\n            trg_mu = trg_e[trg_idc].mean(axis=0)\n            centroid_loss.append((src_mu, trg_mu))\n\n        return {\n            \'ce\'           : (src_p, src_y),\n            \'corr\'         : (src_e, trg_e),\n            \'centroid\'     : centroid_loss,\n            \'acc_s\'        : (src_p, src_y),\n            \'acc_t\'        : (trg_p, trg_y___)\n        }\n\nclass CorrelationDistanceLoss(object):\n\n    def __init__(self, model, n_steps_recompute = 10, nullspace = False):\n        self.model = model\n\n        self.nullspace         = nullspace\n        self.last_transform    = n_steps_recompute\n        self.n_steps_recompute = n_steps_recompute\n        self.proj              = None\n\n    def _estimate_nullspace(self):\n\n        if self.last_transform < self.n_steps_recompute:\n            self.last_transform += 1\n            return self.proj\n\n        with torch.no_grad():\n            U,S,Vh = torch.svd(self.model.classifier.weight, some=True)\n            self.proj = Vh.mm(Vh.transpose(1,0))\n\n        self.last_transform = 0\n        return self.proj\n\n    def __call__(self, batch):\n        (src_x, src_y), (trg_x, trg_y___) = batch\n\n        src_e, src_p = self.model(src_x)\n        trg_e, trg_p = self.model(trg_x)\n\n        if self.nullspace:\n            N = self._estimate_nullspace()\n            src_e = src_e.mm(N)\n            trg_e = trg_e.mm(N)\n\n        return {\n            \'ce\'           : (src_p, src_y),\n            \'corr\'         : (src_e, trg_e),\n            \'acc_s\'        : (src_p, src_y),\n            \'acc_t\'        : (trg_p, trg_y___)\n        }\n\n####################################################################\n\nclass CorrelationDistanceSolver(DABaseSolver):\n\n    def __init__(self, model, dataset, *args, **kwargs):\n        super().__init__(model, dataset, *args, **kwargs)\n\n    def _init_losses(self, corr_weight = 1., corr_dist = None, **kwargs):\n        super()._init_losses(**kwargs)\n        self.register_loss(corr_dist, corr_weight, \'corr\')\n\n    def _init_optims(self, lr = 3e-4, use_nullspace = False, **kwargs):\n        super()._init_optims(**kwargs)\n        self.register_optimizer(torch.optim.Adam(self.model.parameters(),\n                                                lr=lr),\n                                CorrelationDistanceLoss(self.model, nullspace = use_nullspace))\n\nclass DeepCoralSolver(CorrelationDistanceSolver):\n    r""""""\n    Deep CORAL: Correlation Alignment for Deep Domain Adaptation\n    Paper: [https://arxiv.org/pdf/1607.01719.pdf](https://arxiv.org/pdf/1607.01719.pdf)\n\n    Loss Functions:\n\n    .. math::\n        \n        \\mathcal{L}(x^s, x^t) = \\frac{1}{4d^2} \\| C_s - C_t \\|\n\n    """"""\n\n    def __init__(self, model, dataset, *args, **kwargs):\n        super().__init__(model, dataset, corr_dist = CoralLoss(), *args, **kwargs)\n\nclass DeepLogCoralSolver(CorrelationDistanceSolver):\n    """"""\n    Minimal Entropy Correlation Alignment for Unsupervised Domain Adaptation\n    Paper: https://openreview.net/pdf?id=rJWechg0Z\n    """"""\n\n    def __init__(self, model, dataset, *args, **kwargs):\n        super().__init__(model, dataset, corr_dist = LogCoralLoss(), *args, **kwargs)\n\nclass CorrDistanceSolver(CorrelationDistanceSolver):\n    """"""\n    Minimal Entropy Correlation Alignment for Unsupervised Domain Adaptation\n    Paper: https://openreview.net/pdf?id=rJWechg0Z\n    """"""\n\n    def __init__(self, model, dataset, *args, **kwargs):\n        super().__init__(model, dataset, corr_dist = CorrelationDistance(), *args, **kwargs)\n\nclass CentroidDistanceLossSolver(CorrelationDistanceSolver):\n    """"""\n    Notes\n    -----\n\n    Needs work.\n    """"""\n\n    def __init__(self, model, dataset, *args, **kwargs):\n        super().__init__(model, dataset, corr_dist = CentroidLoss(), *args, **kwargs)\n\n    def _init_losses(self, centroid = 1., **kwargs):\n        super()._init_losses(**kwargs)\n\n        def loss(*args):\n            L = sum( ((x-y)**2).sum() for x,y in args )\n\n        self.register_loss(loss, centroid, name=\'centroid\')'"
salad/solver/da/crossgrad.py,6,"b'"""""" Cross Gradient Training\n\nICLR 2018\n""""""\n\nimport torch\nfrom torch import nn \n\nfrom ...optim import JointOptimizer\nfrom ...solver import Solver\nfrom ...layers import AccuracyScore, MeanAccuracyScore\n\nfrom .base import DGBaseSolver\n\ndef conv2d(m,n,k,act=True):\n    layers =  [nn.Conv2d(m,n,k,padding=1)]\n    if act: layers += [nn.ELU()]\n    return nn.Sequential(\n        *layers\n    )\n\ndef features(inp):\n    return nn.Sequential(\n        conv2d(inp,  32, 3),\n        conv2d(32, 32, 3),\n        conv2d(32, 32, 3),\n        nn.MaxPool2d(2, 2, padding=0),\n        conv2d(32, 64, 3),\n        conv2d(64, 64, 3),\n        conv2d(64, 64, 3),\n        nn.MaxPool2d(2, 2, padding=0),\n        conv2d(64, 128, 3),\n        conv2d(128, 128, 3),\n        conv2d(128, 128, 3),\n    )\n\nclass MultiDomainModule(nn.Module):\n\n    def __init__(self, n_domains):\n\n        super().__init__()\n\n        # TODO\n\n    def parameters_domain(self):\n        for p in self.feats_domain.parameters():\n            yield p \n        for p in self.domain.parameters():\n            yield p\n\n    def parameters_classifier(self):\n        for p in self.feats_class.parameters():\n            yield p \n        for p in self.classifier.parameters():\n            yield p\n\n    def forward(self, x):\n        zd, d = self.forward_domain(x)\n        x_ = concat(x, zd.detach())\n        zy = self.feats_class(x_)\n        zy = self.pool(zy).view(zy.size(0), zy.size(1))\n        y = self.classifier(zy)\n\n        return d, y\n\n    def forward_domain(self, x):\n        zd = self.feats_domain(x)\n        zd = self.pool(zd).view(zd.size(0), zd.size(1))\n        d  = self.domain(zd)\n        return zd, d\n\nclass Model(nn.Module):\n\n    def __init__(self, n_classes, n_domains):\n\n        super().__init__()\n\n        self.feats_domain = features(1)\n        self.feats_class  = features(1 + 128)\n\n        self.pool = nn.AdaptiveAvgPool2d(1)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(128, n_classes)\n        )\n        self.domain = nn.Sequential(\n            nn.Linear(128, n_domains)\n        )\n\n    def parameters_domain(self):\n        for p in self.feats_domain.parameters():\n            yield p \n        for p in self.domain.parameters():\n            yield p\n\n    def parameters_classifier(self):\n        for p in self.feats_class.parameters():\n            yield p \n        for p in self.classifier.parameters():\n            yield p\n\n    def forward(self, x):\n        zd, d = self.forward_domain(x)\n        x_ = concat(x, zd.detach())\n        zy = self.feats_class(x_)\n        zy = self.pool(zy).view(zy.size(0), zy.size(1))\n        y = self.classifier(zy)\n\n        return d, y\n\n    def forward_domain(self, x):\n        zd = self.feats_domain(x)\n        zd = self.pool(zd).view(zd.size(0), zd.size(1))\n        d  = self.domain(zd)\n        return zd, d\n\ndef concat(x, z):\n\n    """""" Concat 4D tensor with expanded 2D tensor\n    """"""\n\n    _,_,h,w = x.size()\n    n,d     = z.size()\n    z_ = z.view(n,d,1,1).expand(-1,-1,h,w)\n\n    return torch.cat([x, z_], dim=1)\n\nclass CrossGradLoss():\n\n    """""" Cross Gradient Training\n\n    References\n    ----------\n\n    ..[1]: http://arxiv.org/abs/1804.10745\n    """"""\n\n    def __init__(self, solver):\n\n        super().__init__()\n\n        self.loss    = solver.compute_loss_dict\n        self.model   = solver.model\n\n    def pertub(self, x, loss, eps = 1e-5):\n        loss.backward(retain_graph = True)\n        with torch.no_grad():\n            dx = x.grad\n            xd = x + eps * dx \n\n        return xd\n\n    def __call__(self, batch):\n        x, y, d = batch\n\n        x.requires_grad_(True)\n        d_, y_ = self.model(x)\n        \n        losses = self.loss({\n            \'ce_y\' : (y_, y),\n            \'ce_d\' : (d_, d)\n        })\n        losses[\'acc_y\'] = losses[\'meanacc_y\'] = (y_, y)\n        losses[\'acc_d\'] = losses[\'meanacc_d\'] = (d_, d)\n\n        x_d = self.pertub(x, losses[\'ce_y\'])\n        x_y = self.pertub(x, losses[\'ce_d\'])\n\n        _, d_ = self.model.forward_domain(x_d)\n        _, y_ = self.model(x_y)\n\n        losses.update({\n            \'cross_y\' : (y_, y), \n            \'cross_d\' : (d_, d)\n        })\n        return losses\n\nclass CrossGradSolver(DGBaseSolver):\n\n    r"""""" Cross Gradient Optimizer\n\n    A domain generalization solver based on Cross Gradient Training [1]_.\n\n    ..math:\n        p(y | x) = \\int_d p(y|x,d) p(d|x) dd\n\n    ..math:\n        x_d = x + \\eps \\Nabla_y L(y) \\\\\n        x_y = x + \\eps \\Nabla_d L(d)\n\n    References\n    ----------\n\n    .. [1] Shankar et al., Generalizing Across Domains via Cross-Gradient Training, ICLR 2018\n\n    """"""\n\n    def __init__(self, model, *args, **kwargs):\n\n        self.model = model\n        \n        super().__init__(*args, **kwargs)\n\n    def _init_models(self, **kwargs):\n        super()._init_models(**kwargs)\n\n        self.register_model(self.model, \'Model\')\n\n    def _init_optims(self, **kwargs):\n        super()._init_optims(**kwargs)\n\n        optim = torch.optim.Adam(self.model.parameters(), lr = 3e-4, amsgrad = True)\n\n        # TODO this might still be a bug?\n        #optim = JointOptimizer(\n        #    torch.optim.Adam(self.model.parameters_classifier(), lr = 3e-4, amsgrad = True),\n        #    torch.optim.Adam(self.model.parameters_domain(), lr = 3e-4, amsgrad = True)\n        #)\n\n        self.register_optimizer(optim, CrossGradLoss(self), name = ""optimizer"")\n\n    def _init_losses(self, **kwargs):\n        super()._init_losses(**kwargs)\n\n        for name in [\'ce_y\', \'ce_d\', \'cross_y\', \'cross_d\']:\n            self.register_loss(nn.CrossEntropyLoss(), weight = 1., name = name)\n        \n        self.register_loss(AccuracyScore(), weight = None, name = \'acc_y\')\n        self.register_loss(AccuracyScore(), weight = None, name = \'acc_d\')\n        self.register_loss(MeanAccuracyScore(), weight = None, name = \'meanacc_y\')\n        self.register_loss(MeanAccuracyScore(), weight = None, name = \'meanacc_d\')\n\ndef get_dataset(noisemodels, batch_size, shuffle = True, num_workers = 0, which=\'train\'):\n    from torchvision import transforms\n\n    data = []\n\n    noisemodels = [\n        transforms.RandomRotation([-1,1]),\n        transforms.RandomRotation([10,11]),\n        transforms.RandomRotation([20,21]),\n        transforms.RandomRotation([30,31]),\n    ]\n    \n    for N in noisemodels:\n    \n        transform = transforms.Compose([\n                transforms.ToTensor(),\n                N,\n                transforms.Normalize(mean=(0.43768448, 0.4437684,  0.4728041 ),\n                                    std= (0.19803017, 0.20101567, 0.19703583))\n        ])\n        mnist = datasets.mnist(\'/tmp/data\', train=True, download=True, transform=transform)\n\n        data.append(torch.utils.data.DataLoader(\n            svhn, batch_size=batch_size,\n            shuffle=shuffle, num_workers=num_workers))\n\n    loader = salad.datasets.JointLoader(*data)\n    return data, loader'"
salad/solver/da/dann.py,8,"b'__author__ = ""Steffen Schneider""\n__email__  = ""steffen.schneider@tum.de""\n\nimport os, time\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\n\nfrom .. import Solver, BaseClassSolver\nfrom ... import layers, optim\nfrom .base import DABaseSolver\n\nimport itertools\n\nclass AdversarialLoss(object):\n\n    def __init__(self, G, D, train_G = True):\n\n        self.D = D\n        self.G = G\n        self.train_G = train_G\n\n    def __call__(self, batch):\n\n        (src_x, src_y), (trg_x, trg_y) = batch\n\n        src_e, src_p = self.G(src_x)\n        trg_e, trg_p = self.G(trg_x)\n\n        # Compute outputs\n        src_logit   = self.D(src_e)\n        trg_logit   = self.D(trg_e)\n\n        if self.train_G:\n            return {\n                \'ce\'        : (src_p, src_y),\n                \'CL_src\'    : (src_logit, torch.zeros_like(src_logit)),\n                \'CL_tgt\'    : (trg_logit, torch.ones_like(trg_logit)),\n                \'acc_s\'     : (src_p, src_y),\n                \'acc_t\'     : (trg_p, trg_y)\n        }\n        else:\n            return {\n                \'D_src\'    : (src_logit, torch.ones_like(src_logit)),\n                \'D_tgt\'    : (trg_logit, torch.zeros_like(trg_logit))\n            }\n\nclass DANNSolver(DABaseSolver):\n\n    """""" Domain Adversarial Neural Networks Solver\n\n    This builds upon the normal classification solver that uses CrossEntropy or\n    BinaryCrossEntropy for optimizing neural networks.\n\n    Parameters\n    ----------\n\n    model : nn.Module\n        The model to train\n    discriminator : nn.Module\n        The domain discriminator. Feature dimension should match the values returned\n        by `model`\n    dataset : Dataset\n        A multi-domain dataset\n    lr_G : float\n        Model learning rate\n    lr_D : float\n        Discriminator learning rate\n    cl_weight : float\n        Classifier weight\n    d_weight : float\n        Discriminator weight\n    """"""\n\n    def __init__(self, model, discriminator, dataset, *args, **kwargs):\n        self.discriminator = discriminator\n        \n        super().__init__(model, dataset, *args, **kwargs)\n\n\n    def _init_models(self, **kwargs):\n        super()._init_models(**kwargs)\n        self.register_model(self.discriminator, \'discriminator\')\n\n    def _init_losses(self, cl_weight=1., d_weight=1., **kwargs):\n        super()._init_losses(**kwargs)\n\n        self.register_loss(nn.BCEWithLogitsLoss(), cl_weight * .5, \'CL_src\')\n        self.register_loss(nn.BCEWithLogitsLoss(), cl_weight * .5, \'CL_tgt\')\n        self.register_loss(nn.BCEWithLogitsLoss(), d_weight  * .5, \'D_src\' )\n        self.register_loss(nn.BCEWithLogitsLoss(), d_weight  * .5, \'D_tgt\' )\n\n\n    def _init_optims(self, lr_G = 3e-4, lr_D = 3e-4, **kwargs):\n        super()._init_optims(**kwargs)\n\n        self.register_optimizer(torch.optim.Adam(self.model.parameters(),\n                                                lr=lr_G),\n                                AdversarialLoss(self.model, self.discriminator, True),\n                                False)\n\n        self.register_optimizer(torch.optim.Adam(self.discriminator.parameters(),\n                                                lr=lr_D),\n                                AdversarialLoss(self.model, self.discriminator, False),\n                                False)'"
salad/solver/da/dirtt.py,11,"b'__author__ = ""Steffen Schneider""\n__email__  = ""steffen.schneider@tum.de""\n\nimport os, time\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\n\nfrom ... import layers, optim\nfrom .. import Solver, BaseClassSolver\nfrom .base import DABaseSolver\nfrom .dann import AdversarialLoss, DANNSolver \n\nimport itertools\n\n\nclass VADA(AdversarialLoss):\n\n    def __call__(self, batch):\n        # TODO improve instead of overriding\n        \n        (src_x, src_y), (trg_x, trg_y___) = batch\n\n        src_e, src_p = self.G(src_x)\n        trg_e, trg_p = self.G(trg_x)\n\n        # Compute outputs\n        real_logit   = self.D(src_e)\n        fake_logit   = self.D(trg_e)\n\n        if self.train_G:\n            return {\n            \'ce\'           : (src_p, src_y),\n            \'CL_src\'       : (real_logit, torch.zeros_like(real_logit)),\n            \'CL_tgt\'       : (fake_logit, torch.ones_like(fake_logit)),\n            \'VAT_src\'      : (src_x, src_p),\n            \'VAT_tgt\'      : (trg_x, trg_p),\n            \'H_tgt\'        : (trg_p,),\n            \'acc_s\'        : (src_p, src_y),\n            \'acc_t\'        : (trg_p, trg_y___)\n            }\n        else:\n            return {\n            \'D_src\'    : (real_logit, torch.ones_like(real_logit)),\n            \'D_tgt\'    : (fake_logit, torch.zeros_like(fake_logit))\n            }\n\nclass DIRTT():\n\n    def __init__(self, model, teacher):\n\n        self.model   = model\n        self.teacher = teacher\n\n    def __call__(self, batch):\n        \n        (trg_x, trg_y___) = batch\n\n        losses_student = {}\n\n        _, trg_y     = self.teacher(trg_x)\n        _, trg_p     = self.model(trg_x)\n\n        losses_student.update({\n            \'DIRT_tgt\'  : (trg_p, trg_y),\n            \'VAT_tgt\'   : (trg_x, trg_p),\n            \'H_tgt\'     : (trg_p,),\n            \'acc_t\'     : (trg_p, trg_y___)\n        })\n\n        return losses_student\n\nclass VADASolver(DANNSolver):\n\n    """""" Virtual Adversarial Domain Adaptation\n    """"""\n\n    def __init__(self, model, discriminator, dataset, *args, **kwargs):\n        super(VADASolver, self).__init__(model, discriminator, dataset, *args, **kwargs)\n\n    def _init_optims(self, **kwargs):\n        # override original call, but call init of higher class\n        DABaseSolver._init_optims(self)\n\n        opt_stud_src  = torch.optim.Adam(self.model.parameters(0), lr=3e-4)\n        opt = optim.JointOptimizer(opt_stud_src)\n        \n        loss_model = VADA(self.model, self.discriminator, train_G = True)\n        loss_disc  = VADA(self.model, self.discriminator, train_G = False)\n\n        self.register_optimizer(opt, loss_model)\n        self.register_optimizer(torch.optim.Adam(\n                                    self.discriminator.parameters(),\n                                    lr=3e-4),\n                                loss_disc)\n\n    def _init_losses(self, **kwargs):\n\n        super()._init_losses(cl_weight=1e-2)\n\n        self.register_loss(layers.VATLoss(self.model),  1, ""VAT_src"")\n        self.register_loss(layers.VATLoss(self.model),  1e-2, ""VAT_tgt"")\n        self.register_loss(layers.ConditionalEntropy(), 1e-2, ""H_tgt"")\n        \nclass DIRTTSolver(Solver):\n\n    """""" DIRT-T Finetuning on the Target Domain\n    """"""\n\n    def __init__(self, model, teacher, dataset, *args, **kwargs):\n        super().__init__(model, dataset, *args, **kwargs)\n        \n        self.model = model\n        self.teacher = teacher\n\n    def _init_models(self):\n        """""" Register student, teacher and discriminator model\n        """"""\n        self.register_model(self.model, \'Target model\')\n        self.register_model(self.teacher, \'Teacher\')\n        \n\n    def _init_optims(self):\n        opt_stud_src  = torch.optim.Adam(self.model.parameters(0), lr=3e-4)\n        opt = optim.JointOptimizer(opt_stud_src)\n        \n        loss_model = VADA(self.model, self.discriminator, train_G = True)\n\n        self.register_optimizer(opt, loss_model)\n        self.register_optimizer(torch.optim.Adam(\n                                    self.discriminator.parameters(),\n                                    lr=3e-4),\n                                loss_disc)\n\n    def _init_losses(self):\n\n        super()._init_losses(cl_weight=1e-2)\n\n        self.register_loss(layers.VATLoss(self.model),  1e-2, ""VAT_tgt"")\n        self.register_loss(layers.ConditionalEntropy(), 1e-2, ""H_tgt"")\n\n\n# class DIRTTSolver(Solver):\n#     """""" Train a Model using DIRT-T\n\n#     Reference:\n#     Shu et al (ICLR 2018).\n#     A DIRT-T approach to unsupervised domain adaptation.\n#     """"""\n\n#     def __init__(self, model, teacher, dataset,\n#                  learning_rate = 3e-4, teacher_alpha = .1,\n#                  *args, **kwargs):\n\n#         super(DIRTTSolver, self).__init__(dataset, *args, **kwargs)\n\n#         # Add the teacher model with Weight EMA training\n#         # Teacher uses gradient-free optimization\n#         self.model = model\n#         self.teacher = teacher\n#         student_params = list(self.model.parameters())\n#         teacher_params = list(self.teacher.parameters())\n#         for param in teacher_params:\n#             param.requires_grad = False\n\n#         self.register_model(self.teacher,\n#                             optim.DelayedWeight(teacher_params, student_params)\n#                             )\n#         self.register_model(self.model,\n#                             torch.optim.Adam(self.model.parameters(), 3e-4)\n#                             )\n\n\n\n#         self.register_loss(layers.VATLoss(self.model), 1, ""VAT_tgt"")\n#         self.register_loss(layers.ConditionalEntropy(), 1, ""H_tgt"")\n#         self.register_loss(layers.KLDivWithLogits(), 1, ""DIRT_tgt"")\n\n'"
salad/solver/da/dirtt_re.py,2,"b'"""""" Self Ensembling for Visual Domain Adaptation\n""""""\nimport torch\nfrom torch import nn\n\nfrom .base import DABaseSolver\nfrom ... import layers\nfrom ...layers import WeightedCE, AccuracyScore, MeanAccuracyScore\nfrom ...optim import WeightEMA\nfrom ... import optim\n\n\nclass DIRTT(object):\n\n    def __init__(self, model, teacher):\n\n        self.model   = model\n        self.teacher = teacher\n\n    def __call__(self, batch):        \n        (x_stud_xt,yt) = batch\n        \n        _, stud_yt  = self.model(x_stud_xt)\n        with torch.no_grad():\n            _, teach_yt = self.teacher(x_teach_xt)\n\n        losses = {        \n            \'ce\'           : (stud_yt, teach_yt.max(dim=-1)[1]),        \n            \'VAT_tgt\'      : (trg_x, trg_p),\n            \'H_tgt\'        : (trg_p,),\n            \'acc_t\'        : (trg_p, yt)\n        }\n\n        return losses\n\nclass DIRTTSolver(DABaseSolver):\n\n    def __init__(self, model, teacher, dataset, learningrate, *args, **kwargs):\n        super().__init__(model, dataset, *args, **kwargs)\n\n        teacher_alpha = 0.98\n        \n        self.register_model(teacher, ""teacher"")\n        self.teacher = teacher\n        \n        opt_stud_src  = torch.optim.Adam(model.parameters(), lr=learningrate)\n        opt_teach = WeightEMA(teacher.parameters(),\n                              model.parameters(),\n                              alpha=teacher_alpha)\n        \n        opt = optim.JointOptimizer(opt_stud_src, opt_teach)\n        \n\n        self.register_optimizer(opt, EnsemblingLoss(self.model, self.teacher),\n                               name=\'Joint Optimizer\')\n        self.register_loss(WeightedCE(), 3, \'ensemble\')\n        self.register_loss(AccuracyScore(), None, \'acc_teacher\')'"
salad/solver/da/djdot.py,0,"b'from .base import BaseClassSolver\n\nclass DJDOTSolver(BaseClassSolver):\n\n    """""" Deep Joint Optimal Transport solver\n\n    TODO\n    """"""\n\n    def __init__(self, model, dataset, *args, **kwargs):\n\n        super(DJDOTSolver, self).__init__(model, dataset, *args, **kwargs)\n\n    def derive_losses(self, batch):\n\n        # compute the\n\n        Gamma = None # optimal transport matrix\n\n        pass'"
salad/solver/da/ensembling.py,2,"b'"""""" Self Ensembling for Visual Domain Adaptation\n""""""\nimport torch\nfrom torch import nn\n\nfrom .base import DABaseSolver\nfrom ... import layers\nfrom ...layers import WeightedCE, AccuracyScore, MeanAccuracyScore\nfrom ...optim import WeightEMA\nfrom ... import optim\n\nclass EnsemblingLoss(object):\n\n    def __init__(self, model, teacher):\n\n        self.model   = model\n        self.teacher = teacher\n\n    def __call__(self, batch):        \n        (x_stud_xs, ys), (x_stud_xt,x_teach_xt,yt) = batch\n        \n        # TODO check if model is able to track domain\n        # TODO maybe write a wrapper to modify networks for this\n        _, stud_ys  = self.model(x_stud_xs, 0)\n        _, stud_yt  = self.model(x_stud_xt, 1)\n        with torch.no_grad():\n            _, teach_yt = self.teacher(x_teach_xt, 0)\n\n        losses = {}\n        losses[\'ce\']         = (stud_ys, ys)\n        losses[\'ensemble\']   = (stud_yt, teach_yt)\n        \n        losses[\'acc_s\']       = (stud_ys, ys)\n        losses[\'acc_t\']       = (stud_yt, yt)\n        losses[\'acc_teacher\'] = (teach_yt, yt)\n\n        return losses\n\nclass SelfEnsemblingSolver(DABaseSolver):\n    """""" Self-Ensembling for Visual Domain Adaptation\n\n    A solver for self-ensembling techniques, using the implementation \n    proposed in [1]_.\n    Note that the default hyperparameters are tuned to the small digit\n    benchmarks used by [1]_, and should be adapted when the solver\n    is used for new problem settings.\n\n    Parameters\n    ----------\n\n    model : nn.Module\n        The student model\n    teacher : nn.Module\n        The teacher model, should be equivalent to the student model\n        in terms of parameters\n    learningrate : float\n        Learningrate for the student model, for use with an Adam\n        optimizer\n    ensemble : float\n        Weight for the ensembling loss (default: 3)\n    confidence_threshold : float\n        Confidence threshold for the teacher model, between 0 and 1.\n        Values close to 1 are recommended (default: 0.96837722)\n    teacher_alpha : float\n        Decay parameter for the exponential moving average optimizer\n        used to determine the teacher weights. Values close to 1 are\n        desired\n\n    References\n    ----------\n\n    ..[1] French, Geoff, Michal Mackiewicz, and Mark Fisher. ""Self-ensembling for visual domain adaptation."" (2018).\n           https://arxiv.org/abs/1706.05208\n    """"""\n\n    def __init__(self, model, teacher, dataset, *args, **kwargs):\n        self.teacher = teacher\n        super(SelfEnsemblingSolver, self).__init__(model, dataset, *args, **kwargs)\n\n    def _init_optims(self, teacher_alpha = 0.99,\n                        learningrate = 3e-4, **kwargs):\n        super()._init_optims(**kwargs)\n        \n        opt_stud_src  = torch.optim.Adam(self.model.parameters(0),\n                                        lr=learningrate)\n        opt_teach = WeightEMA(self.teacher.parameters(0),\n                              self.model.parameters(0),\n                              alpha=teacher_alpha)\n        \n        opt = optim.JointOptimizer(opt_stud_src, opt_teach)\n\n        self.register_optimizer(opt,\n            EnsemblingLoss(self.model, self.teacher),\n            name=\'Joint Optimizer\')\n\n    def _init_models(self, **kwargs):\n        super()._init_models(**kwargs)\n        self.register_model(self.teacher, ""teacher"")\n\n    def _init_losses(self, ensemble = 3,\n                        confidence_threshold = 0.96837722,\n                        **kwargs):\n        super()._init_losses(**kwargs)\n\n        self.register_loss(WeightedCE(), ensemble, \'ensemble\')\n        self.register_loss(AccuracyScore(), None, \'acc_teacher\')'"
