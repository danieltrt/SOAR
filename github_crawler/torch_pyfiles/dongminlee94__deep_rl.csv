file_path,api_count,code
__init__.py,0,b'# coding=utf-8'
run_cartpole.py,5,"b'import os\nimport gym\nimport time\nimport argparse\nimport datetime\nimport numpy as np\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Configurations\nparser = argparse.ArgumentParser(description=\'RL algorithms with PyTorch in CartPole environment\')\nparser.add_argument(\'--env\', type=str, default=\'CartPole-v1\', \n                    help=\'cartpole environment\')\nparser.add_argument(\'--algo\', type=str, default=\'dqn\', \n                    help=\'select an algorithm among dqn, ddqn, a2c\')\nparser.add_argument(\'--seed\', type=int, default=0, \n                    help=\'seed for random number generators\')\nparser.add_argument(\'--training_eps\', type=int, default=500, \n                    help=\'training episode number\')\nparser.add_argument(\'--eval_per_train\', type=int, default=50, \n                    help=\'evaluation number per training\')\nparser.add_argument(\'--evaluation_eps\', type=int, default=100,\n                    help=\'evaluation episode number\')\nparser.add_argument(\'--max_step\', type=int, default=500,\n                    help=\'max episode step\')\nparser.add_argument(\'--threshold_return\', type=int, default=500,\n                    help=\'solved requirement for success in given environment\')\nparser.add_argument(\'--tensorboard\', type=bool, default=True)\nparser.add_argument(\'--gpu_index\', type=int, default=0)\nargs = parser.parse_args()\ndevice = torch.device(\'cuda\', index=args.gpu_index) if torch.cuda.is_available() else torch.device(\'cpu\')\n\nif args.algo == \'dqn\':\n    from agents.dqn import Agent\nelif args.algo == \'ddqn\': # Just replace the target of DQN with Double DQN\n    from agents.dqn import Agent\nelif args.algo == \'a2c\':\n    from agents.a2c import Agent\n\n\ndef main():\n    """"""Main.""""""\n    # Initialize environment\n    env = gym.make(args.env)\n    obs_dim = env.observation_space.shape[0]\n    act_num = env.action_space.n\n    print(\'State dimension:\', obs_dim)\n    print(\'Action number:\', act_num)\n\n    # Set a random seed\n    env.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    # Create an agent\n    agent = Agent(env, args, device, obs_dim, act_num)\n\n    # Create a SummaryWriter object by TensorBoard\n    if args.tensorboard:\n        dir_name = \'runs/\' + args.env + \'/\' \\\n                           + args.algo \\\n                           + \'_s_\' + str(args.seed) \\\n                           + \'_t_\' + datetime.datetime.now().strftime(""%Y-%m-%d-%H-%M-%S"")\n        writer = SummaryWriter(log_dir=dir_name)\n\n    start_time = time.time()\n\n    train_num_steps = 0\n    train_sum_returns = 0.\n    train_num_episodes = 0\n\n    # Runs a full experiment, spread over multiple training episodes\n    for episode in range(1, args.training_eps+1):\n        # Perform the training phase, during which the agent learns\n        agent.eval_mode = False\n        \n        # Run one episode\n        train_step_length, train_episode_return = agent.run(args.max_step)\n        \n        train_num_steps += train_step_length\n        train_sum_returns += train_episode_return\n        train_num_episodes += 1\n\n        train_average_return = train_sum_returns / train_num_episodes if train_num_episodes > 0 else 0.0\n\n        # Log experiment result for training episodes\n        if args.tensorboard:\n            writer.add_scalar(\'Train/AverageReturns\', train_average_return, episode)\n            writer.add_scalar(\'Train/EpisodeReturns\', train_episode_return, episode)\n\n        # Perform the evaluation phase -- no learning\n        if episode > 0 and episode % args.eval_per_train == 0:\n            agent.eval_mode = True\n            \n            eval_sum_returns = 0.\n            eval_num_episodes = 0\n\n            for _ in range(args.evaluation_eps):\n                # Run one episode\n                eval_step_length, eval_episode_return = agent.run(args.max_step)\n\n                eval_sum_returns += eval_episode_return\n                eval_num_episodes += 1\n\n            eval_average_return = eval_sum_returns / eval_num_episodes if eval_num_episodes > 0 else 0.0\n\n            # Log experiment result for evaluation episodes\n            if args.tensorboard:\n                writer.add_scalar(\'Eval/AverageReturns\', eval_average_return, episode)\n                writer.add_scalar(\'Eval/EpisodeReturns\', eval_episode_return, episode)\n\n            print(\'---------------------------------------\')\n            print(\'Steps:\', train_num_steps)\n            print(\'Episodes:\', train_num_episodes)\n            print(\'AverageReturn:\', round(train_average_return, 2))\n            print(\'EvalEpisodes:\', eval_num_episodes)\n            print(\'EvalAverageReturn:\', round(eval_average_return, 2))\n            print(\'OtherLogs:\', agent.logger)\n            print(\'Time:\', int(time.time() - start_time))\n            print(\'---------------------------------------\')\n\n            # Save the trained model\n            if eval_average_return >= args.threshold_return:\n                if not os.path.exists(\'./tests/save_model\'):\n                    os.mkdir(\'./tests/save_model\')\n                \n                ckpt_path = os.path.join(\'./tests/save_model/\' + args.env + \'_\' + args.algo \\\n                                                                          + \'_s_\' + str(args.seed) \\\n                                                                          + \'_ep_\' + str(train_num_episodes) \\\n                                                                          + \'_tr_\' + str(round(train_average_return, 2)) \\\n                                                                          + \'_er_\' + str(round(eval_average_return, 2)) + \'.pt\')\n                \n                if args.algo == \'dqn\' or args.algo == \'ddqn\':\n                    torch.save(agent.qf.state_dict(), ckpt_path)\n                else:\n                    torch.save(agent.policy.state_dict(), ckpt_path)\n\nif __name__ == ""__main__"":\n    main()\n'"
run_mujoco.py,4,"b'import os\nimport gym\nimport time\nimport argparse\nimport datetime\nimport numpy as np\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Configurations\nparser = argparse.ArgumentParser(description=\'RL algorithms with PyTorch in MuJoCo environments\')\nparser.add_argument(\'--env\', type=str, default=\'Humanoid-v2\', \n                    help=\'choose an environment between HalfCheetah-v2, Ant-v2 and Humanoid-v2\')\nparser.add_argument(\'--algo\', type=str, default=\'atac\', \n                    help=\'select an algorithm among vpg, npg, trpo, ppo, ddpg, td3, sac, asac, tac, atac\')\nparser.add_argument(\'--seed\', type=int, default=0, \n                    help=\'seed for random number generators\')\nparser.add_argument(\'--iterations\', type=int, default=200, \n                    help=\'iterations to run and train agent\')\nparser.add_argument(\'--steps_per_iter\', type=int, default=5000, \n                    help=\'steps of interaction for the agent and the environment in each epoch\')\nparser.add_argument(\'--max_step\', type=int, default=1000,\n                    help=\'max episode step\')\nparser.add_argument(\'--tensorboard\', type=bool, default=True)\nparser.add_argument(\'--gpu_index\', type=int, default=0)\nargs = parser.parse_args()\ndevice = torch.device(\'cuda\', index=args.gpu_index) if torch.cuda.is_available() else torch.device(\'cpu\')\n\nif args.algo == \'vpg\':\n    from agents.vpg import Agent\nelif args.algo == \'npg\':\n    from agents.trpo import Agent\nelif args.algo == \'trpo\':\n    from agents.trpo import Agent\nelif args.algo == \'ppo\':\n    from agents.ppo import Agent\nelif args.algo == \'ddpg\':\n    from agents.ddpg import Agent\nelif args.algo == \'td3\':\n    from agents.td3 import Agent\nelif args.algo == \'sac\':\n    from agents.sac import Agent\nelif args.algo == \'asac\': # Automating entropy adjustment on SAC\n    from agents.sac import Agent\nelif args.algo == \'tac\': \n    from agents.sac import Agent\nelif args.algo == \'atac\': # Automating entropy adjustment on TAC\n    from agents.sac import Agent\n\n\ndef main():\n    """"""Main.""""""\n    # Initialize environment\n    env = gym.make(args.env)\n    obs_dim = env.observation_space.shape[0]\n    act_dim = env.action_space.shape[0]\n    act_limit = env.action_space.high[0]\n    print(\'State dimension:\', obs_dim)\n    print(\'Action dimension:\', act_dim)\n\n    # Set a random seed\n    env.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    # Create an agent\n    if args.algo == \'ddpg\' or args.algo == \'td3\':\n        agent = Agent(env, args, device, obs_dim, act_dim, act_limit, \n                      expl_before=10000, \n                      act_noise=0.1, \n                      hidden_sizes=(256,256), \n                      buffer_size=int(1e6), \n                      batch_size=256,\n                      policy_lr=3e-4, \n                      qf_lr=3e-4)\n    elif args.algo == \'sac\':                                                                                    \n        agent = Agent(env, args, device, obs_dim, act_dim, act_limit,                   \n                      expl_before=10000,                                \n                      alpha=0.2,                        # In HalfCheetah-v2 and Ant-v2, SAC with 0.2  \n                      hidden_sizes=(256,256),           # shows the best performance in entropy coefficient \n                      buffer_size=int(1e6),             # while, in Humanoid-v2, SAC with 0.05 shows the best performance.\n                      batch_size=256,\n                      policy_lr=3e-4, \n                      qf_lr=3e-4)     \n    elif args.algo == \'asac\':\n        agent = Agent(env, args, device, obs_dim, act_dim, act_limit, \n                      expl_before=10000, \n                      automatic_entropy_tuning=True, \n                      hidden_sizes=(256,256), \n                      buffer_size=int(1e6), \n                      batch_size=256,\n                      policy_lr=3e-4,\n                      qf_lr=3e-4)\n    elif args.algo == \'tac\':                                                                                    \n        agent = Agent(env, args, device, obs_dim, act_dim, act_limit,                   \n                      expl_before=10000,                                \n                      alpha=0.2,                                       \n                      log_type=\'log-q\',                 \n                      entropic_index=1.2,               \n                      hidden_sizes=(256,256),          \n                      buffer_size=int(1e6), \n                      batch_size=256,\n                      policy_lr=3e-4,\n                      qf_lr=3e-4)\n    elif args.algo == \'atac\':\n        agent = Agent(env, args, device, obs_dim, act_dim, act_limit, \n                      expl_before=10000, \n                      log_type=\'log-q\', \n                      entropic_index=1.2, \n                      automatic_entropy_tuning=True,\n                      hidden_sizes=(256,256), \n                      buffer_size=int(1e6), \n                      batch_size=256,\n                      policy_lr=3e-4,\n                      qf_lr=3e-4)\n    else: # vpg, npg, trpo, ppo\n        agent = Agent(env, args, device, obs_dim, act_dim, act_limit, sample_size=4096)\n\n    # Create a SummaryWriter object by TensorBoard\n    if args.tensorboard:\n        dir_name = \'runs/\' + args.env + \'/\' \\\n                           + args.algo \\\n                           + \'_s_\' + str(args.seed) \\\n                           + \'_t_\' + datetime.datetime.now().strftime(""%Y-%m-%d-%H-%M-%S"")\n        writer = SummaryWriter(log_dir=dir_name)\n\n    start_time = time.time()\n\n    total_num_steps = 0\n    train_sum_returns = 0.\n    train_num_episodes = 0\n\n    # Main loop\n    for i in range(args.iterations):\n        train_step_count = 0\n        while train_step_count <= args.steps_per_iter:\n            # Perform the training phase, during which the agent learns\n            agent.eval_mode = False\n            \n            # Run one episode\n            train_step_length, train_episode_return = agent.run(args.max_step)\n            \n            total_num_steps += train_step_length\n            train_step_count += train_step_length\n            train_sum_returns += train_episode_return\n            train_num_episodes += 1\n\n            train_average_return = train_sum_returns / train_num_episodes if train_num_episodes > 0 else 0.0\n\n            # Log experiment result for training steps\n            if args.tensorboard:\n                writer.add_scalar(\'Train/AverageReturns\', train_average_return, total_num_steps)\n                writer.add_scalar(\'Train/EpisodeReturns\', train_episode_return, total_num_steps)\n                if args.algo == \'asac\' or args.algo == \'atac\':\n                    writer.add_scalar(\'Train/Alpha\', agent.alpha, total_num_steps)\n\n        # Perform the evaluation phase -- no learning\n        agent.eval_mode = True\n        \n        eval_sum_returns = 0.\n        eval_num_episodes = 0\n\n        for _ in range(10):\n            # Run one episode\n            eval_step_length, eval_episode_return = agent.run(args.max_step)\n\n            eval_sum_returns += eval_episode_return\n            eval_num_episodes += 1\n\n        eval_average_return = eval_sum_returns / eval_num_episodes if eval_num_episodes > 0 else 0.0\n\n        # Log experiment result for evaluation steps\n        if args.tensorboard:\n            writer.add_scalar(\'Eval/AverageReturns\', eval_average_return, total_num_steps)\n            writer.add_scalar(\'Eval/EpisodeReturns\', eval_episode_return, total_num_steps)\n\n        print(\'---------------------------------------\')\n        print(\'Iterations:\', i)\n        print(\'Steps:\', total_num_steps)\n        print(\'Episodes:\', train_num_episodes)\n        print(\'AverageReturn:\', round(train_average_return, 2))\n        print(\'EvalEpisodes:\', eval_num_episodes)\n        print(\'EvalAverageReturn:\', round(eval_average_return, 2))\n        print(\'OtherLogs:\', agent.logger)\n        print(\'Time:\', int(time.time() - start_time))\n        print(\'---------------------------------------\')\n\n        # Save the trained model\n        if (i + 1) % 20 == 0:\n            if not os.path.exists(\'./tests/save_model\'):\n                os.mkdir(\'./tests/save_model\')\n            \n            ckpt_path = os.path.join(\'./tests/save_model/\' + args.env + \'_\' + args.algo \\\n                                                                      + \'_s_\' + str(args.seed) \\\n                                                                      + \'_i_\' + str(i) \\\n                                                                      + \'_tr_\' + str(round(train_average_return, 2)) \\\n                                                                      + \'_er_\' + str(round(eval_average_return, 2)) + \'.pt\')\n            \n            torch.save(agent.policy.state_dict(), ckpt_path)\n\nif __name__ == ""__main__"":\n    main()\n'"
run_pendulum.py,4,"b'import os\nimport gym\nimport time\nimport argparse\nimport datetime\nimport numpy as np\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Configurations\nparser = argparse.ArgumentParser(description=\'RL algorithms with PyTorch in Pendulum environment\')\nparser.add_argument(\'--env\', type=str, default=\'Pendulum-v0\', \n                    help=\'pendulum environment\')\nparser.add_argument(\'--algo\', type=str, default=\'atac\', \n                    help=\'select an algorithm among vpg, npg, trpo, ppo, ddpg, td3, sac, asac, tac, atac\')\nparser.add_argument(\'--seed\', type=int, default=0, \n                    help=\'seed for random number generators\')\nparser.add_argument(\'--training_eps\', type=int, default=1000, \n                    help=\'training episode number\')\nparser.add_argument(\'--eval_per_train\', type=int, default=50, \n                    help=\'evaluation number per training\')\nparser.add_argument(\'--evaluation_eps\', type=int, default=100,\n                    help=\'evaluation episode number\')\nparser.add_argument(\'--max_step\', type=int, default=200,\n                    help=\'max episode step\')\nparser.add_argument(\'--threshold_return\', type=int, default=-230,\n                    help=\'solved requirement for success in given environment\')\nparser.add_argument(\'--tensorboard\', type=bool, default=True)\nparser.add_argument(\'--gpu_index\', type=int, default=0)\nargs = parser.parse_args()\ndevice = torch.device(\'cuda\', index=args.gpu_index) if torch.cuda.is_available() else torch.device(\'cpu\')\n\nif args.algo == \'vpg\':\n    from agents.vpg import Agent\nelif args.algo == \'npg\':\n    from agents.trpo import Agent\nelif args.algo == \'trpo\':\n    from agents.trpo import Agent\nelif args.algo == \'ppo\':\n    from agents.ppo import Agent\nelif args.algo == \'ddpg\':\n    from agents.ddpg import Agent\nelif args.algo == \'td3\':\n    from agents.td3 import Agent\nelif args.algo == \'sac\':\n    from agents.sac import Agent\nelif args.algo == \'asac\': # Automating entropy adjustment on SAC\n    from agents.sac import Agent\nelif args.algo == \'tac\': \n    from agents.sac import Agent\nelif args.algo == \'atac\': # Automating entropy adjustment on TAC\n    from agents.sac import Agent\n\n\ndef main():\n    """"""Main.""""""\n    # Initialize environment\n    env = gym.make(args.env)\n    obs_dim = env.observation_space.shape[0]\n    act_dim = env.action_space.shape[0]\n    act_limit = env.action_space.high[0]\n    print(\'State dimension:\', obs_dim)\n    print(\'Action dimension:\', act_dim)\n\n    # Set a random seed\n    env.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    # Create an agent\n    if args.algo == \'ddpg\' or args.algo == \'td3\':\n        agent = Agent(env, args, device, obs_dim, act_dim, act_limit)\n    elif args.algo == \'sac\':\n        agent = Agent(env, args, device, obs_dim, act_dim, act_limit, \n                      alpha=0.5)\n    elif args.algo == \'asac\':\n        agent = Agent(env, args, device, obs_dim, act_dim, act_limit, \n                      automatic_entropy_tuning=True)\n    elif args.algo == \'tac\':\n        agent = Agent(env, args, device, obs_dim, act_dim, act_limit, \n                      alpha=0.5,\n                      log_type=\'log-q\', \n                      entropic_index=1.2)\n    elif args.algo == \'atac\':\n        agent = Agent(env, args, device, obs_dim, act_dim, act_limit, \n                      log_type=\'log-q\', \n                      entropic_index=1.2, \n                      automatic_entropy_tuning=True)\n    else: # vpg, npg, trpo, ppo\n        agent = Agent(env, args, device, obs_dim, act_dim, act_limit)\n\n    # Create a SummaryWriter object by TensorBoard\n    if args.tensorboard:\n        dir_name = \'runs/\' + args.env + \'/\' \\\n                           + args.algo \\\n                           + \'_s_\' + str(args.seed) \\\n                           + \'_t_\' + datetime.datetime.now().strftime(""%Y-%m-%d-%H-%M-%S"")\n        writer = SummaryWriter(log_dir=dir_name)\n\n    start_time = time.time()\n\n    train_num_steps = 0\n    train_sum_returns = 0.\n    train_num_episodes = 0\n\n    # Runs a full experiment, spread over multiple training episodes\n    for episode in range(1, args.training_eps+1):\n        # Perform the training phase, during which the agent learns\n        agent.eval_mode = False\n        \n        # Run one episode\n        train_step_length, train_episode_return = agent.run(args.max_step)\n        \n        train_num_steps += train_step_length\n        train_sum_returns += train_episode_return\n        train_num_episodes += 1\n\n        train_average_return = train_sum_returns / train_num_episodes if train_num_episodes > 0 else 0.0\n\n        # Log experiment result for training episodes\n        if args.tensorboard:\n            writer.add_scalar(\'Train/AverageReturns\', train_average_return, episode)\n            writer.add_scalar(\'Train/EpisodeReturns\', train_episode_return, episode)\n            if args.algo == \'asac\' or args.algo == \'atac\':\n                writer.add_scalar(\'Train/Alpha\', agent.alpha, episode)\n\n        # Perform the evaluation phase -- no learning\n        if episode > 0 and episode % args.eval_per_train == 0:\n            agent.eval_mode = True\n            \n            eval_sum_returns = 0.\n            eval_num_episodes = 0\n\n            for _ in range(args.evaluation_eps):\n                # Run one episode\n                eval_step_length, eval_episode_return = agent.run(args.max_step)\n\n                eval_sum_returns += eval_episode_return\n                eval_num_episodes += 1\n\n            eval_average_return = eval_sum_returns / eval_num_episodes if eval_num_episodes > 0 else 0.0\n\n            # Log experiment result for evaluation episodes\n            if args.tensorboard:\n                writer.add_scalar(\'Eval/AverageReturns\', eval_average_return, episode)\n                writer.add_scalar(\'Eval/EpisodeReturns\', eval_episode_return, episode)\n\n            print(\'---------------------------------------\')\n            print(\'Steps:\', train_num_steps)\n            print(\'Episodes:\', train_num_episodes)\n            print(\'AverageReturn:\', round(train_average_return, 2))\n            print(\'EvalEpisodes:\', eval_num_episodes)\n            print(\'EvalAverageReturn:\', round(eval_average_return, 2))\n            print(\'OtherLogs:\', agent.logger)\n            print(\'Time:\', int(time.time() - start_time))\n            print(\'---------------------------------------\')\n\n            # Save the trained model\n            if eval_average_return >= args.threshold_return:\n                if not os.path.exists(\'./tests/save_model\'):\n                    os.mkdir(\'./tests/save_model\')\n                \n                ckpt_path = os.path.join(\'./tests/save_model/\' + args.env + \'_\' + args.algo \\\n                                                                          + \'_s_\' + str(args.seed) \\\n                                                                          + \'_ep_\' + str(train_num_episodes) \\\n                                                                          + \'_tr_\' + str(round(train_average_return, 2)) \\\n                                                                          + \'_er_\' + str(round(eval_average_return, 2)) + \'.pt\')\n                \n                torch.save(agent.policy.state_dict(), ckpt_path)\n\nif __name__ == ""__main__"":\n    main()\n'"
agents/__init__.py,0,b'# coding=utf-8'
agents/a2c.py,7,"b'import numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom agents.common.networks import *\n\n\nclass Agent(object):\n   """"""An implementation of the Advantage Actor-Critic (A2C) agent.""""""\n\n   def __init__(self,\n                env,\n                args,\n                device,\n                obs_dim,\n                act_num,\n                steps=0,\n                gamma=0.99,\n                policy_lr=3e-4,\n                vf_lr=1e-3,\n                eval_mode=False,\n                policy_losses=list(),\n                vf_losses=list(),\n                logger=dict(),\n   ):\n\n      self.env = env\n      self.args = args\n      self.device = device\n      self.obs_dim = obs_dim\n      self.act_num = act_num\n      self.steps = steps \n      self.gamma = gamma\n      self.policy_lr = policy_lr\n      self.vf_lr = vf_lr\n      self.eval_mode = eval_mode\n      self.policy_losses = policy_losses\n      self.vf_losses = vf_losses\n      self.logger = logger\n\n      # Policy network\n      self.policy = CategoricalPolicy(self.obs_dim, self.act_num, activation=torch.tanh).to(self.device)\n      # Value network\n      self.vf = MLP(self.obs_dim, 1, activation=torch.tanh).to(self.device)\n      \n      # Create optimizers\n      self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=self.policy_lr)\n      self.vf_optimizer = optim.Adam(self.vf.parameters(), lr=self.vf_lr)\n      \n   def select_action(self, obs):\n      """"""Select an action from the set of available actions.""""""\n      action, _, log_pi  = self.policy(obs)\n      \n      # Prediction V(s)\n      v = self.vf(obs)\n\n      self.transition.extend([log_pi, v])\n      return action.detach().cpu().numpy()\n\n   def train_model(self):\n      log_pi, v, next_obs, reward, done = self.transition\n\n      # Prediction V(s\')\n      next_v = self.vf(torch.Tensor(next_obs).to(self.device))\n      \n      # Target for Q regression\n      q = reward + self.gamma*(1-done)*next_v\n      q.to(self.device)\n\n      # Advantage = Q - V\n      advant = q - v\n\n      if 0: # Check shape of prediction and target\n         print(""q"", q.shape)\n         print(""v"", v.shape)\n         print(""log_pi"", log_pi.shape)\n\n      # A2C losses\n      policy_loss = -log_pi*advant.detach()\n      vf_loss = F.mse_loss(v, q.detach())\n\n      # Update value network parameter\n      self.vf_optimizer.zero_grad()\n      vf_loss.backward()\n      self.vf_optimizer.step()\n      \n      # Update policy network parameter\n      self.policy_optimizer.zero_grad()\n      policy_loss.backward()\n      self.policy_optimizer.step()\n\n      # Save losses\n      self.policy_losses.append(policy_loss.item())\n      self.vf_losses.append(vf_loss.item())\n\n   def run(self, max_step):\n      step_number = 0\n      total_reward = 0.\n\n      obs = self.env.reset()\n      done = False\n\n      # Keep interacting until agent reaches a terminal state.\n      while not (done or step_number == max_step):\n         if self.eval_mode:\n            _, pi, _ = self.policy(torch.Tensor(obs).to(self.device))\n            action = pi.argmax().detach().cpu().numpy()\n            next_obs, reward, done, _ = self.env.step(action)\n         else:\n            self.steps += 1\n\n            # Create a transition list\n            self.transition = []\n\n            # Collect experience (s, a, r, s\') using some policy\n            action = self.select_action(torch.Tensor(obs).to(self.device))\n            next_obs, reward, done, _ = self.env.step(action)\n\n            self.transition.extend([next_obs, reward, done])\n            \n            self.train_model()\n\n         total_reward += reward\n         step_number += 1\n         obs = next_obs\n      \n      # Save total average losses\n      self.logger[\'LossPi\'] = round(np.mean(self.policy_losses), 5)\n      self.logger[\'LossV\'] = round(np.mean(self.vf_losses), 5)\n      return step_number, total_reward\n'"
agents/ddpg.py,7,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom agents.common.utils import *\nfrom agents.common.buffers import *\nfrom agents.common.networks import *\n\n\nclass Agent(object):\n   """"""An implementation of the Deep Deterministic Policy Gradient (DDPG) agent.""""""\n\n   def __init__(self,\n                env,\n                args,\n                device,\n                obs_dim,\n                act_dim,\n                act_limit,\n                steps=0,\n                expl_before=2000,\n                train_after=1000,\n                gamma=0.99,\n                act_noise=0.1,\n                hidden_sizes=(128,128),\n                buffer_size=int(1e4),\n                batch_size=64,\n                policy_lr=3e-4,\n                qf_lr=3e-4,\n                gradient_clip_policy=0.5,\n                gradient_clip_qf=1.0,\n                eval_mode=False,\n                policy_losses=list(),\n                qf_losses=list(),\n                logger=dict(),\n   ):\n\n      self.env = env\n      self.args = args\n      self.device = device\n      self.obs_dim = obs_dim\n      self.act_dim = act_dim\n      self.act_limit = act_limit\n      self.steps = steps \n      self.expl_before = expl_before\n      self.train_after = train_after\n      self.gamma = gamma\n      self.act_noise = act_noise\n      self.hidden_sizes = hidden_sizes\n      self.buffer_size = buffer_size\n      self.batch_size = batch_size\n      self.policy_lr = policy_lr\n      self.qf_lr = qf_lr\n      self.gradient_clip_policy = gradient_clip_policy\n      self.gradient_clip_qf = gradient_clip_qf\n      self.eval_mode = eval_mode\n      self.policy_losses = policy_losses\n      self.qf_losses = qf_losses\n      self.logger = logger\n\n      # Main network\n      self.policy = MLP(self.obs_dim, self.act_dim, hidden_sizes=self.hidden_sizes, \n                                                   output_activation=torch.tanh).to(self.device)\n      self.qf = FlattenMLP(self.obs_dim+self.act_dim, 1, hidden_sizes=self.hidden_sizes).to(self.device)\n      # Target network\n      self.policy_target = MLP(self.obs_dim, self.act_dim, hidden_sizes=self.hidden_sizes, \n                                                          output_activation=torch.tanh).to(self.device)\n      self.qf_target = FlattenMLP(self.obs_dim+self.act_dim, 1, hidden_sizes=self.hidden_sizes).to(self.device)\n      \n      # Initialize target parameters to match main parameters\n      hard_target_update(self.policy, self.policy_target)\n      hard_target_update(self.qf, self.qf_target)\n\n      # Create optimizers\n      self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=self.policy_lr)\n      self.qf_optimizer = optim.Adam(self.qf.parameters(), lr=self.qf_lr)\n      \n      # Experience buffer\n      self.replay_buffer = ReplayBuffer(self.obs_dim, self.act_dim, self.buffer_size, self.device)\n\n   def select_action(self, obs):\n      action = self.policy(obs).detach().cpu().numpy()\n      action += self.act_noise * np.random.randn(self.act_dim)\n      return np.clip(action, -self.act_limit, self.act_limit)\n\n   def train_model(self):\n      batch = self.replay_buffer.sample(self.batch_size)\n      obs1 = batch[\'obs1\']\n      obs2 = batch[\'obs2\']\n      acts = batch[\'acts\']\n      rews = batch[\'rews\']\n      done = batch[\'done\']\n\n      if 0: # Check shape of experiences\n         print(""obs1"", obs1.shape)\n         print(""obs2"", obs2.shape)\n         print(""acts"", acts.shape)\n         print(""rews"", rews.shape)\n         print(""done"", done.shape)\n\n      # Prediction Q(s,\xcf\x80(s)), Q(s,a), Q\xe2\x80\xbe(s\',\xcf\x80\xe2\x80\xbe(s\'))\n      pi = self.policy(obs1)\n      q_pi = self.qf(obs1, pi)\n      q = self.qf(obs1, acts).squeeze(1)\n      pi_target = self.policy_target(obs2)\n      q_pi_target = self.qf_target(obs2, pi_target).squeeze(1)\n      \n      # Target for Q regression\n      q_backup = rews + self.gamma*(1-done)*q_pi_target\n      q_backup.to(self.device)\n\n      if 0: # Check shape of prediction and target\n         print(""q"", q.shape)\n         print(""q_backup"", q_backup.shape)\n\n      # DDPG losses\n      policy_loss = -q_pi.mean()\n      qf_loss = F.mse_loss(q, q_backup.detach())\n\n      # Update Q-function network parameter\n      self.qf_optimizer.zero_grad()\n      qf_loss.backward()\n      nn.utils.clip_grad_norm_(self.qf.parameters(), self.gradient_clip_qf)\n      self.qf_optimizer.step()\n      \n      # Update policy network parameter\n      self.policy_optimizer.zero_grad()\n      policy_loss.backward()\n      nn.utils.clip_grad_norm_(self.policy.parameters(), self.gradient_clip_policy)\n      self.policy_optimizer.step()\n\n      # Polyak averaging for target parameter\n      soft_target_update(self.policy, self.policy_target)\n      soft_target_update(self.qf, self.qf_target)\n      \n      # Save losses\n      self.policy_losses.append(policy_loss.item())\n      self.qf_losses.append(qf_loss.item())\n\n   def run(self, max_step):\n      step_number = 0\n      total_reward = 0.\n\n      obs = self.env.reset()\n      done = False\n\n      # Keep interacting until agent reaches a terminal state.\n      while not (done or step_number == max_step):\n         if self.eval_mode:\n            action = self.policy(torch.Tensor(obs).to(self.device))\n            action = action.detach().cpu().numpy()\n            next_obs, reward, done, _ = self.env.step(action)\n         else:\n            self.steps += 1\n\n            # Until expl_before have elapsed, randomly sample actions \n            # from a uniform distribution for better exploration. \n            # Afterwards, use the learned policy.\n            if self.steps > self.expl_before:\n               action = self.select_action(torch.Tensor(obs).to(self.device))\n            else:\n               action = self.env.action_space.sample()\n\n            # Collect experience (s, a, r, s\') using some policy\n            next_obs, reward, done, _ = self.env.step(action)\n\n            # Add experience to replay buffer\n            self.replay_buffer.add(obs, action, reward, next_obs, done)\n            \n            # Start training when the number of experience is greater than train_after\n            if self.steps > self.train_after:\n               self.train_model()\n\n         total_reward += reward\n         step_number += 1\n         obs = next_obs\n      \n      # Save logs\n      self.logger[\'LossPi\'] = round(np.mean(self.policy_losses), 5)\n      self.logger[\'LossQ\'] = round(np.mean(self.qf_losses), 5)\n      return step_number, total_reward\n'"
agents/dqn.py,4,"b'import numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom agents.common.utils import *\nfrom agents.common.buffers import *\nfrom agents.common.networks import *\n\n\nclass Agent(object):\n   """"""An implementation of the Deep Q-Network (DQN), Double DQN agents.""""""\n\n   def __init__(self,\n                env,\n                args,\n                device,\n                obs_dim,\n                act_num,\n                steps=0,\n                gamma=0.99,\n                epsilon=1.0,\n                epsilon_decay=0.995,\n                buffer_size=int(1e4),\n                batch_size=64,\n                target_update_step=100,\n                eval_mode=False,\n                q_losses=list(),\n                logger=dict(),\n   ):\n\n      self.env = env\n      self.args = args\n      self.device = device\n      self.obs_dim = obs_dim\n      self.act_num = act_num\n      self.steps = steps\n      self.gamma = gamma\n      self.epsilon = epsilon\n      self.epsilon_decay = epsilon_decay\n      self.buffer_size = buffer_size\n      self.batch_size = batch_size\n      self.target_update_step = target_update_step\n      self.eval_mode = eval_mode\n      self.q_losses = q_losses\n      self.logger = logger\n\n      # Main network\n      self.qf = MLP(self.obs_dim, self.act_num).to(self.device)\n      # Target network\n      self.qf_target = MLP(self.obs_dim, self.act_num).to(self.device)\n      \n      # Initialize target parameters to match main parameters\n      hard_target_update(self.qf, self.qf_target)\n\n      # Create an optimizer\n      self.qf_optimizer = optim.Adam(self.qf.parameters(), lr=1e-3)\n\n      # Experience buffer\n      self.replay_buffer = ReplayBuffer(self.obs_dim, 1, self.buffer_size, self.device)\n\n   def select_action(self, obs):\n      """"""Select an action from the set of available actions.""""""\n      # Decaying epsilon\n      self.epsilon *= self.epsilon_decay\n      self.epsilon = max(self.epsilon, 0.01)\n\n      if np.random.rand() <= self.epsilon:\n         # Choose a random action with probability epsilon\n         return np.random.randint(self.act_num)\n      else:\n         # Choose the action with highest Q-value at the current state\n         action = self.qf(obs).argmax()\n         return action.detach().cpu().numpy()\n\n   def train_model(self):\n      batch = self.replay_buffer.sample(self.batch_size)\n      obs1 = batch[\'obs1\']\n      obs2 = batch[\'obs2\']\n      acts = batch[\'acts\']\n      rews = batch[\'rews\']\n      done = batch[\'done\']\n\n      if 0: # Check shape of experiences\n         print(""obs1"", obs1.shape)\n         print(""obs2"", obs2.shape)\n         print(""acts"", acts.shape)\n         print(""rews"", rews.shape)\n         print(""done"", done.shape)\n\n      # Prediction Q(s)\n      q = self.qf(obs1).gather(1, acts.long()).squeeze(1)\n      \n      # Target for Q regression\n      if self.args.algo == \'dqn\':      # DQN\n         q_target = self.qf_target(obs2)\n      elif self.args.algo == \'ddqn\':   # Double DQN\n         q2 = self.qf(obs2)\n         q_target = self.qf_target(obs2)\n         q_target = q_target.gather(1, q2.max(1)[1].unsqueeze(1))\n      q_backup = rews + self.gamma*(1-done)*q_target.max(1)[0]\n      q_backup.to(self.device)\n\n      if 0: # Check shape of prediction and target\n         print(""q"", q.shape)\n         print(""q_backup"", q_backup.shape)\n\n      # Update perdiction network parameter\n      qf_loss = F.mse_loss(q, q_backup.detach())\n      self.qf_optimizer.zero_grad()\n      qf_loss.backward()\n      self.qf_optimizer.step()\n\n      # Synchronize target parameters \xf0\x9d\x9c\x83\xe2\x80\xbe as \xf0\x9d\x9c\x83 every C steps\n      if self.steps % self.target_update_step == 0:\n         hard_target_update(self.qf, self.qf_target)\n      \n      # Save loss\n      self.q_losses.append(qf_loss.item())\n\n   def run(self, max_step):\n      step_number = 0\n      total_reward = 0.\n\n      obs = self.env.reset()\n      done = False\n\n      # Keep interacting until agent reaches a terminal state.\n      while not (done or step_number == max_step):\n         if self.eval_mode:\n            q_value = self.qf(torch.Tensor(obs).to(self.device)).argmax()\n            action = q_value.detach().cpu().numpy()\n            next_obs, reward, done, _ = self.env.step(action)\n         else:\n            self.steps += 1\n\n            # Collect experience (s, a, r, s\') using some policy\n            action = self.select_action(torch.Tensor(obs).to(self.device))\n            next_obs, reward, done, _ = self.env.step(action)\n         \n            # Add experience to replay buffer\n            self.replay_buffer.add(obs, action, reward, next_obs, done)\n            \n            # Start training when the number of experience is greater than batch_size\n            if self.steps > self.batch_size:\n               self.train_model()\n\n         total_reward += reward\n         step_number += 1\n         obs = next_obs\n      \n      # Save logs\n      self.logger[\'LossQ\'] = round(np.mean(self.q_losses), 5)\n      return step_number, total_reward\n'"
agents/ppo.py,12,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom agents.common.utils import *\nfrom agents.common.buffers import *\nfrom agents.common.networks import *\n\n\nclass Agent(object):\n   """"""\n   An implementation of the Proximal Policy Optimization (PPO) (by clipping) agent, \n   with early stopping based on approximate KL.\n   """"""\n\n   def __init__(self,\n                env,\n                args,\n                device,\n                obs_dim,\n                act_dim,\n                act_limit,\n                steps=0,\n                gamma=0.99,\n                lam=0.97,\n                hidden_sizes=(64,64),\n                sample_size=2048,\n                train_policy_iters=80,\n                train_vf_iters=80,\n                clip_param=0.2,\n                target_kl=0.01,\n                policy_lr=3e-4,\n                vf_lr=1e-3,\n                eval_mode=False,\n                policy_losses=list(),\n                vf_losses=list(),\n                kls=list(),\n                logger=dict(),\n   ):\n\n      self.env = env\n      self.args = args\n      self.device = device\n      self.obs_dim = obs_dim\n      self.act_dim = act_dim\n      self.act_limit = act_limit\n      self.steps = steps \n      self.gamma = gamma\n      self.lam = lam\n      self.hidden_sizes = hidden_sizes\n      self.sample_size = sample_size\n      self.train_policy_iters = train_policy_iters\n      self.train_vf_iters = train_vf_iters\n      self.clip_param = clip_param\n      self.target_kl = target_kl\n      self.policy_lr = policy_lr\n      self.vf_lr = vf_lr\n      self.eval_mode = eval_mode\n      self.policy_losses = policy_losses\n      self.vf_losses = vf_losses\n      self.kls = kls\n      self.logger = logger\n\n      # Main network\n      self.policy = GaussianPolicy(self.obs_dim, self.act_dim).to(self.device)\n      self.vf = MLP(self.obs_dim, 1, activation=torch.tanh).to(self.device)\n      \n      # Create optimizers\n      self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=self.policy_lr)\n      self.vf_optimizer = optim.Adam(self.vf.parameters(), lr=self.vf_lr)\n      \n      # Experience buffer\n      self.buffer = Buffer(self.obs_dim, self.act_dim, self.sample_size, self.device, self.gamma, self.lam)\n\n   def compute_vf_loss(self, obs, ret, v_old):\n      # Prediction V(s)\n      v = self.vf(obs).squeeze(1)\n\n      # Value loss\n      clip_v = v_old + torch.clamp(v-v_old, -self.clip_param, self.clip_param)\n      vf_loss = torch.max(F.mse_loss(v, ret), F.mse_loss(clip_v, ret)).mean()\n      return vf_loss\n\n   def compute_policy_loss(self, obs, act, adv, log_pi_old):\n      # Prediction log\xcf\x80(s)\n      _, _, _, log_pi = self.policy(obs, act)\n      \n      # Policy loss\n      ratio = torch.exp(log_pi - log_pi_old)\n      clip_adv = (torch.clamp(ratio, 1.-self.clip_param, 1.+self.clip_param)*adv)\n      policy_loss = -torch.min(ratio*adv, clip_adv).mean()\n\n      # A sample estimate for KL-divergence, easy to compute\n      approx_kl = (log_pi_old - log_pi).mean()\n      return policy_loss, approx_kl\n\n   def train_model(self):\n      batch = self.buffer.get()\n      obs = batch[\'obs\']\n      act = batch[\'act\']\n      ret = batch[\'ret\']\n      adv = batch[\'adv\']\n      \n      # Prediction log\xcf\x80_old(s), V_old(s)\n      _, _, _, log_pi_old = self.policy(obs, act)\n      log_pi_old = log_pi_old.detach()\n      v_old = self.vf(obs).squeeze(1)\n      v_old = v_old.detach()\n\n      # Train value with multiple steps of gradient descent\n      for i in range(self.train_vf_iters):\n         vf_loss = self.compute_vf_loss(obs, ret, v_old)\n\n         # Update value network parameter\n         self.vf_optimizer.zero_grad()\n         vf_loss.backward()\n         self.vf_optimizer.step()\n      \n      # Train policy with multiple steps of gradient descent\n      for i in range(self.train_policy_iters):\n         policy_loss, kl = self.compute_policy_loss(obs, act, adv, log_pi_old)\n         \n         # Early stopping at step i due to reaching max kl\n         if kl > 1.5 * self.target_kl:\n            break\n         \n         # Update policy network parameter\n         self.policy_optimizer.zero_grad()\n         policy_loss.backward()\n         self.policy_optimizer.step()\n\n      # Save losses\n      self.policy_losses.append(policy_loss.item())\n      self.vf_losses.append(vf_loss.item())\n      self.kls.append(kl.item())\n\n   def run(self, max_step):\n      step_number = 0\n      total_reward = 0.\n\n      obs = self.env.reset()\n      done = False\n\n      # Keep interacting until agent reaches a terminal state.\n      while not (done or step_number == max_step):\n         if self.eval_mode:\n            action, _, _, _ = self.policy(torch.Tensor(obs).to(self.device))\n            action = action.detach().cpu().numpy()\n            next_obs, reward, done, _ = self.env.step(action)\n         else:\n            self.steps += 1\n            \n            # Collect experience (s, a, r, s\') using some policy\n            _, _, action, _ = self.policy(torch.Tensor(obs).to(self.device))\n            action = action.detach().cpu().numpy()\n            next_obs, reward, done, _ = self.env.step(action)\n\n            # Add experience to buffer\n            v = self.vf(torch.Tensor(obs).to(self.device))\n            self.buffer.add(obs, action, reward, done, v)\n            \n            # Start training when the number of experience is equal to sample size\n            if self.steps == self.sample_size:\n               self.buffer.finish_path()\n               self.train_model()\n               self.steps = 0\n\n         total_reward += reward\n         step_number += 1\n         obs = next_obs\n      \n      # Save logs\n      self.logger[\'LossPi\'] = round(np.mean(self.policy_losses), 5)\n      self.logger[\'LossV\'] = round(np.mean(self.vf_losses), 5)\n      self.logger[\'KL\'] = round(np.mean(self.kls), 5)\n      return step_number, total_reward\n'"
agents/sac.py,8,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom agents.common.utils import *\nfrom agents.common.buffers import *\nfrom agents.common.networks import *\n\n\nclass Agent(object):\n   """"""\n   An implementation of agents for Soft Actor-Critic (SAC), Automatic entropy adjustment on SAC (ASAC), \n   Tsallis Actor-Critic (TAC) and Automatic entropy adjustment on TAC (ATAC).\n   """"""\n\n   def __init__(self,\n                env,\n                args,\n                device,\n                obs_dim,\n                act_dim,\n                act_limit,\n                steps=0,\n                expl_before=2000,\n                train_after=1000,\n                gamma=0.99,\n                alpha=0.2,\n                log_type=\'log\',\n                entropic_index=1.5,\n                automatic_entropy_tuning=False,\n                hidden_sizes=(128,128),\n                buffer_size=int(1e4),\n                batch_size=64,\n                policy_lr=3e-4,\n                qf_lr=3e-4,\n                eval_mode=False,\n                policy_losses=list(),\n                qf1_losses=list(),\n                qf2_losses=list(),\n                alpha_losses=list(),\n                logger=dict(),\n   ):\n\n      self.env = env\n      self.args = args\n      self.device = device\n      self.obs_dim = obs_dim\n      self.act_dim = act_dim\n      self.act_limit = act_limit\n      self.steps = steps \n      self.expl_before = expl_before\n      self.train_after = train_after\n      self.gamma = gamma\n      self.alpha = alpha\n      self.log_type = log_type\n      self.entropic_index = entropic_index\n      self.automatic_entropy_tuning = automatic_entropy_tuning\n      self.hidden_sizes = hidden_sizes\n      self.buffer_size = buffer_size\n      self.batch_size = batch_size\n      self.policy_lr = policy_lr\n      self.qf_lr = qf_lr\n      self.eval_mode = eval_mode\n      self.policy_losses = policy_losses\n      self.qf1_losses = qf1_losses\n      self.qf2_losses = qf2_losses\n      self.alpha_losses = alpha_losses\n      self.logger = logger\n\n      # Main network\n      self.policy = ReparamGaussianPolicy(self.obs_dim, self.act_dim, hidden_sizes=self.hidden_sizes, \n                                                                     action_scale=self.act_limit, \n                                                                     log_type=self.log_type, \n                                                                     q=self.entropic_index,\n                                                                     device=self.device).to(self.device)\n      self.qf1 = FlattenMLP(self.obs_dim+self.act_dim, 1, hidden_sizes=self.hidden_sizes).to(self.device)\n      self.qf2 = FlattenMLP(self.obs_dim+self.act_dim, 1, hidden_sizes=self.hidden_sizes).to(self.device)\n      # Target network\n      self.qf1_target = FlattenMLP(self.obs_dim+self.act_dim, 1, hidden_sizes=self.hidden_sizes).to(self.device)\n      self.qf2_target = FlattenMLP(self.obs_dim+self.act_dim, 1, hidden_sizes=self.hidden_sizes).to(self.device)\n      \n      # Initialize target parameters to match main parameters\n      hard_target_update(self.qf1, self.qf1_target)\n      hard_target_update(self.qf2, self.qf2_target)\n\n      # Concat the Q-network parameters to use one optim\n      self.qf_parameters = list(self.qf1.parameters()) + list(self.qf2.parameters())\n      # Create optimizers\n      self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=self.policy_lr)\n      self.qf_optimizer = optim.Adam(self.qf_parameters, lr=self.qf_lr)\n      \n      # Experience buffer\n      self.replay_buffer = ReplayBuffer(self.obs_dim, self.act_dim, self.buffer_size, self.device)\n\n      # If automatic entropy tuning is True, \n      # initialize a target entropy, a log alpha and an alpha optimizer\n      if self.automatic_entropy_tuning:\n         self.target_entropy = -np.prod((act_dim,)).item()\n         self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n         self.alpha_optimizer = optim.Adam([self.log_alpha], lr=self.policy_lr)\n\n   def train_model(self):\n      batch = self.replay_buffer.sample(self.batch_size)\n      obs1 = batch[\'obs1\']\n      obs2 = batch[\'obs2\']\n      acts = batch[\'acts\']\n      rews = batch[\'rews\']\n      done = batch[\'done\']\n\n      if 0: # Check shape of experiences\n         print(""obs1"", obs1.shape)\n         print(""obs2"", obs2.shape)\n         print(""acts"", acts.shape)\n         print(""rews"", rews.shape)\n         print(""done"", done.shape)\n\n      # Prediction \xcf\x80(s), log\xcf\x80(s), \xcf\x80(s\'), log\xcf\x80(s\'), Q1(s,a), Q2(s,a)\n      _, pi, log_pi = self.policy(obs1)\n      _, next_pi, next_log_pi = self.policy(obs2)\n      q1 = self.qf1(obs1, acts).squeeze(1)\n      q2 = self.qf2(obs1, acts).squeeze(1)\n\n      # Min Double-Q: min(Q1(s,\xcf\x80(s)), Q2(s,\xcf\x80(s))), min(Q1\xe2\x80\xbe(s\',\xcf\x80(s\')), Q2\xe2\x80\xbe(s\',\xcf\x80(s\')))\n      min_q_pi = torch.min(self.qf1(obs1, pi), self.qf2(obs1, pi)).squeeze(1).to(self.device)\n      min_q_next_pi = torch.min(self.qf1_target(obs2, next_pi), \n                                self.qf2_target(obs2, next_pi)).squeeze(1).to(self.device)\n\n      # Targets for Q and V regression\n      v_backup = min_q_next_pi - self.alpha*next_log_pi\n      q_backup = rews + self.gamma*(1-done)*v_backup\n      q_backup.to(self.device)\n\n      if 0: # Check shape of prediction and target\n         print(""log_pi"", log_pi.shape)\n         print(""next_log_pi"", next_log_pi.shape)\n         print(""q1"", q1.shape)\n         print(""q2"", q2.shape)\n         print(""min_q_pi"", min_q_pi.shape)\n         print(""min_q_next_pi"", min_q_next_pi.shape)\n         print(""q_backup"", q_backup.shape)\n\n      # SAC losses\n      policy_loss = (self.alpha*log_pi - min_q_pi).mean()\n      qf1_loss = F.mse_loss(q1, q_backup.detach())\n      qf2_loss = F.mse_loss(q2, q_backup.detach())\n      qf_loss = qf1_loss + qf2_loss\n\n      # Update two Q-network parameter\n      self.qf_optimizer.zero_grad()\n      qf_loss.backward()\n      self.qf_optimizer.step()\n\n      # Update policy network parameter\n      self.policy_optimizer.zero_grad()\n      policy_loss.backward()\n      self.policy_optimizer.step()\n\n      # If automatic entropy tuning is True, update alpha\n      if self.automatic_entropy_tuning:\n         alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n         self.alpha_optimizer.zero_grad()\n         alpha_loss.backward()\n         self.alpha_optimizer.step()\n\n         self.alpha = self.log_alpha.exp()\n\n         # Save alpha loss\n         self.alpha_losses.append(alpha_loss.item())\n\n      # Polyak averaging for target parameter\n      soft_target_update(self.qf1, self.qf1_target)\n      soft_target_update(self.qf2, self.qf2_target)\n      \n      # Save losses\n      self.policy_losses.append(policy_loss.item())\n      self.qf1_losses.append(qf1_loss.item())\n      self.qf2_losses.append(qf2_loss.item())\n\n   def run(self, max_step):\n      step_number = 0\n      total_reward = 0.\n\n      obs = self.env.reset()\n      done = False\n\n      # Keep interacting until agent reaches a terminal state.\n      while not (done or step_number == max_step):\n         if self.eval_mode:\n            action, _, _ = self.policy(torch.Tensor(obs).to(self.device))\n            action = action.detach().cpu().numpy()\n            next_obs, reward, done, _ = self.env.step(action)\n         else:\n            self.steps += 1\n\n            # Until expl_before have elapsed, randomly sample actions \n            # from a uniform distribution for better exploration. \n            # Afterwards, use the learned policy.\n            if self.steps > self.expl_before:\n               _, action, _ = self.policy(torch.Tensor(obs).to(self.device))\n               action = action.detach().cpu().numpy()\n            else:\n               action = self.env.action_space.sample()\n\n            # Collect experience (s, a, r, s\') using some policy\n            next_obs, reward, done, _ = self.env.step(action)\n\n            # Add experience to replay buffer\n            self.replay_buffer.add(obs, action, reward, next_obs, done)\n            \n            # Start training when the number of experience is greater than train_after\n            if self.steps > self.train_after:\n               self.train_model()\n\n         total_reward += reward\n         step_number += 1\n         obs = next_obs\n      \n      # Save logs\n      self.logger[\'LossPi\'] = round(np.mean(self.policy_losses), 5)\n      self.logger[\'LossQ1\'] = round(np.mean(self.qf1_losses), 5)\n      self.logger[\'LossQ2\'] = round(np.mean(self.qf2_losses), 5)\n      if self.automatic_entropy_tuning:\n         self.logger[\'LossAlpha\'] = round(np.mean(self.alpha_losses), 5)\n      return step_number, total_reward\n'"
agents/td3.py,11,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom agents.common.utils import *\nfrom agents.common.buffers import *\nfrom agents.common.networks import *\n\n\nclass Agent(object):\n   """"""An implementation of the Twin Delayed DDPG (TD3) agent.""""""\n\n   def __init__(self,\n                env,\n                args,\n                device,\n                obs_dim,\n                act_dim,\n                act_limit,\n                steps=0,\n                expl_before=2000,\n                train_after=1000,\n                gamma=0.99,\n                act_noise=0.1,\n                target_noise=0.2,\n                noise_clip=0.5,\n                policy_delay=2,\n                hidden_sizes=(128,128),\n                buffer_size=int(1e4),\n                batch_size=64,\n                policy_lr=3e-4,\n                qf_lr=3e-4,\n                eval_mode=False,\n                policy_losses=list(),\n                qf_losses=list(),\n                logger=dict(),\n   ):\n\n      self.env = env\n      self.args = args\n      self.device = device\n      self.obs_dim = obs_dim\n      self.act_dim = act_dim\n      self.act_limit = act_limit\n      self.steps = steps \n      self.expl_before = expl_before\n      self.train_after = train_after\n      self.gamma = gamma\n      self.act_noise = act_noise\n      self.target_noise = target_noise\n      self.noise_clip = noise_clip\n      self.policy_delay = policy_delay\n      self.hidden_sizes = hidden_sizes\n      self.buffer_size = buffer_size\n      self.batch_size = batch_size\n      self.policy_lr = policy_lr\n      self.qf_lr = qf_lr\n      self.eval_mode = eval_mode\n      self.policy_losses = policy_losses\n      self.qf_losses = qf_losses\n      self.logger = logger\n\n      # Main network\n      self.policy = MLP(self.obs_dim, self.act_dim, hidden_sizes=self.hidden_sizes, \n                                                   output_activation=torch.tanh).to(self.device)\n      self.qf1 = FlattenMLP(self.obs_dim+self.act_dim, 1, hidden_sizes=self.hidden_sizes).to(self.device)\n      self.qf2 = FlattenMLP(self.obs_dim+self.act_dim, 1, hidden_sizes=self.hidden_sizes).to(self.device)\n      # Target network\n      self.policy_target = MLP(self.obs_dim, self.act_dim, hidden_sizes=self.hidden_sizes, \n                                                          output_activation=torch.tanh).to(self.device)\n      self.qf1_target = FlattenMLP(self.obs_dim+self.act_dim, 1, hidden_sizes=self.hidden_sizes).to(self.device)\n      self.qf2_target = FlattenMLP(self.obs_dim+self.act_dim, 1, hidden_sizes=self.hidden_sizes).to(self.device)\n      \n      # Initialize target parameters to match main parameters\n      hard_target_update(self.policy, self.policy_target)\n      hard_target_update(self.qf1, self.qf1_target)\n      hard_target_update(self.qf2, self.qf2_target)\n\n      # Concat the Q-network parameters to use one optim\n      self.qf_parameters = list(self.qf1.parameters()) + list(self.qf2.parameters())\n      # Create optimizers\n      self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=self.policy_lr)\n      self.qf_optimizer = optim.Adam(self.qf_parameters, lr=self.qf_lr)\n      \n      # Experience buffer\n      self.replay_buffer = ReplayBuffer(self.obs_dim, self.act_dim, self.buffer_size, self.device)\n\n   def select_action(self, obs):\n      action = self.policy(obs).detach().cpu().numpy()\n      action += self.act_noise * np.random.randn(self.act_dim)\n      return np.clip(action, -self.act_limit, self.act_limit)\n\n   def train_model(self):\n      batch = self.replay_buffer.sample(self.batch_size)\n      obs1 = batch[\'obs1\']\n      obs2 = batch[\'obs2\']\n      acts = batch[\'acts\']\n      rews = batch[\'rews\']\n      done = batch[\'done\']\n\n      if 0: # Check shape of experiences\n         print(""obs1"", obs1.shape)\n         print(""obs2"", obs2.shape)\n         print(""acts"", acts.shape)\n         print(""rews"", rews.shape)\n         print(""done"", done.shape)\n\n      # Prediction Q1(s,\xcf\x80(s)), Q1(s,a), Q2(s,a)\n      pi = self.policy(obs1)\n      q1_pi = self.qf1(obs1, pi)\n      q1 = self.qf1(obs1, acts).squeeze(1)\n      q2 = self.qf2(obs1, acts).squeeze(1)\n\n      # Target policy smoothing, by adding clipped noise to target actions\n      pi_target = self.policy_target(obs2)\n      epsilon = torch.normal(mean=0, std=self.target_noise, size=pi_target.size()).to(self.device)\n      epsilon = torch.clamp(epsilon, -self.noise_clip, self.noise_clip).to(self.device)\n      pi_target = torch.clamp(pi_target+epsilon, -self.act_limit, self.act_limit).to(self.device)\n\n      # Min Double-Q: min(Q1\xe2\x80\xbe(s\',\xcf\x80(s\')), Q2\xe2\x80\xbe(s\',\xcf\x80(s\')))\n      min_q_pi_target = torch.min(self.qf1_target(obs2, pi_target), \n                                  self.qf2_target(obs2, pi_target)).squeeze(1).to(self.device)\n      \n      # Target for Q regression\n      q_backup = rews + self.gamma*(1-done)*min_q_pi_target\n      q_backup.to(self.device)\n\n      if 0: # Check shape of prediction and target\n         print(""pi_target"", pi_target.shape)\n         print(""epsilon"", epsilon.shape)\n         print(""q1"", q1.shape)\n         print(""q2"", q2.shape)\n         print(""min_q_pi_target"", min_q_pi_target.shape)\n         print(""q_backup"", q_backup.shape)\n\n      # TD3 losses\n      policy_loss = -q1_pi.mean()\n      qf1_loss = F.mse_loss(q1, q_backup.detach())\n      qf2_loss = F.mse_loss(q2, q_backup.detach())\n      qf_loss = qf1_loss + qf2_loss\n\n      # Update two Q-network parameter\n      self.qf_optimizer.zero_grad()\n      qf_loss.backward()\n      self.qf_optimizer.step()\n      \n      # Delayed policy update\n      if self.steps % self.policy_delay == 0:\n         # Update policy network parameter\n         self.policy_optimizer.zero_grad()\n         policy_loss.backward()\n         self.policy_optimizer.step()\n\n         # Polyak averaging for target parameter\n         soft_target_update(self.policy, self.policy_target)\n         soft_target_update(self.qf1, self.qf1_target)\n         soft_target_update(self.qf2, self.qf2_target)\n         \n      # Save losses\n      self.policy_losses.append(policy_loss.item())\n      self.qf_losses.append(qf_loss.item())\n\n   def run(self, max_step):\n      step_number = 0\n      total_reward = 0.\n\n      obs = self.env.reset()\n      done = False\n\n      # Keep interacting until agent reaches a terminal state.\n      while not (done or step_number == max_step):\n         if self.eval_mode:\n            action = self.policy(torch.Tensor(obs).to(self.device))\n            action = action.detach().cpu().numpy()\n            next_obs, reward, done, _ = self.env.step(action)\n         else:\n            self.steps += 1\n            \n            # Until expl_before have elapsed, randomly sample actions \n            # from a uniform distribution for better exploration. \n            # Afterwards, use the learned policy.\n            if self.steps > self.expl_before:\n               action = self.select_action(torch.Tensor(obs).to(self.device))\n            else:\n               action = self.env.action_space.sample()\n            \n            # Collect experience (s, a, r, s\') using some policy\n            next_obs, reward, done, _ = self.env.step(action)\n\n            # Add experience to replay buffer\n            self.replay_buffer.add(obs, action, reward, next_obs, done)\n            \n            # Start training when the number of experience is greater than train_after\n            if self.steps > self.train_after:\n               self.train_model()\n\n         total_reward += reward\n         step_number += 1\n         obs = next_obs\n      \n      # Save logs\n      self.logger[\'LossPi\'] = round(np.mean(self.policy_losses), 5)\n      self.logger[\'LossQ\'] = round(np.mean(self.qf_losses), 5)\n      return step_number, total_reward\n'"
agents/trpo.py,21,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom agents.common.utils import *\nfrom agents.common.buffers import *\nfrom agents.common.networks import *\n\n\nclass Agent(object):\n   """"""\n   An implementation of the Trust Region Policy Optimization (TRPO) agent\n   with support for Natural Policy Gradient (NPG).\n   """"""\n\n   def __init__(self,\n                env,\n                args,\n                device,\n                obs_dim,\n                act_dim,\n                act_limit,\n                steps=0,\n                gamma=0.99,\n                lam=0.97,\n                hidden_sizes=(64,64),\n                sample_size=2048,\n                vf_lr=1e-3,\n                train_vf_iters=80,\n                delta=0.01,\n                backtrack_iter=10,\n                backtrack_coeff=1.0,\n                backtrack_alpha=0.5,\n                eval_mode=False,\n                policy_losses=list(),\n                vf_losses=list(),\n                kls=list(),\n                backtrack_iters=list(),\n                logger=dict(),\n   ):\n\n      self.env = env\n      self.args = args\n      self.device = device\n      self.obs_dim = obs_dim\n      self.act_dim = act_dim\n      self.act_limit = act_limit\n      self.steps = steps \n      self.gamma = gamma\n      self.lam = lam\n      self.hidden_sizes = hidden_sizes\n      self.sample_size = sample_size\n      self.vf_lr = vf_lr\n      self.train_vf_iters = train_vf_iters\n      self.delta = delta\n      self.backtrack_iter = backtrack_iter\n      self.backtrack_coeff = backtrack_coeff\n      self.backtrack_alpha = backtrack_alpha\n      self.eval_mode = eval_mode\n      self.policy_losses = policy_losses\n      self.vf_losses = vf_losses\n      self.kls = kls\n      self.backtrack_iters = backtrack_iters\n      self.logger = logger\n\n      # Main network\n      self.policy = GaussianPolicy(self.obs_dim, self.act_dim).to(self.device)\n      self.old_policy = GaussianPolicy(self.obs_dim, self.act_dim).to(self.device)\n      self.vf = MLP(self.obs_dim, 1, activation=torch.tanh).to(self.device)\n      \n      # Create optimizers\n      self.vf_optimizer = optim.Adam(self.vf.parameters(), lr=self.vf_lr)\n      \n      # Experience buffer\n      self.buffer = Buffer(self.obs_dim, self.act_dim, self.sample_size, self.device, self.gamma, self.lam)\n\n   def cg(self, obs, b, cg_iters=10, EPS=1e-8, residual_tol=1e-10):\n      # Conjugate gradient algorithm\n      # (https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n      x = torch.zeros(b.size()).to(self.device)\n      r = b.clone()\n      p = r.clone()\n      rdotr = torch.dot(r,r).to(self.device)\n\n      for _ in range(cg_iters):\n         Ap = self.hessian_vector_product(obs, p)\n         alpha = rdotr / (torch.dot(p, Ap).to(self.device) + EPS)\n         \n         x += alpha * p\n         r -= alpha * Ap\n         \n         new_rdotr = torch.dot(r, r)\n         p = r + (new_rdotr / rdotr) * p\n         rdotr = new_rdotr\n\n         if rdotr < residual_tol:\n            break\n      return x\n\n   def hessian_vector_product(self, obs, p, damping_coeff=0.1):\n      p.detach()\n      kl = self.gaussian_kl(old_policy=self.policy, new_policy=self.policy, obs=obs)\n      kl_grad = torch.autograd.grad(kl, self.policy.parameters(), create_graph=True)\n      kl_grad = self.flat_grad(kl_grad)\n\n      kl_grad_p = (kl_grad * p).sum() \n      kl_hessian = torch.autograd.grad(kl_grad_p, self.policy.parameters())\n      kl_hessian = self.flat_grad(kl_hessian, hessian=True)\n      return kl_hessian + p * damping_coeff\n   \n   def gaussian_kl(self, old_policy, new_policy, obs):\n      mu_old, std_old, _, _ = old_policy(obs)\n      mu_old, std_old = mu_old.detach(), std_old.detach()\n      mu, std, _, _ = new_policy(obs)\n\n      # kl divergence between old policy and new policy : D( pi_old || pi_new )\n      # (https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians)\n      kl = torch.log(std/std_old) + (std_old.pow(2)+(mu_old-mu).pow(2))/(2.0*std.pow(2)) - 0.5\n      return kl.sum(-1, keepdim=True).mean()\n\n   def flat_grad(self, grads, hessian=False):\n      grad_flatten = []\n      if hessian == False:\n         for grad in grads:\n            grad_flatten.append(grad.view(-1))\n         grad_flatten = torch.cat(grad_flatten)\n         return grad_flatten\n      elif hessian == True:\n         for grad in grads:\n            grad_flatten.append(grad.contiguous().view(-1))\n         grad_flatten = torch.cat(grad_flatten).data\n         return grad_flatten\n\n   def flat_params(self, model):\n      params = []\n      for param in model.parameters():\n         params.append(param.data.view(-1))\n      params_flatten = torch.cat(params)\n      return params_flatten\n\n   def update_model(self, model, new_params):\n      index = 0\n      for params in model.parameters():\n         params_length = len(params.view(-1))\n         new_param = new_params[index: index + params_length]\n         new_param = new_param.view(params.size())\n         params.data.copy_(new_param)\n         index += params_length\n\n   def train_model(self):\n      batch = self.buffer.get()\n      obs = batch[\'obs\']\n      act = batch[\'act\']\n      ret = batch[\'ret\']\n      adv = batch[\'adv\']\n      \n      # Update value network parameter\n      for _ in range(self.train_vf_iters):\n         # Prediction V(s)\n         v = self.vf(obs).squeeze(1)\n         \n         # Value loss\n         vf_loss = F.mse_loss(v, ret)\n\n         self.vf_optimizer.zero_grad()\n         vf_loss.backward()\n         self.vf_optimizer.step()\n\n      # Prediction log\xcf\x80_old(s), log\xcf\x80(s)\n      _, _, _, log_pi_old = self.policy(obs, act)\n      log_pi_old = log_pi_old.detach()\n      _, _, _, log_pi = self.policy(obs, act)\n   \n      # Policy loss\n      ratio_old = torch.exp(log_pi - log_pi_old)\n      policy_loss_old = (ratio_old*adv).mean()\n\n      # Symbols needed for Conjugate gradient solver\n      gradient = torch.autograd.grad(policy_loss_old, self.policy.parameters())\n      gradient = self.flat_grad(gradient)\n\n      # Core calculations for NPG or TRPO\n      search_dir = self.cg(obs, gradient.data)\n      gHg = (self.hessian_vector_product(obs, search_dir) * search_dir).sum(0)\n      step_size = torch.sqrt(2 * self.delta / gHg)\n      old_params = self.flat_params(self.policy)\n      self.update_model(self.old_policy, old_params)\n\n      if self.args.algo == \'npg\':\n         params = old_params + step_size * search_dir\n         self.update_model(self.policy, params)\n\n         kl = self.gaussian_kl(new_policy=self.policy, old_policy=self.old_policy, obs=obs)\n      elif self.args.algo == \'trpo\':\n         expected_improve = (gradient * step_size * search_dir).sum(0, keepdim=True)\n\n         for i in range(self.backtrack_iter):\n            # Backtracking line search\n            # (https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf) 464p.\n            params = old_params + self.backtrack_coeff * step_size * search_dir\n            self.update_model(self.policy, params)\n\n            _, _, _, log_pi = self.policy(obs, act)\n            ratio = torch.exp(log_pi - log_pi_old)\n            policy_loss = (ratio*adv).mean()\n\n            loss_improve = policy_loss - policy_loss_old\n            expected_improve *= self.backtrack_coeff\n            improve_condition = loss_improve / expected_improve\n\n            kl = self.gaussian_kl(new_policy=self.policy, old_policy=self.old_policy, obs=obs)\n            \n            if kl < self.delta and improve_condition > self.backtrack_alpha:\n               print(\'Accepting new params at step %d of line search.\'%i)\n               self.backtrack_iters.append(i)\n               break\n\n            if i == self.backtrack_iter-1:\n               print(\'Line search failed! Keeping old params.\')\n               self.backtrack_iters.append(i)\n\n               params = self.flat_params(self.old_policy)\n               self.update_model(self.policy, params)\n\n            self.backtrack_coeff *= 0.5\n\n      # Save losses\n      self.policy_losses.append(policy_loss_old.item())\n      self.vf_losses.append(vf_loss.item())\n      self.kls.append(kl.item())\n\n   def run(self, max_step):\n      step_number = 0\n      total_reward = 0.\n\n      obs = self.env.reset()\n      done = False\n\n      # Keep interacting until agent reaches a terminal state.\n      while not (done or step_number == max_step):\n         if self.eval_mode:\n            action, _, _, _ = self.policy(torch.Tensor(obs).to(self.device))\n            action = action.detach().cpu().numpy()\n            next_obs, reward, done, _ = self.env.step(action)\n         else:\n            self.steps += 1\n            \n            # Collect experience (s, a, r, s\') using some policy\n            _, _, action, _ = self.policy(torch.Tensor(obs).to(self.device))\n            action = action.detach().cpu().numpy()\n            next_obs, reward, done, _ = self.env.step(action)\n\n            # Add experience to buffer\n            v = self.vf(torch.Tensor(obs).to(self.device))\n            self.buffer.add(obs, action, reward, done, v)\n            \n            # Start training when the number of experience is equal to sample size\n            if self.steps == self.sample_size:\n               self.buffer.finish_path()\n               self.train_model()\n               self.steps = 0\n\n         total_reward += reward\n         step_number += 1\n         obs = next_obs\n      \n      # Save logs\n      self.logger[\'LossPi\'] = round(np.mean(self.policy_losses), 5)\n      self.logger[\'LossV\'] = round(np.mean(self.vf_losses), 5)\n      self.logger[\'KL\'] = round(np.mean(self.kls), 5)\n      if self.args.algo == \'trpo\':\n         self.logger[\'BacktrackIters\'] = np.mean(self.backtrack_iters)\n      return step_number, total_reward\n'"
agents/vpg.py,7,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nfrom agents.common.utils import *\nfrom agents.common.buffers import *\nfrom agents.common.networks import *\n\n\nclass Agent(object):\n   """"""\n   An implementation of the Vanilla Policy Gradient (VPG) agent\n   with GAE-Lambda for advantage estimation.\n   """"""\n\n   def __init__(self,\n                env,\n                args,\n                device,\n                obs_dim,\n                act_dim,\n                act_limit,\n                steps=0,\n                gamma=0.99,\n                lam=0.97,\n                hidden_sizes=(64,64),\n                sample_size=2048,\n                policy_lr=1e-3,\n                vf_lr=1e-3,\n                gradient_clip=0.5,\n                train_vf_iters=80,\n                eval_mode=False,\n                policy_losses=list(),\n                vf_losses=list(),\n                kls=list(),\n                logger=dict(),\n   ):\n\n      self.env = env\n      self.args = args\n      self.device = device\n      self.obs_dim = obs_dim\n      self.act_dim = act_dim\n      self.act_limit = act_limit\n      self.steps = steps \n      self.gamma = gamma\n      self.lam = lam\n      self.hidden_sizes = hidden_sizes\n      self.sample_size = sample_size\n      self.policy_lr = policy_lr\n      self.vf_lr = vf_lr\n      self.gradient_clip = gradient_clip\n      self.train_vf_iters = train_vf_iters\n      self.eval_mode = eval_mode\n      self.policy_losses = policy_losses\n      self.vf_losses = vf_losses\n      self.kls = kls\n      self.logger = logger\n\n      # Main network\n      self.policy = GaussianPolicy(self.obs_dim, self.act_dim).to(self.device)\n      self.vf = MLP(self.obs_dim, 1, activation=torch.tanh).to(self.device)\n      \n      # Create optimizers\n      self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=self.policy_lr)\n      self.vf_optimizer = optim.Adam(self.vf.parameters(), lr=self.vf_lr)\n      \n      # Experience buffer\n      self.buffer = Buffer(self.obs_dim, self.act_dim, self.sample_size, self.device, self.gamma, self.lam)\n\n   def train_model(self):\n      batch = self.buffer.get()\n      obs = batch[\'obs\']\n      act = batch[\'act\'].detach()\n      ret = batch[\'ret\']\n      adv = batch[\'adv\']\n      log_pi_old = batch[\'log_pi\'].detach()\n      \n      if 0: # Check shape of experiences\n         print(""obs"", obs.shape)\n         print(""act"", act.shape)\n         print(""ret"", ret.shape)\n         print(""adv"", adv.shape)\n         print(""log_pi_old"", log_pi_old.shape)\n\n      # Update value network parameter\n      for _ in range(self.train_vf_iters):\n         # Prediction V(s)\n         v = self.vf(obs).squeeze(1)\n         \n         # Value loss\n         vf_loss = F.mse_loss(v, ret)\n\n         self.vf_optimizer.zero_grad()\n         vf_loss.backward()\n         nn.utils.clip_grad_norm_(self.vf.parameters(), self.gradient_clip)\n         self.vf_optimizer.step()\n      \n      # Prediction log\xcf\x80(s)\n      _, _, _, log_pi = self.policy(obs, act)\n      \n      # Policy loss\n      policy_loss = -(log_pi_old*adv).mean()\n\n      # Update policy network parameter\n      self.policy_optimizer.zero_grad()\n      policy_loss.backward()\n      nn.utils.clip_grad_norm_(self.policy.parameters(), self.gradient_clip)\n      self.policy_optimizer.step()\n\n      # A sample estimate for KL-divergence, easy to compute\n      approx_kl = (log_pi_old - log_pi).mean()     \n\n      # Save losses\n      self.policy_losses.append(policy_loss.item())\n      self.vf_losses.append(vf_loss.item())\n      self.kls.append(approx_kl.item())\n\n   def run(self, max_step):\n      step_number = 0\n      total_reward = 0.\n\n      obs = self.env.reset()\n      done = False\n\n      # Keep interacting until agent reaches a terminal state.\n      while not (done or step_number == max_step):\n         if self.eval_mode:\n            action, _, _, _ = self.policy(torch.Tensor(obs).to(self.device))\n            action = action.detach().cpu().numpy()\n            next_obs, reward, done, _ = self.env.step(action)\n         else:\n            self.steps += 1\n            \n            # Collect experience (s, a, r, s\') using some policy\n            _, _, action, log_pi = self.policy(torch.Tensor(obs).to(self.device))\n            action = action.detach().cpu().numpy()\n            next_obs, reward, done, _ = self.env.step(action)\n\n            # Add experience to buffer\n            v = self.vf(torch.Tensor(obs).to(self.device))\n            self.buffer.add(obs, action, reward, done, log_pi, v)\n            \n            # Start training when the number of experience is equal to sample size\n            if self.steps == self.sample_size:\n               self.buffer.finish_path()\n               self.train_model()\n               self.steps = 0\n\n         total_reward += reward\n         step_number += 1\n         obs = next_obs\n      \n      # Save logs\n      self.logger[\'LossPi\'] = round(np.mean(self.policy_losses), 5)\n      self.logger[\'LossV\'] = round(np.mean(self.vf_losses), 5)\n      self.logger[\'KL\'] = round(np.mean(self.kls), 5)\n      return step_number, total_reward\n'"
tests/cartpole_test.py,5,"b'import os\nimport gym\nimport argparse\nimport numpy as np\nimport torch\nfrom common.networks import *\n\n# Configurations\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--algo\', type=str, default=\'dqn\',\n                    help=\'select an algorithm among dqn, ddqn, a2c\')\nparser.add_argument(\'--load\', type=str, default=None,\n                    help=\'copy & paste the saved model name, and load it (ex. --load=CartPole-v1_...)\')\nparser.add_argument(\'--render\', action=""store_true"", default=True,\n                    help=\'if you want to render, set this to True\')\nparser.add_argument(\'--test_eps\', type=int, default=10000,\n                    help=\'testing episode number\')\nparser.add_argument(\'--gpu_index\', type=int, default=0)\nargs = parser.parse_args()\ndevice = torch.device(\'cuda\', index=args.gpu_index) if torch.cuda.is_available() else torch.device(\'cpu\')\n\n\ndef main():\n    """"""Main.""""""\n    env = gym.make(\'CartPole-v1\')\n    obs_dim = env.observation_space.shape[0]\n    act_num = env.action_space.n\n\n    if args.algo == \'dqn\' or args.algo == \'ddqn\':\n        mlp = MLP(obs_dim, act_num).to(device)\n    elif args.algo == \'a2c\':\n        mlp = CategoricalPolicy(obs_dim, act_num, activation=torch.tanh).to(device)\n\n    if args.load is not None:\n        pretrained_model_path = os.path.join(\'./save_model/\' + str(args.load))\n        pretrained_model = torch.load(pretrained_model_path, map_location=device)\n        mlp.load_state_dict(pretrained_model)\n\n    test_sum_returns = 0.\n    test_num_episodes = 0\n\n    for episode in range(1, args.test_eps+1):\n        total_reward = 0.\n\n        obs = env.reset()\n        done = False\n\n        while not done:\n            if args.render:\n                env.render()\n            \n            if args.algo == \'dqn\' or args.algo == \'ddqn\':\n                action = mlp(torch.Tensor(obs).to(device)).argmax().detach().cpu().numpy()\n            elif args.algo == \'a2c\':\n                _, _, _, pi = mlp(torch.Tensor(obs).to(device))\n                action = pi.argmax().detach().cpu().numpy()\n            \n            next_obs, reward, done, _ = env.step(action)\n            \n            total_reward += reward\n            obs = next_obs\n        \n        test_sum_returns += total_reward\n        test_num_episodes += 1\n\n        test_average_return = test_sum_returns / test_num_episodes if test_num_episodes > 0 else 0.0\n\n        if episode % 10 == 0:\n            print(\'---------------------------------------\')\n            print(\'Episodes:\', test_num_episodes)\n            print(\'TestAverageReturn:\', test_average_return)\n            print(\'---------------------------------------\')\n\nif __name__ == ""__main__"":\n    main()\n'"
tests/mujoco_test.py,6,"b'import os\nimport gym\nimport argparse\nimport numpy as np\nimport torch\nfrom common.networks import *\n\n# Configurations\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--env\', type=str, default=\'HalfCheetah-v2\', \n                    help=\'choose an environment between HalfCheetah-v2, Ant-v2 and Humanoid-v2\')\nparser.add_argument(\'--algo\', type=str, default=\'atac\',\n                    help=\'select an algorithm among vpg, trpo, ppo, ddpg, td3, sac, asac, tac, atac\')\nparser.add_argument(\'--load\', type=str, default=None,\n                    help=\'copy & paste the saved model name, and load it (ex. --load=Humanoid-v2_...)\')\nparser.add_argument(\'--render\', action=""store_true"", default=True,\n                    help=\'if you want to render, set this to True\')\nparser.add_argument(\'--test_eps\', type=int, default=10000,\n                    help=\'testing episode number\')\nparser.add_argument(\'--gpu_index\', type=int, default=0)\nargs = parser.parse_args()\ndevice = torch.device(\'cuda\', index=args.gpu_index) if torch.cuda.is_available() else torch.device(\'cpu\')\n\n\ndef main():\n    """"""Main.""""""\n    env = gym.make(args.env)\n    obs_dim = env.observation_space.shape[0]\n    act_dim = env.action_space.shape[0]\n\n    if args.algo == \'trpo\' or args.algo == \'ppo\':\n        mlp = GaussianPolicy(obs_dim, act_dim).to(device)\n    elif args.algo == \'ddpg\' or args.algo == \'td3\':\n        mlp = MLP(obs_dim, act_dim, hidden_sizes=(300,300), output_activation=torch.tanh).to(device)\n    elif args.algo == \'sac\' or args.algo == \'asac\' or args.algo == \'tac\' or args.algo == \'atac\':\n        mlp = ReparamGaussianPolicy(obs_dim, act_dim, hidden_sizes=(300,300)).to(device)\n\n    if args.load is not None:\n        pretrained_model_path = os.path.join(\'./save_model/\' + str(args.load))\n        pretrained_model = torch.load(pretrained_model_path, map_location=device)\n        mlp.load_state_dict(pretrained_model)\n\n    test_sum_returns = 0.\n    test_num_episodes = 0\n\n    for episode in range(1, args.test_eps+1):\n        total_reward = 0.\n\n        obs = env.reset()\n        done = False\n\n        while not done:\n            if args.render:\n                env.render()\n            \n            if args.algo == \'trpo\' or args.algo == \'ppo\':\n                action, _, _, _ = mlp(torch.Tensor(obs).to(device))\n                action = action.detach().cpu().numpy()\n            elif args.algo == \'ddpg\' or args.algo == \'td3\':\n                action = mlp(torch.Tensor(obs).to(device)).detach().cpu().numpy()\n            elif args.algo == \'sac\' or args.algo == \'asac\' or args.algo == \'tac\' or args.algo == \'atac\':\n                action, _, _ = mlp(torch.Tensor(obs).to(device))\n                action = action.detach().cpu().numpy()\n            \n            next_obs, reward, done, _ = env.step(action)\n            \n            total_reward += reward\n            obs = next_obs\n        \n        test_sum_returns += total_reward\n        test_num_episodes += 1\n\n        test_average_return = test_sum_returns / test_num_episodes if test_num_episodes > 0 else 0.0\n\n        if episode % 10 == 0:\n            print(\'---------------------------------------\')\n            print(\'Episodes:\', test_num_episodes)\n            print(\'TestAverageReturn:\', test_average_return)\n            print(\'---------------------------------------\')\n\nif __name__ == ""__main__"":\n    main()\n'"
